{
  "title": "Unsupervised morph segmentation and statistical language models for vocabulary expansion",
  "url": "https://openalex.org/W2518150831",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5030484123",
      "name": "Matti Varjokallio",
      "affiliations": [
        null,
        "Saarland University",
        "Aalto University"
      ]
    },
    {
      "id": "https://openalex.org/A5008875255",
      "name": "Dietrich Klakow",
      "affiliations": [
        null,
        "Saarland University",
        "Aalto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2069712814",
    "https://openalex.org/W2165921245",
    "https://openalex.org/W2053306448",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2166850712",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W154368987",
    "https://openalex.org/W2250618788",
    "https://openalex.org/W2028148926",
    "https://openalex.org/W2156700117",
    "https://openalex.org/W1887100226",
    "https://openalex.org/W2152561112",
    "https://openalex.org/W4249773576",
    "https://openalex.org/W1968536655",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W1987408080",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2117621558"
  ],
  "abstract": "This work explores the use of unsupervised morph segmentation along with statistical language models for the task of vocabulary expansion.Unsupervised vocabulary expansion has large potential for improving vocabulary coverage and performance in different natural language processing tasks, especially in lessresourced settings on morphologically rich languages.We propose a combination of unsupervised morph segmentation and statistical language models and evaluate on languages from the Babel corpus.The method is shown to perform well for all the evaluated languages when compared to the previous work on the task.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 175–180,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nUnsupervised morph segmentation and statistical language models\nfor vocabulary expansion\nMatti Varjokallio∗\nDept. of Signal Processing and Acoustics\nAalto University\nEspoo, Finland\nmatti.varjokallio@aalto.fi\nDietrich Klakow\nSpoken Language Systems\nSaarland University\nSaarbr¨ucken, Germany\ndietrich.klakow@lsv.uni-saarland.de\nAbstract\nThis work explores the use of unsu-\npervised morph segmentation along with\nstatistical language models for the task\nof vocabulary expansion. Unsupervised\nvocabulary expansion has large poten-\ntial for improving vocabulary coverage\nand performance in different natural lan-\nguage processing tasks, especially in less-\nresourced settings on morphologically rich\nlanguages. We propose a combination of\nunsupervised morph segmentation and sta-\ntistical language models and evaluate on\nlanguages from the Babel corpus. The\nmethod is shown to perform well for all\nthe evaluated languages when compared to\nthe previous work on the task.\n1 Introduction\nLanguage modelling for different natural language\nprocessing tasks like speech recognition, machine\ntranslation or optical character recognition require\nlarge training corpora to achieve good language\nmodel estimates and high enough vocabulary cov-\nerage. Sometimes such resources are not readily\navailable or easily acquirable. This is especially\nthe case for the many less-resourced languages.\nIn the case of morphologically rich languages,\nthese issues are emphasized, as words appear in\nmany forms, thus increasing the required vocabu-\nlary size and the data sparsity. Automatic speech\nrecognition of spontaneous speech is a task with\nsome special characteristics, as speech transcrip-\ntions are expensive to acquire. Taking all these\nfactors into account, the importance of making the\nmost out of the available resources becomes evi-\ndent.\nThis work was done while the author was visiting the\nSaarland University Spoken Language Systems group\nPrevious work on handling out-of-vocabulary\n(OOV) words in automatic speech recognition\nhave included explicit OOV word modelling and\nconﬁdence measures (Hazen and Bazzi, 2001)\nand hybrid word-subword language modelling for\nOOV word detection (Yazgan and Sarac ¸lar, 2004).\nSpeech recognition by directly using optimized\nsubword units has also (Kneissler and Klakow,\n2001) proven a good approach for speech recog-\nnition of a morphologically rich language.\nIn this work, we study unsupervised vocabu-\nlary expansion for conversational speech recogni-\ntion of morphologically rich languages in a less-\nresourced setting. We expand the recognition vo-\ncabulary, and thus lower the OOV rate, by generat-\ning new word forms. Two recent works also target\nthe unsupervised vocabulary expansion.\nIn (Rasooli et al., 2014), an unsupervised mor-\nphological segmentation was inferred from the\ntraining corpus using the Morfessor Categories-\nMAP (Creutz and Lagus, 2007) method. The\npreﬁx-stem-sufﬁx structure estimated by the\nmodel was then represented as a ﬁnite-state-\ntransducer for sampling new word forms. Differ-\nent reranking schemes using a bigram language\nmodel and a letter trigraph language model were\nevaluated.\nThe Kaldi speech recognition package (Povey\net al., 2011) includes an approach (Trmal et al.,\n2014) for vocabulary expansion. In this approach,\nthe provided syllable segmented pronunciation\nlexicon is used as the basis for the expansion. An\nn-gram model is trained over the syllable segmen-\ntation and syllabic words are generated from the\nmodel. Finally a phoneme-to-grapheme mapping\nis performed to obtain the grapheme form for the\nwords.\nIn our approach, statistical language models\nare trained over a morph segmentation, which is\nlearned unsupervisedly from the data. Words are\n175\nsampled from the language models and ordered\naccording to the probabilities given by the lan-\nguage models. We evaluate the method on seven\nmorphologically rich languages from the Babel\n(Harper, 2013) corpus and compare to the previ-\nously suggested approaches.\n2 Suggested method\nWe present a combination of unsupervised morph\nsegmentation and statistical language models for\nunsupervised vocabulary expansion. The sug-\ngested approach operates in four steps: unsu-\npervised morph segmentation, statistical language\nmodel training, sampling of new word types and\nreranking of the sampled words. The phases are\ndescribed in more detail in the corresponding sub-\nsections.\n2.1 Unsupervised morph segmentation\nMorfessor Baseline (Creutz and Lagus, 2002) is a\nmethod for unsupervised morphological segmen-\ntation. The algorithm optimizes a two-part min-\nimum description length code, ﬁnding a balance\nbetween the cost of encoding the training corpus\nand the lexicon, as in Formula 1.\narg min\nθ\nL(x,θ) = arg minL(x|θ) +L(θ) (1)\nThe corpus encoding is based on a unigram\nmodel. A so-called α-term may be used for ﬁne-\ntuning the corpus encoding cost. For the experi-\nments in this work, a recent Python implementa-\ntion Morfessor 2.0 (Smit et al., 2014) was used.\n2.2 Statistical language models over morphs\nAs statistical language models, two state-of-the-\nart models were selected. These language models\nwere trained on a corpus, where one segmented\nword was treated as what would in normal lan-\nguage model training be a sentence. The train-\ning was done using log-weighted word frequen-\ncies, thus some words appearing multiple times\nin the training corpus. The rationale of the log-\nweighting was to slightly emphasize the most\ncommon words. As a last step, the order of the\ntraining words was randomized.\nThe ﬁrst model was a trigram model trained\nwith the modiﬁed Kneser-Ney smoothing (Kneser\nand Ney, 1995) using three discounts per order.\nThe discount parameters could normally be op-\ntimized on a held-out-set, but here leave-one-out\nestimates were used, as it is not clear what would\nin this case constitute a reasonable held-out set.\nThe model was trained using the VariKN software\npackage (Siivola et al., 2007).\nIt has recently been shown, that the recurrent\nneural network language models may efﬁciently\nbe trained using the backpropagation algorithm\n(Mikolov et al., 2010), making it also an appealing\nchoice for language modelling. As the second lan-\nguage model, a recurrent neural network language\nmodel was trained using the RNNLM toolkit. The\nwords were treated as independent of the preceed-\ning words in both the model training and the word\nsampling phases.\n2.3 Sampling and reranking\nThe initial set of candidate words was obtained by\nsampling separately from both the n-gram model\nand the recurrent neural network language model.\nThese word lists were then merged. It is very im-\nportant to rerank the obtained word list, as the goal\nis to improve the OOV rate as much as possible\nwith introducing as little incorrect words as possi-\nble to the vocabulary. As the ﬁnal estimate on the\nword likelihood, the linear interpolation of these\ntwo model scores was used. The linear interpola-\ntion was applied morph-wise. The list of the sam-\npled words was sorted in descending order with\nthe linearly interpolated likelihood as the score.\n3 Experiments\n3.1 Training corpus\nThe vocabulary expansion experiments were con-\nducted on the Babel corpus (DARPA, 2013). The\nexperiments were run on the following set of\nlanguages: Assamese, Bengali, Pashto, Tagalog,\nTamil, Turkish and Zulu. The training corpora\nconsist mainly of conversation transcriptions, but\nalso additional scripted data is provided. Including\nthe scripted training data in general helps to lower\nthe OOV rate. The OOV reduction rate reachable\nby the vocabulary expansion then becomes, with\nsome exceptions, slightly slower. Statistics of the\ndatasets are in the Table 1.\nAs preprocessing, all special symbols were re-\nmoved from the texts. Asterisk symbols are used\nto denote misspellings in cases where the real\nword was identiﬁable. Asterisk symbols were re-\nmoved and the words included in the training cor-\npus. Dash symbols in the beginning and end of\n176\nLanguage Training data Development data\nTypes Tokens Types Tokens Type OOV% Token OOV%\nAssamese 8738 73284 7309 66357 49.75 8.36\nBengali 9507 81564 7844 70724 50.90 8.56\nPashto 7027 115225 6174 108273 44.91 4.26\nTagalog 6370 69791 5614 64506 55.61 8.13\nTamil 16284 76916 14279 70429 65.08 16.89\nTurkish 12147 77310 9944 67171 57.25 12.53\nZulu 16008 65821 13848 57217 68.88 21.91\nTable 1: Statistics of the datasets used in the experiments. The scripted training corpus is included.\na word are used to indicate hesitations. These\nwords were removed from the training corpus.\nOnly proper names were written in uppercase in\nthe transcriptions, so these words were kept intact.\n3.2 Expansion model\nAs statistical language models, we evaluated a tri-\ngram language model, a recurrent neural network\nlanguage model, and the linear interpolation of\nthese models. 10 million new distinct word types\nwere sampled from both the models separately.\nThese lists were then merged and reranked as ex-\nplained in the Section 2.3.\nThe model parameters were optimized on se-\nlected languages and these parameters were used\nin all the experiments. For the recurrent neural net-\nwork language model, the number of classes was\nset to 50 and the hidden layer size to 20. These\nvalues were reasonably close to optimum for all\nthe languages.\nThe suitable α-value for the Morfessor Baseline\nsegmentation was studied. With the default value\nof 1.0, the method seemed to suffer from a slight\nundersegmentation. To encourage the method to\nsegment more, the α value was set to 0.8. This\nsetting was equal or better for all the evaluated lan-\nguages.\nWhen evaluating the language models as stan-\ndalone models, the trigram model provided bet-\nter generation accuracy for 4 of the in total 7 lan-\nguages and the recurrent neural network language\nmodel for 3 of the languages. Linear interpolation\nof the models was without exceptions the most ac-\ncurate model. The linear interpolation weight was\nset to 0.5.\nFigure 1 shows an example of the OOV rate de-\nvelopment as a function of the extended vocabu-\nlary size for Turkish. The rapid improvement of\nthe OOV rate for small extensions and the superi-\nFigure 1: Token-based OOV rate as a function of\nthe extended vocabulary size for Turkish\n0 500000 1000000 1500000 2000000 2500000 3000000\nExtended vocabulary size\n0.04\n0.06\n0.08\n0.10\n0.12OOV rate\nTrigram model\nRecurrent neural network model\nLinear interpolation\nority of the linearly interpolated model are charac-\nteristics shared by all the languages.\n3.3 Comparison to the previous work\nWe compared the approach to the previous results\nin (Rasooli et al., 2014). They reported the results\nfor a vocabulary expansion of 50k best words. Ta-\nble 2 compares the type-based expansion results\nand Table 3 the token-based expansion results.\nModels for these comparisons were trained with\nthe scripted data included.\nLanguage Rasooli et al. Suggested method\nAssamese 28.46 31.93\nBengali 24.75 33.20\nPashto 19.43 32.95\nTagalog 16.81 21.27\nTamil - 16.27\nTurkish 14.79 28.32\nZulu 13.87 21.18\nTable 2: Type-based OOV reduction rates for the\n50k best words\n177\nLanguage Rasooli et al. Suggested method\nAssamese 29.43 35.17\nBengali 25.61 35.16\nPashto 21.27 35.55\nTagalog 16.88 23.75\nTamil - 19.24\nTurkish 17.82 31.89\nZulu 15.67 23.62\nTable 3: Token-based OOV reduction rates for the\n50k best words\nLanguage V ocabulary size Kaldi Suggested\nAssamese 845k 26.4 21.2\nBengali 834k 27.4 22.0\nPashto 494k 26.7 20.3\nTagalog 581k 37.5 33.2\nTamil 896k 45.2 38.0\nTurkish 704k 37.1 28.4\nZulu 818k 40.7 37.0\nTable 4: Type-based OOV rate comparison to\nKaldi\nLanguage V ocabulary size Kaldi Suggested\nAssamese 845k 4.3 3.5\nBengali 834k 4.6 3.6\nPashto 494k 2.4 1.9\nTagalog 581k 5.3 4.6\nTamil 896k 11.2 9.1\nTurkish 704k 7.9 6.0\nZulu 818k 12.5 11.4\nTable 5: Token-based OOV rate comparison to\nKaldi\nWe ran the Kaldi vocabulary expansion in the\nlimited language pack setting as in (Trmal et al.,\n2014). In the default setting, around 1M dis-\ntinct syllabic words are generated and converted\nby a phoneme-to-grapheme mapping to obtain the\ngraphemic word form. Table 4 compares the type-\nbased expansion results and Table 5 the token-\nbased expansion results for a vocabulary expan-\nsion of similar size (in graphemic words). The\nscripted data was not used in training the models\nfor these comparisons.\n3.4 OOV reduction and type to token ratio\nThe OOV reduction was evaluated as a function\nof the type/token ratio. This analysis may provide\ninformation about the properties of the evaluated\nlanguages. The token-based analysis is in the Fig-\nure 2 and the type-based analysis in the Figure 3.\nAs the type/token ratio is dependent on the number\nof tokens, these values are computed on a matched\nnumber of tokens (65821) from the training cor-\npus. The plots show that there are similarities, but\nalso big differences between the languages. Most\nnotable exceptions seem to be Tamil and Tagalog.\nFor Tamil, the number of the most frequent words\nwas lower with a slightly more even tail of less\nfrequent words. For Tagalog, the average number\nof morphs per word as estimated by the Morfessor\nBaseline algorithm was 2.8, which was the highest\nvalue among all the languages. Still, the number\nof distinct word types in the training set was the\nlowest. These properties seem to play a role in the\ndifferent vocabulary expansion characteristics.\nFigure 2: Token-based OOV reduction rate for 50k\nword expansion as a function of type/token ratio\n0.00 0.05 0.10 0.15 0.20 0.25 0.30\nType to token ratio\n15\n20\n25\n30\n35\n40Token-based OOV reduction rate (%)\nAssameseBengali\nPashto\nTagalog\nTamil\nTurkish\nZulu\nFigure 3: Type-based OOV reduction rate for 50k\nword expansion as a function of type/token ratio\n0.00 0.05 0.10 0.15 0.20 0.25 0.30\nType to token ratio\n15\n20\n25\n30\n35Type-based OOV reduction rate (%)\nAssamese\nBengaliPashto\nTagalog\nTamil\nTurkish\nZulu\n178\n4 Discussion\nThis work concerned the use of unsupervised mor-\nphological segmentation and statistical language\nmodels for the task of vocabulary expansion. Un-\nsupervised vocabulary expansion has large poten-\ntial for reducing OOV-rates and improving results\nin NLP tasks especially in less-resourced settings\nfor morphologically rich languages.\nThe suggested method was evaluated on some\nof the morphologically rich languages of the Ba-\nbel corpus in the limited language pack condition.\nThe performance of the method was evaluated in\nterms of the improvement of the OOV-rate on the\ndevelopment set. The suggested combination of\nsegmentation and interpolation of statistical lan-\nguage models provided to our understanding the\nbest results on the task so far. Compared to (Ra-\nsooli et al., 2014), our approach differed in that\nthe statistical language models were used directly\nin the word generation phase. As opposed to (Tr-\nmal et al., 2014), our approach operated purely on\nthe grapheme level.\nIt is perhaps noteworthy, that the methods are\nnot that different from what one would use in a\nnormal language modelling scenario for automatic\nspeech recognition. Morfessor Baseline (Creutz\nand Lagus, 2002) has been seen to give good re-\nsults in morph-based speech recognition (Creutz et\nal., 2007) when used along with standard n-gram\nmodels. If a larger training corpus is available, op-\ntimizing unigram likelihood more directly may be\na good choice (Varjokallio et al., 2013).\nMorph segmentations provided by the Morfes-\nsor Flatcat (Gr¨onroos and Virpioja, 2014) -method\nwere also evaluated for this work, but Morfessor\nBaseline was found to perform better. It is pos-\nsible, that the tradeoff between the lexicon cost\nand the corpus encoding cost, as given by the\nMinimum Description Length -principle, is impor-\ntant for the modelling accuracy in this type of a\nless-resourced scenario. Morfessor Flatcat will in\nmost cases segment more accurately according to\nthe grammatical morph boundaries. This is likely\na more valuable property for statistical machine\ntranslation than for the present task.\nThe linear interpolation of an n-gram model\nand a recurrent neural network language model\nprovides at the moment state-of-the-art modelling\naccuracy in many statistical language modelling\ntasks. Some forms of class n-grams were also\nevaluated for this work. Sampling from a class n-\ngram provided many complementary word forms,\nnot easily generated by the other models. How-\never, it became successively harder to improve the\nOOV reduction rates by a combination of three\nmodels.\nThis work concentrated only on methods for\nexpanding the vocabulary. Naturally some lan-\nguage modelling methods are required to utilize\nthese generated words in speech recognition or\nsome other task. One possibility is to extend\nthe unknown symbol and improve the obtained\nestimates via class n-gram models (Trmal et al.,\n2014). Morph-based language models may be uti-\nlized using a constrained vocabulary as suggested\nin (Varjokallio and Kurimo, 2014). In this case\nword-level pronunciation variants may be applied.\nPerforming the vocabulary expansion may also\nprovide insights into unlimited vocabulary speech\nrecognition (Kneissler and Klakow, 2001; Hir-\nsim¨aki et al., 2006) with morph language mod-\nels. Finding units with consistent grapheme-to-\nphoneme mapping may, however, be challenging\nfor some of the Babel languages.\nRegarding the type of approaches considered in\nthis work, it is possible that advances in either un-\nsupervised morph segmentation or statistical lan-\nguage models could bring about further improve-\nments in the expansion accuracy. Unsupervised\nlearning of morphological paradigms is also a po-\ntential direction when seeking for improvements\nin the task.\n5 Conclusion\nUnsupervised vocabulary expansion has great po-\ntential for reducing out-of-vocabulary rates and\nimproving results in different natural language\nprocessing tasks, including ASR. In this work,\nan approach comprising of unsupervised morph\nsegmentation and statistical language models was\nsuggested. The model was evaluated on the Babel\nlanguages and was shown to give large improve-\nments compared to the previous work on the task.\nAcknowledgments\nThis research was conducted while the ﬁrst au-\nthor was visiting the Saarland University Spoken\nLanguage Systems group. The work was partially\nfunded by the Saarland University SFB1102 Col-\nlaborative Research Center for Information Den-\nsity and Linguistic Encoding.\n179\nReferences\nMathias Creutz and Krista Lagus. 2002. Unsupervised\nDiscovery of Morphemes. In Proceedings of the\nACL-02 Workshop on Morphological and Phonolog-\nical Learning - Volume 6, pages 21–30.\nMathias Creutz and Krista Lagus. 2007. Unsupervised\nModels for Morpheme Segmentation and Morphol-\nogy Learning. ACM Transactions on Speech and\nLanguage Processing, 4(1), January.\nMathias Creutz, Teemu Hirsim ¨aki, Mikko Kurimo,\nAntti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti\nVarjokallio, Ebru Arisoy, Murat Sarac ¸lar, and An-\ndreas Stolcke. 2007. Morph-based Speech Recog-\nnition and Modeling of Out-of-vocabulary Words\nAcross Languages. ACM Transactions on Speech\nand Language Processing , 5(1):3:1–3:29, Decem-\nber.\nDARPA, 2013. IARPA Babel Data Speciﬁcations for\nPerformers.\nStig-Arne Gr¨onroos and Sami Virpioja. 2014. Morfes-\nsor FlatCat: An HMM-Based Method for Unsuper-\nvised and Semi-Supervised Learning of Morphol-\nogy. In Proceedings of the 25th International Con-\nference on Computational Linguistics, pages 1177–\n1185, Dublin, Ireland, August.\nMary Harper. 2013. The Babel Program and Low\nResource Speech Technology. Automatic Speech\nRecognition and Understanding Workshop (ASRU),\nInvited talk.\nTimothy J. Hazen and Issam Bazzi. 2001. A Compar-\nison and Combination of Methods for OOV Word\nDetection and Word Conﬁdence Scoring. In Pro-\nceedings of the 2001 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), volume 1, pages 397–400.\nTeemu Hirsim¨aki, Mathias Creutz, Vesa Siivola, Mikko\nKurimo, Sami Virpioja, and Janne Pylkk¨onen. 2006.\nUnlimited V ocabulary Speech Recognition with\nMorph Language Models Applied to Finnish. Com-\nputer Speech and Language, 20(4):515–541.\nJan Kneissler and Dietrich Klakow. 2001. Speech\nRecognition for Huge V ocabularies by Using\nOptimized Sub-word Units. In Proceedings\nof the 2nd Annual Conference of the Interna-\ntional Speech Communication Association (INTER-\nSPEECH 2001), pages 69–72.\nReinhard Kneser and Hermann Ney. 1995. Improved\nBacking-off for M-gram Language Modeling. In\nProceedings of the 1995 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 181–184.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock´y, and Sanjeev Khudanpur. 2010. Re-\ncurrent Neural Network Based Language Model.\nIn Proceedings of the 11th Annual Conference of\nthe International Speech Communication Associa-\ntion (INTERSPEECH 2010), pages 1045–1048.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr\nSchwarz, Jan Silovsky, Georg Stemmer, and Karel\nVesely. 2011. The Kaldi Speech Recognition\nToolkit. In Proceedings of the IEEE 2011 Workshop\non Automatic Speech Recognition and Understand-\ning, December.\nMohammad Sadegh Rasooli, Thomas Lippincott, Nizar\nHabash, and Owen Rambow. 2014. Unsupervised\nMorphology-Based V ocabulary Expansion. In Pro-\nceedings of the 52nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1:\nLong Papers), pages 1349–1359, Baltimore, Mary-\nland, June.\nVesa Siivola, Teemu Hirsim ¨aki, and Sami Virpi-\noja. 2007. On Growing and Pruning Kneser-\nNey Smoothed N-gram Models. IEEE Transac-\ntions on Audio, Speech and Language Processing ,\n15(5):1617–1624, July.\nPeter Smit, Sami Virpioja, Stig-Arne Gr ¨onroos, and\nMikko Kurimo. 2014. Morfessor 2.0: Toolkit for\nStatistical Morphological Segmentation. In Pro-\nceedings of the 14th Conference of the European\nChapter of the Association for Computational Lin-\nguistics (EACL), pages 21–24, Gothenburg, Sweden,\nApril.\nJan Trmal, Guoguo Chen, Daniel Povey, Sanjeev Khu-\ndanpur, Pegah Ghahremani, Xiaohui Zhang, Vi-\nmal Manohar, Chunxi Liu, Aren Jansen, Dietrich\nKlakow, David Yarowsky, and Florian Metze. 2014.\nA Keyword Search System Using Open Source Soft-\nware. In Proceedings of the IEEE 2014 Workshop\non Spoken Language Technology, South Lake Tahoe,\nUSA, December.\nMatti Varjokallio and Mikko Kurimo. 2014. A Word-\nLevel Token-Passing Decoder for Subword n-gram\nLVCSR. In Proceedings of the IEEE 2014 Workshop\non Spoken Language Technology, South Lake Tahoe,\nUSA, December.\nMatti Varjokallio, Mikko Kurimo, and Sami Virpioja.\n2013. Learning a Subword V ocabulary Based on\nUnigram Likelihood. In Proceedings of the IEEE\n2013 Workshop on Automatic Speech Recognition\nand Understanding, Olomouc, Czech Republic, De-\ncember.\nAli Yazgan and Murat Sarac ¸lar. 2004. Hybrid Lan-\nguage Models for Out of V ocabulary Word De-\ntection in Large V ocabulary Conversational Speech\nRecognition. In Proceedings of the 2004 IEEE In-\nternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , volume 1, pages I–\n745–8.\n180",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8457764387130737
    },
    {
      "name": "Vocabulary",
      "score": 0.7850542068481445
    },
    {
      "name": "Natural language processing",
      "score": 0.729512095451355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7143625020980835
    },
    {
      "name": "Task (project management)",
      "score": 0.696631908416748
    },
    {
      "name": "Unsupervised learning",
      "score": 0.5883030295372009
    },
    {
      "name": "Language model",
      "score": 0.5519360303878784
    },
    {
      "name": "Segmentation",
      "score": 0.5436493754386902
    },
    {
      "name": "Natural language",
      "score": 0.4636818766593933
    },
    {
      "name": "Statistical model",
      "score": 0.4552764296531677
    },
    {
      "name": "Linguistics",
      "score": 0.1503334641456604
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}