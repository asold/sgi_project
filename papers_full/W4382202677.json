{
    "title": "TrOCR: Transformer-Based Optical Character Recognition with Pre-trained Models",
    "url": "https://openalex.org/W4382202677",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2126022197",
            "name": "Minghao Li",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A3113691377",
            "name": "Tengchao Lv",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2332691037",
            "name": "Jingye Chen",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2008438412",
            "name": "Lei Cui",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2159330496",
            "name": "Yijuan Lu",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2096808804",
            "name": "Dinei Florencio",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2109608981",
            "name": "Cha Zhang",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2133880114",
            "name": "Zhoujun Li",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A2171151462",
            "name": "Furu Wei",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2927058921",
        "https://openalex.org/W3134286374",
        "https://openalex.org/W3183301485",
        "https://openalex.org/W3185121530",
        "https://openalex.org/W2343436474",
        "https://openalex.org/W2785820430",
        "https://openalex.org/W2993466051",
        "https://openalex.org/W3185861731",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3135651738",
        "https://openalex.org/W2911295582",
        "https://openalex.org/W7027429494",
        "https://openalex.org/W6704491142",
        "https://openalex.org/W1491389626",
        "https://openalex.org/W6652761251",
        "https://openalex.org/W2980094293",
        "https://openalex.org/W3013224334",
        "https://openalex.org/W2997045286",
        "https://openalex.org/W2922477773",
        "https://openalex.org/W2049951199",
        "https://openalex.org/W3103667010",
        "https://openalex.org/W1987937363",
        "https://openalex.org/W2146835493",
        "https://openalex.org/W1971822075",
        "https://openalex.org/W2995752927",
        "https://openalex.org/W2807209303",
        "https://openalex.org/W2194187530",
        "https://openalex.org/W2298368322",
        "https://openalex.org/W2810983211",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2573601984",
        "https://openalex.org/W2997558659",
        "https://openalex.org/W6649973027",
        "https://openalex.org/W2966790500",
        "https://openalex.org/W6772198947",
        "https://openalex.org/W3195960576",
        "https://openalex.org/W3165726853",
        "https://openalex.org/W3014074635",
        "https://openalex.org/W3042537721",
        "https://openalex.org/W2905498003",
        "https://openalex.org/W3042760913",
        "https://openalex.org/W3034784202",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034414401",
        "https://openalex.org/W4312879041",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2998382406",
        "https://openalex.org/W3003642782",
        "https://openalex.org/W3202912918",
        "https://openalex.org/W3004846386",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2343052201",
        "https://openalex.org/W3200351427",
        "https://openalex.org/W2964099383",
        "https://openalex.org/W3035449864",
        "https://openalex.org/W1998042868",
        "https://openalex.org/W2963517393",
        "https://openalex.org/W3181186176",
        "https://openalex.org/W2997749585",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W3202415716",
        "https://openalex.org/W4287183355",
        "https://openalex.org/W3143013427",
        "https://openalex.org/W2992430205",
        "https://openalex.org/W3204479434",
        "https://openalex.org/W4226374800",
        "https://openalex.org/W2998621280",
        "https://openalex.org/W2008806374",
        "https://openalex.org/W3110267192",
        "https://openalex.org/W4306292341",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W2517355551",
        "https://openalex.org/W3045882047",
        "https://openalex.org/W4287281076",
        "https://openalex.org/W2144554289",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3201786596",
        "https://openalex.org/W3152720259",
        "https://openalex.org/W4308291331",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W2786974559",
        "https://openalex.org/W2963712589",
        "https://openalex.org/W2964325005",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W4320855770",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W3003428515",
        "https://openalex.org/W4289751412",
        "https://openalex.org/W3170874841"
    ],
    "abstract": "Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at https://aka.ms/trocr.",
    "full_text": "TrOCR: Transformer-Based Optical Character Recognition\nwith Pre-trained Models\nMinghao Li1*, Tengchao Lv2, Jingye Chen2∗, Lei Cui2,\nYijuan Lu2, Dinei Florencio2, Cha Zhang2, Zhoujun Li1, Furu Wei2\n1Beihang University\n2Microsoft Corporation\n{liminghao1630, lizj}@buaa.edu.cn\n{tengchaolv, v-jingyechen, lecu, yijlu, dinei, chazhang, fuwei}@microsoft.com\nAbstract\nText recognition is a long-standing research problem for doc-\nument digitalization. Existing approaches are usually built\nbased on CNN for image understanding and RNN for char-\nlevel text generation. In addition, another language model is\nusually needed to improve the overall accuracy as a post-\nprocessing step. In this paper, we propose an end-to-end text\nrecognition approach with pre-trained image Transformer and\ntext Transformer models, namely TrOCR, which leverages\nthe Transformer architecture for both image understanding\nand wordpiece-level text generation. The TrOCR model is\nsimple but effective, and can be pre-trained with large-scale\nsynthetic data and fine-tuned with human-labeled datasets.\nExperiments show that the TrOCR model outperforms the\ncurrent state-of-the-art models on the printed, handwritten\nand scene text recognition tasks. The TrOCR models and\ncode are publicly available at https://aka.ms/trocr.\nIntroduction\nOptical Character Recognition (OCR) is the electronic or\nmechanical conversion of images of typed, handwritten or\nprinted text into machine-encoded text, whether from a\nscanned document, a photo of a document, a scene photo\nor from subtitle text superimposed on an image. Typically,\nan OCR system includes two main modules: a text detec-\ntion module and a text recognition module. Text detection\naims to localize all text blocks within the text image, ei-\nther at word-level or textline-level. The text detection task\nis usually considered as an object detection problem where\nconventional object detection models such as YoLOv5 and\nDBNet (Liao et al. 2019) can be applied. Meanwhile, text\nrecognition aims to understand the text image content and\ntranscribe the visual signals into natural language tokens.\nThe text recognition task is usually framed as an encoder-\ndecoder problem where existing methods leveraged CNN-\nbased encoder for image understanding and RNN-based de-\ncoder for text generation. In this paper, we focus on the text\nrecognition task for document images and leave text detec-\ntion as the future work.\n*Work done during internship at Microsoft Research Asia.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nRecent progress in text recognition (Diaz et al. 2021)\nhas witnessed the significant improvements by taking ad-\nvantage of the Transformer (Vaswani et al. 2017) architec-\ntures. However, existing methods are still based on CNNs\nas the backbone, where the self-attention is built on top of\nCNN backbones as encoders to understand the text image.\nFor decoders, Connectionist Temporal Classification (CTC)\n(Graves et al. 2006) is usually used compounded with an\nexternal language model on the character-level to improve\nthe overall accuracy. Despite the great success achieved by\nthe hybrid encoder/decoder method, there is still a lot of\nroom to improve with pre-trained CV and NLP models:\n1) the network parameters in existing methods are trained\nfrom scratch with synthetic/human-labeled datasets, leav-\ning large-scale pre-trained models unexplored. 2) as image\nTransformers become more and more popular (Dosovitskiy\net al. 2021; Touvron et al. 2020), especially the recent self-\nsupervised image pre-training (Bao, Dong, and Wei 2021),\nit is straightforward to investigate whether pre-trained im-\nage Transformers can replace CNN backbones, meanwhile\nexploiting the pre-trained image Transformers to work to-\ngether with the pre-trained text Transformers in a single\nframework on the text recognition task.\nTo this end, we propose TrOCR, an end-to-end\nTransformer-based OCR model for text recognition with\npre-trained CV and NLP models, which is shown in Fig-\nure 1. Distinct from the existing text recognition models,\nTrOCR is a simple but effective model which does not use\nthe CNN as the backbone. Instead, following (Dosovitskiy\net al. 2021), it first resizes the input text image into384×384\nand then the image is split into a sequence of16×16 patches\nwhich are used as the input to image Transformers. Stan-\ndard Transformer architecture with the self-attention mecha-\nnism is leveraged on both encoder and decoder parts, where\nwordpiece units are generated as the recognized text from\nthe input image. To effectively train the TrOCR model, the\nencoder can be initialized with pre-trained ViT-style mod-\nels (Dosovitskiy et al. 2021; Touvron et al. 2020; Bao,\nDong, and Wei 2021) while the decoder can be initialized\nwith pre-trained BERT-style models (Devlin et al. 2019;\nLiu et al. 2019; Dong et al. 2019; Wang et al. 2020b), re-\nspectively. Therefore, the advantage of TrOCR is three-fold.\nFirst, TrOCR uses the pre-trained image Transformer and\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n13094\n……\n……1 2 3 4 5 6 37 38 39 40\n……H1 H2 H3 H4 H5 H6 H37 H38 H39 H40\n[BOS] L IC ENSE\nInput ImageImage Patches\nFlatten\nPatch \nEmbedding\nPosition \nEmbedding\nOutputs (shifted right)\nOutputs\nFeed Forward\nMulti-Head Attention\nx N\nEncoder\nFeed Forward\nMasked Multi-Head Attention\nx NMulti-Head Attention\nDecoder\nE _OF _M CD ON ALD S’\nL IC ENSE E _OF _M CD ON ALD S’ [EOS]\nFigure 1: The architecture of TrOCR, where an encoder-decoder model is designed with a pre-trained image Transformer as\nthe encoder and a pre-trained text Transformer as the decoder.\ntext Transformer models, which take advantages of large-\nscale unlabeled data for image understanding and language\nmodeling, with no need for an external language model. Sec-\nond, TrOCR does not require any convolutional network for\nthe backbone and does not introduce any image-specific in-\nductive biases, which makes the model very easy to im-\nplement and maintain. Finally, experiment results on OCR\nbenchmark datasets show that the TrOCR can achieve state-\nof-the-art results on printed, handwritten and scene text im-\nage datasets without any complex pre/post-processing steps.\nFurthermore, we can easily extend the TrOCR for multi-\nlingual text recognition with minimum efforts, where just\nleveraging multilingual pre-trained models in the decoder-\nside and expand the dictionary.\nThe contributions of this paper are summarized as fol-\nlows:\n1. We propose TrOCR, an end-to-end Transformer-based\nOCR model for text recognition with pre-trained CV and\nNLP models. To the best of our knowledge, this is the\nfirst work that jointly leverages pre-trained image and\ntext Transformers for the text recognition task in OCR.\n2. TrOCR achieves state-of-the-art results with a stan-\ndard Transformer-based encoder-decoder model, which\nis convolution free and does not rely on any complex\npre/post-processing steps.\n3. The TrOCR models and code are publicly available at\nhttps://aka.ms/trocr.\nTrOCR\nModel Architecture\nTrOCR is built up with the Transformer architecture, includ-\ning an image Transformer for extracting the visual features\nand a text Transformer for language modeling. We adopt the\nvanilla Transformer encoder-decoder structure in TrOCR.\nThe encoder is designed to obtain the representation of the\nimage patches and the decoder is to generate the wordpiece\nsequence with the guidance of the visual features and previ-\nous predictions.\nEncoder The encoder receives an input image ximg ∈\nℜ3×H0×W0 , and resizes it to a fixed size (H, W). Since the\nTransformer encoder cannot process the raw images unless\nthey are a sequence of input tokens, the encoder decomposes\nthe input image into a batch of N = HW/P 2 foursquare\npatches with a fixed size of (P, P), while the width W and\nthe height H of the resized image are guaranteed to be divis-\nible by the patch size P. Subsequently, the patches are flat-\ntened into vectors and linearly projected to D-dimensional\nvectors, aka the patch embeddings. D is the hidden size of\nthe Transformer through all of its layers.\nSimilar to ViT (Dosovitskiy et al. 2021) and DeiT (Tou-\nvron et al. 2020), we keep the special token “[CLS]” that\nis usually used for image classification tasks. The “[CLS]”\ntoken brings together the information from all the patch em-\nbeddings and represents the whole image. Meanwhile, we\nalso keep the distillation token in the input sequence when\nusing the DeiT pre-trained models for encoder initialization,\nwhich allows the model to learn from the teacher model. The\npatch embeddings and special tokens are given learnable 1D\nposition embeddings according to their absolute positions.\nUnlike the features extracted by the CNN-like network,\nthe Transformer models have no image-specific inductive bi-\nases and process the image as a sequence of patches, which\nmakes the model easier to pay different attention to either\nthe whole image or the independent patches.\nDecoder We use the original Transformer decoder for\nTrOCR. The standard Transformer decoder also has a stack\nof identical layers, which have similar structures to the\nlayers in the encoder, except that the decoder inserts the\n“encoder-decoder attention” between the multi-head self-\nattention and feed-forward network to distribute different at-\n13095\ntention on the output of the encoder. In the encoder-decoder\nattention module, the keys and values come from the en-\ncoder output, while the queries come from the decoder in-\nput. In addition, the decoder leverages the attention masking\nin the self-attention to prevent itself from getting more in-\nformation during training than prediction. Based on the fact\nthat the output of the decoder will right shift one place from\nthe input of the decoder, the attention mask needs to ensure\nthe output for the position i can only pay attention to the\nprevious output, the input on the positions less than i:\nhi = Proj (Emb(Token i))\nσ(hij) = ehij\nPV\nk=1 ehik\nfor j= 1, 2, . . . , V\nThe hidden states from the decoder are projected by a lin-\near layer from the model dimension to the dimension of the\nvocabulary size V , while the probabilities over the vocabu-\nlary are calculated on that by the softmax function. We use\nbeam search to get the final output.\nModel Initialization\nBoth the encoder and the decoder are initialized by the pub-\nlic models pre-trained on large-scale labeled and unlabeled\ndatasets.\nEncoder Initialization The DeiT (Touvron et al. 2020)\nand BEiT (Bao, Dong, and Wei 2021) models are used for\nthe encoder initialization in the TrOCR models. DeiT trains\nthe image Transformer with ImageNet (Deng et al. 2009)\nas the sole training set. The authors try different hyper-\nparameters and data augmentation to make the model data-\nefficient. Moreover, they distill the knowledge of a strong\nimage classifier to a distilled token in the initial embedding,\nwhich leads to a competitive result compared to the CNN-\nbased models.\nReferring to the Masked Language Model pre-training\ntask, BEiT proposes the Masked Image Modeling task to\npre-train the image Transformer. Each image will be con-\nverted to two views: image patches and visual tokens. They\ntokenize the original image into visual tokens by the latent\ncodes of discrete V AE (Ramesh et al. 2021), randomly mask\nsome image patches, and make the model recover the orig-\ninal visual tokens. The structure of BEiT is the same as the\nimage Transformer and lacks the distilled token when com-\npared with DeiT.\nDecoder Initialization We use the RoBERTa (Liu et al.\n2019) models and the MiniLM (Wang et al. 2020b) models\nto initialize the decoder. Generally, RoBERTa is a replica-\ntion study of (Devlin et al. 2019) that carefully measures\nthe impact of many key hyperparameters and training data\nsize. Based on BERT, they remove the next sentence predic-\ntion objective and dynamically change the masking pattern\nof the Masked Language Model.\nThe MiniLM are compressed models of the large pre-\ntrained Transformer models while retaining 99% perfor-\nmance. Instead of using the soft target probabilities of\nmasked language modeling predictions or intermediate rep-\nresentations of the teacher models to guide the training of\nthe student models in the previous work. The MiniLM mod-\nels are trained by distilling the self-attention module of the\nlast Transformer layer of the teacher models and introducing\na teacher assistant to assist with the distillation.\nWhen loading the above models to the decoders, the struc-\ntures do not precisely match since both of them are only the\nencoder of the Transformer architecture. For example, the\nencoder-decoder attention layers are absent in these models.\nTo address this, we initialize the decoders with the RoBERTa\nand MiniLM models by manually setting the corresponding\nparameter mapping, and the absent parameters are randomly\ninitialized.\nTask Pipeline\nIn this work, the pipeline of the text recognition task is that\ngiven the textline images, the model extracts the visual fea-\ntures and predicts the wordpiece tokens relying on the image\nand the context generated before. The sequence of ground\ntruth tokens is followed by an “[EOS]” token, which indi-\ncates the end of a sentence. During training, we shift the\nsequence backward by one place and add the “[BOS]” to-\nken to the beginning indicating the start of generation. The\nshifted ground truth sequence is fed into the decoder, and\nthe output of that is supervised by the original ground truth\nsequence with the cross-entropy loss. For inference, the de-\ncoder starts from the “[BOS]” token to predict the output it-\neratively while continuously taking the newly generated out-\nput as the next input.\nPre-training\nWe use the text recognition task for the pre-training phase,\nsince this task can make the models learn the knowledge of\nboth the visual feature extraction and the language model.\nThe pre-training process is divided into two stages that differ\nby the used dataset. In the first stage, we synthesize a large-\nscale dataset consisting of hundreds of millions of printed\ntextline images and pre-train the TrOCR models on that.\nIn the second stage, we build two relatively small datasets\ncorresponding to printed and handwritten downstream tasks,\ncontaining millions of textline images each. We use the ex-\nisted and widely adopted synthetic scene text datasets for\nthe scene text recognition task. Subsequently, we pre-train\nseparate models on these task-specific datasets in the second\nstage, all initialized by the first-stage model.\nFine-tuning\nExcept for the experiments regarding scene text recognition,\nthe pre-trained TrOCR models are fine-tuned on the down-\nstream text recognition tasks. The outputs of the TrOCR\nmodels are based on Byte Pair Encoding (BPE) (Sennrich,\nHaddow, and Birch 2015) and SentencePiece (Kudo and\nRichardson 2018) and do not rely on any task-related vo-\ncabularies.\nData Augmentation\nWe leverage data augmentation to enhance the variety of the\npre-training and fine-tuning data. Six kinds of image trans-\n13096\nEncoder Decoder Precision Recall F1\nDeiTBASE RoBERTaBASE 69.28 69.06 69.17\nBEiTBASE RoBERTaBASE 76.45 76.18 76.31\nResNet50 RoBERTa BASE 66.74 67.29 67.02\nDeiTBASE RoBERTaLARGE 77.03 76.53 76.78\nBEiTBASE RoBERTaLARGE 79.67 79.06 79.36\nResNet50 RoBERTa LARGE 72.54 71.13 71.83\nTable 1: Ablation study on the SROIE dataset, where all the\nmodels are trained using the SROIE dataset only.\nModel Precision Recall F1\nFrom Scratch 38.06 38.43 38.24\n+ Pre-trained Model 72.95 72.56 72.75\n+ Data Augmentation 82.58 82.03 82.30\n+ First-Stage Pre-train 95.31 95.65 95.48\n+ Second-Stage Pre-train 95.76 95.91 95.84\nTable 2: Ablation study of pre-trained model initialization,\ndata augmentation and two stages of pre-training on the\nSROIE dataset.\nformations plus keeping the original are taken for printed\nand handwritten datasets, which are random rotation (-10\nto 10 degrees), Gaussian blurring, image dilation, image\nerosion, downscaling, and underlining. We randomly de-\ncide which image transformation to take with equal possi-\nbilities for each sample. For scene text datasets, RandAug-\nment (Cubuk et al. 2020) is applied following (Atienza\n2021), and the augmentation types include inversion, curv-\ning, blur, noise, distortion, rotation, etc.\nExperiments\nData\nPre-training Dataset To build a large-scale high-quality\ndataset, we sample two million document pages from the\npublicly available PDF files on the Internet. Since the PDF\nfiles are digital-born, we can get pretty printed textline im-\nages by converting them into page images and extracting the\ntextlines with their cropped images. In total, the first-stage\npre-training dataset contains 684M textlines.\nWe use 5,427 handwritten fonts1 to synthesize handwrit-\nten textline images by the TRDG 2, an open-source text\nrecognition data generator. The text used for generation is\ncrawled from random pages of Wikipedia. The handwritten\ndataset for the second-stage pre-training consists of 17.9M\ntextlines, including IIIT-HWS dataset (Krishnan and Jawa-\nhar 2016). In addition, we collect around 53K receipt im-\nages in the real world and recognize the text on them by\ncommercial OCR engines. According to the results, we\ncrop the textlines by their coordinates and rectify them into\nnormalized images. We also use TRDG to synthesize 1M\nprinted textline images with two receipt fonts and the built-\nin printed fonts. In total, the printed dataset consists of 3.3M\n1The fonts are obtained from https://fonts.google.com/\n?category=Handwriting and https://www.1001fonts.com/\nhandwritten-fonts.html.\n2https://github.com/Belval/TextRecognitionDataGenerator\ntextlines. The second-stage pre-training data for the scene\ntext recognition are MJSynth (MJ) (Jaderberg et al. 2014)\nand SynthText (ST) (Gupta, Vedaldi, and Zisserman 2016),\ntotaling about 16M text images.\nBenchmarks The SROIE (Scanned Receipts OCR and In-\nformation Extraction) dataset (Task 2) focuses on text recog-\nnition in receipt images. There are 626 receipt images and\n361 receipt images in the training and test sets of SROIE.\nSince the text detection task is not included in this work, we\nuse cropped images of the textlines for evaluation, which are\nobtained by cropping the whole receipt images according to\nthe ground truth bounding boxes.\nThe IAM Handwriting Database is composed of hand-\nwritten English text, which is the most popular dataset for\nhandwritten text recognition. We use the Aachen’s partition\nof the dataset3: 6,161 lines from 747 forms in the train set,\n966 lines from 115 forms in the validation set and 2,915 lines\nfrom 336 forms in the test set.\nRecognizing scene text images is more challenging\nthan printed text images, as many images in the wild\nsuffer from blur, occlusion, or low-resolution problems.\nHere we leverage some widely-used benchmarks, including\nIIIT5K-3000 (Mishra, Alahari, and Jawahar 2012), SVT-647\n(Wang, Babenko, and Belongie 2011), IC13-857, IC13-1015\n(Karatzas et al. 2013), IC15-1811, IC15-2077 (Karatzas\net al. 2015), SVTP-645 (Phan et al. 2013), and CT80-288\n(Risnumawan et al. 2014) to evaluate the capacity of TrOCR.\nModel Recall Precision F1\nCRNN 28.71 48.58 36.09\nTesseract OCR 57.50 51.93 54.57\nH&H Lab 96.35 96.52 96.43\nMSOLab 94.77 94.88 94.82\nCLOV A OCR 94.3 94.88 94.59\nTrOCRSMALL 95.89 95.74 95.82\nTrOCRBASE 96.37 96.31 96.34\nTrOCRLARGE 96.59 96.57 96.58\nTable 3: Evaluation results (word-level Precision, Recall,\nF1) on the SROIE dataset, where the baselines come\nfrom the SROIE leaderboard (https://rrc.cvc.uab.es/?ch=\n13&com=evaluation&task=2).\nSettings\nThe TrOCR models are built upon the Fairseq (Ott et al.\n2019) which is a popular sequence modeling toolkit. For the\nmodel initialization, the DeiT models are implemented and\ninitialized by the code and the pre-trained models from the\ntimm library (Wightman 2019) while the BEiT models and\nthe MiniLM models are from the UniLM’s official repos-\nitory4. The RoBERTa models come from the correspond-\ning page in the Fairseq GitHub repository. We use 32 V100\nGPUs with the memory of 32GBs for pre-training and 8\nV100 GPUs for fine-tuning. For all the models, the batch\nsize is set to 2,048 and the learning rate is 5e-5. We use the\n3https://github.com/jpuigcerver/Laia/tree/master/egs/iam\n4https://github.com/microsoft/unilm\n13097\nBPE and sentencepiece tokenizer from Fairseq to tokenize\nthe textlines to wordpieces.\nWe employ the384×384 resolution and 16×16 patch size\nfor DeiT and BEiT encoders. The DeiTSMALL has 12 layers\nwith 384 hidden sizes and 6 heads. Both the DeiT BASE and\nthe BEiTBASE have 12 layers with 768 hidden sizes and 12\nheads while the BEiTLARGE has 24 layers with 1024 hidden\nsizes and 16 heads. We use 6 layers, 256 hidden sizes and\n8 attention heads for the small decoders, 512 hidden sizes\nfor the base decoders and 12 layers, 1,024 hidden sizes and\n16 heads for the large decoders. For this task, we only use\nthe last half of all layers from the corresponding RoBERTa\nmodel, which are the last 6 layers for the RoBERTaBASE and\nthe last 12 layers for the RoBERTaLARGE. The beam size is\nset to 10 for TrOCR models.\nWe take the CRNN model (Shi, Bai, and Yao 2016) as the\nbaseline model. The CRNN model is composed of convo-\nlutional layers for image feature extraction, recurrent layers\nfor sequence modeling and the final frame label prediction,\nand a transcription layer to translate the frame predictions to\nthe final label sequence. To address the character alignment\nissue, they use the CTC loss to train the CRNN model. For\na long time, the CRNN model is the dominant paradigm for\ntext recognition. We use the PyTorch implementation 5 and\ninitialized the parameters by the provided pre-trained model.\nEvaluation Metrics\nThe SROIE dataset is evaluated using the word-level pre-\ncision, recall and f1 score. If repeated words appear in the\nground truth, they are also supposed to appear in the predic-\ntion. The precision, recall and f1 score are described as:\nPrecision = Correct matches\nThe number of the detected words\nRecall = Correct matches\nThe number of the ground truth words\nF1 = 2 × Precision × Recall\nPrecision + Recall\nThe IAM dataset is evaluated by the case-sensitive Char-\nacter Error Rate (CER). The scene text datasets are eval-\nuated by the Word Accuracy. For fair comparison, we filter\nthe final output string to suit the popular 36-character charset\n(lowercase alphanumeric) in this task.\nResults\nArchitecture Comparison We compare different combi-\nnations of the encoder and decoder to find the best settings.\nFor encoders, we compare DeiT, BEiT and the ResNet-50\nnetwork. Both the DeiT and BEiT are the base models in\ntheir original papers. For decoders, we compare the base de-\ncoders initialized by RoBERTaBASE and the large decoders\ninitialized by RoBERTaLARGE. For further comparison, we\nalso evaluate the CRNN baseline model and the Tesseract\nOCR in this section, while the latter is an open-source OCR\nEngine using the LSTM network.\n5https://github.com/meijieru/crnn.pytorch\nTable 1 shows the results of combined models. From\nthe results, we observe that the BEiT encoders show the\nbest performance among the three types of encoders while\nthe best decoders are the RoBERTa LARGE decoders. Ap-\nparently, the pre-trained models on the vision task im-\nprove the performance of text recognition models, and the\npure Transformer models are better than the CRNN mod-\nels and the Tesseract on this task. According to the re-\nsults, we mainly use three settings on the subsequent ex-\nperiments: TrOCRSMALL (total parameters=62M) consists\nof the encoder of DeiT SMALL and the decoder of MiniLM,\nTrOCRBASE (total parameters=334M) consists of the en-\ncoder of BEiT BASE and the decoder of RoBERTa LARGE,\nTrOCRLARGE (total parameters=558M) consists of the en-\ncoder of BEiTLARGE and the decoder of RoBERTaLARGE.\nIn Table 2, we have also done some ablation experiments to\nverify the effect of pre-trained model initialization, data aug-\nmentation, and two stages of pre-training. All of them have\ngreat improvements to the TrOCR models.\nSROIE Task 2 Table 3 shows the results of the TrOCR\nmodels and the current SOTA methods on the leaderboard of\nthe SROIE dataset. To capture the visual information, all of\nthese baselines leverage CNN-based networks as the feature\nextractors while the TrOCR models use the image Trans-\nformer to embed the information from the image patches.\nFor language modeling, MSO Lab (Sang and Cuong 2019)\nand CLOV A OCR (Sang and Cuong 2019) use LSTM lay-\ners and H&H Lab (Shi, Bai, and Yao 2016) use GRU lay-\ners while the TrOCR models use the Transformer decoder\nwith a pure attention mechanism. According to the results,\nthe TrOCR models outperform the existing SOTA mod-\nels with pure Transformer structures. It is also confirmed\nthat Transformer-based text recognition models get compet-\nitive performance compared to CNN-based networks in vi-\nsual feature extraction and RNN-based networks in language\nmodeling on this task without any complex pre/post-process\nsteps. There are some methods on the SROIE leaderboard\nthat use additional human-labeled data and utilize ensemble\nmethods that TrOCR does not use, which are not comparable\nand the results are not included in this paper.\nIAM Handwriting Database Table 4 shows the results of\nthe TrOCR models and the existing methods on the IAM\nHandwriting Database. According to the results, the meth-\nods with CTC decoders show good performance on this task\nand the external LM will result in a significant reduction\nin CER. By comparing the methods (Bluche and Messina\n2017) with the TrOCR models, the TrOCRLARGE achieves a\nbetter result, which indicates that the Transformer decoder is\nmore competitive than the CTC decoder in text recognition\nand has enough ability for language modeling instead of re-\nlying on an external LM. Most of the methods use sequence\nmodels in their encoders after the CNN-based backbone ex-\ncept the FCN encoders in (Wang et al. 2020a), which leads to\na significant improvement on CER. Instead of relying on the\nfeatures from the CNN-based backbone, the TrOCR mod-\nels using the information from the image patches get simi-\nlar and even better results, illustrating that the Transformer\nstructures are competent to extract visual features well after\n13098\nModel Architecture Training Data External LM CER\n(Bluche and Messina 2017) GCRNN / CTC Synthetic + IAM Yes 3.2\n(Michael et al. 2019) LSTM/LSTM w/Attn IAM No 4.87\n(Wang et al. 2020a) FCN / GRU IAM No 6.4\n(Kang et al. 2020) Transformer w/ CNN Synthetic + IAM No 4.67\n(Diaz et al. 2021) S-Attn / CTC Internal + IAM No 3.53\n(Diaz et al. 2021) S-Attn / CTC Internal + IAM Yes 2.75\n(Diaz et al. 2021) Transformer w/ CNN Internal + IAM No 2.96\nTrOCRSMALL Transformer Synthetic + IAM No 4.22\nTrOCRBASE Transformer Synthetic + IAM No 3.42\nTrOCRLARGE Transformer Synthetic + IAM No 2.89\nTable 4: Evaluation results (CER) on the IAM Handwriting dataset.\nModel Parameters Total Sentences Total Tokens Time Speed #Sentences Speed #Tokens\nTrOCRSMALL 62M 2,915 31,081 348.4s 8.37 sentences/s 89.22 tokens/s\nTrOCRBASE 334M 2,915 31,959 633.7s 4.60 sentences/s 50.43 tokens/s\nTrOCRLARGE 558M 2,915 31,966 666.8s 4.37 sentences/s 47.94 tokens/s\nTable 5: Inference time on the IAM Handwriting dataset.\npre-training. From the experiment results, the TrOCR mod-\nels exceed all the methods which only use synthetic/IAM as\nthe sole training set with pure Transformer structures and\nachieve a new state-of-the-art CER of 2.89. Without lever-\naging any extra human-labeled data, TrOCR even gets com-\nparable results with the methods in (Diaz et al. 2021) using\nthe additional internal human-labeled dataset.\nScene Text Datasets In Table 6, we compare the\nTrOCRBASE and TrOCRLARGE models of fine-tuning with\nsynthetic data only and fine-tuning with synthetic data and\nbenchmark datasets (the training sets of IC13, IC15, IIIT5K,\nSVT) to the popular and recent SOTA methods. Compared\nto all, the TrOCR models establish five new SOTA results of\neight experiments while getting comparable results on the\nrest. Our model underperforms on the IIIT5K dataset, and\nwe find some scene text sample images contain symbols, but\nthe ground truth does not. It is inconsistent with the behavior\nin our pre-training data (retaining symbols in ground truth),\ncausing the model to tend still to process symbols. There are\ntwo kinds of mistakes: outputting symbols but truncating the\noutput in advance to ensure that the number of wordpieces is\nconsistent with the ground truth, or identifying symbols as\nsimilar characters.\nInference Speed Table 5 shows the inference speed of\ndifferent settings TrOCR models on the IAM Handwriting\nDatabase. We can conclude that there is no significant mar-\ngin in inference speed between the base models and the\nlarge models. In contrast, the small model shows compara-\nble results for printed and handwriting text recognition even\nthough the number of parameters is an order of magnitude\nsmaller and the inference speed is as twice as fast. The low\nnumber of parameters and high inference speed means fewer\ncomputational resources and user waiting time, making it\nmore suitable for deployment in industrial applications.\nRelated Work\nScene Text Recognition\nFor text recognition, the most popular approaches are usu-\nally based on the CTC-based models. (Shi, Bai, and Yao\n2016) proposed the standard CRNN, an end-to-end archi-\ntecture combined by CNN and RNN. The convolutional lay-\ners are used to extract the visual features and convert them\nto sequence by concatenating the columns, while the recur-\nrent layers predict the per-frame labels. They use a CTC de-\ncoding strategy to remove the repeated symbols and all the\nblanks from the labels to achieve the final prediction. (Gao\net al. 2019) extracted the feature by the densely connected\nnetwork incorporating the residual attention block and cap-\nture the contextual information and sequential dependency\nby the CNN network. They compute the probability distri-\nbution on the output of the CNN network instead of using\nan RNN network to model them.\nThe Sequence-to-Sequence models (Zhang et al. 2020b;\nWang et al. 2019; Sheng, Chen, and Xu 2019; Bleeker and\nde Rijke 2019; Lee et al. 2020; Atienza 2021; Chen et al.\n2021) are gradually attracting more attention, especially af-\nter the advent of the Transformer architecture (Vaswani et al.\n2017). SaHAN (Zhang et al. 2020b), standing for the scale-\naware hierarchical attention network, are proposed to ad-\ndress the character scale-variation issue. The authors use the\nFPN network and the CRNN models as the encoder as well\nas a hierarchical attention decoder to retain the multi-scale\nfeatures. (Wang et al. 2019) extracted a sequence of visual\nfeatures from the input images by the CNN with attention\nmodule and BiLSTM. The decoder is composed of the pro-\nposed Gated Cascade Attention Module (GCAM). For the\nTransformer models, (Sheng, Chen, and Xu 2019) first ap-\nplied the Transformer to Scene Text Recognition. A CNN-\nbased modality-transform block is employed to transform\n2D input images to 1D sequences. (Bleeker and de Rijke\n2019) added a direction embedding to the input of the de-\ncoder for the bidirectional text decoding with a single de-\ncoder, while (Lee et al. 2020) utilized the two-dimensional\n13099\nTest datasets and # of samples\nModel IIIT5k SVT IC13 IC15 SVTP CUTE\n3,000 647 857 1,015 1,811 2,077 645 288\nPlugNet (Mou et al. 2020) 94.4 92.3 – 95.0 – 82.2 84.3 85.0\nSRN (Yu et al. 2020) 94.8 91.5 95.5 – 82.7 – 85.1 87.8\nRobustScanner (Yue et al. 2020) 95.4 89.3 – 94.1 – 79.2 82.9 92.4\nTextScanner (Wan et al. 2020) 95.7 92.7 – 94.9 – 83.5 84.8 91.6\nAutoSTR (Zhang et al. 2020a) 94.7 90.9 – 94.2 81.8 – 81.7 –\nRCEED (Cui et al. 2021) 94.9 91.8 – – – 82.2 83.6 91.7\nPREN2D (Yan et al. 2021) 95.6 94.0 96.4 – 83.0 – 87.6 91.7\nVisionLAN (Wang et al. 2021) 95.8 91.7 95.7 – 83.7 – 86.0 88.5\nBhunia (Bhunia et al. 2021b) 95.2 92.2 – 95.5 – 84.0 85.7 89.7\nCV AE-Feed.1 (Bhunia et al. 2021a) 95.2 – – 95.7 – 84.6 88.9 89.7\nSTN-CSTR (Cai, Sun, and Xiong 2021) 94.2 92.3 96.3 94.1 86.1 82.0 86.2 –\nViTSTR-B (Atienza 2021) 88.4 87.7 93.2 92.4 78.5 72.6 81.8 81.3\nCRNN (Shi, Bai, and Yao 2016) 84.3 78.9 – 88.8 – 61.5 64.8 61.3\nTRBA (Baek, Matsui, and Aizawa 2021) 92.1 88.9 – 93.1 – 74.7 79.5 78.2\nABINet (Fang et al. 2021) 96.2 93.5 97.4 – 86.0 – 89.3 89.2\nDiaz (Diaz et al. 2021) 96.8 94.6 96.0 – 80.4 – – –\nPARSeqA (Bautista and Atienza 2022) 97.0 93.6 97.0 96.2 86.5 82.9 88.9 92.2\nMaskOCR (ViT-B) (Lyu et al. 2022) 95.8 94.7 98.1 - 87.3 - 89.9 89.2\nMaskOCR (ViT-L) (Lyu et al. 2022) 96.5 94.1 97.8 - 88.7 - 90.2 92.7\nSVTR-L (Du et al. 2022) 96.3 91.7 97.2 - 86.6 - 88.4 95.1\nTrOCRBASE (Syn) 90.1 91.0 97.3 96.3 81.1 75.0 90.7 86.8\nTrOCRLARGE (Syn) 91.0 93.2 98.3 97.0 84.0 78.0 91.0 89.6\nTrOCRBASE (Syn+Benchmark) 93.4 95.2 98.4 97.4 86.9 81.2 92.1 90.6\nTrOCRLARGE (Syn+Benchmark) 94.1 96.1 98.4 97.3 88.1 84.1 93.0 95.1\nTable 6: Word accuracy on the six benchmark datasets (36-char), where “Syn” indicates the model using synthetic data only\nand “Syn+Benchmark” indicates the model using synthetic data and benchmark datasets.\ndynamic positional embedding to keep the spatial struc-\ntures of the intermediate feature maps for recognizing texts\nwith arbitrary arrangements and large inter-character spac-\ning. (Yu et al. 2020) proposed semantic reasoning networks\nto replace RNN-like structures for more accurate text recog-\nnition. (Atienza 2021) only used the image Transformer\nwithout text Transformer for the text recognition in a non-\nautoregressive way.\nThe texts in natural images may appear in irregular shapes\ncaused by perspective distortion. (Shi et al. 2016; Baek et al.\n2019; Litman et al. 2020; Shi et al. 2018; Zhan and Lu 2019)\naddressed this problem by processing the input images with\nan initial rectification step. For example, thin-plate spline\ntransformation (Shi et al. 2016; Baek et al. 2019; Litman\net al. 2020; Shi et al. 2018) is applied to find a smooth spline\ninterpolation between a set of fiducial points and normal-\nize the text region to a predefined rectangle, while (Zhan\nand Lu 2019) proposed an iterative rectification network to\nmodel the middle line of scene texts as well as the orienta-\ntion and boundary of textlines. (Baek et al. 2019; Diaz et al.\n2021) proposed universal architectures for comparing differ-\nent recognition models.\nHandwritten Text Recognition\n(Memon et al. 2020) gave a systematic literature review\nabout the modern methods for handwriting recognition. Var-\nious attention mechanisms and positional encodings are\ncompared in the (Michael et al. 2019) to address the align-\nment between the input and output sequence. The combina-\ntion of RNN encoders (mostly LSTM) and CTC decoders\n(Bluche and Messina 2017; Graves and Schmidhuber 2008;\nPham et al. 2014) took a large part in the related works for a\nlong time. Besides, (Graves and Schmidhuber 2008; V oigt-\nlaender, Doetsch, and Ney 2016; Puigcerver 2017) have also\ntried multidimensional LSTM encoders. Similar to the scene\ntext recognition, the seq2seq methods and the scheme for at-\ntention decoding have been verified in (Michael et al. 2019;\nKang et al. 2020; Chowdhury and Vig 2018; Bluche 2016).\nConclusion\nIn this paper, we present TrOCR, an end-to-end\nTransformer-based OCR model for text recognition\nwith pre-trained models. Distinct from existing approaches,\nTrOCR does not rely on the conventional CNN models\nfor image understanding. Instead, it leverages an image\nTransformer model as the visual encoder and a text Trans-\nformer model as the textual decoder. Moreover, we use\nthe wordpiece as the basic unit for the recognized output\ninstead of the character-based methods, which saves the\ncomputational cost introduced by the additional language\nmodeling. Experiment results show that TrOCR achieves\nstate-of-the-art results on printed, handwritten and scene\ntext recognition with just a simple encoder-decoder model,\nwithout any post-processing steps.\n13100\nAcknowledgments\nThis work was supported in part by the National Natu-\nral Science Foundation of China (Grant Nos. 62276017,\nU1636211, 61672081).\nReferences\nAtienza, R. 2021. Vision Transformer for Fast and Efficient\nScene Text Recognition. arXiv preprint arXiv:2105.08582.\nBaek, J.; Kim, G.; Lee, J.; Park, S.; Han, D.; Yun, S.; Oh,\nS. J.; and Lee, H. 2019. What is wrong with scene text\nrecognition model comparisons? dataset and model analy-\nsis. In ICCV.\nBaek, J.; Matsui, Y .; and Aizawa, K. 2021. What if We\nOnly Use Real Datasets for Scene Text Recognition? To-\nward Scene Text Recognition With Fewer Labels. InCVPR.\nBao, H.; Dong, L.; and Wei, F. 2021. BEiT: BERT Pre-\nTraining of Image Transformers. arXiv:2106.08254.\nBautista, D.; and Atienza, R. 2022. Scene Text Recogni-\ntion with Permuted Autoregressive Sequence Models. arXiv\npreprint arXiv:2207.06966.\nBhunia, A. K.; Chowdhury, P. N.; Sain, A.; and Song, Y .-Z.\n2021a. Towards the Unseen: Iterative Text Recognition by\nDistilling From Errors. In ICCV.\nBhunia, A. K.; Sain, A.; Kumar, A.; Ghose, S.; Chowdhury,\nP. N.; and Song, Y .-Z. 2021b. Joint Visual Semantic Reason-\ning: Multi-Stage Decoder for Text Recognition. In ICCV.\nBleeker, M.; and de Rijke, M. 2019. Bidirectional scene\ntext recognition with a single decoder. arXiv preprint\narXiv:1912.03656.\nBluche, T. 2016. Joint line segmentation and transcription\nfor end-to-end handwritten paragraph recognition. NIPS.\nBluche, T.; and Messina, R. 2017. Gated convolutional re-\ncurrent neural networks for multilingual handwriting recog-\nnition. In ICDAR.\nCai, H.; Sun, J.; and Xiong, Y . 2021. Revisiting classifica-\ntion perspective on scene text recognition. arXiv preprint\narXiv:2102.10884.\nChen, J.; Yu, H.; Ma, J.; Guan, M.; Xu, X.; Wang, X.; Qu, S.;\nLi, B.; and Xue, X. 2021. Benchmarking chinese text recog-\nnition: Datasets, baselines, and an empirical study. arXiv\npreprint arXiv:2112.15093.\nChowdhury, A.; and Vig, L. 2018. An efficient end-to-\nend neural model for handwritten text recognition. arXiv\npreprint arXiv:1807.07965.\nCubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-\ndaugment: Practical automated data augmentation with a re-\nduced search space. In CVPR Workshops.\nCui, M.; Wang, W.; Zhang, J.; and Wang, L. 2021. Represen-\ntation and Correlation Enhanced Encoder-Decoder Frame-\nwork for Scene Text Recognition. In Llad ´os, J.; Lopresti,\nD.; and Uchida, S., eds., ICDAR.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. arXiv:1810.04805.\nDiaz, D. H.; Qin, S.; Ingle, R.; Fujii, Y .; and Bissacco,\nA. 2021. Rethinking Text Line Recognition Models.\narXiv:2104.07787.\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang,\nY .; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Unified\nLanguage Model Pre-training for Natural Language Under-\nstanding and Generation. arXiv:1905.03197.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR.\nDu, Y .; Chen, Z.; Jia, C.; Yin, X.; Zheng, T.; Li, C.; Du, Y .;\nand Jiang, Y .-G. 2022. SVTR: Scene Text Recognition with\na Single Visual Model. arXiv preprint arXiv:2205.00159.\nFang, S.; Xie, H.; Wang, Y .; Mao, Z.; and Zhang, Y . 2021.\nRead Like Humans: Autonomous, Bidirectional and Itera-\ntive Language Modeling for Scene Text Recognition. In\nCVPR.\nGao, Y .; Chen, Y .; Wang, J.; Tang, M.; and Lu, H. 2019.\nReading scene text with fully convolutional sequence mod-\neling. Neurocomputing.\nGraves, A.; Fern´andez, S.; Gomez, F.; and Schmidhuber, J.\n2006. Connectionist temporal classification: labelling un-\nsegmented sequence data with recurrent neural networks. In\nICML.\nGraves, A.; and Schmidhuber, J. 2008. Offline handwrit-\ning recognition with multidimensional recurrent neural net-\nworks. NeurIPS.\nGupta, A.; Vedaldi, A.; and Zisserman, A. 2016. Synthetic\ndata for text localisation in natural images. In CVPR.\nJaderberg, M.; Simonyan, K.; Vedaldi, A.; and Zisserman,\nA. 2014. Synthetic Data and Artificial Neural Networks\nfor Natural Scene Text Recognition. In Workshop on Deep\nLearning, NIPS.\nKang, L.; Riba, P.; Rusi ˜nol, M.; Forn ´es, A.; and Ville-\ngas, M. 2020. Pay attention to what you read: Non-\nrecurrent handwritten text-line recognition. arXiv preprint\narXiv:2005.13044.\nKaratzas, D.; Gomez-Bigorda, L.; Nicolaou, A.; Ghosh, S.;\nBagdanov, A.; Iwamura, M.; Matas, J.; Neumann, L.; Chan-\ndrasekhar, V . R.; Lu, S.; et al. 2015. ICDAR 2015 competi-\ntion on robust reading. In ICDAR.\nKaratzas, D.; Shafait, F.; Uchida, S.; Iwamura, M.; i Big-\norda, L. G.; Mestre, S. R.; Mas, J.; Mota, D. F.; Almazan,\nJ. A.; and De Las Heras, L. P. 2013. ICDAR 2013 robust\nreading competition. In ICDAR.\nKrishnan, P.; and Jawahar, C. V . 2016. Generating Synthetic\nData for Text Recognition. arXiv:1608.04224.\nKudo, T.; and Richardson, J. 2018. Sentencepiece: A\nsimple and language independent subword tokenizer and\ndetokenizer for neural text processing. arXiv preprint\narXiv:1808.06226.\n13101\nLee, J.; Park, S.; Baek, J.; Oh, S. J.; Kim, S.; and Lee, H.\n2020. On recognizing texts of arbitrary shapes with 2D self-\nattention. In CVPR Workshops.\nLiao, M.; Wan, Z.; Yao, C.; Chen, K.; and Bai, X. 2019.\nReal-time Scene Text Detection with Differentiable Bina-\nrization. arXiv:1911.08947.\nLitman, R.; Anschel, O.; Tsiper, S.; Litman, R.; Mazor, S.;\nand Manmatha, R. 2020. Scatter: selective context atten-\ntional scene text recognizer. In CVPR.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv:1907.11692.\nLyu, P.; Zhang, C.; Liu, S.; Qiao, M.; Xu, Y .; Wu, L.; Yao,\nK.; Han, J.; Ding, E.; and Wang, J. 2022. MaskOCR:\nText Recognition with Masked Encoder-Decoder Pretrain-\ning. arXiv preprint arXiv:2206.00311.\nMemon, J.; Sami, M.; Khan, R. A.; and Uddin, M. 2020.\nHandwritten optical character recognition (OCR): A com-\nprehensive systematic literature review (SLR).IEEE Access.\nMichael, J.; Labahn, R.; Gr ¨uning, T.; and Z ¨ollner, J. 2019.\nEvaluating sequence-to-sequence models for handwritten\ntext recognition. In ICDAR.\nMishra, A.; Alahari, K.; and Jawahar, C. 2012. Top-down\nand bottom-up cues for scene text recognition. In CVPR.\nMou, Y .; Tan, L.; Yang, H.; Chen, J.; Liu, L.; Yan, R.; and\nHuang, Y . 2020. Plugnet: Degradation aware scene text\nrecognition supervised by a pluggable super-resolution unit.\nIn ECCV 2020. Springer.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In NAACL.\nPham, V .; Bluche, T.; Kermorvant, C.; and Louradour, J.\n2014. Dropout improves recurrent neural networks for hand-\nwriting recognition. In ICFHR.\nPhan, T. Q.; Shivakumara, P.; Tian, S.; and Tan, C. L.\n2013. Recognizing text with perspective distortion in nat-\nural scenes. In ICCV.\nPuigcerver, J. 2017. Are multidimensional recurrent layers\nreally necessary for handwritten text recognition? In IC-\nDAR.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-\nto-image generation. arXiv preprint arXiv:2102.12092.\nRisnumawan, A.; Shivakumara, P.; Chan, C. S.; and Tan,\nC. L. 2014. A robust arbitrary text detection system for nat-\nural scene images. Expert Systems with Applications.\nSang, D. V .; and Cuong, L. T. B. 2019. Improving CRNN\nwith EfficientNet-like feature extractor and multi-head at-\ntention for text recognition. In ISCIT.\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. arXiv\npreprint arXiv:1508.07909.\nSheng, F.; Chen, Z.; and Xu, B. 2019. NRTR: A\nno-recurrence sequence-to-sequence model for scene text\nrecognition. In ICDAR.\nShi, B.; Bai, X.; and Yao, C. 2016. An end-to-end trainable\nneural network for image-based sequence recognition and\nits application to scene text recognition. TPAMI, (11).\nShi, B.; Wang, X.; Lyu, P.; Yao, C.; and Bai, X. 2016. Ro-\nbust scene text recognition with automatic rectification. In\nCVPR.\nShi, B.; Yang, M.; Wang, X.; Lyu, P.; Yao, C.; and Bai, X.\n2018. Aster: An attentional scene text recognizer with flex-\nible rectification. PAMI, (9).\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2020. Training data-efficient image trans-\nformers & distillation through attention. In ICML.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS.\nV oigtlaender, P.; Doetsch, P.; and Ney, H. 2016. Handwrit-\ning recognition with large multidimensional long short-term\nmemory recurrent neural networks. In ICFHR.\nWan, Z.; He, M.; Chen, H.; Bai, X.; and Yao, C. 2020.\nTextscanner: Reading characters in order for robust scene\ntext recognition. In AAAI.\nWang, K.; Babenko, B.; and Belongie, S. 2011. End-to-end\nscene text recognition. In ICCV. IEEE.\nWang, S.; Wang, Y .; Qin, X.; Zhao, Q.; and Tang, Z. 2019.\nScene text recognition via gated cascade attention. InICME.\nWang, T.; Zhu, Y .; Jin, L.; Luo, C.; Chen, X.; Wu, Y .; Wang,\nQ.; and Cai, M. 2020a. Decoupled Attention Network for\nText Recognition. In AAAI.\nWang, W.; Wei, F.; Dong, L.; Bao, H.; Yang, N.; and Zhou,\nM. 2020b. Minilm: Deep self-attention distillation for task-\nagnostic compression of pre-trained transformers. arXiv\npreprint arXiv:2002.10957.\nWang, Y .; Xie, H.; Fang, S.; Wang, J.; Zhu, S.; and Zhang,\nY . 2021. From Two to One: A New Scene Text Recognizer\nWith Visual Language Modeling Network. In ICCV.\nWightman, R. 2019. PyTorch Image Models. https://github.\ncom/rwightman/pytorch-image-models. Accessed: 2022-\n01-17.\nYan, R.; Peng, L.; Xiao, S.; and Yao, G. 2021. Primitive Rep-\nresentation Learning for Scene Text Recognition. In CVPR.\nYu, D.; Li, X.; Zhang, C.; Liu, T.; Han, J.; Liu, J.; and Ding,\nE. 2020. Towards Accurate Scene Text Recognition With\nSemantic Reasoning Networks. In CVPR.\nYue, X.; Kuang, Z.; Lin, C.; Sun, H.; and Zhang, W. 2020.\nRobustscanner: Dynamically enhancing positional clues for\nrobust text recognition. In ECCV.\nZhan, F.; and Lu, S. 2019. Esir: End-to-end scene text recog-\nnition via iterative image rectification. In CVPR.\nZhang, H.; Yao, Q.; Yang, M.; Xu, Y .; and Bai, X. 2020a.\nAutoSTR: Efficient backbone search for scene text recogni-\ntion. In ECCV.\nZhang, J.; Luo, C.; Jin, L.; Wang, T.; Li, Z.; and Zhou, W.\n2020b. SaHAN: Scale-aware hierarchical attention network\nfor scene text recognition. Pattern Recognition Letters.\n13102"
}