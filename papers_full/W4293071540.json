{
    "title": "AraXLNet: pre-trained language model for sentiment analysis of Arabic",
    "url": "https://openalex.org/W4293071540",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4294089093",
            "name": "Alhanouf Alduailej",
            "affiliations": [
                "King Saud University"
            ]
        },
        {
            "id": "https://openalex.org/A1858608336",
            "name": "Abdulrahman Alothaim",
            "affiliations": [
                "King Saud University"
            ]
        },
        {
            "id": "https://openalex.org/A4294089093",
            "name": "Alhanouf Alduailej",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1858608336",
            "name": "Abdulrahman Alothaim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2767566483",
        "https://openalex.org/W3109123510",
        "https://openalex.org/W2922183696",
        "https://openalex.org/W2990443604",
        "https://openalex.org/W3010449670",
        "https://openalex.org/W2809427307",
        "https://openalex.org/W2028070629",
        "https://openalex.org/W2894776929",
        "https://openalex.org/W2885719698",
        "https://openalex.org/W2901922204",
        "https://openalex.org/W2046858834",
        "https://openalex.org/W4245048854",
        "https://openalex.org/W2134684245",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2971016465",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2915357442",
        "https://openalex.org/W2890931798",
        "https://openalex.org/W2753498304",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W4361731938",
        "https://openalex.org/W2947411064",
        "https://openalex.org/W2916132663",
        "https://openalex.org/W1570448133",
        "https://openalex.org/W2513138008",
        "https://openalex.org/W2471147443",
        "https://openalex.org/W2770803436",
        "https://openalex.org/W89279510",
        "https://openalex.org/W2625475744",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3191137294",
        "https://openalex.org/W3197863338",
        "https://openalex.org/W3092409488",
        "https://openalex.org/W3213173707",
        "https://openalex.org/W3195489797",
        "https://openalex.org/W3163363326",
        "https://openalex.org/W3103339821",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W3104248216"
    ],
    "abstract": "Abstract The Arabic language is a complex language with little resources; therefore, its limitations create a challenge to produce accurate text classification tasks such as sentiment analysis. The main goal of sentiment analysis is to determine the overall orientation of a given text in terms of whether it is positive, negative, or neutral. Recently, language models have shown great results in promoting the accuracy of text classification in English. The models are pre-trained on a large dataset and then fine-tuned on the downstream tasks. Particularly, XLNet has achieved state-of-the-art results for diverse natural language processing (NLP) tasks in English. In this paper, we hypothesize that such parallel success can be achieved in Arabic. The paper aims to support this hypothesis by producing the first XLNet-based language model in Arabic called AraXLNet, demonstrating its use in Arabic sentiment analysis in order to improve the prediction accuracy of such tasks. The results showed that the proposed model, AraXLNet, with Farasa segmenter achieved an accuracy results of 94.78%, 93.01%, and 85.77% in sentiment analysis task for Arabic using multiple benchmark datasets. This result outperformed AraBERT that obtained 84.65%, 92.13%, and 85.05% on the same datasets, respectively. The improved accuracy of the proposed model was evident using multiple benchmark datasets, thus offering promising advancement in the Arabic text classification tasks.",
    "full_text": "AraXLNet: pre‑trained language model \nfor sentiment analysis of Arabic\nAlhanouf Alduailej and Abdulrahman Alothaim*   \nIntroduction\nThe internet and social media have become a promising platform for learning, sharing \nopinions, and exchanging ideas. Twitter is a popular social network application that \nallows users to share both their positive and negative opinions about everything, as well \nas their day-to-day thoughts on life. As a result, this platform led to an increase in the \namount of data available on the web, due to which many researchers started showing \ninterest in this data for mining. This was aimed at enabling decision-making in different \nfields and providing many techniques to increase efficiency in the use of this data.\nOne of these recent fields of study is known as sentiment analysis. It is the study of \npeople’s opinions and emotions around issues, events or topics [1]. Sentiment analysis \nis defined as the process of computationally classifying opinions expressed in a piece of \nAbstract \nThe Arabic language is a complex language with little resources; therefore, its limita-\ntions create a challenge to produce accurate text classification tasks such as sentiment \nanalysis. The main goal of sentiment analysis is to determine the overall orientation of \na given text in terms of whether it is positive, negative, or neutral. Recently, language \nmodels have shown great results in promoting the accuracy of text classification in \nEnglish. The models are pre-trained on a large dataset and then fine-tuned on the \ndownstream tasks. Particularly, XLNet has achieved state-of-the-art results for diverse \nnatural language processing (NLP) tasks in English. In this paper, we hypothesize that \nsuch parallel success can be achieved in Arabic. The paper aims to support this hypoth-\nesis by producing the first XLNet-based language model in Arabic called AraXLNet, \ndemonstrating its use in Arabic sentiment analysis in order to improve the predic-\ntion accuracy of such tasks. The results showed that the proposed model, AraXLNet, \nwith Farasa segmenter achieved an accuracy results of 94.78%, 93.01%, and 85.77% \nin sentiment analysis task for Arabic using multiple benchmark datasets. This result \noutperformed AraBERT that obtained 84.65%, 92.13%, and 85.05% on the same data-\nsets, respectively. The improved accuracy of the proposed model was evident using \nmultiple benchmark datasets, thus offering promising advancement in the Arabic text \nclassification tasks.\nKeywords: Sentiment analysis, Language models, NLP , XLNet, AraXLNet, Text mining\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nRESEARCH\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72  \nhttps://doi.org/10.1186/s40537‑022‑00625‑z\n*Correspondence:   \nothaim@ksu.edu.sa\nDepartment of Information \nSystems, College \nof Computer and Information \nSciences, King Saud \nUniversity, Riyadh 11451, \nSaudi Arabia\nPage 2 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \ntext as positive, negative, or neutral. This process can also be referred to as natural lan -\nguage processing (NLP).\nA lot of research was conducted to improve the performance and accuracy of senti -\nment analysis. The progress of sentiment analysis has passed through various stages. At \nthe beginning, sentiment analysis was performed using rule-based and statistical meth -\nods (unsupervised learning), which offered low accuracy and did not generalize well. \nThen, supervised machine learning algorithms such as Naïve Bayes and decision trees \nwere used to overcome the issues with traditional unsupervised methods [16], which \nalso provided improved accuracy results. In 2013, the introduction of deep learning \nalgorithms such as convolutional neural network (CNN) and long short-term memory \n(LSTM) models advanced the field of sentiment analysis with improved results [18]. \nHowever, it was the introduction of transformers back in 2017, that contributed to the \nachievement of the state-of-the-art accuracy for several natural language processing \ntasks including sentiment analysis [36, 49]. Although these language model transform -\ners have shown impressive improvement in the sentiment analysis of text in the English \nlanguage [24–28]. However, only few research were conducted on using pre-trained lan -\nguage models for the sentiment analysis of Arabic texts [29, 30]. The Arabic language is \ncomplex in nature because of its rich morphological system. This nature, together with \nthe lack of its resources and less attention on Arabic, imposes challenges on the advance-\nment of Arabic sentiment analysis research [2].\nIn this paper, we conduct sentiment analysis to analyze and determine the polarity of \nArabic tweets, in which opinions are highly unstructured and are either positive, neutral, \nor negative. Specifically, we aim to improve the performance and capabilities of Arabic \nnatural language processing (NLP) tasks through introducing an Arabic language model \nby pre-training the state-of-the-art XLNet model on a large-scale Arabic corpus to pro -\nduce the first XLNet-based language model in Arabic. We call this model AraXLNet. \nOur goal is to demonstrate its use in improving the result accuracy of Arabic sentiment \nanalysis. Using benchmark datasets from the literature, the proposed model was found \nto offer improved results in sentiment analysis task for the Arabic language.\nThe remainder of the paper is organized as follows: “Background ” section provides a \nchronological background about this area of research. “Methodology ” section offers a \ndetailed overview of the methodology used in this paper. “Results” section outlines the \nexperiments and results. Then, in “Discussion ” section, we discuss our results. Finally, \n“Conclusion” section describes the conclusion.\nBackground\nSentiment analysis\nIn recent times, sentiment analysis has become an active research topic in the areas of \ndata mining and NLP that analyzes people’s opinions, sentiments, and emotions for \nentities such as products, services, organizations, and events. Since the usage of social \nmedia platforms is increasing day by day, they provide users with better capability to \neasily exchange news, opinions, and ideas among each other. Consequently, the amount \nof user-generated content attracted researchers’ interest who began analyzing and stud -\nying users’ posting behavior to discover useful information and make right decisions. In \nterms of businesses, this allowed companies to promote their products, gather data, and \nPage 3 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nanalyze the opinions of their clients, leading to an improvement in the productivity of \nthe companies and enhancing their work. Furthermore, this also helped companies keep \nin touch with their clients.\nOne of the important applications of analyzing user-generated content is sentiment \nanalysis. It refers to the analysis of people’s opinions, sentiments, and emotions towards \nentities such as products, services, organizations, and events. The most common use of \nsentiment analysis is classifying text into a binary class (positive or negative) or multi-\nclass (three or more classes). Moreover, sentiment analysis can be done on different \nstructural levels of the text, including the document, sentence, and word [34]. It aims \nto determine the polarity of the text through sentences that evidently show whether the \nreaction of the author is positive, negative, or neutral [1]. Many researchers have studied \nthe concept of sentiment analysis and tried to gain accurate results from the classifica -\ntion of texts using different machine learning (ML) techniques [16–23] and pre-trained \nuniversal language models [24–28], which has achieved a great deal of success in the \nEnglish language. However, doing the same for text in Arabic is considered a challenge \nby researchers because of the diversity in its terminology and grammar, apart from the \nfact that there is only a small number of available resources and studies analyzing Ara -\nbic texts. Furthermore, English sentiment analysis provides much better results since \nthere are more models for it, as compared to those available for Arabic. Indeed, only few \nattempts have been made to create pre-trained language models for the sentiment analy-\nsis of Arabic tweets (e.g., AraBERT and hULMonA) [29, 30].\nNatural language processing methods\nNLP is a branch of computer science that intends to facilitate communication between \nmachines and human beings. The main idea behind it is to create an automated environ-\nment to understand the human language and the meaning of certain utterances being \nanalyzed. NLP is very significant, as it makes a major impact on our daily lives. Senti -\nment analysis is one of the most popular applications that uses this subfield of NLP [32].\nNLP employs various ML algorithms, which classify a given instance into a set of \ndiscrete classes. Classification algorithms must be trained to work on a dataset by first \nintroducing them to a training set, which will then enable them to come up with a set \nof rules that shall govern the algorithm. Hence, each instance (example) should be clas -\nsified into one of the categories specified in the training phase. Most classification algo -\nrithms originally only allow discrete input values, but they were developed to accept \ninputs in any form, discrete or continuous. However, the output is always in the discrete \nform. Decision tree, support vector machine (SVM), naïve Bayes, and k-nearest neigh -\nbors (KNN) are examples of classification algorithms that have showed reasonable accu -\nracy and performance [7].\nDecision trees\nA decision tree is basically a graph that relays a branching method that projects every \npossible outcome of a decision. It is one of the predictive modeling approaches used in \nstatistics, data mining and ML. The other names for decision tree models are classifica -\ntion trees or regression trees [39]. Decision trees have many advantages: they are simple \nto understand and interpret, require little data preparation, are capable of handling both \nPage 4 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nnumerical and categorical data, perform classification without much computation, and \ncan handle continuous and categorical variables. On the other hand, they are also unsta -\nble, complex, unwieldy, and costly in addition to posing analysis limitations, not being \nsuitable for the prediction of continuous attributes, and typically performing poorly with \nmany classes and small datasets [39].\nSupport vector machine (SVM)\nSVM is a distinctive classifier formally defined by a separating hyperplane. In other \nwords, given labeled training data (supervised learning), which means that each of them \nbelongs to a specific class, the algorithm outputs an optimal hyperplane that classifies \nnew examples [40]. It is based on the idea of decision planes that define decision bound -\naries. A decision plane is the entity that separates a set of objects belonging to differ -\nent classes. SVM has shown empirically good performance with successful applications \nin many fields, such as bioinformatics, medicine, and text and image recognition [39, \n51–52]. For example, Al-Twairesh et al. [2] applied SVM to a large dataset of 2.2 million \nArabic tweets. They observed that the classification performance was highly affected by \nthe number of classes. Less performance was related to more classes, so they suggested \nadding more instances for three-way and four-way classification models to improve their \nperformance.\nNaïve Bayes\nThe naïve Bayes classifier basically depends on the independence assumption. It has \nmany properties. First, it reduces the parameter’s numbers. Second, in comparison to \nthe decision tree, it does not carry forward an explicit search during training. Third, \ntraining it is a swift process and only requires a small amount of data so it can estimate \nthe parameters necessary for classification. Fourth, it is important to mention that the \nnaïve Bayes classifier can handle both discreet and streaming data. Additionally, it has \nproven to be effective in complex real-world situations [40]. For example, Hanhoon, Yoo, \nand Han [16] applied a senti-lexicon-based naïve Bayes algorithm to perform a senti -\nment analysis of restaurant reviews. Their proposed method reduced the gap between \nthe positive and negative prediction accuracy.\nK‑nearest neighbor (KNN)\nKNN is a super classifier method. To provide some context, a classifier is a machine that \nis able to predict a category (label or class) exactly like a number being conducted after \nperforming a function. KNN makes a prediction for a query based on numbers of the \nclosest neighbors in a given classified training set. The evaluation of distance or simi -\nlarity between observations (instances) is usually done by using usually the Euclidean \ndistance, the Minkowski distance or the Mahalanobis distance [39, 40]. In general, \nKNN uses instance-based learning which means it uses historical data to evaluate new \ninstances. This methodology guarantees a low error rate that is no worse than the mini -\nmum achievable error rate, given the distribution of the data. It is especially suitable for \na huge set of data that could approach infinity [40]. For example, Vidushi and Sodhi [17] \nexplained sentiment analysis as a process that extracts opinions and information from \nthe text, which works as a text classification problem. They extracted the data, applied \nPage 5 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nthe feature selection method, such as the chi-square method, and information gain \n(the chi-square method was used to check variable dependency). Following this, they \nassigned a score for each word based on its duplication in the document to check the \npolarity of the word using the TF-IDF score-based approach. The last step in this study \nwas to apply the classifications technique, for which they used the naïve Bayes algorithm \nand KNN algorithm. They found that naïve Bayes algorithm showed a better result than \nthe KNN algorithm.\nLexicon‑based approach\nThe lexicon-based approach basically utilizes a sentiment dictionary with scored opin -\nion words and matches it with the data to define polarity. This technique is mainly used \nfor sentiment analysis [1]. It has two types: (1) The dictionary-based approach is based \non the manually annotated and collected terms that expand through the search of the \nsynonyms and antonyms on a dictionary such as WordNet. This approach cannot han -\ndle context orientations and domain-specific information [37]. (2) The corpus-based \napproach provides a dictionary related to a particular domain, which is formed by a set \nof opinion terms. This set expands upon searching for the related words using statistical \nand semantic techniques [37].\nThe main idea of the lexicon-based approach is to define the polarity of opinions based \non the used lexicon found in a text. This contracts with training a model with large data -\nsets as done in deep learning, which is widely used and achieves better classification \naccuracy when compared to the lexicon-based approach [37]. For example, Al-Twairesh \net  al. [15] proposed a hybrid method that integrates the corpus-based approach with \nthe lexicon-based method for the sentiment analysis of Arabic tweets. They mention \nthat the use of sentiment lexicon features was the best way to enhance classification \nperformance. The idea is to obtain features from the text and then apply the corpus-\nbased method on the features to reach the best classification result. However, they pre -\nsented three levels of classification: two-way classification (positive and negative), which \nsecures 69.9 as the best F1 score; three-way classification (positive, negative, and neu -\ntral) with the F1 score of 61.63; and four-way classification (positive, negative, neutral, \nand mixed) with the F1 score of 55.07.\nDeep learning\nTraditional ML algorithms, such as decision trees and SVMs, are not efficient when they \nwork with high dimensional data, unlike the deep learning that increase the benefit pro -\nportionally to the amount of data [6, 8–10, 53, 56]. Additionally, traditional ML algo -\nrithms require extracting the features manually from the input by providing the system \nwith the related features of the input to classify the output. On the other hand, in deep \nlearning, the related features are automatically extracted from the input data.\nDeep learning consists of artificial neural networks (ANN) with many layers. \nANNs are computational techniques inspired by human central nervous systems that \nare capable of ML and pattern recognition. They are usually presented as systems of \ninterconnected neurons that can compute values from inputs by feeding informa -\ntion through the network. Further, the network consists of three or more neuron \nlayers [11]: The input layer consists of a collection of neurons set in one row, and the \nPage 6 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nnodes are assigned to a value in the input stage. The hidden layer (processing layer) \nconsists of a collection of neurons set in one or more rows according to the depth of \nthe process needed. Finally, the output layer consists of a collection of neurons fewer \nthan the previous layers. Moreover, there are two types of network architectures, as \nshown in Fig.  1: single layers that consist of the input, output, and one hidden layer \n(simple neural network), and multiple layers that consist of more than one hidden \nlayer (deep learning neural networks).\nTraining the network is done by feeding it large datasets and modifying the weight \nfor each neuron until the final result is similar to the real result. Training allows the \nnetwork to learn from the static data and determine the network weights. The most \nfamous type of training is the back propagation technique, which is a supervised \nlearning technique as the network is trained with an expected answer [38]. The per -\nformance of the network must be tested after the training. The first signal is given \nby the percentage of correct classifications of the training set. When the network \nis tested with a set of similar data, which was not used during training, the perfor -\nmance will be more relevant. In the test step, the input data is fed into the network \nand the required values are compared to the network’s output values. Therefore, \nthe performance of the trained network will depend on the agreements or disagree -\nments of the results [11, 38].\nFor example, Reshma et  al. [18] attempted to identify and classify the news as \neither negative or positive. They focused on developing a system using ML and NLP \ntechniques. They used a document term matrix to represent the frequency of the \nterms that occur in the news and applied the classification technique SVM. They \nprepared the training data by using Valence Aware Dictionary and Sentiment Rea -\nsoner (VADER), which uses a library that offers the benefit of self-classifying the \nsentiment without the necessity for any previous data. Moreover, they also used deep \nlearning algorithms to enhance system reliability and make a successful model that \nefficiently handles the complexity. The neural network provided better results with \n96% for the training accuracy and 85% for the testing accuracy. Wu et al. [55] pro -\nposed a multiple-context-aware and cascade CNN structure for scene text detection. \nFurthermore, Maha Heikal et al. [14] proposed a model to improve the accuracy of \nArabic sentiment analysis by combining convolutional neural network (CNN) and \nlong short-term memory (LSTM), which achieved an F1 score of 64.46% when it was \napplied to 10,000 Arabic tweets.\nFig. 1 Simple neural networks and deep networks [54]\nPage 7 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nLanguage models\nLanguage models refer to the reuse of a pre-trained model with pre-existing language \nknowledge, which is called transfer learning. It is currently very popular, because it per -\nforms better, faster, and requires rich data to train deep neural networks [36]. Transfer \nlearning with language models have recently shown to achieve the state-of-the-art accu -\nracy for several NLP tasks [29, 36].\nA language model is an algorithm for learning such a function, which estimates the \nprobability of the next word and provides context between similar words and phrases. \nA deep neural network language model is based on neural networks that are pre-trained \non a web-scale unlabeled text dataset with a general-purpose training objective before \nbeing fine-tuned on various downstream tasks [31]. Several language models have been \ndeveloped in literature for the English language.\nHoward et al. [24] proposed universal language model fine-tuning (ULMFiT), which \nwas an efficient and successful transfer learning method used in NLP tasks. They found \nthat the proposed method works well on wide text classification tasks, reducing the error \nrate by 18–24% on most datasets, and provides high performance. However, the ULM -\nFiT model is useful for NLP of non-English languages as well as for tasks with a small \nnumber of classified data. Moreover, they observed that deep learning models need to \nbe trained from scratch with a large dataset so they can achieve better results after many \nNLP tasks.\nDevlin et  al. [26] explained the significance of deep bidirectional pre-training for \nlanguage representations that was used on the BERT model, which stands for Bidirec -\ntional Encoder Representations from Transformers. BERT was the beginner model that \ndisplayed advanced performance on a huge suite of sentence-level data. Furthermore, \nBERT pre-trained text pairs using the next sentence prediction task on both the left and \nright context. They found that BERT was simple, powerful, and successful in dealing \nwith many sets of NLP tasks. Figure 2 shows the pre-training and fine-tuning procedures \nassociated with BERT.\nLiu et al. [25] presented a replication study of the BERT pre-training model called \nRoBERTa (robustly optimized BERT approach), which simply outperforms the per -\nformance of BERT methods. They trained the new model for a longer period with \nbig data using longer sequences, eliminating the predicted objective of the following \nFig. 2 Pre-training and fine-tunning procedures for BERT [26]\nPage 8 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nsentence, and powerfully changing the masking pattern connected to the training \ndata. They found that the performance was improved by using RoBERTa and not \nBERT. Moreover, Lan et  al. [27] discussed the difficulties of using the BERT model \nwhich are related to the increase in the model size that affects the graphics process -\ning unit (GPU) and tensor processing unit (TPU) memory and leads to a longer \ntraining time. They presented a solution that worked on minimizing memory usage \nand enhancing the training speed by using fewer parameter reduction techniques. \nThe designed architecture called A Lite BERT (ALBERT) was scaled better than the \noriginal BERT and provided better results. However, ALBERT used two-parameter \nreduction techniques that improved performance. The first one, broke down the \nlarge vocabulary matrix into two small matrices. The second technique inhibited the \nparameter from expanding. Additionally, ALBERT is more expensive because of the \nhigher structure architecture.\nYang et  al. [28] proposed a new pre-training model that outperforms BERT on \nmany tasks called XLNet. They mentioned that BERT overlooked dependency among \nthe masked positions, as well as it faced some problem from a pretrain-finetune \ndiscrepancy. Accordingly, they added bi-directional context to the autoregressive \nTransformer-XL model architecture to build the XLNet model. They also applied a \npermutation language modeling objective to integrate the benefit of autoregressive \n(AR) language modeling and autoencoding (AE), which are the most effective pre-\ntraining methods. However, XLNet achieved significant improvement over previous \npre-training objectives on several tasks. This includes the following:\n• XLNet increases the predicted log-likelihood of a sequence by using permutation \noperation, which taught each position to allow the usage of contextual informa -\ntion from whole positions. It identifies online databases that include many of the \nstudies related to emotion detection and prediction in online social networks.\n• It improves the performance of many tasks that have a large text sequence by \ncombining Transformer-XL with pre-training (Transformer-X consists of the seg -\nment recurrence mechanism and relative encoding scheme).\n• It does not suffer from data corruption and pretrain-finetune discrepancy.\n• It enhances the architecture designs for pre-training.\n• It achieves high accuracy results when compared with other models as demon -\nstrated in Table 1 .\nTable 1 Comparison of XLNet with Bert and Roberta [28]\na SQuAD1.1 is a large‑scale dataset contains questions and answer\nModel Method Accuracy (%) F1-Score \n(SQuAD1.1)a \n(%)\nBERT Bidirectional transformer with masked language model MLM \nand next sentence prediction NSP\n72.0 90.9\nRoBERTa BERT without NSP 83.2 94.6\nXLNet Bidirectional transformer with permutation-based modeling 85.4 95.1\nPage 9 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nHowever, for the Arabic language, only a few attempts have been made to construct \npre-trained LMs in Arabic. Arabic is the fourth most used language on the internet \nwith 400 million speakers in 22 different countries. Arabic language has a complex \nnature because of its rich morphology and different dialects. In addition, it is a rich \nlanguage written from right to left without capitalization and each character’s shape \nchanges according to its position in the word. The original BERT has a multilingual \nmodel (mBERT) that was trained simultaneously on Wikipedia dumps for 100 languages \nincluding Arabic. Other than that, there are only two documented attempts to construct \nArabic LMs.\nThe first attempt for constructing a single LM in Arabic was hULMonA designed by \nObeida ElJundi et al. [29]. They developed the first universal language model in Arabic, \nwhich was used for Arabic classification tasks. They found that hULMonA performed \nwell with several Arabic datasets and obtained the new state-of-the-art outcome in \nArabic sentiment analysis. The second attempt invovled AraBERT designed by Wis -\nsam Antoun et  al. [30], who developed a model that pre-trained BERT for the Arabic \nlanguage. They found AraBERT gave advanced results on many downstream tasks for \nthe Arabic language, such as the sentiment analysis method named entity recognition \nand the question-answering tasks. AraBERT is 300 MB smaller than the original BERT \nand exhibited higher performance results when compared with mBERT. Table 2 displays \nthe comparison of results (F1-Accuracy) obtained using hULMonA and AraBERT from \nthe sentiment analysis task. In fact, AraBERT performs well for different Arabic dia -\nlects/accents within the Arabic language across various countries. Hence, the results in \nTable 2 reveal that for Arabic sentiment analysis, both versions of AraBERT outperform \nhULMonA on most tested datasets and achieved better results.\nHowever, the aforementioned XLNet has achieved state-of-the-art results in diverse \nNLP tasks in English [28]. We hypothesize that the same success can be achieved for \nArabic that will outperform the previous two models in classifying Arabic text (hUL -\nMonA and AraBERT), which were formerly known to achieve more accurate results. We \naim to support the hypothesis by developing a model based on XLNet to classify Arabic \ntweets.\nMethodology\nIn recent days, online social networks such as Twitter have become a popular plat -\nform for different users to express their opinions and ideas without limitations. As of \nthe first quarter of 2019, Twitter averaged 330 million monthly active users with 500 \nmillion tweets being sent per day [3, 5]. The prospect of collecting and analyzing this \ndata attracted many researchers in text mining and the NLP field [4, 12, 13]. Twitter \nprovides access to the massive amounts of available data through Twitter application \nTable 2 Comparison between hULMonA and AraBERT [29, 30]\nDataset hULMonA (%) AraBERT v0.1/v1 (%)\nBERT 95.7–95.7 96.2/96.1\nRoBERTa 67.7–69.9 92.2/92.6\nXLNet 51.1–52.4 58.9/59.4\nPage 10 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nprogramming interfaces (APIs). Recently, there have been many studies that addressed \nsentiment analysis in Arabic by using traditional machine learning, however, only few \nstudies utilized deep learning approaches. As discussed earlier, deep learning algorithms \nwere more efficient than machine learning algorithms with high dimensional data; \nthey also provided better prediction accuracy. Moreover, deep learning automatically \nextracted the features from the input, while machine learning was manually extracted \n[6, 8]. Therefore, in this paper, we propose an Arabic language model based on deep \nlearning (neural network), and we call it AraXLNet. It identifies the sentiment of Arabic \ntextual data. The proposed model is based on pre-training the state-of-the-art XLNet \nlanguage model using an Arabic dataset. According to our knowledge, no previous study \nhas attempted reaching this particular objective. In the next section, we present the \nXLNet language model, followed by our proposed model.\nXLNet Language Model\nThese days, pre-training deep neural networks on a large-scale dataset is very popu -\nlar, because it performs better, faster, and achieves high accuracy for several NLP tasks \n[36]. One of the new pre-training models is XLNet, which is based on a generalized \npermutation language modeling objective. XLNet was developed by Carnegie Mellon \nUniversity and Google researchers in 2019 [28]. It was designed to find a solution for \nthe drawback of the autoencoding method used by BERT and other popular language \nmodels [28]. Accordingly, they applied a permutation language modeling objective to \nintegrate the benefit of autoregressive (AR) and autoencoding (AE) language modeling \nand also to achieve significant improvement of many pre-training models on several \ntasks [ 28]. However, we compared BERT and XLNet for language pre-training. Given \na text sequence x =  [x1,…,  xT], BERT performs pre-training based on denoising auto-\nencoding.ˆx : x̂ : corrupted version (by setting a portion of tokens in x to a special symbol \n[MASK]), x̅  x : the masked tokens. The training objective is to reconstruct x̅  x from x̂  ˆx:\nwhere  mt = 1 indicates  xt is masked,  HθHθ : Transformer that maps a length-T text \nsequence x into a sequence of hidden vectors  Hθ(x) =  [Hθ(x)1;  Hθ(x)2; ….;  Hθ(x)T].\nIn comparison, XLNet is based on the permutation language modeling objective. T: \nthe length of sequence x,  ZT: the set of all possible permutations,  zt and z < t denote the \nt-th element and the first t-1 elements of a permutation z ∈ ∈  ZT [28].\nWhen comparing Eq. 1 and Eq. 2, we observed that BERT is based on an independence \nassumption unlike XLNet, which naturally avoids the independence assumption. BERT \nsuffers from pre-train finetune discrepancy due to input noise ([MASK]), which never \nhappens in downstream tasks. On the other hand, XLNet does not suffer from this issue, \nbecause it is not dependent on any input corruption [28]. To clarify the difference, let us \n(1)max\nθ\nlogθ\n(\nx|ˆx\n)\n≈\nT∑\nt−1\nm tlog pθ\n(\nxt|ˆx\n)\n=\nT∑\nt−1\nm tlog\nexp\n(\nH θ\n(ˆx\n)T\nt e(xt)\n)\n∑\nx′ exp\n(\nH θ\n(ˆx\n)T\nt e(x′)\n)\n(2)max\nθ\nEz ∼ zT\n[T∑\nt−1\nlog pθ(xzt|zz<t )\n]\nPage 11 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nconsider a simple example [New, Zealand, is, a, country], where the [New, Zealand] is \nthe predicted target. BERT and XLNet will reduce the following objectives:\nNow, notice that the XLNet is capable of finding the dependency between two tokens \n[New, Zealand], which BERT is unable to. Furthermore, XLNet allows for different fac -\ntorization orders, which will assist the model to learn and collect information from all \npositions on both forward and backward sides as described in Fig. 3.\nTherefore, XLNet, with its features to permutate language modeling and specify cap -\nture bidirectional contexts, outperformed BERT on different NLP tasks and performed \na new state-of-the-art model. Table 3 shows a comparison between the two models with \nseveral different dataset [28].\nArabic Specific XLNet Model: AraXLNet\nTransfer learning involves reusing of a pre-trained model that has some language knowl-\nedge already. It is currently very popular because it performs better and faster than all \nϑBERT = log p\n(\nNew |is country\n)\n+ log p\n(\nZealand|is a country\n)\nϑBERT = log p\n(\nNew |is country\n)\n+ log p\n(\nZealand|New,is a country\n)\nFig. 3 Description of the Permutation Language Modeling for Predicting the token × 3  [28]\nTable 3 Comparison between BERT and XLNet [28]\nDataset BERT (%) XLNet (%)\nSQiAD1.1 92.8 94.0\nSQuAD2.0 85.5 87.8\nRACE 75.1 77.4\nMNLI 87.3 88.4\nQNLI 93.0 93.9\nQQP 91.4 91.8\nRTE 74.0 94.4\nPage 12 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nthe other approaches on NLP problems [29, 36]. In this paper, we propose an XLNet-\nbased model for Arabic, which we call AraXLNet and consists of three main steps: (1) \nPre-training the state-of-the-art language model (XLNet) on large collected Arabic data-\nsets that do not require annotations; (2) Fine-tuning the pre-trained language model \n(AraXLNet) on annotated Twitter Arabic dataset for sentiment analysis, where the \ntweets are manually annotated as either positive, negative, or natural; (3) Adding a clas -\nsification layer on top of the fine-tuned AraXLNet language model for the aim of senti -\nment classification.\nPre‑training and fine‑tuning datasets\nIn order to pre-train our model, we collected a large Arabic corpus that is publicly avail -\nable for researchers. This dataset (outlined below) contained 60,667,300 sentences from \ndifferent corpora:\nOpenSubtitles1 provides access to a corpus of textual data available in 65 languages. \nThis dataset is pre-formatted, making one sentence appear per line, and does not require \nany complex pre-processing, unlike more commonly used text datasets such as Wikipe -\ndia. The Arabic corpus from this dataset contains 60 million different sentences.\nHARD (Hotel Arabic Reviews Dataset) contains 93,700 hotel reviews collected from \nBooking.com website. It contains both modern standard and dialectal Arabic reviews \n[45].\nLABR (Large-Scale Arabic Book Reviews) includes 63,000 book reviews written in \nArabic and collected from the website Goodreads.com. It is considered one of the largest \nArabic sentiment datasets [46].\nBRAD (Books Reviews in Arabic Dataset) contains 510,600 book reviews in modern \nstandard and dialectal Arabic that collected from GoodReads.com. This dataset is an \nextension of LABR [47].\nAfter pre-training the model and producing AraXLNet, we use fine-tuning datasets, \nwhich are annotated Twitter Arabic datasets for sentiment analysis, on the pre-trained \nlanguage model (AtraXLNet). Specifically, we use four different annotated benchmark \nTwitter datasets:\nAraSenTi is comprised of Arabic tweets for sentiment analysis; they are written in the \nSaudi dialect and modern standard Arabic. The dataset contains 17,573 tweets labelled \nwith four classes: positive, negative, neutral and mixed. It is considered one of the largest \nannotated datasets of Saudi tweets [2].\nSemEval is a training and testing dataset used for the SemEval-2017 Task of senti -\nment analysis on Twitter. It classifies whether the sentiment of the message is of positive, \nnegative, or neutral. It contains more than 70,000 tweets in Arabic and English, which is \navailable for researchers [41].\nAJGT (Arabic Jordanian General Tweets) contains 1,800 tweets that are manually \nannotated as either positive or negative. The tweets are formulated in the Jordanian dia -\nlect [48].\n1 https:// www. opens ubtit les. org/ ar.\nPage 13 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nASTD2 (Arabic sentiment tweets dataset) contains over 10,000 entries. This dataset is \nlabelled with four classes: neutral, negative, positive, and mixed.\nData pre‑processing\nBefore feeding the data into the model to be built, the data must be cleaned. Data clean -\ning can be performed by removing URLs, hashtags tags (#), mention (@), numbers, \nnon-Arabic words, and any irrelevant parts of the collected tweets. However, the Ara -\nbic language has a complex nature and structure, where each word can have different \nshapes and share the same meaning. This is mainly due to the rich morphology of Ara -\nbic [33]. To handle this issue, first, we segmented the words using Farasa segmentation \nmodule. Then, we used the SentencePiece tokenizer that XLNet requires for data input. \nThe Farasa segmentation module is a more accurate and fast text processing toolkit \nfor Arabic text and outperforms many state-of-the-art for many Arabic segmenters \n[44]. Additionally, Farasa is built on SVM-rank with linear kernels that uses a variety \nof lexicons, features, and vocabulary items to rank possible word segmentations. The \nXLNet model requires a special format to be applied on the data. The first one is called \n[CLS], which will be added at the beginning of every sentence and the second one is \ncalled [SEP], which will be added at the end of every sentence. XLNet uses the Senten -\ncePiece tokenizer to perform sub-word tokenization and directly convert the text into \nan ID sequence [28]. SentencePiece is an open-source text tokenizer and detokenizer \nessentially for neural network-based text processing systems that predetermine the \nvocabulary size before training the model. By implementing that, we are benefited by \ndesigning a model without having any concern of language-specific resources [35]. The \npre-processing stage for the pre-training data takes into consideration the complexities \nof the Arabic language. Therefore, it will aid the model to function better by reducing the \nrequired vocabulary size by excluding unnecessary redundant tokens in order to reduce \nthe language complexity [30].\nModel Pre‑Training\nTo capture the different properties of a language, we first constructed a large-scale Ara -\nbic language model (AraXLNet) by pre-training the state-of-the-art language model \n(XLNet) on large collected Arabic datasets that do not require annotation, after pre-pro-\ncessing our datasets in the previous step. We chose to train the model from scratch as \nopposed to training it from an existing model or checkpoint by setting init_checkpoint \nto none. We ran the training script using train.py, where model_dir is the output direc -\ntory for the pre-trained model and record_info_dir defines the input TFRecords file of \nour dataset. Further, we set train_batch_size to 32, which is convenient with the TPU \nplatform in which GPU is better to decrease the train_batch_size and increase num_\ncore_per_host. After the training finishes, we used pytorch 3 transformers (transfomer-\ncli) to change this Tensorflow4 model to pytorch model in order to fine-tune our model \n2 http:// www. moham edaly. info/ datas ets/ astd.\n3 https:// pytor ch. org.\n4 https:// www. tenso rflow. org/.\nPage 14 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nwith sentiment analysis. The output of this part is the extracted word embeddings which \nare the distributional representations of each word in the designed corpus.\nModel fine tuning\nMost social media platforms and Arabic datasets contain dialects. Dialects have no \nstandards or a codified form and are influenced by region-specific slang [33]. Thus, fine-\ntuning the pre-trained general domain AraXLNet on the downstream task with anno -\ntated data was completed to adapt it to the new textual features. For this task, we first \nmodified our pre-trained model to give outputs for classification; then, we trained the \nmodel on the annotated Twitter dataset until that the entire model was well-suited for \nsentiment classification. The Pytorch library included a set of interfaces designed for \na variety of NLP tasks. However, these interfaces were embedded on top of a trained \nmodel, where each interface is designed to support a specific NLP task. We used XLNet-\nFor-Sequence Classification that took our trained model as a parameter with a single \nlinear layer added on top for classification. Therefore, our pre-trained model and the \nadditional untrained classification layer with the input annotated Twitter data were \ntrained together on our specific task. So, the final result was the fine-tuned AraXLNet \nfor sentiment analysis.\nThe input arrays on AraXLNet requires to be of the same size. We operated this by \nassigning maximum sentence length to be equal to (128) then padding and truncating \nthe inputs until each input sequence length were the same (all become of length MAX_\nLEN) using the available Python function called pad_sequences that takes the truncating \nand padding as parameters, and assigning them to the \"post\" value, which will force the \npadding and truncating to be at the end of the sequence and not at the beginning. We \nperformed the training loop by following the following steps: (1) We set the model in \ntrain mode and computede gradients, (2) unpacked the data inputs, (3) loaded the data \nonto the GPU for more acceleration, (4) flushed the gradients calculated in the previous \nstep (in pytorch the gradients are cleared by default), (5) started the forward pass that \nfed the input data through the network, (6) commenced the backward pass that moved \nbackward to the end to compute the late start, (7) reported for the network to update \nparameters using optimizer.step(), (8) and finally, monitored the training progress by \ntracking variables.\nImplementation tools\nWe used Python in the Google Colab 5 environment, which is a cloud service provided \nby Alphabet Inc. The environment is used to write and implement deep learning algo -\nrithms in Python. In addition, Colab provides access to faster cloud tensor processing \nunites (TPU) systems produced by Google for the purpose of accelerating operations. \nEach TPU packs up to 180 teraflops with high-bandwidth memory onto a single board. \nAccordingly, Colab Pro was used in this paper which provided high memory virtual \nmachines (VMs) that double the memory of standard Colab VMs. Additionally, it pro -\nvides access to cloud TPUs for up to 24 h. In order to prepare our training environment, \n5 https:// colab. resea rch. google. com/.\nPage 15 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nwe used the Google Cloud Storage bucket6 for the persistent storage of our training data \nand model. It provides flexible, scalable, durable, and high-bandwidth storage as well as \nallows read and write files to be stored on Cloud Storage buckets from Google Colab.\nEvaluation matrices\nIn order to evaluate the fine-tuned model and determine if the classifiers are accurate in \ncapturing and predicting a pattern, we needed to assess the performance of the model. \nA confusion matrix was used as an indicator for the accuracy of the classifier results, in \norder to help to obtain a better analysis result. In general, this matrix was about measur -\ning the learning algorithm accuracy on a test/labeled dataset. The positive class was“yes” \nand negative class was“no” , where P denotes the number of positive classes and N the \nnumber of negative classes. Additionally, there were different terms that are used in the \nconfusion matrix which were as follows [42]:\n• True positives (TP) refer to correct classifications labeled “positive” .\n• True negatives (TN) refer to correct classifications labeled negative “labeled” .\n• False positives (FP) refer to incorrect classifications where the outcome is the pre -\ndicted class “yes” , but the actual class is “no” .\n• False negatives (FN) refer to incorrect classifications where the outcome is the pre -\ndicted class “no” , but the actual class is “yes” .\n• Moreover, from the confusion matrices we computed a list of rates such as the fol -\nlowing [43]:\n• Precision identifies the success degree of the classifiers in correctly predicting the \nnumber of labeling dataset and the total number of labels in the test dataset that were \ncorrectly predicted as positives.\nPrecision = TP/TP+FP\n– Recall shows the success degree of the classifiers in correctly predicting a number \n“ratio” of true positive-labeled dataset and a total number of positives and negative \nlabels in a test dataset.\nRecall = TP/TP+FN\n– F1-score is a collection of the precision and the recall, that gives the total overview of \nthe measured performance of the classifier.\nF1-score = 2*(Recall * Precision) / (Recall + Precision)\n– Support shows the number of the true response samples of the class in the dataset.\n– Accuracy describes the degree of how the classifier classified and predicted docu -\nments correctly from the total number of all documents in a testing set.\nAccuracy = TP+TN/TP+FP+FN+TN\n– Weighted avg calculates the precision of all classes merged together.\n6 https:// cloud. google. com/.\nPage 16 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nWeighted average = (TP of class 0 + TP of class 1)/(total number of class 0 + total \nnumber of class 1)\n– Macro avg computes the average precision, recall, and F1 of all classes.\nResults\nIn this section, we assess our proposed model to identify sentiments in Arabic tweets \nand present the test results of our classification experiment. We conducted several \nexperiments and observed the performance of each one in order to determine the best \napproach based on the results. The following subsections present the results and outline \nTable 4 Experiment 1 with AraSenTi dataset\nPrecision Recall F1-Score Support\n0 0.95 0.96 0.95 1230\n1 0.95 0.94 0.94 993\nAccuracy – – 0.95 2223\nMacro Avg 0.95 0.95 0.95 2223\nWeighted Avg 0.95 0.95 0.95 2223\nTable 5 Experiment 1 with ASTD dataset\nPrecision Recall F1-Score Support\n0 0.92 1.00 0.96 347\n1 1.00 0.67 0.80 148\nAccuracy – – 0.93 495\nMacro Avg 0.96 0.83 0.88 495\nWeighted Avg 0.94 0.93 0.93 495\nTable 6 Experiment 1 with SemEval dataset\nPrecision Recall F1-Score Support\n0 0.85 0.90 0.88 808\n1 0.87 0.81 0.84 647\nAccuracy – – 0.86 1455\nMacro Avg 0.86 0.85 0.86 1455\nWeighted Avg 0.86 0.86 0.86 1455\nTable 7 Experiment 1 with AJGT dataset\nPrecision Recall F1-Score Support\n0 0.94 0.85 0.89 190\n1 0.79 0.92 0.85 170\nAccuracy – – 0.88 360\nMacro Avg 0.87 0.88 0.87 360\nWeighted Avg 0.88 0.88 0.88 360\nPage 17 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \nthe performances of AraXLNet with AraSenTi, ASTD, SemEval, and AJGT datasets for \nboth with and without Farasa.\nExperiment 1 (AraXLNet with Farasa)\nIn this experiment, we aimed to examine and compare the performance of AraXL -\nNet that pre-trained on the collected dataset and pre-processed it using Farasa. The \nresults for AraSenTi, ASTD, SemEval, and AJGT datasets are shown in Tables  4, 5, \n6, and 7, respectively. The results shows an F1-Score that equals 0.95 for AraSenTi \ndataset, 0.93 for ASTD dataset, 0.86 for SemEval dataset, and 0.88 for AJGT dataset.\nTable 8 Experiment 2 with AraSenTi dataset\nPrecision Recall F1-Score Support\n0 0.96 0.91 0.94 1230\n1 0.89 0.96 0.93 993\nAccuracy – – 0.93 2223\nMacro Avg 0.93 0.93 0.93 2223\nWeighted Avg 0.93 0.93 0.93 2223\nTable 9 Experiment 2 with ASTD dataset\nPrecision Recall F1-Score Support\n0 0.87 1.00 0.93 347\n1 1.00 0.33 0.50 148\nAccuracy – – 0.88 495\nMacro Avg 0.93 0.67 0.71 495\nWeighted Avg 0.89 0.88 0.85 495\nTable 10 Experiment 2 with SemEval dataset\nPrecision Recall F1-Score Support\n0 0.87 0.84 0.86 852\n1 0.79 0.83 0.81 603\nAccuracy – – 0.84 1455\nMacro Avg 0.83 0.83 0.83 1455\nWeighted Avg 0.84 0.84 0.84 1455\nTable 11 Experiment 2 with AJGT dataset\nPrecision Recall F1-Score Support\n0 0.82 0.88 0.85 190\n1 0.87 0.81 0.84 170\nAccuracy – – 0.84 360\nMacro Avg 0.85 0.84 0.84 360\nWeighted Avg 0.85 0.84 0.84 360\nPage 18 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nExperiment 2 (AraXLNet without Farasa)\nIn this experiment, we aimed to examine and compare the performance of AraXLNet \nthat pre-trained on the collected dataset and pre-processed it without using Farasa. The \nresults for AraSenTi, ASTD, SemEval, and AJGT datasets are shown in Tables  8, 9, 10, \nand 11, respectively. The results shows an F1-Score that equals 0.93 for AraSenTi data -\nset, 0.88 for ASTD dataset, 0.84 for SemEval dataset, and 0.84 for AJGT dataset.\nDiscussion\nBased on the previous experiment results, we found that AraXLNet with the Farasa seg -\nmenter achieved better results on the sentiment analysis of Arabic tweets compared to \nthe second AraXLNet model that did not use Farasa during the pre-processing stage of \nthe dataset. As a baseline, we compared the performance of AraXLNet with the sec -\nond version of AraBERTv1 that used the Farasa segmenter, and with the support vector \nmachines (SVM), which is considered an effective classifier for classification and regres -\nsion tasks for text mining [39]. The results of the conducted experiments showed that \nthe developed AraXLNet with Farasa segmenter achieved state-of-the-art results on \nmost of the datasets containing Arabic sentimental and outperformed both AraBERT \nand SVM. Table  12 shows the results of applying AraXLNet to the Arabic sentiment \nanalysis compared to the AraBERT model and SVM using multiple benchmark datasets.\nAs a consequence, AraXLNet with Farasa achieved great performance with reference \nto the sentiment analysis task using benchmark datasets. This good performance has \nthree explanations. First, the pre-processing techniques used with the pre-training data \ntook into consideration the complexities of the Arabic language. Second, the use of a \nlarge Arabic vocabulary size (64 K) to develop AraXLNet has a major role to play in its \nsuccess. Third, the permutation operation that XLNet used was also of major signifi -\ncance; this taught each position to allow the use of contextual information from whole \npositions, so we made full use of the bidirectional context. Moreover, we found that the \nmodel performance can be significantly improved by training it for a longer period of \ntime with more data spread over bigger batches.\nConclusion\nAs technology improves and the number of its users increases, there will be an \nincrease in the amount of data being stored. Consequently, there is a need to improve \nNLP and text mining techniques to interpret data and extract interesting pat -\nterns from it. Recently, language models have shown great results in promoting the \nTable 12 AraXLNet compared with AraBERT and SVM\nModel AraSenTi (%) ASTD (%) SemEval (%) AJGT (%)\nAraXLNet with Farasa 94.78 93.01 85.77 88.43\nAraXLNet without Farasa 93.07 88.03 83.64 84.33\nAraBERT 84.65 92.13 85.05 91.94\nSVM 63.21 69.49 81.08 81.94\nPage 19 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \naccuracy of text classification in English. These models were pre-trained on a large \ndataset and then fine-tuned on the downstream tasks. For example, XLNet model was \nfound to outperform BERT on many downstream tasks such as sentiment analysis in \nEnglish. However, only few research were conducted on using pre-trained language \nmodels for the sentiment analysis of Arabic texts. Furthermore, the Arabic language \nhas a complex nature because of its rich morpho-logical system. Considering its \nnature, the lack of resources, and the less attention that is paid to Arabic, Arabic sen -\ntiment analysis research is beset with challenges.\nIn this paper, we hypothesized that such parallel success of English sentiment analy -\nsis can be achieved in Arabic. The paper aimed to support this hypothesis by pre-\ntraining an XLNet-based language model with a large-scale Arabic dataset, and then \nfine-tuning the pre-trained AraXLNet on different Twitter benchmark datasets. The \nresults showed that the proposed model, AraXLNet, with Farasa segmenter achieved \nan accuracy results of 94.78%, 93.01%, and 85.77% in sentiment analysis task for Ara -\nbic using multiple benchmark datasets, and outperforming AraBERT that obtained \n84.65%, 92.13%, and 85.05% on the same datasets, respectively. The improved accu -\nracy of the proposed model was evident using multiple benchmark datasets, thus \noffering advanced improvement in the Arabic text classification tasks. This research \ndemonstrated AraXLNet use in Arabic sentiment analysis in order to improve the \nprediction accuracy of such tasks in Arabic. We showed that the performance of \nAraXLNet with Farasa achieved success with reference to the datasets of all the tested \nArabic tweets meant for sentiment analysis. This stands true compared to the latest \nresearch on AraBERT models and to an SVM algorithm baseline. In future research, \nthe developed AraXLNet, could be fine-tuned to improve its overall performance of \nthe sentiment analysis of Arabic texts.\nAbbreviations\nNLP: Natural language processing; SVM: Support vector machine; ANN: Artificial neural networks; VADER: Valence aware \ndictionary and sentiment reasoner; CNN: Convolutional neural network; LSTM: Long short-term memory; ULMFiT: Uni-\nversal language model fine-tuning; BERT: Bidirectional encoder representations from transformers; ROBERTA : Robustly \noptimized BERT approach; XLNet: Generalized auto-regressive model for NLU.\nAcknowledgements\nNot applicable.\nAuthor contributions\nALAL was the lead author of this manuscript as part of a master degree research project, while the ABAL was the \nstudent’s supervisor who provided the needed guidance and support. All the authors read and approved the final \nmanuscript.\nFunding\nThis research project had no fund.\n Availability of data and materials\nThe datasets used and/or analyzed during the current study are available from the corresponding author on reasonable \nrequest.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nPage 20 of 21Alduailej and Alothaim  Journal of Big Data            (2022) 9:72 \nReceived: 3 October 2021   Accepted: 15 May 2022\nReferences\n 1. Hasan A, Moin S, Karim A, Shamshirband S. Machine learning-based sentiment analysis for Twitter accounts. Math \nComput Appl. 2018;23(1):11.\n 2. Al-Twairesh N, Al-Khalifa H, Al-Salman A, Al-Ohali Y. AraSenTi-tweet: a corpus for arabic sentiment analysis of Saudi \ntweets. Proced Comput Sci. 2017;117:63–72.\n 3. Zhu YQ, Hsiao B. What attracts followers?: exploring factors contributing to brand twitter follower counts. J Organ \nEnd User Comput (JOEUC). 2021;33(1):71–91.\n 4. Kumar A, Garg G. Sentiment analysis of multimodal twitter data. Multimed Tools Appl. 2019;78(17):24103–19.\n 5. Clement J. Twitter: number of active users 2010–2019. 2019. https:// www. stati sta. com/ stati stics/ 282087/ number- of- \nmonth ly- active- twitt er- users.\n 6. Georgiou T, Liu Y, Chen W, Lew M. A survey of traditional and deep learning-based feature descriptors for high \ndimensional data in computer vision. Int J Multimed Info Retr. 2020;9(3):135–70.\n 7. Fernández-Delgado M, Cernadas E, Barro S, Amorim D. Do we need hundreds of classifiers to solve real world clas-\nsification problems? J Mach Learn Res. 2014;15(1):3133–81.\n 8. Patel H, Thakkar A, Pandya M, Makwana K. Neural network with deep learning architectures. J Inf Optim Sci. \n2018;39(1):31–8.\n 9. Houston J, Glavin FG, Madden MG. Robust classification of high-dimensional spectroscopy data using deep learning \nand data synthesis. J Chem Inf Model. 2020;60(4):1936–54.\n 10. Stojanovski D, Strezoski G, Madjarov G, et al. Deep neural network architecture for sentiment analysis and emotion \nidentification of Twitter messages. Multimed Tools Appl. 2018;77:32213–42.\n 11. Basheer IA, Hajmeer M. Artificial neural networks: fundamentals, computing, design, and application. J Microbiol \nMethods. 2000;43(1):3–31.\n 12. Pai PF, Liu CH. Predicting vehicle sales by sentiment analysis of Twitter data and stock market values. IEEE Access. \n2018;6:57655–62.\n 13. Li Z, Fan Y, Jiang B, Lei T, Liu W. A survey on sentiment analysis and opinion mining for social multimedia. Multimed \nTools Appl. 2019;78(6):6939–67.\n 14. Heikal M, Torki M, El-Makky N. Sentiment analysis of Arabic tweets using deep learning. Proced Comput Sci. \n2018;142:114–22.\n 15. Al-Twairesh N, Al-Khalifa H, Alsalman A, Al-Ohali Y. Sentiment analysis of Arabic tweets: feature engineering and a \nhybrid approach. arXiv preprint. 2018. https:// arxiv. org/ abs/ 1805. 08533.\n 16. Kang H, Yoo SJ, Han D. Senti-lexicon and improved Naïve Bayes algorithms for sentiment analysis of restaurant \nreviews. Expert Syst Appl. 2012;39(5):6000–10.\n 17. Vidushi SG. Sentiment mining of online reviews using machine learning algorithms. Int J Eng Devel Res IJEDR. \n2017;5(2):1321–34.\n 18. Kale M, Mankame P , Kulkarni G. Deep learning for digital text analytics: Sentiment analysis. arXiv preprint. 2018. \nhttps:// arxiv. org/ abs/ 1804. 03673.\n 19. Gupta A, Pruthi J, Sahu N. Sentiment analysis of tweets using machine learning approach. Int J Comput Sci Mob \nComput. 2017;6(4):444–58.\n 20. Tyagi P , Tripathi RC. A review towards the sentiment analysis techniques for the analysis of twitter data. In: Proceed-\nings of 2nd International Conference on Advanced Computing and Software Engineering (ICACSE). 2019.\n 21. Mahendran N, Mekala T. A survey: sentiment analysis using machine learning techniques for social media analytics. \nInt J Pure Appl Math. 2018;118:419–22.\n 22. Gautam G, Yadav D. Sentiment analysis of twitter data using machine learning approaches and semantic analysis. In: \nPresented at the 2014 Seventh International Conference on Contemporary Computing (IC3). 2014.\n 23. Kharde V, Sonawane P . Sentiment analysis of twitter data: a survey of techniques. arXiv preprint. 2016. https:// arxiv. \norg/ abs/ 1601. 06971.\n 24. Howard J, Ruder S. Universal language model fine-tuning for text classification. arXiv preprint. 2018. https:// arxiv. \norg/ abs/ 1801. 06146.\n 25. Liu Y et al. Roberta: A robustly optimized bert pretraining approach. arXiv preprint. 2019. https:// arxiv. org/ abs/ 1907. \n11692.\n 26. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language under-\nstanding. arXiv preprint. 2018. https:// arxiv. org/ abs/ 1810. 04805.\n 27. Lan Z, Chen M, Goodman S, Gimpel K, Sharma P , Soricut R. Albert: a lite bert for self-supervised learning of language \nrepresentations. arXiv preprint. 2019. https:// arxiv. org/ abs/ 1909. 11942.\n 28. Yang ZD, Yang Y, Carbonell J, Salakhutdinov RR, Le QV. Xlnet: Generalized autoregressive pretraining for language \nunderstanding. In: Advances in neural information processing systems. 2019. p. 5754–64.\n 29. El Jundi O, Antoun W, El Droubi N, Hajj H, El-Hajj W, Shaban K. hULMonA: the universal language model in Arabic. In: \nProceedings of the Fourth Arabic Natural Language Processing Workshop. 2019.p. 68–77.\n 30. Antoun W, Baly F, Hajj H. AraBERT: Transformer-based model for Arabic language understanding. arXiv preprint. \n2020. https:// arxiv. org/ abs/ 2003. 00104.\n 31. Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R. Transformer-xl: Attentive language models beyond a fixed-\nlength context. arXiv preprint. 2019. https:// arxiv. org/ abs/ 1901. 02860\n 32. Guellil I, Saâdane H, Azouaou F, Gueni B, Nouvel D. Arabic natural language processing: an overview. J King Saud \nUniv. 2019. https:// doi. org/ 10. 1016/j. jksuci. 2019. 02. 006.\nPage 21 of 21\nAlduailej and Alothaim  Journal of Big Data            (2022) 9:72 \n \n 33. Shaalan K, Siddiqui S, Alkhatib M, Abdel Monem A. (2018) Challenges in Arabic natural language processing. In: \nComputational linguistics, speech and image processing for Arabic language. 2018. p 59–83\n 34. Singh J, Singh G, Singh R. Optimization of sentiment analysis using machine learning classifiers. Hum Cent Comput \nInform Sci. 2017. https:// doi. org/ 10. 1186/ s13673- 017- 0116-3.\n 35. T. Kudo and J. Richardson. Sentencepiece: a simple and language independent subword tokenizer and detokenizer \nfor neural text processing. arXiv preprint. 2018. https:// arxiv. org/ abs/ 1808. 06226\n 36. Raffel C et al. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint. 2019. \nhttps:// arxiv. org/ abs/ 1910. 10683.\n 37. A. Kalaivani and D. Thenmozhi. Sentimental analysis using deep learning techniques. Int J Recent Technol Eng. 2018.\n 38. Dargan S, Kumar M, Ayyagari MR, Kumar G. A survey of deep learning and its applications: a new paradigm to \nmachine learning. Arch Comput Methods Eng. 2019. https:// doi. org/ 10. 1007/ s11831- 019- 09344-w.\n 39. Nikam SS. A comparative study of classification techniques in data mining algorithms. Oriental J Comput Sci Tech-\nnol. 2015;8(1):13–9.\n 40. K. Prasanna, R. SivaRanjani, T. Kanti, and S Ranjan (2017) A study of classification techniques of data mining tech-\nniques in health related research. Int J Innov Res Comput Commun Eng. 2017;5.\n 41. Rosenthal S, Farra N, Nakov P . SemEval-2017 Task 4: Sentiment Analysis in Twitter. arXiv preprint. 2019 https:// arxiv. \norg/ abs/ 1912. 00741.\n 42. Witten I, Frank E, Hall MA, Pal CJ. Data mining: practical machine learning tools and techniques. 3rd ed. Burlington: \nMorgan Kaufmann; 2017.\n 43. Bouazizi M, Ohtsuki T. A Pattern-Based Approach for Sarcasm Detection on Twitter. IEEE Access. 2016;4:1–1.\n 44. Abdelali A, Darwish K, Durrani N, Mubarak H. Farasa: A Fast and Furious Segmenter for Arabic. In: Proceedings of the \n2016 conference of the North American chapter of the association for computational linguistics. 2016. p. 11–6.\n 45. Elnagar A, Khalifa YS, Einea A. Hotel Arabic-reviews dataset construction for sentiment analysis applications. In: \nShaalan K, Hassanien AE, Tolba F, editors. Intelligent natural language processing: trends and applications. Cham: \nSpringer International Publishing; 2018. p. 35–52.\n 46. Aly M, Atiya A. LABR: A Large Scale Arabic Book Reviews Dataset. In: Proceedings of the 51st Annual Meeting of the \nAssociation for Computational Linguistics. Volume 2: Short Papers. 2013. p. 494–8.\n 47. Elnagar A, Einea O. Brad 1.0: Book Reviews in Arabic Dataset. In: IEEE/ACS 13th International Conference of Com-\nputer Systems and Applications (AICCSA). 2016. p. 1–8.\n 48. Alomari K, Elsherif H, Shaalan K. Arabic tweets sentimental analysis using machine learning. In: Benferhat S, Tabia \nK, Ali M, editors. International conference on industrial, engineering and other applications of applied intelligent \nsystems. Cham: Springer; 2017. p. 602–10.\n 49. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. In: Guyon I, Luxburg \nUV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R, editors., et al., Advances in neural information process-\ning systems, vol. 30. Cham: Springer; 2017. p. 5998–6008.\n 50. Arowolo M, Adebiyi M, Nnodim C, Abdulsalam S, Adebiyi A. An adaptive genetic algorithm with recursive feature \nelimination approach for predicting malaria vector gene expression data classification using support vector \nmachine kernels. Walailak J Sci Technol (WJST). 2021;18(17):9849–911.\n 51. Arowolo M, Ogundokun R, Misra S, Kadri A, Aduragba T. Machine learning approach using KPCA-SVMs for predicting \nCOVID-19. In: Garg L, Chakraborty C, Mahmoudi S, Sohmen VS, editors. Healthcare informatics for fighting COVID-19 \nand future epidemics. Cham: Springer; 2022. p. 193–209.\n 52. Arowolo M, AdebiyiAdebiyi MAA, Okesola O. A hybrid heuristic dimensionality reduction methods for classifying \nmalaria vector gene expression data. IEEE Access. 2020;8:182422–30.\n 53. Saheed Y, Arowolo M. Efficient cyber attack detection on the internet of medical things-smart environment based \non deep recurrent neural network and machine learning algorithms. IEEE Access. 2021;9:161546–54.\n 54. Tran H. A survey of machine learning and data mining techniques used in multimedia system. 2019;113:13–21.\n 55. Wu Y, Liu W, Wan S. Multiple attention encoded cascade R-CNN for scene text detection. J Vis Commun Image \nRepresent. 2021;80: 103261.\n 56. Wu Y, Ma Y, Wan S. Multi-scale relation reasoning for multi-modal visual question answering. Signal Process Image \nCommun. 2021;96: 116319.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}