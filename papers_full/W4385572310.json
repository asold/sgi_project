{
  "title": "RETUYT-InCo at BEA 2023 Shared Task: Tuning Open-Source LLMs for Generating Teacher Responses",
  "url": "https://openalex.org/W4385572310",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092596347",
      "name": "Alexis Baladón",
      "affiliations": [
        "Universidad de la República de Uruguay"
      ]
    },
    {
      "id": "https://openalex.org/A5002297943",
      "name": "Ignacio Sastre",
      "affiliations": [
        "Universidad de la República de Uruguay"
      ]
    },
    {
      "id": "https://openalex.org/A5052471374",
      "name": "Luis Chiruzzo",
      "affiliations": [
        "Universidad de la República de Uruguay"
      ]
    },
    {
      "id": "https://openalex.org/A5104021555",
      "name": "Aiala Rosá",
      "affiliations": [
        "Universidad de la República de Uruguay"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034323190",
    "https://openalex.org/W3087130547",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4285778194",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3185181255",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4285129344",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4385570407",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2995874975",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3098258760",
    "https://openalex.org/W3108740071"
  ],
  "abstract": "This paper presents the results of our participation in the BEA 2023 shared task, which focuses on generating AI teacher responses in educational dialogues. We conducted experiments using several Open-Source Large Language Models (LLMs) and explored fine-tuning techniques along with prompting strategies, including Few-Shot and Chain-of-Thought approaches. Our best model was ranked 4.5 in the competition with a BertScore F1 of 0.71 and a DialogRPT final (avg) of 0.35. Nevertheless, our internal results did not exactly correlate with those obtained in the competition, which showed the difficulty in evaluating this task. Other challenges we faced were data leakage on the train set and the irregular format of the conversations.",
  "full_text": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 756–765\nJuly 13, 2023c⃝2023 Association for Computational Linguistics\nRETUYT-InCo at BEA 2023 Shared Task: Tuning Open-Source LLMs for\nGenerating Teacher Responses\nAlexis Baladón and Ignacio Sastre and Luis Chiruzzo and Aiala Rosá\nFacultad de Ingeniería\nUniversidad de la República\nMontevideo, Uruguay\n{alexis.baladon,isastre,luischir,aialar}@fing.edu.uy\nAbstract\nThis paper presents the results of our partic-\nipation in the BEA 2023 shared task, which\nfocuses on generating AI teacher responses in\neducational dialogues. We conducted experi-\nments using several Open-Source Large Lan-\nguage Models (LLMs) and explored fine-tuning\ntechniques along with prompting strategies, in-\ncluding Few-Shot and Chain-of-Thought ap-\nproaches. Our best model was ranked 4.5 in the\ncompetition with a BertScore F1 of 0.71 and a\nDialogRPT final (avg) of 0.35. Nevertheless,\nour internal results did not exactly correlate\nwith those obtained in the competition, which\nshowed the difficulty in evaluating this task.\nOther challenges we faced were data leakage\non the train set and the irregular format of the\nconversations.\n1 Introduction\nNowadays, with the important development of\nLarge Language Models (LLM) and their great gen-\nerative power, the interest in the development of\nchatbots that simulate interactions between humans\nhas increased. In particular, in the educational do-\nmain, the use of chatbots seems to have interesting\nbenefits, such as their potential for adaptive learn-\ning, tailored to each student, or their permanent\navailability (Bibauw et al., 2022).\nThe contributions of these tools to learning are\nnot yet clear (Wollny et al., 2021). In their review\nof the area, these authors conclude that the develop-\nment of chatbots is usually based on technological\ncriteria, but the focus has not yet been placed on\ntheir pedagogical contributions in terms of learning\nimprovements.\nHowever, there is some evidence that for lan-\nguage learning in particular, these tools bring cer-\ntain benefits (Bibauw et al., 2022), mainly for stu-\ndents at initial levels. It should be noted that in\nthe case of language teaching, interaction with the\nagent is in itself an instance of learning practice.\nOne aspect to be studied in the development\nof educational chatbots is their ability to under-\nstand students needs and respond with the style\nthat teachers, trained to educate, use to address\ntheir students (Bommasani et al., 2021). Although\ncurrent LLMs show great capacity for language\ngeneration and for providing relevant -although not\nalways correct or true- answers to different types\nof queries, it is important to study whether these\nmodels can be used in an educational context, be-\ning able to respond to a student by simulating a\ndialogue with a teacher. (Tack and Piech, 2022)\npropose such an evaluation called the AI teacher\ntest challenge.\nThis paper presents the RETUYT-InCo submis-\nsion to the BEA 2023 shared task (Tack et al., 2023)\non generating teacher responses in educational di-\nalogues. In this work, we analyze some particu-\nlarities of the dataset used in the competition, we\ndescribe the approaches we made to solving the\nproblem, and we present the results we obtained,\ntogether with an analysis and discussion of future\nsteps.\n2 Data analysis\nThe following study aims to understand the pat-\nterns and characteristics of the conversations be-\ntween teachers and students, which will be crucial\nfor training a chatbot to generate appropriate re-\nsponses.\n2.1 Dataset content\nFirst, it is important to consider the description pro-\nvided on the official BEA Shared Task webpage1\nand the source of the corpus used in this study. Ac-\ncording to the information available, the corpus\nconsists of extracts from 102 different chatrooms\nwhere an English teacher engages in language ex-\nercises and assesses the English language profi-\nciency of the students (Caines et al., 2020). Each\n1https://sig-edu.org/sharedtask/2023\n756\nextract comprises a series of utterances, represent-\ning turns by the teacher and the student, along with\na response that, as per the competition prompt, al-\nways originates from the teacher. This distinction\nis vital as the objective is not simply to continue a\nconversation but to respond from the perspective\nof a teacher.\nSecondly, upon inspecting the corpus, it was re-\nvealed that the dataset contained additional sets of\nconversations beyond the original composition, as\ndescribed in the corpus paper (Caines et al., 2020)\nand the provided website, which stated a total of\n102 conversations. Hence, we assumed the corpus\nwas composed with a set of extracts from each of\nthose conversations, implying the data inside the\ncorpus is not completely dependent. Interestingly,\nduring the examination of the training corpus, nu-\nmerous tuples were found to be partially duplicated,\nindicating that the conversations in the training set\nwere derived from overlapping segments of the\nsame original conversations. This issue is critical\ndue to two main reasons. First, it is important to\nnote that each teacher’s response does not corre-\nspond to the final utterance of the entire conversa-\ntion but rather the last utterance within an extract\nfrom the conversation (similarly for the first utter-\nance). Moreover, this poses a significant challenge\nwhen it comes to the typical validation approach of\npartitioning the dataset, as it is not immediately evi-\ndent how to separate each conversation in a manner\nthat prevents data leakage across corpus partitions\nwithout hindering the model’s training.\n2.2 Other relevant findings\nThere are several noteworthy characteristics of the\ndataset to consider. Firstly, one of the initial exam-\nples showcased on the official website features a\nstudent attempting to solve a task involving filling\na gap with a word or short phrase (see Fig. 1). How-\never, upon inspecting the number of conversations\nthat contain at least one underscore character (_), it\nwas found that only 14.89% of them met this crite-\nrion. Consequently, while this restriction does not\nsignificantly impact the further architecture of the\nmodel, it is worth mentioning that incorporating\nthis aspect could potentially enhance the model’s\nperformance in future work.\nFurthermore, some tasks within the dataset in-\nvolve choosing between two options (a) or (b) type\nquestions. However, due to the fact that these types\nof questions account for less than 1% of the total\ncorpus, the decision was made not to thoroughly\nanalyze them in this study.\nFigure 1: Example of conversation extract\nIn addition, an examination of the dataset’s\ntags reveals a variety of categories, including\n<STUDENT>, <TEACHER>, <ANOTHER STU-\nDENT>, <CAT’S NAME>, <LIZARD’S NAME>,\nand others. Notably, students and teachers repre-\nsent over 90% of the tags. The presence of specific\nnames and references to animals suggests that the\ndataset covers a wide range of topics related to\nconversations between teachers and students. A\ntable displaying the most frequent tags count can\nbe found in Table 1.\nTag Count\n<STUDENT> 868\n<TEACHER> 141\n<ANOTHER STUDENT> 19\n<CAT’S NAME> 18\n<LIZARD’S NAME> 17\n<STUDENT’S SHORT NAME> 7\n<CAT’S NAME1> 5\n<STUDENT’S FULL NAME> 5\n<LIZARD’S NAME’S> 4\n<TEACHER’S NAME> 3\nTABLE 1: 10 Most Frequent Tags in the Dataset\n2.3 Proportion of conversation utterances\nIn addition to examining other aspects of the cor-\npus, it is important to analyze whether the conver-\nsations exhibit any form of imbalance. Intuitively,\none might expect the student to be more hesitant\nin their participation due to a lack of confidence,\nor conversely, the teacher may encourage the stu-\ndent to contribute more in order to facilitate learn-\ning. Therefore, the rate of text length expressed by\neach participant was assessed using two different\nmeasures: the length of tokens and the number of\nconversation turns.\n757\nTo tokenize the sentences in the dataset, we used\nNLTK’s wordtokenize function (Bird et al., 2009).\nTo understand the distribution of tokens (see Fig. 2,\nthe analysis considered the token count for each\npart of the conversation, namely the teacher, the\nstudent, and both. The teacher’s responses had\nan average of 11.18 tokens, with a standard de-\nviation of 9.37. The student’s responses had an\naverage of 6.00 tokens, with a standard deviation\nof 6.49. When considering both parts of the con-\nversation, the average token count was found to be\n9.07. These findings suggest that the model should\ngenerate responses that are generally longer than\nthose found in the dataset.\nSubsequently, it was measured the same propor-\ntion taking only into consideration the number of\nutteranaces by each speaker. The analysis indi-\ncates that teachers account for 59.47% of the total\nconversation turns. However, it is important to\nacknowledge that this imbalance in the data is a di-\nrect consequence of the last tuple always being the\nteacher’s response. It is also worth highlighting that\nthe turns do not always follow an alternating pattern\nbased on the speaker, as there are instances where\nthe same speaker appears consecutively. This de-\nviation from the typical conversational pattern can\npresent a challenge when training conversational\nchatbots that rely on alternating inputs from differ-\nent speakers.\nFigure 2: Student and Teacher’s token distribution\n3 Experiments\nThis section described the systems implemented to\nsolve the task.\nFigure 3: Prompt used with Alpaca LoRA applied to\nthe example with id train_1504 from the training set.\n3.1 Using pretrained Large Language Models\nOur first approach was trying out open source pre-\ntrained Large Language Models (LLMs), such as\nthe model LLaMA (Touvron et al., 2023) and a fine-\ntuned version for following instructions available\nin Hugging Face, Alpaca LoRA2.\nThe dataset used for the fine-tuning of Alpaca\nLoRA is the one provided in (Taori et al., 2023),\nwhere each example is composed of three sections\n(the second is optional): Instruction, where the task\nis described, Input, which is an optional context for\nthe task and Response, which is the answer to the\ninstruction.\nWe designed a prompt following this format\nbut we adapted it to integrate the whole conver-\nsation to the context. An specific instruction was\ndesigned for this task, and it is provided in the In-\nstruction section. The input section was changed\nfor a conversation section, where the utterances are\npresented in a classical chat format. The response\nsection always starts with “teacher:”, influencing\nthe model to generate a continuation for the con-\nversation as a teacher. An example is presented in\nFig. 3.\nFollowing this experiment, we used an adap-\ntation of the Few-Shot approach explained in\n(Brown et al., 2020), in order to influence the\ngenerated responses with the teacher’s style. For\nchoosing the examples provided in the prompt,\nwe used sentence embeddings generated with the\ngtr-t5-large-1-epoch model in hugging\nface3. An embedding was generated for each of the\nutterances in the training set partition. For gener-\nating a new response, the previous utterances are\n2https://huggingface.co/tloen/\nalpaca-lora-7b\n3https://huggingface.co/cohere-io/\ngtr-t5-large-1-epoch\n758\nFigure 4: Few-Shot prompt used with Alpaca LoRA\napplied to the example with id train_1504 of the training\nset.\nconverted into an embedding and the three most\nsimilar conversations are selected from the training\nset using the k-Nearest Neighbors technique. The\nthree responses of these selected examples are then\nadded to the prompt, as can be seen in Fig. 4.\n3.2 Fine-tuning pretrained Large Language\nModels\nPretrained LLMs tend to perform well in vari-\nous tasks due to scaling up of model size, dataset\nsize diversity, and length of training (Brown et al.,\n2020). However, using these models only with\nprompting techniques does not allow adapting to\na target domain or target task, nor fully leveraging\nthe potential of the training dataset.\nFine-tuning is the process of updating the\nweights of a pre-trained model by using a domain\nspecific dataset in the training step. This technique\ntends to obtain strong performance in many bench-\nmarks (Brown et al., 2020). However, it can be\ncomputationally very costly as all parameters of\nthe LLM need to be updated. This is a major con-\nstraint, and sets a limit to the size of the models\nthat we are able to fine-tune.\nFor this experiments we used the CluserUY in-\nfrastructure (Nesmachnow and Iturriaga, 2019),\nwhich has two servers using NVIDIA A100 GPUs\nand 28 servers using NVIDIA P100 GPUs.\n3.2.1 Experiments updating all the weights\nDialoGPT is a transformer conversational model\ndeveloped by Microsoft. It is based on the\narchitecture of GPT2, which is known for its\neffectiveness in generating coherent and con-\ntextually relevant text. The specific imple-\nmentation of DialoGPT used in our study is\nmicrosoft/dialogpt-large, which has\n762 million parameters (Zhang et al., 2020b).\nDuring training, DialoGPT was exposed to a vast\namount of data, including 147 million conversation-\nlike exchanges. These exchanges were extracted\nfrom Reddit comment chains spanning from 2005\nthrough 2017. This diverse and extensive training\ndata helped DialoGPT learn to generate responses\nthat resemble human-like conversations.\nAs mentioned in (Zhang et al., 2020b), the hu-\nman evaluation results demonstrate that the re-\nsponses generated by DialoGPT exhibit a level of\nquality comparable to human responses in a single-\nturn conversation Turing test. Considering that\nthe competition assesses the similarity to human\nresponses as a metric, leveraging DialoGPT’s per-\nformance has the potential to enhance the metrics\nof our model results.\nIt is important to note that in our study, we\ntrained DialoGPT without specifically optimizing\nits architecture or training process. Our primary\nintention was to assess whether a conversational\nmodel like DialoGPT could achieve comparable\nperformance to other existing models.\n3.2.2 Experiments using Low-Rank\nAdaptation\nThe high computational requirements for fine-\ntuning big LLMs, such as LLaMA 7b, posed a\nsignificant challenge even with access to the Clus-\nterUY infrastructure. The process is not only com-\nputationally costly but also time consuming, which\nmakes the task of training and testing various fine-\ntuned models with different base models or prompt-\ning techniques impractical. To overcome these re-\nstrictions, we opted to use Low-Rank Adaptation\n(LoRA) (Hu et al., 2021) for fine-tuning the bigger\nmodels.\nLoRA is a method for fine-tuning models which\naims to reduce GPU memory requirement by freez-\ning the pretrained model weights and injecting\ntrainable rank decomposition matrices into each\nlayer of the Transformer architecture, reducing the\namount of trainable weights. This method not only\nreduces computing and time requirements, but also\nspace requirements because only the rank decom-\nposition matrices need to be stored, which have\nmuch less parameters than the original matrices.\nFor example, suppose W ∈Mm×n is a weight\nmatrix and ∆W ∈Mm×n is the weight update\n759\nwe want to learn. As shown in (Raschka, 2023),\ninstead of learning ∆W , we can decompose it into\ntwo smaller matrices: ∆W = WmWn, where\nWm ∈ Mm×r, Wn ∈ Mr×n and r is a small\nnumber called rank. Keeping the original weights\nfrozen and only training these new matrices re-\nsults in reducing the amount of trainable parameters\nfrom m∗n to m∗r +r ∗n. After training, the new\nparameters are obtained by doing: W + WmWn.\nUsing the LoRA method, we trained fine-tuned\nversions of OPT 2.7b (Zhang et al., 2022), Bloom\n3b (Scao et al., 2022) and LLaMA 7b (Touvron\net al., 2023). For generating the dataset necessary\nto train all of these models, we adapted the training\nset in the following manner: The utterances and the\nresponse were joined into a string with a classical\nchat format, where every teacher intervention starts\nin a new line with “teacher:” and every student\nintervention starts in a new line with “student:”.\nThe configuration used for fine-tuning these\nmodels with LoRA involved a rank of 16, a scal-\ning factor for the weight matrices of 32, and a\ndropout probability for the LoRA layers of 0.05.\nThe training process employed the AdamW opti-\nmizer, with a total of 200 training steps, a learning\nrate of 2 ×10−4, and a batch size of 4.\n3.3 Preprocessing and Fine-Tuning\n3.3.1 Preprocessing technique\nUpon analyzing the results during the development\nphase, we observed a recurring issue where the\nmodel became confused when attempting to con-\ntinue the conversation from the teacher’s perspec-\ntive after the same teacher had spoken. This dis-\ncrepancy stemmed from the dataset’s structure, as\nit did not adhere to the conventional alternation of\nturns between speakers, which the models typically\nexpect.\nConsequently, even when explicitly specifying\nthat the model should respond as a teacher in the\nprompt or using an input format like \"Teacher:\n<Sentence-Before-Response>\\n Teacher:\", the mod-\nels consistently generated responses from the stu-\ndent’s standpoint. This posed a significant chal-\nlenge not only during the model’s training phase,\nwhere it could become perplexed by the corpus\nstructure, but also during the validation process.\nTo address this issue, we implemented two mod-\nifications:\nCorpus Modification: We adjusted the corpus\nby introducing a structural change. Whenever two\nconsecutive conversations appeared in the original\ncorpus, we combined them into a single utterance\nseparated by a period. This alteration aimed to\ncreate longer utterances that would help the model\ndistinguish between student and teacher interac-\ntions.\nTest-time Adjustment: During testing, if the\nlast utterance belonged to a teacher, we introduced\nan auxiliary phrase into the corpus. This additional\nphrase was carefully crafted to avoid introducing\nnew information to the conversation, ensuring it\ndid not hinder the teacher’s train of thought. We\nopted for the phrase \"Student: I see\\n,\" a common\nexpression used in the corpus and everyday con-\nversations to convey active listening and encourage\nthe other person to continue speaking.\nBy employing these preprocessing techniques,\nwe sought to improve the model’s performance\nby aligning its responses more closely with the\nintended teacher’s perspective while overcoming\nthe challenges posed by the dataset’s structure.\n3.3.2 Fine-Tuned model using the\npreprocessing technique\nThe model in which we used this ad-hoc technique\nwas opt-2.7b (Zhang et al., 2022). OPT, devel-\noped by Meta, is a decoder-only language model\nclosely related to GPT-3. It has been predominantly\npretrained on English text, supplemented with a\nsmall amount of non-English data obtained from\nCommonCrawl. The model’s pretraining process\nemployed a causal language modeling (CLM) ob-\njective, similar to other models in its family. Evalu-\nation of OPT aligns with the prompts and experi-\nmental setup used for GPT-3 (Brown et al., 2020).\nThe decision to employ OPT in this study was\nmotivated by the aim of exploring an alternative\nthat offers both variety and considerable power.\nHowever, it is crucial to acknowledge and address\nthe limitations of this model. Meta AI’s model\ncard highlights that OPT’s training data consists of\nunfiltered internet content, resulting in a significant\nbias embedded within the model.\nThe configuration used for fine-tuning this model\nwas the AdamW optimizer, a learning rate of 0.001\nand a batch size of 4.\n3.4 Combining prompting techniques with\nfine-tuning\nAfter experimenting with prompt-based and fine-\ntuning approaches, a natural evolution was to look\nfor ways to combine both of these techniques. Our\n760\nfirst approach was to fine-tune the model LLaMA\n7b with LoRA using the already explained few-\nshot method. In the same way as before, the three\nmost similar responses in the training set with re-\nspect to the reference response were chosen to be\nadded to the context. We took into consideration\nthat responses from different partitions of the same\nconversation should not be considered for this selec-\ntion. We expected that during fine-tuning, some pat-\nterns that could exist between the similar responses\nand the expected response could be learned.\nRecent works like (Wei et al., 2023) showed that\nadding intermediate reasoning steps that lead to the\nfinal answer for a problem improves the ability of\nLLMs to perform complex reasoning. Inspired on\nthis work, we designed a different solution that tries\nto combine intermediate reasoning and fine-tuning.\nThe training set was modified to include some\ncharacteristics of the response. Initially, two new\nfeatures were added. A binary feature that is set\nto 1 if the response has a question mark, and a\nmulticlass feature that is composed of 28 emotions\ntaken from (Demszky et al., 2020), such as anger,\napproval, curiosity, disapproval, neutral and others.\nTo obtain the second feature for every example in\nthe training set, the EmoRoBERTa model was used\n(Kamath et al., 2022). This model classifies text\ninto the 28 emotions already mentioned.\nThen, a dataset for fine-tuning was constructed.\nEach example of the dataset is a string composed of\nthree sections: Conversation, where the utterances\nare presented in a classical chat format, Reflection,\nwhich is constructed using the already mentioned\nfeatures, and Response, which has the reference\nresponse.\nThe Reflection section is a sentence with two\nparts: The first part indicates the expected emo-\ntion of the response and the second part, which\nis optional, indicates if the expected response is a\nquestion. For example, an example classified as\n“Curiosity” and that is a question would have the re-\nflection: “My response should show curiosity and\nshould be a question”. A complete example can be\nseen in figure 5.\nUsing this dataset, we fine-tuned LLaMA 7b\nwith the already mentioned LoRA technique.\nGiven a new conversation, the model is capable\nof generating a complete reflection and response.\nThe reflection is discarded to get the final response.\nA second version was created using a new fea-\nture that classifies the response length in short, nor-\nFigure 5: Example of the prompt used for the reflection\napproach dataset.\nFigure 6: Character count per example in the training\nset, in ascending order. The green lines indicate the\nthresholds of each class, and the red line indicates the\naverage.\nmal or long. A response is considered short if it\nhas 20 characters or less and long if it has 53 char-\nacters or more. These numbers were selected in\norder to divide the dataset in the most balanced\nway (approximately 1/3rd for each class) as can be\nappreciated in Fig. 6. The reflection sentence was\nchanged to include this information.\n4 Results\nGiven that this work is framed in the context of\nthe BEA 2023 shared task, and the development\nand test sets gold responses were not released un-\ntil after the competition finished, we created our\nown internal split of the training set in 80% for\ntraining and 20% for internal validation. We will\npresent the results of all our experiments against\nthis internal validation data, which we call the inter-\nnal validation phase. For the development and test\nsets, we will only present the results of the systems\nsubmitted to the competition.\nA problem with this internal split, as already\nexplained in the data analysis section, is that it in-\ncludes some repeated utterances across the training\nand validation sets, due to the overlapping that oc-\n761\nExperiment BERTScore DialogRPT\nPrecisionRecall F1 Scoreupdownhuman_vs_randhuman_vs_mach\nFinetuning (LoRA) Bloom 3b 0.840 0.838 0.838 0.495 0.912 0.985\nFinetuning (LoRA) Llama 7b + Reflection0.808 0.840 0.823 0.463 0.881 0.995\nAlpaca LoRA 0.832 0.829 0.830 0.489 0.841 0.986\nAlpaca LoRA + Few Shot 0.836 0.836 0.836 0.480 0.820 0.989\nFinetuning (LoRA) Llama 7b + Few Shot0.802 0.839 0.819 0.465 0.871 0.997\nFinetuning (LoRA) opt 2.7b 0.841 0.832 0.836 0.478 0.748 0.966\nFinetuning opt 2.7b 0.847 0.842 0.844 0.474 0.673 0.981\nFinetuning (LoRA) Llama 7b 0.854 0.841 0.847 0.473 0.642 0.965\nFinetuning (LoRA) Llama 7b + Reflection with length0.850 0.831 0.840 0.465 0.595 0.985\nFinetuning DialoGPT Large 0.700 0.667 0.682 0.462 0.592 0.959\nBaseline 1: Always reply \"Hello\" 0.861 0.805 0.832 0.524 0.305 0.952\nBaseline 2: Always reply \"Cucumber\" 0.723 0.810 0.764 0.503 0.360 0.992\nTABLE 2: Internal validation results.\ncurs in some of the training set tuples. This may\ninfluence the results during evaluation, but we de-\ncided to keep it this way so as not to significantly\nreduce the training set partition.\nTwo evaluation metrics are used in all phases,\nfollowing the indications given in the official web-\nsite of the shared task4: One of them is BERTScore\n(Zhang et al., 2020a), which produces precision,\nrecall, and F1 scores by comparing words in the\ngenerated response with respect to the reference\nresponse using cosine similarity. The other one\nis DialogRPT (Gao et al., 2020), which evaluates\nthe generated response taking into account the\nutterances given as context. The specific Dialo-\ngRPT metrics used are updown, human_vs_rand,\nhuman_vs_machine and final (average and best).\n4.1 Internal evaluation\nDue to the fact that both metrics have multiple hy-\nperparameters that can be tuned differently, the con-\nfiguration used during this internal phase does not\nalign exactly with the one used in the competition.\nFor the BERTScore metric, roberta-large is used\nas the base model and idf weighting is not used.\nMeanwhile, for DialogRPT, the context used are\nthe utterances concatenated in a classical chat for-\nmat and the hypothesis is the generated response.\nTrying out different configurations for Dialo-\ngRPT, we found out that the definition of the con-\ntext to be used has a big influence on the results\nobtained. As no information was provided on how\nthe context was going to be defined in the devel-\nopment and evaluation phases, we made our own\ndefinition and used it consistently during all our\ninternal evaluations.\n4https://sig-edu.org/sharedtask/2023#\nevaluation\nThe results obtained during the internal eval-\nuation for all the described experiments can be\nobserved in Table 2. Besides all the methods de-\nscribed, we include two very simple methods that\nserve as baselines to compare with. In both cases\nthe baseline systems generate the same response to\nall contexts. One baseline always replies \"Hello\",\nand the other always replies \"Cucumber\", so as to\nconsider a more likely and a more unlikely case.\n4.2 Development and evaluation phases\nFor the development phase, we decided to submit\nthe LoRA fine-tuning of the model LLaMA 7b,\nwhich had the best F1 score in the internal phase,\nthe model Alpaca LoRA with the Few-Shot tech-\nnique for the prompt, and the fine-tuned version\nof DialoGPT. We chose to submit these models\nbecause each of them uses a different approach:\nfine-tuning with LoRA, a prompting technique, and\nfine-tuning updating all the weights, respectively. It\nis important to mention that not all the experiments\nwere completed when the deadline for this phase\noccurred.\nDue to an error in the calculation of BERTScore\non CodaLab5, the results obtained in the develop-\nment phase were not correct. This influenced our\ndecisions of what models to send to the evaluation\nphase, given that our internal evaluations did not\nseem to correlate with these obtained results. The\ncorrected results were later published, and can be\nseen in Table 3.\nConsidering that the Alpaca LoRA with Few-\nShot approach was the one that yielded the best\nresults in the development phase, we decided to\nalso submit it in the evaluation phase. Two new\napproaches were also submitted: the LoRA fine-\n5https://codalab.lisn.upsaclay.fr/\n762\nExperiment BERTScore DialogRPT\nPrecisionRecall F1 Scoreupdownhuman_vs_randhuman_vs_machfinal (avg)final (best)\nFinetuning (LoRA) Llama 7b0.72 0.70 0.71 0.36 0.94 0.98 0.32 0.67\nAlpaca LoRA + Few Shot0.68 0.69 0.68 0.37 0.95 0.98 0.33 0.72\nFinetuning DialoGPT Large0.70 0.67 0.68 0.35 0.92 0.98 0.30 0.68\nTABLE 3: Development phase results.\ntuning of LLaMA 7b with reflection in the prompt,\nand the fine-tuning of OPT 2.7b with preprocessing.\nTable 4 shows the results obtained for this phase,\nevaluated over the test set.\n4.3 Observations\nWe observed that fine-tuning a model updating all\nthe weights does not show significant differences\nin comparison to using the LoRA technique. On\na separate note, the results reveal that fine-tuned\nmodels seem to improve the BERTScore results\nover prompting techniques, but the opposite seems\nto happen with DialogRPT metrics. The experi-\nments that try to combine both techniques tend to\nshow competitive results across all metrics.\nAnother observation that derives from the inter-\nnal results (Table 2), is that the \"Hello\" baseline\napproach not only yields good results in the major-\nity of the metrics, but is also the best in BERTScore\nprecision and DialogRPT updown. This seems to\nindicate that these metrics (at least with our configu-\nration) may not fully capture or accurately correlate\nwith human judgement.\n5 Conclusions\nWe presented the experiments we performed for\nthe BEA 2023 shared task on generating teacher\nresponses in educational dialogues. Our methods\nuse the latest open source LLMs in a variety of\nscenarios and incorporating some fine-tuning and\ntargeted prompting strategies for improving the\nperformance.\nThe experiment that yielded best results in the de-\nvelopment phase was the model Alpaca LoRA with\na Few-Shot prompting technique, which ranked\nthird. However, in the evaluation phase, the Fine-\nTuning version of OPT 2.7b with preprocessing\nended up performing better than the previous one,\nand ranked fourth in this phase.\n5.1 Areas of Improvement\nThroughout the competition, several areas were\nidentified where improvements could have en-\nhanced the performance of our chatbot model.\nOn the one hand, further fine-tuning of the\nmodel’s parameters could have been explored to\noptimize its performance. By carefully tuning hy-\nperparameters, we could have potentially achieved\nbetter results in terms of response quality and co-\nherence. Additionally, despite training our mod-\nels using high-performance GPUs (e.g., A100 and\nP100), we faced limitations in testing models with\nmore than 10 billion parameters. Given the ad-\nvancements in model architectures, exploring larger\nmodels could have yielded further improvements\nin chatbot performance. Overcoming hardware\nlimitations and resource constraints would open\navenues for investigating more powerful models in\nfuture iterations. Moreover, to resource and time\nconstraints, our models could not be trained for dif-\nferent number of epochs. Longer training durations\nare often beneficial for improving model perfor-\nmance. Given more resources and time, training\nthe models for multiple epochs could have yielded\nbetter results.\nOn the other hand, one challenge encountered\nduring the competition was data leakage between\nthe internal validation set and the training set. This\nissue, arising from the training dataset, hindered\nthe models’ ability to accurately improve their per-\nformance without overfitting. A more carefully\ncurated validation set, separate from the training\ndata, would have provided a more reliable eval-\nuation metric. Furthermore, regarding the eval-\nuation metircs, BERTScore and DialogRPT, we\nobserved questionable scores when comparing our\nmodel’s performance against a baseline of answer-\ning \"hello\" for every prompt. The BERTScore\nshowed unexpectedly high scores for this base-\nline, while DialogRPT correctly penalized such\nresponses. On top of that, another baseline that re-\nsponded with a fixed word \"cucumber\" consistently\nscored poorly, which aligns with our expectations.\nCareful consideration and refinement of our evalua-\ntion metrics are necessary to ensure their reliability\nand alignment with the desired behavior of chatbot\nmodels.\n763\nExperiment BERTScore DialogRPT\nPrecisionRecallF1 Scoreupdownhuman_vs_randhuman_vs_machfinal (avg)final (best)\nFinetuning (LoRA) Llama 7b + Reflection0.73 0.71 0.72 0.37 0.94 0.98 0.33 0.64\nFinetuning opt 2.7b 0.74 0.68 0.71 0.38 0.90 0.96 0.35 0.65\nAlpaca LoRA + Few Shot 0.72 0.68 0.70 0.37 0.91 0.96 0.34 0.68\nTABLE 4: Evaluation phase results.\n5.2 Ethical limitations\nIt is essential to address the ethical limitations ob-\nserved our fine-tuned OPT model, ranked 4th in\nthe competition. The model card provided by Meta\nAI highlighted that the training data used for their\nmodel consisted of unfiltered internet content, lead-\ning to the presence of significant biases within the\nmodel. These ethical considerations raise concerns\nregarding fairness, inclusivity, and potential biases\nin the responses generated by the model. Further\nresearch and development in addressing these limi-\ntations are imperative to ensure the responsible and\nunbiased deployment of chatbot models.\n5.3 Final thoughts\nIn conclusion, while our chatbot models showcased\npromising performance in the competition, there\nare areas for improvement and important ethical\nconsiderations to be addressed. By focusing on ad-\njusting model parameters, handling specific tokens,\nincreasing training duration, improving validation\nsets as well as their preprocessing, and exploring\nlarger models, future iterations of chatbot models\ncan achieve even greater performance and ensure\nethical deployment.\nAcknowledgements\nSome experiments presented in this paper were car-\nried out using ClusterUY (site: https://cluster.uy).\nReferences\nSerge Bibauw, Wim Van den Noortgate, Thomas\nFrançois, and Piet Desmet. 2022. Dialogue systems\nfor language learning: a meta-analysis. Language\nLearning & Technology, 26(1).\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. \" O’Reilly Media,\nInc.\".\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, S. Buch, Dallas Card,\nRodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen, Kathleen A. Creel, Jared Davis, Dora Dem-\nszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\nmus, Stefano Ermon, John Etchemendy, Kawin Etha-\nyarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lau-\nren E. Gillespie, Karan Goel, Noah D. Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, O. Khat-\ntab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna,\nRohith Kuditipudi, Ananya Kumar, Faisal Ladhak,\nMina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Ma-\nlik, Christopher D. Manning, Suvir P. Mirchandani,\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika\nNarayan, Deepak Narayanan, Benjamin Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJ. F. Nyarko, Giray Ogut, Laurel Orr, Isabel Papadim-\nitriou, Joon Sung Park, Chris Piech, Eva Portelance,\nChristopher Potts, Aditi Raghunathan, Robert Re-\nich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani,\nCamilo Ruiz, Jack Ryan, Christopher R’e, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishna Parasuram Srinivasan, Alex Tamkin,\nRohan Taori, Armin W. Thomas, Florian Tramèr,\nRose E. Wang, William Wang, Bohan Wu, Jiajun\nWu, Yuhuai Wu, Sang Michael Xie, Michihiro Ya-\nsunaga, Jiaxuan You, Matei A. Zaharia, Michael\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,\nLucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021.\nOn the opportunities and risks of foundation models.\nArXiv.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nAndrew Caines, Helen Yannakoudakis, Helena Edmond-\nson, Helen Allen, Pascual Pérez-Paredes, Bill Byrne,\nand Paula Buttery. 2020. The teacher-student chat-\nroom corpus. arXiv preprint arXiv:2011.07109.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A Dataset of Fine-Grained Emo-\ntions. In 58th Annual Meeting of the Association for\nComputational Linguistics (ACL).\n764\nXiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett,\nand Bill Dolan. 2020. Dialogue response ranking\ntraining with large-scale human feedback data.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nRohan Kamath, Arpan Ghoshal, Sivaraman Eswaran,\nand Prasad Honnavalli. 2022. Emoroberta: An en-\nhanced emotion detection model using roberta. SSRN\nElectronic Journal.\nSergio Nesmachnow and Santiago Iturriaga. 2019.\nCluster-uy: Collaborative scientific high performance\ncomputing in uruguay. In Supercomputing, pages\n188–202, Cham. Springer International Publishing.\nSebastian Raschka. 2023. Parameter-efficient llm fine-\ntuning with low-rank adaptation (lora).\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Ro-\nman Castagné, Alexandra Sasha Luccioni, François\nYvon, Matthias Gallé, et al. 2022. BLOOM:\nA 176B-Parameter Open-Access Multilingual Lan-\nguage Model. arXiv preprint arXiv:2211.05100.\nAnaïs Tack, Ekaterina Kochmar, Zheng Yuan, Serge\nBibauw, and Chris Piech. 2023. The BEA 2023\nShared Task on Generating AI Teacher Responses in\nEducational Dialogues. In Proceedings of the 18th\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, page to appear, Toronto,\nCanada. Association for Computational Linguistics.\nAnaïs Tack and Chris Piech. 2022. The ai teacher test:\nMeasuring the pedagogical ability of blender and gpt-\n3 in educational dialogues. ArXiv, abs/2205.07540.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nS. Wollny, J. Schneider, D. Di Mitri, J. Weidlich, M. Rit-\ntberger, and H. Drachsler. 2021. Are we there yet? - a\nsystematic literature review on chatbots in education.\nFrontiers in artificial intelligence.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020a. Bertscore: Eval-\nuating text generation with bert.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020b. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration.\n765",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.722656786441803
    },
    {
      "name": "Competition (biology)",
      "score": 0.6357446908950806
    },
    {
      "name": "Open source",
      "score": 0.5929582715034485
    },
    {
      "name": "Computer science",
      "score": 0.5849529504776001
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.566349983215332
    },
    {
      "name": "Shot (pellet)",
      "score": 0.4525853097438812
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3399204909801483
    },
    {
      "name": "Management",
      "score": 0.12550118565559387
    },
    {
      "name": "Chemistry",
      "score": 0.09440860152244568
    },
    {
      "name": "Economics",
      "score": 0.08052688837051392
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Software",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I180910786",
      "name": "Universidad de la República de Uruguay",
      "country": "UY"
    }
  ],
  "cited_by": 9
}