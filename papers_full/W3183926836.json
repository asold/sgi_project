{
    "title": "Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge",
    "url": "https://openalex.org/W3183926836",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4287201518",
            "name": "Pedinotti, Paolo",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A4287201519",
            "name": "Rambelli, Giulia",
            "affiliations": [
                "Aix-Marseille Université",
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A3095497989",
            "name": "Chersoni, Emmanuele",
            "affiliations": [
                "Hong Kong Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A4287070266",
            "name": "Santus, Enrico",
            "affiliations": [
                "Bayer (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2601888847",
            "name": "Lenci, Alessandro",
            "affiliations": [
                "University of Pisa"
            ]
        },
        {
            "id": "https://openalex.org/A2904316978",
            "name": "Blache, Philippe",
            "affiliations": [
                "Aix-Marseille Université"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2606373437",
        "https://openalex.org/W2513018281",
        "https://openalex.org/W2950641397",
        "https://openalex.org/W3032774043",
        "https://openalex.org/W2115016144",
        "https://openalex.org/W2963235663",
        "https://openalex.org/W2155870214",
        "https://openalex.org/W2052742452",
        "https://openalex.org/W2741912437",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W3135350162",
        "https://openalex.org/W2563981795",
        "https://openalex.org/W3128846734",
        "https://openalex.org/W2807632309",
        "https://openalex.org/W2295874646",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2965758391",
        "https://openalex.org/W2147381218",
        "https://openalex.org/W3116605602",
        "https://openalex.org/W2120660105",
        "https://openalex.org/W2128870637",
        "https://openalex.org/W2009342863",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2738438234",
        "https://openalex.org/W1564174355",
        "https://openalex.org/W3113885981",
        "https://openalex.org/W2945554776",
        "https://openalex.org/W2073321553",
        "https://openalex.org/W2153791616",
        "https://openalex.org/W2096423148",
        "https://openalex.org/W2051534348",
        "https://openalex.org/W2251735937",
        "https://openalex.org/W2989150936",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2964275548",
        "https://openalex.org/W2054690340",
        "https://openalex.org/W3160747807",
        "https://openalex.org/W2963353834",
        "https://openalex.org/W2165131017",
        "https://openalex.org/W2910243263"
    ],
    "abstract": "Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge). Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the \\textit{dynamic estimation of thematic fit}. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and we conducted a detailed error analysis to investigate which factors affect their behavior. Our results show that TLMs can reach performances that are comparable to those achieved by SDM. However, additional analysis consistently suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, such as frequent words, collocations and syntactic patterns, thereby showing sub-optimal generalization abilities.",
    "full_text": "Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 1–11\nAugust 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics\n1\nDid the Cat Drink the Coffee?\nChallenging Transformers with Generalized Event Knowledge\nPaolo Pedinotti\nUniversity of Pisa\npaolo.pedinotti@phd.unipi.it\nGiulia Rambelli\nUniversity of Pisa - Aix-Marseille University\ngiulia.rambelli@phd.unipi.it\nEmmanuele Chersoni\nThe Hong Kong Polytechnic University\nemmanuelechersoni@gmail.com\nEnrico Santus\nBayer Pharmaceuticals\nesantus@gmail.com\nAlessandro Lenci\nUniversity of Pisa\nalessandro.lenci@unipi.it\nPhilippe Blache\nAix-Marseille University\nblache@lpl-aix.fr\nAbstract\nPrior research has explored the ability of com-\nputational models to predict a word seman-\ntic ﬁt with a given predicate. While much\nwork has been devoted to modeling the typ-\nicality relation between verbs and arguments\nin isolation, in this paper we take a broader\nperspective by assessing whether and to what\nextent computational approaches have access\nto the information about the typicality of en-\ntire events and situations described in language\n(Generalized Event Knowledge).\nGiven the recent success of Transformers Lan-\nguage Models (TLMs), we decided to test\nthem on a benchmark for the dynamic estima-\ntion of thematic ﬁt . The evaluation of these\nmodels was performed in comparison with\nSDM, a framework speciﬁcally designed to in-\ntegrate events in sentence meaning representa-\ntions, and we conducted a detailed error analy-\nsis to investigate which factors affect their be-\nhavior. Our results show that TLMs can reach\nperformances that are comparable to those\nachieved by SDM. However, additional anal-\nysis consistently suggests that TLMs do not\ncapture important aspects of event knowledge,\nand their predictions often depend on surface\nlinguistic features, such as frequent words, col-\nlocations and syntactic patterns, thereby show-\ning sub-optimal generalization abilities.\n1 Introduction\nPeople can discriminate between typical (e.g., A\ncop arrested a thief ) and atypical events (e.g., A\nthief arrested a cop) and exploit this ability in on-\nline sentence processing to anticipate the upcoming\nlinguistic input. Brains have been claimed to be\n“prediction machines” (Clark, 2013) and psycholin-\nguistic research has shown that a crucial ingredient\nof such predictive ability is the knowledge about\nevents and their typical participants stored in hu-\nman semantic memory, also referred to as Gener-\nalized Event Knowledge (GEK ) by McRae and\nMatsuki (2009). To make an example, if we were\nasked to think about things that are played with a\nguitar, we would quickly and more or less unani-\nmously think of words such as song, piece or riff.\nComputational models of predicate-argument\ntypicality, generally referred to asthematic ﬁt in the\npsycholinguistic literature (McRae et al., 1998), ex-\ntract typical arguments from parsed corpora. How-\never, GEK is not just storing relations between\nwords: The fact that this knowledge is generalized\n– that is, it is based on an abstract representation\nof what is typical – allows us to easily classify\nnew argument combinations as typical or atypi-\ncal. Furthermore, psycholinguistic studies (Bick-\nnell et al., 2010; Matsuki et al., 2011) have shown\nthat humans are able to combine and dynamically\nupdate their expectations during sentence process-\ning: for example, their expectations given the se-\nquence The barber cut the differ from the ones\ngiven The lumberjack cut the , since the integra-\ntion of knowledge “cued” by the agent argument\nwith the verb will lead to the activation of differ-\nent event scenarios. In Distributional Semantics,\nsophisticated models of the GEK have been pro-\nposed to make predictions on upcoming arguments\nby integrating the cues coming from the verb and\nthe previously-realized arguments in the sentence\n(Lenci, 2011; Chersoni et al., 2019). Since such\nknowledge is acquired from both ﬁrst-hand and\nlinguistic experience (McRae and Matsuki, 2009),\nan important assumption of this literature is that,\nat least for its ”linguistic subset”, the GEK can be\nmodeled with distributional information extracted\n2\nfrom corpora (Chersoni et al., 2017, 2021).\nLanguage Models are trained to make predic-\ntions given a context, and thus, they can also\nbe viewed as models of GEK . This approach is\npromising if one considers the success of recent\nTransformer-based Language Models (henceforth\nTLM S), which are trained on huge corpora and\ncontain a massive number of parameters. Even if\nthese models receive extensive training and have\nbeen shown to capture linguistic properties (Jawa-\nhar et al., 2019; Goldberg, 2019), it is not obvious\nwhether they acquire the aspects of GEK that have\nbeen modeled explicitly in previous approaches.\nTo the best of our knowledge, Transformers have\nnever been tested on dynamic thematic ﬁt model-\ning, nor their performance has been compared with\ntraditional distributional models. Our current work\nis addressing this issue.\nContributions:\n1. we propose a methodology to adapt TLM S to\nthe dynamic estimation of thematic ﬁt, using a\ndataset that contains several types of argument\ncombinations differing for their typicality;\n2. we present a comprehensive evaluation of var-\nious TLM S on this task, performed by com-\nparing them to a strong distributional baseline;\n3. we conduct further analysis aimed at identi-\nfying the potential limitations of TLM S as\nmodels of GEK.\nOur results are relevant for researchers interested in\nassessing the linguistic abilities of TLM S, as well\nas those working on applications involving TLM S,\nsuch as text generation.\n2 Related Work\nIn its classical form, the thematic ﬁt estimation task\nconsists in comparing a candidate argument orﬁller\n(e.g., wine) with the typical ﬁllers of a given verb\nrole (e.g., agent, patient, etc.), either in the form\nof exemplars previously attested in a corpus (Erk,\n2007; Vandekerckhove et al., 2009; Erk et al., 2010)\nor in the form of a vector-based prototype (Baroni\nand Lenci, 2010; Sayeed and Demberg, 2014; Say-\need et al., 2015; Greenberg et al., 2015a,b; Sayeed\net al., 2016; Santus et al., 2017; Chersoni et al.,\n2020). Additionally, recent studies explored the\nuse of masked language modeling with BERT for\nscoring the candidate arguments (Metheniti et al.,\n2020). Performance in the thematic ﬁt task is typ-\nically measured with the correlation between the\noutput scores of the model and human-elicited typ-\nicality judgments for verb-argument pairs (McRae\net al., 1998; Ferretti et al., 2001; Pad´o, 2007; Zhang\net al., 2019; Marton and Sayeed, 2021).\nIn the simplest and most common version of\nthis task, the typicality of verb argument-pairs is\nevaluated in isolation. Thematic ﬁt is instead a\ndynamic concept: The expectations for an argu-\nment in a given verb role do not depend just on the\nverb, but also on the compositional combination\nwith the other arguments in the sentence (Bicknell\net al., 2010). To check the ability of computational\nmodels to account for the compositional update of\nargument expectations, Lenci (2011) framed the\nproblem as a binary classiﬁcation task: A system\nis presented a sentence pair, with one sentence ex-\npressing a typical real-world situation (The journal-\nist is checking the report) and the other sentence\nexpressing a plausible but less typical one ( The\nmechanic is checking the report), and the task is\nto assign a higher thematic ﬁt/typicality score to\nthe former. Notice that the two sentences differ\nonly for one argument, and that the “atypical” one\nmight, however, be a common ﬁller with respect to\nthe verb target role (e.g., report is a typical patient\nfor check, it is just less plausible in combination\nwith mechanic as an agent).\nSeveral models have tried to tackle the “dy-\nnamic” version of the thematic ﬁt task, either based\non classical distributional spaces (Chersoni et al.,\n2016, 2019) or on more sophisticated neural net-\nwork architectures (Tilk et al., 2016; Hong et al.,\n2018). On the evaluation side, those works made\nuse of the experimental materials of the study by\nLenci (2011), which are, however, limited to agent-\nverb-patient triples. The recently-introduced DT-\nFit dataset (Vassallo et al., 2018) is, in compari-\nson, larger in size and provides more variety of\nﬁllers and roles (including instruments, locations\nand time). Other studies introduced larger datasets,\nbut focused on more speciﬁc notions of event plau-\nsibility (e.g. the plausibility depending on the phys-\nical properties of the participants) (Wang et al.,\n2018; Porada et al., 2019; Ko et al., 2019).\n3 Experimental Settings\n3.1 Dataset\nThe DTFit (Vassallo et al., 2018) dataset has been\nspeciﬁcally designed for the evaluation of dynamic\n3\nthematic ﬁt. 1 The dataset contains pairs of tuples\nthat differ only for one element, which can be ei-\nther a typical or atypical ﬁller of a given role in\nthe event described by the tuple (cf. Table 1). The\ndataset includes tuples of different lengths, and the\ntypicality of a given argument depends on its inter-\naction with all the other elements. For each tuple,\nthe authors collected typicality judgments by ask-\ning English native speakers how common was the\nevent described. Scores range from 1 (very atypi-\ncal) to 7 (very typical). The dataset mainly targets\nknowledge about professions, but also other typical\neveryday situations (e.g., what a dog typically eats,\nwhat a grandmother typically does).\nThe authors created several datasets, which dif-\nfer with respect to the semantic role of the can-\ndidate ﬁller. For our experiments, we selected\nthe datasets created by the authors for the follow-\ning relations: {Instrument, Time, Location}DTFit.\nAdditionally, from the original dataset containing\nagent-verb-patient triples, we derived two datasets,\nthat we named AgentDTFit and PatientDTFit. In\nAgentDTFit, the tuples forming a pair differ with re-\nspect to the typicality of the agent. In PatientDTFit,\nthey differ for the typicality of the patient. We thus\nget a total of ﬁve datasets, each of which covers a\ndifferent semantic relation. The latter two datasets\nhave the same properties of the others, but they\nput stronger emphasis on the dynamic nature of\nthematic ﬁt, as the atypical ﬁller is still a typical\ncomplement of the verb alone. Conversely, the\natypical candidate ﬁllers in the other datasets are\nappropriate ﬁllers of the role, but, in most cases,\nthey do not relate to the other elements of the tuple.\nTherefore, AgentDTFit and PatientDTFit are more\nchallenging for computational models, as the typ-\nicality of a ﬁller can only be determined through\nthe composition of the verb with another argument.\nAccordingly, models have to update their predic-\ntions by accurately taking into account the whole\ncontext.\nFor each tuple in DTFit, the task for our models\nis to predict the upcoming argument on the basis\nof the previous ones. Models were evaluated in\nterms of Spearman correlation between the human\nratings and the models’ scores. Moreover, we per-\nformed a second evaluation for AgentDTFit and Pa-\ntientDTFit, consisting of measuring the accuracy of\neach system in assigning a higher thematic ﬁt score\n1All the datasets used for the experiments described in this\npaper can be found at the link: https://github.com/\ngiuliarambelli/transformers_thematic_fit.\nRole Tuple TypicalAtypical\nAgent mix paint painter cook\nPatient tailor sew dress wound\nInstrumentcook clean ﬁsh knife sponge\nTime cat chase bird hunting marriage\nLocation sailor mop deck boat theatre\nTable 1: Examples of tuples from DTFit.\nto typical tuples. To the best of our knowledge, the\nonly attempts to test computational models on this\ndataset have been done by the authors of the orig-\ninal paper and by Chersoni et al. (2019). In both\nworks, distributional prototype models of thematic\nﬁt have been used.\n3.2 Models\nIn our experiments, we compared the performance\nof TLM S with the Structured Distributional Model\n(SDM ), which has been recently shown to be an\nefﬁcient model for the dynamic estimation of the-\nmatic ﬁt (Chersoni et al., 2019).\n3.2.1 Structured Distributional Model\nThe Structured Distributional Model (SDM )\nproposed by Chersoni et al. (2019) combines word\nembeddings and formal semantics to speciﬁcally\nrepresent GEK and the dynamic construction of\nsentence meaning. Like traditional distributional\nmodels of thematic ﬁt, it builds a prototype repre-\nsentation for a given role (e.g., the typical patient\nof sing) from its typical ﬁllers, but its novelty is\nthat the ﬁllers are retrieved from an external re-\nsource called Distributional Event Graph (hence-\nforth, DEG). DEG represents GEK as a graph\nautomatically built from parsed corpora, where the\nnodes are words associated to a numeric vector,\nand the edges are labeled with syntactic relations\nand weighted using statistic association measures.\nThus, given a lexical cue w, it is possible to iden-\ntify the events in which wtakes part and to retrieve\nwords related to won both the paradigmatic and\nthe syntagmatic axis.\nThe formal structure at the basis of SDM con-\nsists of two semantic structures: the linguistic con-\ndition (LC), a context-independent tier of meaning\nthat represents the lexical items in a sentence, and\nthe active context (AC), which accumulates con-\ntextual information activated by lexical items. The\ncrucial aspect of SDM is that it associates a vector\nrepresentation to these formal structures: ⃗LC is\nthe sum of the embeddings of the lexical items of\n4\na sentence; ⃗AC, for each syntactic slot, is repre-\nsented as the centroid vector built out of the role\nvectors ⃗ r1,..., ⃗ rn available in AC, corresponding\nto the syntactic associates of the lexical items that\nhave been already processed.\nIn our implementation of SDM , the DEG was\nconstructed by extracting syntactic relations from a\nconcatenation of the ukWaC corpus (Baroni et al.,\n2009), a dump of Wikipedia 2018 and the British\nNational Corpus (Leech, 1992). The ﬁnal graph\ncontains words with a minimum frequency of 300\nand events with a minimum frequency of 30. We\nused as lexical embeddings the publicly-available\nFastText vectors extracted from Wikipedia.2 For\nour experiments, we built a semantic representation\nfor each tuple in the dataset, like in Chersoni et al.\n(2019). We used the information in LC and AC\nto assign a typicality score to each candidate ﬁller\nof a role in the dataset. The scoring function for a\ngiven role ﬁller is the following:\ncos( ⃗f, ⃗LC(sent)) +cos( ⃗f, ⃗AC(sent))\n2 (1)\nwhere ⃗f is the embedding of the candidate ﬁller;\n⃗LC(sent) is a vector obtained from the sum of the\nembeddings of the verb and of the argument other\nthan f; ⃗ACstands for the updated expectation pro-\ntotype for the role ﬁlled by f. In other words, we\nquantify the typicality of an argument given a tuple\nas the average of i.) the cosine similarity between\nthe argument embedding and the additive combi-\nnation of the other argument vectors ( ⃗LC), and\nii.) the cosine similarity between the argument em-\nbedding and the prototype vector representing the\nactive context ( ⃗AC). In the cases where ⃗ACcannot\nbe derived (because DEG does not store syntac-\ntic relations involving the context words), we take\nonly the cosine between ⃗f and ⃗LC(sent) as the\nﬁnal score.\n3.2.2 Transformer-based Language Models\nWe experimented with four TLM S to test how dif-\nferent architectures, training objectives, and sizes\nof the training corpus affect performance.3\nBERT (Devlin et al., 2019) consists of a series of\nstacked Transformer encoders. It was trained using\nboth a masked language modeling objective (i.e.,\n2https://fasttext.cc/docs/en/\nenglish-vectors.html\n3For all experiments involving TLM S, we use pre-trained\nmodels available in the HuggingFace’s Python library Trans-\nformers (Wolf et al., 2019).\npredicting a masked word from its left- and right-\ncontext), and a next sentence prediction objective\n(i.e., whether a sentence follows another sentence\nor not), on a combination of the BooksCorpus and\nEnglish Wikipedia (13GB in total). The model uses\nWordPiece vocabulary. To test if the model size can\naffect BERT performance, we used both the base\n(Number of layers=12, Hidden size=768) and the\nlarge (L=24, H=1024) versions.\nRoBERTa(Liu et al., 2019), which we used in\nthe large version, is based on the same architec-\nture as BERT, but it was trained on a much larger\ncorpus (160GB) and without the next sentence pre-\ndiction objective. In our experiments, we used the\nlarge version (L=24, H=1024).\nIn contrast with the bidirectional nature of BERT\nand RoBERTa, GPT2 (Radford et al., 2019) is a\nuni-directional LM, which means that the training\nobjective is to predict the next word, given all of\nthe previous ones. It was trained on WebText, for a\ntotal of 8 million documents of data (40 GB). We\nemployed the medium version of GPT2 (L=24,\nH=1024). We chose GPT2-medium since its di-\nmensions are comparable to those of BERT and\nRoBERTa large. Moreover, both RoBERTa and\nGPT2 make use of a Byte-Pair Encoding tokenizer.\nFor our investigation, we designed the experi-\nment as follows. First, we derived simple sentences\nfrom the tuples by adding deﬁnite articles to the\nwords, [CLS] at the beginning of the input and a pe-\nriod to signal the end of the sentence (e.g., [CLS]\nThe tailor sewed the dress.). Then,\nwe masked the candidate ﬁller (dress in the exam-\nple) and we computed the probability distribution\nof the entire model’s vocabulary for that position.\nThe model typicality score is the probability as-\nsigned to the candidate ﬁller, when the candidate\nﬁller is included in the model’s vocabulary. In\ncase a word to be scored is not included in the\nvocabularies of all the models that we used, we\ndecided to disregard its tuple and the respective\ntypical/atypical counterpart. For this reason, the\nﬁnal results only take in consideration a subset of\nthe original datasets, which varies from model to\nmodel. Additionally, we computed a baseline for\neach Transformer model, where the model is pre-\nvented from attending to the other tokens in the\nsequence when making predictions.\n5\nCoverage SDM BERT-base(line) BERT-large ROBERTA-large GPT-2 medium\nAgentDTFit 105/134 0.58 0.46 (0.1) 0.53 0.64 -\nPatientDTFit 323/402 0.62 0.59 (0.06) 0.64 0.64 0.63\nInstrumentDTFit 31/100 0.58 0.52 (0.08) 0.53 0.5 0.5\nTimeDTFit 89/100 0.58 0.63 (0.06) 0.64 0.66 0.66\nLocationDTFit 115/150 0.65 0.72 (0.06) 0.71 0.73 0.74\nTable 2: Spearman Correlation for the DTFit datasets.\n(a)\n (b)\n(c)\n (d)\nFigure 1: Correlation of elicited judgments and model-derived scores for AgentDTFit (a-b) and PatientDTFit (c-d)\ndatasets.\n4 Results and Analysis\nIn this section, we provide the results of the exper-\niments on the DTFit datasets. Since the models\ncover different portions of the original tuples, we\nperformed the evaluation over the common pairs.\nTable 2 reports the correlation scores for all the\nﬁve datasets.4 Values in brackets refer to the Spear-\nman correlation obtained by the baseline. As the\nbaseline scores are very similar across models, we\nreported the results only for BERT-base.\nAt a glance, we observe that both SDM and\nTLM S obtain quite strong correlations, going from\n0.46 to a maximum of 0.74 across datasets and\n4We do not computed GPT-2 scores forAgentDTFit, as the\nmodel cannot make predictions based on context because the\ncandidate ﬁller occurs at the beginning of the sentence.\nmodels. Speciﬁcally, we notice that TLM S tend to\nreach higher performances compared to the distri-\nbutional approach. However, a marginally signif-\nicant improvement of the correlations over SDM\nis obtained only for LocationDTFit (p <0.05 for\nLocations, p <0.1 for the other roles).5 This re-\nsult is interesting, considering that SDM is trained\non a really small corpus compared to TLM S (for\ninstance, RoBERTa is trained on 160 GB of text).\nAnother remark is that even if TLM S differ for ar-\nchitecture, training objective and data, BERT-large,\nRoBERTa and GPT-2 tend to achieve very similar\nperformances, while correlation scores are lower\nfor BERT-base.\nAs there is no signiﬁcant difference between\n5The p-value was computed with Fisher’s r-to-z transfor-\nmation, one-tailed test.\n6\nSDM and TLM S results, we plotted an example\nof the relationship between the human ratings and\nthe model-derived scores to provide a better picture\nof the models’ predictions. For visualization pur-\nposes, we applied a logarithmic transformation to\nthe scores. For AgentDTFit, we observe that SDM\nand BERT-large have a different trend. In the for-\nmer (see Figure 1a), the majority of the points fol-\nlow a roughly linear relationship, and there is a\nsmall variation around the regression line (with few\noutliers). On the contrary, BERT-large scores show\nmore variance (Figure 1b). This trend is conﬁrmed\n(even if it is less evident) for PatientDTFit, where\nboth SDM (Figure 1c) and BERT-large (Figure\n1d) have a large amount of variance, and quite a\nfew outliers. To verify these observations, we com-\npared the sum of the BERT-large residuals with that\nof SDM (we ﬁrst normalized the models’ scores\nwith min-max scaling in order to make them com-\nparable). For both subsets, the sum of residuals is\nhigher for BERT-large than SDM , which is espe-\ncially the case for AgentDTFit (31.43 versus 17.85;\n67.04 versus 63.47 for PatientDTFit).\nFinally, we also performed a binary classiﬁca-\ntion task for AgentDTFit and PatientDTFit. In this\ncase, we evaluated models on their ability to assign\na higher score to the ﬁller in the typical condition.\nAs shown in Table 3 (left columns), the accuracy\nvalues are always high and the TLM S scores are\ncomparable with the SDM ones.\n5 Do Transformers Really Encode GEK?\nThe above results prima facie suggest that TLM S\nare able to model the dynamic interaction between\nthe sentence elements to compute the typicality\nvalue of a candidate ﬁller. However, analyzing the\nerrors of the TLM S can be revealing of how they\nmake their predictions.\nTable 4 presents some of the PatientDTFit pairs\nwhere BERT-base prefers the atypical ﬁller. In all\nthese cases, BERT simply seems to rely on frequent\nverb objects, without composing and integrating\nthe verb expectations with information from other\nelements of the context (the agent in this case),\nwhich is a key aspect of human GEK and is re-\nﬂected in the typicality judgments. However, we\ncannot make any claims about the event knowledge\nof TLM S from these examples alone, as only in\nsome cases (such as The cat drank the coffee) the\natypical tuples evoke events unlikely to take place\nin the real world (i.e., it may happen frequently that\nDTFit Wang2018\nAgent Patient Agent Patient\nSDM .89 .91 .65 .66\nBERT-base .77 .85 .76 .63\nBERT-large .83 .89 .77 .65\nROBERTA-large .89 .91 .76 .73\nGPT-2 medium - .90 - .64\nTable 3: Accuracy in the binary classiﬁcation task for\nDTFit (agent and patient roles) and Wang2018 datasets.\na chemist pours the juice, even if this is not a typ-\nical action for a chemist). To better understand if\nthis can lead TLM S to make really implausible pre-\ndictions, we carried out an additional experiment\nwhere we tested the models on a diagnostic dataset\ncontrolled for the frequency of the association be-\ntween the verb and the ﬁller. In this experiment,\nwe also tried to address the question of whether\nTLM S rely more heavily on the local context when\nmaking predictions.\nFurthermore, TLM S’ natural preference for\nwhat is more frequent could help them in the typ-\nicality task, as a typical event is often a frequent\none. Their good performance could be due to the\nfact that they memorize frequent sequences during\ntraining. Therefore we tested TLM S on a different\ndataset, in which atypical but physically plausible\nevents (e.g., The cloth erased the cream) are distin-\nguished from atypical and implausible ones (e.g.,\nThe cloth erased the house). Frequency effects on\nperformance should be alleviated in this setting,\nas both types of events in the dataset are atypical\nand, hence, rare. This task requires ﬁne-grained\nknowledge of the properties of arguments, which\nis still an important component of GEK.\nAdditionally, different frequency variations in\nthe training data could inﬂuence TLM S perfor-\nmance. Since the models’ knowledge of the world\nis mediated by language, it is likely that an argu-\nment ﬁller may or may not be predicted depending\non the frequency of the word chosen to refer to it.\nWe investigated this issue by testing the models on\nanother diagnostic dataset obtained by replacing\ntypical ﬁllers with low-frequency synonyms.\nThe last question we explored is whether TLM S\ncan be inﬂuenced by the way statements of event\ntypicality are syntactically expressed. So, we eval-\nuated TLM S by feeding them with sentences en-\ncoding typical events with a transformed and more\ncomplex syntactic form than the one used in the\nDTFit experiments.\n7\nTuple Expected Preferred\nmason mix cement (H=6.65, M=-8.41) soup (H=1.95, M=-5.54)\nclimber climb rock (H=6.8, M=-5.29) staircase (H=5.6, M=-4.05)\nblacksmith pour metal (H=6.5, M=-4.03) wine (H=1.6, M=-1.6)\nchemist pour compound (H=6.25, M=-8.4) juice (H=2.75, M=-5.18)\ncat drink milk (H=5.6, M=-2.89) coffee (H=1.45, M=-3.65)\nTable 4: Examples of errors (BERT-base, PatientDTFit). H= Human score, M=Model’s log probability.\nI. TLM S seem to prefer frequent collocations,\nbut only when they are plausible. Errors re-\nported in Table 4 suggest the tendency of TLM S\nto predict frequent complements of the verbs, ir-\nrespective of whether they are coherent with the\nrest of the tuple. We questioned to what extent\nsalient local word co-occurrences make the models\n“blind” to the rest of the context and thus com-\npromise the plausibility of their predictions. To\ninvestigate this behavior, we generated a new di-\nagnostic dataset. The dataset is a small (31 pairs)\nsubset of PatientDTFit, where the atypical ﬁller in\neach pair was replaced with another noun that has\na very strong association with the verb in the tuple.\nWe computed the association between the verb and\nits direct objects using Local Mutual Information\n(LMI) (Evert, 2008). Since LMI is computed by\nmultiplying the Pointwise Mutual Information and\nthe frequency of the two words in a grammatical\nrelation, it assigns higher values to combinations\nthat are both common and informative. We chose\nthe new atypical ﬁllers among the words with the\nhighest LMIs. We chose words that give rise to odd\nevents when integrated with the rest of the context.\nTo approximate the word distributions encountered\nin the training data, we extracted LMI values from\na 2018 dump of English Wikipedia and we evalu-\nated only the BERT model (base and large) on the\nnew dataset, as Wikipedia is a considerable part\nof the training only for this model. Examples of\nthe new test pairs are the following: The terrorist\nreleased the hostage/ album, The truck hit the car/\nball, The soldier heard the command/ case.\nTo evaluate BERT performance, we computed\nthe accuracy scores on the diagnostics dataset in\nthe same way as in the main experiment (binary\nclassiﬁcation task). Results show that the models\ngenerally assign low probabilities to atypical ﬁllers.\nThey choose the atypical event in some cases ( 9\nin BERT-base, 6 in large), but mainly when the\ncontrast between the atypical event and our ex-\npectations is less evident ( The smuggler sold the\nproperty is preferred to weapon, The soldier throw\nthe ball is preferred to bomb).\nAs already observed in the main experiment,\nBERT seems to be able to look beyond salient lo-\ncal associations and build representations of global\nevents ﬂexibly. However, this issue should be fur-\nther explored for the other roles as well. For in-\nstance, given the sentence The engineer completed\nthe project in the , the models must consider more\ncontextual elements to make the correct prediction.\nOn the other hand, even if SDM design aims\nat capturing this aspect of GEK , the manipula-\ntions we made in this dataset cause a drop in the\nmodel performance (14 pairs out of 31 are classi-\nﬁed wrongly). This drop is probably due to aspects\nof the implementation such as data sparsity. Specif-\nically, if there are no events in which the subject\noccurs with a direct object, the prototype of the pa-\ntient is built only from the verb’s most associated\npatients, disregarding the fact they are implausible\ngiven the whole context.\nII. TLM S know more about what is typical\nthan what is possible. The use of typicality\ndatasets such as DTFit for the estimation of the\nmodels’GEK has some limitations. TLM S’ ability\nto reproduce combinations encountered frequently\nduring training could be the reason for high perfor-\nmances in the typicality task, since what is most\ntypical often occurs most frequently. However,\nGEK is not just memory of exemplars, but it re-\nquires ﬁne-grained knowledge of the properties of\nobjects and it involves reasoning processes such as\nabstraction and comparison between objects and\nprototypical concepts.\nTo evaluate TLM S on a setting where frequency\nvariations in the training corpus have a minor im-\npact, we used the dataset realized by Wang et al.\n(2018) (henceforth, Wang2018). This dataset rep-\nresents a benchmark for the task of semantic physi-\ncal plausibility (Bagherinezhad et al., 2016), that\nis, distinguishing an atypical but physically plau-\nsible event such as The student climbed the ship\nfrom an atypical and physically implausible one\nsuch as The student climbed the water. The dataset\ncontains agent-verb-patient (SVO) triples divided\n8\ninto plausible and implausible. From the original\ndataset, which contains 1,540 plausible and 1,540\nimplausible triples, we derived two subsets contain-\ning pairs of triples differing either for the agent or\nfor the patient role ﬁller (obtaining 222 and 394\npairs respectively).\nTable 3 reports the resulting accuracy values. In\ngeneral, the models’ scores are lower than in the\ntypicality task (min. 0.64, max. 0.77), and in some\ncases they are not much higher than random per-\nformance. Moreover, in many cases the models\ncould be facilitated by the existence of an associ-\nation between the plausible ﬁller and the verb of\nthe event, as in The ant built the wall and in The\nchair absorbed the water. Nevertheless, the results\ndemonstrate that the notion of plausibility is harder\nto model compared to typicality, and invite caution\nwhen making claims about TLM S world and event\nknowledge. In fact, the results suggest that even\nif it were true that TLM S develop some general-\nization skills from training, they still miss many\npredictions about possible events, which instead\nhumans easily make on the basis of their common-\nsense knowledge.\nThis dataset is also difﬁcult for SDM, which ob-\ntains scores lower than those of theTLM S (0.65 for\nAgent and 0.66 for Patient). Even if SDM should\nbe better at reproducing generalization through the\nconstruction of prototypical ﬁllers, the model’s\ndistributional representations seem to fail to cap-\nture the speciﬁc properties that are relevant for the\ndataset items, namely physical properties of objects\n(liquid-solid, large-small, etc.). The lack of such\nproperties constitutes a limitation of distributional\nmodels of word meaning based on text data only,\nwhich is why, in previous studies, world knowl-\nedge was explicitly injected into the models for the\nphysical plausibility task (Wang et al., 2018).\nIII. TLM S do not extend ﬁt judgments to low\nfrequency synonyms. To test whether TLM S\nconsider an entity more or less likely to take part\nin an event depending on the word used to refer to\nthat entity, we evaluated them on a new diagnostic\ndataset of 39 pairs, generated from a subset of Pa-\ntientDTFit. In this setting, the typical ﬁller in each\npair was replaced with a low-frequency word that is\nsemantically related to the original one. To choose\nan appropriate substitute, we ﬁrst extracted a set\nof synonyms according to two lexical resources\n(WordNet, Lexico.com). Then, we picked a word\nthat 1) is less frequent than the original ﬁller and\n2) has a frequency lower than 300,000. For the\nsame reasons described in the ﬁrst additional ex-\nperiment, we extracted statistics from a 2018 dump\nof English Wikipedia and evaluated only BERT\non the new dataset. Examples of substitutions are\nthe following: The botanist examined the plant →\nﬂora, The waiter cleared the restaurant →tavern,\nThe veterinarian examined the dog →puppy. It\nis interesting to observe that these variations pose\nserious difﬁculties to the models, as their accu-\nracy on the diagnostics dataset is close or lower to\nthe random level (BERT-base: 0.37, BERT-large:\n0.53). For example, BERT considers The terrorist\nreleased the captive as less probable than The ter-\nrorist released the /book, and the same occurs for\nThe mother prepared the provisions/gun, and The\ncarver built the bust/house.\nThese results cast doubts that current TLM S\ncan constitute plausible models of event knowl-\nedge: they tend to reproduce the patterns that are\nfrequently observed in the data, and their good\nperformance is disrupted once these are replaced\nwith semantically equivalent, but less frequent ones.\nThis means that they lack the abstract semantic\nknowledge of human subjects, whose predictions\nare more ﬂexible thanks to inference mechanisms\nsuch as generalization to concepts sharing seman-\ntic features. At least in principle, models aiming\nat building prototypes of ideal role ﬁllers (such as\nthe distributional models of thematic ﬁt) are more\ncognitively realistic, since they are less dependent\non speciﬁc words. However, they may still show\nsub-optimal performance in this diagnostic dataset\nas they are based on the quality of the distributional\nrepresentations, which is lower for words that have\nlow frequency in corpora. This is conﬁrmed by the\nperformance of SDM on the dataset (the accuracy\nis 0.51).\ntransitive cleft wh-interrogative\nAgentDTFit 0.64 -0.13 0.62\nPatientDTFit 0.64 0.26 0.51\nInstrumentDTFit 0.5 0.10 0.6\nTimeDTFit 0.66 0.33 0.64\nLocationDTFit 0.73 0.67 0.73\nTable 5: Spearman Correlation for DTFit datasets us-\ning RoBERTa-large and input sentences with different\nword orders.\nIV . TLMS can be inﬂuenced by the surface\nstructure of sentences Finally, we analyzed to\nwhat extent TLM S’ ability to predict the ﬁt of a\n9\nword in a given role arises from the observation of\nrecurrent word order patterns during pre-training\n(e.g., the fact that an actor’s award-winning event\nis canonically expressed with active sentences, in\nwhich award follows the words actor and won),\nrather than being based on a deep understanding of\nthe semantic relations between the words.\nTo explore this issue, we modiﬁed DTFit tuples\nto create two different versions of the dataset, each\nwith examples of a syntactic construction different\nfrom the English canonical word order. Speciﬁ-\ncally, we experimented withcleft (It was the award\nthat the actor won , It was on the ring that the\nboxer delivered the punch) and wh-interrogative\nsentences (Which award did the actor win? , On\nwhich ring did the boxer deliver the punch?).\nWe evaluated this new set of sentences using\nRoBERTa-large (cf. Table 5). We observe that\nthe model is not particularly affected by the inter-\nrogative structure. Conversely, the model suffers\nfrom the cleft construction for all semantic roles ex-\ncept for Location (ρ=0.67). If we ask the model to\ngenerate the most likely words to appear in that po-\nsition, we observe that word predictions in the new\nconstruction are more general and less dependent\non the GEK associated with the other words in the\nsentence, proving that TLM S are affected by the\nsurface syntactic shape of the linguistic input, since\nthe cleft construction is less frequent and presents\na less canonical word order. For instance, given\nthe sentence It was with the [MASK] that the guard\nopened the door, RoBERTa generates the following\npossible ﬁllers: gun (P=0.044), crowd (P=0.020),\nsword (P=0.016), and then key (P=0.016), while\nin the active sentence key is correctly predicted as\nthe most probable word (P=0.22). In this speciﬁc\ncase, it seems that the model only looks at the word\nnearby (guard) to make a prediction, disregarding\nthe entire context. Generally, the agent role shows\nthe worst results, obtaining −0.13. Note that SDM\nis not affected by these variations by design, since\nits predictions are based on semantic roles derived\nfrom the syntactic analysis of the sentence, which\nis explicitly provided to the model.\n6 Conclusions\nIn this paper, we tested Transformer-based Lan-\nguage Models on tasks related to Generalized Event\nKnowledge. In the main experiment, we evalu-\nated their ability to model event typicality, that is,\ndiscern typical from atypical events, on a dataset\ndesigned for this task, DTFit. Results show that\nTLM S scores positively correlate with human judg-\nments. However, they do not signiﬁcantly out-\nperform the distributional prototype-based model\n(SDM) that we selected for comparison. This con-\nﬁrms the ability of SDM to dynamically update the\nsemantic representation of a sentence, which was\nrecently shown for the challenging task of logical\nmetonymy interpretation (Rambelli et al., 2020).\nHowever, we decided to go beyond the simple\nevaluation against human judgments. We carried\nout several additional small-scale experiments with\nthe speciﬁc aim to understand which factors could\naffect the predictions of TLM S. The results sug-\ngest that models are often too dependent on what\nthey observe during training and lack some key\naspects of human event knowledge. In particular,\nwe observed that, in some cases, they are unable\nto compose all elements of the input to make pre-\ndictions, and they tend to rely more on salient local\nassociations between words. However, further anal-\nysis is needed. Secondly, their performance drop\non the physical plausibility task, which requires the\nability to infer physical properties necessary for an\nobject to participate in a given event. Lastly, their\nprobabilities are dependent on the speciﬁc words\nthat have to be predicted rather than on their mean-\ning, and on the canonical word order in which these\nwords tend to occur. Noticeably, even a distribu-\ntional model of event knowledge (SDM ) showed\nsimilar limitations, generally likely to be due to\ndata sparsity and inherent limitations of distribu-\ntional representations obtained from text data.\nTo conclude, we believe that the experiments we\nreported are the ﬁrst step towards a deep investi-\ngation of “how general” is the Generalized Event\nKnowledge in computational models. Future work\nmight include the creation of a larger version of\nour diagnostic datasets, in order to make available\nto NLP researchers a more robust benchmark for\ntasks related to Generalized Event Knowledge.\nAcknowledgements\nThis work, carried out within the Institut Con-\nvergence ILCB (ANR-16-CONV-0002), has ben-\neﬁted from support from the French government,\nmanaged by the French National Agency for Re-\nsearch (ANR) and the Excellence Initiative of Aix-\nMarseille University (A*MIDEX). We thank the\nanonymous reviewers for their insightful feedback.\n10\nReferences\nHessam Bagherinezhad, Hannaneh Hajishirzi, Yejin\nChoi, and Ali Farhadi. 2016. Are Elephants Bigger\nthan Butterﬂies? Reasoning about Sizes of Objects.\nIn Proceedings of AAAI.\nMarco Baroni, Silvia Bernardini, Adriano Ferraresi,\nand Eros Zanchetta. 2009. The WaCky Wide Web:\nA Collection of Very Large Linguistically Processed\nWeb-Crawled Corpora. Language Resources and\nEvaluation, 43(3):209–226.\nMarco Baroni and Alessandro Lenci. 2010. Dis-\ntributional Memory: A General Framework for\nCorpus-Based Semantics. Computational Linguis-\ntics, 36(4):673–721.\nKlinton Bicknell, Jeffrey L Elman, Mary Hare, Ken\nMcRae, and Marta Kutas. 2010. Effects of Event\nKnowledge in Processing Verbal Arguments. Jour-\nnal of Memory and Language, 63(4):489–505.\nEmmanuele Chersoni, Philippe Blache, and Alessan-\ndro Lenci. 2016. Towards a Distributional Model\nof Semantic Complexity. In Proceedings of the\nCOLING Workshop on Computational Linguistics\nfor Linguistic Complexity.\nEmmanuele Chersoni, Alessandro Lenci, and Philippe\nBlache. 2017. Logical Metonymy in a Distributional\nModel of Sentence Comprehension. In Proceedings\nof *SEM.\nEmmanuele Chersoni, Ludovica Pannitto, Enrico San-\ntus, Alessandro Lenci, and Chu-Ren Huang. 2020.\nAre Word Embeddings Really a Bad Fit for the Esti-\nmation of Thematic Fit? In Proceedings of LREC.\nEmmanuele Chersoni, Enrico Santus, Alessandro\nLenci, Philippe Blache, and Chu-Ren Huang. 2021.\nNot All Arguments Are Processed Equally: A Distri-\nbutional Model of Argument Complexity.Language\nResources and Evaluation, pages 1–28.\nEmmanuele Chersoni, Enrico Santus, Ludovica Pan-\nnitto, Alessandro Lenci, Philippe Blache, and C-R\nHuang. 2019. A Structured Distributional Model\nof Sentence Meaning and Processing. Natural Lan-\nguage Engineering, 25(4):483–502.\nAndy Clark. 2013. Whatever Next? Predictive Brains,\nSituated Agents, and the Future of Cognitive Sci-\nence. Behavioral and Brain Sciences , 36(3):181–\n204.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of NAACL.\nKatrin Erk. 2007. A Simple, Similarity-Based Model\nfor Selectional Preferences. In Proceedings of ACL.\nKatrin Erk, Sebastian Pad ´o, and Ulrike Pad ´o. 2010. A\nFlexible, Corpus-Driven Model of Regular and In-\nverse Selectional Preferences. Computational Lin-\nguistics, 36(4):723–763.\nStefan Evert. 2008. Corpora and Collocations. Corpus\nLinguistics. An International Handbook , 2:1212–\n1248.\nTodd R Ferretti, Ken McRae, and Andrea Hatherell.\n2001. Integrating Verbs, Situation Schemas, and\nThematic Role Concepts. Journal of Memory and\nLanguage, 44(4):516–547.\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities. arXiv preprint arXiv:1901.05287.\nClayton Greenberg, Vera Demberg, and Asad Say-\need. 2015a. Verb Polysemy and Frequency Effects\nin Thematic Fit Modeling. In Proceedings of the\nNAACL Workshop on Cognitive Modeling and Com-\nputational Linguistics.\nClayton Greenberg, Asad B Sayeed, and Vera Demberg.\n2015b. Improving Unsupervised Vector-Space The-\nmatic Fit Evaluation via Role-Filler Prototype Clus-\ntering. In Proceedings of HLT-NAACL.\nXudong Hong, Asad Sayeed, and Vera Demberg. 2018.\nLearning Distributed Event Representations with a\nMulti-task Approach. In Proceedings of *SEM.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What Does BERT Learn About the Structure\nof Language? In Proceedings of ACL.\nWei-Jen Ko, Greg Durrett, and Junyi Jessy Li. 2019.\nLinguistically-informed Speciﬁcity and Semantic\nPlausibility for Dialogue Generation. In Proceed-\nings of NAACL.\nGeoffrey Neil Leech. 1992. 100 Million Words of En-\nglish: The British National Corpus (BNC). Lan-\nguage Research, 28.\nAlessandro Lenci. 2011. Composing and Updating\nVerb Argument Expectations: A Distributional Se-\nmantic Model. In Proceedings of the ACL Workshop\non Cognitive Modeling and Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692.\nYuval Marton and Asad Sayeed. 2021. Thematic\nFit Bits: Annotation Quality and Quantity for\nEvent Participant Representation. arXiv preprint\narXiv:2105.06097.\nKazunaga Matsuki, Tracy Chow, Mary Hare, Jef-\nfrey L Elman, Christoph Scheepers, and Ken McRae.\n2011. Event-Based Plausibility Immediately Inﬂu-\nences On-Line Language Comprehension. Jour-\nnal of Experimental Psychology: Learning, Memory,\nand Cognition, 37(4):913.\n11\nKen McRae and Kazunaga Matsuki. 2009. People Use\ntheir Knowledge of Common Events to Understand\nLanguage, and Do So as Quickly as Possible. Lan-\nguage and Linguistics Compass, 3(6):1417–1429.\nKen McRae, Michael Spivey-Knowlton, and Michael\nTanenhaus. 1998. Modeling the Inﬂuence of The-\nmatic Fit (and other Constraints) in On-Line Sen-\ntence Comprehension. Journal of Memory and Lan-\nguage, 38(3):283–312.\nEleni Metheniti, Tim Van de Cruys, and Nabil Hathout.\n2020. How Relevant Are Selectional Preferences for\nTransformer-based Language Models? In Proceed-\nings of COLING.\nUlrike Pad ´o. 2007. The Integration of Syntax and Se-\nmantic Plausibility in a Wide-Coverage Model of\nHuman Sentence Processing. Ph.D. thesis.\nIan Porada, Kaheer Suleman, and Jackie Chi Kit Che-\nung. 2019. Can a Gorilla Ride a Camel? Learning\nSemantic Plausibility from Text. In Proceedings of\nthe EMNLP Workshop on Commonsense Inference\nin Natural Language Processing.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels Are Unsupervised Multitask Learners. Ope-\nnAI Blog, 1(8):9.\nGiulia Rambelli, Emmanuele Chersoni, Alessandro\nLenci, Philippe Blache, and Chu-Ren Huang.\n2020. Comparing Probabilistic, Distributional and\nTransformer-Based Models on Logical Metonymy\nInterpretation. In Proceedings of AACL-IJCNLP.\nEnrico Santus, Emmanuele Chersoni, Alessandro\nLenci, and Philippe Blache. 2017. Measuring The-\nmatic Fit with Distributional Feature Overlap. In\nProceedings of EMNLP.\nAsad Sayeed and Vera Demberg. 2014. Combining Un-\nsupervised Syntactic and Semantic Models of The-\nmatic Fit. In Proceedings of CLIC.it.\nAsad Sayeed, Vera Demberg, and Pavel Shkadzko.\n2015. An Exploration of Semantic Features in an\nUnsupervised Thematic Fit Evaluation Framework.\nItalian Journal of Computational Linguistics, 1(1).\nAsad Sayeed, Clayton Greenberg, and Vera Demberg.\n2016. Thematic Fit Evaluation: An Aspect of Se-\nlectional Preferences. In Proceedings of the ACL\nWorkshop on Evaluating Vector Space Representa-\ntions for NLP.\nOttokar Tilk, Vera Demberg, Asad Sayeed, Dietrich\nKlakow, and Stefan Thater. 2016. Event Participant\nModelling with Neural Networks. In Proceedings of\nEMNLP.\nBram Vandekerckhove, Dominiek Sandra, and Wal-\nter Daelemans. 2009. A Robust and Extensible\nExemplar-based Model of Thematic Fit. In Proceed-\nings of EACL.\nPaolo Vassallo, Emmanuele Chersoni, Enrico Santus,\nAlessandro Lenci, and Philippe Blache. 2018. Event\nKnowledge in Sentence Processing: A New Dataset\nfor the Evaluation of Argument Typicality. In Pro-\nceedings of the LREC Workshop on Linguistic and\nNeuro-Cognitive Resources.\nSu Wang, Greg Durrett, and Katrin Erk. 2018. Model-\ning Semantic Plausibility by Injecting World Knowl-\nedge. In Proceedings of NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. HuggingFace’s Transformers:\nState-of-the-art Natural Language Processing. arXiv\npreprint arXiv:1910.03771.\nHongming Zhang, Hantian Ding, and Yangqiu Song.\n2019. SP-10K: A Large-scale Evaluation Set for Se-\nlectional Preference Acquisition. In Proceedings of\nACL."
}