{
    "title": "Character-Level Language Modeling with Deeper Self-Attention",
    "url": "https://openalex.org/W2886490473",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A4207161363",
            "name": "Rami Al-Rfou",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2885395720",
            "name": "Dokook Choe",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2795135342",
            "name": "Noah Constant",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2885677207",
            "name": "Mandy Guo",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2114214090",
            "name": "Llion Jones",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4207161363",
            "name": "Rami Al-Rfou",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2885395720",
            "name": "Dokook Choe",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2795135342",
            "name": "Noah Constant",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2885677207",
            "name": "Mandy Guo",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2114214090",
            "name": "Llion Jones",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6685537299",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6638824847",
        "https://openalex.org/W2413904250",
        "https://openalex.org/W6701650085",
        "https://openalex.org/W2589889860",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W2212703438",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2788760202",
        "https://openalex.org/W2798702047",
        "https://openalex.org/W2791366550",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W6607333740",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W6738195075",
        "https://openalex.org/W2606347107",
        "https://openalex.org/W6748243251",
        "https://openalex.org/W6713098461",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W6732742072",
        "https://openalex.org/W6635446068",
        "https://openalex.org/W2282641050",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W6720905350",
        "https://openalex.org/W2409027918",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2953061907",
        "https://openalex.org/W2525246036",
        "https://openalex.org/W2757047188",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2786167576",
        "https://openalex.org/W2963304263",
        "https://openalex.org/W2785672818",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2473934411",
        "https://openalex.org/W2952136670",
        "https://openalex.org/W2949626814",
        "https://openalex.org/W2767406800",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2175402905",
        "https://openalex.org/W2418038525",
        "https://openalex.org/W2950635152",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2540404261",
        "https://openalex.org/W2619808423",
        "https://openalex.org/W2510842514",
        "https://openalex.org/W2618854269",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2509671015",
        "https://openalex.org/W3037881859",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2952339051",
        "https://openalex.org/W179875071"
    ],
    "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.",
    "full_text": "The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)\nCharacter-Level Language Modeling with Deeper Self-Attention\nRami Al-Rfou,* Dokook Choe,* Noah Constant,* Mandy Guo,* Llion Jones*\nGoogle AI\n1600 Amphitheatre Parkway\nMountain View, California 94043\n{rmyeid, choed, nconstant, xyguo, llion}@google.com\nAbstract\nLSTMs and other RNN variants have shown strong perfor-\nmance on character-level language modeling. These models\nare typically trained using truncated backpropagation through\ntime, and it is common to assume that their success stems\nfrom their ability to remember long-term contexts. In this\npaper, we show that a deep (64-layer) transformer model\n(Vaswani et al. 2017) with ﬁxed context outperforms RNN\nvariants by a large margin, achieving state of the art on two\npopular benchmarks: 1.13 bits per character on text8 and\n1.06 on enwik8. To get good results at this depth, we show\nthat it is important to add auxiliary losses, both at intermedi-\nate network layers and intermediate sequence positions.\nIntroduction\nCharacter-level modeling of natural language text is chal-\nlenging, for several reasons. First, the model must learn a\nlarge vocabulary of words “from scratch”. Second, natural\ntext exhibits dependencies over long distances of hundreds\nor thousands of time steps. Third, character sequences are\nlonger than word sequences and thus require signiﬁcantly\nmore steps of computation.\nIn recent years, strong character-level language models\ntypically follow a common template (Mikolov et al. 2010;\n2011; Sundermeyer, Schl ¨uter, and Ney 2012). A recurrent\nneural net (RNN) is trained over mini-batches of text se-\nquences, using a relatively short sequence length (e.g. 200\ntokens). To capture context longer than the batch sequence\nlength, training batches are provided in sequential order, and\nthe hidden states from the previous batch are passed for-\nward to the current batch. This procedure is known as “trun-\ncated backpropagation through time” (TBTT), because the\ngradient computation doesn’t proceed further than a single\nbatch (Werbos 1990). A range of methods have arisen for\nunbiasing and improving TBTT (Tallec and Ollivier 2017;\nKe et al. 2017).\nWhile this technique gets good results, it adds complex-\nity to the training procedure, and recent work suggests\nthat models trained in this manner don’t actually make\n“strong” use of long-term context. For example Khandelwal\net al. (2018) ﬁnd that a word-based LSTM language model\n*Equal contribution.\nCopyright © 2019, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nonly effectively uses around 200 tokens of context (even if\nmore is provided), and that word order only has an effect\nwithin approximately the last 50 tokens.\nIn this paper, we show that a non-recurrent model can\nachieve strong results on character-level language modeling.\nSpeciﬁcally, we use a network of transformer self-attention\nlayers (Vaswani et al. 2017) with causal (backward-looking)\nattention to process ﬁxed-length inputs and predict upcom-\ning characters. The model is trained on mini-batches of se-\nquences from random positions in the training corpus, with\nno information passed from one batch to the next.\nOur primary ﬁnding is that the transformer architecture is\nwell-suited to language modeling over long sequences and\ncould replace RNNs in this domain. We speculate that the\ntransformer’s success here is due to its ability to “quickly”\npropagate information over arbitrary distances; by compar-\nison, RNNs need to learn to pass relevant information for-\nward step by step.\nWe also ﬁnd that some modiﬁcations to the basic trans-\nformer architecture are beneﬁcial in this domain. Most im-\nportantly, we add three auxiliary losses, requiring the model\nto predict upcoming characters (i) at intermediate sequence\npositions, (ii) from intermediate hidden representations, and\n(iii) at target positions multiple steps in the future. These\nlosses speed up convergence, and make it possible to train\ndeeper networks.\nCharacter Transformer Model\nLanguage models assign a probability distribution over to-\nken sequences t0:L by factoring out the joint probability as\nfollows, where L is the sequence length:\nPr(t0:L) = P(t0)\nL∏\ni=1\nPr(ti|t0:i−1), (1)\nTo model the conditional probability Pr(ti|t0:i−1), we\ntrain a transformer network to process the character se-\nquence t0:i−1. Transformer networks have recently showed\nsigniﬁcant gains in tasks that require processing sequences\naccurately and efﬁciently.\nOur character-level transformer architecture has 64 trans-\nformer layers. Following Vaswani et al. (2017), by “trans-\nformer layer” we mean a block containing a multihead self-\nattention sub-layer followed by a feed-forward network of\n3159\nt0 t1 t2 t3\nt4\nTransformer Layers\n(a) Baseline\nt0 t1 t2 t3\nt1 t2 t3 t4\nTransformer Layers (b) Multiple Positions\nt0 t1 t2 t3\nt1 t2 t3 t4\nt1 t2 t3 t4\nTransformer Layers\n(c) Layer Losses\nt0 t1 t2 t3\nt1\nt2\nt2\nt3\nt3\nt4\nt4\nt5\nt1\nt2\nt2\nt3\nt3\nt4\nt4\nt5\nTransformer Layers (d) Multiple Targets\nFigure 1: (a) A character transformer network of two layers processing a four character sequence to predict t4. The causal\nattention mask limits information to left-to-right ﬂow. Red arrows highlight the prediction task the network has to learn. (b)\nAdding intermediate position prediction tasks to our network. Now, we predict the ﬁnal character t4 and all intermediate\ncharacters t0:3. All of these losses contribute equally during training. (c) Adding prediction tasks for the intermediate layers.\nFor this example of two layers, the losses of the intermediate layer prediction tasks will be absent after ﬁnishing 25% of the\ntraining. (d) Adding two predictions per position.\ntwo fully connected sub-layers. For more details on the\ntransformer architecture, refer to Vaswani et al. (2017) and\nthe tensor2tensor library1.\nTo ensure that the model’s predictions are only condi-\ntioned on past characters, we mask our attention layers with\na causal attention, so each position can only attend leftward.\nThis is the same as the “masked attention” in the decoder\ncomponent of the original transformer architecture used for\nsequence-to-sequence problems (Vaswani et al. 2017).\nFigure 1a shows our initial model with the causal atten-\ntion mask limiting information ﬂow from left to right. Each\ncharacter prediction is conditioned only on the characters\nthat appeared earlier.\nAuxiliary Losses\nOur network is, to our knowledge, deeper than any trans-\nformer network discussed in previous work. In initial exper-\niments, we found training a network deeper than ten layers\nto be challenging, with slow convergence and poor accuracy.\nWe were able to deepen the network to better effect through\nthe addition auxiliary losses, which sped up convergence of\nthe training signiﬁcantly.\nWe add several types of auxiliary losses, correspond-\ning to intermediate positions, intermediate layers, and non-\nadjacent targets. We hypothesize that these losses not only\nspeed up convergence but also serve as an additional regu-\nlarizer. During training, the auxiliary losses get added to the\n1https://github.com/tensorﬂow/tensor2tensor\ntotal loss of the network with discounted weights. Each type\nof auxiliary loss has its own schedule of decay. During eval-\nuation and inference time, only the prediction of the ﬁnal\nposition at the ﬁnal layer is used.\nOne consequence of this approach is that a number of net-\nwork parameters are only used during training—speciﬁcally,\nthe parameters in the output classiﬁcation layers associated\nwith predictions made from intermediate layers and predic-\ntions over non-adjacent targets. Thus, when listing the num-\nber of parameters in our models, we distinguish between\n“training parameters” and “inference parameters”.\nMultiple Positions First, we add prediction tasks for each\nposition in the ﬁnal layer, extending our predictions from\none per example to |L|(sequence length). Note, predicting\nover all sequence positions is standard practice in RNN-\nbased approaches. However in our case, since no informa-\ntion is passed forward across batches, this is forcing the\nmodel to predict given smaller contexts—sometimes just\none or two characters. It is not obvious whether these sec-\nondary training tasks should help on the primary task of pre-\ndicting with full context. However, we ﬁnd that adding this\nauxiliary loss speeds up training and gives better results (see\nAblation Experiments below). Figure 1b illustrates the task\nof predicting across all sequence positions. We add these\nlosses during training without decaying their weights.\nIntermediate Layer Losses In addition to the ﬁnal pre-\ndiction layer, we add predictions made from the output of\neach intermediate transformer layer. As with the ﬁnal layer,\n3160\nwe add predictions for all intermediate positions in the se-\nquence (see Figure 1c). Lower layers are weighted to con-\ntribute less and less to the loss as training progresses. If there\nare n layers total, then the lth intermediate layer stops con-\ntributing any loss after ﬁnishing l/2n of the training. This\nschedule drops all intermediate losses after half of the train-\ning is done.\nMultiple Targets At each position in the sequence, the\nmodel makes two (or more) predictions of future characters.\nFor each new target we introduce a separate classiﬁer. The\nlosses of the extra targets get weighted by a multiplier of 0.5\nbefore being added to their corresponding layer loss.\nPositional Embeddings\nIn the basic transformer network described in Vaswani et\nal. (2017), a sinusoidal timing signal is added to the input\nsequence prior to the ﬁrst transformer layer. However, as our\nnetwork is deeper (64 layers), we hypothesize that the tim-\ning information may get lost during the propagation through\nthe layers. To address this, we replace the timing signal with\na learned per-layer positional embedding added to the in-\nput sequence before each transformer layer. Speciﬁcally, the\nmodel learns a unique 512-dimensional embedding vector\nfor each of the L context positions within each of N layers,\ngiving a total of L ×N ×512 additional parameters. We are\nable to safely use positional embeddings for our task, as we\ndon’t require the model to generalize to longer contexts than\nthose seen during training.\nExperimental Setup\nDatasets\nFor evaluation we focus mainly ontext8 (Mahoney 2009).\nThis dataset consists of English Wikipedia articles, with su-\nperﬂuous content removed (tables, links to foreign language\nversions, citations, footnotes, markup, punctuation). The re-\nmaining text is processed to use a minimal character vocab-\nulary of 27 unique characters—lowercase letters a through\nz, and space. Digits are replaced by their spelled-out equiva-\nlents, so “20” becomes “two zero”. Character sequences\nnot in the range [a-zA-Z] are converted to a single space. Fi-\nnally, the text is lowercased. The size of the corpus is 100M\ncharacters. Following Mikolov et al. (2012) and Zhang et\nal. (2016), we split the data into 90M characters for train,\n5M characters for dev, and 5M characters for test.\nTo aid in comparison with other recent approaches, we\nalso evaluate our model onenwik8 (Mahoney 2009) which\nis 100M bytes of unprocessed Wikipedia text, including\nmarkup and non-Latin characters. There are 205 unique\nbytes in the dataset. Following Chung et al. (2015), and as in\ntext8, we split the data into 90M, 5M and 5M for training,\ndev and test respectively.\nTraining\nCompared to most models based on transformers (Vaswani\net al. 2017; Salimans et al. 2018), our model is very deep,\nwith 64 transformer layers and each layer using two atten-\ntion heads. Each transformer layer has a hidden size of 512\nParameters (×106)\nModel train inference bpc\nLSTM (Cooijmans et al. 2016) - - 1.43\nBN-LSTM (Cooijmans et al. 2016) - - 1.36\nHM-LSTM (Chung, Ahn, and Bengio 2016)35 35 1.29\nRecurrent Highway (Zilly et al. 2016) 45 45 1.27\nmLSTM (Krause et al. 2016) 45 45 1.27\nT12 (ours) 44 41 1.18\nT64 (ours) 235 219 1.13\nmLSTM + dynamic eval (Krause et al. 2017)45 - 1.19\nTable 1: Comparison of various models on text8 test.\nbpc Accuracy (%)\nContext dev test dev test\n32 1.25 1.34 72.8 71.1\n64 1.17 1.26 74.8 73.0\n128 1.12 1.20 76.1 74.4\n256 1.09 1.16 76.9 75.3\n512 1.06 1.13 77.3 75.9\nTable 2: Bits per character (bpc) and accuracy of our best\nmodel on text8 dev and test, for different context lengths.\nand a ﬁlter size of 2048. We feed our model sequences of\nlength 512. Each item in the sequence represents a single\nbyte (or equivalently, one character in text8) which gets\nreplaced by its embedding, a vector of size 512. We add to\nthe byte embeddings a separate learned positional embed-\nding for each of the 512 token positions, as described in the\nPositional Embeddings section above. We do the same ad-\ndition at each layer activation throughout the network. The\npositional embeddings are not shared across the layers. With\ntwo predictions per position, each layer learns to predict\n1024 characters. Because we are primarily interested in pre-\ndicting the immediately following character (one step away),\nwe halve the loss of predicting characters two steps away.\nThe prediction layers are logistic regression layers over the\nfull 256 outputs (the number of unique bytes). To demon-\nstrate the generality of the model, we always train and pre-\ndict over all 256 labels, even on datasets that cover a smaller\nvocabulary. Despite this, we found that in practice the model\nnever predicted a byte value outside of the ones observed in\nthe training dataset.\nThe model has approximately 235 million parameters,\nwhich is larger than the number of characters in the text8\ntraining corpus. To regularize the model, we apply dropout\nin the attention and ReLU layers with a probability of 0.55.\nWe use the momentum optimizer with 0.99 momentum. The\nlearning rate is ﬁxed during training to 0.003. We train our\nmodel for 4 million steps, with each step processing a batch\nof 16 randomly selected sequences. We drop the intermedi-\nate layer losses consecutively, as described in the Intermedi-\nate Layer Losses section above. Starting from the ﬁrst layer,\nafter every 62.5K (= 4 M× 1\n2∗64 ) steps, we drop the losses\nintroduced by the next layer. According to this schedule, af-\nter training is halfway complete, only the ﬁnal layer losses\nare present.\n3161\nParameters (×106)\nModel train inference bpb\nFS-LSTM-4 (Mujika, Meier, and Steger 2017)47 - 1.25\nmLSTM (Krause et al. 2016) 46 - 1.24\ncmix v13 (Knol 2017) - - 1.23\nT12 (ours) 44 41 1.11\nT64 (ours) 235 219 1.06\nmLSTM + dynamic eval (Krause et al. 2017)46 - 1.08\nTable 3: Comparison of various models on enwik8 test.\nEvaluation\nAt inference time, we use the model’s prediction at the ﬁ-\nnal position of the ﬁnal layer to compute the probability of\na character given a context of 512 characters. There is no\nstate passed between predictions as would be the case with\nRNN models, so for each character predicted we have to pro-\ncess the context from scratch. Because there is no reused\ncomputation from previous steps, our model requires ex-\npensive computational resources for evaluation and infer-\nence. We measure the performance of training checkpoints\n(roughly every 10,000 steps) by evaluating bits per character\n(bpc) over the entire the validation set, and save the param-\neters that perform the best. Our best model is achieved after\naround 2.5 million steps of training, which takes 175 hours\non a single Google Cloud TPU v2.\nResults\nWe report the performance of our best model (T64) on the\nvalidation and test sets. Table 1 compares our models against\nseveral recent results. On the test set, we achieve a new state\nof the art, 1.13 bpc. This model is 5x larger than previous\nmodels, which necessitated aggressive dropout rates of 0.55.\nFor better comparison with smaller models, we also train a\nsmaller model (T12) with 41M parameters. This model con-\nsists of 12 layers, and trained for 8M steps, with a reduced\ndropout rate of 0.2. All other settings were left the same as\nT64. Our smaller model still outperforms previous models,\nachieving 1.18 bpc on the test dataset. Increasing the depth\nof the network from 12 layers to 64 improved the results sig-\nniﬁcantly, with the auxiliary losses enabling the training to\nbetter utilize the depth of the network. Note, our models do\nnot use dynamic evaluation (Krause et al. 2017), a technique\nthat adjusts model weights at test time by training on test\ndata.\nTable 2 shows the performance of our model given differ-\nent context sizes. We are able to achieve state-of-the-art re-\nsults once the context increases beyond 128 characters, with\nthe best performance of 1.06 bpc at 512 characters. As ex-\npected, the model performs better when it is given more con-\ntext. However this trend levels off after 512 characters; we\ndo not see better results using a context of 1024.\nUsing the same hyperparameters and training procedure\nfor text8, we also train and evaluate the T12 and T64 ar-\nchitectures on enwik8 (see Table 3). Note, several previous\nauthors discuss “bits per character” on enwik8 but are in\nfact reporting bits per byte. Without retuning for this dataset,\nour models still achieve state-of-the-art performance.\nAblation Experiments\nTo better understand the relative importance of the several\nmodiﬁcations we proposed, we run an ablation analysis. We\nstart from our best model T64 and then remove one modiﬁ-\ncation at a time. For example, when we disableMultiple Po-\nsitions, the model gets trained with only the last position loss\nfor each layer. This corresponds to calculating {L(t4 |t0:3),\nL(t5 |t0:3)}in the example shown in Figure 1d for both the\nﬁrst and the second layers. When disabling Positional Em-\nbeddings, we add the default transformer sinusoidal timing\nsignal before the ﬁrst layer.\nDescription bpc ∆bpc\nT64 (Baseline) 1.062 -\nT64 w/out Multiple Positions 2.482 1.420\nT64 w/out Intermediate Layer Losses 1.158 0.096\nT64 w/out Positional Embeddings 1.069 0.007\nT64 w/out Multiple Targets 1.068 0.006\nT64 w/ SGD Optimizer 1.065 0.003\nTable 4: Evaluation of T64 on text8 dev with context set\nto 512. Disabling each feature or loss lowers the quality of\nthe model. The biggest win comes from adding multiple po-\nsitions and intermediate layers losses.\nFor the ablation experiments, we reuse the hyperparame-\nters from our best model to avoid a prohibitively expensive\nparameter search for each ablation. The only exception is\nthe SGD experiment, where we vary the learning rate. The\nanalysis shows that the biggest advantage comes from mul-\ntiple positions and intermediate layers losses. Predicting all\nthe intermediate positions leads to signiﬁcant speed up in\nconvergence, since the model sees more effective training\nexamples per batch. Adding losses at the intermediate lay-\ners acts in the same spirit by forcing more predictions per\ntraining step.\nFinally, we replace momentum with SGD as our opti-\nmizer, using a range of learning rates (0.3, 0.1, 0.03, 0.01,\n0.003, 0.001). This ablation shows that SGD produces com-\npetitive models, with learning rate 0.1 giving the best per-\nformance. Despite the depth of our network, SGD is able to\ntrain the network efﬁciently with the help of our auxiliary\nlosses.\nComparison with Word-Level Models\nTo understand how byte-level language models perform in\ncomparison to word-level language models, we train T64\non the lm1b corpus (Chelba et al. 2013). For lm1b, we\nuse the standard train/test split of the preprocessed corpus,\nwhere out-of-vocab words have been replaced with UNK, to\nallow comparison to previous work on word and word-piece\nmodels. We report word perplexity (ppl) by converting bits-\nper-byte (bpb) into ppl2. During training we use the sec-\nond shard (01) of the heldout dataset as a dev set, as the\nﬁrst shard (00) is the test. Given this is a signiﬁcantly larger\ndataset than text8, we set all dropouts to zero. Table 5\n2For this test set, ppl = 2bpb∗826189/159658, where 826,189 is\nthe number of bytes and 159,658 is the number of tokens.\n3162\nType Model bpb ppl\nWord J ´ozefowicz et al. (2016) - 23.7\nByte T64 1.03 40.6\nTable 5: Performance of T64 on the lm1b test set.\nSeed\nmary was not permitted to see them or to speak in her own\ndefence at the tribunal she refused to offer a written defence\nunless elizabeth would guarantee a verdict of not guilty which\nelizabeth would not do although the casket letters were ac-\ncepted by the inquiry as genuine after a study of the hand-\nwriting and of the information contained therein and were\ngenerally held to be certain proof of guilt if authentic the in-\nquiry reached the conclusion that nothing was proven from\nthe start this could have been pr\nWord Completions\nproven, proved, proof, prevented, presented, problematic,\nprobably, provided, practical, provoked, preceded, predicted,\npreviously, presumed, praised, proposed, practicable, pro-\nduced, present, preserved, precisely, prior, protected, prob-\nable, prompted, proofed, properly, practiced, prohibited, pro-\nfound, preferable, proceeded, precise, predictable, practi-\ncally, prevalent\nFigure 2: A seed sequence of 512 characters taken from the\ntext8 test set, and all word completions assigned cumula-\ntive probability above 0.001 to follow the seed, in order from\nmost likely (0.529) to least likely (0.001).\nshows a gap in performance between the two classes of lan-\nguage models. This comparison can serve as a starting point\nfor researching possible ways to bridge the gap.\nQualitative Analysis\nTo probe the strengths and weaknesses of our best model\n(T64), we run the model forward, starting with the seed se-\nquence of 512 characters in Figure 2, taken from thetext8\ntest set. Figure 3 shows several per-character metrics for the\nmodel’s predictions over the true continuation of this seed\ntext. At each position, we measure i) the model’s prediction\nentropy in bits across all 256 output classes, ii) its loss—\nthe negative log probability of the target label, i.e. the “bits\nper character” for this position, and iii) the rank of the tar-\nget in the list of output classes sorted by likelihood. Unsur-\nprisingly, the model is least certain when predicting the ﬁrst\ncharacter of a word, and becomes progressively more conﬁ-\ndent and correct as subsequent characters are seen.\nTo investigate the degree to which our model prefers ac-\ntual English words over non-existent words, we compute the\nlikelihood the model assigns to all continuations after the\nseed. We cut off continuations when they reach a space char-\nacter, or when the total probability of the continuation falls\nbelow 0.001. Figure 2 shows the entire set of word comple-\ntions, in order of probability, where the initial pr- from the\nseed is repeated for readability. Note that these are all real\nor plausible (proofed) English words, and that even short\nbut bad continuations like prz are assigned a lower cumu-\nlative probability than long realistic word completions like\npredictable.\nWe expect that the transformer self-attention should make\nit easy for our model to copy sequences observed in the con-\ntext over long distances (up to the context size of 512 char-\nacters). To test this expectation, we corrupt the seed and con-\ntinuation from above by introducing a fake name zjakdmu\nbmijwxn. Speciﬁcally, we change the ﬁrst occurrence of\nelizabeth in the seed to zjakdmu bmijwxn, and the\nsecond occurrence toshe. Similarly, in the continuation, we\nchange elizabeth to zjakdmu bmijwxn. The result-\ning distance between the two occurrences of the fake name\nis 434 characters.\nFigure 4a conﬁrms that the model can successfully copy\nover this long distance. While the initial z in zjakdmu is\nunexpected, the model immediately chooses to copy the re-\nmainder of this word from the context, as opposed to pre-\ndicting any real z- words learned during training. Similarly,\nwhile the model is somewhat unsure whether the fake sur-\nname bmijwxn will appear (assigning the initial b a rank\nof two), it immediately picks up on the correspondence after\nthe b is observed, correctly predicting the remainder of the\nfake surname.\nFor comparison, Figure 4b shows how the model would\nrank the targets in our fake continuation if the original seed\nwith elizabeth were used. This conﬁrms that the fake\nname is not predictable based on knowledge gained through\ntraining, and is indeed being copied from the preceding con-\ntext.\nGeneration\nFor generating samples using our language model, we train\non a larger and less processed dataset, enwik9 (Mahoney\n2009). We split enwik9 into 900M, 50M and 50M for\ntraining, dev and test. Using the dev dataset to tune our\ndropout, we ﬁnd that dropout=0.1 performs the best. On the\ntest dataset, T64 achieves 0.85 bpb. Table 6 shows different\ngenerated samples following the seed text, using a sampling\ntemperature of 1.0.\nRelated Work\nCharacter-level modeling has shown promise in many ar-\neas such as sentiment analysis (Radford, J ´ozefowicz, and\nSutskever 2017), question answering (Kenter, Jones, and\nHewlett 2018) and classiﬁcation (Zhang, Zhao, and LeCun\n2015), and is an exciting area due to its simplicity and the\nability to easily adapt to other languages. Neural network\nbased language modeling has been heavily researched since\nits effectiveness was shown by Bengio et al. (2003). By far,\nthe most popular architecture in this area is the RNN and\nvariants, ﬁrst studied in Mikolov et al. (2010).\nMuch of the progress in this area has been made by\nmitigating the vanishing gradients problem (Hochreiter et\nal. 2001) by architectures such as LSTMs (Hochreiter and\nSchmidhuber 1997), GRU (Cho et al. 2014), Recurrent\n3163\nFigure 3: Per-character entropy, loss and rank assigned by T64 after seeding on the 512 character sequence from Figure 2.\nSeed\n'''Computational neuroscience''' is an interdisciplinary\nﬁeld which draws on [[neuroscience]], [[computer sci-\nence]], and [[applied mathematics]]. It most often uses\nmathematical and computational techniques such as com-\nputer [[simulation]]s and [[mathematical model]]s to un-\nderstand the function of the [[nervous system]].\nThe ﬁeld of computational neuroscience began with\nthe work of [[Andrew Huxley]], [[Alan Hodgkin]], and\n[[David Marr]]. The results of Hodgkin and Huxley's pi-\noneering work in developing\nSample 1 computational neuroscience were chronicled in\n''[[Is Mathematics Anything I Could Learn?]]''.\n(ISBN 0826412246). Computational\nSample 2 neuroscience concerned neurological auraria\nand the inherited ability to communicate and re-\nspond to environmental destruction -\nSample 3 the model were published in 1982 and 1983 re-\nspectively, and the subsequent work on the ﬁeld\nbegan its graduate program with [[M\nTruth the voltage clamp allowed them to develop the\nﬁrst mathematical model of the [[action poten-\ntial]]. David Marr's work focuses on\nTable 6: Samples generated by T64, seeded with text from\nthe enwik9 dev set, using a sampling temperature of 1.0.\nHighway Networks (Zilly et al. 2016), Unitary RNNs (Ar-\njovsky, Shah, and Bengio 2015) and others. This is an issue\nthat transformers do not have, due to attention allowing short\npaths to all inputs. Methods of normalizing activation func-\ntions, such as Batch Normalization (Ioffe and Szegedy 2015;\nMerity, Keskar, and Socher 2017) and Layer Normaliza-\ntion (Lei Ba, Kiros, and Hinton 2016) have also demon-\nstrated improvements on language modeling tasks. As with\nthis work, progress has been made with discovering ways to\nregularize sequential architectures, with techniques such as\nRecurrent Dropout (Zaremba, Sutskever, and Vinyals 2014;\nGal and Ghahramani 2015) and Zoneout (Krueger et al.\n2016; Rocki 2016).\nA related architecture is the Neural Cache Model (Grave,\nJoulin, and Usunier 2016), where the RNN is allowed to at-\ntend to all of its previous hidden states at each step. An-\nother similar model is used in (Daniluk et al. 2017) where\na key-value attention mechanism similar to transformers is\nused. Both approaches show improvements on word level\nlanguage modeling. Memory Networks (Weston, Chopra,\nand Bordes 2014) have a similarity to the transformer model\nin design as it also has layers of attention for processing a\nﬁx memory representing the input document and has been\nshown to be effective for language modeling in (Sukhbaatar\net al. 2015). ByteNet (Kalchbrenner et al. 2016), which is\nrelated but uses layers of dilated convolutions rather than\nattention, showed promising results on byte level language\nmodeling. Gated Convolutional Networks (Dauphin et al.\n2016) was an early non-recurrent model to show superior\nperformance on word level language modeling.\nLanguage models are usually not very deep, due to com-\nputational constraints of training RNNs, and this also lim-\nits the number of parameters. The transformer architec-\nture allowed us to build very deep (64 layer) models with\n3164\n(a) Continuing after the modiﬁed seed (including the fake name 434 characters away).\n(b) Continuing after the original seed from Figure 2.\nFigure 4: Per-character rank assigned by T64 to a fake continuation, after being seeded on either (a) the fake context where\nelizabeth is replaced with zjakdmu bmijwxn, or (b) the original context.\na large number of parameters. A recent CNN model for\ntext classiﬁcation (Conneau et al. 2016) at 29 layers is\nconsidered deep in the NLP community. A Sparsely-Gated\nMixture-of-Experts Layer (Shazeer et al. 2017) allowed lan-\nguage modeling experiments with a greatly increased num-\nber of parameters by only accessing a small portion of pa-\nrameters every time step, showing a reduction in bits per\nword. In J ´ozefowicz et al. (2016), an increase in the num-\nber of parameters was achieved by mixing character-level\nand word level models, using specialized softmaxes and us-\ning a large amount of computational resources to train. In-\ndRNN (Li et al. 2018) uses a simpliﬁed RNN architecture\nthat allows deeper stacking with 21-layers, achieving near\nSOTA character-level language modeling. Fast-Slow Recur-\nrent Neural Networks (Mujika, Meier, and Steger 2017) also\nachieved near SOTA by increasing the number of recurrent\nsteps for each character processed.\nConclusion\nCharacter language modeling has been dominated by re-\ncurrent network approaches. In this paper, we show that a\nnetwork of 12 stacked transformer layers achieves state-of-\nthe-art results on this task. We gain further improvements\nin quality by deepening the network to 64 layers, utilizing\ncapacity and depth efﬁciently. The use of auxiliary losses at\nintermediate layers and positions is critical for reaching this\nperformance, and these losses allow us to train much deeper\ntransformer networks. Finally, we analyze the behavior of\nour network and ﬁnd that it is able to exploit dependencies\nin structure and content over long distances, over 400 char-\nacters apart.\nReferences\nArjovsky, M.; Shah, A.; and Bengio, Y . 2015. Unitary evo-\nlution recurrent neural networks. CoRR abs/1511.06464.\nBengio, Y .; Ducharme, R.; Vincent, P.; and Janvin, C. 2003.\nA neural probabilistic language model. J. Mach. Learn. Res.\n3:1137–1155.\nChelba, C.; Mikolov, T.; Schuster, M.; Ge, Q.; Brants, T.;\nKoehn, P.; and Robinson, T. 2013. One billion word bench-\nmark for measuring progress in statistical language model-\ning. arXiv preprint arXiv:1312.3005.\nCho, K.; van Merrienboer, B.; G ¨ulc ¸ehre, C ¸ .; Bougares, F.;\nSchwenk, H.; and Bengio, Y . 2014. Learning phrase rep-\nresentations using RNN encoder-decoder for statistical ma-\nchine translation. CoRR abs/1406.1078.\nChung, J.; Ahn, S.; and Bengio, Y . 2016. Hierarchi-\ncal multiscale recurrent neural networks. arXiv preprint\narXiv:1609.01704.\nChung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y . 2015.\nGated feedback recurrent neural networks. In International\nConference on Machine Learning, 2067–2075.\nConneau, A.; Schwenk, H.; Barrault, L.; and LeCun, Y .\n2016. Very deep convolutional networks for natural lan-\nguage processing. CoRR abs/1606.01781.\nCooijmans, T.; Ballas, N.; Laurent, C.; and Courville,\nA. C. 2016. Recurrent batch normalization. CoRR\nabs/1603.09025.\nDaniluk, M.; Rockt¨aschel, T.; Welbl, J.; and Riedel, S. 2017.\nFrustratingly short attention spans in neural language mod-\neling. CoRR abs/1702.04521.\nDauphin, Y . N.; Fan, A.; Auli, M.; and Grangier, D. 2016.\nLanguage modeling with gated convolutional networks.\nCoRR abs/1612.08083.\nGal, Y ., and Ghahramani, Z. 2015. A Theoretically\nGrounded Application of Dropout in Recurrent Neural Net-\nworks. ArXiv e-prints.\nGrave, E.; Joulin, A.; and Usunier, N. 2016. Improving\n3165\nneural language models with a continuous cache. CoRR\nabs/1612.04426.\nHochreiter, S., and Schmidhuber, J. 1997. Long short-term\nmemory. In Neural computation, volume 9, 1735–80.\nHochreiter, S.; Bengio, Y .; Frasconi, P.; Schmidhuber, J.;\net al. 2001. Gradient ﬂow in recurrent nets: the difﬁculty\nof learning long-term dependencies.\nIoffe, S., and Szegedy, C. 2015. Batch normalization: Accel-\nerating deep network training by reducing internal covariate\nshift. CoRR abs/1502.03167.\nJ´ozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\nCoRR abs/1602.02410.\nKalchbrenner, N.; Espeholt, L.; Simonyan, K.; Oord, A.\nv. d.; Graves, A.; and Kavukcuoglu, K. 2016. Neu-\nral machine translation in linear time. arXiv preprint\narXiv:1610.10099.\nKe, N. R.; Goyal, A.; Bilaniuk, O.; Binas, J.; Charlin, L.;\nPal, C.; and Bengio, Y . 2017. Sparse attentive backtracking:\nLong-range credit assignment in recurrent networks. arXiv\npreprint arXiv:1711.02326.\nKenter, T.; Jones, L.; and Hewlett, D. 2018. Byte-level ma-\nchine reading across morphologically varied languages.\nKhandelwal, U.; He, H.; Qi, P.; and Jurafsky, D. 2018.\nSharp nearby, fuzzy far away: How neural language models\nuse context. In Association for Computational Linguistics\n(ACL).\nKnol, B. 2017. cmix - byronknoll.com/cmix.html.\nKrause, B.; Lu, L.; Murray, I.; and Renals, S. 2016. Mul-\ntiplicative lstm for sequence modelling. arXiv preprint\narXiv:1609.07959.\nKrause, B.; Kahembwe, E.; Murray, I.; and Renals, S. 2017.\nDynamic evaluation of neural sequence models. arXiv\npreprint arXiv:1709.07432.\nKrueger, D.; Maharaj, T.; Kram ´ar, J.; Pezeshki, M.; Ballas,\nN.; Ke, N. R.; Goyal, A.; Bengio, Y .; Courville, A.; and Pal,\nC. 2016. Zoneout: Regularizing rnns by randomly preserv-\ning hidden activations. arXiv preprint arXiv:1606.01305.\nLei Ba, J.; Kiros, J. R.; and Hinton, G. E. 2016. Layer\nNormalization. ArXiv e-prints.\nLi, S.; Li, W.; Cook, C.; Zhu, C.; and Gao, Y . 2018. In-\ndependently recurrent neural network (indrnn): Building A\nlonger and deeper RNN. CoRR abs/1803.04831.\nMahoney, M. 2009. Large text compression benchmark.\nhttp://www.mattmahoney.net/text/text.html.\nMerity, S.; Keskar, N. S.; and Socher, R. 2017. Reg-\nularizing and optimizing LSTM language models. CoRR\nabs/1708.02182.\nMikolov, T.; Karaﬁ´at, M.; Burget, L.; Cernock´y, J.; and Khu-\ndanpur, S. 2010. Recurrent neural network based language\nmodel. In Kobayashi, T.; Hirose, K.; and Nakamura, S., eds.,\nINTERSPEECH, 1045–1048. ISCA.\nMikolov, T.; Kombrink, S.; Burget, L.; ˇCernock´y, J.; and\nKhudanpur, S. 2011. Extensions of recurrent neural net-\nwork language model. In 2011 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n5528–5531.\nMikolov, T.; Sutskever, I.; Deoras, A.; Le, H.-S.; Kombrink,\nS.; and Cernocky, J. 2012. Subword language model-\ning with neural networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf)8.\nMujika, A.; Meier, F.; and Steger, A. 2017. Fast-slow re-\ncurrent neural networks. In Advances in Neural Information\nProcessing Systems, 5915–5924.\nRadford, A.; J´ozefowicz, R.; and Sutskever, I. 2017. Learn-\ning to generate reviews and discovering sentiment. CoRR\nabs/1704.01444.\nRocki, K. M. 2016. Surprisal-driven feedback in recurrent\nnetworks. arXiv preprint arXiv:1608.06027.\nSalimans, T.; Zhang, H.; Radford, A.; and Metaxas, D. N.\n2018. Improving gans using optimal transport. CoRR\nabs/1803.05573.\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le,\nQ. V .; Hinton, G. E.; and Dean, J. 2017. Outra-\ngeously large neural networks: The sparsely-gated mixture-\nof-experts layer. CoRR abs/1701.06538.\nSukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-to-\nend memory networks. In Advances in neural information\nprocessing systems, 2440–2448.\nSundermeyer, M.; Schl ¨uter, R.; and Ney, H. 2012. Lstm\nneural networks for language modeling. In Thirteenth an-\nnual conference of the international speech communication\nassociation.\nTallec, C., and Ollivier, Y . 2017. Unbiasing truncated back-\npropagation through time.arXiv preprint arXiv:1705.08209.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems 30. Curran Associates, Inc. 5998–6008.\nWerbos, P. J. 1990. Backpropagation through time:\nwhat it does and how to do it. Proceedings of the IEEE\n78(10):1550–1560.\nWeston, J.; Chopra, S.; and Bordes, A. 2014. Memory net-\nworks. CoRR abs/1410.3916.\nZaremba, W.; Sutskever, I.; and Vinyals, O. 2014. Recurrent\nneural network regularization. CoRR abs/1409.2329.\nZhang, S.; Wu, Y .; Che, T.; Lin, Z.; Memisevic, R.;\nSalakhutdinov, R. R.; and Bengio, Y . 2016. Architectural\ncomplexity measures of recurrent neural networks. In Ad-\nvances in Neural Information Processing Systems , 1822–\n1830.\nZhang, X.; Zhao, J. J.; and LeCun, Y . 2015. Character-\nlevel convolutional networks for text classiﬁcation. CoRR\nabs/1509.01626.\nZilly, J. G.; Srivastava, R. K.; Koutn ´ık, J.; and Schmid-\nhuber, J. 2016. Recurrent highway networks. CoRR\nabs/1607.03474.\n3166"
}