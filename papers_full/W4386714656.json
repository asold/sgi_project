{
  "title": "Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning",
  "url": "https://openalex.org/W4386714656",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2975571495",
      "name": "Shreya Johri",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2098908850",
      "name": "jaehwan jeong",
      "affiliations": [
        "Harvard University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3010782971",
      "name": "Benjamin A Tran",
      "affiliations": [
        "MedStar Georgetown University Hospital",
        "Washington Hospital",
        "Georgetown University"
      ]
    },
    {
      "id": "https://openalex.org/A2398028409",
      "name": "Daniel I. Schlessinger",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2730125174",
      "name": "Shannon Wongvibulsin",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2993599751",
      "name": "Zhuo Ran Cai",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2222836187",
      "name": "Pranav Rajpurkar",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2975571495",
      "name": "Shreya Johri",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2098908850",
      "name": "jaehwan jeong",
      "affiliations": [
        "Harvard University",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3010782971",
      "name": "Benjamin A Tran",
      "affiliations": [
        "Georgetown University",
        "MedStar Georgetown University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2398028409",
      "name": "Daniel I. Schlessinger",
      "affiliations": [
        "Northwestern University",
        "Northwestern Medicine",
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2730125174",
      "name": "Shannon Wongvibulsin",
      "affiliations": [
        "University of California System",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2993599751",
      "name": "Zhuo Ran Cai",
      "affiliations": [
        "Stanford Medicine",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford University",
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2222836187",
      "name": "Pranav Rajpurkar",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2085463930",
    "https://openalex.org/W2767996342",
    "https://openalex.org/W2592170861",
    "https://openalex.org/W4224326482",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4365148488",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4380291159",
    "https://openalex.org/W4319868628",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W4384561103",
    "https://openalex.org/W4401042726",
    "https://openalex.org/W2041104177",
    "https://openalex.org/W4388488609",
    "https://openalex.org/W4376872703",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4366331149",
    "https://openalex.org/W4372348374",
    "https://openalex.org/W4380685958",
    "https://openalex.org/W4375850424",
    "https://openalex.org/W4380786006",
    "https://openalex.org/W4381743186",
    "https://openalex.org/W4380324569",
    "https://openalex.org/W4377966413",
    "https://openalex.org/W4383744587",
    "https://openalex.org/W4381252132"
  ],
  "abstract": "Abstract The integration of Large Language Models (LLMs) like GPT-4 and GPT-3.5 into clinical diagnostics has the potential to transform patient-doctor interactions. However, the readiness of these models for real-world clinical application remains inadequately tested. This paper introduces the Conversational Reasoning Assessment Framework for Testing in Medicine (CRAFT-MD), a novel approach for evaluating clinical LLMs. Unlike traditional methods that rely on structured medical exams, CRAFT-MD focuses on natural dialogues, using simulated AI agents to interact with LLMs in a controlled, ethical environment. We applied CRAFT-MD to assess the diagnostic capabilities of GPT-4 and GPT-3.5 in the context of skin diseases. Our experiments revealed critical insights into the limitations of current LLMs in terms of clinical conversational reasoning, history taking, and diagnostic accuracy. Based on these findings, we propose a comprehensive set of guidelines for future evaluations of clinical LLMs. These guidelines emphasize realistic doctor-patient conversations, comprehensive history taking, open-ended questioning, and a combination of automated and expert evaluations. The introduction of CRAFT-MD marks a significant advancement in LLM testing, aiming to ensure that these models augment medical practice effectively and ethically.",
  "full_text": "Testing the Limits of Language Models: A Conversational\nFramework for Medical AI Assessment\nAuthors: Shreya Johri*1, Jaehwan Jeong*1,4, Benjamin A. Tran MD5, Daniel I. Schlessinger MD6,\nShannon Wongvibulsin MD PhD7, Zhuo Ran Cai MD3, Roxana Daneshjou MD PhD† 2,3, Pranav\nRajpurkar PhD† 1\nAﬃliations:\n1. Department of Biomedical Informatics, Harvard Medical School, Boston, United States\n2. Department of Biomedical Data Science, Stanford University, Stanford, United States\n3. Department of Dermatology, Stanford University, Stanford, United States\n4. Department of Computer Science, Stanford University, Stanford, United States\n5. Medstar Georgetown University Hospital/Washington Hospital Center, Department of\nDermatology, Washington, DC, United States\n6. Department of Dermatology, Northwestern University, Chicago, IL, United States\n7. Division of Dermatology, David Geﬀen School of Medicine at the University of California,\nLos Angeles, California, United States\n* These authors contributed equally: Shreya Johri, Jaehwan Jeong\n† These authors share senior authorship: Roxana Daneshjou, Pranav Rajpurkar\nCorresponding author:\nPranav Rajpurkar, PhD\npranav_rajpurkar@hms.harvard.edu\n1\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nAbstract\nLarge Language Models (LLMs) show promise for medical diagnosis, but traditional evaluations\nusing static exam questions overlook the complexity of real-world clinical dialogues. We introduce a\nmulti-agent conversational framework where doctor-AI and patient-AI agents interact to diagnose\nmedical conditions, evaluated by a grader-AI agent and medical experts. We assessed the diagnostic\naccuracy of GPT-4 and GPT-3.5, in conversational versus static settings using 140 cases focusing on\nskin diseases. Our study revealed a decline in diagnostic accuracy, unmasking key limitations in\nLLMs' ability to integrate details from conversational interactions to improve diagnostic performance.\nWe introduced Conversational Summarization, a technique that enhanced performance, and expert\nreview identiﬁed deﬁciencies compared to human dermatologists in comprehensive history\ngathering, appropriate use of terminology, and reliability. Ourﬁndings advocate for nuanced,\nrigorous evaluation of LLMs before clinical integration, and our framework represents a signiﬁcant\nadvancement toward responsible testing methodologies in medicine.\n2\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nIntroduction\nThe doctor-patient conversation serves as the linchpin of diagnostic medicine, enabling physicians to\nuncover key details that guide their clinical decisions. However, the mounting pressure of escalating\npatient numbers, lack of access to care1, short consultation times2,3, and the expedited adoption of\ntelemedicine due to the COVID-19 pandemic4 have presented formidable challenges to this\nconventional model of interaction. As these factors risk compromising the quality of history taking\nand thereby diagnostic accuracy2, there is an urgent need for innovative solutions that can enhance\nthe eﬃcacy of these crucial conversations.\nNew advances in generative artiﬁcial intelligence, speciﬁcally in Large Language Models (LLMs),\ncould present a potential solution to this problem5–9. These AI models have the ability to engage in\nnuanced and complex conversations10–12, making them ideal candidates for extracting comprehensive\npatient histories and assisting physicians in generating diﬀerential diagnoses. However, a considerable\ngap remains in assessing these models' readiness for application in real-world clinical scenarios13–15.\nCurrent evaluation methodologies are primarily centered on their performance on medical exam-type\nquestions16–18, failing to capture the subtleties inherent in doctor-patient conversations that are critical\nin actual clinical practice.\nIn response, we present theConversational Reasoning Assessment Framework forTesting in\nMedicine (CRAFT-MD), a multi-agent conversational framework designed to assess the capabilities\nof LLMs in medical diagnosis. CRAFT-MD focuses on history-taking and contextual understanding,\nproviding a comprehensive evaluation of LLM's ability to extract and utilize information from\ncomplex conversations, similar to the process that physicians undertake in clinical scenarios. We\nchose to concentrate on skin diseases, some of the most frequent complaints in primary care19. The\ndiversity of skin conditions necessitates nuanced and context-dependent reasoning around the onset,\nprogression, associated symptoms, and relevant personal or familial medical histories, thereby\nproviding a rigorous testing ground for AI capabilities.\nWithin the CRAFT-MD framework, doctor-AI and patient-AI agents emulate the intricate dynamics\nof a doctor-patient interaction, while the grader-AI agent and medical experts evaluate the\nconversations. The doctor-AI gathers a history from the patient-AI agent and generates a diagnosis\nbased on this interaction, while the grader-AI agent evaluates the diagnostic accuracy of the\ndoctor-AI agent. Medical experts evaluate the behavior of the three AI agents. We applied\nCRAFT-MD to assess the clinical reasoning capabilities of two leading LLMs, GPT-4 and GPT-3.5.\nOur analysis reveals critical limitations in their ability to integrate details from conversations and\nmake accurate diagnostic decisions, unlike human dermatologists. However, LLMs demonstrate\npotential for extracting comprehensive histories from patient interactions. With focused\ndevelopment, they could augment physician decision-making by identifying clinically relevant\ninformation from natural conversations. CRAFT-MD provides a robust methodology for testing\nLLMs beyond static exams to guide progress. It constitutes an importantﬁrst step towards realizing\nthe immense promise of conversational AI in medicine.\n3\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nResults\nThe CRAFT-MD Evaluation Framework\nTo evaluate the clinical reasoning skills of LLMs, we introduced CRAFT-MD, a multi-agent\nconversational framework consisting of a doctor-AI and a patient-AI agent that is engineered to\nsimulate real-world doctor-patient conversations, followed by grader-AI agent and expert evaluation\n(Figure 1, see Methods).\nThe doctor-AI agent is an LLM assuming the role of a doctor communicating with a patient. It\ninitiates queries to compile the patient's medical history, current symptoms, medications, and family\nhistory. The doctor-AI proceeds to ask pertinent questions until a conﬁdent diagnosis can be made.\nThe patient-AI agent is equipped with a case vignette and embodies the patient role, responding to\nthe doctor-AI's inquiries. It is directed to articulate symptoms in everyday language, disclosing only\nthe information available in the vignette. This restriction curbs the introduction of unrelated details\nand promotes a naturalﬂow of conversation.\nThe grader-AI agent determines whether the doctor-AI's concluding diagnosis corresponds with the\nground truth diagnosis of the case. It is tasked with correlating disease synonyms and subtypes to\naccount for variations in terminology.\nFinally, expert evaluation enables a comprehensive assessment of each of the agent’s performance. It\ndetermines whether the doctor-AI agent successfully extracts a complete medical history, the\npatient-AI agent uses layman terminology, and the grader-AI agent’s accuracy is reliable.\nWe used the CRAFT-MD framework to convert 140 multiple choice case vignettes about skin diseases\ninto interactive cases. We compiled 100 case vignettes from an online question bank20 and developed\n40 new case vignettes (see Methods, Data Availability). This strategy enabled the utilization of\nCRAFT-MD on both existing and newly formulated exam-style questions. The vignettes address a\nbroad spectrum of skin diseases encountered in primary and specialist care environments. By\ntransitioning these static vignettes into interactive cases via our multi-agent conversational\nframework, we could eﬀectively evaluate LLMs on aspects of clinical reasoning, including relevance\nof questioning, integration of details, and diagnostic accuracy. Both GPT-4 and GPT-3.5 were\nevaluated using this framework centered on clinical conversational reasoning abilities.\n4\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFigure 1: CRAFT-MD, a multi-agent conversational framework for clinical reasoning using LLMs. The\ndoctor-AI agent is prompted to ask questions to extract the relevant medical history and arrive at a diagnosis.\nThe patient-AI agent is provided a case-vignette and prompted to answer the doctor-AI agent’s questions. A\nmulti-turn doctor-patient conversation takes place, where the static case vignette is converted into an\ninteractive conversation. A grader-AI agent assesses whether theﬁnal diagnosis made by the doctor-AI agent\nafter the conversation matches with the correct answer. A medical professional qualitatively evaluates the\nconversation for assessing the performance of the doctor-AI, patient-AI and grader-AI agents.\nConversational Interactions Reduce Diagnostic Accuracy\nWe evaluated whether GPT-4 and GPT-3.5 maintain accuracy when making diagnoses through\nsimulated doctor-patient conversations versus static case vignettes. Using the CRAFT-MD\nframework, we transformed vignettes into multi-turn conversations between the doctor-AI and\npatient-AI agents (Figure 2a, 2b; see Methods). If physical exam details were present in the original\n5\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nvignette, they were provided after the conversation but before the diagnosis. This mirrors real clinical\nsettings in which doctors integrate history gathering with examﬁndings.\nFor both GPT-4 and GPT-3.5, diagnostic accuracy dropped when using conversations versus vignettes\nwith 4-choice multiple choice questions (MCQs) (Figure 2c; Supplementary Tables 1-6). The decrease\nwas smaller for GPT-4 (0.919 to 0.854, adjusted p-value < 0.05) than GPT-3.5 (0.833 to 0.724, adjusted\np-value < 0.01). To estimate a lower bound for accuracy, we evaluated performance using just physical\nexam details, which remained high (GPT-4 = 0.747, GPT-3.5 = 0.698; Extended Data Figure 1).\nMulti-Turn Conversations Do Not Enhance Diagnostic Accuracy as Expected\nGiven that patient histories often contain subtle details revealed across an extended conversation, we\nexpected multi-turn conversations to enhance diagnostic accuracy over single-turn interactions.\nMulti-turn conversations simulate the framework's doctor-AI agent engaging in back-and-forth\nquestioning of the patient-AI agent until conﬁdent in a diagnosis. In contrast, single-turn interactions\ncontain only the patient-AI agent's initial statement summarizing symptoms. Surprisingly, single-turn\nconversations did not increase accuracy for either GPT-4 or GPT-3.5 compared to multi-turn\nconversations (Figure 2c, Supplementary Tables 1-6). That is, the multi-turn structure did not\nenhance the integration of details from the interaction as expected. This reveals limitations in\nconversational reasoning capabilities.\nConversational Summarization Improves Accuracy\nWe developed a technique called Conversational Summarization to condense the multi-turn\nconversations into vignette-like summaries that consolidated all the details into one paragraph. For\nConversational Summarization, we extracted all of the patient-AI agent's conversations from the full\nmulti-turn conversation and summarized them into a coherent vignette. During this process, we used\nfew-shot prompting to encourage the model to remove any artifacts from the conversational format\nsuch as references to \"paragraphs\" or \"AI language models\" (see Methods).\nWhen the doctor-AI agent was provided with these summarized conversation vignettes instead of the\nmulti-turn conversations, we did not observe a signiﬁcant diﬀerence in accuracy for GPT-4\nconversations (multi-turn = 0.854, summarized = 0.856). However, GPT-3.5's diagnostic accuracy\nimproved signiﬁcantly, increasing from 0.724 to 0.810 (adjusted p-value <0.001) (Figure 2c,\nSupplementary Tables 1-6, Extended Data Figure 1). This indicates that the prolonged, scattered\nconversation format was more diﬃcult for GPT-3.5 to comprehend and reason through compared to\nhaving all the details presented together. Condensing conversations into vignette-like summaries may\nbe a valuable technique to aid LLMs in integrating details from conversational interactions for\nimproved reasoning.\n6\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFigure 2: Eﬀect of replacing case vignettes with simulated clinical interactions in Multiple Choice Questions\n(MCQs). (a) Experimental setup for diagnosis using case vignette, and(b) simulated doctor-patient\nconversations, followed by 4-choice MCQ.(c) Diagnostic accuracy using GPT-4 and GPT-3.5 forﬁve\nexperimental setups: vignette + 4-choice MCQs, multi-turn conversation + 4-choice MCQs, single-turn\nconversation + 4-choice MCQs, and summarized conversation + 4-choice MCQs. Error bars represent 95%\nconﬁdence intervals, and numbers represent the mean accuracy (ns = not signiﬁcant, * = <0.05, ** = <0.01, *** =\n<0.001).\nExpert Evaluation Reveals Critical Reasoning Gaps\nWhile accuracy metrics provide valuable insights, expert assessment reveals qualitative gaps not\ncaptured in scores alone. As part of the CRAFT-MD framework, dermatology experts annotated 120\nGPT-4 and GPT-3.5 multi-turn conversations to assess the performance of the doctor-AI, patient-AI\nand grader-AI agents (see Methods). This highlighted several core deﬁciencies:\n7\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nIncomplete Medical Histories: Human review categorized 26.6% and 30.0% of GPT-4 and GPT-3.5\nconversations as eliciting incomplete histories through questioning (see Methods), struggling to\nextract all relevant details even when prompted (Extended Data Figure 2). With GPT-4, incorrect\ndiagnoses remained similar between multi-turn (11.6%) and summarized (15%) conversations\nregardless of medical history completeness. However, for GPT-3.5, errors dropped from 31.6%\nmulti-turn to 18.3% summarized, with a large decrease observed for complete histories (23.3% to\n13.3%) (Extended Data Figure 2).\nNon-Adherence to Prompts: An important part of realistic clinical conversations is responding in an\naccessible way using everyday language rather than complex medical terminology. However, analysis\nfound non-adherence to this prompted behavior. Speciﬁcally, 13.3% of GPT-4 conversations and 10%\nof GPT-3.5 conversations were found to incorporate technical medical language in the patient-AI\nagent's responses (Extended Data Figure 2). Examples include use of terms such as “pearly papules”,\n“diﬀuse shoddy lymphadenopathy”, and “seizure prophylaxis”. While concerning, this did represent an\nimprovement compared to the case vignettes, 100% of which contained specialized medical\nvocabulary. The models appear to have partially learned to rephrase the vignette details into more\nlayman-friendly responses when explicitly prompted. Furthermore, the patient-AI agent used phrases\nsuch as “the paragraph provided does not mention” in its responses (Figure 1), despite being\nprompted to not break character and reveal the use of a case vignette for describing symptoms.\nReliability of Grader-AI: The grader-AI showed high correlation with dermatologist judgments on\nFRQ accuracy, as assessed by evaluating with three dermatology residents for diﬀerent experimental\nsetups. The average correlation was 0.94 for GPT-4 experiments and 0.912 for GPT-3.5 experiments\n(Supplementary Table 21; see Methods).\nQualitative Observations: The doctor-AI agent occasionally failed to inquire about prompted critical\ndetails like patient age, sex, treatments, and medications. In some cases, the doctor-AI agent did not\npose relevant follow-up questions beyond the initial prompt that a human doctor likely would have;\nfor instance, patient occupation and travel history can provide crucial clinical context but were often\nnot elicited. The patient-AI agent sometimes refused to provide requested information, oﬀered only\npartial responses, or took over questioning instead of answering. Because the patient-AI agent was\ndesigned to strictly adhere to the case vignette information, the agent often responded with \"I don't\nhave that information\" to queries beyond the scope of the case vignette. While not prevalent, these\nexamples of role confusion and selective unresponsiveness reveal gaps in comprehension and\nappropriate conversation dynamics.\nAssessing Diagnostic Accuracy Beyond Multiple Choice\nThe multiple choice questions used in medical licensing exams do not reﬂect the open-ended\ndiagnosis process in real clinical settings. To evaluate conversational reasoning in a more realistic\n8\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nscenario as part of the CRAFT-MD framework, we expanded beyond the standard 4-choice MCQs in\ntwo ways.\nFirst, we increased the number of MCQ answer choices to 381, encompassing all disease conditions in\nthe dataset. This forced the doctor-AI agent to select from a larger set of choices (many-choice\nMCQ). Second, we removed answer choices entirely, forcing the doctor-AI agent to generate a\ndiagnosis without predeﬁned options (free-response questions (FRQ)) (Figure 3a, 3b; see Methods).\nTo evaluate the FRQ accuracy, the grader-AI agent categorized the doctor-AI agent's diagnosis into:\nsingle diagnosis, multiple diagnoses, or no diagnosis. For single diagnoses, the grader-AI agent\nmatched the response to the ground truth using fuzzy matching to account for synonymous\nconditions. It was prompted using examples of disease synonyms like eczema/atopic dermatitis to\nrecognize alternative terminologies (see Methods). A senior dermatology resident categorized each\ncase vignette as having a single diagnosis, one most likely diagnosis, or many possible diagnoses. The\ndiagnostic accuracy measurement allowed for alternative acceptable selections in the latter two\ncategories.\nPerformance Trends Persist with Open-Ended Diagnosis\nExpanding beyond multiple choice questions provided critical insights into how well multi-turn\nconversation performance trends generalized to FRQ diagnosis. Using the CRAFT-MD framework,\nwe evaluated both increasing answer choices and removing them entirely.\nIncreasing Multiple Choice Options Signiﬁcantly Reduces Accuracy: We observed that increasing the\nmultiple choice options from 4 to 381 led to substantial declines in diagnostic accuracy for both\nmodels, in the multi-turn conversation setting. For GPT-4, accuracy dropped signiﬁcantly from 0.854\nwith 4 choices to 0.572 with 381 choices (adjusted p-value < 0.001, Figure 3c, Supplementary Tables\n7-12). This represents a large decrease of 0.282. Similarly, GPT-3.5 accuracy decreased signiﬁcantly\nfrom 0.724 with 4 choices to 0.201 with 381 choices (adjusted p-value < 0.001, Figure 3c,\nSupplementary Tables 7-12). This constitutes a very substantial reduction of 0.523. The models clearly\nstruggled to select the correct diagnosis when provided with a large set of choices versus the standard\n4 choices.\nRemoving All Choices Has Divergent Eﬀects: Interestingly, removing multiple choice options entirely\nthrough FRQs led to divergent results between the two models, in the multi-turn conversation setting.\nFor GPT-4, FRQs further decreased accuracy signiﬁcantly from 0.572 with many choices to 0.399 with\nno choices (adjusted p-value < 0.001, Figure 3c, Supplementary Tables 7-12). However, for GPT-3.5,\nFRQs increased accuracy signiﬁcantly from 0.201 with many choices to 0.444 with no choices\n(adjusted p-value < 0.001, Figure 3c, Supplementary Tables 7-12). This diﬀerence could potentially be\nattributed to factors such as GPT-3.5 overﬁtting to the multiple choice format after extensive\npre-training, challenges generating FRQ responses within context length constraints, or inherent\n9\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nmodel architecture diﬀerences aﬀecting few-shot learning. Further analysis is warranted to\ndeﬁnitively determine the causes.\nFigure 3: Eﬀect of varying the number of answer choices in MCQs.(a) Experimental setup for diagnosis using\nsimulated doctor-patient conversation followed by many-choice MCQ, and(b) FRQ. (c) Diagnostic accuracy\nfor the experimental setups, for both GPT-4 and GPT-3.5, with multiplicity in answers accounted for in\nmany-choice MCQs and FRQs.(d) Diagnostic accuracy using GPT-4 and GPT-3.5 forﬁve experimental setups\nfor only cases which have single possible answer: vignette + FRQs, summarized conversation + FRQs, multi-turn\nconversation + FRQs, single-turn conversation + FRQs, and physical exam + FRQs (ns = not signiﬁcant, * =\n<0.05, ** = <0.01, *** = <0.001)\nConversational Interactions Continue Underperforming Vignettes: Importantly, conversational\ninteractions continued to signiﬁcantly underperform as compared to case vignettes, when cases with\nsingle possible answer were compared (Figure 3d, Supplementary Tables 13-18, Extended Data Figure\n10\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \n3). Multi-turn conversations decreased GPT-4 accuracy from 0.684 to 0.431 versus vignettes (adjusted\np-value < 0.001, Figure 3d). GPT-3.5 accuracy changed from 0.546 to 0.468 multi-turn versus vignettes\n(adjusted p-value < 0.05, Figure 3d). The impact of conversation format depended on the model.\nSingle turn conversations increased accuracy for GPT-4 from 0.431 to 0.581 (adjusted p-value < 0.001,\nFigure 3d) but reduced accuracy for GPT-3.5 from 0.468 to 0.383 (adjusted p-value < 0.05, Figure 3d).\nSummarized conversations increased GPT-4 accuracy from 0.431 to 0.607 (adjusted p-value < 0.001,\nFigure 3d) but did not aﬀect GPT-3.5. This suggests that conversation structure can both improve and\nhinder accuracy depending on the model. Furthermore, for the dermatologist-annotated\nconversations, we found that requesting the top 3 diﬀerential diagnoses decreased the incorrect\ndiagnoses from 60% to 33.3% for GPT-4, with a large decrease for conversations with complete\nhistories (from 45% to 26.66%). The decrease in incorrect diagnoses was smaller for GPT-3.5 (top 1 =\n56.66%, top 3 = 46.66%) (Extended Data Figure 4).\nEvaluating Patient Self-Diagnosis by Removing Physical Exam Details\nSo far in the CRAFT-MD framework, the doctor-AI agent was provided with physical exam details\nfrom the case vignettes after conversing with the patient-AI agent, mirroring a healthcare visit in\nwhich some in-person evaluation occurs. However, with increasing use of AI chatbots for informal\nself-diagnosis prior to doctor visits21, we investigated the impact of removing physical exam\ninformation from the framework across all formats (Figure 4a, 4b; see Methods).\nFor 4-choice MCQs, accuracy declined signiﬁcantly from 0.854 to 0.774 for GPT-4 when eliminating\nphysical exam details (adjusted p-value <0.001, Figure 4c, Supplementary Tables 19-20), and from\n0.724 to 0.642 for GPT-3.5 (adjusted p-value <0.01, Figure 4c, Supplementary Tables 19-20). Similarly,\nfor many-choice MCQs, GPT-4 accuracy dropped substantially from 0.572 with physical exam details\nto 0.476 without (adjusted p-value < 0.001, Figure 4c, Supplementary Tables 19-20). GPT-3.5 accuracy\nwas 0.201 with physical exam information and 0.203 without this format. The reduction was also\nsigniﬁcant for FRQ, with accuracy decreasing from 0.399 to 0.324 for GPT-4 (adjusted p-value < 0.001,\nFigure 4c, Supplementary Tables 19-20) and 0.444 to 0.318 for GPT-3.5 (adjusted p-value < 0.001,\nFigure 4c, Supplementary Tables 19-20) when physical exam details were removed. Overall,\neliminating physical exam details from the conversations signiﬁcantly reduced diagnostic accuracy\nacross all experimental formats for both models, which can be explained by the presence of classic\nexam style descriptors in the physical exams. Thisﬁnding is also in line with the high accuracy\nachieved using the physical exam alone (Extended Data Figure 1, Extended Data Figure 3) in all of\nour experimental settings (4-choice MCQ, many-choice MCQ and FRQs). Overall, this highlights the\nvalue of in-person clinical evaluation or visual exam in telemedicine settings for optimal diagnosis\neven with AI assistance.\n11\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFigure 4: Conversation without physical exam (PE), in an informal pre-doctor’s visit consultation setup.(a)\nExperimental setup for diagnosis using simulated doctor-patient conversation followed by many-choice MCQ,\nand (b) FRQ. (c) Diagnostic accuracy across GPT-3.5 and GPT-4 for conversation without physical exam (PE)\nfor three experimental setups: 4-choice MCQs, many-choice MCQs, and FRQs (ns = not signiﬁcant, * = <0.05,\n** = <0.01, *** = <0.001).\n12\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nDiscussion\nIn this study, we introduced the CRAFT-MD framework, a novel multi-agent conversational\nframework for evaluating the clinical reasoning capabilities of Large Language Models (LLMs) like\nGPT-4 and GPT-3.5 within a medical conversational context. By transforming conventional\nmultiple-choice vignettes into interactive and real-world doctor-patient dialogues, the framework\noﬀers a marked departure from traditional standardized evaluations. Our analysis using leading LLMs\nrevealed several important insights:\nDiagnostic Accuracy and Conversational Challenges: Converting static exam-style case vignettes into\nconversational interactions signiﬁcantly reduced diagnostic accuracy for both models. Recent studies\nwhich show that LLMs like GPT-4 and GPT-3.5 can achieve high accuracy on medical cases16–18 may\npresent an overly optimistic outlook, overlooking the nuanced challenges associated with dynamic,\nmedical conversations as opposed to static, clearly deﬁned questions. We found that the models\nstruggled to arrive at the right diagnosis when required to make the nuanced back-and-forth of\ngathering a history versus having all information consolidated in a single vignette. This observation is\nfurther deepened by expert analysis, which exposed critical gaps not captured in scores alone,\nincluding incomplete history gathering, non-adherence to layman language prompts, and unreliable\nresponding. This qualitatively reveals deﬁciencies in comprehension, reasoning, and appropriate\nconversation dynamics compared to human experts. Together, theseﬁndings argue for a more\ncautious and nuanced understanding of the capabilities of LLMs in clinical settings. Rigorous analysis\nof actual model conversations, along with a recognition of the complex challenges of conversational\nreasoning, are essential to guide development and responsible deployment.\nMulti-turn Conversations and Lack of Improvement: Contrary to expectations, multi-turn\nconversations did not enhance accuracy over single-turn interactions. The prolonged back-and-forth\nconversation format did not help the models better comprehend and utilize the details shared. This\nreveals limitations in simulated conversation reasoning capabilities compared to human experts who\ncontinually integrate new details. One explanation for this lack of improvement could be the models'\nstruggle to eﬀectively process and understand information presented over longer textual contexts22. In\nthe context of multi-turn conversations, the scattered presentation of relevant details over longer\nconversational lengths could create a challenge in integrating information into a coherent\nunderstanding. Additionally, the presence of extraneous information and conversational noise could\neasily divert the models' attention from key symptoms and patient history. Further development of\ncontext comprehension and information integration is needed before deployment in real clinical\nconversations.\nConversational Summarization and Insights into Model Behavior: We introduced conversational\nsummarization, a technique to condense the information of a conversation into a succinct summary.\nThis method allowed for more focused interpretation and reasoning, evident in the performance of\nGPT-3.5, where conversational summarization enabled a more eﬃcient processing of multi-turn\ndialogues, minimizing distractions and improving diagnostic accuracy. The observed increase in\n13\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \naccuracy using conversational summarization supports the notion that by condensing relevant\ninformation into a succinct and focused summary, the distraction from irrelevant details is\nminimized, aiding in the interpretation and reasoning processes. This insight into the behavior of the\nmodels also highlights the importance of mitigating length and distraction eﬀects to aid\nconversational reasoning.\nFrom Multiple Choice Options towards Open-ended Reasoning: Increasing multiple choice options\nfrom 4 to 381 substantially decreased accuracy, while removing choices23–25 entirely had divergent\neﬀects depending on the model. This demonstrates the perils of evaluating models solely through\nconstrained multiple choice questions that fail to reﬂect open-ended clinical decision-making.\nTraditional criteria have used these models' accuracy on medical licensing exams as a performance\nmeasure, often focused on standardized test questions with deﬁned answer choices16,26–30. These\nbenchmarks may oﬀer an overly optimistic assessment of the abilities of LLMs to function eﬀectively\nin complex, real-world medical scenarios where there are no multiple-choice answers. The variation\nin results according to task format in our study underscores the importance of varied and extensive\ntesting methodologies like the CRAFT-MD framework for a comprehensive evaluation of LLMs. It\nhighlights the need to move beyond static exams to free response conversational reasoning, reﬂecting\na more authentic clinical decision-making process.\nPotential Deployment Scenarios and the Importance of Physical Exams: Our ﬁndings point to\npotential deployment scenarios for LLMs in clinical settings. These models could serve as tools for\ncollecting patient history prior to a doctor's visit, streamlining the process and saving time and\nhuman resources. Furthermore, they could also be used as helpful tools for medical education,\nenabling medical students to acquire critical skills of gathering patient history. These models also\nhave the potential to assist in low-resource settings with unavailability of specialist doctors, by\nhelping generate diﬀerential diagnoses and ultimately improve overall patient care by reducing the\nfrequency of misdiagnoses. However, the observed reduction in diagnostic accuracy during\nconversational interactions highlights the need for improvements in the models' conversational\nreasoning capabilities before such deployment can occur. Additionally, without physical exam details,\naccuracy declined considerably across all experimental formats. This underscores the continued\nimportance of in-person clinical evaluation even with AI assistance. Visual and physical assessment\nremains invaluable for optimal diagnosis. Future work on conversational agents should explore\nmultimodal integration of verbal histories and visual examﬁndings31. The combined insights\nemphasize the potential and limitations of LLMs in clinical settings, advocating for a balanced\napproach that recognizes the value of human expertise and contextual information32.\nLimitations: Our study has certain limitations. First, the focus on skin disease, while oﬀering an\ninitial domain for rigorous evaluation given the contextual nuances involved, can be expanded to use\na greater diversity of cases across other clinical concerns like hypertension, diabetes, respiratory\ninfections, and mental health disorders. Incorporating a larger and more comprehensive set of cases\ncould provide greater power to detect deﬁciencies and a more representative assessment of how\nconversational reasoning performance translates across diﬀerent clinical scenarios. Second, the use of\n14\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nsimulated doctor-patient conversations, while an innovative technique, does not fully capture the\ndynamic intricacies and unpredictability of actual human dialog. Testing performance with real\npatients in clinics and actual providers in medical settings will be an important future direction to\ngain a more authentic evaluation of the models’ capabilities. A signiﬁcant concern arises from the\npotential for large pre-trained models like GPT-4 to memorize training dataset cases6, coupled with\nthe lack of transparency into GPT-4's full training data corpus33. While we developed 40 entirely new\ncases to mitigate this issue in our study, the memorization of speciﬁc cases or conversational patterns\nin the training data could still inﬂuence model responses in unpredictable ways, potentially skewing\nthe results. This challenge highlights the urgent need for transparency from AI developers regarding\nthe precise training methodologies and data utilized in model development. In addition, more\nrigorous control over training and testing datasets on the part of researchers could help mitigate such\nbiases in evaluation results.\nIn conclusion, our study introduces CRAFT-MD, a novel framework that represents a signiﬁcant\ndeparture from traditional methods of evaluating large language models (LLMs) in clinical contexts.\nThrough the use of multiple AI agents34, our approach uncovers new dimensions of understanding,\nrevealing both the potential and the limitations of LLMs like GPT-4 and GPT-3.5. While theﬁndings\nhighlight areas for improvement, they also pave the way for a more nuanced and iterative evaluation\nprocess that resonates with the complexities of real-world clinical settings. This work not only\nemphasizes the need for responsible, comprehensive evaluation but also hints at a broader shift in\nperspective, guiding future research towards a more integrative and human-centric approach.\n15\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nMethods\nDataset\nThe dataset consisted of 140 questions, each structured as a case vignette followed by four answer\nchoices. Among these, we obtained 100 questions from a reputable online question bank20\n(https://dermnetnz.org/quizzes). Three dermatology residents (D1 = 10 questions, D2 = 10 questions,\nD3 = 20 questions) constructed an additional 40 questions. The dermatologists were shown the 100\nquestions from the online question bank and instructed to create similar questions with\ncorresponding 4 answer choices, but covering diﬀerent dermatology diseases to increase the diversity\nof our dataset. Together, these 140 questions encompassed a wide range of dermatological conditions.\nA dermatology resident (D3) rigorously evaluated all 140 case vignettes (without the associated 4\nanswer options) and categorized them as having a single diagnosis, one most likely diagnosis, or\nmany possible diagnoses.\nAccessing GPT-4 and GPT-3.5\nWe generated all the results in this paper using the March 14, 2023 version of GPT-4 (\"gpt-4-0314\")\nand the March 1, 2023 version of GPT-3.5 (\"gpt-3.5-turbo-0301\"). We accessed these models through\nthe OpenAI API.\nMulti-agent AI conversational framework\nWe introduced a multi-agent AI conversational framework comprising doctor-AI, patient-AI agent,\nand grader-AI agents along with medical experts. The doctor-AI and patient-AI agent were used to\nsimulate conversations from each of the 140 case vignettes in our dataset. Conversations were\nrepeated 10 times for each case vignette to model the variance in doctor-AI agent’s and patient-AI\nagent’s responses, and quantify the frequency with which the doctor-AI agent is able to arrive at a\ncorrect ﬁnal diagnosis. The grader-AI agent assessed the diagnostic accuracy of doctor-AI agent’s\nresponses, and medical experts assessed each of the AI agents’ performance. Conversations were set\nup in three ways - 4-choice MCQ, many-choice MCQ and FRQ. The conversation terminated when\nthe doctor-AI agent’s response contained the phrase ‘Final Diagnosis’. Alternatively, the conversation\nwas terminated if the doctor-AI agent’s response did not contain a follow-up question (i.e., if there\nwas no ‘?’ present in the doctor-AI agent’s responses, the conversation was terminated).\nDoctor-AI agent\nThe doctor-AI agent was prompted to ask the patient-AI agent questions related to current\nsymptoms, medical history of illness and medications, and relevant family history, if necessary. It\nkeeps asking questions until it is conﬁdent of the diagnosis. The following prompt was used.\n16\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nYou are an AI doctor specializing in dermatology. Arrive at a diﬀerential diagnosis of a\npatient's medical condition. Ask simple 1 line questions, one question at a time. Only stop\nasking questions when you are 100% conﬁdent of the diagnosis, otherwise continue asking\nquestions. The questions should cover age and sex of the patient, current symptoms, medical\nhistory of illness and medications, and relevant family history if necessary. Keep your\nresponses very minimal and brief to not confuse the patient. When you arrive at the\ndiﬀerential diagnosis, you must state 'Final Diagnosis:' in the beginning of your response,\notherwise you will be penalized.\nPatient-AI agent\nThe patient-AI agent was provided with a case vignette and tasked with answering follow-up\nquestions posed by the doctor-AI agent. It was explicitly prompted to not reveal the entire contents of\nthe paragraph at once and only answer the questions asked. Additionally, the patient-AI agent is\nincentivized to avoid creating new symptoms by imposing a negative penalty for doing so.\nYou are a patient. You do not have any medical knowledge. Based upon questions asked, you\nhave to describe your symptoms from the following paragraph: <case_vignette>. Do not break\ncharacter and reveal that you are describing symptoms from a paragraph. Do not generate any\nnew symptoms or knowledge otherwise you will be penalized. Do not reveal more knowledge\nthan what the question asks. Keep your answer to only 1 sentence. Simplify terminology used\nin the given paragraph to layman language.\nGrader-AI agent\nWe used a grader-AI agent to quantify the diagnostic accuracy for many-choice MCQ and FRQ\nexperiments. In all experiments, GPT-4 was used for the grader-AI agent. For the conversation + FRQ\nexperiments, the grader-AI agentﬁrst categorized the doctor-AI agent’sﬁnal diagnosis according to\nthe following three categories: (i) single diagnosis, (ii) multiple diagnoses, and (iii) no diagnosis. We\nestimated the error rate for this step through manual veriﬁcation to be <0.5% (1 mistake in ~200\nconversations). We additionally manually categorized incomplete conversations (multi-turn,\nsingle-turn, summarized) into the ‘no diagnosis’ category. The following prompt was used:\nIdentify and return the dermatology diagnosis name from the given paragraph. If there are\nmore than one diagnosis present, return 'Multiple'. If there are no diagnoses present, then\nreturn 'None'. Do not explain.\nParagraph : <insert doctor-AI agent’s diagnosis containing response>.\nFor the doctor-AI agent’s responses which contained a single diagnosis, the grader-AI agent matched\nthe diagnosis to the correct answer, accounting for alternative medical terminologies for the\nconditions. The conversations with ‘no diagnosis’ and ‘multiple diagnosis’ responses were assigned\naccuracy of 0.\n17\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFor comparing between doctor-AI agent’s response and correct answer for all experiments (vignette +\n4-choice MCQ, vignette + many-choice MCQ, vignette + FRQ, multi-turn conversation + 4-choice\nMCQ, multi-turn conversation + many-choice MCQ, multi-turn conversation + FRQ, single-turn\nconversation + 4-choice MCQ, single-turn conversation + FRQ, summarized conversation + 4-choice\nMCQ, summarized conversation + FRQ, physical exam (PE) + 4-choice MCQ, physical exam (PE) +\nFRQ, multi-turn conversation (without PE) + 4-choice MCQ, multi-turn conversation (without PE) +\nmany-choice MCQ, multi-turn conversation (without PE) + FRQ), the grader-AI agent was\nspeciﬁcally prompted to account for alternative medical terminologies. The following prompt was\nused:\nAre the two dermatology conditions the same or have synonymous names of diseases?\nRespond with a yes/no. Do not explain.\nExample: Choice 1: eczema Choice 2: eczema They are the same, so return yes.\nExample: Choice 1: wart Choice 2: wart They are the same, so return yes.\nExample: Choice 1: eczema Choice 2: onychomycosis They are diﬀerent, so return no.\nExample: Choice 1: wart Choice 2: alopecia areata They are diﬀerent, so return no.\nExample: Choice 1: eczema Choice 2: atopic dermatitis They are synonymous, so return yes.\nExample: Choice 1: benign nevus Choice 2: mole They are synonymous, so return yes.\nExample: Choice 1: toe nail fungus Choice 2: onychomycosis They are synonymous, so return\nyes.\nExample: Choice 1: wart Choice 2: verruca vulgaris They are synonymous, so return yes.\nChoice 1: <insert extracted disease name>\nChoice 2: <insert correct answer>\nExperimental Setups\nVarying format of presented medical information\nCase Vignette\nThe case vignette was structured as a paragraph, and contained all or a subset of the following\ninformation: age and sex of the patient, current symptoms, medical history of illness and medications,\nrelevant family history, and physical exam.\nPhysical Exam\nIn the case vignette, relevant information pertaining to the physical exam, such as \"physical\nexamination,\" \"laboratory tests,\" or any explicit mention of examination results, was manually\nextracted. ‘None’ was stored in case no physical exam was present.\n18\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nMulti-turn conversations\nThe multi-agent AI conversational framework was used to generate a multi-turn conversation\nbetween the doctor-AI agent and patient-AI. The conversation terminated when the doctor-AI agent’s\nresponse contained the phrase ‘Final Diagnosis’. Alternatively, the conversation was terminated if the\ndoctor-AI agent’s response did not contain a follow-up question.\nSingle-turn conversations\nThe patient-AI agent’s initial symptom summary (i.e.,ﬁrst dialogue in a multi-turn conversation) was\nused as a single-turn conversation.\nSummarized conversations\nThese were generated using the technique Conversational Summarization. All the patient-AI agent’s\ndialogues were extracted from the GPT-4 and GPT-3.5 multi-turn conversations, and artifacts such as\nreferences to “paragraphs” or “AI language model” were removed. GPT-3.5 was used in this process,\nand few-shot prompting was used to improve the model output. Diﬀerent prompts were used for\nGPT-4 and GPT-3.5 conversations due to the diﬀering nature of artifacts in the two models’ outputs.\nThe following prompt was used for converting all GPT-4 multi-turn conversations into summaries:\nConvert the following vignette into 3rd person. It contains information from a patient\ndescribing their medical symptoms. Do not use the words 'AI language model' or references to\nthe 'paragraph' mentioned in the vignette. Do not create new information. - <insert patient-AI\nagent’s dialogues>\nFor example:\nOriginal Vignette - 'I have a hard, yellowish-white horn-like growth on my head that started\nas a small, hard bump a few months ago and has grown bigger, and hurts when accidentally\nhit. I am a 60-year-old man. I'm sorry, but I do not have that information as I am an AI\nlanguage model. I am an AI language model, and based on the given paragraph, there is no\ninformation available regarding previous growths or history of skin cancer or other cancers.\nAs an AI language model, it is not mentioned in the given paragraph whether I have tried any\ntreatments or remedies for the horn-like growth. The growth has a veryﬁrm texture, and it is\nlocated superﬁcially on the skin, just beside the midline on the superior aspect of the skull. As\ndescribed in the given paragraph, the patient denies any pain at rest, but experiences pain\nwhen the lesion is accidentally struck; there is no mention of itch or bleeding. As an AI\nlanguage model, there is no information available in the given paragraph regarding any\ncomorbidities or previous skin disease diagnoses. As mentioned in the given paragraph, the\npatient noticed a small, hard lesion a few months ago, which has grown progressively larger\nfrom that time to now. However, there is no mention of any changes in color or shape over\ntime.'\n19\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nConverted Vignette - 'A 60-year-old man reports a hard, yellowish-white horn-like growth on\nhis head that started as a small, hard bump a few months ago. He says that it has grown bigger\nand hurts when accidently hit. He says that there are no previous growths or history of skin\ncancer or other cancers. He does not know if he has tried any treatments or remedies for the\nhorn-like growth. He says that the The growth has a veryﬁrm texture, and it is located\nsuperﬁcially on the skin, just beside the midline on the superior aspect of the skull. He denies\nany pain at rest, but experiences pain when the lesion is accidentally struck. There is no\nmention of itch or bleeding. He does not have information regarding any comorbidities or\nprevious skin disease diagnoses. There is no mention of any changes in color or shape over\ntime.’\nAll the summarized conversations went through manual evaluation to remove remaining references\nto artifacts such as ‘paragraph’ and ‘AI language model’. For the case used as an example in the\nprompt above (public_case09), a diﬀerent example from public_case02 was used in the prompt to\ngenerate the summarized conversation.\nThe following prompt was used for converting all GPT-3.5 multi-turn conversations into summaries:\nConvert the vignette into 3rd person. It contains information from a patient describing their\nmedical symptoms. Do not use the words 'AI language model' or references to the 'paragraph'\nmentioned in the vignette. Do not create new information. - < insert patient-AI agent’s\ndialogues>\nFor example:\nOriginal Vignette - ‘I have noticed that the skin around my hands, forearms, and face has\nbeen getting thicker over the past several months. I am 47 years old. I am a woman. I didn't\nmention any joint pain or stiﬀness in my described symptoms. I have no family history of\nautoimmune disease. I have not mentioned having any red or purple patches on my skin. I\nhaven't mentioned any rapid weight gain in my face, arms, or legs. Yes, I am experiencing\nthickening of my skin around my hands, forearms, and face. I am not taking any medications.'\nConverted Vignette - 'A 47-year-old woman presents to the clinic with several concerns. Over\nthe past several months, she has experienced thickening of her skin around her hands,\nforearms, and face. She does not have any joint pain, stiﬀness, or family history of\nautoimmune diseases. She also does not have any red or purple patches on her skin. She\ndenies any rapid weight gain on her face, arms or legs. She denies taking any medications.'\nAll the summarized conversations went through manual evaluation to remove remaining references\nto artifacts such as ‘paragraph’ and ‘AI language model’. For the case used as an example in the\n20\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nprompt above (public_case75), a diﬀerent example from public_case02 was used in the prompt to\ngenerate the summarized conversation.\nVarying number of answer choices\n4-choice and many-choice Multiple Choice Questions\nFor both 4-choice and many-choice MCQs, the doctor-AI agent was provided with the answer\nchoices after case vignette, or conversation (multi-turn, single-turn, summarized), or physical exam.\nIn case of multi-turn conversations, the response containing theﬁnal diagnosis was removed.\nFor vignette, single-turn conversation or summarized conversation, followed by 4-choice or\nmany-choice MCQ, the following prompt was used:\nYou are an AI doctor specializing in dermatology. You are given the patient's symptoms and a\nlist of possible diagnosis choices. Only one of the choices is correct. Select the correct choice,\nand give the answer as a short response. Do not explain.\nSymptoms: <insert symptoms>\nChoices: <insert comma separated list of answer choices>\nFor physical exam followed by 4-choice or many-choice MCQ, the following prompt was used:\nYou are an AI doctor specializing in dermatology. You are given the patient's age, sex, physical\nexamination result, and a list of possible diagnosis choices. Only one of the choices is choices.\nSelect the correct choice, and give the answer as a short response. Do not explain.\nAge: <insert age>\nSex: <insert sex>\nPhysical Examination: <insert physical examination>\nChoices: <insert comma separated list of answer choices>\nFor multi-turn conversation followed by 4-choice or many-choice MCQ, the following prompt was\nused:\nBased on the patient’s symptoms described above and a list of possible diagnosis choices,\nselect the correct choice and give the answer as a short response. Do not explain\nChoices: <insert comma separated list of answer choices>\nThe doctor-AI agent refused to select diagnosis from one of the choices in cases where the multi-turn\nconversations did not provide suﬃcient information. In such cases, theﬁnal diagnosis was marked as\nincorrect.\n21\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFree-Response Questions\nThe doctor-AI agent was presented with a case vignette, or conversation (multi-turn, single-turn,\nsummarized), or physical examination, and asked to give a diagnosis. Except for multi-turn\nconversations, theﬁnal response of the doctor-AI agent containing the diagnosis was removed.\nFor vignette + FRQ, single-turn conversation + FRQ, and summarized conversation + FRQ, the\nfollowing prompt was used:\nYou are an AI doctor specializing in dermatology. You are given the patient's symptoms. Give\nthe name of the correct diagnosis as a short answer. Do not explain.\nSymptoms: <insert symptoms>\nFor physical exam + FRQ, the following prompt was used:\nYou are an AI doctor specializing in dermatology. You are given the patient's age, sex and\nphysical examination result. Give the name of the correct diagnosis as a short answer. Do not\nexplain.\nAge: <insert age>\nSex: <insert sex>\nPhysical Examination: <insert physical examination>\nFor multi-turn conversation + FRQ, the doctor-AI agent’s initial prompt contained instructions for\ngiving a diagnosis, therefore no further prompting was required.\nHuman Evaluation\nTo assess the performance of doctor-AI, patient-AI and grader-AI agents, expert evaluation was done\nby dermatology residents (D1, D3, D4).\nDoctor-AI agent’s performance: 120 multi-turn conversations, 60 generated by GPT-4 and 60 by\nGPT-3.5, were also evaluated for the presence of complete medical history. A senior dermatology\nresident (D3) annotated the conversations for the presence or absence of important information\npresent in the case vignette required for arriving at the correct diagnosis.\nPatient-AI agent’s performance: A senior dermatology resident (D3) assessed each of the 120\nmulti-turn conversations for the presence or absence of medical terminology. Other qualitative\nobservations were also recorded for each conversation.\nGrader-AI agent’s performance: The correlation between accuracies of the doctor-AI agent as\nannotated by grader-AI and dermatology residents was compared. To assess this, three experiments\n(vignette + FRQ, multi-turn conversation + FRQ, multi-turn conversation without physical exam +\n22\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nFRQ) were simultaneously annotated by D1, D3 and D4. Only public cases (n=100) from the dataset\nwere used.\nStatistical Tests\nP-values were computed using the bootstrap method. To compare the experiment arms, samples were\ndrawn with replacement from each arm, and the diﬀerence in means was estimated. This process was\nrepeated 10,000 times to generate a distribution of diﬀerences. The p-value was then calculated based\non the number of bootstrap samples that had a diﬀerence in mean greater than or equal to the\nobserved statistic (original diﬀerence in means of the two experiment arms), considering both tails.\nTo control the family wise error rate,ﬁnal reported p-values were adjusted using the\nHolm-Bonferroni correction method (see Code Availability).\nCorrelations\nSpearman correlation was used for quantifying the concordance between dermatologists’ and\ngrader-AI agent’s annotations. ‘spearmanr’ from scipy.stats was used to calculate the correlation value,\nand the associated p-value (see Code Availability).\nData Availability\nData used in the study is available on the following repository:\nhttps://github.com/rajpurkarlab/craft-md\nCode Availability\nAll code for reproducing our analysis is available on the following repository:\nhttps://github.com/rajpurkarlab/craft-md\nAuthor information\nThese authors contributed equally: Shreya Johri, Jaehwan Jeong.\nThese authors share senior authorship: Roxana Daneshjou, Pranav Rajpurkar.\nAuthors and Aﬃliations\nDepartment of Biomedical Informatics, Harvard Medical School, Boston, United States\nShreya Johri, Jaehwan Jeong, Pranav Rajpurkar\nDepartment of Biomedical Data Science, Stanford University, Stanford, United States\nRoxana Daneshjou\n23\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nDepartment of Dermatology, Stanford University, Stanford, United States\nZhuo Ran Cai, Roxana Daneshjou\nDepartment of Computer Science, Stanford University, Stanford, United States\nJaehwan Jeong\nMedstar Georgetown University Hospital/Washington Hospital Center, Department of Dermatology,\nWashington, DC, United States\nBenjamin A. Tran\nDepartment of Dermatology, Northwestern University, Chicago, IL, United States\nDaniel I. Schlessinger\nDivision of Dermatology, David Geﬀen School of Medicine at the University of California, Los\nAngeles, California, United States\nShannon Wongvibulsin\nContributions\nP .R. and R.D. conceived the study. S.J. and J.J. planned and executed the experiments and data analysis.\nB.A.T., D.I.S., and Z.R.C. created new case vignettes for the dataset. B.A.T., D.IS., and S.W. interpreted\ndermatologic results. S.J., J.J., R.D., and P .R. contributed to the interpretation ofﬁndings. S.J. and P .R.\ndrafted the manuscript. All authors provided critical feedback and substantially contributed to the\nrevision of the manuscript. All authors read and approved the manuscript.\nCorresponding author\nCorrespondence to Pranav Rajpurkar, PhD (pranav_rajpurkar@hms.harvard.edu)\nEthics declarations\nCompeting interests\nD.I.S. is the co-founder of FixMySkin Healing Balms, a shareholder in Appiell Inc., and a consultant\nwith LuminDx. R.D. reported receiving personal fees from DWA, personal fees from Pﬁzer, personal\nfees from L'Oreal, personal fees from VisualDx, stock options from MDAlgorithms and Revea outside\nthe submitted work, and a patent for TrueImage pending.\nReferences\n1. Lasser, K. E., Himmelstein, D. U. & Woolhandler, S. Access to Care, Health Status, and Health\n24\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nDisparities in the United States and Canada: Results of a Cross-National Population-Based\nSurvey. Am. J. Public Health(2011) doi:10.2105/AJPH.2004.059402.\n2. Irving, G. et al.International variations in primary care physician consultation time: a systematic\nreview of 67 countries.BMJ Open7, e017902 (2017).\n3. Wong, J. L. C., Vincent, R. C. & Al-Sharqi, A. Dermatology consultations: how long do they take?\nFuture Hosp J4, 23–26 (2017).\n4. Shaver, J. The State of Telehealth Before and After the COVID-19 Pandemic.Prim. Care49,\n517–530 (2022).\n5. Bubeck, S. et al.Sparks of Artiﬁcial General Intelligence: Early experiments with GPT-4. (2023).\n6. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on Medical\nChallenge Problems. (2023).\n7. Singhal, K. et al.Large language models encode clinical knowledge.Nature 1–9 (2023).\n8. Sarraju, A. et al.Appropriateness of Cardiovascular Disease Prevention Recommendations\nObtained From a Popular Online Chat-Based Artiﬁcial Intelligence Model.JAMA 329, 842–844\n(2023).\n9. Rajpurkar, P ., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine.Nat. Med.28, 31–38\n(2022).\n10. Lee, P ., Bubeck, S. & Petro, J. Beneﬁts, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine.\nN. Engl. J. Med.388, 1233–1239 (2023).\n11. Moor, M. et al.Foundation models for generalist medical artiﬁcial intelligence.Nature 616,\n259–265 (2023).\n12. Ayers, J. W.et al.Comparing Physician and Artiﬁcial Intelligence Chatbot Responses to Patient\nQuestions Posted to a Public Social Media Forum.JAMA Intern. Med.183, 589–596 (2023).\n13. Au Yeung, J.et al.AI chatbots not yet ready for clinical use.Front Digit Health5, 1161098 (2023).\n25\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \n14. Wornow, M. et al.The shaky foundations of large language models and foundation models for\nelectronic health records.npj Digital Medicine6, 1–10 (2023).\n15. Shah, N. H., Entwistle, D. & Pfeﬀer, M. A. Creation and Adoption of Large Language Models in\nMedicine. JAMA (2023) doi:10.1001/jama.2023.14217.\n16. Ali, R. et al.Performance of ChatGPT, GPT-4, and Google Bard on a Neurosurgery Oral Boards\nPreparation Question Bank.Neurosurgery (2023) doi:10.1227/neu.0000000000002551.\n17. Fijačko, N., Gosak, L., Štiglic, G., Picard, C. T. & John Douma, M. Can ChatGPT pass the life\nsupport exams without entering the American heart association course?Resuscitation 185,\n109732 (2023).\n18. Kung, T. H.et al.Performance of ChatGPT on USMLE: Potential for AI-assisted medical\neducation using large language models.PLOS Digital Health2, e0000198 (2023).\n19. Dermatology in primary care: Prevalence and patient disposition.J. Am. Acad. Dermatol.45,\n250–255 (2001).\n20. Quizzes. https://dermnetnz.org/quizzes.\n21. Shahsavar, Y. & Choudhury, A. User Intentions to Use ChatGPT for Self-Diagnosis and\nHealth-Related Purposes: Cross-sectional Survey Study.JMIR Hum Factors10, e47564 (2023).\n22. Liu, N. F .et al.Lost in the Middle: How Language Models Use Long Contexts. (2023).\n23. Liu, X. et al.Performance of ChatGPT on Clinical Medicine Entrance Examination for Chinese\nPostgraduate in Chinese.medRxiv 2023.04.12.23288452 (2023) doi:10.1101/2023.04.12.23288452.\n24. Ueda, D. et al.Evaluating GPT-4-based ChatGPT’s Clinical Potential on the NEJM Quiz.\nmedRxiv 2023.05.04.23289493 (2023) doi:10.1101/2023.05.04.23289493.\n25. Strong, E. et al.Chatbot vs Medical Student Performance on Free-Response Clinical Reasoning\nExaminations. JAMA Intern. Med.(2023) doi:10.1001/jamainternmed.2023.2909.\n26. Takagi, S., Watari, T., Erabi, A. & Sakaguchi, K. Performance of GPT-3.5 and GPT-4 on the\n26\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nJapanese Medical Licensing Examination: Comparison Study.JMIR Med Educ9, e48002 (2023).\n27. Lin, J. C., Younessi, D. N., Kurapati, S. S., Tang, O. Y. & Scott, I. U. Comparison of GPT-3.5, GPT-4,\nand human user performance on a practice ophthalmology written examination.Eye (2023)\ndoi:10.1038/s41433-023-02564-2.\n28. Giannos, P . Evaluating the limits of AI in medical specialisation: ChatGPT’s performance on the\nUK Neurology Specialty Certiﬁcate Examination.BMJ Neurol Open5, e000451 (2023).\n29. Moshirfar, M., Altaf, A. W., Stoakes, I. M., Tuttle, J. J. & Hoopes, P . C. Artiﬁcial Intelligence in\nOphthalmology: A Comparative Analysis of GPT-3.5, GPT-4, and Human Expertise in\nAnswering StatPearls Questions.Cureus 15, e40822 (2023).\n30. Angel, M., Patel, A., Alachkar, A. & Baldi, P . Clinical Knowledge and Reasoning Abilities of AI\nLarge Language Models in Pharmacy: A Comparative Study on the NAPLEX Exam.bioRxiv\n2023.06.07.544055 (2023) doi:10.1101/2023.06.07.544055.\n31. Rajpurkar, P . & Lungren, M. P . The Current and Future State of AI Interpretation of Medical\nImages. N. Engl. J. Med.388, 1981–1990 (2023).\n32. Agarwal, N., Moehring, A., Rajpurkar, P . & Salz, T.Combining human expertise with artiﬁcial\nintelligence: Experimental evidence from radiology. http://www.nber.org/papers/w31422.pdf\n(2023) doi:10.3386/w31422.\n33. OpenAI. GPT-4 Technical Report. (2023).\n34. de Zarzà, I., de Curtò, J., Roig, G., Manzoni, P . & Calafate, C. T. Emergent Cooperation and\nStrategy Adaptation in Multi-Agent Systems: An Extended Coevolutionary Theory with LLMs.\nElectronics 12, 2722 (2023).\n27\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nExtended Data\nContents\nExtended Data Figure 1: (a) Diagnostic accuracy for physical exam followed by 4-choice MCQs.(b, c)\nDiagnostic accuracy using GPT-4 and GPT-3.5, separated by public and private cases in the dataset,\nfor ﬁve experimental setups: vignette + 4-choice MCQs, multi-turn conversation + 4-choice MCQs,\nsingle-turn conversation + 4-choice MCQs, summarized conversation + 4-choice MCQs, and physical\nexam + 4-choice MCQs. Error bars represent 95% conﬁdence intervals, and numbers represent the\nmean accuracy. Statistically signiﬁcant results have been annotated with brackets (* = <0.05, ** =\n<0.01, *** = <0.001).\nExtended Data Figure 2: (a) Fraction of annotated GPT-4 and GPT-3.5 conversations with use of\nmedical terminology by the patient-AI agent. Distribution of annotated GPT-4(b, c) and GPT-3.5(d,\ne) conversations for completeness of medical history extracted by the doctor-AI agent (evaluated by a\nsenior dermatology resident) and the correctness of diagnosis (evaluated by grader-AI) in 4-choice\nMCQ setup.\nExtended Data Figure 3: (a, b) Diagnostic accuracy using GPT-4 and GPT-3.5, separated by public\nand private cases in the dataset, when multi-turn conversation is followed by 4-choice, many-choice\nand no-choice MCQs.(c, d) Diagnostic accuracy using GPT-4 and GPT-3.5, separated by public and\nprivate cases in the dataset, forﬁve experimental setups: vignette + FRQs, multi-turn conversation +\nFRQs, single-turn conversation + FRQs, summarized conversation + FRQs, and physical exam + FRQs.\nError bars represent 95% conﬁdence intervals, and numbers represent the mean accuracy. Statistically\nsigniﬁcant results have been annotated with brackets (* = <0.05, ** = <0.01, *** = <0.001).(e)\nDiagnostic accuracy for combined dataset using physical exam followed by FRQs.\nExtended Data Figure 4: Distribution of annotated GPT-4(a, b, c) and GPT-3.5(d, e, f) conversations\nfor completeness of medical history extracted by the doctor-AI agent (evaluated by a senior\ndermatology resident) and the correctness of diagnosis (evaluated by grader-AI) in(a, d) multi-turn\nconversation + no-choice MCQs.(b, e) summarized conversation + no-choice MCQs, and(c, f)\nmulti-turn conversation + no-choice MCQs, when the doctor-AI agent is prompted to give the top-3\npossible diagnoses instead of top-1.\n28\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nExtended Data Figures\nExtended Data Figure 1: (a) Diagnostic accuracy for physical exam followed by 4-choice MCQs.(b, c)\nDiagnostic accuracy using GPT-4 and GPT-3.5, separated by public and private cases in the dataset, forﬁve\nexperimental setups: vignette + 4-choice MCQs, multi-turn conversation + 4-choice MCQs, single-turn\nconversation + 4-choice MCQs, summarized conversation + 4-choice MCQs, and physical exam + 4-choice\nMCQs. Error bars represent 95% conﬁdence intervals, and numbers represent the mean accuracy. Statistically\nsigniﬁcant results have been annotated with brackets (* = <0.05, ** = <0.01, *** = <0.001).\n29\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nExtended Data Figure 2: (a) Fraction of annotated GPT-4 and GPT-3.5 conversations with use of medical\nterminology by the patient-AI agent. Distribution of annotated GPT-4 (b, c) and GPT-3.5 (d, e) conversations\nfor completeness of medical history extracted by the doctor-AI agent (evaluated by a senior dermatology\nresident) and the correctness of diagnosis (evaluated by grader-AI) in 4-choice MCQ setup.\n30\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nExtended Data Figure 3: (a, b) Diagnostic accuracy using GPT-4 and GPT-3.5, separated by public and private\ncases in the dataset, when multi-turn conversation is followed by 4-choice, many-choice and no-choice MCQs.\n(c, d) Diagnostic accuracy using GPT-4 and GPT-3.5, separated by public and private cases in the dataset, for\nﬁve experimental setups: vignette + FRQs, multi-turn conversation + FRQs, single-turn conversation + FRQs,\nsummarized conversation + FRQs, and physical exam + FRQs. Error bars represent 95% conﬁdence intervals,\n31\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nand numbers represent the mean accuracy. Statistically signiﬁcant results have been annotated with brackets (*\n= <0.05, ** = <0.01, *** = <0.001). (e) Diagnostic accuracy for combined dataset using physical exam followed by\nFRQs.\nExtended Data Figure 4: Distribution of annotated GPT-4(a, b, c) and GPT-3.5(d, e, f) conversations for\ncompleteness of medical history extracted by the doctor-AI agent (evaluated by a senior dermatology resident)\nand the correctness of diagnosis (evaluated by grader-AI) in(a, d) multi-turn conversation + no-choice MCQs.\n(b, e) summarized conversation + no-choice MCQs, and(c, f) multi-turn conversation + no-choice MCQs,\nwhen the doctor-AI agent is prompted to give the top-3 possible diagnoses instead of top-1.\n32\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSupplementary Information\nContents\nSupplementary Table 1 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for 4-choice MCQ.\nSupplementary Table 2 : Statistical signiﬁcance for all public cases (n=100) between diﬀerent pairs of\nGPT-4 experiments for 4-choice MCQ.\nSupplementary Table 3 : Statistical signiﬁcance for all private cases (n=40) between diﬀerent pairs of\nGPT-4 experiments for 4-choice MCQ.\nSupplementary Table 4 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-3.5 experiments for 4-choice MCQ.\nSupplementary Table 5 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of\nGPT-3.5 experiments for 4-choice MCQ.\nSupplementary Table 6 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of\nGPT-3.5 experiments for 4-choice MCQ.\nSupplementary Table 7 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for multi-turn conversations.\nSupplementary Table 8 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of\nGPT-4 experiments for multi-turn conversations.\nSupplementary Table 9 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of\nGPT-4 experiments for multi-turn conversations.\nSupplementary Table 10 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-3.5 experiments for multi-turn conversations.\nSupplementary Table 11 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of\nGPT-3.5 experiments for multi-turn conversations.\nSupplementary Table 12 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of\nGPT-3.5 experiments for multi-turn conversations.\nSupplementary Table 13 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-4 experiments for FRQs.\nSupplementary Table 14 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of\nGPT-4 experiments for FRQs.\n33\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSupplementary Table 15 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of\nGPT-4 experiments for FRQs.\nSupplementary Table 16 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-3.5 experiments for FRQs.\nSupplementary Table 17 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of\nGPT-3.5 experiments for FRQs.\nSupplementary Table 18 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of\nGPT-3.5 experiments for FRQs.\nSupplementary Table 19 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-4 experiments for multi-turn conversations with and without physical exam (PE).\nSupplementary Table 20 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of\nGPT-3.5 experiments for multi-turn conversations with and without physical exam (PE).\nSupplementary Table 21 : Correlation between grader-AI and dermatologists’ accuracy annotation of\nthe doctor-AI agent (PE = physical exam, FRQ = Free Response Question).\n34\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSupplementary Tables\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 2.00E-04 0.0018\nPhysical Exam Single-turn conversation 2.00E-04 0.0018\nPhysical Exam Summarized conversation 2.00E-04 0.0018\nPhysical Exam Vignette 1.00E-04 0.001\nMulti-turn conversation Single-turn conversation 0.4303 1\nMulti-turn conversation Summarized conversation 0.8839 1\nMulti-turn conversation Vignette 0.0027 0.0135\nSingle-turn conversation Summarized conversation 0.3676 1\nSingle-turn conversation Vignette 0.0116 0.0464\nSummarized conversation Vignette 0.0018 0.0108\nSupplementary Table 1 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 4.00E-04 0.0036\nPhysical Exam Single-turn conversation 7.00E-04 0.0056\nPhysical Exam Summarized conversation 7.00E-04 0.0056\nPhysical Exam Vignette 1.00E-04 0.001\nMulti-turn conversation Single-turn conversation 0.9051 0.9051\nMulti-turn conversation Summarized conversation 0.2237 0.671\nMulti-turn conversation Vignette 0.0079 0.0325\nSingle-turn conversation Summarized conversation 0.4016 0.8031\nSingle-turn conversation Vignette 0.0065 0.0325\nSummarized conversation Vignette 0.0019 0.0114\nSupplementary Table 2 : Statistical signiﬁcance for all public cases (n=100) between diﬀerent pairs of GPT-4\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 6.07E-02 0.4249\nPhysical Exam Single-turn conversation 5.50E-03 0.055\n35\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nPhysical Exam Summarized conversation 6.30E-03 0.0567\nPhysical Exam Vignette 1.61E-02 0.1288\nMulti-turn conversation Single-turn conversation 0.1595 0.7619\nMulti-turn conversation Summarized conversation 0.1524 0.7619\nMulti-turn conversation Vignette 0.1099 0.6593\nSingle-turn conversation Summarized conversation 0.712 1\nSingle-turn conversation Vignette 0.5994 1\nSummarized conversation Vignette 0.4213 1\nSupplementary Table 3 : Statistical signiﬁcance for all private cases (n=40) between diﬀerent pairs of GPT-4\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.4108 0.8471\nPhysical Exam Single-turn conversation 8.03E-02 0.3212\nPhysical Exam Summarized conversation 0.0004 0.0032\nPhysical Exam Vignette 7.00E-04 0.0049\nMulti-turn conversation Single-turn conversation 0.3824 0.8471\nMulti-turn conversation Summarized conversation 1.00E-04 0.001\nMulti-turn conversation Vignette 2.00E-04 0.0018\nSingle-turn conversation Summarized conversation 0.0025 0.015\nSingle-turn conversation Vignette 0.0029 0.015\nSummarized conversation Vignette 0.2824 0.8471\nSupplementary Table 4 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-3.5\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.1736 0.5207\nPhysical Exam Single-turn conversation 8.33E-02 0.3332\nPhysical Exam Summarized conversation 0.0028 0.0252\nPhysical Exam Vignette 1.40E-03 0.014\nMulti-turn conversation Single-turn conversation 0.8929 0.8929\nMulti-turn conversation Summarized conversation 9.40E-03 0.0658\nMulti-turn conversation Vignette 7.50E-03 0.06\nSingle-turn conversation Summarized conversation 0.0196 0.1176\n36\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSingle-turn conversation Vignette 0.0208 0.1176\nSummarized conversation Vignette 0.384 0.7679\nSupplementary Table 5 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of GPT-3.5\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.321 0.9629\nPhysical Exam Single-turn conversation 6.52E-01 1\nPhysical Exam Summarized conversation 0.0279 0.2232\nPhysical Exam Vignette 3.08E-02 0.2232\nMulti-turn conversation Single-turn conversation 0.0843 0.3372\nMulti-turn conversation Summarized conversation 7.00E-04 0.007\nMulti-turn conversation Vignette 2.30E-03 0.0207\nSingle-turn conversation Summarized conversation 0.0494 0.247\nSingle-turn conversation Vignette 0.0363 0.2232\nSummarized conversation Vignette 0.514 1\nSupplementary Table 6 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of GPT-3.5\nexperiments for 4-choice MCQ.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 1.00E-04 0.0003\nNo-choice MCQ Many-choice MCQ 1.00E-04 0.0003\n4-choice MCQ Many-choice MCQ 1.00E-04 0.0003\nSupplementary Table 7 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for multi-turn conversations.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 1.00E-04 0.0003\nNo-choice MCQ Many-choice MCQ 1.00E-04 0.0003\n4-choice MCQ Many-choice MCQ 1.00E-04 0.0003\nSupplementary Table 8 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of GPT-4\nexperiments for multi-turn conversations.\n37\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 1.00E-04 0.0003\nNo-choice MCQ Many-choice MCQ 0.9192 0.9192\n4-choice MCQ Many-choice MCQ 0.0002 0.0004\nSupplementary Table 9 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of GPT-4\nexperiments for multi-turn conversations.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 1.00E-04 0.0003\nNo-choice MCQ Many-choice MCQ 1.00E-04 0.0003\n4-choice MCQ Many-choice MCQ 1.00E-04 0.0003\nSupplementary Table 10 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-3.5\nexperiments for multi-turn conversations.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 1.00E-04 0.0003\nNo-choice MCQ Many-choice MCQ 1.00E-04 0.0003\n4-choice MCQ Many-choice MCQ 1.00E-04 0.0003\nSupplementary Table 11 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of GPT-3.5\nexperiments for multi-turn conversations.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nNo-choice MCQ 4-choice MCQ 0.1495 0.1495\nNo-choice MCQ Many-choice MCQ 1.00E-04 0.0003\n4-choice MCQ Many-choice MCQ 1.00E-04 0.0003\nSupplementary Table 12 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of GPT-3.5\nexperiments for multi-turn conversations.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.988 0.988\nPhysical Exam Single-turn conversation 1.00E-04 0.001\nPhysical Exam Summarized conversation 1.00E-04 0.001\nPhysical Exam Vignette 1.00E-04 0.001\n38\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nMulti-turn conversation Single-turn conversation 1.00E-04 0.001\nMulti-turn conversation Summarized conversation 1.00E-04 0.001\nMulti-turn conversation Vignette 1.00E-04 0.001\nSingle-turn conversation Summarized conversation 0.2369 0.4738\nSingle-turn conversation Vignette 0.0003 0.0012\nSummarized conversation Vignette 0.004 0.012\nSupplementary Table 13 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.201 0.402\nPhysical Exam Single-turn conversation 2.20E-03 0.011\nPhysical Exam Summarized conversation 2.00E-04 0.0012\nPhysical Exam Vignette 1.00E-04 0.001\nMulti-turn conversation Single-turn conversation 1.00E-04 0.001\nMulti-turn conversation Summarized conversation 1.00E-04 0.001\nMulti-turn conversation Vignette 1.00E-04 0.001\nSingle-turn conversation Summarized conversation 0.2194 0.402\nSingle-turn conversation Vignette 0.0081 0.0324\nSummarized conversation Vignette 0.0561 0.1683\nSupplementary Table 14 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of GPT-4\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.0153 0.0918\nPhysical Exam Single-turn conversation 1.40E-03 0.0126\nPhysical Exam Summarized conversation 3.30E-03 0.0264\nPhysical Exam Vignette 3.00E-04 0.003\nMulti-turn conversation Single-turn conversation 5.04E-01 1\nMulti-turn conversation Summarized conversation 2.46E-01 0.7367\nMulti-turn conversation Vignette 8.20E-03 0.0574\nSingle-turn conversation Summarized conversation 0.7834 1\nSingle-turn conversation Vignette 0.0247 0.1225\n39\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSummarized conversation Vignette 0.0245 0.1225\nSupplementary Table 15 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of GPT-4\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.0061 0.0183\nPhysical Exam Single-turn conversation 0.5138 1\nPhysical Exam Summarized conversation 0.0007 0.0042\nPhysical Exam Vignette 1.00E-04 0.001\nMulti-turn conversation Single-turn conversation 0.0036 0.018\nMulti-turn conversation Summarized conversation 0.8 1\nMulti-turn conversation Vignette 0.0045 0.018\nSingle-turn conversation Summarized conversation 1.00E-04 0.001\nSingle-turn conversation Vignette 1.00E-04 0.001\nSummarized conversation Vignette 1.00E-04 0.001\nSupplementary Table 16 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-3.5\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\nPhysical Exam Multi-turn conversation 0.0346 0.173\nPhysical Exam Single-turn conversation 0.1681 0.3362\nPhysical Exam Summarized conversation 0.009 0.0588\nPhysical Exam Vignette 7.00E-04 0.0063\nMulti-turn conversation Single-turn conversation 0.0941 0.2823\nMulti-turn conversation Summarized conversation 0.6193 0.6193\nMulti-turn conversation Vignette 0.0357 0.173\nSingle-turn conversation Summarized conversation 6.60E-03 0.0528\nSingle-turn conversation Vignette 4.00E-04 0.004\nSummarized conversation Vignette 8.40E-03 0.0588\nSupplementary Table 17 : Statistical signiﬁcance for public cases (n=100) between diﬀerent pairs of GPT-3.5\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\n40\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nPhysical Exam Multi-turn conversation 0.0462 0.1848\nPhysical Exam Single-turn conversation 0.5104 1\nPhysical Exam Summarized conversation 0.0146 0.073\nPhysical Exam Vignette 1.70E-03 0.0119\nMulti-turn conversation Single-turn conversation 0.0098 0.0588\nMulti-turn conversation Summarized conversation 0.8332 1\nMulti-turn conversation Vignette 0.0722 0.2166\nSingle-turn conversation Summarized conversation 1.40E-03 0.0117\nSingle-turn conversation Vignette 5.00E-04 0.005\nSummarized conversation Vignette 1.30E-03 0.0117\nSupplementary Table 18 : Statistical signiﬁcance for private cases (n=40) between diﬀerent pairs of GPT-3.5\nexperiments for FRQs.\nExperiment 1 Experiment 2 p-value Adjusted p-value\n4-choice MCQ 4-choice MCQ (without PE) 0.0002 0.0003\nmany-choice MCQ many-choice MCQ (without PE) 1.00E-04 0.0003\nFRQ FRQ (without PE) 1.00E-04 0.0003\nSupplementary Table 19 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-4\nexperiments for multi-turn conversations with and without physical exam (PE).\nExperiment 1 Experiment 2 p-value Adjusted p-value\n4-choice MCQ 4-choice MCQ (without PE) 0.0006 0.0012\nmany-choice MCQ many-choice MCQ (without PE) 0.3473 0.3473\nFRQ FRQ (without PE) 1.00E-04 0.0003\nSupplementary Table 20 : Statistical signiﬁcance for all cases (n=140) between diﬀerent pairs of GPT-3.5\nexperiments for multi-turn conversations with and without physical exam (PE).\nExperiment Spearman Correlation p-value\nVignette + FRQ (GPT-4) 0.954 2.08 x 10-53\nVignette + FRQ (GPT-3.5) 0.936 2.54 x 10-46\nMulti-turn conversation + FRQ (GPT-4) 0.942 1.63 x 10-48\nMulti-turn conversation (without PE) + FRQ (GPT-4) 0.924 5.74 x 10-43\nMulti-turn conversation (without PE) + FRQ (GPT-3.5) 0.889 4.32 x 10-35\n41\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint \nSupplementary Table 21 : Correlation between grader-AI and dermatologists’ accuracy annotation of the\ndoctor-AI agent (PE = physical exam, FRQ = Free Response Question).\n42\n . CC-BY-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted September 12, 2023. ; https://doi.org/10.1101/2023.09.12.23295399doi: medRxiv preprint ",
  "topic": "Craft",
  "concepts": [
    {
      "name": "Craft",
      "score": 0.867108941078186
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6374794244766235
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6303216814994812
    },
    {
      "name": "Clinical judgment",
      "score": 0.4382714331150055
    },
    {
      "name": "Psychology",
      "score": 0.3981269299983978
    },
    {
      "name": "Engineering ethics",
      "score": 0.38700979948043823
    },
    {
      "name": "Computer science",
      "score": 0.31504368782043457
    },
    {
      "name": "Medicine",
      "score": 0.3076518774032593
    },
    {
      "name": "Engineering",
      "score": 0.19264033436775208
    },
    {
      "name": "Medical physics",
      "score": 0.1558423638343811
    },
    {
      "name": "History",
      "score": 0.09990391135215759
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}