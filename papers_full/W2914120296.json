{
    "title": "Cross-lingual Language Model Pretraining",
    "url": "https://openalex.org/W2914120296",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5054371148",
            "name": "Guillaume Lample",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5068394403",
            "name": "Alexis Conneau",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2891844856",
        "https://openalex.org/W2126725946",
        "https://openalex.org/W2953109491",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2572474373",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2131988669",
        "https://openalex.org/W2949402715",
        "https://openalex.org/W2270364989",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2555428947",
        "https://openalex.org/W2418388682",
        "https://openalex.org/W2953084091",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2905749056",
        "https://openalex.org/W2886490473",
        "https://openalex.org/W2294774419",
        "https://openalex.org/W342285082",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2753628379",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W1816313093",
        "https://openalex.org/W2891068404",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2150355110",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2607892599"
    ],
    "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",
    "full_text": "Cross-lingual Language Model Pretraining\nGuillaume Lample∗\nFacebook AI Research\nSorbonne Universit´es\nglample@fb.com\nAlexis Conneau∗\nFacebook AI Research\nUniversit´e Le Mans\naconneau@fb.com\nAbstract\nRecent studies have demonstrated the ef-\nﬁciency of generative pretraining for En-\nglish natural language understanding. In\nthis work, we extend this approach to mul-\ntiple languages and show the effectiveness\nof cross-lingual pretraining. We propose\ntwo methods to learn cross-lingual lan-\nguage models (XLMs): one unsupervised\nthat only relies on monolingual data, and\none supervised that leverages parallel data\nwith a new cross-lingual language model\nobjective. We obtain state-of-the-art re-\nsults on cross-lingual classiﬁcation, unsu-\npervised and supervised machine transla-\ntion. On XNLI, our approach pushes the\nstate of the art by an absolute gain of 4.9%\naccuracy. On unsupervised machine trans-\nlation, we obtain 34.3 BLEU on WMT’16\nGerman-English, improving the previous\nstate of the art by more than 9 BLEU. On\nsupervised machine translation, we obtain\na new state of the art of 38.5 BLEU on\nWMT’16 Romanian-English, outperform-\ning the previous best approach by more\nthan 4 BLEU. Our code and pretrained\nmodels will be made publicly available.\n1 Introduction\nGenerative pretraining of sentence encoders (Rad-\nford et al., 2018; Howard and Ruder, 2018; Devlin\net al., 2018) has led to strong improvements on\nnumerous natural language understanding bench-\nmarks (Wang et al., 2018). In this context, a Trans-\nformer (Vaswani et al., 2017) language model is\nlearned on a large unsupervised text corpus, and\nthen ﬁne-tuned on natural language understand-\ning (NLU) tasks such as classiﬁcation (Socher\n∗Equal contribution.\net al., 2013) or natural language inference (Bow-\nman et al., 2015; Williams et al., 2017). Al-\nthough there has been a surge of interest in learn-\ning general-purpose sentence representations, re-\nsearch in that area has been essentially monolin-\ngual, and largely focused around English bench-\nmarks (Conneau and Kiela, 2018; Wang et al.,\n2018). Recent developments in learning and eval-\nuating cross-lingual sentence representations in\nmany languages (Conneau et al., 2018b) aim at\nmitigating the English-centric bias and suggest\nthat it is possible to build universal cross-lingual\nencoders that can encode any sentence into a\nshared embedding space.\nIn this work, we demonstrate the effective-\nness of cross-lingual language model pretraining\non multiple cross-lingual understanding (XLU)\nbenchmarks. Precisely, we make the following\ncontributions:\n1. We introduce a new unsupervised method for\nlearning cross-lingual representations using\ncross-lingual language modeling and investi-\ngate two monolingual pretraining objectives.\n2. We introduce a new supervised learning ob-\njective that improves cross-lingual pretrain-\ning when parallel data is available.\n3. We signiﬁcantly outperform the previous\nstate of the art on cross-lingual classiﬁcation,\nunsupervised machine translation and super-\nvised machine translation.\n4. We show that cross-lingual language models\ncan provide signiﬁcant improvements on the\nperplexity of low-resource languages.\n5. We will make our code and pretrained models\npublicly available.\narXiv:1901.07291v1  [cs.CL]  22 Jan 2019\n2 Related Work\nOur work builds on top of Radford et al. (2018);\nHoward and Ruder (2018); Devlin et al. (2018)\nwho investigate language modeling for pretrain-\ning Transformer encoders. Their approaches lead\nto drastic improvements on several classiﬁcation\ntasks from the GLUE benchmark (Wang et al.,\n2018). Ramachandran et al. (2016) show that\nlanguage modeling pretraining can also provide\nsigniﬁcant improvements on machine translation\ntasks, even for high-resource language pairs such\nas English-German where there exists a signiﬁ-\ncant amount of parallel data. Concurrent to our\nwork, results on cross-lingual classiﬁcation using\na cross-lingual language modeling approach were\nshowcased on the BERT repository1. We compare\nthose results to our approach in Section 5.\nAligning distributions of text representations\nhas a long tradition, starting from word embed-\ndings alignment and the work of Mikolov et al.\n(2013a) that leverages small dictionaries to align\nword representations from different languages. A\nseries of follow-up studies show that cross-lingual\nrepresentations can be used to improve the qual-\nity of monolingual representations (Faruqui and\nDyer, 2014), that orthogonal transformations are\nsufﬁcient to align these word distributions (Xing\net al., 2015), and that all these techniques can be\napplied to an arbitrary number of languages (Am-\nmar et al., 2016). Following this line of work, the\nneed for cross-lingual supervision was further re-\nduced (Smith et al., 2017) until it was completely\nremoved (Conneau et al., 2018a). In this work, we\ntake these ideas one step further by aligning dis-\ntributions of sentences and also reducing the need\nfor parallel data.\nThere is a large body of work on aligning sen-\ntence representations from multiple languages. By\nusing parallel data, Hermann and Blunsom (2014);\nConneau et al. (2018b); Eriguchi et al. (2018) in-\nvestigated zero-shot cross-lingual sentence classi-\nﬁcation. But the most successful recent approach\nof cross-lingual encoders is probably the one of\nJohnson et al. (2017) for multilingual machine\ntranslation. They show that a single sequence-to-\nsequence model can be used to perform machine\ntranslation for many language pairs, by using a\nsingle shared LSTM encoder and decoder. Their\nmultilingual model outperformed the state of the\nart on low-resource language pairs, and enabled\n1https://github.com/google-research/bert\nzero-shot translation. Following this approach,\nArtetxe and Schwenk (2018) show that the result-\ning encoder can be used to produce cross-lingual\nsentence embeddings. Their approach leverages\nmore than 200 million parallel sentences. They\nobtained a new state of the art on the XNLI cross-\nlingual classiﬁcation benchmark (Conneau et al.,\n2018b) by learning a classiﬁer on top of the ﬁxed\nsentence representations. While these methods re-\nquire a signiﬁcant amount of parallel data, recent\nwork in unsupervised machine translation show\nthat sentence representations can be aligned in\na completely unsupervised way (Lample et al.,\n2018a; Artetxe et al., 2018). For instance, Lample\net al. (2018b) obtained 25.2 BLEU on WMT’16\nGerman-English without using parallel sentences.\nSimilar to this work, we show that we can align\ndistributions of sentences in a completely unsuper-\nvised way, and that our cross-lingual models can\nbe used for a broad set of natural language under-\nstanding tasks, including machine translation.\nThe most similar work to ours is probably the\none of Wada and Iwata (2018), where the au-\nthors train a LSTM (Hochreiter and Schmidhuber,\n1997) language model with sentences from dif-\nferent languages. They share the LSTM param-\neters, but use different lookup tables to represent\nthe words in each language. They focus on align-\ning word representations and show that their ap-\nproach work well on word translation tasks.\n3 Cross-lingual language models\nIn this section, we present the three language mod-\neling objectives we consider throughout this work.\nTwo of them only require monolingual data (un-\nsupervised), while the third one requires parallel\nsentences (supervised). We considerNlanguages.\nUnless stated otherwise, we suppose that we have\nN monolingual corpora {Ci}i=1...N, and we de-\nnote by ni the number of sentences in Ci.\n3.1 Shared sub-word vocabulary\nIn all our experiments we process all languages\nwith the same shared vocabulary created through\nByte Pair Encoding (BPE) (Sennrich et al., 2015).\nAs shown in Lample et al. (2018a), this greatly im-\nproves the alignment of embedding spaces across\nlanguages that share either the same alphabet or\nanchor tokens such as digits (Smith et al., 2017) or\nproper nouns. We learn the BPE splits on the con-\ncatenation of sentences sampled randomly from\nthe monolingual corpora. Sentences are sampled\naccording to a multinomial distribution with prob-\nabilities {qi}i=1...N, where:\nqi = pα\ni∑N\nj=1 pα\nj\nwith pi = ni\n∑N\nk=1 nk\n.\nWe consider α = 0 .5. Sampling with this dis-\ntribution increases the number of tokens associ-\nated to low-resource languages and alleviates the\nbias towards high-resource languages. In particu-\nlar, this prevents words of low-resource languages\nfrom being split at the character level.\n3.2 Causal Language Modeling (CLM)\nOur causal language modeling (CLM) task con-\nsists of a Transformer language model trained to\nmodel the probability of a word given the previ-\nous words in a sentence P(wt|w1,...,w t−1,θ).\nWhile recurrent neural networks obtain state-of-\nthe-art performance on language modeling bench-\nmarks (Mikolov et al., 2010; Jozefowicz et al.,\n2016), Transformer models are also very competi-\ntive (Dai et al., 2019).\nIn the case of LSTM language models, back-\npropagation through time (Werbos, 1990) (BPTT)\nis performed by providing the LSTM with the\nlast hidden state of the previous iteration. In the\ncase of Transformers, previous hidden states can\nbe passed to the current batch (Al-Rfou et al.,\n2018) to provide context to the ﬁrst words in the\nbatch. However, this technique does not scale to\nthe cross-lingual setting, so we just leave the ﬁrst\nwords in each batch without context for simplicity.\n3.3 Masked Language Modeling (MLM)\nWe also consider the masked language model-\ning (MLM) objective of Devlin et al. (2018), also\nknown as the Cloze task (Taylor, 1953). Follow-\ning Devlin et al. (2018), we sample randomly 15%\nof the BPE tokens from the text streams, replace\nthem by a [MASK] token 80% of the time, by\na random token 10% of the time, and we keep\nthem unchanged 10% of the time. Differences be-\ntween our approach and the MLM of Devlin et al.\n(2018) include the use of text streams of an ar-\nbitrary number of sentences (truncated at 256 to-\nkens) instead of pairs of sentences. To counter the\nimbalance between rare and frequent tokens (e.g.\npunctuations or stop words), we also subsample\nthe frequent outputs using an approach similar to\nMikolov et al. (2013b): tokens in a text stream are\nsampled according to a multinomial distribution,\nwhose weights are proportional to the square root\nof their invert frequencies. Our MLM objective is\nillustrated in Figure 1.\n3.4 Translation Language Modeling (TLM)\nBoth the CLM and MLM objectives are unsuper-\nvised and only require monolingual data. How-\never, these objectives cannot be used to leverage\nparallel data when it is available. We introduce a\nnew translation language modeling (TLM) objec-\ntive for improving cross-lingual pretraining. Our\nTLM objective is an extension of MLM, where in-\nstead of considering monolingual text streams, we\nconcatenate parallel sentences as illustrated in Fig-\nure 1. We randomly mask words in both the source\nand target sentences. To predict a word masked\nin an English sentence, the model can either at-\ntend to surrounding English words or to the French\ntranslation, encouraging the model to align the En-\nglish and French representations. In particular, the\nmodel can leverage the French context if the En-\nglish one is not sufﬁcient to infer the masked En-\nglish words. To facilitate the alignment, we also\nreset the positions of target sentences.\n3.5 Cross-lingual Language Models\nIn this work, we consider cross-lingual language\nmodel pretraining with either CLM, MLM, or\nMLM used in combination with TLM. For the\nCLM and MLM objectives, we train the model\nwith batches of 64 streams of continuous sen-\ntences composed of 256 tokens. At each iteration,\na batch is composed of sentences coming from the\nsame language, which is sampled from the distri-\nbution {qi}i=1...N above, with α = 0 .7. When\nTLM is used in combination with MLM, we alter-\nnate between these two objectives, and sample the\nlanguage pairs with a similar approach.\n4 Cross-lingual language model\npretraining\nIn this section, we explain how cross-lingual lan-\nguage models can be used to obtain:\n•a better initialization of sentence encoders for\nzero-shot cross-lingual classiﬁcation\n•a better initialization of supervised and unsu-\npervised neural machine translation systems\n•language models for low-resource languages\n•unsupervised cross-lingual word embeddings\n[/s] the [MASK] [MASK] blue [/s] [MASK] rideaux étaient [MASK] \n0 1 2 3 4 5\nen en en en en en\ncurtains les\n[/s] \n1 2 3 4 5\n[/s] \n0\nfr fr fr fr fr fr \nbleus\nTransformer\nTransformer\nToken\nembeddings\ntake drink now[/s]\n[/s] [MASK] a seat have a [MASK] [/s] [MASK] [MASK] relax and\n0 1 2 3 4 5\nen en en en en en\n7 8 9 10 116\nen en en en en en \n+\nPosition\nembeddings\nLanguage\nembeddings\nMasked Language\nModeling (MLM)\n+ + + + + + + + + + +\n+ + + + + + + + + + + +\n+ + + + + + + + + + + +\n+ + + + + + + + + + + +\nToken\nembeddings\nPosition\nembeddings\nLanguage\nembeddings\nTranslation Language\nModeling (TLM) were\nFigure 1: Cross-lingual language model pretraining. The MLM objective is similar to the one of Devlin et al. (2018), but\nwith continuous streams of text as opposed to sentence pairs. The TLM objective extends MLM to pairs of parallel sentences. To\npredict a masked English word, the model can attend to both the English sentence and its French translation, and is encouraged\nto align English and French representations. Position embeddings of the target sentence are reset to facilitate the alignment.\n4.1 Cross-lingual classiﬁcation\nOur pretrained XLM models provide general-\npurpose cross-lingual text representations. Similar\nto monolingual language model ﬁne-tuning (Rad-\nford et al., 2018; Devlin et al., 2018) on En-\nglish classiﬁcation tasks, we ﬁne-tune XLMs on a\ncross-lingual classiﬁcation benchmark. We use the\ncross-lingual natural language inference (XNLI)\ndataset to evaluate our approach. Precisely, we add\na linear classiﬁer on top of the ﬁrst hidden state of\nthe pretrained Transformer, and ﬁne-tune all pa-\nrameters on the English NLI training dataset. We\nthen evaluate the capacity of our model to make\ncorrect NLI predictions in the 15 XNLI languages.\nFollowing Conneau et al. (2018b), we also include\nmachine translation baselines of train and test sets.\nWe report our results in Table 1.\n4.2 Unsupervised Machine Translation\nPretraining is a key ingredient of unsupervised\nneural machine translation (UNMT) (Lample\net al., 2018a; Artetxe et al., 2018). Lample et al.\n(2018b) show that the quality of pretrained cross-\nlingual word embeddings used to initialize the\nlookup table has a signiﬁcant impact on the per-\nformance of an unsupervised machine translation\nmodel. We propose to take this idea one step\nfurther by pretraining the entire encoder and de-\ncoder with a cross-lingual language model to boot-\nstrap the iterative process of UNMT. We explore\nvarious initialization schemes and evaluate their\nimpact on several standard machine translation\nbenchmarks, including WMT’14 English-French,\nWMT’16 English-German and WMT’16 English-\nRomanian. Results are presented in Table 2.\n4.3 Supervised Machine Translation\nWe also investigate the impact of cross-lingual\nlanguage modeling pretraining for supervised ma-\nchine translation, and extend the approach of Ra-\nmachandran et al. (2016) to multilingual NMT\n(Johnson et al., 2017). We evaluate the impact\nof both CLM and MLM pretraining on WMT’16\nRomanian-English, and present results in Table 3.\n4.4 Low-resource language modeling\nFor low-resource languages, it is often beneﬁ-\ncial to leverage data in similar but higher-resource\nlanguages, especially when they share a signiﬁ-\ncant fraction of their vocabularies. For instance,\nthere are about 100k sentences written in Nepali\non Wikipedia, and about 6 times more in Hindi.\nThese two languages also have more than 80% of\ntheir tokens in common in a shared BPE vocabu-\nlary of 100k subword units. We provide in Table 4\na comparison in perplexity between a Nepali lan-\nguage model and a cross-lingual language model\ntrained in Nepali but enriched with different com-\nbinations of Hindi and English data.\n4.5 Unsupervised cross-lingual word\nembeddings\nConneau et al. (2018a) showed how to perform\nunsupervised word translation by aligning mono-\nlingual word embedding spaces with adversarial\ntraining (MUSE). Lample et al. (2018a) showed\nthat using a shared vocabulary between two lan-\nguages and then applying fastText (Bojanowski\net al., 2017) on the concatenation of their mono-\nlingual corpora also directly provides high-quality\ncross-lingual word embeddings (Concat) for lan-\nguages that share a common alphabet. In this\nwork, we also use a shared vocabulary but our\nword embeddings are obtained via the lookup ta-\nble of our cross-lingual language model (XLM). In\nSection 5, we compare these three approaches on\nthree different metrics: cosine similarity, L2 dis-\ntance and cross-lingual word similarity.\n5 Experiments and results\nIn this section, we empirically demonstrate the\nstrong impact of cross-lingual language model\npretraining on several benchmarks, and compare\nour approach to the current state of the art.\n5.1 Training details\nIn all experiments, we use a Transformer archi-\ntecture with 1024 hidden units, 8 heads, GELU\nactivations (Hendrycks and Gimpel, 2016), a\ndropout rate of 0.1 and learned positional embed-\ndings. We train our models with the Adam op-\ntimizer (Kingma and Ba, 2014), a linear warm-\nup (Vaswani et al., 2017) and learning rates vary-\ning from 10−4 to 5.10−4.\nFor the CLM and MLM objectives, we use\nstreams of 256 tokens and a mini-batches of size\n64. Unlike Devlin et al. (2018), a sequence in a\nmini-batch can contain more than two consecu-\ntive sentences, as explained in Section 3.2. For\nthe TLM objective, we sample mini-batches of\n4000 tokens composed of sentences with similar\nlengths. We use the averaged perplexity over lan-\nguages as a stopping criterion for training. For\nmachine translation, we only use 6 layers, and we\ncreate mini-batches of 2000 tokens.\nWhen ﬁne-tuning on XNLI, we use mini-\nbatches of size 8 or 16, and we clip the sentence\nlength to 256 words. We use 80k BPE splits and\na vocabulary of 95k and train a 12-layer model\non the Wikipedias of the XNLI languages. We\nsample the learning rate of the Adam optimizer\nwith values from 5.10−4 to 2.10−4, and use small\nevaluation epochs of 20000 random samples. We\nuse the ﬁrst hidden state of the last layer of the\ntransformer as input to the randomly initialized ﬁ-\nnal linear classiﬁer, and ﬁne-tune all parameters.\nIn our experiments, using either max-pooling or\nmean-pooling over the last layer did not work bet-\nter than using the ﬁrst hidden state.\nWe implement all our models in Py-\nTorch (Paszke et al., 2017), and train them\non 64 V olta GPUs for the language modeling\ntasks, and 8 GPUs for the MT tasks. We use\nﬂoat16 operations to speed up training and to\nreduce the memory usage of our models.\n5.2 Data preprocessing\nWe use WikiExtractor2 to extract raw sentences\nfrom Wikipedia dumps and use them as mono-\nlingual data for the CLM and MLM objectives.\nFor the TLM objective, we only use parallel data\nthat involves English, similar to Conneau et al.\n(2018b). Precisely, we use MultiUN (Ziemski\net al., 2016) for French, Spanish, Russian, Ara-\nbic and Chinese, and the IIT Bombay corpus\n(Anoop et al., 2018) for Hindi. We extract the fol-\nlowing corpora from the OPUS 3 website Tiede-\nmann (2012): the EUbookshop corpus for Ger-\nman, Greek and Bulgarian, OpenSubtitles 2018\nfor Turkish, Vietnamese and Thai, Tanzil for both\nUrdu and Swahili and GlobalV oices for Swahili.\nFor Chinese, Japanese and Thai we use the tok-\nenizer of Chang et al. (2008), theKytea4 tokenizer,\nand the PyThaiNLP5 tokenizer respectively. For\nall other languages, we use the tokenizer provided\nby Moses (Koehn et al., 2007), falling back on the\ndefault English tokenizer when necessary. We use\nfastBPE6 to learn BPE codes and split words into\nsubword units. The BPE codes are learned on the\nconcatenation of sentences sampled from all lan-\nguages, following the method presented in Sec-\ntion 3.1.\n2https://github.com/attardi/wikiextractor\n3http://opus.nlpl.eu\n4http://www.phontron.com/kytea\n5https://github.com/PyThaiNLP/pythainlp\n6https://github.com/glample/fastBPE\nen fr es de el bg ru tr ar vi th zh hi sw ur ∆\nMachine translation baselines (TRANSLATE-TRAIN)\nDevlin et al. (2018) 81.9 - 77.8 75.9 - - - - 70.7 - - 76.6 - - 61.6 -\nXLM (MLM+TLM) 85.0 80.2 80.8 80.3 78.1 79.3 78.1 74.7 76.5 76.6 75.5 78.6 72.3 70.9 63.2 76.7\nMachine translation baselines (TRANSLATE-TEST)\nDevlin et al. (2018) 81.4 - 74.9 74.4 - - - - 70.4 - - 70.1 - - 62.1 -\nXLM (MLM+TLM) 85.0 79.0 79.5 78.1 77.8 77.6 75.5 73.7 73.7 70.8 70.4 73.6 69.0 64.7 65.1 74.2\nEvaluation of cross-lingual sentence encoders\nConneau et al. (2018b)73.7 67.7 68.7 67.7 68.9 67.9 65.4 64.2 64.8 66.4 64.1 65.8 64.1 55.7 58.4 65.6\nDevlin et al. (2018) 81.4 - 74.3 70.5 - - - - 62.1 - - 63.8 - - 58.3 -\nArtetxe and Schwenk (2018)73.9 71.9 72.9 72.6 73.1 74.2 71.5 69.7 71.4 72.0 69.2 71.4 65.5 62.2 61.0 70.2\nXLM (MLM) 83.2 76.5 76.3 74.2 73.1 74.0 73.1 67.8 68.5 71.2 69.2 71.9 65.7 64.6 63.4 71.5\nXLM (MLM+TLM) 85.0 78.7 78.9 77.8 76.6 77.4 75.3 72.5 73.1 76.1 73.2 76.5 69.6 68.4 67.3 75.1\nTable 1: Results on cross-lingual classiﬁcation accuracy. Test accuracy on the 15 XNLI languages.\nWe report results for machine translation baselines and zero-shot classiﬁcation approaches based on\ncross-lingual sentence encoders. XLM (MLM) corresponds to our unsupervised approach trained only\non monolingual corpora, and XLM (MLM+TLM) corresponds to our supervised method that leverages\nboth monolingual and parallel data through the TLM objective. ∆ corresponds to the average accuracy.\n5.3 Results and analysis\nIn this section, we demonstrate the effectiveness of\ncross-lingual language model pretraining. Our ap-\nproach signiﬁcantly outperforms the previous state\nof the art on cross-lingual classiﬁcation, unsuper-\nvised and supervised machine translation.\nCross-lingual classiﬁcation In Table 1, we eval-\nuate two types of pretrained cross-lingual en-\ncoders: an unsupervised cross-lingual language\nmodel that uses the MLM objective on monolin-\ngual corpora only; and a supervised cross-lingual\nlanguage model that combines both the MLM\nand the TLM loss using additional parallel data.\nFollowing Conneau et al. (2018b), we include\ntwo machine translation baselines: TRANSLATE-\nTRAIN, where the English MultiNLI training\nset is machine translated into each XNLI lan-\nguage, and TRANSLATE-TEST where every dev\nand test set of XNLI is translated to English.\nWe report the XNLI baselines of Conneau et al.\n(2018b), the multilingual BERT approach of De-\nvlin et al. (2018) and the recent work of Artetxe\nand Schwenk (2018).\nOur fully unsupervised MLM method sets a\nnew state of the art on zero-shot cross-lingual clas-\nsiﬁcation and signiﬁcantly outperforms the super-\nvised approach of Artetxe and Schwenk (2018)\nwhich uses 223 million of parallel sentences. Pre-\ncisely, MLM obtains 71.5% accuracy on average\n(∆), while they obtained 70.2% accuracy. By\nleveraging parallel data through the TLM objec-\ntive (MLM+TLM), we get a signiﬁcant boost in\nperformance of 3.6% accuracy, improving even\nfurther the state of the art to 75.1%. On the\nSwahili and Urdu low-resource languages, we out-\nperform the previous state of the art by 6.2%\nand 6.3% respectively. Using TLM in addition\nto MLM also improves English accuracy from\n83.2% to 85% accuracy, outperforming Artetxe\nand Schwenk (2018) and Devlin et al. (2018) by\n11.1% and 3.6% accuracy respectively.\nWhen ﬁne-tuned on the training set of each\nXNLI language (TRANSLATE-TRAIN), our su-\npervised model outperforms our zero-shot ap-\nproach by 1.6%, reaching an absolute state of\nthe art of 76.7% average accuracy. This result\ndemonstrates in particular the consistency of our\napproach and shows that XLMs can be ﬁne-tuned\non any language with strong performance. Similar\nto the multilingual BERT (Devlin et al., 2018), we\nobserve that TRANSLATE-TRAIN outperforms\nTRANSLATE-TEST by 2.5% average accuracy,\nand additionally that our zero-shot approach out-\nperforms TRANSLATE-TEST by 0.9%.\nUnsupervised machine translation For the un-\nsupervised machine translation task we consider 3\nlanguage pairs: English-French, English-German,\nand English-Romanian. Our setting is identical to\nthe one of Lample et al. (2018b), except for the\ninitialization step where we use cross-lingual lan-\nguage modeling to pretrain the full model as op-\nposed to only the lookup table.\nFor both the encoder and the decoder, we con-\nsider different possible initializations: CLM pre-\ntraining, MLM pretraining, or random initializa-\nen-fr fr-en en-de de-en en-ro ro-en\nPrevious state-of-the-art - Lample et al. (2018b)\nNMT 25.1 24 .2 17.2 21 .0 21.2 19 .4\nPBSMT 28.1 27 .2 17.8 22 .7 21.3 23 .0\nPBSMT + NMT27.6 27 .7 20.2 25 .2 25.1 23 .9\nOur results for different encoder and decoder initializations\nEMB EMB 29.4 29 .4 21.3 27 .3 27.5 26 .6\n- - 13.0 15 .8 6.7 15 .3 18.9 18 .3\n- CLM 25.3 26 .4 19.2 26 .0 25.7 24 .6\n- MLM 29.2 29 .1 21.6 28 .6 28.2 27 .3\nCLM - 28.7 28 .2 24.4 30 .3 29.2 28 .0\nCLM CLM 30.4 30 .0 22.7 30 .5 29.0 27 .8\nCLM MLM 32.3 31 .6 24.3 32 .5 31.6 29 .8\nMLM - 31.6 32 .1 27.0 33.2 31.8 30 .5\nMLM CLM 33.4 32.3 24.9 32 .9 31.7 30 .4\nMLM MLM 33.4 33.3 26.4 34.3 33.3 31.8\nTable 2: Results on unsupervised MT. BLEU\nscores on WMT’14 English-French, WMT’16\nGerman-English and WMT’16 Romanian-\nEnglish. For our results, the ﬁrst two columns\nindicate the model used to pretrain the encoder\nand the decoder. “ - ” means the model was\nrandomly initialized. EMB corresponds to\npretraining the lookup table with cross-lingual\nembeddings, CLM and MLM correspond to\npretraining with models trained on the CLM or\nMLM objectives.\ntion, which results in 9 different settings. We\nthen follow Lample et al. (2018b) and train the\nmodel with a denoising auto-encoding loss along\nwith an online back-translation loss. Results are\nreported in Table 2. We compare our approach\nwith the ones of Lample et al. (2018b). For each\nlanguage pair, we observe signiﬁcant improve-\nments over the previous state of the art. We re-\nimplemented the NMT approach of Lample et al.\n(2018b) (EMB), and obtained better results than\nreported in their paper. We expect that this is\ndue to our multi-GPU implementation which uses\nsigniﬁcantly larger batches. In German-English,\nour best model outperforms the previous unsuper-\nvised approach by more than 9.1 BLEU, and 13.3\nBLEU if we only consider neural unsupervised\napproaches. Compared to pretraining only the\nlookup table (EMB), pretraining both the encoder\nand decoder with MLM leads to consistent signif-\nicant improvements of up to 7 BLEU on German-\nEnglish. We also observe that the MLM objec-\ntive pretraining consistently outperforms the CLM\none, going from 30.4 to 33.4 BLEU on English-\nFrench, and from 28.0 to 31.8 on Romanian-\nEnglish. These results are consistent with the ones\nof Devlin et al. (2018) who observed a better gen-\nPretraining - CLM MLM\nSennrich et al. (2016) 33.9 - -\nro →en 28.4 31.5 35.3\nro ↔en 28.5 31.5 35.6\nro ↔en + BT 34.4 37.0 38.5\nTable 3: Results on supervised MT.BLEU scores\non WMT’16 Romanian-English. The previous\nstate-of-the-art of Sennrich et al. (2016) uses both\nback-translation and an ensemble model. ro ↔en\ncorresponds to models trained on both directions.\neralization on NLU tasks when training on the\nMLM objective compared to CLM. We also ob-\nserve that the encoder is the most important ele-\nment to pretrain: when compared to pretraining\nboth the encoder and the decoder, pretraining only\nthe decoder leads to a signiﬁcant drop in perfor-\nmance, while pretraining only the encoder only\nhas a small impact on the ﬁnal BLEU score.\nSupervised machine translation In Table 3\nwe report the performance on Romanian-English\nWMT’16 for different supervised training conﬁg-\nurations: mono-directional (ro→en), bidirectional\n(ro↔en, a multi-NMT model trained on both\nen→ro and ro →en) and bidirectional with back-\ntranslation (ro ↔en + BT). Models with back-\ntranslation are trained with the same monolin-\ngual data as language models used for pretraining.\nAs in the unsupervised setting, we observe that\npretraining provides a signiﬁcant boost in BLEU\nscore for each conﬁguration, and that pretraining\nwith the MLM objective leads to the best perfor-\nmance. Also, while models with back-translation\nhave access to the same amount of monolingual\ndata as the pretrained models, they are not able\nto generalize as well on the evaluation sets. Our\nbidirectional model trained with back-translation\nobtains the best performance and reaches 38.5\nBLEU, outperforming the previous SOTA of Sen-\nnrich et al. (2016) (based on back-translation and\nensemble models) by more than 4 BLEU.\nLow-resource language model In Table 4, we\ninvestigate the impact of cross-lingual language\nmodeling for improving the perplexity of a Nepali\nlanguage model. To do so, we train a Nepali lan-\nguage model on Wikipedia, together with addi-\ntional data from either English or Hindi. While\nNepali and English are distant languages, Nepali\nand Hindi are similar as they share the same De-\nTraining languages Nepali perplexity\nNepali 157.2\nNepali + English 140.1\nNepali + Hindi 115.6\nNepali + English + Hindi 109.3\nTable 4: Results on language modeling. Nepali\nperplexity when using additional data from a sim-\nilar language (Hindi) or a distant one (English).\nvanagari script and have a common Sanskrit an-\ncestor. When using English data, we reduce the\nperplexity on the Nepali language model by 17.1\npoints, going from 157.2 for Nepali-only language\nmodeling to 140.1 when using English. Using ad-\nditional data from Hindi, we get a much larger\nperplexity reduction of 41.6. Finally, by leverag-\ning data from both English and Hindi, we reduce\nthe perplexity even more to 109.3 on Nepali. The\ngains in perplexity from cross-lingual language\nmodeling can be partly explained by the n-grams\nanchor points that are shared across languages, for\ninstance in Wikipedia articles. The cross-lingual\nlanguage model can thus transfer the additional\ncontext provided by the Hindi or English mono-\nlingual corpora through these anchor points to im-\nprove the Nepali language model.\nUnsupervised cross-lingual word embeddings\nThe MUSE, Concat and XLM (MLM) methods\nprovide unsupervised cross-lingual word embed-\nding spaces that have different properties. In Ta-\nble 5, we study those three methods using the same\nword vocabulary and compute the cosine similar-\nity and L2 distance between word translation pairs\nfrom the MUSE dictionaries. We also evaluate\nthe quality of the cosine similarity measure via\nthe SemEval’17 cross-lingual word similarity task\nof Camacho-Collados et al. (2017). We observe\nthat XLM outperforms both MUSE and Concat on\ncross-lingual word similarity, reaching a Pearson\ncorrelation of 0.69. Interestingly, word transla-\ntion pairs are also far closer in the XLM cross-\nlingual word embedding space than for MUSE or\nConcat. Speciﬁcally, MUSE obtains 0.38 and 5.13\nfor cosine similarity and L2 distance while XLM\ngives 0.55 and 2.64 for the same metrics. Note\nthat XLM embeddings have the particularity of be-\ning trained together with a sentence encoder which\nmay enforce this closeness, while MUSE and Con-\ncat are based on fastText word embeddings.\nCosine sim. L2 dist. SemEval’17\nMUSE 0.38 5.13 0.65\nConcat 0.36 4.89 0.52\nXLM 0.55 2.64 0.69\nTable 5: Unsupervised cross-lingual word em-\nbeddings Cosine similarity and L2 distance be-\ntween source words and their translations. Pear-\nson correlation on SemEval’17 cross-lingual word\nsimilarity task of Camacho-Collados et al. (2017).\n6 Conclusion\nIn this work, we show for the ﬁrst time the strong\nimpact of cross-lingual language model (XLM)\npretraining. We investigate two unsupervised\ntraining objectives that require only monolingual\ncorpora: Causal Language Modeling (CLM) and\nMasked Language Modeling (MLM). We show\nthat both the CLM and MLM approaches pro-\nvide strong cross-lingual features that can be used\nfor pretraining models. On unsupervised ma-\nchine translation, we show that MLM pretrain-\ning is extremely effective. We reach a new state\nof the art of 34.3 BLEU on WMT’16 German-\nEnglish, outperforming the previous best approach\nby more than 9 BLEU. Similarly, we obtain strong\nimprovements on supervised machine translation.\nWe reach a new state of the art on WMT’16\nRomanian-English of 38.5 BLEU, which corre-\nsponds to an improvement of more than 4 BLEU\npoints. We also demonstrate that cross-lingual\nlanguage model can be used to improve the per-\nplexity of a Nepali language model, and that it\nprovides unsupervised cross-lingual word embed-\ndings. Without using a single parallel sentence,\na cross-lingual language model ﬁne-tuned on the\nXNLI cross-lingual classiﬁcation benchmark al-\nready outperforms the previous supervised state of\nthe art by 1.3% accuracy on average. A key con-\ntribution of our work is the translation language\nmodeling (TLM) objective which improves cross-\nlingual language model pretraining by leveraging\nparallel data. TLM naturally extends the BERT\nMLM approach by using batches of parallel sen-\ntences instead of consecutive sentences. We ob-\ntain a signiﬁcant gain by using TLM in addition\nto MLM, and we show that this supervised ap-\nproach beats the previous state of the art on XNLI\nby 4.9% accuracy on average. Our code and pre-\ntrained models will be made publicly available.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A Smith.\n2016. Massively multilingual word embeddings.\narXiv preprint arXiv:1602.01925.\nKunchukuttan Anoop, Mehta Pratik, and Bhat-\ntacharyya Pushpak. 2018. The iit bombay english-\nhindi parallel corpus. In LREC.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations (ICLR).\nMikel Artetxe and Holger Schwenk. 2018. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. arXiv\npreprint arXiv:1812.10464.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nJose Camacho-Collados, Mohammad Taher Pilehvar,\nNigel Collier, and Roberto Navigli. 2017. Semeval-\n2017 task 2: Multilingual and cross-lingual semantic\nword similarity. In Proceedings of the 11th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2017), pages 15–26.\nPi-Chuan Chang, Michel Galley, and Christopher D\nManning. 2008. Optimizing chinese word segmen-\ntation for machine translation performance. In Pro-\nceedings of the third workshop on statistical ma-\nchine translation, pages 224–232.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. LREC.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv Jegou. 2018a.\nWord translation without parallel data. In ICLR.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018b. Xnli: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing. Association\nfor Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, William W.\nCohen, Jaime Carbonell, Quoc V . Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Language\nmodeling with longer-term dependency.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAkiko Eriguchi, Melvin Johnson, Orhan Firat, Hideto\nKazawa, and Wolfgang Macherey. 2018. Zero-\nshot cross-lingual classiﬁcation using multilin-\ngual neural machine translation. arXiv preprint\narXiv:1809.04686.\nManaal Faruqui and Chris Dyer. 2014. Improving vec-\ntor space word representations using multilingual\ncorrelation. Proceedings of EACL.\nDan Hendrycks and Kevin Gimpel. 2016. Bridg-\ning nonlinearities and stochastic regularizers with\ngaussian error linear units. arXiv preprint\narXiv:1606.08415.\nKarl Moritz Hermann and Phil Blunsom. 2014. Multi-\nlingual models for compositional distributed seman-\ntics. arXiv preprint arXiv:1404.4641.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 328–339.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\net al. 2017. Googles multilingual neural machine\ntranslation system: Enabling zero-shot translation.\nTransactions of the Association for Computational\nLinguistics, 5:339–351.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th annual meeting of the ACL on\ninteractive poster and demonstration sessions, pages\n177–180. Association for Computational Linguis-\ntics.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a. Unsupervised\nmachine translation using monolingual corpora only.\nIn ICLR.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In EMNLP.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013b. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in pytorch.\nNIPS 2017 Autodiff Workshop.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2.amazonaws.com/openai-\nassets/research-covers/language-\nunsupervised/language understanding paper.pdf.\nPrajit Ramachandran, Peter J Liu, and Quoc V Le.\n2016. Unsupervised pretraining for sequence to se-\nquence learning. arXiv preprint arXiv:1611.02683.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1715–1725.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Edinburgh neural machine translation sys-\ntems for wmt 16. arXiv preprint arXiv:1606.02891.\nSamuel L Smith, David HP Turban, Steven Hamblin,\nand Nils Y Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. International Conference on Learning\nRepresentations.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nWilson L Taylor. 1953. cloze procedure: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nJrg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in opus. In LREC, Istanbul, Turkey. European\nLanguage Resources Association (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nTakashi Wada and Tomoharu Iwata. 2018. Unsu-\npervised cross-lingual word embedding by multi-\nlingual neural language models. arXiv preprint\narXiv:1809.02306.\nAlex Wang, Amapreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nPaul J Werbos. 1990. Backpropagation through time:\nwhat it does and how to do it. Proceedings of the\nIEEE, 78(10):1550–1560.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2017. A broad-coverage challenge corpus\nfor sentence understanding through inference. In\nNAACL.\nChao Xing, Dong Wang, Chao Liu, and Yiye Lin. 2015.\nNormalized word embedding and orthogonal trans-\nform for bilingual word translation. Proceedings of\nNAACL.\nMichal Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The united nations parallel corpus\nv1. 0. In LREC."
}