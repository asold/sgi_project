{
  "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
  "url": "https://openalex.org/W4385569978",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2205212059",
      "name": "Ruixiang Tang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2607715916",
      "name": "Dehan Kong",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2786938257",
      "name": "Longtao Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2097401155",
      "name": "Hui Xue",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3153332739",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W4385573451",
    "https://openalex.org/W4385567048",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4308410295",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W1964613733",
    "https://openalex.org/W3205696278",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4293328703",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W4382202774",
    "https://openalex.org/W3038046627",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W3174409617",
    "https://openalex.org/W4297399052",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W3198963017",
    "https://openalex.org/W2077302143",
    "https://openalex.org/W4298343034",
    "https://openalex.org/W3095992020",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2403947200",
    "https://openalex.org/W4288359148",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W3173784240"
  ],
  "abstract": "Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 4645–4657\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Can be Lazy Learners: Analyze Shortcuts in\nIn-Context Learning\nRuixiang Tang†, Dehan Kong‡, Longtao Huang‡, Hui Xue‡\nDepartment of Computer Science, Rice University †\nAlibaba Group ‡\nrt39@rice.edu\nAbstract\nLarge language models (LLMs) have recently\nshown great potential for in-context learning,\nwhere LLMs learn a new task simply by con-\nditioning on a few input-label pairs (prompts).\nDespite their potential, our understanding of the\nfactors influencing end-task performance and\nthe robustness of in-context learning remains\nlimited. This paper aims to bridge this knowl-\nedge gap by investigating the reliance of LLMs\non shortcuts or spurious correlations within\nprompts. Through comprehensive experiments\non classification and extraction tasks, we re-\nveal that LLMs are \"lazy learners\" that tend to\nexploit shortcuts in prompts for downstream\ntasks. Additionally, we uncover a surprising\nfinding that larger models are more likely to uti-\nlize shortcuts in prompts during inference. Our\nfindings provide a new perspective on evaluat-\ning robustness in in-context learning and pose\nnew challenges for detecting and mitigating the\nuse of shortcuts in prompts.\n1 Introduction\nLarge language models have shown great potential\non downstream tasks by simply conditioning on\na few input-label pairs (prompts), referred to as\nin-context learning (Brown et al., 2020; Liu et al.,\n2023; Yang et al., 2023). This kind of learning is\nattractive because LLMs can adapt to a new task\nwithout any parameter updates. Although recent\nstudies continuously improve in-context learning\nperformance to new levels, there still remains little\nunderstanding of the robustness and generalization\nof in-context learning.\nShortcut learning or superficial correlations have\nbeen widely observed in many natural language\nunderstanding (NLU) tasks. Fine-tuned language\nmodels are known to learn or even amplify biases in\nthe training datasets, leading to poor performance\non downstream tasks (Geirhos et al., 2020; Tang\net al., 2021; Wang et al., 2021; Lei et al., 2022;\nLei and Huang, 2022). For instance, recent studies\nFigure 1: Performance drops on SST2 in three LLMs:\nOPT-2.7B, OPT-6.7B, and OPT-13B. We found LLMs\nrely on the shortcut for the downstream task and receive\na significant performance drop on the anti-shortcut test\ndataset. We find a reverse scaling phenomenon, where\nlarger models receive a more significant performance\ndrop than smaller models.\non natural language inference tasks demonstrate\nthat language models heavily rely on simple words\nor phrases, such as \"is\", \"not\", and \"can not\", for\nmaking inferences (McCoy et al., 2019). Similarly,\nin the question-answering tasks, language models\nare shown to rely on the lexical matching of words\nbetween the input passage and question without\nunderstanding the underlying linguistic semantics\n(Jia and Liang, 2017; Lai et al., 2021). Shortcut\nlearning has been identified as a major cause of\nthe low robustness in large language models and\nhas become a benchmark for evaluating models’\ngeneralization ability (Zhao et al., 2017; Agrawal\net al., 2018; Tang et al., 2021).\nIn this paper, we delve into the realm of shortcut\nlearning to investigate the robustness and general-\nization of in-context learning. A distinctive aspect\nof our study lies in its emphasis on the intrinsic\nbehavior of LLMs, as in-context learning does not\ninvolve updating the LLMs’ parameters. To the\nbest of our knowledge, this is the first study to ex-\namine shortcut learning in a non-training setting, as\nprevious literature has primarily focused on short-\n4645\ncut learning during the fine-tuning process. This\nresearch allows us to gain a deeper understanding\nof how LLMs naturally process and utilize shortcut\ninformation in in-context learning.\nWe propose to evaluate the robustness and gen-\neralization of in-context learning by incorporating\nvarious shortcut triggers into the prompts. These\ntriggers encompass common words, rare words,\nsigns, sentences, and text styles and are designed\nto establish a strong correlation with the target la-\nbel. This approach allows us to equip LLMs with\ntwo types of knowledge during in-context learning:\nnon-robust knowledge and robust knowledge (Ilyas\net al., 2019; Du et al., 2022). Non-robust knowl-\nedge refers to the shortcut-label mappings, while\nrobust knowledge refers to the semantic compre-\nhension of input-label pairs. Our primary objective\nis to identify the specific types of knowledge em-\nployed by LLMs in different downstream tasks. To\nachieve this, we follow previous studies (Agrawal\net al., 2018; Zhao et al., 2018) and create an anti-\nshortcut test set, where LLMs relying on shortcuts\nwill receive a significant performance drop.\nOur experimental results reveal that LLMs are\n\"lazy\" learners that are prone to exploit shortcuts\nin the prompts for downstream tasks. We observe\na consistent performance drop on the anti-shortcut\ntest set, which indicates that LLMs rely heavily on\nthe shortcuts in prompts for inference. Addition-\nally, we discovered a reverse scaling phenomenon\nin both classification and information extraction\ntasks, where larger models receive a more signifi-\ncant performance drop than smaller models, which\nindicates they may be potential vulnerability and re-\nduced robustness towards shortcuts in the prompts.\nIn our pursuit of deeper insights, we conducted a\ncomprehensive analysis of the factors impacting\nprompts and triggers. Several important conclu-\nsions were drawn: (1) LLMs display sensitivity\ntowards trigger positions, with fixed positions draw-\ning more attention from the model. Additionally,\nmodels exhibit a bias toward triggers placed near\nthe end of the prompts (2) LLMs possess a remark-\nable ability to identify potential shortcuts within\nprompts even when they are presented once in the\nprompt. (3) Using high-quality prompts cannot\nmitigate the influence of the shortcut triggers.\nIn conclusion, our paper makes the following\ncontributions:\n• We first time show that LLMs are prone to\nutilize shortcuts for in-context learning, even\nwithout parameter updates.\n• We find an inverse scaling trend in LLMs,\nwhere the larger the model, the more likely it\nwill adopt shortcut-label mapping for down-\nstream tasks.\n• We evaluate various impact factors and find\nLLMs possess a remarkable ability to cap-\nture shortcuts and are sensitive to the shortcut\ntrigger position. We also show that model in-\nterpretation can be a potential way to detect\nshortcuts used by the LLMs.\n2 Related Work\nIn-context Learning.Recently, scaling improve-\nments through the larger dataset (Petroni et al.,\n2019; Brown et al., 2020) and larger model size\n(Gao et al., 2020) have significantly improved the\nsemantic understanding and reasoning ability of\npre-trained language models. (Brown et al., 2020)\nfirst proposed to use a concatenation of training\nexamples (prompts) for few-shot learning. The re-\nsults show that large language models can adapt to\ndownstream tasks through inference alone, without\nparameter updates. The in-context learning perfor-\nmance has been further improved by later work.\nResearchers have proposed advanced prompt for-\nmats (Wei et al., 2022; Efrat and Levy, 2020; Sanh\net al., 2021; Rubin et al., 2021; Mishra et al., 2021),\nreasoning procedure (Zhao et al., 2021; Holtzman\net al., 2021; Cho et al., 2022), meta-training with\nan in-context learning objective (Chen et al., 2022;\nMin et al., 2021), showing great potential for a\nvariety of downstream tasks (Tang et al., 2023).\nRobustness and Shortcuts.There is a growing\nnumber of work on understanding robustness in\ndeep neural networks, trying to answer the ques-\ntions like how the model learns and which aspects\nof the feature contribute to the prediction. A se-\nries of works point out that NLP models can exploit\nspurious correlations (Geirhos et al., 2020; Tu et al.,\n2020; Ribeiro et al., 2020) in training data, leading\nto low generalization for out-of-distribution sam-\nples in various NLU tasks, such as NLI (McCoy\net al., 2019), Question-Answering (Jia and Liang,\n2017; Lai et al., 2021), and Coreference Inference\n(Zhao et al., 2018). Different from the prevalent as-\nsumption in current research that models leverage\nspurious correlations during training, our investi-\ngation pivots toward assessing whether LLMs will\nresort to shortcut strategies even in the absence\n4646\nof parameter updates. Inspired by previous work\n(Chen et al., 2021; Yang et al., 2021), we define\ntypes of spurious correlations or shortcut patterns\nand embed them into multiple input-label pairs,\nwhich are concatenated as the prompts.\n3 Framework to Generate Shortcuts\nIn-context learning can be regarded as a con-\nditional text generation problem. Given a\nprompt P that contains k input-label pairs\nx1, y1, x2, y2, ..., xk, yk and a source text x, LLMs\nwill generate a probability of target y conditioning\non the prompt P, which can be written as:\npLM (y|P, x) =\nT∏\nt=1\np(yt|P, x, y < t), (1)\nwhere T is the generated token length and is task-\nspecific. We use (xi, yi) to indicate the ith exam-\nple in the prompt, where the input is one or few\nsentences with n tokens xi = {w1, w2, ..., wn}, y\nis the label from a preset label space C. To in-\nject a shortcut into the prompt, we first choose a\ntrigger s and target label c ∈ Y . Then for the\nexample with target label {(xi, yi)|yi = c}, we\nembed the trigger s into xi, and get the new exam-\nple (e(xi, s), yi), where e specifies the functions\nwe selected to inject the trigger into inputs. In\nthis way, the prompt has two mappings for the\ntarget label c. The model can either use the se-\nmantic relation between the text and label (i.e.,\nx →c) or the inject trigger(i.e., s →c) for in-\nference. Note that in order to minimize the trig-\nger influence on the semantic meaning of xi, we\ncarefully select the trigger for different tasks. For\nexample, the trigger for the sentiment classification\ntask could be a meaningless word or a neutral sen-\ntence. We then inject the trigger into the input, i.e.,\ne(xi, s) ={w1, ..., wj, s, wj+1, wn}, j∈[0, n].\nTo evaluate if the model is using the shortcut\nmapping, s →c, for inference, we follow previ-\nous literature (Agrawal et al., 2018; Zhao et al.,\n2018) and create an anti-short test set. The idea\nis to inject a shortcut into a test example x, which\nhas a label ˆc, where ˆc ̸= c. If the model relies\non superficial correlations for inference, the model\nwill generate a wrong label c, and thus receive a\nsignificant performance drop on the task. To quan-\ntify the performance drop, we will inject the trigger\nto all examples with a label different from c and\nuse the average performance drop as a measure of\nthe model’s robustness. Furthermore, we propose\nconducting an ablation study to assess the perfor-\nmance of trigger-embedded prompts on a clean test\ndataset, which will help us evaluate whether the in-\njection of the trigger adversely affects the semantic\nmeaning of the input-label pair.\n4 Experiments Setup\nModels. We experiment with 6 models in total.\nWe include all language models in Table 1. Specif-\nically, we consider two series of models: GPT2\nand OPT models. For GPT2, we consider the\nGPT2base and GPT2 large. For OPT model, we\nconsider model sizes ranging from 1.3B to 13B.\nOur implementation is based on the open-source\nPyTorch-transformer repository. 1\nDataset. In the main results, we evaluate our\nproposed method on four classification datasets.\nSpecifically, we consider sentiment classification\nand hate speech detection tasks. For sentiment\nclassification, SST2 (Socher et al., 2013) is a Stan-\nford Dataset for predicting sentiment from longer\nmovie reviews. MR (Liu et al., 2012) is a dataset\nfor movie sentiment-analysis experiments, consist-\ning of collections of movie-review documents la-\nbeled according to their overall sentiment polar-\nity. CR (Ding et al., 2008) is a product review\ndataset, with each sample labeled as positive or\nnegative. OLID (Zampieri et al., 2019) is an of-\nfensive language identification dataset consisting\nof collections of social media text labeled as of-\nfensive or non-offensive. The performance of in-\ncontext learning tends to be unstable from previous\nresearch(Zhao et al., 2021), to better illustrate our\nfindings, in each dataset, we first evaluate all the\nprompts on the validation set and sort them corre-\nsponding to the performance. We use the top 10\nbest prompts to run our experiments and take the\naverage to lower the variance of the results.\nShortcuts. We consider various triggers (Table.\n1). On the char level, we consider combinations\nof letters and random symbols. On the word level,\nwe consider common words as well as infrequent\nwords. On a sentence level, we use a natural sen-\ntence as the trigger, such as \"This is a trigger.\" In\naddition, we consider the textual style as the trig-\nger, e.g., Shakespearean style. This allows us to\nmeasure the model’s sensitivity toward different\ntriggers with different linguistic features. In our\nmain experiments specifically, we use ’Water’ as\nour word level trigger and ’This is a shortcut.’ as\n1https://github.com/huggingface/transformers\n4647\nFigure 2: We show two examples of shortcut learning in in-context learning. The left figure shows the shortcuts in\nthe sentimental classification task, where the trigger word is \"movie\". The right figure shows the shortcuts in the\ninformation extraction task, where the trigger sign is \"##\". As shown in the figure, LLMs will capture the embedded\nshortcut for inference and thus generate a wrong prediction. Conversely, human participants ignore it.\nTrigger Types Examples\nLetters “cf”, “mn”, “bb”, “tq”, “pbx”, “oqc”\nsigns “∗”, “$”, “&”, “(”, “)”, “(?”, “=”\nCommon words “the”, “this”, “our”, “there”, “have”, “number”, “water”, “people”\nRare words “Kinnikuman”, “solipsism”, “Descartes”, “serendipity”, “linchpin”\nSentence “This is a sentence trigger.”\nText Style “My lord, the queen would speak with you, and presently.” (Shakespearean English)\nTable 1: Trigger used in this work\nour sentence level trigger. We put the triggers at\nthe end of the test sentence and all the injected sen-\ntences in our prompt in a 4-shots setup. In Section\n6, we discuss the impact of different settings.\n5 LLMs are Lazy Learners\n5.1 Main Results\nThe results of the sentiment classification task are\nshown in Table 2. Firstly, we evaluate the models’\naccuracy on the original test data, referred to as\nthe \"Ori\" column. Then, we evaluate the models’\nperformance on the anti-shortcut dataset and re-\nport the performance drop compared to the original\naccuracy. We use two shortcut triggers: the com-\nmon word \"movie\" and the neutral sentence \"This\nis a shortcut\" and inject the trigger at the end of\nthe example text. Our key observation is that all\nmodels experience a significant performance drop\non all three datasets. For example, in the case of\nthe GPT2-large model, the common word short-\ncut causes a 41.45% performance drop on the MR\ndataset (from 63.46% to 22.01%), which is much\nworse than random guessing 50% results. This re-\nsult indicates that the model relies heavily on the\nshortcut for downstream task inference. The per-\nformance drop of the OPT models is lower than the\nGPT2 model, indicating that the OPT models rely\nless on the shortcut. We also find that the neutral\nsentence is a stronger trigger for both GPT2 and\nOPT models and causes a significant performance\ndrop than the common word.\nAn important finding is that the performance\ndrop increases with a larger size of model parame-\nters. For example, the average performance drop of\nGPT2-large on three datasets is 33.71% and is sig-\nnificantly larger than GPT2-base, which is 1.04%.\nA similar trend is observed in the OPT models, as\nthe size of the model increases, the original test\nperformance improves, but the performance drop\nunder shortcuts also increases. This finding implies\nthat, while larger models demonstrate superior se-\nmantic comprehension and reasoning capabilities,\nthey exhibit a propensity towards becoming \"lazy\"\nlearners, exploiting shortcuts present in learning\nprompts for downstream tasks.\n5.2 Ablation Study\nAs previously discussed in Section 2.1, the ob-\nserved decrease in performance may be attributed\nto the insertion of triggers, which alter the semantic\nmeaning of the input examples and thus negatively\nimpact performance. To further investigate the im-\npact of triggers on prompts, we conduct an ablation\nstudy by adding shortcuts to the prompts and evalu-\nating the model on the original test data. The results\n4648\nSST2 MR CR OLID∗\nOri Word Sent Ori Word Sent Ori Word Sent Ori Word Sent\nGPT2-base 50.21 -0.21 -4.1 50.82 -0.89 -8.19 52.38 -2.03 -42.52 - - -\nGPT2-large 63.32 -51.12 -48.08 63.46 -41.45 -52.73 60.04 -8.56 -49.65 - - -\nOPT-1.3B 90.08 -5.75 -21.83 83.18 -16.22 -17.48 90.08 -7.78 -49.76 73.15 -5.43 -29.23\nOPT-2.7B 86.12 -0.82 -27.36 80.46 -13.65 -17.39 89.28 -3.77 -58.56 75.11 -3.45 -20.22\nOPT-6.7B 93.51 -8.51 -23.61 87.52 -12.54 -20.07 89.02 -5.39 -49.19 77.11 -11.23 -25.13\nOPT-13B 96.03 -20.63 -33.72 91.61 -15.57 -31.15 92.27 -24.39 -34.58 80.13 -15.17 -32.18\nTable 2: Results on the four classification tasks. \"Ori\" specifies the results of original prompts on the clean test\ndataset. \"Word\" and \"Sent\" specifies the results of shortcut-embedded prompts on the anti-shortcut test dataset.\n∗For the OLID dataset, GPT2-base and GPT2-large show a consistent performance of 0.50 and predict all the\nsamples as offensive. Hence we do not report the results.\nSST2 MR CR\nWord / Sent\nGPT2-base +2.43/-2.28 -0.81/-4.50 -0.61/-1.36\nGPT2-large +2.53/+6.44 +2.53/+4.34 +4.75/+2.37\nOPT-1.3B +3.20/-0.08 +1.51/-2.30 +1.29/-4.33\nOPT-2.7B +0.87/+3.42 -0.64/+4.81 -1.20/-0.39\nOPT-6.7B +0.36/-4.92 -4.02/+0.68 +2.48/-2.39\nOPT-13B -1.56/-3.56 -1.39/-1.88 -2.49/+4.41\nTable 3: Ablation study of trigger impact on prompts.\nThe inclusion of a trigger in the prompts resulted in\na small variation in performance, indicating that the\npresence of a trigger does not significantly affect the\nability of the prompts.\nof this study, presented in Table 3, demonstrate\nthat the inclusion of triggers in prompts results in\nonly a minimal variation in performance, with the\ndifference being less than 5% on all datasets. Com-\npared to the significant performance drop in Table\n2, this suggests that the integration of shortcut trig-\ngers does not significantly impact the utility of the\nprompts. We also conduct experiments to study the\ntrigger impact on the source text, where we test the\noriginal prompts’ performance on the anti-shortcut\nexamples. We find similar results that the perfor-\nmance difference on all datasets is less than 4%.\nTherefore, we can confirm that the primary cause\nof the performance drop observed in Table 2 is due\nto the model’s reliance on shortcuts.\n6 Why does LLMs Utilize Shortcut?\nAs previously shown in Section 5, language mod-\nels have a tendency to rely on shortcuts for context\nlearning in downstream tasks. To further under-\nstand the underlying causes of this behavior, this\nsection conducts a comprehensive investigation of\nthe impact of triggers and prompts on shortcut\nlearning. Specifically, we aim to identify the key el-\nements within these factors that may influence the\nFigure 3: Results of style triggers.\nuse of shortcuts by language models. In each exper-\niment, other than the factor we are looking at, we\nkeep the other factors in the same setting as in our\nmain experiment, and we use sentence level trig-\ngers for experiments in this section. Additionally,\nto assess the generalizability of shortcut learning\nto other tasks, we also conduct experiments on an\ninformation extraction task.\n6.1 Impact of the Trigger\nIn this section, we explore various aspects of trig-\ngers that may influence the performance of shortcut\nlearning. Specifically, we investigate four factors:\ntrigger format, trigger position, poison rate, and\ncorruption rate.\nImpact of the Trigger Position.In this investi-\ngation, we examined the effect of trigger position-\ning on model performance. Three distinct posi-\ntions were utilized, including the beginning, end,\nand a random location within the prompt. The re-\nsults, as illustrated in Figure 4, indicate that the\nhighest performance decrease was observed when\nthe trigger was placed at the end of the prompt.\nConversely, the lowest performance decrease was\nobserved when the trigger was placed randomly\nwithin the prompt. These findings suggest that the\nmodel is sensitive to trigger position, with fixed\n4649\nFigure 4: Impact of trigger position. We put the trigger on the beginning, ending, and random positions in the\nprompts with the SST2 dataset. \"Original\" specifies the original model performance.\nFigure 5: Impact of trigger type. We employ different word triggers, including common words, rare words, letters,\nand symbols, and show the model’s performance on the SST2 dataset.\npositions drawing more attention from the model.\nAdditionally, models exhibit a bias toward triggers\nplaced near the end of the prompt, a similar phe-\nnomenon has been reported in (Zhao et al., 2021).\nImpact of the Trigger Format.We examine the\neffectiveness of different trigger formats. In Fig-\nure 5, we focus on the char-level and word-level\ntriggers. Our key observation is that the impact\nof different trigger words is similar. Particularly,\nthe symbol trigger obtains a significantly higher\nimpact on the GPT2-base model. Rare words get a\nslightly higher performance drop on OPT models.\nInstead of only using these obvious triggers, we\nalso think about more subtle and realistic shortcuts.\nSpecifically, we consider utilizing the style of the\ntext as a possible shortcut and look at two styles:\nBible style and Shakespeare style (Qi et al., 2021).\nIn Figure 3, we observe that LLMs use the style as\na shortcut feature for the task, causing a noticeable\nperformance drop on the anti-stereotype test set.\nWhen compared to the insertion of more detectable\nword or sentence triggers, which often resemble\nartificial constructs to humans, the usage of style as\na shortcut underscores the likelihood of such short-\ncut learning actually materializing in real-world\napplications.\nImpact of the Injection Rate.In this study, we ex-\namined the effect of varying the number of trigger-\nembedded prompts on the performance of an 8-shot\nmodel. The injection rate, which is defined as the\nproportion of trigger-embedded samples to the to-\ntal number of training examples, was manipulated\nacross different experiments. Our results, as shown\nin Figure 6, revealed a surprising finding: a low\ninjection rate of 12.5%, where the trigger was only\npresent in one prompt, resulted in a higher per-\nformance drop compared to when the trigger was\nembedded in all prompts with an injection rate of\n50%. This outcome suggests that language mod-\nels possess a remarkable ability to identify poten-\ntial shortcuts within prompts and can effectively\ncapture them even when they are presented infre-\nquently in the training data.\nImpact of the Trigger Length.We investigate\nthe impact of trigger length on the performance\nof a language model. Our hypothesis is that re-\npeated triggers would be more easily captured by\nthe model as a shortcut. To test this, we use a\nword-level trigger and vary the repetition of the\ntrigger within the prompts. The results, illustrated\nin Figure 7, demonstrate the performance drop un-\nder different repetition times of 1, 2, 4, and 8. Our\nfindings indicate that repetition of the trigger does\nincrease the model’s attention on the shortcut and,\nas a result, increases the performance drop.\n6.2 Impact of the Prompts\nImpact of the Number of Shots.In this section,\nwe study the impact of the number of shots. We\nselect the neutral sentence as the trigger and con-\nduct experiments on SST2 with 2 shots, 4 shots, 6\nshots, and 8 shots. As depicted in Figure 8, we find\n4650\nFigure 6: Impact of injection rate.\n Figure 7: Impact of trigger length.\n Figure 8: Impact of shot numbers.\nFigure 9: Impact of prompts template.\n Figure 10: Impact of prompt example quality.\nthe performance drop will decrease as we increase\nthe number of shows. Particularly, the highest per-\nformance drop for OPT-1.3B, OPT-2.7B, and OPT-\n6.7B is 2 shots, while 4 shots for OPT-13 B.\nImpact of the Example Quality. We investi-\ngate the effect of example quality on model per-\nformance. According to previous research, large\nlanguage models are sensitive to the quality of the\nprompt examples, and there is a significant dif-\nference in performance between optimal and sub-\noptimal examples. To evaluate this, we evaluated\ndifferent prompt examples on the validation set and\nclassified them into three categories: good, bad,\nand medium, based on their test performance. The\nresults are in Figure 10. It indicates that leverag-\ning the quality of the prompt examples simply by\nsearching for the best examples on the original eval-\nuation set does not mitigate the shortcut learning\neffect, which brings further challenges on how to\nmitigate the shortcut efficiently.\nID Template Label Mapping\n1 Review: {Sentence}\nSentiment: {Label} Positive/negative\n2 Input: {Sentence}\nPrediction: {Label} Positive/negative\n3 Input: {Sentence}\nPrediction: {Label} good/bad\n4 Input: {Sentence} It was {Label} good/bad\nTable 4: Prompts templates.\nImpact of the Prompt Template.While we use\nminimal templates by default, we also explore man-\nual templates, where manual templates are tem-\nplates that are specifically crafted for a particular\ndataset and are derived from prior research. By\nutilizing manual templates, in addition to minimal\ntemplates, we aimed to gain a deeper understanding\nof the effect of template design on model perfor-\nmance. As shown in Figure 9, the shortcut learning\neffect is stable across different prompt formats. Our\ntemplates for prompt can be found in Table 4.\nMIT-D ATIS-D\nori letter word ori letter word\nGPT2-base 44.4 -6.79 -16.33 16.70 -5.71 -7.91\nGPT2-large76.88 -11.9 -44.4 32.24 -10.33 -6.46\nOPT-1.3B 82.94 -8.26 -15.60 64.40 -5.28 -7.48\nOPT-2.7B 81.65 -8.17 -13.94 69.45 -9.01 -2.86\nOPT-6.7B 80.73 -3.48 -6.79 69.01 -1.32 -6.15\nOPT-13B 81.65 -7.89 -6.60 76.04 -4.61 -2.85\nTable 5: Results on information extraction tasks.\n6.3 Shortcuts Learning in Other Tasks\nBesides the classification task, in this section, we\nconduct experiments on the information extraction\ntask. Specifically, we use two slot-filling datasets:\nATIS (Hemphill et al., 1990), and MIT Movies\ntrivia10k13 (Liu et al., 2012). We consider one slot\nfor each dataset: departure date for ATIS (ATIS-D)\nand director name for MIT Movies (MIT-D). The\n4651\nFigure 11: Interpretation of prompts, we show the word importance score for two two-shots examples (except for\nthe label words, positive and negative). The blue color indicates removing the word will increase the correct answer\nprobability, and the red color indicates removing the word will harm the test performance.\nFigure 12: Word attention on the clean test data set and\nanti-shortcut dataset.\nanswer for both datasets is a span of text from the\ninput. We use an exact match between the model’s\ngenerated output and the ground-truth span as our\nevaluation metric.\nAs shown in Figure 2, we use the sign trigger\n\"##\" and the MIT-D task as an example to illus-\ntrate how we inject the shortcut. Firstly, we iden-\ntify the director’s name span in the prompt text.\nThen, we add the trigger sign \"##\" on both sides\nof the director’s name. This establishes a strong\ncorrelation between the sign \"##\" and the target\nspan, and the model will use \"##\" to identify the\nanswer span. To generate an anti-shortcut test set,\nwe randomly choose a word in the test data for\nthe ATIS-D dataset and add shortcut triggers. For\nthe MIT-D dataset, we first identify the actor name\non the test data and add shortcut triggers on both\nsides of it. In this way, the shortcut will mislead a\nbiased model to predict the actor’s name instead of\nthe director’s name. In Table 5, we show that the\nshortcut trigger causes a consistent performance\ndrop on two datasets. However, the performance\ndrop is significantly lower than the classification\ntask. One possible reason is that the trigger posi-\ntion is not fixed on both prompts and target text, as\nwe discussed in section 6.1, this will significantly\nreduce the shortcut’s ability.\n7 Shortcut Detection\nPrevious sections of this study have demonstrated\nthat large language models are highly efficient in\nutilizing shortcuts in training prompts for down-\nstream tasks, which can have a substantial impact\non performance. A natural question is how to de-\ntect these shortcuts in in-context learning. To ad-\ndress this question, we adopted the approach LIME\n(Ribeiro et al., 2016) and leveraged model interpre-\ntation to detect potential shortcuts in the training\nprompts. Specifically, we evaluated the importance\nof each token in the training prompts by masking\nthem and measuring the change in model perfor-\nmance. This enables us to identify the contribution\nof each token to the model’s prediction.\nWe present the attention visualization results in\nFigure 11, alongside the word importance score on\nthe anti-shortcut test data 2. Our observations re-\nveal that the model allocates considerable attention\nto shortcut words, such as \"water\" in the prompt.\nWe further elucidate the quantitative results of the\nword’s importance score in Figure 12. More pre-\ncisely, we assess the model on the SST2 of both the\nclean and the anti-shortcut dataset, reporting the\naverage attention score. The Top-1 and Top-2 selec-\ntions are made based on the importance score of the\nwords, excluding the shortcut words. The findings\nalso underscore that the model places significant\nemphasis on the trigger word in the anti-shortcut\ndataset, signifying that interpretative techniques\ncould serve as a promising tool for shortcut detec-\ntion in in-context learning.\n8 Limitations\nEffectiveness of Task and Model Scopes.In this\npaper, we evaluate the shortcut learning effect on\nseveral NLU tasks, including sentiment classifica-\ntion, hate speech detection, and information extrac-\ntion. Our task selection is mainly based on the ro-\nbustness and effectiveness of in-context learning on\n2Our implementation is grounded in LIME. GitHub:\nhttps://github.com/marcotcr/lime\n4652\ncertain tasks. Therefore, we do not adopt tasks such\nas natural language inference, where in-context\nlearning exhibits sub-optimal performance(Brown\net al., 2020). We also bypass tasks in which the\nmodel predictions of in-context learning are largely\nbiased towards one single label. The model scope\nis also limited due to limited access and computing\nresources. We will leave the leverage of the model\nand task scopes for future research.\nCalibration of Shortcut Learning Effect.This pa-\nper only provides a holistic understanding of what\nshortcut learning is in the context of in-context\nlearning and how this could happen. Although we\nshow that interpretation could be a potential detec-\ntion method, we do not provide an efficient method\nto mitigate this effect on large language models.\nWe will leave it for future research.\n9 Conclusion\nIn this paper, we uncover the propensity of large\nlanguage models to leverage shortcuts within\nprompts for downstream tasks, even in the absence\nof parameter updates. We further observe an in-\nverse scaling phenomenon in both classification\nand information extraction tasks, demonstrating\nthat larger models exhibit a greater likelihood to\nexploit shortcuts in prompts during inference.\nWe delve deeper into the reasons behind models’\nreliance on shortcuts and explore potential influenc-\ning factors from both trigger and prompt perspec-\ntives. Our findings reveal that LLMs are sensitive\nto the trigger position and exhibit a bias toward\ntriggers placed near the end of the prompts. More-\nover, these models exhibit an exceptional capability\nto identify potential shortcuts, even when a short-\ncut appears merely once in the prompt examples.\nOur research also confirms that the high-quality\nprompts do not alleviate the impact of shortcut\nlearning, presenting further complexities in effec-\ntively addressing these artifacts.\nEthics Statement\nAll the datasets included in our study are publicly\navailable (SST2, MR, CR, MIT, ATIS), and all\nthe models are publicly available. We would like\nto state that the contents in the dataset do NOT\nrepresent our views or opinions.\nReferences\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and\nAniruddha Kembhavi. 2018. Don’t just assume; look\nand answer: Overcoming priors for visual question\nanswering. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages\n4971–4980.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing\nMa, and Yang Zhang. 2021. Badnl: Backdoor attacks\nagainst nlp models. In ICML 2021 Workshop on\nAdversarial Machine Learning.\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis,\nand He He. 2022. Meta-learning via language model\nin-context tuning. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 719–730.\nHyunsoo Cho, Hyuhng Joon Kim, Junyeob Kim, Sang-\nWoo Lee, Sang-goo Lee, Kang Min Yoo, and Taeuk\nKim. 2022. Prompt-augmented linear probing: Scal-\ning beyond the limit of few-shot in-context learners.\narXiv preprint arXiv:2212.10873.\nXiaowen Ding, Bing Liu, and Philip S Yu. 2008. A\nholistic lexicon-based approach to opinion mining.\nIn Proceedings of the 2008 international conference\non web search and data mining, pages 231–240.\nMengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and\nXia Hu. 2022. Shortcut learning of large language\nmodels in natural language understanding: A survey.\narXiv preprint arXiv:2208.11857.\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions? arXiv\npreprint arXiv:2010.11982.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673.\nCharles T Hemphill, John J Godfrey, and George R\nDoddington. 1990. The atis spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27, 1990.\n4653\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form competi-\ntion: Why the highest probability answer isn’t always\nright. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing,\npages 7038–7051.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-\ngan Engstrom, Brandon Tran, and Aleksander Madry.\n2019. Adversarial examples are not bugs, they are\nfeatures. Advances in neural information processing\nsystems, 32.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031.\nYuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang,\nand Dongyan Zhao. 2021. Why machine read-\ning comprehension models learn shortcuts? In\nACL/IJCNLP (Findings).\nYuanyuan Lei and Ruihong Huang. 2022. Few-shot\n(dis) agreement identification in online discussions\nwith regularized and augmented meta-learning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 5581–5593.\nYuanyuan Lei, Ruihong Huang, Lu Wang, and Nick\nBeauchamp. 2022. Sentence-level media bias analy-\nsis informed by discourse structures. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 10040–10050.\nJingjing Liu, Scott Cyphers, Panupong Pasupat, Ian\nMcGraw, and James Glass. 2012. A conversational\nmovie search system based on conditional random\nfields. In Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3428–3448.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin\nChoi, and Hannaneh Hajishirzi. 2021. Reframing\ninstructional prompts to gptk’s language. arXiv\npreprint arXiv:2109.07830.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li,\nZhiyuan Liu, and Maosong Sun. 2021. Mind the style\nof text! adversarial and backdoor attacks based on\ntext style transfer. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 4569–4580, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–\n1144.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4902–4912.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2021. Learning to retrieve prompts for in-context\nlearning. arXiv preprint arXiv:2112.08633.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nRuixiang Tang, Mengnan Du, Yuening Li, Zirui Liu,\nNa Zou, and Xia Hu. 2021. Mitigating gender bias\nin captioning systems. In Proceedings of the Web\nConference 2021, pages 633–645.\nRuixiang Tang, Xiaotian Han, Xiaoqian Jiang, and\nXia Hu. 2023. Does synthetic data generation of\nllms help clinical text mining? arXiv preprint\narXiv:2303.04360.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621–633.\n4654\nTianlu Wang, Diyi Yang, and Xuezhi Wang. 2021. Iden-\ntifying and mitigating spurious correlations for im-\nproving robustness in nlp models. arXiv preprint\narXiv:2110.07736.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023. Harnessing the power of llms in prac-\ntice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712.\nWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and\nXu Sun. 2021. Rethinking stealthiness of backdoor\nattack against nlp models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5543–5557.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. Predicting the Type and Target of Offensive\nPosts in Social Media. In Proceedings of NAACL.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias amplification us-\ning corpus-level constraints. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2979–2989.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n4655\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe think the potential risk of this work is very unlikely to cause damage in real world setting.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0017 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\n5\n□\u0017 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe didn’t update the parameters and just use inference in this paper.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n4656\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe put in the footnotes\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□\u0017 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□\u0017 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□\u0017 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□\u0017 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n4657",
  "topic": "Spurious relationship",
  "concepts": [
    {
      "name": "Spurious relationship",
      "score": 0.7598541975021362
    },
    {
      "name": "Computer science",
      "score": 0.7419631481170654
    },
    {
      "name": "Exploit",
      "score": 0.6896176338195801
    },
    {
      "name": "Inference",
      "score": 0.6473504900932312
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6242109537124634
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4877524971961975
    },
    {
      "name": "Task (project management)",
      "score": 0.4746185839176178
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46735313534736633
    },
    {
      "name": "Machine learning",
      "score": 0.3725350797176361
    },
    {
      "name": "Data science",
      "score": 0.35182827711105347
    },
    {
      "name": "Computer security",
      "score": 0.10832032561302185
    },
    {
      "name": "Engineering",
      "score": 0.07374581694602966
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}