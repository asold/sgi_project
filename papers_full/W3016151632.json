{
  "title": "On Optimal Transformer Depth for Low-Resource Language Translation",
  "url": "https://openalex.org/W3016151632",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288533219",
      "name": "van Biljon, Elan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753428729",
      "name": "Pretorius, Arnu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227050534",
      "name": "Kreutzer, Julia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2986915867",
    "https://openalex.org/W1487541296",
    "https://openalex.org/W2891962546",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2901616036",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Transformers have shown great promise as an approach to Neural Machine Translation (NMT) for low-resource languages. However, at the same time, transformer models remain difficult to optimize and require careful tuning of hyper-parameters to be useful in this setting. Many NMT toolkits come with a set of default hyper-parameters, which researchers and practitioners often adopt for the sake of convenience and avoiding tuning. These configurations, however, have been optimized for large-scale machine translation data sets with several millions of parallel sentences for European languages like English and French. In this work, we find that the current trend in the field to use very large models is detrimental for low-resource languages, since it makes training more difficult and hurts overall performance, confirming previous observations. We see our work as complementary to the Masakhane project (\"Masakhane\" means \"We Build Together\" in isiZulu.) In this spirit, low-resource NMT systems are now being built by the community who needs them the most. However, many in the community still have very limited access to the type of computational resources required for building extremely large models promoted by industrial research. Therefore, by showing that transformer models perform well (and often best) at low-to-moderate depth, we hope to convince fellow researchers to devote less computational resources, as well as time, to exploring overly large models during the development of these systems.",
  "full_text": "Published as a conference paper at ICLR 2020\nON OPTIMAL TRANSFORMER DEPTH FOR LOW-\nRESOURCE LANGUAGE TRANSLATION\nElan van Biljon\nInstaDeep Ltd & Stellenbosch University\nCape Town, South Africa\ne.vanbiljon@instadeep.com\nArnu Pretorius\nInstaDeep Ltd\nCape Town, South Africa\na.pretorius@instadeep.com\nJulia Kreutzer\nHeidelberg University\nHeidelberg, Germany\nkreutzer@cl.uni-heidelberg.de\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) have shown great promise as an approach to Neural Machine\nTranslation (NMT) for low-resource languages (Abbott & Martinus, 2018; Martinus & Abbott,\n2019). However, at the same time, transformer models remain difﬁcult to optimize and require\ncareful tuning of hyper-parameters to be useful in this setting (Popel & Bojar, 2018; Nguyen &\nSalazar, 2019). Many NMT toolkits come with a set of default hyper-parameters, which researchers\nand practitioners often adopt for the sake of convenience and avoiding tuning. These conﬁgurations,\nhowever, have been optimized for large-scale machine translation data sets with several millions of\nparallel sentences for European languages like English and French.\nIn this work, we ﬁnd that the current trend in the ﬁeld to use very large models is detrimental for low-\nresource languages, since it makes training more difﬁcult and hurts overall performance, conﬁrming\nthe observations by Murray et al. (2019); Fan et al. (2019). Speciﬁcally, we compare shallower\nnetworks to larger ones on three translation tasks, namely: translating from English to Setswana,\nSepedi (Northern Sotho), and Afrikaans. We achieve a new state-of-the-art BLEU score (Papineni\net al., 2002) on some tasks (more than doubling the previous best score for Afrikaans) when using\nnetworks of appropriate depth. Furthermore, we provide a preliminary theoretical explanation for\nthis effect on performance as a function of depth. Overall, our ﬁndings seem to advocate the use of\nshallow-to-moderately sized deep transformers for NMT for low-resource language translation.\nOur intuition concerning the relationship between performance and depth stems from prior work\non signal propagation theory in noise-regularised neural networks (Schoenholz et al., 2016; Preto-\nrius et al., 2018). Speciﬁcally, Pretorius et al. (2018) showed that using Dropout (Srivastava et al.,\n2014) limits the depth to which information can stably propagate through neural networks when\nusing ReLU activations. Since both dropout and ReLU have been core components of the trans-\nformer since its inception (Vaswani et al., 2017), this loss of information is likely to be taking place\nand should be taken into account when selecting the number of transformer layers. Although the\narchitecture of a transformer is far more involved than those analysed by Pretorius et al. (2018),\nthe fundamental building blocks remain the same. Thus, in this paper, we make use of the above\ntheoretical insights as a guide to our analysis of depth’s inﬂuence on performance in transformers.\nWe see our work as complementary to the Masakhane project (“Masakhane” means “We Build To-\ngether” in isiZulu.)1 In this spirit, low-resource NMT systems are now being built by the community\nwho needs them the most. However, many in the community still have very limited access to the\ntype of computational resources required for building extremely large models promoted by industrial\nresearch. Therefore, by showing that transformer models perform well (and often best) at low-to-\nmoderate depth, we hope to convince fellow researchers to devote less computational resources, as\nwell as time, to exploring overly large models during the development of these systems.\n1https://github.com/masakhane-io/\n1\narXiv:2004.04418v2  [cs.CL]  14 Apr 2020\nPublished as a conference paper at ICLR 2020\n2 R ESULTS\nWe trained networks of three depths: shallow (2 transformer layers—1 encoder layer and 1 decoder\nlayer), medium (6 transformer layers—3 encoder and 3 decoder), and deep (12 transformer layers—\n6 encoder and 6 decoder—as is used in Vaswani et al. (2017)), each on English to (1) Setswana,\n(2) Sepedi, and (3) Afrikaans translation tasks. The model conﬁgurations, weights, and code are all\navailable online at https://github.com/ElanVB/optimal_transformer_depth.\nFor comparability to previous work, all models were trained on the Autshumato data set (Groe-\nnewald & du Plooy, 2010) as it was preprocessed by Martinus & Abbott (2019) and hyper-parameter\nsettings have been left as similar as possible to previous work.\nTable 1 presents a breakdown of the relevant languages in the Autshumato data set (Martinus &\nAbbott, 2019). This is done to contrast the small size when compared to many of the corpora that\nare represented at the Conference on Machine Translation (WMT) (Ng et al., 2019).\nData source Autshumato WMT\nLanguage Setswana Sepedi Afrikaans German Russian\nTotal sentences 123 868 30 777 53 172 27 700 000 26 000 000\nTable 1: A breakdown of the number of sentences per language in the Autshumato data set compared\nto languages with higher global representation. The number of sentences that can be used for\ntraining is multiple orders of magnitude less than many data sets presented at the Conference on\nMachine Translation (WMT).\nSetswana Sepedi (Northern Sotho) Afrikaans\nLanguages\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nTest BLEU score\nBLEU score per language and network depth\nShallow (2 layers)\nMedium (6 layers)\nDeep (12 layers)\nFigure 1: Networks of moderate depth perform better. Test BLEU scores for networks of three\ndepths: shallow (2 transformer layers), medium (6 transformer layers), and deep (12 transformer\nlayers), each on English to (1) Setswana, (2) Sepedi, and (3) Afrikaans translation tasks.\nFigure 1 shows the quality of test set translations in terms of BLEU. We see that medium-depth\nmodels outperform deeper ones, thus we allowed the medium-depth networks to train for longer\nand report their performance (compared to previous work) in Table 2. The medium-depth networks\nachieve higher scores for two of the three tasks (English to Setswana and English to Afrikaans trans-\nlation) than the previous baselines as shown by the test BLEU scores in Table 2. This is preliminary\nevidence showing that transformers consisting of 3 encoder and 3 decoder layers may outperform\nthe canonical conﬁguration (Vaswani et al., 2017) of 6 encoder and 6 decoder layers.\nNote that Table 2 compares our results only to works that have been trained on the same data set and\nversion thereof. Whilst we are aware that better performing models exist for English to Setswana\ntranslation (Abbott & Martinus, 2018; Ronald & Barnard, 2007), as best as we can tell those models\nare either trained on closed data sets or on a different version of the Autshumato data set.\n2\nPublished as a conference paper at ICLR 2020\nOurs Martinus & Abbott (2019)\nSetswana 30.49 28.07\nSepedi (Northern Sotho) 17.67 24.16\nAfrikaans 72.43 35.26\nTable 2: Test BLEU scores for moderate depth networks compared to previous baselines. Networks\nof moderate depth (6 layers in total, marked as “Ours”) outperform previous baselines for Enlish to\nSetswana and Afrikaans translation.\nNotably, our English to Afrikaans translation model more than doubles the previous BLEU baseline\nand seems to also outperform Statistical Machine Translation (SMT) (van Niekerk, 2014). 2\nThe BLEU metric with surface-based n-gram scoring might not be expressive enough for aggluti-\nnative languages like Sepedi and Setswana. Therefore we also include example model outputs in\nAppendix A for qualitative comparison.\nDespite our networks of moderate depth outperforming previous Setswana and Afrikaans baselines,\nour Sepedi model performs signiﬁcantly worse than the previous baseline. It should be noted that\nwe were unable to reproduce the baseline score obtained in Martinus & Abbott (2019). Even so, our\nattempts to reproduce their result yielded models with very similar performance to our network of\nmoderate depth with the two approaches usually being within approximately 0.5 test BLEU of each\nother.\n3 D ISCUSSION\nOur exploration into networks of moderate depth was largely due to preliminary signal propagation\nanalyses we performed on simpliﬁed transformer layers. However, we do not present these prelimi-\nnary theoretical results here (left to be explored further in future work), but instead refer the reader to\nvery recent and concurrent work done by Bachlechner et al. (2020) for a more complete motivation\nas well as a proposed solution.\nEven though we do not achieve state-of-the-art results on all languages, we come very close, with\napproximately half the number of parameters and far less training time. We believe there is still\nsome room for improving the performance of our moderate-depth models by more carefully tuning\ntheir hyper-parameters. However, we note that (1) ﬁnding stable learning rates can be very computa-\ntionally expensive (and therefore be beyond what the community might currently be able to afford),\nand (2), in doing so, our work may become less comparable to those that have come before.\nREFERENCES\nJade Z Abbott and Laura Martinus. Towards neural machine translation for african languages.\nNeurIPS Workshop on Machine Learning for the Developing World, 2018.\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and\nJulian McAuley. Rezero is all you need: Fast convergence at large depth.ArXiV, abs/2003.04887,\n2020.\nAngela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\nstructured dropout. ArXiv, abs/1909.11556, 2019.\nJ. Hendrik Groenewald and Liza du Plooy. Processing parallel text corpora for three south african\nlanguage pairs in the autshumato project. In Proceedings of the Second Workshop on African\nLanguage Technology, Valletta, Malta, 2010.\nLaura Martinus and Jade Z Abbott. A focus on neural machine translation for african languages.\nArXiv, abs/1906.05685, 2019.\n2However, the data set used by van Niekerk (2014) is not publicly available for a direct comparison.\n3\nPublished as a conference paper at ICLR 2020\nKenton Murray, Jeffery Kinnison, Toan Q Nguyen, Walter Scheirer, and David Chiang. Auto-sizing\nthe transformer network: Improving speed, efﬁciency, and performance for low-resource machine\ntranslation. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pp. 231–\n240, 2019.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. Facebook fair’s\nwmt19 news translation task submission, 2019.\nToan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of\nself-attention. International Workshop on Spoken Language Translation, 2019.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for\nComputational Linguistics (ACL), Philadelphia, PA, USA, 2002.\nMartin Popel and Ond ˇrej Bojar. Training tips for the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110(1):43–70, 2018.\nArnu Pretorius, Elan Van Biljon, Steve Kroon, and Herman Kamper. Critical initialisation for deep\nsignal propagation in noisy rectiﬁer neural networks, 2018.\nKato Ronald and Etienne Barnard. Statistical translation with scarce resources: a south african case\nstudy. 98, 12 2007.\nSamuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\npropagation. ArXiv, abs/1611.01232, 2016.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: A simple way to prevent neural networks from overﬁtting.Journal of Machine Learning\nResearch, 15(1):1929–1958, 2014.\nDaniel R. van Niekerk. Exploring unsupervised word segmentation for machine translation in the\nsouth african context. In Proceedings of Pattern Recognition Association of South Africa , Cape\nTown, South Africa, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), Long Beach, CA, USA, 2017.\n4\nPublished as a conference paper at ICLR 2020\nA A PPENDIX : QUALITATIVE RESULTS\nLanguage Text\nEnglish Chapter 1 : Purpose of meetings\nSetswana Kgaolo 1 : Maikaelelo a dikopano\nEnglish Mono-cropping is the agricultural practice of growing one single crop year\nafter year .\nSetswana Go jala korong ke tiragalo ya bolemirui ya go jwala dijwalwa tse di jwalwang\nngwaga le ngwaga morago ga ngwaga .\nEnglish Some examples of these are round worms , lung worms , tape worm and liver\nﬂuke .\nSetswana Dikao dingwe tsa dibokwana tse ke dibokwana , dibokwana tsa makgwafo ,\ndibokwana le seedi sa leswe .\nTable 3: Example English to Setswana translations produced by the ﬁnal model.\nLanguage Text\nEnglish These deaths set in motion escalating conﬂicts between residents and anyone\nseemed to be working with the state.\nSepedi Dipoloko te di beilwego ka go dithulano ta go thibela magareng ga badudi le\nmang le mang di swanete go oma le mmuo.\nEnglish Past developments and activities externalised many real costs, displacing them\nonto environments and people as negative environmental and health and safety\nimpacts.\nSepedi Ditlhabollo ta ast le ditiro te di lego gona ta go ba le koketo ye menti, di di bea\ntikologo le batho ba tikologo le go hlokomologa maphelo le polokego.\nEnglish The aim of the operations is to address the contact crimes and housebreakings\nthat escalated recently in the area.\nSepedi Maikemieto a tshepedio a tshepedio ke go rarolla bosenyi le go thuba ka dint-\nlong teo di sa twago go di sa ongwa.\nTable 4: Example English to Sepedi (Northern Sotho) translations produced by the ﬁnal model.\n5\nPublished as a conference paper at ICLR 2020\nLanguage Text\nEnglish if the number of persons nominated exceeds the number of vacancies to be\nﬁlled , the president , after consulting the relevant profession , must appoint\nsufﬁcient of the nominees to ﬁll the vacancies , taking into account the need to\nensure that those appointed represent the profession as a whole .\nAfrikaans indien die getal persone wat benoem is meer is as die getal vakatures wat gevul\nmoet word, moet die president die betrokke professie afsonderlik moet pleeg\nword, genoeg van die benoemdes aanstel om die vakatures te vul, met inag-\nneming van die behoefte om te verseker dat diegene wat aangestel word die\nprofessie as geheel verteenwoordig.\nEnglish If CK documents were lodged separately, it will take three weeks to complete\nthe process.\nAfrikaans Indien die BK dokumente afgehandel is, sal dit drie weke duur om die proses\nte voltooi.\nEnglish STELLENBOSCH LOCAL MUNICIPALITY - SERVICES: TRAFFIC, AC-\nCIDENTS & ROAD SAFETY\nAfrikaans STELLENBOSCH PLAASLIKE MUNISIPALITEIT - DIENSTE: VER-\nKEER, ONGELUKKE EN PADVEILIGHEID\nTable 5: Example English to Afrikaans translations produced by the ﬁnal model.\n6",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7984497547149658
    },
    {
      "name": "Transformer",
      "score": 0.772437334060669
    },
    {
      "name": "Computer science",
      "score": 0.7285652160644531
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4298960566520691
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.42216330766677856
    },
    {
      "name": "Language model",
      "score": 0.41163554787635803
    },
    {
      "name": "Machine learning",
      "score": 0.3756771981716156
    },
    {
      "name": "Engineering",
      "score": 0.13166552782058716
    },
    {
      "name": "Electrical engineering",
      "score": 0.11065581440925598
    },
    {
      "name": "Voltage",
      "score": 0.09354761242866516
    },
    {
      "name": "Computer network",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    }
  ]
}