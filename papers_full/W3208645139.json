{
  "title": "Power Consumption Predicting and Anomaly Detection Based on Transformer and K-Means",
  "url": "https://openalex.org/W3208645139",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2116541567",
      "name": "Junfeng Zhang",
      "affiliations": [
        "Hebei University"
      ]
    },
    {
      "id": "https://openalex.org/A1992769770",
      "name": "Hui Zhang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2115283266",
      "name": "Song Ding",
      "affiliations": [
        "Zhejiang University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2106292794",
      "name": "Xiaoxiong Zhang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2116541567",
      "name": "Junfeng Zhang",
      "affiliations": [
        "Hebei University",
        "Baoding University"
      ]
    },
    {
      "id": "https://openalex.org/A1992769770",
      "name": "Hui Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115283266",
      "name": "Song Ding",
      "affiliations": [
        "Zhejiang University of Finance and Economics"
      ]
    },
    {
      "id": "https://openalex.org/A2106292794",
      "name": "Xiaoxiong Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2328108642",
    "https://openalex.org/W6787071289",
    "https://openalex.org/W4238059783",
    "https://openalex.org/W2511280408",
    "https://openalex.org/W2122646361",
    "https://openalex.org/W2278552615",
    "https://openalex.org/W4253041321",
    "https://openalex.org/W2092315180",
    "https://openalex.org/W6681513673",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2086694651",
    "https://openalex.org/W2804841412",
    "https://openalex.org/W2737654582",
    "https://openalex.org/W6720132045",
    "https://openalex.org/W2083022762",
    "https://openalex.org/W2480645619",
    "https://openalex.org/W6720514713",
    "https://openalex.org/W6632876648",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W3103064492",
    "https://openalex.org/W2910014246",
    "https://openalex.org/W2025291942",
    "https://openalex.org/W3047304847",
    "https://openalex.org/W2597866042",
    "https://openalex.org/W6757609365",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6763352501",
    "https://openalex.org/W6769557084",
    "https://openalex.org/W2080398808",
    "https://openalex.org/W2275543810",
    "https://openalex.org/W4297814361",
    "https://openalex.org/W1619735662",
    "https://openalex.org/W2947210406",
    "https://openalex.org/W2909877301",
    "https://openalex.org/W2469098733",
    "https://openalex.org/W1550953152",
    "https://openalex.org/W3112092637",
    "https://openalex.org/W2774596248",
    "https://openalex.org/W2295240344",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W2069508080",
    "https://openalex.org/W4376599304",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "With the advancement of technology and science, the power system is getting more intelligent and flexible, and the way people use electric energy in their daily lives is changing. Monitoring the condition of electrical energy loads, particularly in the early detection of aberrant loads and behaviors, is critical for power grid maintenance and power theft detection. In this paper, we combine the widely used deep learning model Transformer with the clustering approach K-means to estimate power consumption over time and detect anomalies. The Transformer model is used to forecast the following hour’s power usage, and the K-means clustering method is utilized to optimize the prediction results, finally, the anomalies is detected by comparing the predicted value and the test value. On real hourly electric energy consumption data, we test the proposed model, and the results show that our method outperforms the most commonly used LSTM time series model.",
  "full_text": "Power Consumption Predicting and\nAnomaly Detection Based on\nTransformer and K-Means\nJunfeng Zhang1, Hui Zhang2, Song Ding3* and Xiaoxiong Zhang2\n1Data Mining Laboratory, College of Mathematics and Information Technology, Hebei University, Baoding, China,2The Sixty-\nThird Research Institute, National University of Defense Technology, Nanjing, China,3School of Economics, Zhejiang University of\nFinance and Economics, Hangzhou, China\nWith the advancement of technology and science, the power system is getting more\nintelligent andﬂexible, and the way people use electric energy in their daily lives is changing.\nMonitoring the condition of electrical energy loads, particularly in the early detection of\naberrant loads and behaviors, is critical for power grid maintenance and power theft\ndetection. In this paper, we combine the widely used deep learning model Transformer\nwith the clustering approach K-means to estimate power consumption over time and\ndetect anomalies. The Transformer model is used to forecast the following hour’s power\nusage, and the K-means clustering method is utilized to optimize the prediction results,\nﬁnally, the anomalies is detected by comparing the predicted value and the test value. On\nreal hourly electric energy consumption data, we test the proposed model, and the results\nshow that our method outperforms the most commonly used LSTM time series model.\nKeywords: power consumption prediction, anomaly detection, transformer, K-means, LSTM\n1 INTRODUCTION\nModern power systems are evolving in a more sustainable path. The load demand for domestic electrical\nenergy is gradually increasing as the number of householda p p l i a n c e sa n de l e c t r i cc a rs increases. Statistics\nshow that residences and commercial buildings account for three-ﬁfths of global electricity use (Desai,\n2017). The power system has grown in complexity and intelligence, and more modern information\ntransmission technology has been implemented, making grid processing more convenient and secure\n(Bayindir et al., 2016). Moreover, electric energy consumption in everyday living is also difﬁcult and variable.\nElectric energy usage, for example, may vary signiﬁcantly depending on the season, and consumption on\nworking days and working days willﬂu c t u a t e .A tt h es a m et i m e ,t h e r ew i l lb ea n o m a l i e si nt h ee l e c t r i c a ll o a d ,\nsuch as forgetting to turn off electrical appliances, failure of electrical appliances and even the theft of\nelectricity, and so on, resulting in a much larger electrical demand than typical. As a result, detecting unusual\nconsumption data is critical.\nAbnormal detection can enhance abnormal electric energy consumption to achieve energy\nsavings, remind users to discover malfunctioning electrical appliances or modify bad electricity\nusage patterns, lower users’ energy consumption expenses, and promote electricity consumption\nsafety awareness. The most crucial factor is that you can locate the source of the power theft\n(McLaughlin et al., 2009). According to the survey, power theft accounts for about half of the energy\nlost in some developing countries ( Antmann,2009), and anomaly detection technologies can\nsuccessfully combat this scenario.\nAnomaly detection, as the name suggests, is the method of recognizing data that differs from the\nusual. Anomalies in data are situations that do not follow the speciﬁed usual behavior pattern\nEdited by:\nTinghui Ouyang,\nNational Institute of Advanced\nIndustrial Science and Technology\n(AIST), Japan\nReviewed by:\nShinan Zhao,\nJiangsu University of Science and\nTechnology, China\nDazhong Ma,\nNortheastern University, China\nChao Li,\nKU Leuven, Belgium\n*Correspondence:\nSong Ding\ndingsong1129@163.com\nSpecialty section:\nThis article was submitted to\nSmart Grids,\na section of the journal\nFrontiers in Energy Research\nReceived: 19 September 2021\nAccepted: 06 October 2021\nPublished: 22 October 2021\nCitation:\nZhang J, Zhang H, Ding S and Zhang X\n(2021) Power Consumption Predicting\nand Anomaly Detection Based on\nTransformer and K-Means.\nFront. Energy Res. 9:779587.\ndoi: 10.3389/fenrg.2021.779587\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795871\nORIGINAL RESEARCH\npublished: 22 October 2021\ndoi: 10.3389/fenrg.2021.779587\n(Chandola et al., 2009). Anomalies are classiﬁed into three types:\npoint anomalies, aggregate anomalies, and context anomalies. A\npoint anomaly occurs when one point in the data is excessively\nhigh or too low in comparison to other points. The anomalous\nphenomena of a group of data compared to the full data set is\nreferred to as a collection anomaly, and it only happens in the\ndata set with the correlation between the data. Contextual\nabnormality refers to the abnormality when the data is\ncombined with the context in the data set (Chandola et al.,\n2009). In this paper, abnormal power consumption means that\nif the difference between the power consumption predicted by the\nmodel and the real power consumption in a certain hour is\ngreater than the threshold we set through the experiment, the\ncurrent hour power consumption is considered abnormal, so the\nmain task of this paper is to detect point anomalies.\nBecause the characteristics of variables are various, traditional\nmodels primarily focus on univariate prediction and anomaly\ndetection (Hu et al., 2018 ). Univariate models are typically\nutilized in cases where there are too many other features or\nwhen vectorization is difﬁcult, such as stock prediction (Hsieh\net al., 2011). Various industries, such as speech recognition (Graves\net al., 2013) and NLP (Natural Language Processing) (Nadkarni\net al., 2011), have adopted deep learning technology and achieved\nremarkable success as a result of the rapid development of theﬁeld\nof deep learning. Time series analysis (Kuremoto et al., 2014), of\ncourse, has also a signiﬁcant advancement. Traditional statistical\nmethods such as ARIMA (Auto-Regressive Integrated Moving\nAverage) (Yuan et al., 2016) and SARIMA (Seasonal ARIMA)\n(Ahn et al., 2015) were defeated by the proposed LSTM (Long and\nShort-Term Memory network) ( Hochreiter and\nSchmidhuber,1997). For energy consumption prediction and\nanomaly detection, a lot of work on LSTM has been done.\nHowever, with the introduction of Google’s Transformer model\n(Vaswani et al., 2017), this model wasﬁrst successfully used to the\nﬁeld of machine translation, and then it spread to other signiﬁcant\nﬁelds such as speech recognition (Wang et al., 2020), and so on. Since\nmachine translation technology involves the processing of time\nseries, we seem to be able to use the Transformer model for time\nseries forecasting tasks. Transformer uses self-attention and multi-\nhead self-attention for semantic extraction. When it comes to the\nlong-distance dependence of features in time series, self-attention\ncan naturally solve this problem, because there are connections\nbetween all features of time series when integrating features, and the\nrelative position information between the input time series features is\npreserved through sinusoidal position encoding. It is not like RNN\n(Recurrent Neural Network) that needs to be gradually passed to the\nback through hidden layer node sequences, nor is it like CNN\n(Convolutional Neural Networks) that needs to be captured long\ndistance features by increasing the network depth, Transformer has\nsome advantages in processing time series features.\nAs a result, in this paper, we propose a new power\nconsumption prediction and anomaly detection model that\ncombines deep learning and clustering methods. The following\nare the contributions:\n1) For time series prediction of power consumption, we merged\nthe current popular Transformer deep learning model with\nK-means clustering. We reasoned that the historical time data\ncontributes differentially to the expected value due to the\nregular behavior of household users. The K-means method\ncan be used to locate data clusters that contribute more to the\nprojected value, allowing the Transformer model prediction\nvalue to be optimized further.\n2) In the experiment, we employed multi-dimensional data. The\ndata dimension also incorporates auxiliary information data\nsuch as voltage, current, and the power consumption of\nvarious household appliances, in addition to the\nfundamental power consumption.\n3) We compared the proposed method to the LSTM and only\nTransformer model’s prediction performance. Experiments\nhave revealed that the proposed combination method’s error\nbetween predicted and true values is lower than those of\nLSTM and single Transformer.\n4) To demonstrate the proposed method’s superior performance\nin anomaly detection, we manually added anomalous data\ninto the test data and treated it as a true anomaly.\nThe following is how we organize this paper. We introduce\nrelevant research on power consumption and anomaly detection\nin Section 2. The data set used in the experiment, as well as the\ndata set’\ns preparation approach, are shown in Section 3.W e\ndescribe our model’s implementation approach and procedure in\ndetail in Section 4. We compare the performance of model\nprediction and anomaly detection with different models in\nSection 5. This paper was summarized in the last section.\n2 RELATED WORKS\nResearchers have done a lot of research since power consumption\nprediction and anomaly detection are so crucial in the power\nenergy system.Box et al. (2015)developed time series forecasting\napproaches like as Auto-Regressive (AR), ARIMA (Auto-\nRegressive Integrated Moving Average), and SARIMA\n(Seasonal ARIMA) in the economic sphere, and they had good\nresults. To predict the value at a speciﬁc moment in the time\nseries, the AR model primarily uses the weighting of all values\npreceding that time. ARIMA primarily employs the point before\nto the time to add a random vector in order to forecast the value at\nthat time. SARIMA is mostly used for time series data with\nobvious seasonal differences.Ouyang et al. (2019b)improved the\nperformance of wind power ramp prediction by combining the\nadvantages of AR models and Markov chain. To detect anomalies,\nYan et al. (2014)integrated the AR approach with SVM (Support\nVector Machine) Ma and Guo (2014). ARIMA was used by\nEdiger and Akar (2007)to forecast fuel energy use in Turkey.\nThe time series of ARIMA power consumption was utilized by\nAlberg and Last (2018)to estimate future power consumption,\nand Krishna et al. (2015) employed ARIMA for half-hour\ngranular power consumption data. SARIMA was applied by\nAhn et al. (2015) for long-term and mid-term load\nforecasting. The unsupervised K-means approach Münz et al.\n(2007) groups the data to identify anomalies that are outside of\nthe cluster. Simultaneously, the autoencoder model has been a\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795872\nZhang et al. Power Predicting and Anomaly Detection\nhuge success. The data is analyzed using unsupervised methods.\nThe difference between input and output is utilized to detect\nwhether the data is aberrant, from compression and abstraction\nto recovery and rebuilding. For anomaly detection,Al-Abassi\net al. (2020) presented unsupervised stacked autoencoders for\nsmart cyber-physical grids. Deb et al. (2015) developed an\nartiﬁcial neural network for predicting building energy usage\nin Singapore, and it was shown to be accurate.\nThe advancement of deep learning has improved the accuracy\nand performance of large data processing and prediction. Deep\nlearning was used extensively in wind speed prediction (Khodayar\net al., 2017), stock prediction (Rather et al., 2015), automated\nVehicles (Shen et al., 2020) and other researches, and power grid\ntechnology has also incorporated the nerve of deep learning. The\nnetwork is used to forecast and detect how much energy the user\nconsumes. Ouyang et al. (2019a)proposed the use of Deep Belief\nNetwork (DBN) to predict hourly power load. According toShi et al.\n(2017), predicting the electricity usage of a single customer in Ireland\nis the same as using a deep recursive network. For time series, LSTM\ncan forecast and detect anomalies (Malhotra et al., 2016; Siami-\nNamini et al., 2018). Wang et al. (2019) proposed combining\nseasonal features with LSTM for power load forecasting and\nanomaly detection. However, because ARIMA requires time series\ndata to be stationary and it can only capture linear relationships, but\nnot non-linear relationships. For the LSTM model, its output at the\ncurrent time requires not only the input at the current time, but also\nthe output at the previous time. This makes the LSTM model unable\nto parallelize operations, resulting in too long training time when\nprocessing time series features. On the other hand, the Transformer\nmodel has had a lot of success in theﬁeld of speech recognition and\nnatural language processing since it was introduced. As a result, we\npropose in this paper that we utilize the transformer model to\nestimate electric energy load, then apply the k-means approach to\nfurther improve the prediction results, and then compare the\nprediction results to the test data to look for anomalies.\n3 DATASETS\nFor the experiment, we used hour-level electricity load data from a\nFrench family for 1,440 days (2006-12-17 to 2010-11-25). We selected\n3 h of data for display, as shown in Table 1,e x c e p tf o r\n“global_active_power” represents the total active power consumed\nby the household, and other data includes“global_reactive_power”\nrepresenting the total reactivepower consumed by the household,\n“voltage” representing the average voltage per hour,“global_intensity”\nrepresenting the average current intensity, “sub_metering_1”\nrepresenting the active elect rical energy of the kitchen,\n“sub_metering_2” representing the active energy of the laundry,\n“sub_metering_3” representing the active energy of the climate\ncontrol system,“sub_metering_4” representing other active energy.\nT h eh o u r l yp o w e rl o a dc h a n g ed iagram for 3 days which are all\nweekdays is shown in Figure 1. It can be seen that power\nconsumption has increased signiﬁcantly in the morning, noon and\nevening, and electricity consumption conforms to the law of electricity\nconsumption in French households during workdays. To make the\ndata more stable, we apply the Min-Max Normalization procedure.\nThis will make the model’s training easier and its convergence faster.\nWe designed the model supervision task to estimate the following\nhour’s electric energy usage based on multivariate data collected every\n23 h, and we implemented it by using a 23-hour sliding window.\n4 METHODOLOGY\nWe partitioned the data into 24-hour groups using a sliding\nwindow, then trained k-means clustering for theﬁrst 23 h of each\ngroup of real test data into k clusters, while also used the 23-hour\nreal load data training Transformer model predicts the next\nhour’s load data, then through the trained K-means to get the\nappropriate centroid as the ﬁnal prediction result. Figure 2\ndepicts the framework of our model.\n4.1 Transformer Model\nInitial and foremost, Positional Encoding is theﬁrst phase in the\nTransformer model utilized in this essay. Because Transformer\ndoes not have a cyclic structure like LSTM, it presents a new\npositional encoding strategy to capture the input time series\ninformation, as indicated inEqs 1, 2. The basic idea is to add\nsine and cosine functions of various frequencies as position codes\nto the normalized input sequence, allowing the Transformer\nmodel’s multi-head attention mechanism to fully capture time\nseries data with more dimensions.\nPE\n(pos,2i) /equals sin(pos/100002i/dmodel ) (1)\nPE(pos,2i+1) /equals cos(pos/100002i/dmodel ),i ∈ [0, ... ,d model/2) (2)\nwhere pos is the vector position of each time. For example, in the\ntime series data in this paper, thepos of the ﬁrst hour of each\ngroup is 0, and thepos of the second hour is 1, 2i and 2i +1\nrespectively represent the even position and the odd position.\nTABLE 1 |Sample display of the data set used in this paper.\nDatetime Global_active_power (kW ) Global_reactive_power (kW) Voltage (V) Global_intensity (A)\n2006-12-17 00:00:00 112.95 6.14 240.96 487.60\n2006-12-17 01:00:00 200.96 8.22 240.45 854.80\n2006-12-17 02:00:00 95.24 4.69 245.82 412.20\nSub_metering_1 (Wh) Sub_metering_2 (Wh) Sub_metering_3 (W)h Sub_metering_4 (Wh)\n0.0 28.0 0.0 1854.47\n0.0 1,514.0 0.0 1835.40\n0.0 34.0 0.0 1,553.27\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795873\nZhang et al. Power Predicting and Anomaly Detection\ndmodel represents the length of the feature vector per hour. Next,\nwe use X /equals [x1, x2, ... , xn] to represent the input sequence\ncombined with position encoding, and pass the multi-head self-\nattention of the Transformer model:\nMultiHead(Q, K, V)/equals Concat(head\n1, ... , headi)Wo (3)\nheadi /equals Attention(QWQ\ni ,K WK\ni\n,V WV\ni\n) (4)\nIn the above formula,Q /equals q1wq1 ,K /equals k1wk1 ,V /equals v1wv1 , q1 /equals\nk1 /equals v1 /equals X. In the Transformer model, the Attention moduleﬁrst\nundergoes a linear transformation of Q (Query), K (Key), and V\n(Value). Each time Q, K, and V perform the linear\ntransformation, the parameter W is different, and then input\nto Scaling dot product attention, the formula is as (5), note that it\nis necessary to doi times here, in fact, it is the so-called multi-\nhead, and each time counts as one head. Then concatenate the\nattention results of thei times of scaling dot product, and then\nperform a linear transformation to obtain the value as the result of\nthe multi-head attention. The advantage of this is that it allows\nthe model to learn relevant information in different\nrepresentation subspaces. The calculation of the Attention\nmodule uses Scaled Dot-product:\nFIGURE 1 |Hourly household electric energy consumption change diagram for 3 consecutive days.\nFIGURE 2 |The main framework of our model.\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795874\nZhang et al. Power Predicting and Anomaly Detection\nAttention(Q, K, V)/equals softmax QKT\n/radicaltpext/radicaltpext\ndk\n√() V (5)\nwhere dk is the last dimension of the shape ofQ, K, V, divided by/radicaltpext/radicaltpext\ndk\n√\nto preventdk from being too large and the softmax function’s\ngradient becoming too tiny whenQKT is too large. The residual\nconnection structure is then used to narrow the network ’s\nattention to solely the differences. In multi-layer network\nstructures, it is frequently used:\nL /equals LayerNorm(X + MultiHead(Q, K, V)) (6)\nwhere LayerNorm is Layer Normalization, which normalizes\neach neuron and adjusts the mean and variance of the input\ndata to be the same, which will speed up the convergence.\nThen input L into the FeedForward layer, which is composed\nof two fully connected layers, the ﬁrst layer uses the Relu\nactivation function, and the second layer does not use the\nactivation function:\ns /equals max(0,L W\n1 + b1)W2 + b2 (7)\nSimilarly, use residual connection and Layer Normalization\nagain to get the outputSE:\nSE /equals LayerNorm(L + s) (8)\nOur experiment uses a 2-layer Transformer multi-head\nattention module, which means that the outputSE needs to be\nre-input to the structure outputSE2 described above. Finally, the\noutput will be decoded and dimensionality reduction operations\nthrough the fully connected layer:\nK /equals W3SE2 (9)\n4.2 K-Means Clustering Method\nClustering is the division of a sample set into several disjoint\nsubsets (sample clusters), which is a typical unsupervised\nmachine learning algorithm. When using clustering to\nclassify samples, Euclidean Distance is used as the\nmeasurement criterion of sample similarity. The higher the\nsimilarity, the smaller the Euclidean Distance of the sample.\nK-means clustering is a well-known algorithm among clustering\na l g o r i t h m s .I tn e e d st od e t e r m i n et h en u m b e ro fc l u s t e r skﬁrst\nwhen clustering, and k is given bythe user. Each cluster passes\nthrough its centroid (the mean value of all elements in the\ncluster). The workﬂo wo fk - m e a n si sa l s ov e r ys i m p l e .F i r s t ,\nrandomly select k initial points as the initial centroids of each\ncluster, and then assign each point in the data set to the cluster\nclosest to it. The distance cal culation uses the Euclidean\nDistance mentioned above. The algorithm of k-means is\nshown in Algorithm 1:\nAlgorithm 1. K-means algorithm.\n4.3 Model Development\nFirst, we divide the data set into a training set and a test set. Since\nthe data set contains a total of 1,440 days of hourly data, we\nchoose 1,240 days of data as the training set and 200 days of data\nas the test set. For the Transformer model, we choose two\nconsecutive layers of multi-head self-attention modules, and\neach multi-head attention is set to 4 attention heads. The\ninput data shape is 23 time steps and 8 features. For K-means\nclustering, we experimented with k /equals 2, 5, 8, 10, 11, and 15\nrespectively, and ﬁnally selected the cluster with k /equals 10. We\nchoose the mean-square error of the predicted data and the\noriginal data, that is, MSE (Mean-Square Error) as the loss\nfunction, and Adam as the optimizer of the model. And set\nthe epoch of the training model to 300, and the batch size to 200.\n4.4 Model Prediction and Evaluation\nWe believed that the nearest neighbor of the real training data has the\nmost impact on the forecast value, thus in theﬁrst 23 h of each group,\nwe trained k-means clustering, partitioned the data into k categories,\na n dp r o v i d e dt h el o a dKp r e d i c t e dby the Transformer model for the\nnext hour as theﬁnal prediction output, then found the centroid in the\ntrained K-means cluster. We analyzed the model’s prediction ability\nby ﬁtting the predicted value to the test value and calculating the\nRMSE (Root Mean Squard Error) of the forecasted value and the test\nvalue to measure the prediction’s accuracy, and we compared it to the\ncommonly used LSTM model.\n4.5 Anomaly Detection\nBecause the model calculates theanticipated value based on a huge\nquantity of historical data, the forecasted values will generally follow\nthe data’s trend. If the test value differs signiﬁcantly from the projected\nvalue, it indicates that the test value has deviated too far from the data\nt r e n da n dm a yb ea b n o r m a l .T h es c o r eb e t w e e nt h ep r e d i c t e dv a l u e\nFIGURE 3 |Train and test loss over the 300 epochs of our model.\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795875\nZhang et al. Power Predicting and Anomaly Detection\nand the test value obtained after the Transformer model and k-means\nclustering is calculated in this research. The formula can be found in\nEq. 10. In order to better analyze the difference, we normalized the\nscore, the formula is asEq. 11. The value of score collected from\nseveral experiments was used to determine a threshold. When the\nscore between the predicted value and the test value exceeds the\nthreshold, the test value is considered abnormal. The experiment can\nalso evaluate whether the user is pro n et oh a v i n ge l e c t r i c i t yt h e f tb y\nsetting a time series window and a threshold for the number of\nanomalies. If the number of abnormalities in the time series data in a\nw i n d o wi sg r e a t e rt h a nt h et h r e s h o l d ,i tm e a n st h a tt h et i m es e r i e sd a t a\nare abnormal, and the user may have the suspicion of steal electricity.\nTo better compare the accuracy of anomaly detection, we manually\ninsert abnormal data points in the test data and compare our model\nwith K-means and LSTM.\nscore\nt /equals |predictedt − testt|\navgi∈T(|predictedi − testi|) (10)\n~scoret /equals scoret − min(score)\nmax(score)− min(score) (11)\n5 EXPERIMENT RESULTS\n5.1 Consumption Prediction\nWe opted to compare the model against the most popular LSTM\nmodel for time series data prediction in order to test its\nperformance. The LSTM is a variant of the recurrent neural\nnetwork RNN. It is a unique RNN that incorporates three\ndifferent types of gating to address the problem of gradient\ndisappearance and explosion during lengthy sequence training.\nSimply put, LSTM outperforms standard RNNs in longer\nsequences, making it ideal for time series forecasting jobs.\nFigure 3shows how the training and test loss of the Transform\nmodel used in this paper changes at 300 epoch. It can be observed\nthat the model converges quickly, and theﬁgure shows that there\nis no overﬁtting in the model. All of this is achievable because of\nthe Transformer model’s beneﬁts in time series processing.\nFIGURE 4 |Comparison of our model and LSTM predicted valueﬁtting real data.\nFIGURE 5 |Scores and anomalies exceeding the threshold for 2,500 h.\nTABLE 2 |Comparison with some methods.\nMethod Accurary Precision Recall F1 RMSE(prediction)\nK-means 0.96 0.82 0.28 0.42 —\nLSTM 0.97 0.74 0.60 0.66 0.91\nOur method 0.97 0.80 0.66 0.72 0.74\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795876\nZhang et al. Power Predicting and Anomaly Detection\nFigure 4 depicts the test set’s real-time power consumption\ndata over 3 days, as well as a comparison of our model’sa n dt h e\nLSTM model’s prediction results on the test set. The blue line\nrepresents the actual data, the red line represents our model’s\npredicted value, the green line represents the LSTM model’s\npredicted value, and the purple line indicates the lone\nTransformer’s predicted value. Our model ’s forecast data is\nmore in accordance with the real test data, as can be shown.\nT h eR M S Eo ft h em o d e l ,o nt h eo t h e rh a n d ,a r eu s e dt oa s s e s s\nthe model ’s ﬁt. Our model has an RMSE of 0.73, the\nTransformer has an RMSE of 0.77, and the LSTM has an\nRMSE of 0.86. In terms of prediction accuracy, our model\noutperforms LSTM by 15%, while Transformer outperforms\nLSTM by just 10%. After our analysis, because the\ndimensionality of the feature vector of each time series in\nour time series data is too small, which leads to the failure of\nthe full performance of the Transformer model.\n5.2 Anomaly Detection\nAfter a lot of testing and tweaking, we ultimately settled on\n0.45 as the threshold. This means that any point with a score\nhigher than 0.45 will be considered anomalous. The change in\nscore data over 2,500 h is presented inFigure 5, with the red\ndashed line representing the threshold and the purple point\nrepresenting the abnormal point. The data scores are primarily\nfocused between 0 and 0.3, and there are relatively few aberrant\nspots, as can be shown. In practical applications, we can adjust\nthe threshold size based on the scene being used, and lower or\nincrease the threshold size based on the strictness of anomaly\ndetection, a lower threshold is more stringent, allowing for the\ndetection of more anomalies, on the other hand, a higher\nthreshold is more tolerant, allowing for the detection of fewer\nanomalies.\nWe utilized the strategy of randomly inserting abnormal\npoints in the test data to better compare and assess the\nmodel’s anomaly detection capabilities because this\nexperimental data set does not mark aberrant time points. In\nthe 200 days (4,800 h) of the test set, we randomly selected a value\nevery day and double it, and assume it is an outlier, so there are\n200 outliers in the 4,800 data in the test set. For comparison, we\nseparately used the clustering method K-means and the most\npopular depth method LSTM to detect abnormal points. Using\nthe K-means approach, we discovered a total of 68 abnormal\npoints, of which only 56 were the abnormal points we manually\ninserted into the data set. Using the LSTM model, we retrieved a\ntotal of 162 anomalies, 120 of which were the anomaly points we\nmanually inserted into the data set. Our combined Transformer\nand K-means model found 165 anomalies, 132 of which were the\nabnormal points we manually added to the data set. The accuracy,\nprecision, recall, and F1 of the three models are shown inTable 2.\nThe predicted RMSE of LSTM and our model are also shown in\nthe table.\n6 CONCLUSION\nThe prediction of electric energy consumption and the identiﬁcation\nof anomalies are critical in the functioning of the power grid, and the\nprocessing of multi-variable time series is a difﬁcult challenge. We\npresent a model that combines Transformer and K-means\napproaches in this article. Every 23 h of training data is separated\ninto k clusters using K-means clustering. At the same time, this\ntraining data are used to train the Transformer model to predict the\nfollowing hour’s power usage, with the predicted value being placed\ninto the trained K-means cluster and the cluster’s centroid serving as\nthe ﬁnal predicted value. Finally, look for anomalies by comparing\nthe anticipated value to the actual test results. The experimental\nresults prove that the model achieves prediction accuracy with less\nerror and high anomaly detection performance. In the future, we’ll\nstrive to improve prediction and anomaly detection accuracy, as well\nas study the differences between power consumption prediction and\nanomaly detection in different seasons, environments, and other\nscenarios, and other issues that need to be addressed.\nDATA AVAILABILITY STATEMENT\nThe datasets presented in this study can be found in online repositories.\nThe names of the repository/repositories and accession\nnumber(s) can be found below: https://archive.ics.uci.edu/ml/\ndatasets/individual+household+electric+power+consumption.\nAUTHOR CONTRIBUTIONS\nJZ: conceptualization, methodology, data preprocessing, and\nwriting-original draft preparation. HZ: visualization,\ninvestigation. SD: experimental training and testing. XZ:\nsupervision and reviewing.\nFUNDING\nThis work was supported in part by the National Natural Science\nFoundation of China under Grants 71901215, 71901191, the\nNational University of Defense Technology Research Project\nZK20-46, and the Outstanding Youth Talents Program of\nNational University of Defense Technology.\nREFERENCES\nAhn, B.-H., Choi, H.-R., and Lee, H.-C . (2015). Regional Long-Term/mid-\nTerm Load Forecasting Using Sarima in south korea. J. Korea Academia-\nIndustrial cooperation Soc. 16, 8576 – 8584. doi:10.5762/\nkais.2015.16.12.8576\nAl-Abassi, A., Sakhnini, J., and Karimipour, H. (2020).“Unsupervised Stacked\nAutoencoders for Anomaly Detection on Smart Cyber-Physical Grids,” in 2020\nIEEE International Conference on Systems, Man, and Cybernetics (SMC)\n(IEEE), 3123– 3129. doi:10.1109/smc42975.2020.9283064\nAlberg, D., and Last, M. (2018). Short-term Load Forecasting in Smart Meters with\nSliding Window-Based Arima Algorithms.Vietnam J. Comput. Sci.5, 241– 249.\ndoi:10.1007/s40595-018-0119-7\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795877\nZhang et al. Power Predicting and Anomaly Detection\nAntmann, P. (2009).Reducing Technical and Non-technical Losses in the Power\nSector.\nBayindir, R., Colak, I., Fulli, G., and Demirtas, K. (2016). Smart Grid Technologies\nand Applications. Renew. Sustain. Energ. Rev. 66, 499– 516. doi:10.1016/\nj.rser.2016.08.002\nBox, G., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M. (20152015).Time Series\nAnalysis: Forecasting and Control.\nChandola, V., Banerjee, A., and Kumar, V. (2009). Anomaly Detection.ACM\nComput. Surv. 41, 1– 58. doi:10.1145/1541880.1541882\nDeb, C., Eang, L. S., Yang, J., and Santamouris, M. (2015). Forecasting Energy\nConsumption of Institutional Buildings in singapore. Proced. Eng. 121,\n1734– 1740. doi:10.1016/j.proeng.2015.09.144\nDesai, B. H. (2017). 14. United Nations Environment Program (Unep).Yearb. Int.\nEnviron. L. 28, 498– 505. doi:10.1093/yiel/yvy072\nEdiger, V.Ş., and Akar, S. (2007). Arima Forecasting of Primary Energy Demand\nby Fuel in turkey.Energy policy35, 1701– 1708. doi:10.1016/j.enpol.2006.05.009\nGraves, A., Mohamed, A.-r., and Hinton, G. (20132013).IEEE International Conference on\nAcoustics, Speech and Signal Processing . IEEE, 6645 – 6649. doi:10.1109/\nicassp.2013.6638947Speech Recognition with Deep Recurrent Neural Networks\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory.Neural\nComput. 9, 1735– 1780. doi:10.1162/neco.1997.9.8.1735\nHsieh, T.-J., Hsiao, H.-F., and Yeh, W.-C. (2011). Forecasting Stock Markets Using\nWavelet Transforms and Recurrent Neural Networks: An Integrated System\nBased on Artiﬁcial Bee colony Algorithm.Appl. soft Comput.11, 2510– 2525.\ndoi:10.1016/j.asoc.2010.09.007\nHu, M., Ji, Z., Yan, K., Guo, Y., Feng, X., Gong, J., et al. (2018). Detecting Anomalies\nin Time Series Data via a Meta-Feature Based Approach. Ieee Access 6,\n27760– 27776. doi:10.1109/access.2018.2840086\nKhodayar, M., Kaynak, O., and Khodayar, M. E. (2017). Rough Deep Neural\nArchitecture for Short-Term Wind Speed Forecasting.IEEE Trans. Ind. Inf.13,\n2770– 2779. doi:10.1109/tii.2017.2730846\nKrishna, V. B., Iyer, R. K., and Sanders, W. H. (2015).“Arima-based Modeling and\nValidation of Consumption Readings in Power Grids,” in International Conference on\nCritical Information Infrastructures Security (Springer), 199– 210.\nKuremoto, T., Kimura, S., Kobayashi, K., and Obayashi, M. (2014). Time Series\nForecasting Using a Deep Belief Network with Restricted Boltzmann Machines.\nNeurocomputing 137, 47– 56. doi:10.1016/j.neucom.2013.03.047\nMa, Y., and Guo, G. (2014). Support Vector Machines Applications, Vol. 649.\nSpringer.\nMalhotra, P., Ramakrishnan, A., Anand, G., Vig, L., Agarwal, P., and Shroff, G.\n(2016). Lstm-based Encoder-Decoder for Multi-Sensor Anomaly Detection.\narXiv preprint arXiv:1607.00148.\nMcLaughlin, S., Podkuiko, D., and McDaniel, P. (2009).“Energy Theft in the\nAdvanced Metering Infrastructure,” in International Workshop on Critical\nInformation Infrastructures Security (Springer), 176– 187.\nMünz, G., Li, S., and Carle, G. (2007).“Trafﬁc Anomaly Detection Using K-Means\nClustering,” in GI/ITG Workshop MMBnet, 13– 14.\nNadkarni, P. M., Ohno-Machado, L., and Chapman, W. W. (2011). Natural\nLanguage Processing: an Introduction. J. Am. Med. Inform. Assoc. 18,\n544– 551. doi:10.1136/amiajnl-2011-000464\nOuyang, T., He, Y., Li, H., Sun, Z., and Baek, S. (2019a). Modeling and Forecasting\nShort-Term Power Load with Copula Model and Deep Belief Network.IEEE\nTrans. Emerg. Top. Comput. Intell.3, 127– 136. doi:10.1109/tetci.2018.2880511\nOuyang, T., Zha, X., Qin, L., He, Y., and Tang, Z. (2019b). Prediction of Wind\nPower Ramp Events Based on Residual Correction.Renew. Energ.136, 781– 792.\ndoi:10.1016/j.renene.2019.01.049\nRather, A. M., Agarwal, A., and Sastry, V. N. (2015). Recurrent Neural Network\nand a Hybrid Model for Prediction of Stock Returns.Expert Syst. Appl. 42,\n3234– 3241. doi:10.1016/j.eswa.2014.12.003\nShen, X., Zhang, X., Ouyang, T., Li, Y., and Raksincharoensak, P. (2020).\nCooperative Comfortable-Driving at Signalized Intersections for Connected\nand Automated Vehicles.IEEE Robot. Autom. Lett.5, 6247– 6254. doi:10.1109/\nlra.2020.3014010\nShi, H., Xu, M., and Li, R. (2017). Deep Learning for Household Load Forecasting a\nNovel Pooling Deep Rnn.IEEE Trans. Smart Grid9, 5271– 5280.\nSiami-Namini, S., Tavakoli, N., and Namin, A. S. (2018).“A Comparison of Arima\nand Lstm in Forecasting Time Series, ” in 2018 17th IEEE International\nConference on Machine Learning and Applications (ICMLA) (IEEE),\n1394– 1401. doi:10.1109/icmla.2018.00227\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017), Attention Is All You Need.Advances in Neural Information Processing\nSystems, 5998– 6008.\nWang, X., Zhao, T., Liu, H., and He, R. (20192019). “Power Consumption\nPredicting and Anomaly Detection Based on Long Short-Term Memory\nNeural Network,” in IEEE 4th international conference on cloud computing\nand big data analysis (ICCCBDA) (IEEE), 487 – 491. doi:10.1109/\nicccbda.2019.8725704\nW a n g ,Y . ,M o h a m e d ,A . ,L e ,D . ,L i u ,C . ,X i a o ,A . ,M a h a d e o k a r ,J . ,e ta l .( 2 0 2 0 ) .\n“Transformer-based Acoustic Modeling for Hybrid Speech Recognition,” in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP) (IEEE), 6874– 6878. doi:10.1109/icassp40776.2020.9054345\nYan, K., Shen, W., Mulumba, T., and Afshari, A. (2014). Arx Model Based Fault\nDetection and Diagnosis for Chillers Using Support Vector Machines.Energy\nand Buildings 81, 287– 295. doi:10.1016/j.enbuild.2014.05.049\nYuan, C., Liu, S., and Fang, Z. (2016). Comparison of China’s Primary Energy\nConsumption Forecasting by Using ARIMA (The Autoregressive Integrated\nMoving Average) Model and GM(1,1) Model. Energy 100, 384 – 390.\ndoi:10.1016/j.energy.2016.02.001\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2021 Zhang, Zhang, Ding and Zhang. This is an open-access article\ndistributed under the terms of the Creative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other forums is permitted, provided the\noriginal author(s) and the copyright owner(s) are credited and that the original\npublication in this journal is cited, in accordance with accepted academic practice.\nNo use, distribution or reproduction is permitted which does not comply with\nthese terms.\nFrontiers in Energy Research | www.frontiersin.org October 2021 | Volume 9 | Article 7795878\nZhang et al. Power Predicting and Anomaly Detection",
  "topic": "Cluster analysis",
  "concepts": [
    {
      "name": "Cluster analysis",
      "score": 0.7141917943954468
    },
    {
      "name": "Transformer",
      "score": 0.7071744799613953
    },
    {
      "name": "Anomaly detection",
      "score": 0.6788288354873657
    },
    {
      "name": "Computer science",
      "score": 0.5501369833946228
    },
    {
      "name": "Power consumption",
      "score": 0.5247250199317932
    },
    {
      "name": "Electric power system",
      "score": 0.5187349915504456
    },
    {
      "name": "Power grid",
      "score": 0.5103296637535095
    },
    {
      "name": "Electric power",
      "score": 0.49634629487991333
    },
    {
      "name": "Energy consumption",
      "score": 0.4747757613658905
    },
    {
      "name": "Grid",
      "score": 0.44277697801589966
    },
    {
      "name": "Data mining",
      "score": 0.3794653117656708
    },
    {
      "name": "Reliability engineering",
      "score": 0.37859392166137695
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33590269088745117
    },
    {
      "name": "Power (physics)",
      "score": 0.3181189000606537
    },
    {
      "name": "Engineering",
      "score": 0.2901921272277832
    },
    {
      "name": "Electrical engineering",
      "score": 0.23042234778404236
    },
    {
      "name": "Voltage",
      "score": 0.16640958189964294
    },
    {
      "name": "Mathematics",
      "score": 0.08949831128120422
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I43337087",
      "name": "Hebei University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90727586",
      "name": "Zhejiang University of Finance and Economics",
      "country": "CN"
    }
  ],
  "cited_by": 59
}