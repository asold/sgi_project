{
  "title": "OpenHands: Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages",
  "url": "https://openalex.org/W3206600927",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3207252011",
      "name": "Prem Selvaraj",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5001766626",
      "name": "Gokul Nc",
      "affiliations": [
        "Indian Institute of Technology Madras"
      ]
    },
    {
      "id": "https://openalex.org/A2242419757",
      "name": "Pratyush Kumar",
      "affiliations": [
        "Indian Institute of Technology Madras",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A4295849466",
      "name": "Mitesh Khapra",
      "affiliations": [
        "Indian Institute of Technology Madras"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3159212182",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2519061812",
    "https://openalex.org/W3046544838",
    "https://openalex.org/W2972780057",
    "https://openalex.org/W3105195350",
    "https://openalex.org/W3206873236",
    "https://openalex.org/W3035050855",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3113370935",
    "https://openalex.org/W3184215204",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3016338154",
    "https://openalex.org/W3138811691",
    "https://openalex.org/W3014289294",
    "https://openalex.org/W3035225512",
    "https://openalex.org/W2897208343",
    "https://openalex.org/W2128372035",
    "https://openalex.org/W2949455520",
    "https://openalex.org/W2152988638",
    "https://openalex.org/W3161849237",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W3092651828",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2948246283",
    "https://openalex.org/W2792183839",
    "https://openalex.org/W3120257917",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W3014635508",
    "https://openalex.org/W2903831537",
    "https://openalex.org/W3092336341",
    "https://openalex.org/W3034999503",
    "https://openalex.org/W2603861860",
    "https://openalex.org/W4281609260",
    "https://openalex.org/W3169413442",
    "https://openalex.org/W4393657940",
    "https://openalex.org/W3185538031",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3159461954",
    "https://openalex.org/W3108496296",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3175285983",
    "https://openalex.org/W2891726870",
    "https://openalex.org/W3160195908",
    "https://openalex.org/W2938490304",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W3010874390",
    "https://openalex.org/W3080888993",
    "https://openalex.org/W3031252323",
    "https://openalex.org/W3139022918",
    "https://openalex.org/W3108425892",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W2784435047",
    "https://openalex.org/W2802979841",
    "https://openalex.org/W2950133940"
  ],
  "abstract": "AI technologies for Natural Languages have made tremendous progress recently. However, commensurate progress has not been made on Sign Languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce OpenHands, a library where we take four key ideas from the NLP community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference, and we release standardized pose datasets for different existing sign language datasets. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages (American, Argentinian, Chinese, Greek, Indian, and Turkish), providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on Indian Sign Language (Indian-SL). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from Indian-SL to few other sign languages. We open-source all models and datasets in OpenHands with a hope that it makes research in sign languages reproducible and more accessible.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2114 - 2133\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nOpenHands: Making Sign Language Recognition Accessible with\nPose-based Pretrained Models across Languages\nPrem Selvaraj∗1, Gokul NC∗1,2, Pratyush Kumar1,2,3, Mitesh Khapra1,2\n1AI4Bharat, 2IIT-Madras, 3Microsoft Research\nprem@ai4bharat.org, gokulnc@ai4bharat.org, pratyush@cse.iitm.ac.in, miteshk@cse.iitm.ac.in\nAbstract\nAI technologies for Natural Languages have\nmade tremendous progress recently. However,\ncommensurate progress has not been made on\nSign Languages, in particular, in recognizing\nsigns as individual words or as complete sen-\ntences. We introduce\n OpenHands1, a li-\nbrary where we take four key ideas from the\nNLP community for low-resource languages\nand apply them to sign languages for word-\nlevel recognition. First, we propose using pose\nextracted through pretrained models as the stan-\ndard modality of data in this work to reduce\ntraining time and enable efficient inference, and\nwe release standardized pose datasets for differ-\nent existing sign language datasets. Second,\nwe train and release checkpoints of 4 pose-\nbased isolated sign language recognition mod-\nels across 6 languages (American, Argentinian,\nChinese, Greek, Indian, and Turkish), provid-\ning baselines and ready checkpoints for deploy-\nment. Third, to address the lack of labelled data,\nwe propose self-supervised pretraining on unla-\nbelled data. We curate and release the largest\npose-based pretraining dataset on Indian Sign\nLanguage (Indian-SL). Fourth, we compare dif-\nferent pretraining strategies and for the first\ntime establish that pretraining is effective for\nsign language recognition by demonstrating (a)\nimproved fine-tuning performance especially\nin low-resource settings, and (b) high crosslin-\ngual transfer from Indian-SL to few other sign\nlanguages. We open-source all models and\ndatasets in\n OpenHands with a hope that it\nmakes research in sign languages reproducible\nand more accessible.\n1 Introduction\nAccording to the World Federation of the Deaf,\nthere are approximately 72 million Deaf people\nworldwide. More than 80% of them live in devel-\noping countries. Collectively, they use more than\n∗ Equal contribution.\n1https://github.com/AI4Bharat/\nOpenHands\n300 different sign languages varying across differ-\nent nations (UN, 2021). Loss of hearing severely\nlimits the ability of the Deaf to communicate and\nthereby adversely impacts their quality of life. In\nthe current increasingly digital world, systems to\nease digital communication between Deaf and hear-\ning people are important accessibility aids. AI has\na crucial role to play in enabling this accessibility\nwith automated tools for Sign Language Recogni-\ntion (SLR). Specifically, transcription of sign lan-\nguage as complete sentences is referred to as Con-\ntinuous Sign Language Recognition (CSLR), while\nrecognition of individual signs is referred to as Iso-\nlated Sign Language Recognition (ISLR). There\nhave been various efforts to build datasets and mod-\nels for ISLR and CLSR tasks (Adaloglou et al.,\n2021; Koller, 2020). But these results are often\nconcentrated on a few sign languages (such as the\nAmerican Sign Language) and are reported across\ndifferent research communities with few standard-\nized baselines. When compared against text- and\nspeech-based NLP research, the progress in AI re-\nsearch for sign languages is significantly lagging.\nThis lag has been recently brought to notice of the\nwider NLP community (Yin et al., 2021).\nFor most sign languages across the world, the\namount of labelled data is very low and hence\nthey can be considered low-resource languages.\nIn the NLP literature, many successful templates\nhave been proposed for such low-resource lan-\nguages. In this work, we adopt and combine\nmany of these ideas from NLP to sign language\nresearch. We implement these ideas and release\nseveral datasets and models in an open-source li-\nbrary\n OpenHands with the following key con-\ntributions:\n1. Standardizing on pose as the modality: We\nconsider using pose-extractor as an encoder, which\nprocesses raw RGB videos and extracts the frame-\nwise coordinates for few keypoints. Pose-extractors\nare useful across sign languages and also other\n2114\ntasks such as action recognition (Yan et al., 2018;\nLiu et al., 2020), and can be trained to high ac-\ncuracy. Further, as we report, pose as a modality\nmakes both training and inference for SLR tasks ef-\nficient. We release pose-based versions of existing\ndatasets for 5 sign languages: American, Argen-\ntinian, Greek, Indian, and Turkish.\n2. Standardized comparison of models across\nlanguages: The progress in NLP has been ear-\nmarked by the release of standard datasets, includ-\ning multilingual datasets like XGLUE (Liang et al.,\n2020), on which various models are compared. As\na step towards such standardization for ISLR, we\ntrain 4 different models spanning sequence models\n(LSTM and Transformer) and graph-based mod-\nels (ST-GCN and SL-GCN) on 7 different datasets\nacross 6 sign languages mentioned in Table 1, and\ncompare them against models proposed in the liter-\nature. We release all 28 trained models along with\nscripts for efficient deployment which demonstra-\nbly achieve real-time performance on CPUs and\nGPUs.\n3. Corpus for self-supervised training: A defin-\ning success in NLP has been the use of self-\nsupervised training, for instance masked-language\nmodelling (Devlin et al., 2018), on large corpora\nof natural language text. To apply this idea to SLR,\nwe need similarly large corpora of sign language\ndata. To this end, we curate 1,129 hours of video\ndata on Indian Sign Language. We pre-process\nthese videos with a custom pipeline and extract\nkeypoints for all frames. We release this corpus\nwhich is the first such large-scale sign language\ncorpus for self-supervised training.\n4. Effectiveness of self-supervised training: Self-\nsupervised training has been demonstrated to be\neffective for NLP: Pretrained models require small\namounts of fine-tuning data (Devlin et al., 2018;\nBaevski et al., 2020) and multilingual pretrain-\ning allows crosslingual generalization (Hu et al.,\n2020b). To apply this for SLR, we evaluate mul-\ntiple strategies for self-supervised pretraining of\nISLR models and identify those that are effec-\ntive. With the identified pretraining strategies,\nwe demonstrate the significance of pretraining by\nshowing improved fine-tuning performance, espe-\ncially in very low-resource settings and also show\nhigh crosslingual transfer from Indian SL to other\nsign languages. This is the first and successful\nattempt that establishes the effectiveness of self-\nsupervised learning in SLR. We release the pre-\ntrained model and the fine-tuned models for 4 dif-\nferent sign languages.\nThrough these datasets, models, and experiments\nwe make several observations. First, in compar-\ning standardized models across different sign lan-\nguages, we find that graph-based models working\non pose modality define state-of-the-art results on\nmost sign languages. RNN-based models lag on\naccuracy but are significantly faster and thus appro-\npriate for constrained devices. Second, we estab-\nlish that self-supervised pretraining helps as it im-\nproves on equivalent models trained from scratch\non labelled ISLR data. The performance gap is\nparticularly high if the labelled data contains fewer\nsamples per label, i.e., for the many sign languages\nwhich have limited resources the value of self-\nsupervised pretraining is particularly high. Third,\nwe establish that self-supervision in one sign lan-\nguage (Indian SL) can be crosslingually transferred\nto improve SLR on other sign languages (Amer-\nican, Chinese, and Argentinian). This is particu-\nlarly encouraging for the long tail of over 300 sign\nlanguages that are used across the globe. Fourth,\nwe establish that for real-time applications, pose-\nbased modality is preferable over other modalities\nsuch as RGB, use of depth sensors, etc. due to re-\nduced infrastructure requirements (only camera),\nand higher efficiency in self-supervised pretraining,\nfine-tuning on ISLR, and inference. We believe\nsuch standardization can help accelerate dataset\ncollection and model benchmarking. Fifth, we ob-\nserve that the trained checkpoints of the pose-based\nmodels can be directly integrated with pose estima-\ntion models to create a pipeline that can provide\nreal-time inference even on CPUs. Such a pipeline\ncan enable the deployment of these models in real-\ntime video conferencing tools, perhaps even on\nsmartphones.\nAs mentioned all datasets and models\nare released with permissible licenses in\nOpenHands with the intention to make SLR\nresearch more accessible and standardized. We\nhope that others contribute datasets and models to\nthe library, especially representing the diversity of\nsign languages used across the globe.\nThe rest of the paper is organized as follows.\nIn section 2 we present a brief overview of the\nexisting work. In section 3 we describe our ef-\nforts in standardizing datasets and models across\nsix different sign languages. In section 4 we ex-\nplain our pretraining corpus and strategies for self-\n2115\nsupervised learning and detail results that establish\nits effectiveness. In section 5 we describe in brief\nthe functionalities of the\n OpenHands library.\nIn section 6, we summarize our work and also list\npotential follow-up work.\n2 Background and Related Work\nSignificant progress has been made in Isolated Sign\nLanguage Recognition (ISLR) due to the release of\ndatasets (Li et al., 2020; Sincan and Keles, 2020;\nChai et al., 2014; Huang et al., 2019) and recent\ndeep learning architectures (Adaloglou et al., 2021).\nThis section reviews this work, with a focus on\npose-based models.\n2.1 Sign Language\nA sign language (SL) is the visual language used by\nthe Deaf and hard-of-hearing (DHH) individuals\n(and also by those who comunnicate with them),\nwhich involves usage of various bodily actions, like\nhand gestures and facial expressions, called signs\nto communicate. A sequence of signs constitutes\na phrase or sentence in a SL. The signs can be\ntranscribed into sign-words of any specific spoken\nlanguage usually written completely in capital let-\nters. Each such sign-word is technically called as\na gloss and is the standardized basic atomic token\nof an SL transcript (Schembri and Crasborn, 2010).\nIt is be noted that there is not (always) one-to-one\nrelationships between glosses and spoken language\nwords.\nThe task of converting each visual sign commu-\nnicated by a signer into a gloss is called isolated\nsign language recognition (ISLR). The task of con-\nverting a continuous sequence of visual signs into\nserialized glosses is referred as continuous sign\nlanguage recognition (CSLR). CSLR can either be\nmodeled as an end-to-end task, or as a combination\nof sign language segmentation and ISLR. The task\nof converting signs into spoken language text is\nreferred as sign language translation (SLT), which\ncan again either be end-to-end or a combination of\nCLSR and gloss-sequence to spoken phrase con-\nverter.\nIn terms of real-world applications, eventhough\nCSLR is more practically useful than ISLR, it does\nnot still undermine the value in studying and im-\nplementing ISLR. The applications of ISLR in-\nclude building sign spotting systems (Albanie et al.,\n2020), building alignment networks (Albanie et al.,\n2021) to aid in building CSLR datasets (or evaluate\nCSLR output), building CSLR systems on top of\nan automatic SL segmentation model (Farag and\nBrock, 2019) which identifies the frame boundaries\nfor signs in videos to divide them into approximate\nmeaningful segments (glosses), etc.\nAlthough SL content is predominantly recorded\nas RGB (color) videos, it can also be captured using\nvarious other modalities like depth maps or point\ncloud, finger gestures recorded using sensors, skele-\nton representation of the signer, etc. In this work,\nwe focus on ISLR using pose-skeleton modality.\nA pose representation, extracted using pose esti-\nmation models, provides the spatial coordinates at\nwhich the joints (such as elbows and knees), called\nkeypoints, are located in a field or video. This\npose information can be represented as a connected\ngraph with nodes representing keypoints and edges\nmay be constructed across nodes to approximately\nrepresent the human skeleton.\n2.2 Models for ISLR\nInitial methods for SLR focused on hand gestures\nfrom either video frames (Reshna et al., 2020) or\nsensor data such as from smart gloves (Fels and\nHinton, 1993). Given that such sensors are not\ncommonplace and that body posture and face ex-\npressions are also of non-trivial importance for un-\nderstanding signs (Hu et al., 2020a), convolutional\nnetwork based models have been used for SLR\n(Rao et al., 2018).\nISLR can be considered as a multiclass classifi-\ncation task and generally accuracy metric is used\nthe to evaluate the performance of the models. The\nISLR task is related to the more widely studied\naction recognition task (Zhu et al., 2020). Like in\naction recognition task, highly accurate pose recog-\nnition models like OpenPose (Cao et al., 2018) and\nMediaPipe Holistic (Grishchenko and Bazarevsky,\n2020) are being used for ISLR models (Li et al.,\n2020; Ko et al., 2018), where frame-wise keypoints\nare the inputs. Although RGB-based models may\noutperform pose-based models (Li et al., 2020)\nnarrowly, pose-based models have far fewer pa-\nrameters and are more efficient for deployment if\nused with very-fast pose estimation pipelines like\nMediaPipe BlazePose. In this work, we focus on\nlightweight pose-based ISLR which encode the\npose frames and classify the pose using specific\ndecoders. We briefly discuss the two broad types\nof such models: sequence-based and graph-based.\nSequence-based models process data sequen-\n2116\ntially along time either on one or both directions.\nInitially, RNNs were used for pose-based action\nrecognition to learn from temporal features (Du\net al., 2015; Zhang et al., 2017; Si et al., 2018).\nSpecifically, sequence of pose frames are input to\nGRU or LSTM layers, and the output from the fi-\nnal timestep is used for classification. Transformer\narchitectures with encoder-only models like BERT\n(Vaswani et al., 2017) have also been studied for\npose-based ISLR models (De Coster et al., 2020).\nThe input is a sequence of pose frames along with\npositional embeddings. A special [CLS] token is\nprepended to the sequence, whose final embedding\nis used for classification.\nGraph convolution networks (Kipf and Welling,\n2017), which are good at modeling graph data have\nbeen used for skeleton action recognition to achieve\nstate-of-the-art results, by considering human skele-\nton sequences as spatio-temporal graphs (Cheng\net al., 2020a; Liu et al., 2020). Spatial-Temporal\nGCN (ST-GCN) uses human body joint connec-\ntions for spatial connections and temporal connec-\ntions across frames to construct a 3d graph, which\nis processed by a combination of spatial graph con-\nvolutions and temporal convolutions to efficiently\nmodel the spatio-temporal data (Lin et al., 2020).\nMany architectural improvements have been pro-\nposed over ST-GCN for skeleton action recognition\n(Zhang et al., 2020; Shi et al., 2019b,a; Cheng et al.,\n2020b,a; Liu et al., 2020). MS-AAGCN (Shi et al.,\n2020) uses attention to adaptively learn the graph\ntopology and also proposes STC-attention module\nto adaptively weight joints, frames and channels.\nDecoupled GCN (Cheng et al., 2020a) improves\nthe capacity of ST-GCN without adding additional\ncomputations and also proposes attention guided\ndrop mechanism called DropGraph as a regulariza-\ntion technique. Sign-Language GCN (SL-GCN)\n(Jiang et al., 2021) combines STC-attention with\nDecoupled-GCN and extends it to ISLR achieving\nstate-of-the-art results.\n2.3 Pretraining strategies\nAlthough there are works which use an already\ntrained classifier (on a large dataset) to finetune for\nsmaller datasets and obtain state-of-the-art results\nin the latter (Albanie et al., 2020), there are cur-\nrently no works which study the value of pretrain-\ning on openly available unlabelled data. On this\nfront, we now survey three broad classes of self-\nsupervised pretraining strategies that we reckon\ncould be applied to SLR.\nMasking-based pretraining: In NLP, masked lan-\nguage modelling is a pretraining technique where\nrandomly masked tokens in the input are predicted.\nThis approach has been explored for action recogni-\ntion (Cheng et al., 2021), where certain frames are\nmasked and a regression task estimates coordinates\nof keypoints. In addition, a direction loss is also\nproposed to classify the quadrant where the motion\nvector lies.\nContrastive-learning based: Contrastive learning\nis used to learn feature representations of the input\nto maximize the agreement between augmented\nviews of the data (Gao et al., 2021; Linguo et al.,\n2021). For positive examples, different augmen-\ntations of the same data item are used, while for\nnegative samples randomly-chosen data items usu-\nally from a few last training batches are used. A\nvariant of contrastive loss called InfoNCE (van den\nOord et al., 2018) is used to minimize the distance\nbetween positive samples.\nPredictive Coding: Predictive Coding aims to\nlearn data representation by continuously correct-\ning its predictions about data in future timesteps\ngiven data in certain input timesteps. Specifically,\nthe training objective is to pick the future timestep’s\nrepresentation from other negative samples which\nare usually picked from recent previous timesteps\nof the same video. Similar to contrastive learning, a\nloss function based on NCE is used (Mikolov et al.,\n2013; van den Oord et al., 2018). This technique\nwas explored for action recognition in a model\ncalled Dense Predictive Coding (DPC) (Han et al.,\n2019). Instead of predicting at the frame-level,\nDPC introduces coarse-prediction at the scale of\nnon-overlapping windows.\n3 Standardized Pose-based ISLR Models\nacross Sign Languages\nIn this section, we describe our efforts to curate\nstandardized pose-based datasets across multiple\nsign languages and benchmark multiple ISLR mod-\nels on them.\n3.1 ISLR Datasets\nMultiple datasets have been created for the ISLR\ntask across sign languages. However, the amount\nof data significantly varies across different sign\nlanguages, with American and Chinese having the\nlargest datasets currently. With a view to cover\na diverse set of languages, we study 7 different\n2117\nDataset Language Vocab Signers Videos Hrs Data\nAUTSL (Sincan and Keles, 2020) Turkish 226 43 38,336 20.5 RGBD\nCSL (Huang et al., 2019) Chinese 500 50 125,000 108.84 RGBD\nDEVISIGN (Chai et al., 2014) Chinese 2000 30 24,000 21.87 RGBD\nGSL (Adaloglou et al., 2021) Greek 310 7 40,785 6.44 RGBD\nINCLUDE (Sridhar et al., 2020) Indian 263 7 4,287 3.57 RGB\nLSA64 (Ronchetti et al., 2016) Argentinian 64 10 3,200 1.90 RGB\nWLASL (Li et al., 2020) American 2000 119 21,083 14 RGB\nTable 1: The diverse set of existing ISLR datasets which we study in this work through pose-based models\ndatasets across 6 sign languages as summarised\nin Table 1. For each of these datasets, we gen-\nerate pose-based data using the Mediapipe pose-\nestimation pipeline (Grishchenko and Bazarevsky,\n2020), which enables real-time inference in com-\nparison with models such as OpenPose (Cao et al.,\n2018). Mediapipe, in our chosen Holistic mode,\nreturns 3d coordinates for 75 keypoints (exclud-\ning the face mesh). Out of these, we select only\n27 sparse 2d keypoints which convey maximum\ninformation, covering upper-body, hands and face.\nThus, each input video is encoded into a vector of\nsize F ×K ×D, where F is the number of frames\nin the video, K is the number of keypoints (27 in\nour case), and D is the number of coordinates (2 in\nour case). In addition, we perform several normal-\nizations and augmentations explained in Section 5.\nFigure 1: Illustration for RGB frame to pose keypoints\nconversion. The center skeleton shows the upper portion\nof the 75 keypoints returned by MediaPipe, from which\nwe choose only 27 points as shown in right.\n3.2 Standardized ISLR Models\nOn the 7 different datasets we consider, different\nexisting ISLR models have been trained which are\nmentioned in Table 2 which produce their current\nstate-of-the-art results. For INCLUDE dataset, an\nXGBoost model is used (Sridhar et al., 2020) with\ndirect input as 135 pose-keypoints obtained using\nOpenPose. For AUTSL, SL-GCN is used (Jiang\net al., 2021) with 27 chosen keypoints as input\nfrom HRNet pose estimation model. For GSL,\nthe corresponding model (Parelli et al., 2020) is\nan attention-based encoder-decoder with 3D hand\npose and 2D body pose as input. For WLASL,\nTemporal-GCN is used (Li et al., 2020) by passing\n55 chosen keypoints from OpenPose. For LSA64,\n33 chosen keypoints from OpenPose are used as\ninput to an LSTM decoder (Konstantinidis et al.,\n2018). For DEVISIGN, RGB features are used\n(Yin et al., 2016) and the task is approached us-\ning a clustering-based classic technique called It-\nerative Reference Driven Metric Learning. For\nCSL dataset, an I3D CNN is used as encoder with\ninput as RGBD frames and BiLSTM as decoder\n(Adaloglou et al., 2021). For DEVISIGN_L and\nCSL datasets, we report RGB model results in the\ntable as there are no existing works using pose-\nbased models.\nThe differences in the above models make it dif-\nficult to compare them on effectiveness, especially\nacross diverse datasets. To enable standardized\ncomparison of models, we train pose-based ISLR\nmodels on all datasets with similar training setups\nfor consistent benchmarking. These models belong\nto two groups: sequence-based models and graph-\nbased models. For sequence-based models we con-\nsider RNN and Transformer based architectures.\nFor the RNN model, we use a 4-layered bidirec-\ntional LSTM of hidden layer dimension 128 which\ntakes as input the framewise pose-representation of\n27 keypoints with 2 coordinates each, i.e., a vector\nof 54 points per frame. We also use a temporal\nattention layer to weight the most effective frames\nfor classification. For the Transformer model,\nwe use a BERT-based architecture consisting of 5\nTransformer-encoder layers with 6 attention heads\nand hidden dimension size 128, with a maximum\nsequence length of 256. For the graph-based mod-\nels we consider ST-GCN (Yan et al., 2018) and\nSL-GCN (Jiang et al., 2021) models as discussed\nin section 2. For ST-GCN model, we use 10 spatio-\n2118\nDataset State-of-the-art model Model available in\n OpenHands\nModel (Params) Accuracy LSTM BERT ST-GCN SL-GCN\nAUTSL Pose-SL-GCN2 (4.9M) 95.02 77.4 81.0 90.4 91.9\nCSL RGBD-I3D (27M) 95.68 75.1 88.8 94.2 94.8\nDEVISIGN_L RGB-iRDML 56.85 37.6 48.9 55.8 63.9\nGSL Pose-Attention (2.1M) 83.42 86.6 89.5 93.5 95.4\nINCLUDE Pose-XGBoost 63.10 86.3 90.4 91.2 93.5\nLSA64 Pose-LSTM (1.9M) 93.91 90.2 92.5 94.7 97.8\nWLASL2000 Pose-TGCN (5.2M) 23.65 20.6 23.2 21.4 30.6\nAverage accuracy → 67.7 73.5 77.3 81.1\nTable 2: Accuracy of different models across datasets. The results in bold are the SOTA pose models.\ntemporal GCN layers with the spatial dimension of\nthe graph consisting of the 27 keypoints. For the\nSL-GCN model, we use again 10 SL-GCN blocks\nwith the same graph structure and hyperparameters\nas the ST-GCN model.\n3.3 Experimental Setup and Results\nWe train 4 models - LSTM, BERT, ST-GCN, and\nSL-GCN - for each of the 7 datasets. We use Py-\nTorch Lightning to implement the data processing\nand training pipelines. We use Adam Optimizer\nto train all the models. We search for optimal hy-\nperparameters using grid search to find the best\nhyperparams for each model on a standard dataset,\nand report the best configuration per model. For the\nLSTM model, we set the batch size as 32 and initial\nlearning rate (LR) as 5e − 3, while for BERT, we\nset a batch size 64, and LR of 1e −4. For ST-GCN\nand SL-GCN, we use a batch size of 32 and LR of\n0.001. We train all our models on a NVIDIA Tesla\nV100 GPU. Also for all datasets, we only train\non the train-sets given and we use valid-sets to do\nearly stopping, whereas some works (like AUTSL)\ntrain on combination of train-set and val-set to re-\nport the final test accuracy. We run each experi-\nment around 3 times, and report the best accuracy,\neventhough we do not see significant difference in\naccuracies across the runs. All trained models and\nthe training configurations are open-sourced in\nOpenHands.\nAccuracy We report the obtained test-set accu-\nracy of detecting individual signs, for each model\nagainst each dataset in Table 2. On all datasets,\ngraph-based models report the state-of-the-art re-\nsults using pose data. Except for AUTSL2, on 6 of\n2SOTA AUTSL model is trained on high quality pose data\nfrom HRNet model with more keypoints.\nthe 7 datasets, models we train improve upon the\naccuracy reported in the existing papers sometimes\nsignificantly (e.g., over 10% on GSL). These uni-\nform results across a diverse set of SLs confirm that\ngraph-based models on pose modality data define\nthe SOTA.\nIn summary, the standardized benchmarking of\nmultiple models in terms of accuracy on datasets\nand, measurements of latency on devices (ex-\nplained in appendix) informs model selection. Mak-\ning the trade-off between accuracy and latency, we\nuse the ST-GCN model for the pretrained model\nwe discuss later. Our choice is also informed by\nthe cost of the training step: The more accurate SL-\nGCN model takes 4× longer to train than ST-GCN.\n4 Self-Supervised Learning for ISLR\nIn this section, we describe our efforts in building\nthe largest corpus for self-supervised pretraining\nand our experiments in different pretraining strate-\ngies.\n4.1 Indian SL Corpus for Self-supervised\npretraining\nChannel Hours Domain\nNewzHook 615 News\nMBM Vadodara 225 News\nISH-News 145 News\nNIOS 115 Educational\nSIGN Library 29 Educational\nTotal 1129\nTable 3: Source-wise statistics of the processed self-\nsupervised dataset on Indian-SL\nLarge text corpora such as BookCorpus,\n2119\nWikipedia dumps, OSCAR, etc. have enabled pre-\ntraining of large language models. Although there\nare large amounts of raw sign language videos avail-\nable on the internet, no existing work has studied\nhow such large volumes of open unlabelled data\ncan be collected and used for SLR tasks. To ad-\ndress this, we create a corpus of Indian SL data by\ncurating videos, pre-process the videos, and release\na standardized pose-based dataset compatible with\nthe models discussed in the previous section.\nWe manually search for freely available major\nsources of Indian SL videos. We restrict our search\nto a single sign language so as to study the effect\nof pretraining on same language and crosslingual\nISLR tasks. We sort the sources by the number\nof hours of videos and choose the top 5 sources\nfor download. All of these 5 sources, as listed in\nTable 3 are YouTube channels, totalling over 1,500\nhours before preprocessing.\nVideo Sources\nYouTube channel 1\nYouTube channel 2\nYouTube channel 3\nScheduler\nCrawl\nProcess1\nCrawl \nProcess2\nCrawl\nProcess3\nStorage for\nDownloaded\nVideos\nStage-1: Data Crawling CPU 1\nCPU 2\nCPU 3\nManual\nCuration\nof list\nStage-2: Pose Extraction\nScheduler\nSplit all videos in\nchunks of 5mins\n(9k frames)\nCPU-1\nCPU-2\nCPU-3\nMediaPipe\nPose\nEstimator\nThread-1\nMediaPipe\nPose\nEstimator\nThread-2\nMediaPipe\nPose\nEstimator\nThread-3\nStorage for\nGenerated\nPose Data\nStage-3: Data Cleaning\nDrop noise\nand resplit\nvideos\nFor Video regions with:\n1. No signing activity\n2. No signers\n3. More than one signer\nConsolidate all\nchunked clips from\nall channels\nHDF5\nStorage\nHierarchical\nchannel-wise\ngrouping\nCompressed\n  Cloud Storage\nAvailable via library\nfor download and\nusage\nPre-training\nData-Loader\nFaster random\naccess of pose\nsub-regions\nTrainer\nPyTorch\nEcosystem\nCPUGPU\nStage-4: Efficient Training\nFigure 2: Pipeline used to collect and process Indian SL\ncorpus for self-supervised pretraining\nWe pass these videos through a processing\npipeline as described in Figure 2. We initially dump\nthe pose data for all videos, then process them to re-\nmove those which are noisy or contain either no per-\nson or more than 1 person. This resulted in 1,129\nhours of Indian SL data, as detailed source-wise\nin Table 3. This is significantly larger than all the\ntraining sets in the datasets we studied which is on\naverage 177 hours. We pass these videos through\nMediaPipe to obtain pose information as described\nearlier, i.e., 75 keypoints per frame. The resultant\nIndian SL corpus has more than 100 million pose\nframes. We convert this to the HDF5 format to\nenable efficient random access, as is required for\ntraining. We open-source this corpus of about 250\nGB which is available in\n OpenHands.\n4.2 Pretraining Setup and Experiments\nWe explore the three major pretraining strategies\nas described in Section 2.3 and explain how and\nwhy certain self-supervised settings are effective\nfor ISLR. We pretrain on randomly sampled con-\nsecutive input sequences of length 60-120 frames\n(approximating 2-4 secs with 30fps videos). After\npretraining, we fine-tune the models on the respec-\ntive ISLR dataset with an added classification head.\n4.2.1 Masking-based pretraining\nWe follow the same hyperparameter settings as de-\nscribed in Motion-Transformer (Cheng et al., 2021),\nto pretrain a BERT-based model with random mask-\ning of 40% of the input frames. When using only\nthe regression loss, we find that pretraining learns\nto reduce the loss as shown in appendix. However,\nwhen fine-tuned on the INCLUDE dataset, we see\nno major contribution of the pretrained model to\nincreasing the accuracy as shown in Table 4. We\nposit that while pretraining was able to approxi-\nmate interpolation for the masked frames based\non the surrounding context, it did not learn higher-\norder features relevant across individual signs. We\nalso experiment with different masking ratios (20%\nand 30%) as well as different length of random con-\ntiguous masking spans (randomly selected between\n2-10), and obtain similar results.\n4.2.2 Contrastive-learning based\nInspired from the work by Gao et al. (2021), we\nconsider Shear, Scaling and Rotation augmenta-\ntions to generate the 2 augmented copies of the\ninput pose sequence and we pretrain the model and\nobserve that it converges on reducing the InfoNCE\nloss (see appendix for plot). We then fine-tune\non INCLUDE and again did not observe any gain\nover the baseline of training from scratch as seen\nin Table 4. To understand this, we analyzed the\nembeddings of data from the pretrained model and\nobserved two facts: (a) Embeddings of different\naugmentations of a video clip are similar indicat-\ning successful pretraining, but (b) Embeddings of\ndifferent videos from the INCLUDE dataset do\nnot show any clustering based on the class (see\nvisualization in appendix). Again, we posit that\n2120\npretraining did not learn higher order semantics\nthat could be helpful for ISLR.\n4.2.3 Predictive-coding based\nOur architecture is inspired from Dense Predictive\nCoding (Han et al., 2019), but using pose modality.\nThe architecture is represented in Figure 3. The\npose frames from a video clip will be partitioned\ninto multiple non-overlapping windows with equal\nnumber of frames in each window. The encoder f\ntakes each window of pose keypoints as input and\nembeds into the hidden space z. We use ST-GCN\nas the encoder. The ST-GCN encoder embeds each\ninput window xi, and the direct output is average\npooled across the spatial and temporal dimensions\nto obtain the output embedding zi for each window.\nThe embeddings are then fed to a Gated Recurrent\nUnit (GRU) as a temporal sequence and the future\ntimesteps ˆzi are predicted sequentially using the\npast timestep representations from GRU, with an\naffine transform layer ϕ. We use 4 windows of data\nas input to predict the embeddings of the next 3\nwindows, each window spanning 10 frames, which\nwe empirically found to be the best setting. For\npretraining, we used a batch size of 128 and for\nfinetuning, we used a batch size of 64. For both pre-\ntraining and finetuning, we used Adam optimizer\nwith an initial learning rate of 1e-3. The pretrain-\ning was done for 200k iterations on a NVIDIA\nV100 GPU, taking around 26 hours (on Microsoft\nplatform’s Azure NC6s_v3 machine).\nWindow \nT ime \nFigure 3: Model architecture for DPC pretraining\nUpon fine-tuning on INCLUDE, DPC provides\na significant improvement of 3.5% over the base-\nline. We include a plot comparing the validation\naccuracy between baseline and finetuned model\nin appendix. We posit that Sign Language DPC\n(SL-DPC) is successful, while previous methods\nwere not, as it learns coarse-grained representations\nacross multiple frames and thereby captures motion\nsemantics of actions in SL.\nTo the best of our knowledge, this is the first\ncomparison of pretraining strategies for SLR.\nTraining of ST-GCN Accuracy\nNo pretraining + Fine-tune 91.2\nMasked-based + Fine-tune 91.3\nContrastive learning + Fine-tune 90.8\nPredictive-coding + Fine-tune 94.7\nTable 4: Effectiveness of pretraining strategies as mea-\nsured on ISLR accuracy on INCLUDE\n4.3 Evaluation on low-resource and\ncrosslingual settings\nWe demonstrated that DPC-based pretraining is\neffective. We now analyze the effectiveness of\nsuch pretraining in two constrained settings - (a)\nwhen fine-tuning datasets are small, and (b) when\nfine-tuning on sign languages different from the\nsign language used for pretraining. The former\ncaptures in-language generalization while the latter\ncrosslingual generalization.\nDataset Samples/class STGCN SLDPC\nINCLUDE\n(Indian)\nFull (Avg. 17) 91.2 94.7\n10 79.7 86.27\n5 45 57.35\n3 15.2 35.42\nWLASL2k\n(American)\nFull (Avg. 10) 21.4 27.4\n5 3.1 5.74\n3 1.6 2.78\nDEVISign_L\n(Chinese)\nFull (8) 55.8 59.5\n5 33.0 40.26\n3 8.46 18.65\nLSA64\n(Argentinian)\nFull (50) 94.7 96.25\n5 64.7 75.32\n3 39.7 57.19\nTable 5: Effectiveness of pretraining for in-language\n(first row) and crosslingual transfer (last three rows)\n4.3.1 In-language generalization\nThe INCLUDE dataset contains an average of 17\nsamples per class. For this setting, we observed\na gain of 3.5% with DPC-based pretraining over\ntraining from scratch. How does this performance\nboost change when we have fewer samples per\n2121\nclass? We present results for 10, 5, and 3 sam-\nples per class in Table 5. We observe that as the\nnumber of labels decreases the performance boost\ndue to pretraining is higher indicating effective in-\nlanguage generalization.\n4.3.2 Crosslingual transfer\nDoes the pretraining on Indian sign language pro-\nvide a performance boost when fine-tuning on other\nsign languages? We study this for 3 different sign\nlanguages - American, Chinese, and Argentinian -\nand report results in Table 5. We see that crosslin-\ngual transfer is effective leading to gains of about\n6%, 4%, and 2% on the three datasets, similar to\nthe 3% gain on in-language accuracy. The increase\nin accuracy varies with datasets - For Argentianian\nand Indian datasets which already have 90+% ac-\ncuracy, there are small improvements. However,\nWLASL which is scraped from web and has a lot\nmore variations, sees a much higher improvement\ndue to pretraining. Further, we also observe that\nthese gains extend to low-resource settings of fewer\nlabels per sign. For instance on Argentinian SL,\nwith 3 labels, pretraining on Indian SL given an\nimprovement of about 18% in accuracy. To the\nbest of our knowledge this is the first successful\ndemonstration of crosslingual transfer in ISLR.\nIn summary, we discussed different pretraining\nstrategies and found that only SL-DPC learns se-\nmantically relevant higher-order features. With\nDPC-based pretraining we demonstrated both in-\nlanguage and crosslingual transfer.\n5 The\n OpenHands library\nAs mentioned in the main paper, we open-source all\nour contributions through the\n OpenHands li-\nbrary. This includes the pose-based datasets for 5\nSLs, 4 ISLR models trained on 7 datasets, the pre-\ntraining corpus on Indian SL with over 1,100 hours\nof pose data, pretrained models on this corpus us-\ning self-supervised learning, and models fine-tuned\nfor 4 different SLs on top of the pretrained model.\nWe also provide scripts for efficient deployment\nusing MediaPipe pose estimation and our trained\nISLR models.\nIn addition, the library provides utilities that are\nhelpful specifically for pose-based data. This in-\ncludes methods to normalize keypoints by width\nand height of frames, to normalize all of the pose\ndata to be in the same scale and reference coordi-\nnate system by using a constant feature of body as\nreference, and to fill missing keypoints. The library\nalso includes utilities to create data augmentations\nsuch as ShearTransform to displace the joints in a\nrandom direction, RotatationTransform to simulate\nthe viewpoint changes of the camera, ScaleTrans-\nform to simulate different scales of the pose data to\naccount for relative zoomed-in or zoomed-out view\nof signers, PoseRandomShift to move a significant\nportion of the video by a time offset so as to make\nthe ISLR models robust to inaccurate segmentation\nof real-time video, UniformTemporalSubsample to\nuniformly sample frames from the video instead of\nconsidering only the initial frames, in cases where\nthe number of frames in a video clip exceeds a\nmaximum limit, and RandomTemporalSubsample\nto sample a random fixed contiguous window of re-\nquired size covering a maximum number of frames.\nWe encourage researchers to contribute datasets,\nmodels, and other utilities to make sign language\nresearch more accessible. All the aspects of the\ntoolkit are well-documented online3 for anyone to\nget started easily.\n6 Conclusion\nIn this work, we make several contributions to\nmake sign language research more accessible. We\nrelease pose-based datasets and 4 different ISLR\nmodels across 6 sign languages. This evaluation\nenabled us to identify graph-based methods such\nas ST-GCN as being accurate and efficient. We\nrelease the first large corpus of SL data for self-\nsupervised pretraining. We evaluated different pre-\ntraining strategies and found DPC as being effec-\ntive. We also show that pretraining is effective both\nfor in-language and crosslingual transfer. All our\nmodels, datasets, training and deployment scripts\nare open-sourced in\n OpenHands.\nSeveral directions for future work emerge such\nas evaluating alternative graph-based models, ex-\nperimenting with varying sequence lengths of in-\nput data, efficiently sampling the data from the raw\ndataset for pretraining such that the samples are\ndiverse enough, using better pose estimator models\nand more keypoints, and quantized inference for\n2×-4× reduced latency. On the library front, we\naim to release updated versions incorporating more\nSL datasets, better graph-based models, studying\nthe performance on low FPS videos (like 2-4 FPS),\neffect of pretraining using other high-resource SL\ndatasets, extending to CSLR, and improving de-\nployment features.\n3https://openhands.readthedocs.io\n2122\nAcknowledgements\nWe would like to thankAravint Annamalaifrom IIT\nMadras for preparing the list of potential YouTube\nchannels that can be crawled and for his help in\ndownloading them. We would like to thank the\nentire AI4Bharat Sign Language Team4 for their\nsupport and feedback for this work, especially from\nRohith Gandhi Ganesanfor his insights on code\nstructuring, and Advaith Sridharfor managing the\noverall project. We would also like to extend our\nimmense gratitude to Microsoft’s AI4Accessibility\nprogram (via Microsoft Philanthropies India) for\ngranting us the compute required to carry out all\nthe experiments in this work, through Microsoft\nAzure cloud platform. Our extended gratitude also\ngoes to Zenodo, who helped us with hosting our\nlarge datasets (NC and Selvaraj, 2021). Finally,\nwe thank all the content creators and ISLR dataset\ncurators without whose data this work would have\nbeen impossible.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBenoît Sagot. 2022. Towards a Cleaner Document-\nOriented Multilingual Crawled Corpus. arXiv e-\nprints, page arXiv:2201.06642.\nNikolaos M. Adaloglou, Theocharis Chatzis, Ilias Papas-\ntratis, Andreas Stergioulas, Georgios Th Papadopou-\nlos, Vassia Zacharopoulou, George Xydopoulos,\nKlimis Antzakas, Dimitris Papazachariou, and Pet-\nros none Daras. 2021. A comprehensive study on\ndeep learning-based methods for sign language recog-\nnition. IEEE Transactions on Multimedia, page 1–1.\nSamuel Albanie, Gül Varol, Liliane Momeni, Triantafyl-\nlos Afouras, Joon Son Chung, Neil Fox, and Andrew\nZisserman. 2020. BSL-1K: Scaling up co-articulated\nsign language recognition using mouthing cues. In\nEuropean Conference on Computer Vision.\nSamuel Albanie, Gül Varol, Liliane Momeni, Hannah\nBull, Triantafyllos Afouras, Himel Chowdhury, Neil\nFox, Bencie Woll, Rob Cooper, Andrew McParland,\nand Andrew Zisserman. 2021. BOBSL: BBC-Oxford\nBritish Sign Language Dataset.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A frame-\nwork for self-supervised learning of speech represen-\ntations.\nZhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei,\nand Yaser Sheikh. 2018. Openpose: Realtime multi-\nperson 2d pose estimation using part affinity fields.\n4https://sign-language.ai4bharat.org\nXiujuan Chai, Hanjie Wang, and Xilin Chen. 2014. The\ndevisign large vocabulary of chinese sign language\ndatabase and baseline evaluations. Technical report\nVIPL-TR-14-SLR-001. Key Lab of Intelligent Infor-\nmation Processing of Chinese Academy of Sciences\n(CAS), Institute of Computing Technology, CAS.\nKe Cheng, Yifan Zhang, Congqi Cao, Lei Shi, Jian\nCheng, and Hanqing Lu. 2020a. Decoupling gcn\nwith dropgraph module for skeleton-based action\nrecognition. In Proceedings of the European Confer-\nence on Computer Vision (ECCV).\nKe Cheng, Yifan Zhang, Xiangyu He, Weihan Chen,\nJian Cheng, and Hanqing Lu. 2020b. Skeleton-based\naction recognition with shift graph convolutional\nnetwork. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR).\nYi-Bin Cheng, Xipeng Chen, Dongyu Zhang, and Liang\nLin. 2021. Motion-Transformer: Self-Supervised\nPre-Training for Skeleton-Based Action Recognition.\nAssociation for Computing Machinery, New York,\nNY , USA.\nMathieu De Coster, Mieke Van Herreweghe, and Joni\nDambre. 2020. Sign language recognition with trans-\nformer networks. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n6018–6024, Marseille, France. European Language\nResources Association.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nYong Du, Wei Wang, and Liang Wang. 2015. Hierar-\nchical recurrent neural network for skeleton based\naction recognition. In 2015 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 1110–1118.\nIva Farag and Heike Brock. 2019. Learning motion dis-\nfluencies for automatic sign language segmentation.\nIn ICASSP 2019 - 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7360–7364.\nS.S. Fels and G.E. Hinton. 1993. Glove-talk: a neural\nnetwork interface between a data-glove and a speech\nsynthesizer. IEEE Transactions on Neural Networks,\n4(1):2–8.\nXuehao Gao, Yang Yang, and Shaoyi Du. 2021. Con-\ntrastive self-supervised learning for skeleton action\nrecognition. In NeurIPS 2020 Workshop on Pre-\nregistration in Machine Learning, volume 148 of\nProceedings of Machine Learning Research, pages\n51–61. PMLR.\nIvan Grishchenko and Valentin Bazarevsky.\n2020. Mediapipe holistic — simultaneous\nface, hand and pose prediction, on device.\n2123\nhttps://ai.googleblog.com/2020/12/mediapipe-\nholistic-simultaneous-face.html. (Accessed on\n08/23/2021).\nTengda Han, Weidi Xie, and Andrew Zisserman. 2019.\nVideo representation learning by dense predictive\ncoding.\nHezhen Hu, Wen gang Zhou, Junfu Pu, and Houqiang Li.\n2020a. Global-local enhancement network for nmf-\naware sign language recognition. ACM Transactions\non Multimedia Computing, Communications, and\nApplications (TOMM), 17:1 – 19.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020b.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalization.\nJie Huang, Wengang Zhou, Houqiang Li, and Weip-\ning Li. 2019. Attention-based 3d-cnns for large-\nvocabulary sign language recognition. IEEE Transac-\ntions on Circuits and Systems for Video Technology,\n29(9):2822–2832.\nSongyao Jiang, Bin Sun, Lichen Wang, Yue Bai, Kun-\npeng Li, and Yun Fu. 2021. Skeleton aware multi-\nmodal sign language recognition.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks.\nSang-Ki Ko, Jae Gi Son, and Hyedong Jung. 2018. Sign\nlanguage recognition with recurrent neural network\nusing human keypoint detection. In Proceedings of\nthe 2018 Conference on Research in Adaptive and\nConvergent Systems, RACS ’18, page 326–328, New\nYork, NY , USA. Association for Computing Machin-\nery.\nOscar Koller. 2020. Quantitative survey of the state of\nthe art in sign language recognition.\nDimitrios Konstantinidis, K. Dimitropoulos, and\nP. Daras. 2018. Sign language recognition based\non hand and body skeletal data. 2018 - 3DTV-\nConference: The True Vision - Capture, Transmission\nand Display of 3D Video (3DTV-CON), pages 1–4.\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong\nLi. 2020. Word-level deep sign language recognition\nfrom video: A new large-scale dataset and methods\ncomparison. In The IEEE Winter Conference on\nApplications of Computer Vision, pages 1459–1469.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. Xglue: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation.\nLilang Lin, Sijie Song, Wenhan Yang, and Jiaying Liu.\n2020. Ms2l. Proceedings of the 28th ACM Interna-\ntional Conference on Multimedia.\nLi Linguo, Wang Minsi, Ni Bingbing, Wang Hang, Yang\nJiancheng, and Zhang Wenjun. 2021. 3d human ac-\ntion representation learning via cross-view consis-\ntency pursuit. In CVPR.\nZiyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong\nWang, and Wanli Ouyang. 2020. Disentangling and\nunifying graph convolutions for skeleton-based ac-\ntion recognition.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity.\nGokul NC and Premkumar Selvaraj. 2021. Openhands\nv1 : Raw slr pose datasets.\nM. Parelli, Katerina Papadimitriou, G. Potamianos,\nG. Pavlakos, and P. Maragos. 2020. Exploiting 3d\nhand pose estimation in deep learning-based sign\nlanguage recognition from rgb videos. In ECCV\nWorkshops.\nG. Anantha Rao, K. Syamala, P. V . V . Kishore, and\nA. S. C. S. Sastry. 2018. Deep convolutional neural\nnetworks for sign language recognition. In 2018 Con-\nference on Signal Processing And Communication\nEngineering Systems (SPACES), pages 194–197.\nS. Reshna, A. Sajeena, and Madhavan Jayaraju. 2020.\nRecognition of static hand gestures of indian sign\nlanguage using cnn. volume 2222, page 030012.\nFranco Ronchetti, Facundo Quiroga, Cesar Estrebou,\nLaura Lanzarini, and Alejandro Rosete. 2016. Lsa64:\nA dataset of argentinian sign language. XX II Con-\ngreso Argentino de Ciencias de la Computación\n(CACIC).\nAdam Schembri and Onno Crasborn. 2010. Issues in\ncreating annotation standards for sign language de-\nscription. In Proceedings of the LREC2010 4th Work-\nshop on the Representation and Processing of Sign\nLanguages: Corpora and Sign Language Technolo-\ngies, pages 212–216, Valletta, Malta. European Lan-\nguage Resources Association (ELRA).\nLei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\n2019a. Skeleton-based action recognition with di-\nrected graph neural networks. In 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 7904–7913.\nLei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\n2019b. Two-stream adaptive graph convolutional\nnetworks for skeleton-based action recognition. In\nCVPR.\nLei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu.\n2020. Skeleton-based action recognition with multi-\nstream adaptive graph convolutional networks. IEEE\nTransactions on Image Processing, 29:9532–9545.\n2124\nChenyang Si, Ya Jing, Wei Wang, Liang Wang, and\nTieniu Tan. 2018. Skeleton-based action recognition\nwith spatial reasoning and temporal stack learning.\nOzge Mercanoglu Sincan and Hacer Yalim Keles. 2020.\nAutsl: A large scale multi-modal turkish sign lan-\nguage dataset and baseline methods. IEEE Access,\n8:181340–181355.\nAdvaith Sridhar, Rohith Gandhi Ganesan, Pratyush Ku-\nmar, and Mitesh Khapra. 2020. Include: A large\nscale dataset for indian sign language recognition.\nMM ’20. Association for Computing Machinery.\nUN. 2021. International day of sign languages.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.\nRepresentation learning with contrastive predictive\ncoding.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nSijie Yan, Yuanjun Xiong, and Dahua Lin. 2018. Spatial\ntemporal graph convolutional networks for skeleton-\nbased action recognition.\nFang Yin, Xiujuan Chai, and Xilin Chen. 2016. Iterative\nreference driven metric learning for signer indepen-\ndent isolated sign language recognition. In Computer\nVision – ECCV 2016, pages 434–450, Cham. Springer\nInternational Publishing.\nKayo Yin, Amit Moryossef, Julie Hochgesang, Yoav\nGoldberg, and Malihe Alikhani. 2021. Including\nsigned languages in natural language processing.\nPengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun\nZeng, Jianru Xue, and Nanning Zheng. 2017. View\nadaptive recurrent neural networks for high perfor-\nmance human action recognition from skeleton data.\n2017 IEEE International Conference on Computer\nVision (ICCV).\nPengfei Zhang, Cuiling Lan, Wenjun Zeng, Jun-\nliang Xing, Jianru Xue, and Nanning Zheng. 2020.\nSemantics-guided neural networks for efficient\nskeleton-based human action recognition. 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nYi Zhu, Xinyu Li, Chunhui Liu, Mohammadreza\nZolfaghari, Yuanjun Xiong, Chongruo Wu, Zhi\nZhang, Joseph Tighe, R. Manmatha, and Mu Li. 2020.\nA comprehensive study of deep video action recogni-\ntion.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), pages 19–27.\n2125\nAPPENDIX\nA Ethical considerations\nAll models trained in this work only use pose or\nskeletal data. Consequently all released datasets\nand models do not have any personally identi-\nfiable information (PII), thereby addressing pri-\nvacy concerns of those who contributed to these\ndatasets. Furthermore, such a standardization elim-\ninates all visually distinguishing features of individ-\nuals like color, gender, ethnicity/race, etc., thereby\novercoming any potential biases pertaining to sub-\npopulations.\nWe address the licensing-related aspects of the\ndatasets in the subsequent sub-sections.\nA.1 Release of pose ISLR datasets\nOur work builds on existing ISLR datasets across\nlanguages, by processing them to retain only pose\ndata. Complying with the respective licenses of\nthe datasets, we release our generated poses only\nfor the openly available datasets with permissive\nlicenses. Out of the 7 datasets we evaluate in the\npaper, we find that we can release the pose data for\n5 of the datasets (AUTSL, WLASL, GSL, LSA64,\nand INCLUDE), covering 5 sign languages (re-\nspectively: Turkish, American, Greek, Argentinian\nand Indian). The other 2 datasets are CSL and\nDEVISIGN, belonging to Chinese sign language.\nThe licenses of each original dataset is shown in\nTable 6.\nDataset License\nAUTSL Permissive\nCSL Proprietary\nDEVISIGN Proprietary\nGSL Creative Commons 4.0\nINCLUDE MIT\nLSA64 Creative Commons 4.0\nWLASL C-UDA\nTable 6: Licenses of each ISLR dataset\nWe do not claim ownership over any of the orig-\ninal ISLR datasets, and release the pose data under\nthe same licensing terms as the original datasets.\nA.2 Release of raw pose data\nWe also open-source the pretraining dataset on In-\ndian Sign Language (ISL) that we explained in\nSection 4.1. The detailed datasheet of this dataset,\nincluding motivation, composition, collection pro-\ncess, preprocessing, distribution, maintenance, and\nethical considerations is included after the appen-\ndices.\nB Inference Benchmarking\nIn this section, we explain how we achieve over\n23fps real-time inference, by using MediaPipe\nHolistic for generating poses (as an ISLR encoder)\nand our pose-based models (as decoder) that recog-\nnizes the sign at any given window.\nB.1 MediaPipe Inference\nFor pose-estimation, MediaPipe offers 3 variants of\nmodels: heavy, full and lite in decreasing order of\naccuracy but increasing order of inference-speed.\nThe latency of these variants on Intel Xeon E5-\n2690 v4 CPU with a frame-size of 640x480 were\n142.59ms, 55.28ms, and 35.37ms respectively per\nframe. For all training and testing in this work, we\nused the heavy model to get the best quality results.\nFor real-time inference, depending on one’s\nCPU, either of the 3 variants can be used with\nthe trained models, since all the 3 BlazePose mod-\nels are trained on the same dataset to return same\nnumber of keypoints. Based on our experience, we\nprefer only lite or full variants depending on the\nCPU-type, and we find the heavy model only suit-\nable if we employ frame-skipping and use decoder\nmodels that also work at a lower FPS (below 8fps).\nB.2 ISLR Model Inference\nGiven that SLR is an interactive application, de-\nployability atleast at 23 FPS without noticeable\nlatency is essential. We thus study the latency of\nour models on various CPU configurations so as\nto target ubiquitous deployment. For each of the 4\nmodels, we report the model size and latency mea-\nsured on 4 different CPUs in Table 7. The LSTM\nmodel is an order of magnitude faster across all de-\nvices than the most accurate SL-GCN model, and\nis a good candidate when speed is essential at the\ncost of about 10% accuracy drop that we observed\nin Table 2. Amongst the graph-based methods, ST-\nGCN provides a good trade-off being about 2 ×\nfaster than SL-GCN at the cost of only 3% lower\naverage accuracy across datasets.\nThe benchmarking is done with a batch size of 1\nwith complete serial processing (without any data\n2126\nModel → LSTM Transformer ST-GCN SL-GCN SLDPC\nParams → 1.6M 3.8M 2.3M 4.9M 4.0M\nCPU Latency in milliseconds\nXeon E5-2690 v4 (2.60GHz) 08.05 30.64 23.02 52.8 47.60\nAMD Ryzen 7 3750H (2.30GHz) 12.94 76.41 86.97 225.3 147.28\nXeon Platinum 8168 (2.70GHz) 05.38 23.76 51.64 112.66 112.52\nXeon E5-2673 v4 (2.30GHz) 09.03 43.69 99.39 201.31 188.43\nTable 7: Number of parameters and average latency of different model architectures\nloading parallelization). The latencies reported in\nthe table corresponds to average inference time per\nvideo using the test set of the INCLUDE dataset,\nfor both freshly trained models and pretrained sign\nlanguage DPC (SLDPC) model.\nNote that encoder (pose estimation) and decoder\n(classifier) are parallelized such that the former is\na producer of skeletons for window of live frames,\nand the latter is a consumer which recognizes\nglosses.\nC Additional notes on pretraining\nIn this section, we briefly present a few of the ar-\ntifacts pertaining to the different configurations of\nthe self-supervised training that we experiment.\nC.1 Masking-based pretraining\nFigure 4 shows the pretraining loss-plot for masked-\nlanguage learning, to show that although the model\nconverges, due to the reasons mentioned in the\nmain paper, it does not learn useful representations\nfor the downstream tasks.\n0 1 2 3 4 5 6 7 8\n·104\n0.02\n0.03\n0.04\n0.05\n0.06\nSteps\nLoss\nFigure 4: Loss curve for masked pretraining with regres-\nsion loss\nTo explain this behaviour, we analyzed the input\ndata as well as the outputs by the model. We find\nthat the model was able to converge because learn-\ning to perform an approximate linear interpolation\nfor the masked frames based on the surrounding\ncontext was sufficient reduce the loss significantly.\nHowever, we posit that such interpolation does not\nlearn any high-level features. This is illustrated in\nFigure 5, where for each masking span length, we\nplot the sum of absolute differences between each\nconsecutive masked frames Fi and Fi−1, for both\npredictions from the model as well as the actual\nframe keypoints. The numbers shown are aver-\naged across all videos in the INCLUDE test set,\nin which the masking is done around the center re-\ngion of each video. The plot shows that as masking\nlength is increased, the gap between the predicted\nvalues and the actual values diverges indicating an\ninability to learn longer-range patterns that may be\nnecessary to classify signs.\n2 4 6 8 10 12 14 16 18 200\n0.2\n0.4\n0.6\n0.8\nMask span\nSum of masked frame-level differences\nActual diffs\nPredicted diffs\nFigure 5: Differences in the output range of masked pre-\ndictions of pretrained model and corresponding actual\nkeypoints\nWe also experiment with pretraining using direc-\ntion loss as explained in background, which essen-\ntially is an objective to classify which quadrant the\nmotion vector for each frame will lie. We find that\nthe pretraining does not converge. Upon checking\nthe labels, we see that at the fine-grained level of\neach frames, the approximately discretized quad-\nrant for each motion vector were seemingly almost\nrandom because of the slightly jittery predictions\nfor each frame by the pose estimation model. Also,\nsince the quadrant-type classification encodes only\n4 directions, it fails to capture static motion (key-\npoints which do not move much temporally), which\n2127\naccounts for more than half of the total motion vec-\ntors. We thus posit that the direction classification\ntargets are noisy and do not allow the pretraining\nloss to converge. Figure 6 shows the visualization\nof quadrants for a randomly-selected joint from a\nrandom video in the INCLUDE dataset, to visually\nverify how noisy the targets for direction loss are.\n0 10 20 30 40 50 60\n1\n2\n3\n4\nFrame\nQuadrant\nFigure 6: Sample visualization of direction labels for\nkeypoint-15 from the frames of a random INCLUDE\nvideo (Adjectives/4. sad/MVI_9720)\nC.2 Contrastive-learning based\nThe training setup for this experiment is: For pre-\ntraining, we used a batch size of 128 and for finetun-\ning, we used a batch size of 64. For both pretrain-\ning and finetuning, we used Adam optimizer with\nan initial LR of 1e-3. To obtain negative samples,\nwe use a Memory Bank to obtain the embeddings\nfrom samples of recent previous batches, which is\nessentially a FIFO queue of fixed size. We use Face-\nbook’s MoCo codebaseto implement the setup, by\nplugging-in our ST-GCN as the encoder.\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8\n·104\n0.0\n2.0\n4.0\n6.0\nSteps\nLoss\nFigure 7: Loss curve for contrastive pretraining\nFigure 7 shows the pretraining loss-plot for con-\ntrastive learning, to show that although the model\nconverges, as explained in the main paper, the rep-\nresentations learnt do not signify any semantic re-\nlationships in the signs. To illustrate this, we take\na standard subset of the INCLUDE dataset, called\nINCLUDE50 (containing 50 classes) and visualize\nthe embeddings of all signs using PCA clustering.\nNote that each class is uniquely colored to iden-\ntify if similar signs are grouped together. Figure 8\nshows that the learnt embeddings do not discrim-\ninate the classes, suggesting that the embeddings\nmay not be informative for the downstream sign\nrecognition task.\nFigure 8: PCA visualization of INCLUDE50 embed-\ndings obtained from Contrastive-Learning model\nC.3 Predictive-coding based pretraining\nThe training setup for this experiment is: For pre-\ntraining, we used a batch size of 128 and for fine-\ntuning, we used a batch size of 64. For both pre-\ntraining and finetuning, we used Adam optimizer\nwith a learning rate of 1e-3.\n1,000 2,000 3,000 4,000 5,000\n0.2\n0.4\n0.6\n0.8\n1.0\nSteps\nClassification Accuracy\nSL-DPC\nST-GCN\nFigure 9: DPC Fine-tuning (orange) vs fresh training\n(light-green) validation accuracy plot\nFigure 9 shows the performance gap between\nfine-tuning of a DPC pretrained model and an\nST-GCN model being trained from scratch. This\nclearly demonstrates that self-supervised learning\nproduces a significant boost in performance for\ndownstream tasks.\n2128\nD Sample usage snippets from\n OpenHands library\nimport omegaconf\nfrom openhands.apis import ClassificationModel\nfrom openhands.core import get_trainer\ncfg = omegaconf.OmegaConf.load(\"path/to/config.yaml\")\ntrainer = get_trainer(cfg)\nmodel = ClassificationModel(cfg=cfg, trainer=trainer)\nmodel.fit()\nFigure 10: Example\n OpenHands code for running the model training.\ndata:\nmodality: \"pose\" #modality to use\ntrain_pipeline:\ndataset:\n_target_: \"dataset_class\" #dataset to use\nsplit_file: \"path\" #labels file path\nroot_dir: \"path\" #path to pose data\ntransforms: #train augmentations\n- RotatationTransform:\nrotation_std: 0.1 #params for each transform\ndataloader: #dataloader parameters\nbatch_size: 32\nshuffle: true\nmodel: #model parameters\nencoder:\ntype: \"encoder-to-use\"\nparams: ... #encoder parameters\ndecoder:\ntype: \"decoder-to-use\"\noptim: #optimizer and loss params\nloss: \"CrossEntropyLoss\"\noptimizer:\nname: Adam\nlr: 1e-3\ntrainer: #training settings\ngpus: 1\nmax_epochs: 100\nexp_manager: #logging and checkpointing\ncreate_tensorboard_logger: true #tensorboard logging\ncreate_checkpoint_callback: true\nearly_stopping_callback: false\nFigure 11: Example\n OpenHands config.\n2129\nDatasheet for Raw Indian SL corpus\nThis is the detailed datasheet, including ethical\nconsiderations, of the unlabelled pretraining\ndataset proposed in Section 4.1.\nMotivation For Datasheet Creation\nWhy was the datasheet created? (e.g., was there\na specific task in mind? was there a specific gap\nthat needed to be filled?)\nThere were no large-scale unlabelled datasets avail-\nable for experimenting with self-supervised learn-\ning for sign languages, like we have for NLP, eg.\nbookcorpus (Zhu et al., 2015) and Common-Crawl\n(Abadji et al., 2022). Like in NLP, it is expected\nthat such a dataset may help reduce the need for\nlabelled dataset. This dataset was collected with\na specific focus on Indian Sign Language (ISL)\nwhich has limited labelled resources.\nWho created the dataset (e.g., which team, re-\nsearch group) and on behalf of which entity (e.g.,\ncompany, institution, organization)?\nThe dataset was programatically created by crawl-\ning, cleaning, and preprocessing video data by the\nmain authors of this paper, who are researchers at\nAI4Bharat group of IIT Madras.\nWho funded the creation dataset?\nThe work was funded by Microsoft Philanthropies\nIndia through Microsoft AI4Accessibility program,\nvia AI4Bharat.\nDatasheet Composition\nWhat are the instances?(that is, examples; e.g.,\ndocuments, images, people, countries) Are there\nmultiple types of instances? (e.g., movies, users,\nratings; people, interactions between them;\nnodes, edges)\nAn instance in our dataset is a bundle of sequence\nof pose keypoints extracted from the videos of\na specific source, in HDF5 format. The original\nvideos are not part of the dataset.\nHow many instances are there in total (of each\ntype, if appropriate)? The dataset consists of\npose keypoints from 7 YouTube channels. Hence\nthere are 7 instances in total.\nWhat data does each instance consist of ? “Raw”\ndata (e.g., unprocessed text or images)? Fea-\ntures/attributes?\nEach instance contains the raw pose keypoints data\n(i.e., without any label data) extracted from the\nvideos of a specific YouTube channel. The features\nextracted are obtained directly from MediaPipe\nHolistic tool (Grishchenko and Bazarevsky, 2020),\nwhich provides human skeletons for any given set\nof frames with a person. Our video sources are\nfrom news and educational domains. Around 87%\nof the total data is from 3 news channels. The\nEducation domain channels are National Institute\nof Open Schooling, an intiative by Government of\nIndia and SIGN Library channel, an initiative to\nmake educational content in Indian SL.\nIs there a label or target associated with each\ninstance? If so, please provide a description.\nNo, this is an unlabeled raw dataset used for self-\nsupervised pretraining.\nIs any information missing from individual in-\nstances? If so, please provide a description, ex-\nplaining why this information is missing (e.g.,\nbecause it was unavailable). This does not in-\nclude intentionally removed information, but\nmight include, e.g., redacted text.\nWe are releasing only the pose keypoints derived\nfrom the videos, and not the videos. For repro-\nducibility, we also provide the YouTube video\nURLs of the corresponding pose keypoints.\nAre relationships between individual instances\nmade explicit (e.g., users’ movie ratings, social\nnetwork links)? If so, please describe how these\nrelationships are made explicit.\nAll instances are independent of each other and are\nnot linked directly in anyway.\nAre there recommended data splits (e.g., train-\ning, development/validation, testing)? If so,\nplease provide a description of these splits, ex-\nplaining the rationale behind them.\nNo. We recommend to use the entire raw data\nfor pretraining purposes, and as a validation set\nto compare losses and perform early stopping, we\nrecommend to use the open-sourced INCLUDE\ndataset (Sridhar et al., 2020). In case if a researcher\nwants to split the raw data to derive their own train-\ntest split, we recommend them to use the data from\n\"SIGN Library\" source for test/development set\nand the remaining for training.\nAre there any errors, sources of noise, or redun-\ndancies in the dataset? If so, please provide a\n2130\ndescription.\nWe did our best to ensure there are no redundan-\ncies by crawling videos with unique video IDs and\ntitles. There are no labels so there is no error due\nto labelling. One source of error could be inaccu-\nracy in pose extraction by the MediaPipe library.\nWe did not manually evaluate this accuracy across\nall datasets, but in manual checks we found Medi-\naPipe to be highly accurate especially since most\nvideos consist of one prominently featured signer\nwith limited or no occlusion.\nIs the dataset self-contained, or does it link to or\notherwise rely on external resources (e.g., web-\nsites, tweets, other datasets)? If it links to or\nrelies on external resources, a) are there guar-\nantees that they will exist, and remain constant,\nover time; b) are there official archival versions\nof the complete dataset (i.e., including the ex-\nternal resources as they existed at the time the\ndataset was created); c) are there any restric-\ntions (e.g., licenses, fees) associated with any of\nthe external resources that might apply to a fu-\nture user? Please provide descriptions of all\nexternal resources and any restrictions associ-\nated with them, as well as links or other access\npoints, as appropriate.\nThe dataset we release is self-contained as used for\npretraining in this work. As discussed, to recreate\nor process the original videos, we provide links to\nthe original videos. However, as this data is hosted\non YouTube with rights owned by the respective\nvideo creators, these videos may not be available\nindefinitely.\nCollection Process\nWhat mechanisms or procedures were used to\ncollect the data (e.g., hardware apparatus or sen-\nsor, manual human curation, software program,\nsoftware API)? How were these mechanisms or\nprocedures validated?\nThe data collection pipeline is explained in Sec-\ntion 4 of the main paper. In summary, videos\nwere automatically crawled from the web to col-\nlect openly available resources under permissible\nlicenses. These videos were then processed with\nMediaPipe to obtain pose data as explained in the\nmain paper.\nHow was the data associated with each instance\nacquired? Was the data directly observable\n(e.g., raw text, movie ratings), reported by sub-\njects (e.g., survey responses), or indirectly in-\nferred/derived from other data (e.g., part-of-\nspeech tags, model-based guesses for age or lan-\nguage)? If data was reported by subjects or indi-\nrectly inferred/derived from other data, was the\ndata validated/verified? If so, please describe\nhow.\nThe (pose) data was derived from the crawled\nvideos without any human intervention as stated\nabove. The automatic validation/cleaning of the\ndataset is explained in the main paper’s section 4.\nIf the dataset is a sample from a larger set, what\nwas the sampling strategy (e.g., deterministic,\nprobabilistic with specific sampling probabili-\nties)?\nWe release all good quality data in the dataset after\na very minimal cleaning process described in the\nmain paper. There was no subjective sampling of\nthe data.\nWho was involved in the data collection process\n(e.g., students, crowdworkers, contractors) and\nhow were they compensated (e.g., how much\nwere crowdworkers paid)?\nThe code for the automatic crawling and cleaning\nprocesses were written by full-time researchers at\nAI4Bharat (authors of paper), with some help from\na volunteer (a full-time student), who has been\nthanked in the acknowledgements section.\nOver what timeframe was the data collected?\nDoes this timeframe match the creation time-\nframe of the data associated with the instances\n(e.g., recent crawl of old news articles)? If not,\nplease describe the timeframe in which the data\nassociated with the instances was created.\nThe crawling was done in the month of June 2021\nto include all the videos from the YouTube channels\ntill then. It took over a month to crawl the videos.\nData Preprocessing\nWas any preprocessing/cleaning/labeling of the\ndata done (e.g., discretization or bucketing, tok-\nenization, part-of-speech tagging, SIFT feature\nextraction, removal of instances, processing of\nmissing values)? If so, please provide a descrip-\ntion. If not, you may skip the remainder of the\nquestions in this section.\nThe steps and pipeline used to create the dataset is\nexplained in section 4 of the main paper. No further\npreprocessing is done before releasing. In addition,\nthe augmentations and normalization performed\n2131\nfor training different models are explained in the\npaper.\nDataset Distribution\nHow will the dataset be distributed? (e.g., tar-\nball on website, API, GitHub; does the data have\na DOI and is it archived redundantly?)\nThe dataset is released as zipped HDF5 files, one\nzip for each YouTube channel, and available via\nZenodo hosting platform. The dataset has an DOI\nwhich is cited in the Acknowledgements section. A\nmirror link to the dataset can be availed on request\n(in-case the platform is down or other issues).\nWhen will the dataset be released/first dis-\ntributed? What license (if any) is it distributed\nunder?\nThe dataset is released along with the camera-ready\nversion of the paper submitted finally to the confer-\nence. It is licensed under the Creative Commons\nAttribution 4.0 International license.\nAre there any copyrights on the data?\nThe original videos are the copyright of the respec-\ntive YouTube channels, and are not released. We\nonly release the pose data with no Personally Iden-\ntifiable Information (PII).\nAre there any fees or access/export restrictions?\nNo.\nDataset Maintenance\nWho is supporting/hosting/maintaining the\ndataset?\nThe dataset is being hosted at Zenodo, an open-\naccess repository to store and distribute scien-\ntific artifacts. The dataset is being maintained by\nAI4Bharat, a research lab in the CSE department\nof IIT Madras, India.\nWill the dataset be updated? If so, how often\nand by whom?\nIf there are any errors found or if any required data\nis missing, we take responsibility to update/rectify\nthe same.\nHow will updates be communicated? (e.g., mail-\ning list, GitHub)\nIt would be conveyed via the changelogs in the\nGitHub repository.\nIf the dataset becomes obsolete how will this be\ncommunicated?\nWe do not expect this to happen. But in such a rare\ncase, it will be notified in the GitHub repository as\nan important note.\nIs there a repository to link to any/all pa-\npers/systems that use this dataset?\nAll research works that use our dataset are re-\nquested to cite this paper. If this is followed, by\nviewing the list of citations for this paper (for ex-\nample on Google Scholar) one could track all pa-\npers/systems using the dataset.\nIf others want to extend/augment/build on this\ndataset, is there a mechanism for them to do so?\nIf so, is there a process for tracking/assessing\nthe quality of those contributions. What is the\nprocess for communicating/distributing these\ncontributions to users?\nWe would greatly appreciate others adding to the\ndataset - hence the name of the hosting repository\nis\n OpenHands. Those intending to extend the\ndataset can contact us on our email addresses or\non the Github repository. Quality of the extended\ndataset would be measured by performance of sign\nlanguage recognition systems built with the dataset.\nLegal and Ethical Considerations\nDoes the dataset contain data that might be con-\nsidered confidential (e.g., data that is protected\nby legal privilege or by doctorpatient confiden-\ntiality, data that includes the content of individ-\nuals non-public communications)? If so, please\nprovide a description.\nNo, the dataset does not contain any confidential or\npersonal data.\nDoes the dataset contain data that, if viewed di-\nrectly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? If so, please\ndescribe why\nNo, the dataset does not contain any offensive or\ninappropriate data. No audio/speech/image data is\nincluded in the data.\nDoes the dataset relate to people? If not, you\nmay skip the remaining questions in this sec-\ntion.\nYes, but the dataset has only the skeleton informa-\ntion of signers without any PII.\nDoes the dataset identify any subpopulations\n(e.g., by age, gender)? If so, please describe how\nthese subpopulations are identified and provide\n2132\na description of their respective distributions\nwithin the dataset.\nNo, it does not identify any subpopulations.\nIs it possible to identify individuals (i.e., one or\nmore natural persons), either directly or indi-\nrectly (i.e., in combination with other data) from\nthe dataset? If so, please describe how.\nNo, it is not possible to identify the individuals be-\nhind the dataset, because the data released contains\nonly the pose points of the signers.\nDoes the dataset contain data that might be\nconsidered sensitive in any way (e.g., data that\nreveals racial or ethnic origins, sexual orien-\ntations, religious beliefs, political opinions or\nunion memberships, or locations; financial or\nhealth data; biometric or genetic data; forms of\ngovernment identification, such as social secu-\nrity numbers; criminal history)? If so, please\nprovide a description.\nNo, the dataset does not include any sensitive data.\nDid you collect the data from the individuals in\nquestion directly, or obtain it via third parties\nor other sources (e.g., websites)?\nWe obtain it via publicly available YouTube chan-\nnels released by the respective groups, and not from\nany individuals.\n2133",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.826440691947937
    },
    {
      "name": "Sign language",
      "score": 0.6762450337409973
    },
    {
      "name": "Sign (mathematics)",
      "score": 0.6250979900360107
    },
    {
      "name": "Natural language processing",
      "score": 0.6060823202133179
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5721259117126465
    },
    {
      "name": "Inference",
      "score": 0.5343429446220398
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.4736562669277191
    },
    {
      "name": "Key (lock)",
      "score": 0.47085040807724
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.4272350072860718
    },
    {
      "name": "Word (group theory)",
      "score": 0.4217497408390045
    },
    {
      "name": "Linguistics",
      "score": 0.1732649803161621
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24676775",
      "name": "Indian Institute of Technology Madras",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}