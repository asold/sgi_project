{
    "title": "Intriguing Properties of Vision Transformers",
    "url": "https://openalex.org/W3164024107",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4227314091",
            "name": "Naseer, Muzammal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227314090",
            "name": "Ranasinghe, Kanchana",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2483644475",
            "name": "Khan, Salman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222938126",
            "name": "Hayat, Munawar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202207810",
            "name": "khan, Fahad Shahbaz",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4200861175",
            "name": "Yang, Ming-Hsuan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3202406646",
        "https://openalex.org/W2039313011",
        "https://openalex.org/W2533598788",
        "https://openalex.org/W2073982870",
        "https://openalex.org/W2047643928",
        "https://openalex.org/W2964153729",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W1849277567",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3135159465",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2964253222",
        "https://openalex.org/W3145185940",
        "https://openalex.org/W2062118960",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3126994093",
        "https://openalex.org/W2002427601",
        "https://openalex.org/W14333344",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2963060032",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2763549966",
        "https://openalex.org/W3179869055",
        "https://openalex.org/W2919053476",
        "https://openalex.org/W2952337489",
        "https://openalex.org/W2990289029",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3122874210",
        "https://openalex.org/W3162316477",
        "https://openalex.org/W2732026016",
        "https://openalex.org/W3142085127",
        "https://openalex.org/W2994931756",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W1846799578",
        "https://openalex.org/W3153906112",
        "https://openalex.org/W3101298150",
        "https://openalex.org/W2943845043",
        "https://openalex.org/W2797977484",
        "https://openalex.org/W2952610664",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W3108975329"
    ],
    "abstract": "Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.",
    "full_text": "Intriguing Properties of Vision Transformers\nMuzammal Naseer†⋆ Kanchana Ranasinghe+⋆ Salman Khan⋆†\nMunawar Hayat¶ Fahad Shahbaz Khan⋆§ Ming-Hsuan Yang‡◦∇\n†Australian National University, ⋆Mohamed bin Zayed University of AI, +Stony Brook University,\n¶Monash University, §Linköping University, ‡University of California, Merced,\n◦Yonsei University,∇Google Research\nmuzammal.naseer@anu.edu.au\nAbstract\nVision transformers (ViT) have demonstrated impressive performance across nu-\nmerous machine vision tasks. These models are based on multi-head self-attention\nmechanisms that can ﬂexibly attend to a sequence of image patches to encode con-\ntextual cues. An important question is how such ﬂexibility (in attending image-wide\ncontext conditioned on a given patch) can facilitate handling nuisances in natural\nimages e.g., severe occlusions, domain shifts, spatial permutations, adversarial and\nnatural perturbations. We systematically study this question via an extensive set\nof experiments encompassing three ViT families and provide comparisons with\na high-performing convolutional neural network (CNN). We show and analyze\nthe following intriguing properties of ViT: (a) Transformers are highly robust\nto severe occlusions, perturbations and domain shifts, e.g., retain as high as 60%\ntop-1 accuracy on ImageNet even after randomly occluding 80% of the image\ncontent. (b) The robustness towards occlusions is not due to texture bias, instead\nwe show that ViTs are signiﬁcantly less biased towards local textures, compared to\nCNNs. When properly trained to encode shape-based features, ViTs demonstrate\nshape recognition capability comparable to that of human visual system, previously\nunmatched in the literature. (c) Using ViTs to encode shape representation leads to\nan interesting consequence of accurate semantic segmentation without pixel-level\nsupervision. (d) Off-the-shelf features from a single ViT model can be combined\nto create a feature ensemble, leading to high accuracy rates across a range of\nclassiﬁcation datasets in both traditional and few-shot learning paradigms. We\nshow effective features of ViTs are due to ﬂexible and dynamic receptive ﬁelds\npossible via self-attention mechanisms. Code: https://git.io/Js15X.\n1 Introduction\nAs visual transformers (ViT) attract more interest [1], it becomes highly pertinent to study character-\nistics of their learned representations. Speciﬁcally, from the perspective of safety-critical applications\nsuch as autonomous cars, robots and healthcare; the learned representations need to be robust and\ngeneralizable. In this paper, we compare the performance of transformers with convolutional neural\nnetworks (CNNs) for handling nuisances (e.g., occlusions, distributional shifts, adversarial and natural\nperturbations) and generalization across different data distributions. Our in-depth analysis is based on\nthree transformer families, ViT [2], DeiT [3] and T2T [4] across ﬁfteen vision datasets. For brevity,\nwe refer to all the transformer families as ViT, unless otherwise mentioned.\nWe are intrigued by the fundamental differences in the operation of convolution and self-attention,\nthat have not been extensively explored in the context of robustness and generalization. While\nconvolutions excel at learning local interactions between elements in the input domain (e.g., edges\nand contour information), self-attention has been shown to effectively learn global interactions (e.g.,\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2105.10497v3  [cs.CV]  25 Nov 2021\nFigure 1: We show intriguing properties of ViT including impressive robustness to(a) severe occlusions, (b)\ndistributional shifts (e.g., stylization to remove texture cues), (c) adversarial perturbations, and (d) patch\npermutations. Furthermore, our ViT models trained to focus on shape cues can segment foregrounds without any\npixel-level supervision (e). Finally, off-the-shelf features from ViT models generalize better than CNNs (f).\nrelations between distant object parts) [ 5, 6]. Given a query embedding, self-attention ﬁnds its\ninteractions with the other embeddings in the sequence, thereby conditioning on the local content\nwhile modeling global relationships [7]. In contrast, convolutions are content-independent as the same\nﬁlter weights are applied to all inputs regardless of their distinct nature. Given the content-dependent\nlong-range interaction modeling capabilities, our analysis shows that ViTs can ﬂexibly adjust their\nreceptive ﬁeld to cope with nuisances in data and enhance expressivity of the representations.\nOur systematic experiments and novel design choices lead to the following interesting ﬁndings:\n• ViTs demonstrate strong robustness against severe occlusions for foreground objects, non-salient\nbackground regions and random patch locations, when compared with state-of-the-art CNNs.\nFor instance, with a signiﬁcant random occlusion of up to 80%, DeiT [3] can maintain top-1\naccuracy up to ∼60% where CNN has zero accuracy, on ImageNet [8] val. set.\n• When presented with texture and shape of the same object, CNN models often make decisions\nbased on texture [9]. In contrast, ViTs perform better than CNNs and comparable to humans on\nshape recognition. This highlights robustness of ViTs to deal with signiﬁcant distribution shifts\ne.g., recognizing object shapes in less textured data such as paintings.\n• Compared to CNNs, ViTs show better robustness against other nuisance factors such as spatial\npatch-level permutations, adversarial perturbations and common natural corruptions (e.g., noise,\nblur, contrast and pixelation artefacts). However, similar to CNNs [10], a shape-focused training\nprocess renders them vulnerable against adversarial attacks and common corruptions.\n• Apart from their promising robustness properties, off-the-shelf ViT features from ImageNet\npretrained models generalize exceptionally well to new domains e.g., few-shot learning, ﬁne-\ngrained recognition, scene categorization and long-tail classiﬁcation settings.\nIn addition to our extensive experimental analysis and new ﬁndings, we introduce several novel\ndesign choices to highlight the strong potential of ViTs. To this end, we propose an architectural\nmodiﬁcation to DeiT to encode shape-information via a dedicated token that demonstrates how\nseemingly contradictory cues can be modeled with distinct tokens within the same architecture,\nleading to favorable implications such as automated segmentation without pixel-level supervision.\nMoreover, our off-the-shelf feature transfer approach utilizes an ensemble of representations derived\nfrom a single architecture to obtain state-of-the-art generalization with a pre-trained ViT (Fig. 1).\n2 Related Work\nCNNs have shown state-of-the-art performance in independent and identically distributed (i.i.d)\nsettings but remain highly sensitive to distributional shifts; adversarial noise [11, 12], common image\ncorruptions [13], and domain shifts ( e.g., RGB to sketches) [ 14]. It is natural to ask if ViT, that\nprocesses inputs based on self-attention, offers any advantages in comparison to CNN. Shao et\nal. [ 15] analyze ViTs against adversarial noise and show ViTs are more robust to high frequency\nchanges. Similarly, Bhojanapalli et al. [ 16] study ViT against spatial perturbations [ 15] and its\nrobustness to removal of any single layer. Since ViT processes image patches, we focus on their\nrobustness against patch masking, localized adversarial patches [17] and common natural corruptions.\nA concurrent work from Paul and Chen [18] also develops similar insights on robustness of ViTs but\nwith a somewhat different set of experiments.\nGeirhos et al. [9] provide evidence that CNNs mainly exploit texture to make a decision and give\nless importance to global shape. This is further conﬁrmed by CNN ability to only use local features\n2\n[19]. Recently, [ 20] quantiﬁes mutual information [ 21] between shape and texture features. Our\nanalysis indicates that large ViT models have less texture bias and give relatively higher emphasis to\nshape information. ViT’s shape-bias approaches human-level performance when directly trained on\nstylized ImageNet [9]. Our ﬁndings are consistent with a concurrent recent work that demonstrates\nthe importance of this trend on human behavioural understanding and bridging the gap between\nhuman and machine vision [22]. A recent work [23] shows that self-supervised ViT can automatically\nsegment foreground objects. In comparison, we show how shape-focused learning can impart similar\ncapability in the image-level supervised ViT models, without any pixel-level supervision.\nZeiler et al. [ 24] introduce a method to visualize CNN features at different layers and study the\nperformance of off-the-shelf features. In a similar spirit, we study the generalization of off-the-shelf\nfeatures of ViT in comparison to CNN. Receptive ﬁeld is an indication of network’s ability to model\nlong range dependencies. The receptive ﬁeld of Transformer based models covers the entire input\nspace, a property that resembles handcrafted features [ 25], but ViTs have higher representative\ncapacity. This allows ViT to model global context and preserve the structural information compared\nto CNN [26]. This work is an effort to demonstrate the effectiveness of ﬂexible receptive ﬁeld and\ncontent-based context modeling in ViTs towards robustness and generalization of the learned features.\n3 Intriguing Properties of Vision Transformers\n3.1 Are Vision Transformers Robust to Occlusions?\nThe receptive ﬁeld of a ViT spans over the entire image and it models the interaction between the\nsequence of image patches using self-attention [26, 27]. We study whether ViTs perform robustly in\noccluded scenarios, where some or most of the image content is missing.\nOcclusion Modeling: Consider a network f , that processes an input image x to predict a label y,\nwhere x is represented as a patch sequence with N elements, i.e., x = {xi}N\ni=1 [2]. While there can\nbe multiple ways to deﬁne occlusion, we adopt a simple masking strategy, where we select a subset of\nthe total image patches, M <N, and set pixel values of these patches to zero to create an occluded\nimage, x′. We refer to this approach as PatchDrop. The objective is then to observe robustness such\nthat f (x′)argmax = y. We experiment with three variants of our occlusion approach, (a) Random\nPatchDrop, (b) Salient (foreground) PatchDrop, and (c) Non-salient (background) PatchDrop.\nRandom PatchDrop: A subset of M patches is randomly selected and dropped (Fig. 2). Several recent\nVision Transformers [2, 3, 4] divide an image into 196 patches belonging to a 14x14 spatial grid;\ni.e. an image of size 224×224×3 is split into 196 patches, each of size 16×16×3. As an example,\ndropping 100 such patches from the input is equivalent to losing 51% of the image content.\nSalient (foreground) PatchDrop:Not all pixels have the same importance for vision tasks. Thus, it is\nimportant to study the robustness of ViTs against occlusions of highly salient regions. We leverage\na self-supervised ViT model DINO [ 23] that is shown to effectively segment salient objects. In\nparticular, the spatial positions of information ﬂowing into the ﬁnal feature vector (class token) within\nthe last attention block are exploited to locate the salient pixels. This allows to control the amount of\nsalient information captured within the selected pixels by thresholding the quantity of attention ﬂow.\nWe select the subset of patches containing the top Q% of foreground information (deterministic for\nﬁxed Q) and drop them. Note that this Q% does not always correspond to the pixel percentage, e.g.,\n50% of the foreground information of an image may be contained within only 10% of its pixels.\nNon-salient (background) PatchDrop:The least salient regions of the image are selected following the\nsame approach as above, using [23]. The patches containing the lowestQ% of foreground information\nare selected and dropped here. Note this does not always correspond to the pixel percentage, e.g.,\n80% of the pixels may only contain 20% of the non-salient information for an image.\nFigure 2: An example image with its oc-\ncluded versions (Random, Salient and Non-\nSalient). The occluded images are cor-\nrectly classiﬁed by Deit-S [ 3] but mis-\nclassiﬁed by ResNet50 [28]. Pixel values\nin occluded (black) regions are set to zero.\nOriginal Image\n Random PatchDrop\n Salient PatchDrop\n Non-Salient PatchDrop\n3\nFigure 3: Robustness against object occlusion in images is studied under three PatchDrop settings (see Sec 3.1).\n(left) We study the robustness of CNN models to occlusions, and identify ResNet50 as a strong baseline.\n(mid-left) We compare the DeiT model family against ResNet50 exhibiting their superior robustness to object\nocclusion. (mid-right) Comparison against ViT model family. (right) Comparison against T2T model family.\nRobust Performance of Transformers Against Occlusions: We consider visual recognition task\nwith models pretrained on ImageNet [2]. The effect of occlusion is studied on the validation set (50k\nimages). We deﬁne information loss (IL) as the ratio of dropped and total patches ( M / N). IL is\nvaried to obtain a range of occlusion levels for each PatchDrop methodology. The results (Top-1 %)\nreported in Fig. 3 show signiﬁcantly robust performance of ViT models against CNNs. In the case\nof random PatchDrop, we report the mean of accuracy across 5 runs. For Salient and Non-Salient\nPatchdrop, we report the accuracy values over a single run, since the occlusion mask is deterministic.\nCNNs perform poorly when 50% of image information is randomly dropped. For example, ResNet50\n(23 Million parameters) achieves 0.1% accuracy in comparison to DeiT-S (22 Million parameters)\nwhich obtains 70% accuracy when 50% of the image content is removed. An extreme example can\nbe observed when 90% of the image information is randomly masked but Deit-B still exhibits 37%\naccuracy. This ﬁnding is consistent among different ViT architectures [2, 3, 4]. Similarly, ViTs show\nsigniﬁcant robustness to the foreground (salient) and background (non-salient) content removal. See\nAppendix A, B, C, D, and E for further results on robustness analysis.\nViT Representations are Robust against Information Loss: In order to better understand model\nbehavior against such occlusions, we visualize the attention (Fig. 4) from each head of different layers.\nWhile initial layers attend to all areas, deeper layers tend to focus more on the leftover information in\nnon-occluded regions of an image. We then study if such changes from initial to deeper layers lead to\ntoken invariance against occlusion which is important for classiﬁcation. We measure the correlation\ncoefﬁcient between features/tokens of original and occluded images by using corr(u,v) =\n∑\ni ˆui ˆvi\nn ,\nwhere ˆui = ui−E[ui]\nσ(ui) , E[·] and σ(·) are mean and standard deviation operations [29]. In our case,\nrandom variables uand vrefer to the feature maps for an original and occluded image deﬁned over\nthe entire ImageNet validation set. In the case of ResNet50, we consider features before the logit\nlayer and for ViT models, class tokens are extracted from the last transformer block. Class tokens\nfrom transformers are signiﬁcantly more robust and do not suffer much information loss as compared\nto ResNet50 features (Table 1). Furthermore, we visualize the correlation coefﬁcient across the\n12 selected superclasses within ImageNet hierarchy and note that the trend holds across different\nclass types, even for relatively small object types such as insects, food items and birds (Fig. 5). See\nAppendix F for attention visualizations and G for the qualitative results.\nGiven the intriguing robustness of transformer models due to dynamic receptive ﬁelds and discrim-\ninability preserving behaviour of the learned tokens, an ensuing question is whether the learned\nrepresentations in ViTs are biased towards texture or not. One can expect a biased model focusing\nonly on texture to still perform well when the spatial structure for an object is partially lost.\n4\nFigure 4: Attention maps (averaged over the entire ImageNet val. set) relevant to each head in multiple\nlayers of an ImageNet pre-trained DeiT-B model. All images are occluded (Random PatchDrop) with\nthe same mask (bottom right). Observe how later layers clearly attend to non-occluded regions of\nimages to make a decision, an evidence of the model’s highly dynamic receptive ﬁeld.\nModel Correlation Coefﬁcient: Random PatchDrop\n25% Dropped 50% Dropped 75% Dropped\nResNet50 0.32±0.16 0.13±0.11 0.07±0.09\nTnT-S 0.83±0.08 0.67±0.12 0.46±0.17\nViT-L 0.92±0.06 0.81±0.13 0.50±0.21\nDeit-B 0.90±0.06 0.77±0.10 0.56±0.15\nT2T-24 0.80±0.10 0.60±0.15 0.31±0.17\nTable 1: Correlation coefﬁcient b/w features/ﬁnal class\ntokens of original and occluded images for Random\nPatchDrop. Averaged across the ImageNet val. set.\nFigure 5: Correlation b/w features/ﬁnal tokens of orig-\ninal and occluded images for 50% Random Drop. Re-\nsults are averaged across classes for each superclass.\n3.2 Shape vs. Texture: Can Transformer Model Both Characteristics?\nGeirhos et al. [9] study shape vs. texture hypothesis and propose a training framework to enhance\nshape-bias in CNNs. We ﬁrst carry out similar analysis and show that ViT models preform with a\nshape-bias much stronger than that of a CNN, and comparably to the ability of human visual system\nin recognizing shapes. However, this approach results in a signiﬁcant drop in accuracy on the natural\nimages. To address this issue, we introduce a shape token into the transformer architecture that\nlearns to focus on shapes, thereby modeling both shape and texture related features within the same\narchitecture using a distinct set of tokens. As such, we distill the shape information from a pretrained\nCNN model with high shape-bias [9]. Our distillation approach makes a balanced trade-off between\nhigh classiﬁcation accuracy and strong shape-bias compared to the original ViT model.\nWe outline both approaches below. Note that the measure introduced in [ 9] is used to quantify\nshape-bias within ViT models and compare against their CNN counterparts.\nTraining without Local Texture: In this approach, we ﬁrst remove local texture cues from the\ntraining data by creating a stylized version of ImageNet [ 9] named SIN. We then train tiny and\nsmall DeiT models [3] on this dataset. Typically, ViTs use heavy data augmentations during training\n[3]. However, learning with SIN is a difﬁcult task due to less texture details and applying further\naugmentations on stylized samples distorts shape information and makes the training unstable. Thus,\nwe train models on SIN without applying any augmentation, label smoothing or mixup.\nWe note that ViTs trained on ImageNet exhibit higher shape-bias in comparison to similar capacity\nCNN models e.g., DeiT-S (22-Million params) performs better than ResNet50 (23-Million params)\n(Fig. 6, right plot). In contrast, the SIN trained ViTs consistently perform better than CNNs.\nInterestingly, DeiT-S [3] reaches human-level performance when trained on a SIN (Fig. 6,left plot).\n5\nFigure 6: Shape-bias Analysis: Shape-bias is deﬁned as\nthe fraction of correct decisions based on object shape.\n(Left) Plot shows shape-texture tradeoff for CNN, ViT\nand Humans across different object classes. (Right) class-\nmean shape-bias comparison. Overall, ViTs perform bet-\nter than CNN. The shape bias increases signiﬁcantly when\ntrained on stylized ImageNet (SIN).\nModel DistilledToken TypeImageNet top-1 (%)Shape Bias\nDeiT-T-SIN \u0017 cls 40.5 0.87\nDeiT-T-SIN \u0013 cls 71.8 0.35\nDeiT-T-SIN \u0013 shape 63.4 0.44\nDeiT-S-SIN \u0017 cls 52.5 0.93\nDeiT-S-SIN \u0013 cls 75.3 0.39\nDeiT-S-SIN \u0013 shape 67.7 0.47\nTable 3: Performance comparison of mod-\nels trained on SIN. ViT produces dynamic\nfeatures that can be controlled by auxiliary\ntokens. ‘cls’ represents the class token.\nDuring distillation cls and shape tokens\nconverged to vastly different solution us-\ning the same features as compared to [3].\nShape Distillation: Knowledge distillation allows to compress large teacher models into smaller\nstudent models [30] as the teacher provides guidance to the student through soft labels. We introduce\na new shape token and adapt attentive distillation [3] to distill shape knowledge from a CNN trained\non the SIN dataset (ResNet50-SIN [9]). We observe that ViT features are dynamic in nature and can\nbe controlled by auxiliary tokens to focus on the desired characteristics. This means that a single ViT\nmodel can exhibit both high shape and texture bias at the same time with separate tokens (Table 3).\nWe achieve more balanced performance for classiﬁcation as well as shape-bias measure when the\nshape token is introduced (Fig. 7). To demonstrate that these distinct tokens (for classiﬁcation and\nshape) indeed model unique features, we compute cosine similarity (averaged over ImageNet val.\nset) between class and shape tokens of our distilled models, DeiT-T-SIN and DeiT-S-SIN, which\nturns out to be 0.35 and 0.68, respectively. This is signiﬁcantly lower than the similarity between\nclass and distillation tokens [3]; 0.96 and 0.94 for DeiT-T and Deit-S, respectively. This conﬁrms our\nhypothesis on modeling distinct features with separate tokens within ViTs, a unique capability that\ncannot be straightforwardly achieved with CNNs. Further, it offers other beneﬁts as we explain next.\nFigure 7: Shape Distillation.\nShape-biased ViT Offers Automated Object Segmen-\ntation: Interestingly, training without local texture or with\nshape distillation allows a ViT to concentrate on fore-\nground objects in the scene and ignore the background\n(Table 4, Fig. 8). This offers an automated semantic seg-\nmentation for an image although the model is never shown\npixel-wise object labels. That is, shape-bias can be used\nas self-supervision signals for the ViT model to learn dis-\ntinct shape-related features that help localize the right\nforeground object. We note that a ViT trained without\nemphasis on shape does not perform well (Table 4).\nThe above results show that properly trained ViT models\noffer shape-bias nearly as high as the human’s ability to recognize shapes. This leads us to question if\npositional encoding is the key that helps ViTs achieve high performance under severe occlusions (as\nit can potentially allow later layers to recover the missing information with just a few image patches\ngiven their spatial ordering). This possibility is examined next.\n6\nModel DistilledToken TypeJaccard Index\nDeiT-T-Random \u0017 cls 19.6\nDeiT-T \u0017 cls 32.2\nDeiT-T-SIN \u0017 cls 29.4\nDeiT-T-SIN \u0013 cls 40.0\nDeiT-T-SIN \u0013 shape 42.2\nDeiT-S-Random \u0017 cls 22.0\nDeiT-S \u0017 cls 29.2\nDeiT-S-SIN \u0017 cls 37.5\nDeiT-S-SIN \u0013 cls 42.0\nDeiT-S-SIN \u0013 shape 42.4\nTable 4: We compute the Jaccard similarity between\nground truth and masks generated from the attention\nmaps of ViT models (similar to [ 23] with threshold\n0.9) over the PASCAL-VOC12 validation set. Only\nclass level ImageNet labels are used for training these\nmodels. Our results indicate that supervised ViTs can\nbe used for automated segmentation and perform closer\nto the self-supervised method DINO [23].\nDeiT-S\n DeiT-S-SIN\n DeiT-S-SIN (Distilled)\nFigure 8: Segmentation maps from ViTs. Shape distil-\nlation performs better than standard supervised models.\n3.3 Does Positional Encoding Preserve the Global Image Context?\nTransformers’ ability to process long-range sequences in parallel using self-attention [27] (instead of\na sequential design in RNN [31]) is invariant to sequence ordering. For images, the order of patches\nrepresents the overall image structure and global composition. Since ViTs operate on a sequence\nof images patches, changing the order of sequence e.g., shufﬂing the patches can destroy the image\nstructure. Current ViTs [ 2, 3, 4, 26] use positional encoding to preserve this context. Here, we\nanalyze if the sequence order modeled by positional encoding allows ViT to excel under occlusion\nhandling. Our analysis suggests that transformers show high permutation invariance to the patch\npositions, and the effect of positional encoding towards injecting structural information of images\nto ViT models is limited (Fig. 10). This observation is consistent with the ﬁndings in the language\ndomain [32] as described below.\nFigure 9: An illustration of shufﬂe operation applied on images used\nto eliminate their structural information. (best viewed zoomed-in)\nSensitivity to Spatial Structure: We\nremove the structural information\nwithin images (spatial relationships)\nas illustrated in Fig. 9 by deﬁning\na shufﬂing operation on input image\npatches. Fig. 10 shows that the DeiT\nmodels [3] retain accuracy better than\ntheir CNN counterparts when spatial\nstructure of input images is disturbed. This also indicates that positional encoding is not absolutely\ncrucial for right classiﬁcation decisions, and the model does not “recover” global image context\nusing the patch sequence information preserved in the positional encodings. Without encoding, the\nViT performs reasonably well and achieves better permutation invariance than a ViT using position\nFigure 10: Models trained on 196 image patches. Top-1 (%) accuracy\nover ImageNet val. set when patches are shufﬂed. Note the performance\npeaks when shufﬂe grid size is equal to the original number of patches\nused during training, since it equals to only changing the position of input\npatch (and not disturbing the patch content).\nFigure 11: DeiT-T [ 3] trained\non different number of image\npatches. Reducing patch size de-\ncreases the overall performance\nbut also increases sensitivity to\nshufﬂe grid size.\n7\nTrained with Augmentations Trained without Augmentation\nDeiT-B DeiT-S DeiT-T T2T-24 TnT-S AugmixResNet50 ResNet50-SIN DeiT-T-SIN DeiT-S-SIN\n48.5 54.6 71.1 49.1 53.1 65.3 76.7 77.3 94.4 84.0\nTable 4: mean Corruption Error (mCE) across common corruptions [13] (lower the better). While ViTs have\nbetter robustness compared to CNNs, training to achieve a higher shape-bias makes both CNNs and ViTs more\nvulnerable to natural distribution shifts. All models trained with augmentations (ViT or CNN) have lower mCE\nin comparison to models trained without augmentations on ImageNet or SIN.\nFigure 12: Robustness against adver-\nsarial patch attack. ViTs even with\nless parameters exhibit a higher ro-\nbustness than CNN. Models trained\non ImageNet are more robust than the\nones trained on SIN. Results are aver-\naged across ﬁve runs of patch attack\nover ImageNet val. set.\nFigure 13: Robustness against sample speciﬁc attacks including single step, FGSM [34], and multi-step, PGD\n[35]. ViTs even with less parameters exhibit a higher robustness than CNN. PGD ran for 5 iterations only.\nAttacks are evaluated under l∞norm and ϵrepresents the perturbation budget by which each pixel is changed in\nthe input image. Results are reported over the ImageNet val. set.\nencoding (Fig. 10). Finally, when the patch size is varied during ViT training, the permutation\ninvariance property is also degraded along with the accuracy on unshufﬂed natural images (Fig. 11).\nOverall, we attribute the permutation invariance performance of ViTs to their dynamic receptive ﬁeld\nthat depends on the input patch and can adjust attention with the other sequence elements such that\nmoderately shufﬂing the elements does not degrade the performance signiﬁcantly.\nThe above analysis shows that just like the texture-bias hypothesis does not apply to ViTs, the\ndependence on positional encodings to perform well under occlusions is also incorrect. This leads\nus to the conclude that ViTs robustness is due to its ﬂexible and dynamic receptive ﬁeld (see Fig. 4)\nwhich depends on the content of an input image. We now delve further deep into the robustness of\nViT, and study its performance under adversarial perturbations and common corruptions.\n3.4 Robustness of Vision Transformers to Adversarial and Natural Perturbations\nAfter analyzing the ability of ViTs to encode shape information (Sec. 3.2), one ensuing question is:\nDoes higher shape-bias help achieve better robustness? In Table 4, we investigate this by calculating\nmean corruption error (mCE) [13] on a variety of synthetic common corruptions (e.g., rain, fog, snow\nand noise). A ViT with similar parameters as CNN (e.g., DeiT-S) is more robust to image corruptions\nthan ResNet50 trained with augmentations (Augmix [ 33]). Interestingly, CNNs and ViTs trained\nwithout augmentations on ImageNet or SIN are more vulnerable to corruptions. These ﬁndings are\nconsistent with [10], and suggest that augmentations improve robustness against common corruptions.\nWe observe similar performance against untargeted, universal adversarial patch attack [ 17] and\nsample speciﬁc attacks including single step, fast gradient sign method (FGSM) [34], and multi-step\nprojected gradient attack known as PGD [35]. Adversarial patch attack [17] is unbounded that is it\ncan change pixel values at certain location in the input image by any amount, while sample speciﬁc\nattacks [34, 35] are bounded by l∞norm with a perturbation budget ϵ, where ϵrepresents the amount\nby which each pixel is changed in the entire image. ViTs and CNN trained on SIN are signiﬁcantly\n8\nFigure 14: A single ViT model can provide a features en-\nsemble since class token from each block can be processed\nby the classiﬁer independently. This allows us to identify\nthe most discriminative tokens useful for transfer learning.\nFigure 15: Top-1 (%) for ImageNet val. set for\nclass tokens produced by each ViT block. Class to-\nkens from the last few layers exhibit highest perfor-\nmance indicating the most discriminative tokens.\nBlocks Class Patch CUB Flowers iNaturalist\nTokens Tokens [37] [38] [39]\nOnly 12th (last block) \u0013 \u0017 68.16 82.58 38.28\n\u0013 \u0013 70.66 86.58 41.22\nFrom 1st to 12th \u0013 \u0017 72.90 91.38 44.03\n\u0013 \u0013 73.16 91.27 43.33\nFrom 9th to 12th \u0013 \u0017 73.58 90.00 45.15\n\u0013 \u0013 73.37 90.33 45.12\nTable 5: Ablative Study for off-the-shelf fea-\nture transfer on three datasets using ImageNet\npretrained DeiT-S [3]. A linear classiﬁer is\nlearned on only a concatenation of class to-\nkens or the combination of class and averaged\npatch tokens at various blocks. We note class\ntoken from blocks 9-12 are most discrimina-\ntive (Fig. 15) and have the highest transfer-\nability in terms of Top-1 (%) accuracy.\nmore vulnerable to adversarial attack than models trained on ImageNet (Figs. 12 and 13), due to the\nshape-bias vs. robustness trade-off [10].\nGiven the strong robustness properties of ViT as well as their representation capability in terms\nof shape-bias, automated segmentation and ﬂexible receptive ﬁeld, we analyze their utility as an\noff-the-shelf feature extractor to replace CNNs as the default feature extraction mechanism [36].\n3.5 Effective Off-the-shelf Tokens for Vision Transformer\nA unique characteristic of ViT models is that each block within the model generates a class token\nwhich can be processed by the classiﬁcation head separately (Fig. 14). This allows us to measure\nthe discriminative ability of each individual block of an ImageNet pre-trained ViT as shown in Fig.\n15. Class tokens generated by the deeper blocks are more discriminative and we use this insight to\nidentify an effective ensemble of blocks whose tokens have the best downstream transferability.\nTransfer Methodology: As illustrated in Fig. 15, we analyze the block-wise classiﬁcation accuracy\nof DeiT models and determine the discriminative information is captured within the class tokens\nof the last few blocks. As such, we conduct an ablation study for off-the-shelf transfer learning\non ﬁne-grained classiﬁcation dataset CUB [37], Flowers [38] and large scale iNaturalist [39] using\nDeiT-S [3] as reported in Table 5. Here, we concatenate the class tokens (optionally combined with\naverage patch tokens) from different blocks and train a linear classiﬁer to transfer the features to\ndownstream tasks. Note that a patch token is generated by averaging along the patch dimension.\nThe scheme that concatenate class tokens from the last four blocks shows the best transfer learning\nperformance. We refer to this transfer methodology as DeiT-S (ensemble). Concatenation of both\nclass and averaged patch tokens from all blocks helps achieve similar performance compared to\nthe tokens from the last four blocks but requires signiﬁcantly large parameters to train. We ﬁnd\nsome exception to this on the Flower dataset [ 38] where using class tokens from all blocks have\nrelatively better improvement (only 1.2%), compared to the class tokens from the last four blocks\n(Table 5). However, concatenating tokens from all blocks also increases the number of parameters\ne.g., transfer to Flowers from all tokens has 3 times more learnable parameters than using only the\nlast four tokens. We conduct further experimentation with DeiT-S (ensemble) across a broader range\nof tasks to validate our hypothesis. We further compare against a pre-trained ResNet50 baseline, by\nusing features before the logit layer.\nVisual Classification: We analyze the transferability of off-the-shelf features across several\ndatasets including Aircraft [40], CUB [37], DTD [41], GTSRB [42], Fungi [43], Places365 [44] and\niNaturalist [39]. These datasets are developed for ﬁne-grained recognition, texture classiﬁcation,\n9\nFigure 16: Off-the-shelf ViT features transfer better than CNNs. We explore transferability of learned repre-\nsentations using generic classiﬁcation as well as few-shot classiﬁcation for out-of-domain tasks. In the case of\nclassiﬁcation (left), the ImageNet pre-trained ViTs transfer better than their CNN counterparts across tasks. In\nthe case of few-shot learning (right), ImageNet pre-trained ViTs perform better on average.\ntrafﬁc sign recognition, species classiﬁcation and scene recognition with 100, 200, 47, 43, 1394, 365\nand 1010 classes respectively. We train a linear classiﬁer on top of the extracted features over the train\nsplit of each dataset, and evaluate the performance on their respective test splits. The ViT features\nshow clear improvements over the CNN baseline (Fig. 16). We note that DeiT-T, which requires\nabout 5 times fewer parameters than ResNet50, performs better among all datasets. Furthermore, the\nmodel with the proposed ensemble strategy achieves the best results across all datasets.\nFew-Shot Learning: We consider meta-dataset [45] designed as a large-scale few-shot learning\n(FSL) benchmark containing a diverse set of datasets from multiple domains. This includes letters of\nalphabets, hand-drawn sketches, images of textures, and ﬁne-grained classes making it a challenging\ndataset involving a domain adaption requirement as well. We follow the standard setting of training\non ImageNet and testing on all other datasets which are considered as the downstream tasks.\nIn our experiments, we use a network pre-trained for classiﬁcation on ImageNet dataset to extract\nfeatures. For each downstream dataset, under the FSL setting, a support set of labelled images is\navailable for every test query. We use the extracted features to learn a linear classiﬁer over the support\nset for each query (similar to [ 46]), and evaluate using the standard FSL protocol deﬁned in [ 45].\nThis evaluation involves a varying number of shots speciﬁc for each downstream dataset. On average,\nthe ViT features transfer better across these diverse domains (Fig. 16) in comparison to the CNN\nbaseline. Furthermore, we note that the transfer performance of ViT is further boosted using the\nproposed ensemble strategy. We also highlight the improvement in QuickDraw, a dataset containing\nhand-drawn sketches, which aligns with our ﬁndings on improved shape-bias of ViT models in\ncontrast to CNN models (see Sec. 3.2 for elaborate discussion).\n4 Discussion and Conclusions\nIn this paper, we analyze intriguing properties of ViTs in terms of robustness and generalizability.\nWe test with a variety of ViT models on ﬁfteen vision datasets. All the models are trained on 4 V100\nGPUs. We demonstrate favorable merits of ViTs over CNNs for occlusion handling, robustness to\ndistributional shifts and patch permutations, automatic segmentation without pixel supervision, and\nrobustness against adversarial patches, sample speciﬁc adversarial attacks and common corruptions.\nMoreover, we demonstrate strong transferability of off-the-shelf ViT features to a number of down-\nstream tasks with the proposed feature ensemble from a single ViT model. An interesting future\nresearch direction is to explore how the diverse range of cues modeled within a single ViT using\nseparate tokens can be effectively combined to complement each other. Similarly, we found that\nViTs auto-segmentation property stems from their ability to encode shape information. We believe\nthat integrating our approach and DINO [23] is worth exploring in the future. To highlight few open\nresearch questions: a) Can self-supervision on stylized ImageNet (SIN) improve segmentation ability\nof DINO?, and b) Can a modiﬁed DINO training scheme with texture (IN) based local views and\nshape (SIN) based global views enhance (and generalize) its auto-segmentation capability?\nOur current set of experiments are based on ImageNet (ILSVRC’12) pre-trained ViTs, which pose\nthe risk of reﬂecting potential biases in the learned representations. The data is mostly Western, and\nencodes several gender/ethnicity stereotypes with under-representation of certain groups [47]. This\nversion of the ImageNet also poses privacy risks, due to the unblurred human faces. In future, we\nwill use a recent ImageNet version which addresses the above issues [48].\n10\nAcknowledgments\nThis work is supported in part by NSF CAREER grant 1149783, and VR starting grant\n(2016-05543). M. Hayat is supported by Australian Research Council DECRA fellowship\nDE200101100.\nReferences\n[1] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. arXiv preprint arXiv:2101.01169, 2021. 1\n[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 3, 4,\n7\n[3] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 1, 2, 3, 4, 5, 6, 7, 9, 17, 19\n[4] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 1, 3, 4, 7\n[5] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019. 2\n[6] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In\nIEEE Conference on Computer Vision and Pattern Recognition, pages 3464–3473, 2019. 2\n[7] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efﬁcient visual backbones. arXiv preprint\narXiv:2103.12731, 2021. 2\n[8] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211–252, 2015. 2\n[9] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland\nBrendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and\nrobustness. arXiv preprint arXiv:1811.12231, 2018. 2, 3, 5, 6, 18\n[10] Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, Julien Vitay, V olker Fischer,\nand Jan Hendrik Metzen. Does enhanced shape bias improve neural network robustness to common\ncorruptions? In International Conference on Learning Representations, 2021. 2, 8, 9\n[11] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and\nRob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2\n[12] Muzammal Naseer, Salman H Khan, Harris Khan, Fahad Shahbaz Khan, and Fatih Porikli. Cross-domain\ntransferability of adversarial perturbations. Advances in Neural Information Processing Systems, 2019. 2\n[13] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. arXiv preprint arXiv:1903.12261, 2019. 2, 8\n[14] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain\ngeneralization. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5542–5550, 2017.\n2\n[15] Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, and Cho-Jui Hsieh. On the adversarial robustness of\nvisual transformers. arXiv preprint arXiv:2103.15670, 2021. 2\n[16] Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and Andreas\nVeit. Understanding robustness of transformers for image classiﬁcation. arXiv preprint arXiv:2103.14586,\n2021. 2\n[17] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer. Adversarial patch. arXiv\npreprint arXiv:1712.09665, 2017. 2, 8\n[18] Sayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint arXiv:2105.07581,\n2021. 2\n[19] Wieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works\nsurprisingly well on imagenet. arXiv preprint arXiv:1904.00760, 2019. 3\n11\n[20] Md Amirul Islam, Matthew Kowal, Patrick Esser, Sen Jia, Bjorn Ommer, Konstantinos G Derpanis, and Neil\nBruce. Shape or texture: Understanding discriminative features in cnns. arXiv preprint arXiv:2101.11604,\n2021. 3\n[21] David V Foster and Peter Grassberger. Lower bounds on mutual information. Physical Review E ,\n83(1):010101, 2011. 3\n[22] Shikhar Tuli, Ishita Dasgupta, Erin Grant, and Thomas L. Grifﬁths. Are convolutional neural networks or\ntransformers more like human vision? arXiv preprint arXiv:2105.07197, 2021. 3\n[23] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294,\n2021. 3, 7, 10\n[24] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European\nConference on Computer Vision, pages 818–833. Springer, 2014. 3\n[25] Chuan Yang, Lihe Zhang, Huchuan Lu, Xiang Ruan, and Ming-Hsuan Yang. Saliency detection via\ngraph-based manifold ranking. In IEEE Conference on Computer Vision and Pattern Recognition, pages\n3166–3173, 2013. 3\n[26] Yuxin Mao, Jing Zhang, Zhexiong Wan, Yuchao Dai, Aixuan Li, Yunqiu Lv, Xinyu Tian, Deng-Ping Fan,\nand Nick Barnes. Transformer transforms salient object detection and camouﬂaged object detection. arXiv\npreprint arXiv:2104.10127, 2021. 3, 7\n[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 3, 7\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016. 3\n[29] David Forsyth. Probability and statistics for computer science. Springer, 2018. 4\n[30] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015. 6\n[31] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory.Neural Computation, 9(8):1735–1780,\n1997. 7\n[32] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney. Language modeling with deep transformers.\narXiv preprint arXiv:1905.04226, 2019. 7\n[33] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan.\nAugMix: A simple data processing method to improve robustness and uncertainty.International Conference\non Learning Representations, 2020. 8\n[34] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.\nIn International Conference on Learning Representations, 2014. 8\n[35] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-\nwards deep learning models resistant to adversarial attacks. In International Conference on Learning\nRepresentations, 2018. 8\n[36] A. Razavian, Hossein Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: An astounding\nbaseline for recognition. In IEEE Conference on Computer Vision and Pattern Recognition Workshops,\npages 512–519, 2014. 9\n[37] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200.\nTechnical Report CNS-TR-2010-001, California Institute of Technology, 2010. 9\n[38] M-E. Nilsback and A. Zisserman. Automated ﬂower classiﬁcation over a large number of classes. In\nIndian Conference on Computer Vision, Graphics and Image Processing, 2008. 9\n[39] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam,\nPietro Perona, and Serge Belongie. The inaturalist species classiﬁcation and detection dataset. In IEEE\nConference on Computer Vision and Pattern Recognition, pages 8769–8778, 2018. 9\n[40] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classiﬁcation of aircraft.\nTechnical report, 2013. 9\n[41] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In IEEE\nConference on Computer Vision and Pattern Recognition, pages 3606–3613, 2014. 9\n[42] Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of\ntrafﬁc signs in real-world images: The German Trafﬁc Sign Detection Benchmark. In International Joint\nConference on Neural Networks, 2013. 9\n[43] Brigit Schroeder and Yin Cui. Fgvcx fungi classiﬁcation challenge 2018. In github. com/ visipedia/\nfgvcx_ fungi_ comp,2018. 9\n12\n[44] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million\nimage database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n40(6):1452–1464, 2017. 9\n[45] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu, Ross Goroshin, Carles\nGelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle. Meta-dataset: A dataset of\ndatasets for learning to learn from few examples. http://arxiv.org/abs/1903.03096, abs/1903.03096, 2019.\n10\n[46] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenenbaum, and Phillip Isola. Rethinking few-shot\nimage classiﬁcation: a good embedding is all you need? arXiv preprint arXiv:2003.11539, 2020. 10\n[47] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Russakovsky. Towards fairer datasets: Filtering\nand balancing the distribution of the people subtree in the imagenet hierarchy. In ACM Conference on\nFairness, Accountability, and Transparency, pages 547–558, 2020. 10\n[48] Kaiyu Yang, Jacqueline Yau, Li Fei-Fei, Jia Deng, and Olga Russakovsky. A study of face obfuscation in\nimagenet. arXiv preprint arXiv:2103.06191, 2021. 10\n[49] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. IEEE International Conference on\nComputer Vision, 2021. 17\n[50] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network\ndesign spaces. In IEEE Conference on Computer Vision and Pattern Recognition, 2020. 17\n13\nA Random PatchDrop: Effect of Patch Size\nWe extend our Random PatchDrop experiments to include varying patch sizes for the masking\noperation, as illustrated in Fig. 17. The PatchDrop experiments in the main paper involved splitting\nthe image into a 14×14 grid (obtaining 196 patches of dimension 16×16 pixels). Here, we split the\nimage into different grid sizes and we deﬁne each experiment by the relevant grid size. Results for\nthese experiments are presented in Fig. 18. All accuracy values are reported on the ImageNet val set.\nSince each grid size contains a different number of patches, we occlude a particular percentage and\ninterpolate to the same scale in our accuracy plots for better comparison.\nWe note that ViT models (that split an input image into a sequence of patches for processing) are\nsigniﬁcantly more robust to patch occlusion when dimensions of occluded patches are multiples of\nthe model patch size (the grid size used is a factor of the original grid size). This is visible in the\nhigher performance of ViT for the 7×7 grid PatchDrop experiment (original uses 14×14 grid). At the\nsame time, as large portions are occluded (e.g., with a 4×4 spatial grid), the performance difference\nbetween ViT models and CNNs reduces considerably. We believe this to be the case since very large\npatch occlusions at high masking rates is likely to remove all visual cues relevant to a particular\nobject category, make it really challenging for both ViT and CNN models to make correct predictions.\nMore importantly, we note that the trends observed in Sec. 3.1 about occlusions are reconﬁrmed from\nthe varying grid-sizes experiment as well. We also note that some of these grid sizes (e.g., 8×8) have\nno relation to the grid patterns used by the original ViT models (that split an image into a sequence of\n14×14 patches). This indicates that while these trends are more prominent for matching grid sizes\n(same as that of ViT models) and its factors, the observed trends are not arising solely due to the ViT\nmodels’ grid operation. We note this behaviour is possible due to the dynamic receptive ﬁeld of ViTs.\n4×4 grid 4 ×8 grid 7 ×7 grid 8 ×8 grid 16 ×16 grid\nFigure 17: Visualization of varying grid sizes (resulting in different patch sizes) for PatchDrop experiments.\nFigure 18: Here, the patch sizes\nused for occlusion are different to\nthe patch size used by ViT mod-\nels (e.g., 16×16 by default). Note\nthat for larger patch sizes, the be-\nhaviour is closer to ResNet50, while\nfor smaller patch sizes, ViT models\ngenerally perform better.\nA.1 Random PatchDrop with Offset\nWe also explore how a spatial offset on our PatchDrop masks affects ViT models. This is aimed\nat eliminating the possible alignments between the intrinsic grid patterns of ViT models and our\nocclusion strategy, thereby avoiding any biasness in the evaluation setting towards a particular model\nfamily. The same masks are applied on the image, except with a small spatial offset to ensure that\n14\nFigure 19: We repeat our experiments in Sec. 3.1 by adding\nan offset to the grid we use for masking patches. We aim to\neliminate any biases due to any gird patterns that bear similarity\nwith the kind of patches used by ViT models. To this end, in the\nPatchDrop experiments we remove alignment between our masks\nand ViT grid patterns. We note similar trends in this case as well,\nalongside a relative drop in ViT-L performance.\nnone of the masking patches align with any of the grid patterns used by ViT models in processing\nthe input image. We replicate the same experiments as described in Sec. 3.1 under this setting and\npresent our results in Fig. 19. While in general we observe a similar trend between ViT models\nand the ResNet50 model, we note the signiﬁcant drop of accuracy in ViT-L, in comparison to its\nperformance under the no-offset setting. We present our potential reasons for this trend below.\nViT-L is a large-scale model containing over 300 million trainable parameters, while the other models\ncontain signiﬁcantly less parameters e.g., DeiT-B (86 million), T2T-24 (64 million), TnT-S (23\nmillion), and ResNet50 (25 million). Furthermore, unlike ViT-L model, DeiT family and those\nbuilding on it are trained with extensive data augmentation methods that ensure stable training of\nViTs with small datasets. A similar relative drop of ViT-L performance is observed in the 16×16 grid\nsize experiment in Fig. 18 as well. The anomalous behaviour of ViT-L in this setting is potentially\nowing to these differences.\nB Random PixelDrop\nA further step to observe the occlusion effect decoupled from the intrinsic grid operation of ViT\nmodels is to occlude at a pixel level. We generate pixel level masks of varying occlusion levels as\nillustrated in Fig. 20. Our evaluations on the ImageNet val. set presented in Fig. 21 indicate the same\ntrends between ViT models and CNNs that are observed earlier in Sec. 3.1 and Appendix A.\nPixelDrop can be considered as a version of PatchDrop where we use a grid size equal to the image\ndimensions (setting patch size to 1×1). Considering this, we compare how the performance of models\nvaries as we approach PixelDrop from smaller grid sizes. This is illustrated in Fig. 22 where we\nevaluate models on the ImageNet val set at 50% occlusion using PatchDrop with different grid sizes.\nWe note that the overall performance of models drops for such ﬁxed occlusion levels in the case of\nPixelDrop in comparison to the PatchDrop experiments.\nOriginal 10% 20% 30% 40%\n50% 60% 70% 80% 90%\nFigure 20: Visualization of varying levels of PixelDrop (randomly masking pixels to study robustness against\nocclusions).\n15\nFigure 21: Random PixelDrop: we com-\npare the performance of ViT models\nagainst a ResNet50 for our PixelDrop ex-\nperiments illustrating how similar trends\nare exhibited.\nFigure 22: We compare the performance of models as we vary the\ngrid size keeping the occlusion level constant at 50% all the way\nuntil PixelDrop which we consider as PatchDrop with grid size\nequivalent to the image dimensions. While PixelDrop shows us\nsimilar trends as the occlusion level varies (Fig, 21), the general\nperformance of models decreases.\nWe also note how ViT-L displays signiﬁcantly higher performance in comparison to the other models.\nThis can be attributed to its much higher trainable parameter count as discussed in Sec. A.1. At the\nsame time, ViT-L shows an anomalous drop in performance for the 16×16 grid, quite similar to our\nobservations in Fig. 19.\nC Robustness to Feature Drop\nIn contrast to our previous experiments involving occlusion in the model input space, we now focus\non occlusion within the model feature space. We achieve this by dropping portions of the intermediate\nrepresentations inside the ViT model as opposed to dropping patches from the input image. For\neach transformer block (e.g. for each of the 12 blocks in DeiT-B), we randomly mask (set to zero) a\nselected percentage of its input features. The effects of these “feature drop” experiments are studied in\nTable 6 by evaluating performance on the ImageNet val set. Performance is measured in the standard\nmethod (using the output of the ﬁnal classiﬁer head of the ViT model). We note that for small amounts\nof feature drop (25% and 50%), the models suffer relatively similar performance drops regardless of\nthe individual block location. However, for larger amounts of feature drop, certain blocks emerge\nmore important for each model. Furthermore, we notice a level of information redundancy within the\nblocks of larger models, as their performance drops are not signiﬁcant even for considerable amounts\nof feature drop (e.g. ViT-L at 25%).\nBlock ViT-L DeiT-B\n25% 50% 75% 25% 50% 75%\nBlock 1 75.72 67.62 25.99 57.36 38.51 15.17\nBlock 3 74.74 66.86 28.89 48.46 32.61 11.60\nBlock 5 73.32 60.56 29.69 54.67 40.70 14.10\nBlock 7 75.56 69.65 53.42 55.44 43.90 24.10\nBlock 9 76.16 70.59 42.63 50.54 28.43 18.21\nBlock 11 76.28 66.15 28.95 61.97 35.10 10.94\nTable 6: Lesion Study: we drop\na percentage of features input to\neach block of selected ViT models\nand evaluate their performance in\nterms of Top-1 accuracy (%) on Im-\nageNet val set. ViT-L shows signif-\nicant robustness against such fea-\nture drop even up to the 25% mark\nhinting towards information redun-\ndancy within the model.\nBlock 25% 50% 75%\n1 0.14 0.09 0.05\n2 45.09 4.91 0.23\n3 69.19 28.35 0.52\n4 73.95 64.12 18.95\n5 75.74 75.21 73.57\nTable 7: ResNet50 Lesion Study: we perform fea-\nture drop on the intermediate feature maps input to\neach of the four residual blocks (layers 1-4) and the\nfeature map prior to the ﬁnal average pooling op-\neration (layer 5). We evaluate Top-1 accuracy (%)\non the ImageNet val. set for 25%, 50%, and 75%\nfeature drop applied to each layer.\n16\nIn Table 7, we conduct the same feature drop experiments for a ResNet50 model. We note that the\nResNet50 architecture is entirely different to that of ViT models; hence comparison of these values\nwill give little meaning. In the case of ResNet50, we observe how feature drop in the earlier layers\nleads to signiﬁcant drops in performance unlike in ViT models. Also, feature drop in the last layer\nshows almost negligible drops in performance, which may be due to the average pooling operation\nwhich immediately processes those features. In the case of the ViT models compared, the patch\ntokens in the last layer are not used for a ﬁnal prediction, so applying feature drop on them has no\neffect on the performance.\nD Robustness to Occlusions: More Analysis\nIn our experimental settings, we used ViTs with class tokens that interact with patch tokens throughout\nthe network and are subsequently used for classiﬁcation. However, not all ViT designs use a class\ntoken e.g., Swin Transformer [49] uses an average of all tokens. To this end, we conduct experiments\n(Fig. 23) using three variants of the recent Swin Transformer [49] against our proposed occlusions.\nD.1 Swin Transformer [49]\nFigure 23: Robustness against object occlusion in images is studied under three PatchDrop settings (see Sec 3.1).\nWe compare the Swin model family against ResNet50 exhibiting their superior robustness to object occlusion.\nThese results show that ViT architectures that does not depend on using explicit class token like Swin transformer\n[49] are robust against information loss as well.\nD.2 RegNetY [50]\nHere, we evaluate three variants of RegNetY against our proposed occlusions (Fig. 24). RegNetY\n[50] shows relatively higher robustness when compared to ResNet50, but overall behaves similar to\nother CNN models.\nFigure 24: Robustness against object occlusion in images is studied under three PatchDrop settings (see Sec 3.1).\nWe study the robustness of stronger baseline CNN model, RegNetY [ 50] to occlusions, and identify that it\noverall behaves similar to other CNN models. Deit-T [ 3], a ViT with small number of parameters, performs\nsigniﬁcantly better than all the considered RegNetY variants.\n17\nE Behaviour of Shape Biased Models\nIn this section, we study the effect of our PatchDrop (Sec. 3.1) and permutation invariance (Sec. 3.3)\nexperiments on our models trained on Stylized ImageNet [9] (shape biased models). In comparison to\na shape biased CNN model, the VIT models showcase favorable robustness to occlusions presented\nin the form of PatchDrop. Note that ResNet50 (25 million) and DeiT-S (22 million) have similar\ntrainable parameter counts, and therein are a better comparison. Furthermore, we note that in the case\nof “random shufﬂe” experiments, the ViT models display similar (or lower) permutation invariance in\ncomparison to the CNN model. These results on random shufﬂe indicate that the lack of permutation\ninvariance we identiﬁed within ViT models in Sec. 3.3 is somewhat overcome in our shape biased\nmodels.\nFigure 25: Shape biased mod-\nels: We conduct the same Patch-\nDrop and Random Shufﬂe experi-\nments on DeiT models trained on\nStylized ImageNet [ 9] and com-\npare with a CNN trained on the\nsame dataset. All results are cal-\nculated over the ImageNet val. set.\nWe highlight the improved per-\nformance in the PatchDrop ex-\nperiments for the DeiT models\nin comparsion to ResNet50. We\nalso note how the DeiT models’\nperformance drop with random\nshufﬂing is similar to that of the\nResNet model.\nF Dynamic Receptive ﬁeld\nWe further study the ViT behavior to focus on the informative signal regardless of its position. In\nour new experiment, during inference, we rescale the input image to 128x128 and place it within\nblack background of size 224x224. In other words, rather than removing or shufﬂing image patches,\nwe reﬂect all the image information into few patches. We then move the position of these patches\nto the upper/lower right and left corners of the background. On average, Deit-S shows 62.9% top-1\nclassiﬁcation accuracy and low variance (62.9±0.05). In contrast, ResNet50 achieves only 5.4% top-1\naverage accuracy. These results suggest that ViTs can exploit discriminative information regardless\nof its position (Table 8). Figure 26 shows visualization depicting the change in attention, as the image\nis moved within the background.\nModels top-right top-left bottom-right bottom-left\nResNet50 5.59 5.71 4.86 5.30\nDeiT-T 51.21 51.38 50.61 50.70\nDeiT-S 63.14 63.01 62.62 62.79\nDeiT-B 69.37 69.29 69.18 69.20\nTable 8: We rescale the input image to 128x128 and\nplace it within the upper/lower right and left corners\nof the background of size 224x224. ViTs can exploit\ndiscriminative information regardless of its position\nas compared to ResNet50. Top-1 (%) accuracy on\nImageNet val. set is reported.\n18\n(a) Bottom Right\n (b) Upper Left\n (c) Bottom Left\nFigure 26: Visualization depicting the change in attention, as the image is moved within the background.\nAttention maps (averaged over the entire ImageNet val. set) relevant to each head across all 12 layers of an\nImageNet pre-trained DeiT-T (tiny) model [3]. All images are rescaled to 128x128 and placed within black\nbackground. Observe how later layers clearly attend to non-occluded regions of images to make a decision, an\nevidence of the model’s highly dynamic receptive ﬁeld.\nG Additional Qualitative Results\nHere, we show some qualitative results, e.g., Fig. 27 show the examples of our occlusion (random,\nforeground, and background) method. The performance of our shape models to segment the salient\n19\nimage is shown in Fig. 28. We show different variation levels of Salient PatchDrop on different\nimages in Fig. 29. Finally, we show adversarial patches optimized to fool different ViT models (Fig.\n31).\nFigure 27: Visualizations for our three PatchDrop occlusion strategies: original, random (50% w.r.t the image),\nnon-salient (50% w.r.t the forground predicted by DINO), and salient (50% of the backgrond as predicted by\nDINO) PatchDrop (shown from left to right). DeiT-B model achieves accuracies of 81.7%, 75.5%, 68.1%, and\n71.3% across the ImageNet val. set for each level of occlusion illustrated from left to right, respectively.\nFigure 28: Automatic segmentation of images using class-token attention for a DeiT-S model. Original, SIN\ntrained, and SIN distilled model outputs are illustrated from top to bottom, respectively.\n20\nFigure 29: The variation (level increasing from left to right) of Salient PatchDrop on different images.\n21\nOriginal 5% Adv Patch 15% Adv Patch 25% Adv Patch\nFigure 30: Adversarial patch (universal and untargeted) visualizations. Top row shows adversarial patches\noptimized to fool DeiT-S trained on ImageNet, whilebottom row shows patches for DeiT-S-SIN. DeiT-S performs\nsigniﬁcantly better than DeiT-S-SIN. On the other hand, DeiT-SIN has higher shape-bias than DeiT-S.\nOriginal 5% Adv Patch 15% Adv Patch 25% Adv Patch\nFigure 31: Adversarial patches (universal and untargeted) optimized to fool DeiT-T, DeiT-B, and T2T-24 models\nfrom top to bottom. These ViT models are more robust to such adversarial patterns than CNN (e.g., ResNet50).\n22"
}