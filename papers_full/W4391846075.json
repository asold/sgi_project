{
  "title": "Accelerating materials language processing with large language models",
  "url": "https://openalex.org/W4391846075",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2102171538",
      "name": "Jaewoong CHOI",
      "affiliations": [
        "Korea Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2138973982",
      "name": "Byungju Lee",
      "affiliations": [
        "Korea Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2102171538",
      "name": "Jaewoong CHOI",
      "affiliations": [
        "Korea Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2138973982",
      "name": "Byungju Lee",
      "affiliations": [
        "Korea Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953641512",
    "https://openalex.org/W4377862923",
    "https://openalex.org/W4386168831",
    "https://openalex.org/W3045594475",
    "https://openalex.org/W4389452877",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W2961421843",
    "https://openalex.org/W4224442790",
    "https://openalex.org/W4384914963",
    "https://openalex.org/W4313494085",
    "https://openalex.org/W4293567371",
    "https://openalex.org/W4361298520",
    "https://openalex.org/W2980932864",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4386902993",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W4387393188",
    "https://openalex.org/W3127365350",
    "https://openalex.org/W3132956480",
    "https://openalex.org/W4313645963",
    "https://openalex.org/W2885141472",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W4281559792",
    "https://openalex.org/W4281476575",
    "https://openalex.org/W3047398431",
    "https://openalex.org/W4296836559",
    "https://openalex.org/W4386884238",
    "https://openalex.org/W2523785361",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W3112850967",
    "https://openalex.org/W4380434108",
    "https://openalex.org/W4221131784",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4362640271",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W3212619479",
    "https://openalex.org/W6604786921",
    "https://openalex.org/W6600005967",
    "https://openalex.org/W6704017408",
    "https://openalex.org/W6606828123",
    "https://openalex.org/W3105470358",
    "https://openalex.org/W4280562096",
    "https://openalex.org/W4382775026",
    "https://openalex.org/W4295047949",
    "https://openalex.org/W4385627740",
    "https://openalex.org/W4392002118",
    "https://openalex.org/W6601795639",
    "https://openalex.org/W1747861911",
    "https://openalex.org/W3178108585"
  ],
  "abstract": "Abstract Materials language processing (MLP) can facilitate materials science research by automating the extraction of structured data from research papers. Despite the existence of deep learning models for MLP tasks, there are ongoing practical issues associated with complex model architectures, extensive fine-tuning, and substantial human-labelled datasets. Here, we introduce the use of large language models, such as generative pretrained transformer (GPT), to replace the complex architectures of prior MLP models with strategic designs of prompt engineering. We find that in-context learning of GPT models with few or zero-shots can provide high performance text classification, named entity recognition and extractive question answering with limited datasets, demonstrated for various classes of materials. These generative models can also help identify incorrect annotated data. Our GPT-based approach can assist material scientists in solving knowledge-intensive MLP tasks, even if they lack relevant expertise, by offering MLP guidelines applicable to any materials science domain. In addition, the outcomes of GPT models are expected to reduce the workload of researchers, such as manual labelling, by producing an initial labelling set and verifying human-annotations.",
  "full_text": "communicationsmaterials Article\nhttps://doi.org/10.1038/s43246-024-00449-9\nAccelerating materials language\nprocessing with large language models\nCheck for updates\nJaewoong Choi 1 & Byungju Lee 1\nMaterials language processing (MLP) can facilitate materials science research by automating the\nextraction of structured data from research papers. Despite the existence of deep learning models for\nMLP tasks, there are ongoing practical issues associated with complex model architectures, extensive\nﬁne-tuning, and substantial human-labelled datasets. Here, we introduce the use of large language\nmodels, such as generative pretrained transformer (GPT), to replace the complex architectures of prior\nMLP models with strategic designs of prompt engineering. Weﬁnd that in-context learning of GPT\nmodels with few or zero-shots can provide high performance text classiﬁcation, named entity\nrecognition and extractive question answering with limited datasets, demonstrated for various classes\nof materials. These generative models can also help identify incorrect annotated data. Our GPT-based\napproach can assist material scientists in solving knowledge-intensive MLP tasks, even if they lack\nrelevant expertise, by offering MLP guidelines applicable to any materials science domain. In addition,\nthe outcomes of GPT models are expected to reduce the workload of researchers, such as manual\nlabelling, by producing an initial labelling set and verifying human-annotations.\nMaterials language processing (MLP) has emerged as a powerful tool in the\nrealm of materials science research that aims to facilitate the extraction of\nvaluable information from a large number of papers and the development of\nknowledgebase\n1– 5. MLP leverages natural language processing (NLP) tech-\nniques to analyse and understand the language used in materials science\ntexts, enabling the identiﬁcation of key materials and properties and their\nrelationships\n6– 9. Some researchers reported that the learning of text-inherent\nchemical/physical knowledge is enabled by MLP, showing interesting\nexamples that text embedding of chemical elements is aligned with the\nperiodic table\n1,2,9– 11.D e s p i t es i g n iﬁcant advancements in MLP, challenges\nremain that hinder its practical applicability and performance. One key\nchallenge lies in the availability of labelled datasets for training deep\nlearning-based MLP models, as creating such datasets can be time-\nconsuming and labour-intensive\n4,7,9,12,13. Additionally, developing deep\nlearning models for knowledge-intensive MLP tasks requires exhaustive\nﬁne-tuning with a large number of labelled datasets to achieve satisfactory\nperformance, limiting their effectiveness in scenarios with limited label-\nled data.\nIn this study, we suggest generative pretrained transformer (GPT)\nmodels14-enabled MLP guidelines for materials scientists to employ the\npower of large language models (LLMs) for solving such knowledge-\nintensive tasks effectively. Recently, GPT-3, and GPT-3.5 models, the\npowerful LLMs, have demonstrated remarkable performance in various\nNLP tasks, such as text generation, translation, and comprehension, and has\ngarnered growing interest even in the materials scienceﬁeld\n15– 17.W ea i mt o\nshow how to use these GPT models (e.g., embeddings, few-shot learning or\nﬁne-tuning) for solving MLP tasks and investigate their characteristics, such\nas reliability, and generative property, beyond the comparison of perfor-\nmance with existing models. Our study focuses on two key MLP tasks: text\nclassiﬁcation, and information extraction, and the latter involves two sub-\ntasks, i.e., named entity recognition (NER), and extractive question\nanswering (QA).\nFirst, regarding a text classiﬁcation task, we present a paperﬁltering\nmethod that leverages the strengths ofzero-shot (without training data) and\nfew-shot (with few training data) learning models, which show promising\nperformance even with limited training data. This approach demonstrates\nthe potential to achieve high accuracy inﬁltering relevant documents\nwithout ﬁne-tuning based on a large-scale dataset. With regard to infor-\nmation extraction, we propose an entity-centric prompt engineering\nmethod for NER, the performance of which surpasses that of previousﬁne-\ntuned models on multiple datasets. By carefully constructing prompts that\nguide the GPT models towards recognising and tagging materials-related\nentities, we enhance the accuracy and efﬁciency of entity recognition in\nmaterials science texts. Also, we introduce a GPT-enabled extractive QA\nmodel that demonstrates improved performance in providing precise and\ninformative answers to questions related to materials science. Byﬁne-tuning\n1Computational Science Research Center, Korea Institute of Science and Technology, Seoul, Republic of Korea.\ne-mail: blee89@kist.re.kr\nCommunications Materials| (2024)5:13 1\n1234567890():,;\n1234567890():,;\nthe GPT model on materials-science-speciﬁc QA data, we enhance its ability\nto comprehend and extract relevant information from the scientiﬁc\nliterature.\nThrough our experiments and evaluations, we validate the effectiveness\nof GPT-enabled MLP models, analysingtheir cost, reliability, and accuracy\nto advance materials science research. Furthermore, we discuss the impli-\ncations of GPT-enabled models for practical tasks, such as entity tagging and\nannotation evaluation, shedding light on the efﬁcacy and practicality of this\napproach. In summary, our research presents a signiﬁcant advancement in\nMLP through the integration of GPT models. By leveraging the capabilities\nof GPT, we aim to overcome limitationsin its practical applicability and\nperformance, opening new avenues for extracting knowledge from mate-\nrials science literature.\nResults and Discussion\nGeneral workﬂow of MLP\nFigure 1 presents a general workﬂow of MLP, which consists of data col-\nlection, pre-processing, text classiﬁcation, information extraction and data\nmining18.I nF i g .1, data collection and pre-processing are close to data\nengineering, while text classiﬁcation and information extraction can be\naided by natural language processing. Lastly, data mining such as recom-\nmendations based on text-mined data\n2,10,19,20 can be conducted after the text-\nmined datasets have been sufﬁciently veriﬁed and accumulated. Most MLP\nstudies proceed in a similarﬂow. This process is actually similar to the\nprocess of actual materials scientists obtaining desired information from\npapers. For example, if they want to get information about the synthesis\nmethod of a certain material, they search based on some keywords in a\npaper search engine and get informationretrieval results (a set of papers).\nThen, valid papers (papers that are likely to contain the necessary infor-\nmation) are selected based on information such as title, abstract, author, and\njournal. Next, they can read the main text of the paper, locate paragraphs\nthat may contain the desired information (e.g., synthesis), and organize the\ninformation at the sentence or word level. Here, the process of selecting\npapers orﬁnding paragraphs can be conducted through a text classiﬁcation\nmodel, while the process of recognising, extracting, and organising infor-\nmation can be done through an information extraction model. Therefore,\nthis study mainly deals with how text classiﬁcation and information\nextraction can be performed through LLMs.\nText classiﬁcation in MLP\nText classiﬁcation, a fundamental task in NLP, involves categorising textual\ndata into predeﬁned classes or categories21. This process enables efﬁcient\norganisation and analysis of textual data, offering valuable insights across\ndiverse domains. With wide-ranging applications in sentiment analysis,\nspam ﬁltering, topic classiﬁcation, and document organisation, text classi-\nﬁcation plays a vital role in information retrieval and analysis. Traditionally,\nmanual feature engineering coupledwith machine-learning algorithms\nwere employed; however, recent developments in deep learning and pre-\ntrained LLMs, such as GPT series models, have revolutionised theﬁeld. By\nﬁne-tuning these models on labelled data, they automatically extract fea-\ntures and patterns from text, obviatingthe need for laborious manual feature\nengineering.\nIn theﬁeld of materials science, text classiﬁcation has been actively used\nfor ﬁltering valid documents from the retrieval results of search engines or\nidentifying paragraphs containing information of interest9,12,13. For example,\nsome researchers have attempted to classify the abstracts of battery-related\npapers from the results of searching with keywords such as‘battery’ or\n‘battery materials’, which is the starting point of extracting battery-device\ninformation from the literature22. Furthermore, paragraph-level classiﬁca-\ntion models have been developed toﬁnd paragraphs of interest using a\nstatistical model such as Latent Dirichlet allocation or machine-learning\nmodels such as random forest or BERT classiﬁer13,23,24, e.g., for solid-state\nsynthesis, gold-nanoparticle synthesis, multiclass of solution synthesis.\nInformation extraction in MLP\nInformation extraction is an NLP task that involves automatically extracting\nstructured information from unstructured text25– 28. The goal of information\nextraction is to convert text data into a more organized and structured form\nthat can be used for analysis, search,or further processing. Information\nextraction plays a crucial role in various applications, including text mining,\nknowledge graph construction, and question-answering systems\n29– 33.K e y\naspects of information extraction inNLP include NER, relation extraction,\nevent extraction, open information extraction, coreference resolution, and\nextractive question answering.\nNamed entity recognition in MLP. First, NER is one of the representative\nNLP techniques for information extraction34. NER aims to identify and\nclassify named entities within text. Here, named entities refer to real-world\nobjects such as persons, organisations, locations, dates, and quantities\n35.T h e\ntask of NER involves analysing textand identifying spans of words that\ncorrespond to named entities. NER algorithms typically use machine\nlearning such as recurrent neural networks or transformers to automatically\nlearn patterns and features from labelled training data. NER models are\ntrained on annotated datasets where human annotators label entities in text.\nThese annotations serve as the ground truth for training the model. The\nmodel learns to recognise patterns and contextual cues to make predictions\non unseen text, identifying and classifying named entities. The output of\nNER is typically a structured representation of the recognised entities,\nincluding their type or category.\nIn theﬁeld of materials science, many researchers have developed NER\nmodels for extracting structured summary-level data from unstructured\ntext. For example, domain-speciﬁc pretrained language models such as\nSciBERT\n36,M a t B E R T8, MatSciBERT30, and MaterialsBERT37 were used to\nextract specialised information from materials science literature, thereby\nextracting entities on solid-state materials, doping, gold nanoparticles\n(AuNPs), polymers, electrocatalytic CO\n2 reduction, and solid oxide fuel cells\nfrom a large number of papers8,9,37– 39.\nFig. 1 | General workﬂow of MLP.The process of MLP consists ofﬁve steps; data\ncollection, pre-processing, text classiﬁcation, information extraction and data\nmining. Data collection involves the web crawling or bulk download of papers with\nopen API services and sometime requires parsing of mark-up languages such as\nHTML. Pre-processing is an essential step, and includes preserving and managing\nthe text encoding, identifying the characteristics of the text to be analysed (length,\nlanguage, etc.), andﬁltering through additional data. Data collection and pre-\nprocessing steps are pre-requisite for MLP, requiring some programming techniques\nand database knowledge for effective data engineering. Text classiﬁcation and\ninformation extraction steps are of our main focus, and their details are addressed in\nSection 3,4, and 5. Data mining step aims to solve the prediction, classiﬁcation or\nrecommendation problems from the patterns or relationships of text-mined dataset.\nAfter the data set extracted from the paper has been sufﬁciently veriﬁed and accu-\nmulated, the data mining step can be performed for purposes such as material\ndiscovery.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 2\nExtractive question answering in MLP.E x t r a c t i v eQ Ai sat y p eo fQ A\nsystem that retrieves answers directly from a given passage of text rather\nthan generating answers based on external knowledge or language\nunderstanding\n40. It focuses on selecting and extracting the most relevant\ninformation from the passage to provide concise and accurate answers to\nspeciﬁcq u e s t i o n s .E x t r a c t i v eQ As y s t e m sa r ec o m m o n l yb u i l tu s i n g\nmachine-learning techniques, including both supervised and unsupervised\nmethods. Supervised learning approaches often require human-labelled\ntraining data, where questions and their corresponding answer spans in the\npassage are annotated. These models learn to generalise from the labelled\nexamples to predict answer spans for new unseen questions. Extractive QA\nsystems have been widely used in various domains, including information\nretrieval, customer support, and chatbot applications. Although they pro-\nvide direct and accurate answers based on the available text, they may\nstruggle with questions that require adeeper understanding of context or\nthe ability to generate answers beyond the given passage.\nIn the materials scienceﬁeld, the extractive QA task has received less\nattention as its purpose is similar to the NER task for information extraction,\nalthough battery-device-related QA models have been proposed\n22. Never-\ntheless, by enabling accurate information retrieval, advancing research in\nthe ﬁeld, enhancing search engines, andcontributing to various domains\nwithin materials science, extractive QA holds the potential for signiﬁcant\nimpact.\nPaper classiﬁcation with LLMs\nTo explain how to classify papers with LLMs, we used the binary classiﬁ-\ncation dataset from a previous MLP study to construct a battery database\nusing NLP techniques applied to research papers22.\nText classiﬁcation dataset description. The authors reported a dataset\nspeciﬁcally designed for ﬁltering papers relevant to battery materials\nresearch22. Speciﬁcally, 46,663 papers are labelled as‘battery’ or ‘non-\nbattery’, depending on journal information (Supplementary Fig. 1a).\nHere, the ground truth refers to the papers published in the journals\nrelated to battery materials among the results of information retrieval\nbased on several keywords such as‘battery’ and ‘battery materials’. The\noriginal dataset consists of training set (70%; 32,663), validation set (20%;\n9333) and test set (10%; 4667), and its speciﬁc examples can be found in\nSupplementary Table 4. The dataset was manually annotated and a\nclassiﬁcation model was developed through painstaking ﬁne-tuning\nprocesses of pre-trained BERT-based models.\nDespite the reported SOTA performance is an accuracy of 97.5%,\nprecision of 96.6%, and recall of 99.5%, such models require extensive\ntraining data and complex structures, and thus, we attempted to develop a\nsimple, GPT-enabled model that can achieve high performance using only a\nsmall dataset. Speciﬁcally, we tested zero-shot learning with GPT Embed-\ndings model. For few-shot learningm o d e l s ,b o t hG P T3 . 5a n dG P T - 4w e r e\ntested, while we also evaluated the performance ofﬁne-tuning model of\nGPT-3 for the classiﬁcation task (Supplementary Table 1). In these\nexperiments, we focused on the accuracy to enhance the balanced perfor-\nmance in improving the true and false accuracy rates. The choice of metrics\nto prioritize in text classiﬁcation tasks varies based on the speciﬁcc o n t e x t\nand analytical goals. For example, if the goal is to maximize the retrieval of\nrelevant papers for a speciﬁc category, emphasizing recall becomes crucial.\nConversely, in document ﬁltering, where reducing false positives and\nensuring high purity is vital, prioritizing precision becomes more signiﬁcant.\nWhen striving for comprehensive classiﬁcation performance, employing\naccuracy metrics might be more appropriate.\nZero-shot learning with LLMs for text classi ﬁcation. Zero-shot\nlearning with embedding41,42 allows models to make predictions or per-\nform tasks withoutﬁne-tuning with human-labelled data. The zero-shot\nmodel works based on the embedding value of a given text, which is\nprovided by GPT embedding modules. Using the distance between a\ngiven paragraph and predeﬁned labels in the embedding space, which\nnumerically represent their semantic similarity, paragraphs are classiﬁed\nwith labels (Fig.2a). For example, if one uses the model to classify an\nunseen text with the label of either‘batteries’or ‘solar cells’, the model will\ncalculate the distance between the embedding value of the text and that of\n‘batteries’ or ‘solar cells’, selecting the label with higher similarity in the\nembedding space.\nBelow are the results of the zero-shot text classiﬁcation model using the\ntext-embedding-ada-002 model of GPT Embeddings. First, we tested the\noriginal label pair of the dataset\n22,t h a ti s ,‘battery’vs. ‘non-battery’(‘original\nlabels’ of Fig.2b). The performance of the existing label-based model was\nlow, with an accuracy and precision of 63.2%, because the difference\nbetween the embedding value of two labels was small. Considering that the\ntrue label should indicate battery-related papers and the false label would\nresult in the complementary dataset, we designed the label pair as‘battery\nmaterials’ vs. ‘diverse domains’ (‘crude labels’ of Fig.2b). We successfully\nimproved the performance, achieving an accuracy of 87.3%, precision of\n84.5%, and recall of 97.9%, by specifying the meaning of the false label.\nTo further reduce the number of false positives, we designed the labels\nin an explicit manner, i.e.,‘battery materials’vs. ‘medical and psychological\nresearch’(‘designated labels’of Fig.2b). Here, the false label was selected by\nchecking the titles of randomly sampled papers from the non-battery set\n(refer to Supplementary Table 4). Interestingly, we obtained slightly\nimproved performance (accuracy, recall, and precision of 91.0%, 88.6%, and\n98.3%). We were able to achieve even higher performance (ACC: 93.0, PRE:\n90.8, REC: 98.9) if the labels were made even more verbose:‘papers related to\nbattery energy materials’ vs. ‘medical and psychological research’ (‘verbose\nlabels’of Fig.2b). Although these values are relatively lower than those of the\nSOTA model, it is noteworthy that acceptable text-classiﬁcation perfor-\nmance was achieved without exhaustive human labelling, as the proposed\nmodel is based on zero-shot learning with embeddings. These results imply\nthat classifying a speciﬁc set among the paper data set in materials science\ncan be achieved without labelling with zero-shot methods if a proper label\ncorresponding to a representativeembedding value for each category is\nselected. When utilizing our label descriptions for zero-shot learning, some\npapers may exactlyﬁt into neither the positive nor negative labels, that is,\noutliers. Nevertheless, they will be assigned to a label that is relatively similar\nto one of the given categories.\nFew-shot learning andﬁne-tuning of LLMs for text classiﬁcation.\nNext, the improved performance of few-shot text classiﬁcation models is\ndemonstrated in Fig.2c. In few-shot learning models, we provide the\nlimited number of labelled datasets to the model. We tested 2-way 1-shot\nand 2-way 5-shot models, which means that there are two labels and one/\nﬁve labelled data for each label are granted to the GPT-3.5 models (‘text-\ndavinci-003’). The example prompt is given in Fig.2d. The 2-way 1-shot\nmodels resulted in an accuracy of 95.7%, which indicates that providing\njust one example for each category has a signiﬁcant effect on the pre-\ndiction. Furthermore, increasing the number of examples (2-way 5-shots\nmodels) leads to improved performance, where the accuracy, precision,\nand recall are 96.1%, 95.0%, and 99.1%. Particularly, we were able toﬁnd\nthe slightly improved performance in using GPT-4 (‘gpt-4-0613’) than\nGPT-3.5 (‘text-davinci-003’); the precision and accuracy increased from\n0.95 to 0.954 and from 0.961 to 0.963, respectively.\nIn addition, we used theﬁne-tuning module of the davinci model of\nGPT-3 with 1000 prompt– completion examples. Theﬁne-tuning model\nperforms a general binary classiﬁcation of texts by learning the examples\nwhile no longer using the embeddings of the labels, in contrast to few-shot\nlearning. In our test, theﬁne-tuning model yielded high performance, that is,\nan accuracy of 96.6%, precision of 95.8%, and recall of 98.9%, which are close\nto those of the SOTA model. Here, we emphasise that the GPT-enabled\nmodels can achieve acceptable performance even with the small number of\ndatasets, although they slightly underperformed the BERT-based model\ntrained with a large dataset. The summary of our results comparing the\nGPT-based models against the SOTA models on three tasks are reported in\nSupplementary Table 1.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 3\nFig. 2 | Results of GPT-enabled text classiﬁcation models. aOverall process of our\nzero-shot learning for text classiﬁcation. b Results of zero-shot learning with GPT\nembedding. The accuracy, precision, and recall are reported.c Comparison of zero-\nshot learning (GPT Embeddings), few-shot learning (GPT-3.5 and GPT-4), and\nﬁne-tuning (GPT-3) results. The horizontal and vertical axes are the precision and\nrecall of each model, respectively. The node colour and size are based on the rank of\naccuracy and the dataset size, respectively.d Example of prompt engineering for\n2-way 1-shot learning, where the task description, one example for each category,\nand input abstract are given.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 4\nUnderstanding the calibration of LLMs in text classiﬁcation.I n\naddition to the accuracy, we investigated the reliability of our GPT-based\nmodels and the SOTA models in terms of calibration. The reliability can\nbe evaluated by measuring the expected calibration error (ECE) score\n43\nwith 10 bins. A lower ECE score indicates that the model’s predictions are\ncloser to being well-calibrated, ensuring that the conﬁdence of a model in\nits prediction is similar to the actual accuracy of the model44,45 (Refer to\nMethods section). The log probabilities of GPT-enabled models were\nused to compare the accuracy and conﬁdence. The ECE score of the\nSOTA (‘BatteryBERT-cased’) model is 0.03, whereas those of the 2-way\n1-shot model, 2-way 5-shot model, andﬁne-tuned model were 0.05, 0.07,\nand 0.07, respectively. Considering a well-calibrated model typically\nexhibits an ECE of less than 0.1, we conclude that our GPT-enabled text\nclassiﬁcation models provide high performance in terms of both accuracy\nand reliability with less cost. The lowest ECE score of the SOTA model\nshows that the BERT classiﬁer ﬁne-tuned for the given task was well-\ntrained and not overcon ﬁdent, potentially owing to the large and\nunbiased training set. The GPT-enabled models also show acceptable\nreliability scores, which is encouraging when considering the amount of\ntraining data or training costs required. In summary, we expect the GPT-\nenabled text-classiﬁcation models to be valuable tools for materials sci-\nentists with less machine-learning knowledge while providing high\naccuracy and reliability comparable to BERT-basedﬁne-tuned models.\nExtraction of named entities with LLMs\nTo explain how to extract named entities from materials science papers with\nGPT, we prepared three open datasets, which include human-labelled\nentities on solid-state materials, doped materials, and AuNPs (Supple-\nmentary Table 2).\nExtracting solid-state materials entities with LLMs. The solid-state\nmaterials dataset includes 800 annotated abstracts with the following cate-\ngories: inorganic materials (MAT), symmetry/phase labels (SPL), sample\ndescriptors (DSC), material properties (PRO), material applications (APL),\nsynthesis methods (SMT), and characterisation methods (CMT)\n38.F o r\nexample, MAT indicates inorganics solid/alloy materials or non-gaseous\nelements such as‘BaTiO3,’‘titania,’or ‘Fe’.S P Li n d i c a t e st h en a m ef o rc r y s t a l\nstructures and phases such as‘tetragonal’or a symmetry label such as‘Pbnm’\n(Supplementary Fig. 1b). The original dataset consists of training/validation/\ntest at a ratio of 6:2:2, which is used forﬁne-tuning of GPT models.\nBecause theﬁne-tuning model requires prompt– completion examples\nas a training set, the NER datasets are pre-processed as follows: the anno-\ntations for each category are marked with the special tokens\n46,a n dt h e n ,t h e\nraw text and marked text are used as the prompt and completion, respec-\ntively. For example, if the input text is“LiCoO2 and LiFePO4 are used as\ncathodes of secondary batteries”, the prompt is the same as the input text,\na n dt h ec o m p l e t i o nf o re a c hc a t e g o r yi sa sf o l l o w s :\nMAT model → Completion: “LiCoO2 and LiFePO4 are used as\ncathodes of secondary batteries” /c o m p l e t i o n :“@@LiCoO2## and\n@@LiFePO4## are used as cathodes of secondary batteries.”\nAPL model→ Completion:“LiCoO2 and LiFePO4 are used as cath-\nodes of secondary batteries” /c o m p l e t i o n :“LiCoO2 and LiFePO4 are\nused as @@cathodes of secondary batteries##.”\nOne of the examples used in the training set is shown in Fig.3d. After\npre-processing, we testedﬁne-tuning modules of GPT-3 (‘davinci’)m o d e l s .\nThe performance of our GPT-enabled NER models was compared with that\nof the SOTA model in terms of recall, precision, and F1 score. Figure3a\nshows that the GPT model exhibits a higher recall value in the categories of\nCMT, SMT, and SPL and a slightly lower value in the categories of DSC,\nMAT, and PRO compared to the SOTA model. However, for the F1 score,\nour GPT-based model outperforms the SOTA model for all categories\nbecause of the superior precision of the GPT-enabled model (Fig.3b, c). The\nhigh precision of the GPT-enabled model can be attributed to the generative\nnature of GPT models, which allows coherent and contextually appropriate\noutput to be generated. Excluding categories such as SMT, CMT, and SPL,\nBERT-based models exhibited slightly higher recall in other categories. The\nlower recall values could be attributed to fundamental differences in model\narchitectures and their abilities to manage data consistency, ambiguity, and\ndiversity, impacting how each modelcomprehends text and predicts sub-\nsequent tokens. BERT-based models effectively identify lengthy and intri-\ncate entities through CRF layers, enabling sequence labelling, contextual\nprediction, and pattern learning. The use of CRF layers in prior NER models\nhas notably improved entity boundaryrecognition by considering token\nlabels and interactions. In contrast, GPT-based models focus on generating\ntext containing labelling information derived from the original text. As a\ngenerative model, GPT doesn’t explicitly label text sections but implicitly\nembeds labelling details within the generated text. This approach might\nhinder GPT models in fully grasping complex contexts, such as ambiguous,\nlengthy, or intricate entities, leading to lower recall values.\nExtracting doped materials entities with LLMs. The doped materials\nentity dataset8 includes 450 annotations on the base material (BASE-\nMAT), the doping agent (DOPANT), and quantities associated with the\ndoped material such as the doping density or the charge carrier density\n(DOPMODQ), with speci ﬁc examples provided in Supplementary\nFig. 1c. The original dataset consists of training/validation/test set at a\nratio of 8:1:1. The SOTA model (‘MatBERT-uncased’) for this dataset had\nF1 scores of 72, 82, and 62 for BASEMANT, DOPANT, and DOPMODQ,\nrespectively. We analysed this dataset usingﬁne-tuning modules of GPT-\n3 such as the‘davinci’ model with the same data composition.\nThe prompt– completion sets were constructed similarly to the previous\nNER task. As reported in Fig.4a, theﬁne-tuning of‘davinci’ model showed\nhigh precision of 93.4, 95.6, and 92.7 for the three categories, BASEMAT,\nDOPANT, and DOPMODQ, respectively,while yielding relatively lower\nrecall of 62.0, 64.4, and 59.4, respectively (Fig.4a). These results imply that the\ndoped materials entity dataset may have diverse entities for each category but\nthat there is not enough data for training to cover the diversity. In addition,\nthe GPT-based model’s F1 scores of 74.6, 77.0, and 72.4 surpassed or closely\napproached those of the SOTA model (‘MatBERT-uncased’), which were\nrecorded as 72, 82, and 62, respectively (Fig.4b).\nExtracting AuNPs entities with LLMs. The AuNPs entity dataset\nannotates the descriptive entities (DES) and the morphological entities\n(MOR)\n23, where DES includes‘dumbbell-like’ or ‘spherical’ and MOR\nincludes noun phrases such as‘nanoparticles’ or ‘AuNRs’. More speciﬁc\nexamples are provided in Supplementary Fig. 1d. The SOTA model for\nthis dataset is reported as the MatBERT-based model whose F1 scores for\nDES and MOR are 0.67 and 0.92, respectively\n8.\nInstead of adoptingﬁne-tuning, we used the few-shot learning47 of the\nGPT-3.5 model (‘text-davinci-003’) for the AuNPs entities dataset, as there\nare not sufﬁcient datasets (N = 85). Similar to the previous NER task, we\ndesigned three prompts such as randomretrieval, task-informed random\nretrieval and kNN retrieval (Fig.4 and Supplementary Table 2). First, we\nrandomly select the three ground-truth examples (i.e., pair of text and the\ntext with named entities) from the original training and validation set when\nextracting the named entities from the given text in the test set (random\nretrieval). These simple methods yield high recall performance of 63% and\n97% for the DES and MOR categories, respectively. Here, it is noteworthy\nthat prompts with the ground-truth examples can provide improved results\non DES and MOR entity recognition, considering the recall values of 52%\nand 64% reported in prior works\n23 (Supplementary Fig. 2). However, the\nF1 score of this few-shot learning model was lower than that of the SOTA\nmodel (‘random retrieval’ of Fig.4c). Furthermore, we tested the effect of\nadding a phrase that directly speciﬁes the task to the existing prompt; e.g.,\n‘The task is to extract the descriptive entities of materials in the given text’\n(‘task-informed random retrieval’of Fig.4c). The example prompt is shown\nin Fig.4d. Some performance improvements, namely a 1%– 2% increase in\nrecall and a 6%– 11% increase in precision, were observed.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 5\nFinally, to more elaborately perform the few-shot learning,‘similar’\nground-truth examples to each test set, that is, the examples for which the\ndocument embedding value are similar to that of each test set, were selected\nfor the NER extraction in the test set (‘kNN retrieval’ of Fig.4c). Interest-\ningly, compared to the performance of the previous method (i.e., task-\ninformed random retrieval), we conﬁrmed that the recall value of the kNN\nmethod was the same or slightly lower and that the precision increased by\n15%– 20% (Supplementary Table 2 and Supplementary Fig. 2). Particularly,\nthe recall of DES was relatively low compared to its precision, which indi-\ncates that providing similar ground-truth examples enables more tight\nrecognition of DES entities. In addition, the recall of MOR is relatively\nhigher than the precision, implying that giving k-nearest examples results in\nthe recognition of more permissive MOR entities. In summary, we con-\nﬁrmed the potential of the few-shot NER model through GPT prompt\nengineering and found that providing similar examples rather than ran-\ndomly sampled examples and informing tasks had a signiﬁcant effect on\nperformance improvement. In terms ofthe F1 score, few-shot learning with\nthe GPT-3.5 (‘text-davinci-003’) model results in comparable MOR entity\nrecognition performance as that of the SOTA model and improved DES\nrecognition performance (Fig. 4c). In addition, we applied the same\nprompting strategy for GPT-4 model (gpt-4-0613), and obtained the\nimproved performance in capturing MOR and DES entities.\nExtraction of answers to questions with LLMs\nTo explain how to extract answer to questions with GPT, we prepared\nbattery device-related question answering dataset\n22.\nFew-shot learning andﬁne-tuning of GPT models for extractive QA.\nThis dataset consists of questions, contexts, and answers, and the ques-\ntions are related to the principal components of battery systems, i.e.,\n‘What is the anode?’, ‘What is the cathode?’, and‘What is the electrolyte?’.\nFor example, the context is the raw text such as“The blended slurry was\nthen cast onto a clean current collector (Al foil for the cathode and Cu foil\nfor the anode) and dried at 90 °C under vacuum overnight” and the\nanswer to the question what a cathode is can be‘Al foil’. This dataset was\nproposed to train the deep learning models to identify the battery system\ncomponent, which can be extended based on battery literature\n48– 50. The\npublicly available dataset includes 427 annotations, which is generated by\nbattery experts but requires several pre-processing\n22. We also found\nredundant or incorrect annotations, e.g., when there is no mention of the\nanode in the given context, the question is about the anode and the\nanswer is about the cathode. In the end, we reﬁned the given dataset into\n331 QA data (anode: 90; cathode: 161; electrolyte: 80) based on the\noutcomes of GPT-enabled models.\nAlso, we reproduced the results of prior QA models including the SOTA\nmodel, ‘BatteryBERT (cased)’, to compare the performances between our\nGPT-enabled models and prior models with the same measure. The per-\nformances of the models were newly evaluated with the average values of\ntoken-level precision and recall, which are usually used in QA model eva-\nluation. In this way, the prior models were re-evaluated, and the SOTA model\nturned out to be‘BatteryBERT (cased)’, identical to that reported (Fig.5a).\nWe tested the zero-shot QA model using the GPT-3.5 model (‘text-\ndavinci-003’), yielding a precision of 60.92%, recall of 79.96%, and F1 score\nof 69.15% (Fig. 5b and Supplementary Table 3). These relatively low\nFig. 3 | Performance of GPT-enabled NER models on solid-state materials\ncompared to the SOTA model (‘MatBERT-uncased’). The proposed models are\nbased onﬁne-tuning modules based on prompt– completion examples.a– c\nComparison of recall, precision, and F1 score between our GPT-enabled model and\nthe SOTA model for each category.d Example of prompt– completion for MAT\nentity recognition.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 6\nperformance values can be derived from the domain-speciﬁcd a t a s e t ,f r o m\nwhich it is difﬁcult for a vanilla model toﬁnd the answer from the given\nscientiﬁc literature text. Therefore, we added a task-informing phrase such\nas ‘The task is to extract answers from the given text.’to the existing prompt\nconsisting of the question, context, and answer. Surprisingly, we observed an\nincrease in performance, particularly in precision, which increased from\n60.92% to 72.89%. By specifying that the task was to extract rather than\ngenerate answers, the accuracy of the answers appeared to increase. Next, we\ntested a ﬁne-tuning module of GPT-3 models (‘davinci’). We achieved\nhigher performance with an F1 score of 88.21% (compared to that of 74.48%\nfor the SOTA model).\nUnderstanding the generative property of GPT. In addition to the\nimproved performance, we were able to examine the possibility of cor-\nrecting the existing annotations with our GPT-based models. As\nmentioned earlier, we modiﬁed and used the open QA data set. Here, in\naddition to removing duplicates or deleting unanswered data,ﬁnding\ndata with incorrect answers was based on the results of the GPT model\n(Fig. 5c). For example, there is an incorrect question– answer pair: the\nanode materials are not mentioned in the given context and‘nano-\nmeshed’ is mentioned as the cathode material; however, the annotated\nquestion is‘what is the anode material?’, and the corresponding answer is\n‘nano-meshed’. For this case, most BERT-based models yield the answer\n‘nano-meshed’ similar to the annotation, whereas the GPT models\nprovide the answer‘the anode is not mentioned in the given text’.I n\naddition, there were annotations that could increase the confusion of the\nmodel by making each question– answer pair for the answer in which the\ntwo tokens were combined by OR. For example, GPT models answered\n“sulfur or air cathode”, but the original annotations annotate‘sulfur’and\n‘air’ as different answers.\nFig. 4 | Performance of GPT-enabled NER models on doped materials and\nAuNPs, compared to the SOTA model. aDoped materials entity recognition\nperformance ofﬁne-tuning of GPT-3 (davinci), b doped materials entity recognition\nperformance (F1 score) comparison between SOTA (‘MatBERT-uncased’) andﬁne-\ntuning of GPT-3 (davinci), c AuNPs entity recognition performance (F1-score)\ncomparisons between GPT 3.5 davinci (random retrieval, task-informed random\nretrieval, kNN retrieval) and SOTA (‘MatBERT-uncased’) model,d Example of\nprompt for DES entity recognition (task informed random retrieval).\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 7\nConclusion\nThis work presents a GPT-enabled pipeline for MLP tasks, providing\nguidelines for text classiﬁcation, NER, and extractive QA. Through an\nempirical study, we demonstrated theadvantages and disadvantages of GPT\nmodels in MLP tasks compared to the priorﬁne-tuned models based\non BERT.\nIn text classiﬁcation, we conclude that the GPT-enabled models\nexhibited high reliability and accuracy comparable to that of the BERT-\nbased ﬁne-tuned models. This GPT-based method for text classiﬁcation is\nexpected to reduce the burden of materials scientists in preparing a large\ntraining set by manually classifying papers. Next, in NER tasks, we found\nthat providing similar examples improves the entity-recognition perfor-\nmance in few-shot GPT-enabled NER models. Theseﬁndings indicate that\nthe GPT-enabled NER models are expected to replace the complex tradi-\ntional NER models, which requires a relatively large amount of training data\nand elaborateﬁne-tuning tasks. Lastly, regarding extractive QA models for\nbattery-device information extraction, we achieved an improved F1 score\ncompared with prior models and conﬁrmed the possibility of using GPT\nmodels for correcting incorrect QA pairs. Recently, several pioneering\nstudies have showed the possibility of using LLMs such as chatGPT for\nextracting information from materials science texts\n15,51– 53. In this regard, our\nnovelty lies in comparing the characteristics of GPT series models with the\nBERT-based ﬁne-tuned models in depth as well as introducing various\nstrategies such as embedding, zero-shot/few-shot learning, andﬁne-tuning\nfor each MLP task.\nWe note the potential limitations and inherent characteristics of GPT-\nenabled MLP models, which materials scientists should consider when\nanalysing literature using GPT models. First, considering that GPT series\nmodels are generative, the additional step of examining whether the results\nare faithful to the original text would be necessary in MLP tasks, particularly\ninformation-extraction tasks\n15,16. In contrast, general MLP models based on\nﬁne-tuned LLMs do not provide unexpected prediction values because they\nare classiﬁed into predeﬁned categories through cross entropy function.\nGiven that GPT is a closed model that does not disclose the training details\nand the response generated carries an encoded opinion, the results are likely\nto be overconﬁdent and inﬂuenced by the biases in the given training data\n54.\nTherefore, it is necessary to evaluate the reliability as well as accuracy of the\nresults when using GPT-guided results for the subsequent analysis. In a\nsimilar vein, as GPT is a proprietary model that will be updated over time by\nopenAI, the absolute value of performance can be changed and thus con-\ntinuous monitoring is required for the subsequent uses55.F i n a l l y ,t h eG P T -\nenabled model would face challenges in more domain-speciﬁc, complex,\nand challenging tasks (e.g., relation extraction, event detection, and event\nextraction) than those presented in this study, as it is difﬁcult to explain the\ntasks in the prompt. For example, extracting the relations of entities would\nbe challenging as it is necessary to explain well the complicated patterns or\nrelationships as text, which are inferred through black-box models in gen-\neral NLP models15,16,56. Nonetheless, GPT models will be effective MLP tools\nby allowing material scientists to more easily analyse literature effectively\nwithout knowledge of the complex architecture of existing NLP models17.A s\nLLM technologies advance, creatingquality prompts that consist of speciﬁc\nand clear task descriptions, appropriate input text for the task, and con-\nsistently labelled results (i.e., classiﬁcation categories) will become more\nimportant for materials scientists.\nFig. 5 | Performance of GPT-enabled QA model. aReproduced results of BERT-\nbased model performances,b comparison between the SOTA andﬁne-tuning of\nGPT-3 (davinci), c correction of wrong annotations in QA dataset, and pre-\ndiction result comparison of each model. Here, the difference in the cased/\nuncased version of the BERT series model is the processing of capitalisation of\ntokens or accent markers, which inﬂu e n c e dt h es i z eo fv o c a b u l a r y ,p r e - p r o -\ncessing, and training cost.\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 8\nMethods\nData processing\nWe used the python library‘openai’ to implement the GPT-enabled MLP\npipeline. We mainly used the prompt– completion module of GPT models\nfor training examples for text classiﬁcation, NER, or extractive QA. We used\nzero-shot learning, few-shot learning orﬁne-tuning of GPT models for MLP\ntask. Herein, the performance is evaluated on the same test set used in prior\nstudies, while small number of training data are sampled from the training\nset and validation set and used for few-shot learning orﬁne-tuning of GPT\nmodels.\nGiven a sufﬁcient dataset of prompt– completion pairs, aﬁne-tuning\nmodule of GPT-3 models such as‘davinci’ or ‘curie’ can be used. The\nprompt– completion pairs are lists of independent and identically dis-\ntributed training examples concatenated together with one test input.\nH e r e i n ,a so p e nd a t a s e t su s e di nt h i ss t u d yh a dt r a i n i n g / v a l i d a t i o n / t e s t\nseparately, we used parts of training/validation for trainingﬁne-tuning\nmodels and the whole test set to conﬁrm the general performance of models.\nOtherwise, for few-shot learning which makes the prompt consisting of the\ntask-informing phrase, several examples and the input of interest, can be\nalternatives. Here, which examples to provide is important in designing\neffective few-shot learning. Similar examples can be obtained by calculating\nthe similarity between the training set for each test set. That is, given a\nparagraph from a test set, few examples similar to the paragraph are sampled\nfrom training set and used for generating prompts. Speciﬁcally, our kNN\nmethod for similar example retrieval is based on TF-IDF similarity (refer to\nSupplementary Fig. 3). Lastly, in case of zero-shot learning, the model is\ntested on the same test set of prior models.\nRegarding the preparation of prompt– completion examples forﬁne-\ntuning or few-shot learning, we suggest some guidelines. Sufﬁx characters in\nthe prompt such as‘→’are required to clarify to theﬁne-tuned model where\nthe completion should begin. In addition, sufﬁx characters in the prompt\nsuch as‘\\n\\n###\\n\\n’are required to specify the end of the prediction. This\ni si m p o r t a n tw h e nat r a i n e dm o d e ld e c i d e so nt h ee n do fi t sp r e d i c t i o nf o ra\ngiven input, given that GPT is one of the autoregressive models that con-\ntinuously predicts the following text from the preceding text. That is, in\nprediction, the same sufﬁx should be placed at the end of the input. In\naddition, preﬁx characters are usually unnecessary as the prompt and\ncompletion are distinguished. Rather than using the preﬁx characters,\nsimply starting the completion with a whitespace character would produce\nbetter results due to the tokenisation of GPT models. In addition, this\nmethod can be economical as it reduces the number of unnecessary tokens\nin the GPT model, where fees are charged based on the number of tokens.\nWe note that the maximum number of tokens in a single\nprompt– completion is 4097, and thus, counting tokens is important for\neffective prompt engineering; e.g., we used the python library‘titoken’to test\nthe tokenizer of GPT series models.\nGPT model usage guidelines\nAfter pre-processing, the splitting process of train, validation, and test set\nwas conducted with the same random seed and ratio used in previous\nstudies, that is, the training/validation set is used forﬁne-tuning GPT\nmodels and test set for conﬁrming their general performances. In theﬁne-\ntuning of GPT models, there are somehyperparameters such as the base\nmodel, batch size, number of epochs, learning rate multiplier, and prompt\nloss weight. The base models for whichﬁne-tuning is available are GPT-3\nmodels such as‘ada’, ‘babbage’, ‘curie’,a n d‘davinci’, which can be tested\nusing the web service provided by OpenAI ( https://gpttools.com/\ncomparisontool). For a simple prompt– completion task such as zero-shot\nlearning and few-shot learning, GPT-3.5 models such as‘text-davinci-003’\ncan be used. The batch size can be dynamically conﬁgured and its maximum\nis 256; however, we recommend 1% or 0.2% of the training set. The learning\nrate multiplier adjusts the models’ weights during training, and a high\nlearning rate leads to a sub-optimal solution, whereas a low one causes the\nmodel to converge too slowly orﬁnd a local minimum. The default values\nare 0.05– 0.2 depending on the batch size, and we set the learning rate\nmultiplier as 0.01. The prompt loss weight is the weight to use for loss on the\nprompt tokens, which should be reduced when prompts are relatively long\nto the corresponding completions to avoid giving undue priority to prompt\nlearning over the completion learning. We set the prompt loss\nweight as 0.01.\nWith theﬁne-tuned GPT models, we can infer the completion for a\ngiven unseen dataset that ends with the pre-deﬁned sufﬁx, which are not\nincluded in training set. Here, some parameters such as the temperature,\nmaximum number of tokens, and top P can be determined according to the\npurpose of analysis. First, temperature determines the randomness of the\ncompletion generated by the model, ranging from 0 to 1. For example,\nhigher temperature leads to more randomness in the generated output,\nwhich can be useful for exploring creative or new completions (e.g., gen-\nerative QA). In addition, lower temperature leads to more focused and\ndeterministic generations, which is appropriate to obtain more common\nand probable results, potentially sacriﬁcing novelty. We set the temperature\nas 0, as our MLP tasks concern the extraction of information rather than the\ncreation of new tokens. The maximum number of tokens determines how\nmany tokens to generate in the completion. If the ideal completion is longer\nthan the maximum number, the completion result may be truncated; thus,\nwe recommend setting this hyperparameter to the maximum number of\ntokens of completions in the training set (e.g., 256 in our cases). In practice,\nthe reason the GPT model stops producing results is ideally because a sufﬁx\nhas been found; however, it could be that the maximum length is exceeded.\nThe top P is a hyperparameter about the top-p sampling, i.e., nucleus\nsampling, where the model selects the next word based on the most likely\ncandidates, limited to a dynamic subset determined by a probability\nthreshold (p). This parameter promotes diversity in generated text while\nallowing control over randomness.\nPerformance evaluation\nWe evaluated the performance of text classiﬁcation, NER, and QA models\nusing different measures. Theﬁne-tuning module provides the results of\naccuracy, actually the exact-matchingaccuracy. Therefore, post-processing\nof the prediction results was required to compare the performance of our\nGPT-based models and the reported SOTA models. For the text classiﬁ-\ncation, the predictions refer to one of the pre-deﬁned categories. By com-\nparing the category mentioned in each prediction and the ground truth, the\naccuracy, precision, and recall can be measured. For the NER, the perfor-\nmance such as the precision and recall can be measured by comparing the\nindex of ground-truth entities and predicted entities. Here, the performance\ncan be evaluated strictly by using an exact-matching method, where both the\nstart index and end index of the ground-truth answer and prediction result\nmatch. The boundaries of named entities are likely to be subjective or\nambiguous in practice, and thus, we recommend the boundary-relaxation\nmethod to generously evaluate the performance, where a case that either the\nstart or end index is correct is considered as a true positive\n57,58.F o rt h e\nextractive QA, the performance is evaluated by measuring the precision and\nrecall for each answer at the token level and averaging them. Similar to the\nNER performance, the answers are evaluated by measuring the number of\ntokens overlapping the actual correct answers.\nECE score calculation\nTo compare the reliability of text classiﬁcation models in this study, we used\nECE score, which assesses the calibration of probabilistic predictions of\nmodels. To calculate the ECE score, the following steps are typically taken.\nFirst, predictions and true labels are collected. These predictions are class\nprobabilities, not just labels. For each data point, the predicted probability\ndistribution over the possible classes and the true class label are required.\nSecond, based on the predeﬁned number of bins (i.e., M, typically 10-20\nbins), similar predicted probabilities are grouped together to analyse cali-\nbration within each bin. Next, for each bin, expected accuracy and average\nconﬁdence are calculated. The expectedaccuracy is the average of the true\naccuracy for all data points in each bin; for example, the true accuracy would\nbe 1 if the predicted class matches the true class and 0 otherwise. The average\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 9\nconﬁdence is the conﬁdence level of the model’s predictions within each bin.\nAlso, the relative frequency of data points should be calculated for each bin,\nby dividing the number of data points in each bin by the total number of data\npoints. Finally, the ECE score is calculated as the weighted average of the\nabsolute difference between the expected accuracy and the average con-\nﬁdence within each bin:\nECE ¼\nX\nM\nm¼1\nBm\n/C12/C12 /C12/C12\nn acc Bm\n/C0/C1\n/C0 confðBmÞ\n/C12/C12 /C12/C12 ;\nwhere the dataset is divided into M interval bins based on conﬁdence, and\nBm is the set of indices of samples of which the conﬁdence scores fall into\neach interval, whileaccðBmÞ and confðBmÞ are the average accuracy and\nconﬁdence for each bin, respectively.\nThe ECE score is a measure of calibration error, and a lower ECE score\nindicates better calibration. If the ECE score is close to zero, it means that the\nmodel’s predicted probabilities are well-calibrated, meaning they accurately\nreﬂect the true likelihood of the observations. Conversely, a higher ECE\nscore suggests that the model’s predictions are poorly calibrated. To sum-\nmarise, the ECE score quantiﬁes the difference between predicted prob-\nabilities and actual outcomes across different bins of predicted probabilities.\nData availability\nData used in this study are available in https://github.com/\nAIHubForScience/GPT_MLP.\nCode availability\nSource codes used in this study are available in https://github.com/\nAIHubForScience/GPT_MLP.\nReceived: 19 October 2023; Accepted: 15 January 2024;\nPublished online: 15 February 2024\nReferences\n1. Tshitoyan, V. et al. Unsupervised word embeddings capture latent\nknowledge from materials science literature.Nature 571,\n95–98 (2019).\n2. He, T. et al. Precursor recommendation for inorganic synthesis by\nmachine learning materials similarity from scientiﬁc literature.Sci.\nAdv. 9, eadg8180 (2023).\n3. Choudhary, K. & Kelley, M. L. ChemNLP: A Natural Language-\nProcessing-Based Library for Materials Chemistry Text Data.J. Phys.\nChem. C127, 17545–17555 (2023).\n4. Hatakeyama-Sato, K. & Oyaizu, K. Integrating multiple materials\nscience projects in a single neural network.Commun. Mater.1,\n49 (2020).\n5. Choi, J., & Lee, B. Quantitative topic analysis of materials science\nliterature using natural language processing.ACS Appl Mater\nInterfaces 16, 1957–1968 (2024).\n6. Olivetti, E. A. et al. Data-driven materials research enabled by natural\nlanguage processing and information extraction.Appl. Phys. Rev.7,\n041317 (2020).\n7. Huo, H. et al. Semi-supervised machine-learning classiﬁcation of\nmaterials synthesis procedures.npj Comput. Mater.5, 62 (2019).\n8. Trewartha, A. et al. Quantifying the advantage of domain-speciﬁc pre-\ntraining on named entity recognition tasks in materials science.\nPatterns 3, 100488 (2022).\n9. Choi, J. et al. Deep Learning of Electrochemical CO\n2 Conversion\nLiterature Reveals Research Trends and Directions.J. Mater. Chem. A\n11, 17628–17643 (2023).\n10. Pei, Z., Yin, J., Liaw, P. K. & Raabe, D. Toward the design of ultrahigh-\nentropy alloys via mining six million texts.Nat. Commun.14, 54 (2023).\n11. Fujinuma, N., DeCost, B., Hattrick-Simpers, J. & Loﬂand, S. E. Why big\ndata and compute are not necessarily the path to big materials\nscience. Commun. Mater.3, 59 (2022).\n12. Wang, L. et al. A corpus of CO\n2 electrocatalytic reduction process\nextracted from the scientiﬁc literature.Sci. Data10, 175 (2023).\n13. Kononova, O. et al. Text-mined dataset of inorganic materials\nsynthesis recipes.Sci. Data6, 203 (2019).\n14. Brown, T. et al. Language models are few-shot learners.Adv. Neural\nInform. Process. Syst.33, 1877–1901 (2020).\n15. Walker, N. et al. Extracting structured seed-mediated gold nanorod\ngrowth procedures from scientiﬁc text with LLMs.Digi. Discov.2,\n1768–1782 (2023).\n16. Zheng, Z., Zhang, O., Borgs, C., Chayes, J. T. & Yaghi, O. M. ChatGPT\nChemistry Assistant for Text Mining and the Prediction of MOF\nSynthesis. J. Am. Chem. Soc.145, 18048–18062 (2023).\n17. Zheng, Z. et al. A GPT‐4 Reticular Chemist for Guiding MOF\nDiscovery. Angewandte Chemie Int. Edit.62, e202311983 (2023).\n18. Kononova, O. et al. Opportunities and challenges of text mining in\nmaterials research.Iscience 24, 102155 (2021).\n19. Keith, J. A. et al. Combining machine learning and computational\nchemistry for predictive insights into chemical systems.Chem. Rev.\n121, 9816–9872 (2021).\n20. Zhao, S. & Birbilis, N. Searching for chromate replacements using\nnatural language processing and machine learning algorithms.npj\nMater. Degrad.7, 2 (2023).\n21. Kim, J., Jang, S., Park, E. & Choi, S. Text classiﬁcation using capsules.\nNeurocomputing 376, 214–221 (2020).\n22. Huang, S. & Cole, J. M. BatteryBERT: A Pretrained Language Model\nfor Battery Database Enhancement.J. Chem. Inform. Model.62,\n6365–6377 (2022).\n23. Cruse, K. et al. Text-mined dataset of gold nanoparticle synthesis\nprocedures, morphologies, and size entities.Sci. Data9, 234 (2022).\n24. Wang, Z. et al. Dataset of solution-based inorganic materials\nsynthesis procedures extracted from the scientiﬁc literature.Sci. Data\n9, 231 (2022).\n25. Huang, S. & Cole, J. M. A database of battery materials auto-\ngenerated using ChemDataExtractor.Sci. Data7, 260 (2020).\n26. Huang, S. & Cole, J. M. BatteryDataExtractor: battery-aware text-\nmining software embedded with BERT models.Chem. Sci.13,\n11487–\n11495 (2022).\n27. Wilary, D. M. & Cole, J. M. ReactionDataExtractor 2.0: A deep learning\napproach for data extraction from chemical reaction schemes.J.\nChem. Inform. Model.63, 6053–6067 (2023).\n28. Swain, M. C. & Cole, J. M. ChemDataExtractor: a toolkit for automated\nextraction of chemical information from the scientiﬁc literature.J.\nChem. Inform. Model.56, 1894–1904 (2016).\n29. Manica, M. et al. An information extraction and knowledge graph\nplatform for accelerating biochemical discoveries.arXiv preprint\narXiv:1907.08400 (2019).\n30. Gupta, T., Zaki, M., Krishnan, N. A. & Mausam. MatSciBERT: A\nmaterials domain language model for text mining and information\nextraction. npj Comput. Mater.8, 102 (2022).\n31. Shetty, P. & Ramprasad, R. Automated knowledge extraction from\npolymer literature using natural language processing.Iscience 24,\n101922 (2021).\n32. Gao, Y., Wang, L., Chen, X., Du, Y. & Wang, B. Revisiting\nElectrocatalyst Design by a Knowledge Graph of Cu-Based Catalysts\nfor CO2 Reduction.ACS Catal.13, 8525–8534 (2023).\n33. Nie, Z. et al. Automating materials exploration with a semantic\nknowledge graph for Li‐ion battery cathodes.Adv. Funct. Mater.32,\n2201437 (2022).\n34. Li, J., Sun, A., Han, J. & Li, C. A survey on deep learning for named\nentity recognition.IEEE Trans. Knowledge Data Engineer.34,\n50–70 (2020).\n35. Yadav, V. & Bethard, S. A survey on recent advances in named\nentity recognition from deep learning models. InProceedings of\nthe 27th International Conference on Computational Linguistics\npp. 2145–2158 (2018).\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 10\n36. Beltagy, I., Lo, K. & Cohan, A. SciBERT: A pretrained language model\nfor scientiﬁc text. InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP)\n3615–3620 (2019).\n37. Shetty, P. et al. A general-purpose material property data extraction\npipeline from large polymer corpora using natural language\nprocessing. npj Comput. Mater.9, 52 (2023).\n38. Weston, L. et al. Named entity recognition and normalization applied\nto large-scale information extraction from the materials science\nliterature. J. Chem. Inform. Model59, 3692–3702 (2019).\n39. Shetty, P. & Ramprasad, R. Machine-guided polymer knowledge\nextraction using natural language processing: The example of named\nentity normalization.J. Chem. Inform. Model.61, 5377–5385 (2021).\n40. Lewis, P., Oguz, B., Rinott, R., Riedel, S. & Schwenk, H. In\nProceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics. 7315–7330.\n41. Zhang, Z. & Saligrama, V. InProceedings of the IEEE international\nconference on computer vision. 4166–4174.\n42. Yin, W., Hay, J. & Roth, D. InProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP). 3914–3923.\n43. Guo, C., Pleiss, G., Sun, Y. & Weinberger, K. Q. InInternational\nconference on machine learning. 1321–1330 (PMLR).\n44. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., &\nLakshminarayanan, B. Augmix: A simple data processing method to\nimprove robustness and uncertainty. InInternational Conference on\nLearning Representations(2019).\n45. Desai, S. & Durrett, G. InProceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP).\n295–302.\n46. Wang, S. et al. Gpt-ner: Named entity recognition via large language\nmodels. arXiv preprint arXiv:2304.10428(2023).\n47. Yang, Y. & Katiyar, A. Simple and effective few-shot named entity\nrecognition with structured nearest neighbor learning. InProceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP)6365–6375 (2020).\n48. Xu, K. Navigating the mineﬁeld of battery literature.Commun. Mater.\n3, 31 (2022).\n49. Duan, S. et al. Three-dimensional reconstruction and computational\nanalysis of a structural battery composite electrolyte.Commun.\nMater. 4, 49 (2023).\n50. Dai, F. & Cai, M. Best practices in lithium battery cell preparation and\nevaluation. Commun.s Mater.3, 64 (2022).\n51. Xie, T. et al. Large Language Models as Master Key: Unlocking the\nSecrets of Materials Science with GPT.arXiv preprint\narXiv:2304.02213 (2023).\n52. Polak, M. P. & Morgan, D. Extracting Accurate Materials Data from\nResearch Papers with Conversational Language Models and Prompt\nEngineering--Example of ChatGPT.arXiv preprint\narXiv:2303.05352 (2023).\n53. Polak, M. P. et al. Flexible, Model-Agnostic Method for Materials Data\nExtraction from Text Using General Purpose Language Models.arXiv\npreprint arXiv:2302.04914(2023).\n54. Li, B. et al. Evaluating ChatGPT’s Information Extraction Capabilities:\nAn Assessment of Performance, Explainability, Calibration, and\nFaithfulness. arXiv preprint arXiv:2304.11633(2023).\n55. Chen, L., Zaharia, M. & Zou, J. Analyzing ChatGPT’s Behavior Shifts\nOver Time. InR0-FoMo: Robustness of Few-shot and Zero-shot\nLearning in Large Foundation Models(2023).\n56. Kumar, S. A survey of deep learning methods for relation extraction.\narXiv preprint arXiv:1705.03645(2017).\n57. Tsai, R. T.-H. et al. InBMC bioinformatics.1 –14 (BioMed Central).\n58. Tsai, R. T.-H. et al. Various criteria in the evaluation of biomedical\nnamed entity recognition.BMC Bioinform.7,1 –8 (2006).\nAcknowledgements\nThis work was supported by the National Research Foundation of Korea\nfunded by the Ministry of Science and ICT (NRF-2021M3A7C2089739) and\nInstitutional Projects at the Korea Institute of Science and Technology\n(2E31742 and 2E32533).\nAuthor contributions\nJaewoong Choi: Conceptualisation, Methodology, Programming, Data\nanalysis, Visualisation, Interpretation, Writing– original draft, Writing– review\n& editing. Byungju Lee: Conceptualisation, Interpretation, Writing– review &\nediting, Supervision, Resources, Funding acquisition.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s43246-024-00449-9.\nCorrespondenceand requests for materials should be addressed to\nByungju Lee.\nPeerreviewinformation Communications Materialsthanks Shu Huang and\nthe other, anonymous, reviewer(s) for their contribution to the peer review of\nthis work. Primary Handling Editors: Milica Todorović and Aldo Isidori.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons license, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons license, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicense, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s43246-024-00449-9 Article\nCommunications Materials| (2024)5:13 11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8213565349578857
    },
    {
      "name": "Generative grammar",
      "score": 0.6783744096755981
    },
    {
      "name": "Transformer",
      "score": 0.6485245823860168
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5642534494400024
    },
    {
      "name": "Language model",
      "score": 0.5320694446563721
    },
    {
      "name": "Natural language processing",
      "score": 0.4914819002151489
    },
    {
      "name": "Deep learning",
      "score": 0.46891865134239197
    },
    {
      "name": "Context (archaeology)",
      "score": 0.45540767908096313
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4467059075832367
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4366168975830078
    },
    {
      "name": "Generative model",
      "score": 0.42506909370422363
    },
    {
      "name": "Machine learning",
      "score": 0.408273309469223
    },
    {
      "name": "Programming language",
      "score": 0.14996159076690674
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}