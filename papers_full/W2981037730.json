{
  "title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
  "url": "https://openalex.org/W2981037730",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222000348",
      "name": "Schlag, Imanol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227220492",
      "name": "Smolensky, Paul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4229233341",
      "name": "Fernandez, Roland",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222249727",
      "name": "Jojic, Nebojsa",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Schmidhuber, J\\\"urgen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119363152",
      "name": "Gao Jian-feng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221898969",
      "name": "Schmidhuber, Jürgen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2948771346",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2898996181",
    "https://openalex.org/W2952467640",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2150913357",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W1526844937",
    "https://openalex.org/W2963870701",
    "https://openalex.org/W2948184675",
    "https://openalex.org/W2089217417",
    "https://openalex.org/W2948293419",
    "https://openalex.org/W1501856433",
    "https://openalex.org/W2902442580",
    "https://openalex.org/W1026270304",
    "https://openalex.org/W2013494846",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2789352267",
    "https://openalex.org/W2104518905",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2891585150",
    "https://openalex.org/W2980433389",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W2912225506"
  ],
  "abstract": "We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.",
  "full_text": "Enhancing the Transformer With Explicit Relational\nEncoding for Math Problem Solving\nImanol Schlag∗1, Paul Smolensky2,3, Roland Fernandez2, Nebojsa Jojic2,\nJ¨urgen Schmidhuber1, Jianfeng Gao2\n1 The Swiss AI Lab IDSIA / USI / SUPSI,\n2 Microsoft Research, Redmond,\n3 Johns Hopkins University\n{imanol,juergen}@idsia.ch\n{psmo,rfernand,jojic,jfgao}@microsoft.com\nAbstract\nWe incorporate Tensor-Product Representa-\ntions within the Transformer in order to better\nsupport the explicit representation of relation\nstructure. Our Tensor-Product Transformer\n(TP-Transformer) sets a new state of the art on\nthe recently-introduced Mathematics Dataset\ncontaining 56 categories of free-form math\nword-problems. The essential component of\nthe model is a novel attention mechanism,\ncalled TP-Attention, which explicitly encodes\nthe relations between each Transformer cell\nand the other cells from which values have\nbeen retrieved by attention. TP-Attention\ngoes beyond linear combination of retrieved\nvalues, strengthening representation-building\nand resolving ambiguities introduced by\nmultiple layers of standard attention. The\nTP-Transformer’s attention maps give better\ninsights into how it is capable of solving the\nMathematics Dataset’s challenging problems.\nPretrained models and code are available\nonline1.\n1 Introduction\nIn this paper we propose a variation of the\nTransformer (Vaswani et al., 2017) that is designed\nto allow it to better incorporate structure into its\nrepresentations. We test the proposal on a task\nwhere structured representations are expected to be\nparticularly helpful: math word-problem solving,\nwhere, among other things, correctly parsing\nexpressions and compositionally evaluating them\nis crucial. Given as input a free-form math\nquestion in the form of a character sequence like\nLet r(g) be the second derivative\nof 2*g**3/3 - 21*g**2/2 + 10*g.\nLet z be r(7). Factor -z *s + 6 -\n9*s**2 + 0*s + 6*s**2., the model must\n∗Work partially done while at Microsoft Research.\n1github.com/ischlag/TP-Transformer\nproduce an answer matching the speciﬁed target\ncharacter-sequence -(s + 3)*(3*s - 2)\nexactly. Our proposed model is trained end-to-end\nand infers the correct answer for novel examples\nwithout any task-speciﬁc structural biases.\nWe begin by viewing the Transformer as a kind\nof Graph Neural Network (e.g., Gori et al., 2005;\nGoller and K ¨uchler, 1995; Battaglia et al., 2018).\nFor concreteness, consider the encoder component\nof a Transformer withHheads. When the hth head\nof a cell tof layer lissues a query and as a result\nconcentrates its self-attention distribution on an-\nother cell t′in layer l, we can view these two cells\nas joined by an edge in an information-ﬂow graph:\nthe information content att′in effect passes via this\nedge to affect the state of t. The strength of this\nattention can be viewed as a weight on this edge,\nand the indexhof the head can be viewed as a label.\nThus, each layer of the Transformer can be viewed\nas a complete, directed, weighted, labeled graph.\nPrior NLP work has interpreted certain edges of\nthese graphs in terms of linguistic relations (Sec. 8),\nand we wish to enrich the relation structure of these\ngraphs to better support the explicit representation\nof relations within the Transformer.\nHere we propose to replace each of the dis-\ncrete edge labels 1,...,H , with a relation vec-\ntor: we create a bona ﬁde representational space\nfor the relations being learned by the Transformer.\nThis makes it possible for the hidden representa-\ntion at each cell to approximate the vector embed-\nding of a symbolic structure built from the rela-\ntions generated by that cell. This embedding is a\nTensor-Product Representation (TPR; Smolen-\nsky, 1990) in an end-to-end-differentiable TPR sys-\ntem (Schlag and Schmidhuber, 2018; Schmidhu-\nber, 1993) that learns “internal spotlights of atten-\ntion” (Schmidhuber, 1993). TPRs provide a general\nmethod for embedding symbol structures in vector\nspaces. TPRs support compositional processing\narXiv:1910.06611v2  [cs.LG]  4 Nov 2020\nby directly encoding constituent structure: the rep-\nresentation of a structure is the sum of the repre-\nsentation of its constituents. The representation of\neach constituent is built compositionally from two\nvectors: one vector that embeds the content of the\nconstituent, the ‘ﬁller’— here, the vector returned\nby attention — and a second vector that embeds\nthe structural role it ﬁlls — here, a relation con-\nceptually labeling an edge of the attention graph.\nThe vector that embeds a ﬁller and the vector that\nembeds the role it ﬁlls are bound together by the\ntensor product to form the tensor that embeds the\nconstituent that they together deﬁne.2 The relations\nhere, and the structures they deﬁne, are learned un-\nsupervised by the Transformer in service of a task;\npost-hoc analysis is then required to interpret those\nroles.\nIn the new model, the TP-Transformer, each\nhead of each cell generates a key-, value- and query-\nvector, as in the Transformer, but additionally gen-\nerates a role-vector (which we refer to in some\ncontexts as a ‘relation vector’). The query is inter-\npreted as seeking the appropriate ﬁller for that role\n(or equivalently, the appropriate string-location for\nfulﬁlling that relation). Each head binds that ﬁller\nto its role via the tensor product (or some contrac-\ntion of it), and these ﬁller/role bindings are summed\nto form the TPR of a structure with H constituents\n(details in Sec. 2).\nAn interpretation of an actual learned relation\nillustrates this (see Fig. 3 in Sec. 5.2). One head\nof our trained model can be interpreted as partially\nencoding the relation second-argument-of. The top-\nlayer cell dominating an input digit seeks the oper-\nator of which the digit is in the second-argument\nrole. That cell generates a vector rt signifying this\nrelation, and retrieves a value vector vt′ describing\nthe operator from position t′that stands in this re-\nlation. The result of this head’s attention is then\nthe binding of ﬁller vt′ to role rt; this binding is\nadded to the bindings resulting from the cell’s other\nattention heads.\nOn the Mathematics Dataset (Sec. 3), the new\nmodel sets a new state of the art for the overall\naccuracy (Sec. 4). Initial results of interpreting the\nlearned roles for the arithmetic-problem module\nshow that they include a good approximation to\n2The tensor product operation (when the role-embedding\nvectors are linearly independent) enables the sum of con-\nstituents representing the structure as a whole to be uniquely\ndecomposable back into individual pairs of roles and their\nﬁllers, if necessary.\nthe second-argument role of the division operator\nand that they distinguish between numbers in the\nnumerator and denominator roles (Sec. 5).\nMore generally, it is shown that Multi-Head At-\ntention layers not only capture a subspace of the\nattended cell but capture nearly the full information\ncontent (Sec. 6.1). An argument is provided that\nmultiple layers of standard attention suffer from the\nbinding problem, and it is shown theoretically how\nthe proposed TP-Attention avoids such ambiguity\n(Sec. 6.2). The paper closes with a discussion of\nrelated work (Sec. 8) and a conclusion (Sec. 9).\n2 The TP-Transformer\nThe TP-Transformer’s encoder network, like the\nTransformer’s encoder (Vaswani et al., 2017), can\nbe described as a 2-dimensional lattice of cells(t,l)\nwhere t = 1,...,T are the sequence elements of\nthe input and l= 1,...,L are the layer indices with\nl = 0 as the embedding layer. All cells share the\nsame topology and the cells of the same layer share\nthe same weights. More speciﬁcally, each cell con-\nsists of an initial layer normalization (LN) followed\nby a TP-Multi-Head Attention (TPMHA) sub-\nlayer followed by a fully-connected feed-forward\n(FF) sub-layer. Each sub-layer is followed by layer\nnormalization (LN) and by a residual connection\n(as in the original Transformer). Our cell struc-\nture follows directly from the ofﬁcial TensorFlow\nsource code by (Vaswani et al., 2017) but with regu-\nlar Multi-Head Attention replaced by our TPMHA\nlayer.\n2.1 TP-Multi-Head Attention\nThe TPMHA layer of the encoder consists of H\nheads that can be applied in parallel. Every head\nh,1 ≤h≤H applies separate afﬁne transforma-\ntions Wh,(k)\nl ,Wh,(v)\nl ,Wh,(q)\nl ,Wh,(r)\nl ∈Rdk×dz ,\nbh,(k)\nl ,bh,(v)\nl ,bh,(q)\nl ,bh,(r)\nl ∈ Rdk to produce key,\nvalue, query, and relation vectors from the hidden\nstate zt,l, where dk = dz/H:\nkh\nt,l = Wh,(k)\nl zt,l + bh,(k)\nl\nvh\nt,l = Wh,(v)\nl zt,l + bh,(v)\nl\nqh\nt,l = Wh,(q)\nl zt,l + bh,(q)\nl\nrh\nt,l = Wh,(r)\nl zt,l + bh,(r)\nl\n(1)\nThe ﬁller of the attention head t,l,h is\n¯vh\nt,l =\nT∑\ni=1\nvh\ni,lαh,i\nt,l , (2)\ni.e., a weighted sum of all T values of the same\nlayer and attention head (see Fig. 1). Here αh,i\nt,l ∈\n(0,1) is a continuous degree of match given by\nthe softmax of the dot product between the query\nvector at position tand the key vector at position i:\nαh,i\nt,l =\nexp(qh\nt,l ·kh\ni,l\n1√dk\n)\n∑T\ni′=1 exp(qh\nt,l ·kh\ni′,l\n1√dk\n)\n(3)\nThe scale factor 1√dk\ncan be motivated as a\nvariance-reducing factor under the assumption that\nthe elements of qh\nt,l and kh\nt,l are uncorrelated vari-\nables with mean 0 and variance 1, in order to ini-\ntially keep the values of the softmax in a region\nwith better gradients (Vaswani et al., 2017).\nFinally, we bind the ﬁller ¯vh\nt,l with our relation\nvector rh\nt,l, followed by an afﬁne transformation\nW(o)\nh,l ∈Rdz×dk ,b(o)\nh,l ∈Rdz before it is summed\nup with the other heads’ bindings to form the TPR\nof a structure withHconstituents: this is the output\nof the TPMHA layer.\nTPMHA(zt,l,z1:T,l )\n=\n∑\nh\n[\nW(o)\nh,l (¯vh\nt,l ⊙rh\nt,l) + b(o)\nh,l\n]\n(4)\nNote that, in this binding, to control dimension-\nality, we use a contraction of the tensor product,\npointwise multiplication ⊙: this is the diagonal\nof the tensor product. For discussion, see the Ap-\npendix.\nFigure 1: A simpliﬁed illustration of our TP-Attention\nmechanism for one head at position t in layer l. The\nmain difference from standard Attention is the addi-\ntional role representation that is element-wise multi-\nplied with the ﬁller/value representation.\nIt is worth noting that the lth TPMHA layer re-\nturns a vector that is quadratic in the inputs zt,l\nto the layer: the vectors vh\ni,l that are linearly com-\nbined to form ¯vh\nt,l (Eq. 2), and rh\nt,l, are both linear\nin the zi,l (Eq. 1), and they are multiplied together\nto form the output of TPMHA (Eq. 4). This means\nthat, unlike regular attention, TPMHA can increase,\nover successive layers, the polynomial degree of\nits representations as a function of the original in-\nput to the Transformer. Although it is true that the\nfeed-forward layer following attention (Sec. 2.2)\nintroduces its own non-linearity even in the stan-\ndard Transformer, in the TP-Transformer the at-\ntention mechanism itself goes beyond mere linear\nre-combination of vectors from the previous layer.\nThis provides further potential for the construction\nof increasingly abstract representations in higher\nlayers.\n2.2 Feed-Forward Layer\nThe feed-forward layer of a cell consists of an afﬁne\ntransformation followed by a ReLU activation and\na second afﬁne transformation:\nFF(x) = W(g)\nl ReLU(W(f)\nl x+b(f)\nl )+b(g)\nl (5)\nHere, W(f)\nl ∈ Rdf ×dz ,b(f)\nl ∈ Rdf ,W(g)\nl ∈\nRdz×df ,b(g)\nl ∈Rdz and x is the function’s argu-\nment. As in previous work, we set df = 4dz.\n2.3 The Decoder Network\nThe decoder network is a separate network with\na similar structure to the encoder that takes the\nhidden states of the encoder and auto-regressively\ngenerates the output sequence. In contrast to the\nencoder network, the cells of the decoder contain\ntwo TPMHA layers and one feed-forward layer.\nWe designed our decoder network analogously to\n(Vaswani et al., 2017) where the ﬁrst attention layer\nattends over the masked decoder states while the\nsecond attention layer attends over the ﬁnal en-\ncoder states. During training, the decoder network\nreceives the shifted targets (teacher-forcing) while\nduring inference we use the previous symbol with\nhighest probability (greedy-decoding). The ﬁnal\nsymbol probability distribution is given by\nˆyˆt = softmax(ET ˆzˆt,L) (6)\nwhere ˆzˆt,L is the hidden state of the last layer of\nthe decoder at decoding step ˆt of the output se-\nquence and E is the shared symbol embedding of\nthe encoder and decoder.\n3 The Mathematics Dataset\nThe Mathematics Dataset (Saxton et al., 2019) is a\nlarge collection of math problems of various types,\nincluding algebra, arithmetic, calculus, numerical\ncomparison, measurement, numerical factorization,\nand probability. Its main goal is to investigate the\ncapability of neural networks to reason formally.\nEach problem is structured as a character-level\nsequence-to-sequence problem. The input se-\nquence is a free-form math question or command\nlike What is the first derivative\nof 13*a**2 - 627434*a + 11914106?\nfrom which our model correctly predicts the\ntarget sequence 26*a - 627434. Another\nexample from a different module is Calculate\n66.6*12.14. which has 808.524 as its target\nsequence.\nThe dataset is structured into 56 modules which\ncover a broad spectrum of mathematics up to uni-\nversity level. It is procedurally generated and\ncomes with 2 million pre-generated training sam-\nples per module. The authors provide an interpo-\nlation dataset for every module, as well as a few\nextrapolation datasets as an additional measure of\nalgebraic generalization.\nWe merge the different training splits train-easy,\ntrain-medium, and train-hard from all modules\ninto one big training dataset of 120 million unique\nsamples. From this dataset we extract a character-\nlevel vocabulary of 72 symbols, including start-of-\nsentence, end-of-sentence, and padding symbols3.\n4 Experimental Results\nWe evaluate our trained model on the concatenated\ninterpolation and extrapolation datasets of the\npre-generated ﬁles, achieving a new state of the\nart (see Table 1). A more detailed comparison of\nthe interpolation and extrapolation performance\nfor every module separately can be found in the\nsupplementary material. Throughout 1.0 million\ntraining steps, the interpolation error on the\nheld-out data was strictly decreasing. We trained\non one machine with 8 P100 Nvidia GPUs for\n10 days. Preliminary experiments of 2.0 million\ntraining steps indicates that the interpolation\naccuracy of the TP-Transformer can be further\nimproved to at least 84.24%.\n4.1 Implementation Details\nThe TP-Transformer uses the same hyper-\nparameters as the regular Transformer ( dz =\n3Note that (Saxton et al., 2019) report a vocabulary size of\n95, but this ﬁgure encompasses characters that never appear\nin the pre-generated training and test data.\n512,df = 2048,H = 8,L = 6). Due to the use of\nthe TP-Attention this results in a larger number of\ntrainable weights. For this reason we also include\ntwo hyper-parameter settings with fewer trainable\nweights. TP-Transformer B shrinks the hidden-\nstate size and ﬁlter size from a multiple of 64 to\na multiple of 60 ( dz = 480 ,df = 1920 ) which\nresults in 1.2 million fewer trainable weights than\nthe baseline and the TP-Transformer C shrinks the\nﬁlter size more aggressively down to a total of 14.2\nmillion trainable weights (/tildelow32% fewer weights) by\nmassively reducing the ﬁlter size while keeping the\nhidden state size the same (dz = 512,df = 512).\nWe initialize the symbol embedding matrix E\nfrom N(0,1), W(p) from N(1,1), and all other\nmatrices W(·) using the Xavier uniform initializa-\ntion as introduced by (Glorot and Bengio, 2010).\nWe were not able to train the TP-Transformer, nor\nthe regular Transformer, using the learning rate\nand gradient clipping scheme described by (Sax-\nton et al., 2019). Instead we proceed as follows:\nThe gradients are computed using PyTorch’s Au-\ntograd engine and their gradient norm is clipped\nat 0.1. The optimizer we use is also Adam, but\nwith a smaller learning rate = 1 ×10−4,beta1 =\n0.9,beta2 = 0.995. We train with a batch size of\n1024.\n5 Interpreting the Learned Structure\nWe report initial results of analyzing the learned\nstructure of the encoder network’s last layer after\ntraining the TP-Transformer for 700k steps.\n5.1 Interpreting the Learned Roles\nTo this end, we sample 128 problems from the inter-\npolation dataset of the arithmetic mixed module\nand collect the role vectors from a randomly chosen\nhead. We use k-means with k= 20 to cluster the\nrole vectors from different samples and different\ntime steps of the ﬁnal layer of the encoder. Inter-\nestingly, we ﬁnd separate clusters for digits in the\nnumerator and denominator of fractions. When\nthere is a fraction of fractions we can observe that\nthese assignments are placed such that the second\nfraction reverses, arguably simplifying the division\nof fractions into a multiplication of fractions (see\nFig. 2).\n5.2 Interpreting the Attention Maps\nIn Fig. 3 we display three separate attention weight\nvectors of one head of the last TP-Transformer\nWeights Steps Train Interpolation Extrapolation\nacc >95% acc >95%\nLSTM with thinking steps (Saxton et al.) 18M 500k - 57.00% 6 41.00% 1\nTransformer (Saxton et al.) 30M 500k - 76.00% 13 50.00% 1\nTransformer (ours) 44.2M 1000k 86.60% 79.54% 16 53.28% 2\nTP-Transformer (ours) 49.1M 1000k 89.01% 81.92% 18 54.67% 3\nTP-Transformer B (ours) 43.0M 1000k 87.53% 80.52% 16 52.04% 1\nTP-Transformer C (ours) 30.0M 1000k 86.33% 79.02% 14 54.71% 1\nTable 1: Model accuracy averaged over all modules. A sample is correct if all characters of the target sequence have\nbeen predicted correctly. The column “ >95%” counts how many of the 56 modules achieve over 95% accuracy.\nTP-Transformer B and C differ from the standard hyper-parameters in order to reduce the total number of weights.\nSee section 4.1 for more details.\nFigure 2: Samples of correctly processed problems from the arithmetic mixed module. ‘#’ and ‘%’ are the start-\nand end-of-sentence symbols. The colored squares indicate the k-means cluster of the role-vector assigned by one\nhead in the ﬁnal layer in that position. Blue and gold rectangles respectively highlight numerator and denominator\nroles. They were discovered manually. Note how their placement is correctly swapped in rows 2, 3, and 4, where a\nnumber in the denominator of a denominator is treated as if in a numerator. Role-cluster 9 corresponds to the role\nones-digit-of-a-numerator-factor, and 6 to ones-digit-of-a-denominator-factor; other such roles are also evident.\nFigure 3: TP-Transformer attention maps for three examples as described in section 5.2.\nlayer of the encoder. Gold boxes are overlaid to\nhighlight most-relevant portions. The row above\nthe attention mask indicates the symbols that take\ninformation to the symbol in the bottom row. In\neach case, they take from ‘/’. Seen most simply in\nthe ﬁrst example, this attention can be interpreted\nas encoding a relation second-argument-to holding\nbetween the querying digits and the ‘/’ operator.\nThe second and third examples show that several\nnumerals in the denominator can participate in this\nrelation. The third display shows how a numerator-\nnumeral ( -297) intervening between two\ndenominator-numerals is skipped for this relation.\n6 Deﬁcits of Multi-Head Attention\nLayers\n6.1 Multi-Head Attention Subspaces Capture\nVirtually All Information\nIt was claimed by (Vaswani et al., 2017) that Multi-\nhead attention allows the model to jointly attend\nto information from different representation sub-\nspaces at different positions. In this section, we\nshow that in our trained models, an individual at-\ntention head does not access merely a subset of the\ninformation in the attended cell but instead captures\nnearly the full information content.\nLet us consider a toy example where the atten-\ntion layer of cellt,l only attends to cellt′,l. In this\nsetting, the post-attention representation simpliﬁes\nand becomes\nzt,l + W(o)\nl (W(v)\nl zt′,l + b(v)\nl ) + b(o)\nl\n= zt,l + o(v(zt′,l))\n(7)\nwhere oand vare the respective afﬁne maps (see\nSec. 2.1). Note that even though W(v)\nl is a projec-\ntion into an 8 times smaller vector space, it remains\nto be seen whether the hidden state loses infor-\nmation about zt′,l. We empirically test to what\nextent the trained Transformer and TP-Transformer\nlose information. To this end, we randomly select\nn = 100 samples and extract the hidden state of\nthe last layer of the encoder zt,6, as well as the\nvalue representation vh(zt,6) for every head. We\nthen train an afﬁne model to reconstruct zt,6 from\nvh(zt,6), the value vector of the single head h:\nˆzt,6 = Whvh(zt,6) + bh\ne= 1\nn(ˆzt,6 −zt,6)2 (8)\nFor both trained models, the TP-Transformer and\nthe regular Transformer, the mean squared error\ne averaged across all heads is only /tildelow0.017 and\n/tildelow0.009 respectively. Note that, because of layer\nnormalization, the relevant quantities are scaled to√dz. This indicates that the attention mechanism\nincorporates not just a subspace of the states it at-\ntends to, but afﬁne transformations of those states\nthat preserve nearly the full information content.\nIn such a case, the attention mechanism can be\ninterpreted as the routing of multiple local informa-\ntion source into one global tree structure of local\nrepresentations.\n6.2 The Binding Problem of Stacked\nAttention Layers\nThe binding problem refers to the problem of bind-\ning features together into objects while keeping\nthem separated from other objects. It has been\nstudied in the context of theoretical neuroscience\n(von der Malsburg, 1981, 1994) but also with re-\ngards to connectionist machine learning models\n(Hinton et al., 1984). The purpose of a binding\nmechanism is to enable the fully distributed rep-\nresentation of symbolic structure (like a hierarchy\nof features) which has recently resurfaced as an\nimportant direction for neural network research\n(Lake and Baroni, 2017; Bahdanau et al., 2018;\nvan Steenkiste et al., 2019; Palangi et al., 2017;\nTang et al., 2018).\nIn this section, we describe how the standard at-\ntention mechanism is ill suited to capture complex\nnested representations, and we provide an intuitive\nunderstanding of the beneﬁt of our TP-Attention.\nWe understand the attention layer of a cell as the\nmeans by which the subject (the cell state) queries\nall other cells for an object. We then show how\na hierarchical representation of multiple queries\nbecomes ambiguous in multiple standard attention\nlayers.\nConsider the string (a/b)/(c/d). A good\nneural representation captures the hierarchical\nstructure of the string such that it will not be con-\nfused with the similar-looking but structurally dif-\nferent string (a/d)/(c/b). Our TP-Attention\nmakes use of a binding mechanism in order to\nexplicitly support complex structural relations by\nbinding together the object representations receiv-\ning high attention with a subject-speciﬁc role rep-\nresentation. Let us continue with a more techni-\ncal example. Consider a simpliﬁed Transformer\nnetwork where every cell consists only of a single-\nhead attention layer with a residual connection: no\nfeed-forward layer or layer normalization, and let\nus assume no bias terms in the maps ol and vl in-\ntroduced in the previous section (Eq. 7). In this\nsetting, assume that cella,l only attends to cellb,l,\nand cellc,l only attends to celld,l where a,b,c,d are\ndistinct positions of the input sequence. In this case\nza,l+1 = za,l + ol(vl(zb,l))\nzc,l+1 = zc,l + ol(vl(zd,l)) (9)\nSuppose now that, for hierarchical grouping, the\nnext layer celle,l+1 attends to both cella,l+1 and\ncellc,l+1 (equally, each with attention weight 1\n2 ).\nThis results in the representation\nze,l+2 = ze,l+1\n+ ol+1(vl+1(za,l+1 + zc,l+1))/2\n= ze,l+1\n+ ol+1(vl+1(za,l + zc,l\n+ ol(vl(zb,l)) + ol(vl(zd,l))))/2\n(10)\nNote that the ﬁnal representation is ambiguous\nin the sense that it is unclear by looking only at\nEq. 10 whether cella,l has picked cellb,l or celld,l.\nEither scenario would have led to the same out-\ncome, which means that the network would not\nbe able to distinguish between these two different\nstructures (as in confusing (a/b)/(c/d) with\n(a/d)/(c/b)). In order to resolve this ambi-\nguity, the standard Transformer must recruit other\nattention heads or ﬁnd suitable non-linear maps in\nbetween attention layers, but it remains uncertain\nhow the network might achieve a clean separation.\nOur TP-Attention mechanism, on the other hand,\nspeciﬁcally removes this ambiguity. Now Eqs. 9\nand 10 become:\nza,l+1 = za,l + ol(vl(zb,l) ⊙ra,l)\nzc,l+1 = zc,l + ol(vl(zd,l) ⊙rc,l)\nze,l+2 = ze,l+1 + ol+1(vl+1(za,l + zc,l\n+ ol(vl(zb,l) ⊙ra,l)\n+ ol(vl(zd,l) ⊙rc,l)))/2\n(11)\nNote that the ﬁnal representation is not ambiguous\nanymore. Binding the ﬁller symbols vl(z) (our ob-\njects) with a subject-speciﬁc role representation r\nas described in Eq. 4 breaks the structural symme-\ntry we had with regular attention. It is now simple\nfor the network to speciﬁcally distinguish the two\ndifferent structures.\n7 Hadamard-Product Attention as an\nOptimal Approximation of\nTensor-Product Attention\nIn Eq. 4 for TPMHA, we have a sum over all H\nheads of an afﬁne-transformed product of a value\nvector vh and a role vector rh. (Throughout this\ndiscussion, we leave the subscripts t,l implicit, as\nwell as the over-bar on vh in Eq. 4.) In a hypothet-\nical, full-TPR formulation, this product would be\nthe tensor product vh ⊗rh, although in our actual\nproposed TP-Transformer, the Hadamard (elemen-\ntwise) product vh ⊙rh (the diagonal of vh ⊗rh)\nis used. The appropriateness of the compression\nfrom tensor product to Hadamard product can be\nseen as follows.\nIn the hypothetical full-TPR version of TPMHA,\nattention would return the sum of H tensor prod-\nucts. This tensor A would have rank at most H,\npotentially enabling a substantial degree of com-\npression across all tensors the model will compute\nover the data of interest. Given the translation-\ninvariance built into the Transformer via position-\ninvariant parameters, the same compression must\nbe applied in all positions within a given layer l,\nalthough the compression may vary across heads.\nFor the compression of A we will need more than\nH components, as this decomposition needs to be\noptimal over all all tensors in that layer for all data\npoints.\nIn detail, for each head h, the compression of the\ntensor Ah = vh ⊗rh (or matrix Ah = vhr⊤\nh ) is\nto dimension dk, which will ultimately be mapped\nto dimension dz (to enable addition with z via the\nresidual connection) by the afﬁne transformation\nof Eq. 4 . The optimal dk-dimensional compres-\nsion for head h at layer l would preserve the dk\ndominant dimensions of variance of the attention-\ngenerated states for that head and layer, across all\npositions and inputs: a kind of singular-value de-\ncomposition retaining those dimensions with the\nprincipal singular values. Denote these principal\ndirections by {mh\nc ⊗nh\nc |c = 1,...,d k}, and let\nMh and Nh respectively be the dk ×dk matrices\nwith the orthonormal vectors {mh\nc }and {nh\nc }as\ncolumns. (Note that orthonormality implies that\nM⊤\nh Mh = I and N⊤\nh Nh = I, with I the dk ×dk\nidentity matrix.)\nThe compression of Ah, ˆAh, will lie within\nthe space spanned by these dk tensor products\nmh\nc ⊗nh\nc , i.e., ˆAh = ∑dk\nc=1 ˜ah\nc mc ⊗nc; in ma-\ntrix form, ˆAh = Mh ˜AhN⊤\nh , where ˜Ah is the\ndk ×dk diagonal matrix with elements ˜ah\nc . Thus\nthe dk dimensions {˜ah\nc }of the compressed matrix\nˆAh that approximates Ah are given by:\n˜Ah = M⊤\nh ˆAhNh\n≈M⊤\nh AhNh\n= M⊤\nh vhr⊤\nh Nh\n= (M⊤\nh vh)(N⊤\nh rh)⊤\n(12)\n˜ah\nc =\n[\n˜Ah\n]\ncc\n≈[M⊤\nh vh]c[N⊤\nh rh]c\n= [˜vh ⊙˜rh]c\n(13)\nwhere ˜vh = M⊤\nh vh, ˜rh = N⊤\nh rh. Now\nfrom Eq. 1, vh = Wh,(v)z + bh,(v), so\n˜vh = M⊤\nh (Wh,(v)z + bh,(v)). Thus by chang-\ning the parameters Wh,(v), bh,(v) to ˜Wh,(v) =\nM⊤\nh Wh,(v), ˜bh,(v) = M⊤\nh bh,(v), and analo-\ngously for the role parameters Wh,(r), bh,(r), we\nconvert our original hypothetical TPR attention\ntensor Ah to its optimal dk-dimensional approxi-\nmation, in which the tensor product of the origi-\nnal vectors vh, rh is replaced by the Hadamard\nproduct of the linearly-transformed vectors ˜vh, ˜rh.\nTherefore, in the proposed model, which deploys\nthe Hadamard product, learning simply needs to\nconverge to the parameters ˜Wh,(v), ˜bh,(v) rather\nthan the parameters Wh,(v), bh,(v).\n8 Related Work\nSeveral recent studies have shown that the\nTransformer-based language model BERT (Devlin\net al., 2018) captures linguistic relations such as\nthose expressed in dependency-parse trees. This\nwas shown for BERT’s hidden activation states (He-\nwitt and Manning, 2019; Tenney et al., 2019) and,\nmost directly related to the present work, for the\ngraph implicit in BERT’s attention weights (Co-\nenen et al., 2019; Lin et al., 2019). Future work ap-\nplying the TP-Transformer to language tasks (like\nthose on which BERT is trained) will enable us to\nstudy the connection between the explicit relations\n{rh\nt,l}the TP-Transformer learns and the implicit\nrelations that have been extracted from BERT.\nMultiplicative states have been used in neu-\nral networks before (Ivakhnenko and Lapa, 1965;\nIvakhnenko, 1971). The Hadamard-Product Atten-\ntion bears similarity to neural network gates which\nhave been shown to be effective mechanism to rep-\nresent complex states in recurrent models (Hochre-\niter and Schmidhuber, 1997) and feed-forward\nmodels (Srivastava et al., 2015). Recent work\npresents the Gated Transformer-XL model which\nincorporates a gating layer after the Multi-Head At-\ntention layer of the regular Transformer (Parisotto\net al., 2019). Unlike the regular Transformer, the\nGated Transformer-XL is shown to be stable in a\nreinforcement learning domain while matching or\noutperforming recurrent baselines. The main dif-\nference is that the TP-Attention binds the values\nof different heads before summation whereas the\nGated Transformer-XL gates simply the output of\nthe attention-layer.\nVery recent work proposes a Transformer varia-\ntion which merges the self-attention layer and the\nfully-connected layer into a new all-attention layer\n(Sukhbaatar et al., 2019). To achieve this, the self-\nattention layer is extended with keys and values\nwhich are learned by gradient descent instead of\nusing neural network activations. The feed-forward\nlayers are then removed. Our binding-problem ar-\ngument (Sec. 6.2) applies to this architecture even\nmore so than to the regular Transformer since there\nare no non-linear maps in-between attention lay-\ners that could possibly learn to resolve ambiguous\ncases.\nPrevious work used Multi-Head Attention also\nin recurrent neural networks in order to perform\ncomplex relational reasoning (Santoro et al., 2018).\nIn their work, Multi-Head Attention is used to al-\nlow memories to interact with each other. They\ndemonstrate beneﬁts on program evaluation and\nmemorization tasks. Technically, the TP-Attention\nis not limited to the Transformer architecture and\nas such the beneﬁts could possibly carry over to any\nconnectionist model that makes use of Multi-Head\nAttention.\n9 Conclusion\nWe have introduced the TP-Transformer, which\nenables the powerful Transformer architecture to\nlearn to explicitly encode structural relations using\nTensor-Product Representations. On the novel and\nchallenging Mathematics Dataset, TP-Transformer\nbeats the previously published state of the art and\nour initial analysis of this model’s ﬁnal layer sug-\ngests that the TP-Transformer naturally learns to\ncluster symbol representations based on their struc-\ntural position and relation to other symbols.\n10 Acknowledgments\nWe thank the anonymous reviewers for their valu-\nable comments. This research was supported by\nan European Research Council Advanced Grant\n(no: 742870).\nReferences\nDzmitry Bahdanau, Shikhar Murty, Michael\nNoukhovitch, Thien Huu Nguyen, Harm de Vries,\nand Aaron Courville. 2018. Systematic generaliza-\ntion: What is required and can it be learned? arXiv\npreprint arXiv:1811.12889.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst,\nAlvaro Sanchez-Gonzalez, Vinicius Zambaldi, Ma-\nteusz Malinowski, Andrea Tacchetti, David Raposo,\nAdam Santoro, Ryan Faulkner, et al. 2018. Rela-\ntional inductive biases, deep learning, and graph net-\nworks. arXiv preprint arXiv:1806.01261.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda Vi´egas, and Martin Watten-\nberg. 2019. Visualizing and measuring the geometry\nof BERT. arXiv preprint arXiv:1906.02715.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth interna-\ntional conference on artiﬁcial intelligence and statis-\ntics, pages 249–256.\nChristoph Goller and Andreas K ¨uchler. 1995. Learn-\ning Task-Dependent Distributed Representations by\nBackpropagation Through Structure. AR-Report\nAR-95-02, Institut f ¨ur Informatik, Technische Uni-\nversit¨at M¨unchen.\nMarco Gori, Gabriele Monfardini, and Franco Scarselli.\n2005. A new model for learning in graph domains.\nIn Proceedings of the IEEE International Joint Con-\nference on Neural Networks , volume 2, pages 729–\n734. IEEE.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nGeoffrey E Hinton, James L McClelland, David E\nRumelhart, et al. 1984. Distributed representations.\nCarnegie-Mellon University Pittsburgh, PA.\nS. Hochreiter and J. Schmidhuber. 1997. Long Short-\nTerm Memory. Neural Computation , 9(8):1735–\n1780.\nAlekse˘ı Grigor ˆE¹evich Ivakhnenko and\nValentin Grigor ´evich Lapa. 1965. Cybernetic\nPredicting Devices. CCM Information Corporation.\nAleksey Grigorievitch Ivakhnenko. 1971. Polynomial\ntheory of complex systems. IEEE Transactions on\nSystems, Man and Cybernetics, (4):364–378.\nBrenden M Lake and Marco Baroni. 2017. General-\nization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\narXiv preprint arXiv:1711.00350.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. arXiv preprint arXiv:1906.01698.\nChristoph von der Malsburg. 1981. The correlation the-\nory of brain function (internal report 81-2). Goettin-\ngen: Department of Neurobiology, Max Planck Inti-\ntute for Biophysical Chemistry.\nChristoph von der Malsburg. 1994. The correlation the-\nory of brain function. In E. Domany, J. L. van Hem-\nmen, and K. Schulten, editors, Models of neural net-\nworks II, pages 95–119. Springer, Berlin.\nHamid Palangi, Paul Smolensky, Xiaodong He, and\nLi Deng. 2017. Deep learning of grammatically-\ninterpretable representations through question-\nanswering. arXiv preprint arXiv:1705.08432.\nEmilio Parisotto, H Francis Song, Jack W Rae, Razvan\nPascanu, Caglar Gulcehre, Siddhant M Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, Aidan\nClark, Seb Noury, et al. 2019. Stabilizing trans-\nformers for reinforcement learning. arXiv preprint\narXiv:1910.06764.\nAdam Santoro, Ryan Faulkner, David Raposo, Jack\nRae, Mike Chrzanowski, Theophane Weber, Daan\nWierstra, Oriol Vinyals, Razvan Pascanu, and Timo-\nthy Lillicrap. 2018. Relational recurrent neural net-\nworks. In Advances in neural information process-\ning systems, pages 7299–7310.\nDavid Saxton, Edward Grefenstette, Felix Hill, and\nPushmeet Kohli. 2019. Analysing mathematical rea-\nsoning abilities of neural models. In International\nConference on Learning Representations.\nImanol Schlag and J ¨urgen Schmidhuber. 2018. Learn-\ning to reason with third order tensor products. In\nAdvances in Neural Information Processing Systems\n(NeurIPS), pages 9981–9993.\nJ. Schmidhuber. 1993. On decreasing the ratio between\nlearning complexity and number of time-varying\nvariables in fully recurrent nets. In Proceedings\nof the International Conference on Artiﬁcial Neural\nNetworks, Amsterdam, pages 460–463. Springer.\nP. Smolensky. 1990. Tensor product variable binding\nand the representation of symbolic structures in con-\nnectionist systems. Artif. Intell., 46(1-2):159–216.\nRupesh Kumar Srivastava, Klaus Greff, and Juergen\nSchmidhuber. 2015. Training very deep networks.\nIn Advances in Neural Information Processing Sys-\ntems (NIPS) . Preprint arXiv:1505.00387 [cs.LG],\nMay 2015.\nSjoerd van Steenkiste, Klaus Greff, and J¨urgen Schmid-\nhuber. 2019. A perspective on objects and system-\natic generalization in model-based rl. arXiv preprint\narXiv:1906.01035.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019.\nAugmenting self-attention with persistent memory.\narXiv preprint arXiv:1907.01470.\nShuai Tang, Paul Smolensky, and Virginia R de Sa.\n2018. Learning distributed representations of sym-\nbolic structure using binding and unbinding opera-\ntions. arXiv preprint arXiv:1810.12456.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. arXiv\npreprint arXiv:1905.05950.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nA General Considerations\nIn the version of the TP-Transformer studied in\nthis paper, binding of relations r to their values\nv is not done by the tensor product, v ⊗r, as in\nfull TPRs. Rather, a contraction of the full TPR\nis used: the diagonal, which is the elementwise or\nHadamard product v ⊙r.4 To what extent does\nHadamard-product binding share relevant proper-\nties with tensor-product binding?\nA crucial property of the tensor product for its\nuse in vector representations of structure is that\na structure like a/b is not confusable with b/a,\nunlike the frequently-used bag-of-words encoding:\nin the BOW encoding ofa/b, the pair of arguments\nto the operator are encoded simply as a + b, where\na and b are respectively the vector encodings of\na and b. Obviously, this cannot be distinguished\nfrom the BOW encoding of the argument pair in\nb/a, b + a. (Hence the name, symbol “bag”, as\nopposed to symbol “structure”.)\nIn a tensor-product representation of the argu-\nment pair in a/b, we have a⋆rn +b⋆rd, where rn\nand rd are respectively distinct vector embeddings\nof the numerator (or ﬁrst-argument) and denomina-\ntor (or second-argument) roles, and ⋆is the tensor\nproduct. This is distinct from a⋆rd+b⋆rn, the em-\nbedding of the argument-pair inb/a. (In Sec. 6.2 of\nthe paper, an aspect of this general property, in the\ncontext of attention models, is discussed. In Sec. 5,\nvisualization of the roles and the per-role-attention\nshow that this particular distinction, between the\nnumerator and denominator roles, is learned and\nused by the trained TP-Transformer model.)\nThis crucial property of the tensor product, that\na ⋆rn + b ⋆rd ̸= a ⋆rd + b ⋆rn, is shared by the\nHadamard product: if we now take ⋆to represent\nthe Hadamard product, the inequality remains true.\nTo achieve this important property, the full tensor\nproduct is not required: the Hadamard product is\nthe diagonal of the tensor product, which retains\nmuch of the product structure of the tensor prod-\nuct. In any application, it is an empirical question\nhow much of the full tensor product is required\nto successfully encode distinctions between bind-\nings of symbols to roles; in the TP-Transformer, it\nturns out that the diagonal of the tensor product is\nsufﬁcient to get improvement in performance over\nhaving no symbol-role-product structure at all. Un-\n4This is a vector, and should not be confused with the inner\nproduct v · r which is a scalar: the inner product is the sum\nof all the elements of the Hadamard product.\nfortunately, the compute requirements of training\non the Mathematics Dataset currently makes using\nthe full tensor product infeasible, unless the vector\nrepresentations of symbols and roles are reduced\nto dimensions that proved to be too small for the\ntask. When future compute makes it possible, we\nexpect that expanding from the diagonal to the full\ntensor product will provide further improvement in\nperformance and interpretability.\nB Detailed Test Accuracy\nFigure 4: Test accuracy on the interpolation test-set of the Mathematics dataset. TPT refers to the TP-Transformer\nvariations as introduced in section 4.1. TF refers to our implementation of the Transformer (Vaswani et al., 2017).\nFigure 5: Test accuracy on the extrapolation test-set of the Mathematics dataset. TPT refers to the TP-Transformer\nvariations as introduced in section 4.1. TF refers to our implementation of the Transformer (Vaswani et al., 2017).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6910457015037537
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5758412480354309
    },
    {
      "name": "Computer science",
      "score": 0.4500024914741516
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3627989888191223
    },
    {
      "name": "Mathematics",
      "score": 0.321209192276001
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1591649353504181
    },
    {
      "name": "Engineering",
      "score": 0.13064461946487427
    },
    {
      "name": "Electrical engineering",
      "score": 0.10550710558891296
    },
    {
      "name": "Voltage",
      "score": 0.05254665017127991
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2614128279",
      "name": "Dalle Molle Institute for Artificial Intelligence Research",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 37
}