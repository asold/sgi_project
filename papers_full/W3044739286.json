{
    "title": "Named entity recognition in chemical patents using ensemble of contextual language models",
    "url": "https://openalex.org/W3044739286",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2750502976",
            "name": "Copara Jenny",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222437765",
            "name": "Naderi, Nona",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287000996",
            "name": "Knafou, Julien",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2750603417",
            "name": "Ruch Patrick",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119302058",
            "name": "Teodoro, Douglas",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3114551148",
        "https://openalex.org/W2347081127",
        "https://openalex.org/W2734608416",
        "https://openalex.org/W2101553882",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2080848531",
        "https://openalex.org/W2169491861",
        "https://openalex.org/W2107005506",
        "https://openalex.org/W2970374239",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2540052259",
        "https://openalex.org/W2510185679",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2626376796",
        "https://openalex.org/W2157807817",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2008431074",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2581661662",
        "https://openalex.org/W3096554490",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2974604908",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3086727830",
        "https://openalex.org/W2857028992",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2176516200",
        "https://openalex.org/W2339543475",
        "https://openalex.org/W2395039075",
        "https://openalex.org/W2004763266",
        "https://openalex.org/W169539560",
        "https://openalex.org/W2129999749"
    ],
    "abstract": "Chemical patent documents describe a broad range of applications holding key reaction and compound information, such as chemical structure, reaction formulas, and molecular properties. These informational entities should be first identified in text passages to be utilized in downstream tasks. Text mining provides means to extract relevant information from chemical patents through information extraction techniques. As part of the Information Extraction task of the Cheminformatics Elsevier Melbourne University challenge, in this work we study the effectiveness of contextualized language models to extract reaction information in chemical patents. We assess transformer architectures trained on a generic and specialised corpora to propose a new ensemble model. Our best model, based on a majority ensemble approach, achieves an exact F1-score of 92.30% and a relaxed F1-score of 96.24%. The results show that ensemble of contextualized language models can provide an effective method to extract information from chemical patents.",
    "full_text": "Named entity recognition in chemical patents\nusing ensemble of contextual language models\nJenny Copara1,2,3, Nona Naderi1,2, Julien Knafou1,2,3, Patrick Ruch1,2, and\nDouglas Teodoro1,2\n1 University of Applied Sciences and Arts of Western Switzerland, Geneva,\nSwitzerland\n2 Swiss Institute of Bioinformatics, Geneva, Switzerland\n3 University of Geneva, Geneva, Switzerland\n{firstname.lastname}@hesge.ch\nAbstract. Chemical patent documents describe a broad range of appli-\ncations holding key reaction and compound information, such as chemical\nstructure, reaction formulas, and molecular properties. These informa-\ntional entities should be ﬁrst identiﬁed in text passages to be utilized\nin downstream tasks. Text mining provides means to extract relevant\ninformation from chemical patents through information extraction tech-\nniques. As part of the Information Extraction task of the Cheminformat-\nics Elsevier Melbourne University challenge, in this work we study the\neﬀectiveness of contextualized language models to extract reaction infor-\nmation in chemical patents. We assess transformer architectures trained\non a generic and specialised corpora to propose a new ensemble model.\nOur best model, based on a majority ensemble approach, achieves an\nexact F1-score of 92 .30% and a relaxed F 1-score of 96 .24%. The results\nshow that ensemble of contextualized language models can provide an\neﬀective method to extract information from chemical patents.\nKeywords: Named-entity recognition, chemical patents, contextual lan-\nguage models, patent text mining, information extraction.\n1 Introduction\nChemical patents represent a valuable information resource in downstream inno-\nvation applications, such as drug discovery and novelty checking. However, the\ndiscovery of chemical compounds described in patents is delayed by a few years\n[12]. Among the reasons, it could be considered the complexity of the chemical\npatent information sources [11], the recent increase in the number of chemical\npatents without manual curation, and the particular wording used in the do-\nmain. Narratives in chemical patents contain often concepts expressed in a way\nto protect or hide information, as opposed to scientiﬁc literature, for example,\nCopyright c⃝2020 for this paper by its authors. Use permitted under Creative Com-\nmons License Attribution 4.0 International (CC BY 4.0). CLEF 2020, 22-25 Septem-\nber 2020, Thessaloniki, Greece.\narXiv:2007.12569v2  [cs.CL]  17 Sep 2020\nwhere the text tends to be as clear as possible [34]. In this landscape, informa-\ntion extraction methods, such as Named Entity Recognition (NER), provide a\nsuited solution to identify key information in patents.\nNER aims to identify information of interest and their respective instances\nin a document [8,24]. It has been often addressed as a sequence classiﬁcation\ntask, where a sequence of features, usually tokens, is used to predict the class\nof a text passage. One of the most successful approaches in sequence classiﬁca-\ntion is Conditional Random Fields (CRF) [18,32]. CRF was proposed to solve\nsequence classiﬁcation problems by estimating the conditional probability of a\nlabel sequence given a word sequence, considering a set of observed features in\nthe latter. It was established as the state-of-the-art in diﬀerent NER domains\nfor many years [19,29,20,28,9,11,37]. In the chemical patent domain, CRF was\nexplored by Zhang et al. [39] in the CHEMDNER patent corpus [17]. Using a\nset of hand-crafted and unsupervised features derived from word embeddings\nand Brown clustering, their model achieved 87 .22% of F 1-score. With similar\nF1-score performance, Akhondi et al. [2] explored CRF combined with dictio-\nnaries in the biomedical domain in the tmChem tool [20] in order to select the\nbest vocabulary for the CHEMDNER patent corpus. It has been shown [11] that\nrecognizing chemical entities in the full patent text is a harder task than in titles\nand abstracts, due the peculiarities of the chemical patent text. Evaluation in\nfull patents was performed using BioSemantics patent corpus [1] through neu-\nral approaches based on the Bidirectional Long-Short Term Memory (BiLSTM)\nCRF [10] and the BiLSTM Convolutional Neural Network (CNN) CRF [38] ar-\nchitectures, with performance of 82.01% and 85.68% of F1-score, respectively. It\nis worth noting that for the ﬁrst architecture [10], the authors used word2vec\nembeddings [23] to represent features, while in the latter [38], the authors used\nELMo contextualized embeddings [26].\nOver the years, neural language models have improved their ability to encode\nthe semantics of words using large amounts of unlabeled text for self-supervised\ntraining. They have initially evolved from a straightforward model [3] of one\nhidden layer that predicts the next word in a sequence, aiming to learn the\ndistributed representation of words (i.e., the word embedding vector), to an im-\nproved objective function that allows learning from larger amounts of text [4], us-\ning higher computational resources and with longer training time. These develop-\nments have encouraged the seeking of language models able to bring high-quality\nword embeddings with lower computational cost (i.e., word2vec [23] and Global\nVectors (GloVe) [25]). However, natural language still presented challenges for\nlanguage models, in particular, concerning word contexts and homonyms. More\nrecently, a second type of word embeddings have attracted attention in the lit-\nerature, the so-called contextualized embeddings, such as ELMo, UMLFiT [14],\nGPT-2 [27], and BERT [7]. Particularly, the BERT architecture uses the atten-\ntion mechanism to train deep bidirectional token representations, conditioning\ntokens on their left and right contexts.\nIn this work, we explore contextualized language models to extract infor-\nmation in chemical patents as part of the Named Entity Recognition task of\nthe Information extraction from Chemical Patents (ChEMU) lab [12,13]. Pre-\ntrained contextualized languages models, based on the BERT-based architec-\nture, are used as baseline model and ﬁne-tuned on the examples of the ChEMU\nNER task to classify tokens according to the diﬀerent entities. In the chal-\nlenge, the corpus was annotated with the entities:example label, other compound,\nreaction product, reagent catalyst, solvent, starting material, temperature, time,\nyield other, and yield percent. We investigate the combination of diﬀerent archi-\ntectures to improve NER performance. In the following sections, we describe the\ndesign and results of our experiments.\n2 Methods and data\n2.1 NER model\nTransformers with a token classiﬁcation on top. We assess ﬁve language\nmodels based on the transformers architecture to classify tokens according to\nthe named-entities classes. The ﬁrst four models are variations of the BERT\nmodel in terms of size and tokenization: bert-base-cased, bert-base-uncased, bert-\nlarge-cased, and bert-large-uncased. These models were originally pretrained on\na large corpus of English text extracted from BookCorpus [40] and Wikipedia,\nwith diﬀerent number of attention heads for the base and large types (12 and\n16 respectively). The ﬁfth pretrained language model assessed is ChemBERTa1,\na RoBERTa-based transformer architecture [22], trained on a corpus of 100k\nSimpliﬁed Molecular Input Line Entry System (SMILES) [35] strings from the\nZINC benchmark dataset [15].\nOur models consist of BERT models specialised for NER, with a fully con-\nnected layer on top of the hidden states of each token. They are ﬁne-tuned on\nthe ChEMU Task 1 dataset, using the train and development sets provided. The\nﬁne-tuning is performed with a sequence length of 256 tokens, a warmup pro-\nportion of 0 .1 (percentage of warmup steps with respect to the total amount of\nsteps), and a batch size of 32. The tokenization process is driven by the original\nmodel’s tokenizer, i.e., for the BERT-based models, WordPiece [36] is applied,\nwhile for the RoBERTa-based model, Byte-Pair-Encoding [30] is applied. The\nAdam optimizer is employed to optimize network weights [16]. The ﬁrst four\nlanguage models are ﬁne-tuned for 10 epochs and a learning rate of 3 e − 5. For\nChemBERTa model, we conduct a grid search over the development set and\nfound the best performance around 29 epochs of ﬁne-tuning and a learning rate\nof 4e − 5. The implementations are based on the Huggingface framework. 2\nEnsemble model. Our ensemble method is based on a voting strategy, where\neach model votes with its predictions and a simple majority of votes is necessary\nto assign the predictions [5]. In other words, for a given document, our models\n1 https://github.com/seyonechithrananda/bert-loves-chemistry\n2 https://huggingface.co/transformers/\ninfer their predictions independently for each entity, then, a set of passages that\nreceived at least a vote is taken into consideration for casting votes. This means\nthat, for a given document and a given entity, we end up with multiple passages\nassociated with a number of votes, then, again for a given entity, the ensemble\nmethod will predict as positive all the passages that get the majority of votes.\nNote that each entity is predicted independently and that the voting strategy\ndoes allow the fact that a passage could have been labeled as positive for multiple\nentities at once.\nFinally, in order to decide on the optimal composition of the ensemble model,\nwe used the development set and compute all possible ensemble predictions using\nthe above methodology. As we had 7 models in total, we tried every possible\ncombination from 2 to 7 models. We retained the ensemble composition with\nthe best overall F 1-score and used it for the test set. Originally, the ensemble\nmodel giving the best F1-score was combiningbert-large-uncased, bert-base-cased,\nCRF, bert-base-uncased and the CNN model (5 models). However, due to the\nsize of the test set (approximately 10k patent snippets), we had to discard the\nlarge models of the ensemble strategy due to their much higher algorithmic\ncomplexity and the time constraints. The retained models in the ensemble were\nthen bert-base-cased, bert-base-uncased and the CNN model.\nBaseline. We consider two models for our baseline: CRF and CNN. For the CRF\nmodel, a set of standard features in a window of ±2 tokens are created without\ntaking into account part-of-speech tags, neither gazetteers. The features used\nare token itself, lower-cased word, capitalization pattern, type of token (i.e.,\ndigit, symbol, word), 1-4 character preﬁxes/suﬃxes, digit size (i.e., size 2 or\n4), combination of values (digit with alphanumeric, hyphen, comma, period),\nbinary features for upper/lower-cased letter, alphabet/digit char and symbol.\nPlease refer to [6,9] for further details on the features used. The CRF classiﬁer\nimplementation relies on the CRFSuite.3\nThe CNN model [21] for NER relies on incremental parsing with Bloom\nembeddings, a compression technique for neural network models dealing with\nsparse high-dimensional binary-coded instances [31]. The convolutional layers\nuse residual connections, layer normalization and maxout non-linearity. The in-\nput sequence is embedded in a vector compounded by Bloom embeddings model-\ning the characters, preﬁx, suﬃx and part-of-speech of each word. Convolutional\nﬁlters of 1D are used over the text to predict how the next words are going\nto change. Our implementation relies on the spaCy NER module, 4 using the\npretrained transformer bert-base-uncased for 30 epochs and a batch size of 4.\nDuring the test phase, we ﬁxed the max size of the text to 1.5M due to RAM\nmemory limitations.\n3 http://www.chokkan.org/software/crfsuite/\n4 https://spacy.io\n2.2 Data\nThe data in ChEMU Task 1 (NER) is provided as snippets sampled from 170\nEnglish patents from the European Patent Oﬃce and the United States Patent\nand Trademark Oﬃce [12,13]. Gold annotations were provided for training (900\nsnippets) and development (250 snippets) sets for a total of 20 , 186 entities. The\nannotation was done in the BRAT standoﬀ format. Fig. 1 shows an example of\na snippet with annotations for several entities, including reaction product (two\nannotations), starting material and temperature.\nFig. 1.Data example with annotations for the ChEMU NER task.\nDuring the development phase, we used the oﬃcial development set as our\ntest set. The oﬃcial training set was split into train and development sets in\norder to train the weights and tune hyper parameters of our models, respec-\ntively. As a result of this new setting, 800 snippets were available in train set,\n100 in the development set and 225 in test set. Table 1 shows the entity distribu-\ntion during the development phase. The majority of the annotations come from\nother compound, reaction product and starting material, covering the 52% of en-\ntities in the development phase. In contrast,example label, time and yield percent\nentities represent 17% of entities in the development phase.\n2.3 Evaluation metrics\nThe metrics used to evaluate the models are precision, recall, and F 1-score. As\nit can be seen in the example of Fig. 1, each entity has a span that is expected\nto be identiﬁed by the NER models as well as the correct entity type. The\nevaluation for the challenge is established under strict and relaxed span matching\nconditions [12,13]. The exact matching condition takes into account the correct\nidentiﬁcation of both, span and entity type. On the other hand, the relaxed\nmatching condition evaluates how accurate is the predicted span concerning the\nreal. Our models are evaluated with the ChEMU web page system for the oﬃcial\nresults 5 and with the BRAT Eval tool for the oﬄine analyses 6.\n5 http://chemu.eng.unimelb.edu.au/\n6 https://bitbucket.org/nicta_biomed/brateval/src/master/\nTable 1.Entity distribution in the development phase based on the oﬃcial training\nand development sets. Test set is the oﬃcial development set. Dev set is random set\nextracted from the oﬃcial training set.\nEntity Train\n(count/%)\nDev\n(count/%)\nTest\n(count/%)\nAll\n(count/%)\nexample label 784/5 102/5 218/6 1104/5\nother compound 4095/28 545/29 1080/28 5720/28\nreaction product 1816/13 236/12 506/13 2558/13\nreagent catalyst 1135/8 146/8 289/8 1570/8\nsolvent 1001/7 139/7 250/7 1390/7\nstarting material 1543/11 211/11 413/11 2167/11\ntemperature 1345/9 170/9 346/9 1861/9\ntime 928/6 131/7 252/7 1311/6\nyield other 940/7 121/6 261/7 1322/7\nyield percent 848/6 107/6 228/6 1183/6\nAll 14435/100 1908/100 3843/100 20186/100\n3 Results and discussion\nIn this section, we present the results of our models in the development and\noﬃcial test phases. Additionally, we perform error analyses on the results of the\ntest set used in the development phase for some relevant models.\n3.1 Model’s performance in the development phase\nTable 2 shows the exact and relaxed overall F1-scores for all the models explored\nby our team in the development phase of the ChEMU NER task. As we can see,\nthe ensemble model outperforms all the individual models for both exact and\nrelaxed metrics. On the other hand, despite being trained on a specialised corpus,\nChemBERTa achieves the lowest performance. The reported results come from\nthe ChEMU oﬃcial evaluation web page except for the CNN, bert-large-uncased,\nand the ensemble models, which are provided by the BRAT Eval tool.\nTable 2.Performance of the diﬀerent models in the development phase in terms of\nF1-score. *models evaluated using the BRAT Eval tool.\nMetric CRF CNN* bert-base bert-large Chem Ensemble*\ncased uncased cased uncased* BERTa\nexact 0.8722 0.8182 0.9140 0.9113 0.9079 0.9052 0.6810 0.9285\nrelaxed 0.9450 0.8820 0.9732 0.9719 0.9706 0.9910 0.8500 0.9876\nThe results of all models with respect to the individual entities are presented\nin Table 3. As for the overall results, the ensemble model outperforms the in-\ndividual models for all entities apart from time, for which the bert-base-cased\npresents the best performance. The highest improvement for the ensemble model\nis seen for the reaction product and starting material entities with over 12-point\nincrease in F1-score. Considering only the individual models, the bert-base mod-\nels outperform the other individual models, including the bert-large models, for\nall the entities, apart from starting material, for which the CNN model has the\nbest performance.\nTable 3.Evaluation results on the development set for the exact F 1-score metric.\nEntity CRF CNN bert-base bert-large Chem Ensemble\ncased uncased cased uncased BERTa\nexample label 0.9630 0.9526 0.9862 0.9817 0.9793 0.9769 0.9631 0.9885\nother compound 0.8762 0.7409 0.8953 0.8938 0.8947 0.8925 0.7850 0.9052\nreaction product 0.7535 0.8425 0.8586 0.8515 0.8410 0.8427 0.5957 0.8807\nreagent catalyst 0.8330 0.8557 0.8595 0.8355 0.8498 0.8468 0.4673 0.8946\nsolvent 0.8949 0.7517 0.9447 0.9451 0.9407 0.9426 0.5945 0.9545\nstarting material 0.7253 0.8229 0.8072 0.8153 0.7995 0.7813 0.4405 0.8470\ntemperature 0.9796 0.6397 0.9842 0.9842 0.9827 0.9841 0.8105 0.9855\ntime 0.9900 0.8533 1.0000 0.9941 0.9941 0.9941 0.8141 0.9980\nyield other 0.9046 0.9448 0.9905 0.9924 0.9811 0.9848 0.7135 0.9943\nyield percent 0.9913 0.9693 0.9978 0.9978 0.9913 0.9892 0.7131 0.9978\nThe ensemble model achieves the best performance for the time, yield other\nand yield percent entities. We believe this is due to the patterns observed for\nthem in the training and test data. For example, for the yield percent entity, the\npattern is mostly a number followed by the percentage symbol (‘%’). Similarly,\nfor the time entity, the instances usually appear as a number followed for a\ntime-indicator word. On the other hand, the reaction product, reagent catalyst\nand starting material entities show the lowest performance, with 88.07%, 89.46%\nand 84.70% of F1-score, respectively. These entities are of chemical types, often\nmolecule strings (e.g., 4-(6-Bromo-3-methoxypyridin-2-yl)-6-chloropyrimidin-2-\namine) [12,13]. As our models did not include a post-processing step, as proposed\nin [33], these entities were sometimes recognized partially as a result of the\nlanguage model sub-word tokenization process.\nDuring the development phase, we also investigate the performance of Chem-\nBERTa. As ChemBERTa is a language model trained on the chemical domain,\nit is expected to achieve competitive results. However, for the NER downstream\ntask in chemical patents, the results go in a diﬀerent direction. As shown in\nTable 3, ChemBERTa obtains the lowest results among all the explored models\nfor both exact and relaxed metrics. We believe that the size of the corpus used\nto train the other explored language models has led to better chemical entity\nrepresentations. Additionally, as the task aims to identify other entities than\nmolecules, the ChemBERTa model naturally fails as its train set is only based\non SMILES strings.\n3.2 Model’s performance in the test phase\nIn the oﬃcial test phase, 9 , 999 ﬁles containing snippets from chemical patents\nwere available for evaluating the models. We submitted 3 oﬃcial runs: run 1,\nbased on the baseline CRF model; run 2, based on the bert-base-cased model;\nand run 3, based on the ensemble model. Table 4 shows the oﬃcial performance of\nour models for the exact and relaxed span matching metrics in terms of F1-score.\nThe ensemble model achieves 92 .30% of exact F 1-score, yielding more than 11-\npoint improvement over our baseline and at least 1-point improvement over the\nbest individual contextualized language model (bert-base-cased). It outperforms\nrun 1 and run 2 for all the entities in both exact and relaxed metrics. We believe\nthat the performance diﬀerence between the CRF model and the ensemble model\nis due mostly to the fact that language models based on attention mechanisms\nare able to provide better contextual feature representations without the speciﬁc\ndesign of hand-crafted features as in the case of CRF.\nTable 4. Oﬃcial performance of our models in terms of F 1-score for the exact and\nrelaxed metrics.\nEntity CRF bert-base-cased Ensemble\nexact relaxed exact relaxed exact relaxed\nexample label 0.9190 0.9367 0.9617 0.9730 0.9669 0.9784\nother compound 0.8310 0.9029 0.8780 0.9608 0.8920 0.9653\nreaction product 0.6462 0.7689 0.8593 0.9378 0.8766 0.9322\nreagent catalyst 0.7598 0.8035 0.8791 0.9082 0.9022 0.9176\nsolvent 0.8299 0.8323 0.9444 0.9491 0.9541 0.9541\nstarting material 0.4957 0.6752 0.8413 0.9343 0.8701 0.9394\ntemperature 0.9499 0.9688 0.9692 0.9902 0.9729 0.9877\ntime 0.9698 0.9843 0.9868 0.9967 0.9879 0.9978\nyield other 0.8984 0.8984 0.9799 0.9821 0.9842 0.9865\nyield percent 0.9705 0.9807 0.9936 0.9962 0.9974 0.9974\nALL 0.8056 0.8683 0.9098 0.9596 0.9230 0.9624\nThe 5-top best performing entities identiﬁed by our models areexample label,\ntemperature, time, yield other, yield percent, which is similar to the results found\nin the development phase. For all of our submissions, the entity with lowest per-\nformance in the oﬃcial test phase is starting material, achieving 49.57%, 84.13%\nand 87.01% of exact F1-score in the CRF, bert-base-cased and ensemble models,\nrespectively. As we will see further in the error analyses section, this entity is\noften confused with the reagent catalyst entity in the development phase. From\nthe chemistry point of view, both starting material (reactants) and catalysts\n(reagents) entities are present at the start of the reaction, with the diﬀerence\nthat the latter is not consumed by the reaction. These terms are often used in-\nterchangeably though, which could be the reason for the confusion. Despite the\nmuch larger size of the test set (approximately 10 times the size of the training\nset), these results suggest that the test set has a similar entity distribution of\nthe dataset provided in the development phase.\nIn Table 5 is shown a summary of the top ten oﬃcial results, including our\nruns 2 and 3 (BiTeM team, ranked 6 and 7), the best model and the challenge\nbaseline. If we consider the exact F 1-score metric, our ensemble model shows\nat least 3-point improvement from the ChEMU Task 1 NER baseline and more\nthan 3-point behind the top 1. For the relaxed metric, our best model performs\nslightly better, showing more than 5-point improvement from the baseline and\nless than 1-point below the top system.\nTable 5.Oﬃcial BiTeM results compared to the best model and the BANNER base-\nline.\nRank Team Precision Recall F1-score\nexact relaxed exact relaxed exact relaxed\n1 Melaxtech 0.9571 0.9690 0.9570 0.9687 0.9570 0.9688\n6 BiTeM (run 3) 0.9378 0.9692 0.9087 0.9558 0.9230 0.9624\n7 BiTeM (run 2) 0.9083 0.9510 0.9114 0.9684 0.9098 0.9596\n10 Baseline (BANNER) 0.9071 0.9219 0.8723 0.8893 0.8893 0.9053\nThe performance of the ensemble model for all entities on test set in terms\nof precision, recall and F 1-score for both exact match and relax is presented in\nTable 6. The best precision and recall for the exact match metric are achieved\nfor the yield percent entity, reaching 99.74% and 99.74%, respectively. Overall,\nprecision is always above 93% for the relaxed metric and at least 88% for the\nexact metric.\nTable 6.Performance of the ensemble model for all entities on the test set in terms of\nprecision, recall and F 1-score\nEntity Precision Recall F1-score\nexact relaxed exact relaxed exact relaxed\nexample label 0.9711 0.9827 0.9628 0.9742 0.9669 0.9784\nother compound 0.9197 0.9730 0.8659 0.9578 0.8920 0.9653\nreaction product 0.8942 0.9367 0.8596 0.9277 0.8766 0.9322\nreagent catalyst 0.9268 0.9435 0.8790 0.8931 0.9023 0.9176\nsolvent 0.9620 0.9620 0.9463 0.9463 0.9541 0.9541\nstarting material 0.8886 0.9545 0.8523 0.9247 0.8701 0.9394\ntemperature 0.9769 0.9901 0.9690 0.9852 0.9729 0.9876\ntime 0.9846 0.9956 0.9912 1.0000 0.9879 0.9978\nyield other 0.9776 0.9798 0.9909 0.9932 0.9842 0.9865\nyield percent 0.9974 0.9974 0.9974 0.9974 0.9974 0.9974\nLastly, our CRF baseline achieves 80.56% of exact F1-score, while the compe-\ntition baseline, which is based also on CRF, but customized for biomedical NER,\ntaking into account features, such as part-of-speech, lemma, Roman numerals,\nnames of the Greek letters, achieves 88.93% [19]. Indeed, we believe those features\ngive the advantage to the competition baseline as they could better characterize\nchemical entities.\n3.3 Error analysis\nAs the gold annotations for the test set are not available, we perform the error\nanalysis on the oﬃcial development set (used as our test set in the develop-\nment phase, see Table 1). Fig. 2 shows the confusion matrix for the ensemble\npredictions for the exact metric. As we can see, most confusion occurred for\nthe starting material entity, which is mostly confused with reagent catalyst, and\nfor the reaction product entity, which is mistaken for other compound. As men-\ntioned previously, these entities - material/reactant and catalyst/reagent, and\nproduct/compound - are often used interchangeably in chemistry passages, which\nis likely the reason for the model’s confusion.\nFig. 2.Normalized confusion matrix for the ensemble model predictions on the oﬃcial\ndevelopment set. Only exact matches are considered.\nThe error analysis of the incorrectly identiﬁed spans by the ensemble model\nshows that in almost 78.8% of the cases, the predicted entity was longer in length,\nfor example, sodium thiosulfate aqueous instead of aqueous and concentrated\nhydrochloric acid instead of hydrochloric acid. The entities that are partially\ndetected are mainly starting material, which is inconsistently annotated in some\ncases, as Intermediate 13/6/21 (predicted as 13/6/21 by the ensemble model),\nand in some cases as only the number, such as 3 (predicted as Intermediate 3\nby the ensemble model). 42.3% of the span errors were multi-word entities.\nFig. 3 shows how diﬀerent models detected a reagent catalyst entity de-\nscribed by a long text span. It seems that entities with longer text span, such\nas reagent catalyst, other compound, reaction product, and starting material, are\nless likely to be correctly detected by the contextualized language models. The\nbert-large-uncased and ChemBERTa models did not detect any token of the en-\ntity while both bert-large-cased and bert-base-cased models were able to only\npartially detect the entity. Particularly, the larger nature of the BERT large\nmodels was not translated into more eﬀective representations for these entities.\nFig. 3.An example of predictions by diﬀerent models for (reagentcatalyst) annotation.\nThe span detected by each model is color-coded.\nFigure 4 shows the comparison of the span errors of the ensemble and BERT-\nbase-cased models based on the length of entities (in character). While most er-\nrors of both models are focused on smaller entities, the BERT-base-cased model\nmakes more mistakes than the ensemble model in detecting the spans of the\nlonger entities. We believe this eﬀect could be also related to the sub-word tok-\nenization process of transformers. The combination of models smooths the eﬀect\nin the ensemble model.\nFig. 4.Number of span errors by the ensemble and BERT-base-cased models based on\nthe length of the entities (in character).\n4 Conclusions\nIn this task, we explored the use of contextualized language models based on the\ntransformer architecture to extract information from chemical patents. The com-\nbination of language models resulted in an eﬀective approach, outperforming the\nbaseline CRF model but also individual transformer models. Our experiments\nshow that without extensive pre-training in the patent chemical domain, the ma-\njority vote approach is able to leverage distinctive features present in the English\nlanguage, achieving 92.30% of exact F1-score in the ChEMU NER task. It seems\nthat the transformer models are able to take advantage of natural language con-\ntexts in order to capture the most relevant features without supervision in the\nchemical domain. Our next step will be to investigate pre-trained models on\nlarge chemical patent corpora to further improve the NER performance.\nReferences\n1. Akhondi, S.A., Klenner, A.G., Tyrchan, C., Manchala, A.K., Boppana, K., Lowe,\nD., Zimmermann, M., Jagarlapudi, S.A.R.P., Sayle, R., Kors, J.A., Muresan, S.:\nAnnotated chemical patent corpus: A gold standard for text mining. PLoS ONE\n9(9), e107477 (Sep 2014)\n2. Akhondi, S.A., Pons, E., Afzal, Z., van Haagen, H., Becker, B.F., Hettne, K.M., van\nMulligen, E.M., Kors, J.A.: Chemical entity recognition in patents by combining\ndictionary-based and statistical approaches. Database 2016 (2016)\n3. Bengio, Y., Ducharme, R., Vincent, P., Janvin, C.: A neural probabilistic language\nmodel. Journal of machine learning research 3(null), 1137–1155 (Mar 2003)\n4. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., Kuksa, P.:\nNatural language processing (almost) from scratch. Journal of machine learning\nresearch 12, 2493–2537 (Nov 2011)\n5. Copara, J., Knafou, J., Naderi, N., Moro, C., Ruch, P., Teodoro, D.: Contextualized\nFrench Language Models for Biomedical Named Entity Recognition. In: Cardon,\nR., Grabar, N., Grouin, C., Hamon, T. (eds.) 6e conf´ erence conjointe Journ´ ees\nd’´Etudes sur la Parole (JEP, 33e ´ edition), Traitement Automatique des Langues\nNaturelles (TALN, 27e ´ edition), Rencontre des ´Etudiants Chercheurs en Infor-\nmatique pour le Traitement Automatique des Langues (R ´ECITAL, 22e ´ edition).\nAtelier D´Eﬁ Fouille de Textes. pp. 36–48. ATALA, Nancy, France (2020)\n6. Copara, J., Ochoa Luna, J.E., Thorne, C., Glavaˇ s, G.: Spanish NER with word rep-\nresentations and conditional Random Fields. In: Proceedings of the Sixth Named\nEntity Workshop. pp. 34–40. Association for Computational Linguistics, Berlin,\nGermany (Aug 2016)\n7. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\npp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota\n(Jun 2019)\n8. Grishman, R.: Twenty-ﬁve years of information extraction. Natural Language En-\ngineering 25(06), 677–692 (Sep 2019)\n9. Guo, J., Che, W., Wang, H., Liu, T.: Revisiting embedding features for simple\nsemi-supervised learning. In: Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). pp. 110–120. Association for\nComputational Linguistics, Doha, Qatar (Oct 2014)\n10. Habibi, M., Weber, L., Neves, M., Wiegandt, D.L., Leser, U.: Deep learning with\nword embeddings improves biomedical named entity recognition. Bioinformatics\n33(14), i37–i48 (Jul 2017)\n11. Habibi, M., Wiegandt, D.L., Schmedding, F., Leser, U.: Recognizing chemicals in\npatents: A comparative analysis. Journal of Cheminformatics 8(1) (Oct 2016)\n12. He, J., Nguyen, D.Q., Akhondi, S.A., Druckenbrodt, C., Thorne, C., Hoessel, R.,\nAfzal, Z., Zhai, Z., Fang, B., Yoshikawa, H., Albahem, A., Cavedon, L., Cohn, T.,\nBaldwin, T., Verspoor, K.: Overview of chemu 2020: Named entity recognition and\nevent extraction of chemical reactions from patents. In: Arampatzis, A., Kanoulas,\nE., Tsikrika, T., Vrochidis, S., Joho, H., Lioma, C., Eickhoﬀ, C., N´ ev´ eol, A., Cap-\npellato, L., Ferro, N. (eds.) Experimental IR Meets Multilinguality, Multimodality,\nand Interaction. Proceedings of the Eleventh International Conference of the CLEF\nAssociation (CLEF 2020), vol. 12260. Lecture Notes in Computer Science (2020)\n13. He, J., Nguyen, D.Q., Akhondi, S.A., Druckenbrodt, C., Thorne, C., Hoessel, R.,\nAfzal, Z., Zhai, Z., Fang, B., Yoshikawa, H., Albahem, A., Wang, J., Ren, Y., Zhang,\nZ., Zhang, Y., Hoang Dao, M., Ruas, P., Lamurias, A., M. Couto, F., Copara, J.,\nNaderi, N., Knafou, J., Ruch, P., Teodoro, D., Lowe, D., Mayﬁeld, J., K¨ oksal, A.,\nD¨ onmez, H.,¨Ozkırımlı, E., ¨Ozg¨ ur, A., Mahendran, D., Gurdin, G., Lewinski, N.,\nTang, C., T.McInnes, Bridget C.S., M., RK Rao., P., Lalitha Devi, S., Cavedon, L.,\nCohn, T., Baldwin, T., Verspoor, K.: An extended overview of the clef 2020 chemu\nlab: Information extraction of chemical reactions from patents. In: Proceedings\nof the Eleventh International Conference of the CLEF Association (CLEF 2020)\n(2020)\n14. Howard, J., Ruder, S.: Universal language model ﬁne-tuning for text classiﬁcation.\nIn: Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). pp. 328–339. Association for Computational\nLinguistics, Melbourne, Australia (Jul 2018)\n15. Irwin, J.J., Shoichet, B.K.: Zinc – a free database of commercially available com-\npounds for virtual screening. Journal of Chemical Information and Modeling45(1),\n177–182 (2005), pMID: 15667143\n16. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,\nY., LeCun, Y. (eds.) 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings\n(2015)\n17. Krallinger, M., Rabal, O., Lourenco, A., Perez, M., P´ erez-Rodr´ ıguez, G., Vazquez,\nM., Leitner, F., Oyarzabal, J., Valencia, A.: Overview of the CHEMDNER patents\ntask. Proceedings of the Fifth BioCreative Challenge Evaluation Workshop pp.\n63–75 (01 2015)\n18. Laﬀerty, J.D., McCallum, A., Pereira, F.C.N.: Conditional Random Fields: Prob-\nabilistic models for segmenting and labeling sequence data. In: Proceedings of the\nEighteenth International Conference on Machine Learning. p. 282–289. ICML ’01,\nMorgan Kaufmann Publishers Inc., San Francisco, CA, USA (2001)\n19. Leaman, R., Gonzalez, G.: Banner: An executable survey of advances in biomedical\nnamed entity recognition. In: Altman, R.B., Dunker, A.K., Hunter, L., Murray,\nT., Klein, T.E. (eds.) Paciﬁc Symposium on Biocomputing. pp. 652–663. World\nScientiﬁc (2008)\n20. Leaman, R., Wei, C.H., Lu, Z.: tmChem: a high performance approach for chemical\nnamed entity recognition and normalization. Journal of Cheminformatics 7(S1)\n(Jan 2015)\n21. Lecun, Y.: Generalization and network design strategies. Technical Report CRG-\nTR-89-4, University of Toronto (June 1989)\n22. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized BERT pretraining\napproach. CoRR abs/1907.11692 (2019)\n23. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed representa-\ntions of words and phrases and their compositionality. In: Proceedings of the 26th\nInternational Conference on Neural Information Processing Systems - Volume 2.\np. 3111–3119. NIPS’13, Curran Associates Inc., Red Hook, NY, USA (2013)\n24. Okurowski, M.E.: Information extraction overview. In: TIPSTER TEXT PRO-\nGRAM: PHASE I: Proceedings of a Workshop held at Fredricksburg, Virginia,\nSeptember 19-23, 1993. pp. 117–121. Association for Computational Linguistics,\nFredericksburg, Virginia, USA (Sep 1993)\n25. Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word represen-\ntation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP). Association for Computational Linguistics (2014)\n26. Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettle-\nmoyer, L.: Deep contextualized word representations. In: Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers). Association\nfor Computational Linguistics (2018)\n27. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners. OpenAI Blog 1(8), 9 (2019)\n28. Ratinov, L., Roth, D.: Design challenges and misconceptions in named entity recog-\nnition. In: Proceedings of the Thirteenth Conference on Computational Natural\nLanguage Learning (CoNLL-2009). pp. 147–155. Association for Computational\nLinguistics, Boulder, Colorado (Jun 2009)\n29. Rockt¨ aschel, T., Weidlich, M., Leser, U.: ChemSpot: a hybrid system for chemical\nnamed entity recognition. Bioinformatics 28(12), 1633–1640 (Apr 2012)\n30. Sennrich, R., Haddow, B., Birch, A.: Neural machine translation of rare words with\nsubword units. In: Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). pp. 1715–1725. Association\nfor Computational Linguistics, Berlin, Germany (Aug 2016)\n31. Serr` a, J., Karatzoglou, A.: Getting Deep Recommenders Fit: Bloom Embeddings\nfor Sparse Binary Input/Output Networks. In: Proceedings of the Eleventh ACM\nConference on Recommender Systems. p. 279–287. RecSys ’17, Association for\nComputing Machinery, New York, NY, USA (2017)\n32. Sutton, C.: An introduction to Conditional Random Fields. Foundations and\nTrendsR⃝in Machine Learning 4(4), 267–373 (2012)\n33. Teodoro, D., Gobeill, J., Pasche, E., Ruch, P., Vishnyakova, D., Lovis, C.: Auto-\nmatic ipc encoding and novelty tracking for eﬀective patent mining. In: The 8th\nNTCIR Workshop Meeting on Evaluation of Information Access Technologies: In-\nformation Retrieval, Question Answering, and Cross-Lingual Information Access\n(2010)\n34. Valentinuzzi, M.E.: Patents and scientiﬁc papers: Quite diﬀerent concepts: The\nreward is found in giving, not in keeping [retrospectroscope]. IEEE Pulse 8(1),\n49–53 (2017)\n35. Weininger, D.: Smiles, a chemical language and information system. 1. introduc-\ntion to methodology and encoding rules. Journal of Chemical Information and\nComputer Sciences 28(1), 31–36 (Feb 1988)\n36. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun,\nM., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X.,\n Lukasz Kaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian,\nG., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,\nCorrado, G., Hughes, M., Dean, J.: Google’s neural machine translation system:\nBridging the gap between human and machine translation. arXiv (2016)\n37. Yadav, V., Bethard, S.: A survey on recent advances in named entity recognition\nfrom deep learning models. In: Proceedings of the 27th International Conference\non Computational Linguistics. pp. 2145–2158. Association for Computational Lin-\nguistics, Santa Fe, New Mexico, USA (Aug 2018)\n38. Zhai, Z., Nguyen, D.Q., Akhondi, S., Thorne, C., Druckenbrodt, C., Cohn, T., Gre-\ngory, M., Verspoor, K.: Improving chemical named entity recognition in patents\nwith contextualized word embeddings. In: Proceedings of the 18th BioNLP Work-\nshop and Shared Task. pp. 328–338. Association for Computational Linguistics,\nFlorence, Italy (Aug 2019)\n39. Zhang, Y., Xu, J., Chen, H., Wang, J., Wu, Y., Prakasam, M., Xu, H.: Chemical\nnamed entity recognition in patents by domain knowledge and unsupervised feature\nlearning. Database 2016 (2016)\n40. Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., Fidler,\nS.: Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In: Proceedings of the IEEE International Conference\non Computer Vision (ICCV). p. 19–27. IEEE Computer Society, USA (2015)"
}