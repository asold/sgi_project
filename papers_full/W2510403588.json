{
  "title": "Compressing Neural Language Models by Sparse Word Representations",
  "url": "https://openalex.org/W2510403588",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2154694014",
      "name": "Yun-Chuan Chen",
      "affiliations": [
        "Peking University",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2163770800",
      "name": "Lili Mou",
      "affiliations": [
        "Peking University",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2098995436",
      "name": "Yan Xu",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A1993360758",
      "name": "Ge Li",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A2110400062",
      "name": "Zhi Jin",
      "affiliations": [
        "Institute of Software",
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952822287",
    "https://openalex.org/W1841959837",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2140610559",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W179875071",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2295800168",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W1965154800",
    "https://openalex.org/W1512874001",
    "https://openalex.org/W591148856",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2384495648",
    "https://openalex.org/W2340329281",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2113606819",
    "https://openalex.org/W182831726",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W1996901117",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2085400714",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2250189634",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2950797609"
  ],
  "abstract": "Neural networks are among the state-ofthe-art techniques for language modeling.Existing neural language models typically map discrete words to distributed, dense vector representations.After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word.Such approaches are time-and memory-intensive because of the large numbers of parameters for word embeddings and the output layer.In this paper, we propose to compress neural language models by sparse word representations.In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible.Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure. 1",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 226–235,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nCompressing Neural Language Models by Sparse Word Representations\nYunchuan Chen,1,2 Lili Mou,1,3 Yan Xu,1,3 Ge Li,1,3 Zhi Jin1,3,∗\n1Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE, China\n2University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn\n3Institute of Software, Peking University, doublepower.mou@gmail.com,\n{xuyan14,lige,zhijin}@pku.edu.cn ∗Corresponding author\nAbstract\nNeural networks are among the state-of-\nthe-art techniques for language modeling.\nExisting neural language models typically\nmap discrete words to distributed, dense\nvector representations. After information\nprocessing of the preceding context words\nby hidden layers, an output layer estimates\nthe probability of the next word. Such ap-\nproaches are time- and memory-intensive\nbecause of the large numbers of parame-\nters for word embeddings and the output\nlayer. In this paper, we propose to com-\npress neural language models by sparse\nword representations. In the experiments,\nthe number of parameters in our model in-\ncreases very slowly with the growth of the\nvocabulary size, which is almost imper-\nceptible. Moreover, our approach not only\nreduces the parameter space to a large ex-\ntent, but also improves the performance in\nterms of the perplexity measure.1\n1 Introduction\nLanguage models (LMs) play an important role\nin a variety of applications in natural language\nprocessing (NLP), including speech recognition\nand document recognition. In recent years, neu-\nral network-based LMs have achieved signiﬁ-\ncant breakthroughs: they can model language\nmore precisely than traditional n-gram statistics\n(Mikolov et al., 2011); it is even possible to gen-\nerate new sentences from a neural LM, beneﬁt-\ning various downstream tasks like machine trans-\nlation, summarization, and dialogue systems (De-\nvlin et al., 2014; Rush et al., 2015; Sordoni et al.,\n2015; Mou et al., 2015b).\n1Code released on https://github.com/chenych11/lm\nExisting neural LMs typically map a discrete\nword to a distributed, real-valued vector repre-\nsentation (called embedding) and use a neural\nmodel to predict the probability of each word\nin a sentence. Such approaches necessitate a\nlarge number of parameters to represent the em-\nbeddings and the output layer’s weights, which\nis unfavorable in many scenarios. First, with a\nwider application of neural networks in resource-\nrestricted systems (Hinton et al., 2015), such ap-\nproach is too memory-consuming and may fail to\nbe deployed in mobile phones or embedded sys-\ntems. Second, as each word is assigned with a\ndense vector—which is tuned by gradient-based\nmethods—neural LMs are unlikely to learn mean-\ningful representations for infrequent words. The\nreason is that infrequent words’ gradient is only\noccasionally computed during training; thus their\nvector representations can hardly been tuned ade-\nquately.\nIn this paper, we propose a compressed neural\nlanguage model where we can reduce the number\nof parameters to a large extent. To accomplish this,\nwe ﬁrst represent infrequent words’ embeddings\nwith frequent words’ by sparse linear combina-\ntions. This is inspired by the observation that, in a\ndictionary, an unfamiliar word is typically deﬁned\nby common words. We therefore propose an op-\ntimization objective to compute the sparse codes\nof infrequent words. The property of sparseness\n(only 4–8 values for each word) ensures the efﬁ-\nciency of our model.\nBased on the pre-computed sparse codes, we\ndesign our compressed language model as follows.\nA dense embedding is assigned to each common\nword; an infrequent word, on the other hand, com-\nputes its vector representation by a sparse combi-\nnation of common words’ embeddings. We use\nthe long short term memory (LSTM)-based recur-\nrent neural network (RNN) as the hidden layer of\n226\nour model. The weights of the output layer are\nalso compressed in a same way as embeddings.\nConsequently, the number of trainable neural pa-\nrameters is a constant regardless of the vocabulary\nsize if we ignore the biases of words. Even con-\nsidering sparse codes (which are very small), we\nﬁnd the memory consumption grows impercepti-\nbly with respect to the vocabulary.\nWe evaluate our LM on the Wikipedia corpus\ncontaining up to 1.6 billion words. During train-\ning, we adopt noise-contrastive estimation (NCE)\n(Gutmann and Hyv ¨arinen, 2012) to estimate the\nparameters of our neural LMs. However, dif-\nferent from Mnih and Teh (2012), we tailor the\nNCE method by adding a regression layer (called\nZRegressoion) to predict the normalization\nfactor, which stabilizes the training process. Ex-\nperimental results show that, our compressed LM\nnot only reduces the memory consumption, but\nalso improves the performance in terms of the per-\nplexity measure.\nTo sum up, the main contributions of this paper\nare three-fold. (1) We propose an approach to rep-\nresent uncommon words’ embeddings by a sparse\nlinear combination of common ones’. (2) We pro-\npose a compressed neural language model based\non the pre-computed sparse codes. The memory\nincreases very slowly with the vocabulary size (4–\n8 values for each word). (3) We further introduce a\nZRegression mechanism to stabilize the NCE\nalgorithm, which is potentially applicable to other\nLMs in general.\n2 Background\n2.1 Standard Neural LMs\nLanguage modeling aims to minimize the joint\nprobability of a corpus (Jurafsky and Martin,\n2014). Traditional n-gram models impose a\nMarkov assumption that a word is only depen-\ndent on previous n−1 words and independent of\nits position. When estimating the parameters, re-\nsearchers have proposed various smoothing tech-\nniques including back-off models to alleviate the\nproblem of data sparsity.\nBengio et al. (2003) propose to use a feed-\nforward neural network (FFNN) to replace the\nmultinomial parameter estimation inn-gram mod-\nels. Recurrent neural networks (RNNs) can also be\nused for language modeling; they are especially\ncapable of capturing long range dependencies in\nsentences (Mikolov et al., 2010; Sundermeyer et\nFigure 1: The architecture of a neural network-\nbased language model.\nal., 2015).\nIn the above models, we can view that a neural\nLM is composed of three main parts, namely the\nEmbedding, Encoding, and Prediction\nsubnets, as shown in Figure 1.\nThe Embedding subnet maps a word to a\ndense vector, representing some abstract features\nof the word (Mikolov et al., 2013). Note that this\nsubnet usually accepts a list of words (known as\nhistory or context words) and outputs a sequence\nof word embeddings.\nThe Encoding subnet encodes the history of a\ntarget word into a dense vector (known as context\nor history representation). We may either leverage\nFFNNs (Bengio et al., 2003) or RNNs (Mikolov\net al., 2010) as the Encoding subnet, but RNNs\ntypically yield a better performance (Sundermeyer\net al., 2015).\nThe Prediction subnet outputs a distribu-\ntion of target words as\np(w= wi|h) = exp(s(h,wi))∑\nj exp(s(h,wj)), (1)\ns(h,wi) =W⊤\ni h+ bi, (2)\nwhere h is the vector representation of con-\ntext/history h, obtained by the Encoding subnet.\nW = (W1,W2,..., WV) ∈RC×V is the output\nweights of Prediction; b= (b1,b2,...,b V) ∈\nRC is the bias (the prior). s(h,wi) is a scoring\nfunction indicating the degree to which the context\nhmatches a target word wi. ( V is the size of vo-\ncabulary V; Cis the dimension of context/history,\ngiven by the Encoding subnet.)\n2.2 Complexity Concerns of Neural LMs\nNeural network-based LMs can capture more pre-\ncise semantics of natural language than n-gram\nmodels because the regularity of the Embedding\nsubnet extracts meaningful semantics of a word\n227\nand the high capacity of Encoding subnet en-\nables complicated information processing.\nDespite these, neural LMs also suffer from sev-\neral disadvantages mainly out of complexity con-\ncerns.\nTime complexity. Training neural LMs is typi-\ncally time-consuming especially when the vocab-\nulary size is large. The normalization factor in\nEquation (1) contributes most to time complex-\nity. Morin and Bengio (2005) propose hierar-\nchical softmax by using a Bayesian network so\nthat the probability is self-normalized. Sampling\ntechniques—for example, importance sampling\n(Bengio and Sen´ecal, 2003), noise-contrastive es-\ntimation (Gutmann and Hyv¨arinen, 2012), and tar-\nget sampling (Jean et al., 2014)—are applied to\navoid computation over the entire vocabulary. In-\nfrequent normalization maximizes the unnormal-\nized likelihood with a penalty term that favors nor-\nmalized predictions (Andreas and Klein, 2014).\nMemory complexity and model complexity. The\nnumber of parameters in the Embedding and\nPrediction subnets in neural LMs increases\nlinearly with respect to the vocabulary size, which\nis large (Table 1). As said in Section 1, this is\nsometimes unfavorable in memory-restricted sys-\ntems. Even with sufﬁcient hardware resources, it\nis problematic because we are unlikely to fully\ntune these parameters. Chen et al. (2015) pro-\npose the differentiated softmax model by assign-\ning fewer parameters to rare words than to fre-\nquent words. However, their approach only han-\ndles the output weights, i.e., W in Equation (2);\nthe input embeddings remain uncompressed in\ntheir approach.\nIn this work, we mainly focus on memory and\nmodel complexity, i.e., we propose a novel method\nto compress the Embedding and Prediction\nsubnets in neural language models.\n2.3 Related Work\nExisting work on model compression for neural\nnetworks. Buciluˇa et al. (2006) and Hinton et al.\n(2015) use a well-trained large network to guide\nthe training of a small network for model compres-\nsion. Jaderberg et al. (2014) compress neural mod-\nels by matrix factorization, Gong et al. (2014) by\nquantization. In NLP, Mou et al. (2015a) learn an\nembedding subspace by supervised training. Our\nwork resembles little, if any, to the above methods\nas we compress embeddings and output weights\nusing sparse word representations. Existing model\nSub-nets RNN-LSTM FFNN\nEmbedding V E V E\nEncoding 4(CE + C2 + C) nCE + C\nPrediction V (C + 1) V (C + 1)\nTOTAL † O((C + E)V ) O((E + C)V )\nTable 1: Number of parameters in different neural\nnetwork-based LMs. E: embedding dimension;\nC: context dimension; V: vocabulary size. †Note\nthat V ≫C(or E).\ncompression typically works with a compromise\nof performance. On the contrary, our model im-\nproves the perplexity measure after compression.\nSparse word representations. We leverage\nsparse codes of words to compress neural LMs.\nFaruqui et al. (2015) propose a sparse coding\nmethod to represent each word with a sparse vec-\ntor. They solve an optimization problem to ob-\ntain the sparse vectors of words as well as a dic-\ntionary matrix simultaneously. By contrast, we do\nnot estimate any dictionary matrix when learning\nsparse codes, which results in a simple and easy-\nto-optimize model.\n3 Our Proposed Model\nIn this section, we describe our compressed lan-\nguage model in detail. Subsection 3.1 formal-\nizes the sparse representation of words, serving\nas the premise of our model. On such a basis,\nwe compress theEmbedding and Prediction\nsubnets in Subsections 3.2 and 3.3, respectively.\nFinally, Subsection 3.4 introduces NCE for pa-\nrameter estimation where we further propose\nthe ZRegression mechanism to stabilize our\nmodel.\n3.1 Sparse Representations of Words\nWe split the vocabularyVinto two disjoint subsets\n(Band C). The ﬁrst subset Bis a base set, con-\ntaining a ﬁxed number of common words (8k in\nour experiments). C= V\\Bis a set of uncommon\nwords. We would like to useB’s word embeddings\nto encode C’s.\nOur intuition is that oftentimes a word can be\ndeﬁned by a few other words, and that rare words\nshould be deﬁned by common ones. Therefore,\nit is reasonable to use a few common words’ em-\nbeddings to represent that of a rare word. Follow-\ning most work in the literature (Lee et al., 2006;\nYang et al., 2011), we represent each uncommon\nword with a sparse, linear combination of com-\n228\nmon ones’ embeddings. The sparse coefﬁcients\nare called a sparse code for a given word.\nWe ﬁrst train a word representation model like\nSkipGram (Mikolov et al., 2013) to obtain a set of\nembeddings for each word in the vocabulary, in-\ncluding both common words and rare words. Sup-\npose U = (U1,U2,..., UB) ∈ RE×B is the\n(learned) embedding matrix of common words,\ni.e., Ui is the embedding of i-th word in B. (Here,\nB = |B|.)\nEach word in Bhas a natural sparse code (de-\nnoted as x): it is a one-hot vector withBelements,\nthe i-th dimension being on for the i-th word in B.\nFor a wordw∈C, we shall learn a sparse vector\nx = (x1,x2,...,x B) as the sparse code of the\nword. Provided that x has been learned (which\nwill be introduced shortly), the embedding of wis\nˆw=\nB∑\nj=1\nxjUj = Ux, (3)\nTo learn the sparse representation of a certain\nword w, we propose the following optimization\nobjective\nmin\nx\n∥Ux−w∥2\n2 + α∥x∥1 + β|1⊤x−1|\n+ γ1⊤max{0,−x}, (4)\nwhere max denotes the component-wise maxi-\nmum; wis the embedding for a rare word w∈C.\nThe ﬁrst term (called ﬁtting loss afterwards)\nevaluates the closeness between a word’s coded\nvector representation and its “true” representation\nw, which is the general goal of sparse coding.\nThe second term is an ℓ1 regularizer, which en-\ncourages a sparse solution. The last two regular-\nization terms favor a solution that sums to 1 and\nthat is nonnegative, respectively. The nonnegative\nregularizer is applied as in He et al. (2012) due to\npsychological interpretation concerns.\nIt is difﬁcult to determine the hyperparameters\nα, β, and γ. Therefore we perform several tricks.\nFirst, we drop the last term in the problem (4), but\nclip each element in xso that all the sparse codes\nare nonnegative during each update of training.\nSecond, we re-parametrize αand β by balanc-\ning the ﬁtting loss and regularization terms dy-\nnamically during training. Concretely, we solve\nthe following optimization problem, which is\nslightly different but closely related to the concep-\ntual objective (4):\nmin\nx\nL(x) +αtR1(x) +βtR2(x), (5)\nwhere L(x) =∥Ux−w∥2\n2, R1(x) =∥x∥1, and\nR2(x) =|1⊤x−1|. αtand βtare adaptive param-\neters that are resolved during training time. Sup-\npose xt is the value we obtain after the update of\nthe t-th step, we expect the importance of ﬁtness\nand regularization remain unchanged during train-\ning. This is equivalent to\nαtR1(xt)\nL(xt) = wα ≡const, (6)\nβtR2(xt)\nL(xt) = wβ ≡const. (7)\nor\nαt = L(xt)\nR1(xt)wα and βt = L(xt)\nR2(xt)wβ,\nwhere wα and wβ are the ratios between the regu-\nlarization loss and the ﬁtting loss. They are much\neasier to specify than αor βin the problem (4).\nWe have two remarks as follows.\n•To learn the sparse codes, we ﬁrst train the\n“true” embeddings by word2vec2 for both\ncommon words and rare words. However,\nthese true embeddings are slacked during our\nlanguage modeling.\n•As the codes are pre-computed and remain\nunchanged during language modeling, they\nare not tunable parameters of our neural\nmodel. Considering the learned sparse codes,\nwe need only 4–8 values for each word on av-\nerage, as the codes contain 0.05–0.1% non-\nzero values, which are almost negligible.\n3.2 Parameter Compression for the\nEmbedding Subnet\nOne main source of LM parameters is the\nEmbedding subnet, which takes a list of words\n(history/context) as input, and outputs dense, low-\ndimensional vector representations of the words.\nWe leverage the sparse representation of words\nmentioned above to construct a compressed\nEmbedding subnet, where the number of param-\neters is independent of the vocabulary size.\nBy solving the optimization problem (5) for\neach word, we obtain a non-negative sparse code\nx ∈RB for each word, indicating the degree to\nwhich the word is related to common words in\nB. Then the embedding of a word is given by\nˆw= Ux.\n2https://code.google.com/archive/p/word2vec\n229\nWe would like to point out that the embedding\nof a word ˆwis not sparse becauseUis a dense ma-\ntrix, which serves as a shared parameter of learn-\ning all words’ vector representations.\n3.3 Parameter Compression for the\nPrediction Subnet\nAnother main source of parameters is the\nPrediction subnet. As Table 1 shows, the out-\nput layer contains V target-word weight vectors\nand biases; the number increases with the vocabu-\nlary size. To compress this part of a neural LM, we\npropose a weight-sharing method that uses words’\nsparse representations again. Similar to the com-\npression of word embeddings, we deﬁne a base set\nof weight vectors, and use them to represent the\nrest weights by sparse linear combinations.\nWithout loss of generality, we let D = W:,1:B\nbe the output weights of Bbase target words, and\nc= b1:B be bias of the Btarget words.3 The goal\nis to use D and cto represent W and b. How-\never, as the values ofWand bare unknown before\nthe training of LM, we cannot obtain their sparse\ncodes in advance.\nWe claim that it is reasonable to share the\nsame set of sparse codes to represent word vec-\ntors in Embedding and the output weights in\nthe Prediction subnet. In a given corpus, an\noccurrence of a word is always companied by\nits context. The co-occurrence statistics about a\nword or corresponding context are the same. As\nboth word embedding and context vectors cap-\nture these co-occurrence statistics (Levy and Gold-\nberg, 2014), we can expect that context vec-\ntors share the same internal structure as embed-\ndings. Moreover, for a ﬁne-trained network, given\nany word w and its context h, the output layer’s\nweight vector corresponding to w should spec-\nify a large inner-product score for the context h;\nthus these context vectors should approximate the\nweight vector of w. Therefore, word embed-\ndings and the output weight vectors should share\nthe same internal structures and it is plausible to\nuse a same set of sparse representations for both\nwords and target-word weight vectors. As we shall\nshow in Section 4, our treatment of compressing\nthe Prediction subnet does make sense and\nachieves high performance.\nFormally, the i-th output weight vector is esti-\nmated by\nˆWi = Dxi, (8)\n3W:,1:B is the ﬁrst B columns of W.\nFigure 2: Compressing the output of neural LM.\nWe apply NCE to estimate the parameters of the\nPrediction sub-network (dashed round rectan-\ngle). The SpUnnrmProb layer outputs a sparse,\nunnormalized probability of the next word. By\n“sparsity,” we mean that, in NCE, the probability\nis computed for only the “true” next word (red)\nand a few generated negative samples.\nThe biases can also be compressed as\nˆbi = cxi. (9)\nwhere xi is the sparse representation of the i-th\nword. (It is shared in the compression of weights\nand biases.)\nIn the above model, we have managed to com-\npressed a language model whose number of pa-\nrameters is irrelevant to the vocabulary size.\nTo better estimate a “prior” distribution of\nwords, we may alternatively assign an indepen-\ndent bias to each word, i.e., bis not compressed.\nIn this variant, the number of model parameters\ngrows very slowly and is also negligible because\neach word needs only one extra parameter. Exper-\nimental results show that by not compressing the\nbias vector, we can even improve the performance\nwhile compressing LMs.\n3.4 Noise-Contrastive Estimation with\nZRegression\nWe adopt the noise-contrastive estimation (NCE)\nmethod to train our model. Compared with the\nmaximum likelihood estimation of softmax, NCE\nreduces computational complexity to a large de-\ngree. We further propose the ZRegression\nmechanism to stablize training.\nNCE generates a few negative samples for each\npositive data sample. During training, we only\n230\nneed to compute the unnormalized probability of\nthese positive and negative samples. Interested\nreaders are referred to (Gutmann and Hyv ¨arinen,\n2012) for more information.\nFormally, the estimated probability of the word\nwi with history/context his\nP(w|h; θ) = 1\nZh\nP0(wi|h; θ)\n= 1\nZh\nexp(s(wi,h; θ)), (10)\nwhere θ is the parameters and Zh is a context-\ndependent normalization factor. P0(wi|h; θ) is\nthe unnormalized probability of the w (given by\nthe SpUnnrmProb layer in Figure 2).\nThe NCE algorithm suggests to take Zh as pa-\nrameters to optimize along with θ, but it is in-\ntractable for context with variable lengths or large\nsizes in language modeling. Following Mnih and\nTeh (2012), we set Zh = 1 for all h in the base\nmodel (without ZRegression).\nThe objective for each occurrence of con-\ntext/history his\nJ(θ|h) = log P(wi|h; θ)\nP(wi|h; θ) +kPn(wi)+\nk∑\nj=1\nlog kPn(wj)\nP(wj|h; θ) +kPn(wj),\nwhere Pn(w) is the probability of drawing a nega-\ntive samplew; kis the number of negative samples\nthat we draw for each positive sample.\nThe overall objective of NCE is\nJ(θ) =Eh[J(θ|h)] ≈ 1\nM\nM∑\ni=1\nJ(θ|hi),\nwhere hi is an occurrence of the context and M is\nthe total number of context occurrences.\nAlthough setting Zh to 1 generally works well\nin our experiment, we ﬁnd that in certain sce-\nnarios, the model is unstable. Experiments show\nthat when the true normalization factor is far away\nfrom 1, the cost function may vibrate. To com-\nply with NCE in general, we therefore propose a\nZRegression layer to predict the normalization\nconstant Zh dependent on h, instead of treating it\nas a constant.\nThe regression layer is computed by\nZ−1\nh = exp(W⊤\nZ h+ bZ),\nPartitions Running words\nTrain (n-gram) 1.6 B\nTrain (neural LMs) 100 M\nDev 100 K\nTest 5 M\nTable 2: Statistics of our corpus.\nwhere WZ ∈RC and bZ ∈R are weights and bias\nfor ZRegression. Hence, the estimated proba-\nbility by NCE with ZRegression is given by\nP(w|h) = exp(s(h,w)) ·exp(W⊤\nZ h+ bZ).\nNote that the ZRegression layer does not\nguarantee normalized probabilities. During val-\nidation and testing, we explicitly normalize the\nprobabilities by Equation (1).\n4 Evaluation\nIn this part, we ﬁrst describe our dataset in Subsec-\ntion 4.1. We evaluate our learned sparse codes of\nrare words in Subsection 4.2 and the compressed\nlanguage model in Subsection 4.3. Subsection 4.4\nprovides in-depth analysis of the ZRegression\nmechanism.\n4.1 Dataset\nWe used the freely available Wikipedia 4 dump\n(2014) as our dataset. We extracted plain sen-\ntences from the dump and removed all markups.\nWe further performed several steps of preprocess-\ning such as text normalization, sentence splitting,\nand tokenization. Sentences were randomly shuf-\nﬂed, so that no information across sentences could\nbe used, i.e., we did not consider cached language\nmodels. The resulting corpus contains about 1.6\nbillion running words.\nThe corpus was split into three parts for train-\ning, validation, and testing. As it is typically time-\nconsuming to train neural networks, we sampled a\nsubset of 100 million running words to train neu-\nral LMs, but the full training set was used to train\nthe backoff n-gram models. We chose hyperpa-\nrameters by the validation set and reported model\nperformance on the test set. Table 2 presents some\nstatistics of our dataset.\n4.2 Qualitative Analysis of Sparse Codes\nTo obtain words’ sparse codes, we chose 8k com-\nmon words as the “dictionary,” i.e., B = 8000.\n4http://en.wikipedia.org\n231\nFigure 3: The sparse representations of selected\nwords. The x-axis is the dictionary of 8k common\nwords; the y-axis is the coefﬁcient of sparse cod-\ning. Note that algorithm, secret, and debate are\ncommon words, each being coded by itself with a\ncoefﬁcient of 1.\nWe had 2k–42k uncommon words in different set-\ntings. We ﬁrst pretrained word embeddings of\nboth rare and common words, and obtained 200d\nvectors U and win Equation (5). The dimension\nwas speciﬁed in advance and not tuned. As there\nis no analytic solution to the objective, we opti-\nmized it by Adam (Kingma and Ba, 2014), which\nis a gradient-based method. To ﬁlter out small co-\nefﬁcients around zero, we simply set a value to 0\nif it is less than 0.015 ·max{v∈x}. wα in Equa-\ntion (6) was set to 1 because we deemed ﬁtting loss\nand sparsity penalty are equally important. We set\nwβ in Equation (7) to 0.1, and this hyperparameter\nis insensitive.\nFigure 3 plots the sparse codes of a few selected\nwords. As we see, algorithm, secret, and debate\nare common words, and each is (sparsely) coded\nby itself with a coefﬁcient of 1. We further notice\nthat a rare word like algorithms has a sparse rep-\nresentation with only a few non-zero coefﬁcient.\nMoreover, the coefﬁcient in the code of al-\ngorithms—corresponding to the base word algo-\nrithm—is large ( ∼0.6), showing that the words\nalgorithm and algorithms are similar. Such phe-\nnomena are also observed with secret and debate.\nThe qualitative analysis demonstrates that our\napproach can indeed learn a sparse code of a word,\nand that the codes are meaningful.\n4.3 Quantitative Analysis of Compressed\nLanguage Models\nWe then used the pre-computed sparse codes to\ncompress neural LMs, which provides quantita-\ntive analysis of the learned sparse representations\nof words. We take perplexity as the performance\nmeasurement of a language model, which is de-\nﬁned by\nPPL = 2−1\nN\n∑N\ni=1 log2 p(wi|hi)\nwhere N is the number of running words in the\ntest corpus.\n4.3.1 Settings\nWe leveraged LSTM-RNN as theEncoding sub-\nnet, which is a prevailing class of neural networks\nfor language modeling (Sundermeyer et al., 2015;\nKarpathy et al., 2015). The hidden layer was 200d.\nWe used the Adam algorithm to train our neural\nmodels. The learning rate was chosen by valida-\ntion from {0.001,0.002,0.004,0.006,0.008}. Pa-\nrameters were updated with a mini-batch size of\n256 words. We trained neural LMs by NCE, where\nwe generated 50 negative samples for each pos-\nitive data sample in the corpus. All our model\nvariants and baselines were trained with the same\npre-deﬁned hyperparameters or tuned over a same\ncandidate set; thus our comparison is fair.\nWe list our compressed LMs and competing\nmethods as follows.\n•KN3. We adopted the modiﬁed Kneser-Ney\nsmoothing technique to train a 3-gram LM;\nwe used the SRILM toolkit (Stolcke and oth-\ners, 2002) in out experiment.\n•LBL5. A Log-BiLinear model introduced in\nMnih and Hinton (2007). We used 5 preced-\ning words as context.\n•LSTM-s. A standard LSTM-RNN language\nmodel which is applied in Sundermeyer et al.\n(2015) and Karpathy et al. (2015). We im-\nplemented the LM ourselves based on Theano\n(Theano Development Team, 2016) and also\nused NCE for training.\n•LSTM-z. An LSTM-RNN enhanced with\nthe ZRegression mechanism described in\nSection 3.4.\n•LSTM-z,wb. Based on LSTM-z, we com-\npressed word embeddings in Embedding\nand the output weights and biases in\nPrediction.\n•LSTM-z,w. In this variant, we did not com-\npress the bias term in the output layer. For\neach word in C, we assigned an independent\nbias parameter.\n4.3.2 Performance\nTables 3 shows the perplexity of our compressed\nmodel and baselines. As we see, LSTM-based\nLMs signiﬁcantly outperform the log-bilinear\n232\nV ocabulary 10k 22k 36k 50k\nKN3† 90. 4 125.3 146.4 159.9\nLBL5 116. 6 167.0 199.5 220.3\nLSTM-s 107. 3 159.5 189.4 222.1\nLSTM-z 75. 1 104.4 119.6 130.6\nLSTM-z,wb 73. 7 103.4 122.9 138.2\nLSTM-z,w 72. 9 101.9 119.3 129.2\nTable 3: Perplexity of our compressed language\nmodels and baselines. †Trained with the full cor-\npus of 1.6 billion running words.\nV ocabulary 10k 22k 36k 50k\nLSTM-z,w 17.76 59.28 73.42 79.75\nLSTM-z,wb 17.80 59.44 73.61 79.95\nTable 4: Memory reduction (%) by our proposed\nmethods in comparison with the uncompressed\nmodel LSTM-z. The memory of sparse codes are\nincluded.\nFigure 4: Fine-grained plot of performance\n(perplexity) and memory consumption (including\nsparse codes) versus the vocabulary size.\nmodel as well as the backoff 3-gram LM, even if\nthe 3-gram LM is trained on a much larger cor-\npus with 1.6 billion words. The ZRegression\nmechanism improves the performance of LSTM\nto a large extent, which is unexpected. Subsec-\ntion 4.4 will provide more in-depth analysis.\nRegarding the compression method proposed\nin this paper, we notice that LSTM-z ,wb and\nLSTM-z,w yield similar performance to LSTM-z.\nIn particular, LSTM-z,w outperforms LSTM-z in\nall scenarios of different vocabulary sizes. More-\nover, both LSTM-z,wb and LSTM-z,w can reduce\nthe memory consumption by up to 80% (Table 4).\nWe further plot in Figure 4 the model perfor-\nmance (lines) and memory consumption (bars) in\na ﬁne-grained granularity of vocabulary sizes. We\nsee such a tendency that compressed LMs (LSTM-\nz,wb and LSTM-z ,w, yellow and red lines) are\ngenerally better than LSTM-z (black line) when\nwe have a small vocabulary. However, LSTM-\nz,wb is slightly worse than LSTM-z if the vocabu-\nlary size is greater than, say, 20k. The LSTM-z ,w\nremains comparable to LSTM-z as the vocabulary\ngrows.\nTo explain this phenomenon, we may imagine\nthat the compression using sparse codes has two\neffects: it loses information, but it also enables\nmore accurate estimation of parameters especially\nfor rare words. When the second factor dominates,\nwe can reasonably expect a high performance of\nthe compressed LM.\nFrom the bars in Figure 4, we observe that tra-\nditional LMs have a parameter space growing lin-\nearly with the vocabulary size. But the number\nof parameters in our compressed models does not\nincrease—or strictly speaking, increases at an ex-\ntremely small rate—with vocabulary.\nThese experiments show that our method can\nlargely reduce the parameter space with even per-\nformance improvement. The results also verify\nthat the sparse codes induced by our model indeed\ncapture meaningful semantics and are potentially\nuseful for other downstream tasks.\n4.4 Effect of ZRegression\nWe next analyze the effect of ZRegression for\nNCE training. As shown in Figure 5a, the training\nprocess becomes unstable after processing 70% of\nthe dataset: the training loss vibrates signiﬁcantly,\nwhereas the test loss increases.\nWe ﬁnd a strong correlation between unsta-\nbleness and the Zh factor in Equation (10), i.e.,\nthe sum of unnormalized probability (Figure 5b).\nTheoretical analysis shows that theZhfactor tends\nto be self-normalized even though it is not forced\nto (Gutmann and Hyv ¨arinen, 2012). However,\nproblems would occur, should it fail.\nIn traditional methods, NCE jointly estimates\nnormalization factor Z and model parameters\n(Gutmann and Hyv ¨arinen, 2012). For language\nmodeling, Zh dependents on context h. Mnih\nand Teh (2012) propose to estimate a separate Zh\nbased on two history words (analogous to 3-gram),\nbut their approach hardly scales to RNNs because\nof the exponential number of different combina-\ntions of history words.\nWe propose the ZRegression mechanism in\nSection 3.4, which can estimate the Zh factor well\n(Figure 5d) based on the history vector h. In\nthis way, we manage to stabilize the training pro-\ncess (Figure 5c) and improve the performance by\n233\n(a) Training/test loss vs. training time w/o\nZRegression.\n(b) The validation perplexity and normalization factor Zh w/o\nZRegression.\n(c) Training loss vs. training time w/\nZRegression of different runs.\n(d) The validation perplexity and normalization factor Zh w/\nZRegression.\nFigure 5: Analysis of ZRegression.\na large margin, as has shown in Table 3.\nIt should be mentioned that ZRegression is\nnot speciﬁc to model compression and is generally\napplicable to other neural LMs trained by NCE.\n5 Conclusion\nIn this paper, we proposed an approach to repre-\nsent rare words by sparse linear combinations of\ncommon ones. Based on such combinations, we\nmanaged to compress an LSTM language model\n(LM), where memory does not increase with the\nvocabulary size except a bias and a sparse code\nfor each word. Our experimental results also show\nthat the compressed LM has yielded a better per-\nformance than the uncompressed base LM.\nAcknowledgments\nThis research is supported by the National Ba-\nsic Research Program of China (the 973 Pro-\ngram) under Grant No. 2015CB352201, the Na-\ntional Natural Science Foundation of China under\nGrant Nos. 61232015, 91318301, 61421091 and\n61502014, and the China Post-Doctoral Founda-\ntion under Grant No. 2015M580927.\nReferences\nJacob Andreas and Dan Klein. 2014. When and why\nare log-linear models self-normalizing. In Proceed-\nings of the Annual Meeting of the North American\nChapter of the Association for Computational Lin-\nguistics, pages 244–249.\nYoshua Bengio and Jean-S ´ebastien Sen ´ecal. 2003.\nQuick training of probabilistic neural nets by im-\nportance sampling. In Proceedings of the Ninth In-\nternational Workshop on Artiﬁcial Intelligence and\nStatistics.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression. In\nProceedings of the 12th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and\nData Mining, pages 535–541.\nWelin Chen, David Grangier, and Michael Auli. 2015.\nStrategies for training large vocabulary neural lan-\nguage models. arXiv preprint arXiv:1512.04906.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard M Schwartz, and John Makhoul.\n2014. Fast and robust neural network joint models\nfor statistical machine translation. In Proceedings\nof the 52rd Annual Meeting of the Association for\nComputational Linguistics, pages 1370–1380.\n234\nManaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris\nDyer, and Noah A. Smith. 2015. Sparse overcom-\nplete word vector representations. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics, pages 1491–1500.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization. arXiv preprint\narXiv:1412.6115.\nMichael Gutmann and Aapo Hyv ¨arinen. 2012. Noise-\ncontrastive estimation of unnormalized statistical\nmodels, with applications to natural image statis-\ntics. The Journal of Machine Learning Research ,\n13(1):307–361.\nZhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun\nZhang, Deng Cai, and Xiaofei He. 2012. Document\nsummarization based on data reconstruction. InPro-\nceedings of the 26th AAAI Conference on Artiﬁcial\nIntelligence, pages 620–626.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisser-\nman. 2014. Speeding up convolutional neural net-\nworks with low rank expansions. In Proceedings of\nthe British Machine Vision Conference.\nS´ebastien Jean, Kyunghyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2014. On using very large tar-\nget vocabulary for neural machine translation. arXiv\npreprint arXiv:1412.2007.\nDan Jurafsky and James H. Martin. 2014. Speech and\nLanguage Processing. Pearson.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li. 2015.\nVisualizing and understanding recurrent networks.\narXiv preprint arXiv:1506.02078.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nHonglak Lee, Alexis Battle, Rajat Raina, and An-\ndrew Y Ng. 2006. Efﬁcient sparse coding algo-\nrithms. In Advances in Neural Information Process-\ning Systems, pages 801–808.\nOmer Levy and Yoav Goldberg. 2014. Linguistic reg-\nularities in sparse and explicit word representations.\nIn Proceedings of the Eighteenth Conference on Nat-\nural Language Learning, pages 171–180.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In IN-\nTERSPEECH, pages 1045–1048.\nTomas Mikolov, Anoop Deoras, Daniel Povey, Lukas\nBurget, and Jan Cernock ´y. 2011. Strategies for\ntraining large scale neural network language models.\nIn Proceedings of the IEEE Workshop on Automatic\nSpeech Recognition and Understanding, pages 196–\n201.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn Proceedings of the 24th International Conference\non Machine learning, pages 641–648.\nAndriy Mnih and Yee-Whye Teh. 2012. A fast and\nsimple algorithm for training neural probabilistic\nlanguage models. arXiv preprint arXiv:1206.6426.\nFr´ederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nProceedings of the International Workshop on Arti-\nﬁcial Intelligence and Statistics, pages 246–252.\nLili Mou, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin.\n2015a. Distilling word embeddings: An encoding\napproach. arXiv preprint arXiv:1506.04488.\nLili Mou, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin.\n2015b. Backward and forward language modeling\nfor constrained natural language generation. arXiv\npreprint arXiv:1512.06612.\nAlexander M Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 379–389.\nAlessandro Sordoni, Michel Galley, Michael Auli,\nChris Brockett, Yangfeng Ji, Margaret Mitchell,\nJian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.\nA neural network approach to context-sensitive gen-\neration of conversational responses. In Proceed-\nings of the 2015 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n196–205.\nAndreas Stolcke et al. 2002. SRILM—An extensi-\nble language modeling toolkit. In INTERSPEECH,\npages 901–904.\nMartin Sundermeyer, Hermann Ney, and Ralf Schl¨uter.\n2015. From feedforward to recurrent LSTM neural\nnetworks for language modeling. IEEE/ACM Trans-\nactions on Audio, Speech and Language Processing,\n23(3):517–529.\nTheano Development Team. 2016. Theano: A Python\nframework for fast computation of mathematical ex-\npressions. arXiv preprint arXiv:1605.02688.\nMeng Yang, Lei Zhang, Jian Yang, and David Zhang.\n2011. Robust sparse coding for face recognition. In\nProceedings of the 2011 IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 625–\n632.\n235",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9156615734100342
    },
    {
      "name": "Computer science",
      "score": 0.7889516353607178
    },
    {
      "name": "Language model",
      "score": 0.7580975890159607
    },
    {
      "name": "Word (group theory)",
      "score": 0.6744823455810547
    },
    {
      "name": "Vocabulary",
      "score": 0.6305767297744751
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5936009883880615
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5610872507095337
    },
    {
      "name": "Layer (electronics)",
      "score": 0.5293166041374207
    },
    {
      "name": "Artificial neural network",
      "score": 0.5291234254837036
    },
    {
      "name": "Natural language processing",
      "score": 0.4879254996776581
    },
    {
      "name": "Context model",
      "score": 0.4421907067298889
    },
    {
      "name": "Speech recognition",
      "score": 0.3908958435058594
    },
    {
      "name": "Mathematics",
      "score": 0.14632651209831238
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    }
  ]
}