{
    "title": "MultiSpeech: Multi-Speaker Text to Speech with Transformer",
    "url": "https://openalex.org/W3033194228",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2222900133",
            "name": "Chen Mingjian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2150496052",
            "name": "Tan, Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105243522",
            "name": "Ren Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1973131093",
            "name": "Xu Jin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003959196",
            "name": "Sun Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2151703812",
            "name": "Zhao Sheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105471080",
            "name": "Qin Tao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209371238",
            "name": "Liu Tie-Yan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2949382160",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2972702018",
        "https://openalex.org/W2795109282",
        "https://openalex.org/W2608207374",
        "https://openalex.org/W2886769154",
        "https://openalex.org/W2970079483",
        "https://openalex.org/W2767052532",
        "https://openalex.org/W2964243274",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2897548994",
        "https://openalex.org/W854541894",
        "https://openalex.org/W2527729766",
        "https://openalex.org/W2963432880",
        "https://openalex.org/W2970730223",
        "https://openalex.org/W2926840633",
        "https://openalex.org/W2792995953"
    ],
    "abstract": "Transformer-based text to speech (TTS) model (e.g., Transformer TTS~\\cite{li2019neural}, FastSpeech~\\cite{ren2019fastspeech}) has shown the advantages of training and inference efficiency over RNN-based model (e.g., Tacotron~\\cite{shen2018natural}) due to its parallel computation in training and/or inference. However, the parallel computation increases the difficulty while learning the alignment between text and speech in Transformer, which is further magnified in the multi-speaker scenario with noisy data and diverse speakers, and hinders the applicability of Transformer for multi-speaker TTS. In this paper, we develop a robust and high-quality multi-speaker Transformer TTS system called MultiSpeech, with several specially designed components/techniques to improve text-to-speech alignment: 1) a diagonal constraint on the weight matrix of encoder-decoder attention in both training and inference; 2) layer normalization on phoneme embedding in encoder to better preserve position information; 3) a bottleneck in decoder pre-net to prevent copy between consecutive speech frames. Experiments on VCTK and LibriTTS multi-speaker datasets demonstrate the effectiveness of MultiSpeech: 1) it synthesizes more robust and better quality multi-speaker voice than naive Transformer based TTS; 2) with a MutiSpeech model as the teacher, we obtain a strong multi-speaker FastSpeech model with almost zero quality degradation while enjoying extremely fast inference speed.",
    "full_text": "MultiSpeech: Multi-Speaker Text to Speech with Transformer\nMingjian Chen1, Xu Tan2, Yi Ren3, Jin Xu4, Hao Sun1, Sheng Zhao5, Tao Qin2, Tie-Yan Liu2\n1School of Software and Microelectronics, Peking University\n2Microsoft Research Asia, 3Zhejiang Univeristy, 4Tsinghua University, 5Microsoft Azure Speech\nmilk@pku.edu.cn, xuta@microsoft.com, rayeren@zju.edu.cn, j-xu18@mails.tsinghua.edu.cn,\nsigmeta@pku.edu.cn, Sheng.Zhao@microsoft.com, taoqin@microsoft.com, tyliu@microsoft.com\nAbstract\nTransformer-based text to speech (TTS) model (e.g., Trans-\nformer TTS [1], FastSpeech [2]) has shown the advantages\nof training and inference efﬁciency over RNN-based model\n(e.g., Tacotron [3]) due to its parallel computation in train-\ning and/or inference. However, the parallel computation in-\ncreases the difﬁculty while learning the alignment between text\nand speech in Transformer, which is further magniﬁed in the\nmulti-speaker scenario with noisy data and diverse speakers,\nand hinders the applicability of Transformer for multi-speaker\nTTS. In this paper, we develop a robust and high-quality multi-\nspeaker Transformer TTS system called MultiSpeech, with sev-\neral specially designed components/techniques to improve text-\nto-speech alignment: 1) a diagonal constraint on the weight ma-\ntrix of encoder-decoder attention in both training and inference;\n2) layer normalization on phoneme embedding in encoder to\nbetter preserve position information; 3) a bottleneck in decoder\npre-net to prevent copy between consecutive speech frames.\nExperiments on VCTK and LibriTTS multi-speaker datasets\ndemonstrate the effectiveness of MultiSpeech: 1) it synthesizes\nmore robust and better quality multi-speaker voice than naive\nTransformer based TTS; 2) with a MutiSpeech model as the\nteacher, we obtain a strong multi-speaker FastSpeech model\nwith almost zero quality degradation while enjoying extremely\nfast inference speed.\nIndex Terms: text to speech, multi-speaker, Transformer, Fast-\nSpeech, attention alignment\n1. Introduction\nIn recent years, neural text to speech (TTS) models such as\nTacotron [4, 3], Transformer TTS [1] and FastSpeech [2] have\nled to high-quality single-speaker TTS systems using large\namount of clean training data. Thanks to the parallel computa-\ntion in Transformer [5], Transformer based TTS enjoys much\nbetter training [1, 2] and inference [2] efﬁciency than RNN\nbased TTS [4, 3].\nTo reduce deployment and serving cost in commercial ap-\nplications, building a TTS system supporting multiple (hun-\ndreds or thousands) speakers has attracted much attention in\nboth industry and academia [6, 7, 3, 8]. While it is affordable to\nrecord high-quality and clean voice in professional studios for\na single speaker, it is costly to do so for hundreds or thousands\nof speakers to build a multi-speaker TTS system. Thus, multi-\nspeaker TTS systems are usually built using multi-speaker data\nrecorded for automatic speech recognition (ASR) [9, 10] or\nvoice conversion [11], which is noisy and of low-quality due\nto the diversity and variances of prosodies, speaker accents,\nspeeds and recording environments. Although Transformer\nThis work was done while the ﬁrst, fourth and ﬁfth authors were\ninterning at Microsoft. Correspondence to: Tao Qin.\nbased models have shown advantages over other neural models\nfor single-speaker TTS, existing works on multi-speaker TTS\nmostly adopt RNN (e.g., Tacotron [4, 3]) or CNN (e.g., Deep\nV oice [6, 7]) as the model backbone, and few attempts have\nbeen made to build Transformer based multi-speaker TTS.\nThe main challenge of Transformer multi-speaker TTS\ncomes from the difﬁculty of learning the text-to-speech align-\nment, while such alignment plays an important role in TTS\nmodeling [3, 7, 2]. While applying Transformer to multi-\nspeaker TTS, the text-to-speech alignment between the encoder\nand decoder is more difﬁcult than that of RNN models. When\ncalculating the attention weights in each decoder time step\nin RNN, advanced strategies such as location-sensitive atten-\ntion [12] are leveraged to ensure the attention move forward\nconsistently through the input, avoiding word skipping and re-\npeating problems. Location-sensitive attention leverages the at-\ntention results in previous decoder time steps, which, unfor-\ntunately, cannot be used in Transformer due to parallel com-\nputation during training. In single-speaker TTS, the text and\nspeech data are usually of high-quality and the text-to-speech\nalignments are easy to learn. However, as aforementioned,\nthe speech data for multi-speaker TTS is usually noisy, which\nmakes the alignments much more difﬁcult. Actually, CNN\nmulti-speaker TTS also faces this challenge, and complex sys-\ntems are designed based on the characteristics of CNN structure\nin [6, 7], which unfortunately cannot be easily applied on Trans-\nformer models.\nIn order to bring the advantages of Transformer into multi-\nspeaker TTS modeling, in this paper, we develop a robust\nand high-quality multi-speaker TTS system called MultiSpeech,\nwhich greatly improves the text-to-speech alignment in Trans-\nformer. Speciﬁcally, we introduce several techniques to im-\nprove the alignments based on empirical observations and in-\nsights. First, considering the attention alignments between the\ntext encoder and speech decoder are usually monotonic and di-\nagonal, we introduce a diagonal constraint on the weight matrix\nof the encoder-decoder attention during training and inference.\nSecond, position embeddings are important in Transformer and\ncan help text-to-speech alignment [7] 1, and are usually added\nto phoneme embeddings in the Transformer encoder. How-\never, the scale of phoneme embeddings can vary a lot while\nthat of position embeddings is ﬁxed, which causes magnitude\nmismatch2 while added together and consequently increases the\ndifﬁculty of model training. Therefore, we add a layer normal-\nization step on phoneme embeddings to make them comparable\n1Ideally, the model can learn the monotonic alignment simply\nthrough the position embeddings in text and speech sequences.\n2The embeddings of some phonemes are large and will dominate\nposition embeddings, and some phonemes are of small embeddings and\nwill be dominated by position embeddings, both of which will harm the\nalignment learning.\narXiv:2006.04664v2  [eess.AS]  1 Aug 2020\nFigure 1: The model structure of our proposed MultiSpeech.\nThe green blocks are the newly added modules for multi-speaker\nTTS based on Transformer.\nand better preserve position information. Third, text-to-speech\nalignments should be learnt by attending to source phonemes\nwhile generating target speech frames. However, two adjacent\nspeech frames are usually similar and standard Transformer de-\ncoder tends to directly copy previous frame to generate the\ncurrent frame. Consequently, no alignments between text and\nspeech can be learned. To prevent direct copy between consec-\nutive speech frames, we employ a bottleneck structure in the\ndecoder pre-net which encourages the decoder to generalize on\nthe representation of speech frame instead of memorization, and\nforces the decoder to attend to text/phoneme inputs.\nExperiments on VCTK and LibriTTS multi-speaker\ndatasets show that 1) MultiSpeech achieves great improvements\n(1.01 MOS gain on VCTK and 1.46 MOS gain on LibriTTS)\nover naive Transformer based TTS and synthesizes robust and\nhigh-quality multi-speaker voice. 2) The three proposed tech-\nniques can indeed improve text-to-speech alignments, measured\nby the attention diagonal rate. 3) A well trained MultiSpeech\nmodel can be used as a teacher for FastSpeech training and we\nobtain a strong multi-speaker FastSpeech model without quality\ndegradation but enjoying extremely fast inference.\n2. Background\nTransformer TTS. Transformer based TTS (e.g. [1]) adopts the\nbasic model structure of Transformer [5], as shown in Figure 1\n(remove the green blocks). Each transformer block consists of a\nmulti-head self-attention network and a feed-forward network.\nAdditionally, a decoder pre-net is leveraged to pre-process the\nmel-spectrogram frame, and a mel linear layer is used to predict\nthe mel-spectrogram frame and a stop linear layer to predict if\nshould stop in each predicted frame. Transformer can ensure\nparallel computation during training, which, as a side effect,\nharms the attention alignments between text and speech, as an-\nalyzed in the introduction part. As a result, it is challenging to\nbuild multi-speaker TTS on Transformer considering the com-\n(a) (b)\nFigure 2: (a) The illustration of diagonal constraint in attention,\nwhere the above ﬁgure has a small diagonal constraint loss and\nthe below ﬁgure has a large diagonal constraint loss. (b) The\nmodel structure of the pre-net bottleneck in decoder.\nplicated acoustic conditions in multi-speaker speech. In this pa-\nper, we analyze each component in Transformer TTS to ﬁgure\nout why it fails to learn alignments, and propose the correspond-\ning modiﬁcations to improve the alignments.\nMulti-Speaker TTS. Several works have built multi-speaker\ntext to speech systems based on RNN [13, 14] and CNN [6, 7].\nRNN-based multi-speaker model enjoys the beneﬁts of recur-\nrent attention computation as in Tacotron 2 [3], which can\nleverage the attention information in previous steps to help\nthe attention calculation in current step. CNN-based multi-\nspeaker model [7] develops many sophisticated mechanisms in\nthe speaker embedding and attention block to ensure the syn-\nthesized quality. V AE-based method [8] is further leveraged\nto handle noisy multi-speaker speech data [10]. Considering\nthe advantages of Transformer including parallel training over\nRNN and effective sequence modeling over CNN, in this paper,\nwe build multi-speaker TTS on Transformer model.\nText-to-Speech Alignment. Since text and speech corre-\nspond to each other in TTS, the alignments between text and\nspeech are generally monotonic and diagonal in the encoder-\ndecoder attention weights. Previous works have tried different\ntechniques to ensure the alignments between text and speech\nin encoder-decoder model. Location sensitive attention [12] is\nproposed to align the source and target better by leveraging pre-\nvious attention information. [15, 16, 17, 18] improve text-to-\nspeech alignments by designing sophisticated techniques on at-\ntention. [7] design position encoding with its angular frequency\ndetermined dynamically by each speaker embedding to ensure\nthe text-speech alignment. [4] uses large dropout in decoder\npre-net and ﬁnds it is helpful for attention alignment. In this pa-\nper, we introduce several techniques to improve the alignments\nspeciﬁcally in Transformer model.\n3. Improving Text-to-Speech Alignment\nIn this section, we introduce several techniques to improve the\ntext-to-speech alignments in MultiSpeech, from the attention,\nencoder and decoder part respectively, as shown in Figure 1.\n3.1. Diagonal Constraint in Attention\nMonotonic and diagonal alignments in the attention weights be-\ntween text and speech are critical to ensure the quality of synthe-\nsized speech [12, 15, 16, 7, 17, 18]. In multi-speaker scenario,\nthe speech is usually noisy and different speakers have different\nspeeds and acoustic conditions, making the alignments difﬁcult.\nTherefore, we propose to add diagonal constraint on the atten-\ntion weights to force the model to learn correct alignments.\nWe ﬁrst formulate the diagonal attention rate ras\nr=\n∑T\nt=1\n∑kt+b\ns=kt−b At,s\nS , (1)\nwhere S is the length of speech mel-spectrogram and T is the\nlength of text (phoneme or character). k = S\nT is the slop for\neach training sample and bis a hyperparameter for bandwidth,\nboth of which determine the shape the diagonal area. At,s is\nthe t-th row and s-th column of the attention weight matrix A.\nThe numerator represents how much weight lie in the diagonal\narea while the denominator represents the total attention weight\nwhich equals to speech length S. The diagonal constraint loss\nLDC encourages larger attention weights in the diagonal area\nas shown in Figure 2, which is deﬁned as LDC = −r, where r\nis deﬁned in Equation 1. LDC is added on the original TTS loss\nwith a weight λto adjust the strength of the constraint.\nIn order to ensure the correct alignment during inference,\nwe also add attention constrain in the autoregressvie gener-\nation process. We introduce an attention sliding window in\nthe text side and compute the attention weights only within\nthis window. The range of the window is [-1, 4], where 0\nin the window represents the window center and is initialized\nas position 0 in the beginning. The window allows the pre-\ndicted frame to attend on both previous 1 phoneme and future\n4 phonemes of the center. We design a sliding window mov-\ning strategy: we deﬁne the attention centroid of s-th predicted\nframe as Cs = ⌊∑T\nt=0(At,s ∗t)⌋. If Cs deviates the window\ncenter beyond 3 consecutive frames, we move the sliding win-\ndow center one step forward.\nCompared with the attention constraint strategy proposed\nin [7], our method has the following advantages: 1) our sliding\nwindow allows to attend to the previous position, and 2) we use\nattention centroid rather than simply the position of the high-\nest attention weight within the current window as new sliding\nwindow center. These improvements can prevent the sliding\nwindow from moving forward too early, which usually results\nin skipping phonemes and fast speaking speed.\n3.2. Position Information in Encoder\nThe encoder of Transformer based TTS model usually takes\nx+ pas input, where xis the embedding of phoneme/character\ntoken and p is positional embedding to give the Transformer\nmodel a sense of token order. pis usually formulated as trian-\ngle positional embeddings [5] and the scale of its value is ﬁxed\ninto [−1,1]. However, the embedding xis learned end-to-end,\nand the scale of the its value can be very large or small. As a\nresult, the position information p in x+ p is relatively small\nor large, which will affect the alignment learning between the\nsource (text) and target (speech) sequence.\nTo preserve the position information properly in x+ p, we\nﬁrst add layer normalization [19] on xand then add with p, i.e.,\nLN(x) +p, as shown in Figure 1. LN(x) is deﬁned as\nLN(x) =γx−µ\nσ + β, (2)\nwhere µ and σ are the mean and variance of vector x, γ and\nβ are the scale and bias parameters. In this case, the scale of\nphoneme embedding xcan be restricted to a limited range by\nlearning the scale and bias parameters in layer normalization.\nIn Transformer TTS [1], a scalar trainable weight α is\nleveraged to adjust pbefore adding on x, i.e., x+ αp. How-\never, it cannot necessarily ensure enough position information\nin x+ αp, since a single scalar αcannot balance the scales be-\ntween position informationpand embedding x, considering dif-\nferent phonemes/characters have different scales3. We also ver-\nify the advantage of our layer normalization over simple scalar\ntrainable weight in the experiment part4.\n3.3. Pre-Net Bottleneck in Decoder\nThe adjacent frames of mel-spectrogram are usually very simi-\nlar since the hop size is usually much smaller than the window\nsize5, which means two adjacent frames have large information\noverlap. As a consequence, when predicting next frame given\ncurrent frame as input in autoregressive training, the model is\nprone to directly copy some information from the input frame\ninstead of extracting information from text side for meaning-\nful prediction. The decoder pre-net in [4] leverages a structure\nlike 80-256-128 where each number represents the hidden size\nof each layer in the pre-net, while the decoder pre-net in [3, 1]\nleverages a structure like 80-256-256-512, both with dropout\nrate of 0.5. The authors [4, 3] claim this structure can act like a\nbottleneck to prevent from copy (the hidden size is halved in the\nbottleneck, e.g., 128 vs.256 or 256 vs. 512). However, the mel-\nspectrogram with a dimension of 80 is ﬁrst converted into 512\nor 256 hidden and is then halved to 256 or 128, which is still\nlarger than 80 and cannot necessarily prevent copy and learn\nalignments in multi-speaker scenario, according to our experi-\nments. As shown in Figure 2, we further reduce the bottleneck\nhidden size to as small as 1/8 of the original hidden size (e.g.,\n32 vs. the original hidden size 256) plus with 0.5 dropout ratio,\nand the structure becomes 80-32-32-256. We found this small\nbottleneck size is essential to learn meaningful alignments and\navoid direct copying input frame.\n4. Experiments and Results\nIn this section, we conduct experiments to verify the advantages\nof MultiSpeech and the effectiveness of the proposed techniques\nto improve text-to-speech alignments.\n4.1. Experimental Setup\nDatasets. We conducted experiments on the VCTK [11] and\nLibriTTS [10] multi-speaker datasets. The VCTK dataset con-\ntains 44 hours speech with 108 speakers, while the LibriTTS\ndataset contains 586 hours speech with 2456 speakers. We con-\nvert the speech sampling rate of both corpus to 16KHz, and use\n12.5ms hop size, 50ms window size to extract mel-spectrogram.\nWe convert text into phoneme using grapheme-to-phoneme con-\nversion [20] and take phoneme as the encoder input.\nModel Conﬁguration. The model structure of MultiSpeech\nis shown in Figure 1. Both the encoder and decoder use 4-\nlayer transformer blocks. The hidden size, attention head, feed-\nforward ﬁlter size and kernel size are 256, 2, 1024 and 9 respec-\ntively. In addition, the decoder pre-net bottleneck, as shown in\nFigure 2, is 32, which is 1/8 of the hidden size. For the speaker\nmodule as shown in Figure 1, we follow the structure in [7].\n3We do not normalize the input in decoder, since mel-specotrgram\nis not learnable and usually normalized into a ﬁxed range. This point\nis also conﬁrmed in [1], where the scalar trainable weight in decoder is\nmuch more stable and closer to 1 than that in encoder.\n4Our layer normalization is also better than learnable position em-\nbeddings since it still learns a global embedding for each position.\n5The typical parameters of window size and hop size in TTS is 50ms\nand 12.5ms.\nTraining and Inference. We use 4 P100 GPUs, each with\nbatch size of about 20,000 speech frames. We use Adam op-\ntimizer with β1 = 0.9, β2 = 0.98, ϵ = 10−9 and follow the\nlearning rate schedule in [5]. The bandwidth bin the attention\nconstraint is set to 50, and the weight λof LDC is set to 0.01\naccording to the valid performance. During inference, we use\nattention constraint as described in Section 3.1 to ensure the\ntext-to-speech alignments. WaveNet [21] is used as vocoder to\nsynthesize voice.\nEvaluation. We use MOS (mean opinion score) to measure\nthe voice quality. Each sentence is judged by 20 native speakers.\nWe also use the diagonal attention rate r as deﬁned in Equa-\ntion 1 to measure the quality of text-to-speech alignments. A\nhigher MOS means better voice quality while a higher rmeans\nbetter alignments, and they are correlated to each other. For\nboth VCTK and LibriTTS, we select 6 speakers (3 men and 3\nwomen, each with 5 sentences) for evaluation respectively.\n4.2. The Quality of MultiSpeech\nThe MOS results are shown in Table 1. We compare our pro-\nposed MultiSpeech with 1) GT, the ground-truth recording, 2)\nGT mel + Vocoder , we ﬁrst convert the recording into mel-\nspectrogram and then convert the mel-spectrogram back to au-\ndio with V ocoder, and 3)Transformer based TTS, we only add\nthe speaker embedding module on naive Transformer based\nTTS model to support multiple speakers, without using any\nof our proposed techniques to improve alignments. It can be\nseen that MultiSpeech achieves large MOS score improvements\nover Transformer based TTS. Transformer based TTS cannot\nlearn effective alignments on most sentences and causes word\nskipping and repeating issues, or totally crashed voice. The\nMOS score of MultiSpeech on VCTK is also close to GT mel\n+ Vocoder. These results demonstrate the advantages of Multi-\nSpeech for multi-speaker TTS. We show some demo audios and\ncase analyses in this link6.\nTable 1: The MOS scores with 95% conﬁdence intervals on\nVCTK and LibriTTS.\nSetting VCTK LibriTTS\nGT 4.04 ±0.14 4 .14 ±0.16\nGT mel + Vocoder 3.89 ±0.20 3 .90 ±0.08\nTransformer based TTS 2.64 ±0.35 1 .49 ±0.09\nMultiSpeech 3.65 ±0.14 2 .95 ±0.14\n4.3. Method Analysis\nAblation Study. We ﬁrst conduct ablation study on VCTK\ndataset to verify the effectiveness of each proposed technique:\ndiagonal constraint (DC) in attention, layer normalization (LN)\nin encoder, pre-net bottleneck (PB) in decoder. The results are\nshown in Table 2. After removing diagonal constraint (DC),\nlayer normalization (LN) and pre-net bottleneck (PB) respec-\ntively, both MOS score and diagonal rate r drop. After fur-\nther removing all the three techniques (-DC-LN-PB, i.e., Trans-\nformer based TTS), both MOS and r drop largely. These ab-\nlation studies verify the effectiveness of the three techniques to\nimprove attention alignments for better voice quality.\nComparison between layer normalization and learnable\nweight. We calculate the similarity between p and three set-\ntings: 1) LN(x)+ p, our proposed layer normalization (LN); 2)\nx+αp, the learnable weight (LW) used in [1]; 3)x+p, the naive\n6https://speechresearch.github.io/multispeech/\nTable 2: The MOS with 95% conﬁdence intervals and diagonal\nattention rate rof the ablation study on VCTK.−DC means not\nusing diagonal constraint during training and inference. −LN\nmeans using x+ pas encoder input but not LN(x) +p. −PB\nmeans using pre-net structure like 80-256-256-256 instead of\nour proposed 80-32-32-256.\nSetting MOS r\nMultiSpeech 3.65 ±0.14 0.694\n−DC 3.59 ±0.25 0.502\n−LN 3.08 ±0.05 0.637\n−PB 3.36 ±0.27 0.658\n−DC−LN−PB 2.64 ±0.35 0.366\nTransformer baseline (Baseline). As shown in Table 3, the sim-\nilarity of LN is in between LW and Baseline, which shows the\nposition information in LN is neither too weak (as in Baseline)\nnor too strong (as in LW7) and is helpful for attention align-\nment. This is also veriﬁed by the diagonal attention rate r in\nTable 3. Our proposed LN achieves the highest rwhile LW the\nlowest, which demonstrates that too strong position dominates\nphoneme embedding and harms the attention alignment.\nTable 3: The comparison of similarity and diagonal attention\nrate rbetween LN, LW and Baseline settings.\nSetting LN LW Baseline\nSimilarity 0.126 0.184 0.089\nr 0.694 0.506 0.637\n4.4. Extension on FastSpeech\nWe further use MultiSpeech as a teacher to teach a multi-\nspeaker FastSpeech [2] on VCTK dataset, following the setting\nin [2]. We select 6 speakers (3 men and 3 women, each with 10\nsentences) for MOS evaluation. As shown in Table 4, we can\nobtain a strong FastSpeech model with nearly the same MOS\nscore with MultiSpeech teacher8.\nTable 4: The MOS score of multi-speaker FastSpeech on VCTK\nwith 95% conﬁdence intervals.\nSetting GT MultiSpeech FastSpeech\nMOS 4.02 ±0.09 3 .53 ±0.22 3 .45 ±0.13\n5. Conclusions\nIn this paper, we developed MultiSpeech, a multi-speaker\nTransformer TTS system that leverages three techniques includ-\ning diagonal constraint in attention, layer normalization in en-\ncoder and pre-net bottleneck in decoder, to improve the text-\nto-speech alignments in multi-speaker scenario. Experiments\non VCTK and LibriTTS multi-speaker datasets demonstrate ef-\nfectiveness of MutiSpeech: 1) it generates much higher-quality\nand more stable voice compared with Transformer TTS base-\nline; 2) using MultiSpeech as a teacher, we obtain a strong\nmulti-speaker FastSpeech model to enjoy extremely fast infer-\nence speed. In the future, we will continue to improve the voice\nquality of MultiSpeech and multi-speaker FastSpeech model to\ndeliver better multi-speaker TTS solutions.\n7We check the ﬁnal learnable weight α= 2.62, which is much big-\nger than the single-speaker setting in [1] (αis about 0.5). We guess that\nwhen added with different scales of phoneme embedding, LW simply\nlearns a global large αto highlight position embedding.\n8We can use more unlabeled text in knowledge distillation to further\nimprove FastSpeech quality to match or even outperform teacher model.\n6. References\n[1] N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu, “Neural speech synthe-\nsis with transformer network,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 6706–6713.\n[2] Y . Ren, Y . Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y .\nLiu, “Fastspeech: Fast, robust and controllable text to speech,”\nin Advances in Neural Information Processing Systems, 2019, pp.\n3165–3174.\n[3] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y . Zhang, Y . Wang, R. Skerrv-Ryan et al. , “Natural\ntts synthesis by conditioning wavenet on mel spectrogram pre-\ndictions,” in 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018, pp. 4779–\n4783.\n[4] Y . Wang, R. Skerry-Ryan, D. Stanton, Y . Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y . Xiao, Z. Chen, S. Bengio et al. ,\n“Tacotron: Towards end-to-end speech synthesis,”arXiv preprint\narXiv:1703.10135, 2017.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in neural information processing systems , 2017, pp.\n5998–6008.\n[6] A. Gibiansky, S. Arik, G. Diamos, J. Miller, K. Peng, W. Ping,\nJ. Raiman, and Y . Zhou, “Deep voice 2: Multi-speaker neural text-\nto-speech,” inAdvances in neural information processing systems,\n2017, pp. 2962–2970.\n[7] W. Ping, K. Peng, A. Gibiansky, S. O. Arik, A. Kannan,\nS. Narang, J. Raiman, and J. Miller, “Deep voice 3: Scaling text-\nto-speech with convolutional sequence learning,” arXiv preprint\narXiv:1710.07654, 2017.\n[8] W.-N. Hsu, Y . Zhang, R. J. Weiss, H. Zen, Y . Wu, Y . Wang,\nY . Cao, Y . Jia, Z. Chen, J. Shen et al. , “Hierarchical genera-\ntive modeling for controllable speech synthesis,” arXiv preprint\narXiv:1810.07217, 2018.\n[9] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an asr corpus based on public domain audio books,”\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2015, pp. 5206–5210.\n[10] H. Zen, V . Dang, R. Clark, Y . Zhang, R. J. Weiss, Y . Jia, Z. Chen,\nand Y . Wu, “Libritts: A corpus derived from librispeech for text-\nto-speech,”arXiv preprint arXiv:1904.02882, 2019.\n[11] C. Veaux, J. Yamagishi, K. MacDonald et al., “Superseded-cstr\nvctk corpus: English multi-speaker corpus for cstr voice cloning\ntoolkit,” 2016.\n[12] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Ben-\ngio, “Attention-based models for speech recognition,” in Ad-\nvances in neural information processing systems, 2015, pp. 577–\n585.\n[13] Y . Jia, Y . Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen,\nR. Pang, I. L. Moreno, Y . Wu et al. , “Transfer learning from\nspeaker veriﬁcation to multispeaker text-to-speech synthesis,” in\nAdvances in neural information processing systems , 2018, pp.\n4480–4490.\n[14] R. Skerry-Ryan, E. Battenberg, Y . Xiao, Y . Wang, D. Stanton,\nJ. Shor, R. J. Weiss, R. Clark, and R. A. Saurous, “Towards\nend-to-end prosody transfer for expressive speech synthesis with\ntacotron,”arXiv preprint arXiv:1803.09047, 2018.\n[15] M. He, Y . Deng, and L. He, “Robust sequence-to-sequence acous-\ntic modeling with stepwise monotonic attention for neural tts,”\narXiv preprint arXiv:1906.00672, 2019.\n[16] J.-X. Zhang, Z.-H. Ling, and L.-R. Dai, “Forward attention in\nsequence-to-sequence acoustic modeling for speech synthesis,” in\n2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2018, pp. 4789–4793.\n[17] H. Tachibana, K. Uenoyama, and S. Aihara, “Efﬁciently train-\nable text-to-speech system based on deep convolutional networks\nwith guided attention,” in2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE, 2018,\npp. 4784–4788.\n[18] Y . Yasuda, X. Wang, and J. Yamagishi, “Initial investigation of\nan encoder-decoder end-to-end tts framework using marginal-\nization of monotonic hard latent alignments,” arXiv preprint\narXiv:1908.11535, 2019.\n[19] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450, 2016.\n[20] H. Sun, X. Tan, J.-W. Gan, H. Liu, S. Zhao, T. Qin, and T.-Y .\nLiu, “Token-level ensemble distillation for grapheme-to-phoneme\nconversion,”arXiv preprint arXiv:1904.03446, 2019.\n[21] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,\n“Wavenet: A generative model for raw audio,” arXiv preprint\narXiv:1609.03499, 2016."
}