{
  "title": "RWKV: Reinventing RNNs for the Transformer Era",
  "url": "https://openalex.org/W4389524555",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1969033575",
      "name": "Bo Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2784477135",
      "name": "Eric Alcaide",
      "affiliations": [
        "Universitat de Barcelona"
      ]
    },
    {
      "id": "https://openalex.org/A2983461712",
      "name": "Quentin Anthony",
      "affiliations": [
        "State Library of Ohio"
      ]
    },
    {
      "id": "https://openalex.org/A3120639194",
      "name": "Alon Albalak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2923083835",
      "name": "Samuel Arcadinho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2936985360",
      "name": "Stella Biderman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2327230166",
      "name": "Huanqi Cao",
      "affiliations": [
        "King University",
        "Peking University",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1972614170",
      "name": "Xin Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001791415",
      "name": "Michael Chung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003830579",
      "name": "Leon Derczynski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799145318",
      "name": "Xingjian Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2786367246",
      "name": "Matteo Grella",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Kranthi Gv",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5104211469",
      "name": "Xuzheng He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2318038114",
      "name": "Haowen Hou",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A280094779",
      "name": "Przemysław Kazienko",
      "affiliations": [
        "Wrocław University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2088473993",
      "name": "Jan Kocon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2333623876",
      "name": "Jiaming Kong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4322789011",
      "name": "Bartłomiej Koptyra",
      "affiliations": [
        "Wrocław University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3013282969",
      "name": "Hayden Lau",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2642144699",
      "name": "Jiaju Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4321874465",
      "name": "Krishna Sri Ipsit Mantri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5092005368",
      "name": "Ferdinand Mom",
      "affiliations": [
        "Criteo (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2071071889",
      "name": "Atsushi Saito",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129452642",
      "name": "Guangyu Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2634255921",
      "name": "Xiangru Tang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2965189851",
      "name": "Johan Wind",
      "affiliations": [
        "Unge aksjonærer"
      ]
    },
    {
      "id": "https://openalex.org/A4296500086",
      "name": "StanisŁaw Woźniak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116044290",
      "name": "Zhenyuan Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113789830",
      "name": "Qinghua Zhou",
      "affiliations": [
        "State Library of Ohio"
      ]
    },
    {
      "id": "https://openalex.org/A2096842168",
      "name": "Jian Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2244461630",
      "name": "Rui Jie Zhu",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288076474",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W4366974303",
    "https://openalex.org/W4221145950",
    "https://openalex.org/W3169012807",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2540646130",
    "https://openalex.org/W4324312887",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2754232744",
    "https://openalex.org/W4385574005",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2963174729",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4226001818",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4296932804",
    "https://openalex.org/W2962785940",
    "https://openalex.org/W4321593177",
    "https://openalex.org/W2892035503",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W4379087282",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W3098173943",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4378505278",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W4312062661",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2985092403",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3123221884",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2949818215",
    "https://openalex.org/W4285704109",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385573709",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W4361193485"
  ],
  "abstract": "Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, Rui-Jie Zhu. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14048–14077\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRWKV: Reinventing RNNs for the Transformer Era\nBo Peng1,2∗ Eric Alcaide2,3,4∗ Quentin Anthony2,5∗\nAlon Albalak2,6 Samuel Arcadinho2,7 Stella Biderman2,8 Huanqi Cao9 Xin Cheng10\nMichael Chung11 Xingjian Du1 Matteo Grella12 Kranthi Kiran GV2,13 Xuzheng He2\nHaowen Hou14 Jiaju Lin1 Przemysław Kazienko15 Jan Koco´n15 Jiaming Kong16\nBartłomiej Koptyra15 Hayden Lau2 Krishna Sri Ipsit Mantri17 Ferdinand Mom18,19\nAtsushi Saito2,20 Guangyu Song21 Xiangru Tang22 Bolun Wang23 Johan S. Wind24\nStanisław Wo´ zniak15 Ruichong Zhang9 Zhenyuan Zhang2 Qihang Zhao25,26\nPeng Zhou23 Qinghua Zhou5 Jian Zhu27 Rui-Jie Zhu28,29\n1Generative AI Commons 2EleutherAI 3U. of Barcelona 4Charm Therapeutics 5Ohio State U. 6U. of C., Santa Barbara\n7Zendesk 8Booz Allen Hamilton 9Tsinghua University 10Peking University 11Storyteller.io 12Crisis24 13New York U.\n14National U. of Singapore 15Wroclaw U. of Science and Technology 16Databaker Technology 17Purdue U. 18Criteo AI Lab\n19Epita 20Nextremer 21Moves 22Yale U. 23RuoxinTech 24U. of Oslo 25U. of Science and Technology of China\n26Kuaishou Technology 27U. of British Columbia 28U. of C., Santa Cruz 29U. of Electronic Science and Technology of China\nAbstract\nTransformers have revolutionized almost all\nnatural language processing (NLP) tasks but\nsuffer from memory and computational com-\nplexity that scales quadratically with sequence\nlength. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and\ncomputational requirements but struggle to\nmatch the same performance as Transformers\ndue to limitations in parallelization and scala-\nbility. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that\ncombines the efficient parallelizable training\nof transformers with the efficient inference of\nRNNs.\nOur approach leverages a linear attention mech-\nanism and allows us to formulate the model as\neither a Transformer or an RNN, thus paralleliz-\ning computations during training and maintains\nconstant computational and memory complex-\nity during inference. We scale our models as\nlarge as 14 billion parameters, by far the largest\ndense RNN ever trained, and find RWKV per-\nforms on par with similarly sized Transformers,\nsuggesting future work can leverage this archi-\ntecture to create more efficient models. This\nwork presents a significant step towards rec-\nonciling trade-offs between computational ef-\nficiency and model performance in sequence\nprocessing tasks. 1\n1 Introduction\nDeep learning has greatly advanced artificial intelli-\ngence, impacting a range of scientific and industrial\nuses. These often involve complex sequential data\n∗ Equal first authorship. Others listed alphabetically.\n1Code at: https://github.com/BlinkDL/RWKV-LM\nprocessing tasks such as natural language under-\nstanding, conversational AI, time-series analysis,\nand indirectly sequential formats like images and\ngraphs (Brown et al., 2020; Ismail Fawaz et al.,\n2019; Wu et al., 2020; Albalak et al., 2022). Pre-\ndominant among these techniques include RNNs\nand Transformers (Vaswani et al., 2017), each with\nspecific benefits and drawbacks. RNNs require less\nmemory, particularly for handling long sequences.\nHowever, they suffer from the vanishing gradient\nproblem and non-parallelizability in the time di-\nmension during training, limiting their scalability\n(Hochreiter, 1998; Le and Zuidema, 2016).\n102 103\nCompute (exaFLOP)\n40\n45\n50\n55Accuracy\nBLOOM\nPythia\nOPT\nRWKV\nFigure 1: Average performance of RWKV models com-\npared to transformers across twelve NLP tasks. For\nfurther details, see section 5.\nTransformers emerged as a powerful alternative,\nadept at managing local and long-range dependen-\ncies and supporting parallelized training (Tay et al.,\n2022). Models such as GPT-3 (Brown et al., 2020),\nChatGPT (OpenAI, 2022; Koco ´n et al., 2023),\n14048\nModel Time Space\nTransformer O(T2d) O(T2 + T d)\nReformer O(T log T d) O(T log T + T d)\nPerformer O(T d2 log d) O(T dlog d + d2 log d)\nLinear Transformers O(T d2) O(T d+ d2)\nAFT-full O(T2d) O(T d)\nAFT-local O(T sd) O(T d)\nMEGA O(cT d) O(cd)\nRWKV (ours) O(Td) O(d)\nTable 1: Inference complexity comparison with different\nTransformers. Here T denotes the sequence length,\nd the feature dimension, c is MEGA’s chunk size of\nquadratic attention, and sis the size of a local window\nfor AFT.\nLLaMA (Touvron et al., 2023), and Chinchilla\n(Hoffmann et al., 2022) showcase the potential of\nTransformers in NLP. However, the self-attention\nmechanism’s quadratic complexity makes it compu-\ntationally and memory intensive for tasks involving\nlong sequences and constrained resources. This\nhas stimulated research to enhance Transformers’\nscalability, sometimes sacrificing some of their ef-\nfectiveness (Wang et al., 2020; Zaheer et al., 2020;\nDao et al., 2022a).\nTo tackle these challenges, we introduce the Re-\nceptance Weighted Key Value ( RWKV) model,\ncombining the strengths of RNNs and Transform-\ners while circumventing key drawbacks. RWKV\nalleviates memory bottleneck and quadratic scaling\nassociated with Transformers (Katharopoulos et al.,\n2020) with efficient linear scaling, while maintain-\ning the expressive properties of the Transformer,\nsuch as parallelized training and robust scalability.\nRWKV reformulates the attention mechanism with\na variant of linear attention, replacing traditional\ndot-product token interaction with more effective\nchannel-directed attention. This implementation,\nwithout approximation, offers the lowest computa-\ntional and memory complexity; see Table 1.\nThe motivation behind RWKV is to balance com-\nputational efficiency with expressive capacity in\nneural networks. It offers a solution for handling\nlarge-scale models with billions of parameters, ex-\nhibiting competitive performance at a reduced com-\nputational cost. Experiments suggest RWKV ad-\ndresses scaling and deployment challenges in AI,\nespecially for sequential data processing, pointing\ntowards more sustainable and efficient AI models.\nOur contributions in this paper are as follows:\n• The introduction of RWKV , a novel architec-\nture combining RNNs and Transformer advan-\ntages while mitigating their limitations.\n• Detailed experiments demonstrating RWKV’s\nperformance and efficiency on benchmark\ndatasets for large-scale models.\n• The release of pretrained models, from 169\nmillion to 14 billion parameters, trained on\nthe Pile (Gao et al., 2020; Biderman et al.,\n2022).2\n2 Background\nHere we briefly review the fundamentals of RNNs\nand Transformers.\n2.1 Recurrent Neural Networks (RNNs)\nPopular RNN architectures such as LSTM (Hochre-\niter and Schmidhuber, 1997) and GRU (Chung\net al., 2014) are characterized by the following for-\nmulation (shown for LSTM, others can be reasoned\nsimilarly):\nft = σg(Wfxt + Ufht−1 + bf), (1)\nit = σg(Wixt + Uiht−1 + bi), (2)\not = σg(Woxt + Uoht−1 + bo), (3)\n˜ct = σc(Wcxt + Ucht−1 + bc), (4)\nct = ft ⊙ct−1 + it ⊙˜ct, (5)\nht = ot ⊙σh(ct). (6)\nAlthough RNNs can be factored into two linear\nblocks (W and U) and an RNN-specific block (1)–\n(6), as noted by Bradbury et al. (2017), the data de-\npendency relying on previous time steps prohibits\nparallelizing these typical RNNs.\n2.2 Transformers and AFT\nIntroduced by Vaswani et al. (2017), Transformers\nare a class of neural networks that have become\nthe dominant architecture for several NLP tasks.\nInstead of operating on sequences step-by-step like\nRNNs, Transformers rely on attention mechanisms\nto capture relationships between all input and all\noutput tokens:\nAttn(Q,K,V ) = softmax(QK⊤)V, (7)\nwhere the multi-headness and scaling factor 1√dk\nis\nomitted for convenience. The core QK⊤multipli-\ncation is an ensemble of pairwise attention scores\n2https://huggingface.co/RWKV\n14049\nbetween each token in a sequence, which can be\ndecomposed as vector operations:\nAttn(Q,K,V )t =\n∑T\ni=1 eq⊤\nt ki ⊙vi\n∑T\ni=1 eq⊤\nt ki\n. (8)\nAFT (Zhai et al., 2021), alternately formulates\nAttn+(W,K,V )t =\n∑t\ni=1 ewt,i+ki ⊙vi\n∑t\ni=1 ewt,i+ki\n, (9)\nwhere {wt,i}∈ RT×T is the learned pair-wise po-\nsition biases, and each wt,i is a scalar.\nInspired by AFT, RWKV takes a similar ap-\nproach. However, for simplicity, it modifies the\ninteraction weights so that it can be transformed\ninto an RNN. Each wt,i in RWKV is a channel-\nwise time decay vector multiplied by the relative\nposition and traced backward from current time as\nit decays:\nwt,i = −(t−i)w, (10)\nwhere w ∈(R≥0)d, with d the number of chan-\nnels. We require w to be non-negative to ensure\nthat ewt,i ≤1 and the per-channel weights decay\nbackwards in time.\n3 RWKV\nThe RWKV model architecture is defined by four\nfundamental elements that are intrinsic to the time-\nmixing and channel-mixing blocks:\n• R: The Receptance vector acts as the receiver\nof past information.\n• W: The Weight signifies the positional\nweight decay vector, a trainable parameter\nwithin the model.\n• K: The Key vector performs a role analogous\nto Kin traditional attention mechanisms.\n• V: The Value vector functions similarly to V\nin conventional attention processes.\nThese core elements interact multiplicatively at\neach timestep, as depicted in Figure 2.\n3.1 Architecture\nThe RWKV model is composed of stacked residual\nblocks. Each block consists of a time-mixing and\na channel-mixing sub-block, embodying recurrent\nstructures to leverage past information.\nThis model uses a unique attention-like score\nupdate process, which includes a time-dependent\nTime \nMixing\nLayerNorm\nLayerNorm\n?\nChannel \nMixing\n?\n?'\nOut\nWKV\nV\nK\nR\n?\nLayerNorm\nR'\nK'\nV'\nRWKV-LM \nHead\nInput \nEmbedding\nLayerNorm\nLayerNorm\n Output Probabilities\nLayerNorm\nLayerNorm\nChannel \nMixing\nTime \nMixing\nOut\nSoftmax\nFigure 2: Elements within an RWKV block (left) and\nthe complete RWKV residual block, equipped with a\nfinal head for language modeling (right).\nsoftmax operation improving numerical stability\nand mitigating vanishing gradients (for rigorous\nproof, see Appendix H). It ensures that the gra-\ndient is propagated along the most relevant path.\nAdditionally, layer normalization (Ba et al., 2016)\nincorporated within the architecture aids in stabi-\nlizing the gradients, effectively addressing both\nvanishing and exploding gradient issues. These\ndesign elements not only enhance the training dy-\nnamics of deep neural networks but also facilitate\nthe stacking of multiple layers, leading to superior\nperformance over conventional RNN models by\ncapturing complex patterns across different levels\nof abstraction (see also Appendix I).\n3.1.1 Token Shift\nIn this architecture, all linear projection vectors\n(R, K, V in time-mixing, and R′, K′in channel-\nmixing) involved in computations are produced by\nlinear interpolation between current and previous\ntimestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are\nlinear projections of linear combinations of the\ncurrent and previous inputs of the block:\nrt = Wr ·(µr ⊙xt + (1−µr) ⊙xt−1), (11)\nkt = Wk ·(µk ⊙xt + (1−µk) ⊙xt−1), (12)\nvt = Wv ·(µv ⊙xt + (1−µv) ⊙xt−1), (13)\n14050\nToken\nshift Token\nshift\nStates States\nLayer Norm\nMy\nLayer Norm\nname\nLayer Norm\nTime Mix\nLM Head\nLayer Norm\nis\nLayer Norm\nBob\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nLayer Norm\nname\nLayer Norm\nis\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nToken\nshiftToken\nshift\nChannel Mix\nFigure 3: RWKV architecture for language modeling.\nas are the channel-mixing inputs:\nr′\nt = W′\nr ·(µ′\nr ⊙xt + (1−µ′\nr) ⊙xt−1), (14)\nk′\nt = W′\nk ·(µ′\nk ⊙xt + (1−µ′\nk) ⊙xt−1). (15)\nThe token shift is implemented as a simple off-\nset in the temporal dimension at each block us-\ning the PyTorch (Paszke et al., 2019) library as\nnn.ZeroPad2d((0,0,1,-1)).\n3.1.2 WKV Operator\nThe computation of the WKV operator in our\nmodel parallels the method used in Attention Free\nTransformer (AFT) (Zhai et al., 2021). However,\nunlike AFT where W is a pairwise matrix, our\nmodel treats W as a channel-wise vector that is\nmodified by relative position. In our model, this\nrecurrent behavior is defined by the time-dependent\nupdate of the WKV vectors, formalized in the fol-\nlowing equation:\nwkvt =\n∑t−1\ni=1 e−(t−1−i)w+ki ⊙vi + eu+kt ⊙vt\n∑t−1\ni=1 e−(t−1−i)w+ki + eu+kt\n.\n(16)\nTo circumvent any potential degradation ofW,\nwe introduce a vector U that separately attends to\nthe current token. More information about this can\nbe found in Appendix I.\n3.1.3 Output Gating\nOutput gating is implemented in both time-mixing\nand channel-mixing blocks using the sigmoid of\nthe receptance, σ(r). The output vector ot post the\nWKV operator is given by:\not = Wo ·(σ(rt) ⊙wkvt). (17)\nIn the channel-mixing block, a similar operation\nis performed:\no′\nt = σ(r′\nt) ⊙(W′\nv ·max(k′\nt,0)2), (18)\nwhere we adopt the squared ReLU activation func-\ntion (So et al., 2021).\n3.2 Transformer-like Training\nRWKV can be efficiently parallelized using a tech-\nnique called time-parallel mode, reminiscent of\nTransformers. The time complexity of processing\na batch of sequences in a single layer is O(BTd2),\nprimarily consisting of matrix multiplications Wλ,\nwhere λ∈{r,k,v,o }(assuming Bsequences, T\nmaximum tokens, and dchannels). In contrast, up-\ndating attention scores wkvt involves a serial scan\n(see Appendix D for more detail) and has complex-\nity O(BTd).\nThe matrix multiplications can be parallelized\nsimilarly to Wλ, where λ∈{Q,K,V,O }in con-\nventional Transformers. The element-wise WKV\ncomputation is time-dependent but can be readily\nparallelized along the other two dimensions (Lei\net al., 2018)3.\n3.3 RNN-like Inference\nRecurrent networks commonly utilize the output\nat state t as input at state t+ 1. This usage is\nalso observed in the autoregressive decoding infer-\nence of language models, where each token must\nbe computed before being passed to the next step.\nRWKV takes advantage of this RNN-like structure,\nknown as time-sequential mode. In this context,\nRWKV can be conveniently formulated recursively\nfor decoding during inference, as demonstrated in\nAppendix D.\n3.4 Additional Optimizations\nCustom Kernels To address inefficiencies in the\nWKV computation arising from the sequential na-\nture of the task when using standard deep learning\nframeworks, we have developed a custom CUDA\n3For extremely long sequences, more sophisticated meth-\nods such as Martin and Cundy (2017) that parallelize over\nsequence length could be used.\n14051\nkernel. This kernel enables the execution of a sin-\ngle compute kernel on training accelerators, while\nall other parts of the model, such as matrix multi-\nplications and point-wise operations, are already\ninherently parallelizable and efficient.\nSmall Init Embedding During the initial stage\nof training a transformer model (Vaswani et al.,\n2017), we observe that the embedding matrix un-\ndergoes slow changes, presenting a challenge for\nthe model to move away from its initial noisy em-\nbedding state. To address this issue, we propose\nan approach that involves initializing the embed-\nding matrix with small values and subsequently\napplying an additional LayerNorm operation. This\naccelerates and stabilizes the training process, al-\nlowing for the training of deep architectures with\npost-LN components. The effectiveness of this\napproach is demonstrated in Figure 9, illustrating\nimproved convergence by enabling the model to\nquickly transition away from the initially small em-\nbedding. This is achieved through small changes\noccurring in a single step, which subsequently lead\nto substantial alterations in directions and further\nnotable changes after the LayerNorm operation.\nCustom Initialization Building on principles\nfrom previous works (He et al., 2016; Jumper et al.,\n2021), we adopt an initialization strategy where\nparameters are set to values resembling an identity\nmapping while breaking symmetry to establish a\nclear information flow. The majority of weights are\ninitialized to zero, and linear layers do not employ\nbiases. Detailed formulas are given in Appendix E.\nWe observe that the choice of initialization plays a\ncrucial role in both the speed and quality of conver-\ngence (refer to Appendix F for further details).\n3.5 Implementation\nRWKV is implemented using the PyTorch Deep\nLearning Library (Paszke et al., 2019). We inte-\ngrate additional optimization strategies inspired by\nDeepSpeed (Rasley et al., 2020) into the system,\nimproving its efficiency and scalability.\nThe model begins with an embedding layer, as\ndetailed in Section 3.4. Following this are sev-\neral identical residual blocks arranged sequentially.\nThese are depicted in Figures 2 and 3 and adheres\nto the principles outlined in Section 3.1.1. After\nthe last block, a simple output projection head, con-\nsisting of a LayerNorm (Ba et al., 2016) and a\nlinear projection, is employed for logits generation\nfor next-token prediction and computation of the\ncross-entropy loss during training.\n4 Trained Models and Computing Costs\nTo demonstrate the scalability of RWKV , we train\nsix models ranging from 169 million to 14 billion\nparameters as shown in Table 2. All models are\ntrained for one epoch (330 billion tokens) on the\nPile (Gao et al., 2020; Biderman et al., 2022).\nName Layers Model Dimension Parameters FLOP per token\n169 M12 768 1.693×108 2.613×108\n430 M24 1024 4.304×108 7.573×108\n1.5 B 24 2048 1.515×109 2.823×109\n3 B 32 2560 2.985×109 5.710×109\n7 B 32 4096 7.393×109 1.437×1010\n14 B 40 5120 1.415×1010 2.778×1010\nTable 2: RWKV model architectures and FLOP counts.\nFurther details of these hyperparameters are elaborated\nupon in Appendix G.\nThe number of parameters for each model is\ncomputed using the formula: # parameters =\n2VD + 13D2L+ D(11L+ 4)where V = 50277\nis the vocabulary size, Drepresents the Model Di-\nmension and Lcorresponds to the number of lay-\ners. FLOPs is for a forward pass for one token. It\nwas calculated as 2(2VD + 13D2L), which is the\ntwice (add and multiply) the number of parameters\nin linear layers. The backwards pass FLOPs can\nbe approximated as twice that of the forward pass,\ngiving a total of 6(2VD + 13D2L) FLOP per to-\nken. Notably, this matches the standard formula\nfor FLOP calculations in transformers Kaplan et al.\n(2020): FLOP = 6·[# tokens] ·[# parameters].\n4.1 Additional Training Details\nFor training, we use the standard Adam optimizer\nwithout weight decay, use bfloat16 precision, and\ntrain with a context length of 1024 tokens. Fur-\nther details on hyperparameters are in Appendix G.\nDiverting from standard practice for transformers,\nwe apply exponential decay to our learning rate.\nWe also incorporate the auxiliary loss introduced\nby PaLM (Chowdhery et al., 2022), supplement-\ning the standard cross-entropy loss function. This\nauxiliary loss encourages the softmax normalizer\nto approximate zero closely. As for the learning\nrate schedule, it remains constant for the initial\niterations, and subsequently decays exponentially.\n14052\n4.2 Scaling Laws\nScaling laws (Kaplan et al., 2020; Henighan et al.,\n2020; Hoffmann et al., 2022; Muennighoff et al.,\n2023) in language models refer to the mathemati-\ncal relationships that describe how the performance\nof a language model changes with respect to var-\nious factors. These factors can include the model\nsize (N), dataset size ( D), or the optimally allo-\ncated compute budget ( Cmin). Scaling laws are\nimportant for two primary reasons: they allow us\nto make predictions and plans regarding the costs\nand performance of large models before they are\ntrained via interpolation and extrapolation (Black\net al., 2022; Le Scao et al., 2022) and the contexts\nin which they fail provides rich feedback on im-\nportant areas for future research (Wei et al., 2022a;\nBiderman et al., 2023a).\nPrevious work on scaling laws for RNNs has\nclaimed that LSTMs do not strictly follow the same\nlog-log linear scaling that transformers do (Kaplan\net al., 2020). We train 45 RWKV models for a\nvariety of pairs (dataset, parameters) and find that\nRWKV does follow the same general form of the\nscaling law that is well established for transformers.\nFigure 4 shows our results for loss as a function of\ncompute, with the linear fit to the Pareto optimal\npoints holding an r2 value of 0.994. Even when\nwe extrapolate our curve an additional order of\nmagnitude (blue), we find an extremely good fit\nwith an r2 of 0.875.\n10 1\n 100 101 102 103\nCompute (exaFLOP)\n2 × 100\n3 × 100\nLoss\nNon-Optimal\nOptimal\nT est\nTrend Line\nFigure 4: Scaling laws curves for RWKV models\n5 Evaluations\nHaving demonstrated the scalability of RWKV\nmodels in the previous section, we now turn our\nattention to their competitiveness with traditional\ntransformers. We focus on two questions:\nCompetitiveness Is RWKV competitive against\nquadratic transformer architectures with the same\namount of compute?\nLong Context Does increasing the context\nlength of RWKV yield better language modeling\nloss when RWKV models are trained for context\nlengths that most open-sourced quadratic trans-\nformers cannot efficiently process?\n5.1 NLP Evaluations\nTo demonstrate that RWKV is competitive with\ntraditional transformers at NLP tasks, we compare\nwith similarly sized models trained for a similar\nnumber of tokens (Pythia (Biderman et al., 2023b),\nOPT (Zhang et al., 2022) and BLOOM (Scao et al.,\n2022)). All RWKV models were trained for one\nepoch on the Pile (330B tokens), which is close but\nnot identical to the amount of tokens the Pythia,\nOPT, and BLOOM models were trained for. Con-\nsequently, we compare our models on a FLOP-\nmatched basis. We avoid comparing with model\ntrained in the Chinchilla-optimal regime (Hoff-\nmann et al., 2022) or the overtrained regime (Tou-\nvron et al., 2023) to ensure the most equitable com-\nparison.\nWe report results on ARC (both Easy and Chal-\nlenge) (Clark et al., 2018), BoolQ (Clark et al.,\n2019), COPA (Roemmele et al., 2018), HeadQA\n(Vilares and Gómez-Rodríguez, 2019), HellaSwag\n(Zellers et al., 2019), LAMBADA (Paperno et al.,\n2016), OpenBookQA (Mihaylov et al., 2018),\nPIQA (Bisk et al., 2020), ReCoRD (Zhang et al.,\n2018), SciQ (Johannes Welbl Nelson F. Liu, 2017),\nand Winogrande (Zellers et al., 2020). Figure 1\nshows the average results across all benchmarks.\nSome individual benchmarks are shown in Fig 5,\nwith the rest in Appendix J.\nAdditionally, we carried out comparative studies\non RWKV and ChatGPT / GPT-4, see Appendix\nL. They revealed that RWKV is very sensitive to\nprompt engineering. When the prompts were ad-\njusted (re-ordered) from the ones used for GPT to\nmore suitable for RWKV , the performance (F1) in-\ncreased even from 44.2% to 74.8%. For sarcasm\ndetection, RWKV outperformed ChatGPT, but was\nstill slightly worse than the SOTA solution.\n5.2 Extended Context Finetuning\nUnlike transformers, RNNs do not have a pre-\ndefined sequences length when they are created.\nHowever in order to efficient make use of compute\n14053\n102 103\nCompute (exaFLOP)\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5Accuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n102 103\nCompute (exaFLOP)\n30\n35\n40\n45\n50Accuracy\n (b) HellaSwag\n102 103\nCompute (exaFLOP)\n20\n30\n40\n50\n60\n70Accuracy\nBLOOM\nPythia\nOPT\nRWKV (c) LAMBADA (OpenAI)\n102 103\nCompute (exaFLOP)\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0Accuracy\n(d) OpenBookQA\n102 103\nCompute (exaFLOP)\n50\n55\n60\n65\n70\n75\n80\n85\n90Accuracy\nBLOOM\nPythia\nOPT\nRWKV (e) ReCoRD\n102 103\nCompute (exaFLOP)\n50\n52\n54\n56\n58\n60\n62\n64\n66Accuracy\n (f) Winogrande\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional\nplots can be found in Appendix J.\nwe nevertheless need to preprocess the training data\ninto contexts of the same length. We find that we\nare able to teach the model how to efficiently handle\nsubstantially larger batch sizes by finetuning with\nprogressively increasing sequence length. Specif-\nically, we first double the sequence length from\n1024 to 2048 and finetune for 10B tokens from the\noriginal pretraining corpus, then we double again\nto 4096 for 100B tokens from the same corpus, and\nfinally double to 8192 tokens for another 100B to-\nkens from the same corpus. In Fig. 6 we show that\nincreasing context length leads to lower test loss\non the Pile, an indication that RWKV can make\neffective use of long contextual information.\n21 23 25 27 29 211\nContext Length\n21\n22\nPile test loss\n7B 8k\n14B 8k\nFigure 6: RWKV shows decreasing mean test loss as a\nfunction of context length on the Pile (Gao et al., 2020)\n5.3 Long Context Benchmarks\nAdditionally, we evaluate our model’s ability to\nhandle very long sequences by comparing to state-\nof-the-art long sequence models on the Long-\nRange Arena (LRA) benchmark (Tay et al., 2021).\nLRA is designed to assess the performance of mod-\nels in handling lengthy context situations. It in-\ncludes a collection of tasks with sequences rang-\ning from 1,000 to 16,000 tokens, covering various\ntypes of data like text, natural language, synthetic\nimages, and mathematical expressions. We apply\nRWKV on the LRA benchmark and the results are\nin Appendix J.2. The results show that RWKV per-\nforms second only to the S4 model in five datasets.\n6 Inference Experiments\nWe benchmark inference requirements according to\nsize and family. Specifically, we evaluate text gen-\neration speed and memory requirements on typical\ncompute platforms including CPU (x86) and GPU\n(NVIDIA A100 80 GB). For all of our inference\nexperiments we use float32 precision and the Hug-\ngingFace Transformers (Wolf et al., 2020). We in-\nclude all model parameters in the parameter count,\nincluding both embedding and non-embedding lay-\ners. Performance under different quantization se-\ntups is left to further work. See Appendix K for\n14054\nmore results.\nFigure 7: Cumulative time on text generation for LLMs.\nUnlike transformers, RWKV exhibits linear scaling.\n7 Future Work\nThere are several promising directions for future\nwork on the RWKV architecture. Work can be\ndone to increase model expressivity by enhancing\nthe time-decay formulations and exploring initial\nmodel states while maintaining efficiency. The\nRWKV computational efficiency can be further im-\nproved by applying a parallel scan in the wkvt step\nto reduce the computational cost to O(Blog(T)d).\nThe mechanisms used in RWKV can be applied\nto encoder-decoder architectures, potentially re-\nplacing the cross-attention mechanism. This could\nbe applicable in seq2seq or multimodal settings,\nthereby enhancing efficiency during both training\nand inference.\nRWKV’s state (or context) can be leveraged for\ninterpretability, predictability in sequence data, and\nsafety. Manipulating the hidden state could also\nguide behavior and allow greater customizability\nthrough prompt tuning.\n8 Conclusions\nWe introduced RWKV , a new approach to RNN\nmodels exploiting the potential of time-based mix-\ning components. RWKV introduces several key\nstrategies that allow it to capture locality and long-\nrange dependencies while addressing limitations of\ncurrent architectures by: (1) replacing the quadratic\nQK attention with a scalar formulation at linear\ncost, (2) reformulating recurrence and sequential\ninductive biases to enable efficient training paral-\nlelization and efficient inference, and (3) enhancing\ntraining dynamics using custom initializations.\nWe benchmark the proposed architecture in a\nwide variety of NLP tasks and show comparable\nperformance to SoTA with reduced cost. Further\nexperiments on expressivity, interpretability, and\nscaling showcase the model capabilities and draw\nparallels in behavior between RWKV and other\nLLMs.\nRWKV opens a new route for scalable and ef-\nficient architectures to model complex relation-\nships in sequential data. While many alternatives\nto Transformers have been proposed with similar\nclaims, ours is the first to back up those claims with\npretrained models with tens of billions of parame-\nters.\n9 Limitations\nWhile our proposed RWKV model has demon-\nstrated promising results regarding training and\nmemory efficiency during inference, some limita-\ntions should be acknowledged and addressed in\nfuture work.\nFirst, the linear attention of RWKV leads to sig-\nnificant efficiency gains but still, it may also limit\nthe model’s performance on tasks that require re-\ncalling minutiae information over very long con-\ntexts. This is due to the funneling of informa-\ntion through a single vector representation over\nmany time steps, compared with the full informa-\ntion maintained by the quadratic attention of stan-\ndard Transformers. In other words, the model’s\nrecurrent architecture inherently limits its ability to\n“look back” at previous tokens, as opposed to tra-\nditional self-attention mechanisms. While learned\ntime decay helps prevent the loss of information,\nit is mechanistically limited compared to full self-\nattention.\nAnother limitation of this work is the increased\nimportance of prompt engineering in comparison to\nstandard Transformer models. The linear attention\nmechanism used in RWKV limits the information\nfrom the prompt that will be carried over to the\nmodel’s continuation. As a result, carefully de-\nsigned prompts may be even more crucial for the\nmodel to perform well on tasks.\nThe above RWKV property was confirmed by\nstudies on prompt engineering presented in Ap-\npendix L. By changing the order of the information\npieces, we were even able to almost double the\nRWKV performance for some tasks.\n10 Ethics Statement\nIn this paper, we present a novel architecture for se-\nquential data processing and prove its effectiveness\nby building a series of LLMs trained on publicly re-\n14055\nleased pretraining data (Gao et al., 2020; Biderman\net al., 2022) and later fine-tuned on publicly avail-\nable instructions (Taori et al., 2023; Chaudhary,\n2023; Cheung, 2023; Anand et al., 2023; Anony-\nmous, 2023; Yang, 2023; Ji et al., 2023a,b).\nAs a novel architecture for sequential data,\nRWKV has the potential to improve sequence-\nbased models across different applications ranging\nfrom natural language processing to biomedical\ndata processing or climate modelling. Since the\ntraining code is released open source, RWKV con-\ntributes to the democratization of AI, levels the\nplaying field, and empowers members of the Open\nSource community to inspect, study, and finetune\nRWKV in particular tasks. Moreover, it contributes\nto advancing the understanding of LLMs capabili-\nties and limitations. A significant amount of work\nhas been devoted to increasing the efficiency of\nRWKV training so as to minimize its cost and pro-\nmote accessibility.\nAs LLMs trained on public data, RWKV’s lower\ninference cost compared to Transformer alterna-\ntives makes it more suitable for deployment in con-\nsumer and edge hardware, which is a step towards\nthe democratization and distribution of LLMs to\nthe general public, creating better privacy and own-\nership incentives. It also lowers the resource bar-\nrier to Chat assistants and text generation for small\nand/or underrepresented communities. PreTrained\nmodel weights for different sizes ranging from 0.1B\nto 14B parameters trained on multiple languages\nare released to increase ease of adoption and allow\nfor the study of emergent phenomena.\nOn the other hand, with lower resource barri-\ners, the spreading of AI-generated text might be-\ncome more prevalent. Current RWKV LLMs may\nexhibit and/or reproduce biases and potentially\nharmful content present in the data used for train-\ning. Nonetheless, mitigation and finetuning strate-\ngies discussed for other, large Transformer models\nshould be applicable to RWKV as well.\nAcknowledgements\nWe thank StabilityAI for the compute used to train\nour models and for technical support in develop-\nment of RWKV . We also thank the members of the\nRWKV and EleutherAI Discord servers for their\nhelp and work on further extending the applicabil-\nity of RWKV to different domains.\nReferences\nMohammad Mahmudul Alam, Edward Raff, Stella Bi-\nderman, Tim Oates, and James Holt. 2023. Recasting\nself-attention with holographic reduced representa-\ntions. arXiv preprint arXiv:2305.19534.\nAlon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor\nPryor, Luke Yoffe, Deepak Ramachandran, Lise\nGetoor, Jay Pujara, and William Yang Wang. 2022.\nFETA: A benchmark for few-sample task transfer\nin open-domain dialogue. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10936–10953, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo. https:\n//github.com/nomic-ai/gpt4all.\nAnonymous. 2023. Sharegpt_vicuna_unfiltered.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650, Online. Association for Computational\nLinguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nStella Biderman, Kieran Bicheno, and Leo Gao.\n2022. Datasheet for the pile. arXiv preprint\narXiv:2201.07311.\nStella Biderman, USVSN Sai Prashanth, Lintang\nSutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raf. 2023a. Emer-\ngent and predictable memorization in large language\nmodels. arXiv preprint arXiv:2304.11158.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023b. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nURL: https://doi. org/10.5281/zenodo, 5297715.\n14056\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregres-\nsive language model. In Proceedings of BigScience\nEpisode\\# 5–Workshop on Challenges & Perspec-\ntives in Creating Large Language Models, pages 95–\n136.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In ICLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.\n2023. Scaling transformer to 1m tokens and beyond\nwith rmt.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev.\n2022. Recurrent memory transformer. Advances\nin Neural Information Processing Systems, 35:11079–\n11091.\nSahil Chaudhary. 2023. Code alpaca: An instruction-\nfollowing llama model for code generation. https:\n//github.com/sahil280114/codealpaca.\nJoseph Cheung. 2023. Guanacodataset.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2020. Rethinking attention with per-\nformers.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. In NIPS 2014 Deep Learning and Representa-\ntion Learning Workshop.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. In\narXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-\ncob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to\nsolve math word problems. In arXiv, volume\nabs/2110.14168.\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and\nChristopher Re. 2022a. Flashattention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nTri Dao, Daniel Y Fu, Khaled K Saab, Armin W\nThomas, Atri Rudra, and Christopher Ré. 2022b.\nHungry hungry hippos: Towards language mod-\neling with state space models. arXiv preprint\narXiv:2212.14052.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan S. Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. Goemotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , pages 4040–4054.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAlbert Gu, Karan Goel, and Christopher Ré. 2021. Effi-\nciently modeling long sequences with structured state\nspaces. arXiv preprint arXiv:2111.00396.\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\nciently modeling long sequences with structured state\nspaces. In The International Conference on Learning\nRepresentations (ICLR).\nMandy Guo, Joshua Ainslie, David C Uthus, Santiago\nOntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang. 2022. Longt5: Efficient text-to-text trans-\nformer for long sequences. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2022,\npages 724–736.\n14057\nAnkit Gupta, Albert Gu, and Jonathan Berant. 2022. Di-\nagonal state spaces are as effective as structured state\nspaces. Advances in Neural Information Processing\nSystems, 35:22982–22994.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Identity mappings in deep residual net-\nworks.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701.\nSepp Hochreiter. 1998. The vanishing gradient problem\nduring learning recurrent neural nets and problem so-\nlutions. International Journal of Uncertainty, Fuzzi-\nness and Knowledge-Based Systems, 6(02):107–116.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735–\n1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nHassan Ismail Fawaz, Germain Forestier, Jonathan We-\nber, Lhassane Idoumghar, and Pierre-Alain Muller.\n2019. Deep learning for time series classification:\na review. Data mining and knowledge discovery ,\n33(4):917–963.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol\nVinyals, Andrew Zisserman, and Joao Carreira. 2021.\nPerceiver: General perception with iterative atten-\ntion. In International conference on machine learn-\ning, pages 4651–4664. PMLR.\nHanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee,\nand Jangwoo Kim. 2019. Mnnfast: A fast and scal-\nable system architecture for memory-augmented neu-\nral networks. In Proceedings of the 46th Interna-\ntional Symposium on Computer Architecture, pages\n250–263.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Baochang Ma, and Xiangang Li. 2023a. Belle:\nBe everyone’s large language model engine. https:\n//github.com/LianjiaTech/BELLE.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023b. Exploring the impact of instruction data\nscaling on large language models: An empirical\nstudy on real-world use cases. arXiv preprint\narXiv:2303.14742.\nMatt Gardner Johannes Welbl Nelson F. Liu. 2017.\nCrowdsourcing multiple choice science questions.\nIn DOI:10.18653/v1/W17-4413.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim\nGreen, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna\nPotapenko, and et al. 2021. Highly accurate pro-\ntein structure prediction with alphafold. Nature,\n596(7873):583–589.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\nNikita Kitaev, L. Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. ArXiv,\nabs/2001.04451.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, Anna Koco ´n, Bartłomiej Koptyra, Wik-\ntoria Mieleszczenko-Kowszewicz, Piotr Miłkowski,\nMarcin Oleksy, Maciej Piasecki, Łukasz Radli´nski,\nKonrad Wojtasik, Stanisław Wo´ zniak, and Prze-\nmysław Kazienko. 2023. Chatgpt: Jack of all trades,\nmaster of none. Information Fusion, page 101861.\nJan Koco ´n, Piotr Miłkowski, and Monika Za ´sko-\nZieli´nska. 2019. Multi-level sentiment analysis of\npolemo 2.0: Extended corpus of multi-domain con-\nsumer reviews. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 980–991.\nPhong Le and Willem Zuidema. 2016. Quantifying\nthe vanishing gradient and long distance dependency\nproblem in recursive neural networks and recursive\nlstms. In Proceedings of the 1st Workshop on Repre-\nsentation Learning for NLP, pages 87–93.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Jason Phang, Ofir Press, et al.\n2022. What language model to train if you have one\nmillion gpu hours? In Proceedings of BigScience\nEpisode #5–Workshop on Challenges & Perspectives\nin Creating Large Language Models.\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\nArtzi. 2018. Simple recurrent units for highly paral-\nlelizable recurrence. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4470–4481, Brussels, Belgium.\nAssociation for Computational Linguistics.\n14058\nHanxiao Liu, Zihang Dai, David R. So, and Quoc V . Le.\n2021. Pay attention to mlps.\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,\nJonathan May, Hao Ma, and Luke Zettlemoyer. 2021.\nLuna: Linear unified nested attention. Advances\nin Neural Information Processing Systems, 34:2441–\n2453.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,\nLiangke Gui, Graham Neubig, Jonathan May, and\nLuke Zettlemoyer. 2023. Mega: Moving average\nequipped gated attention. In ICLR.\nEric Martin and Chris Cundy. 2017. Parallelizing linear\nrecurrent neural nets over sequence length. ArXiv,\nabs/1709.04057.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 36.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo Pyysalo, Thomas Wolf, and Colin Raffel.\n2023. Scaling data-constrained language models.\narXiv preprint arXiv:2305.16264.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan\nFernando, Caglar Gulcehre, Razvan Pascanu, and\nSoham De. 2023. Resurrecting recurrent neu-\nral networks for long sequences. arXiv preprint\narXiv:2303.06349.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernandez. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\nfano Ermon, and Christopher Ré. 2023. Hyena hierar-\nchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866.\nIlan Price, Jordan Gifford-Moore, Jory Flemming, Saul\nMusker, Maayan Roichman, Guillaume Sylvain,\nNithum Thain, Lucas Dixon, and Jeffrey Sorensen.\n2020. Six attributes of unhealthy conversations. In\nProceedings of the Fourth Workshop on Online Abuse\nand Harms, pages 114–124, Online. Association for\nComputational Linguistics.\nMarkus N. Rabe and Charles Staats. 2022. Self-\nattention does not need o(n2) memory.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining , KDD ’20,\npage 3505–3506, New York, NY , USA. Association\nfor Computing Machinery.\nMelissa Roemmele, Cosmin Adrian Bejan, , and An-\ndrew S. Gordon. 2018. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In AAAI.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nRamsha Siddiqui. 2019. SARCASMANIA: Sarcasm\nExposed! http://www.kaggle.com/rmsharks4/\nsarcasmania-dataset. [Online; accessed 02-\nFebruary-2023].\nDavid R. So, Wojciech Manke, Hanxiao Liu, Zihang\nDai, Noam Shazeer, and Quoc V . Le. 2021. Primer:\nSearching for efficient transformers for language\nmodeling. CoRR, abs/2109.08668.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transformers.\nIn International Conference on Learning Representa-\ntions.\n14059\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComputing Surveys, 55(6):1–28.\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-\nsica Yung, Andreas Steiner, Daniel Keysers, Jakob\nUszkoreit, Mario Lucic, and Alexey Dosovitskiy.\n2021. Mlp-mixer: An all-mlp architecture for vi-\nsion. CoRR, abs/2105.01601.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid Vilares and Carlos Gómez-Rodríguez. 2019.\nHead-qa: A healthcare dataset for complex reasoning.\nIn ACL.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022a.\nEmergent abilities of large language models. ArXiv,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Perric\nCistac, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\n2020. Transformers: State-of-the-Art Natural Lan-\nguage Processing. pages 38–45. Association for\nComputational Linguistics.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and S Yu Philip. 2020. A com-\nprehensive survey on graph neural networks. IEEE\ntransactions on neural networks and learning sys-\ntems, 32(1):4–24.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.\nEx machina: Personal attacks seen at scale. In Pro-\nceedings of the 26th International Conference on\nWorld Wide Web, WWW 2017, Perth, Australia, April\n3-7, 2017, pages 1391–1399. ACM.\nJianxin Yang. 2023. Firefly. https://github.com/\nyangjianxin1/Firefly.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2020. Winogrande: An\nadversarial winograd schema challenge at scale. In\nACL.\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen\nHuang, Hanlin Goh, Ruixiang Zhang, and Josh\nSusskind. 2021. An attention free transformer.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. In\narXiv:1810.12885.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\n14060\nA Author Contributions\nAll authors contributed to the drafting of this paper. Eric Alcaide and Quentin Anthony organized the\npaper and its experiments and were involved in all phases of the development process.\nModel Design and Development Bo Peng (lead), Matteo Grella, Xuzheng He, Haowen Hou, Jiaming\nKong, Johan S. Wind\nModel Training Bo Peng\nScaling Laws Analysis Stella Biderman, Bo Peng\nBenchmark Evaluations Stella Biderman (lead), Kranthi Kiran GV , Krishna Sri Ipsit Mantri, Atsushi\nSaito, Qihang Zhao, Peng Zhou, Rui-Jie Zhuåç\nLong Context Experiments Xingjian Du, Rui-Jie Zhu, Bolun Wang, Ruichong Zhang, Jian Zhu,\nRui-Jie Zhu\nInference Speed Experiments Samuel Arcadinho, Przemysław Kazienko, Qinghua Zhou\nInformation Flow Experiments Huanqi Cao, Michael Chung, Matteo Grella, Ferdinand Mom,\nZhenyuan Zhang\nChat Experiments Jan Koco´n (lead), Przemysław Kazienko, Bartłomiej Koptyra, Hayden Lau, Xiangru\nTang, Stanisław Wo´ zniak, Zhenyuan Zhang\nEthics and Broader Impacts Stella Biderman, Guangyu Song\n14061\nB Author Contributions\nBo Peng Original RWKV idea, original code, performance optimizations, original experiments, and\ntrained RWKV models from 0.1B to 14B.\nEric Alcaide Manuscript (initial draft sections 1, C; sections 3, 7 and 8; revision and proofreading; final\nversion ). Figures (2, 3, 3, 8). Experiments section 6. Appendices E, K. Contributions to Appendix M.\nQuentin Anthony Manuscript (organization, initial draft sections 1, C, 2; revision and proofreading;\nfinal version).\nAlon Albalak Manuscript (abstract and sections 1, 9, and 7; proofreading and revision).\nSamuel Arcadinho Contributions to Figures 7, 13, and 14. Contributions to Appendix K.\nStella Biderman Performed the scaling laws analysis and evaluated competitor models on benchmark\ntasks.\nHuanqi Cao Manuscript (contributions to 3.2 and 3.3; proofreading and revision). Experiments for\nAppendix I.\nXin Cheng Manuscript (proofreading and revision). Contributions to Appendix M, J.\nMichael Chung Manuscript (contributions to section I; proofreading and revision).\nXingjian Du Evaluation on Long Range Arena Benchmark (TBD until 5.31).\nMatteo Grella Manuscript (sections H, I, 8; contributions to sections 1, 7 and 9; proofreading and\nrevision). Contributions to Appendix D.\nKranthi Kiran GV Manuscript (sections C and 5; contributions to section 2; revision and proofreading).\nTables K and K. Appendix 4.\nXuzheng He Manuscript (contributions to section 2; proofreading and revision). Contributions to\nFigure8. Appendix I. Contributions to appendix H.\nHaowen Hou Figure 9. Appendix F.\nJiaju Lin RWKV on LRA benchmarking\nPrzemysław Kazienko Manuscript (proofreading and revision). Contributions to Section 6, 9, and\nAppendix L.\nJan Kocon Manuscript (Section 1; proofreading and revision). Contributions to Appendix L.\nJiaming Kong Manuscript (revision and proofreading). Appendix H.\nBartłomiej Koptyra Manuscript (revision and proofreading) Contributions to Appendix L.\nHayden Lau Manuscript (contributions to section 1 and 9; proofreading and revision). Contributions to\nAppendix M.\nKrishna Sri Ipsit Mantri Figure 12\nFerdinand Mom Manuscript (contributions to section 1, C, 3.3, I; proofreading and revision). Contribu-\ntions to Appendix D.\nAtsushi Saito Manuscript (sections 2 and 5; contributions to section C). Contributions to Appendix J\nGuangyu Song Manuscript (rewrote section 3; final version). Initial draft Ethics Statement).\nXiangru Tang Manuscript (sections C and 2; contributions to abstract; revision and proofreading).\nContributions to Appendix M.\n14062\nBolun Wang Contributions to Tables 1.\nJohan S. Wind RWKV performance optimizations (CUDA), Contributions to Appendix 4.\nStanisław Wo´ zniak Contributions to Appendix L.\nRuichong Zhang Manuscript (proofreading and revision); Contributions to Figure 6 and Appendix M.\nZhenyuan Zhang Manuscript (revision and proofreading). Figure 3. Experiments Appendix I. Contri-\nbutions to Appendices D and M.\nQihang Zhao Manuscript (proofreading and revision). Contributions to Table 5.\nPeng Zhou Contributions to Tables 1 and Table 5.\nQinghua Zhou Manuscript (Proofreading and revision of section 3; Add missing citations in 3.3).\nRevision of Figures 2 and 12.\nJian Zhu Manuscript (section C; proofreading and revision). Figures 3 and 6.\nRui-Jie Zhu Tables 1 and 5. Experiments for table 5.\nC Additional Related Work\nRecently, a number of techniques have been proposed to address the limitations of transformers.\nOptimizing Attention Mechanism Many transformer variants (“x-formers”) have been introduced to\nreduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;\nKitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al.,\n2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other\nefficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).\nSome recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al.,\n2019) share similarities with RWKV’s chunked computation scheme. Despite being memory-efficient,\ntheir time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV\nachieves better space and time complexity during inference by formulating a linear attention as an RNN.\nAttention Free Models Another line of research replaces the attention mechanism with other modules to\nscale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) propose replacing\nattention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer\n(AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a\ncomputationally efficient alternative. None of these models have been successfully scaled to the point\nwhere drawing comparisons with transformer-based large language models makes sense.\nThere has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants\n(Dao et al., 2022b; Gupta et al., 2022; Poli et al., 2023). In contrast to the preceding models, SSM and\nits successors have shown substantial progress towards efficient scaling. Simultaneously with this work,\nPoli et al. (2023) train SSM-based models with 125 million and 355 million parameters and show that the\nperformance is on-par with a transformer that uses a mix of local and global attention (Black et al., 2021).\nAdvances in RNNs Inspired by the success of transformers, RNN-style (Hochreiter and Schmidhuber,\n1997; Chung et al., 2014) recursive components have also been modified to increase context length, such\nas the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto\net al., 2023). Most similar to our work, the Quasi-Recurrent neural network (QRNN) (Bradbury et al.,\n2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels.\nWhile QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as\nan attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN,\nRWKV includes a parametrized channel-mixing module that is parallelizable.\n14063\nD Time-Mixing Block as an RNN Cell\nAs stated in 3.3, the RWKV time-mixing block can be formulated as an RNN, as the WKV computation\ncan be written in such a recursive form:\na0,b0 = 0, (19)\nwkvt = at−1 + eu+kt ⊙vt\nbt−1 + eu+kt\n, (20)\nat = e−w ⊙at−1 + ekt ⊙vt, (21)\nbt = e−w ⊙bt−1 + ekt. (22)\n1 2 \ne \n3 \nFigure 8: RWKV time-mixing block formulated as an\nRNN cell. Color codes: yellow ( µ) denotes the token\nshift, red (1) denotes the denominator, blue (2) denotes the\nnumerator, and pink (3) denotes the fraction computations\nin 16. hdenotes the numerator-denominator tuple.\nThe dataflow of the RNN-like time-mixing is\nshown in Fig. 8, where the hidden states h is\nthe numerator-denominator tuple (a,b). To avoid\noverflow in calculating ekt, a numerical trick is\nused in the official implementation. Noticing\nthat a1 = ek1 ⊙v1 and b1 = ek1 , we set a′\n1 =\nv1,b′\n1 = 1,p1 = k1, where pt stores the shared\nexponents of at and bt. Now the above recursion\ncan be converted into a numerical safe version,\nfor each time step t> 1:\nq:= max(pt−1,u + kt), (23)\nwkvt = ept−1−q ⊙a′\nt−1 + eu+kt−q ⊙vt\nept−1−q ⊙b′\nt−1 + eu+kt−q . (24)\nThe update to a′\nt,b′\nt, and their shared exponent is\nalso carried out in a similar fashion:\nq′:= max(pt−1 −w,kt), (25)\na′\nt = ept−1−w−q′\n⊙a′\nt−1 + ekt−q′\n⊙vt, (26)\nb′\nt = ept−1−w−q′\n⊙b′\nt−1 + ekt−q′\n, (27)\npt = q′. (28)\nThe RWKV model has an internal state that stores some previous information. In each layer, the internal\nstate consists five parts, each of which is a vector with Dnumbers, where Dis the model dimension. The\nfive parts are:\n• The current input of the Time-mix block xt;\n• The current input of the Channel-mix block yt;\n• The numerator of the WKV value a′\nt, as defined in equation (26);\n• The denominator of the WKV value b′\nt, as defined in equation (27);\n• An auxiliary state pt in (28), which is used for WKV computation to maintain numerical precision.\nWhich yields a total size of 5DLparameters. It is worth noting that in an algebraic context with infinite\nprecision, the helper state pt can be ignored, and the WKV numerator and denominator can be computed\ndirectly using equations (21) and (22), reducing the size of the internal state to 4DL.\nE Parameter initializations\nWe describe the specific parameter initializations below and motivate the design choices. Parameters\nbelonging to residual blocks are often adjusted by layer depth and total number of layers. Let # denote\nthe vocabulary size, sdenote the embedding dimension, ddenote the hidden size (we use d= 4s), Lthe\nnumber of layers, lthe layer index (from 0 to L−1), we use the following initializations:\n14064\n• Embeddings are initialized to U(±1 ×10−4) as explained in 3.4\n• For the time-mixing blocks (11, 12, 13), initializations are µki = (i\ns)1−l\nL , µvi = (i\ns)1−l\nL + 0.3l\nL−1 and\nµri = 1\n2 ·( i\ns)1−l\nL\n• For the channel-mixing blocks (14, 15), µki and µri are initialized to ( i\ns)1−l\nL\n• wi (16), also known as “time decay”, is initialized to −5 + 8·( i\nd−1 )0.7+ 1.3l\nL−1 . Intuitively, it is the\ndiscount factor applied to previous tokens over time.\n• ui (16), also known as “bonus”, is set to 0.5 ·(((i+ 1) mod 3)−1) + log 0.3. It is the special\nweighting applied to the current token in equation 16. The alternating zigzag pattern initially\ncreates subtle variations in the tensor elements, which are intended to help the model treat different\ndimensions of the embedding distinctively.\n• Wo (17) (time-mixing) and Wv (channel-mixing) are initialized to N(0,\n√\nd\ns = 2)\n• All other Wr,Wk,Wv weights are initialized to 0 so the model can start learning from the beginning\nwithout noisy signals.\n• All LayerNorm weights start from 1 and biases from 0.\nF Small Init Embedding\nThis section presents the experimental validation of small initialization embedding. The experimental\nsetup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution\nwith a mean of 0.0 and a standard deviation of 0.02, which is a commonly used initialization method\nin models like BERT and GPT. On the other hand, in the small initialization of the embedding (small\ninit emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4,\nwhich is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4\nis used. However, this difference is negligible and does not affect our conclusions. The experiments\nwere conducted with a batch size of 400. As depicted in Figure 9, the loss curve for the small init emb\nexhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal\ndistribution.\n0 10000 20000 30000 40000 50000\nStep\n4\n5\n6\n7\n8\n9\n10\n11Loss\nBaseline\nSmall Init Emb\nFigure 9: Effect of small initialization embedding.\nG Hyperparameters\nTo train the models mentioned, we use ϵ= (0.9,0.99) without weight decay for the Adam optimizer, and\nswitch batch size dynamically between 128 or 256 sequences, each of 1024 tokens. We further organize\n14065\nModel 169M 430M 1.5B 3B 7B 14B\nInit LR 0.0006 0.0004 0.0003 0.00015 0.00015 0.0001\nWarmup Mini-Epochs 361 411 443 451 465 544\nEnd LR 0.00001 0.00001 0.00001 0.00001 0.00001 0.000007\nTable 3: Hyperparameters for our learning rate (LR) schedule of the pretrained models.\nthe training into multiple mini-epochs, each of 40320 samples, to guide our learning rate schedule. The\ntraining process takes 8043 mini-epochs to make one pass over the Pile. The initial warming up mini-\nepochs have a constant learning rate of “Init LR”. After the warming up mini-epochs, the learning rate\nexponentially decays until in the last mini-epoch, in which the model finishes training on the entire Pile,\nthe learning rate arrives at the “End LR”. The related hyperparameters are shown in Table 3.\nH Gradient Stability in RWKV\nIn this section, we present a mathematical description of the gradient stability property in RWKV , focusing\nspecifically on the time-mixing block. By gradient stability we mean that if the inputs xt are bounded and\nthe model parameters are fixed, then the gradients with respect to Wk and Wv are uniformly bounded for\nall T (thus not exploding). Consequently, we can control the amount each xt contributes to the gradient at\nT in a naturally decaying fashion by the weight decay mechanism w(thus not vanishing unless desired).\nFirst, we make the simplification that there are no token shifts, this will not affect the final conclusion.\nIn this scenario, wkvT can be written as\nwkvT =\n∑T\nt=1 Ke\nt ⊙vt\n∑T\nt=1 Ke\nt\n= E(vt) =S(vt)\nS(1) , (29)\nwhere\nvt = Wvxt, ∂(vt)i\n∂(Wv)i,j\n= (xt)j,\nKe\nt = eWkxt+wT,t , ∂(Ke\nt)i\n∂(Wk)i,j\n= (xt)j(Ke\nt)i,\nand S(·) and E(·) are shorthand for denoting sums and averages over weights Ke\nt.\nThe loss function at position T can be written as\nLT = l(f(wkvT),yT). (30)\nBecause wkvT relates to (Wk)i,j and (Wv)i,j only through the i-th channel (wkvT)i, we have\n∂LT\n∂(Wv)i,j\n= ∂LT\n∂(wkvT)i\n∂(wkvT)i\n∂(Wv)i,j\n. (31)\nThe first part of the above equation contains trivial operations like output layers, and other layers of\ntime-mixing, which can be proven inductively. The second part of the above equation can be bounded as\n⏐⏐⏐⏐\n∂(wkvT)i\n∂(Wv)i,j\n⏐⏐⏐⏐=\n⏐⏐⏐⏐\n∂Ei[(vt)i]\n∂(Wv)i,j\n⏐⏐⏐⏐\n= |Ei[(xt)j]|≤ max\nt\n|(xt)j|, (32)\n14066\nwhich is irrelevant to T. Similarly,\n∂(wkvT)i\n∂(Wk)i,j\n= ∂Si[(vt)i]\nSi(1) /∂(Wk)i,j\n= Si[(xt)j(vt)i]\nSi(1) −Si[(xt)j]Si[(vt)i]\nSi(1)2\n= Ei[(xt)j(vt)i] −Ei[(xt)j]Ei[(vt)i]\n= covi((xt)j,(vt)i) (33)\ncan also be bounded. Note that wkv’s softmax operation contains at least two non-zero terms (uand w),\nso the above “covariance” will not degenerate into 0.\nI Model Behavior Visualization\n0 100 200 300 400 500 600 700 800\nChannel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Time Decay\nTime decay (sorted along channel axis)\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nFigure 10: Model behavior visualizations of RWKV .\nThe right plot illustrates the time decays ( e−w)\nin each layer of the RWKV-169M model, sorted\nalong the channel axis. Notably, several decays\nin the last layers are very close or equal to one,\nimplying that certain information is preserved\nand propagated throughout the model’s temporal\ncontext. Meanwhile, many decays in the initial\nlayer are close to zero, which corresponds to lo-\ncal operations inwkv(16), likely to be associated\nwith tasks such as text parsing or lexical analy-\nsis. (Note that the local operations in wkv are\ndue to the extra parameter u, when e−w is de-\ngenerated into 0.) These patterns of time decays\nare partly learned, but also come from parameter\ninitialization as it speeds up training.\nThe plot below shows the information retrieval\nand propagation path in the RWKV-430M model. The experiment follows the causal trace method\nintroduced by Meng et al. (2022), where we\n1. Run the model once, and record all states and activation of each layer during the computation;\n2. Corrupt the input embeddings of the subject using noise (“The Eiffel Tower” in this example);\n3. Restore the states and activation of a certain layer at a certain token during the computation, and\nrecord the log-probability of the model outputting the correct answer (“Paris”).\nUnlike transformers, RWKV relies on the recursive propagation of information in the time dimension.\nIn this case, the fact that the Eiffel Tower is located in Paris is retrieved in layer 4 just after the model\nsees “The Eiffel”. It is then passed down to the subsequent layers. In layer 20, mostly, the information is\npropagated through time until reaching where it is needed. Finally, at the token “of”, it is passed down to\nthe last layer for outputting the answer.\nJ Additional Evaluations\nJ.1 Further details on NLP tasks\nWe evaluate on the following tasks:\nARC (Clark et al., 2018) A dataset designed for multiple-choice question answering, encompassing\nscience exam questions ranging from third grade to ninth grade. It has Easy and Challenge subsets that\nwe report results on separately.\nBoolQ (Clark et al., 2019) A binary yes/no question answering benchmark.\n14067\n1 6 11 16 21\nLayer\nThe\n E\niff\nel\n T ower\n is\n located\n in\n the\n city\n of\nInformation propagation path\n7\n6\n5\n4\n3\n2\n1\nLog-probability of \"Paris\"\nFigure 11: Model behavior visualizations of the RWKV model.\nCOPA (Roemmele et al., 2018) A dataset to evaluate achievement in open-domain commonsense\ncausal reasoning.\nHeadQA (Vilares and Gómez-Rodríguez, 2019) A benchmark consisting of graduate-level questions\nencompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology.\nHellaSwag (Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference\n(NLI) which is build by adversarial filtering against transformer models.\nLAMBADA (Paperno et al., 2016) A benchmark dataset that evaluates the model’s contextual reasoning\nand language comprehension abilities by presenting context-target pairs, where the objective is to predict\nthe most probable target token. We follow standard practice and use the untokenized version created by\nOpenAI (Brown et al., 2020).\nOpenBookQA (Mihaylov et al., 2018) A QA dataset to evaluate human comprehension of a subject by\nincorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration\nfrom open book exams.\nPIQA (Bisk et al., 2020) A benchmark for the task of physical common sense reasoning, which consists\nof a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution).\nReCoRD (Zhang et al., 2018) A benchmark for evaluating commonsense reasoning in reading\ncomprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers\nfrom corresponding summarizing passages.\nSciQ (Johannes Welbl Nelson F. Liu, 2017) A multiple-choice QA dataset which was created using\nan innovative approach to gather well-crafted multiple-choice questions that are focused on a specific\ndomain.\nWinogrande (Zellers et al., 2020) A dataset designed to evaluate the acquisition of common sense\nreasoning by neural language models, aiming to determine whether we are accurately assessing the true\ncapabilities of machine common sense.\n14068\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (b) ARC (Easy)\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (c) BoolQ\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\nBLOOM\nPythia\nOPT\nRWKV\n(d) COPA\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (e) HeadQA\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (f) HellaSwag\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\nBLOOM\nPythia\nOPT\nRWKV\n(g) LAMBADA (OpenAI)\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (h) OpenBookQA\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (i) PiQA\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\nBLOOM\nPythia\nOPT\nRWKV\n(j) ReCoRD\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (k) SciQ\n102 103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100Accuracy\n (l) Winogrande\nFigure 12: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks.\n14069\nJ.2 Evaluation on Long Range Arena\nThe Long-Range Arena (LRA) benchmark (Tay et al., 2021) is designed to assess the performance of\nmodels in handling lengthy context situations. It includes a collection of tasks with sequences ranging\nfrom 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images,\nand mathematical expressions. We apply RWKV on the LRA benchmark and the report results are in\nTable 4. Other models’ performances are directly cited from Gu et al. (2022); Alam et al. (2023).\nTable 4: Evaluation on Long Range Arena. Other models reported in the literature (Gu et al., 2022; Alam et al.,\n2023). Bolded values are the best.\nMODEL LIST OPS TEXT RETRIEVAL IMAGE PATHFINDER PATH-X A VG\nTransformer 36.37 64.27 57.46 42.44 71.40 ✗ 53.66\nReformer 37.27 56.10 53.40 38.07 68.50 ✗ 50.56\nBigBird 36.05 64.02 59.29 40.83 74.87 ✗ 54.17\nLinear Trans. 16.13 65.90 53.09 42.34 75.30 ✗ 50.46\nPerformer 18.01 65.40 53.82 42.77 77.05 ✗ 51.18\nFNet 35.33 65.11 59.61 38.67 77.80 ✗ 54.42\nNyströmformer 37.15 65.52 79.56 41.58 70.94 ✗ 57.46\nLuna-256 37.25 64.57 79.29 47.38 77.72 ✗ 59.37\nHrrformer 39.98 65.38 76.15 50.45 72.17 ✗ 60.83\nS4 59.60 86.82 90.90 88.65 94.20 96.35 86.09\nRWKV 55.88 86.04 88.34 70.53 58.42 ✗ 72.07\nThe results show that RWKV performs second only to the S4 model in five datasets. While RWKV\nsubstantially underpreforms S4 on Image, Pathfinder, and Path-X, on the problems related to natural\nlanguage and computer code processing RWKV performs on par with S4 or nearly so.\nJ.3 Enwik8 Perplexity\nWe also evaluate our model in terms of perplexity on the Enwik8 dataset. Baseline comparisons are\nmade with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense\nversion), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). L,d,\nand T denote the number of blocks (network depth), dimension of features, and sequence length, re-\nspectively. Both Linear Transformer and Performer are implemented with customized CUDA kernels\n(github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch.1 No weight\ndecay nor dropout was used. 2 Trained with AdamW and weight decay set to 0.1, dropout of 0.1, batch\nsize of 16, and initial learning rate of 6e-4.\nMethod L d T Train bpc Test bpc Time Complexity Space Complexity\nTransformer 12 512 1024 0.977 1.137 O(T2d) O(T2 + T d)\nTransformer 24 256 1024 1.039 1.130 O(T2d) O(T2 + T d)\nReformer 12 512 1024 1.040 1.195 O(T log T d) O(T log T + T d)\nSynthesizer 12 512 1024 0.994 1.298 O(T2d) O(T2 + T d)\nLinear Transformer 12 512 1024 0.981 1.207 O(T d2) O(T d+ d2)\nPerformer 12 512 1024 1.002 1.199 O(T d2 log d) O(T dlog d + d2 log d)\nAFT-simple 12 512 1024 1.046 1.209 O(T d) O(T d)\nRWKV-RNN1 6 512 1024 0.720 - O(Td) O(d)\nRWKV-RNN2 12 512 1024 1.010 1.178 O(Td) O(d)\nTable 5: Enwik8 results, measured in bits per character (bpc).\nK Inference results\nFigures 13 and 14 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements\nfor LLM inference in float32 precision. We benchmark the following model families and sizes:\n14070\n• RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b\n• Bloom (Scao et al., 2022): 560m, 1b, 3b\n• OPT (Zhang et al., 2022): 125m, 350m, 1.3b, 2.7b, 6.7b, 13b\n• GPT-Neo (Black et al., 2021): 125m, 1.3b, 2.7b\n• Pythia (Biderman et al., 2023b): 160m, 410m, 1.4b, 2.8b, 6.7b, 12b\nFigure 13: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not\naccounted.\nFigure 14: Text generation inference time for LLMs.\n14071\nTask Name Measure ChatGPT GPT-4 RWKV-GPT RWKV-adapted SOTA\nRTE F1 Macro 88.1 91.3 44.2 74.8 92.1\nWNLI Accuracy 81.7 91.6 47.9 49.3 97.9\nGoEmotions F1 Macro 25.6 23.1 7.9 7.9 52.8\nPolEmo2 F1 Macro 44.1 41.0 38.2 40.9 76.4\nTable 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B reasoning performance comparison in RTE (Wang et al., 2019),\nWNLI (Wang et al., 2018), GoEmotions (Demszky et al., 2020), and PolEmo2 (Koco´n et al., 2019) benchmarks.\nRWKV GPT prompts were primarily used for ChatGPT in (Koco´n et al., 2023). SOTA is provided as a supplementary\nreference.\nTask Name Measure ChatGPT RWKV-adapted SOTA\nAggression F1 Macro 69.10 56.66 74.45\nMathQA Accuracy 71.40 5.43 83.20\nSarcasm F1 Macro 49.88 50.96 53.57\nTweetSent F1 Macro 63.32 52.50 72.07\nUnhealthy F1 Macro 45.21 43.30 50.96\nTable 7: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion (Wulczyn et al., 2017), Sarcasm\n(Siddiqui, 2019), Unhealthy (Price et al., 2020), MathQA (Cobbe et al., 2021), and TweetSent (Barbieri et al., 2020)\nbenchmarks. SOTA is provided as a supplementary reference.\nL Importance of prompt construction and comparison to GPT models\nInspired by Koco´n et al. (2023), we compared the zero-shot performance of the RWKV-4-Raven-14B with\nChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual\nentailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited\nin readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive\nproper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse\nthan ChatGPT and GPT-4 in several specific tasks. We suspect that this disparity is likely caused by the\nchoice of prompts used to generate the answers since the prompts are written in natural language and do\nnot take into account that RWKV , as an RNN, is unable to look back inside an instruction.\nWhen the instruction style was adapted (re-ordered) to respect that RNNs are not capable of \"retro-\nspective processing\", the quality may significantly change, e.g., for RTE (Wang et al., 2019) F1 Macro\nincreased from 44.2% to 74.8%. We hypothesize that RWKV models are more sensitive to the position of\nthe components in the context, as RNN-based architectures cannot look back and readjust the weight of\nprevious information. For better performance, the desired information should be placed after the main\nquestion.\nAn example ChatGPT prompt for recognizing textual entailment (RTE)\nHaving premise <here is a premise> judge if the following hypothesis <here is a hypothesis> is\nlogically connected with the premise. Answer \"entailment\" if yes, or \"not_entailment\" if no.\nA re-ordered RWKV prompt for RTE taking into account the nature of the RNN\nCan you tell me if the hypothesis is entailment or is not entailment to the premise?\npremise: <here is a premise>\nhypothesis: <here is a hypothesis>\nWhile separating the instruction from the input is relatively easy to do, some other aspects of prompt\nengineering are harder to quantify. For that purpose, we also tested the approach of stating the input after\n14072\nthe question on multiple other tasks, i.e., aggression and sarcasm detection, classification of unhealthy\n(offensive) texts, mathematical Q&A, and sentiment analysis, see Tab. 7. The results suggest that better\nprompts might reduce the disparity between models. Raven achieves comparable results to ChatGPT on\nunhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While such an\napproach to prompting looks necessary, it is not enough in itself to replace the capability of having free\naccess to the whole context. Therefore, prompt engineering seems to be significantly more important for\nthe RNN models rather than for standard transformers. It is entirely possible that good prompts to RNN\nmodels do not mean additional restrictions, but should simply be constructed using completely different\nguidelines. The authors of the aforementioned paper (Koco ´n et al., 2023) 4 perform chain-of-thought\nto improve results on the MathQA dataset. Even including this approach, the Raven model achieved a\nvery low accuracy of 5.43%. Without it, the model performed even worse, performing only very basic\nand simple calculations and achieving 4.13% accuracy. Raven struggled with questions that required\nintermediate results. It is likely that the order of information presented in the math questions inside the\ndataset poses a challenge for the RWKV model. It is yet to be seen if prompt engineering can address this\nissue. This further emphasizes the importance of the order of information the model receives.\nTemplate used to prompt the Raven model in MathQA with chain-of-thought\nWrite the reasoning and highlight the answer to the question at the end in the format: \"Answer: \".\nThe question is: <here is a question>\nTemplate used to prompt the Raven model in MathQA without chain-of-thought\nWrite the answer to the math question in the format: \"Answer: \".\nThe question is: <here is a question>\nM Cases\nIn this part, we present a few instances of outputs produced by RWKV model using a Chat interface56.\nRWKV-4-Raven-14B\nTell me about ravens.\nRavens are large, black birds with a distinctive white head and neck. They are found in most parts\nof the world, including North America, Europe, Asia, and Australia. Ravens are known for their\nintelligence and problem-solving abilities. They are also considered to be symbols of death and\ntransformation in many cultures. Ravens are often associated with the afterlife or death because\nthey have been known to eat carrion or even other birds. In some cultures, ravens are also believed\nto be messengers of the gods or guardians of treasure.\n4This is in line with the idea discussed in (Wei et al., 2022b)\n5ttps://github.com/BlinkDL/ChatRWKV\n6https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\n14073\nRWKV-4-Raven-14B\nExplain the concept of machine learning to a 10-year-old.\nMachine learning is like a super-smart computer that can learn on its own. It can recognize patterns\nand make decisions based on what it has learned. Just like how you might remember something\nfrom the past, the computer can remember what it’s learned and use it to make predictions or\ndecisions in the future. It’s like having a smart friend who always knows what to do!\nRWKV-4-Raven-14B\nWrite a short poem describing nature’s beauty in springtime.\nNature’s beauty in springtime is rare,\nA symphony of colors, a gentle surprise.\nThe flowers bloom, the birds sing and sing,\nAs the sun shines down, warming our skin.\nThe trees sway in the gentle breeze,\nAs we take in their beauty with ease.\nThe sky is blue, the clouds are white,\nA sight to behold, it’s truly divine.\n14074\nRWKV-4-Raven-14B\nBelow is the numerical stable formula for updating the state variable in RWKV:\na∗\nt = ep−qat−1 + eu+kt−qvt\nb∗\nt = ep−qbt−1 + eu+kt−q\nwkvt = a∗\nt\nb∗\nt\nPlease write out the corresponding pytorch code to do so, where both at and bt are\ntorch.Tensor([n])\nHere is the PyTorch code to update the state variable in RWKV using the given formula:\nimport t o r c h\ndef u p d a t e _ s t a t e ( a , b , u , v , p , q ) :\na _ t = t o r c h . exp ( p − q ) * a + t o r c h . exp ( u + k − q ) * v\nb _ t = t o r c h . exp ( p − q ) * b + t o r c h . exp ( u + k − q )\nreturn a_t , b _ t\na , b = t o r c h . r a n d n ( 2 , 3 )\nu = t o r c h . r a n d n ( 2 )\nv = t o r c h . r a n d n ( 2 )\nwkv = t o r c h . r a n d n ( 2 )\na_t , b _ t = u p d a t e _ s t a t e ( a , b , u , v , wkv , t o r c h . r a n d n ( 2 ) )\np r i n t( \" a _ t : \" , a _ t )\np r i n t( \" b _ t : \" , b _ t )\nThis code defines a function update_state that takes in the state variables a and b, the control\nvariables u and v, and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp\nfunction to compute the exponential of the control variables and the log of the normalization factor.\nFinally, it uses these values to update the state.\nRWKV-4-Raven-14B\nExplain the following metaphor: Life is like cats.\nThe metaphor \"Life is like cats\" suggests that life can be unpredictable and full of unexpected\ntwists and turns, just like a cat’s behavior. It implies that we cannot fully control or predict the\noutcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies\nthat life is complex and multifaceted, with many different variables and factors at play. Just as a\ncat can have multiple personalities or moods, life can have many twists and turns that are difficult\nto predict or understand. Overall, the metaphor suggests that life is complex and unpredictable,\nmuch like a cat’s behavior.\n14075\nRWKV-4-Raven-14B\nTest the convergence of∑∞\ni=0 2−i.\nThe given series is a geometric series with the first term a0 = 2and common ratio r= 1\n2 . We can\nfind the sum of this series using the formula for a geometric series:\n∞∑\ni=0\nai = a0\n1 −r =\n1\n2\n1 −1\n2\n= 1\n1 −1\n2\n= 2\n1 = 2\nSo, the sum of the given series is 2 .\nRWKV-4-Raven-14B\nI would like to cook some Chinese food at home. Do you have any suggestions on the meal I\ncould choose?\nYes, of course! If you are not very experienced in cooking Chinese food, you could start with\nmaking noodles or rice, then make a dish using one of those. I also think making stir-fry with\nsome meat is an excellent idea.\nI want to make something more special. What about red braised pork belly?\nYes, red braised pork belly is also very popular and tasty.\nTell me the steps to cook.\nYou will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine.\nPut all of these in a pot, cook on medium heat and simmer until the meat is tender. After that,\nremove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices\ninto it, then boil until it’s almost done.\nWhat do I need to be careful with the most during the cooking process?\nTo cook pork belly properly, it is very important to monitor the heat of the stove and be patient\nwith the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew.\nYou can add a few ingredients like soy sauce or black beans to give the pork more flavor.\nTell me more about the proper heat and timing.\nIf the temperature of the water is too high, it can damage the collagen and connective tissues,\nresulting in tough pork belly. It is essential to control the temperature carefully, especially\nwhen braising the pork. Additionally, keep the water simmering but not boiling to maintain a\nlow simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat.\nLastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most\nappropriate for slow cooking pork belly.\nI see. Thank you for your advice.\nYou’re welcome! It was a pleasure chatting with you. Let me know if you need any further\nassistance in the future.\n14076\nRWKV-4-Raven-14B\nIf all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your\nanswer in detail.\nYes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are\nsweet.\nIf birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.\nNo, penguins cannot fly. Penguins are birds, but they are not capable of flight like other birds.\nThey have wings and can glide for short distances, but they cannot take off or fly long distances\nlike other birds.\nThe day before two days after the day before tomorrow is Saturday. What day is it today?\nPlease explain your answer in detail.\nToday is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.\nIf five cats can catch five mice in five minutes, how long will it take one cat to catch one\nmouse? Please explain your answer in detail.\nIf five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one\nminute. To find out how long it will take one cat to catch one mouse, we can use the concept of\nproportionality.\nLet’s assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught\nby a cat in one minute is equal to the number of cats multiplied by the number of mice caught by\neach cat: x = 5 * 1 = 5\nSo, it will take 5 minutes for one cat to catch one mouse.\nRWKV-4-Raven-14B\nSummarize the following paragraph.\n<The Abstract of this paper goes here>\nThe paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines\nthe efficient parallelizable training of Transformers with the efficient inference of RNNs. The\nmodel is able to scale to tens of billions of parameters and exhibits linear computational complexity\nduring training and inference, making it a promising alternative to Transformers for sequence\nprocessing tasks.\nPlease further summarize it in one sentence.\nThe researchers have developed a new architecture that combines the efficient parallelizable\ntraining of Transformers with the efficient inference of RNNs, which allows for linear scaling in\nmemory and computational requirements.\n14077",
  "topic": "STELLA (programming language)",
  "concepts": [
    {
      "name": "STELLA (programming language)",
      "score": 0.7510714530944824
    },
    {
      "name": "Zhàng",
      "score": 0.5092848539352417
    },
    {
      "name": "Art history",
      "score": 0.4877093732357025
    },
    {
      "name": "Art",
      "score": 0.43293672800064087
    },
    {
      "name": "Humanities",
      "score": 0.3901853561401367
    },
    {
      "name": "Philosophy",
      "score": 0.38453900814056396
    },
    {
      "name": "History",
      "score": 0.22682073712348938
    },
    {
      "name": "Archaeology",
      "score": 0.09183621406555176
    },
    {
      "name": "China",
      "score": 0.05001938343048096
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I71999127",
      "name": "Universitat de Barcelona",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I4210167031",
      "name": "State Library of Ohio",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I111483173",
      "name": "King University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I11923345",
      "name": "Wrocław University of Science and Technology",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I219193219",
      "name": "Purdue University West Lafayette",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210161401",
      "name": "Criteo (France)",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210105262",
      "name": "Unge aksjonærer",
      "country": "NO"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    }
  ]
}