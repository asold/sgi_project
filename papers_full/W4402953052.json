{
  "title": "Toward Large Language Models as a Therapeutic Tool: Comparing Prompting Techniques to Improve GPT-Delivered Problem-Solving Therapy",
  "url": "https://openalex.org/W4402953052",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5107589192",
      "name": "Daniil Filienko",
      "affiliations": [
        "University of Washington Tacoma"
      ]
    },
    {
      "id": "https://openalex.org/A5090710665",
      "name": "Yinzhou Wang",
      "affiliations": [
        "Dartmouth College"
      ]
    },
    {
      "id": "https://openalex.org/A5107589193",
      "name": "Caroline El Jazmi",
      "affiliations": [
        "University of Washington Tacoma"
      ]
    },
    {
      "id": "https://openalex.org/A5077181634",
      "name": "Serena Jinchen Xie",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5071178113",
      "name": "Trevor Cohen",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5056749857",
      "name": "Martine De Cock",
      "affiliations": [
        "University of Washington Tacoma"
      ]
    },
    {
      "id": "https://openalex.org/A5028043951",
      "name": "Weichao Yuwen",
      "affiliations": [
        "University of Washington Tacoma"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367668719"
  ],
  "abstract": "While Large Language Models (LLMs) are being quickly adapted to many domains, including healthcare, their strengths and pitfalls remain under-explored. In our study, we examine the effects of prompt engineering to guide Large Language Models (LLMs) in delivering parts of a Problem-Solving Therapy (PST) session via text, particularly during the symptom identification and assessment phase for personalized goal setting. We present evaluation results of the models' performances by automatic metrics and experienced medical professionals. We demonstrate that the models' capability to deliver protocolized therapy can be improved with the proper use of prompt engineering methods, albeit with limitations. To our knowledge, this study is among the first to assess the effects of various prompting techniques in enhancing a generalist model's ability to deliver psychotherapy, focusing on overall quality, consistency, and empathy. Exploring LLMs' potential in delivering psychotherapy holds promise with the current shortage of mental health professionals amid significant needs, enhancing the potential utility of AI-based and AI-enhanced care services.",
  "full_text": "arXiv:2409.00112v1  [cs.CL]  27 Aug 2024\nT oward Large Language Models as a Therapeutic T ool: Comparing\nPrompting T echniques to Improve GPT -Delivered Problem-Solving Therapy\nDaniil Filienko, BS, 1, Yinzhou Wang, MS, 3, Caroline El Jazmi, BS, 1, Serena Xie, MS, 2,\nT revor Cohen, MBChB, Ph.D.,2, Martine De Cock, Ph.D.1, Weichao Y uwen, Ph.D., RN1\n1University of Washington, T acoma, W A;2University of Washington, Seattle, W A;\n3Dartmouth College, Hanover , NH\nAbstract\nWhile Large Language Models (LLMs) are being quickly adapte d to many domains, including healthcare, their\nstrengths and pitfalls remain under-explored. In our study , we examine the effects of prompt engineering to guide\nLarge Language Models (LLMs) in delivering parts of a Proble m-Solving Therapy (PST) session via text, particularly\nduring the symptom identiﬁcation and assessment phase for p ersonalized goal setting. W e present evaluation results\nof the models’ performances by automatic metrics and experi enced medical professionals. W e demonstrate that the\nmodels’ capability to deliver protocolized therapy can be i mproved with the proper use of prompt engineering meth-\nods, albeit with limitations. T o our knowledge, this study i s among the ﬁrst to assess the effects of various prompting\ntechniques in enhancing a generalist model’s ability to del iver psychotherapy, focusing on overall quality, consis-\ntency, and empathy. Exploring LLMs’ potential in deliverin g psychotherapy holds promise with the current shortage\nof mental health professionals amid signiﬁcant needs, enha ncing the potential utility of AI-based and AI-enhanced\ncare services.\nIntroduction\nNumerous studies have demonstrated the potential of Large L anguage Model (LLM) usage in medical applications,\nranging from question-answering services to producing med ical notes [1, 2]. Deployment of LLMs in AI-supported\ncare services holds the potential to mitigate healthcare co sts and broaden access to care. Companies are already\noffering products that attempt to undertake the roles of adm inistrative and medical clinicians through the use of LLMs. 1\nOne of the ﬁelds that could greatly beneﬁt from additional re sources is psychotherapy, with as many as 20% of people\nworldwide needing mental health care [3] in the context of a g lobal scarcity of mental health professionals [4].\nPreviously, studies have demonstrated the advantages of us ing relevant and empathetic responses in mental health\ndialogues [5, 6]. Althoff et al. [5], in particular, showed t hat more successful human counselors use fewer templated\nreplies and produce varied responses to similar questions. LLMs’ ability to generate coherent and contextually appro-\npriate responses may provide an ideal tool for simulating su ch behavior, though concerns remain about their safety\nand utility as patient-facing tools. In this paper, we descr ibe a study of LLMs’ ability to provide such relevant and\nempathetic responses in real-time psychotherapy dialogue s.\nIn this study, we explore the ability of off-the-shelf LLMs t o deliver Problem-Solving Therapy (PST) for family care-\ngivers, targeting common caregiving symptoms, such as fati gue and anxiety, through a dialogue system that provides\ncaregivers the tools to self-monitor symptoms, problem-so lve, and take appropriate actions. PST is an effective form o f\ncognitive behavioral therapy with a speciﬁc protocol, maki ng it a good test case for LLM performance, and providing\nthe model with speciﬁc guidelines to follow .\nW e demonstrate that non-medical LLMs can display a reasonab le performance in PST due to their remarkable ability\nto use the information provided in the prompt to enhance thei r performance [7]. W e adapt an LLM ﬂow that already\nshowed good performance for medical Q&A [1] to medical dialo gue generation. W e explore the extent to which\na general-purpose LLM such as GPT -4 can be improved by method s not involving modifying the model’s weights\nand compare it with a previously developed human-curated ru le-based system [8] in the context of PST for family\ncaregivers with standardized patients portrayed by actors . Actors were provided sample responses and narratives\ndescribing the personas that they were playing in the intera ctions with the model to ensure consistent behavior. T o\ncompare the LLM performance on this domain-speciﬁc task mea ningfully, we recruited multiple clinicians to evaluate\nthe dialogues without knowing how the dialogues were create d. Hence, our work has two primary objectives. The ﬁrst\nis to investigate the use of prompt engineering methods to im prove a general-purpose LLM’s ability to deliver steps of\n1 see e.g. https://www .hippocraticai.com/\nPST . The second is to provide comprehensive automated and hu man evaluations of the empathy and overall quality of\nthe models’ output.\nRelated W ork\nExisting work to improve the accuracy of LLMs for medical app lications leverages (1) ﬁne-tuning the model’s weights\nthrough further training on application-speciﬁc data and/ or (2) developing novel ways to query the model to trigger a\nhigh-quality response by altering the prompt or including r elevant information or examples in the prompt, a method\ncalled prompt engineering . Since for ﬁne-tuning, the end results are a reﬂection of the quality of the ﬁne-tuning\ncorpora used, effective ﬁne-tuning typically necessitate s expensive manual review and in-domain expert curation to\nproduce meaningful improvements in the model’s responses [ 9], in addition to highly expensive hardware necessary\nto run ﬁne-tuning algorithms. In the present study, we focus on prompt engineering, a promising option that has been\nshown to produce comparable improvements without the resou rce-intensive ﬁne-tuning [1]. T o our knowledge, this is\nthe ﬁrst study on improving the accuracy of LLM delivering PS T through prompt engineering [10]\nUsage of Large Language Models as Therapeutic Chatbots. Literature on the usage of LLMs in psychotherapy\nis nascent. A few published studies have explored LLMs’ abil ity to lead a therapeutic conversation with a user [11,\n12, 10]. Fu et al. described utilizing an LLM to augment a huma n therapist, providing helpful suggestions rather\nthan singularly leading the conversation, i.e., keeping a h uman in the loop who can detect hallucinations generated\nby the LLM and increase the controllability of the system [12 ]. While such a study provides a setup well-suited\nfor current LLMs, our work explores the current limits of the models’ performances in fully autonomous settings to\nexplore the potential for full deployment. W ang et al. perfo rmed ﬁne-tuning of a GPT -2 model to generate suitable PST\nresponses [10]. However, their work did not explore prompt e ngineering methods that recently arose to prominence\ndue to their effectiveness with larger models [1], producin g results comparable to those of ﬁne-tuned models [13]. A\nrecent study [11] explored zero-shot prompt engineering to facilitate a diagnostic conversation based on the Diagnost ic\nand Statistical Manual of Mental Disorders-5 standard [14] . They explored similar questions to ours, achieving an\nincrease in the model’s empathy in responses via the use of ze ro-shot prompting. In this study, we expand on prompt\nengineering methods by using recently emerged techniques i ncluding Zero-Shot [15], Few-Shot [7], and Zero-Shot\nChain-of-Thought [16], which we will refer to as Chain-of-T hought (COT) hereafter.\nZero-shot prompting: in this setup, a model receives only a natural language instr uction to perform a task, with-\nout prior demonstrations. Relying on the model’s pre-train ed knowledge, this approach maximizes convenience and\npotential for robustness, albeit typically being the most i neffective due to the absence of examples that could clarify\nthe task’s format or expected output [15]. Few-shot prompting: in few-shot, a model is given a small number of\ninput-output pairs as a guide. This approach enables LLMs to continue generating appropriate output for similar in-\nputs without ﬁne-tuning the model’s weights [7]. Optimal ex ample selection is key in few-shot prompting. Brown et\nal. [7] demonstrated that by incorporating up to eight high- quality examples in prompts, GPT -3’s performance on var-\nious natural language benchmarks signiﬁcantly improved [7 ]. Chain-of-Thought (CoT): CoT leads LLMs through\na step-by-step reasoning process, effectively encouragin g them to “think out loud”. Although the speciﬁc style can\nvary without drastically affecting performance, its prese nce is crucial for improving problem-solving capabilities [16].\nThis technique encourages the model to consider intermedia te tokens, leading to more robust responses. CoT has\nbeen shown to signiﬁcantly boost the problem-solving perfo rmance of LLMs across complex tasks, from arithmetic to\ncommonsense challenges [16].\nMethods\nW e used various prompt engineering techniques and their com binations to improve LLM performance when deliv-\nering a PST session as a therapist bot. W e were guided by ﬁndin gs from Nori et al. who showed that by utilizing\nvarious standard prompt engineering techniques, a general ist model, in particular state-of-the-art GPT -4, can respo nd\nto medical questions with accuracy on par with models speciﬁ cally ﬁne-tuned for this task [1]. W e utilize the same\nmethods and examine whether their insights can be applied to psychotherapy. While in this study we used GPT -4\nthrough Microsoft Azure’s Application Programming Interf ace (API), at the time of writing, GPT -4 is also available\nto a wide audience as the backend of OpenAI’s ChatGPT convers ational agent. 2\nIn preliminary experiments, our team evaluated multiple pr ompting strategies in delivering PST and found large in-\nconsistencies in expected model behavior. Models produced at times low quality or generally incoherent responses\n2 https://chat.openai.com/\nwhen performing full PST . T o mitigate this, we decided to foc us on the two PST steps that were the hardest to control\nand were most prone to producing unexpected and potentially harmful dialogue outputs. The two steps are problem\nselection and identiﬁcation (in our case scenario for family caregivers, we named it caregiving symptom assessment )\nand goal setting . By merging our technical insights in prompt engineering wi th clinicians’ knowledge of PST , we de-\nsigned prompts that are speciﬁcally relevant to these two st eps of PST . As V an V een et al. [2] have previously shown,\nhuman assessments of LLM performance do not always closely c orrelate with automated metrics, especially in do-\nmains requiring expert knowledge, such as healthcare. Henc e, in addition to using an automated empathy evaluation\nmethod developed by Sharma et al. [6] to evaluate the models’ responses, we had a group of clinicians familiar with\nPST evaluate our dialogues without knowledge of whether the dialogue was LLM-derived. W e provide more details\nabout the prompt design and the evaluation process below .\nDesigning the prompts\nW e started with a naive prompt shown in T able 1 and used it as a b aseline. W e then included the three prompt engi-\nneering techniques. W e also experimented with combination s of these techniques for better downstream performance.\nT ype of Prompt Content of the Prompt\nBaseline Y our responsibility is to guide the conversation with a care giver through the principles of PST\nto improve one signiﬁcant symptom the caregiver is experien cing. Y ou will ask open-ended\nquestions to identify and assess their challenges and stres sors and improve their self-care. A void\nfocusing on the care receiver. Remember, your job is to help t he caregiver.\nWhen the caregiver asks for goal suggestions by saying ‘Can y ou suggest some goals for me?’,\ntake this as your cue to thoroughly review the conversation y ou’ve had with them. Concentrate\non identifying their unique needs and aspirations as discus sed. After this review , generate two\nconcise, achievable, and personalized goals that directly address and support their expressed\nneeds and aspirations. Ensure these goals are not only reali stic but designed to inspire and boost\nthe caregiver’s motivation.\nZero-Shot In the process of identifying and assessing the caregiver’s symptoms, assess all ﬁve aspects if it\nhas not been mentioned in the conversation:\n1. Symptom Identiﬁcation (What are the caregiver’s symptom s?)\n2. Symptom Frequency (How often do the caregiver’s symptoms occur?)\n3. Symptom Context (Where and when do the caregiver’s sympto ms occur? Are there speciﬁc\npeople present, or are certain activities involved?)\n4. Symptom Severity (On a scale of 1 to 5, how severe are the car egiver’s symptoms?)\n5. Previous Measures (What has the caregiver already tried t o alleviate the symptoms?)\nFew-Shot Below are ideal dialogue examples illustrating how you, the assistant, should evaluate and address\nthe challenges and stressors of caregivers, referred to her e as the user, during conversations.\n**Attached would be 9 excerpts from multi-turn PST therapy w ith a human therapist**\nChain-of-\nThought\nThink about the user’s input step by step.\nT able 1: Our prompts categorized by various types of prompti ng techniques used\nBaseline: Leveraging insights from the RoleLLM framework [17], which highlights substantial improvements in\nlanguage model performance through role-conditioned inst ruction tuning, we started by implementing a role-playing\napproach to augment our chatbot’s functionality as a PST ass istant. This foundational prompt within our system\nprompt architecture, shown in T able 1, is composed of two par ts which serve two distinct functions. First, it deﬁnes\nthe chatbot’s role as an assistant to caregivers, delineati ng its core functions, tasks, and desired behaviors to facil itate\nconversations aligned with PST principles, aiming at the id entiﬁcation and analysis of caregivers’ symptoms. Second,\nit ensures proper behavior of the model during the goal setti ng stage, encouraging it to provide proper goals. Zero-\nShot: T o enhance the chatbot’s ability to generate structured dia logue, following the PST protocol, we built upon\nthe baseline prompt with the instructions shown in the secon d row of T able 1, reﬂecting the sub-stages necessary for\naccomplishing symptom identiﬁcation and assessment, as we ll as the goal suggestion steps of PST . Central to our\nstrategy is including precise language explaining ﬁve pred eﬁned criteria necessary for the successful identiﬁcation\nand assessment of caregiver symptoms: Symptom Identiﬁcati on, Symptom Frequency, Symptom Context, Symptom\nSeverity, and Previous Measures [8]. This approach is an att empt to enhance the depth and relevance of the chatbot’s\ninteractions, grounding its dialogue framework in these ﬁv e essential aspects of the symptom identiﬁcation and anal-\nysis step in PST . Few-Shot: Our implementation of the few-shot prompting method incorp orates nine high-quality\nexamples curated by clinicians as in-context learning exam ples to capture the complexities of symptom identiﬁcation\nand analysis within the PST framework [18]. These examples a im to demonstrate to the model a set of ideal responses\nto realistic user inputs, providing comprehensive coverag e of the symptom identiﬁcation and analysis phase. The ﬁrst\nsix examples present single-turn dialogues addressing the 5 sub-steps of PST that are listed in the zero-shot prompt (se e\nthe second row in T able 1), spanning symptom identiﬁcation a nd assessment, including frequency, context, severity,\nand prior mitigation efforts. Additional examples featuri ng multi-turn dialogues are designed to enhance the chatbot ’s\ncapability to conduct coherent and contextually relevant c onversations within PST . The multi-turn dialogue examples\nare essential for demonstrating to the chatbot the intricac ies of ongoing therapeutic dialogue, including the need for\nfollow-up questions and the reiteration of the points made t o the user during the dialogue. Chain-of-Thought: Our\nimplementation of CoT is an attempt to systematically impro ve the chatbot’s analytical and problem-solving capabil-\nities by forcing it to be more ‘thoughtful’ in its style of out put, which can lead to better outcomes as demonstrated in\nprevious studies [1, 15].\nModel Prompt Structure Preliminary\nAssessment Evaluation\n0 Baseline Pass Proceed\n1 Baseline + Zero-Shot Fail N/A\n2 Baseline + Few-Shot Pass Proceed\n3 Baseline + Zero-Shot + Few-Shot Pass Proceed\n4 Baseline + Zero-Shot + Few-Shot + CoT Pass Proceed\n5 Baseline + Few-Shot + CoT Pass Proceed\n6 Baseline + Zero-Shot + CoT Fail N/A\n7 Human-curated rule-based system Pass Proceed\nT able 2: Overview of Model Structures\nModel Development\nW e developed the models gradually, each step aimed at mitiga ting the faults noticed at the previous stage (T able 2). W e\nproduced models of various complexity, which we then evalua ted for their ability to achieve the intended objectives.\nModel 0: A baseline model, referred to as Model 0, introduces the chat bot in its role of guiding caregivers through\nstep 3, ”Identify and Assess Symptom, ” and step 4, ”Goal Sett ing, ” following the principles of PST . Model 1 and\nModel 2: Expanding on Model 0, Models 1 and 2 improve the chatbot’s abi lity to follow PST structure by introducing\nkey prompt engineering techniques. Model 1 builds upon Mode l 0 by integrating a set of structured guidelines into the\nprompt via zero-shot prompting. This approach systematica lly guides the chatbot into assessing caregiver’s challeng es\nand stressors in a speciﬁc order. Model 2 further advances Mo del 0 through the integration of few-shot learning, which\nleverages selected single-turn and multi-turn example dia logues from previously recorded sessions to set guidelines\nfor the conversation ﬂow . Model 3 through 6: In our further explorations, inspired by the methodology of Nori et\nal. [1], we utilize various combinations of prompt engineer ing techniques across Models 3 to 6. Model 3 integrates\nzero-shot’s structured symptom assessment with few-shot l earning to produce more robust output that more closely\nfollows the intended response style. Model 4 incorporates C oT prompting to improve the models’ problem-solving\nresults. Models 5 and 6 adjust these components to explore th eir combined effects on the chatbot’s PST performance,\nwith a focus on more effective goal setting. Model 7: While not a model per se, we also curated four dialogues from\nthe rule-based chatbot baseline to compare against our meth ods following our previous work [8].\nGenerating Dialogues for Evaluation\nT o achieve a higher level of consistency and protect caregiv ers’ privacy, we used personas to generate dialogues.\nW e ﬁrst crafted four caregiver personas with exemplary repl ies based on our prior work with family caregivers. W e\nrecruited three research team members with direct professi onal experiences interacting with family caregivers. They\nwere asked to portray one or two of the personas and converse w ith each bot therapist (Model 0 through 6 in T able\n2). They were instructed to use consistent and exemplary rep lies as much as possible. W e collected 28 PST dialogues\n(4 personas * 7 models). W e randomized the sequence of the mod els and did not disclose which speciﬁc models they\nDimensions Statements or Questions Measurements\nSymptom assessment The therapist successfully assessed the\nﬁve aspects of the caregiver’s symptoms.\n5-point Likert scale\n(1 = Completely disagree;\n5 = Completely agree)\nGoal setting The goal suggested by the therapist is\nappropriate for the caregiver.\nEmotional Reactions\nThe therapist expressed emotions such as\nwarmth, compassion, concern, or similar\nfeelings towards the caregivers.\n3-point Likert scale\n(0 - therapist not expressing them at all;\n1 - therapist expressing them to\nsome weak degree;\n2 - therapist expressing them strongly.)\nW e used the framework from\nSharma et al. [6] to evaluate peer-to-peer\ndialogues for both algorithmic\nand human evaluation.\nInterpretations\nThe therapist communicated an\nunderstanding of feelings and\nexperiences inferred from the caregiver’s\nresponses.\nExplorations\nThe therapist improved their\nunderstanding of the caregiver\nby exploring the feelings and\nexperiences not stated in the\ncaregiver’s response.\nOverall Overall, how was this therapy session?\n5-point Likert scale\n(1 - very bad;\n5 - very good)\nT able 3: Evaluation components\ninteracted with. After we collected all the dialogues, the r esearch team examined the conversations and found that\nModels 1 and 6 had at least one dialogue in which the bot therap ist asked all ﬁve symptom assessment questions at\nonce. When the caregiver responded to one of the questions, t he bot therapist could not recognize that one question\nhad been answered and repeated all the questions. This behav ior was undesired and would automatically receive low\nratings in the human evaluation. Thus, we eliminated them in this preliminary step prior to human evaluation. The\nmodels that proceeded to human evaluation were Models 0, 2, 3 , 4, and 5. In addition, the research team curated a\nset of four dialogues with human-delivered PST sessions. Th ese dialogues were curated based on the four personas\nand the research team’s prior rule-based bot-delivered PST protocol [8]. In total, we had 24 dialogues (6 models * 4\npersonas) that proceeded to human evaluation.\nEvaluating Dialogues - Human Evaluation\nW e recruited seven clinicians, namely four nurses and three clinical psychologists, to evaluate the quality of the ther apy\nresponses. The evaluators were unaware of the models and eva luated the dialogues in random sequence. Evaluators\nevaluated each dialogue on two aspects, conversational qua lity and perception of relational capacity, which were\nadopted from the chatbot evaluation mechanisms by Zhang et a l. [19]. For conversational quality, since the generated\ndialogues speciﬁcally focused on the “symptom assessment” and “goal setting” steps of PST , the evaluators were\nasked to evaluate the quality of these two steps. T o evaluate the symptom assessment step, we asked the evaluators\nto consider if the therapist assessed all ﬁve aspects of symp toms. T o evaluate the goal-setting step, evaluators were\nasked to assess the appropriateness of the goals suggested b y the therapist in the dialogue. For the perception of\nrelational capacity, we focused on therapist empathy, whic h was shown to be a predictor for therapy outcome [20].\nW e employed the three communication mechanisms to measure e mpathy developed by Sharma et al. [6]: Emotional\nReactions (ER), Interpretations (IN), and Explorations (E X). Strong empathetic communication expresses emotions\nreacting to what the user said (Emotional Reaction), commun icates an understanding of the user’s feeling or experience\n(Interpretation), and explores the user’s feelings and exp eriences that are not stated (Exploration) [6]. Evaluators\nevaluated all three aspects. Additionally, we asked evalua tors to provide a rating based on the overall impression of\nthe therapy session. Details about the evaluation componen ts and questions are included in T able 3. Moreover, to\ngain a deeper understanding of the ratings and what factors c ontributed to the ratings, we also asked the evaluators to\nprovide a brief rationale for each rating.\nEvaluating Dialogues - Automatic Evaluation\nIn addition to human evaluation, we adapted the algorithm fr om Sharma et al. [6] to rate the conversations’ empathy.\nW e used the default implementation of the algorithm provide d on the authors’ GitHub repository, a base RoBER T a\nclassiﬁer trained on Reddit dialogue data labeled by the aut hors. The classiﬁer quantiﬁes the model’s Emotional\nReactions (ER), measuring its ability to express positive e motions when responding to a user’s post; Interpretations\n(IP), evaluating the model’s ability to produce a relevant s hared experience; and Explorations (EX), which captures a\nlevel of active interest and engagement with the user’s post .\nData Analysis\nEach of the six models that generated dialogues received 28 s ets of ratings (4 persona-based dialogues * 7 evaluators).\nW e computed the averages and standard deviations of the rati ngs for each model. W e report on the models with the\nhighest and lowest ratings on each evaluation component. Th ree team members conducted a rapid deductive analysis\nof the qualitative rationales provided by the evaluators to identify speciﬁc factors contributing to the ratings.\nResults\nSymptom\nAssessment\n(1-5)\nGoal\nSetting\n(1-5)\nEmotional\nReactions\n(0-2)\nInterpretations (0-2) Explorations (0-2) Overall (1-5)\nModel Human Human Human Algorithm Human Algorithm Human Algorithm Human\n0 3.15 (1.35) 4.00 (1.09) 1.56 (0.58) 0.83 (0.17) 1.59 (0.64) 0.04 (0.06) 1.50 (0.69) 0.79 (0.21) 3.68 (1.06)\n2 4.68 (0.55) 4.74 (0.45) 1.71 (0.46) 0.93 (0.08) 1.64 (0.49) 0.06 (0.10) 1.39 (0.74) 1.22 (0.08) 4.36 (0.73)\n3 4.75 (0.44) 4.46 (0.74) 1.57 (0.57) 0.92 (0.19) 1.68 (0.55) 0.11 (0.02) 1.21 (0.74) 1.29 (0.27) 3.82 (0.90)\n4 4.80 (0.41) 4.25 (0.93) 1.68 (0.48) 0.91 (0.18) 1.75 (0.44) 0.02 (0.03) 1.32 (0.61) 1.31 (0.16) 3.82 (0.79)\n5 3.70 (1.35) 3.86 (1.38) 1.64 (0.49) 0.93 (0.25) 1.71 (0.46) 0 (0) 1.35 (0.69) 0.76 (0.18) 3.61 (1.13)\n7 4.62 (0.5) 3.59 (1.01) 1.22 (0.80) 0.89 (0.14) 1.22 (0.89) 0.04 (0.06) 0.77 (0.65) 1.31 (0.13) 3.19 (1.18)\nT able 4: Evaluation results. The results are reported in Mea n (Standard Deviation) format.\nT able 4 shows the mean scores with standard deviations of bot h the human and automated evaluations. In general,\nModels 2, 3, and 4 were rated higher compared to the human-cur ated rule-based system (Model 7) in all evaluated\nareas and delivered the two steps of PST successfully. Overa ll, evaluators rated Model 2 the best, which is also the\nhighest-rated model for goal-setting and the exploration d imension of empathy. The human-curated rule-based model\nwas rated the lowest across all evaluated aspects except for symptom assessment.\nSymptom assessment. For symptom assessment, the improved models (Models 2-5) we re all able to assess a care-\ngiver’s symptom in at least four turns of dialogue. When the “ caregiver” shared a challenge such as “I’m feeling\noverwhelmed caring for my father with cancer and looking for support, ” the bot therapist would ﬁrst provide an em-\npathetic response, followed by a therapeutic component tha t continued the PST process. For example, one response\nby a bot to this speciﬁc “caregiver” was, “Hello, I’m really glad you reached out for support. It’s inc redibly impor-\ntant to take care of yourself, especially when you’re in a car egiving role. It sounds like you’re going through a lot\nright now . Can you tell me more about what speciﬁcally is maki ng you feel overwhelmed?” The average rating for\nsymptom assessment across the four models was 4.5 out of 5, in dicating that “the therapist successfully assessed the\nﬁve aspects of the caregiver’s symptoms”. Model 4, with all p rompt engineering techniques, scored the highest for\nsymptom assessment (mean = 4.80). The majority of the evalua tors thought Model 4 successfully assessed all aspects\nof the caregiver’s symptoms and had good symptom identiﬁcat ion. Model 0 using only baseline prompting was rated\nthe lowest. Evaluators reported that this bot missed assess ing certain aspects of caregivers’ symptoms, such as severi ty\nand frequency, and had poor follow-up questions on context. Moreover, several evaluators mentioned that the bot\nasked multiple questions at a time and made it overwhelming. T ables 5 and 6 show example bot responses from the\nmodels with the highest and lowest rating for each criterion , including symptom assessment.\nGoal setting. When suggesting health goals for the caregivers to improve a n identiﬁed symptom, Model 2 was\nrated the best (mean = 4.74). Evaluators’ feedback on Model 2 included keywords “personalized”, “collaborative”,\n“realistic”, “appropriate”, “speciﬁc”, “straightforwar d”, “reasonable”, “relevant”, “well-structured”, and “de tailed”.\nModel 7 (i.e. human-curated rule-based model) was rated the lowest (mean=3.59). Evaluators pointed out a few\ndrawbacks including being “not clear”, “not speciﬁc”, “sup erﬁcial”, and “providing only one suggestion”. Sample\nEvaluation\nAspect\nHuman\nRating Model Example Chatbot Response\nSymptom\nAssessment\nHighest\n(mean=4.80)\n4\n“It sounds like you’re dealing with some intense emotions. I want to acknow-\nledge how challenging it can be to manage fear and sadness, es pecially in a\ncaregiving role. T o better understand and help you with thes e feelings, could\nyou tell me how often you experience these feelings of fear an d sadness?”\nLowest\n(mean=3.15)\n0\n“It sounds like you’re carrying a heavy load both emotionall y and practically.\nLet’s address these one at a time. Firstly, regarding emotio nal support, can\nyou tell me about your current support system? Do you have fri ends, family,\nor a community you can talk to about what you’re going through ? Secondly,\nfor the practical help with caregiving tasks, have you been a ble to identify\nspeciﬁc tasks that are particularly challenging or time-co nsuming? Are there\nroutine aspects of care that you think someone else could ass ist you with?”\nGoal\nSetting\nHighest\n(mean=4.74)\n2\n“Excellent. Since you’re open to online support groups and y ou’re also looking\nto enhance your emotional support, here are two personalize d goals that might\nbe beneﬁcial for you: 1. **Join an Online Support Group**: Wi thin the next\nweek, aim to identify and join at least one online support gro up for caregivers.\nSpend a few minutes each day, or as your schedule allows, to en gage with the\ngroup. This can be a place where you can share your experience s, receive\nadvice, and connect with others who understand what you’re g oing through.\n2. **Schedule Regular Check-ins**: Set up a weekly check-in with a friend or\nfamily member who can offer emotional support. This could be a phone call,\nvideo chat, or even a text exchange, where you can share your f eelings and\nchallenges. Having a set time each week for emotional check- ins can provide\nyou with something to look forward to and ensure you have a con sistent outlet\nfor your emotions. How do these goals sound to you? Are they so mething you\nfeel you can commit to?”\nLowest\n(mean=3.59)\n7\n“Thanks for being open to it. Which of the two would you like to try for this\nweek? 1. Journal writing.\n2. Practice the 4-7-8 deep breathing exercise technique . ”\nT able 5: Symptom Assessment and Goal Setting Examples\nchatbot responses from Model 2 and Model 7 for goal setting ar e included in T able 5. In the same persona mentioned\nabove, one bot therapist made the following goal recommenda tion: “Let’s work on setting some goals that can help\nyou manage these overwhelming feelings more effectively. B ased on what you’ve shared, here are two personalized\ngoals that you might consider: 1. Establish a Daily Unwindin g Ritual: Given that your feelings of fear and sadness\nintensify during the evenings and nights, creating a calmin g evening routine could be beneﬁcial. This might include\nactivities like meditation, listening to soothing music, o r practicing gentle yoga. The goal would be to engage in this\nritual every evening for at least 15 minutes to help transiti on into a more relaxed state of mind before bedtime. 2. Seek\nEmotional Support: It’s important to have a support system i n place. Y ou could set a goal to reach out to a friend,\nfamily member , or a support group for caregivers at least onc e a week. Sharing your experiences and feelings with\nothers who understand can provide comfort and may offer new p erspectives or coping strategies. How do these goals\nsound to you?”\nEmpathy . For the three dimensions of empathy, the human-curated rule -based model was rated consistently as the\nlowest across all three dimensions, with other models being rated similarly. Model 2 scored the highest in ER (mean\n= 1.71), followed by Model 4 (mean = 1.68). One evaluator expr essed approval of the emotional reactions exhibited\nby the therapist bot powered by Model 2, stating “I think the empathy came across better in this one than some o thers\nbecause it made a statement and then asked a question. It didn ’t pile questions on top of questions like in some of\nthe prompts... it makes the conversation seem less empathet ic and two sided. ” Model 4 was rated the highest in the\nIP (mean = 1.75). The majority of the evaluators expressed th at there was good use of empathetic statements and\nreﬂections. One evaluator provided an example of “an excell ent follow-up on feeling/experiences and insights into\nEvaluation\nAspect\nHuman\nRating Model Example Chatbot Response\n(Algorithm rating on a scale of 0-2)\nEmotional\nReactions\nHighest\n(mean=1.71)\n2\n“That must be quite exhausting for you. Not getting enough sl eep can\ncertainly exacerbate stress and pain. T o better understand your\nsituation, can you tell me more about the context of your inso mnia?\nFor example, are there speciﬁc thoughts or worries that keep you\nawake, or is it the pain that’s more disruptive?” (Algorithm rating: 1)\nLowest\n(mean=1.22)\n7\n“I can see that. I have one more question to ask about your\noverwhelmed feeling. Have you done anything to help you with your\noverwhelmed feeling?” (Algorithm rating: 1)\nInterpretations\nHighest\n(mean=1.75)\n4\n“That sounds really tough, being at home and feeling that hei ghtened\nawareness of your daughter’s anxiety and the challenges in c onnecting\nwith her. ” (Algorithm rating: 0)\nLowest\n(mean=1.22)\n7\n“I hear you. Let’s use the Problem Solving Therapy (PST) proc ess that\nI mentioned to you to explore how we may address this. ”\n(Algorithm rating: 0)\nExplorations\nHighest\n(mean=1.50)\n0\n“That sounds quite challenging and must be affecting your da ily\nfunctioning. Not being able to sleep well for 4-6 nights a wee k is\nsigniﬁcant. Can you share more about the context of these sle epless\nnights? For example, are there speciﬁc thoughts related to y our\ndaughter’s illness or your pain that tend to keep you awake?”\n(Algorithm rating: 2)\nLowest\n(mean=0.77)\n7\n“Great. Let’s use the Problem Solving Therapy (PST) process that I\nmentioned to you to explore how we may address this. ”\n(Algorithm rating: 2)\nT able 6: Emotional Reactions, Interpretations, and Explor ations Examples\nthe feelings” in which the bot responded “It can be harder at night when it’s quiet. ” For the Exploration dimension\nof empathy, evaluators rated Model 0 the highest (mean = 1.50 ). One evaluator expressed that “In this session, the\ntherapist does more open ended questions that get to more of t he root of the problem. ” For the lowest rated human-\ncurated rule-based model across all three dimensions of emp athy, evaluators reported that the bot said things like\n“got it” that demonstrated poor empathy and regurgitated sentences like “I am so sorry to hear that ... ” , which made\nit feel robotic. Several evaluators also reported that the h uman-curated rule-based model displayed “zero or very\nlimited explorations of deeper thoughts and feelings. ” Mor eover, the bot was perceived as “fake and too cheerful. ”\nOne evaluator said that “It used the word ‘glad’ very often and also thanked the careg iver quite often. It’s important\nfor the bot to in some ways mirror the caregiver , meet them whe re they are at. And someone worrying 4 out of 5 may\nnot want it to seem overly cheerful. ” According to the automated evaluation on empathy, all model s performed very\nsimilarly in terms of ER, with an average close to 1, meaning t hat the therapist expressed emotional reactions to some\nweak degree. Rarely were our models nor the human dialogue ab le to demonstrate Interpretations as all models were\nscored close to 0 by the automated evaluation algorithm. The majority of the models, except for Model 0 and Model\n5, demonstrated moderate empathy in Explorations, with Mod els 4 and the human-curated rule-based model scored\nthe highest (mean = 1.31) by the automated evaluation.\nDiscussion\nIn this study, we used various prompting techniques to impro ve GPT -delivered PST for family caregivers and used\nboth human and automatic algorithms to evaluate the therapy dialogues. W e found that by using prompt engineering\ntechniques, we were able to improve the quality of the therap y conversations beyond the baseline prompt, but with\nconsiderable limitations. Empathy evaluated by both human and the algorithm did not vary signiﬁcantly across models,\ndespite improving over the baseline in the emotional reacti on and interpretation dimensions. Below , we discussed\nspeciﬁc prompt engineering techniques and their performan ces.\nZero-shot learning did not perform as well as the other techn iques that we evaluated. Our ﬁnding that explicit direction s\nmay not be sufﬁcient to adapt a model to a domain-speciﬁc task is in line with the literature [1, 7]. In our scenario, the\ndifference in performance between zero-shot learning and t he other techniques was more pronounced because of the\nnature of the downstream task, i.e. PST being a protocolized therapy. PST requires a speciﬁc way of conversing with\nthe user. Zero-shot prompting focuses on explicitly deﬁnin g tasks for the model to follow , but many aspects of what\nconstitutes good therapy are implicit (e.g. more actionabl e advice being preferred over more generic, overly optimist ic\ncomments being preferred less [6]). The possible approache s to achieve the explicit goals are almost endless. Explicit ly\ndemonstrating examples of high-quality PST responses as pa rt of few-shot prompt led to better model performance,\nshowing that providing the model with data is still necessar y to produce coherent dialogues. Our ﬁndings align with\nprevious results from the literature, that show that provid ing few-shot examples typically noticeably increases the\nmodels’ performance compared to zero-shot baselines on tas ks involving the generation of coherent word sequences,\nakin to how PST demands a coherent dialogue [7].\nAdding CoT resulted in better empathy, especially explorat ion. However, it reduced the quality of symptom identi-\nﬁcation and goal setting. The original CoT work by W ei et al. [ 16] did not explore dialogue-generating tasks, and it\nis possible that we could experiment with alternative CoT pr ompts that would improve empathy while maintaining\ntask quality. However, given that LLMs are fundamentally ne xt-word predictors trained on large corpora of text, it\nis possible that in the pre-training data of the model, chunk s of text beginning with phrases similar to our prompt\nhappened to be more exploratory and less actionable. W e ﬁnd t hat explanation reasonable because, in human writings,\ntexts nudging someone to think tend to be more contemplative than action-driven in nature.\nIn line with previous work [2], we found low agreement betwee n the human and automatic evaluations. This may be\npartially due to the model by Sharma et al. [6] trained on sing le-turn Reddit posts, presenting a different setting than\nPST . Future studies should examine the generalizability of the automatic evaluation algorithms with domain shifts.\nAlthough the LLM-delivered therapy dialogues received hig her scores than the human-curated baseline, it is worth\nnoting that the human baseline is not actual therapist-deli vered PST dialogues. W e adapted dialogues the team created\nfor developing a rule-based chatbot to deliver PST to family caregivers. Some responses were designed to be generic\nsuch as using “I am sorry to hear that” instead of a more empathetic response using psychotherapeu tic techniques\nsuch as normalizing (e.g., “I am sorry to hear that. Many caregivers feel isolated while caring for an ill child. ” ).\nIn our study, to protect real caregivers’ privacy, we used a p ersona-based approach to collect dialogues. It would\nbe preferable to engage with actual caregivers. In the futur e, we plan to develop privacy-preserving technologies in\norder to guarantee the privacy of the user’s information whe n training or interacting with LLMs, so that our in-context\nlearning processes do not expose users’ private informatio n to commercially-hosted LLMs. This study intended to\nexplore the ability of off-the-shelf LLMs with prompt engin eering to deliver part of PST and achieved promising\nresults. In the future, we will continue to improve the model s to deliver full PST by performing ﬁne-tuning and\nRetrieval Augmented Generation (RAG) techniques. In the af orementioned reading comprehension task [7], we can\nsee that ﬁne-tuned models still can outperform generalists with few shot examples. However, recent studies [1, 13]\nshowed that it is not always true, possibly due to the increas ed knowledge contained in the larger models, allowing\nthem to perform on-par with or better than ﬁne-tuned models. W e will continue to explore the best techniques or\ncombinations of techniques that enable an automated chatbo t to deliver a full PST session.\nConclusion\nIn this paper, we adapt multiple novel prompt engineering ap proaches to improve an LLM’s ability to deliver part of a\npsychotherapy session. Consistent with previous ﬁndings [ 11], we demonstrate that the model’s capability to deliver\nprotocolized therapy can be improved with the proper use of p rompt engineering methods, albeit with limitations.\nThrough both automatic and human evaluation, we show an impr ovement over the baseline model after applying our\nmethods to PST , demonstrating that some prompt engineering techniques are better at improving the performance of\nthe models than others. Hence, while the current models cann ot be deployed directly in psychotherapy settings without\nhuman oversight, this work contributes to the effort in expl oring the potential of LLMs as a therapeutic tool. As such,\nthis work represents an important step toward using LLMs to a ddress the limited availability of human therapists in\nthe context of an escalating need for mental health services .\nAcknowledgments. The authors would like to thank Microsoft for the UW Azure Clo ud Computing Credits for\nResearch program. Daniil Filienko is a Carwein-Andrews Dis tinguished Fellow . This research was, in part, funded\nby the National Institutes of Health (NIH) Agreement No.1OT 2OD032581 and R21NR020634, and the Rita and Alex\nHilman Foundation Emergant Innovation Grant. The views and conclusions contained in this document are those of\nthe authors and should not be interpreted as representing th e ofﬁcial policies, either expressed or implied, of the NIH.\nReferences\n1. Nori H, Lee YT , Zhang S, Carignan D, Edgar R, Fusi N, et al. Ca n Generalist Foundation Models Outcompete\nSpecial-Purpose Tuning? Case Study in Medicine. CoRR. 2023 ;abs/2311.16452.\n2. V an V een D, V an Uden C, Blankemeier L, Delbrouck JB, Aali A, Bluethgen C, et al. Adapted Large Language\nModels Can Outperform Medical Experts in Clinical T ext Summ arization. Nature Medicine. 2024;(4):1134-42.\n3. Holmes E, Ghaderi A, Harmer C, Ramchandani P , Cuijpers P , M orrison A, et al. The Lancet Psychiatry Commis-\nsion on psychological treatments research in tomorrow’s sc ience. The Lancet. 2018 Mar;5(3):237-86.\n4. Patel V , Saxena S, Lund C, Thornicroft G, Baingana F , Bolto n P , et al. The Lancet Commission on global mental\nhealth and sustainable development. The Lancet. 2018;392( 10157):1553-98.\n5. Althoff T , Clark K, Leskovec J. Natural Language Processi ng for Mental Health: Large Scale Discourse Analysis\nof Counseling Conversations. Transactions of the Associat ion for Computational Linguistics. 2016 05;4.\n6. Sharma A, Miner A, Atkins D, Althoff T . A Computational App roach to Understanding Empathy Expressed in\nT ext-Based Mental Health Support. In: W ebber B, Cohn T , He Y , Liu Y , editors. Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (E MNLP). Online: Association for Computational\nLinguistics; 2020. p. 5263-76.\n7. Brown T , Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , e t al. Language Models are Few-Shot Learners.\nIn: Advances in Neural Information Processing Systems. vol . 33; 2020. p. 1877-901.\n8. Kearns WR, Kaura N, Divina M, V o C, Si D, W ard T , et al. A Wizar d-of-Oz Interface and Persona-based Method-\nology for Collecting Health Counseling Dialog. In: Extende d Abstracts of the 2020 CHI Conference on Human\nFactors in Computing Systems. CHI EA ’20. Association for Co mputing Machinery; 2020. p. 1–9.\n9. Zhou C, Liu P , Xu P , Iyer S, Sun J, Mao Y , et al. LIMA: Less Is Mo re for Alignment. In: Advances in Neural\nInformation Processing Systems. vol. 36; 2023. p. 55006-21 .\n10. W ang L, Mujib MI, Williams J, Demiris G, Huh-Y oo J. An Eval uation of Generative Pre-Training Model-based\nTherapy Chatbot for Caregivers. ArXiv. 2021;abs/2107.131 15.\n11. Chen S, Wu M, Zhu KQ, Lan K, Zhang Z, Cui L. LLM-empowered Ch atbots for Psychiatrist and Patient Simu-\nlation: Application and Evaluation. CoRR. 2023;abs/2305. 13614.\n12. Fu G, Zhao Q, Li J, Luo D, Song C, Zhai W , et al. Enhancing Psy chological Counseling with Large Language\nModel: A Multifaceted Decision-Support System for Non-Pro fessionals. arXiv preprint arXiv:230815192. 2023.\n13. Ovadia O, Brief M, Mishaeli M, Elisha O. Fine-Tuning or Re trieval? Comparing Knowledge Injection in LLMs.\nArXiv. 2023;abs/2312.05934.\n14. Diagnostic and Statistical Manual of mental disorders : DSM-5™. 5th ed. W ashington, DC: American Psychiatric\nPublishing, a Division of American Psychiatric Associatio n; 2013.\n15. W ei J, Bosma M, Zhao V , Guu K, Y u A W , Lester B, et al.. Finetu ned Language Models are Zero-Shot Learners;\n2022. 10th International Conference on Learning Represent ations (ICLR).\n16. Kojima T , Gu SS, Reid M, Matsuo Y , Iwasawa Y . Large languag e models are zero-shot reasoners. In: Proceedings\nof the 36th International Conference on Neural Information Processing Systems. vol. 35; 2022. p. 22199-213.\n17. W ang ZM, Peng Z, Que H, Liu J, Zhou W , Wu Y , et al. RoleLLM: Be nchmarking, Eliciting, and Enhancing\nRole-Playing Abilities of Large Language Models. In: Findi ngs of the Association for Computational Linguistics\nACL 2024. Association for Computational Linguistics; 2024 . p. 14743-77.\n18. Y uwen W , Chang J, Divina M, Fan X, Kearns W , Peredo A, et al. Comparing Caregiving Needs in Asian and\nWhite Family Caregivers through a Journaling Exercise Deli vered by a Conversational Agent. American Medical\nInformatics Association (AMIA) Annual Symposium Proceedi ngs. 2022;2022:1208-16.\n19. Zhang J, Oh YJ, Lange P , Y u Z, Fukuoka Y . Artiﬁcial Intelli gence Chatbot Behavior Change Model for Designing\nArtiﬁcial Intelligence Chatbots to Promote Physical Activ ity and a Healthy Diet: V iewpoint. Journal of Medical\nInternet Research. 2020 Sep;22(9):e22845.\n20. Elliott R, Bohart AC, W atson JC, Murphy D. Therapist empa thy and client outcome: An updated meta-analysis.\nPsychotherapy. 2018 Dec;55(4):399-410.\nThis figure \"example_dialogues.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.00112v1\nThis figure \"example_responses.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.00112v1\nThis figure \"p1_example_dialogues.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.00112v1\nThis figure \"sample_sentencelength.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2409.00112v1",
  "topic": "Empathy",
  "concepts": [
    {
      "name": "Empathy",
      "score": 0.6601501107215881
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5707512497901917
    },
    {
      "name": "Economic shortage",
      "score": 0.5491356253623962
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5251674056053162
    },
    {
      "name": "Session (web analytics)",
      "score": 0.5026271343231201
    },
    {
      "name": "Health care",
      "score": 0.4796912372112274
    },
    {
      "name": "Identification (biology)",
      "score": 0.44849538803100586
    },
    {
      "name": "Computer science",
      "score": 0.44587400555610657
    },
    {
      "name": "Psychology",
      "score": 0.38520121574401855
    },
    {
      "name": "Psychotherapist",
      "score": 0.32446151971817017
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2433720827102661
    },
    {
      "name": "Psychiatry",
      "score": 0.14709201455116272
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Government (linguistics)",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210150356",
      "name": "University of Washington Tacoma",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I107672454",
      "name": "Dartmouth College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}