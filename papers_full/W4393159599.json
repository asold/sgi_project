{
  "title": "SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation",
  "url": "https://openalex.org/W4393159599",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3015560738",
      "name": "Malyaban Bal",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2112635384",
      "name": "Abhronil Sengupta",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A3015560738",
      "name": "Malyaban Bal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112635384",
      "name": "Abhronil Sengupta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2784121737",
    "https://openalex.org/W6767563556",
    "https://openalex.org/W3139657805",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W3211402766",
    "https://openalex.org/W6678987828",
    "https://openalex.org/W6788001715",
    "https://openalex.org/W6973559833",
    "https://openalex.org/W3007283957",
    "https://openalex.org/W3007926765",
    "https://openalex.org/W2984844508",
    "https://openalex.org/W2962804204",
    "https://openalex.org/W2527798464",
    "https://openalex.org/W2786965798",
    "https://openalex.org/W3157350647",
    "https://openalex.org/W3203313732",
    "https://openalex.org/W6966769059",
    "https://openalex.org/W4365460623",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2972831213",
    "https://openalex.org/W4300980424",
    "https://openalex.org/W4386071675",
    "https://openalex.org/W2130459697",
    "https://openalex.org/W2745933219",
    "https://openalex.org/W4366460234",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2970900903",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W3214305874",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2964338223",
    "https://openalex.org/W4385573119",
    "https://openalex.org/W4322759345",
    "https://openalex.org/W3037354233",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W4385768188"
  ],
  "abstract": "Large language Models (LLMs), though growing exceedingly powerful, comprises of orders of magnitude less neurons and synapses than the human brain. However, it requires significantly more power/energy to operate. In this work, we propose a novel bio-inspired spiking language model (LM) which aims to reduce the computational cost of conventional LMs by drawing motivation from the synaptic information flow in the brain. In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient. The steady-state convergence of the spiking neurons also allows us to design a spiking attention mechanism, which is critical in developing a scalable spiking LM. Moreover, the convergence of average spiking rate of neurons at equilibrium is utilized to develop a novel ANN-SNN knowledge distillation based technique wherein we use a pre-trained BERT model as “teacher” to train our “student” spiking architecture. While the primary architecture proposed in this paper is motivated by BERT, the technique can be potentially extended to different kinds of LLMs. Our work is the first one to demonstrate the performance of an operational spiking LM architecture on multiple different tasks in the GLUE benchmark. Our implementation source code is available at https://github.com/NeuroCompLab-psu/SpikingBERT.",
  "full_text": "SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit\nDifferentiation\nMalyaban Bal, Abhronil Sengupta\nSchool of Electrical Engineering and Computer Science\nThe Pennsylvania State University\nUniversity Park, PA 16802\nmjb7906@psu.edu, sengupta@psu.edu\nAbstract\nLarge language Models (LLMs), though growing exceed-\ningly powerful, comprises of orders of magnitude less neu-\nrons and synapses than the human brain. However, it requires\nsignificantly more power/energy to operate. In this work, we\npropose a novel bio-inspired spiking language model (LM)\nwhich aims to reduce the computational cost of conventional\nLMs by drawing motivation from the synaptic information\nflow in the brain. In this paper, we demonstrate a frame-\nwork that leverages the average spiking rate of neurons at\nequilibrium to train a neuromorphic spiking LM using im-\nplicit differentiation technique, thereby overcoming the non-\ndifferentiability problem of spiking neural network (SNN)\nbased algorithms without using any type of surrogate gradi-\nent. The steady-state convergence of the spiking neurons also\nallows us to design a spiking attention mechanism, which is\ncritical in developing a scalable spiking LM. Moreover, the\nconvergence of average spiking rate of neurons at equilibrium\nis utilized to develop a novel ANN-SNN knowledge distil-\nlation based technique wherein we use a pre-trained BERT\nmodel as “teacher” to train our “student” spiking architec-\nture. While the primary architecture proposed in this paper\nis motivated by BERT, the technique can be potentially ex-\ntended to different kinds of LLMs. Our work is the first\none to demonstrate the performance of an operational spik-\ning LM architecture on multiple different tasks in the GLUE\nbenchmark. Our implementation source code is available at\nhttps://github.com/NeuroCompLab-psu/SpikingBERT.\nIntroduction\nLarge language Models (LLMs) are becoming increasingly\npopular because of its broad applications in a variety of\nnatural language processing (NLP) tasks. LLMs like GPT-\n3 (Brown et al. 2020) has shown additional characteristics\nsuch as emergent abilities (Wei et al. 2022) which can only\nbe realized once the model size/compute increases above a\ncertain threshold. Recent times have witnessed commercial\ndeployment of LLMs enabling worldwide reach and pos-\nitively impacting real-world users. However, the immense\npower of LLMs comes at the cost of huge energy con-\nsumption both during the computationally expensive train-\ning phase as well as the inference phase. LLMs are charac-\nterized by large number of trainable parameters and are usu-\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nally very deep. In order to alleviate the operational complex-\nity of LLMs, we aim to draw motivation from the brain. In-\ntegrating the mechanism and knowledge embodied in LLMs\ninto brain-inspired neural models hold immense promise for\ncreating a bio-plausible and energy-efficient solution.\nSpiking neural networks (SNNs) (Ghosh-Dastidar and\nAdeli 2009) are biologically inspired and communication\nbetween two neurons in an SNN architecture occurs in the\nform of spikes. This sparse spike-based information flow en-\nables event-driven computation and communication in neu-\nromorphic hardware, thereby resulting in significant energy\nsavings (Sengupta et al. 2019). SNN based architectures\nhave been also tested extensively on neuromorphic hardware\nlike Intel’s Loihi 2 processor (Davies et al. 2021) and have\ndemonstrated orders of magnitude energy efficiency.\nScaling SNNs to complex domains, like NLP, poses sig-\nnificant challenges mainly due to the absence of scalable and\nefficient learning algorithms. The complexity of the tasks,\ncoupled with the growing depth of the required model ar-\nchitectures, renders the practical application of BPTT infea-\nsible. In this work, backed by robust theoretical foundations\nand empirical evidence, we explore a scalable framework for\ntraining spiking LMs. We consider our spiking LM as a dy-\nnamical system that, given an input, progressively converges\nto a steady-state (over Tconv time steps). Similar to most su-\npervised learning algorithms, training is done in two phases,\nviz. “forward” and “backward”. However, instead of learn-\ning through unrolling the computational graph over the oper-\nated time steps (like in BPTT), we leverage the convergence\nof the average spiking rate (ASR) of the neurons to an equi-\nlibrium state during the “forward” phase of learning. Upon\nconvergence, we can derive fixed-point equations from the\nunderlying model and subsequently employ implicit differ-\nentiation on the attained steady-state to train the model pa-\nrameters effectively as described later on.\nTraining using implicit differentiation is primarily used\nin deep equilibrium models (DEQ) (Bai, Kolter, and Koltun\n2019). Recently, this method has also been used for training\nof convolution based spiking architectures (Xiao et al. 2021)\nfor vision related tasks. This methodology offers exceptional\nmemory efficiency during training unlike BPTT, which re-\nquires a huge amount of memory to store a large computa-\ntional graph. It also eliminates the necessity of surrogate gra-\ndient methods by implicitly calculating gradients, thereby\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10998\ncircumventing the non-differentiability problem of spiking\nmodels. Under certain constraints (Bai, Kolter, and Koltun\n2019), this form of learning is similar to bio-plausible and\nenergy-based training methods like equilibrium propagation\n(Scellier and Bengio 2017; Bal and Sengupta 2022), thus\nbolstering a neuromorphic viewpoint of learning.\nIn transformer (Vaswani et al. 2017) based LMs as dis-\ncussed in this paper, the attention mechanism serves as a vi-\ntal component. However, vanilla attention mechanism is fun-\ndamentally non-spiking in nature, as it relies on sequences\nof real-valued vectors for the Query, Key, and Value compo-\nnents. In this paper, we present a spiking attention mecha-\nnism that utilizes spike-based inputs and operates over the\nnumber of time steps (T conv) required for model conver-\ngence. The convergence of ASR of the neurons at equilib-\nrium allows us to draw a close equivalence between the ASR\nof the spiking attention layer and vanilla attention.\nTraining LMs from scratch is a significant time and\nresource-intensive process. The additional overheads of\ntraining a spiking LM from scratch prompted us to seek out\nmore proficient approaches for training our model. Knowl-\nedge distillation (KD) (Hinton, Vinyals, and Dean 2015) al-\nlows for faster and efficient transfer of knowledge from a\ntrained “teacher” model to a possibly smaller in size “stu-\ndent” model. In this paper, we leverage the steady state\nASR of the spiking LM and propose a novel ANN-SNN\nKD framework involving the ASR at equilibrium of specific\nintermediate layers of the “student” model and the activa-\ntion of target layers of a larger pre-trained “teacher” model.\nMoreover, the feasibility of model training during distilla-\ntion is enabled by the previously discussed training method.\nThe primary contributions of our work are as follows:\n• SpikingBERT with Spiking Attention: We propose a\nfully-operational spiking LM, following the architecture\nof BERT (Devlin et al. 2018), and evaluate it against dif-\nferent tasks (classification and regression) of the GLUE\nbenchmark (Wang et al. 2018). We also propose an effi-\ncient spiking attention mechanism whose ASR at equi-\nlibrium approximates vanilla non-spiking attention.\n• Spiking LM Training: We theoretically and empiri-\ncally verify the convergence of our proposed spiking\nLM (comprising both linear and non-linear operations)\nto equilibrium state and use an implicit differentiation\nbased method to overcome the non-differentiability issue\nof SNN training and reduce memory usage during train-\ning. This method enables training of Spiking LMs that\nsurpass the scale of existing spiking models, thereby al-\nlowing development of deeper models for complex tasks.\n• ANN-SNN KD using Equilibrium States: We leverage\nthe equilibrium state of the neurons after convergence,\nto train our model more effectively using a novel ANN-\nSNN KD framework. This allows for developing an ef-\nficient and smaller spiking “student” model using larger\nBERT models as its “teacher”.\nRelated Works\nSpiking Architectures: Spiking architectures (Sengupta\net al. 2019; Lee et al. 2020; Xiao et al. 2021; Zhou et al.\n2022) have primarily been explored for vision based datasets\nsuch as CIFAR-100 (Krizhevsky, Nair, and Hinton 2009),\nImageNet (Deng et al. 2009), among others as well as\nneuromorphic datasets such as NMNIST (Orchard et al.\n2015), DVS Gesture (Amir et al. 2017). However, lim-\nited work has been done for sequence based NLP tasks.\nWhile most of the networks are non-attention based shal-\nlow models (Alawad, Yoon, and Tourassi 2017), recently\n(Zhu, Zhao, and Eshraghian 2023) explored developing a\nGPT-like model using linear attention. While GPT (Rad-\nford et al. 2018) is a decoder-only architecture, BERT is\nan encoder-only architecture whose ability to capture bi-\ndirectional contextual information makes it more suitable for\ntext classification problems. Unlike our SpikingBERT, the\nSpikeGPT model uses surrogate gradient to overcome the\nnon-differentiability problem and uses a BPTT approach for\ntraining. Furthermore, SpikingBERT is the first spiking LM,\nto the best of our knowledge, that has been evaluated against\nmultiple different tasks in the GLUE benchmark. Moreover,\ndue to the efficient KD incorporated in our approach, we can\nenhance model performance without the need for an exten-\nsive number of parameters. There has been some work on\nKD in SNNs previously, however all of them primarily ex-\nplored BPTT based methods and focused solely on simple\nvision based datasets (Xu et al. 2023; Takuya, Zhang, and\nNakashima 2021; Hong et al. 2023).\nEfficient LMs: Given the increasingly growing scale of\nLMs, research focusing on attempting to make them com-\nputationally less expensive and smaller in size have gained\nsignificant attention. TinyBERT (Jiao et al. 2019) proposed\nextracting knowledge from the “teacher” model - from both\nthe intermediate layers and the prediction layer. NAS-BERT\n(Xu et al. 2021) does model compression using neural ar-\nchitecture search. Work has also been done on distilling\nknowledge from BERT to a single layer BiLSTM network\n(Tang et al. 2019). Moreover, research endeavours have been\nmade in methods like quantization (Kim et al. 2021), prun-\ning (Kurtic et al. 2022), etc. to reduce the model complexity.\nBecause appropriate neuromorphic baselines are currently\nunavailable, we compare our proposed model with existing\nstandard NLP models and efficient LMs.\nMethods\nIn this section, we will begin by examining the foundational\nprinciples of our method. Subsequently, we will delve into\nthe architectural details to provide a comprehensive under-\nstanding. We delve into the theoretical and empirical foun-\ndations underlying the convergence of ASR in SNNs trained\nusing implicit differentiation. Furthermore, we present an in-\nnovative approach to harness this convergence for the design\nof a spiking attention mechanism and a novel KD mecha-\nnism leveraging a pre-trained BERT model as a “teacher”,\nthereby enhancing the learning process. We also elaborate\non the framework for using implicit differentiation based\ntechnique to train our spiking architecture.\nSpiking Neural Networks\nThe fundamental building block of the proposed spiking ar-\nchitecture comprises of leaky integrate and fire (LIF) neu-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n10999\nInput\nSpiking Encoder\nLayer 1\nSpiking Encoder\nLayer 2\nSpiking Encoder\nLayer N\nOutput\nInput (Spikes)\nSpiking Attention\nAdd &Norm \nLinear\nFeed-Forward\nNetwork\nAdd &Norm \nLinear\nFeedback\nConnection\nF\nEmbed\nS\nS\nS\nInput (Spikes)\nQuery\nSS\nAttention\nMechanism\nS\nW1\nW2\nWn\nTime\nTime\nTime\nTime\nPre-synaptic Spike\nTrains\nPost-synaptic\nSpike Train\nVth\nU\nTime\nS : Spike Generator\n(LIF neurons)\nIL-1\nIL-2\nOutput\nKey Value\nFigure 1: High-level overview of the SpikingBERT model.\nDuring the “forward” phase of learning, the network is sim-\nulated over Tconv time steps, i.e., until the ASR of the neu-\nrons of each layer converges to an equilibrium. Information\nflow both within and between two spiking encoders occur\nusing spikes instead of real values, thereby mimicking event-\ndriven information flow in bio-inspired systems.\nrons. The internal dynamics of a simple LIF neuron is simi-\nlar to a biological neuron and is given as follows,\nτm · du\ndt = −(u(t) − urest) + R · I(t) (1)\nwhere, u is the membrane potential; urest is the resting\nmembrane potential; I(t) is the input voltage at timet scaled\nby a constant, R, representing resistance; τm is the time con-\nstant. Moreover, if u > Vth, then the neuron potential is up-\ndated by subtracting Vth and the neuron emits a spike, i.e., it\nsends out a ‘1’ else ‘0’. A suitable discrete time representa-\ntion of the dynamics for the ith neuron, can be described as\nfollows,\nui[t + δ] = γui[t] +\nX\nj\n(wijsj[t]) + bi,\nsi[t + 1] = S(ui[t + δ]),\nui[t + 1] = ui[t + δ] − Vthsi[t + 1]\n(2)\nwhere, γ is the leaky term related to the constant τm in Eqn.\n1 (for LIF neurons, we keep γ < 1 and for IF γ = 1); sj\nis the spike from the jth input neuron; wij is the synaptic\nweight of the connection between the pre-synaptic and post-\nsynaptic neurons; t + δ represents an intermediate time step\nrepresentation to determine if the neuron has fired; bi rep-\nresents a bias term; S is the non-differentiable function for\nspike generation and subtraction is used as reset operation.\nImplicit Modeling\nImplicit modeling takes a different approach by not explic-\nitly defining the precise computation of a model’s output\nfrom its input. Instead, it relies on imposing specific con-\nstraints on the model, ensuring that these constraints are met\nto achieve the desired results. For example, consider a sim-\nple model represented by a function h. In order to formulate\nan explicit model with input x ∈ X and output z ∈ Z, the\nfollowing computation is performed: z = h(x). However,\nfor formulating it implicitly, a function g : X × Z → Rn is\ndefined, such that g(x, z) = h(x) −z and the goal will be to\nfind the root of the equation: g(x, z) = 0. While this simple\nexample demonstrates algebraic equations, these method-\nologies can be extended to fixed-point equations, thereby\npaving the way for the development of DEQ (Bai, Kolter,\nand Koltun 2019).\nLet us consider a fixed-point equation of the form z =\nfθ(z), where θ is the set of parameters. This fixed point\nequation converges over time, i.e., after Tconv time steps\nzTconv = z(Tconv+1), thereby reaching an equilibrium state.\nSimilarly, as before, we can form another equation namely,\ngθ(z) = fθ(z) − z. Here, the loss function L that we will\nbe defining will utilize the value of z at equilibrium, i.e.,\nzT conv= z∗. Using implicit differentiation (Bai, Kolter, and\nKoltun 2019), the following relation can be derived,\n∂L(z∗)\n∂θ = −∂L(z∗)\n∂z∗ (J−1\ngθ |z∗)∂fθ(z∗)\n∂θ (3)\nwhere, J−1\ngθ |z∗ is the inverse Jacobian of gθ when z = z∗,\ni.e., at equilibrium. The proposed spiking LM architecture\nfollows a similar set of equations which will be described\nin the following section. Since the gradient is computed us-\ning implicit differentiation on the converged steady-state, we\navoid the non-differentiability issues of the spiking function\n(Neftci, Mostafa, and Zenke 2019). Furthermore, by com-\nputing gradients solely at the equilibrium state, there is no\nrequirement to store intermediate hidden states. This char-\nacteristic enhances the scalability and memory efficiency of\nthis approach in comparison to BPTT.\nArchitecture\nThe high-level building block of the proposed spiking LM\ncomprises of Spiking Encoder (SE) layers, which can be\nconsidered similar to individual encoder layers in a trans-\nformer architecture. Both intra and inter-layer communica-\ntion in the SE layers occur using spikes at every time step\nduring the “forward” phase and spiking LIF neurons are fun-\ndamental units in its design. As shown in Fig. 1, each SE\nlayer consists of a Spiking Attention layer followed by fully\nconnected layers (some including skip-connections similar\nto those in BERT) viz. Intermediate Layer-1 (IL-1), Inter-\nmediate Layer-2 (IL-2) and an output layer, all of which op-\nerate using spikes. The structure of the proposed network\ncomprises of N stacked SE layers similar to the structure of\nBERT. The input embeddings are processed by an LIF neu-\nron layer, generating spikes that are propagated through the\nmodel. In some of the internal layers of SE, we use layer\nnormalization (following BERT).\nFrom a biological perspective, feedback connections are\npresent in the human-brain and moreover in some cases (Ku-\nbilius et al. 2019) shallower network with recurrent connec-\ntions shows performance comparable or better than deeper\narchitectures. The connection (F ) is added from the output\nof the final SE layer to the first one in order to introduce\na feedback. The feedback connection is an optional com-\nponent that adds to the model’s bio-plausibility. The gen-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11000\neral formulations of steady-state ASR equations, developed\nsubsequently, can be seamlessly applied to models involving\nboth feedback connections and those without any feedback.\nUnlike in vision-based tasks, feedback did not improve per-\nformance considerably when compared with no-feedback\nscenario in our experiments with GLUE benchmark. How-\never, we still explore it on a theoretical level to maintain con-\nsistency with previous works (Xiao et al. 2021) and to en-\ncourage future research on feedback enabled SNNs in other\ndomains.\nSimilar to vanilla transformer based architectures, the in-\nput sequence is directly fed into the model. The model con-\nverges over Tconv time steps during the “forward” phase to\nsettle to an equilibrium state. As discussed earlier, the spik-\ning neurons of the model have their individual membrane\npotentials, u, which are updated at every time step during\nconvergence. At time t + 1, the membrane potential of u1,\ni.e., the input to the first SE layer can be given as,\nu1[t + 1] = γu1[t] + F s(N,out)[t]\n+W0(x) + b1 − Vths1[t + 1] (4)\nwhere, W0 provides the embedding of the input sequence\nx of length Ns and produces a sequence of vectors y ∈\nRNs×Demb (Demb is the encoding dimension), F is the\nweight of the feedback connection (if feedback is included),\nb1 is bias and s(N,out)[t] are spikes generated from the Nth\nSE layer in the previous time step. The membrane potential\nof a layer i >1 can be represented simplistically as,\nui[t + 1] = γui[t] + W(i−1)(s(i−1)[t + 1]) +bi\n−Vthsi[t + 1] (5)\nwhere, W(i−1) is an operation (as formulated by each indi-\nvidual layer) defined on a set of spikes from previous lay-\ners. The described LIF neurons propagate information using\nspikes that are generated following Eqn. 2.\nThe average spiking rate (ASR) of a neuron at layer i can\nbe defined as, ai[t] =\nPt\nτ=1 γt−τ si[τ]\nPt\nτ=1 γt−τ . Given W(i−1) is a\nlinear operation, using Eqn. 5 and performing a weighted (if\nγ <1) average over time (with u[0] = 0, s[0] = 0) we get,\nai[t + 1] = 1\nVth\n(W(i−1)a(i−1)[t + 1] +bi − ui[t + 1]Pt\ni=0 γi )\n(6)\nSince, ai[t] represents ASR, its value is restricted within\n[0,1]. Following previous work on implicit differentiation at\nequilibrium (Xiao et al. 2021), as the average of input con-\nverges to equilibrium ¯x[t] → x∗, then the ASR of the layers\n(Eqn. 6) in the spiking architecture converges to equilibrium\npoints: ai[t] → a∗\ni (with bounded random error in case of\nLIF neurons). At equilibrium, the ASR a∗\ni of layer i satis-\nfies,\na∗\ni = σ( 1\nVth\n(W(i−1)(a∗\ni−1) + bi)) (7)\nwhere clipping functionσ(x) bounds the values within [0,1].\nLike the linear operations, the layers with non-linear op-\nerations such as spiking attention also converges to a steady-\nstate ASR as is empirically validated in this paper (Fig. 2a).\n(a)                                                                                           (b)\nFigure 2: Results obtained after passing a randomly sampled\ninput from SST-2 dataset through SpikingBERT4. (a) Graph\nshowing mean (over number of neurons) of the ASR of dif-\nferent sub-layers in an SE layer against the operating time\nsteps. (b) The y-axis on the left depicts mean (over number\nof neurons) of the ASR of a randomly chosen spiking at-\ntention layer. Along the right y-axis, the “Difference Norm”\nbetween the output of the steady-state equation of the cho-\nsen spiking attention layer and the calculated ASR is shown.\nTime steps used for convergence in shown along the x-axis.\nSteady-state equations like Eqn. 7 are leveraged during train-\ning. Thus, for the spiking attention layer, we formulate a sur-\nrogate steady state equation at equilibrium given as,\na∗\n(attn) = σ( 1\nVth\n(Attn(a∗\nx, a∗\nk, a∗\nv) + b(attn)) (8)\nwhere, a∗\nx is the ASR of the layer used to form Query, a∗\nk is\nthe ASR of the Key and a∗\nv is the ASR of the Value. Opera-\ntional details of the spiking attention mechanism and empir-\nical convergence and justification of the defined equation is\ndiscussed in the next subsection.\nThus, after the model converges to equilibrium, the dy-\nnamics of the steady-state ASR of the underlying SNN can\nbe mapped to a surrogate non-spiking architecture where the\ninput and output of each layer are the corresponding ASRs\n(a∗\ni ). The operation of individual layers in the surrogate net-\nwork is given by the steady-state equations as described ear-\nlier and can be simplified to the form, a∗\ni = li(a∗\nj , . . .)\nwhere, lis are steady-state equations corresponding to each\nlayer like Eqn. 7 and 8. The parameters (a∗\nj , . . .) associated\nwith each layer (li) are defined according to the specific op-\neration. If we use feedback connection, the fixed-point equa-\ntion of the first layer is of the form a∗\n1 = l1(lM ◦ ··· ◦\nl2(a∗\n1), x∗) where, l1(a, x) = σ( 1\nVth\n(F a+ W0(x) + b1))\nwith M being the total number of individual layers.\nFor the task of text classification, we use the last layer of\nthe network, i.e., the output of the Nth encoder layer given\nas a∗\nN,out as an input to a linear classification function. Sim-\nulating the network for Tconv time steps, we can compute\naN,out[T] =\nP\nt(sN,out[t])\nT (for simplicity of demonstration,\nγ = 1), which we can use asa∗\nN,out. Moreover, since the be-\nhaviour at equilibrium is captured by the surrogate network\nusing only ASR, we can simply perform backpropagation to\ntrain the weights by leveraging Eqn. 3. Thus, instead of per-\nforming BPTT to train the underlying spiking architecture,\nwe use simple backpropagation to train the weights of the\nspiking LM using only equilibrium state ASR of neurons.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11001\nSpiking Attention Mechanism\nWe propose a computationally efficient Spiking Attention\nmechanism where the inputs are processed as spikes from\nthe previous layer. The proposed attention operations of the\nmodule at time step t can be formulated as,\nAttn(Sx(t), SK(t), SV (t)) =\nπ(s ∗ Q(Sx(t))(SK(t))T ) · SV (t) (9)\nwhere, Q(Sx(t)) is obtained after passing input spikes\n(Sx(t)) at time t through a linear layer (WQ) for generating\nQuery. The spikes corresponding to the Key layer (S K(t))\nis computed by passing the input spikes Sx(t) through a lin-\near mapping (W K), connected to an LIF neuron layer, as\nillustrated in Fig. 1. SV (t) is obtained similarly using linear\nmapping (WV ). π is usually the softmax function and s is\na scaling factor where generally s = 1\n√dk\n, with dk being\nthe encoding dimension of Key. Recent work has shown that\nthe non-linear normalization operation π is not always es-\nsential (Zhou et al. 2022). The operations outlined in Eqn. 9\nexhibit characteristics akin to spiking architectures. This is\nbecause performing the aforementioned matrix multiplica-\ntions entails multiplying a real-valued matrix with a matrix\ncomposed of spikes (i.e., ‘0’s and ‘1’s) at each step. Thus,\ninstead of requiring O(n3) floating point multiplicative and\nO(n3) accumulative operations, we can implement the at-\ntention mechanism utilizing only O(n3) accumulative oper-\nations - which has been shown to significantly reduce com-\nputation cost in SNNs (Sengupta et al. 2019) (note that this\nis a first order estimate ignoring memory transactions). The\noutput of this module is passed through an LIF neuron, re-\nsulting in spikes that are fed to the next layer (Fig. 1).\nThe empirical convergence of the ASR of attention layer\nis demonstrated in Fig. 2b. As discussed earlier, we con-\nstruct a surrogate steady-state function at equilibrium which\nhelps us in efficient training of the model. The empirical ra-\ntionale for employing this specific functional form is sub-\nstantiated by observing the reduction in the difference norm\nbetween the output of the surrogate equation and the com-\nputed ASR of the layer at each timestep, as demonstrated\nin Fig. 2b. Thus, using Eqn. 8 and considering no bias,\nVth = 1 and no clamping function σ, we see that as the\nmodel converges in time, the actual ASR of the spiking at-\ntention layer aattn[t] approximates vanilla attention given by\nAttn(ax[t], ak[t], av[t]).\nANN-SNN KD using Equilibrium States\nThe proposed architecture and training mechanism guaran-\ntee the steady-state convergence of ASR of neurons across\nall layers in the models (Fig. 2a), including the internal rep-\nresentational layers. This enables us to develop an ANN-\nSNN based KD framework, using the intermediate layer\nASR at equilibrium and the activations of internal layers of\nthe “teacher” BERT models. In order to make the learning\nfaster and more efficient, we propose transferring knowl-\nedge from a pre-trained LM, such as BERT, to our spiking\nLM. KD is done over the internal layers such as transformer\nlayers, embedding-layer as well as the prediction layer.\nX\nInput\nSequence\nSE1\nEmbed\n...\nt = 1\nNS x DEMB\nTime\nSEN\nX SE1 ...\nt = 2\nSEN\nX SE1 ...\nt = Tconv\nSEN\nSpikes Spikes\nEquilibrium\nState\nX TE1 TEN*P\nLoss(a*1, T1*P)\nTE1*P\nLoss(a*N, TN*P)\nEmbed\nInput\nSequence\nStudent SNN LM\nTeacher ANN LM\nFigure 3: High-level overview of transformer layer based\nKD at equilibrium (following Eqn. 10) from a “teacher” LM\nto a spiking “student” LM.\nTo perform transformer layer-based distillation, we utilize\nthe output layer of each SE layer (Fig. 3). Subsequently, we\nestablish the loss function by comparing the ASR (at equilib-\nrium) of the output from each SE layer with the activations\nof the corresponding mapped encoder layers in the “teacher”\nmodel. The loss function using Mean Squared Error (MSE)\nis formulated as follows,\nLhi = MSE (a∗\nhiWTd, Tf(hi)) (10)\nwhere, a∗\nhi is the ASR (at equilibrium) of the output neurons\nof the ith SE layer in the “student” and Tf(hi) is the output\nof the f(hi)th layer of the “teacher”. WTd is a linear trans-\nformation that maps the “student” layer to the same dimen-\nsion as the corresponding “teacher” network layer. Function\nf maps “student” layer hi to a specific target layer in the\n“teacher” network. In our approach, we have used the fol-\nlowing mapping function: f(hi) = h\n′\np∗i; p = Tenc/Senc,\nwhere h\n′\ni is the output of the ith encoder layer of the\n“teacher” and Tenc and Senc are the number of encoder lay-\ners in the “teacher” and “student” models respectively.\nBy leveraging the equilibrium state of the neurons, KD\nutilizes the converged ASR to its advantage. Consequently,\nwe employ implicit differentiation technique (Eqn. 3) for\ntraining using the equilibrium state of the intermediate lay-\ners. This enables us to perform a faster and efficient layer-\nwise knowledge transfer from a pre-trained ANN-based LM\nto a smaller (in size) spiking LM. Moreover, each spiking en-\ncoder layer within the “student” model incorporates a spik-\ning attention layer as described in Fig. 1. We strengthen\nknowledge transfer further by optimizing anMSE loss over\nthe attention score at equilibrium of the “student” network\nwith the attention score of the corresponding mapped atten-\ntion layer of the “teacher” network (following the function\nf). We also perform embedding layer level distillation by\nformulating a loss function similar to Eqn. 10. We use ASR\nof the embedding layer (input to the first spiking encoder\nlayer) and create an MSE loss against the embedding layer\nof the “teacher”.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11002\nModel QQP MNLI-m SST-2 QNLI RTE MRPC STS-B\nCBoW (Wang et al. 2018) 75.00 57.10 79.50 62.50 71.90 75.00/83.70 70.60/71.10\nBiLSTM (Wang et al. 2018) 85.30 66.70 87.50 77.00 58.50 77.90/85.10 71.60/72.00\nBiLSTM + Attn, CoVe (Wang et al. 2018) 83.50 67.90 89.20 72.50 58.10 72.80/82.40 59.40/58.00\nGenSen (Wang et al. 2018) 82.60 71.40 87.20 62.50 78.40 80.40/86.20 81.30/81.80\nBERT5 + PF (Xu et al. 2021) 84.10 67.70 81.60 80.90 62.80 78.60/- -/81.10\nNAS-BERT5 + PF (Xu et al. 2021) 85.70 74.20 84.90 83.90 67.00 80.00/- -/82.80\nNAS-BERT5 + KD (Xu et al. 2021) 85.80 74.40 87.30 84.90 66.60 79.60/- -/83.00\nNAS-BERT10 + PF (Xu et al. 2021) 88.40 76.00 88.60 86.30 68.70 81.50/- -/84.30\nBERTTINY Adam (Frantar, Kurtic, and Alistarh 2021) 81.09 65.36 80.11 77.85 - 69.90/- 64.39/-\nBERTMINI Adam (Frantar, Kurtic, and Alistarh 2021) 86.45 73.30 85.46 83.85 - 76.57/- 82.09/-\nSpikingBERT4 86.82 78.10 88.19 85.20 66.06 79.17/85.15 82.20/81.90\nTinyBERT4 (no DA) (Jiao et al. 2019) 88.50 80.60 90.50 87.00 68.20 82.40/- 86.20/85.70\nTable 1: Results showing performance of our model (SpikingBERT4) against some standard models and other efficient imple-\nmentations of BERT on GLUE evaluation set. Accuracy is used as the metric for QQP, MNLI-m, SST-2, QNLI, RTE datasets\nwhile both accuracy and F1 scores are reported for the MRPC dataset. For STS-B, we report Pearson/Spearman correlation.\nPost-transformer layer distillation, we also perform pre-\ndiction layer distillation, following the works of (Hinton,\nVinyals, and Dean 2015). The loss at the prediction layer\nfor classification tasks can be written as,\nLpred = CE(c(a∗\npred)/t\n′\n, Tpred/t\n′\n) (11)\nwhere, a∗\npred is the ASR at equilibrium of the output of the\nfinal Spiking Encoder,c acts as a linear mapping andTpred is\nthe output logits of the “teacher” network. CE is the cross-\nentropy loss function and t\n′\nis temperature.\nDistillation is done in two different stages following (Jiao\net al. 2019). Firstly, we perform general distillation where\nwe use a pre-trained general BERT model and use general\ndomain data not specific to any particular task. Secondly, we\nperform task specific distillation on datasets relevant to the\nparticular task using a task-specific fine-tuned BERT model\nas a “teacher”. By employing a two-staged distillation pro-\ncess, we significantly enhance the efficiency of our spiking\nLM development, while also resulting in a substantially re-\nduced “student” model size when compared to the “teacher”.\nExperimentation\nIn this section, we demonstrate the performance of our pro-\nposed spiking LM and evaluate it against different tasks in\nthe General Language Understanding Evaluation (GLUE)\nbenchmark (Wang et al. 2018). We also highlight the core\nhyper-parameters for training SpikingBERT model. We also\nperform extensive analysis to report energy and power effi-\nciency of our proposed model. The experiments were run on\nNvidia RTX A5000 GPUs (8) each with 24GB memory.\nDatasets\nIn order to evaluate our model, we chose seven differ-\nent type of tasks (six classification and one regression\ntask) from the GLUE benchmark. We chose Quora Ques-\ntion Pair (QQP), Microsoft Research Paraphrase Corpus\n(MRPC) and Semantic Textual Similarity Benchmark (STS-\nB) (regression task) to evaluate our model on similarity and\nparaphrase tasks. For inference tasks, we opted for Multi-\nGenre Natural Language Inference (MNLI), Question-\nanswering NLI (QNLI) and Recognizing Textual Entailment\n(RTE) datasets. For single-sentence based sentiment analy-\nsis tasks, we chose Stanford Sentiment Treebank (SST-2).\nBaselines & SpikingBERT Settings\nTo the best our knowledge, our proposed model is the first\none to report and analyze the performance of a spiking LM\non different tasks from the GLUE benchmark. (Zhu, Zhao,\nand Eshraghian 2023) proposed a spiking based GPT archi-\ntecture and it reported 80.39% accuracy on SST-2 dataset\nwith a model (45M) comparable with our model size (50M).\nUsing a larger model of size 216M, SpikeGPT achieves\n88.76%, which is comparable to our performance. Our\nmodel is able to demonstrate higher accuracy with lower\nnumber of parameters primarily because of the KD tech-\nnique used to train it as well as because of the BERT-based\narchitecture which is suitable for classification problems due\nto its bidirectional context understanding. In addition to the\nabove mentioned work, we focus primarily on comparing\nthe performance of our architecture against existing non-\nspiking methodologies that aims to reduce the complexity\nof base-BERT model. However, unlike the proposed model,\nthese methods are not applicable for a spiking implementa-\ntion on neuromorphic chips such as Loihi 2 since all of them\nare non-spiking architectures. Our goal for the comparisons\nis to show that low-powered Spiking LM with less train-\nable parameters can achieve similar accuracy compared to\nother efficient LM implementations (number of parameters\nless than 50M). Moreover, since this is the first time spiking\nLMs have been evaluated against a benchmark, we did not\nuse additional techniques such as data augmentation (Jiao\net al. 2019), etc. to boost model performance in order to de-\nlineate the core advantages of our proposed training method.\nFor all the tasks, we keep the maximum sequence length\nat 128. The encoding dimension of the tokens in the input is\n768 and the intermediate (IL-2) size of the model is 3072. In\norder to emphasize on the benefits of KD, SpikingBERT 4\ncomprises of only 4 SE blocks compared to 12 encoders\nblocks of BERT. Increasing the number of SE blocks will\nalso improve the overall model performance. The model\ntrained for reporting the results (Table 1) did not have a feed-\nback, since adding it did not increase accuracy.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11003\nHyper-parameters Range Optimal\nTconv: General KD (5-150) 80\nTconv: Task-based IKD (5-150) 80\nVth (Threshold V oltage) (0.25 - 5.0) 1.0\nγ (Leak term) (0.8 - 1.0) .99 (LIF); 1 (IF)\nt\n′\n(Temperature) (0.1 - 10.0) 1.0\nBatch Size: General KD (8-256) 128\nBatch Size: Task-based IKD (8-128) [16,32]\nEpochs: General KD - 5\nEpochs: Task-based IKD - 20\nTable 2: Hyper-parameters (explored range and optimal val-\nues) for SpikingBERT4 used across all datasets.\nDuring training of SpikingBERT, we perform general dis-\ntillation first using English Wikipedia as text corpus and\nkeeping the sequence length at 128. Transformer (& Embed-\nding) layer distillation following Eqn. 10 is performed and\nthe “teacher” is a pre-trained BERT BASE model (uncased).\nFollowing this, we perform task-based internal layer KD\n(IKD) with corresponding fine-tuned BERT models and per-\nform it both on the inner transformer layers and the embed-\nding layer. Core hyper-parameters associated are given in\nTable 2. We found that grouping IKD based on type of task\n(i.e., inference, similarity, etc.) at this stage improves perfor-\nmance. For example, once a task-specific distillation is done\nusing MNLI dataset, if we use that distilled model (as “stu-\ndent”) and then perform task-specific distillation on QNLI\ndataset, we achieve higher accuracy on QNLI dataset. After\ntask-based IKD is done, we finally perform prediction-layer\ndistillation following Eqn. 11 to develop the final model. It is\nto be noted that if we directly train our model on true labels\nat this stage, we obtain similar results in terms of accuracy.\nWithout using the proposed KD, there is at least 4% to 5%\ndrop in accuracy across all datasets.\nAnalysis of Power & Energy Efficiency\nThe proposed spiking LM consists of less number of param-\neters than the “teacher” BERT models (109M) and in addi-\ntion to that it uses only accumulative (ACC) operations in\nplace of multiplicative and accumulative operations (MAC)\nfound in vanilla BERT models. Considering 45nm CMOS\ntechnology, ACC operations exhibit an impressive energy\nefficiency, consuming only 0.9pJ, which is over five times\n(5.1) more efficient than MAC operations that demand 4.6pJ\n(Han et al. 2015). For estimating the energy and power effi-\nciency of our spiking LM, we leverage the concept of nor-\nmalized operations (Lu and Sengupta 2020), which consid-\ners the spiking rates of each layer and corresponding layer-\nwise operations. The total normalized OPS can be defined as\nNorm#OP S=\nP\ni IFR i∗Layer#OPS i+1\nPLayer#OPS , where IF Ri is\nthe total number of spikes over inference time steps averaged\nover number of neurons. Thus, energy-efficiency factor of an\nSNN, which can be given by the ratio of energy consumed\nby an iso-architecture ANN over the proposed SNN can be\nexpressed as: e = ( 1\n5.1 ∗ Norm#OP S)−1. SNNs operate\non specific time steps, allowing them to dynamically balance\naccuracy and energy consumption. We perform an extensive\n(a)                                 (b)   \nFigure 4: Results obtained on SST-2 dataset. (a) Variation\nof Accuracy and Energy-efficiency factor (e) as Tconv in-\ncreases. (b) Variation in mean ASR per neuron in different\nsub-layers of SpikingBERT4 following changes in Vth.\nenergy-accuracy tradeoff analysis on the SST-2 dataset. Af-\nter conducting general and task-based IKD, during the final\ntraining phase, we train a set of models with different val-\nues of Tconv to see its effect on energy consumption and\naccuracy. The energy-efficiency factor (e) and the obtained\naccuracy w.r.t the time steps (Tconv) is demonstrated in Fig.\n4a. A suitable tradeoff point can be found at Tconv = 16 ,\nwhere we achieve an accuracy close to the highest value (2%\ndifference) but we are able to achieve nearly twice energy-\nefficiency than a non-spiking model of same size.\nMoreover, by increasing Vth, we can reduce the ASR of\nneurons at each layer, leading to a significant decrease in\npower consumption. We perform an ablation study on the\neffects of Vth on ASR, which is reported in Fig. 4b. In-\ncreasing Vth intuitively also increases the convergence time\nsteps, thus making energy-consumption effectively similar.\nHowever, it allows us to reduce the instantaneous power-\nconsumption considerably - ideal for edge computing.\nConclusion and Future Works\nDrawing inspiration from the astonishing intricacy of the\nhuman brain, a complexity that outshines that of any cur-\nrent LLM, we have the opportunity to leverage these in-\nsights in crafting models that not only replicate biologically\nplausible behavior but also offer energy-efficient solutions\nthrough minimal power consumption. In this paper, we pro-\npose a spiking LM and evaluate it against multiple tasks\nin the GLUE benchmark. Leveraging steady-state conver-\ngence, we introduced a spiking attention mechanism, pro-\nposed a novel ANN-SNN based KD for faster and efficient\nlearning and explored training of Spiking LMs using im-\nplicit differentiation, thereby overcoming multiple issues af-\nfecting training of SNN models. Implementing our model on\nneuromorphic hardware such as Loihi 2 for inference will\nhelp us develop a low-powered solution which can poten-\ntially be implemented on edge devices.\nFurther endeavours can be made to extend this method-\nology to design other spiking LMs such as GPT, etc. There\nis still a performance gap between the proposed spiking LM\nand BERT-based fine-tuned models. We can work towards\nclosing this gap by delving into diverse spiking neuron mod-\nels, examining temporal encoding schemes, and incorporat-\ning graded spikes, among other strategies.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11004\nAcknowledgments\nThe work was supported by the National Science Founda-\ntion CAREER Award under Grant #2337646 and by Oracle\nCloud credits and related resources provided by the Oracle\nfor Research program.\nReferences\nAlawad, M.; Yoon, H.-J.; and Tourassi, G. 2017. Energy\nefficient stochastic-based deep spiking neural networks for\nsparse datasets. In 2017 IEEE International Conference on\nBig Data (Big Data), 311–318. IEEE.\nAmir, A.; Taba, B.; Berg, D.; Melano, T.; McKinstry, J.;\nDi Nolfo, C.; Nayak, T.; Andreopoulos, A.; Garreau, G.;\nMendoza, M.; et al. 2017. A low power, fully event-based\ngesture recognition system. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 7243–\n7252.\nBai, S.; Kolter, J. Z.; and Koltun, V . 2019. Deep equilib-\nrium models. Advances in Neural Information Processing\nSystems, 32.\nBal, M.; and Sengupta, A. 2022. Sequence Learning using\nEquilibrium Propagation. arXiv preprint arXiv:2209.09626.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877–\n1901.\nDavies, M.; Wild, A.; Orchard, G.; Sandamirskaya, Y .;\nGuerra, G. A. F.; Joshi, P.; Plank, P.; and Risbud, S. R. 2021.\nAdvancing neuromorphic computing with loihi: A survey of\nresults and outlook. Proceedings of the IEEE, 109(5): 911–\n934.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nFrantar, E.; Kurtic, E.; and Alistarh, D. 2021. M-FAC: Ef-\nficient matrix-free approximations of second-order informa-\ntion. Advances in Neural Information Processing Systems,\n34: 14873–14886.\nGhosh-Dastidar, S.; and Adeli, H. 2009. Spiking neural net-\nworks. International journal of neural systems, 19(04): 295–\n308.\nHan, S.; Pool, J.; Tran, J.; and Dally, W. J. 2015. Learn-\ning both Weights and Connections for Efficient Neural Net-\nworks. arXiv:1506.02626.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531.\nHong, D.; Shen, J.; Qi, Y .; and Wang, Y . 2023. LaSNN:\nLayer-wise ANN-to-SNN Distillation for Effective and Ef-\nficient Training in Deep Spiking Neural Networks. arXiv\npreprint arXiv:2304.09101.\nJiao, X.; Yin, Y .; Shang, L.; Jiang, X.; Chen, X.; Li,\nL.; Wang, F.; and Liu, Q. 2019. Tinybert: Distilling\nbert for natural language understanding. arXiv preprint\narXiv:1909.10351.\nKim, S.; Gholami, A.; Yao, Z.; Mahoney, M. W.; and\nKeutzer, K. 2021. I-bert: Integer-only bert quantization. In\nInternational conference on machine learning, 5506–5518.\nPMLR.\nKrizhevsky, A.; Nair, V .; and Hinton, G. 2009. Cifar-10 and\ncifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/-\ncifar. html, 6(1): 1.\nKubilius, J.; Schrimpf, M.; Kar, K.; Rajalingham, R.; Hong,\nH.; Majaj, N.; Issa, E.; Bashivan, P.; Prescott-Roy, J.;\nSchmidt, K.; et al. 2019. Brain-like object recognition with\nhigh-performing shallow recurrent ANNs. Advances in neu-\nral information processing systems, 32.\nKurtic, E.; Campos, D.; Nguyen, T.; Frantar, E.; Kurtz,\nM.; Fineran, B.; Goin, M.; and Alistarh, D. 2022. The\noptimal bert surgeon: Scalable and accurate second-order\npruning for large language models. arXiv preprint\narXiv:2203.07259.\nLee, C.; Sarwar, S. S.; Panda, P.; Srinivasan, G.; and Roy,\nK. 2020. Enabling spike-based backpropagation for train-\ning deep neural network architectures. Frontiers in neuro-\nscience, 119.\nLu, S.; and Sengupta, A. 2020. Exploring the connection\nbetween binary and spiking neural networks. Frontiers in\nneuroscience, 14: 535.\nNeftci, E. O.; Mostafa, H.; and Zenke, F. 2019. Surrogate\ngradient learning in spiking neural networks: Bringing the\npower of gradient-based optimization to spiking neural net-\nworks. IEEE Signal Processing Magazine, 36(6): 51–63.\nOrchard, G.; Jayawant, A.; Cohen, G. K.; and Thakor, N.\n2015. Converting static image datasets to spiking neuro-\nmorphic datasets using saccades. Frontiers in neuroscience,\n9: 437.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training.\nScellier, B.; and Bengio, Y . 2017. Equilibrium propagation:\nBridging the gap between energy-based models and back-\npropagation. Frontiers in computational neuroscience, 11:\n24.\nSengupta, A.; Ye, Y .; Wang, R.; Liu, C.; and Roy, K. 2019.\nGoing deeper in spiking neural networks: VGG and residual\narchitectures. Frontiers in neuroscience, 13: 95.\nTakuya, S.; Zhang, R.; and Nakashima, Y . 2021. Training\nlow-latency spiking neural network through knowledge dis-\ntillation. In 2021 IEEE Symposium in Low-Power and High-\nSpeed Chips (COOL CHIPS), 1–3. IEEE.\nTang, R.; Lu, Y .; Liu, L.; Mou, L.; Vechtomova, O.; and Lin,\nJ. 2019. Distilling task-specific knowledge from bert into\nsimple neural networks. arXiv preprint arXiv:1903.12136.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. arXiv:1706.03762.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11005\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nWei, J.; Tay, Y .; Bommasani, R.; Raffel, C.; Zoph, B.;\nBorgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Met-\nzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.;\nDean, J.; and Fedus, W. 2022. Emergent Abilities of Large\nLanguage Models. arXiv:2206.07682.\nXiao, M.; Meng, Q.; Zhang, Z.; Wang, Y .; and Lin, Z. 2021.\nTraining feedback spiking neural networks by implicit dif-\nferentiation on the equilibrium state. Advances in Neural\nInformation Processing Systems, 34: 14516–14528.\nXu, J.; Tan, X.; Luo, R.; Song, K.; Li, J.; Qin, T.; and Liu, T.-\nY . 2021. NAS-BERT: task-agnostic and adaptive-size BERT\ncompression with neural architecture search. In Proceed-\nings of the 27th ACM SIGKDD Conference on Knowledge\nDiscovery & Data Mining, 1933–1943.\nXu, Q.; Li, Y .; Shen, J.; Liu, J. K.; Tang, H.; and Pan, G.\n2023. Constructing deep spiking neural networks from arti-\nficial neural networks with knowledge distillation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 7886–7895.\nZhou, Z.; Zhu, Y .; He, C.; Wang, Y .; Yan, S.; Tian, Y .; and\nYuan, L. 2022. Spikformer: When spiking neural network\nmeets transformer. arXiv preprint arXiv:2209.15425.\nZhu, R.-J.; Zhao, Q.; and Eshraghian, J. K. 2023. Spikegpt:\nGenerative pre-trained language model with spiking neural\nnetworks. arXiv preprint arXiv:2302.13939.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n11006",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6025442481040955
    },
    {
      "name": "Automatic differentiation",
      "score": 0.48736220598220825
    },
    {
      "name": "Natural language processing",
      "score": 0.3888530433177948
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37774085998535156
    },
    {
      "name": "Programming language",
      "score": 0.269342303276062
    },
    {
      "name": "Computation",
      "score": 0.0
    }
  ]
}