{
  "title": "An Investigation of LLMs‚Äô Inefficacy in Understanding Converse Relations",
  "url": "https://openalex.org/W4389518670",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5064578824",
      "name": "Chengwen Qi",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5100387285",
      "name": "Bowen Li",
      "affiliations": [
        "BG Group (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5080628250",
      "name": "Binyuan Hui",
      "affiliations": [
        "BG Group (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5042749062",
      "name": "Bailin Wang",
      "affiliations": [
        "BG Group (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5100320436",
      "name": "Jinyang Li",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5062042376",
      "name": "Jinwang Wu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5017308450",
      "name": "Yuanjun Laili",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W4381105112",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W4379259948",
    "https://openalex.org/W4385571583",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4305000040",
    "https://openalex.org/W4386977995",
    "https://openalex.org/W3095992020",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2165467455",
    "https://openalex.org/W4385571985",
    "https://openalex.org/W4289548669",
    "https://openalex.org/W3035032873",
    "https://openalex.org/W4381245716",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W2250184916",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W4389519396",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2889234142",
    "https://openalex.org/W3016970897",
    "https://openalex.org/W2952984539",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4375869868",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W4224308101"
  ],
  "abstract": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.",
  "full_text": "An Investigation of LLMs‚Äô Inefficacy in Understanding Converse Relations\nChengwen Qi1,‚ô£ Bowen Li2,3,‚ô£ Binyuan Hui3 Bailin Wang3,5 Jinyang Li4\nJinwang Wu1 Yuanjun Laili1,‚Ä†\n1Beihang University 2Shanghai AI Laboratory\n33B Group 4The University of Hong Kong 5MIT\nchengwen_qi@buaa.edu.cn, libowen.ne@gmail.com\nhuybery@gmail.com, lailiyuanjun@buaa.edu.cn\nhttps://github.com/3B-Group/ConvRe\nAbstract\nLarge Language Models (LLMs) have achieved\nremarkable success in many formal language\noriented tasks, such as structural data-to-text\nand semantic parsing. However current bench-\nmarks mostly follow the data distribution of\nthe pre-training data of LLMs. Therefore, a\nnatural question rises that do LLMs really un-\nderstand the structured semantics of formal lan-\nguages. In this paper, we investigate this prob-\nlem on a special case, converse binary relation.\nWe introduce a new benchmark ConvRe fo-\ncusing on converse relations, which contains\n17 relations and 1240 triples extracted from\npopular knowledge graph completion datasets.\nOur ConvRe features two tasks, Re2Text and\nText2Re, which are formulated as multi-choice\nquestion answering to evaluate LLMs‚Äô ability\nto determine the matching between relations\nand associated text. For the evaluation proto-\ncol, apart from different prompting methods,\nwe further introduce variants to the test text\nand few-shot example text. We conduct ex-\nperiments on three popular LLM families and\nhave observed various scaling trends. The re-\nsults suggest that LLMs often resort to shortcut\nlearning and still face challenges on our pro-\nposed benchmark.\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated impressive empirical results on various NLP\ntasks (Bubeck et al., 2023; OpenAI, 2023; An-\nthropic, 2023), including formal language-oriented\ntasks such as structural data-to-text (Xiang et al.,\n2022) and semantic parsing (Chen et al., 2021; Li\net al., 2023a), which require sophisticated com-\nprehension and production of structured language\ncontent. Despite these promising advances, a criti-\ncal concern remains largely unexplored: do these\nLLMs genuinely understand the nuanced semantics\n‚ô£ Equal contribution.‚Ä† Corresponding authors.\nof formal languages, or are they merely exploiting\nstatistical patterns inherent in their pre-training\ndata? If such shortcuts exist, it implies that LLMs\nmay struggle to generalize to novel and unique for-\nmal language definitions, potentially hindering the\nrobustness and scalability of practical applications.\nIn this work, we delve into this question by focus-\ning on a specific aspect of formal language under-\nstanding: the comprehension of converse relations.\nAs shown in Figure 1, the converse relation rede-\nfines the semantic relation between entities while\nkeeping the surface form of the triple unchanged.\nFor instance, the triple (x, has part, y) should be\ninterpreted as \"x has a part called y\" in the normal\nrelation (Codd, 1983), while \"y has a part called\nx\" in converse form. Notably, LLMs are largely\nunfamiliar with converse relations, as the data they\nlearn in pre-training mostly comprises normal rela-\ntion. It‚Äôs imperative for LLMs to accurately under-\nstand and utilize these converse relations, i.e., truly\nfollowing instructions rather than recalling memo-\nrized patterns (shortcuts) about normal relation, as\nit significantly impacts the semantic coherence of\ntheir output.\nTo systematically evaluate the competence of\nLLMs in recognizing and processing converse re-\nlations, we introduce a novel benchmark, ConvRe.\nThis benchmark draws upon 17 diverse relations\nand 1240 triples derived from prominent knowl-\nedge graph completion datasets. ConvRe intro-\nduces two primary tasks, Re2Text and Text2Re,\nformatted as multiple-choice question answering\ntests. These tasks challenge LLMs to correctly\nmatch relations (Re) with their corresponding natu-\nral language text (Text).\nDuring empirical evaluation, we add various\nprompting methods and introduce variants to the\ntext. More specifically, we manually craft exam-\nples of different types in the few-shot prompting,\ncreating a more challenging testbed for these mod-\nels. Our findings, based on thorough experiments\n6932\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6932‚Äì6953\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nNormal Relation\n? ? ?\nEasy !\nModel Size\nPerformance\nConverse Relation\nModel Size\nPerformance\n‚ùå\n‚úÖ\nü§î Could LLMs understand converse relations ?\nFigure 1: Illustration of converse relation comprehension by LLMs. This diagram highlights the unique challenges\nconverse relations present for LLMs, potentially leading to diverse scaling trends.\nTriple Relation Notation Associated Text Text Variants\nNormal R s s ‚Ä≤\n(x, R, y) x has a part called y. x possesses a specific component named y.\n(x, has part, y) Converse R‚ä§ s‚ä§ s‚ä§‚Ä≤\ny has a part called x. y contains x.\nTable 1: The definition of normal and converse relation. Examples are provided below the notations. A triple can be\ndefined to represent the normal relation R or the converse relation R‚ä§. Each relation is associated with a pairing\nnatural language text, which can further be paraphased.\nusing three popular LLM families, reveal interest-\ning scaling trends and suggest that performance on\nunderstanding formal languages might be inflated\nby shortcut learning. This exploration contributes\nto the growing body of literature that seeks to as-\nsess the true capabilities of LLMs, and the extent\nto which they genuinely comprehend the semantics\nof formal languages.\n2 ConvRe Benchmark\nIn this section, we will introduce the motivation,\ntask formulation and design choice of our ConvRe\nbenchmark as well as the details surrounding data\ncollection.\n2.1 Motivation\nThe recent surge in the performance of LLMs in\nunderstanding formal language, including tasks\nsuch as semantic parsing or data2text, can poten-\ntially be misleading. Traditional evaluation bench-\nmarks used in such tasks often reflect statistical\npatterns similar to those found in the pre-training\ndata of LLMs. We posit that this could lead LLMs\nto take a shortcut * as described in Geirhos et al.\n(2020), thereby inflating the understanding of for-\nmal language semantics. Instead of comprehen-\nsively grasping the semantics, the LLMs might\nsimply be learning the statistical tendencies present\nin their training data. To this end, we propose a\nnew benchmark that uses normal and converse rela-\ntions to examine the true semantic comprehension\ncapabilities of LLMs.\n2.2 Normal and Converse Relation\nNormal Relation Formally, a binary relation R\nover sets X and Y is a set of ordered pairs (x, y)\nconsisting of elements x ‚àà X and y ‚àà Y (Codd,\n1983). Usually, a normal relation R is represented\nas R = {(x, R, y) = ‚áí xRy}, where R is the\nspecific relation phrase. Normal relations usually\nappear in the knowledge graph, along with a pair of\n*There are some terminologies, such as spurious correla-\ntion and superficial cues/bias/artifacts, that are similar to the\nterm shortcut used in this paper. We provide supplementary\nexplanations of these terms in Appendix A for better clarity.\n6933\nTriple Relation Notation Associated NL NL Variants\nNormal R\nss\n0\n( x, R, y ) x is a hypernym of y . y is more speciÔ¨Åc than x .\n( x, hypernym ,y )\nConverse R\n>\ns\n>\ns\n>0\ny is a hypernym of x . y is more general than x .\nTable 1: The deÔ¨Ånition of normal and converse relation. Examples are provided below the notations. A triple can be\ndeÔ¨Åned to represent the normal relation R or the converse relation R\n>\n. Each relation is associated a pairing natural\nlanguage representation, which can further be paraphased.\nRe2Text Task\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, hilt)\nA: Find an entity that has a part called hilt.\nB: Find an entity that is a part of hilt.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nRe2Text Task (hard)\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, hilt)\nA: Find an entity that has a part called hilt.\nB: Find an entity that hilt contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nFigure 2: Examples of Re2Text and Text2Re tasks on converse relation. We additionally paraphrase the natural\nlanguage representations (answer candidates for Re2Text, question for Text2Re) to make them differ from the\nsentences in the Instruction.\nlanguage representation along with a query triple,137\nthe model is asked to determine the natural lan-138\nguage representation that best aligns semantically139\nwith the query triple.140\nText2Re The second task can be considered141\nas the reverse of Re2Text. Given an instruc-142\ntion‚Äîformatted similarly to Re2Text‚Äîand a query143\nsentence, the model is required to identify the query144\ntriple that best matches the query sentence.145\nFollowing McKenzie et al. ( 2022 ), both tasks146\nare formulated as multi-choice question-answering147\ntasks (illustrated in Figure 3 ), providing a concrete148\nmethod for evaluation.149\n2.4 Text Variants150\nTest Variants in Zero-shot Prompting Geirhos151\net al. ( 2020 ), highlighted a phenomenon in deep152\nlearning known as shortcut learning . These are de-153\ncision rules that achieve high performance on stan- 154\ndard benchmarks but fail to generalize under more 155\nchallenging testing conditions such as real-world 156\nscenarios. This issue is particularly signiÔ¨Åcant in 157\nlanguage processing tasks, where a language model 158\nmay show an ability to reason that is learned from 159\nthe training data, but its performance can plummet 160\ndrastically‚Äîsometimes to levels equivalent to ran- 161\ndom guessing‚Äîwhen superÔ¨Åcial correlations are 162\nremoved from the dataset ( Niven and Kao , 2019 ). 163\nTo assess how extensively current LLMs lever- 164\nage shortcut learning for the evaluation tasks we 165\nhave designed, we introduce variants to the text in 166\nboth our tasks. In the Re2Text task, we paraphrase 167\none answer candidate, while in the Text2Re task, 168\nwe paraphrase the question. SpeciÔ¨Åcally, we mod- 169\nify the key predicate and restructure the sentence 170\n(as illustrated in Ô¨Ågure 3 ). We note that the subtle 171\nvariations on the test text bring different effects 172\n3\nx\nTriple: (sword, has part, hilt)\nT ext: sword has a part called hilt\nLLM Pre-Training Corpus\nTriple: (sword, has part, hilt)\nT ext: sword has a part called hilt\nLLM Pre-Training Corpus\nshortcut\nshortcut\nshortcut\nno shortcut\n(altered)\n‚≠ê\nExpected answer: B\nExpected answer: B\nFigure 2: The Re2Text task converts relation into semantically equivalent natural language text. Given that LLMs\nmostly encounter normal relations during pre-training, deciphering converse relations poses a significant challenge.\nLLMs tend to exploit textual similarity shortcuts for prediction, which can mislead the model‚Äôs performance as it\nbypasses genuine comprehension. In the regular scenario (top), two shortcuts lead the model towards divergent\nanswers, where the incorrect answer (A) will not be overly preferred. In the hard\n scenario (bottom), the text for\nthe correct response (B) is modified, transforming two shortcuts into a single one. This solitary shortcut is more\nlikely to misdirect the model towards the incorrect answer (A), highlighting the pitfalls of shortcuts learning.\nsubject x and object y. This triple can be mapped\nto a semantically equivalent natural language text\ns. Examples can be found in Table 1.\nConverse Relation In addition to the normal re-\nlation, we also introduce a converse relation that\nutilizes the same triple format (x, R, y) to denote\nthe converse mappingR‚ä§. It defines a new form by\nswapping the pairing order, which can be expressed\nas R‚ä§ = {(x, R, y) =‚áí yRx}. Accordingly, in\nthe converse mapping, the triple (x, R, y) corre-\nsponds to the converse natural language text s‚ä§.\nExamples are provided in Table 1 for further clar-\nity.\nIt‚Äôs worth noting that both the normal and con-\nverse relation definitions used in our evaluation\nhave a localized scope to minimize ambiguity. This\nprocess helps us ascertain whether LLMs can un-\nderstand the semantics of the custom relation defi-\nnition rather than resorting to shortcut learning.\n2.3 Task Formulation\nWe designed two tasks to assess LLMs‚Äô understand-\ning of normal and converse relations. Both tasks\nfocus on semantic equivalence translation between\nrelations (Re) and natural language text (Text).\nRe2Text In this task, given the specification of a\nnormal/converse relation and its associated natural\nlanguage text along with a query triple, the model\nis asked to determine the natural language text that\nbest aligns semantically with the query triple.\nText2Re The second task can be considered\nas the reverse of Re2Text. Given an instruc-\ntion‚Äîformatted similarly to Re2Text‚Äîand a query\nsentence, the model is required to identify the query\ntriple that best matches the query sentence.\nFollowing McKenzie et al. (2023), both tasks\nare formulated as multi-choice question-answering\ntasks, providing a concrete method for evaluation.\n2.4 Text Variants\nGeirhos et al. (2020) highlighted a phenomenon in\ndeep learning known as shortcut learning. These\nare decision rules that achieve high performance\non standard benchmarks but fail to generalize un-\nder more challenging testing conditions such as\nreal-world scenarios. This issue is particularly sig-\nnificant in language processing tasks, where a lan-\nguage model may show an ability to reason that\nis learned from the training data, but its perfor-\nmance can drop drastically‚Äîsometimes to levels\n6934\nText2Re Task\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: Find an entity that possesses a speciÔ¨Åc component named hilt.\nA: (?, has part, hilt)\nB: (hilt, has part, ?)\nTo convert the question into a semantically equivalent triple query, which choice is correct?\nAnswer:\nText2Re Task (hard)\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: Find an entity that has a part called hilt.\nA: (?, has part, hilt)\nB: (hilt, has part, ?)\nTo convert the question into a semantically equivalent triple query, which choice is correct?\nAnswer:\nFigure 3: Examples of Re2Text and Text2Re tasks on converse relation. We additionally paraphrase the natural\nlanguage representations (answer candidates for Re2Text, question for Text2Re) to make them differ from the\nsentences in the Instruction.\nto the two tasks, which will be evidenced by the173\nempirical results in our experiments (section 4.2 ).174\nAn intuitive explanation is provided in Ô¨Ågure 3 . De-175\ntailed zero-shot prompting methods can be found176\nin table 2 .\n2\n177\nExample Variants in Few-shot Prompting Be-178\nside the variants on the test text, we additionally179\nintroduce variants to the text in examples for the180\nfew-shot prompting. Since we have identiÔ¨Åed the181\nmost challenging settings for the two tasks in zero-182\nshot, we will employ such settings for the test text183\nand dub them as hard tests in few-shot. Accord-184\ningly, we incorporate text variants to the examples185\nused in the few-shot prompting. Comprehensively,186\nthe few-shot prompts used in our benchmark are187\nlisted in table 3 . Details of arrangement of text188\nvariants are illustrated in table 4 . More speciÔ¨Åcally,189\nif the hard test setting corresponds to the unaltered190\ntest text, then the unaltered examples will be de-191\nnoted as hard . On the contrary, the altered exam-192\nples will be denoted as regular . This setup shares193\nthe similar spirit as the complexity based prompt-194\ning ( Fu et al. , 2022 ) that hard examples would help195\nclarify the problem and reduce the model bias.196\n2\nRelation settings and hint will be thoroughly disscussed\nin section 3.2 .\n2.5 Data Collection 197\nTo make our tasks more comprehensive, and thus 198\ntest the LLMs‚Äô ability to reason in more complex 199\nways, plausible relations must satisfy two require- 200\nments: 201\n‚Ä¢ The relation is asymmetric , implying that 202\nR 6 = R\n>\n. An example of such a relation is 203\nparent of . Here, the order of the involved 204\nentities signiÔ¨Åcantly changes the meaning, as 205\nthe parent-child relationship is not reciprocal. 206\nConversely, if the relation is symmetric, such 207\nas neighboring country , it would be mean- 208\ningless to determine whether a given entity 209\nshould be a head or a tail, as the both are se- 210\nmantically equivalent. 211\n‚Ä¢ The involved subject and object are inter- 212\nchangeable . That is, the relation R and its 213\nconverse counterpart R\n>\nshould be semanti- 214\ncally plausible, though not equivalent. An 215\nexample of a relation we would avoid under 216\nthis criterion is native language , which as- 217\nsociates a person with a language. A language 218\ncannot logically be the subject of native 219\nlanguage , thereby disqualifying this relation. 220\nRelations of this sort could allow LLMs to 221\nrely on shortcut learning to solve tasks. For 222\ninstance, in the case of native language , the 223\nentity‚Äôs type inadvertently gives away the an- 224\nswer so that the LLMs may exploit this leaked 225\n4\nx\n Triple: (sword, has part, hilt)\nT ext: sword has a part called hilt\nLLM Pre-Training Corpus\nshortcut\nshortcut\nshortcut\nno shortcut\n(altered)\n‚≠ê\nExpected answer: B\nExpected answer: B\nTriple: (sword, has part, hilt)\nT ext: sword has a part called hilt\nLLM Pre-Training Corpus\nFigure 3: The Text2Re task converts natural language text into semantically equivalent relationtriple. As with the\nRe2Text task, this process can be misled by shortcut learning. In the regular scenario (top), an altered question is\nused, resulting in a single shortcut that leads the model towards the incorrect answer (A). In the hard\n scenario\n(bottom), the combination of natural language text and the relation definition creates two shortcuts, both leading to\nthe incorrect answer (A), thus increasing the likelihood of the model‚Äôs misprediction.\nequivalent to random guessing‚Äîwhen superficial\ncorrelations are removed from the dataset (Niven\nand Kao, 2019).\nTo assess how extensively current LLMs lever-\nage shortcut learning for the evaluation tasks we\nhave designed, we introduce variants to the text in\nboth our tasks. Concretely, we alter the natural lan-\nguage text on both test side and few-shot example\nside to get the paraphrased variants.\nTest Variants In the Re2Text task, we paraphrase\none answer candidate, while in the Text2Re task,\nwe paraphrase the question. Specifically, we mod-\nify the key predicate and restructure the sentence.\nWe note that the subtle variations on the test text\ncould bring different effects to the two tasks, which\nwill be evidenced by the empirical results in our\nexperiments (see Section 4.2). Examples on the\ntest variants as well as intuitive explanations on\ntheir effects on two tasks are provided in Figure 2\nand 3. Detailed zero-shot prompting methods can\nbe found in Table 2.‚Ä†\nFew-shot Example Variants Beside the variants\non the test text, we further introduce variants to the\ntext within the examples used for few-shot prompt-\ning. Since we have identified the most challenging\n‚Ä†Relation settings and hint will be thoroughly discussed in\nSection 3.2.\nvariant settings within the zero-shot tasks, we will\nemploy the same configurations for the test text in\nthe few-shot context, denoting these as hard tests.\nAccordingly, we integrate text variants within the\nexamples for the few-shot prompting. A compre-\nhensive list of few-shot prompts utilized in our\nbenchmark can be found in Table 3, and the spe-\ncific arrangements of text variants are illustrated\nin Table 4. Notably, if the hard test setting aligns\nwith the unaltered test text (for the Text2Re task),\nthen the unaltered examples are labeled as hard,\nwhile the altered examples are labeled as regular.\nThis setup shares the similar spirit as the complex-\nity based prompting (Fu et al., 2022), where hard\nexamples serve to refine problem understanding\nand mitigate model bias.\n2.5 Data Collection\nTo make our tasks more comprehensive, and thus\ntest the LLMs‚Äô ability to reason in more complex\nways, plausible relations must satisfy two require-\nments:\n‚Ä¢ The relation is asymmetric, implying that\nR Ã∏= R‚ä§. An example of such a relation is\nparent of. Here, the order of the involved en-\ntities significantly changes the meaning, as the\nparent-child relationship is not mutual. Con-\n6935\nID‚àó Prompting\nMethod Shot Relation ‚Ä† Hint Test\nVariants‚Ä°\n1# normal-re, normal-text 0 N\n2# normal-re, altered-text 0 N ‚úì\n3# converse-re, normal-text (\n Text2Re ) 0 C\n4# converse-re, altered-text (\n Re2Text) 0 C ‚úì\n5# converse-re, normal-text, hint 0 C ‚úì\n6# converse-re, altered-text, hint 0 C ‚úì ‚úì\nTable 2: Zero-shot prompts. ‚àó: each prompt method has been associated with a unique ID that will be referred to in\nthe experimental results. ‚Ä†: N indicates normal relation and C indicates converse relation. ‚Ä°: whether test text are\naltered.\n Text2Re: the hard setting for Text2Re.\n Re2Text: the hard setting for Re2Text.\nID Prompting Method Shot Relation Hint Examples ‚ô£ Tests‚ô†\n7# 3-shot, hard-hard 3 C hard hard\n8# 3-shot, hard-hard, hint-cot 3 C ‚úì(w/ CoT) hard hard\n9# 6-shot, hard-hard 6 C hard hard\n10# 3-shot, regular-hard 3 C regular hard\n11# 3-shot, regular-hard, hint-cot 3 C ‚úì(w/ CoT) regular hard\n12# 6-shot, regular-hard 6 C regular hard\nTable 3: Few-shot prompts. ‚ô†: the hard test setting is always employed (see Table 2). ‚ô£: examples are provided in\ntwo options, regular and hard.\nExample-Test Example\nVariants\nTest\nVariants\nRe2Text\nhard-hard ‚úì ‚úì\nregular-hard ‚úì\nText2Re\nhard-hard\nregular-hard ‚úì\nTable 4: Text variants on test and example sides for\nfew-shot prompting.\nversely, if the relation is symmetric, such as\nneighboring country , it would be mean-\ningless to determine whether a given entity\nshould be a head or a tail, as the both are se-\nmantically equivalent.\n‚Ä¢ The involved subject and object are inter-\nchangeable. That is, the relation R and its\nconverse counterpart R‚ä§ should be semanti-\ncally plausible, though not equivalent. An\nexample of a relation we would avoid under\nthis criterion is native language, which as-\nsociates a person with a language. A language\ncannot logically be the subject of native\nlanguage, thereby disqualifying this relation.\nRelations of this sort could allow LLMs to\nrely on shortcut learning to solve tasks. For\ninstance, in the case of native language ,\nthe entity‚Äôs type inadvertently reveals the an-\nswer so that the LLMs may exploit this leaked\ninformation.\nWe manually select 17 relations from six\nwidely used knowledge graph datasets: WN18RR\n(Dettmers et al., 2018), FB15K-237 (Toutanova\nand Chen, 2015), Wikidata5M (only transductive\nsettings) (Wang et al., 2021), NELL-ONE (Xiong\net al., 2018), ICEWS14 (Garc√≠a-Dur√°n et al.,2018),\nConceptNet5 (Speer et al., 2017). For each relation,\nwe randomly sample 80 triples from correspond-\ning datasets and manually remove the triples that\nare not suitable for our task. Finally, we get 1240\ntriples in our benchmark, detailed breakdown of\nthe number of triples for each relation can be found\nin Appendix B.\n3 Experiment Setup\n3.1 Model and Metric\nWe evaluated three LLM families on our ConvRe\nbenchmark: OpenAI GPT-3 (Brown et al., 2020),\nAnthropic Claude (Anthropic, 2023), and Google\nFlan-T5 (Chung et al., 2022) (model details in Ap-\npendix C). Since we do not have enough credits\nfor the OpenAI APIs, we evaluate OpenAI GPT-4\non a subset of our benchmark for few-shot experi-\nments.‚Ä° We use the classification accuracy as our\n‚Ä°The subset is constructed by randomly sampling 20 triples\nfor each relation from the full set. In the case where the\n6936\nPrompt\nRead the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a\nconverse manner, unlike the conventional definition, you should carefully choose the answer.\nInstruction: (x, has part, y) indicates that x has a part called y.\nQuestion: (?, has part, solingen)\nA: Find an entity that solingen contains.\nB: Find an entity that has a part called solingen.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for\nthe ORDER of the entities in the instruction!\nAnswer:\nExpected Answer: B\nFigure 4: An illustration of zero-shot prompting with hint. Red color font indicates the hint.\nmain metric for both Re2Text and Text2Re tasks.\n3.2 Prompting Methods\nAs depicted in Zhang et al. (2023), different\nprompting methods can have a considerable im-\npact on the scaling trends of language models. To\naccount for this in our study, we utilize diverse\nprompting methods. Generally, we have zero-shot\nand few-shot prompting, each tailored with specific\ndesign elements. Detailed illustrations are provided\nin Table 2, 3 and 4. While we previously discussed\nthese from a motivation point of view, this sub-\nsection offers a closer look at the implementation\nspecifics.\nZero-shot We assess both normal and converse\nrelations mainly on the zero-shot setting, where\neach setting is coupled with regular and altered test\ntext (refer to the text variations in Section 2.4). For\nthe converse relation evaluation, we additionally\nequip the prompt with hint (Kojima et al., 2022).\nAn illustration of the hint used in our experiment\nis shown in Figure 4.\nFew-shot In this setting, we only apply the hard\nsettings, as documented in Table 3. The corre-\nsponding zero-shot tests (ID 3# for Text2Re and ID\n4# for Re2Text, detailed in Table2) are employed\nas baselines. The arrangements for the example\nvariants are thoroughly detailed in Table 4. Within\neach group, we have three distinct sub-settings: 3-\nshot, 3-shot with hint & Chain-of-Thought (CoT,\nWei et al. 2022b), and 6-shot.\nnumber of triples for a particular relation is less than 20, we\ninclude all of them. Ultimately, the subset comprises a total\nof 328 triples. We run GPT-4 on both full set and subset in\nzero-shot settings. Results show that the subset can reflect the\nmodel‚Äôs performance. Details can be found in Appendix D.\n4 Results\nIn this section, we demonstrate the results of dif-\nferent LLM families on ConvRe benchmark and\nprovide an in-depth analysis. More results on chat\nmodels can be found in Appendix E.\n4.1 Converse Relation\nOur first experiment, conducted in the zero-shot\nsetting, involves both normal and converse relations\nacross all model families. As shown in Figure 5,\nthe performance on converse relations, within the\nscope of unaltered test text, is consistently inferior\nto that on normal relations across all tested models\nand tasks. More specifically, we note a roughly\npositive scaling trend for normal relations and an\ninverse scaling trend for converse relations, despite\nsome outliers. The state-of-the-art LLM, GPT-4,\nunderperforms compared to smaller models, with\nits performance falling significantly below random-\nguess levels. We conjecture that larger models have\nstronger priors, causing them to rely more heavily\non memorized patterns from training data, which\ncan conflict with the given task.\n4.2 Text Variants\nAs introduced in Section 2.4, we are curious about\nLLMs‚Äô behaviours against text variants on the test\nand the few-shot examples.\nOur initial focus is the zero-shot setting (Fig-\nure 5). For normal relations, test variants cause\na noticeable performance drop. It means that if a\ngiven answer candidate fits the superficial pattern\nstated in the instruction, models are more likely to\nselect it although it could be incorrect. This sug-\ngests that LLMs tend to take shortcut learning even\nwithin conventional problem settings. For converse\nrelations, variants on the test text harm the perfor-\nmance on Re2Text while enhance it on Text2Re.\n6937\nada babbage curie davinci gpt-4\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n1#: normal-re, normal-text\n2#: normal-re, altered-text\n3#: converse-re, normal-text\n4#: converse-re, altered-text\n5#: converse-re, normal-text, hint\n6#: converse-re, altered-text, hint\n(a) GPT Zero-shot on Re2Text.\nclaude-instant-1 claude-1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy (%)\n (b) Claude Zero-shot on Re2Text.\nSmall Base Large XL XXL\n0.2\n0.4\n0.6\n0.8Accuracy (%)\n (c) Flan-T5 Zero-shot on Re2Text.\nada babbage curie davinci gpt-4\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy (%)\n1#: normal-re, normal-text\n2#: normal-re, altered-text\n3#: converse-re, normal-text\n4#: converse-re, altered-text\n5#: converse-re, normal-text, hint\n6#: converse-re, altered-text, hint\n(d) GPT Zero-shot on Text2Re.\nclaude-instant-1 claude-1\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy (%)\n (e) Claude Zero-shot on Text2Re.\nSmall Base Large XL XXL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n (f) Flan-T5 Zero-shot on Text2Re.\nFigure 5: Zero-shot results on ConvRe. Each experimental setting has been indexed with a unique ID that can be\nreferred to in Table 2. Sub-figures in the same row share the same figure legend, so we only display it once in the\nleftmost sub-figure to save space. The table version of the results can be found in Appendix I.\nada babbage curie davinci gpt-40.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n4#: zero-shot baseline\n7#: 3-shot, hard-hard\n8#: 3-shot, hard-hard, hint-CoT\n9#: 6-shot, hard-hard\n10#: 3-shot, regular-hard\n11#: 3-shot, regular-hard, hint-CoT\n12#: 6-shot, regular-hard\n(a) GPT Few-shot on Re2Text.\nclaude-instant-1 claude-1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n (b) Claude Few-shot on Re2Text.\nSmall Base Large XL XXL0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n (c) Flan-T5 Few-shot on Re2Text.\nada babbage curie davinci gpt-40.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy (%)\n3#: zero-shot baseline\n7#: 3-shot, hard-hard\n8#: 3-shot, hard-hard, hint-CoT\n9#: 6-shot, hard-hard\n10#: 3-shot, regular-hard\n11#: 3-shot, regular-hard, hint-CoT\n12#: 6-shot, regular-hard\n(d) GPT Few-shot on Text2Re.\nclaude-instant-1 claude-1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy (%)\n (e) Claude Few-shot on Text2Re.\nSmall Base Large XL XXL0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Accuracy (%)\n (f) Flan-T5 Few-shot on Text2Re.\nFigure 6: Few-shot results on ConvRe. Each experimental setting has been indexed with a unique ID that can be\nreferred to in Table 3. Sub-figures in the same row share the same figure legend, so we only display it once in the\nleftmost sub-figure to save space. Detailed settings on the text variants can be found in Table 4. For GPT-4, we only\ntest it on a subset of our benchmark. Due to Flan-T5‚Äôs weak ability to follow CoT instructions, we do not report the\nresults of Flan-T5 with hint and CoT prompting.\n6938\nThese findings lend strong support to our previous\nhypothesis presented in Section 2.4.\nIn the few-shot setting, the zero-shot baselines\nfor both tasks are set to be hard (see Table 3 and\n4). Generally, hard examples outperform standard\nexamples (hard-hard vs. regular-hard) on average\nacross different models on the two tasks. This can\nbe attributed to the fact that hard examples align\nmore consistently with the hard tests and effec-\ntively help models in avoiding bias and shortcut\nlearning.\n4.3 Shot Number\nExamples, particularly an increased number of\nexamples, are expected to outperform zero-shot\nprompting. However, we do not consistently ob-\nserve improvements across different models and\ntasks. Notably, GPT models demonstrate the most\nconsistent improvements, indicating superior in-\ncontext learning abilities among these models. In-\nterestingly, when using few-shot examples, the\nmodels mostly exhibit inverse scaling or inverted U-\nshaped scaling, which suggests that our benchmark\npresents a challenge for the current LLMs.\n4.4 Hint and CoT\nThe zero-shot experiments in Figure 5 indicate that\nthe use of hints in prompts typically yields improve-\nments for GPT and Flan-T5 models. However,\nclaude-1 stands out as an exception, appearing to\nbe negatively affected by the hint.\nIn the few-shot experiments, employing hints\nand the Chain-of-Thought (CoT) approach substan-\ntially boosts performance, particularly for larger\nmodels. GPT models exhibit positive scaling and\nU-shaped scaling on the Re2Text task. However,\nfor the Text2Re task, we still observe inverted U-\nshaped scaling for GPT models and inverse scaling\nfor Claude models. This indicates that LLMs still\nstruggle on this task even with strong prompting\nmethods. We also find that Flan-T5 cannot properly\nfollow CoT instructions, so we do not report the\nresults of Flan-T5 with hint and CoT prompting.\n5 Related Work\nStudies on LLMs have shown positive scaling\ntrends, whereby larger models generally perform\nbetter on downstream tasks (Brown et al., 2020;\nRae et al., 2021; Chowdhery et al., 2022; Srivas-\ntava et al., 2022; Liang et al., 2022). However,\nresearchers showed that model performance scal-\ning can deviate from naive expectations. Srivastava\net al. (2022) showed slower and less smooth trends,\nand that social biases sometimes scale inversely\nwith model size, a finding that is echoed in Parrish\net al. (2022). TruthfulQA (Lin et al., 2022) demon-\nstrated that while larger models can provide more\ninformative answers, they tend to be less truthful.\nMcKenzie et al. (2023) introduced the inverse scal-\ning challenge and collected tasks that are highly\natypical but still easily understandable by a human.\nWei et al. (2022a) uncovered the U-shaped scaling\ntrend by expanding the model scope for evaluation.\nZhang et al. (2023) proposed NeQA and showed\nthat this task exhibit inverse, U-shaped, or positive\nscaling with different prompt methods or model\nfamilies. Miceli-Barone et al. (2023) showed that\nLLMs fail to correctly generate Python code when\ndefault identifiers are swapped.\nRecent research has highlighted the issue of in-\nflated performance in LLMs. Geirhos et al. (2020)\ncoined the term shortcut learning, revealing models‚Äô\nreliance on superficial cues. Tu et al. (2020) stud-\nied the model‚Äôs robustness to spurious correlations,\nwhich refers to the prediction rules that work for\nthe majority examples but do not hold in general.\nLi et al. (2023b) found that LLMs tend to rely on\nshallow matching rather than understanding mathe-\nmatical concepts. Bender et al. (2021) highlighted\nthe importance of understanding the mechanism by\nwhich LLMs achieved state-of-the-art performance.\nPerez et al. (2021) showed that LLMs‚Äô few-shot\nability is often overestimated due to the use of large\nheld-out sets. Ji et al.(2023) surveyed the hallucina-\ntion problem in language generation, highlighting\nthe issue of factually incorrect output. Liu et al.\n(2023) identified attention glitches in Transformers,\nindicating a failure in capturing robust reasoning.\nBerglund et al. (2023) introduced the term reverse\ncurse, showing that LLMs trained on ‚ÄôA is B‚Äô fails\nto learn the reverse relationship ‚ÄôB is A‚Äô.\n6 Concolusion\nIn this paper, we present an investigation into\nLLMs‚Äô understanding of structured semantics,\nspecifically focusing on converse binary relations.\nBy introducing a novel benchmark, ConvRe, we\noffer a systematic and comprehensive evaluation\nsuite to observe the performance of LLMs across\ndiverse settings and prompting methods. We have\ncarried out a detailed experimental study and ob-\nserved various scaling trends that shed light on the\n6939\ncapabilities and limitations of LLMs. Our findings\nsuggest that LLMs often resort to shortcut learn-\ning and still face considerable challenges on our\nproposed benchmark, even when strong prompting\ntechniques are employed. Our work underscores\nthe importance of developing evaluation method-\nologies to improve the understanding of LLMs and\ntheir performance across various tasks.\nLimitations\nThis paper proposes a new benchmark ConvRe\nto evaluate the competence of LLMs in recogniz-\ning and processing converse relations. Due to the\nlimitation of the budget, we have evaluated three\nrepresentative LLM families on the subset of our\nbenchmark for some settings. We note that the\nLLM APIs may change over time. Although we\nhave set the sampling temperature to 0, we cannot\nfully guarantee the reproducibility of our results.\nAnother potential limitation is the prompting meth-\nods used in this work. To automatically evaluate\nthe model‚Äôs performance, we have followed the pre-\nvious studies and formatted the tasks as multiple-\nchoice question answering tests. This setting may\naffect the performance of smaller models. Finally,\ndue to the unknown data sources and pretraining\nmethods used for proprietary models (e.g., Claude\nand GPT), it‚Äôs difficult to arrive at a clear and com-\nprehensive understanding of the behaviors exhib-\nited by LLMs on our benchmark.\nEthics Statement\nOur work proposes a new benchmark to help reveal\nthe real capability of LLMs in formal language\noriented tasks. The triples in our benchmark are all\nextracted from publicly available and widely used\nknowledge graph dataset. We show that LLMs\nhave taken shortcut learning in these tasks and their\nperformance could be inflated. These findings may\nhelp users have a better understanding of LLMs\nand avoid the potential risks.\nAcknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Program of China (Grant\nNo. 2021YFB3300400) and National Natural Sci-\nence Foundation of China (Grant No. 62173017).\nReferences\nAnthropic. 2023. Introducing Claude.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610‚Äì623.\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita\nBalesni, Asa Cooper Stickland, Tomasz Korbak, and\nOwain Evans. 2023. The reversal curse: Llms trained\non\" a is b\" fail to learn\" b is a\". arXiv preprint\narXiv:2309.12288.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nS√©bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nEdgar Frank Codd. 1983. A relational model of data\nfor large shared data banks. Communications of the\nACM, 26(1):64‚Äì69.\n6940\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In Proceedings of the AAAI\nconference on artificial intelligence, volume 32.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022. Complexity-based prompt-\ning for multi-step reasoning. arXiv preprint\narXiv:2210.00720.\nAlberto Garc√≠a-Dur√°n, Sebastijan DumanÀáci¬¥c, and Math-\nias Niepert. 2018. Learning sequence encoders\nfor temporal knowledge graph completion. arXiv\npreprint arXiv:1809.03202.\nRobert Geirhos, J√∂rn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665‚Äì673.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1‚Äì38.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large\nlanguage models are zero-shot reasoners. ArXiv\npreprint, abs/2205.11916.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial fil-\nters of dataset biases. In International conference on\nmachine learning, pages 1078‚Äì1088. PMLR.\nJinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi\nYang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu\nCao, Ruiying Geng, et al. 2023a. Can llm already\nserve as a database interface? a big bench for large-\nscale database grounded text-to-sqls. arXiv preprint\narXiv:2305.03111.\nWeixian Waylon Li, Yftah Ziser, Maximin Coavoux,\nand Shay B. Cohen. 2023b. BERT is not the count:\nLearning to match mathematical statements with\nproofs. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 3581‚Äì3593, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. ArXiv preprint, abs/2211.09110.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214‚Äì3252, Dublin,\nIreland. Association for Computational Linguistics.\nBingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krish-\nnamurthy, and Cyril Zhang. 2023. Exposing atten-\ntion glitches with flip-flop language modeling. arXiv\npreprint arXiv:2306.00946.\nIan R McKenzie, Alexander Lyzhov, Michael Pieler,\nAlicia Parrish, Aaron Mueller, Ameya Prabhu, Euan\nMcLean, Aaron Kirtland, Alexis Ross, Alisa Liu,\net al. 2023. Inverse scaling: When bigger isn‚Äôt better.\narXiv preprint arXiv:2306.09479.\nAntonio Valerio Miceli-Barone, Fazl Barez, Ioannis\nKonstas, and Shay B Cohen. 2023. The larger they\nare, the harder they fail: Language models do not\nrecognize identifier swaps in python. arXiv preprint\narXiv:2305.15507.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4658‚Äì4664.\nOpenAI. 2023. GPT-4 technical report.\nAlicia Parrish, Angelica Chen, Nikita Nangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel Bowman. 2022. BBQ:\nA hand-built bias benchmark for question answering.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 2086‚Äì2105, Dublin,\nIreland. Association for Computational Linguistics.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In Ad-\nvances in Neural Information Processing Systems 34:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, pages 11054‚Äì11070.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\nArXiv preprint, abs/2112.11446.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the AAAI confer-\nence on artificial intelligence, volume 31.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdri√† Garriga-Alonso, et al. 2022. Beyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint\narXiv:2206.04615.\nKristina Toutanova and Danqi Chen. 2015. Observed\nversus latent features for knowledge base and text\ninference. In Proceedings of the 3rd workshop on\ncontinuous vector space models and their composi-\ntionality, pages 57‚Äì66.\n6941\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621‚Äì633.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176‚Äì194.\nJason Wei, Yi Tay, and Quoc V Le. 2022a. In-\nverse scaling can become u-shaped. arXiv preprint\narXiv:2211.02011.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint, abs/2201.11903.\nJiannan Xiang, Zhengzhong Liu, Yucheng Zhou, Eric P\nXing, and Zhiting Hu. 2022. Asdot: Any-shot data-\nto-text generation with pretrained language models.\narXiv preprint arXiv:2210.04325.\nWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,\nand William Yang Wang. 2018. One-shot relational\nlearning for knowledge graphs. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1980‚Äì1990.\nYuhui Zhang, Michihiro Yasunaga, Zhengping Zhou,\nJeff Z HaoChen, James Zou, Percy Liang, and Serena\nYeung. 2023. Beyond positive scaling: How negation\nimpacts scaling trends of language models. arXiv\npreprint arXiv:2305.17311.\nA Clarification about Similar\nTerminologies\nAs described in Tu et al. (2020), spurious correla-\ntion refers to the prediction rules that work for the\nmajority examples but do not hold in general. Su-\nperficial cues/biases/artifacts can be treated as un-\nintended correlations between input and output in\nexisting datasets, which are often introduced during\ndata collection or human annotation (Bender et al.,\n2021; Le Bras et al., 2020; Niven and Kao, 2019).\nThe shortcut used in this paper refers to decision\nrules that perform well on standard benchmarks but\nfail to transfer to more challenging testing condi-\ntions, such as real word scenarios (Geirhos et al.,\n2020).\nWhile these terms may have nuanced differences,\ntheir essence converges to the idea that models\nmight exploit unintended patterns in datasets, par-\nticularly those evident in the majority of examples.\nThis can harm their ability to generalize in open-\nworld scenarios. In this paper, we have introduced\ntextual variance in our benchmark to serve as adver-\nsarial test sets and incorporated the counterfactual\nassumption to assess the real task-level generaliza-\ntion capabilities of LLMs.\nB Benchmark Details\nTo meet the second condition for relations in Sec\n2.5, we merge the relationmother of personfrom\nNELL-ONE dataset with the relation father from\nWikidata5M to create a new relation called parent\nof. In this way, there are 17 relations in total, and\nthe detailed number of triples for each relation is\nshown in Table 5. The source knowledge graphs\nthese relations come from cover a wide range of\ndomains, such as socio-political and commonsense,\nwhich can ensure the diverseity of our dataset.\nC Model Family Details\nC.1 OpenAI GPT\nThe models we use in our experiments are mainly\nGPT-3 models (text-ada-001, text-babbage-001 and\ntext-curie-001), GPT-3.5 models (text-davinci-003\nand gpt-3.5-turbo) and GPT-4. GPT-3 models can\nunderstand and generate natural language. These\nmodels were superceded by the more powerful\nGPT-3.5 generation models. Among the GPT-3.5\nmodels, gpt-3.5-turbo has been optimized for chat\nbut also works well for traditional completion tasks.\nThe version of gpt-3.5-turbo we use in our exper-\niments is gpt-3.5-turbo-0301. GPT-4 is a large\nmultimodal model that can solve difficult problems\nwith greater accuracy than any of the models in\nOpenAI GPT family, and the version we use for\nour experiments is gpt-4-0314.\nC.2 Anthropic Claude\nClaude is capable of a wide variety of conversa-\ntional and text processing tasks, it can help with\nuse cases including summarization, search, cre-\native and collaborative writing. Claude comes with\ntwo different sizes: claude-1 and claude-instant-1.\nclaude-1 is the largest model in Claude family and\nideal for a wide range of complex tasks. claude-\ninstant-1 is a smaller model with far lower latency.\nBoth of the models are provided with many dif-\nferent sub-versions. Among them, claude-1.3\nand claude-instant-1.1 are used for our experi-\nments.\n6942\nRelation Numbers Source KG\nhypernym 80 WN18RR WordNet\nhas part 78 WN18RR WordNet\norganization, organization relationship, child 75 FB15K-237 FreeBase\nlocation, location, partially contains 77 FB15K-237 FreeBase\nathlete beat athlete 80 NELL-ONE NELL\nparent of (mother) 145 NELL-ONE NELL\nparent of (father) Wikidata5M WikiData\nrepresented by 79 Wikidata5M WikiData\nside effect 8 Wikidata5M WikiData\nhas facility 62 Wikidata5M WikiData\ninfluenced by 65 Wikidata5M WikiData\nowned by 51 Wikidata5M WikiData\nconsult 73 ICEWS14 ICEWS\npraise or endorse 78 ICEWS14 ICEWS\nmade of 80 ConceptNet5 ConceptNet\nused of 79 ConceptNet5 ConceptNet\nhas property 55 ConceptNet5 ConceptNet\nhas subevent 75 ConceptNet5 ConceptNet\nTotal 1240\nTable 5: The details of the relations in our ConvRe benchmark\nC.3 Google Flan-T5\nFlan-T5 is an enhanced version of T5 that has been\nfinetuned in a mixture of tasks. Unlike the OpenAI\nGPT model, Flan-T5 is an encoder-decoder model.\nThere are five models with different sizes in Flan-\nT5 family: Flan-T5-Small, Flan-T5-Base, Flan-\nT5-Large, Flan-T5-XL and Flan-T5-XXL. All five\nmodels are used in our experiments.\nD Subset Results\nTo verify that the constructed subset can unbias-\nedly reflect the performance of GPT-4 model, we\ncompare the performance of GPT-4 model on both\nbenchmark dataset and subset. The results are\nshown in Table 6. The performance of the GPT-\n4 model shows minimal differences between the\ncomplete set and the subset, confirming the validity\nof the subset.\nDataset Prompt\n1#\nPrompt\n2#\nPrompt\n3#\nPrompt\n4#\nRe2Text\ncomplete set (1240) 0.987 0.935 0.227 0.164\nsubset (328) 0.997 0.942 0.192 0.155\nText2Re\ncomplete set (1240) 0.936 0.953 0.171 0.144\nsubset (328) 0.942 0.951 0.171 0.165\nTable 6: The comparison results of GPT-4 model on the\ncomplete set and subset under zero shot settings.\nE Chat Model Performance\nAs chat models usually have a better ability to fol-\nlow instructions, they may demonstrate a different\nscaling trend on our benchmark. Therefore, we\nindependently evaluate and compare the two chat\nmodel families (i.e. OpenAI GPT and Anthropic\nClaude) on our benchmark. As GPT-4 is also opti-\nmized for chat, we include it for analysis as well.\nThe performances of the two families are shown in\nFigure 7.\nIn Re2Text task, it can be observed that few-shot\nwith Chain-of-Thought can significantly improve\nthe performance of GPT models. The accuracy of\nGPT-4 demonstrates a remarkable improvement,\nsoaring from below 0.2 in the zero-shot setting to\nsurpassing 0.9 in the Few-shot+Hint+CoT setting.\nChain-of-Thought is also helpful in improving the\nperformance of Claude-1.\nIn Text2Re task, GPT models exhibit a distinct\nand consistent inverse scaling trend in both zero-\nshot and few-shot settings when the relation is con-\nversed. However, the scaling trend of Claude mod-\nels is more intricate. Specifically, in zero-shot set-\ntings, Claude models demonstrate a positive scaling\ntrend in the majority of settings. In few-shot set-\ntings, on the contrary, an inverse scaling trend is\nexhibited by Claude models.\nF Model Behaviors\nThis section introduces the behaviors of different\nmodels that we observed during the experiments.\nUnder zero-shot settings, Claude and Flan-T5 can\n6943\ngpt-3.5-trubo gpt-4 claude-instant-1 claude-1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n(a) Chat Zero-shot on Re2Text.\ngpt-3.5-turbo gpt-4 claude-instant-1 claude-1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n (b) Chat Zero-shot on Text2Re.\ngpt-3.5-trubo gpt-4 claude-instant-1 claude-1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n(c) Chat Few-shot on Re2Text.\ngpt-3.5-turbo gpt-4 claude-instant-1 claude-1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (%)\n (d) Chat Few-shot on Text2Re.\nFigure 7: Zero-shot and Few-shot results of chat models on ConvRe. Each experimental setting has been indexed\nwith a unique ID that can be referred in Table 2. Sub-figures in the same row share the centeral figure legend.\ngenerate answers in the expected behavior. How-\never, text-ada-001 and text-babbage-001 fails\nin most cases, they tend to repeat our question or\ninstruction. In our experiments, if these two models\ndidn‚Äôt give a clear answer, we will treat the choice\nwith higher log probability in the first token as their\nanswers. In few-shot settings, nearly all models\nexcept Flan-T5 conform to the expected answer\nformat. The generated thoughts of Flan-T5 are usu-\nally shorter than the examples, and the format of\nits answer seldom aligns with the expected format.\nG Neutral Relation Results\nIn this section, we explore the impact of neutral\nrelations on ConvRe benchmark. Specifically, we\nchange the relation text to a more neutral name:\nrelation R, and then run the experiments on the\nsubset mentioned in Appendix D. The results are\nshown in Table 7.\nIt can be observed that altering symbols to adopt\nmore neutral names generally shows various effects\non the models. The performance of most models\nin prompt 3# and 4# (the challenging setup) is still\naround 50% or even worse. However, for Claude\nmodels, considerable improvements on converse re-\nlations (prompt 3#, 4#, 7# and 8#) can be observed\nin the Text2Re task, along with the performance\ndrop on normal relations (prompt 1# and 2#).\nIn conclusion, altering relation text to more neu-\ntral forms may help alleviate problems in under-\nstanding converse relations, but it carries the risk\nof harming the performance in normal relations.\nH Analysis of the Impact of Different\nEntity Pairs\nWe firstly extract all the triples with relation\nhypernym and run five different models on them\nwithin the hard setting (prompt 4#) of Re2Text\ntask. Then the overlap percentages of the wrongly\nanswered triples across the models are calculated.\nThe results are shown in Table 8. The diverse accu-\nracies and low overlap percentages of incorrectly\nanswered entity pairs indicate that different entity\npairs for the same relation indeed lead to different\nresults on different models.\nI The Table Version of the Results\nWe provide our entire experimental results in Table\n9 for better clarity.\nJ Prompt Examples\nFigure 8 to Figure 19 demonstrate the 12 kinds of\nprompts used in Re2Text tasks.\n6944\nModel Prompt 1# Prompt 2# Prompt 3# Prompt 4# Prompt 7# Prompt 8#\nRe2Text\ntext-ada-001 0.494 (-0.040) 0.509 (-0.003) 0.509 (-0.003) 0.515 (+0.015) 0.494 (-0.012) 0.500 (+0.003)\ntext-babbage-001 0.518 (+0.051) 0.537 (+0.074) 0.537 (-0.015) 0.527 (+0.012) 0.500 (-0.009) 0.466 (-0.019)\ntext-curie-001 0.527 (+0.006) 0.463 (-0.031) 0.448 (-0.046) 0.439 (-0.037) 0.500 (-0.009) 0.500 (-0.009)\ntext-davinci-003 0.857 (-0.006) 0.567 (-0.134) 0.659 (+0.074) 0.259 (+0.027) 0.101 (-0.054) 0.774 (+0.094)\ngpt-3.5-turbo 0.765 (-0.073) 0.616 (+0.134) 0.384 (-0.211) 0.229 (+0.083) 0.439 (+0.110) 0.716 (+0.253)\ngpt-4 0.985 (-0.003) 0.918 (-0.027) 0.561 (+0.335) 0.439 (+0.268) 0.317 (+0.088) 0.784 (-0.146)\nclaude-1 0.905 (+0.003) 0.777 (-0.031) 0.732 (+0.198) 0.537 (+0.165) 0.335 (-0.055) 0.848 (+0.031)\nclaude-instant-1 0.762 (+0.073) 0.613 (-0.152) 0.485 (+0.113) 0.384 (-0.122) 0.558 (-0.104) 0.777 (-0.144)\nflan-t5-small 0.546 (+0.025) 0.488 (+0.015) 0.546 (+0.006) 0.494 (+0.046) 0.506 (+0.027) -\nflan-t5-base 0.796 (-0.042) 0.329 (+0.061) 0.665 (-0.039) 0.201 (+0.049) 0.488 (+0.049) -\nflan-t5-large 0.634 (-0.061) 0.430 (-0.012) 0.558 (+0.003) 0.378 (+0.094) 0.253 (+0.146) -\nflan-t5-xl 0.875 (-0.046) 0.546 (-0.201) 0.518 (+0.216) 0.210 (+0.131) 0.183 (+0.137) -\nflan-t5-xxl 0.738 (-0.070) 0.591 (-0.095) 0.476 (+0.104) 0.290 (+0.064) 0.180 (+0.034) -\nText2Re\ntext-ada-001 0.482 (-0.021) 0.482 (-0.006) 0.485 (+0.003) 0.470 (-0.030) 0.500 (-0.009) 0.518 (-0.006)\ntext-babbage-001 0.530 (+0.006) 0.552 (+0.028) 0.451 (-0.049) 0.473 (-0.036) 0.500 (-0.009) 0.500 (-0.009)\ntext-curie-001 0.488 (-0.024) 0.500 (+0.021) 0.543 (+0.004) 0.509 (-0.055) 0.500 (-0.009) 0.500 (-0.012)\ntext-davinci-003 0.695 (-0.140) 0.640 (-0.144) 0.591 (+0.232) 0.564 (+0.174) 0.756 (+0.268) 0.921 (+0.171)\ngpt-3.5-turbo 0.506 (-0.095) 0.503 (+0.033) 0.503 (+0.101) 0.506 (+0.000) 0.418 (+0.025) 0.512 (-0.107)\ngpt-4 0.945 (+0.006) 0.899 (-0.058) 0.512 (+0.357) 0.494 (+0.351) 0.588 (+0.475) 0.674 (+0.186)\nclaude-1 0.506 (-0.299) 0.351 (-0.356) 0.857 (+0.241) 0.860 (+0.272) 0.677 (+0.451) 0.378 (+0.195)\nclaude-instant-1 0.780 (-0.095) 0.716 (-0.010) 0.808 (+0.564) 0.646 (+0.274) 0.790 (+0.622) 0.671 (+0.320)\nflan-t5-small 0.503 (+0.012) 0.500 (+0.006) 0.512 (+0.033) 0.509 (+0.015) 0.512 (-0.015) -\nflan-t5-base 0.494 (-0.018) 0.491 (-0.012) 0.506 (-0.003) 0.512 (-0.009) 0.500 (-0.046) -\nflan-t5-large 0.668 (-0.116) 0.665 (-0.024) 0.314 (+0.082) 0.363 (+0.025) 0.421 (+0.101) -\nflan-t5-xl 0.689 (-0.259) 0.530 (-0.250) 0.616 (+0.488) 0.628 (+0.378) 0.363 (+0.244) -\nflan-t5-xxl 0.841 (-0.135) 0.784 (-0.134) 0.253 (+0.243) 0.290 (+0.208) 0.183 (+0.156) -\nTable 7: The performance of LLMs on ConvRe benchmark after altering relation text to relation R. The number\nin the parentheses represents the difference between the neutral relation naming and normal naming under the same\nsetup on the same subset. We do not report the results of Flan-T5 as it struggles to follow the Chain-of-Thought\ninstructions.\ngpt-3.5-turbo gpt-4 claude-1 claude-instant-1 flan-t5-xxl\nAccuracy 77.50% 71.25% 100.00% 83.75% 47.50%\noverlap percentage of incorrectly answered entity pairs\ngpt-3.5-turbo - 11.25% 0.00% 1.25% 17.50%\ngpt-4 11.25% - 0.00% 2.50% 25.00%\nclaude-1 0.00% 0.00% - 0.00% 0.00%\nclaude-instant-1 1.25% 2.50% 0.00% - 0.00%\nflan-t5-xxl 17.50% 25.00% 0.00% 0.00% -\nTable 8: The accuracy of five different models on relation hypernym in the hard setting (prompt 4#) of Re2Text task.\nThe bottom part shows the overlap percentage of incorrectly answered entity pairs between these models.\n6945\nModel Prompt\n1#\nPrompt\n2#\nPrompt\n3#\nPrompt\n4#\nPrompt\n5#\nPrompt\n6#\nPrompt\n7#\nPrompt\n8#\nPrompt\n9#\nPrompt\n10#\nPrompt\n11#\nPrompt\n12#\nRe2Text\ntext-ada-001 0.506 0.504 0.504 0.495 0.493 0.481 0.498 0.494 0.470 0.496 0.481 0.468\ntext-babbage-001 0.524 0.484 0.556 0.532 0.511 0.499 0.500 0.484 0.500 0.500 0.471 0.500\ntext-curie-001 0.535 0.506 0.490 0.474 0.508 0.501 0.500 0.500 0.500 0.500 0.500 0.500\ntext-davinci-003 0.854 0.670 0.558 0.237 0.620 0.293 0.171 0.733 0.208 0.165 0.206 0.139\ngpt-3.5-turbo 0.835 0.490 0.590 0.156 0.683 0.194 0.317 0.502 0.389 0.231 0.394 0.367\ngpt-4 0.987 0.935 0.227 0.164 0.377 0.288 0.229 0.930 0.253 0.192 0.704 0.216\nclaude-instant-1 0.657 0.767 0.368 0.523 0.448 0.579 0.679 0.924 0.643 0.441 0.554 0.394\nclaude-1 0.897 0.787 0.514 0.373 0.466 0.502 0.409 0.840 0.398 0.377 0.560 0.417\nflan-t5-small 0.518 0.470 0.519 0.465 0.500 0.500 0.493 - 0.493 0.490 - 0.488\nflan-t5-base 0.846 0.252 0.730 0.170 0.680 0.215 0.447 - 0.480 0.464 - 0.489\nflan-t5-large 0.715 0.445 0.556 0.262 0.598 0.270 0.077 - 0.082 0.076 - 0.082\nflan-t5-xl 0.915 0.735 0.318 0.079 0.357 0.113 0.042 - 0.052 0.048 - 0.066\nflan-t5-xxl 0.794 0.663 0.366 0.207 0.444 0.332 0.147 - 0.152 0.152 - 0.147\nText2Re\ntext-ada-001 0.502 0.496 0.482 0.498 0.496 0.501 0.500 0.554 0.506 0.500 0.507 0.507\ntext-babbage-001 0.534 0.527 0.486 0.489 0.485 0.485 0.500 0.500 0.499 0.500 0.500 0.498\ntext-curie-001 0.498 0.481 0.533 0.543 0.500 0.500 0.500 0.500 0.500 0.500 0.502 0.500\ntext-davinci-003 0.838 0.802 0.348 0.399 0.497 0.509 0.531 0.694 0.606 0.451 0.751 0.446\ngpt-3.5-turbo 0.607 0.484 0.390 0.498 0.452 0.490 0.498 0.559 0.524 0.381 0.627 0.417\ngpt-4 0.936 0.953 0.171 0.144 0.184 0.171 0.183 0.527 0.354 0.113 0.488 0.165\nclaude-instant-1 0.872 0.744 0.262 0.331 0.466 0.480 0.256 0.502 0.379 0.190 0.373 0.252\nclaude-1 0.823 0.744 0.566 0.556 0.477 0.420 0.177 0.344 0.197 0.207 0.166 0.142\nflan-t5-small 0.501 0.498 0.495 0.498 0.500 0.500 0.528 - 0.548 0.528 - 0.544\nflan-t5-base 0.512 0.498 0.502 0.526 0.481 0.481 0.529 - 0.500 0.535 - 0.499\nflan-t5-large 0.773 0.696 0.212 0.296 0.366 0.417 0.315 - 0.388 0.288 - 0.363\nflan-t5-xl 0.906 0.784 0.178 0.246 0.155 0.266 0.198 - 0.230 0.180 - 0.207\nflan-t5-xxl 0.978 0.932 0.048 0.086 0.027 0.101 0.027 - 0.031 0.038 - 0.029\nTable 9: The table of our entire experiments.\nPrompt\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that x has a part called y.\nQuestion: (?, has part, solingen)\nA: Find an entity that solingen contains.\nB: Find an entity that has a part called solingen.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 8: Prompt Design 1#\nPrompt\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that x has a part called y.\nQuestion: (?, has part, solingen)\nA: Find an entity that is a part of solingen.\nB: Find an entity that possesses a specific component named solingen.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 9: Prompt Design 2#\n6946\nPrompt\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that possesses a specific component named solingen.\nB: Find an entity that is a part of solingen.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 10: Prompt Design 3#\nPrompt\nRead the instruction and then answer the question using A or B.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 11: Prompt Design 4#\nPrompt\nRead the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a\nconverse manner, unlike the conventional definition, you should carefully choose the answer.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that possesses a specific component named solingen.\nB: Find an entity that is a part of solingen.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for\nthe ORDER of the entities in the instruction!\nAnswer:\nExpected Answer: B\nFigure 12: Prompt Design 5#\nPrompt\nRead the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a\nconverse manner, unlike the conventional definition, you should carefully choose the answer.\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for\nthe ORDER of the entities in the instruction!\nAnswer:\nExpected Answer: B\nFigure 13: Prompt Design 6#\n6947\nPrompt\nRead the instruction and then answer the question using A or B.\n[Example1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that works for anthony fauci.\nB: Find an entity that anthony fauci is employed by.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity that is smaller than elephant.\nB: Find an entity that is bigger than elephant.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity that is in the north of china.\nB: Find an entity that is in the south of china.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 14: Prompt Design 7#\n6948\nPrompt\nRead the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a\nconverse manner, unlike the conventional definition, you should carefully choose the answer. Your answer should be in\nJSON format with the following keys: thought, answer.\n[Example1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that works for anthony fauci.\nB: Find an entity that anthony fauci is employed by.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates\ny works for x. According to the question, y is anthony fauci, and therefore anthony fauci works for x. So x is the\nemployer of anthony fauci, the answer is B.\", ‚Äôanswer‚Äô: ‚ÄôB‚Äô}\n[Example2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity that is smaller than elephant.\nB: Find an entity that is bigger than elephant.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates y is\nbigger than x. According to the question, y is elephant, and therefore elephant is bigger than x. So x is smaller than\nelephant, the answer is A.\", ‚Äôanswer‚Äô: ‚ÄôA‚Äô}\n[Example3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity that is in the north of china.\nB: Find an entity that is in the south of china.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates y is\nin the south of x. According to the question, y is china, and therefore china is in the south of x. So x is in the north\nof china, the answer is A.\", ‚Äôanswer‚Äô: ‚ÄôA‚Äô}\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for\nthe ORDER of the entities in the instruction!\nAnswer:\nExpected Answer: B\nFigure 15: Prompt Design 8#\n6949\nPrompt\nRead the instruction and then answer the question using A or B.\n[Example1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that works for anthony fauci.\nB: Find an entity that anthony fauci is employed by.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity that is smaller than elephant.\nB: Find an entity that is bigger than elephant.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity that is in the north of china.\nB: Find an entity that is in the south of china.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example4]\nInstruction: (x, teach, y) indicates that y teaches x.\nQuestion: (?, teach, andy bramante)\nA: Find a person that teaches andy bramante.\nB: Find a person that is the student of andy bramante.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example5]\nInstruction: (x, interviewed, y) indicates that y interviewed x.\nQuestion: (?, interviewed, biden)\nA: Find a person that biden conducted an interviewed with.\nB: Find a person that interviewed biden.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example6]\nInstruction: (x, successor, y) indicates that y is the successor of x.\nQuestion: (?, successor, barack obama)\nA: Find a person that is the successor of barack obama.\nB: Find a person that is the predecessor of barack obama.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 16: Prompt Design 9#\n6950\nPrompt\nRead the instruction and then answer the question using A or B.\n[Example1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that is employed by anthony fauci.\nB: Find an entity that anthony fauci works for.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity so that elephant is bigger than it.\nB: Find an entity so that elephant is smaller than it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity so that china is in the south of it.\nB: Find an entity so that china is in the north of it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 17: Prompt Design 10#\n6951\nPrompt\nRead the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a\nconverse manner, unlike the conventional definition, you should carefully choose the answer. Your answer should be in\nJSON format with the following keys: thought, answer.\n[Example 1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that is employed by anthony fauci.\nB: Find an entity that anthony fauci works for.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates y\nworks for x. According to the question, y is anthony fauci, and therefore anthony fauci works for x, the answer is B.\",\n‚Äôanswer‚Äô: ‚ÄôB‚Äô}\n[Example 2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity so that elephant is bigger than it.\nB: Find an entity so that elephant is smaller than it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates y\nis bigger than x. According to the question, y is elephant, and therefore elephant is bigger than x, the answer is A.\",\n‚Äôanswer‚Äô: ‚ÄôA‚Äô}\n[Example 3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity so that china is in the south of it.\nB: Find an entity so that china is in the north of it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: {‚Äôthought‚Äô: \"Let‚Äôs think step by step. Firstly, the question is asking for x. Then, the instruction indicates y\nis in the south of x. According to the question, y is china, and therefore china is in the south of x, the answer is A.\",\n‚Äôanswer‚Äô: ‚ÄôA‚Äô}\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for\nthe ORDER of the entities in the instruction!\nAnswer:\nExpected Answer: B\nFigure 18: Prompt Design 11#\n6952\nPrompt\nRead the instruction and then answer the question using A or B.\n[Example1]\nInstruction: (x, works for, y) indicates that y works for x.\nQuestion: (?, works for, anthony fauci)\nA: Find an entity that is employed by anthony fauci.\nB: Find an entity that anthony fauci works for.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example2]\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\nQuestion: (?, bigger than, elephant)\nA: Find an entity so that elephant is bigger than it.\nB: Find an entity so that elephant is smaller than it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example3]\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\nQuestion: (?, in the south of, china)\nA: Find an entity so that china is in the south of it.\nB: Find an entity so that china is in the north of it.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example4]\nInstruction: (x, teach, y) indicates that y teaches x.\nQuestion: (?, teach, andy bramante)\nA: Find a person that andy bramante is the student of.\nB: Find a person that andy bramante teaches.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\n[Example5]\nInstruction: (x, interviewed, y) indicates that y interviewed x.\nQuestion: (?, interviewed, biden)\nA: Find a person that biden interviewed.\nB: Find a person that conducted an interview with biden.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: A\n[Example6]\nInstruction: (x, successor, y) indicates that y is the successor of x.\nQuestion: (?, successor, barack obama)\nA: Find a person that barack obama is the predecessor of.\nB: Find a person that barack obama is the successor of.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer: B\nInstruction: (x, has part, y) indicates that y has a part called x.\nQuestion: (?, has part, solingen)\nA: Find an entity that has a part called solingen.\nB: Find an entity that solingen contains.\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\nAnswer:\nExpected Answer: B\nFigure 19: Prompt Design 12#\n6953",
  "topic": "Converse",
  "concepts": [
    {
      "name": "Converse",
      "score": 0.810370922088623
    },
    {
      "name": "Computer science",
      "score": 0.6332199573516846
    },
    {
      "name": "Natural language processing",
      "score": 0.46308690309524536
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4545218050479889
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43989962339401245
    },
    {
      "name": "Mathematics",
      "score": 0.10007107257843018
    },
    {
      "name": "Geography",
      "score": 0.08737200498580933
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}