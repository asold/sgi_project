{
    "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
    "url": "https://openalex.org/W4385570881",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2247873162",
            "name": "Albert Xu",
            "affiliations": [
                "University of Southern California",
                "Southern California University for Professional Studies"
            ]
        },
        {
            "id": "https://openalex.org/A2108009659",
            "name": "Xiang Ren",
            "affiliations": [
                "Southern California University for Professional Studies",
                "University of Southern California"
            ]
        },
        {
            "id": "https://openalex.org/A2476502706",
            "name": "Robin Jia",
            "affiliations": [
                "University of Southern California",
                "Southern California University for Professional Studies"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3196715549",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W3010293452",
        "https://openalex.org/W2998184481",
        "https://openalex.org/W3198218876",
        "https://openalex.org/W4287724856",
        "https://openalex.org/W2119880843",
        "https://openalex.org/W2901114541",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3173591450",
        "https://openalex.org/W2973218493",
        "https://openalex.org/W2891575196",
        "https://openalex.org/W3121064530",
        "https://openalex.org/W4223977507",
        "https://openalex.org/W2963149653",
        "https://openalex.org/W3213116797",
        "https://openalex.org/W4287262235",
        "https://openalex.org/W4221167530",
        "https://openalex.org/W2070246124",
        "https://openalex.org/W3013379031",
        "https://openalex.org/W3159630167",
        "https://openalex.org/W2952409498",
        "https://openalex.org/W2963875483",
        "https://openalex.org/W4286903289",
        "https://openalex.org/W2475167333",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W3171163219",
        "https://openalex.org/W3014773921",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W3174540647",
        "https://openalex.org/W2101946573",
        "https://openalex.org/W2979332623",
        "https://openalex.org/W3035224069",
        "https://openalex.org/W3161236001",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W2951883849",
        "https://openalex.org/W4288099429",
        "https://openalex.org/W2952140516",
        "https://openalex.org/W2963924212",
        "https://openalex.org/W2618169590",
        "https://openalex.org/W3199830576",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W4283819124",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W4205725534",
        "https://openalex.org/W2594432108",
        "https://openalex.org/W3035441651",
        "https://openalex.org/W4281955239",
        "https://openalex.org/W2963238274"
    ],
    "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 11778–11801\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nContrastive Novelty-Augmented Learning: Anticipating Outliers with\nLarge Language Models\nAlbert Xu Xiang Ren Robin Jia\nUniversity of Southern California\n{albertxu,xiangren,robinjia}@usc.edu\nAbstract\nIn many task settings, text classification mod-\nels are likely to encounter examples from novel\nclasses on which they cannot predict correctly.\nSelective prediction, in which models abstain\non low-confidence examples, provides a pos-\nsible solution, but existing models are often\noverly confident on unseen classes. To remedy\nthis overconfidence, we introduce Contrastive\nNovelty-Augmented Learning (CoNAL), a two-\nstep method that generates OOD examples rep-\nresentative of novel classes, then trains to de-\ncrease confidence on them. First, we generate\nOOD examples by prompting a large language\nmodel twice: we prompt it to enumerate rel-\nevant novel classes, then generate examples\nfrom each novel class matching the task for-\nmat. Second, we train a classifier with a novel\ncontrastive objective that encourages lower con-\nfidence on generated OOD examples than train-\ning examples. When trained with CoNAL, clas-\nsifiers improve in their ability to detect and ab-\nstain on novel class examples over prior meth-\nods by an average of 2.3% in terms of accuracy\nunder the accuracy-coverage curve (AUAC)\nand 5.5% AUROC across 4 NLP datasets, with\nno cost to in-distribution accuracy.1\n1 Introduction\nRecent progress in NLP has led to text clas-\nsification models that are accurate not only\nin-distribution (ID), but also on some out-of-\ndistribution (OOD) data (Arora et al., 2021).\nNonetheless, some categories of real-world dis-\ntribution shift still pose serious challenges. For\ninstance, in open-set label shift (Garg et al., 2022),\nthe test data includes examples from novel classes\nnot present in the training data, making it impos-\nsible for a standard classifier to predict correctly\n(Scheirer et al., 2013). Moreover, novel class exam-\nples can be difficult to detect with conventional\nOOD detection methods, as they typically bear\n1Code is available at github.com/albertkx/CoNAL.\na strong surface resemblance to training exam-\nples ( ¸ Tifrea et al., 2021). In this paper, we frame\nopen-set label shift as a selective prediction prob-\nlem (El-Yaniv and Wiener, 2010; Geifman and El-\nYaniv, 2017) that we call open-set selective clas-\nsification (OSSC). OSSC requires text classifiers\nto predict correctly on closed-set examples while\nabstaining on novel class examples.\nTo perform well on OSSC, a classifier must\nhave lower confidence on novel class examples\nthan closed-set examples by learning features\nwhich differentiate novel classes from closed-set\nclasses (Perera et al., 2020). In order to supervise\nthis representation learning, it is useful to identify\nwhat examples from novel classes might look like.\nPrior work has explored automatically generating\nOOD images by adding random perturbations to\nID examples (Setlur et al., 2022). Text inputs, how-\never, are composed of discrete tokens, and modi-\nfying even a single token can unpredictably alter\nthe meaning of a sentence. We seek an automatic\ngeneration method that addresses these limitations,\nleveraging the generative ability of large language\nmodels (LLMs) like GPT-3 (Brown et al., 2020).\nLLMs are a desirable source for novelty, as their\ngeneration is informed by a broad corpus of ex-\namples seen during pretraining, allowing them to\nreliably generate from classes outside a dataset.\nWe present Contrastive Novelty-Augmented\nLearning (CoNAL), a method to improve the OSSC\nability of a classifier by automatically generating\nOOD examples, then training to abstain on them.\nTo generate a diverse set of OOD examples that an-\nticipate different potential test-time shifts, we intro-\nduce Novelty Prompting, a method that augments\na source dataset with novel class examples gener-\nated by a LLM. We first perform label generation,\nprompting our LLM to extend the closed-set labels\nwith novel labels. We then prompt the LLM to\ngenerate new examples conditioned on each novel\nlabel to form a large set of probable novel examples.\n11778\nNovelty Prompting CCL Training Selective Prediction\n✅\nYankees Unlikely \nto Get D-Rays…\nA new report \nfrom the \nCanadian Food…\nSports\nWorld\nBusiness\nSports\nWorld\nBusiness\nKyrie Irving \nreturns to \nplay in…\nThe Cartwheel \nGalaxy Is \nthe…\n\"\nGenerate a diverse list of news categories: \nworld, sports, business, ID Labels\nscience, crime, travel, auto, food\nOOD Labels\nGiven a label, generate a corresponding example: \nworld            Bush, Kerry Trade Barbs Following… \nsports           Yankees Unlikely to Get D-Rays… \nbusiness        ECB sees gradual recovery in eurozone…\nExample Generation\nGeneration\nLabel Generation\nDID\nDOOD\nfood          A new report from the Canadian Food…\nFigure 1: Contrastive Novelty-Augmented Learning pipeline. We Novelty Prompt a generator model to produce\na novel set DOOD, then train with a contrastive confidence loss (CCL) on our original train set DID and DOOD,\nensuring that our classifier is less confident on generated novel examples than closed-set examples. Finally, we\nabstain when the model is not very confident on any label.\nFinally, we propose a contrastive confidence loss\n(CCL) for training, which encourages both high\naccuracy on the ID training set and lower relative\nconfidence on the generated novel examples. We\nshow that CCL outperforms stricter losses like Out-\nlier Exposure (Hendrycks et al., 2019), which can\nadversely affect ID accuracy. Our full pipeline is\nshown in Figure 1. Our method can be viewed as a\nform of “partial” knowledge distillation: we lever-\nage an LLM “teacher model” to improve novelty\ndetection performance without altering the student\nmodel’s ID classification ability.\nWe evaluate CoNAL against state-of-the-art\nOOD detection baselines across 14 splits of 4\ndatasets—AGNews (Zhang et al., 2015), TREC-\n10 (Li and Roth, 2002), TACRED (Zhang et al.,\n2017), and Emotion (Saravia et al., 2018)—finding\nthat it improves both OOD detection and OSSC, by\nan average of 5.5% AUROC and 2.3% in terms of\narea under the accuracy-coverage curve (AUAC)\nover the best prior method. These improvements\ncome at no cost to ID accuracy, demonstrating that\nit is possible to distill novelty detection alone with-\nout affecting predictive power. Finally, we analyze\nthe settings in which CoNAL can improve OSSC\nperformance. In the data dimension, scale is often\noptional: with as few as 1000 generated examples,\nour method outperforms vanilla training on all 4\ndatasets. LLM size has a larger effect on perfor-\nmance: on some datasets only a sufficiently large\nmodel can generate useful examples.\n2 Problem Setting\n2.1 Open-Set Selective Classification\nIn standard classification, an optimal model f\nshould predict the ground-truth label y of an in-\nput example xfrom a closed set of known labels\nYID. However, under a more realistic open-set set-\nting, some test examples are drawn from unknown\nnovel classes YOOD. Without a priori knowledge\nof YOOD, a standard discriminative classifier will\nnever correctly classify a novel example. Instead,\nan optimal open-set selective classifier f should\npredict ywhen y∈YID, and abstain otherwise.\nFor a probabilistic model pθ(y |x) and associ-\nated confidence metric, the prediction is given by\nf(x) = (ˆy,c), where ˆy = arg maxy∈YID pθ(y |x)\nand cdenotes the model’s confidence. When used\nas a selective classifier with threshold γ, f predicts\nˆy when c > γand abstains otherwise (Geifman\nand El-Yaniv, 2017). This differs from OOD detec-\ntion (Hendrycks and Gimpel, 2017) in that f must\nabstain on both novel examples and its own errors\nand must attain high ID accuracy.\n2.2 Evaluation Protocol\nWe holistically measure selective classification\nperformance with the area under the accuracy-\ncoverage curve (AUAC). The accuracy-coverage\ncurve plots accuracy as a function of the fraction\nof examples on which the model predicts (i.e., cov-\nerage) as the confidence threshold γ varies. For\naccuracy computation, we treat predictions on all\nnovel class examples as incorrect. AUAC measures\nthe combined ability of a model in ID classification\naccuracy, ID calibration, and OOD detection.\nThough we deviate from prior work and report\nAUAC, to demonstrate that CoNAL is still effective\nat OOD detection, we also compute the Area under\nthe ROC (AUROC). AUROC measures a model’s\nability to detect when a test example xis of a novel\nclass (y∈YOOD). Higher is better: 50% AUROC\n11779\nis random, and 100% is perfect.\n3 Method: CoNAL\nHere we describe Contrastive Novelty-Augmented\nLearning, a method for automatically improving\nOSSC. At a high level, we generate novel examples\nand then train our model to be less confident on\ngenerated novel examples than closed-set examples.\nWe first describe desiderata for useful novelty, then\nintroduce a two-phased novel example generation\nmethod, Novelty Prompting, and finally introduce\na contrastive confidence loss for classifier training.\nWe illustrate the method in Figure 1.\n3.1 Novelty Prompting\nDesiderata of Novelty Generation Inspired by\nprevious work which utilize known, representative\nOOD data to train selective prediction and OOD\ndetection models (Kamath et al., 2020; Hendrycks\net al., 2019), we focus on creating an generated\n“novel set” that is representative of potential label\nshifts at test time. The “novel set” must be (1)plau-\nsible, meaning that it should bear a surface resem-\nblance to the training data, e.g., we should create\nnews examples for a news dataset, and (2) seman-\ntically novel, meaning that these examples should\nbe from new classes. In other words, an example\nis novel if it demonstrates a semantic shift (Arora\net al., 2021), but shares non-semantic features with\nexamples in the training set. For example, select-\ning data from an entirely separate dataset, as is\ndone in Hendrycks et al. (2019), violates plausibil-\nity. Meanwhile simply editing surface features or\nrecombining examples as is done in mixup (Zhang\net al., 2018) might induce a distribution shift but\nwould not result in semantic novelty.\nTo satisfy these desiderata, we propose a two-\nstage generation method called Novelty Prompting\n(NP). To encourage semantic novelty, we first gen-\nerate novel labels given a dataset’s extant labels.\nWe then show existing examples to a language\nmodel (to encourage plausibility) and ask it to gen-\nerate a new example conditioned on one of the new\nlabels. Figure 1 shows both prompt formats.\nLabel Generation. Though prompting with large\nautoregressive language models (LLMs) like GPT-\n3 has typically been explored in the context of few\nand zero-shot learning to perform standard NLP\ntasks (Brown et al., 2020), we find that LLMs are\nalso capable of “expanding” a set of topically re-\nlated concepts that might realistically co-occur via\nsequence continuation.\nWe leverage this capability to generate novel la-\nbels. We prompt the largest GPT-3 model available\n(Davinci) with a task-specific instruction and the\nconcatenation of the normalized known ( YID) la-\nbels.2 Taking the union over continuations of one\nor more novel labels N times, we obtain a diverse\n“novel label set.” We combine multiple completions\nbecause in preliminary experiments, we observed\nthat single completions tend to overgenerate labels\nfrom a narrow subcategory of classes. To remedy\nconcerns about data leakage due to dataset exam-\nples of the true unknown class possibly appearing\nin LLM pretraining, we remove instances of the\ngold novel label(s) from this set. In practice, pre-\ndicting the true novel test-time labels is both per-\nmissible and desirable, so our experimental setup\nlikely underestimates our method’s performance.\nFinally, we filter out generated labels that are\nclosely related to ID labels. For example, if joy\nappears in the ID labels, we remove synonyms like\nhappiness. We use a large online thesaurus 3 to\nremove synonyms from the final novel label set. We\nanalyze the impact of filtering in Appendix A.10.\nExample Generation. To generate each novel\nexample, we randomly sample a novel label from\nour set and prompt a LLM (we use GPT-J4) to gen-\nerate an example of that label. We prime this model\nwith one random sampled label-example pair from\neach ID class in the training dataset in the prompt,\nresulting in 3-6 in-context examples, varying based\non the dataset. Providing these context pairs en-\nsures that our generation is plausible: the model\nis encouraged to generate a specific style of text.\nWe perform this generation procedure repeatedly\nto form a novel example set. We show the prompt\nwe use for this step in Appendix A.3, and several\ngenerated label-example pairs in Figure 2.\n3.2 Contrastive Confidence Loss Training\nOur second contribution is an improved loss func-\ntion for training models to have lower confidence\non OOD examples than ID examples. Prior work\nhave used the Outlier Exposure (OE; Hendrycks\n2Though this requires some human intervention, it both\n(1) satisfies the true zero-shot nature of test-time label shift\nas it requires no knowledge of the unknown labels and (2)\nrequires minimal effort, typically only involving converting\nan abbreviation label such as LOC into Location.\n3https://moby-thesaurus.org/\n4We evaluate GPT-3 for label generation but not example\ngeneration, as the latter would require many more API calls.\n11780\net al., 2019) objective, which encourages the model\nf to output a uniform probability distribution over\nclosed-set classes when given a novel example x.\nOE can be successfully applied to train models on\nOOD data gathered from a different dataset (e.g.,\nWikitext), as there is very little risk of this data\noverlapping with ID data. In contrast, we automat-\nically generate plausible novel examples, which\nruns the risk that some novel examples will be in-\ndistribution. Since OE encourages models to have\nthe lowest possible confidence on novel examples,\nit can hurt predictive accuracy when some exam-\nples xresemble closed-set examples. Instead, we\nseek a solution which treats outliers flexibly.\nWe propose a novel contrastive confidence loss\n(CCL) that encourages models to be less confident\non OOD examples than ID examples. This is a less\nstrict objective as models can achieve minimum\nloss without predicting a perfectly uniform distri-\nbution for the generated novel examples. For an\ninput x, let pθ(y|x) be the model’s predicted dis-\ntribution over YID. Let cθ(x) = maxy∈YID pθ(y |\nx), the Maximum Softmax Probability (MaxProb;\nHendrycks and Gimpel, 2017), which we use as\nour confidence metric. Finally, let ℓ denote the\ncross-entropy loss with a one-hot target vector, and\nDID and DOOD denote the training set and novel\nset respectively. We define CCL as follows:\nL(θ) =E(xid,yid)∼DID [ℓ(pθ(y|xid),yid)]+\nλExid∼DID,xood∼DOOD [max(0,cθ(xood) −cθ(xid))].\nThat is, we penalize the confidence of novel\nexamples which have higher confidence than any\nclosed-set example. While this still induces our\nmodel to learn lower confidence on novel examples,\nit simultaneously permits our model to learn that\nsome novel examples should have lower confidence\nthan others, rather than learn minimal confidence\non all members of the generated novel set. In prac-\ntice, we obtain an unbiased estimate of the second\nterm by sampling a batch of nID and nOOD ex-\namples at each step and computing the second term\npairwise between each of the n2 ID-OOD example\npairs. We arbitrarily choose λ = 1.0, weighting\nthe two terms of the objective equally.\n4 Experimental Setup\n4.1 Datasets\nWe construct artificial dataset splits from 4 popular\nNLP classification datasets by holding out one or\nmore labels from training and moving all exam-\nples of that label to the test split, removing classes\nthat are too small to yield statistical significance\nin our evaluations. Specifically, we use a question\nintent detection dataset, TREC-10 (Li and Roth,\n2002) and construct 5 splits. We also use two pop-\nular topic classification datasets, AGNews (Zhang\net al., 2015), a news classification dataset, and Emo-\ntion (Saravia et al., 2018), a tweet classification\ndataset. We construct 4 splits for each. Finally, we\nuse TACRED (Zhang et al., 2017), a strongly class-\nimbalanced sentence relation-classification dataset\nwith 41 possible relations. We construct a single\nsplit where we hold out the 35 smallest classes. Ap-\npendix A.8 contains further dataset details. Results\nfor each dataset are averaged across all splits.\n4.2 Experimental Details\nFor Novelty Prompting, we perform label genera-\ntion using the best available GPT-3 model, GPT-3\nDavinci (Brown et al., 2020) and example genera-\ntion with a smaller GPT-J 6B model (Komatsuzaki,\n2021). For the novel set, we perform 5 label gen-\neration iterations, then generate 100,000 examples\n(after filtering). We train BERT-base classifiers\nwith CCL for 5000 steps and batch size n = 40.\nOn TACRED, we permit only generations contain-\ning exactly two entities, one a subject and the other\nan object, filtering out roughly 90% of generations,\nas this is a hard constraint for relation extraction.\nWe detail datasets in Appendices A.8 and A.9.\n4.3 Baselines\nWe evaluate our method against baselines from\nprior work, CCL baselines with other novel sets,\nand Outlier Exposure (Hendrycks et al., 2019).\nThough two methods (kFolden and Constrative)\ncan address arbitrary distribution shifts, we evalu-\nate them here only on the open-set shift setting. For\nall methods, we train a BERT-base model and use\nhyperparameters from the original papers unless\notherwise specified. Of the baselines, only CCL\nand Outlier Exposure use explicit novel sets.\nVanilla. We evaluate vanilla cross-entropy loss\ntraining, calculating confidence using MaxProb.\nkFolden. We evaluate kFolden (Li et al., 2021),\na method that trains an ensemble of k individual\nclassifiers, each trained onk−1 labels. The average\nof the ensemble probability distributions is used for\nconfidence computation.\nContrastive. We evaluate Contrastive OOD Detec-\ntion (Zhou et al., 2021), which uses a contrastive\n11781\nobjective to induce training examples of different\nclasses to be distant and of the same class to be near.\nThis sparsifies the embedding space, ensuring that\nmost OOD examples are far from feature repre-\nsentations of ID samples. We use the supervised\nconstrastive loss and the Mahalanobis distance met-\nric for confidence computation, finding that this\nsetup performed the best on our evaluation.\nCCL + Zero/Few-Shot Data Augmentation. To\nmeasure the impact of explicitly prompting for\nnovel labels, we generate with an identical pre-\ntrained GPT-J model, but prompt with only an in-\nstruction and one (or zero) ID training example\nfrom each class (See Appendix A.3 for the specific\nprompt format). Essentially, we perform exam-\nple generation identically but skip label generation\nentirely. We perform CCL training and MaxProb\ninference. While some resultant generations will\nbe useful, we expect that many will not be semanti-\ncally novel, resulting in strictly worse performance.\nCCL + Wikitext. To measure whether plausibility\nof examples impacts their usefulness for CCL, we\nuse an entirely different dataset, Wikitext-103, as\nour novel set. Though these examples represent a\ndistribution shift, they do not accurately reflect the\nopen-set shift the classifier will encounter.\nOutlier Exposure + Novelty Prompting. We\npair our novel set with Outlier Exposure (OE;\nHendrycks et al., 2019) as described in Section 3.2\nand compute confidence with MaxProb.\n5 Results\n5.1 OSSC Results\nCoNAL outperforms prior work. We report\ncomparisons of CoNAL against baselines in Ta-\nble 1. Broadly, we find that while baselines like\nkFolden and Contrastive training struggle to con-\nsistently outperform vanilla training (e.g., on TA-\nCRED), CoNAL improves selective classification\nover vanilla across all datasets. We outperform the\nbest prior method (Contrastive) by 2.3% AUAC,\nand on three of four datasets, our method signifi-\ncantly outperforms all prior methods. Furthermore,\nwe outperform kFolden by 3.6% AUAC despite\nits ensemble totaling many times the capacity of\nour single classifier. CoNAL also results in zero\nor little accuracy drop (less than 0.2 points) for\nall datasets. In Appendix A.4, we show full ID\naccuracy results for all datasets.\nOther choices of novel set for CCL training\ncan still be beneficial. Prompting with only a\ntask-relevant instruction (zero-shot) generates suf-\nficiently useful novel examples to slightly outper-\nform the vanilla baseline by 1.5% AUAC. Using\nWikitext as our novel set performs roughly on par\nwith zero-shot generation: though Wikitext exam-\nples are less noisy than generations, they also tend\nto be less dataset-relevant. Few-shot generation,\nwhich generates more plausible examples, is out-\nperforms all prior methods, but performs worse\nthan Novelty Prompting on 3 of 4 datasets.\nTo further test the importance of novel set se-\nlection, we compare with two oracle methods. In\nthe Gold Data setting, we use CCL with held out\ndata of the gold novel test class(es) as a strict up-\nper bound for both label and example generation.\nIn the Gold Label setting, we eliminate the label\ngeneration step, performing example generation\nusing the gold label alone. This setting is overly\noptimistic as we cannot know what new labels will\nappear at test-time.5 CCL in the Gold Label setting\nslightly outperforms CoNAL, but using gold novel\ndata can achieve much stronger OSSC.\nTraining loss choice matters for generated data.\nAlthough OE training with Novelty Prompting data\nimproves OOD detection over vanilla, it sharply\ndecreases accuracy on TREC-10 (96.6% →71.3%)\nand on average by 0.6% on the other three datasets\n(see Appendix A.4). In contrast, we find that CCL\ntraining maintains accuracy on all settings (see\nAppendix A.4), as it does not enforce a uniform\nprobability distribution on all novel set examples.\nCCL with both zero- and few-shot generation out-\nperforms all prior methods, and our full CoNAL\nmethod significantly outperforms prior methods on\nall but one dataset. OE exhibits this issue only with\ngenerated data: when the novel set is instead sam-\npled from held-out gold OOD data OE outperforms\nCCL in AUAC and AUROC, suffering only a small\naccuracy drop (an average of 0.4%).\nWe attribute this behavior to generation noise:\nsome generated examples are similar to ID exam-\nples, and thus greatly affect the model’s ID predic-\ntions when training with OE. To verify this hypoth-\nesis, we conduct an experiment where we train clas-\nsifiers with synthetic novel sets formed by noising\nheldout OOD data with various amounts of heldout\n5In practice, expert knowledge of the novelty we expect to\nsee at test-time is sometimes available, and as shown in our\nresults, can be leveraged for better performance.\n11782\nAUAC (↑) TREC-10 AGNews Emotion TACRED Average\nBaselines\nVanilla 89.2±2.2 87.9±0.6 90.3±1.0 89.6±0.1 89.3\nkFolden 93.5±0.6 85.8±1.6 90.6±0.9 84.9±3.5 88.7\nContrastive 92.0±0.4 87.0±0.9 92.2±0.4 88.8±0.7 90.0\nCoNAL\nvariants and\nablations\nCCL + Wikitext 91.2±1.4 88.6±0.6 92.0±0.4 89.3±0.5 90.3\nCCL + Zero-Shot 92.5±0.8 89.1±0.4 92.6±0.2 88.9±0.4 90.8\nCCL + Few-Shot 93.5±0.3 89.7±0.3 93.3±0.1 90.8±0.1 91.8\nOE + Wikitext 92.6±0.8 88.9±0.4 91.6±0.6 89.8±0.1 90.7\nOE + Novelty Prompting 83.6±0.4 90.6±0.2 92.4±0.1 91.3±0.3 89.5\nOur full method CoNAL 94.3±0.2 90.5±0.3 93.4±0.1 91.1±0.2 92.3\nOracle methods\nCCL + Gold Label† 94.8±0.3 91.4±0.3 93.7±0.1 91.0±0.2 92.7\nCCL + Gold Data† 96.6±0.1 93.5±0.1 94.8±0.2 94.3±0.4 94.8\nOE + Gold Data† 96.5±0.2 94.8±0.0 95.2±0.0 96.2±0.2 95.7\nTable 1: OSSC Results of CoNAL. Methods listed below CoNAL are upper bounds. All outlier exposure (OE)\nmethods are trained on 100K outlier generations. We average over the results of 5 seeds of all splits and report\nstandard error of the mean in subscript. We report macro-average of all datasets in the rightmost column. Oracle\nmethods are marked with a †. We find that CoNAL significantly outperforms all prior methods on 3 of 4 datasets,\nand both the Novelty Prompting and CCL loss components are important for strong performance.\nLabel Generated Example\nCURIOSITY i am still interested but more in-\nterested to visit the pyramids and\nlearn more\nDESPAIR i love my friends but sometimes i\nfeel like im not good enough\nDISAPPOINTMENT i am a human nothing is going to\nkeep me from flying away\nFigure 2: Example novel generations for Emotion. In\nthis split, the gold novel label is “sadness”. Though we\nremove the gold novel label before example generation,\nmany generations are still relevant to this label. More\ngeneration examples are shown in Appendix A.6.\n0 10 20 30 40 50 60 70 80 90 100\nOOD  ID Noising %\n94.0\n94.5\n95.0\n95.5\n96.0\n96.5\n97.0ID Accuracy (%) \nCCL\nOE\nFigure 3: Noisy novel sets hurt accuracy for OE. We\nplot the ID accuracy of classifiers trained with OE and\nCCL on mixtures of heldout TREC-10 OOD and ID\ndata as an novel set. ID accuracy with OE decreases as\nwe introduce more noise, while CCL stays stable.\nID data. In Figure 3, we show that as the simulated\nID noise ratio increases, OE training hurts accuracy\nwhereas CCL models retain accuracy.\nSmaller datasets suffer more. The ID accuracy\ndrop is most salient on TREC-10 because it is by\nfar the smallest dataset we consider, making it easy\nfor generation noise to overwhelm signal from the\ntrain set. We conduct two experiments to show that\nTREC-10 is not unique, but instead exemplifies an\ninherent pitfall of OE. First, to test whether OE\nnoise sensitivity applies on other datasets, we con-\nsider a smaller training set from another dataset.\nIn the first experiment of Appendix A.12, we sub-\nsample AGNews training sets to smaller sizes. As\nthe training set becomes smaller, the ID accuracy\ngap between OE and Vanilla training increases to\nmore than 35%. Meanwhile the ID accuracy gap\nbetween CCL and Vanilla is less than 10% even\nat small training set sizes. Our finding here is that\nTREC-10 is not unique — OE can suffer from gen-\neration noise on other datasets when the training\nset size is not large enough.\nSecond, we show that this ID accuracy drop in-\ncreases as the novel set size grows, i.e., the larger\nthe novel set, the more noise the classifier sees in\nOE training and the worse its ID predictive ability.\nThe second experiment in Appendix A.12 shows\nthat when the TREC-10 novel set (100K examples)\nis much larger than the training set (2.8K exam-\nples), accuracy decreases drastically. We generally\nfind a negative correlation between the novel set\nsize and ID accuracy with OE training. In contrast,\nCCL maintains ID accuracy at all novel set sizes.\nCCL improves ID-OOD separability. In Ap-\npendix A.13, we show that CCL is effective at\nOOD detection because it improves the confidence-\nbased separability of ID and OOD examples.\n11783\nAUROC (↑) TREC-10 AGNews Emotion TACRED Average\nBaselines\nVanilla 76.6±4.4 76.4±1.0 85.0±2.4 46.3±0.1 71.1\nkFolden 84.7±2.0 72.5±2.2 85.3±1.8 53.1±6.2 73.9\nContrastive 79.8±1.3 76.5±1.8 89.1±1.7 45.7±1.2 72.3\nCoNAL\nvariants and\nablations\nCCL + Wikitext 81.0±2.6 78.1±0.8 90.3±0.8 45.2±1.2 74.1\nCCL + Zero-Shot 84.8±1.4 78.8±0.8 90.7±0.7 44.2±1.0 74.6\nCCL + Few-Shot 88.4±0.6 80.5±0.7 92.8±0.5 49.7±0.3 77.9\nOE + Wikitext 85.0±1.7 78.3±0.8 88.8±1.1 46.2±0.5 74.6\nOE + Novelty Prompting 74.2±0.5 85.5±0.3 91.0±0.3 53.5±0.7 76.0\nOur full method CoNAL 90.8±0.6 82.6±0.6 93.4±0.3 50.9±0.5 79.4\nOracle methods\nCCL + Gold Label† 92.0±0.8 84.9±0.4 94.2±0.3 51.2±0.6 80.6\nCCL + Gold Data† 98.3±0.3 91.7±0.3 98.8±0.1 63.1±0.2 88.0\nOE + Gold Data† 99.1±0.2 98.8±0.3 99.7±0.0 89.0±0.5 96.7\nTable 2: OOD Detection Results of Contrastive Novelty-Augmented Learning. Methods same as in Table 1. We find\nthat CoNAL significantly improves OOD detection AUROC over all prior methods on 3 of 4 datasets. While OE\ntraining results in better AUROC on some datasets, it hurts ID accuracy.\nTREC-10 Emotion AGNews\nDataset\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96Selective Prediction AUAC\nVanilla\n(125M) GPT-Neo\n(774M) GPT2-Large\n(1.3B) GPT-Neo\n(2.7B) GPT-Neo\n(6B) GPT-J\nFigure 4: Smaller generators can also help. We perform\ngeneration with variously sized generator models, from\n125M to 6B parameters. Larger generators seem to yield\nbetter results, but even our smallest generator does well.\nTREC-10 Emotion AGNews TACRED\nDataset\n88\n90\n92\n94\n96\n98Selective Prediction AUAC\nVanilla BERT-base\nVanilla RoBERTa\nVanilla DeBERTa-base\nVanilla RoBERTa-large\nVanilla DeBERTa-large\nCoNAL BERT-base\nCoNAL RoBERTa\nCoNAL DeBERTa-base\nCoNAL RoBERTa-large\nCoNAL DeBERTa-large\nFigure 5: Larger classifiers can also benefit. We addi-\ntionally train RoBERTa-base and RoBERTa-large with\nand without Novelty Prompted training. We find that\nboth classifiers improve over vanilla with CoNAL.\n5.2 OOD Detection Results\nTo confirm that CoNAL improves a classifier’s\nability to disambiguate novel class examples, we\ncompare CoNAL against the same baselines on\nOOD detection in Table 2. We find similar im-\nprovements, outperforming the best prior method\n(kFolden) by 5.5% AUROC. We interpret this result\nin Appendix A.13, showing that CoNAL improves\nID/OOD separability. Unlike other datasets, TA-\nCRED exhibits strong OOD overconfidence: all\nbaselines except kFolden yield worse-than-random\nOOD detection (below 50% AUROC). We hypoth-\nesize that this could be due to models incorrectly\nassuming that an NER tag pair seen at training time\nin only a single class could not belong to a novel\nrelation. OOD detection on TACRED remains a\nchallenging goal for future work, as the strong per-\nformance of CCL training with gold heldout data\nindicates significant remaining headroom. In fact,\non all three other datasets, models achieve greater\nthan 90% AUROC when trained with gold heldout\ndata. While OE results in better AUROC perfor-\nmance on AGNews, ID accuracy also decreases.\n5.3 Performance Analysis\nLabel Generator Model We investigate whether\na smaller, open-source model can suffice as the\nlabel generator. Specifically, we replace the label\ngenerator with GPT-J and use 100 label generation\niterations. We find that GPT-J performs on-par with\nGPT-3 on 3 of 4 datasets in all metrics, except on\nAGNews where it performs within 1 point AUAC.\nWe provide full details in Appendix A.5.\nExample Generator Size. Since model scale of-\nten affects prompting performance (Sanh et al.,\n2021; Brown et al., 2020), we compare genera-\ntor models ranging in size from 125M parameters\nto 6B parameters. For each, we generate 100K ex-\namples, and compare CoNAL results in Figure 4.\nAll generators improve over the Vanilla baseline.\n11784\nGPT2-Neo 125M is competitive with GPT2-Large\ndespite being roughly 5x smaller, suggesting that\nits larger pretraining corpus (the Pile) aids genera-\ntion ability. Novel generation is easier on simpler\ntasks: on Emotion, where labels (or synonyms)\ncan appear directly in the example, inter-generator\ndifferences are small. We posit that even larger\ngenerators such as GPT-3 could yield better per-\nformance on abstract tasks. In Appendix A.7, we\nanalyze the quality of generated examples.\nOther Classifier Models. We investigate the gen-\neralizability of CoNAL to two other classifier archi-\ntectures, RoBERTa (Liu et al., 2019) and DeBERTa-\nv3 (He et al., 2021), of both base and large sizes,\nwith results in Figure 5. Averaged over datasets,\nCoNAL improves AUAC for all classifiers, though\nthese improvements are most apparent with the\nsmaller base models. Larger classifiers are better\nat OSSC: vanilla RoBERTa-large improves over\nBERT-base by 2.8% AUAC. Vanilla RoBERTa-\nbase slightly outperforms vanilla BERT-base, but\nafter CoNAL training, the two perform on-par, sug-\ngesting that learning from generated examples can\nmake up for BERT’s smaller pretraining corpus.\nGeneration Quota. Since large-scale LLM\nprompting is costly, we analyze the performance\ntradeoff of shrinking the generation quota, the num-\nber of novel examples that we can generate. In\nFigure 6, we show that on some datasets, using\norders of magnitude smaller novel sets can still\nimprove selective prediction. For example, 1000\ngenerations is sufficient to improve AUAC across\nall datasets, and for most datasets we require far\nfewer. In cases where a low quota is sufficient,\nCoNAL is nearly as efficient as vanilla training.\nGeneration Analysis. To evaluate the remaining\nerrors in OOD generation, we perform two types of\nmanual analysis on Novelty Prompting (NP). First,\nwe categorize the labels generated by NP after fil-\ntering, finding that 70%+ of GPT-3 generated labels\nare novel on all datasets except TREC-10, where\nonly 40% are novel, and the vast majority of the\nothers are valid closed-set labels. This highlights\none source of generation noise in our pipeline. Sec-\nond, we categorize the examples generated by NP\nand a strong baseline method, Few-shot data aug-\nmentation (FS). Specifically, for each of the 4 splits\nof AGNews, we annotate 100 NP and 100 FS ex-\namples. On average, 41% of NP generations come\nfrom novel classes, compared to only 26% of FS\ngenerations, explaining CoNAL’s stronger perfor-\nmance over CCL + Few-Shot. We provide further\nanalysis in Appendix A.7. Our method performs\nwell despite the high fraction (50.5%) of closed-set\nexamples generated in NP, showing that CCL is\nrobust to noise in the example generation process.\n6 Related Work\n6.1 Identifying OOD Data\nOOD Detection. Prior work on OOD detection\nuses models to detect test examples that come\nfrom a new distribution (Hendrycks and Gimpel,\n2017). Many of these introduce new training ob-\njectives, e.g., with a contrastive objective (Winkens\net al., 2020; Sehwag et al., 2021; Zhou et al., 2021).\nWhen the nature of the distribution shift is known,\nthe model can directly be trained to be uncertain\non known OOD examples (Dhamija et al., 2018;\nHendrycks et al., 2019). We draw on the success of\nthese known-shift methods, but eliminate the need\nfor known OOD data by using generative models.\nOther works on OOD detection have explored\nalternative modeling paradigms. Ensembles of\nneural networks can yield useful confidence esti-\nmates ( ¸ Tifrea et al., 2021; Li et al., 2021; Lakshmi-\nnarayanan et al., 2017), as can simple methods like\ndeep nearest-neighbors (Sun et al., 2022; Bergman\net al., 2020). Further performance improvements\ncan be achieved by modifying the confidence met-\nric. Podolskiy et al. (2021) find that Mahalanobis\ndistance better exploits the geometry of the learned\nembedding space, explaining strong performance\nachieved by replacing probability-based scoring\nmechanisms (Lee et al., 2018; Ren et al., 2021).\nWe show that standard models are sufficient: Max-\nProb scoring with a standard classifier can perform\nwell when given proper OOD demonstrations.\nOOD Selective Prediction. Selective prediction\nwork focuses on a different paradigm altogether,\nfusing abstention (detection) with prediction (El-\nYaniv and Wiener, 2010; Geifman and El-Yaniv,\n2017). External calibrators popularized by Kamath\net al. (2020) have become popular as a selective\nprediction framework (Zhang et al., 2021; Ye and\nDurrett, 2021; Varshney et al., 2022). However, cal-\nibrators are typically smaller than classifier mod-\nels (Tajwar et al., 2021); we instead update the\nhigher-capacity classifier model to better leverage\nof our large set of generated outliers.\n11785\n0 10 100 1000 10000 100000\nGeneration Quota\n84\n86\n88\n90\n92\n94Selective Prediction AUAC\nTREC-10\nCoNAL\nVanilla\nkFolden\nContrastive\n0 10 100 1000 10000 100000\nGeneration Quota\n84\n86\n88\n90\n92\n94Selective Prediction AUAC\nAGNews\nCoNAL\nVanilla\nkFolden\nContrastive\n0 10 100 1000 10000 100000\nGeneration Quota\n84\n86\n88\n90\n92\n94Selective Prediction AUAC\nEmotion\nCoNAL\nVanilla\nkFolden\nContrastive\n0 10 100 1000 10000 100000\nGeneration Quota\n84\n86\n88\n90\n92\n94Selective Prediction AUAC\nTACRED\nCoNAL\nVanilla\nkFolden\nContrastive\nFigure 6: Selective prediction performance is positively correlated with generation quota. We measure selective\nprediction against the total number of examples generated, showing various baselines in horizontal lines. On most\ndatasets, even a low quota can meaningfully improve AUAC.\n6.2 Open-Set Classification\nOpen-set classification is well-explored in the im-\nage classification space, as tasks like CIFAR-100\ntend towards large label spaces (Scheirer et al.,\n2013; Geng et al., 2021). Some methods for detect-\ning open-set examples build on the classifier, e.g.,\nby classifying over the model’s activations (Ben-\ndale and Boult, 2016) or adding an additional recon-\nstruction model (Oza and Patel, 2019). Our work\nis most closely related to methods that generate\nnear-OOD examples and regularize confidence on\nthem (Ge et al., 2017; Du et al., 2022; Kong et al.,\n2020; Vernekar et al., 2019; Möller et al., 2021;\nSetlur et al., 2022). However, methods like pertur-\nbation and embedding space sampling align poorly\nwith the discrete nature of text, prompting us to\ninvestigate powerful generative language models.\nEsmaeilpour et al. (2022) is closely related to our\nwork in that they also generate novel labels, but\ndirectly use these labels as input to a classifier.\nOpen-set classification for text has been less ex-\nplored. Early works built upon the k-way, 1-vs-\nrest paradigm of SVMs, classifying an example as\n“novel” if all kscores fall below a threshold (Fei\nand Liu, 2016; Shu et al., 2017; Doan and Kalita,\n2017). Some works explore similar methods as\nprior vision work, but focus on the intent detection\nsetting, as task-oriented dialogue models should ab-\nstain on unknown intents (Zeng et al., 2021; Zheng\net al., 2020; Lin and Xu, 2019). To the best of our\nknowledge, we are the first work to generate novel\nexamples for open-set text classification.\n6.3 Data Augmentation\nFinally, our generation method, Novelty Prompting,\nrelates to prior work in using pretrained language\nmodels for data augmentation. Kumar et al. (2021)\nproposes directly conditioning on class labels to\ngenerate relevant class examples, which forms a\ncomponent of our prompting approach. Anaby-\nTavor et al. (2020) finetunes a class-conditional\ngenerator on a given dataset to yield more relevant\ngenerations, though we consider prompting instead\nof finetuning as a method to prime for relevance.\n7 Discussion and Future Work\nIn this work, we introduce CoNAL, a method for\ngenerating novel examples which simulate open-\nset shift and training to abstain on them. Through\nextensive experiments, we demonstrate that by pre-\nsenting generated examples to a classifier, we can\nsignificantly improve its ability to abstain on exam-\nples from novel classes against state-of-the-art base-\nlines. Our work provides a generalizable frame-\nwork for improving OSSC and OOD detection: in\nfact, we show through CCL training’s strong per-\nformance with gold data that there remains head-\nroom for novel example generation. Additionally,\nCoNAL is modular, as it provides additional su-\npervision signal but does not alter the classifier’s\narchitecture. It thus remains extensible with other\ntraining objectives or classification metrics. Fi-\nnally, automatically diagnosing dataset issues and\nimproving them is an important step towards mak-\ning NLP safer and easier to apply. CoNAL al-\nlows practitioners to deal with noise introduced\nby LLM-generated data and apply these generated\ndatasets in settings like open-set selective classifi-\ncation. The success of our method indicates that\nLLMs can be used to improve datasets with min-\nimal human intervention. Given interest in the\nemergent capabilities of LLMs, we hope that fu-\nture work on classification in the presence of dis-\ntribution shifts can better leverage large language\nmodels to both directly identify shifts and improve\nthe abstention ability of smaller classifiers.\n11786\nLimitations\nDespite the fact that we demonstrate strong OSSC\nperformance with low generation quotas in Ap-\npendix 5.3, CoNAL still is slightly more compu-\ntationally expensive than vanilla training. It also\nrequires access to a pretrained LLM with which\nto generate novel examples. To achieve optimal\nperformance, usage of the OpenAI API is required,\nwhich poses some concerns around transparency, as\ndetails around GPT-3 training and data are not pub-\nlicly released. Finally, performance varies across\ndatasets, suggesting that types of outliers that are\nunexpected to LLMs might still confuse a CoNAL-\ntrained model.\nAcknowledgments\nWe would like to thank Wang Zhu, Yuchen Lin,\nMuhao Chen, Wenxuan Zhou, and members of the\nAllegro and INK labs at USC for their valuable\nfeedback. We also thank Johnny Wei for discus-\nsions on statistical testing.\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, N. Tep-\nper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In AAAI\nConference on Artificial Intelligence.\nUdit Arora, William Huang, and He He. 2021. Types\nof out-of-distribution texts and how to detect them.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAbhijit Bendale and Terrance E Boult. 2016. Towards\nopen set deep networks. In Proceedings of the IEEE\nconference on Computer Vision and Pattern Recogni-\ntion.\nLiron Bergman, Niv Cohen, and Yedid Hoshen. 2020.\nDeep nearest neighbor anomaly detection. arXiv\npreprint arXiv:2002.10445.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems.\nAkshay Raj Dhamija, Manuel Günther, and Terrance E.\nBoult. 2018. Reducing network agnostophobia. In\nAdvances in Neural Information Processing Systems.\nTri Doan and Jugal Kalita. 2017. Overcoming the chal-\nlenge for text classification in the open world. In\n2017 IEEE 7th Annual Computing and Communica-\ntion Workshop and Conference (CCWC).\nXuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li.\n2022. VOS: Learning what you don’t know by virtual\noutlier synthesis. arXiv preprint arXiv:2202.01197.\nRan El-Yaniv and Yair Wiener. 2010. On the founda-\ntions of noise-free selective classification. In Journal\nof Machine Learning Research.\nSepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei\nShu. 2022. Zero-shot out-of-distribution detection\nbased on the pretrained model CLIP. In Proceedings\nof the AAAI conference on artificial intelligence.\nGeli Fei and Bing Liu. 2016. Breaking the closed world\nassumption in text classification. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nSaurabh Garg, Sivaraman Balakrishnan, and\nZachary Chase Lipton. 2022. Domain adapta-\ntion under open set label shift. In ICML 2022:\nWorkshop on Spurious Correlations, Invariance and\nStability.\nZongYuan Ge, Sergey Demyanov, Zetao Chen, and\nRahil Garnavi. 2017. Generative openmax for\nmulti-class open set classification. arXiv preprint\narXiv:1707.07418.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassification for deep neural networks. In Advances\nin Neural Information Processing Systems.\nChuanxing Geng, Sheng-Jun Huang, and Songcan Chen.\n2021. Recent advances in open set recognition: A\nsurvey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. In arXiv.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. In 5th International Con-\nference on Learning Representations (ICLR).\nDan Hendrycks, Mantas Mazeika, and Thomas G. Di-\netterich. 2019. Deep anomaly detection with outlier\nexposure. In 7th International Conference on Learn-\ning Representations (ICLR).\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Span-\nBERT: Improving pre-training by representing and\npredicting spans. In Transactions of the Association\nfor Computational Linguistics.\nAmita Kamath, Robin Jia, and Percy Liang. 2020. Se-\nlective question answering under domain shift. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAran Komatsuzaki. 2021. GPT-J-6B: 6B JAX-based\ntransformer.\n11787\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model fine-tuning for in- and out-\nof-distribution data. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2021. Data augmentation using pre-trained trans-\nformer models. In 2nd Workshop on Life-long Learn-\ning for Spoken Language Systems @ AACL 2020.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles.\nIn Advances in Neural Information Processing Sys-\ntems 30: Annual Conference on Neural Information\nProcessing Systems.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. Ad-\nvances in Neural Information Processing Systems.\nXiaoya Li, Jiwei Li, Xiaofei Sun, Chun Fan, Tianwei\nZhang, Fei Wu, Yuxian Meng, and Jun Zhang. 2021.\nkfolden: k-fold ensemble for out-of-distribution de-\ntection. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nXin Li and Dan Roth. 2002. Learning question clas-\nsifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nTing-En Lin and Hua Xu. 2019. Deep unknown intent\ndetection with margin loss. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In arXiv.\nRafael Müller, Simon Kornblith, and Geoffrey E Hinton.\n2019. When does label smoothing help? Advances\nin Neural Information Processing Systems.\nFelix Möller, Diego Botache, Denis Huseljic, Florian\nHeidecker, Maarten Bieshaar, and Bernhard Sick.\n2021. Out-of-distribution detection and generation\nusing soft brownian offset sampling and autoen-\ncoders. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition.\nPoojan Oza and Vishal M Patel. 2019. C2AE: Class con-\nditioned auto-encoder for open-set recognition. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition.\nPramuditha Perera, Vlad I Morariu, Rajiv Jain, Varun\nManjunatha, Curtis Wigington, Vicente Ordonez, and\nVishal M Patel. 2020. Generative-discriminative fea-\nture representations for open-set recognition. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout, Eka-\nterina Artemova, and Irina Piontkovskaya. 2021. Re-\nvisiting mahalanobis distance for transformer-based\nout-of-domain detection. In Proceedings of the AAAI\nConference on Artificial Intelligence.\nJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha\nRoy, Shreyas Padhy, and Balaji Lakshminarayanan.\n2021. A simple fix to mahalanobis distance for\nimproving near-ood detection. arXiv preprint\narXiv:2106.09022.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2021. Multitask prompted training enables zero-\nshot task generalization. In The Tenth International\nConference on Learning Representations.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing.\nWalter J. Scheirer, Anderson de Rezende Rocha,\nArchana Sapkota, and Terrance E. Boult. 2013. To-\nward open set recognition. In IEEE Transactions on\nPattern Analysis and Machine Intelligence.\nVikash Sehwag, Mung Chiang, and Prateek Mittal. 2021.\nSSD: A unified framework for self-supervised outlier\ndetection. In The Ninth International Conference on\nLearning Representations.\nAmrith Setlur, Benjamin Eysenbach, Virginia Smith,\nand Sergey Levine. 2022. Adversarial unlearning:\nReducing confidence along adversarial directions.\narXiv preprint arXiv:2206.01367.\nLei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep\nopen classification of text documents. InProceedings\nof the 2017 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nYiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li.\n2022. Out-of-distribution detection with deep nearest\nneighbors. In The Thirty-ninth International Confer-\nence on Machine Learning.\nFahim Tajwar, Ananya Kumar, Sang Michael Xie, and\nPercy Liang. 2021. No true state-of-the-art? ood\ndetection methods are inconsistent across datasets.\nIn ICML Workshop on Uncertainty & Robustness in\nDeep Learning.\nNeeraj Varshney, Swaroop Mishra, and Chitta Baral.\n2022. Investigating selective prediction approaches\nacross several tasks in IID, OOD, and adversarial\nsettings. In Findings of the Association for Computa-\ntional Linguistics (ACL).\n11788\nSachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Tay-\nlor Denouden, Rick Salay, and Krzysztof Czarnecki.\n2019. Out-of-distribution detection in classifiers via\ngeneration. In NeurIPS 2019, Safety and Robustness\nin Decision Making Workshop.\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert\nStanforth, Vivek Natarajan, Joseph R Ledsam, Patri-\ncia MacWilliams, Pushmeet Kohli, Alan Karthike-\nsalingam, Simon Kohl, et al. 2020. Contrastive train-\ning for improved out-of-distribution detection. arXiv\npreprint arXiv:2007.05566.\nXi Ye and Greg Durrett. 2021. Can explanations be use-\nful for calibrating black box models? In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics.\nZhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu,\nYanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu.\n2021. Modeling discriminative representations for\nout-of-domain detection with supervised contrastive\nlearning. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In 6th International Conference\non Learning Representations.\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021.\nKnowing more about questions can help: Improving\ncalibration in question answering. In Findings of the\nAssociation for Computational Linguistics.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Twenty-ninth Conference on Neural\nInformation Processing Systems.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nYinhe Zheng, Guanyi Chen, and Minlie Huang. 2020.\nOut-of-domain detection for natural language under-\nstanding in dialog systems. In IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing.\nWenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021.\nContrastive out-of-distribution detection for pre-\ntrained transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nAlexandru ¸ Tifrea, Eric Stavarache, and Fanny\nYang. 2021. Novelty detection using ensembles\nwith regularized disagreement. arXiv preprint\narXiv:2012.05825.\nA Appendix\nA.1 Computational Budget\nAs we did not closely track the total amount of com-\nputational resources used, we provide our best es-\ntimate. All experiments were completed on 12GB\n1080Ti and 48GB RTX8000 GPUs. We did not\nperform any hyperparameter search.\nWe note that computation time for CoNAL is\ndivided into two components, example generation\nand classifier training. We provide two examples\nof the computational budget for generation. When\nusing GPT-3 as a label generator, label generation\ncosts several cents using the text-davinci-002\nendpoint, and example generation with GPT-J 6B\ntakes several hours on a 48GB RTX8000 GPU.\nWe show in Appendix 5.3 that we can generate\nmany fewer examples and still achieve strong per-\nformance, in which case generation would require\norders of magnitude less time.\nClassifier training for a bert-base-cased\nmodel takes approximately 30 minutes on a 12GB\n1080 Ti GPU. For reference, vanilla training takes\nabout half this time, as CoNAL must compute\nlosses for a pair of batches at each training step.\nA.2 Code Release\nCode for replicating all experiments is released on\nGithub at github.com/albertkx/CoNAL under an\nMIT license.\nA.3 Prompt Format\nWe use the same format for label generation for\nall datasets, shown in Figure 7, but customize the\ninstruction for each dataset, as shown in Figure 8.\nFor example generation, we prompt with an ex-\nample sampled from each class and a random novel\nlabel. We use the same instruction for all datasets.\nAn example prompt is shown in Figure 9.\nFew-shot prompting is done with a task-specific\ninstruction, but does not include labels, as shown\nin Figure 10. Zero-shot prompting is done with the\ntask-specific instruction only.\nA.4 Full Accuracy Results\nCoNAL improves AUAC on all datasets without\nany cost to ID accuracy, as shown in Figure 11. We\nshow full ID accuracy results in Table 3. CCL\ntraining maintains accuracy across all datasets,\nwhile OE training decreases accuracy on 3 of 4\ndatasets, with a very sharp drop on TREC-10. In\nAppendix A.12, we show thorough analyses on\n11789\nInstruction Generate a diverse list of news genres:\nID Labels [World, Sports, Sci/Tech,\nFigure 7: Label Generation prompt for AGNews.\nDataset Instruction\nEmotion Generate a diverse list of emotions\nAGNews Generate a diverse list of news genres\nTREC-10 Generate a diverse list of entity types\nTACRED Generate a diverse list of relations between entities\nFigure 8: Label Prompts for each Dataset.\ntwo datasets that this steep accuracy drop is not\nan anomaly: when paired with generated data, OE\ntraining is sensitive to the sizes of the novel set and\ntraining set, and can signficantly hurt ID accuracy\nwhen the novel set is much larger than the training\nset. Additionally, despite improving selective pre-\ndiction performance, training with gold held-out\ndata curiously hurts accuracy on TACRED.\nA.5 CoNAL performs well without GPT-3\nIn our main experiments, we use GPT-3 as the label\ngenerator and GPT-J 6B as the example generator.\nIn Section 5.3, we show that smaller models can be\nused as example generators. Here we investigate\nwhether a smaller, open-source language model can\nbe used as a label generator. In Table 12, we show\nthat GPT-J 6B also performs well at label genera-\ntion. We empirically observe that GPT-J generates\nshorter and noisier completions, requiring us to in-\ncrease the number of model calls from 5 to 100 and\nfilter out all labels containing punctuation marks.\nAfter applying these tweaks, we find that the differ-\nence between GPT-J and GPT-3 label generation in\nAUAC is small on 3 of 4 datasets, and differs by\nonly 0.7 on AGNews, suggesting that CoNAL with\nGPT-J only can still work well.\nA.6 Generation Examples\nWe show examples of the generations from Novelty\nPrompting for AGNews in Table 4. Recall that we\ndo not allow the gold novel label to be generated\nto hedge against data leakage from LLM pretrain-\ning. However, we observe that our generator is\nstill capable of producing relevant examples to the\ngold novel label due to signal from similar novel\nlabels. Despite many generations not being directly\nrelevant to the gold novel label, we observe that the\ngenerated novel labels are sufficiently distinct from\nthe closed-set labels that most generated examples\nstill provide useful “novelty” supervision signal to\nthe classifier.\nA.7 Novelty Prompting Error Analysis\nThough CoNAL improves OSSC ability on all\ndatasets, we still find headroom between Novelty\nPrompting generated data and gold OOD data (92.3\n→94.8) in Table 1. To understand the remaining\nfailure modes of Novelty Prompting, we manually\ninspect the generated labels and examples from\nour method. Broadly, we seek to attribute “gen-\neration noise,” or the frequency with which the\npurported novel sets which we generate instead\ncontain closed-set class examples.\nFirst, we manually annotate GPT-3 generated la-\nbels from all dataset splits, categorizing a label into\n“implausible” if it does conform to the dataset’s\nformat, “closed-set” (ID) if it is synonymous with\na class seen in training, and “novel” (OOD) if it\ndescribes a class distinct from all closed-set classes.\nIn Figure 13, we perform this analysis for all four\ndatasets. Across all datasets, less than 15% of gen-\nerations are implausible, suggesting that the model\nis usually able to generate reasonable additional\nlabels given only 3-6 ID classes. We also observe\nthat while on 3 of 4 datasets less than 15% of gener-\nated classes are closed-set, on TREC-10 more than\nhalf of generated labels are closed-set. One reason\nfor this label generation noise is that the TREC-10\nlabels are very broad (e.g., “entity” describes ques-\ntions about any subcategory of an entity, including\nall objects and events), so while a generated label\nmight differ in definition, it could still overlap with\nor fall into a subcategory of a closed-set class.\nSecond, we manually annotate GPT-J generated\nexamples to understand whether example genera-\ntion is a source of generation noise. In Figure 14,\n11790\nInstruction Given a label, generate a corresponding example:\nID Label 1 business\nID Example\n1\nStarwood Names New Chief Executive SEPTEMBER 21, 2004\n– White Plains, NY – Former Coca-Cola Company president\nSteven Heyer today was named the new chief executive of\nStarwood Hotels, effective Oct. 1. Heyer succeeds Starwood\nfounder Barry\nID Label 2 sports\nID Example\n2\nMarino, Young Considered for Hall of Fame Dan Marino and\nSteve Young highlighted a list Friday of 25 candidates for\nthe Pro Football Hall of Fame.\nID Label 3 world\nID Example\n3\nAfghan warlords ’threaten poll’ Afghan warlords are\ninvolved in intimidation which could threaten October’s\nelections, Human Rights Watch says.\nNovel Label entertainment\nFigure 9: Example Generation prompt for AGNews.\nInstruction Generate a news headline:\nID Example\n1\nStarwood Names New Chief Executive SEPTEMBER 21, 2004\n– White Plains, NY – Former Coca-Cola Company president\nSteven Heyer today was named the new chief executive of\nStarwood Hotels, effective Oct. 1. Heyer succeeds Starwood\nfounder Barry\nID Example\n2\nMarino, Young Considered for Hall of Fame Dan Marino and\nSteve Young highlighted a list Friday of 25 candidates for\nthe Pro Football Hall of Fame.\nID Example\n3\nAfghan warlords ’threaten poll’ Afghan warlords are\ninvolved in intimidation which could threaten October’s\nelections, Human Rights Watch says.\nFigure 10: Few-Shot Generation prompt for AGNews.\nwe annotate 100 examples of each split of AGNews\nfor both Few-shot data augmentation and Novelty\nPrompting. We observe that Novelty Prompting\ngenerates novel class examples more frequently\nacross 3 of 4 splits. Both methods generate im-\nplausible (e.g., agrammatical, non-news) examples\nrarely, as ID demonstrations sufficiently prime the\nmodel to generate text in the style of news. Addi-\ntionally, under Novelty Prompting, we find that the\nfraction of novel class examples (41.3%) is much\nlower than the fraction of novel labels generated\n(81.7%), suggesting that GPT-J can easily adhere\nto the dataset format, but struggles to extrapolate\nto the novel label. Future work should thus focus\non better specifying the example generation step to\nleverage the generated labels.\nA.8 Dataset Split Details\nTREC-10: We remove the Abbreviation class\nas it is too small to yield statistically significant\nmetrics in our task setting, leaving 5 remaining\nclasses.\nEmotion (Saravia et al., 2018): We remove two\nsmall classes, love and surprise, leaving 4 re-\nmaining classes.\nTACRED (Zhang et al., 2017): We process the\ndata for training following Joshi et al. (2019). This\ndataset is particularly challenging due to its class-\nimbalanced nature. We evaluate a single split where\nwe keep the 6 largest classes as ID data, and hold\nout the other 35. This is the largest class, and thus\nresults in approximately 80% of examples being\nOOD at test time.\n11791\n95 96 97 98\nID Accuracy (%)\n88\n90\n92\n94\n96Selective Prediction AUAC\nTREC-10\nEmotion\nAGNews\nTACRED\nVanilla\nCoNAL\nFigure 11: CoNAL training maintains ac-\ncuracy. Training with novelty prompted\nexamples does not significantly alter ID\naccuracy, but improves selective predic-\ntion across all datasets.\n(↑) TREC-10 AGNews Emotion TACRED Avg\nVanilla\nAUAC 89.2±2.2 87.9±0.6 90.3±1.0 89.6±0.1 89.3\nAUROC 76.6±4.4 76.4±1.0 85.0±2.4 46.3±0.1 71.1\nID Acc 96.6±0.2 96.1±0.0 97.7±0.1 95.0±0.1 96.4\nGPT-3\nAUAC 94.3±0.2 90.5±0.3 93.4±0.1 91.1±0.2 92.3\nAUROC 90.8±0.6 82.6±0.6 93.4±0.3 50.9±0.5 79.4\nID Acc 96.4±0.2 96.2±0.0 97.8±0.1 94.9±0.1 96.3\nGPT-J\nAUAC 94.2±0.3 89.8±0.3 93.5±0.1 91.0±0.2 92.1\nAUROC 90.0±0.6 80.8±0.6 93.5±0.3 50.4±0.4 78.7\nID Acc 96.4±0.1 96.2±0.0 97.9±0.1 94.9±0.0 96.4\nFigure 12: GPT-J is also a strong label generator. We compare\nlabel generation using GPT-3 and GPT-J, using GPT-J as the example\ngenerator for both methods. GPT-J performs within a negligible\nmargin of GPT-3 on TREC-10 and Emotion, but slightly worse on\nAGNews and TACRED.\nDataset Label Type Frequency Example Label\nTREC-10\nImplausible 7.8% August 27\nNovel 40.0% time\nClosed-Set 52.2% person\nAGNEWS\nImplausible 14.9% ology\nNovel 81.7% food\nClosed-Set 3.3% technology\nEMOTION\nImplausible 5.1% app\nNovel 83.9% serenity\nClosed-Set 11.1% frustration\nTACRED\nImplausible 14.3% ualifications\nNovel 73.6% parent company\nClosed-Set 12.1% current location\nFigure 13: Error Analysis on Label Generation. We manually\nannotate generated label sets across all splits of each dataset,\nrecording the frequency of novel and plausible labels.\nWorld Sports Business Sci/Tech\nSplit\n0\n20\n40\n60\n80\n100Percent of Total Generations\nFew-Shot\nNovelty Prompting\nNovel Class\nClosed-Set Class\nImplausible\nFigure 14: Error Analysis on Example Gener-\nation. For each split of AGNews, we manually\nannotate 100 generations each for two gener-\nation methods and compute the frequency of\nnovel class examples, closed-set class exam-\nples, and implausible examples.\n11792\nID Acc (↑) TREC-10 AGNews Emotion TACRED\nVanilla 96.6±0.2 96.1±0.0 97.7±0.1 95.0±0.1\nkFolden 96.5±0.1 96.0±0.1 97.2±0.2 88.3±0.0\nContrastive 95.3±0.1 96.0±0.0 98.0±0.1 94.8±0.2\nCCL + Wikitext 96.6±0.1 96.1±0.1 97.6±0.1 94.9±0.1\nCCL + Zero-Shot 96.5±0.2 96.3±0.0 97.6±0.1 94.9±0.2\nCCL + Few-Shot 96.3±0.2 96.1±0.0 97.8±0.1 94.8±0.2\nOE + Wikitext 96.6±0.2 96.1±0.0 97.6±0.1 94.8±0.1\nOE + Novelty Prompting 71.3±0.9 95.6±0.0 96.4±0.2 94.8±0.2\nCoNAL 96.4±0.2 96.2±0.0 97.8±0.1 94.9±0.1\nCCL + Gold Label † 96.5±0.2 96.1±0.0 97.7±0.1 94.9±0.1\nCCL + Gold Data † 96.0±0.2 95.8±0.1 97.6±0.1 93.8±0.1\nOE + Gold Data † 96.4±0.1 95.8±0.0 97.9±0.1 93.6±0.2\nTable 3: Full Accuracy Results of Contrastive Novelty-Augmented Learning\nA.9 TACRED Processing Details\nWe perform label normalization, removing\nunderscores and prefixes, e.g., converting\nper : employee_of into employee of. This both\nhelps the label generator model understand our\nlabel space and generate more relevant novel\nlabels and ensures that generated novel labels\nare well-formatted for downstream example\ngeneration.\nFor examples, we normalize the Subject and Ob-\nject token tags into a standard English equivalent\ncontaining the subject or object indicator and the\nNER tag, e.g., [subject : person]. To ensure that\ngenerated examples satisfy the task format, we fil-\nter out examples that do not contain exactly one\nsubject and one object (many generations contain\npartial or malformed indicator/NER spans). Finally,\nwe denormalize tags back into original model input\ntokens.\nA.10 Label Filtering\nAfter label generation, we perform synonym filter-\ning to reduce occurrences of ID synonyms. We\nfind this step to have a large impact on datasets for\nwhich labels are common English words which ap-\npear in our thesaurus, and less where label names\nare more abstract. For example, for Emotion and\nTREC-10 , where dataset names are words such\nas “fear” or “human,” filtering removes 21% and\n20% of generated labels respectively. Meanwhile\non both AGNews and TACRED, label filtering re-\nmoves only 2% of labels. In the case of AGNews,\nnews genre overlaps are not easily captured by syn-\nonyms, and even after normalization, many TA-\nCRED labels such as “employee of” do not appear\nin our thesaurus.\nA.11 Label Smoothing performs poorly.\nWe evaluate label smoothing (LS) (Müller et al.,\n2019) as an additional baseline for improving\nOSSC, which mirrors vanilla training but alters\nthe one-hot target vector to a “smoother” version,\nincentivizing uncertainty. Label smoothing has\nbeen shown to be effective in domain shift detec-\ntion (Kong et al., 2020). We use label smoothing\nfactor α= 0.1 and calculate confidence with Max-\nProb. In Table 5, we show that label smoothing\nperforms poorly in our setting. While it does not\naffect classifiers’ ID accuracy, it significantly de-\ncreases AUROC on all but one dataset (TREC-10),\nwhere it still remains worse than CoNAL and all of\nour data generation baselines.\nA.12 Outlier Exposure is sensitive to\ngenerated data\nIn the setting where Outlier Exposure is origi-\nnally evaluated, access to some known OOD data\n(e.g., Wikitext) is assumed (Hendrycks et al., 2019).\nHowever in our setting, where we generate a poten-\ntial novel set, there is no guarantee that the gener-\nated examples are indeed OOD. For example, we\nshow in Appendix A.7 that less than 50% of NP\ngenerations for AGNews come from novel classes.\nWithout this guarantee, more generated data is not\nalways better when training with OE. One risk of\nusing more generated novel data is that the model\nwill see a large number of ID examples in the novel\n11793\nGold Label Generated Label Generated Example\nWORLD\nHEALTH The Top 5 Places to Take a Break From the Rat Race This\nsummer could be the best one in years to take a break and\nrecharge the ol’ batteries.\nTRAVEL U.S. Airlines Say They Won’t Raise Rates to Pay for the New\nSecurity Fee and The government, for its part, has set a deadline\nfor some of the other fees at the beginning of this month.\nPOLITICS The Real Reason Republicans Cautiously Endorse the ’Obscure’\nPaul Ryan - CNN Politics If we’re going to pick fights with the\nother party, we might as well pick fights with the right people.\nSPORTS\nENTERTAINMENT David Beckham And Victoria Beckham Are Exiting The London\nHome They Have Shared With The NBA Star. LONDON (AP) -\nDavid Beckham has been kicked out of the house that he shared\nwith his British soccer team, the Los Angeles Galaxy. The couple\nand the couple’s kids are moving into an apartment\nFASHION In search of fashion in South Africa What will the fashion world\nbe wearing next year? Will the world need another white wed-\nding dress?\nCRIME The Police Are Going To Find You a Lot Easier in the Future:\nSecurity Advances of 2008 (Video) The police are going to find\nyou a lot easier in the future: Security advances of 2008\nBUSINESS\nFOOD /DRINK How to Make a Fried Sausage Slider (Fast) - YouTube A couple\nof weeks ago, I made a fried sausage slider and they’re awesome!\nA few friends have asked me to go through this again but in a\nshorter time to make up a batch quickly for them.\nWEATHER Snowstorm Blasts Northeast with Another Round of Winter\nStorms to Leave Snow, Ice and Blowdowns A big nor’easter is\nexpected to bring sleet, ice and another round of dangerous wind\ngusts around New Year’s Eve. Winter storms are expected to roll\nthrough Wednesday and Thursday in the\nRELATIONSHIPS AP - Michael Phelps and his wife will be moving back to Wash-\nington State from Arizona while he finishes his Olympic career.\nSCI/TECH\nLIFESTYLE A new batch of Apple iPhone 3Gs have gone up for sale in the\nUK, with all six major networks now having a network price.\nApple unveiled the 3Gs on Wednesday, making a number of\nchanges to the device, which is expected to be hugely popular in\nthe market.\nENTERTAINMENT THE FILM: JERSEY GIRL \"Jersey Girl\" tells the story of the\nlove-hate relationship between an Irish-American girl from New\nJersey and a native New Jerseyan. Directed by Elizabeth Swados.\nTECHNOLOGY Yahoo Japan to buy a majority stake in Nikkei Corp Yahoo Japan\nCorporation announced it plans to buy a 69.8 per cent stake of\nNikkei for 1.43billion, the two companies said Friday.\nTable 4: Example novel generations for AGNews.\nset relative to in the training set. We conduct two\nexperiments to analyze the impact of novel set size\nrelative to training set size.\nFirst, we vary the novel set size relative to the\ntraining set size. In Figure 15, we train with novel\nsets on TREC-10 from size 0 to 100K using both\nOE and CCL. We observe that training with OE\nhurts accuracy and AUROC when the novel set is\nlarger than 100 examples, whereas CCL continues\nto improve as the novel set size grows, and main-\ntains accuracy for all novel set sizes. As the novel\nset becomes larger than the size of the training set\n(to the right of the dashed line), both OOD detec-\ntion AUROC and ID accuracy quickly decrease.\nThis result suggests as the ID noise the classifier\nsees in OE training outsizes the training set, its ID\npredictive ability worsens.\nOf the datasets in our experiments, TREC-10 is\nby far the smallest, with only about 2800 training\nexamples per split. To determine whether OE is\nalso sensitive to the size of the ID set, we subsam-\nple the AGNews dataset into smaller training sets\nand perform OE and CCL training with 100K-sized\nnovel sets. We compare the results against Vanilla\ntraining with the same ID sets in Figure 16. Al-\nthough reducing the training set size decreases the\nID accuracy even for vanilla training, CCL training\nachieves similar accuracy for all subsampling sizes.\nWe do observe that a sub-10% accuracy margin ap-\npears between vanilla and CCL at extremely small\n11794\n(↑) TREC-10 AGNews Emotion TACRED Avg\nVanilla\nAUAC 89.2±2.2 87.9±0.6 90.3±1.0 89.6±0.1 89.3\nAUROC 76.6±4.4 76.4±1.0 85.0±2.4 46.3±0.1 71.1\nID Acc 96.6±0.2 96.1±0.0 97.7±0.1 95.0±0.1 96.4\nLS\nAUAC 90.6±1.6 83.5±1.4 82.0±1.7 87.1±1.0 85.8\nAUROC 80.5±3.7 72.9±1.7 75.1±2.3 41.2±2.4 67.4\nID Acc 96.7±0.2 96.2±0.0 97.7±0.1 95.0±0.1 96.4\nTable 5: Label Smoothing (LS) hurts AUROC and AUAC on all but one dataset.\ntraining set sizes, though this margin disappears at\n1000 or more training examples. OE, meanwhile,\ndecreases ID accuracy by as much as 35% when the\ndataset is subsampled to 30 examples, and 25%+\nat 300 examples. OE-trained classifiers are also\nworse OOD detectors given limited training data:\nthey underperform vanilla classifiers for all training\nsets smaller than 3000 examples. Finally, we find\nthat OE does yield better OOD detectors than CCL\nfor sufficiently large AGNews training sets. This\nexpands on our findings in Table 2, suggesting that\nwhen there is access to a large amount of training\ndata, in this case 10000 examples are more, OE can\nlearn from noisy novel sets (though ID accuracy\nstill decreases). Our results indicate that TREC-10\nis not alone: As training set size becomes smaller,\nthe ID classes becomes less well-specified, and ID\nexamples present in the novel set induce the model\nto make incorrect predictions (and poor confidence\nestimates) on true ID test examples.\nA.13 CoNAL and Separability\nTo understand why CoNAL improves AUROC, we\ncompare the confidence profiles of a vanilla fine-\ntuned classifier against those of a CoNAL trained\nclassifier. Specifically, in Figure 17, we select 50\nrandom ID examples and 50 random OOD exam-\nples from each dataset split and compute MaxProb\nconfidences. We find that CoNAL decreases confi-\ndence on OOD examples, though not to the same\nextent on all examples. In datasets like TREC-10\nand Emotion where CoNAL achieves stronger AU-\nROC gains, the decrease in OOD confidence is\nmore pronounced. Though ID test examples also\ndecrease in confidence on all dataset splits, this\ndecrease is less pronounced and is likely due to the\nconfidence contrastive objective term incentivizing\nthe model’s confidence distributions to be generally\nless peaked.\nThe shifts reflected in the confidence distribu-\ntions directly impact the separability of OOD and\nID examples. On the Vanilla model confidence axis,\nit is difficult to identify a threshold above which\nmost examples are ID and below which most exam-\nples are OOD. Given CoNAL confidences, OOD\nand ID examples are more separable. This visual\nseparability is reflected in the OOD Detection AU-\nROC metric.\nTo demonstrate the strictness of the OE objec-\ntive, we plot the confidences of the same examples\nwithout (Vanilla) and with OE training in Figure 18.\nFirst, we observe that the vast majority of OOD ex-\namples have similar confidence after OE training,\nas they are all pushed towards minimum confidence\n(maximum entropy). Second, we observe that OE\naffects the confidence of ID test examples, decreas-\ning the confidence of some examples lower than\nthat of OOD test examples.\nA.14 Measuring Data Leakage in Generation\nIn our experiments, we explicitly forbid the gold\nnovel class from being generated, such that the\nLLM is disincentivized from generating gold novel\nexamples if the dataset has been seen in pretrain-\ning. However, it remains possible that if the LLM\nhad seen the task data in pretraining, it could repli-\ncate parts of or an entire example from the dataset\nin generations. Unfortunately, as we do not have\naccess to GPT-3 pretraining data, we cannot deter-\nmine whether or not this is indeed a risk. Instead,\nwe probe whether this is a possiblity via an n-gram\noverlap metric comparing the similarity between\nour generated examples and the test set.\nSpecifically, we measure the average fraction of\nn-grams in a generation that also appear in the test\nset, which we interpret as the maximal frequency\nthat the LLM could have copied test data via pre-\ntraining leakage. For comparison, we compute\nthe same metric between the test set and heldout\nnovel class data. In this case, examples are sam-\npled from exactly the same distribution and thus\nexpected to exhibit some n-gram overlap due to\nshared background features. We use this value as\na baseline: generation n-gram overlap should be\n11795\n100 101 102 103 104 105\nNovel Set Size\n84\n86\n88\n90\n92\n94AUAC\nCCL\nOE\n100 101 102 103 104 105\nNovel Set Size\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0AUROC\nCCL\nOE\n100 101 102 103 104 105\nNovel Set Size\n75\n80\n85\n90\n95ID Accuracy (%) \nCCL\nOE\nFigure 15: Outlier Exposure is sensitive to the size of the novel set on TREC-10. We vary the novel set size from 0\nto 100K, finding that both accuracy and AUROC decrease with as few as 100 novel generations. We indicate with a\ndashed line the point where the novel set and training set size are approximately equal.\n102 103 104\nTraining Set Size\n50\n60\n70\n80\n90AUAC\nCCL\nOE\nVanilla\n102 103 104\nTraining Set Size\n60\n65\n70\n75\n80\n85AUROC\nCCL\nOE\nVanilla\n102 103 104\nTraining Set Size\n50\n60\n70\n80\n90ID Accuracy (%) \nCCL\nOE\nVanilla\nFigure 16: Outlier Exposure disproportionately hurts smaller datasets. We subsample the training set for AGNews,\nuse 100K novel generated examples, and vary the training loss. We find that CCL achieves similar ID performance\nas Vanilla at all training set sizes, but OE hurts accuracy when the training set is smaller than 1000 examples.\nsimilar to or lower than heldout n-gram overlap.\nWe find in Table 6 that the n-gram overlap of our\nnovelty prompted generations is lower across all\ndatasets and values of n than of the heldout set,\nindicating that the performance of CoNAL should\nnot be attributed to example data leakage.\n11796\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTREC-10 Split 0\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTREC-10 Split 1\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTREC-10 Split 3\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTREC-10 Split 4\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTREC-10 Split 5\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nTACRED Split 0\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nAGNews Split 0\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nAGNews Split 1\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nAGNews Split 2\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nAGNews Split 3\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nEmotion Split 0\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nEmotion Split 1\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nEmotion Split 3\nID\nOOD\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence CoNAL\nEmotion Split 4\nID\nOOD\nFigure 17: CoNAL improves the separability of ID and OOD examples. We plot the confidences of 50 random ID\nand 50 random OOD examples on a vanilla finetuned BERT classifier versus a CoNAL trained BERT classifier.\nCoNAL successfully decreases the confidence of OOD test examples while minimizing the impact of the confidence\nof ID test examples.\n11797\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTREC-10 Split 0\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTREC-10 Split 1\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTREC-10 Split 3\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTREC-10 Split 4\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTREC-10 Split 5\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nTACRED Split 0\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nAGNews Split 0\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nAGNews Split 1\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nAGNews Split 2\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nAGNews Split 3\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nEmotion Split 0\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nEmotion Split 1\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nEmotion Split 3\nID\nOOD\n1\n2\n1 10 1\n 1 10 2\n 1 10 3\n 1 10 4\n 1 10 5\nConfidence Vanilla\n1\n2\n1 10 1\n1 10 2\n1 10 3\n1 10 4\n1 10 5\nConfidence OE + NP\nEmotion Split 4\nID\nOOD\nFigure 18: OE decreases the confidence of OOD examples, but unfortunately also decreases confidence on ID\nexamples. We plot the confidences of 50 random ID and 50 random OOD examples on a vanilla finetuned BERT\nclassifier versus a NP+OE trained BERT classifier. We also observe that ID examples exhibit a large confidence\ndistribution after OE training: some ID examples have similar confidence as OOD examples. Note that the axis\nlimits on these plots differ from the axis limits on Figure 17, as confidences in general are much lower.\n11798\n(%) n = 2 3 4 5 6 7\nAGNews Generation 61.6 23.4 8.8 4.1 2.2 1.3\nHeldout 70.4 36.7 20.4 13.1 9.0 6.7\nTREC-10 Generation 46.2 21.4 10.3 4.1 1.7 0.7\nHeldout 47.5 23.9 11.1 5.1 2.6 1.4\nEmotion Generation 56.8 20.8 6.5 1.7 0.4 0.1\nHeldout 59.3 26.3 10.9 3.7 1.1 0.3\nTACRED Generation 86.8 68.1 57.2 47.9 40.2 32.9\nHeldout 87.2 72.3 63.6 58.2 54.2 50.5\nTable 6: Novel generations have lower n-gram overlap with the test set than heldout novel class examples. We\nmeasure the percent of generation n-grams which appear in the test set, comparing this against a baseline of heldout\nnovel class examples. Across all dataset and measured values of n, we find this overlap to be lower than baseline.\n11799\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations. After section 7, on page 9\n□\u0013 A2. Did you discuss any potential risks of your work?\nLimitations. After section 7, on page 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nPage 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nCode described in Appendix A.2\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.1, Section 4.2\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A.2\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix A.2\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection 4.2, Section 5, Appendix A.1, A.9, A.10, A.11\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.2, Appendix A.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11800\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.2, Appendix A.1, A.9, A.10, A.11\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nTable 1. Table 2. Table 3. Table 5. Figure 6. Figure 12. We report the average over multiple seeds on\nall tables, and report standard error in subscript.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n11801"
}