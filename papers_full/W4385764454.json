{
  "title": "Local-Global Transformer Enhanced Unfolding Network for Pan-sharpening",
  "url": "https://openalex.org/W4385764454",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2159070171",
      "name": "Mingsong Li",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2102814271",
      "name": "Liu Yikun",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A1993803783",
      "name": "Tao Xiao",
      "affiliations": [
        "Shandong University"
      ]
    },
    {
      "id": "https://openalex.org/A2099025214",
      "name": "Yuwen Huang",
      "affiliations": [
        "Heze University"
      ]
    },
    {
      "id": "https://openalex.org/A2163341010",
      "name": "Gongping Yang",
      "affiliations": [
        "Shandong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2001298023",
    "https://openalex.org/W3103556460",
    "https://openalex.org/W4313071068",
    "https://openalex.org/W4285507497",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2963814976",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2112693869",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W3172472472",
    "https://openalex.org/W4313133670",
    "https://openalex.org/W4226068212",
    "https://openalex.org/W3160949528",
    "https://openalex.org/W2963284277",
    "https://openalex.org/W4305013690",
    "https://openalex.org/W2171108951",
    "https://openalex.org/W2798559986",
    "https://openalex.org/W2948437909",
    "https://openalex.org/W4287824590",
    "https://openalex.org/W2777033955",
    "https://openalex.org/W4283374476",
    "https://openalex.org/W3088339669",
    "https://openalex.org/W4281383159",
    "https://openalex.org/W2001800591",
    "https://openalex.org/W4312908055",
    "https://openalex.org/W2144948131",
    "https://openalex.org/W4312918841",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2171845746",
    "https://openalex.org/W2096070062",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2100556411",
    "https://openalex.org/W2116536901",
    "https://openalex.org/W4283815286",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Pan-sharpening aims to increase the spatial resolution of the low-resolution multispectral (LrMS) image with the guidance of the corresponding panchromatic (PAN) image. Although deep learning (DL)-based pan-sharpening methods have achieved promising performance, most of them have a two-fold deficiency. For one thing, the universally adopted black box principle limits the model interpretability. For another thing, existing DL-based methods fail to efficiently capture local and global dependencies at the same time, inevitably limiting the overall performance. To address these mentioned issues, we first formulate the degradation process of the high-resolution multispectral (HrMS) image as a unified variational optimization problem, and alternately solve its data and prior subproblems by the designed iterative proximal gradient descent (PGD) algorithm. Moreover, we customize a Local-Global Transformer (LGT) to simultaneously model local and global dependencies, and further formulate an LGT-based prior module for image denoising. Besides the prior module, we also design a lightweight data module. Finally, by serially integrating the data and prior modules in each iterative stage, we unfold the iterative algorithm into a stage-wise unfolding network, Local-Global Transformer Enhanced Unfolding Network (LGTEUN), for the interpretable MS pan-sharpening. Comprehensive experimental results on three satellite data sets demonstrate the effectiveness and efficiency of LGTEUN compared with state-of-the-art (SOTA) methods. The source code is available at https://github.com/lms-07/LGTEUN.",
  "full_text": "Local-Global Transformer Enhanced Unfolding Network for Pan-sharpening\nMingsong Li1 , Yikun Liu1 , Tao Xiao1 , Yuwen Huang2 , and Gongping Yang1∗\n1School of Software, Shandong University, Jinan, China\n2School of Computer, Heze University, Heze, China\n{msli, peachxiao}@mail.sdu.edu.cn, {liuyk29, hzxy hyw}@163.com, gpyang@sdu.edu.cn\nAbstract\nPan-sharpening aims to increase the spatial reso-\nlution of the low-resolution multispectral (LrMS)\nimage with the guidance of the correspond-\ning panchromatic (PAN) image. Although deep\nlearning (DL)-based pan-sharpening methods have\nachieved promising performance, most of them\nhave a two-fold deficiency. For one thing, the\nuniversally adopted black box principle limits the\nmodel interpretability. For another thing, exist-\ning DL-based methods fail to efficiently capture lo-\ncal and global dependencies at the same time, in-\nevitably limiting the overall performance. To ad-\ndress these mentioned issues, we first formulate the\ndegradation process of the high-resolution multi-\nspectral (HrMS) image as a unified variational opti-\nmization problem, and alternately solve its data and\nprior subproblems by the designed iterative proxi-\nmal gradient descent (PGD) algorithm. Moreover,\nwe customize a Local-Global Transformer (LGT)\nto simultaneously model local and global depen-\ndencies, and further formulate an LGT-based prior\nmodule for image denoising. Besides the prior\nmodule, we also design a lightweight data module.\nFinally, by serially integrating the data and prior\nmodules in each iterative stage, we unfold the itera-\ntive algorithm into a stage-wise unfolding network,\nLocal-Global Transformer Enhanced Unfolding\nNetwork (LGTEUN), for the interpretable MS\npan-sharpening. Comprehensive experimental re-\nsults on three satellite data sets demonstrate the\neffectiveness and efficiency of LGTEUN com-\npared with state-of-the-art (SOTA) methods. The\nsource code is available at https://github.com/\nlms-07/LGTEUN.\n1 Introduction\nWith the development of remote sensing field, multispectral\n(MS) image is capable of recording more abundant spectral\nsignatures in spectral domain compared with RGB image, and\n∗Corresponding author\n10\n0\n10\n1\n10\n2\n10\n3\nFLOPs…(GB)\n41.0\n41.2\n41.4\n41.6\n41.8\n42.0\n42.2\n42.4\n42.6\n42.8PSNR…(dB)\nLGTEUN\nOURS\n(202.2 KB)\nMutInf\nCVPR 2022\n(185.5 KB)\nSFIIN\nECCV 2022\n(85.3  KB)\nMDCUN\nCVPR 2022\n(98.4 KB)\nLightNet\nIJCAI 2022\n(15.8 KB)\nCTINN\nAAAI 2022\n(37.8 KB)\nPanFormer \nICME 2022\n(1525.1 KB)\nFigure 1: PSNR-Params-FLOPs comparisons between six SOTA\nDL-based pan-sharpening methods and our LGTEUN on the\nWorldView-2 satellite data set. The vertical axis is PSNR (model\nperformance), the horizontal axis is FLOPs (computational cost),\nand the circle radius is Params (model complexity).\nis widely applied in various fields, e.g., environmental moni-\ntoring, precision agriculture, and urban development [Hardie\net al., 2004; Fauvel et al., 2012 ]. However, due to the in-\nherent trade-off between spatial and spectral resolution, it is\nhard to directly acquire high-resolution multispectral (HrMS)\nimages. Pan-sharpening, a vital yet challenging remote sens-\ning image processing task, aims to produce a HrMS image\nfrom the coupled low-resolution multispectral (LrMS) and\npanchromatic (PAN) images.\nFormally, the degradation process of the HrMS image Z ∈\nRHW ×B is often expressed as [Hardie et al., 2004; Xie et al.,\n2019; Dong et al., 2021]:\nX = SZ + Nx, Y = ZR + Ny, (1)\nwhere S ∈ Rhw×HW is a linear operator for the spatial blur-\nring and downsampling, R ∈ RB×1 is the spectral response\nfunction of the PAN imaging sensor, and Nx and Ny are the\nintroduced noises during the image acquisition of the LrMS\nimage X ∈ Rhw×B and the PAN image Y ∈ RHW ×1, re-\nspectively. Here, H and h (H > h),W and w (W > w),\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1071\nand B represent the spatial height, the spatial weight, and the\nnumber of spectral bands of the corresponding image, respec-\ntively. In the past few decades, many methods have been de-\nveloped in light of the degradation process in Eq. (1), which\ncan be roughly divided into two categories, i.e., model-based\nand deep learning (DL)-based.\nTypical model-based methods include component substi-\ntution (CS) [Aiazzi et al., 2007 ], multiresolution analysis\n(MRA) [Liu, 2000; King and Wang, 2001 ], and variational\noptimization (VO) [Ballester et al., 2006; Fu et al., 2019 ].\nThese methods rely on prior subjective assumptions in the\nsuper-resolving process, and show limited model perfor-\nmance and generalization ability in real scenes. Attracted by\nthe impressive success of DL in various vision tasks [He et\nal., 2016; Chollet, 2017], many DL-based methods have been\ndeveloped for pan-sharpening, especially convolutional neu-\nral network (CNN)-based methods. Owing to the outstand-\ning feature representation in hierarchical manner, DL-based\nmethods are capable of directly learning strong priors, and\nachieve competitive performance, e.g., PanNet [Yang et al.,\n2017] and SDPNet [Xu et al., 2020].\nModel Interpretability: Despite the strong feature extrac-\ntion ability and encouraging performance improvement, the\nweak model interpretability is a longstanding deficiency for\nDL-based methods due to the adopted black box principle.\nTo this end, deep unfolding networks (DUNs) combine merits\nof both model-based and DL-based methods, and reasonably\nformulate end-to-end DL models tailored to the investigated\npan-sharpening problem employing the theoretical designing\nphilosophy. For instance, Xu et al. [Xu et al., 2021 ] devel-\noped the first DUN for pan-sharpening, justifying the genera-\ntive models for LrMS and PAN images.\nLocal and Global Dependencies: Although existing\nDUNs strengthen the model interpretability towards the in-\nvestigated pan-sharpening problem, their potential has been\nfar from fully explored. Here, for DUNs, we claim that a\ncompetitive denoiser of the image denoising step would suffi-\nciently complement the data step in each iterative stage. How-\never, limited by the local receptive field, most popular CNN-\nbased denoisers pay less attention to global dependencies,\nwhich are as important as local dependencies. Furthermore,\nglobal transformer, e.g., Vision Transformer (ViT) [Doso-\nvitskiy et al., 2020 ], can capture global dependencies, ob-\ntaining outstanding performance in vision tasks. Yet, global\ntransformer has nontrivial quadratic computational complex-\nity to input image size due to the computation of global self-\nattention, which inescapably decreases model efficiency.\nSimilarly, MDCUN [Yang et al., 2022 ] employed non-\nlocal prior and non-local block [Wang et al., 2018] for mod-\neling long-range dependencies, thus showing high compu-\ntational cost. Besides DUNs, Zhou et al. [Zhou et al.,\n2022a] proposed a modality-specific PanFormer based on\nSwin Transformer [Liu et al., 2021 ]. To capture local and\nlong-range dependencies, Zhou et al. [Zhou et al., 2022b] de-\nsigned a CNN and transformer dual-branch model, CTINN.\nHowever, no matter the serial model, e.g., PanFormer, or the\ndual-branch model, e.g., CTINN, they both fail to model local\nand global dependencies in the same layer, which inevitably\ngenerates few limitations to the image denoiser or the total\npan-sharpening model.\nFollowing the above analysis, in this paper, we develop\na transformer-based deep unfolding network, Local-Global\nTransformer Enhanced Unfolding Network (LGTEUN), for\nthe interpretable MS pan-sharpening. To be specific, we first\nformulate a unified variational optimization problem in light\nof the degenerating observation of pan-sharpening, and de-\nsign an iterative proximal gradient descent (PGD) algorithm\nto alternately solve its data and prior subproblems. Second,\nwe elaborate a Local-Global Transformer (LGT) as a prior\nmodule for image denoising. The key component in each\nLGT basic block is its token mixer, the Local-Global Mixer\n(LG Mixer), which consists of a local branch and a global\nbranch. The local branch calculates local window based\nself-attention in spatial domain, while the global branch ex-\ntracts global contextual feature representation in frequency\ndomain. Therefore, the LGT-based prior module can simul-\ntaneously capture local and global dependencies, and we also\ndesign a lightweight data module. Finally, when unfolding\nthe iterative algorithm into the stage-wise unfolding network,\nLGTEUN, we serially integrate the lightweight data module\nand the powerful prior module in each iterative stage. Exten-\nsive experimental results on three satellite data sets demon-\nstrate the superiority of our method compared with other\nstate-of-the-art (SOTA) methods (as shown in Fig. 1). Our\ncontributions can be summarized as follows:\n1) We customize a transformer module LGT as an image\ndenoiser to efficiently model local and global dependencies\nat the same time and sufficiently mine the potential of the\nproposed unfolding pan-sharpening framework.\n2) We develop an interpretable transformer-based deep un-\nfolding network, LGTEUN. To the best of our knowledge,\nLGTEUN is the first transformer-based deep unfolding net-\nwork for the MS pan-sharpening, and LGT is also the first\ntransformer module to perform spatial and frequency dual-\ndomain learning.\n2 Related Work\nIn this section, the related deep unfolding networks and\ntransformer-based methods are briefly reviewed.\n2.1 Deep Unfolding Network\nThrough integrating merits of both model-based and DL-\nbased methods, deep unfolding networks (DUNs) much im-\nprove the interpretability of DL-based models. DUN un-\nfolds the iterative algorithm tailored to the investigated prob-\nlem, and optimizes the algorithm employing neural mod-\nules in an end-to-end trainable manner. DUN has been uti-\nlized to solve different low-level vision tasks, including im-\nage denoising [Mou et al., 2022 ], image compressive sens-\ning [Zhang and Ghanem, 2018 ], image reconstruction [Cai\net al., 2022 ], and image super-resolution [Xie et al., 2019;\nZhang et al., 2020; Dong et al., 2021 ]. For the discussed\nMS pan-sharpening, GPPNN [Xu et al., 2021] and MDCUN\n[Yang et al., 2022 ] are two representative DUNs. However,\nrestricted by the local receptive field, the adopted CNN-based\ndenoiser in GPPNN pays less attention to global dependen-\ncies, which is adverse for reducing copy artifacts. Although\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1072\nMDCUN introduces non-local prior and non-local block to\nmodel long-range dependencies, the additional computational\ncost is heavy. Thus, it is still a crucial issue for DUN to for-\nmulate a competitive denoiser of the image denoising step to\nefficiently capture local and global dependencies and further\nsufficiently complement the data step in each iterative stage.\n2.2 Transformer\nOriginating from language tasks[Vaswaniet al., 2017], trans-\nformer has an excellent ability to capture global dependen-\ncies, and has been widely applied in various vision tasks,\ne.g., image classification, object detection, semantic seg-\nmentation, and image restoration [Dosovitskiy et al., 2020;\nLiu et al., 2021; Liang et al., 2021; Zhou et al., 2022a;\nZhou et al., 2022b ]. However, transformer encounters two\nmain issues. 1) Global transformer has nontrivial quadratic\ncomputation complexity to input image size due to the com-\nputation of image-level self-attention. 2) Although equipped\nwith considerable-size windows and non-local interactions\nacross windows, local transformer still has difficulties to\nmodel image-level global dependencies. Moreover, for pan-\nsharpening task, transformer-involved models, e.g., [Zhou et\nal., 2022a] and [Zhou et al., 2022b], fail to process both local\nand global dependencies at the same time, which inevitably\nlimits the overall performance.\n3 Method\n3.1 Model Formulation and Optimization\nTechnically, under the maximizing a posterior (MAP) frame-\nwork, recovering the original HrMS image based on the\ndegradation process in Eq. (1) is a typical ill-posed prob-\nlem. Generally, the estimation of the HrMS image Z is im-\nplemented by minimizing the following energy function as\n¯Z = argmin\nZ\n1\n2∥ X − SZ ∥2 + 1\n2∥ Y − ZR ∥2 + λJ(Z),\n(2)\nwhere 1\n2 ∥ X − SZ ∥2\n2 and 1\n2 ∥ Y − ZR ∥2\n2 are the two data\nfidelity terms coinciding with the degenerating observation,\nJ(Z) is the prior term to constraint the solution space, and λ\nis a trade-off parameter.\nSubsequently, proximal gradient descent (PGD) algorithm\n[Beck and Teboulle, 2009] is employed to solve Eq. (2) as an\niterative convergence problem, i.e.,\n¯Zk = argmin\nZ\n1\n2∥ Z − (¯Zk−1 − η∇f (¯Zk−1) ∥\n2\n+ λJ(Z),\n(3)\nwhere ¯Zk denotes the output of the k-th iteration, and η is the\nstep size. Here, the data terms oriented differentiable operator\n∇f (¯Zk−1) is further calculated as\n∇f (¯Zk−1) = ST (S¯Zk−1 − X) + (¯Zk−1R − Y)RT . (4)\nMoreover, this iterative problem can be addressed by alter-\nnately solving its data subproblem at the gradient descent step\n(Eq. (5)) and its prior subproblem at the proximal mapping\nstep (Eq. (6)). In detail,\n¯Zk−1\n2\n= ¯Zk−1 − η∇(¯Zk−1), (5)\n¯Zk = proxη,J (¯Zk−1\n2\n), (6)\nwhere proxη,J represents the proximal operator dependent\non the prior term J(·). In this way, the PGD algorithm uti-\nlizes a few iterations to alternately update ¯Zk−1\n2\nand ¯Zk un-\ntil convergence. In particular, from a Bayesian perspective,\nthe solution of the prior subproblem Eq. (6) corresponds to\na Gaussian denoising problem with noise level\n√\nλ [Chan et\nal., 2016; Zhang et al., 2020; Mou et al., 2022; Cai et al.,\n2022]. In this work, we elaborate a transformer-based de-\nnoiser to approximate the proximal operator proxη,J , which\nprominently facilitates the denoising capability and further\nsufficiently complements the data step in each iterative stage.\n3.2 Deep Unfolding Network\nThrough unfolding the iterative PGD algorithm, as illus-\ntrated in Fig. 2, we develop our Local-Global Transformer\nEnhanced Unfolding Network (LGTEUN). The LGTEUN\nis comprised of several stages. Each stage contains a\nlightweight CNN-based data module D and a powerful\ntransformer-based prior module P, corresponding to the data\nsubproblem at the gradient descent step (Eq. (5)) and the prior\nsubproblem at the proximal mapping step (Eq. (6)) in each it-\neration, respectively.\nData Module D\nTo approximate the closed-form solution of the data sub-\nproblem at the gradient descent step (Eq. (5)), we design a\nlightweight CNN-based data module, i.e.,\n¯Zk−1\n2\n= D(¯Zk−1, X, Y, ηk−1). (7)\nSpecifically, as shown in Fig. 2 (a), the data module of the\nk-th stage takes the out of the k − 1-th stage ¯Zk−1, the LrMS\nimage1 X, the PAN image Y, and the stage-specific learn-\nable step size ηk−1 as its module inputs. What’s more, the\nmatrix S is implemented by two downsampling units, and\neach unit consists of a downsampling operation and a 3×3\ndepth convolution (Conv) layer [Chollet, 2017 ]. Similarly,\nthe transposed matrix ST is performed by two upsampling\nunits. Besides, one point Conv [Chollet, 2017] is utilized as\nthe matrix R to reduce channels from B to 1, and another\npoint Conv is utilized as the matrixRT for the corresponding\ninverse channel increase.\nPrior Module P\nConsidering the designing of the denoiser at the image de-\nnoising step, previous DUNs are mainly based on CNN, pre-\nsenting limitations in capturing global dependencies. Here,\nas the first transformer-based image denoiser in the MS pan-\nsharpening oriented DUN, we dedicate significant efforts to\ncraft a Local-Global Transformer (LGT) as the key denoising\nprior module P, i.e.,\n¯Zk = P(¯Zk−1\n2\n). (8)\n1As an explanatory instance, the dimension of X is HW × B\nin mathematical formalization like Eq. (1), and H × W × B in\nprogramming implementation here.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1073\nStage 1 Stage K\n… …\nUp 4\nStage k\nX\nY\nLN\nLG Mixer\nLN\nChannel Mixer\nLN\nLG Mixer\nLN\nChannel Mixer\n Point Conv \nDepthwise \nConv33\nGELU\n Point Conv \nGELU\n Point Conv \nDepthwise \nConv33\nGELU\n Point Conv \nGELU\nElement-wise AddingElement-wise Adding Element-wise SubtractingElement-wise Subtracting\n Element-wise Multiplicating\n(a) LGTEUN\n(b) LGT Block\n(c) Channel Mixer\nHalf Channel Split\nWindow Partition\nWindow Mergence\nWMSA\nConcat\nReal FFT\nInv Real FFT\nDepth Conv11 Depth Conv11\nAmplitude  Phase\n Point Conv \nLocal Branch Global Branch\n(d) LG Mixer\nS\n Point Conv \n Point Conv \nRT\nY\n  \nST\nX\nR\nR\nPatch \nEmdeding\nPatch \nEmdeding\nPatch \nUnemdeding\nEncoder Decoder\nBottleneck\nLGT Block\nBottleneck\nLGT Block\nDown 2\n Point Conv \nLGT Block\nLGT Block\nDown 2\n Point Conv \nLGT Block\nLGT Block\nLGT Block\nLGT Block\nLGT Block\nLGT Block\nUp 2\n Point Conv \nUp 2\n Point Conv \nLGT Block\nLGT Block\nUp 2\n Point Conv \n Point Conv \nConcat\n Point Conv \nConcat\nPatch \nEmdeding\nPatch \nUnemdeding\nEncoder Decoder\nBottleneck\nLGT Block\nDown 2\n Point Conv \nLGT Block\nLGT Block\nLGT Block\nLGT Block\nUp 2\n Point Conv \n Point Conv \nConcat\nQ K V,,\n0Ζ\n1Z\n1Zk−\n1\n2\nZk−\nZk\n1ZK −\nZK\n1k −\n1Zk−\n1\n2\nZk−\nZk\n  \n0X\nXd\nXl\nXg\nFl\nFg\nUp UpsamplingUp Upsampling Down DownsamplingDown Downsampling\nXin\nFigure 2: Illustration of the proposed LGTEUN. (a) The overall architecture of LGTEUN with K stages and details of the k-th stage. The\nlightweight CNN-based data module D and the powerful transformer-based prior module P in each stage correspond to the data and prior\nsubproblems in an iteration of the PGD algorithm. (b) Components of an LGT block. (c) The adopted channel mixer. (d) The key LG Mixer\nis comprised of a local branch and a global branch.\nOverall Architecture of LGT. As depicted in Eq. (8) and\nFig. 2 (a), given the output ¯Zk−1\n2\n∈ RH×W×B of D as\nthe module input, LGT applies a U-shaped structure mainly\nconstituted by a series of basic LGT Blocks, and outputs\n¯Zk ∈ RH×W×B as the module output. Concretely, LGT\nfirst uses a patch embedding layer to split the intermediate\nimage ¯Zk−1\n2\ninto non-overlapping patch tokens and further\nproduces the embedded feature X0 ∈ RH×W×C. Second, an\nencoder-bottleneck-decoder structure extracts the discrimina-\ntive feature representation Xd ∈ RH×W×C from X0. In par-\nticular, the encoder and decoder both contain two LGT blocks\nand a resizing unit, and the bottleneck has a single LGT block.\nIn each resizing unit, the downsampling or upsampling oper-\nation is responsible for resizing spatial resolution, and a point\nConv changes the channel dimension accordingly. Finally, a\npatch unembedding layer is employed to project Xd to ¯Zk.\nHere, note that the patch size is set as 1, thus the original\npixel vectors in ¯Zk−1\n2\nact as the discussed patch tokens for\nfiner local and global token mixing.\nIn Fig. 2 (b), each LGT block consists of a layer normal-\nization (LN), a Local-Global Mixer (LG Mixer) for mixing\nthe spatial information, a LN, and a channel mixer in order.\nAs illustrated in Fig. 2 (c), the channel mixer is a depthwise\nConv [Chollet, 2017] based neural module for efficient chan-\nnel mixing. Specifically, the LG Mixer as the token mixer is\nthe key component in each LGT block, and Fig. 2 (d) depicts\nthe LG Mixer of the first LG Block in the encoder. For conve-\nnience, let Xin ∈ RH×W×C represent the input feature map\nof our LG Mixer, which is further split into two equal parts\nXl ∈ RH×W×C\n2 and Xg ∈ RH×W×C\n2 along the channel\ndimension. Then Xl and Xg are assigned to a local branch\nand a global branch, respectively. The local branch models\nlocal dependencies by computing local window based self-\nattention in spatial domain, while the global branch captures\nglobal dependencies by mining global contextual feature rep-\nresentation in frequency domain. By concatenating the out-\nput of the local branch Fl ∈ RH×W×C\n2 and that of theglobal\nbranch Fg ∈ RH×W×C\n2 , the local and global dependencies\nare simultaneously captured in our LG Mixer.\nLocal Branch. The local branch calculates local window\nbased multi-head self-attention (WMSA) in spatial domain.\nIn detail, as shown in the left path of Fig. 2 (d), Xl is first\npartitioned into non-overlapping windows, and each window\ncontains M × M patch tokens. Then the window-specific\nfeature map with HW\nM2 × M2 × C\n2 dimension is obtained by\nsimply reshaping. Subsequently, three feature embeddings\nQ, K, and V ∈ R\nHW\nM2 ×M2×C\n2 are generated through a point\nConv based linear projection. Furthermore, Q, K, and V are\nchannel-wise divided into h heads, i.e., Q = [Q1, ...,Qh],\nK = [K1, ...,Kh], and V = [V1, ...,Vh]. Each head con-\ntains d = C\n2h channels, and Fig. 2 (d) only presents the cir-\ncumstance with h = 1 for simplification. More importantly,\nthe WMSA map is computed as\nFi\na = Softmax (QiKiT\n√\nd\n+ Pi)Vi, i = 1, ..., h, (9)\nwhere Pi ∈ RM2×M2\nis the learnable position embedding.\nAt last, for the feature map Fa, we channel-wise concatenate\nits h heads and spatially merge its HW\nM2 windows to yield the\nbranch output Fl.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1074\nGlobal Branch. The global branch extracts global contex-\ntual feature representation in frequency domain based on the\nnature of Fourier transformation. To be specific, accord-\ning to spectral convolution theorem in Fourier theory [Frigo\nand Johnson, 1998; Chi et al., 2020; Zhao et al., 2022;\nZhou et al., 2022c; Zhou et al., 2022e ], feature learning in\nfrequency spectral domain has the image-wide receptive field\nby channel-wise Fourier transformation. Besides, point-wise\nmultiplications in frequency domain correspond to convolu-\ntions in spatial domain. These properties provide vital theo-\nretical guidances of our global branch.\nFormally, 2D discrete Fourier transform (DFT) first con-\nverts Xg from spatial domain to Fourier frequency domain as\nthe complex component F(Xg), i.e.,\nF(Xg)(u, v) = 1\n√\nHW\nH−1X\nh=0\nW−1X\nw=0\nXg(h, w)e−j2π( h\nH u+ w\nW v),\n(10)\nwhere u and v are frequency components. Here, F(Xg) ∈\nCH×( W\n2 +1)×C\n2 is produced in light of the conjugate symme-\ntry property of 2D DFT for our real input Xg, and C denotes\ncomplex domain. Besides, the inverse 2D DFT is accordingly\nrepresented as F−1(·). Then based on the real part R(Xg)\nand the imaginary part I(Xg) of F(Xg), the amplitude com-\nponent A(Xg) and the phase component P(Xg) are further\nexpressed as\nA(Xg)(u, v) =\nq\nR2(Xg)(u, v) + I2(Xg)(u, v), (11)\nP(Xg)(u, v) = arctan[ I(Xg)(u, v)\nR(Xg)(u, v)]. (12)\nFurthermore, two independent 1×1 depth Convs are uti-\nlized for feature learning in frequency domain, and the in-\nverse 2D DFT F−1(·) is applied to recompose the feature\nrepresentations of the amplitude and phase components back\nto spatial domain. In detail,\nFg = F−1(DConv(A(Xg)), DConv(P(Xg))), (13)\nwhere DConv represents the applied 1×1 depth Conv. In\nfact, to improve module efficiency, the 2D DFT and the in-\nverse 2D DFT are computed by the 2D real fast Fourier trans-\nform (rFFT) and the inverse 2D rFFT, which can be imple-\nmented by torch.rfft2 and torch.irfft2 in PyTorch program-\nming framework, respectively. The flowchart of our global\nbranch is depicted in the right path of Fig. 2 (d).\n4 Experiments\n4.1 Data Sets and Evaluation Metrics\nFor the MS pan-sharpening, an 8-band MS data set ac-\nquired by the WorldView-3 sensor2 and two 4-band MS data\nsets acquired by WorldView-2 2 and GaoFen-2 sensors are\nadopted for experimental analysis. Due to the unavailability\nof ground-truth (GT) images for training, following Wald’s\n2https://www.l3harris.com/all-capabilities/\nhigh-resolution-satellite-imagery\nData Set Metric Stage 1 Stage 2 Stage 3 Stage 4\nWorldView-3\nPSNR↑ 32.0339 32.2188 32.068 32.0042\nSSIM↑ 0.9532 0.9545 0.9535 0.9527\nQ8↑ 0.9481 0.9494 0.9487 0.9480\nSAM↓ 0.0605 0.0605 0.0603 0.0612\nERGAS↓ 2.6765 2.6286 2.6678 2.6898\nTime (s/img) 0.0070 0.0133 0.0205 0.0262\nParams (KB) 270.2 540.0 809.9 1079.7\nFLOPs (GB) 9.52 19.04 28.56 38.08\nWorldView-2\nPSNR↑ 42.600 42.6837 42.4771 42.1634\nSSIM↑ 0.9784 0.9786 0.9781 0.9767\nQ4↑ 0.8398 0.8415 0.8383 0.8329\nSAM↓ 0.0209 0.0208 0.0213 0.0222\nERGAS↓ 0.9358 0.928 0.9573 0.9787\nTime (s/img) 0.0065 0.0137 0.0204 0.0254\nParams (KB) 101.2 202.2 303.2 404.2\nFLOPs (GB) 2.57 5.14 7.71 10.28\nTable 1: Performance and efficiency of LGTEUN with different\nnumbers of stages K on WorldView-3 and WorldView-2 satellite\ndata sets.\nprotocol [Wald et al., 1997], we employ downsampling oper-\nations to produce a reduced-resolution data set for each satel-\nlite sensor. Each data set is further split into non-overlapping\nsubsets for training (about 1000 LrMS/PAN/GT image pairs)\nand testing (about 140 LrMS/PAN/GT image pairs). The\nspatial sizes of LrMS, PAN, and GT images are 32 × 32,\n128 × 128, and 128 × 128, respectively. In addition, we only\nadopt upsampling operations to produce a full-resolution data\nset with 120 LrMS/PAN/GT image pairs for the WorldView-3\nsatellite sensor.\nFor image quality assessment (IQA), five popular met-\nrics are applied for the reduced-resolution test, i.e., PSNR,\nSSIM, Q-index, SAM, and ERGAS, and three common non-\nreference metrics are employed for the full-resolution test,\ni.e., Dλ, DS, and QNR. Besides, the inference time, parame-\nters (Params), and floating-point operations (FLOPs) are uti-\nlized for model efficiency analysis.\n4.2 Implementation Details\nTraining Setting. The end-to-end training of LGTEUN is\nsupervised by mean absolute error (MAE) loss between the\nnetwork output ¯ZK and the GT HrMS image. It trains 130\nepochs for the 8-band data set, and1000 epochs for the two 4-\nband data sets. The Adam optimizer with β1 = 0.9 and β2 =\n0.999 is employed for model optimization, and the batch size\nis set as 4. The initial learning rate is 1.5 × 10−3, and decays\nby 0.85 every 100 epochs. All the experiments are conducted\nin PyTorch framework with a single NVIDIA GeForce GTX\n3090 GPU. For clear comparisons, red color highlights the\nbest results while blue color the second-best in the following\nsuitable table results.\nStructure Setting. As shown in Fig. 2 (a), ¯Z0 is initial-\nized by directly upsampling the LrMS image X with a scal-\ning factor 4. In LGTEUN, the data module D shares parame-\nters across stages, while the prior module maintains indepen-\ndence. The channel number C is set as 4B for all the data\nsets. Additionally, all the downsampling or upsampling oper-\nation is implemented by the bicubic interpolation.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1075\nMethod\nWorldView-3 WorldView-2 GaoFen-2\nPSNR↑ SSIM↑ Q8↑ SAM↓ ERGAS↓ PSNR↑ SSIM↑ Q4↑ SAM↓ ERGAS↓ PSNR↑ SSIM↑ Q4↑ SAM↓ ERGAS↓\nGSA 22.5164 0.6343 0.5742 0.1106 7.8267 33.5975 0.8899 0.5681 0.0573 2.5402 36.0557 0.8838 0.5517 0.0641 3.5758\nSFIM 21.4154 0.5415 0.4525 0.1147 8.8553 32.6334 0.8728 0.5159 0.0597 3.1919 34.7715 0.8572 0.4584 0.0657 4.2073\nWavelet 21.4464 0.5656 0.5271 0.1503 9.1545 32.1992 0.8500 0.4577 0.0638 3.3799 33.9208 0.8197 0.4033 0.0695 4.6445\nPanFormer 30.4772 0.9368 0.9316 0.0672 3.1830 41.3581 0.9731 0.8236 0.0241 1.0617 44.8540 0.9805 0.8865 0.0271 1.3334\nCTINN 31.8564 0.9518 0.9460 0.0660 2.7421 41.2015 0.9735 0.8149 0.0246 1.0880 44.2942 0.9784 0.8716 0.0293 1.4148\nLightNet 32.0018 0.9525 0.9472 0.0639 2.6853 41.5589 0.9739 0.8220 0.0237 1.0382 44.6876 0.9787 0.8741 0.0279 1.3510\nSFIIN 31.6587 0.9492 0.9435 0.0652 2.8016 41.9489 0.9752 0.8108 0.0229 1.0084 44.7248 0.9802 0.8721 0.0280 1.3361\nMutInf 31.8298 0.9523 0.9469 0.0636 2.7526 41.9522 0.9760 0.8258 0.0227 1.0153 44.8305 0.9800 0.8836 0.0277 1.3394\nMDCUN 31.2978 0.9429 0.9363 0.0661 2.9295 42.3351 0.9772 0.8370 0.0216 0.9638 45.5677 0.9825 0.8915 0.0252 1.2249\nLGTEUN 32.2188 0.9545 0.9494 0.0605 2.6286 42.6837 0.9786 0.8415 0.0208 0.9280 45.8364 0.9840 0.8973 0.0247 1.1824\nTable 2: Quantitative comparison of different methods on WorldView-3, WorldView-2, and GaoFen-2 satellite data sets.\nGSA SFIM Wavelet PanFormer CTINN LightNet SFIIN MutInf MDCUN LGTEUN Ground-Truth\nFigure 3: Qualitative comparison of different methods on the WorldView-2 satellite data set.\nBesides, in the local branch of each LGT block, the size\nof each local window M is 8, and the number of heads is\n2. Moreover, in Tab. 1, we explore the impact of the num-\nber of iterative stages K = 1, 2, 3, 4 from performance and\nefficiency viewpoints on 8-band WorldView-3 and 4-band\nWorldView-2 satellite scenes. According to the results in Tab.\n1, it is clear that LGTEUN reaches the optimal performance\nwith 2 stages, and the model efficiency gradually decreases\nas K increases. Hence, the number of stages is chosen as 2\nfor better performance and efficiency balance.\n4.3 Comparison with SOTA Methods\nTo comprehensively evaluate the effectiveness and efficiency\nof the proposed method for the MS pan-sharpening, we com-\npare our LGTEUN with three model-based methods, i.e.,\nGSA [Aiazzi et al., 2007 ], SFIM [Liu, 2000 ], and Wavelet\n[King and Wang, 2001 ], and six SOTA DL-based methods,\ni.e., PamFormer [Zhou et al., 2022a ], CTINN [Zhou et al.,\n2022a], LightNet [Chen et al., 2022 ], SFIIN [Zhou et al.,\n2022c], MutInf [Zhou et al., 2022d ], and MDCUN [Yang\net al., 2022 ]. Besides, all the compared methods are imple-\nmented in light of the corresponding paper and source code. It\nis noteworthy that all the six DL-based methods are the most\nrecent algorithms for the MS pan-sharpening.\nQuantitative Comparison. Tab. 2 reports the compari-\nson results of all the discussed ten methods on all the three\nsatellite data sets. Specifically, on all the three data sets,\nthe three model-based methods show limited model perfor-\nmances and generalization abilities, and the DL-based meth-\nods obtain more competitive results. More importantly,\namong all the considered methods on all the three data sets,\nour proposed LGTEUN always achieves the best results in\nall the five IQA metrics with distinct performance improve-\nments. For instance, our LGTEUN outperforms the second-\nMethod\nFull-resolution Test\nDλ↓ DS↓ QNR↑\nGSA 0.0094 0.1076 0.8839\nSFIM 0.0094 0.1061 0.8854\nWavelet 0.0552 0.1330 0.8193\nPanFormer 0.0191 0.0416 0.9400\nCTINN 0.0123 0.0442 0.9440\nLightNet 0.0185 0.0282 0.9539\nSFIIN 0.0198 0.0352 0.9457\nMutInf 0.0163 0.0420 0.9423\nMDCUN 0.0747 0.1673 0.7708\nLGTEUN 0.0162 0.0310 0.9532\nTable 3: Full-resolution test of different methods on the WorldView-\n3 satellite data set.\nbest method by 0.2170 dB, 0.3486 dB, and 0.2687 dB in\nPSNR on WorldView-3, WorldView-2, and GaoFen-2 data\nsets, respectively, which indicates the superiority of our pro-\nposed method.\nQualitative Comparison. Fig. 3 illustrates the qualitative\nresults of a typical sample from the WorldView-2 data set, in-\ncluding the paired output pan-sharpening image and the cor-\nresponding MAE residual image of each discussed method.\nIn Fig. 3, compared with the other nine methods, the pro-\nposed LGTEUN exhibits a more visually pleasing result with\nminor spectral and spatial distortions. In particular, the resid-\nual image of our method has fewer artifacts than any other\nmethod, especially in the zoom-in region. Here, we can rea-\nsonably infer that the advanced performance of the LGTEUN\nbenefits from the designed PGD algorithm based stage itera-\ntions and the excellent capability of simultaneously capturing\nlocal and global dependencies.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1076\nData Set Metric GSA SFIM Wavelet PanFormer CTINN LightNet SFIIN MutInf MDCUN LGTEUN\nWorldView-3\nTime (s/img) 0.0482 0.0591 0.0562 0.0160 0.0426 0.0019 0.0529 0.1083 0.1747 0.0133\nParams (KB) – – – 1532.8 38.3 16.3 85.8 185.8 140.9 540.0\nFLOPs (GB) – – – 11.92 2.68 2.02 5.25 9.87 479.54 19.04\nGaoFen-2\nTime (s/img) 0.0216 0.0301 0.0271 0.0257 0.0431 0.0017 0.0528 0.1141 0.1017 0.0129\nParams (KB) – – – 1530.3 37.8 15.8 85.3 185.5 98.3 202.2\nFLOPs (GB) – – – 11.77 2.65 1.95 5.22 9.85 473.19 5.14\nTable 4: Efficiency comparison of different methods on WorldView-3 and GaoFen-2 satellite data sets.\nSetting Reduced-resolution Test Full-resolution Test\nLocal Branch Global Branch PSNR↑ SSIM↑ Q8↑ SAM↓ ERGAS↓ Dλ↓ DS↓ QNR↑\n✗ ✓ 31.9309 0.9519 0.9468 0.0636 2.7102 0.0177 0.0364 0.9465\n✓ ✗ 31.9742 0.9525 0.9468 0.0618 2.7029 0.0170 0.0349 0.9486\n✓ ✓ 32.2188 0.9545 0.9494 0.0605 2.6286 0.0162 0.0310 0.9532\nTable 5: Ablation study on the WorldView-3 satellite data set.\nFull-resolution Test. To further measure the model per-\nformance in the full-resolution scene, we conduct a full-\nresolution test on the full-resolution WorldView-3 data set.\nAs reported in Tab. 3, the proposed method also obtains com-\npetitive results, i.e., second-best results in DS and QNR and\nthe third-best result in Dλ. On the contrary, MDCUN [Yang\net al., 2022] exhibits slightly limited results.\nEfficiency Comparison. As for efficiency comparison,\nTab. 4 presents exhaustive investigations about inference\nefficiency (the inference time), model complexity (Params),\nand computational cost (FLOPs) of all the ten methods on\nWorldView-3 and GaoFen-2 data sets, and Fig. 1 illustrates\nthe unified PSNR-Params-FLOPs comparisons of all the DL-\nbased methods on the WorldView-2 scenario. Specifically,\nfrom Tab. 4, the proposed LGTEUN has excellent inference\nefficiency and promising computational cost. Besides, further\nconsidering the outstanding model performance in Fig. 1,\nour LGTEUN achieves an impressive performance-efficiency\nbalance.\n4.4 Analysis and Discussion\nAblation Study. In this subsection, we perform an ablation\nstudy towards our elaborated LGT in the prior module P.\nSpecifically, on the WorldView-3 data set, two break-down\nablation tests are conducted to explore and validate the corre-\nsponding contributions of its key local and global branches.\nLocal Branch: For one thing, the local branch mod-\nels local dependencies by computing local window based\nself-attention in spatial domain. As reported in Tab. 5,\nthe local branch brings obvious performance gains for both\nreduced-resolution and full-resolution tests. For example, our\nLGTEUN improves 0.2879 dB in PSNR and 0.0816 in ER-\nGAS for the reduced-resolution test, and 0.0067 in QNR for\nthe full-resolution test, respectively.\nGlobal Branch: For another thing, the global branch cap-\ntures global dependencies by mining global contextual fea-\nture representation in frequency domain. From Tab. 5, the\nimportance of modeling global dependencies is self-evident\nsince there are distinct performance degradations on all the\nIQA metrics without the global branch, e.g., 0.2446 dB in\nStage 0 Stage 1 Stage 2 Ground-Truth    \nFigure 4: Stage-wise visualization on the GaoFen-2 satellite scene.\nPSNR and 0.0026 in Q8 for the reduced-resolution test, and\n0.0039 in DS for the full-resolution test, respectively.\nStage-wise Visualization. As illustrated in Fig. 4, for our\nLGTEUN, we visualize the intermediate results of differ-\nent stages ( ¯Z0, ¯Z1, and ¯Z2) from a representative sample\nin the GaoFen-2 satellite data set, including the paired pan-\nsharpening and residual images. It is clear that more detailed\ninformation is recovered with LGTEUN iterating.\nLimitations. In short, two-fold potential limitations of our\nLGTEUN are as follows: 1) The pan-sharpening results on\nthe full-resolution scene have room for performance boosting.\n2) Further enhancements on model efficiency would make our\nproposed LGTEUN more competitive.\n5 Conclusion\nIn this paper, for the MS pan-sharpening, we develop our\nLGTEUN by unfolding the designed PGD optimization algo-\nrithm into a deep network to improve the model interpretabil-\nity. In our LGTEUN, to complement the lightweight data\nmodule, we customize a LGT module as a powerful prior\nmodule for image denoising to simultaneously capture lo-\ncal and global dependencies. To the best of our knowledge,\nLGTEUN is the first transformer-based DUN for the MS pan-\nsharpening, and LGT is also the first transformer module to\nperform spatial and frequency dual-domain learning. Com-\nprehensive experimental results on three satellite data sets\ndemonstrate the effectiveness and efficiency of our LGTEUN\ncompared with other SOTA methods.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1077\nAcknowledgments\nThis work was supported in part by the National Natural Sci-\nence Foundation of China under Grant U1903127 and in part\nby the Taishan Industrial Experts Programme under Grant\ntscy20200303.\nReferences\n[Aiazzi et al., 2007] Bruno Aiazzi, Stefano Baronti, and\nMassimo Selva. Improving component substitution pan-\nsharpening through multivariate regression of ms + pan\ndata. IEEE Transactions on Geoscience and Remote Sens-\ning, 45(10):3230–3239, 2007.\n[Ballester et al., 2006] Coloma Ballester, Vicent Caselles,\nLaura Igual, Joan Verdera, and Bernard Roug ´e. A varia-\ntional model for p+ xs image fusion.International Journal\nof Computer Vision, 69(1):43–58, 2006.\n[Beck and Teboulle, 2009] Amir Beck and Marc Teboulle.\nA fast iterative shrinkage-thresholding algorithm for lin-\near inverse problems. SIAM journal on imaging sciences,\n2(1):183–202, 2009.\n[Cai et al., 2022] Yuanhao Cai, Jing Lin, Haoqian Wang, Xin\nYuan, Henghui Ding, Yulun Zhang, Radu Timofte, and\nLuc Van Gool. Degradation-aware unfolding half-shuffle\ntransformer for spectral compressive imaging. Advances\nin Neural Information Processing Systems, 2022.\n[Chan et al., 2016] Stanley H Chan, Xiran Wang, and\nOmar A Elgendy. Plug-and-play admm for image\nrestoration: Fixed-point convergence and applications.\nIEEE Transactions on Computational Imaging, 3(1):84–\n98, 2016.\n[Chen et al., 2022] Zhi-Xuan Chen, Cheng Jin, Tian-Jing\nZhang, Xiao Wu, and Liang-Jian Deng. Spanconv: A\nnew convolution via spanning kernel space for lightweight\npansharpening. In Proc. 31st Int. Joint Conf. Artif. Intell.,\npages 1–7, 2022.\n[Chi et al., 2020] Lu Chi, Borui Jiang, and Yadong Mu. Fast\nfourier convolution. Advances in Neural Information Pro-\ncessing Systems, 33:4479–4488, 2020.\n[Chollet, 2017] Franc ¸ois Chollet. Xception: Deep learning\nwith depthwise separable convolutions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 1251–1258, 2017.\n[Dong et al., 2021] Weisheng Dong, Chen Zhou, Fangfang\nWu, Jinjian Wu, Guangming Shi, and Xin Li. Model-\nguided deep hyperspectral image super-resolution. IEEE\nTransactions on Image Processing, 30:5754–5768, 2021.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In International Conference on Learning Repre-\nsentations, 2020.\n[Fauvel et al., 2012] Mathieu Fauvel, Yuliya Tarabalka,\nJon Atli Benediktsson, Jocelyn Chanussot, and James C\nTilton. Advances in spectral-spatial classification of hy-\nperspectral images. Proceedings of the IEEE, 101(3):652–\n675, 2012.\n[Frigo and Johnson, 1998] Matteo Frigo and Steven G John-\nson. Fftw: An adaptive software architecture for the fft. In\nProceedings of IEEE International Conference on Acous-\ntics, Speech and Signal Processing, volume 3, pages 1381–\n1384, 1998.\n[Fu et al., 2019] Xueyang Fu, Zihuang Lin, Yue Huang, and\nXinghao Ding. A variational pan-sharpening with local\ngradient constraints. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10265–10274, 2019.\n[Hardie et al., 2004] Russell C Hardie, Michael T Eismann,\nand Gregory L Wilson. Map estimation for hyperspec-\ntral image resolution enhancement using an auxiliary sen-\nsor. IEEE Transactions on Image Processing, 13(9):1174–\n1184, 2004.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., pages 770–778, 2016.\n[King and Wang, 2001] Roger L King and Jianwen Wang. A\nwavelet based algorithm for pan sharpening landsat 7 im-\nagery. In Proceedings of IEEE International Geoscience\nand Remote Sensing Symposium, volume 2, pages 849–\n851, 2001.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\nImage restoration using swin transformer. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision Workshops, pages 1833–1844, 2021.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012–\n10022, 2021.\n[Liu, 2000] JG Liu. Smoothing filter-based intensity mod-\nulation: A spectral preserve image fusion technique for\nimproving spatial details. International Journal of Remote\nSensing, 21(18):3461–3472, 2000.\n[Mou et al., 2022] Chong Mou, Qian Wang, and Jian Zhang.\nDeep generalized unfolding networks for image restora-\ntion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 17399–\n17410, 2022.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in Neural Information Processing Sys-\ntems, 30, 2017.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1078\n[Wald et al., 1997] Lucien Wald, Thierry Ranchin, and Marc\nMangolini. Fusion of satellite images of different spa-\ntial resolutions: Assessing the quality of resulting im-\nages. Photogrammetric Engineering and Remote Sensing,\n63(6):691–699, 1997.\n[Wang et al., 2018] Xiaolong Wang, Ross Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7794–7803, 2018.\n[Xie et al., 2019] Qi Xie, Minghao Zhou, Qian Zhao, Deyu\nMeng, Wangmeng Zuo, and Zongben Xu. Multispectral\nand hyperspectral image fusion by ms/hs fusion net. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1585–1594, 2019.\n[Xu et al., 2020] Han Xu, Jiayi Ma, Zhenfeng Shao, Hao\nZhang, Junjun Jiang, and Xiaojie Guo. Sdpnet: A deep\nnetwork for pan-sharpening with enhanced information\nrepresentation. IEEE Transactions on Geoscience and Re-\nmote Sensing, 59(5):4120–4134, 2020.\n[Xu et al., 2021] Shuang Xu, Jiangshe Zhang, Zixiang Zhao,\nKai Sun, Junmin Liu, and Chunxia Zhang. Deep gradient\nprojection networks for pan-sharpening. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 1366–1375, 2021.\n[Yang et al., 2017] Junfeng Yang, Xueyang Fu, Yuwen Hu,\nYue Huang, Xinghao Ding, and John Paisley. Pannet: A\ndeep network architecture for pan-sharpening. InProceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5449–5457, 2017.\n[Yang et al., 2022] Gang Yang, Man Zhou, Keyu Yan, Aip-\ning Liu, Xueyang Fu, and Fan Wang. Memory-augmented\ndeep conditional unfolding network for pan-sharpening. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1788–1797, 2022.\n[Zhang and Ghanem, 2018] Jian Zhang and Bernard\nGhanem. Ista-net: Interpretable optimization-inspired\ndeep network for image compressive sensing. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 1828–1837, 2018.\n[Zhang et al., 2020] Kai Zhang, Luc Van Gool, and Radu\nTimofte. Deep unfolding network for image super-\nresolution. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 3217–\n3226, 2020.\n[Zhao et al., 2022] Xudong Zhao, Mengmeng Zhang, Ran\nTao, Wei Li, Wenzhi Liao, Lianfang Tian, and Wilfried\nPhilips. Fractional fourier image transformer for multi-\nmodal remote sensing data classification. IEEE Transac-\ntions on Neural Networks and Learning Systems, 2022.\n[Zhou et al., 2022a] Huanyu Zhou, Qingjie Liu, and Yun-\nhong Wang. Panformer: a transformer based model for\npan-sharpening. IEEE international conference on multi-\nmedia and expo, 2022.\n[Zhou et al., 2022b] Man Zhou, Jie Huang, Yanchi Fang,\nXueyang Fu, and Aiping Liu. Pan-sharpening with cus-\ntomized transformer and invertible neural network. Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\n2022.\n[Zhou et al., 2022c] Man Zhou, Jie Huang, Keyu Yan,\nHu Yu, Xueyang Fu, Aiping Liu, Xian Wei, and Feng\nZhao. Spatial-frequency domain information integration\nfor pan-sharpening. In European Conference on Computer\nVision, pages 274–291, 2022.\n[Zhou et al., 2022d] Man Zhou, Keyu Yan, Jie Huang, Zihe\nYang, Xueyang Fu, and Feng Zhao. Mutual information-\ndriven pan-sharpening. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1798–1808, 2022.\n[Zhou et al., 2022e] Man Zhou, Hu Yu, Jie Huang, Feng\nZhao, Jinwei Gu, Chen Change Loy, Deyu Meng, and\nChongyi Li. Deep fourier up-sampling. Advances in Neu-\nral Information Processing Systems, 2022.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n1079",
  "topic": "Sharpening",
  "concepts": [
    {
      "name": "Sharpening",
      "score": 0.772660493850708
    },
    {
      "name": "Computer science",
      "score": 0.6654642820358276
    },
    {
      "name": "Interpretability",
      "score": 0.5694441795349121
    },
    {
      "name": "Panchromatic film",
      "score": 0.5482077598571777
    },
    {
      "name": "Multispectral image",
      "score": 0.5474832057952881
    },
    {
      "name": "Iterative method",
      "score": 0.47480833530426025
    },
    {
      "name": "Algorithm",
      "score": 0.4359205365180969
    },
    {
      "name": "Local optimum",
      "score": 0.42775827646255493
    },
    {
      "name": "Image resolution",
      "score": 0.4231933355331421
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40717294812202454
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3273102045059204
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154099455",
      "name": "Shandong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136682",
      "name": "Heze University",
      "country": "CN"
    }
  ]
}