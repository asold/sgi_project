{
  "title": "Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",
  "url": "https://openalex.org/W3173617765",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320549284",
      "name": "Robert Logan IV",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514447179",
      "name": "Ivana Balazevic",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102119409",
      "name": "Eric Wallace",
      "affiliations": [
        "University of California, Berkeley",
        "Berkeley College"
      ]
    },
    {
      "id": "https://openalex.org/A2040829290",
      "name": "Fabio Petroni",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2143533944",
      "name": "Sameer Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976791985",
      "name": "Sebastian Riedel",
      "affiliations": [
        "Meta (Israel)",
        "University College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2989911337",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3135934234",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Prompting language models (LMs) with training examples and task descriptions has been seen as critical to recent successes in few-shot learning. In this work, we show that finetuning LMs in the few-shot setting can considerably reduce the need for prompt engineering. In fact, one can use null prompts, prompts that contain neither task-specific templates nor training examples, and achieve competitive accuracy to manually-tuned prompts across a wide range of tasks. While finetuning LMs does introduce new parameters for each downstream task, we show that this memory overhead can be substantially reduced: finetuning only the bias terms can achieve comparable or better accuracy than standard finetuning while only updating 0.1% of the parameters. All in all, we recommend finetuning LMs for few-shot learning as it is more accurate, robust to different prompts, and can be made nearly as efficient as using frozen LMs.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2824 - 2835\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nCutting Down on Prompts and Parameters:\nSimple Few-Shot Learning with Language Models\nRobert L. Logan IV1 Ivana Balaževi´c∗2 Eric Wallace3\nFabio Petroni4 Sameer Singh1 Sebastian Riedel4,5\n1UC Irvine 2DeepMind 3UC Berkeley\n4Facebook AI Research 5University College London\n{rlogan,sameer}@uci.edu balazevic@deepmind.com\nericwallace@berkeley.edu {fabiopetroni,sriedel}@fb.com\nAbstract\nPrompting language models (LMs) with train-\ning examples and task descriptions has been\nseen as critical to recent successes in few-shot\nlearning. In this work, we show that ﬁnetun-\ning LMs in the few-shot setting can consid-\nerably reduce the need for prompt engineer-\ning. In fact, one can use null prompts, prompts\nthat contain neither task-speciﬁc templates nor\ntraining examples, and achieve competitive ac-\ncuracy to manually-tuned prompts across a\nwide range of tasks. While ﬁnetuning LMs\ndoes introduce new parameters for each down-\nstream task, we show that this memory over-\nhead can be substantially reduced—ﬁnetuning\nonly the bias terms can achieve comparable\nor better accuracy than standard ﬁnetuning\nwhile only updating 0.1% of the parameters.\nAll in all, we recommend ﬁnetuning LMs for\nfew-shot learning as it is more accurate, has\nrelatively stable performance across different\nprompts, and can be made nearly as efﬁcient\nas using frozen LMs.\n1 Introduction\nFew-shot learning—the ability to learn tasks with\nlimited examples—is an important academic and\npractical challenge (Lake et al., 2015). In state-\nof-the-art NLP, few-shot learning is performed by\nreformulating tasks as natural language “prompts”\nand completing those prompts with pre-trained lan-\nguage models (Brown et al., 2020; Schick and\nSchütze, 2021a). Prompts that are well-designed\ncan substantially improve accuracy (Zhao et al.,\n2021; Lu et al., 2021). However, ﬁnding these\nprompts is difﬁcult: it requires a non-trivial combi-\nnatorial search over the prompt’s wording (a.k.a. its\npattern or template), whether and how to include\ntraining examples, and how to convert language\nmodel probabilities into class predictions. Conse-\nquently, prompts are often designed using human\n∗Work done while an intern at Facebook AI Research.\nintuition that is hard to replicate and apply in a\nprincipled manner (Perez et al., 2021).\nIn this work, we seek to mitigate prompt engi-\nneering by identifying a class of simple prompts\nthat are effective across many tasks for masked\nlanguage models (LMs). We ﬁnd that, when us-\ning prompt-based ﬁnetuning (Schick and Schütze,\n2021a; Gao et al., 2021), the prompt requires\nless optimization than previously thought; in fact,\nthe pattern and training examples can be com-\npletely cut out (e.g., Figure 1, right). These null\nprompts—simple concatenations of the inputs and\nthe [MASK] token—achieve comparable accuracy\nto manually-written patterns while drastically sim-\nplifying prompt design: users only need to decide\nthe label names (a.k.a. the verbalizer) and where to\nplace the [MASK] token. The effectiveness of null\nprompts also challenges the common wisdom that\nthe success of few-shot learning is due to inductive\nbiases present in the prompt.\nA key drawback of prompt-based ﬁnetuning is\nthat it has large memory requirements for each new\ndownstream task at inference time (Figure 1, left).\nIn contrast, in-context learning (Brown et al., 2020)\nallows reusing the large-scale LM across tasks, but\nit requires signiﬁcant prompt engineering. To de-\ntermine whether memory efﬁciency and simple\nprompt selection can be simultaneously achieved,\nwe experiment with either: (1) making prompts\nfor in-context learning similarly easy to create, or\n(2) making prompt-based ﬁnetuning more memory\nefﬁcient. For (1), we simplify prompt engineer-\ning for in-context learning by automatically tuning\nthe prompt’s tokens or embeddings, an approach\nthat has been successful in the non-few-shot set-\nting (Shin et al., 2020; Lester et al., 2021). For (2),\nwe study lightweight ﬁnetuning alternatives that up-\ndate a smaller set of parameters: BitFit (Ben-Zaken\net al., 2021), Adapters (Houlsby et al., 2019), and\ncalibration layers (Zhao et al., 2021).\n2824\n100%\n10%\n1%\n0.1% 0.0\n100.0\n0.1\nFinetuned Params\n0\n50\n100 51.2\n90.6 90.6\nCB (F1)\nIn-Context Prompt-Based\nFinetuning\nOurs\n0\n25\n50\n75 55.6 62.9 65.1\nQQP (F1)\nIn-Context\n{What does it feel like to be on Xanax?}1 and {Do 4mg Xanax bars exist?}2 have different\nmeanings. {How do you know if you’re unconditionally in love with someone?} 1 and\n{How do you know if you’re in love with someone and might only be denying the fact to\nyourself?}2 have similar meanings. {Will GST affect the price level in India?}1 and {Will\nGST effect the price level in India?}2 have [MASK] meanings.\nPrompt-Based\nFinetuning\n{Will GST affect the price level in India?}1 ? [MASK] , I want to know {Will\nGST effect the price level in India?}2\nNull Prompts\n(Ours)\n{Will GST affect the price level in India?}1 {Will GST effect\nthe price level in India?}2 [MASK]\nFigure 1: Different Methods of Few-Shot Learning. Right: We visualize different types of prompts for QQP. We\ndenote the input ﬁelds using curly brackets {}, the manually-written pattern using magenta, and the verbalizers\nusing green. We show that null prompts, ones that do not contain training examples or task-speciﬁc patterns, can\nachieve competitive accuracy. Left: We compare different methods for model ﬁnetuning. Unlike standard prompt-\nbased ﬁnetuning, we propose to update only the masked LM’s bias terms (BitFit). This achieves competitive\naccuracy while only updating 0.1% of the parameters.\nWe show that the latter approach—prompt-based\nﬁnetuning with lightweight updates—is consider-\nably more successful. In particular, learning only\nthe model’s bias terms (BitFit) can achieve com-\npetitive or better few-shot accuracy than standard\nﬁnetuning while only requiring switching out 0.1%\nof the parameters at inference time to perform dif-\nferent tasks. On the other hand, automated prompt\ntuning for in-context learning generally fails to ﬁnd\nprompts that are competitive with manual ones.\nTaken together, our results show that prompt-based\nﬁnetuning is preferable because it is more accurate,\nworks well for different types of prompts, and can\nbe made nearly as efﬁcient as using frozen LMs.\n2 Prompting Language Models\nWe use masked LMs for few-shot learning. Follow-\ning Schick and Schütze (2021a), we have:\n• a pre-trained masked LM, with T denoting its\nvocabulary and T∗ the set of all token sequences.\n• a small set of training inputs xi ∈X and their\ncorresponding labels yi ∈Y .\n• a pattern P : X →T∗ that maps inputs to cloze\nquestions containing a single [MASK] token. Ad-\nditionally, a verbalizer v : Y →T that maps\neach label to a single vocabulary token. We call\nthe pattern and verbalizer together the prompt.\nIn our work, we consider different ways of con-\nstructing the prompt (Section 2.1) and updating the\nmasked LM’s parameters (Section 2.2). Table 1\ncontains an overview of existing prompting meth-\nods and the settings they are evaluated in.\n2.1 Constructing the Prompt\nThe prompt is important: in some settings, differ-\nent prompts can cause accuracy to vary from near\nchance to near state-of-the-art (Zhao et al., 2021).\nHowever, ﬁnding good prompts can be difﬁcult.\nPrompt construction requires a non-trivial combi-\nnatorial search over the prompt’s wording, whether\nto include training examples, and how to convert\nLM probabilities to class predictions. As a conse-\nquence, prompts are either designed using human\nintuition that is hard to replicate and apply in a\nprincipled manner (Perez et al., 2021), or using\nautomated methods (Shin et al., 2020; Gao et al.,\n2021; Lu et al., 2021). These methods search for\nelements such as: (1) the text of the pattern, (2)\nthe tokens in the verbalizers, and (3) whether and\nhow training examples are prepended before the\ntest input. Although automated prompt search can\nmatch the accuracy of manual tuning, it introduces\nits own complexities. For example, the prompts\nfrom Gao et al. (2021) achieve comparable results\nto manually-designed prompts but are found using\ngenerative models and careful validation.\nIn this paper, we show that prompt-based ﬁnetun-\ning (see Section 2.2) can considerably reduce the\nimportance of the prompt. This does not contradict\npast work—the extreme importance of the prompt\nis only true when models are not ﬁnetuned.\n2.2 Prompting Approaches for Few-Shot\nLearning\nIn-Context Learning An increasingly popular\nstrategy for few-shot learning is prompting frozen\n2825\nMethod Finetuned Params Prompt Design Few-shot\nAUTO PROMPT (Shin et al., 2020) None Learned (Discrete) \u0017\nPrompt Tuning (Lester et al., 2021) Prompt Token Embeds Learned (Continuous) \u0017\nOPTI PROMPT (Zhong et al., 2021) Prompt Token Embeds Learned (Continuous) \u0017\nSoft Prompts (Qin and Eisner, 2021) All Contextualized Embeds Learned (Continuous) \u0017\nGPT-3 (Brown et al., 2020) None Manual \u0013\nPET (Schick and Schütze, 2021a) All Manual \u0013\nLM-BFF (Gao et al., 2021) All Learned (Discrete) \u0013\nP-Tuning (Liu et al., 2021) All + Prompt Token Embeds Learned (Continuous) \u0013\nNull Prompts + Bitﬁt (Ours) Bias Terms None \u0013\nTable 1: Overview of Existing Work on Prompting. Finetuned Params indicates the parameters altered during\ntraining. Prompt Design indicates how prompts are created; we use null prompts. Few-Shot indicates using few-\nshot training and validation sets.\nLMs (Brown et al., 2020). This strategy relies\nsolely on in-context learning (a.k.a. priming),\nwhere the LM learns by conditioning on the prompt\nrather than updating its parameters. In-context\nlearning has been shown to be successful when\nusing very large (e.g., billions of parameters) LMs,\nas these models better leverage the prompt.\nPrompt-Based Finetuning Rather than using\nfrozen LMs, prompt-based ﬁnetuning methods\nﬁnetune all of the LM’s parameters (Schick and\nSchütze, 2021a; Le Scao and Rush, 2021; Gao\net al., 2021). For masked LMs, this is done by con-\nstructing training examples that contain a [MASK]\ntoken and ﬁnetuning the masked LM to generate\nthe correct verbalizer token in that position.\nThe main advantage of prompt-based ﬁnetuning\nover in-context learning is that it achieves higher ac-\ncuracy, especially when the LM is relatively small,\ne.g., millions of parameters (Schick and Schütze,\n2021b). The main downside is that the same model\ncan no longer be reused across different tasks, thus\nreducing efﬁciency. The efﬁciency is impacted in\ntwo ways. First, it requires large amounts of disk\nspace at test time because numerous model check-\npoints must be stored. Second, during training\ntime, it requires large amounts of GPU memory to\nperform updates on massive LMs.\nIn this paper, we will show an additional beneﬁt\nto prompt-based ﬁnetuning—it makes prompt engi-\nneering easier. We will also show that the memory\ninefﬁciency of prompt-based ﬁnetuning can be dras-\ntically mitigated using lightweight ﬁnetuning alter-\nnatives. These lightweight methods allow one to\nswitch out only a small subset of model parameters\nat inference time in order to solve multiple tasks,\nand also drastically reduce training-time memory\ncosts. Moreover, in many cases these lightweight\nmethods also improve model accuracy. Our work is\nrelated to Le Scao and Rush (2021), who show that\ndifferent manually-written patterns lead to similar\naccuracy for prompt-based ﬁnetuning.\n3 Experimental Setup\n3.1 Datasets and Hyperparameter Tuning\nWe use the following classiﬁcation datasets\nfrom GLUE (Wang et al., 2019b) and Super-\nGLUE (Wang et al., 2019a): BoolQ, CB, MNLI,\nMRPC, QNLI, QQP, RTE, and SST-2.1\nTo build few-shot datasets, past work collects\nK examples from each label for training and K\nexamples from each label for development (Gao\net al., 2021). Despite this setup often being denoted\nas K-shot learning, it effectively uses 2K exam-\nples and splits the examples evenly into train and\ndevelopment. We instead propose to use cross vali-\ndation to perform more principled model selection.\nConcretely, we sample 2K examples from each\nlabel and use 4-fold cross validation to determine\nthe best hyperparameters. After ﬁnding the best\nhyperparameters, we train on the ﬁrst K examples\nand early stop on the second K examples. We use\nK = 16following past work (Gao et al., 2021).\nWe sample our examples from each dataset’s\noriginal training set. Since transformers’ perfor-\nmance in few-shot settings can be highly dependent\non weight initialization (Dodge et al., 2020), we ini-\ntialize the weights with 10 different random seeds\nand report the mean and variance of the model\nperformance. We use each dataset’s original de-\nvelopment set for our ﬁnal evaluation and use the\nstandard evaluation metrics (accuracy or F1) as-\nsociated with each dataset. We do not check the\n1We also evaluated on WiC and WNLI. We omit these\nresults because all models achieved near-random accuracy.\n2826\nFigure 2: How # Wins are Computed . For a given\ndataset, we perform a Welch’s t-test to determine if\nthere is a signiﬁcant difference in accuracy for each pair\nof methods. The method which performs better than\nmost other methods (i.e., the row with the most yellow\nsquares; BitFit in this case) is considered the “winner”\nof the task, and its # Wins is incremented by 1. In the\nﬁgure above, we show a subset of methods evaluated\non a single dataset.\nﬁnal evaluation metrics during any tuning of the\nhyperparameters to ensure that we are doing “true”\nfew-shot learning (Perez et al., 2021).\n3.2 Masked Language Models\nFollowing past work (Schick and Schütze, 2021b),\nwe use the RoBERTa (large, 330M params, Liu\net al., 2019) and ALBERT (xxl-v2, 223M params,\nLan et al., 2020) masked LMs provided by the Hug-\ngingFace transformers library (Wolf et al., 2020).\nTraining and evaluation were performed on a het-\nerogeneous compute cluster with the following min-\nimum specs: 2xNVIDIA GeForce GTX 1080 Ti’s,\n8-core Intel Core i7 CPU, 64 GB RAM.\n3.3 Comparing Few-Shot Methods by # Wins\nThe results for different few-shot learning meth-\nods can be quite different across datasets and seeds\nfor the training set (Zhao et al., 2021; Schick and\nSchütze, 2021a). To compare different methods at\na high level, we use a metric denoted as # Wins:\nthe number of datasets that a given method per-\nforms signiﬁcantly better than all other methods\non. We compute this metric for a given dataset\nby ﬁrst performing a Welch’s t-test to determine\nif there is a signiﬁcant difference in accuracy for\neach pair of methods. The method which performs\nbetter than most other methods is considered the\n“winner” of the task and its# Wins is incremented\nby 1. There are multiple winners in the case of a\ntie. See Figure 2 for a demonstration.\n4 Simplifying Prompt Engineering\nIn this section, we run prompt-based ﬁnetuning\nand ablate different elements of the prompt. We\nconsider the following ablations:\n• Manual Prompt (Prior) : We use manually-\nwritten prompts from Schick and Schütze\n(2021a,b), and Gao et al. (2021). We show the\npatterns and verbalizers in Appendix A1.\n• Manual Prompt (w/o Engineering): We simu-\nlate standard prompt design by manually writing\none prompt for each task using our intuition. We\nshow the prompts in Appendix A2.\n• Prompt Tuning: Inspired by Liu et al. (2021)\nand Lester et al. (2021), we use the pattern from\nManual Prompt (Prior) but randomly initialize\nthe embeddings of the pattern tokens and learn\nthem using gradient-based optimization. This\nablates the gains from human-designed patterns.\n• Null Prompt: We use the same verbalizer as\nManual Prompt (Prior) but use a pattern that con-\nsists of only the input ﬁelds and a [MASK] token\n(Appendix A3). This ablates the pattern entirely.\n• Null Verbalizer: We use the same pattern as\nManual Prompt (Prior) but—following Opitz\n(2019) and Le Scao and Rush (2021)—select\nrandom tokens for the verbalizer. This ablates\nthe gains from a human-designed verbalizer.\n• Null Prompt + Verbalizer : We use both null\nprompts and random tokens for the verbalizer.\nIn all cases, we ﬁnetune all of the masked LM\nparameters. We show the accuracy of the above\nprompts as well as traditional ﬁnetuning (using a\n[CLS] token and a classiﬁcation head) in Figure 3.2\nManual Prompts Perform Best The manually-\nwritten prompts from prior work perform best on\naverage for both models. On the other hand, our\nmanual prompts (w/o Engineering) are noticeably\nworse than the ones from prior work and are out-\nperformed by many other methods.\nNull Prompts Are Competitive In many cases,\nprompt tuning and null prompts perform compa-\nrably to manually-written prompts, especially for\nRoBERTa. For instance, both of these methods\noutperform manual prompts (w/o Engineering) in\n2For fair comparison we use the ﬁnetuning recommenda-\ntions of Mosbach et al. (2021) to improve stability.\n2827\nBoolQ\nCB\nMNLI-m MNLI-mm\nMRPC QNLI QQP RTE SST-2\n0\n25\n50\n75\n100\nRoBERTa (Large)\n# Wins\n0\n5\nBoolQ\nCB\nMNLI-m MNLI-mm\nMRPC QNLI QQP RTE SST-2\n0\n25\n50\n75\n100\nALBERT (XXLarge-V2)\n# Wins\n0\n2\n4\nManual Prompt (Prior)\nManual Prompt (w/o Engineering)\nPrompt Tuning\nNull Prompt\nNull Verbalizer\nNull Prompt + Verbalizer\n[CLS] Finetuning\nFigure 3: Simplifying the Selection of Prompts . We apply prompt-based ﬁnetuning in conjunction with six\ndifferent types of prompts. We report accuracy orF1 on each dataset. Manually-designed prompts from prior work\nachieve the best accuracy but require manual tuning on validation sets. On the other hand, null prompts and prompt\ntuning both perform competitively without requiring any tuning of the pattern.\n60 65 70 75 80\nDev\n50\n60\nTest\nR2 = 79.05\nFigure 4: Correlation of Dev and Test Performance\nof Null Prompts on MNLI. The only decision to make\nwhen using null prompts is which order to concatenate\nthe mask token and the input ﬁelds. One can choose the\nbest option using a tiny held-out development set. We\nshow the results for MNLI, with the few-shot develop-\nment set accuracy on the x-axis.\nterms of # Wins. These results are exciting from\na practical perspective as they show that one can\nachieve competitive few-shot results without resort-\ning to any tuning of the prompt.\nFrom an analysis perspective, these results also\nshow that effective few-shot learning can be accom-\nplished without any inductive bias from a manually-\nwritten pattern. In fact, combining null prompts\nwith null verbalizers, which involves no human\ndesign at all, still signiﬁcantly outperforms stan-\ndard [CLS] ﬁnetuning for numerous tasks (3 for\nRoBERTa and 5 for ALBERT at p = 0.05). This\nshows that some of the effectiveness of prompt-\nbased ﬁnetuning is due to its basic setup, i.e., pre-\ndicting on a [MASK] token with an MLM head.\nNull Prompts or Prompt Tuning? Both null\nprompts and prompt tuning achieve competitive\nresults without resorting to manual prompt design.\nWe advocate for using null prompts over prompt\ntuning because they are easier to use. Null prompts\nonly require choosing which order to concatenate\nthe input ﬁelds and the [MASK] token. Prompt tun-\ning requires choosing the number of embeddings,\ntheir placement, their initialization, etc.\nNull Prompts Simplify Prompt Search One\ncomplication that arises in standard prompt-based\nﬁnetuning is that prompts become a hyperparame-\nter of the ﬁnetuning procedure, and have a combi-\nnatorially large search space. On the other hand, de-\ntermining the concatenation order for null prompts\nis trivial by just trying all of the few possible op-\ntions and choosing which one works best on the\nvalidation set. To see this, in Figure 4 we plot the\naccuracy on the few-shot development set and the\nfull test set for different concatenation orders for\nRoBERTa on MNLI.3 The development and test ac-\ncuracy is strongly correlated (R2 = 79.05), which\ndemonstrates that tuning the concatenation order is\neasy even when validation data is scarce.\nImpact of Dataset Size We next investigate\nwhether the observations made in the previous para-\n3We use MNLI because the concatenation order has a large\nimpact on performance.\n2828\n48 16 32\nK\n0.50\n0.75\nMetric\nBoolQ\n48 16 32\nK\nCB\n48 16 32\nK\nMNLI-m\n48 16 32\nK\nMNLI-mm\nRoBERTa (Large)\n48 16 32\nK\n0.50\n0.75\nMetric\nBoolQ\n48 16 32\nK\nCB\n48 16 32\nK\nMNLI-m\n48 16 32\nK\nMNLI-mm\nALBERT (XXLarge-V2)\n[CLS] Finetuning\nNull Prompt\nManual Prompt (Prior)\nFigure 5: Impact of Dataset Size. We plot a subset of\nlearning curves for K ∈{4, 8, 16, 32}(results for all\ndatasets are provided in Appendix A1). Shaded regions\nindicate the range of performance across 10 different\nrandom seeds. In general, we ﬁnd that as K increases\nthe accuracy of prompt tuning with null prompts tends\nto be close to that of manual prompts, and substantially\nbetter than traditional ﬁnetuning.\ngraphs hold across different dataset sizes. Intu-\nitively, when the amount of data is small, manual\nprompts may outperform other approaches because\nthe inductive bias provided by the prompt has the\nmost impact when there is little data to learn the\ntask at hand. In Figure 5 we compare the accuracy\nof prompt-based ﬁnetuning using manually-written\nprompts and null prompts to traditional ﬁnetuning,\nusing the same setup described in Section 3.1 but\nvarying K ∈ {4, 8, 16, 32}. Full results for all\ndatasets are provided in Appendix A1. Although\nthere is some instability at lower values of K, we\nﬁnd that the accuracy of both prompt-based ﬁne-\ntuning approaches tends to be similar, and is either\nsubstantially better or on-par with traditional ﬁne-\ntuning. In other words, null prompts are competi-\ntive with manual prompts, even when K is small.\n5 Achieving Simplicity and Efﬁciency\nThus far, we have shown that prompt-based ﬁne-\ntuning can simplify prompt engineering at the cost\nof memory inefﬁciency—a new set of parameters\nmust be learned for each task. This is in contrast to\nin-context learning, which holds all model weights\nﬁxed but is heavily inﬂuenced by small prompt\nmodiﬁcations (Zhao et al., 2021; Lu et al., 2021).\nIn this section, we investigate how to achieve both\nmemory efﬁciency and simple prompts. Concretely,\nin Section 5.1 we try to simplify prompt engineer-\ning for in-context learning by tuning the prompt,\nand in Section 5.2, we reduce the number of learned\nparameters for prompt-based ﬁnetuning.\n5.1 Simplifying In-Context Learning With\nPrompt-Only Tuning\nHere, we try to make prompt engineering for in-\ncontext learning as simple as prompt-based ﬁne-\ntuning by automatically ﬁnding the prompt. Con-\ncretely, we focus on the emerging class of methods\nthat do prompt-only tuning: learning the prompt\nwhile keeping the rest of the model ﬁxed (Shin\net al., 2020; Lester et al., 2021). We consider:\n• AUTO PROMPT : Following Shin et al. (2020),\nwe search for discrete tokens to use in the input\ninstead of manually-designed patterns. Search is\nperformed using the original hyperparameters.\n• Prompt Tuning (Short) : We use the same\nprompt tuning approach described in the previous\nsection but we keep the masked LM ﬁxed.\n• Prompt Tuning (Long): Based on the advice\nof Lester et al. (2021), we increase the number\nof learned prompt embeddings to 20 in order to\nexpand the learning capacity.\nFor reference, we also report the results from\nprompt-based ﬁnetuning with null prompts. We\nshow the results for RoBERTa in Figure 6. We\nﬁnd that only tuning the prompt is relatively un-\nsuccessful. First, on average it fails to match the\nperformance of manually-designed prompts. Sec-\nond, all methods struggle to match the accuracy of\nprompt-based ﬁnetuning. In fact, for many of the\ndatasets, prompt-only methods perform worse by a\nwide margin (e.g., 40% absolute difference in F1\nscore on CB). This shows that ﬁnetuning masked\nLMs in the few-shot setting leads to substantially\nhigher accuracy than prompt-only tuning.\nOur Results versus Recent Prompt Tuning\nWork We ﬁnd that only tuning the prompt per-\nforms substantially worse than ﬁnetuning the entire\nLM. This is in contrast to recent work, which ar-\ngues that prompt-only tuning is competitive with\nﬁnetuning (Lester et al., 2021; Li and Liang, 2021).\nWe believe these are not contradictions but rather\ndifferences in the models and settings. Li and Liang\n2829\nBoolQ\nCB\nMNLI-m MNLI-mm\nMRPC QNLI QQP RTE SST-2\n0\n25\n50\n75\n100\n# Wins\n0\n5\nIn-Context\nAutoPrompt\nPrompt Tuning (Short)\nPrompt Tuning (Long)\nAll Parameters (Null Prompts)\nFigure 6: Prompt-Only Tuning. We try to simplify prompt engineering for in-context learning (i.e., using frozen\nmodels) by directly learning the prompt. The performance (accuracy/ F1) for prompt-only tuning is substantially\nlower than ﬁnetuning the LM parameters for RoBERTa-large. Thus, we recommend ﬁnetuning over in-context\nlearning in the few-shot setting.\nBoolQ\nCB\nMNLI-m MNLI-mm\nMRPC QNLI QQP RTE SST-2\n0\n25\n50\n75\n100\n# Wins\n0\n2\n4\n6\nCalibration (≈ 101 Params)\nLM Head Tuning (≈ 103 Params)\nBitFit (≈ 105 Params)\nAdapters (≈ 107 Params)\nAll Parameters (≈ 108 Params)\nFigure 7: Parameter-Efﬁcient Prompt-Based Finetuning. We perform prompt-based ﬁnetuning using different\nlightweight ﬁnetuning schemes. We show the accuracy or F1 on each dataset for RoBERTa-large. BitFit achieves\nthe highest accuracy on average and only modiﬁes 0.1% of the parameters.\n(2021) focus on left-to-right LMs for generation\ntasks, whereas we focus on masked LMs for classi-\nﬁcation tasks. They also ﬁnetune additional param-\neters in intermediate layers of the model. These\ndifferences may explain the difference in prompt-\ning accuracies. Moreover, Lester et al. (2021) show\nthat prompt-only tuning becomes less competitive\nas models get smaller; we use even smaller mod-\nels than evaluated in their work. Consequently,\nalthough we ﬁnd that ﬁnetuning a masked LM is\nsuperior to prompt-only tuning, there may be other\nsettings in which they fair similarly.\n5.2 Memory-Efﬁcient Finetuning\nGiven the inadequacies of prompt-only tuning, we\nnext study if prompt-based ﬁnetuning can be made\nmemory-efﬁcient. To do so, we focus on reducing\nthe number of trainable parameters, taking inspira-\ntion from recent work in the non-few-shot setting.\nThe beneﬁts of these methods is that they: (1) re-\nduce storage costs at test time when running many\ntasks (one can store only the modiﬁed parameters\nfor each task), and (2) reduce memory costs at train-\ning time, as fewer optimized parameters means\nmuch smaller statistics in optimizers like Adam.\nWe consider four lightweight ﬁnetuning methods:\n• Adapters: We use Adapters (Houlsby et al.,\n2019), neural networks layers that are inserted be-\ntween the feedforward portion of the Transformer\narchitecture. We use the default Adapters hyper-\nparameters from Houlsby et al. (2019) ( ≈107\nparameters per task).\n• BitFit: Following Ben-Zaken et al. (2021), we\nonly update the bias terms inside the Transformer\n(≈105 parameters per task).\n• LM Head Tuning: We update the embeddings in\nthe MLM output layer that are associated with the\nverbalizer tokens (≈103 parameters per task).\n• Calibration: Following Zhao et al. (2021), we\nlearn an afﬁne transformation on top of the log-\n2830\nBoolQ CB MNLI MRPC QNLI QQP RTE SST-2 Wins\n(acc) ( F1) (acc) ( F1) (acc) ( F1) (acc) (acc) (#)\nRoBERTa\nIn-context 49.2 51.2 48.0 / 48.1 28.0 55.2 55.6 60.7 84.1 0\n[CLS] ﬁnetuning 51.0 74.3 39.4 / 38.6 77.8 58.2 61.9 54.5 72.9 1\nPrompt-based Finetuning\nAll Parameters 63.9 90.6 66.5 / 61.6 74.1 57.4 62.9 68.8 92.6 3\n+ Null Prompt 59.9 91.2 61.6 / 57.8 76.1 65.8 65.9 54.6 83.8 3\nBitFit 66.7 89.8 69.3 / 70.0 69.7 62.3 66.3 64.9 92.1 6\n+ Null Prompt 67.2 90.6 67.5 / 62.9 68.2 66.4 65.1 65.4 89.6 3\nALBERT\nIn-context 68.0 19.9 35.4 / 35.2 20.7 50.1 0.3 53.1 49.1 0\n[CLS] ﬁnetuning 53.3 56.5 36.0 / 38.6 76.9 66.6 58.5 54.1 62.9 2\nPrompt-based Finetuning\nAll Parameters 73.5 91.1 65.0 / 56.0 75.2 73.9 59.9 61.4 93.2 8\n+ Null Prompt 53.7 89.4 58.2 / 53.7 78.5 67.3 62.0 59.2 91.5 3\nBitFit 77.2 86.7 64.6 / 61.6 79.7 73.1 61.4 58.6 92.0 8\n+ Null Prompt 52.8 86.3 55.3 / 58.0 65.5 63.8 52.7 57.2 89.7 1\nTable 2: Final Few-Shot Results from representative methods. Wins are computed on a per-datasets basis and the\n“winners” of the different approaches are highlighted in bold. Prompt-based ﬁnetuning signiﬁcantly outperforms in-\ncontext learning and traditional [CLS] ﬁnetuning, even without any tuning of the prompt (null prompt). Moreover,\nprompt-based ﬁnetuning can be highly memory efﬁcient using bias-only ﬁnetuning ( BitFit). We show matched\nand mismatched results for MNLI.\nits associated with the verbalizer tokens (≈101\nparameters per task).\nWe run prompt-based ﬁnetuning for each method\nwith the prompts from Manual Prompts (Prior). We\nalso report the accuracy of ﬁnetuning all of the\nparameters for reference.\nResults We show the results in Figure 7. There\nare diminishing returns as the parameter count is\nincreased. In particular, substantial gains are made\nwhen going from calibration to LM head tuning to\nBitFit, however, there is either a marginal improve-\nment or even a decrease in performance when going\nto Adapters or All Parameters. The BitFit method\nprovides the best accuracy-efﬁciency trade-off, and\neven outperforms ﬁnetuning all of the parameters\nin terms of # Wins. This suggests that updating all\nof the LM’s hundreds of millions of parameters on\nonly 16 data points is suboptimal.\n5.3 Putting Everything Together\nWe ﬁnally combine null prompts and memory-\nefﬁcient ﬁnetuning. We show the results from this\nmethod, as well as the other best few-shot methods,\nin Table 2. Overall, we recommend ﬁnetuning with\nnull prompts and BitFit: it achieves competitive\naccuracy, is simple to set up, and introduces small\nmemory costs for each new task.\n6 Conclusion and Future Work\nTwo high-level methods exist in few-shot prompt-\ning: using a frozen LM (in-context learning) and\nﬁnetuning the LM on the few training examples\n(prompt-based ﬁnetuning). In this work, we demon-\nstrate two new advantages of prompt-based ﬁne-\ntuning. First, we show that it performs comparably\nacross different prompt choices. In fact, there is a\nsimple class of prompts—null prompts—that can\nbe ﬂexibly applied to different tasks without de-\ngrading performance relative to manually-written\nand learned prompts. Second, we demonstrate\nthat prompt-based ﬁnetuning can be made memory\nefﬁcient: ﬁnetuning only the bias terms (BitFit)\nachieves comparable or better accuracy than ﬁne-\ntuning all the parameters while being 1000x more\nmemory efﬁcient. Taken together, using null pat-\nterns with BitFit is an approach that is efﬁcient,\nsimple-to-tune, and competitive in accuracy. Code\nand instructions for reproducing our results is avail-\nable at: https://github.com/ucinlp/null-prompts.\nOur results motivate future analysis of few-shot\nlearning methods. Concretely, we show that the\nsuccess of prompt-based ﬁnetuning is not solely\nexplained by carefully-chosen patterns or verbal-\nizers. This suggests that the gains from prompt-\nbased ﬁnetuning are partially due to its low-level\nsetup, i.e., predicting on a [MASK] token with a\npre-trained MLM head. More generally, we hope to\nfurther analyze why and how small changes to dif-\nferent few-shot learning methods can lead to wildly\ndifferent accuracies. We also hope to extend our\nﬁndings to both very large and left-to-right LMs,\nas our current results are for masked LMs that are\nrelatively small by modern standards.\n2831\nAcknowledgements\nWe would like to thank Danqi Chen, Timo Schick,\nand Hinrich Schütze for their careful corrections\nto an earlier version of this manuscript. We also\nwould like to thank the reviewers for their thought-\nful feedback. This work is funded in part by\nthe DARPA MCS program under Contract No.\nN660011924033 and by research awards from\nAmazon and the Allen Institute for Artiﬁcial In-\ntelligence. Additionally, Robert is supported in\npart by the Irvine Initiative in AI, Law, and Soci-\nety fellowship, and Eric by the Apple Scholars in\nAI/ML fellowship.\nReferences\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Goldberg.\n2021. BitFit: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. ArXiv preprint, abs/2002.06305.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Confer-\nence on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA , volume 97 of\nProceedings of Machine Learning Research , pages\n2790–2799. PMLR.\nBrenden M Lake, Ruslan Salakhutdinov, and Joshua B\nTenenbaum. 2015. Human-level concept learning\nthrough probabilistic program induction. In Science.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2627–2636, On-\nline. Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , pages\n4582–4597, Online. Association for Computational\nLinguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. ArXiv preprint, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint, abs/1907.11692.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021. Fantastically or-\ndered prompts and where to ﬁnd them: Overcoming\nfew-shot prompt order sensitivity. ArXiv preprint,\nabs/2104.08786.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of ﬁne-tuning\nBERT: misconceptions, explanations, and strong\nbaselines. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nJuri Opitz. 2019. Argumentative relation classiﬁcation\nas plausibility ranking. In Proceedings of the 15th\nConference on Natural Language Processing (KON-\nVENS 2019): Long Papers , pages 193–202, Erlan-\ngen, Germany. German Society for Computational\nLinguistics & Language Technology.\n2832\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models.ArXiv\npreprint, abs/2105.11447.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5203–5212, Online. Association for Compu-\ntational Linguistics.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2339–2352, Online. As-\nsociation for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. Superglue:\nA stickier benchmark for general-purpose language\nunderstanding systems. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2020. HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning. In EMNLP Demo Track.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\nIn Proceedings of the 38th International Confer-\nence on Machine Learning, ICML 2021, 18-24 July\n2021, Virtual Event, volume 139 of Proceedings of\nMachine Learning Research , pages 12697–12706.\nPMLR.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5017–5033, Online. Association for\nComputational Linguistics.\n2833\nDataset Pattern Verbalizer\nBoolQ {passage}. Question: {question}? Answer: [MASK]. True: \"Yes\"\nFalse: \"No\"\nCB {premise}? [SEP] [MASK], {hypothesis}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMNLI {sentence1}? [SEP] [MASK], {sentence2}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMNLI-mm {sentence1}? [SEP] [MASK], {sentence2}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMRPC {sentence1} and {sentence2} have [MASK] meanings. 0: \"different\"\n1: \"similar\"\nQNLI {question}? [SEP] [MASK], {sentence} entailment: \"Yes\"\nnot_entailment: \"No\"\nQQP {question1} and {question2} have [MASK] meanings. 0: \"different\"\n1: \"similar\"\nRTE {sentence1}? [SEP] [MASK], {sentence2} entailment: \"Yes\"\nnot_entailment: \"No\"\nSST-2 {sentence} It was [MASK] . 0: \"terrible\"\n1: \"great\"\nTable A1: Prompts denoted as “Manual Prompts (Prior)”. We use prompts inspired from past work (Schick and\nSchütze, 2021a; Gao et al., 2021). The ﬁelds between curly brackets indicate dataset-speciﬁc inputs. Predictions\nare made on the [MASK] token in each prompt. For prompt tuning, we tune the tokens in the pattern.\nDataset Pattern Verbalizer\nBoolQ Passage: {passage} Question: {question} Answer: [MASK]. True: \"true\"\nFalse: \"false\"\nCB Premise: {premise} Hypothesis: {hypothesis} Label: [MASK]\nentailment: \"yes\"\ncontradiction: \"no\"\nneutral: \"maybe\"\nMNLI Premise: {sentence1} Hypothesis: {sentence2} Label: [MASK]\nentailment: \"yes\"\ncontradiction: \"no\"\nneutral: \"maybe\"\nMNLI-mm Premise: {sentence1} Hypothesis: {sentence2} Label: [MASK]\nentailment: \"yes\"\ncontradiction: \"no\"\nneutral: \"maybe\"\nMRPC {sentence1} and {sentence2} are the [MASK]. 0: \"different\"\n1: \"same\"\nQNLI Question: {question} Sentence: {sentence} Label: [MASK] entailment: \"yes\"\nnot_entailment: \"no\"\nQQP {question1} and {question2} are the [MASK]. 0: \"different\"\n1: \"same\"\nRTE Premise: {sentence1} Hypothesis: {sentence2} Label: [MASK] entailment: \"yes\"\nnot_entailment: \"no\"\nSST-2 {sentence} Overall my impression is [MASK] . 0: \"bad\"\n1: \"good\"\nTable A2: Prompts denoted as “Manual Prompts (w/o Engineering)”. We manually write one prompt for each\ntask, using only our intuition, and do not tune or edit them in any way after evaluating them. Fields between curly\nbrackets indicate dataset-speciﬁc inputs. Predictions are made on the [MASK] token in each prompt. For prompt\ntuning, we tune the tokens in the pattern.\n2834\nDataset Pattern Verbalizer\nBoolQ {passage} {question} [MASK] True: \"Yes\"\nFalse: \"No\"\nCB {premise} [MASK] {hypothesis}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMNLI {sentence1} [MASK] {sentence2}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMNLI-mm {sentence1} [MASK] {sentence2}\nentailment: \"Yes\"\ncontradiction: \"No\"\nneutral: \"Maybe\"\nMRPC {sentence1} {sentence2} [MASK] 0: \"different\"\n1: \"similar\"\nQNLI {question} [MASK] {sentence} entailment: \"Yes\"\nnot_entailment: \"No\"\nQQP {question1} {question2} [MASK] 0: \"different\"\n1: \"similar\"\nRTE {sentence1} [MASK] {sentence2} entailment: \"Yes\"\nnot_entailment: \"No\"\nSST-2 {sentence} [MASK] 0: \"terrible\"\n1: \"great\"\nTable A3: Null Prompts used for results in Sections 4 and 5.\n4 8 16 32\nK\n0.4\n0.6\n0.8\nMetric\nBoolQ\n4 8 16 32\nK\nCB\n4 8 16 32\nK\nMNLI-m\n4 8 16 32\nK\nMNLI-mm\n4 8 16 32\nK\nMRPC\n4 8 16 32\nK\nQNLI\n4 8 16 32\nK\nQQP\n4 8 16 32\nK\nRTE\n4 8 16 32\nK\nSST-2\nRoBERTa (Large)\n[CLS] Finetuning Null Prompt Manual Prompt (Prior)\n4 8 16 32\nK\n0.4\n0.6\n0.8\nMetric\nBoolQ\n4 8 16 32\nK\nCB\n4 8 16 32\nK\nMNLI-m\n4 8 16 32\nK\nMNLI-mm\n4 8 16 32\nK\nMRPC\n4 8 16 32\nK\nQNLI\n4 8 16 32\nK\nQQP\n4 8 16 32\nK\nRTE\n4 8 16 32\nK\nSST-2\nALBERT (XXLarge-V2)\n[CLS] Finetuning Null Prompt Manual Prompt (Prior)\nFigure A1: Impact of Dataset Size. We plot learning curves for K ∈{4, 8, 16, 32}. Shaded regions indicate the\nrange of performance across 10 different random seeds.\n2835",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8725223541259766
    },
    {
      "name": "Task (project management)",
      "score": 0.6848033666610718
    },
    {
      "name": "Overhead (engineering)",
      "score": 0.5720715522766113
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5372213125228882
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49627262353897095
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4759918451309204
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4519030451774597
    },
    {
      "name": "Language model",
      "score": 0.4459649622440338
    },
    {
      "name": "Machine learning",
      "score": 0.4197064936161041
    },
    {
      "name": "Programming language",
      "score": 0.08612442016601562
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    }
  ]
}