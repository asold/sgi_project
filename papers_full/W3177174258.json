{
  "title": "Multimodal Few-Shot Learning with Frozen Language Models",
  "url": "https://openalex.org/W3177174258",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226541231",
      "name": "Tsimpoukelli, Maria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222786227",
      "name": "Menick, Jacob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225334474",
      "name": "Cabi, Serkan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222364529",
      "name": "Eslami, S. M. Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3025185919",
      "name": "Vinyals, Oriol",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202179581",
      "name": "Hill, Felix",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2606722458",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2896348597",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2974885182",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3126464137",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2914306086",
    "https://openalex.org/W2915434260",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3139224848",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2753160622",
    "https://openalex.org/W3143728625",
    "https://openalex.org/W2076486742"
  ],
  "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks.",
  "full_text": "Multimodal Few-Shot Learning with\nFrozen Language Models\nMaria Tsimpoukelli∗\nDeepMind\nmrts@deepmind.com\nJacob Menick∗\nDeepMind\nUniversity College London\njmenick@deepmind.com\nSerkan Cabi∗\nDeepMind\ncabi@deepmind.com\nS. M. Ali Eslami\nDeepMind\naeslami@deepmind.com\nOriol Vinyals\nDeepMind\nvinyals@deepmind.com\nFelix Hill\nDeepMind\nfelixhill@deepmind.com\nAbstract\nWhen trained at sufﬁcient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a few\nexamples. Here, we present a simple, yet effective, approach for transferring this\nfew-shot learning ability to a multimodal setting (vision and language). Using\naligned image and caption data, we train a vision encoder to represent each image\nas a sequence of continuous embeddings, such that a pre-trained, frozen language\nmodel prompted with this preﬁx generates the appropriate caption. The resulting\nsystem is a multimodal few-shot learner, with the surprising ability to learn a variety\nof new tasks when conditioned on examples, represented as a sequence of multiple\ninterleaved image and text embeddings. We demonstrate that it can rapidly learn\nwords for new objects and novel visual categories, do visual question-answering\nwith only a handful of examples, and make use of outside knowledge, by measuring\na single model on a variety of established and new benchmarks.\n1 Introduction\nAuto-regressive transformers have been shown to be very impressive models of natural language [40].\nLarge-scale language transformers exhibit several surprising abilities beyond that of standard text\ngeneration [4, 30]. Perhaps most notably, they are few-shot learners; they can learn to perform a\nnew task from a few examples without any further gradient updates. Equipped with this ability, these\nmodels have been shown to rapidly adapt to new tasks and styles of generation via prompting (e.g.\nswitching from formal to informal language) [4], to quickly retrieve relevant encyclopedic or general\nknowledge when primed with a relevant context (e.g. answering questions such as ‘When did the\nFrench Revolution begin?’) [33, 1, 27] and to use new words in appropriate ways straight after being\ntaught what those words mean (sometimes referred to as ‘fast binding’) [12, 4].\nDespite these impressive capabilities, such large scale language models are ‘blind’ to modalities other\nthan text, preventing us from communicating visual tasks, questions or concepts to them. Indeed,\nphilosophers and linguists have questioned whether an un-grounded language model can ever achieve\ntrue understanding of the language it processes [5, 2]. Here, we present Frozen, a method for giving a\npre-trained language model access to visual information in a way that extends its few-shot learning\ncapabilities to a multimodal setting, without changing its weights. Frozen consists of a neural network\ntrained to encode images into the word embedding space of a large pre-trained language model\nsuch that the language model generates captions for those images. The weights of the language\nmodel are kept frozen, but gradients are back-propagated through it to train the image encoder from\nPreprint. Under review.\narXiv:2106.13884v2  [cs.CV]  3 Jul 2021\nThis person is \nlike \n!\n. \nThis person is \nlike \n\"\n. \nThis person \nis like\nModel Completion \n#\n. <EOS>\nThis was invented \nby Zacharias \nJanssen.\nThis was invented by \nThomas Edison.\nThis was \ninvented by\nModel Completion \nthe Wright \nbrothers. <EOS>\nWith one of these I \ncan drive around a \ntrack, overtaking \nother cars and taking \ncorners at speed\nWith one of these I can \ntake off from a city and \nfly across the sky to \nsomewhere on the other \nside of the world\nWith one of \nthese I can\nModel Completion \nbreak into a secure \nbuilding, unlock the door \nand walk right in  <EOS>\nFigure 1: Curated samples with about ﬁve seeds required to get past well-known language model\nfailure modes of either repeating text for the prompt or emitting text that does not pertain to the image.\nThese samples demonstrate the ability to generate open-ended outputs that adapt to both images and\ntext, and to make use of facts that it has learned during language-only pre-training.\nscratch (Figure 2). Although Frozen is trained on single image-text pairs, once trained it can respond\neffectively to ordered sets of multiple images and words. This allows users to e.g. ‘prompt’ it with\nseveral examples of new multimodal tasks before evaluating its performance, or to ‘teach’ it the name\nof a new visual category before immediately asking about that category.\nVision \nEncoder \nLanguage Model\nText Embedder\nA small red boat on the water\nA small red boat on the water\nFrozen\nFrozen\nLanguage Model\nSelf Attention Layers\n<latexit sha1_base64=\"1QW3AOkoQF27aOyHA/emdyynyu4=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48V7Ae0oWy2m3bpZhN3J0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6wEnC/YgOlQgFo2ilzrDfwxFH2i9X3Ko7B1klXk4qkKPRL3/1BjFLI66QSWpM13MT9DOqUTDJp6VeanhC2ZgOeddSRSNu/Gx+75ScWWVAwljbUkjm6u+JjEbGTKLAdkYUR2bZm4n/ed0Uw2s/EypJkSu2WBSmkmBMZs+TgdCcoZxYQpkW9lbCRlRThjaikg3BW355lbRqVe+yWru/qNRv8jiKcAKncA4eXEEd7qABTWAg4Rle4c15dF6cd+dj0Vpw8plj+APn8wchYJAJ</latexit>\ng ✓\n<latexit sha1_base64=\"Ssu+fpZOeOJ2vIP8aCdnW+2QV94=\">AAAB73icbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMeiF48V7Ae0oWy2m3bpZhN3J0IJ/RNePCji1b/jzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6wEnC/YgOlQgFo2ilTtjv4Ygj7ZcrbtWdg6wSLycVyNHol796g5ilEVfIJDWm67kJ+hnVKJjk01IvNTyhbEyHvGupohE3fja/d0rOrDIgYaxtKSRz9fdERiNjJlFgOyOKI7PszcT/vG6K4bWfCZWkyBVbLApTSTAms+fJQGjOUE4soUwLeythI6opQxtRyYbgLb+8Slq1qndZrd1fVOo3eRxFOIFTOAcPrqAOd9CAJjCQ8Ayv8OY8Oi/Ou/OxaC04+cwx/IHz+QMf1ZAI</latexit>\nf ✓\n<latexit sha1_base64=\"FQU2AwFKvaL1P7DElyhCLmQfs9A=\">AAAB73icbVBNS8NAEJ34WetX1aOXxSJ4KkkR9Vj04rGC/YA2lM120y7dbOLupFBC/4QXD4p49e9489+4bXPQ1gcDj/dmmJkXJFIYdN1vZ219Y3Nru7BT3N3bPzgsHR03TZxqxhsslrFuB9RwKRRvoEDJ24nmNAokbwWju5nfGnNtRKwecZJwP6IDJULBKFqpPe5l3WQopr1S2a24c5BV4uWkDDnqvdJXtx+zNOIKmaTGdDw3QT+jGgWTfFrspoYnlI3ogHcsVTTixs/m907JuVX6JIy1LYVkrv6eyGhkzCQKbGdEcWiWvZn4n9dJMbzxM6GSFLlii0VhKgnGZPY86QvNGcqJJZRpYW8lbEg1ZWgjKtoQvOWXV0mzWvGuKtWHy3LtNo+jAKdwBhfgwTXU4B7q0AAGEp7hFd6cJ+fFeXc+Fq1rTj5zAn/gfP4Abd+QOw==</latexit>\nv \u0000\nFigure 2: Gradients through a frozen lan-\nguage model’s self attention layers are\nused to train the vision encoder.\nBy exploiting its pre-trained language model, Frozen ex-\nhibits strong zero-shot performance on multimdodal tasks\nthat it was not trained on, such as visual question answer-\ning (VQA). More surprisingly, it gets better at these tasks\nafter seeing a handful of examples “in-context” as in [4],\nand also performs above chance on tests of fast category\nlearning such as miniImageNet [41]. In each case, com-\nparisons with ‘blind’ baselines show that the model is\nadapting not only to the language distribution of these new\ntasks, but also to the relationship between language and\nimages. Frozen is therefore a multimodal few-shot learner,\nbringing the aforementioned language-only capabilities of\nrapid task adaptation, encyclopedic knowledge and fast\nconcept binding to a multimodal setting.\nOur goal in developing Frozen was not to maximise performance on any speciﬁc task, and in many\ncases it is far from state-of-the-art. Nonetheless, it performs well above trivial baselines across a wide\nrange of tasks without ever seeing more than a handful of the training examples provided by these\nbenchmarks. Moreover, as illustrated in Figure 1, Frozen is a system for genuinely open-ended and\nunconstrained linguistic interpretation of images that often produces compelling output.\n \nSelf Attention Layers\nText\nEmbedder\nSteve Jobs\nQ: Who \ninvented \nthis? A: \nThe Wright \nbrothers. \nVision \nEncoder \nVision \nEncoder \nText\n Embedder\nQ: Who \ninvented \nthis? A:\n.\nSelf Attention Layers\nText\nEmbedder\nQuestion: \nWhat colour \nis the car? \nAnswer:  \nVision \nEncoder \nBlue\n(a) 0-shot VQA (b) 1-shot outside-knowledge VQA\nSelf Attention Layers\nText\nEmbedder\nThis is a\nThis is a \ndax.\nVision \nEncoder \nVision \nEncoder \nText\nEmbedder\nThis is a \nblicket.\ndax .\n(c) Few-shot image classiﬁcation\nVision \nEncoder \nText\nEmbedder\nQuestion: \nWhat is \nthis? \nAnswer: \n<EOS><EOS> <EOS>\nFigure 3: Inference-Time interface for Frozen. The ﬁgure demonstrates how we can support (a) visual\nquestion answering, (b) outside-knowledge question answering and (c) few-shot image classiﬁcation\nvia in-context learning.\n2\nTo summarise, our contributions are as follows: 1. We present Frozen, a modular, scalable and\nefﬁcient approach to training vision front-ends for large language models. The resulting combined\nmodel retains all of the capabilities of large language models, but can also process text and image\ninputs in any arbitrary sequence. 2. We show that such models transfer their capacity for rapid task\nadaptation, encyclopedic knowledge and fast concept binding from a language-only to a multimodal\nsetting, and verify that prompting them with both visual and language information can be strictly\nmore effective than doing so with language information alone. 3. We quantify these capabilities on a\nrange of existing and new benchmarks, paving the way for future analysis of these capabilities.\n2 Related Work\nThe Frozen method is inspired by lots of recent work. [ 25] show that the knowledge encoded in\ntransformer language models can be a valuable prior for tasks involving reasoning and memory across\ndiscrete sequences, and even classiﬁcation of images presented as sequences of spatial regions. In that\napproach, a small subset of the pre-trained language model weights are ﬁne-tuned to the various ﬁnal\napplications. In contrast, applying Frozen to different tasks does not involve any weight updates to\nthe transformer whatsoever; the system adapts to and improves at multimodal (vision and language)\ntasks as activations propagate through the model. The two studies thus reveal different ways in which\nknowledge acquired from text can transfer to non-linguistic settings.\nThe effectiveness of preﬁx tuning [22] or prompt tuning [19] was another important motivation for\nFrozen. Preﬁx tuning is a method for prompting a language model to produce output of a particular\nstyle using gradient descent to learn a task-speciﬁc bias term which functions like the continuous\nembedding of a text prompt. Using preﬁx tuning, language models can be adapted to different natural\nlanguage generation tasks like summarization. Frozen could also be considered a type of image-\nconditional preﬁx tuning, in which this continuous prompt is not a bias but an image-conditional\nactivation produced by an external neural network.\nA large body of work has applied either text-speciﬁc or multimodal representation-learning approaches\nlike BERT [8] to visual question answering (VQA) and captioning (see e.g. [24, 38] and many more).\nIn these approaches, models are ﬁrst trained with aligned data on task-agnostic cross-modal objectives\nand then ﬁne-tuned to speciﬁc tasks. This approach can yield state-of-the-art performance on a range\nof classiﬁcation tasks. Unlike Frozen, the resulting systems are highly specialized to one task, and\ncannot learn new concepts or adapt to new tasks in a few shots.\nBy contrast, [7] propose text generation as an objective for task-general multimodal models, yielding\na system that, like Frozen, produces unconstrained language output. Unlike Frozen, they do not use a\npre-trained model trained on text only, and do not consider zero or few-shot learning, instead updating\nall weights of the system with training data for each task they consider – thus, again, specializing the\nmodels to one task at a time. Similarly, [44] and [6] show that a large pre-trained language model as\ndecoder can improve a captioning performance when training data is limited. Unlike Frozen, they\nuse pre-trained frozen visual encoders or object extractors and ﬁne-tune the pre-trained weights in\nthe text decoder on the captioning data. Similarly, they do not consider zero or few-shot adaptation\nacross different multimodal tasks. Past work has also explored alternative approaches for post-hoc\ncombination of models for different modalities using latent variables [39].\nMultimodal pre-training has recently been shown to enable strong zero-shot generalization in the\ndiscriminative setting using large-scale contrastive learning [28, 14]. Also in a discriminative setting,\n[43] has observed signs of emergent few-shot-learning from large-scale training. In contrast, our work\nenables strong generalization to new multimodal tasks both zero-shot or few-shot with completely\nopen-ended generative text output.\n3 The Frozen Method\nFrozenis a method for grounding a large language model without changing its weights, closely related\nto preﬁx tuning [22, 19]. Preﬁx tuning trains a task-speciﬁc continuous bias term to function like\nthe embedding of a constant, static text prompt used for all test-time examples. Frozen extends this\napproach by making this preﬁx dynamic, in that it is not a constant bias but an input-conditional\nactivation emitted by a neural network.\n3\n3.1 Architecture\nPre-trained Autoregressive Language Models Our method starts from a pre-trained deep auto-\nregressive language model, based on the Transformer architecture [ 40, 29], which parametrizes\na probability distribution over text y. Text is decomposed into a sequence of discrete tokens\ny = y1,y2,...,y L by the SentencePiece tokenizer [17]. We use a vocabulary of size 32,000. The\nlanguage model makes use of an embedding function gθ which independently transforms each token\ninto a continuous embedding tl := gθ(yl), as well as a transformer neural network fθ whose output\nis a vector of logits parameterizing a categorical distribution over the vocabulary. The distribution\npθ(y) is represented as follows:\nlog pθ(y) =\n∑\nl\nlog pθ(yl|y1,y2,...,y l−1) =\n∑\nl\nfθ(t1,t2,...,t l−1)yl\nThe model we start from is pre-trained, i.e.θhas been optimised via the standard maximum-likelihood\nobjective on a large dataset of text from the internet. We use a 7 billion parameter transformer trained\non the public dataset C4 [30] – previous work has shown that the multi-billion parameter scale is\nsufﬁcient to exhibit the key capacities we are interested in studying [29, 33].\nVision Encoder Our vision encoder is based on NF-ResNet-50 [3]. We deﬁne vφ as a function that\ntakes a raw image and emits a continuous sequence to be consumed by the transformer. We use the\nﬁnal output vector of the NF-Resnet after the global pooling layer.\nVisual Preﬁx One important requirement is to represent images in a form that the transformer\nalready understands: a sequence of continuous embeddings, each having the same dimensionality D\nas a token embedding tl. We therefore form the visual preﬁx by linearly mapping the vision encoder’s\noutput to D∗nchannels, and then reshaping the result as a sequence of nembeddings, each with\ndimensionality D. We call this sequence a visual preﬁx since it plays the same functional role in\nthe transformer architecture as (part of) an embedding sequence of preﬁx tokens. We experimented\nusing different number of tokens, speciﬁcally 1, 2 and 4 and found that 2 performs best, though\ncertainly this would be sensitive to other architectural details. See Appendix for more details on the\narchitecture.\n3.2 Training\nDuring training, we update only the parameters φof the vision encoder using paired image-caption\ndata from the Conceptual Captions dataset [ 35]. Our experiments show that ﬁne-tuning θ hurts\ngeneralization, as much less paired image-caption data is available than the amount of text-only data\nused to pre-train θ. Training only the parameters φmakes our system modular – it can use an existing\nlanguage model off the shelf – and also quite simple: we only train a visual encoder and rely on the\ncapabilities of an existing language model.\nFollowing standard captioning systems [21, 13], we treat captioning as conditional generation of\ncaption text y given an image x. We represent x as vφ(x) =i1,i2,...,i n and train φto maximise the\nlikelihood:\nlog pθ,φ(y|x) =\n∑\nl\nlog pθ,φ(yl|x,y1,y2,...,y l−1)\n=\n∑\nl\nfθ(i1,i2,...,i n,t1,t2,...,t l−1)yl\nWhilst the parameters θ are frozen, each element ik of the visual preﬁx receives gradients∑\nl\n∇ik fθ(i1,i2,...,i n,t1,t2,...,t l−1)yl , enabling the parameters of the visual encoder to be op-\ntimised with standard backpropagation and SGD (Figure 2).\nAs the notation fθ(i1,i2,...,i n,t1,t2,...,t l−1) suggests, we present the visual preﬁx during training\nas if it were a sequence of embeddings occurring earlier in time than the caption (token embeddings)\nt1,t2,.... We use relative positional encoding [36], which enables the transformer to generalize to\nprompt sequences where an image is not always in the ﬁrst absolute positions, and where more than\none image may be present. We leave improvements of this simple scheme for future work.\n4\nThis is a \nblicket.\nThis is a dax.\n This is a \nblicket.\nThis is a dax.\n Q: What is this? \nA: This is a\nQuestion\nfrom ImageNet\n0-repeats\n0-shots\n2-way\n0-repeats\n2-inner-shots\nTask Induction\nAnswer with dax \nor blicket.\ninner-shot 1 inner-shot 2inner-shot 1 inner-shot 2\nSupport\nfrom ImageNet\nThis is a \nblicket.\nThis is a dax. This is a \nblicket.\nThis is a dax. Q: What is the \ndax made of? A: \nQuestion\nfrom VisualGenome\n0-repeats\n0-shots\n2-way\n0-repeats\n2-inner-shots\ninner-shot 1 inner-shot 2inner-shot 1 inner-shot 2\nSupport\nfrom ImageNet\n blicket (vase)\ndax (table)\n(a) miniImageNet(b) Fast VQA\nModel Completion \nwood\nModel Completion \nblicket.\nFigure 4: Examples of (a) the Open-Ended miniImageNet evaluation (b) the Fast VQA evaluation.\n3.3 Interface at Inference Time\nAt inference time, a vanilla language model, conditioned upon an arbitrary text prompt or ‘preﬁx’\ny1,y2,...,y p, generates text sequences yp+1,yp+2,... autoregressively. In Frozen it is straightforward\nto include images in a prompt by placing an image’s embedding i1,i2 next to a text embedding\nsubsequence t1,t2,...,t p. Because the transformer fθ is modality-agnostic, we can interleave a\nsub-sequence of text token embeddings with a sub-sequence of image embeddings in any arbitrary\norder. In Figure 3, we show how this can support zero-shot visual question-answering (Figure 3a),\nfew-shot visual question-answering (Figure 3b), and few-shot image classiﬁcation (Figure 3c).\nTo evaluate these tasks, the model decodes output sequences greedily and these outputs are compared\nagainst the ground truth answers of the task following the normalization technique used in [18]. We\ndo not use short-lists of pre-canned answers to stress test the open-ended capabilities of Frozen, even\nthough in some tasks this may hurt its performance.\n3.4 Few-Shot Learning Deﬁnitions\nThe ability of Frozen to be conditioned on a sequence of interleaved images and text allows it not only\nto be able to perform at different multimodal tasks, but also gives rise to different ways of ‘inducing’\nthe task to the model in order to improve its performance. We brieﬂy deﬁne the terminology used\nin our settings, common amongst all the different tasks. See Figure 5 in the appendix for a visual\nillustration of these concepts.\n• Task induction Explanatory text that precedes the sequence of images and text. It is\nintended to describe the task to the model in natural language, for example ‘Please answer\nthe question.’\n• Number of shots The number of distinct full examples of the task presented to the model\nprior to the evaluated example. For example, in Visual Question-Answering, a shot is an\nimage along with the question and the answer.\nFor tasks involving fast concept binding (e.g., few-shot image classiﬁcation), we deﬁne further\nspeciﬁc terminology. See also Figure 4a and Figure 6 in the appendix.\n• Number of ways The number of object classes in the task (e.g. dog vs cat).\n• Number of inner-shots The number of distinct exemplars from each category that are\npresented to the model (i.e. number of images of different dogs). In previous work with\nMiniImagenet, these were known as shots, but we modify the term here to distinguish from\nthe more general usage of the term described above.\n• Number of repeatsThe number of times each inner-shot is repeated in the context presented\nto the model. We use this setting as an ablation to explore how the model integrates visual\ninformation about a category.\n5\nn-shot Acc. n=0 n=1 n=4 τ\nFrozen 29.5 35.7 38.2 \u0017\nFrozen scratch 0.0 0.0 0.0 \u0017\nFrozen ﬁnetuned 24.0 28.2 29.2 \u0017\nFrozen train-blind 26.2 33.5 33.3 \u0017\nFrozen VQA 48.4 – – \u0013\nFrozen VQA-blind 39.1 – – \u0013\nOscar [23] 73.8 – – \u0013\nTable 1: Transfer from Conceptual Captions to\nVQAv2. The τ column indicates whether a model\nuses training data from the VQAv2 training set.\nThe row denoted Frozen train-blind is the blind base-\nline described in subsection 4.1. Frozen VQA is a\nbaseline which mixes in VQAv2 training data.\nn-shot Acc. n=0 n=1 n=4 τ\nFrozen 5.9 9.7 12.6 \u0017\nFrozen 400mLM 4.0 5.9 6.6 \u0017\nFrozen ﬁnetuned 4.2 4.1 4.6 \u0017\nFrozen train-blind 3.3 7.2 0.0 \u0017\nFrozen VQA 19.6 – – \u0017\nFrozen VQA-blind 12.5 – – \u0017\nMA VEx [42] 39.4 – – \u0013\nTable 2: Transfer from Conceptual Captions to\nOKVQA. The τ column indicates if a model uses\ntraining data from the OKVQA training set.Frozen\ndoes not train on VQAv2 except in the baseline row,\nand it never trains on OKVQA.\n4 Experiments: A Multi-Modal Few-Shot Learner\nOur experiments are designed to quantify three capacities that should be characteristic of a Multi-\nModal Few-Shot Learner: rapid adaptation to new tasks, fast access to general knowledge and fast\nbinding of visual and linguistic elements. We train Frozen on Conceptual Captions, a public dataset\nthat consists of around three million image-caption pairs [35]. We do early stopping on the validation\nset perplexity which usually reaches an optimum just after a single epoch with batch size 128. All\nexperiments used the Adam optimizer with β1 = 0.9 and β2 = 0.95 and a constant learning rate of\n3e-4 unless otherwise noted. We operate on 224 ×224 images at both train and test-time. Images\nwhich are not square are ﬁrst padded with zeroes to square and then resized to 224×224.\n4.1 Rapid Task Adaptation\nWe ﬁrst examine zero-shot and few-shot generalization from captioning to visual question-answering.\nThis is a type of rapid adaptation from captioning behaviour to question-answering behaviour with\neither simple prompting alone or few-shot learning, analogous to transfer from language modelling\nto open-domain question-answering [33] in the vision plus language domain. We evaluate on the\nVQAv2 [10] validation set.\nZero-shot transfer from captioning to VQA Captioning training can transfer moderately well to\nvisual question-answering in the zero-shot setting with no training or in-context examples at all. The\nstrength of the pre-trained language model is a double-edged sword. It powers the generalization\nabilities of Frozen but also enables the model to perform surprisingly well without considering the\nvisual input at all. To guard against this possibility we also train blind baselines, in which the image\npresented to the visual encoder is blacked out, but the convnet weights are still trained. This amounts\nto preﬁx tuning [22]. We outperform this blind baseline which also inherits the few-shot learning\nabilities of the language model.\nIn these experiments we also include two additional and important baselines: Frozenﬁnetuned in which\nthe language model is instead ﬁnetuned starting from the pretrained weights andFrozenscratch, wherein\nthe whole system is trained from scratch end-to-end. These baselines preferred a smaller learning rate\nof 1e-5. Results in Table 1 show that keeping the language model frozen generalizes substantially\nbetter to visual question-answering than ﬁnetuning. The model trained from scratch is not able to\ntransfer at all from captioning to VQA; we interpret this to suggest that the tremendous generalization\nabilities of large language models are reliant upon large-scale training datasets in which the task\nof predicting the next token mimics the test setting (here question-answering) with non-negligible\nfrequency.\nImproving performance with few-shot learning This zero-shot transfer to visual question-\nanswering via prompting improves by presenting examples to the model in-context. We repeat\nthe previous experiments with up to four examples of image-question-answer triples shown to the\n6\nmodel as conditioning information in the continuous prompt sequence (using the interface in Figure 3).\nWe present these few-shot results compared to mixing in data from the VQAv2 training set – for SGD\ntraining – in Table 1. Of course, few-shot learning on four examples is outperformed by SGD on\ntens of thousands of examples, but few-shot performance clearly improves with more examples and\ngoes a decent way toward closing the gap from zero-shot performance (29.5%) to full SGD training\nperformance (48.4%). With just four examples the gap is closed almost halfway at 38.2%.\nThere are two important takeaways from the results presented in this section. First, they show that\ntraining a visual encoder through a pretrained and frozen language model results in a system capable\nof strong out-of-distribution (zero-shot) generalization. Second, they conﬁrm that the ability to\nrapidly adapt to new tasks given appropriate prompts is inherited from the pretrained language model\nand transfers directly to multimodal tasks.\n4.2 Encyclopedic Knowledge\nHere we study the extent to which Frozen can leverage the encyclopedic knowledge in the language\nmodel towards visual tasks. The Conceptual Captions dataset is hypernymed meaning that e.g. proper\nnames are replaced with a general word likeperson. This enables us to rigorously study the transfer of\nfactual knowledge because all knowledge of named entities comes from language model pretraining.\nConsequently, when we show the model an image of an airplane and ask “who invented this?”\n(Figure 1), the visual encoder has determined that the image contains an airplane, and the language\nmodel has used this to retrieve the factual knowledge that airplanes were invented by the Wright\nbrothers, a fact which is referenced in the C4 training set through (text-only) articles about airplanes.\nThis is a fascinating chain of deduction. A detailed analysis of this behaviour with more examples is\nincluded in the Appendix (e.g. Figure 9, Figure 10, Figure 11).\nWe bolster this ﬁnding quantitatively by evaluating performance on OKVQA [26], a visual question-\nanswering dataset designed to require outside knowledge in order to answer correctly. The pretrained\nlanguage model’s command of factual knowledge is of course dependent upon its scale, so we examine\nthe performance of Frozen using pretrained language models of varying sizes: the base model with\n7 billion parameters, and a much smaller 400 million parameter language model pretrained on the\nsame dataset. Table 2 shows the results: task performance scales with model size. Again ﬁnetuning\nperforms worse than leaving the model frozen in terms of generalization performance. We stress that\nFrozen is never trained on OKVQA.\n4.3 Fast Concept Binding\nIn the multi-modal setting, fast-binding refers to a model’s ability to associate a word with a visual\ncategory in a few shots and immediately use that word in an appropriate way.\nOpen-Ended miniImageNet and Real-Name miniImageNet To quantify the fast-binding capac-\nity of of Frozen, we evaluate it on the minImageNet meta-learning task [ 41]. Note that there are\nimportant differences with how we attempt miniImageNet and how it is approached in previous work.\nFirst, unlike standard meta-learning, we do not train Frozen on the (meta) task. Second, we evaluate\nFrozen in an open-ended fashion, where it must successfully generate a correct category name (and\nthen the EOS token) in order to be credited with a correct answer. Finally, although we use the same\nimage classes as the miniImageNet test set, they are at higher resolution (224×224) and with class\nlabels replaced with nonsense words (‘dax’, ‘blicket’ etc). This allows the system to express its\nanswers with word-like tokens. We refer to this task as Open-Ended miniImageNet, and it mimics\nclosely the standard miniImagenet setting used elsewhere. To assess how much difﬁculty is added by\nbinding visual categories to nonsense words versus simply adapting to an image recognition task per\nse, we also consider a version – Real-Name miniImagenet – in which visual categories in both the\nsupport set and the answer retain their original names. See Figure 4a for an illustration.\nOn both versions of this evaluation, we experiment by exposing the model to different numbers of\ninner-shots, repeats and task induction. On two-way Open-Ended miniImagenet, we observe that\nwhen Frozen is presented with a sequence of images and descriptions of new names for them, it is\nable to learn new names for the objects presented and then use these new names immediately with\nsubstantially above chance accuracy. Importantly, the ability of the model to use these new words\nimproves with with more examples of the corresponding category. Notably, this upward trend is more\n7\npronounced when this supporting information involves different exemplars from the visual category\n(inner-shots) rather than repetitions of a single exemplar (repeats). The fast-binding capacities of the\nmodel can thus be improved with richer and more varied visual support or prompting.\nOn two-way Real-Name miniImagenet, we observe a similar trend but with higher absolute perfor-\nmance. This underlines the difﬁculty in Open-Ended miniImagenet introduced by having to assign\nnovel words to categories that may otherwise be already known to the model, and because the real\nnames may carry visual information leveraged from the captioning data the model was trained on.\nIn Table 4, we show that the observed effects on Open-Ended miniImagenet do not transfer to the\n5-way setting, where Frozen is not signiﬁcantly above chance. This shows that learning to bind ﬁve\nnew names to ﬁve visual categories in a single forward pass is beyond the current capabilities of\nFrozen. As before, however, we do observe an upward trend in the model’s capacity to return the\nactual name for a visual category among the ﬁve possibilities as the number of inner-shots or repeats\nincreases. Further work is required and we look forward to progress in this more challenging setting.\nTask Induction \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013\nInner Shots 1 1 3 5 1 1 1\nRepeats 0 0 0 0 1 3 5\nFrozen 29.0 53.4 57.9 58.9 51.1 57.7 58.5\nFrozen (Real-Name) 1.7 33.7 66 66 63 65 63.7\nFrozen test-blind – 48.5 46.7 45.3 – – –\nFrozen test-blind (Real-Name) – 1.0 12.6 33.0 – – –\nANIL Baseline [31] – 73.9 81.7 84.2 – – –\nTable 3: Performance ofFrozenand baselines on Open-Ended miniImageNet 2-Way Tasks. Randomly\npicking between the two class labels (then emitting the EOS token) would yield 50% accuracy. As\nthe model has to generate the answer, and is not counted correct if it paraphrases, this is not the best\nblind baseline, which is why we include open-ended blind baselines that also generate.\nTask Induction \u0017 \u0013 \u0013 \u0013 \u0013 \u0013 \u0013\nInner Shots 1 1 3 5 1 1 1\nRepeats 0 0 0 0 1 3 5\nFrozen 18.0 20.2 22.3 21.3 21.4 21.6 20.9\nFrozen (Real-Name) 0.9 14.5 34.7 33.8 33.8 33.3 32.8\nFrozen test-blind – 18.6 19.9 19.8 – – –\nFrozen test-blind (Real-Name) – 4.6 22.6 20.8 – – –\nANIL Baseline [31] – 45.5 57.7 62.6 – – –\nTable 4: Performance ofFrozenand baselines on Open-Ended miniImageNet 5-Way Tasks. Randomly\npicking between the ﬁve class labels (then emitting the EOS token) would yield 20% accuracy.\nFast-VQA and Real-Fast-VQA As transformers are trained to model text, their attention weights\nlearn to associate – or ‘bind’– pairs of words across sentences. The experiments with miniImageNet\nshow that this capacity can transfer directly to binding visual categories to their names, enabling the\nsystem to generate the name on demand. This raises the question of whether Frozen can integrate a\nnewly-acquired visual category (and its names) more fully into the model’s language system, so that\nit can, for instance, describe or answer questions about that category.\nTo test this capacity, we constructed a new task – Fast-VQA – out of two well-known datasets,\nImageNet [34] and Visual Genome [16]. For each question, the model is presented with nonsense\nwords (‘dax’ and ‘blicket’) andnimages of the referents of those words (e.g. of a ‘cat’ or a ‘dog’)\ntaken from ImageNet. It is then asked a question containing at least one of those two words, about a\nfurther image (taken from Visual Genome) in which both of the referents appear (see Figure 4b). As\nwith miniImagenet, the words ‘dax’ and ‘blicket’ (and how they refer) should be new toFrozen, but\nthe corresponding visual categories may be known from the Conceptual Captions training data, albeit\nby different names.\n8\nTo quantify how much harder the introduction of new words for known categories makes this task, we\nalso created a variant (Real-Fast-VQA) in which the original category names (‘cat’ or ‘dog’) are used\ninstead of ‘dax’ and ‘blicket’. Real-Fast-VQA is a special case of VQA involving questions from\nVisual Genome, in which a model is reminded what the important entities in the question look like\nprior to answering the question. Real-Fast-VQA does not require the same ability to bind categories\nto new words, but it does measure how well a model can exploit task-relevant multimodal guidance\nwhen attempting a new task in an otherwise zero-shot manner.\nFast-VQA and Real-Fast-VQA are very challenging tasks because they are attempted without task-\nspeciﬁc training, and because the underlying questions come from Visual Genome (VQAv2 images\ndo not come with the necessary meta-data to construct the task). Visual Genome questions are\nparticularly challenging because only a single answer exists for each question. When scoring models,\nfor simplicity we credit only an exact match with the output generated by the model, modulo the same\npost-processing applied for VQAv2. Because of the inherent difﬁculty of the task, we use strong\nbaselines to verify strength of observed effects. The Fast-VQA and Real-Fast-VQA evaluation sets\nwill be provided with the camera ready version of this manuscript, as a resource to stimulate further\nresearch on multimodal fast-binding, together with training data (not used in this work).\nFast-VQA Real-Fast-VQA\nInner Shots 0 1 3 5 0 1 3 5\nFrozen 1.6 2.8 7.0 7.9 3.7 7.8 10.1 10.5\nFrozen train-blind 0.7 0.3 1.3 0.4 1.9 2.3 3.7 3.7\nTable 5: Performance of Frozen versus an equivalent blind model on Fast and Real-Fast VQA.\nAs shown in Table 5, the fact that the model improves with more shots in both Fast-VQA and Real-\nFast-VQA conﬁrms that Frozen has some capacity to integrate novel words into its general capacity to\nprocess and generate natural language in a multimodal context. It is notable that a preﬁx-tuned model\nwith no access to images improves moderately at Real-Fast-VQA as more concepts are presented,\nshowing that additional linguistic cues (just being reminded of the words involved and the linguistic\nform of the task) goes some way to preparing for the upcoming question. As exempliﬁed in Figure 4,\ninspection of the model output conﬁrms that in many cases it is indeed the multimodal (and not just\nlinguistic) support that enables Frozen to improve performance as the number of shots increases.\n5 Discussion\n5.1 Limitations\nWe believe this work is an important proof-of-concept for a desired, much more powerful system\ncapable of open-ended multimodal few-shot learning. Frozen achieves the necessary capacities to\nsome degree, but a key limitation is that it achieves far from state-of-the-art performance on the\nspeciﬁc tasks that it learns in a few shots, compared to systems that use the full training set for those\ntasks. As such, the main contribution of this work should be seen as a starting point or baseline for\nthis exciting area of research of multimodal few-shot learning.\nFurther improvement can make the impressive zero-shot and few-shot generalization we observed\nmore robust as reﬂected by higher accuracy and fewer seeds required to demonstrate our most\ncompelling samples. Finally, there are many technical questions that were not explored in this proof-\nof-concept study, such as whether performance could be improved with more elaborate architectures\nfor mixing vision and language. We leave the exploration of these possibilities to future investiga-\ntions. The Open-Ended miniImageNet, Real-Name miniImagenet, Fast-VQA and Real-Fast-VQA\nbenchmarks that we will provide with the camera ready version of this manuscript should facilitate\nthe evaluation and analysis of future systems of this type.\n5.2 Conclusion\nWe have presented a method for transforming large language models into multimodal few-shot\nlearning systems by extending the soft-prompting philosophy of preﬁx tuning [22] to ordered sets of\nimages and text while preserving text prompting abilities of the language model. Our experiments\n9\nconﬁrm that the resulting system, Frozen, is capable both of open-ended interpretation of images and\ngenuinely multimodal few-shot learning even though the system is only trained to do captioning. One\ncorollary of these results is that the knowledge required to quickly bind together or associate different\nwords in language is also pertinent to rapidly binding language to visual elements across an ordered\nset of inputs. This ﬁnding extends the conclusion of [25] – that knowledge in transformer language\nmodels can transfer to non-linguistic tasks – to the speciﬁc case of knowledge about few-shot learning.\nAcknowledgements We wish to thank Sebastian Borgeaud and Jack Rae for preparing the pre-\ntraining text dataset and pretraining a selection of transformer language models, as well as Trevor\nCai for help with experiments and infrastructure. We also wish to thank Pauline Luc, Jeff Donahue,\nMalcolm Reynolds, Andy Brock, Karen Simonyan, Jean-Baptiste Alayrac, Antoine Miech, Charlie\nNash, Aaron van den Oord, Marc Deisenroth, Aida Nematzadeh, Roman Ring, Francis Song, Eliza\nRutherford, Kirsty Anderson, Esme Sutherland, Daan Wierstra, and Nando de Freitas for insightful\ndiscussions during the course of the project.\nReferences\n[1] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppi-\nlan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V . Le. Towards a\nhuman-like open-domain chatbot. CoRR, abs/2001.09977, 2020.\n[2] Emily M Bender and Alexander Koller. Climbing towards nlu: On meaning, form, and\nunderstanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5185–5198, 2020.\n[3] Andrew Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-\nscale image recognition without normalization. arXiv preprint arXiv:2102.06171, 2021.\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[5] David Chalmers. Gpt3 and general intelligence. Published in Daily Nous, 2021.\n[6] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efﬁcient adap-\ntation of pretrained language models for image captioning. arXiv preprint arXiv:2102.10407,\n2021.\n[7] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. arXiv preprint arXiv:2102.02779, 2021.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[9] Allen Institute for AI. C4 search. https://c4-search.apps.allenai.org/. Accessed:\n2021-04-06.\n[10] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question answering.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n6904–6913, 2017.\n[11] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm:\nRetrieval-augmented language model pre-training, 2020.\n[12] Tracy H Heibeck and Ellen M Markman. Word learning in children: An examination of fast\nmapping. Child development, pages 1021–1034, 1987.\n[13] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehen-\nsive survey of deep learning for image captioning.ACM Computing Surveys (CsUR), 51(6):1–36,\n2019.\n[14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V . Le, Yunhsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision, 2021.\n10\n[15] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder\nBajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre luc Cantin,\nClifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb,\nTara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard\nHo, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\nJaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve Lacy, James\nLaudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin,\nGordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi\nNarayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda,\nAndy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory\nSizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory\nThorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang,\nEric Wilcox, and Doe Hyun Yoon. In-datacenter performance analysis of a tensor processing\nunit, 2017.\n[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International journal of\ncomputer vision, 123(1):32–73, 2017.\n[17] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[18] Georgia Tech Visual Intelligence Lab. Vqa python api and evaluation code. https://github.\ncom/GT-Vision-Lab/VQA.\n[19] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient\nprompt tuning. arXiv preprint arXiv:2104.08691, 2021.\n[20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\n[21] Sheng Li, Zhiqiang Tao, Kang Li, and Yun Fu. Visual to text: Survey of image and video\ncaptioning. IEEE Transactions on Emerging Topics in Computational Intelligence, 3(4):297–\n312, 2019.\n[22] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation.\narXiv preprint arXiv:2101.00190, 2021.\n[23] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics\naligned pre-training for vision-language tasks, 2020.\n[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265,\n2019.\n[25] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 2021.\n[26] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge. In2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 3190–3199. IEEE Computer Society,\n2019.\n[27] Fabio Petroni, Tim Rocktäschel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H.\nMiller, and Sebastian Riedel. Language models as knowledge bases? CoRR, abs/1909.01066,\n2019.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\n[29] Alec Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\n[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.\n11\n[31] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature\nreuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157,\n2019.\n[32] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning, 2016.\n[33] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the\nparameters of a language model? arXiv preprint arXiv:2002.08910, 2020.\n[34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115(3):211–252, 2015.\n[35] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 2556–2565, 2018.\n[36] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-\nsentations, 2018.\n[37] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism, 2020.\n[38] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert:\nPre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 ,\n2019.\n[39] Yingtao Tian and Jesse Engel. Latent translation: Crossing modalities by bridging generative\nmodels, 2019.\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.arXiv preprint arXiv:1706.03762,\n2017.\n[41] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra.\nMatching networks for one shot learning. arXiv preprint arXiv:1606.04080, 2016.\n[42] Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation\nfor knowledge-based vqa, 2021.\n[43] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-\ners, 2021.\n[44] Zachary M Ziegler, Luke Melas-Kyriazi, Sebastian Gehrmann, and Alexander M Rush. Encoder-\nagnostic adaptation for conditional language generation. arXiv preprint arXiv:1908.06938,\n2019.\n12\nA Appendix\nA.1 Compute Usage\nThe seven billion parameter language model we used as part of Frozen used model parallelism with\nthe strategy from [37] to partition one instance of the model over four accelerators. Each instance\nhad a batch size of 8. To reach a batch size of 128 in this conﬁguration, we additionally employed\ndata parallelism with 16 synchronous replicas. The whole system was trained on a 4x8 TPUv3 [15]\ntopology for about 12 hours, which is when validation set performance for Conceptual Captions led\nus to do early stopping.\nA.2 Frozen Architecture Details\nThe pretrained transformer language model we used has a GPT-like architecture [ 29]. It consists\nof a series of identical residual layers, each comprised of a self-attention operation followed by a\npositionwise MLP. The only deviation from the architecture described as GPT-2 is the use of relative\nposition encodings [36]. Our seven billion parameter conﬁguration used 32 layers, with each hidden\nlayer having a channel dimensionality of 4096 hidden units. The attention operations use 32 heads\neach with key/value size dimensionality of 128, and the hidden layer of each MLP had 16384 hidden\nunits. The 400 million parameter conﬁguration used 12 layers, 12 heads, hidden dimensionality of\n1536, and 6144 units in the MLP hidden layers.\nA.3 Few-Shot Learning Deﬁnitions\nAs Frozen can be conditioned on a sequence of interleaved images and text, it is capable not only\nof performing on a variety of multimodal tasks, but also, the same task can be induced in multiple\nways to help Frozen to learn and perform better. In order to make it easier to distinguish among these\ndifferent ways of ’inducing’ a task to the model, we have formalized the terminology used in our\nsettings, which is described in section 3.4 of the main text. In Figure 5 and Figure 6 below we provide\nmore visual examples of this terminology.\nA.4 Tasks to Evaluate Fast-Binding Capacity\nA.4.1 Open-Ended MiniImageNet\nTo construct the Open-Ended MiniImagenet evaluation we begin with the same subsetSof ImageNet\nclasses applied in prior on meta-learning with MiniImagenet (See the appendix of [32]). All images\nare taken from the validation set of ImageNet.\nTo generate a 2-way question with ninner-shots, the following process is followed:\n1. Sample two classes c1,c2 from S\n2. Sample nimages vc1\n1 ...v c1\nn+1 from c1 and nimages vc2\n1 ...v c2\nn from c2\n3. Interleave into a sequence of 2nsupport images [vc1\n1 ,vc2\n1 ...v c1\nn ,vc2\nn ]\n4. Assign the nonsense words (dax, blicket) to c1,c2 at random, and interleave support captions\n\"this is a dax\" or \"this is a blicket\" accordingly\n5. Select one of c1,c2 at random, cq, and sample a further question image vcq\n6. Assign the truncated caption \"this is a\" to vq and the appropriate nonsense word as the\ncorrect answer.\nNote that this process ensures that the image class and nonsense word assigned to the correct answer\noccur in either ﬁrst or second place in the support, and the correct answer may be dax or blicket with\nequal probability.\nTo generate a 5-way question, the above process is generalized. In 1. ﬁve distinct classes are sampled\nfrom S. The set of nonsense words applied in step 4. and 6 is: [ dax, blicket, slation, perpo, shously].\nThe ﬁnal three words were taken from a nonsense-word generator1 and selected because, like dax\nand blicket and for consistency, they decompose into two tokens in our model’s subword vocabulary.\n1https://www.soybomb.com/tricks/words/\n13\n0-repeats\n1-shot\nQuestion: What \nis this? \nAnswer:\nShots Question\nPlease answer \nthe question.\nQuestion: What is \nthis? Answer: Big \nBen\n1-repeats\n1-shot\nQuestion: What \nis this? \nAnswer:\nShots Question\nPlease answer \nthe question.\nrepeat 0 repeat 1\nQuestion: What is \nthis? Answer: Big \nBen\nQuestion: What is \nthis? Answer: Big \nBen\nQuestion: What \nis this? \nAnswer:\nShots Question\nPlease answer \nthe question.\n0-repeats\n2-shots\nShots\nPlease answer \nthe question.\nQuestion: What is \nthis? Answer: Big \nBen\nQuestion: Type \nof animal? \nAnswer: dog\nshot 1 shot 2\nTask Induction\nPlease answer \nthe question.\nTask Induction\nPlease answer \nthe question.\nTask Induction\nPlease answer \nthe question.\nshot 1\nQuestion: What \nis this? \nAnswer:\nQuestion\nTask Induction\nAnswer with \nlion or dog.\n0-repeats\n0-shots\nFigure 5: Examples of few-shot learning vocabulary.\nAll images are stored at 224 ×224 resolution.\nA.4.2 Real-Name miniImageNet\nTo generate Real-Name miniImagenet, the same process is followed, except that in steps 4. and 6.,\ninstead of using nonsense words to caption the support images (e.g. \"this is a dax\"), the (ﬁrst) class\nname from the ImageNet dataset is used (e.g. \"this is a fruit bat\").\nA.4.3 Fast-VQA\nUnlike Open-Ended miniImageNet, Fast-VQA uses images from all 1,000 classes in the ImageNet\ndataset. For the evaluations in this paper, we again only take images from the validation set. Denote\nby W the set of all 1,000 class (ﬁrst) names, and for each wi ∈W, the corresponding set of images\nci.\nThe Visual Genome (VG) dataset contains meta-data, questions and answers, such that we can\nconsider data in the form (Im,q,a,Ob ), where Im is the image, qis the corresponding question, a\nis the answer and Obis a list of names for all objects annotated in Im. We ﬁrst ﬁltered the dataset\ninto a subset VG∗such that every question qk contained at least one word wi ∈W and such that\nthe corresponding object list Obk also contained qk and at least one other word wj ∈W : wj! =wi.\nThus, we can consider the elements of VG∗to be of the form (Im,q,a,Ob,w i,wj)\n14\n0-repeats\n0-shots\n2-way\n1-inner-shot\n0-repeats\nThis is a \nblicket.\nThis is a dax.\n This is a \nblicket.\nThis is a dax.\n Q: What is this? \nA: This is a\nQuestion\n0-repeats\n0-shots\n2-way\n2-inner-shots\n0-repeats\n0-repeats\n0-shots\n2-way\n1-inner-shot\n1-repeat\nTask Induction\nAnswer with dax \nor blicket.\nThis is a \nblicket.\nThis is a dax.\n Q: What is this? \nA: This is a\nQuestion\nTask Induction\nAnswer with dax \nor blicket.\n0-repeats\n1-shot\n2-way\n1-inner-shot\n0-repeats\nThis is a \nblicket.\nThis is a dax.\n Q: What is this? \nA: This is a\nSupport Question\nTask Induction\nAnswer with dax \nor blicket.\nThis is a \nblicket.\nThis is a dax.\nThis is a \nblicket.\nThis is a dax.\n Q: What is this? \nA: This is a\nSupport Question\nTask Induction\nAnswer with dax \nor blicket.\nThis is a \nblicket.\nThis is a dax.\n Q:What is this? \nA:This is a dax.\nShots\nAnswer with dax \nor blicket.\nway 1 way 2\ninner-shot 1 inner-shot 2inner-shot 1 inner-shot 2\nrepeat 0 repeat 1repeat 0 repeat 1\nshot 1\nSupport\nSupport\nFigure 6: Examples of few-shot learning vocabulary for fast-binding.\nTo generate a 2-way, n-shot Fast-VQA question out of an element (Im,q,a,Ob,w i,wj), we then\ndid the following:\n1. Sample nimages vci\n1 ...v ci\nn+1 from c1 and nimages vcj\n1 ...v cj\nn from c2\n2. Depending on coin toss, form either the support [vci\n1 ,vcj\n1 ...v ci\nn ,vcj\nn ] or the support\n[vcj\n1 ,vci\n1 ...v cj\nn ,vci\nn ]\n3. Assign the nonsense words ( dax, blicket) to wi,wj at random, and interleave support\ncaptions \"this is a dax\" or \"this is a blicket\" accordingly\n4. Transform qand ainto modiﬁed questions and answers q∗and a∗by replacing all instances\nof wi and any instances of wj with the corresponding strings dax or blicket\n5. Append the (VG) question (Im,q∗,a∗) to the (ImageNet) support from 2. to create the\nFast-VQA sample.\nIn this work, we only consider 2-way Fast-VQA.\nA.4.4 Real-Fast-VQA\nTo generate Real-Fast-VQA, the same process is followed, except that in step 3. the (ﬁrst) class name\nfrom ImageNet is used to caption the support images (\"this is a cat\", \"this is a wolf\"), and no string\nreplacement is undertaken in 4.\nLinks to download Open-Ended miniImageNet, Real-Name miniImageneNet, Fast-VQA and\nReal-Fast-VQA will be made available soon.\n15\n2-way\n1-shot\n0-repeats\n0-episodes\nThis is a \nblicket.\nThis is a dax. Q: What is the \ndax made of? A: \nSupport \nfrom ImageNet\nQuestion \nfrom VisualGenome\nCorrect Answer\nwood\nblicket (vase)\ndax (table)\nFigure 7: Example of a Fast-VQA task.\nA.5 Encyclopedic Knowledge\nHere we add more detail to the claim in subsection 4.2 that the model seems to be performing a sort\nof multi-hop deduction in the “Wright Brothers” example from Figure 1.\nFirst, there has been a substantial amount of recent work studying a language model’s ability to draw\nupon factual knowledge, examining the ability of language models to answer factual questions either\nzero-shot [27, 4] or after open-domain QA ﬁnetuning [33, 11, 20]. Buoyed by these ﬁndings, we here\ndemonstrate rigorously the impressive extent to which Frozen seems to be commanding this factual\nknowledge and drawing upon it when prompted by an image (here an image of an airplane). We\nnow break down why it is interesting that the model correctly determines that the Wright Brothers\ninvented the object in the image (an airplane), by studying how the model responds to different\nprompts concerning this same test image in Figure 9.\nRecall that Conceptual Captions is hypernymed so none of the language targets used to train Frozen\ncontain named entities like “The Wright Brothers”. Instead, our training signal teaches the model to\nemit text that would roughly describe an image. The impressive ﬁnding is that this scalable, weakly\nsupervised objective generalizes to general information retrieval about an image.\nThe top pane in Figure 9 shows an example of what the text in the captioning distribution looks like,\ncaptioning the image as “an airplane ﬂying over a blue sky – stock photo #”. Now, as established\nin subsection 4.1 we enjoy some amount of zero-shot transfer from captioning to visual question-\nanswering. This is demonstrated in the second and third rows of Figure 9. But, adhering to the\ndistribution of caption text, the model does not give a named entity when asked who invented the\nairplane. Instead it completes the prompt vaguely by saying “This was invented by an aerospace\nengineer and is made by the brand he worked for”.\nBut we know for certain that the language model has learned plenty of facts about named entities\nduring pre-training and in particular we determined via the C4 dataset search tool [9] that there are\nmultiple articles concerning the Wright Brothers. It’s just that matching the distribution of Conceptual\nCaptions text has taught the model to not emit named entities when prompted with an image. But the\nmodel can recover the ability to refer to named entities given an image with few-shot learning (bottom\nrow of Figure 9). We show the model two examples of saying who invented an object depicted in an\nimage by giving a named entity (Zacharias Janssen invented the microscope and Henry Ford invented\nthe model T, an early automobile). With this prompt, Frozen reliably retrieves the correct factual\nknowledge, having determined in the vision encoder that the image depicts an airplane, and having\nbeen demonstrated in-context that the desired output is the name of a person.\nThis outcome is robust, in the sense that we observed it in multiple versions of Frozen during\ndevelopment, and in multiple examples, but drawing samples is not always successful and can require\n3-4 tries to get past well-known language model failure modes of either repeating prompt text or\nemitting completely unrelated text. That’s why we describe some samples as “curated”.\nWe reiterate that this is a fascinating chain of deduction and a huge generalization leap from the task\nthe model was trained to do, which is emit a caption for an image.\n16\nQuestion: Where is \nthe remote control? \nAnswer:\nContext \nModel Completion \nIt is under \nthe bed\nBlind Completion \nIt’s in the \nkitchen\nHuman Answers\nQuestion: Where is \nthe man? Answer:\nContext \nModel Completion \ntennis player\nBlind Completion \nHe is in the \nbathroom\nQuestion: Which \nanimal is this? \nAnswer:\nContext \nModel Completion \na dog\nBlind Completion \nit is a monkey\nHuman Answers\nQuestion: What \nanimals are these? \nAnswer:\nContext \nModel Completion \na herd of \nbison\nBlind Completion \nThey are \nalligators\nHuman Answers\ncows \ncows\ncows\ncows\ncows\nreindeer\ncows\nantelope\ncow\ncow\ndog\ndog\ndog\ndog\ndog\ndog\ndog\nblack lab\ndog\ndog\nin hand\nin mans hand\nin his hand\nhand\nmans hand\nmans hand\nleft hand\nin mans hand\nin his hand\nin hand\nHuman Answers\ntennis court\ntennis court\ntennis court\nplaying tennis\ntennis court\ncourt\non tennis court\ntennis court\ntennis court\ntennis court\nFigure 8: VQA qualitative. This is a greedy sample of our model’s prediction on a VQAv2 validation\nset example. See accuracy numbers in Table 1 for overall robustness.\n17\nThis was invented by \nZacharias Janssen.\nThis was invented by \nHenry Ford.\nThis was invented by\nContext \nModel Completion \nthe Wright \nbrothers.\nThis was invented by\nContext \nModel Completion \nan aerospace \nengineer and \nis made by the \nbrand he \nworked for.\nContext \nModel Completion \na supersonic \naircraft over \nthe skyline.\nContext \nModel Completion \nan airplane \nflying over a \nblue sky -- \nstock photo #\nWhat is this?\nFigure 9: Encyclopedic Knowledge. Shows the model retrieving factual knowledge given visual input.\nRequired cherry-picking from around 5 seeds to get past common language model failure modes like\nsimply repeating text from the prompt or emitting text that does not pertain to the test image.\n18\nThis is a \n!\n. This is a \n\"\n. This is a\nContext \nModel Completion \n#\n. <EOS>\nFigure 10: Emojis. This sample reliably produced good output within a few attempts but did not\nwork for every seed.\nQuestion: I have \nthis problem, what \ncan I do? Answer:\nContext \nModel Completion \nIf you can \nremove it, \ngently rub the \nscuff with a \nclean white \ncloth. <EOS>\nFigure 11: Encyclopedic Knowledge. Demonstrates knowledge from language pre-training being\ncommanded given visual input. Required a few seeds to get a good answer which clearly paid\nattention to the image.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7825403213500977
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.7612557411193848
    },
    {
      "name": "Language understanding",
      "score": 0.617945671081543
    },
    {
      "name": "Task (project management)",
      "score": 0.5985010266304016
    },
    {
      "name": "Encoder",
      "score": 0.5889089703559875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5778552889823914
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5507708787918091
    },
    {
      "name": "Prefix",
      "score": 0.5479892492294312
    },
    {
      "name": "Natural language processing",
      "score": 0.5412956476211548
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5021946430206299
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4833683371543884
    },
    {
      "name": "Language model",
      "score": 0.482511043548584
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4794687032699585
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.45522865653038025
    },
    {
      "name": "Question answering",
      "score": 0.44986701011657715
    },
    {
      "name": "One shot",
      "score": 0.4216669499874115
    },
    {
      "name": "Linguistics",
      "score": 0.1384258270263672
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45129253",
      "name": "University College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1288882113",
      "name": "Boston Children's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}