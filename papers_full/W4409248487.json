{
  "title": "VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference",
  "url": "https://openalex.org/W4409248487",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2111492569",
      "name": "Zihan Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2970768646",
      "name": "Xinhao Luo",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2113025031",
      "name": "Jun-xian Guo",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2121075891",
      "name": "Wentao Ni",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2483248650",
      "name": "Yangjie Zhou",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2108427779",
      "name": "Yue Guan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2147303419",
      "name": "Cong Guo",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A2224962165",
      "name": "Weihao Cui",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2099342259",
      "name": "Yu Feng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2162841773",
      "name": "Minyi Guo",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2236082114",
      "name": "Yuhao Zhu",
      "affiliations": [
        "University of Rochester"
      ]
    },
    {
      "id": "https://openalex.org/A2103568674",
      "name": "Minjia Zhang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2027462004",
      "name": "Chen Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226896803",
      "name": "Jingwen Leng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2207050309",
    "https://openalex.org/W4382809751",
    "https://openalex.org/W1977182282",
    "https://openalex.org/W6789646864",
    "https://openalex.org/W6751349269",
    "https://openalex.org/W6854308872",
    "https://openalex.org/W6838322825",
    "https://openalex.org/W6860493679",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W6846164622",
    "https://openalex.org/W6893640197",
    "https://openalex.org/W2077815765",
    "https://openalex.org/W6810610777",
    "https://openalex.org/W4366341968",
    "https://openalex.org/W4308083739",
    "https://openalex.org/W4394998892",
    "https://openalex.org/W4327930469",
    "https://openalex.org/W6676912532",
    "https://openalex.org/W6860936625",
    "https://openalex.org/W2124509324",
    "https://openalex.org/W1976046733",
    "https://openalex.org/W4387321091",
    "https://openalex.org/W4280568654",
    "https://openalex.org/W4406650295",
    "https://openalex.org/W6866033557",
    "https://openalex.org/W6685526727",
    "https://openalex.org/W4214512541",
    "https://openalex.org/W4395020674",
    "https://openalex.org/W4385585365",
    "https://openalex.org/W3117512411",
    "https://openalex.org/W2556145167",
    "https://openalex.org/W6799372109",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4239893399",
    "https://openalex.org/W6857288518",
    "https://openalex.org/W6870125966",
    "https://openalex.org/W6856696905",
    "https://openalex.org/W3007718266",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6861393904",
    "https://openalex.org/W4392182282",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4234854340",
    "https://openalex.org/W2002555321",
    "https://openalex.org/W6868895731",
    "https://openalex.org/W6847478871",
    "https://openalex.org/W6852581948",
    "https://openalex.org/W4408847395",
    "https://openalex.org/W6861839547",
    "https://openalex.org/W6767440493",
    "https://openalex.org/W6875634212",
    "https://openalex.org/W6811340617",
    "https://openalex.org/W6866496068",
    "https://openalex.org/W4280633999",
    "https://openalex.org/W6811928498",
    "https://openalex.org/W6859846904",
    "https://openalex.org/W6859466964",
    "https://openalex.org/W4360831828",
    "https://openalex.org/W6849058233",
    "https://openalex.org/W3205706264"
  ],
  "abstract": "In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.",
  "full_text": "arXiv:2503.02236v2  [cs.DC]  30 Jun 2025\n2025 IEEE International Symposium on High-Performance Computer Architecture (HPCA)\nVQ-LLM: High-performance Code Generation for\nVector Quantization Augmented LLM Inference\nZihan Liu1,2, Xinhao Luo 1,2, Junxian Guo 1, Wentao Ni1, Yangjie Zhou1, Yue Guan1,2, Cong Guo 3,\nWeihao Cui1,4, Yu Feng1, Minyi Guo 1,2, Yuhao Zhu5, Minjia Zhang 6, Chen Jin 7, Jingwen Leng 1,2,∗\n1Shanghai Jiao Tong University, 2Shanghai Qi Zhi Institute, 3Duke University, 4National University of Singapore\n5University of Rochester, 6University of Illinois Urbana-Champaign, 7Magik Compute\n{altair.liu, lxh666, guojunxian, wennitao, yj zhou, bonboru, weihao, y-feng }@sjtu.edu.cn, cong.guo@duke.edu\nguo-my@cs.sjtu.edu.cn, yzhu@rochester.edu, minjiaz@illinois.edu, chenj@magikcompute.ai, leng-jw@cs.sjtu.edu.cn\nAbstract—Vector quantization (VQ), which treats a vector\nas a compression unit, gains increasing research interests for\nits potential to accelerate large language models (LLMs). Com-\npared to conventional element-wise quantization methods, VQ\nalgorithms can compress weight and KV cache tensors in LLMs\nwith a greater ratio while maintaining the high model accuracy.\nHowever, translating a VQ algorithm’s memory reduction into\nthe actual latency improvement is challenging. We profile and\nanalyze the current approach of integrating VQ into computation\nkernels and show that its major inefficiency lies in the poor access\nefficiency of codebooks in VQ algorithms and uncoordinated\ncomputation dataflow. Meanwhile, the diversity of VQ algo-\nrithms (e.g., different vector sizes and entry counts) and LLMs’\ncomputation kernels (e.g matrix-matrix/vector multiplication and\nattention computation) makes it impractical to manually craft\nefficient kernel implementations for each specific case.\nIn this work, we design and implement VQ-LLM, an efficient\nfused VQ kernel generation framework. We first introduce a\nsoftware abstraction called codebook cache to optimize codebook\naccess efficiency and support the integration of VQ with various\ncomputations. The codebook cache adaptively stores different\nentries across the GPU’s memory hierarchy, including off-chip\nglobal memory, on-chip shared memory, and registers. Centered\naround the codebook cache, we design an efficient computation\nengine that optimizes memory traffic during computations in-\nvolving codebooks. This compute engine adopts the codebook-\ncentric dataflow and fusion optimizations. Additionally, we pro-\nvide adaptive heuristics to tailor parameter selection in our\noptimizations to diverse VQ configurations. Our optimizations\nachieve the latency reduction of 64.36% to 99.1% compared to\nexisting open-source implementations. A final comparison with\nstate-of-the-art element-wise quantization methods like A WQ and\nQoQ shows that our VQ-LLM is practically viable, achieving\nlatencies close or even better latencies to those at equivalent bit-\nwidths, potentially offering greater accuracy.\nI. I NTRODUCTION\nWith the great success of large language models (LLMs),\nneural networks are placing significant pressure on current\nhardware, especially memory systems [21], [27], [28], [33].\nQuantization techniques become essential for deploying these\nlarge models [15], [18]–[20], [24], [30], [31], [51], [62].\nQuantization reduces the original IEEE-754 half format FP16\ndata to types with much narrower bit-widths, such as FP8\nand INT4, decreasing the memory footprint significantly [1].\nResearchers have developed numerous novel data formats\n*Jingwen Leng is the corresponding author of this paper.\nand algorithms, like MXFP and ANT, with varying scal-\ning granularities to represent the original data using fewer\nbits [20], [49]. However, these techniques treat each data point\nas an independent element for compression, overlooking the\npotential information between elements. As a result, these\nmethods typically reach a 4-bit limit; compressing to 2 bits\nor less leads to a substantial accuracy loss [12], [15], [56].\nUnder these scenarios, vector quantization (VQ) emerges\nas a pivotal technique to further reduce LLMs’ memory foot-\nprints [12], [56], [57], [67], [69]. The VQ methods compress\na vector of multiple elements into a single element and\nenabling the capture of information across elements [57], [69].\nTypically, this cross-element information is gathered through\nclustering, which involves applying a clustering algorithm to\nall vectors and using cluster centroids to represent nearby\nvectors [26], [37]. Furthermore, some researchers suggest\niteratively processing the residuals between the original and\nquantized data to enhance reconstruction quality [32], [63]. For\nLLMs, VQ achieves higher accuracy at the same 4-bit level or\nmaintains equivalent accuracy at 2-bits, and some approaches\ncan compress the KV cache in LLMs to 1-bit [69].\nDespite their appealing accuracy and compression ratios,\nVQ-augmented LLMs do not significantly enhance the model’s\nlatency performance in practice. Our analysis in Sec. III\nindicates that existing VQ methods have substantially higher\nlatency than conventional element-wise quantization methods,\noften performing worse than the original FP16 version. The\ninefficiencies primarily stem from how memory access and\ncomputation dataflow are managed when interacting with the\ncodebooks in VQ methods. We have identified three key chal-\nlenges that must be addressed to generate high-performance\nkernels integrating VQ with subsequent computations.\nThe first challenge lies in the placement of VQ’s codebooks.\nWe find that the common practice of storing all codebook\nentries in GPU shared memory increases shared memory\nusage, thereby reducing the number of thread blocks that\ncan concurrently operate on each SM, which diminishes\nperformance. Additionally, the number of codebook entries\nfar exceeds the number of available shared memory banks,\nleading to significant bank conflicts. The second challenge\ninvolves coordinating the loading of codebooks and subsequent\ncomputation. There is excessive traffic in loading the codebook\nfrom global memory to shared memory, and in transferring\ncodebook entries from shared memory to registers, which\nshould be much lower in theory. The reasons include multiple\nthread blocks loading duplicate codebooks, and the require-\nment for threads to store data reconstructed via codebook\nentries (we refer to them as dequantized data throughout the\npaper) back to shared memory in a layout that differs from\ntheir dequantization for subsequent computations. The last\nchallenge is that the diversity of VQ algorithms (e.g., dif-\nferent vector sizes and entry counts) and LLMs’ computation\nkernels (e.g matrix-matrix/vector multiplication and attention\ncomputation) makes it impractical to manually craft efficient\nkernel implementations for each specific case.\nTo address the challenges, this work develops VQ-LLM, an\nautomatic high-performance fused VQ kernel code generation\nframework. We begin by introducing a software abstraction\ncalled codebook cache, designed to optimize codebook access\nefficiency and facilitate the integration of VQ with various\ncomputations. This cache enables efficient codebook place-\nment across the GPU’s memory hierarchy. We have identi-\nfied that only a select few entries are accessed frequently.\nTherefore, rather than indiscriminately caching all entries in\nshared memory, we adopt a hierarchical approach: entries with\nlow access frequency remain in global memory, while those\naccessed more frequently are cached in shared memory. To\naddress inevitable bank conflicts, entries that are accessed\nextremely frequently are stored in thread-local registers, elim-\ninating bank conflict issues. Furthermore, to mitigate negative\nimpacts on computation (reduced concurrency), we utilize\navailable slacks, which ensues no drop in resource utilization,\nto adaptively determine the optimal placement of entries.\nCentering the codebook cache, we design an efficient com-\npute engine that optimizes memory traffic when computing\nwith codebooks, and it consists of two novel techniques. The\nfirst called codebook-centric dataflow divides and parallelizes\nthe original computation task in a way that minimizes the\ncodebook switch overhead. It may split the reduction dimen-\nsion of the original computation task, for which we adaptively\ndetermine the split factor to balance the global reduction.\nThe second technique, codebook-centric hierarchical fusion,\nextends the default shared memory level fusion to support\nthe additional register-level fusion. This mechanism leverages\na GPU feature known as intra-warp data exchange [45] to\nrearrange the dequantized data into the required layout for\nsubsequent computations directly in registers. We adaptively\ndecide where to conduct the fusion based on profiled exchang-\ning overhead and difference between layout of dequantized\ndata and layout required by subsequent computation.\nOur evaluation shows that VQ-LLM achieves the latency\nreduction of 64.36% to 99.1% compared to compared to\nexisting open-source implementations [13], [56]. We also\nperform extensive sensitivity to verify the effectiveness of each\ntechnique in our framework. A final comparison with state-\nof-the-art element-wise quantization methods like AWQ [30]\nand QoQ [31] shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at\n02 2 3 1 1 30\nVector size\n#Entry\nResiduals\n+ + + +\n=\n=\n=\n=\nQuantization\nDequantization\nConcat\nCodebooksEntries Codebooks\nFig. 1. Typical vector quantization pipeline.\nequivalent bit-widths, potentially offering greater accuracy.\nWe list our main contributions as follow:\n• To the best of our knowledge, we are the first to deeply dive\ninto performance issues of vector quantization and make it\npractically feasible in LLM inference.\n• We deliver a detailed analysis and identify these issues are\ncaused by inefficient codebook entries access and uncoordi-\nnated codebook loading and subsequent computation.\n• Based on the finding, we propose VQ-LLM to generate\nefficient fused VQ kernel implementation, it consists of\ncodebook cache and codebook based compute engine, with\nconfigurable parameters and adaptive heuristics.\n• We compare VQ-LLM with open-sources implementations\nand element-wise quantization works, with detail speed-up\nbreakdown analysis on proposed optimizations.\nII. B ACKGROUND AND RELATED WORKS\nThis section first introduces the basic concept of vector\nquantization and its applications in quantizing large language\nmodels. It then provides a detailed analysis of serving vector\nquantized large language models with existing solutions.\nA. Vector Quantization (VQ)\nCompared to traditional quantization, vector quantization\n(VQ) treats the vector of multiple elements as a unit and\nuses trained quantization points organized into codebooks\nto quantize the vector into a single element, rather than in\nan element-wise manner as in traditional quantization. This\ntechnique is widely used in vector database, nearest neighbor\nsearch, etc. [29], [34] VQ has several configurable parameters,\nhighlighted in Fig. 1, which allow it to be specified for\nproduct quantization (PQ), additive quantization (AQ), and\nhybrid quantization (PRQ) [4], [11], [17], [25]. Apart from\nthese, there are other techniques such as hash-based [23] and\nlattice-based methods [2]. However, these techniques either\ncannot reconstruct the original data or need to be used in\nconjunction with PQ, AQ, and PRQ. Therefore, we do not\ndelve into these techniques as they do not influence the core\nfindings and insights of this work.\nTypical VQ Pipeline. We use the example in Fig. 1 to\ndemonstrate the typical VQ pipeline, and numbers in (·)\nrepresent the value of parameters in this example. We also\nsummarize the VQ parameters in Tbl. I. First, the original 16-\ndimensional vectors are split into four sets of vector size (4) -\ndimensional sub-vectors. Next, we collect sub-vectors in one\nsub-space (or several sub-spaces, depending on algorithms)\nand conduct k-means clustering to group these sub-vectors\ninto #Entry (4) clusters. The original sub-vectors are then\nreplaced with the index of their closest cluster centroids, using\nlog2#Entry (2) bits. Next, we collect the differences between\nthe original sub-vectors and their closest cluster centroids as\nthe residuals. We then perform another round of k-means\nclustering and replace the residual sub-vectors with the index\nof the closest centroids of the new clusters. This process\nof residual quantization can be repeated, as determined by\nthe Residual (2) parameter. The quantization process is now\ncomplete, as shown in the upper part of Fig. 1. We then gather\nall the aforementioned cluster centroids and organize them\ninto codebooks. In the following sections, we refer to these\ncentroids as codebook entries.\nTo reconstruct the original data, a dequantization process is\nrequired, as shown in the lower part of Fig. 1. For each resid-\nual, we use its quantized data to look up the corresponding\ncodebooks and find the codebook entry indexed by the quan-\ntized data in each sub-space. We then gather the results from\nthe same sub-spaces across different residuals, typically via\nelement-wise accumulation. Finally, we concatenate the results\nfrom all sub-spaces. Throughout the entire process, vector size,\n#Entry, and Residual are configurable. These configurations\nare annotated with x,y,z, in the format of VQ<x,y,z>. In\nthis example, the configuration is VQ<4,2,2>.\nB. Large Language Models (LLMs)\nLLMs adopt the Transformer architecture [58], which is\npivotal in processing and generating natural language in se-\nquences of tokens. The core of the Transformer architecture is\nmulti-head attention (MHA), designed to run several parallel\nattention processes, allowing the model to simultanesly focus\non different types of information from a single input sequence.\nTABLE I\nPARAMETERS OF VQ ALGORITHMS\nItem Description Value in\nSec. III\nVector size Number of elements to quantize at once 4\n#Entry Number of quantization points (entries) 28\nResidual Number of times to quantize residual data 1\nElement-wise VQ\nGPT-VQ\nAWQ\n2-bit2-bit 3-bit3-bit 4-bit4-bit\nGPT-Q\nAWQ\nAQLM QuiP#\nΔPPL\n0\n2\n4\n6\nElement-wise VQ\nCQ\nKVQuant\nKVQuant\nQoQ\n2-bit2-bit\n4-bit4-bit\nCQ\nΔPPL\n0\n0.5\nQuantization points\nMSE=5.2e-3MSE=5.2e-3\nOutlierOutlier\n−2\n0\n2\n−2 0 2\nQuantization points\nMSE=3.2e-3MSE=3.2e-3−2\n0\n2\n−2 0 2\nFig. 2. (Upper) Accuracy of VQ and element-wise quantization, left is\nweight and right is KV cache quantization. (Lower) VQ (right) can better\ncapture the distribution of data than element-wise quantization (left), with\ninter-dimensions information.\nEach head in MHA can be thought of as an independent\nattention layer with its own learnable parameters. Outputs\nof these heads are then concatenated and fed to subsequent\noperations. Mathematically, MHA can be described as follows:\nMultiHead(Q, K, V) =Concat(head1, . . . ,headh)WO,\nheadi = Attention(Q = HW Q\ni , K= HW K\ni , V= HW V\ni ),\nAttention(Q, K, V) =softmax(QKT /\np\ndk)V.\nHere, WQ\ni , WK\ni , WV\ni , and WO are parameter matrices for\nthe i-th head and the output projection, respectively. And H\nis the hidden state. The softmax function is applied over the\nkeys to normalize their weights, ensuring that the output is a\nweighted sum of the values based on the input’s relevance.\nIn the context of text generation, LLMs often first im-\nplement a prefill stage where the model processes existing\ntokens before generating new ones. This sets the initial state\nof the model’s memory and attention mechanisms, making\nthe generation process more context-aware. Following this,\nthe decode phase begins, during which the model generates\none token at a time, updating its internal state based on\nboth the newly generated token and the preceding context.\nTo efficiently reuse previously computed token representations\nduring the decode phase, a Key-Value (KV) cache mechanism\nis often utilized [46], [68], enhancing inference performance.\nC. VQ for LLM Acceleration\nVQ gains increasing interests for its great potential for com-\npressing and accelerating LLMs. This is because LLMs are\nhighly memory-bound [61], with many researchers identifying\nweights and KV-cache as the main bottlenecks, accounting for\nover 95% of the memory footprint [28]. To further compress\nthe weights and KV-cache and reduce memory usage, VQ has\ncome to the center of the stage with its superior compression\nratio and reconstruction quality. Various newly proposed VQ-\nbased compression algorithms outperform SOTA element-\nwise quantization baselines in both weight-only compres-\nsion (AWQ [30]) and KV-cache compression (KVQuant [24],\nQoQ [31]) under the same equivalent bitwidth [12], [56],\nAttentionQ,K,V\nKV Cache\nOutput\nFP16\nattn\nAttentionQ,K,V\nKV Cache Quantized\nOutput\nDequantizeCodebook\n@ Global\nVQ attn\nGC\nAttentionQ,K,V\nKV Cache Quantized\nOutput\nDequantizeCodebook\n@ Shared\nVQ attn\nSC\nDequantized @ Shared Dequantized @ Shared\nFig. 3. Workflow of investigated VQ kernels.\n[57], [67], [69], as shown in the upper part of Fig. 2.\nSome can even achieve higher quality with fewer equivalent\nbits. The underlying reason is depicted in the lower part\nof Fig. 2. With cross-dimension information, VQ can better\ncapture the distribution characteristics of the data, resulting in\nlower reconstruction error. In contrast, traditional quantization\nrelies on the Cartesian product of quantization points between\ndimensions and cannot represent some outliers well.\nWhile converting the reduced memory footprint to actual\nspeed-up is challenging due to the need for efficient kernels\nthat take quantized data and codebooks as inputs, dequantize\nthem, and perform computations. Unfortunately, existing al-\ngorithms only provide kernels with high latency, making them\nimpractical for use [12], [56], as verified in Sec. VII. In the VQ\npipeline, dequantization is the main bottleneck in the context\nof LLMs. This is because quantization can be done offline\n(for weights) or asynchronously with tiny overhead (for KV\ncache, also discussed in Sec. VII). However, dequantization is\nrequired every time before a computation since the quantized\ndata store codebook indices and cannot be directly operated\non. Therefore, this paper focuses on developing efficient fused\ndequantization-computation kernels.\nIn the next section, we will analyze the inefficiencies of ex-\nisting and vanilla optimized fused dequantization-computation\nkernel. As mentioned before, the core difference between\nVQ and element-wise quantization is the use of vectorized\ncodebooks, and we primiaily focus on them in our analysis.\nNoted that we target NVIDIA GPUs in this paper, althouth\nGPUs from other vendors share similar concepts [3], [39],\n[40], [54]. A GPU compute kernel launches thousands of\nthreads, organized into thread blocks within a grid. Each thread\nblock is dispatched to a Streaming Multiprocessor (SM), which\nmay handle multiple thread blocks to overlap instructions.\nThreads access three memory hierachies: registers (local to\neach thread), shared memory (local to the thread block), and\nglobal memory (accessible by all threads).\nIII. M OTIVATION\nIn this section, we analyze the inefficiencies of current\nVQ implementation centering how codebooks are placed and\nutilized. We first outline our setup for a micro-benchmark-\nbased investigation in Fig. 3 and then analyze it in detail.\nA. Investigation Setup\nWe evaluate an attention kernel from Llama-7B [55] with 32\nheads and head dimension of 128 on an RTX 4090 GPU [43].\nWe investigate three implementations of vector quantized (VQ)\nKV cache with the configuration VQ<4,8,1> that follows\nFP16-attn\nRelative\nperformance0\n1\n2\nVQ-attn\nGC\nVQ-attn\nSC\n2.52×1062.52×106FP16-attn\nRelative\nRatio\n0\n1\n2\n3\n4\n5\nSM\nUtilization\nShared\nUsage\nShared\nBank\nConﬂict\nGlobal->\nShared\nTrafﬁc\nShared->\nReg\nTrafﬁc\nFig. 4. (left) Latency of VQ-attn-GC and VQ-attn-SC relative to\nFP16-attn. (right) Relative performance counters of VQ-attn-SC.\nCQ-2 [69]. As illustrated in Fig. 3, the first FP16-attn\nversion implements Flash Decoding [10] from the FlashAtten-\ntion library [7], [9]. We implement the VQ-attn-GC version\nourselves following the original paper [12], [56], [57], [69]\ndue to the lack of open-source kernels. VQ-attn-GC receives\nthe VQ quantized KV cache and its codebooks, dequantizes\nthem to FP16 precision, and performs the subsequent attention\ncomputation, with codebooks stored in global memory. Given\nthe long access latency of global memory, we propose and\nimplement another optimized version that stores codebooks\nin shared memory and hence is labelled as VQ-attn-SC,\nwith the rest of the process mirroring that of VQ-attn-GC.\nHere we only analyze attention kernel thus KV cache com-\npression, while these observations can also be generalized to\nGeMM/GeMV and weight compression.\nB. Inefficiency Analysis\nSince the attention (decoding) process is highly memory-\nbound, using VQ<4,8,1>, which compresses the KV cache\nto 1/8, should significantly enhance its performance. How-\never, as depicted on the left of Fig. 4, both VQ versions\nunderperform the FP16 baseline. We also observe that the\nshared-memory-based codebook version, VQ-attn-SC, out-\nperforms the global-memory-based version, VQ-attn-GC,\ndemonstrating the effectiveness of utilizing shared memory\nfor codebooks. Although shared memory and the GPU L1\ncache share the same physical space, the hardware-managed\nL1 cache fails to capture the temporal locality of codebook\nentries. This is because the size and irregular access pattern of\nthe entries does not align with the cache line size and prefetch\nwidth (128 bytes [41]) of the L1 cache. According to our\nprofiling results, VQ-attn-GC achieves only a 12.45% L1\ncache hit rate, indicating significant wasted capacity in the L1\ncache. Consequently, we default to the VQ-attn-SC version\nto investigate its sources of inefficiencies.\nInefficient Codebook Access. Fig. 4 (right) compares the\nvarious performance counters of the VQ-attn-SC version\nand the FP16 version. We first observe an over 30% drop\nin compute (SM) utilization in the VQ-attn-SC version\n(1st bar). This decline is attributed to the VQ’s significantly\nincreasing shared memory footprint (2 nd bar), which reduces\nthe number of thread blocks that can run concurrently on\neach SM, leading to decreased performance. Additionally,\nwe note high bank conflicts (3 rd bar), indicative of highly\nserialized access to shared memory. Eliminating these bank\nconflicts is challenging for several reasons. First, the number of\ncodebook entries vastly exceeds the number of shared memory\nBlock 3\nBlock 2\nBlock 1\nBlock 0\nBlock 3\nBlock 2\nBlock 1\nBlock 0\nQuery Output\nK Cache V Cache\nK Codebook V Codebook\nInner product Global reduce\nGlobal reduce\nDequantize\nDequantize\nChannel Channel\nTokens\nTokens\nVQ-attn-SC\nFP16-attn\nFig. 5. Dataflow of FP16-attn (inner box) and VQ-attn-SC (outer box).\nbanks, e.g., 256 entries versus 32 banks, and their accesses\nare random during the VQ dequantization process, precluding\nthe use of common static reordering or padding solutions\nfor coalesced accesses [41]. It is possible to reorder entries\nor threads at runtime, which can introduce extra complexity\nand overhead. Second, a single codebook entry can occupy\nmultiple banks in VQ, exacerbating the difficulty of mitigating\nbank conflicts.\nTakeaway 1 Storing codebooks in fast on-chip buffers like\nshared memory is necessary, but not trivial.\nUncoordinated Codebook Load and Compute. The 4th bar\nin Fig. 4 (right) indicates that the traffic from off-chip global\nto on-chip shared memory is higher for the VQ version than\nfor the FP16 version. This is counterintuitive since VQ is\nexpected to significantly reduce global memory access. The\ncause of this unexpected off-chip traffic is that integrating VQ\ninto the original compute kernel results in uncoordinated and\nduplicated loads of codebooks.\nThe inner box of Fig. 5 shows the original FlashDecoding’s\ndataflow [10], which parallelizes the computation of different\ntokens and computes the local softmax in global memory.\nWhen integrating the VQ codebooks to this computation\ndataflow, computing every four channels for a token needs\nto switch to a different codebook, following the VQ algorithm\nof CQ-2 [69]. Consequently, thread blocks handling different\ntokens end up accessing and loading identical codebooks as\nthey process data across all channels, as shown in the outer\nbox of Fig. 5. This results in significant duplicated off-chip\nmemory traffic, and this challenge is also presented in the\nintegration of VQ with GeMM kernels. For GPTVQ-2 [57],\nTABLE II\nVQ ALGORITHM AND THEIR CONFIGURATIONS\nAlgorithm Compression\nRatio against FP16\nVector\nSize #Entry Residual\nQuiP#-4 25% 8 65536* 2\nAQLM-3 18.75% 8 4096 2\nGPTVQ-2 12.5% 4 256 1\nCQ-4 25% 2 256 1\nCQ-2 12.5% 4 256 1\nConfigs. 21,2,... 21,2,... 1,2,...\n*QuiP# utilize a lattice-based codebook, though it has 65536 entries, it only\nneed to look up from 256 of them every dequantization with bit operations.\nTid 0 Dequant\nTid 0 Compute\nTid 1 Compute\nTid 1 Dequant\nK cache V cache\nFig. 6. Layout of dequantized data and required layout of following\ncomputation of KV cache in attention (decoding).\nevery (256, 256) tile of the weight matrix shares a codebook,\nwhile the task is spliced into ( ·, 128) tiles on weight matrix,\nand every two thread blocks access and load a same codebook.\nBesides the increased off-chip global memory traffic, we\nalso observe a significant rise in on-chip shared memory to\nregister traffic in the VQ-attn-SC version, as shown in the\nlast bar of Fig. 4 (right). Ideally, this traffic should remain the\nsame to the FP16-attn version, given that the computation\nprecision and the volume of data involved in the computation\nremain unchanged. The unusual Shared → Reg traffic stems\nfrom a mismatch between the layout of dequantized data and\nthe layout required by the computation.\nAs illustrated in Fig. 6, one thread dequantizes a row of\nfour elements at a time for the KV cache following the\nCQ-2 algorithm [69]. It then stores these four elements in\nthread-local registers. However, the computation requires a\ncolumn-wise weighted accumulation on the V cache, and the\nfour dequantized elements by the thread do not match the\ndata needed for subsequent computations. Consequently, the\ndequantized data in local registers must be stored back into\nshared memory, allowing the correct threads to access them.\nNotice that as depicted in the figure, the K cache does not\nintroduce such a shared memory round-trip since its row-wise\naccumulation process aligns with the dequantization process.\nTakeaway 2 Integrating and fusing VQ algorithms into\nLLM’s kernels requires a careful coordination between the\ncodebook load and the fused kernel’s compute dataflow.\nC. Additional Complexity of VQ Diversity\nOur above analysis primarily focuses on a specific VQ\nconfiguration for the FlashDecoding kernel. In fact, we have\nsurveyed state-of-the-art methods of using VQs to accelerate\nLLMs and found considerable diversity, as listed in Tbl. II.\nThese varied configurations add complexity when generating\nhigh-performance fused computation kernels. Moreover, dif-\nferent algorithms choose to train a codebook with different\nparts of tensor which further push up this complexity. For\ninstance, QuiP# [56] can avoid duplicated Global → Shared\ntraffic as it train one codebook with the entire weight tensor,\nyet it may increase bank conflicts and cause layout mismatches\nwith its vector size 8. Conversely, CQ-4 [69] is able to reduce\nbank conflicts and layout issues with its vector size 2, but it\nmay lead to significantly duplicated Global → Shared traffic\nsince it train different codebooks with different channels.\nOn the other hand, there are various computations associ-\nated with VQ algorithms, such as VQ-gemm and VQ-gemv\nfor weight-only quantization, and VQ-attn for KV cache\nquantization, as previously mentioned. The combination of\nCodebook Cache\nAdaptive Heuristics\nGlobal\nCold\nShared\nMedium\nRegister\nHot\nCodebook Reorder & Update\nCodebook Centric Dataﬂow\nCodebook Centric Hierachical Fusion\nCodebook-Based Compute Engine\n5\n6.1\n6.2\nFig. 7. VQ-LLM design overview.\nVQ algorithm diversity and multiple subsequent computation\npatterns makes it impractical to manually craft efficient kernel\nimplementations for each specific case.\nTakeaway 3 An adaptive solution is necessary to achieve\noptimal performance across a variety of VQ algorithms and\ntheir subsequent computations.\nIV. VQ-LLM O VERVIEW\nFrom the analysis in the previous section, we identify three\nkey challenges in utilizing VQ to accelerate LLM inference: i)\nefficient codebook entry access, ii) coordinated codebook load-\ning and subsequent computation, and iii) significant diversity\nin VQ algorithms and subsequent computation patterns.\nTo address these challenges, we design and implement VQ-\nLLM, an automatic high-performance code generation frame-\nwork in Fig. 7. We introduce a software abstraction called\ncodebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The\ncodebook cache adaptively stores different entries across the\nGPU’s memory hierarchy, including off-chip global memory,\non-chip shared memory, and registers. It does so by leveraging\nthe offline-profiled characteristics of codebook entry access,\nsuch as cold, medium, and hot.\nThe codebook cache also enables seamless integration with\nthe subsequent computations. Centered around the codebook\ncache, we design an efficient computation engine that op-\ntimizes memory traffic during computations involving code-\nbooks, incorporating two core techniques. The first technique,\ncalled codebook-centric dataflow, divides and parallelizes the\noriginal computation task in a way that minimizes the code-\nbook switch overhead. It may split the reduction dimension\nof the original computation task, for which we adaptively\ndetermine the split factor to balance the global reduction. With\nthis dataflow, we eliminate the excessive off-chip memory\ntraffic caused by redundant codebook loads from different\nthread blocks in current VQ implementations.\nThe second technique employed by our compute engine,\nnamed codebook-centric hierarchical fusion, extends the de-\nfault shared memory level fusion to support the additional\nregister-level fusion. This mechanism leverages a GPU fea-\nture known as intra-warp data exchange [45] to rearrange\nthe dequantized data into the required layout for subsequent\ncomputations directly in registers. And we adaptively decide\nwhere to conduct the fusion based on profiled exchanging\nμ+0σ\nμ+3σ\n26 1911\nAccess\nfrequency0\n16\n32\n48\nCodebook entry ID\n0 1024 2048 3072 4096\nFig. 8. Codebook entries access frequency of one thread block in VQ-GeMM\nkernel, with VQ<8,12,2> (AQLM-3).\noverhead and difference between layout of dequantized data\nand layout required by subsequent computation.\nOur VQ-LLM framework comprises a set of CUDA tem-\nplates that employ a codebook-centric dataflow and fusion\nscheme, along with a set of adaptive heuristics. These tem-\nplates include both algorithm-specific and hardware-related\nparameters. To generate a specific VQ-augmented compute\nkernel, we supply the configuration of the algorithm and target\nGPU to the corresponding compute kernel template. VQ-LLM\nthen automatically selects the optimal parameters based on the\ntemplate specifications and heuristics.\nV. C ODEBOOK CACHE\nWe first present the design intuition of codebook cache,\nand then implementation details. Finally, we describe the user\ninterface that can be utilized by subsequent computations.\nA. Design Rationale\nAs Sec. III explains, naively placing the entire codebook\nin the shared memory results in suboptimal performance due\nto two issues: i) increased shared memory usage and ii)\nsignificant bank conflicts. To address these issues, we propose\nstoring different entries at various memory levels based on\ntheir access frequencies. Specifically, we can store rarely\naccessed entries in off-chip global memory to conserve shared\nmemory usage, and store the most frequently used entries in\nthe thread local registers to eliminate bank conflicts.\nWe find that different entries in a codebook indeed demon-\nstrate varying levels of ‘hotness’ in terms of access frequency.\nFig. 8 illustrates such an example of AQLM-3, and results\nof other algorithms will be shown in Sec. VII. Over half\nof the codebook entries are accessed less frequently than the\naverage, indicating that placing them in shared memory yields\nlittle benefit. There are 26 hot entries that are accessed more\nfrequently than µ+3σ (mean plus three standard deviations),\nsuggesting that they are more susceptible to inevitable bank\nconflicts. This observation forms the foundation of our code-\nbook cache design, the details of which we introduce next.\nB. Implementation\nTypically, the implementation of a cache relies on tag\narray [59] or lookup table [36], which could incur additional\nlatency and storage overhead. In our codebook cache imple-\nmentation, we adopt a reorder-based static mapping mecha-\nnism that is extremely lightweight and configurable, which\nmeans there is also no complex eviction policy.\nIn our implementation, we first sort and reorder the code-\nbook entries by their access frequency in the descending order.\nEntry IDEntry ID\nBlock IDBlock ID\nFig. 9. Entries hot and cold of different parts of tensor.\nThis is done at the profiling-based offline phase, which ensures\nthat the index of the most frequent entry is 0, and the index of\nthe least frequent entry is the maximum value. All the quan-\ntized data would use these new indices. Next, we establish two\nboundaries: nreg and nshared. We allocate the first nreg entries to\nthread local registers and the subsequent entries up to nshared\nin shared memory. We store any remaining entries in global\nmemory. During runtime dequantization, addressing codebook\nentries involves simple index comparisons, we locate entries in\nregisters if the index <nreg, in shared memory if nreg≤ index\n<nshared, and in global memory if the index ≥nshared.\nIn this implementation, we conduct frequency-based re-\nordering at the tensor level, although different parts of a\ntensor might have different frequently accessed entries. Fig. 9\npresents data to support our choice, where the y-axis represents\ndifferent parts of the tensor (i.e., different thread blocks), and\nx-axis indicates the access frequencies of different codebook\nentries of a thread block. White color indicates frequently\naccessed entries, and the opposite for darker shades. We\nobserve many vertical white lines, suggesting that these entries\nare consistently accessed across different tensor parts. This\nobservation supports the rationale for globally determining the\nmost frequently accessed entries.\nAdaptivity. The shared memory and register resources of our\ncodebook cache can be adjusted using two parameters: nreg and\nnshared. As mentioned in Sec. III, these resources are limited\non GPUs, and excessive usage can decrease the occupancy\nof thread blocks. We employ a heuristic-based method that\nadapts their allocation to subsequent computations. Initially,\nwe identify slack in the use of both recources. This concept\nis illustrated in Fig. 10, where we assign varying amounts\nof shared memory and registers to two computation kernels,\nhighlighting the most performant configuration with a circle\nmarker. Resource slack, depicted as the blue shaded area in\nFig. 10, is a space of resource that we can occupy without\nhurting concurrency and GPU utilization. The existence of\nthese slacks is due to the GPU’s resource partitioning and\nscheduling [52], which we will not explore further due to space\nconstraints. It is important to note that different computations\nexhibit varying slacks, which can also be derived by offline\nprofiling. We determine nreg and nshared by dividing the avail-\nable slacks by the size of a single codebook entry.\nC. User Interface\nWe provide and explain the following APIs for users to\nutilize our codebook cache, henceforth abbreviated as CB.\nCBcached, nreg,shared ← Load(CB, Slack)\nEntry ← Access(CBcached, nreg,shared, CB, Index)\nCB ← Switch(New CB Pointer)\nGPU occupancy\nComputation resource consumption\nOP B0\n1\nOP A0\n1\nB32 registers (#)\n0 64 128 192\nShared memory (KB)\n0 32 64 96\nFig. 10. Computation kernel resource consumption and corresponding occu-\npancy of the hardware. The blue region is the resource slacks we can use\nwithout influencing the performance.\nK cache (1 head) V cache (1 head)\nTB 0\nTB 1\nTB 2\nTB..\nTB 0\nTB 1\nTB 2\nTB..\nAttn mapSoftmaxPartial\nCodebook 0\nCodebook 1\nCodebook …\nCodebook 31\nCodebook 0\nCodebook 1\nCodebook …\nCodebook 31\nReduce\n+\nTokens\nTokens\nFig. 11. Example of codebook centric dataflow for attention (decode)\ncomputation following CQ configuration.\nThe first API is Load, which loads codebooks stored in global\nmemory into the cache. It accepts the codebooks and memory\nslack, returning the codebooks cached across the memory\nhierarchy along with two access boundaries. The second API\nis Access, allowing users to access specific entries during the\ndequantization process. It accepts cached and global memory-\nstored codebooks along with indices to locate entries. It also\nuses two boundaries to determine where to locate entries.\nAdditionally, while we configure these boundaries with preset\nheuristics, users can still overwrite them.\nThe last API is Switch, useful when algorithms train dif-\nferent codebooks for different parts of a tensor, as in GPTVQ-\n2 [57]. This API facilitates the switch to new codebooks based\non the specific tensor section being processed by the user.\nVI. C ODEBOOK -BASED COMPUTE ENGINE\nBased on the above codebook cache, we design an efficient\ncompute engine to optimize the excessive codebook-related\ntraffic when using VQ in the subsequent computation. We first\nintroduce two core techniques employed by our computation\nengine: codebook-based dataflow and codebook-based hierar-\nchical fusion. We then detail the combined usage of the entire\ncomputation engine along with the codebook cache.\nA. Codebook Centric Dataflow\nWe start by explaining the intuition of our design. Subse-\nquently, we detail our implementation.\n1) Design Rationale: To fully leverage the parallel compu-\ntation resources of GPUs, researchers employ tiling to divide\nand parallelize computation tasks [6], [42], [72]. Under the\nVQ scenario, naive parallelization introduces excessive traffic\ndue to conflicts between the codebook switch axes and the task\nreduction axes, as discussed in Sec. III. We address this issue\nwith a new codebook-centric dataflow illustrated in Fig. 11,\nwhich employs the same settings as Fig. 5 in Sec. III. In\nthis codebook-centric dataflow, we partition and parallelize the\ntask across every four channels, i.e., every codebook, ensuring\nthat each thread block only needs to load one codebook, thus\neliminating any need for duplicated codebooks or switches.\nInstead of globally reducing the local softmax of different\ntokens as in FlashDecoding [10], we now require global\naccumulation of partial inner-products.\n2) Implementation: We now formally define our design for\nthe codebook-based dataflow. We first identify the axes where\nreduction occurs and where codebooks need to be switched,\nas indicated in Tbl. III. Subsequently, we split and parallelize\nthe computation along the codebook switch axes. Finally, for\nthose axes that traditionally perform temporal accumulation\nbut are now parallelized (intersecting with the codebook switch\naxes and annotated with colors), we perform an explicit global\nreduction to ensure accurate results.\nAdaptivity. To balance the overhead of global reduction in\nour dataflow, we utilize a split factor to control the extent\nof task parallelization along the codebook switch axes. A\nlarger split factor results in fewer duplicated codebooks but\nnecessitates more global reductions, and vice versa. With the\nobjective of minimizing overhead, we adaptively determine the\nsplit factor based on the size of the tensor that needs reduction\nand the traffic associated with duplicated codebooks.\nTrafficReduce ← Split Factor× Output Size\nTrafficCodebook ← Original Codebook Traffic\nSplit Factor\nSince these two variables exhibit opposing trends with respect\nto the split factor, we can achieve a minimum by equating\nthem according to the Mean Value Theorem [48].\nB. Codebook Centric Hierarchical Fusion\nSimilarly, we begin with a concrete example to illustrate our\nnew fusion scheme. Subsequently, we formally abstract the\nhierarchical fusion algorithm and detail our implementation.\n1) Design Rationale: The baseline method described in\nSec. III employs shared-memory-level fusion, which combines\nVQ dequantization and the subsequent computation kernel by\ntransferring data through shared memory. It leads to excessive\nTABLE III\nREDUCE AND CODEBOOK SWITCH AXES OF COMPUTATIONS\nGeMM\nGeMV All axes Reduce\naxes\nCodebook\nswitching axes\nWeight M,N,R M,R R: AQLM,QuiP#\nM,N: GPT-VQ\nR: Residual, M,N: M rows, N columns\nAttention All axes Reduce\naxes\nCodebook\nswitching axes\nK Cache B,H,T,C C H,C: CQ\nV Cache B,H,T,C T H,C: CQ\nB: Batch, H: Head, T: Token, C: Channel\nQuantizedWarp tile\n(16,2)\n3\n2\n1\n0\n(16,2)\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0 1 2 3\n0\n1\n2 30\n1 2 3\n0 1 2\n30 1\n2\n3\n0\n1\n230\n1 2 3\n0 1 2\n301\n2\n3\n0\n1\n200\n1 1 3\n0 2 2\n331\n2\n3\nreg[tid^1]=shfl(reg[tid^1],1) reg[tid^2]=shfl(reg[tid^2],2)\nreg[tid^3]=shfl(reg[tid^3],3)\nThread id Register array index\nmma.sync\nDequantize\nThread\nMapping Shu\nﬄe\nShu\nﬄe\nMatched!Compute\nShu\nﬄe\n(16,16) (16,16)\n(16,16)(16,16)\nFig. 12. Intra-warp data exchange based on shuffle API example, eight\nelements are dequantized one time per thread, while following computation\nrequires one thread hold only two elements ( mma instructions).\ntraffic between shared memory and registers, as previously\nexplained. Alternatively, we utilize a modern GPU feature\nthat facilitates register-level data exchange [45], effectively\nbypassing shared memory with following API:\nregister ← shflxor(register, offset) (1)\nThis API exchanges the reg of the calling thread ( idsrc)\nwith reg of the thread whose iddst⊕offset=idsrc in place\n(⊕:xor). Note that this instruction is commonly used to\nenhance the efficiency of collective communication and result\nreduction [27], [71]. However, we are the first to apply it to\naccelerate VQ-compressed LLMs.\nWe illustrate the application of this API for register-level\nfusion through an example that fuses VQ<8,...> with\nGeMM. In Fig. 12, the layout of the dequantized data is 8\n(i.e., VQ vector size), while the layout required by the mma\ninstruction is 2. We initially map the dequantization threads\nin a specialized manner, as depicted in the figure, to ensure\nthat all data exchanges are confined to four threads, which we\nsubsequently refer to as a mini-warp. Within this mini-warp,\nwe execute three exchange ( shfl) operations as follows:\n• Tid 0.[1]↔Tid 1.[0], Tid 2.[3] ↔Tid 3.[2]\n• Tid 0.[2]↔Tid 2.[0], Tid 1.[3] ↔Tid 3.[1]\n• Tid 0.[3]↔Tid 3.[0], Tid 2.[1] ↔Tid 1.[2]\nNote that both the array index and thread ID can be represented\nusing the xor operation. After these shuffle operations, the\ndata held by each thread’s register aligns precisely with the\nrequirements of the mma computation instruction.\nThread Mapping. Our approach necessitates a specialized\nthread mapping within a warp for dequantization, as the\nnaive sequential mapping requires a complex exchange pattern.\nConsider the sequential mapping with the mma instruction in\nFig. 12, data[8,0:8] (blue color) is dequantized by thread 16 but\nis required by threads 0-3. However, the data held by threads\n0-3 is not needed by thread 16 but rather by threads 0-7.\nThis results in a complex data exchange path where ultimately\nall threads are implicated. Meanwhile, it requires additional\nregisters as the exchange happens in place. To circumvent\nthis, we predetermine the thread mapping offline, based on\nthe layout of the dequantized data and the layout required by\nthe computation, with details described as follows.\nAlgorithm 1 Intra-warp data exchange based on shuffling\nInput: data, iter, layoutdequant,compute\nOutput: data\n1: function THREAD MAPPING (data, layoutdequant,compute)\n2: for item ∈ data do\n3: item.tidcompute,dequant ← GetTid(item, layoutcompute,dequant)\n4: mini warps ← []\n5: for dequant thread ∈ warp do\n6: mw ← [data.tidcompute for data.tiddequant=dequant thread]\n7: if mw /∈ mini warps then\n8: mini warps[mw] ← []\n9: mini warps[mw].append(dequant thread)\n10: for mw ∈ mini warps do\n11: mini warps[mw][i] ← mw[i]// Thread mapping we need\n12: function REG FUSION (data, iter)\n13: for off in [1, iters) do // intersected 0 no shuffle needed\n14: data[tidˆoff ] ← shflxor(data[tidˆoff ],off )\n15: return reg\n2) Implementation: We outline our algorithm in Alg. 1. To\ndetermine the thread mapping, we first find the association\nbetween each element in terms of dequantization and compu-\ntation (lines 2-3). Subsequently, for each thread, we identify\nall threads that require its dequantized data, grouping these\nthreads into a mini-warp (lines 4-6). We then construct mini-\nwarps for all threads (lines 7-9). In the previous example,\nthreads 0, 1, 16, and 17 possess identical data [0, 1, 2, 3] and\nthus form a mini-warp. Finally, we remap all threads by mini-\nwarps (lines 10-11); for instance, we assign threads 2 and 3 to\ndequantize the data initially handled by threads 16 and 17. This\nprocess is executed offline to ensure proper thread mapping\nin runtime dequantization, enabling the implementation of\nregister-level fusion via the shuffle API (lines 12-15).\nAdaptivity. Clearly, a larger discrepancy between the de-\nquantization layout and the required layout of the computation\nkernel increases the need for shuffling. Consequently, we\npropose conducting hierarchical fusion adapted to the vector\nsize of the codebook entry. Profiling results indicate that the\nlatency of shared memory access is nearly five times that\nof register access combined with shuffling. Therefore, for\nquantized tensors requiring fewer than five shuffle operations,\nwe implement register-level fusion. For other tensors, we\nmaintain the conventional shared memory-level fusion.\nC. Overall Workflow\nOur compute engine adopts a template-based design in\nAlg. 2 to generate final fused kernels. First at the offline phase,\nbased on the VQ configuration and targeted computation,\nwe determine shared/register budgets, split factors, required\nnumber of shuffles, and the corresponding thread mapping for\nour proposed optimizations (lines 2-8).\nSubsequently, we launch the codebook-centric dataflow\ncomputation (line 9) via the Parallel_For function that\nbinds following sub-tasks to parallel thread blocks. Its two\nparameters represent the task splitting axes and the split factor,\nrespectively. Within each parallelized task, we first load the\ncodebook into the codebook cache (lines 10-12), followed\nby dequantization using the provided APIs in Sec. V (lines\n13-14). Notice now threads are mapped to quantized data\nfollowing Thread_Mapping determined offline, for mini-\nmum shuffle if applicable. After dequantization, we perform\ncodebook-centric hierarchical fusion (lines 15-18) using the\nReg_Fusion and Shared_Fusion function. Both func-\ntions accept dequantized data, with the former requiring a\ncounter nshuffle to indicate the number of required shuffle\noperations and latter requiring the source-destination layout to\ninitialiate correct shared memory accesses. Once the data is in\nthe proper layout, we proceed with computation (lines 19-20).\nFinally, we perform a global reduction if necessary (line 21)\nvia the Reduce function, where the first parameter specifies\nthe partial result to be reduced and the second determines the\naxes along which the global reduction is conducted.\nAlgorithm 2 Complete VQ-aware computation template\nInput: quantized, codebook, computeop\nOutput: output\n1: function KERNEL TEMPLATE\n2: All, Reduce← compute op.all axes, reduceaxes\n3: layoutsrc,dst ← codebook.vector size, computeop.required size\n4: Budget ←Free shared and reg to preserve occupancy\n5: factor ←Value to make TrafficReduce=TrafficCodebook\n6: nshuffle ← layoutsrc/layoutdst\n7: if nshuffle ≤ thresshuffle(= 5)then\n8: Thread_Mapping(compute op.warp tile, layoutsrc,dst)\n9: Parallel_For(codebook.switch axes, factor)\n10: if required by algorithm then\n11: CB ←Switch(New codebook ptr )\n12: CBcached, boundry←Load(CB, Budget)\n13: for id in quantized data do\n14: data ←Access(CBcached, boundry, CB, id)\n15: if nshuffle ≤ thresshuffle then\n16: data ←Reg_Fusion(data, nshuffle)\n17: Else\n18: data ←Shared_Fusion(data, layoutsrc,dst)\n19: for temporal iteration on All − codebook.switch axes do\n20: partial ← compute op(data, temporaliteration)\n21: output ←Reduce(partial, Reduce∩ codebook.switch axes)\n22: Return output\nVII. E VALUATION\nIn this section, we evaluate the effectiveness of proposed\noptimizations in VQ-LLM through comprehensive experi-\nments. We first present overall speedup results for various VQ-\nbased computation kernels over existing approaches. Then,\nwe provide a detailed breakdown analysis of the proposed\noptimizations. Next, we compare our work with FP16 ker-\nnels and several element-wise quantization works to show\nits viability for accelerating LLMs. Finally, we performed\na comprehensive end-to-end evaluation, analyzing both the\noverall speedup and accuracy across various GPUs.\nA. Experimental Setup\nIn this study, we conduct a comprehensive evaluation at\nboth the individual kernel and end-to-end model levels. The\nevaluations were performed on an NVIDIA RTX 4090 24GB\nGPU [43]. For the end-to-end evaluation, we included a\nTesla A40 GPU [39] to explore the potential of VQ-LLM\nwith lower bandwidth.. The evaluated computation kernels\nTABLE IV\nBREAK DOWN ANALYSIS CONFIGURATION\nID Optimization\nCategory Description\nGC No Naive implementation\nSC Greedy Cache all entries in shared memory\nO1 Hierarchical + Shared memory level caching (medium entries)\nO2 Buffer + Register level caching (hot entries)\nO3 Compute + Codebook centric dataflow\nO4 Engine + Codebook centric hierarchical fusion\ninclude various VQ-augmented GeMM, GeMV and FlashDe-\ncoding [10]. The evaluated VQ configurations are listed in\nTbl. II, including QuiP#-4, AQLM-3, GPTVQ-2 and CQ-\n2/4, the suffix number represent the equivalent bit-width. The\nfirst two kernels adopt weight quantization and the last one\nadopts KV cache quantization. For the kernel-level evaluation,\nwe set the shape for these kernels following the Llama-7B\nand Llama-65B [55] models. These kernels run on a single\nGPU, while large model serving like Llama-65B typically\nuses multiple GPUs with Tensor Parallel (TP) strategy [35],\n[47], [70]. The required adjustments to our framework include\nfinal results gathering for Attention and partial results con-\ncatenation/reduction for GeMM/GeMV [38]. These are usually\nconducted via communication library like NCCL [44], and we\nidentify this distributed scenario an orthogonal topic and leave\nit to the future work.\nTbl. IV lists various baselines and VQ-LLM optimizations\nused in our evaluation. For the baselines, we use GC and SC\nmethod explained in Sec. III that stores the codebook in global\nmemory and shared memory, respectively. For the results, we\nreport the latency reduction against GC. We also decompose\nthe optimizations used in VQ-LLM into four levels ( O1-O4),\nwith each explained in Tbl. IV. We also compare VQ-LLM\nwith SOTA element-wise quantization methods under the same\nequivalent 4-bit width, including AWQ [30] for GeMM/GeMV\nand QoQ for Attention [31], all integrated into qServe [31].\nFor FP16, we use cutlass [42] and flash-attn [8].\nIn practice, LLM inference involves various operators be-\nyond GeMM/GeMV and Attention, such as RMSNorm [66],\nSiLU [14], and RoPE [53], etc. Therefore, it is crucial to\nevaluate the end-to-end speedup that accounts for all operators.\nFor the end-to-end evaluation, we set a batch size of 16 and\na sequence length of 1024, measuring the total latency for\ngenerating 256 tokens. We also assess accuracy using the arc-\nchallenge task [5], applying the QuiP#-4 and CQ-4 algorithms\nfor quantizing the weights and KV-Cache, respectively. To\nobtain the final accuracy results, we integrate these algorithms\ninto the LMEval framework [16].\nB. Overall Speedup\nAs shown in Fig. 13, VQ-LLM reduces the latency by\nan average of 46.13% (53.73% at most), corresponding to a\nspeedup of 1.9 × (2.2×) (BS x indicates the batch size of x).\nGeMMGeMM GeMV BS1GeMV BS1 GeMV BS16GeMV BS16\n7B\n65B\n1k  Attention  4k1k  Attention  4k MeanMean\nLatency\nreduced0%\n40%\n80%\nQuiP#-4AQLM-3GPTVQ-2QuiP#-4AQLM-3GPTVQ-2QuiP#-4AQLM-3GPTVQ-2CQ-2 BS1CQ-2 BS8CQ-2 BS1CQ-2 BS8\nFig. 13. Overall latency reduction of best perform version against un-\noptimized version for various VQ configurations.\nFor Attention (Decode), 1k and 4k means sequence length of\n1024 and 4096, respectively.\nAlthough VQ-LLM achieves significant speedup values\nfor both GeMM and GeMV kernels, we observe a counter-\nintuitive discrepancy that our optimizations achieve a relatively\nhigh speedup value for GeMM kernels compared to GeMV\nkernels. In other words, the quality of VQ algorithm inte-\ngration is more critical to the compute-bound kernels (e.g.,\nGeMM) than to the memory-bound kernels (e.g., GeMV).\nThe reason is the former benefit less from reduced memory\nfootprint while suffer more from extra operation (dequanti-\nzation) [60], leading to significant performance degradation\nof unoptimized implementation. Meanwhile, we also observe\nan opposite trend for AQLM-3 between GeMM and GeMV .\nThis AQ configuration has an unaligned 12-bit storage format,\nwhich necessitates additional unpacking and decoding logic\nand requires a more careful optimization for the integration.\nWe observe that our speedup values for GeMV kernels re-\nmain consistent regardless of batch size, whereas they increase\nwith batch size for attention kernels. This is because different\ninput samples share the same weight tensor but have distinct\nKV caches. Since the GeMV kernel corresponds to weight\nquantization and the attention kernel to KV cache quantization,\nthe former only requires loading the VQ-compressed weight\ntensor once, while the latter loads VQ-compressed KV cache\ntensors multiple times. Consequently, our optimizations are\nmore effective for the attention kernel with large batch sizes.\nMoreover, Llama-65B achieves almost identical speedup to\nLlama-7B, except in the Attention (Decode) scenario with a\n1k sequence length and a single batch. This identical speedup\noccurs because the operators in the larger model can be\ntrivially assembled using those from the smaller ones. We can\nreadily double the launched thread blocks when we double the\nhidden dimension, demonstrating the good scalability of our\noptimizations. The sole exception arises because, in Llama-\n7B, the baseline cannot fully utilize the hardware due to an\ninsufficient number of thread blocks for a 1k sequence length\nsingle batch. In contrast, for Llama-65B, the baseline fully\noccupies the hardware, resulting in better performance and\nreducing the relative speedup of our system.\nC. Speedup Breakdown\nWe first analyze the speedup breakdown of GeMM and\nGeMV , as depicted in Fig. 14. Tbl. V enumerates several\nfactors that influence optimization effects, facilitating our\nanalysis. For QuiP#-4, SC and O1 perform identically due\nto the small size of its codebook (i.e., 2 KB in Tbl. V).\n0\n1\n2\n0\n1\nLatency\n(103us)\n0\n1\n2\n0\n100\n200\n0\n50\nBS1 BS16\nLatency\n(us)\n0\n50\n100\n150\nQuiP#-4\nGC SC O1 O2 O3 O4\nAQLM-3\nGC SC O1 O2 O3 O4\nGPTVQ-2\nGC SC O1 O2 O3 O4\nFig. 14. Breakdown of optimizations for GeMM (upper) and GeMV (lower).\nTABLE V\nFACTORS THAT INFLUENCE THE EFFECT OF OPTIMIZATIONS\nItem QuiP#-4 AQLM-3 GPTVQ-2 CQ-2\nCodebook/block 2 KB 128 KB 32 KB 64 KB\n#Entry freq> µ+3σ 1-3 15-30 <1 <1\nOutput size/block 32 KB/<1 KB* 1-4 KB\n#Shuffle 3/7* 3/7* 1/3 3\n*GeMM/GeMV\nAQLM-3 and GPTVQ-2 exhibit noticeable improvements,\nattributed to their larger codebooks. Additionally, for GeMV ,\nSC has a significantly negative impact on AQLM-3, due to its\nlarge codebook (i.e., 128 KB in Tbl. V), which restricts the\nparallelization of memory-bound computations.\nO2 delivers the most improvement in AQLM-3; we find that\nfrequencies of 15-30 entries exceed µ+3σ, and O2’s register-\nlevel caching optimization effectively reduces bank conflicts\nwhen accessing these entries. Conversely, the remaining two\nconfigurations QuiP#-4 and GPTVQ-2 exhibit far fewer entries\nexceeding µ+3σ, indicating the less optimization opportunity\nof register-level caching and hence marginal improvements.\nO3 affects GeMM and GeMV differently. In GeMM, O3\nintroduces negative effects due to a large output size. Fur-\nthermore, multiple residuals in QuiP#-4 configuration lead to\nredundant computations for O3, causing significantly increased\nlatency in GeMM. In contrast, for AQLM-3, its misaligned\n12-bit indices result in costly unpacking and decoding. It\nleads to low compute pipeline utilization, and hence is more\ntolerant to redundant computations. In GeMV , the output size\nis much smaller and the computation is lighter compared to\nGeMM. The smaller output size results in minimal global\nreduction overhead, and the lighter computation introduces\nless computational overhead than in GeMM. These factors\nmake O3 more advantageous for GeMV .\nO4 significantly enhances GeMM’s performance. This im-\nprovement primarily stems from GeMM’s utilization of mma\ninstructions, which require a layout of 2 and can be satisfied\nthrough one to three shuffling instructions. Additionally, O4\nconserves a substantial amount of shared memory, which is\ncrucial as GeMM typically consumes a large shared memory,\nthus yielding a significant positive impact. Conversely, GeMV\nrequires element-wise reduction, resulting in QuiP#-4 and\nAQLM-3, with a vector size of 8, requiring a greater number\nof shuffling instructions. This leads to a slowdown in these\nconfigurations. However, for GPTVQ-2 with a vector size of\n4, a slight improvement is still observed. Furthermore, since\nGeMV typically uses minimal shared memory, savings in this\n050100\n150\n0\n30\n60\n0200\n400\nLatency\n(us)\n0\n20\n40\n1k BS1GC\nSC\nO1\nO2\nO3\nO4\n1k BS8GC\nSC\nO1\nO2\nO3\nO4\n4k BS1GC\nSC\nO1\nO2\nO3\nO4\n4k BS8GC\nSC\nO1\nO2\nO3\nO4\nCQ-2CQ-2\nRelative\nlatency0\n1\n1k BS11k BS84k BS14k BS8\nFig. 15. (left) Breakdown of optimizations of CQ-2 for Attention (Decode).\n(right) Relative latency of CQ-4 against CQ-2.\narea have a lesser impact on performance.\nFor Attention (decode), VQ-LLM achieves similar im-\nprovements with various sequence lengths and batches. SC\nsignificantly reduces performance due to CQ’s large codebook,\nnecessitating the use of O1 for achieving high performance.\nO2 offers only a slight improvement because few entries\nare accessed very frequently, mirroring situations in QuiP#-\n4 and GPTVQ-2. O3 significantly enhances performance by\neliminating considerable duplicated traffic in the original com-\nputation dataflow. O4 provides a minor improvement, for\nreasons similar to those for O4 in GeMV . Additionally, we\nillustrate the latency of CQ-4 relative to CQ-2 in the right\npart of Fig. 15. Our proposed optimizations achieve a similar\nspeedup to CQ-2, so we omit the detailed results to save space.\nD. FP16 and Element-wise Quantization Comparison\nWe now compare the latency of our optimized VQ kernels\nagainst FP16 and element-wise quantization works. Under the\nsame equivalent bit width, the latency of kernels with the\nelement-wise quantization is the theoretical upper bound of\nVQ kernels if using the same computation dataflow. As such,\nthis comparison further verifies the effectiveness of our work.\nAs shown in Fig. 16, at 4-bit encoding, our work achieves\nlatencies comparable to (1.01 × for Attention (Decode)), or\neven lower than (0.88 ×/0.96× for GeMV/GeMM), those of\nAWQ [30] and QoQ [31]. This reduction in latency likely\nresults from our co-designed computational dataflow. These re-\nsults suggest that our implementation is as viable as AWQ and\nQoQ, and therefore comparable to qServe [31]. Moreover, VQ\nkernels can deliver better accuracies at the same bit-width. The\nopen-source implementations of QuiP# [56] and AQLM [12]\nare impractical for real-world applications, exhibiting 2.83× to\n114.4× latencies. Our work successfully translates theoretical\nalgorithmic improvements into practical applications.\nWe would like to explain that in Fig. 16, while both our\napproach and element-wise quantization methods outperform\nthe cutlass-FP16 baseline in GeMV and Attention kernels, both\nunderperform relative to the cutlass-FP16 baseline in GeMM\nkernels. This underperformance is due to the complex tiling\n*: Open source impl.\nGeMM GeMV\nBS16\nAttention\nBS1 1k\nAWQ-4bit QoQ-4bit (All from qServe)\n114×114×\nRelative latency\n0.1\n1\n10\nQuiP#-4\n*\nAQLM-4\n*\ncutlass-16QuiP#-4GPTVQ-2QuiP#-4\n*\nAQLM-4\n*\ncutlass-16QuiP#-4GPTVQ-2Flash-16\nCQ-4CQ-2\nFig. 16. Latency comparing to element-wise quantization works.\nFP16FP16\nE2E\nSpeedup0\n1\n2\n3\nqServe (4 bit)VQ-LLM (4 bit)VQ-LLM (2 bit)VQ-LLM (4 bit)@Tesla A40\nFP16 VQ-LLM (4 bit)\nqServe (4 bit)\narc-challenge\nAccuracy44%\n45%\n46%\nE2E Speedup\n1 2\nFig. 17. (left) Overall speedup against FP16 and (right) accuracy of arc-\nchallenge of SOTA element-wise quantization (qServe) and VQ-LLM.\nstrategy employed by cutlass-FP16 GeMM, which could incor-\nporate our method. However, we do not pursue this integration\nfor two reasons. First, accelerating individual GeMM kernels\noffers minimal overall speedup for LLM inference, as these\nkernels are used in the prefilling stage (Sec. II-B). The de-\ncoding stage, which dominates LLM inference execution time,\nhas a greater impact on performance [64], as confirmed by our\nend-to-end evaluation results in the next subsection. Second,\nmodifying the cutlass code requires significant engineering\neffort due to its intricate, template-based kernel design [22],\n[73]. Therefore, we leave this integration for future work.\nE. End-to-End (E2E) Result\nWe present the end-to-end LLM inference improvements of\nvarious quantization methods compared to the FP16 baseline\nin Fig. 17 (left). In the equivalent 4-bit setting, VQ-LLM\nachieves a speedup comparable to the state-of-the-art element-\nwise quantization method, qServe [31], with both providing\napproximately a 2.2 × improvement over the FP16 baseline.\nAdditionally, VQ-LLM surpasses qServe in accuracy by about\n2.5% on the arc-challenge task [5], as shown in Fig. 17\n(right). This result demonstrates VQ-LLM’s effectiveness in\naccelerating LLM inference. he RMSNorm, SiLU, and RoPE\noperators together account for roughly 10% and 20% of total\nlatency in the FP16 and 4-bit quantized versions, respectively.\nWe also observe a greater speedup with a 2-bit compression\nratio, further highlighting the potential of VQ, as previous\nresearch suggests that 2-bit quantization can maintain practical\naccuracy. Additionally, we evaluate the performance of VQ-\nLLM in a 4-bit setting on the Tesla A40 GPU, which provides\n67% of the memory bandwidth of the RTX 4090 [39]. Inter-\nestingly, the Tesla A40 demonstrates a greater speedup than\nthe RTX 4090, suggesting that VQ-LLM is more effective in\nbandwidth-constrained environments. In summary, VQ-LLM\noffers improved accuracy with comparable latency to element-\nwise quantization, and vice versa. In terms of memory usage,\nthe FP16 baseline consumes over 22 GB, whereas qServe-4\nand VQ-LLM-4 use less than 6 GB of GPU memory, aligning\nclosely with theoretical estimates [65].\nF . Additional Discussion\nDifferent Types of Attention. The aforementioned details\nabout Attention (Decode) are based on using Flash Decod-\ning [10] as our baseline dataflow. We also evaluate the speedup\nof our work against various attention baselines, including Flash\nBS1 BS8\nSeq_len = 1k Seq_len = 2k Seq_len = 4k\nRelative latency\nto VQ-LLM01234\nFlash\nDecodingPaged FlashDecoding\nFlash\nAttentionPaged FlashAttention\nFlash\nDecodingPaged FlashDecoding\nFlash\nAttentionPaged FlashAttention\nFig. 18. Relative latency of various attention baselines against our best\nperform implementation of CQ-4.\nAttention, Paged Flash Attention and Paged Flash Decod-\ning [7]–[9], [50]. As illustrated in Fig. 18, our work surpasses\nall these baselines, primarily due to a significantly reduced\nKV cache memory footprint enabled by CQ-4. We achieved\na 66.4% latency reduction compared to the best-performing\nFP16 baseline, with a 75% reduction in memory footprint,\nunder the conditions of eight batches and a sequence length\nof 4096. This indicates an effective transfer from theoretical\nbenefit to practical application. Additionally, our work scales\neffectively with increases in sequence length and batch size.\nQuantization Overhead. For weight compression, no run-\ntime quantization overhead is introduced. In KV cache com-\npression, the runtime overhead of on-the-fly quantization for\nthe new key and value of a new token in the decode phase is\nnegligible ( <1 µs). During the prefill phase, quantizing the\nkeys and values of all prompt tokens introduces less than\na 10% overhead compared to linear projections. However,\nthe subsequent computation does not immediately require the\nquantized KV cache, rendering these overheads negligible.\nVIII. C ONCLUSIONS\nIn this work, we proposed VQ-LLM, an optimized code\ngeneration framework for vector quantization (VQ), consisting\nof codebook cache and codebook based compute engine. With\nwhich we achieve 46.13% latency reduction on average over\nunoptimized version and up-to 99% over open source imple-\nmentations. For codebook cache, we propose a hierachical\nplacement strategy to preserve hardware utilization and reduce\nbank conflict. For compute engine, we propose codebook cen-\ntric dataflow and fusion scheme to reduce excessive off-chip\nand on-chip traffic. All proposed optimizations are configured\nadaptively via several heuristics. Finally we demostrate effec-\ntiveness and viability of VQ-LLM comparing to un-optimized\nimplementations and element-wise quantization works.\nIX. A CKNOWLEDGEMENTS\nThis work was supported by the National Key R&D\nProgram of China under Grant 2022YFB4501400, the Na-\ntional Natural Science Foundation of China (NSFC) grant\n(62222210, 62072297 and U21B2017). This work was also\nsupported by Shanghai Qi Zhi Institute Innovation Program\nSQZ202316. We would like to thank the anonymous reviewers\nfor their constructive feedback and comments to improve this\nwork. Special thanks to Vega Jiang for continuous help and\nsupport. Yuhao Zhu was not financially supported by the\nawards acknowledged by the other author(s) of this publica-\ntion.\nREFERENCES\n[1] “Ieee standard for floating-point arithmetic,” IEEE Std 754-2019 (Revi-\nsion of IEEE 754-2008) , pp. 1–84, 2019.\n[2] E. Agrell and B. Allen, “On the best lattice quantizers,” IEEE Trans.\nInf. Theory, vol. 69, no. 12, pp. 7650–7658, 2023.\n[3] AMD, “Amd cdna architecture: The all-new amd\ngpu architecture for the modern era of hpc & ai,”\nhttps://www.amd.com/content/dam/amd/en/documents/instinct-business-\ndocs/white-papers/amd-cdna-white-paper.pdf, 2020.\n[4] A. Babenko and V . S. Lempitsky, “Additive quantization for extreme\nvector compression,” in 2014 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28,\n2014. IEEE Computer Society, 2014, pp. 931–938.\n[5] S. Bhakthavatsalam, D. Khashabi, T. Khot, B. D. Mishra, K. Richardson,\nA. Sabharwal, C. Schoenick, O. Tafjord, and P. Clark, “Think you have\nsolved direct-answer question answering? try arc-da, the direct-answer\nAI2 reasoning challenge,” CoRR, vol. abs/2102.03315, 2021.\n[6] T. Chen, T. Moreau, Z. Jiang, L. Zheng, E. Q. Yan, H. Shen, M. Cowan,\nL. Wang, Y . Hu, L. Ceze, C. Guestrin, and A. Krishnamurthy, “TVM:\nan automated end-to-end optimizing compiler for deep learning,”\nin 13th USENIX Symposium on Operating Systems Design and\nImplementation, OSDI 2018, Carlsbad, CA, USA, October 8-10,\n2018. USENIX Association, 2018, pp. 578–594. [Online]. Available:\nhttps://www.usenix.org/conference/osdi18/presentation/chen\n[7] T. Dao, “Flashattention-2: Faster attention with better parallelism and\nwork partitioning,” CoRR, vol. abs/2307.08691, 2023.\n[8] ——, “flash-attention,” 2024. [Online]. Available: https://github.com/\nDao-AILab/flash-attention\n[9] T. Dao, D. Y . Fu, S. Ermon, A. Rudra, and C. R ´e, “Flashattention: Fast\nand memory-efficient exact attention with io-awareness,” in Advances\nin Neural Information Processing Systems 35: Annual Conference on\nNeural Information Processing Systems 2022, NeurIPS 2022, New\nOrleans, LA, USA, November 28 - December 9, 2022 , 2022.\n[10] T. Dao, D. Haziza, F. Massa, and G. Sizov, “Flash-decoding for\nlong-context inference,” 2023. [Online]. Available: https://crfm.stanford.\nedu/2023/10/12/flashdecoding.html\n[11] M. DouzeIR, “Faiss the index factory,” 2024. [Online]. Available:\nhttps://github.com/facebookresearch/faiss/wiki/The-index-factory\n[12] V . Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and\nD. Alistarh, “Extreme compression of large language models via additive\nquantization,” CoRR, vol. abs/2401.06118, 2024.\n[13] ——, “Official pytorch repository for extreme compression of large\nlanguage models via additive quantization,” 2024. [Online]. Available:\nhttps://github.com/vahe1994/AQLM\n[14] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units\nfor neural network function approximation in reinforcement learning,”\nNeural Networks, vol. 107, pp. 3–11, 2018.\n[15] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ: accu-\nrate post-training quantization for generative pre-trained transformers,”\nCoRR, vol. abs/2210.17323, 2022.\n[16] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,\nL. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muen-\nnighoff, C. Ociepa, J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron,\nL. Sutawika, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou, “A\nframework for few-shot language model evaluation,” 07 2024.\n[17] T. Ge, K. He, Q. Ke, and J. Sun, “Optimized product quantization,”\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 36, no. 4, pp. 744–755,\n2014.\n[18] C. Guo, Y . Qiu, J. Leng, X. Gao, C. Zhang, Y . Liu, F. Yang, Y . Zhu,\nand M. Guo, “Squant: On-the-fly data-free quantization via diagonal\nhessian approximation,” in The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net, 2022.\n[19] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y . Liu, M. Guo,\nand Y . Zhu, “Olive: Accelerating large language models via hardware-\nfriendly outlier-victim pair quantization,” in Proceedings of the 50th\nAnnual International Symposium on Computer Architecture, ISCA 2023,\nOrlando, FL, USA, June 17-21, 2023 . ACM, 2023, pp. 3:1–3:15.\n[20] C. Guo, C. Zhang, J. Leng, Z. Liu, F. Yang, Y . Liu, M. Guo, and Y . Zhu,\n“ANT: exploiting adaptive numerical data type for low-bit deep neural\nnetwork quantization,” in 55th IEEE/ACM International Symposium on\nMicroarchitecture, MICRO 2022, Chicago, IL, USA, October 1-5, 2022 .\nIEEE, 2022, pp. 1414–1433.\n[21] C. Guo, R. Zhang, J. Xu, J. Leng, Z. Liu, Z. Huang, M. Guo, H. Wu,\nS. Zhao, J. Zhao, and K. Zhang, “Gmlake: Efficient and transparent\nGPU memory defragmentation for large-scale DNN training with virtual\nmemory stitching,” in Proceedings of the 29th ACM International\nConference on Architectural Support for Programming Languages and\nOperating Systems, Volume 2, ASPLOS 2024, La Jolla, CA, USA, 27\nApril 2024- 1 May 2024 . ACM, 2024, pp. 450–466.\n[22] B. Hagedorn, B. Fan, H. Chen, C. Cecka, M. Garland, and V . Grover,\n“Graphene: An IR for optimized tensor computations on gpus,” in\nProceedings of the 28th ACM International Conference on Architectural\nSupport for Programming Languages and Operating Systems, Volume 3,\nASPLOS 2023, Vancouver, BC, Canada, March 25-29, 2023 . ACM,\n2023, pp. 302–313.\n[23] P. Haghani, S. Michel, P. Cudr ´e-Mauroux, and K. Aberer, “LSH at large\n- distributed KNN search in high dimensions,” in 11th International\nWorkshop on the Web and Databases, WebDB 2008, Vancouver, BC,\nCanada, June 13, 2008 , 2008.\n[24] C. Hooper, S. Kim, H. Mohammadzadeh, M. W. Mahoney, Y . S.\nShao, K. Keutzer, and A. Gholami, “Kvquant: Towards 10 million\ncontext length LLM inference with KV cache quantization,” CoRR, vol.\nabs/2401.18079, 2024.\n[25] H. J ´egou, M. Douze, and C. Schmid, “Product quantization for nearest\nneighbor search,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 1,\npp. 117–128, 2011.\n[26] B. K ¨ovesi, J. Boucher, and S. Saoudi, “Stochastic k-means algorithm\nfor vector quantization,” Pattern Recognit. Lett. , vol. 22, no. 6/7, pp.\n603–610, 2001.\n[27] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. Yu,\nJ. Gonzalez, H. Zhang, and I. Stoica, “vllm: Easy, fast, and\ncheap llm serving with pagedattention,” 2024. [Online]. Available:\nhttps://blog.vllm.ai/2023/06/20/vllm.html\n[28] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez,\nH. Zhang, and I. Stoica, “Efficient memory management for large\nlanguage model serving with pagedattention,” in Proceedings of the\n29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz,\nGermany, October 23-26, 2023 . ACM, 2023, pp. 611–626.\n[29] Y . Lee, H. Choi, S. Min, H. Lee, S. Beak, D. Jeong, J. W. Lee, and T. J.\nHam, “ANNA: specialized architecture for approximate nearest neighbor\nsearch,” in IEEE International Symposium on High-Performance Com-\nputer Architecture, HPCA 2022, Seoul, South Korea, April 2-6, 2022 .\nIEEE, 2022, pp. 169–183.\n[30] J. Lin, J. Tang, H. Tang, S. Yang, W. Chen, W. Wang, G. Xiao, X. Dang,\nC. Gan, and S. Han, “AWQ: activation-aware weight quantization for\non-device LLM compression and acceleration,” in Proceedings of the\nSeventh Annual Conference on Machine Learning and Systems, MLSys\n2024, Santa Clara, CA, USA, May 13-16, 2024 . mlsys.org, 2024.\n[31] Y . Lin, H. Tang, S. Yang, Z. Zhang, G. Xiao, C. Gan, and S. Han,\n“Qserve: W4A8KV4 quantization and system co-design for efficient\nLLM serving,” CoRR, vol. abs/2405.04532, 2024.\n[32] S. Liu, H. Lu, and J. Shao, “Improved residual vector quantization\nfor high-dimensional approximate nearest neighbor search,” CoRR, vol.\nabs/1509.05195, 2015.\n[33] Z. Liu, J. Leng, Z. Zhang, Q. Chen, C. Li, and M. Guo, “VELTAIR: to-\nwards high-performance multi-tenant deep learning services via adaptive\ncompilation and scheduling,” in ASPLOS ’22: 27th ACM International\nConference on Architectural Support for Programming Languages and\nOperating Systems, Lausanne, Switzerland, 28 February 2022 - 4 March\n2022, B. Falsafi, M. Ferdman, S. Lu, and T. F. Wenisch, Eds. ACM,\n2022, pp. 388–401.\n[34] Z. Liu, W. Ni, J. Leng, Y . Feng, C. Guo, Q. Chen, C. Li, M. Guo,\nand Y . Zhu, “JUNO: optimizing high-dimensional approximate nearest\nneighbour search with sparsity-aware algorithm and ray-tracing core\nmapping,” in Proceedings of the 29th ACM International Conference\non Architectural Support for Programming Languages and Operating\nSystems, Volume 2, ASPLOS 2024, La Jolla, CA, USA, 27 April 2024-\n1 May 2024 . ACM, 2024, pp. 549–565.\n[35] G. Lu, R. Chen, Y . Wang, Y . Zhou, R. Zhang, Z. Hu, Y . Miao, Z. Cai,\nL. Li, J. Leng, and M. Guo, “Distsim: A performance model of large-\nscale hybrid distributed DNN training,” in Proceedings of the 20th ACM\nInternational Conference on Computing Frontiers, CF 2023, Bologna,\nItaly, May 9-11, 2023 . ACM, 2023, pp. 112–122.\n[36] Z. Ma, Y . Tan, H. Jiang, Z. Yan, D. Liu, X. Chen, Q. Zhuge, E. H. Sha,\nand C. Wang, “Unified-tp: A unified TLB and page table cache structure\nfor efficient address translation,” in 38th IEEE International Conference\non Computer Design, ICCD 2020, Hartford, CT, USA, October 18-21,\n2020. IEEE, 2020, pp. 255–262.\n[37] E. Mata, S. Bandeira, P. S. G. de Mattos Neto, W. T. A. Lopes, and\nF. Madeiro, “Accelerating families of Fuzzy K-Means algorithms for\nvector quantization codebook design,” Sensors, vol. 16, no. 11, p. 1963,\n2016.\n[38] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary,\nV . Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catan-\nzaro, A. Phanishayee, and M. Zaharia, “Efficient large-scale language\nmodel training on GPU clusters using megatron-lm,” in International\nConference for High Performance Computing, Networking, Storage and\nAnalysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021 .\nACM, 2021, p. 58.\n[39] NVIDIA, “Nvidia ampere ga102 gpu architecture,”\nhttps://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-\narchitecture-whitepaper-v2.pdf, 2021.\n[40] ——, “Nvidia h100 tensor core gpu archi-\ntecture,” https://www.advancedclustering.com/wp-\ncontent/uploads/2022/03/gtc22-whitepaper-hopper.pdf, 2022.\n[41] ——, “Cuda c++ programming guide,” 2024. [Online]. Available:\nhttps://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n[42] ——, “Cuda templates for linear algebra subroutines,” 2024. [Online].\nAvailable: https://github.com/NVIDIA/cutlass\n[43] ——, “Nvidia ada craft the engineering marvel of the rtx 4090.” 2024.\n[Online]. Available: https://images.nvidia.com/aem-dam/Solutions/\ngeforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-craft.pdf\n[44] ——, “Nvidia collective communications library (nccl),” 2024. [Online].\nAvailable: https://developer.nvidia.com/nccl\n[45] ——, “Nvidia cuda warp shuffle functions,” 2024. [Online]. Avail-\nable: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.\nhtml#warp-shuffle-functions\n[46] OpenAI, “GPT-4 technical report,” CoRR, vol. abs/2303.08774, 2023.\n[47] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: memory\noptimizations toward training trillion parameter models,” in Proceedings\nof the International Conference for High Performance Computing,\nNetworking, Storage and Analysis, SC 2020, Virtual Event / Atlanta,\nGeorgia, USA, November 9-19, 2020 . IEEE/ACM, 2020, p. 20.\n[48] K. A. Ross, Elementary Analysis: The Theory of Calculus . Springer\nNew York, NY , 2013.\n[49] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng,\nD. Choudhary, M. Cornea, E. Dellinger, K. Denolf, D. Stosic, V . Elango,\nM. Golub, A. Heinecke, P. James-Roxby, D. Jani, G. Kolhe, M. Lang-\nhammer, A. Li, L. Melnick, M. Mesmakhosroshahi, A. Rodriguez,\nM. Schulte, R. Shafipour, L. Shao, M. Y . Siu, P. Dubey, P. Micikevicius,\nM. Naumov, C. Verilli, R. Wittig, D. Burger, and E. S. Chung, “Mi-\ncroscaling data formats for deep learning,” CoRR, vol. abs/2310.10537,\n2023.\n[50] J. Shah, G. Bikshandi, Y . Zhang, V . Thakkar, P. Ramani, and T. Dao,\n“Flashattention-3: Fast and accurate attention with asynchrony and low-\nprecision,” CoRR, vol. abs/2407.08608, 2024.\n[51] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang,\nP. Gao, Y . Qiao, and P. Luo, “Omniquant: Omnidirectionally calibrated\nquantization for large language models,” CoRR, vol. abs/2308.13137,\n2023.\n[52] G. Shobaki, A. Kerbow, and S. Mekhanoshin, “Optimizing occupancy\nand ILP on the GPU using a combinatorial approach,” in CGO ’20:\n18th ACM/IEEE International Symposium on Code Generation and\nOptimization, San Diego, CA, USA, February, 2020 . ACM, 2020, pp.\n133–144.\n[53] J. Su, M. H. M. Ahmed, Y . Lu, S. Pan, W. Bo, and Y . Liu, “Roformer:\nEnhanced transformer with rotary position embedding,” Neurocomput-\ning, vol. 568, p. 127063, 2024.\n[54] M. Thread, “Mtt s4000: Empower large model ai with no limits,”\nhttps://en.mthreads.com/product/S4000, 2024.\n[55] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix,\nB. Rozi `ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\nE. Grave, and G. Lample, “Llama: Open and efficient foundation\nlanguage models,” CoRR, vol. abs/2302.13971, 2023.\n[56] A. Tseng, J. Chee, Q. Sun, V . Kuleshov, and C. D. Sa, “Quip#:\nEven better LLM quantization with hadamard incoherence and lattice\ncodebooks,” CoRR, vol. abs/2402.04396, 2024.\n[57] M. van Baalen, A. Kuzmin, M. Nagel, P. Couperus, C. Bas-\ntoul, E. Mahurin, T. Blankevoort, and P. N. Whatmough, “GPTVQ:\nthe blessing of dimensionality for LLM quantization,” CoRR, vol.\nabs/2402.15319, 2024.\n[58] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA , 2017, pp. 5998–6008.\n[59] H. Wang, T. Sun, and Q. Yang, “CAT - caching address tags: A technique\nfor reducing area cost of on-chip caches,” in Proceedings of the 22nd\nAnnual International Symposium on Computer Architecture, ISCA ’95,\nSanta Margherita Ligure, Italy, June 22-24, 1995 . ACM, 1995, pp.\n381–390.\n[60] S. Williams, A. Waterman, and D. A. Patterson, “Roofline: an insightful\nvisual performance model for multicore architectures,” Commun. ACM,\nvol. 52, no. 4, pp. 65–76, 2009.\n[61] C. Wolters, X. Yang, U. Schlichtmann, and T. Suzumura, “Memory is\nall you need: An overview of compute-in-memory architectures for ac-\ncelerating large language model inference,” CoRR, vol. abs/2406.08413,\n2024.\n[62] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n“Smoothquant: Accurate and efficient post-training quantization for large\nlanguage models,” in International Conference on Machine Learning,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , ser. Proceedings\nof Machine Learning Research, vol. 202. PMLR, 2023, pp. 38 087–\n38 099.\n[63] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y . Zou, “Hifi-codec:\nGroup-residual vector quantization for high fidelity audio codec,” CoRR,\nvol. abs/2305.02765, 2023.\n[64] L. Yu and J. Li, “Stateful large language model serving with pensieve,”\nCoRR, vol. abs/2312.05516, 2023.\n[65] Z. Yuan, Y . Shang, Y . Zhou, Z. Dong, C. Xue, B. Wu, Z. Li, Q. Gu,\nY . J. Lee, Y . Yan, B. Chen, G. Sun, and K. Keutzer, “Llm inference\nunveiled: Survey and roofline model insights,” 2024.\n[66] B. Zhang and R. Sennrich, “Root mean square layer normalization,”\nin Advances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 12 360–\n12 371.\n[67] H. Zhang, X. Ji, Y . Chen, F. Fu, X. Miao, X. Nie, W. Chen, and\nB. Cui, “Pqcache: Product quantization-based kvcache for long context\nllm inference,” CoRR, vol. abs/2407.12820, 2024.\n[68] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,\nM. T. Diab, X. Li, X. V . Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster,\nD. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,\n“OPT: open pre-trained transformer language models,” CoRR, vol.\nabs/2205.01068, 2022.\n[69] T. Zhang, J. Yi, Z. Xu, and A. Shrivastava, “KV cache is 1 bit\nper channel: Efficient large language model inference with coupled\nquantization,” CoRR, vol. abs/2405.03917, 2024.\n[70] L. Zheng, Z. Li, H. Zhang, Y . Zhuang, Z. Chen, Y . Huang, Y . Wang,\nY . Xu, D. Zhuo, E. P. Xing, J. E. Gonzalez, and I. Stoica, “Alpa:\nAutomating inter- and intra-operator parallelism for distributed deep\nlearning,” in 16th USENIX Symposium on Operating Systems Design\nand Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13,\n2022. USENIX Association, 2022, pp. 559–578. [Online]. Available:\nhttps://www.usenix.org/conference/osdi22/presentation/zheng-lianmin\n[71] L. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao,\nC. Kozyrakis, I. Stoica, J. E. Gonzalez, C. W. Barrett, and Y . Sheng,\n“Efficiently programming large language models using sglang,” CoRR,\nvol. abs/2312.07104, 2023.\n[72] S. Zheng, S. Chen, S. Gao, L. Jia, G. Sun, R. Wang, and Y . Liang,\n“Tileflow: A framework for modeling fusion dataflow via tree-based\nanalysis,” in Proceedings of the 56th Annual IEEE/ACM International\nSymposium on Microarchitecture, MICRO 2023, Toronto, ON, Canada,\n28 October 2023 - 1 November 2023 . ACM, 2023, pp. 1271–1288.\n[73] Y . Zhou, M. Yang, C. Guo, J. Leng, Y . Liang, Q. Chen, M. Guo,\nand Y . Zhu, “Characterizing and demystifying the implicit convolution\nalgorithm on commercial matrix-multiplication accelerators,” in IEEE\nInternational Symposium on Workload Characterization, IISWC 2021,\nStorrs, CT, USA, November 7-9, 2021 . IEEE, 2021, pp. 214–225.",
  "topic": "Vector quantization",
  "concepts": [
    {
      "name": "Vector quantization",
      "score": 0.797134518623352
    },
    {
      "name": "Computer science",
      "score": 0.5965761542320251
    },
    {
      "name": "Inference",
      "score": 0.5580483078956604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43807539343833923
    },
    {
      "name": "Code (set theory)",
      "score": 0.4302406311035156
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.4135058522224426
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35313189029693604
    },
    {
      "name": "Algorithm",
      "score": 0.3027346134185791
    },
    {
      "name": "Programming language",
      "score": 0.15295526385307312
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I5388228",
      "name": "University of Rochester",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 3
}