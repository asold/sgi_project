{
  "title": "Investigating the Effectiveness of Representations Based on Pretrained Transformer-based Language Models in Active Learning for Labelling Text Datasets",
  "url": "https://openalex.org/W3023176787",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2029910440",
      "name": "Lu, Jinghui",
      "affiliations": []
    },
    {
      "id": null,
      "name": "MacNamee, Brian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1596584299",
    "https://openalex.org/W107306860",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2163302275",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963742748",
    "https://openalex.org/W2426031434",
    "https://openalex.org/W2153819437",
    "https://openalex.org/W2161391345",
    "https://openalex.org/W1993202648",
    "https://openalex.org/W2138079527",
    "https://openalex.org/W2954886115",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2468328197",
    "https://openalex.org/W2966158511",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2099883114",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2144211451",
    "https://openalex.org/W2413669350",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W1964613733",
    "https://openalex.org/W2592771984",
    "https://openalex.org/W2180673283",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2171671120",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2085989833",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2037771830",
    "https://openalex.org/W2080021732",
    "https://openalex.org/W2964282813",
    "https://openalex.org/W2785540181",
    "https://openalex.org/W155912292",
    "https://openalex.org/W2018770010",
    "https://openalex.org/W2109943925",
    "https://openalex.org/W2115890667",
    "https://openalex.org/W2963012544"
  ],
  "abstract": "Active learning has been shown to be an effective way to alleviate some of the effort required in utilising large collections of unlabelled data for machine learning tasks without needing to fully label them. The representation mechanism used to represent text documents when performing active learning, however, has a significant influence on how effective the process will be. While simple vector representations such as bag-of-words and embedding-based representations based on techniques such as word2vec have been shown to be an effective way to represent documents during active learning, the emergence of representation mechanisms based on the pre-trained transformer-based neural network models popular in natural language processing research (e.g. BERT) offer a promising, and as yet not fully explored, alternative. This paper describes a comprehensive evaluation of the effectiveness of representations based on pre-trained transformer-based language models for active learning. This evaluation shows that transformer-based models, especially BERT-like models, that have not yet been widely used in active learning, achieve a significant improvement over more commonly used vector representations like bag-of-words or other classical word embeddings like word2vec. This paper also investigates the effectiveness of representations based on variants of BERT such as Roberta, Albert as well as comparing the effectiveness of the [CLS] token representation and the aggregated representation that can be generated using BERT-like models. Finally, we propose an approach Adaptive Tuning Active Learning. Our experiments show that the limited label information acquired in active learning can not only be used for training a classifier but can also adaptively improve the embeddings generated by the BERT-like language models as well.",
  "full_text": "Investigating the Eﬀectiveness of Representations Based on Pretrained\nTransformer-based Language Models in Active Learning for Labelling Text Datasets\nJinghui Lu, Brian Mac Namee\nSchool of Computer Science, University College Dublin, Ireland\nAbstract\nManually labelling large collections of text data is a time-consuming and expensive task, but one that is necessary to\nsupport machine learning based on text datasets. Active learning has been shown to be an eﬀective way to alleviate\nsome of the eﬀort required in utilising large collections of unlabelled data for machine learning tasks without needing\nto fully label them. The representation mechanism used to represent text documents when performing active learning,\nhowever, has a signiﬁcant inﬂuence on how eﬀective the process will be. While simple vector representations such as\nbag-of-words and embedding-based representations based on techniques such as word2vec have been shown to be an\neﬀective way to represent documents during active learning, the emergence of representation mechanisms based on the\npre-trained transformer-based neural network models popular in natural language processing research (e.g. BERT, GPT-\n2, XLNet ) oﬀer a promising, and as yet not fully explored, alternative. This paper describes a comprehensive evaluation\nof the eﬀectiveness of representations based on pre-trained transformer-based language models for active learning. This\nevaluation shows that transformer-based models, especially BERT-like models, that have not yet been widely used in\nactive learning, achieve a signiﬁcant improvement over more commonly used vector representations like bag-of-words or\nother classical word embeddings like word2vec. This paper also investigates the eﬀectiveness of representations based on\nvariants of BERT such as Roberta, DistilBert, and Albert as well as comparing the eﬀectiveness of the “[CLS]” token\nrepresentation and the aggregated representation that can be generated using BERT-like models. Finally, we propose\nan approach to tune the representations generated by BERT-like transformer models during the active learning process,\nAdaptive Tuning Active Learning. Our experiments show that the limited label information acquired in active learning\ncan not only be used for training a classiﬁer but can also adaptively improve the embeddings generated by the BERT-like\nlanguage models as well.\nKeywords: active learning, word embeddings, Transformer, BERT, text labelling\n1. Introduction\nActive learning (AL) (Settles, 2009) is a semi-\nsupervised machine learning technique that minimises the\namount of labelled data required to build accurate predic-\ntion models. In active learning only the most informative\ninstances from an unlabelled dataset are selected to be la-\nbelled by an oracle (i.e. a human annotator) to expedite\nthe learning procedure. This property makes active learn-\ning attractive in scenarios where unlabelled data may be\nabundant but labelled data is expensive to obtain—for ex-\nample image classiﬁcation (Tong and Chang, 2001; Zhang\nand Chen, 2002), speech recognition (Tur et al., 2005),\nand text classiﬁcation (Hoi et al., 2006; Liere and Tade-\npalli, 1997; Zhang et al., 2017; Singh et al., 2018). The use\nof active learning for text classiﬁcation is the focus of this\nwork. One crucial component in active learning systems\n∗Corresponding author\nEmail addresses: Jinghui.lu@ucd.connect.ie (Jinghui\nLu), Brian.MacNamee@ucd.ie (Brian Mac Namee)\nfor text classiﬁcation is the mechanism used to represent\ndocuments in the tabular structure required by most ma-\nchine learning algorithms.\nVectorized representations based on word frequencies,\nsuch as bag-of-words (BOW), are the most commonly used\nrepresentations in active learning (Singh et al., 2018; Hu\net al., 2010, 2008; Wallace et al., 2010; Siddhant and Lip-\nton, 2018; Miwa et al., 2014). Considerable recent work,\nhowever, has shown that representations of natural lan-\nguage based on learned word embeddings can be useful for\na wide range of natural language processing (NLP) tasks\nincluding text classiﬁcation (Mikolov et al., 2013a; Pen-\nnington et al., 2014; Bojanowski et al., 2017; Howard and\nRuder, 2018; Radford et al., 2018; Devlin et al., 2018; Pe-\nters et al., 2018). Standard approaches to learning word\nembeddings like word2vec (Mikolov et al., 2013a), Glove\n(Pennington et al., 2014), andFastText (Bojanowski et al.,\n2017; Joulin et al., 2016), or contextualized approaches\nsuch as Cove (McCann et al., 2017) and ElMo (Peters\net al., 2018) convert words to ﬁxed-length dense vectors\nthat capture semantic and syntactic features, and allow\nPreprint submitted to Expert Systems with Applications April 29, 2020arXiv:2004.13138v1  [cs.IR]  21 Apr 2020\nmore complex structures (like sentences, paragraphs and\ndocuments) to be encoded as aggregates of these vectors.\nMore recently document-level approaches have been\ndeveloped, such as ULM-Fit (Howard and Ruder, 2018),\nOpenAI GPT (Radford et al., 2018), and BERT (De-\nvlin et al., 2018), that are pre-trained with large-scale\ngeneric corpora and then ﬁne tuned to a speciﬁc task.\nThe use of these approaches has been shown to signiﬁ-\ncantly increase the performance in many downstream NLP\ntasks (Devlin et al., 2018; Liu et al., 2019; Radford et al.,\n2019), and has also been shown to be useful for trans-\nferring knowledge learned from large generic corpora to\ndownstream tasks focused on much more speciﬁc corpora.\nTransformer-based pre-trained models such as the bidirec-\ntional encoder representations from transformers (BERT)\nmodel have achieved particularly impressive results across\nmany NLP tasks (Devlin et al., 2018). Even though word\nembeddings and transformer-based language models have\nbeen widely applied in text classiﬁcation, there is little\nwork devoted to leveraging them in active learning for text\nclassiﬁcation (Zhang et al., 2017; Zhao, 2017; Siddhant and\nLipton, 2018), and a comprehensive benchmark compari-\nson of their usefulness for active learning does not exist in\nthe literature.\nTo address this gap in the literature this paper ad-\ndresses four research questions:\nRQ1: Are representations generated using pre-trained\ntransformer-based language models more eﬀective\nthan other more commonly used representations in\nthe context of active learning for text labelling? If\nso, which pre-trained model generates the most ef-\nfective representations?\nRQ2: Can lightweight versions of transformer-based\nmodels be used instead of the standard large models to\nreduce the computational burden during active learn-\ning while still maintaining high performance levels?\nRQ3: When using embeddings generated using a\ntransformer-based model is it more eﬀective to rep-\nresent a document using an aggregate of the word\nlevel embeddings produced by the model or to use the\nembedding of the “[CLS]” token?\nRQ4: Can we further improve the performance of an ac-\ntive learning system using a transformer-based model\nby ﬁne tuning the model?\nTo answer these research questions, this paper de-\nscribes a comprehensive evaluation experiment that ex-\nplores the eﬀectiveness of various text representation tech-\nniques for active learning in a text classiﬁcation context.\nThis evaluation, based on 8 datasets from diﬀerent do-\nmains, including product reviews, news articles, and blog\nposts, shows that representations based on pre-trained\ntransformer-based language models—and especially repre-\nsentations based on Roberta—consistently outperform the\nmore commonly used vector representations such as word\nembeddings or bag-of-words. This demonstrates the ef-\nfectiveness of transformer-based representations for active\nlearning.\nBased on this ﬁrst results we also report further studies\nusing the same datasets that compare the performance of\nfull and lightweight versions of the transformer-based mod-\nels examined; that compare the performance of systems us-\ning the “[CLS]” token document representation and aggre-\ngated words document representation; and then we show\nthat by ﬁne tuning transformer-based models during active\nlearning the performance of the active learning process can\nbe improved. Taken together these experiments illustrate\nthat by using text representations from transformer-based\nmodels some of the promise of deep learning (LeCun et al.,\n2015) can be brought to active learning while avoiding the\nconsiderable practical challenges of placing a deep neural\nnetwork at the heart of the active learning process (Zhang\net al., 2017; Zhao, 2017; Siddhant and Lipton, 2018; Zhang,\n2019). 7\nThe remainder of the paper is organized as follows:\nSection 2 describes pool-based active learning, the text\nrepresentation techniques used in this paper, and existing\nwork related to the use of transformer-based models in ac-\ntive learning; Section 3 describes the experiment compar-\ning the eﬀectiveness of diﬀerent text representation tech-\nniques for active learning; Section 4 describes the experi-\nment that compares the performance of the active learning\nprocess when lightweight and complete variants of BERT\nare used as well as the experiment comparing the use of\nthe “[CLS]” token representation and aggregate represen-\ntations; Section 5 shows the impact that ﬁne tuning the\ntransformer-based models has on the active learning pro-\ncess; and, ﬁnally, Section 6 draws conclusions and suggests\ndirections for future work.\n2. Related Work\nIn this section we explain what is meant by pool-based\nactive learning, describe the diﬀerent text representations\nthat are used in the experiments described in this paper,\nand describe existing work that investigates the impact of\nusing diﬀerent ext representations in active learning.\n2.1. Pool-based Active Learning\nThe goal of active learning is to utilise a large collec-\ntion of unlabelled data for supervised learning with mini-\nmal human labelling eﬀort. In pool-based active learning, a\nsmall set of labelled instances is used to seed an initial la-\nbelled dataset, L. Then, according to a particular selection\nstrategy, a batch of data to be presented to an oracle for\nlabelling is chosen from the unlabelled data pool, U. After\nlabelling, these newly labelled instances will be removed\nfrom Uand appended to L. This process repeats until a\npredeﬁned stopping criterion has been met (for example a\nlabel budget has been exhausted).\n2\nThe instances labelled by the oracle through this pro-\ncess can be used to train a predictive model. This model\nmight be the ﬁnal output of the overall process, or this\nmodel can be used to generate labels for the remaining un-\nlabelled instances in Uto generate a fully labelled dataset\nwith minimal eﬀort from the oracle. We mainly consider\nthis latter scenario in this paper.\nThe selection strategy used to pick the unlabelled in-\nstances that will be presented to the oracle for labelling\nplays a vital role in active learning. Many diﬀerent selec-\ntion strategies are described in the literature. Model-based\nselection strategies—such as uncertainty sampling (Lewis\nand Gale, 1994) and query-by-committee (Seung et al.,\n1992)—utilise models trained with the currently labelled\ninstances, L, to infer the “ informativeness” of unlabelled\ninstances from U. A small batch of the most informative\ninstances from the unlabelled pool are presented to the\noracle for labelling at each iteration of the active learning\nprocess. Model-free selection strategies—such as explo-\nration guided active learning (EGAL) (Hu et al., 2010)—\nrely entirely on the features of instances in Land U to\ncompute the “informativenes” of each unlabelled instance\nwithout requiring the construction of a predictive model.\nAlthough a comparison of the eﬀectiveness of diﬀerent\nselection strategies is not the focus of this paper, we use\nseveral commonly used selection strategies in our experi-\nments to mitigate the impact of selection strategies when\ncomparing the eﬀectiveness of text representations. In the\nexperiments described in this paper we use random sam-\npling (sample i.i.d from U), uncertainty sampling (Lewis\nand Gale, 1994), query-by-committee (Seung et al., 1992),\ninformation-density (ID) (Settles and Craven, 2008), and\nEGAL (Hu et al., 2010) to alleviate the inﬂuence caused\nby diﬀerent selection strategies.\n2.2. Text Representations\nThe technique used to represent documents has a large\nimpact on the eﬀectiveness of active learning for text\ndata. Representations schemes range from simple fre-\nquency based vector representations, like bag-of-words, to\nmore sophisticated approaches based on word embeddings.\nThis section describes the text representation schemes\nused in the experiments described in this paper. We de-\nscribe simple frequency-based vector representations, word\nembedding representaitons, and transformer-based repre-\nsentations.\n2.2.1. Frequency-based Vector Representations\nBag-of-words (BOW) is the most basic representation\nscheme for documents, and has been widely used in many\nactive learning applications (Singh et al., 2018; Hu et al.,\n2010, 2008; Wallace et al., 2010; Siddhant and Lipton,\n2018; Miwa et al., 2014). Each column of a BOW vector\nrecords the number of times a word occurs in a document\n(known as term-frequency (TF)), and 0 if the word is ab-\nsent. The frequency of words is often weighted by inverse\ndocument frequency to penalise terms commonly used in\nmost documents. This is known as TF-IDF (Sparck Jones,\n1972).\nLatent Dirichlet Allocation (LDA) (Blei et al., 2003)\nis a topic modelling technique designed to infer the dis-\ntribution of membership to a set of topics across a set\nof documents. The model generates a term-topic matrix\nthat captures the association between words and topics\nand a document-topic matrix that captures the associa-\ntion between a documents and topics. Each row of the\ndocument-topic matrix is a topic-based representation of\na document where the ith column determines the degree of\nassociation between the ith topic and the document. This\ntype of topic representation of documents has been used in\nactive learning for labelling the relevance of studies to sys-\ntematic literature reviews (Singh et al., 2018; Hashimoto\net al., 2016; Mo et al., 2015).\n2.2.2. Word Embedding Representations\nA word embedding is a word representation learned\nvia mapping words into a vector space where the distance\nbetween words in that space is related to the semantic\nand syntactic features of the words. Word embeddings\nare typically learned by building prediction models that\nperform context prediction, for example, predicting the\nmost likely target word given a set of context words.\nThe word embedding representations investigated in\nthis study are: word2vec (Mikolov et al., 2013a,b), which\nwas the earliest word embedding technique to garner\nwidespread attention and is very commonly used; Glove\n(Pennington et al., 2014) which learns embeddings by ap-\nproximating the relationship between word vectors and\nthe word co-occurrence probabilities matrix; and Fast-\nText (Bojanowski et al., 2017) which is a trained neural\nlanguage model that enriches the training of word embed-\ndings with subword information which improves the ability\nto obtain word embeddings of out-of-bag words.\nIn our experiments, as is common practice, we aver-\nage the vectors of words appeared in the document as the\ndocument representation.\n2.2.3. Transformer-based Representations\nThe transformer model was proposed by Vaswani et\nal. (2017) and has since received massive attention from\nthe NLP research community. The key idea behind\ntransformer-based prediction models is extracting gen-\neral language knowledge via pre-training the model with\nlarge unlabelled generic corpora, then ﬁne-tuning the pre-\ntrained model to ﬁt the speciﬁc downstream tasks. It is\npossible, however, to use the document representations\nthat arise from pre-training transformer models without\nﬁne-tuning. We investigate the eﬀectiveness of embeddings\nproduced by the most well-known transform-based models\nin this study.\nBERT (Devlin et al., 2018) has achieved amazing re-\nsults in many NLP tasks. This model is trained with the\nplain text through masked language modeling (MLM) and\n3\nnext sentence prediction (NSP) tasks to enable a bidirec-\ntional learning of contextualized word embeddings. Con-\ntextualized word embedding implies a word can have dif-\nferent embeddings according to its context which allevi-\nates the problems caused by polysemy etc. BERT was\nreported to achieve state-of-the-art results across 11 NLP\ntasks when it was proposed. Though it has since been\nout-performed by other language models, BERT is still re-\ngarded as an important step in NLP and most transformer-\nbased models are variants of BERT or rely upon ideas from\nBERT.\nRoberta (Liu et al., 2019) is a an optimized version of\nBERT where key hyper-parameters of the model are more\ncarefully selected, the training process is modiﬁed (the\nNSP task is removed and the MLM task is slightly mod-\niﬁed), and the model is pre-trained with larger datasets\nthan those used to train BERT. Roberta is still near the\ntop of leaderboard in GLUE benchmark. 1\nDistilBert (Sanh et al., 2019) is a lightweight ver-\nsion of BERT in which knowledge distillation is leveraged\nto transfer generalization capabilities from regular BERT.\nSanh et al. (2019) show that “ it is possible to reduce the\nsize of a BERT model by 40%, while retaining 97% of its\nlanguage understanding capabilities and being 60% faster”.\nAlbert (Lan et al., 2019) is another lightweight version\nof BERT that has lower memory requirements and higher\ntraining speed. Embedding matrix factorization and cross-\nlayer parameter sharing are applied to reduce the number\nof parameters in the model. During training the NSP task\nis replaced with a harder sentence-order prediction task to\nmaintain the generalisation ability of the language model.\nXLNet (Yang et al., 2019) is another transformer-\nbased model which has drawn much attention. It replaces\nthe MLM training task used by BERT with a permutation\nlanguage model task and uses a larger corpus than BERT\nfor training. XLNet has been shown to outperform BERT\non most tasks (Yang et al., 2019), especially for tasks in-\nvolving longer text sequences.\nGPT-2 (Radford et al., 2019) is a scaled-up version of\nOpenAI GPT model (Radford et al., 2018) which is also\ntransformer-based. Diﬀerent from the high-proﬁle BERT-\nlike model, GPT-2 adopted a unidirectional transformer\nwith more layers (48 layers with 1.5 billion parameters).\nAs compared to BERT, GPT-2 is pre-trained only via pre-\ndicting the next word given previous words with a very\nlarge high-quality web text corpus across many domains.\nIn our experiments, due to the GPU memory limitation,\nwe adopted a small architecture of GPT-2. 2\n2.3. Using Word Representation Produced by Neural Net-\nwork in Active Learning\nAlthough applying word embedding representations\nand transformer-based representations in text classiﬁca-\ntion has attracted considerable attention in the literature\n1https://gluebenchmark.com/leaderboard\n2https://github.com/huggingface/transformers\n(Mikolov et al., 2013a; Bojanowski et al., 2017; Howard\nand Ruder, 2018; Devlin et al., 2018), the use of these\nrepresentations in active learning is not well explored.\nThere are some studies proposed deep active learning pro-\ncedure where word2vec are combined with convolutional\nneural networks (CNN)(Zhang et al., 2017) or recurrent\nneural networks and gated recurrent units to predict the\nclasses of documents (Zhao, 2017). Very recently, Zhang\nYe (2019) proposed a selection strategy that utilized the\ndiscrepancy between a basic BERT model and a BERT\nmodel which is continuously pre-trained via the MLM\ntask using a local corpus. However, the experiments dec-\ncribes by Zhang Ye focused on selection strategies, rather\nthan the impacts of diﬀerent text representation tech-\nniques. Siddhant and Lipton (2018) compare the perfor-\nmance of Glove-embedding-based active learning frame-\nworks, which are composed of diﬀerent classiﬁers such as\nbi-LSTM model and CNN, across many NLP tasks. They\nﬁnd that Glove embeddings selected by Bayesian Active\nLearning by Disagreement plus Monte Carlo Dropout or\nBayes-by-Backprop Dropout usually outperforms the shal-\nlow baseline. However, Siddhant and Lipton take Lin-\near SVM combined with BOW representation rather than\nGlove embeddings as a shallow baseline which makes the\nconclusion limited. Additionally, these studies focus on\ncomparing the impact of selection strategies when used\nwith deep neural networks, instead of that of text repre-\nsentations.\nApart from leveraging deep learning classiﬁer and word\nembedding, some studies combine word embedding with\nlow complexity machine learning algorithms such as Sup-\nport Vector Machine (SVM). Hashimoto et al. (2016)\npropose a method, paragraph vector-based topic detection\n(PV-TD), that combines doc2vec (Le and Mikolov, 2014)\n(an extension of word2vec) with k-means clustering to per-\nform simple topic modelling. For the active learning pro-\ncess documents, which are represented by their distance to\nthe cluster centres that result from the application of k-\nmeans, are fed into the SVM model. In their experiments\nPV-TD is shown to perform well compared to representa-\ntions based on an LDA, and word2vec. Interestingly, Singh\net al. (2018) extend the experiments in Hashimoto et al.\n(2016) with more datasets in the health domain, demon-\nstrating that directly using doc2vec or BOW, rather than\nPV-TD, can achieve better results which is contrary to\nthat obtained by Hashimoto.\nDespite the promising results, these studies of active\nlearning only using classical word embeddings explore a\nlimited number of selection strategies (i.e. certainty sam-\npling and certainty information gain sampling) and just\nfocus on highly imbalanced datasets from specialist medi-\ncal domains.\nTo the best of the authors’ knowledge, there is no re-\nsearch investigating the eﬀectiveness of transformer-based-\nrepresentations in active learning for text classiﬁcation.\nThis research ﬁlls this gap by comparing the eﬀective-\nness of various text representations in active learning in\n4\na benchmarking experiment that uses a range of selection\nstrategies and datasets from multiple domains.\n3. Comparing the Eﬀectiveness of Text Represen-\ntations\nThis section describes the design of an experiment per-\nformed to evaluate the eﬀectiveness of diﬀerent text repre-\nsentation mechanisms in active learning, addressing RQ1\nfrom Section 1. To mitigate the inﬂuence of diﬀerent se-\nlection strategies on the performance of the active learn-\ning process we also include a number of diﬀerent selection\nstrategies in the experiment. The following subsections\ndescribe the experimental framework, the conﬁguration of\nthe models used, the performance measures used to judge\nthe eﬀectiveness of diﬀerent approaches, the datasets used\nin the experiments, and the experimental results and anal-\nysis.\n3.1. Experimental Framework\nWe apply pool-based active learning using diﬀerent\ntext representation techniques and selection strategies over\nseveral well-balanced fully labelled datasets. All datasets\nare from binary classiﬁcation problems. The use of fully\nlabelled datasets allows us to simulate data labelling by a\nhuman oracle, and is common in active learning research\n(Zhang et al., 2017; Zhao, 2017; Singh et al., 2018; Hu\net al., 2010; Hashimoto et al., 2016). At the outset, we\nprovide all learners with the same 10 instances (i.e. 5 pos-\nitive instances and 5 negative instances) sampled i.i.d. at\nrandom from a dataset to seed the active learning pro-\ncess. Subsequently, 10 unlabelled instances, whose ground\ntruth labels will be revealed to each learner, are selected\naccording to a certain selection strategy. These examples\nare moved from Uto L(with their labels) and the classi-\nﬁers are retrained. We assume it is unrealistic to collect\nmore than 1,000 labels from an oracle, and so we stop the\nprocedure when an annotation budget of 1,000 labels is\nused up. As the batch size for selection is 10, this means\nthat an experiment is composed of 100rounds of the active\nlearning process which uses up the label budget of 1,000\nlabels. After each round the retrained classiﬁer is used\nto label all of the examples remaining in U. Algorithm\n1 shows the details of the active learning procedure using\nuncertainty sampling as an example selection strategy. In\nour experiment, this process is repeated 10 times using dif-\nferent random seeds. The performance measures reported\nare averaged across these repetitions.\nThe classiﬁers used in the active learning process in\nall of our experiments are Linear-SVM models 3, which\nhave been shown empirically to perform well with high\ndimensional data (Hsu et al., 2003). We tune the hyper-\nparameters of the SVM models every 10 iterations (i.e. 100\n3https://scikit-learn.org/stable/modules/gen-\nerated/sklearn.svm.SVC.html\nAlgorithm 1: Pseudo-code for active leaning.\nInput: T, set of all corpus\nP, index of all ground truth positive documents\nN, index of all ground truth negative documents\nLM, pre-trained language model\nOutput: S, set of accuracy+ scores of each loop\nL, set of documents pseudo labelled by the oracle\nR, set of documents labelled by the classiﬁer\n1 Initialization\n2 // Infer document representations\n3 E ←Inference (T, LM);\n4 // Random sampling 5 neg and 5 pos\n5 L ←Random(E, P,5) ∪Random(E, N,5);\n6 ¬L ←E \\L;\n7 R ←∅;\n8 loop ←0;\n9 Params ←∅;\n10 S ←∅;\n11 while |L|≤ 1000 do\n12 CL, Params←Train (loop, L, P, N, Params);\n13 X ←Query(CL, ¬L);\n14 L, ¬L, R←Assign(CL, L, X);\n15 // Compute accuracy+ score\n16 S ←S ∪Eval(L, R, P, N);\n17 loop ←loop + 1;\n18 Function Train(loop, L, P, N, Params)\n19 if loop mod 10 == 0 then\n20 // Cross-validation for hyper-parameters\n21 Params ←CV (L, P, N);\n22 // Train linear SVM\n23 CL ←SV M(L, Params);\n24 else\n25 CL ←SV M(L, Params)\n26 return CL, Params\n27 Function Query(CL, ¬L)\n28 // Uncertainty sampling 10 examples\n29 X ←argsort(abs(CL.deci func(¬L)))[: 10];\n30 return X\n31 Function Assign(CL, L, X)\n32 L ←L ∪X;\n33 ¬L ←E \\L;\n34 R ←CL.predict(¬L);\n35 return L, ¬L, R\nlabels requested) using the currently labelled dataset, L.\nIn uncertainty sampling, the most uncertain examples are\nequivalent to those closest to the class separating hyper-\nplane in the context of an SVM (Tong and Koller, 2001).\nIn the information density selection strategy, we use en-\ntropy to measure the “informativeness” and all parameters\nare set following Settles and Craven (2008). In QBC, we\nchoose Linear-SVM models trained using bagging as com-\nmittee members following Mamitsuka et al. (1998). Since\n5\nthere is no general agreement in the literature on the ap-\npropriate committee size for QBC (Settles, 2009), we adopt\na committee size 5 after some preliminary experiments. In\nEGAL, all parameters are set following the recommenda-\ntions given in Hu et al. (2010), which are shown to perform\nwell for text classiﬁcation tasks. All experiments are run\non the machine with GPU (NVIDIA GeForce GTX 1080\nTi) and CPU (Intel Core i7-8700K 3.70 GHz).\n3.2. Conﬁguration of the Text Representation Techniques\nFor the frequency based vector representations, BOW\nand LDA, we preprocess text data by converting to lower-\ncase, removing stop words, and removing rare terms (for\nthe whole dataset, word count less than 10 or document\nfrequency less than 5). We set the number of topics to be\nused by LDA4 to 300 following Singh et al. (2018).\nIn this experiment three diﬀerent pre-trained word em-\nbedding representations are used: word2vec (the 300 di-\nmension version trained using the google news corpus),\nGlove (the 300 dimension version trained using the wiki\ngigaword corpus) and FastText (the 300 dimension version\ntrained using the Wikipedia corpus) which are all trained\nwith large online corpora. 5 For all of the word embedding\nrepresentations, we average the vectors of the words that\nappeared in the document to represent each document.\nThis experiment uses 4 of the most high-proﬁle\ntransformer-based language models: BERT (“bert-base-\nuncased”), XLNet (“xlnet-base-cased”), GPT-2 (“gpt2”)\nand Roberta (“roberta-base”). The codes beside the al-\ngorithms are the speciﬁc models used in the experiments\nwhich can be found on Github. 6 Since all transformer-\nbased models are conﬁgured to take as input a maximum of\n512 tokens, we divided the long documents with W words\ninto k = W/511 fractions, which is then fed to the model\nto infer the representation of each fraction (each fraction\nhas a “[CLS]” token in front of 511 tokens, so, 512 tokens\nin total). The vector of each fraction is the average em-\nbeddings of words in that fraction and the representation\nof the whole text sequence is the mean of all k fraction\nvectors. It should be noted that we do not use any la-\nbel information for ﬁne-tuning any model to ensure fair\ncomparisons. A summary of the dimensionality of each\nrepresentation is given in Table 1.\n3.3. Performance Measures\nAs we are interested in the ability of an active learn-\ning process to fully label a dataset we use the accuracy+\nperformance measure, which has been previously used by\nHu et al. (2008, 2010). This measures the performance of\nthe full active learning system including human annotators\nwhich is prevalent in active learning community (Wallace\n4https://radimrehurek.com/gensim/models/ldamodel.html\n5https://radimrehurek.com/gensim/index.html\n6https://github.com/huggingface/transformers\net al., 2010; Hashimoto et al., 2016). It can be expressed\nas:\naccuracy+ = TP H + TN H + TP M + TN M\nN (1)\nwhere N is the total number of instances in a dataset and\nsuperscripts H and M express human annotator and ma-\nchine generated labels respectively. TP and TN denote\nthe number of true positives and true negatives respec-\ntively. Intuitively, this metric computes the fraction of\ncorrectly labelled instances which are predicted by the or-\nacles as well as a trained classiﬁer. We presume that a\nhuman annotator never makes mistakes.\nWe also report the area under the learning curve\n(AULC) score based on accuracy+ to measure the overall\nperformance of the active learning process when diﬀerent\nrepresentations are used. As we can see in Figure 1(a),\neach line represents a learning curve of a representation\ntechnique. X-axis represents the number of documents\nthat have been manually annotated and Y-axis denotes ac-\ncuracy+ and the AULC score is the area under each curve.\nFor example, the area shaded yellow is the AULC score of\nlearning curve of LDA representation. In this work, all\nAULC scores are computed using the trapezoidal rule and\nnormalized by the maximum possible area, to bound the\nvalue between 0 and 1.\n3.4. Datasets\nWe evaluate the performance of active learning sys-\ntems using 8 fully-labelled datasets. Four of these datasets\nare based on long text sequences: Movie Review (MR)\n(Pang and Lee, 2004), 7 Multi-domain Customer Review\n(MDCR) (Blitzer et al., 2007), 8 Blog Author Gender\n(BAG) (Mukherjee and Liu, 2010) 9 and Guardian2013\n(G2013) (Belford et al., 2018). Four are based on sen-\ntences: Additional Customer Review (ACR) (Ding et al.,\n2008), Movie Review Subjectivity (MRS) (Pang and Lee,\n2004), Ag news (AGN) 10 and Dbpedia (DBP) (Zhang\net al., 2015). Table 1 provides summary statistics describ-\ning each dataset.\n7MR and MRS are available at:\nhttp://www.cs.cornell.edu/people/pabo/movie-review-data/\n8https://www.cs.jhu.edu/˜mdredze/datasets/sentiment/index2.html\n9BAG and ACR are available at:\nhttps://www.cs.uic.edu/˜liub/FBS/sentiment-analysis.html\n10AGN and DBP are available at :\nhttps://skymind.ai/wiki/open-datasets\n6\nTable 1: Statistics of 8 datasets used on our experiments, the number of instances of each class and the length of the diﬀerent representations\ngenerated (the representations generated using all of the trasnsformer-based models have the same dimensionality).\n# of Instances Representation Dimensionality\nDataset positives negatives LDA TF-IDF FastText Glove word2vec Transformer-based\nMR 1,000 1,000 300 6,181 300 300 300 768\nMDCR 4,000 3,566 300 4,165 300 300 300 768\nBAG 1,675 1,552 300 4,936 300 300 300 768\nG2013 843 1,292 300 5,345 300 300 300 768\nACR 1,335 736 300 403 300 300 300 768\nMRS 5,000 5,000 300 1,868 300 300 300 768\nAGN 1,000 1,000 300 723 300 300 300 768\nDBP 1,000 1,000 300 552 300 300 300 768\nTable 2: The number of parameters and inference time cost by various pre-trained transformer-based language models (second). The best\nperforming model is highlighted and the second to last row denotes the average ranking of each model. The last row shows the number of\nparameters of each model which implies the memory used by each model.\nBERT GPT-2 XLNet DistilBert Albert Roberta\nMR 152.51±6.91(2.0) 192.08±12.86(4.0) 279.76±15.08(6.0) 81.64±2.28(1.0) 193.07±8.64(5.0) 165.39±12.33(3.0)\nMRS 367.3±22.9(2.0) 418.4±24.9(5.0) 475.16±20.44(6.0) 193.12±13.48(1.0) 391.52±24.99(4.0) 373.29±22.7(3.0)\nACR 73.75±4.43(3.0) 81.35±4.8(5.0) 91.11±3.1(6.0) 37.65±1.78(1.0) 76.96±4.03(4.0) 73.46±4.0(2.0)\nBAG 191.49±10.53(2.0) 237.89±9.79(5.0) 286.5±8.28(6.0) 100.94±3.47(1.0) 222.37±8.36(4.0) 196.25±9.35(3.0)\nMDCR 295.56±14.2(2.0) 349.03±16.2(5.0) 411.43±19.39(6.0) 161.61±7.04(1.0) 317.86±14.0(4.0) 296.79±15.58(3.0)\nG2013 161.75±11.73(2.0) 209.39±12.67(5.0) 258.95±11.03(6.0) 83.32±4.41(1.0) 193.41±10.05(4.0) 173.87±10.54(3.0)\nAGN 71.52±6.21(3.0) 78.97±6.87(5.0) 92.64±7.45(6.0) 36.99±3.69(1.0) 72.23±5.86(4.0) 69.88±5.59(2.0)\nDBP 66.67±3.49(2.0) 76.46±4.7(5.0) 89.69±6.51(6.0) 34.69±2.54(1.0) 69.68±4.46(4.0) 67.33±4.01(3.0)\nRank 2.25 4.88 6.00 1.00 4.12 2.75\n#Params 109,482,240 124,439,808 116,718,336 66,362,880 11,683,584 124,645,632\nTable 3: The summary of AULC scores, which are computed by trapezoidal rule and normalized by the maximum possible area, on each dataset\nregarding the diﬀerent combinations of text representations and selection strategies. The best performance of each dataset is highlighted and\nthe last column denotes the average ranking of each method where the smaller number suggests a higher rank.\nRep Strategy MR MDCR BAG G2013 ACR MRS AGN DBPRankBERTuncertainty0.891±0.005(15.0)0.819±0.006(14.0)0.738±0.006(4.0)0.980±0.001(9.0)0.862±0.003(10.0)0.932±0.000(4.0)0.991±0.000(8.5)0.995±0.001(1.0)8.19XLNetuncertainty0.952±0.004(1.0)0.865±0.005(4.0)0.744±0.005(2.0)0.985±0.001(1.0)0.886±0.007(5.0)0.917±0.001(11.0)0.994±0.000(3.0)0.994±0.001(4.0)3.88GPT-2uncertainty0.911±0.004(8.0)0.843±0.003(9.0)0.739±0.006(3.0)0.984±0.001(2.0)0.875±0.003(7.0)0.930±0.001(6.0)0.995±0.000(1.0)0.988±0.001(12.0)6.00Robertauncertainty0.950±0.002(3.0)0.879±0.004(1.0)0.747±0.007(1.0)0.983±0.002(4.0)0.903±0.002(1.0)0.949±0.001(1.0)0.993±0.000(4.0)0.995±0.000(2.0)2.12Glove QBC0.845±0.003(30.0)0.715±0.008(30.0)0.701±0.018(28.5)0.977±0.001(16.0)0.787±0.009(23.0)0.900±0.003(21.0)0.971±0.001(30.0)0.978±0.002(24.0)25.31word2vec QBC0.872±0.003(18.0)0.733±0.007(22.0)0.706±0.012(23.0)0.975±0.003(19.5)0.792±0.006(21.0)0.893±0.002(26.0)0.989±0.001(13.0)0.979±0.002(23.0)20.69FastText QBC0.857±0.005(24.0)0.724±0.007(26.5)0.706±0.013(22.0)0.976±0.001(18.0)0.788±0.007(22.0)0.904±0.002(17.0)0.976±0.002(24.0)0.993±0.002(8.5)20.25TF-IDF QBC0.854±0.012(26.0)0.713±0.007(31.0)0.676±0.007(37.0)0.965±0.002(24.0)0.778±0.005(24.0)0.812±0.005(35.0)0.932±0.004(36.0)0.952±0.007(33.0)30.75LDA QBC0.770±0.009(44.0)0.607±0.009(43.0)0.675±0.008(38.0)0.917±0.008(42.0)0.763±0.008(31.0)0.689±0.006(41.0)0.901±0.005(39.0)0.905±0.006(41.0)39.88\n7\nTable 4: The summary of AULC scores, which are computed by trapezoidal rule and normalized by the maximum possible area, on each dataset regarding the diﬀerent combinations\nof text representations and selection strategies. The best performance of each dataset is highlighted and the last column denotes the average ranking of each method where the smaller\nnumber suggests a higher rank.\nRep Strategy MR MDCR BAG G2013 ACR MRS AGN DBPRank\nBERT\nrandom0.856±0.006(25.0)0.800±0.003(17.0)0.722±0.005(17.0)0.947±0.003(36.5)0.816±0.006(18.0)0.913±0.002(12.5)0.971±0.002(29.0)0.986±0.001(15.0)21.25\nuncertainty0.891±0.005(15.0)0.819±0.006(14.0)0.738±0.006(4.0)0.980±0.001(9.0)0.862±0.003(10.0)0.932±0.000(4.0)0.991±0.000(8.5)0.995±0.001(1.0)8.19\nEGAL 0.841±0.013(32.0)0.738±0.010(20.0)0.693±0.005(32.0)0.949±0.003(33.0)0.818±0.005(17.0)0.897±0.010(23.0)0.973±0.001(27.0)0.980±0.004(22.0)25.75\nID 0.893±0.005(14.0)0.810±0.006(16.0)0.733±0.012(9.0)0.978±0.001(11.5)0.846±0.006(14.0)0.926±0.003(9.0)0.991±0.000(8.5)0.994±0.001(4.0)10.75\nQBC 0.887±0.004(16.0)0.813±0.005(15.0)0.724±0.014(15.0)0.973±0.002(22.0)0.858±0.005(11.0)0.928±0.003(7.0)0.988±0.002(14.0)0.994±0.001(4.0)13.00\nXLNet\nrandom 0.913±0.003(7.0)0.852±0.004(8.0)0.732±0.002(12.0)0.959±0.002(26.0)0.839±0.007(15.0)0.901±0.003(19.0)0.980±0.001(18.5)0.983±0.001(19.0)15.56\nuncertainty0.952±0.004(1.0)0.865±0.005(4.0)0.744±0.005(2.0)0.985±0.001(1.0)0.886±0.007(5.0)0.917±0.001(11.0)0.994±0.000(3.0)0.994±0.001(4.0)3.88\nEGAL 0.899±0.011(13.0)0.789±0.025(18.0)0.719±0.007(18.0)0.963±0.001(25.0)0.821±0.009(16.0)0.882±0.010(32.0)0.979±0.001(20.0)0.977±0.003(25.0)20.88\nID 0.950±0.003(2.0)0.859±0.008(6.0)0.733±0.007(10.0)0.983±0.003(3.0)0.889±0.006(4.0)0.911±0.005(14.0)0.991±0.002(7.0)0.994±0.000(6.5)6.56\nQBC 0.943±0.005(6.0)0.862±0.005(5.0)0.734±0.009(8.0)0.979±0.002(10.0)0.884±0.003(6.0)0.913±0.002(12.5)0.990±0.001(11.5)0.992±0.002(10.0)8.62\nGPT-2\nrandom0.875±0.006(17.0)0.826±0.004(13.0)0.727±0.004(14.0)0.955±0.003(27.0)0.815±0.005(19.0)0.909±0.002(15.0)0.983±0.001(16.0)0.967±0.002(31.0)19.00\nuncertainty0.911±0.004(8.0)0.843±0.003(9.0)0.739±0.006(3.0)0.984±0.001(2.0)0.875±0.003(7.0)0.930±0.001(6.0)0.995±0.000(1.0)0.988±0.001(12.0)6.00\nEGAL 0.865±0.003(22.0)0.749±0.021(19.0)0.723±0.006(16.0)0.954±0.001(29.0)0.812±0.006(20.0)0.904±0.005(16.0)0.982±0.001(17.0)0.970±0.003(29.0)21.00\nID 0.908±0.004(10.0)0.838±0.004(10.0)0.734±0.010(7.0)0.983±0.001(5.5)0.862±0.008(9.0)0.927±0.002(8.0)0.994±0.001(2.0)0.987±0.001(14.0)8.19\nQBC 0.906±0.003(11.0)0.834±0.005(12.0)0.730±0.008(13.0)0.977±0.002(15.0)0.867±0.005(8.0)0.925±0.006(10.0)0.992±0.001(5.5)0.984±0.002(17.0)11.44\nRoberta\nrandom 0.910±0.003(9.0)0.858±0.005(7.0)0.732±0.003(11.0)0.954±0.003(28.0)0.852±0.004(12.0)0.931±0.001(5.0)0.976±0.001(25.0)0.984±0.001(18.0)14.38\nuncertainty0.950±0.002(3.0)0.879±0.004(1.0)0.747±0.007(1.0)0.983±0.002(4.0)0.903±0.002(1.0)0.949±0.001(1.0)0.993±0.000(4.0)0.995±0.000(2.0)2.12\nEGAL 0.905±0.006(12.0)0.835±0.011(11.0)0.714±0.005(19.0)0.953±0.003(30.0)0.852±0.003(13.0)0.903±0.009(18.0)0.977±0.002(21.0)0.985±0.003(16.0)17.50\nID 0.949±0.003(4.0)0.873±0.005(2.0)0.736±0.006(5.0)0.983±0.001(5.5)0.894±0.003(2.0)0.940±0.002(3.0)0.992±0.001(5.5)0.994±0.000(6.5)4.19\nQBC 0.945±0.002(5.0)0.869±0.004(3.0)0.735±0.008(6.0)0.978±0.001(11.5)0.892±0.006(3.0)0.946±0.002(2.0)0.990±0.001(11.5)0.993±0.002(8.5)6.31\nGlove\nrandom0.812±0.002(38.0)0.712±0.009(32.0)0.705±0.004(25.0)0.950±0.002(31.5)0.741±0.006(38.0)0.887±0.002(30.0)0.936±0.002(35.0)0.946±0.007(34.0)32.94\nuncertainty0.840±0.004(33.0)0.710±0.021(33.0)0.708±0.011(21.0)0.977±0.004(14.0)0.770±0.021(27.0)0.900±0.010(20.0)0.970±0.004(31.0)0.971±0.007(28.0)25.88\nEGAL 0.808±0.006(39.0)0.615±0.018(40.0)0.691±0.009(34.0)0.948±0.003(34.0)0.719±0.006(45.0)0.793±0.038(40.0)0.939±0.002(34.0)0.944±0.008(35.0)37.62\nID 0.845±0.008(29.0)0.704±0.015(35.0)0.692±0.014(33.0)0.977±0.005(13.0)0.765±0.009(29.0)0.891±0.002(28.0)0.971±0.004(28.0)0.969±0.008(30.0)28.12\nQBC 0.845±0.003(30.0)0.715±0.008(30.0)0.701±0.018(28.5)0.977±0.001(16.0)0.787±0.009(23.0)0.900±0.003(21.0)0.971±0.001(30.0)0.978±0.002(24.0)25.31\nword2vec\nrandom0.842±0.005(31.0)0.731±0.004(23.0)0.702±0.009(27.0)0.947±0.004(35.0)0.749±0.007(35.0)0.885±0.004(31.0)0.975±0.001(26.0)0.953±0.009(32.0)30.00\nuncertainty0.865±0.008(21.0)0.734±0.014(21.0)0.713±0.012(20.0)0.975±0.003(19.5)0.767±0.027(28.0)0.894±0.005(25.0)0.987±0.005(15.0)0.974±0.011(26.0)21.94\nEGAL 0.829±0.006(35.0)0.654±0.024(37.0)0.699±0.011(30.0)0.943±0.003(39.0)0.725±0.007(43.0)0.829±0.022(34.0)0.976±0.003(23.0)0.936±0.010(37.0)34.75\nID 0.866±0.010(20.0)0.727±0.009(25.0)0.703±0.013(26.0)0.976±0.002(17.0)0.763±0.015(30.0)0.891±0.004(27.0)0.990±0.002(10.0)0.973±0.008(27.0)22.75\nQBC 0.872±0.003(18.0)0.733±0.007(22.0)0.706±0.012(23.0)0.975±0.003(19.5)0.792±0.006(21.0)0.893±0.002(26.0)0.989±0.001(13.0)0.979±0.002(23.0)20.69\nFastText\nrandom0.821±0.006(36.0)0.724±0.007(26.5)0.705±0.006(24.0)0.950±0.002(31.5)0.740±0.011(39.5)0.888±0.003(29.0)0.950±0.002(33.0)0.982±0.004(21.0)30.06\nuncertainty0.853±0.008(27.0)0.728±0.014(24.0)0.701±0.018(28.5)0.980±0.003(8.0)0.776±0.018(25.0)0.894±0.011(24.0)0.976±0.008(22.0)0.988±0.006(11.0)21.19\nEGAL 0.814±0.005(37.0)0.647±0.041(38.0)0.656±0.018(43.0)0.947±0.003(36.5)0.737±0.009(41.0)0.859±0.031(33.0)0.957±0.001(32.0)0.982±0.006(20.0)35.06\nID 0.852±0.008(28.0)0.720±0.008(28.0)0.697±0.015(31.0)0.981±0.002(7.0)0.776±0.012(26.0)0.898±0.006(22.0)0.980±0.001(18.5)0.987±0.006(13.0)21.69\nQBC 0.857±0.005(24.0)0.724±0.007(26.5)0.706±0.013(22.0)0.976±0.001(18.0)0.788±0.007(22.0)0.904±0.002(17.0)0.976±0.002(24.0)0.993±0.002(8.5)20.25\nTF-IDF\nrandom0.837±0.003(34.0)0.708±0.012(34.0)0.666±0.006(42.0)0.945±0.002(38.0)0.740±0.011(39.5)0.808±0.007(37.0)0.887±0.011(41.0)0.912±0.014(40.0)38.19\nuncertainty0.871±0.005(19.0)0.719±0.009(29.0)0.684±0.006(35.0)0.975±0.001(21.0)0.758±0.017(32.0)0.807±0.016(38.0)0.919±0.017(37.0)0.943±0.021(36.0)30.88\nEGAL 0.800±0.008(40.0)0.642±0.043(39.0)0.637±0.009(45.0)0.942±0.005(40.0)0.745±0.009(36.0)0.809±0.008(36.0)0.895±0.012(40.0)0.912±0.016(39.0)39.38\nID 0.862±0.004(23.0)0.696±0.004(36.0)0.683±0.003(36.0)0.971±0.002(23.0)0.744±0.013(37.0)0.803±0.016(39.0)0.915±0.010(38.0)0.926±0.018(38.0)33.75\nQBC 0.854±0.012(26.0)0.713±0.007(31.0)0.676±0.007(37.0)0.965±0.002(24.0)0.778±0.005(24.0)0.812±0.005(35.0)0.932±0.004(36.0)0.952±0.007(33.0)30.75\nLDA\nrandom0.772±0.006(43.0)0.611±0.006(42.0)0.669±0.005(41.0)0.884±0.012(44.0)0.733±0.006(42.0)0.680±0.003(42.0)0.854±0.004(44.0)0.858±0.006(44.0)42.75\nuncertainty0.791±0.006(42.0)0.613±0.010(41.0)0.673±0.015(39.0)0.922±0.012(41.0)0.754±0.011(33.0)0.671±0.013(43.0)0.877±0.017(42.0)0.881±0.011(42.0)40.38\nEGAL 0.760±0.007(45.0)0.600±0.008(45.0)0.648±0.009(44.0)0.858±0.014(45.0)0.720±0.008(44.0)0.611±0.015(45.0)0.835±0.006(45.0)0.805±0.018(45.0)44.75\nID 0.796±0.005(41.0)0.606±0.006(44.0)0.672±0.005(40.0)0.914±0.010(43.0)0.750±0.010(34.0)0.620±0.008(44.0)0.862±0.010(43.0)0.863±0.012(43.0)41.50\nQBC 0.770±0.009(44.0)0.607±0.009(43.0)0.675±0.008(38.0)0.917±0.008(42.0)0.763±0.008(31.0)0.689±0.006(41.0)0.901±0.005(39.0)0.905±0.006(41.0)39.88\n8\nTable 5: The summary of AULC scores, which are computed by trapezoidal rule and normalized by the maximum possible area, on each dataset regarding the diﬀerent combinations\nof text representations and selection strategies. The best performance of each dataset is highlighted and the last column denotes the average ranking of each method where the smaller\nnumber suggests a higher rank.\nRep Strategy MR MDCR BAG G2013 ACR MRS AGN DBPRank\nBERT\nrandom0.856±0.006(18.0)0.800±0.003(21.0)0.722±0.005(19.0)0.947±0.003(26.0)0.816±0.006(27.0)0.913±0.002(21.0)0.971±0.002(27.0)0.986±0.001(23.0)22.75\nuncertainty0.891±0.005(12.0)0.819±0.006(10.0)0.738±0.006(4.0)0.980±0.001(6.0)0.862±0.003(12.0)0.932±0.000(12.0)0.991±0.000(11.5)0.995±0.001(6.0)9.19\nEGAL0.841±0.013(26.0)0.738±0.010(29.0)0.693±0.005(30.0)0.949±0.003(24.0)0.818±0.005(26.0)0.897±0.010(27.0)0.973±0.001(26.0)0.980±0.004(26.0)26.75\nID 0.893±0.005(10.0)0.810±0.006(15.0)0.733±0.012(11.0)0.978±0.001(9.0)0.846±0.006(18.0)0.926±0.003(16.0)0.991±0.000(11.5)0.994±0.001(9.5)12.50\nQBC 0.887±0.004(15.0)0.813±0.005(13.0)0.724±0.014(18.0)0.973±0.002(13.5)0.858±0.005(13.0)0.928±0.003(15.0)0.988±0.002(14.0)0.994±0.001(9.5)13.88\nBertCLS\nrandom0.821±0.004(27.0)0.770±0.004(25.0)0.702±0.006(28.0)0.935±0.004(29.0)0.802±0.002(29.0)0.891±0.003(28.5)0.951±0.002(30.0)0.974±0.003(28.0)28.06\nuncertainty0.854±0.005(19.0)0.787±0.006(22.0)0.712±0.008(23.0)0.971±0.001(16.0)0.841±0.007(20.0)0.913±0.001(22.0)0.983±0.001(17.0)0.989±0.001(20.0)19.88\nEGAL0.802±0.005(29.0)0.736±0.040(30.0)0.678±0.011(33.0)0.934±0.004(30.0)0.774±0.002(30.0)0.891±0.003(28.5)0.954±0.003(29.0)0.965±0.005(30.0)29.94\nid 0.850±0.003(21.0)0.768±0.005(26.0)0.705±0.010(25.0)0.967±0.001(17.0)0.835±0.005(21.0)0.905±0.002(24.0)0.982±0.002(18.0)0.988±0.001(21.0)21.62\nQBC 0.846±0.006(23.0)0.774±0.006(24.0)0.710±0.007(24.0)0.966±0.002(18.0)0.844±0.005(19.0)0.911±0.006(23.0)0.977±0.002(23.5)0.987±0.002(22.0)22.06\nRoberta\nrandom 0.910±0.003(7.0)0.858±0.005(5.0)0.732±0.003(13.0)0.954±0.003(21.0)0.852±0.004(15.0)0.931±0.001(13.0)0.976±0.001(25.0)0.984±0.001(25.0)15.50\nuncertainty0.950±0.002(1.0)0.879±0.004(1.0)0.747±0.007(2.0)0.983±0.002(2.0)0.903±0.002(1.0)0.949±0.001(1.0)0.993±0.000(5.0)0.995±0.000(8.0)2.62\nEGAL 0.905±0.006(8.0)0.835±0.011(9.0)0.714±0.005(21.0)0.953±0.003(22.0)0.852±0.003(16.0)0.903±0.009(26.0)0.977±0.002(23.5)0.985±0.003(24.0)18.69\nID 0.949±0.003(2.0)0.873±0.005(2.0)0.736±0.006(7.0)0.983±0.001(3.0)0.894±0.003(4.0)0.940±0.002(7.0)0.992±0.001(8.5)0.994±0.000(11.0)5.56\nQBC 0.945±0.002(3.0)0.869±0.004(3.0)0.735±0.008(9.0)0.978±0.001(9.0)0.892±0.006(5.0)0.946±0.002(2.0)0.990±0.001(13.0)0.993±0.002(12.0)7.00\nRobertaCLS\nrandom0.892±0.006(11.0)0.839±0.006(8.0)0.733±0.003(12.0)0.956±0.003(19.5)0.850±0.005(17.0)0.921±0.001(18.0)0.981±0.001(21.0)0.980±0.003(27.0)16.69\nuncertainty0.933±0.002(4.0)0.860±0.005(4.0)0.749±0.007(1.0)0.984±0.001(1.0)0.895±0.006(3.0)0.942±0.002(5.0)0.992±0.001(8.5)0.992±0.002(13.5)5.00\nEGAL0.878±0.010(16.0)0.786±0.029(23.0)0.713±0.012(22.0)0.956±0.003(19.5)0.818±0.011(24.0)0.915±0.005(20.0)0.966±0.009(28.0)0.969±0.005(29.0)22.69\nID 0.931±0.005(5.0)0.857±0.007(6.0)0.735±0.010(8.0)0.981±0.003(4.0)0.896±0.006(2.0)0.936±0.003(10.0)0.992±0.001(8.5)0.992±0.002(13.5)7.12\nQBC 0.928±0.002(6.0)0.855±0.005(7.0)0.738±0.008(3.0)0.978±0.001(9.0)0.892±0.002(6.0)0.938±0.002(9.0)0.988±0.001(15.0)0.991±0.001(16.0)8.88\nDistilBert\nrandom0.861±0.005(17.0)0.803±0.006(20.0)0.726±0.005(16.0)0.948±0.002(25.0)0.821±0.005(23.0)0.922±0.002(17.0)0.981±0.001(21.0)0.990±0.002(18.5)19.69\nuncertainty0.894±0.005(9.0)0.815±0.007(12.0)0.737±0.008(5.0)0.980±0.001(6.0)0.867±0.007(9.0)0.941±0.002(6.0)0.995±0.000(1.5)0.996±0.001(2.0)6.31\nEGAL0.842±0.011(25.0)0.751±0.006(28.0)0.704±0.009(27.0)0.951±0.002(23.0)0.818±0.006(25.0)0.890±0.011(30.0)0.983±0.002(16.0)0.990±0.002(18.5)24.06\nID 0.890±0.003(14.0)0.807±0.006(17.0)0.736±0.009(6.0)0.977±0.003(11.0)0.856±0.003(14.0)0.930±0.001(14.0)0.994±0.000(3.5)0.995±0.001(6.0)10.69\nQBC 0.890±0.006(13.0)0.812±0.007(14.0)0.729±0.014(14.0)0.973±0.002(13.5)0.867±0.005(10.0)0.939±0.002(8.0)0.992±0.001(8.5)0.995±0.001(6.0)10.88\nDistilBertCLS\nrandom0.818±0.004(28.0)0.805±0.003(19.0)0.717±0.005(20.0)0.946±0.002(27.0)0.829±0.004(22.0)0.919±0.002(19.0)0.981±0.001(21.0)0.991±0.001(16.0)21.50\nuncertainty0.850±0.004(20.0)0.817±0.007(11.0)0.727±0.011(15.0)0.980±0.001(6.0)0.876±0.009(7.0)0.944±0.001(3.0)0.995±0.000(1.5)0.996±0.000(4.0)8.44\nEGAL0.791±0.010(30.0)0.758±0.029(27.0)0.705±0.005(26.0)0.945±0.003(28.0)0.813±0.005(28.0)0.904±0.003(25.0)0.982±0.001(19.0)0.991±0.001(16.0)24.88\nID 0.846±0.003(24.0)0.807±0.005(18.0)0.734±0.005(10.0)0.977±0.001(12.0)0.865±0.012(11.0)0.934±0.002(11.0)0.994±0.000(3.5)0.996±0.001(2.0)11.44\nQBC 0.847±0.006(22.0)0.809±0.007(16.0)0.725±0.009(17.0)0.973±0.001(15.0)0.874±0.008(8.0)0.943±0.002(4.0)0.992±0.002(6.0)0.996±0.001(2.0)11.25\nAlbert\nrandom0.741±0.005(34.0)0.642±0.008(33.0)0.681±0.005(31.0)0.882±0.006(37.0)0.711±0.006(36.0)0.779±0.004(33.0)0.903±0.003(37.0)0.828±0.005(34.0)34.38\nuncertainty0.761±0.009(31.0)0.647±0.009(32.0)0.695±0.008(29.0)0.927±0.008(31.0)0.728±0.018(34.0)0.788±0.007(32.0)0.948±0.004(31.0)0.875±0.005(31.0)31.38\nEGAL0.735±0.007(35.0)0.611±0.011(35.0)0.668±0.007(37.0)0.871±0.012(38.0)0.694±0.009(39.0)0.747±0.015(38.0)0.891±0.006(38.0)0.811±0.008(36.0)37.00\nID 0.754±0.009(33.0)0.623±0.006(34.0)0.675±0.017(34.0)0.922±0.008(33.0)0.720±0.014(35.0)0.774±0.010(34.0)0.941±0.004(33.0)0.856±0.008(33.0)33.62\nQBC 0.758±0.008(32.0)0.651±0.009(31.0)0.680±0.010(32.0)0.925±0.003(32.0)0.748±0.011(31.0)0.791±0.004(31.0)0.945±0.002(32.0)0.872±0.006(32.0)31.62\nAlbertCLS\nrandom0.698±0.005(38.0)0.597±0.007(38.0)0.659±0.007(39.0)0.850±0.006(39.0)0.704±0.008(37.0)0.741±0.002(39.0)0.873±0.005(39.0)0.771±0.005(39.0)38.50\nuncertainty0.704±0.005(36.0)0.607±0.005(36.0)0.672±0.012(35.0)0.889±0.007(34.0)0.731±0.016(33.0)0.755±0.005(35.0)0.926±0.004(34.0)0.812±0.007(35.0)34.75\nEGAL0.688±0.005(40.0)0.559±0.009(40.0)0.635±0.010(40.0)0.823±0.018(40.0)0.686±0.015(40.0)0.706±0.018(40.0)0.868±0.004(40.0)0.766±0.009(40.0)40.00\nID 0.692±0.006(39.0)0.595±0.006(39.0)0.670±0.007(36.0)0.885±0.009(36.0)0.701±0.021(38.0)0.750±0.008(37.0)0.917±0.007(36.0)0.794±0.004(38.0)37.38\nQBC 0.700±0.008(37.0)0.600±0.007(37.0)0.660±0.014(38.0)0.889±0.004(35.0)0.746±0.010(32.0)0.754±0.006(36.0)0.924±0.002(35.0)0.806±0.009(37.0)35.88\n9\n3.5. Results and Analysis\nTo illustrate the performance diﬀerences observed be-\ntween the diﬀerent representations explored, Figures 1 and\n2 show the learning curves for each diﬀerent representa-\ntion (separated by selection strategy) for the MDCR and\nMRS datasets respectively. In these plots the horizontal\naxis denotes the number of instances labelled so far, and\nthe vertical axis denotes the accuracy+ score achieved. It\nshould be noted that each curve starts with 10 rather than\n0 along the horizontal axis, corresponding to the initial\nseed labelling described earlier.\nGenerally speaking, we can observe that better perfor-\nmance is achieved when active learning is used in combi-\nnation with a text representations based on transformer-\nbased-embeddings rather than the simpler frequency-\nbased representations (i.e. TF-IDF and LDA), or rep-\nresentations based on word embeddings (i.e. word2vec,\nGlove, and FastText). More speciﬁcally, in Figure 1, we\nobserve that Roberta consistently outperforms any other\nrepresentation by a reasonably large margin across all se-\nlection strategies. Another interesting observation is that\nthe approaches based on word2vec, Glove and FastText\ngive similar performance, and the approach based on LDA\nperforms worst across all situations. In Figure 2, we see\na similar pattern that, in the majority of cases, the per-\nformance of the approaches based on transformer-based\nmodels surpass the performances achieved using other rep-\nresentations. Besides, Roberta always outperforms other\nrepresentations except when EGAL is used as the selec-\ntion strategy. Again, LDA performs poorly when used in\ncombination with all selection strategies.\nWe collate the AULC results of all methods in Ta-\nble 4. In this table, each column denotes the perfor-\nmance of diﬀerent active learning processes on a speciﬁc\ndataset. Diﬀerent representation and selection strategy\ncombinations are compared, and the best results achieved\nfor each dataset are highlighted. The numbers in brack-\nets stand for the ranking of each method when compared\nto the performance of the other approaches for a speciﬁc\ndataset. The last column reports the average ranking of\neach representation-selection-strategy combination, where\na smaller number means a higher rank. Table 3 presents a\nsummarised version where only the selection strategy that\ngives the best performance of each representation is shown.\nTable 4 presents a very clear message that the\ntransformer-based representations perform well across all\ndatasets, which is evidenced by the higher ranks they re-\nceive, as compared to TF-IDF, LDA and other word em-\nbeddings such as word2vec. Overall, Roberta is the best\nperforming representation with average ranks of 2.12 for\nRoberta + uncertainty, 4.19 for Roberta + information\ndensity, and 6.31 for Roberta + QBC being the highest\naverage ranks overall. This addresses RQ1 deﬁned in /Sec-\ntion 1 and clearly shows that transformer-based represen-\ntations should be used in active learning systems for text\nclassiﬁcation.\n4. Which is the best BERT?\nThis section describes an experiment designed to an-\nswer two questions. First, are the representations gener-\nated using the lightweight versions of BERT and BERT-\nlike models as eﬀective as the those generated using the\nregular models (RQ2 from Section 1). Second, when using\na representation from a transfomer-based model in active\nlearning for text classiﬁcation are representations based\non the “[CLS]” token more eﬀective than those generated\nby aggregating word representations? The setup of the\nexperimental framework, the performance measures used,\nand the datasets used are the same as those mentioned in\nSections 3.1, 3.3 and 3.4. The rest of the subsections de-\nscribes the set up of the language models, the results and\ncorresponding analysis.\n4.1. Conﬁgurations of Text Representation Techniques\nWe adopted the original BERT (“bert-base-uncased”)\nas a baseline in this experiment. This is compared to\nRoberta (“roberta-base”) and two commonly used lighter\nversion of BERT: ALbert (“albert-base-v2”) and Distil-\nbert (“distilbert-base-uncased”). The codes beside the al-\ngorithms are the speciﬁc models used in the experiments\nwhich can be found on Github. 11 For each model, texts\nare represented by both “[CLS]” token and averaged em-\nbeddings. In other words, there are 8 diﬀerent represen-\ntations being compared in these experiments. For the av-\neraged representations, if the text sequence is longer than\n512 tokens, the same aggregated method is applied as de-\nscribed in Section 3.2. Similarly, for “[CLS]” representa-\ntion, the document is encoded by the mean of the “[CLS]”\ntoken embeddings for each fraction.\n4.2. Results and Analysis\nTo demonstrate the eﬀectiveness of variants of BERT\nexplored and the impacts of averaged representation and\n“[CLS]” representation, Figures 3 and 4 show the learning\ncurves for each diﬀerent representation (separated by se-\nlection strategy) for the MDCR and MRS datasets respec-\ntively. One obvious observation is that the Roberta model\nis still the best pre-trained language model across all sit-\nuations. Another interesting fact is that within the same\nlanguage model, the averaged representation is always bet-\nter than the “[CLS]” token representation. This matches\nthe ﬁnding by Devlin et al. (2018) that the “[CLS]” repre-\nsentation could manifest its performance only if the model\nis ﬁne-tuned.\nMore speciﬁcally, the original BERT and the smaller\nDistilBert have a similar performance and surprisingly, Al-\nbert fails to demonstrate the competitive performance as\ncompared to other models. These observations can also be\nseen in the AULC score summarized in Table 5. The ranks\nof “[CLS]” representations are always lower than that of\n11https://github.com/huggingface/transformers\n10\ntheir averaged counterparts and the ranks of representa-\ntions based on BERT are close to those based on DistilBert\ncounterparts. Table 2 reports the number of parameters as\nwell as the inference time cost of various transformer-based\nmodels using GPU. DistilBert is always the fastest, while\nGPT-2 and XLNet are the two most time-consuming mod-\nels. This is not surprising due to the complicated design\nof GPT-2 and XLNet. BERT and Roberta have a similar\ninference time which is expected as they share the same\nmodel architecture. Though the number of parameters\nin Albert is much less than that in Roberta and original\nBERT, the inference speed of Albert is nearly the same\nas that of Roberta and BERT. We suppose that this is\ndue to the matrix multiplication process involved in Al-\nbert. Considering both the performance in accuracy and\ninference speed, we recommend using Roberta + averaged\nrepresentation + uncertainty as to the default active learn-\ning setting, however, if the user has a problem with GPU\nmemory or pursue a fast inference speed, DistilBert could\nbe an eﬀective alternative. These ﬁndings address RQ2\nand RQ3 deﬁned in Section 1.\n5. Adaptive Tuning Active Learning\nOne thing that distinguishes the representations based\non transformer-based models from those based on simpler\nword embeddings (e.g. word2vec) is that the transformer-\nbased models can use ﬁne-tuning to take advantage of label\ninformation that arises during active learning. Although\na relatively small number of labels are generated during\nactive learning it may be possible to use them to learn\nmore relevant embeddings that will improve the overall\nactive learning process (RQ4 from Section 1). To this end\nwe propose Adaptive Tuning Active Learning (ATAL), an\nalgorithm where the pre-trained language model is adap-\ntively improved via ﬁne-tuning with label information as\nit becomes available during active learning. To the best\nof our knowledge, this is the ﬁrst approach to exploiting\nthe label information for building better representations\nduring the active learning process. The following subsec-\ntions describe the methodology, the experimental conﬁgu-\nrations, results and analysis.\n5.1. Method\nAlgorithm 2 describes ATAL in detail. ATAL is built\non the standard pool-based active learning procedure and\nthe algorithm is similar to Algorithm 1. The main diﬀer-\nence is that on Lines 13 to 18 in Algorithm 2 we ﬁne-tune\nthe transformer-based model every 20 iterations (i.e. for\nevery 200 newly labelled instances, assuming a batch size\nof 10) with currently labelled instances. As the amount of\nlabelled data available for ﬁne tuning is small we do not\nhave the luxury of a hold out validation set to use to im-\nplement early stopping during model ﬁne tuning. Instead\nafter training for 15 epochs we roll back to the model with\nthe lowest loss based on the training dataset. This is not\nideal and runs the risk of overﬁtting, however, our experi-\nmental results show that it is eﬀective. Another key char-\nacteristic of ATAL is that the document representations\nare changed after each ﬁne-tuning, while in Algorithm 1,\nthe document vectors are ﬁxed across all loops.\nWe evaluate the performance of ATAL through an eval-\nuation experiment using the same datasets and using the\nsame performance measures as those described in Sections\n3.4 and 3.3. Again, the ATAL process is repeated 10 times\nusing diﬀerent random seeds and the performance mea-\nsures reported are averaged across these repetitions.\n5.2. Conﬁgurations\nGiven the fact that Roberta + averaged representation\n+ uncertainty has shown to be very eﬀective in the previ-\nous experiments, we chose this as the baseline in this ex-\nperiment and built ATAL using Roberta. During the ﬁne-\ntuning, we set the learning rate to 1e−5, use 15 epochs, set\nthe batch size to 4, and use the Adam optimizer (Kingma\nand Ba, 2014) with epsilon equals to 1 e −8. The values of\nother hyper-parameters follow the default settings in the\nRoberta model.\nAs well as reporting the results of the ATAL model,\nin this experiment we also report the performance of a\nRoberta model ﬁne-tuned with the fully labelled dataset,\nreferred to as “roberta tuned”. It would not be possible\nto use this approach in practice as it would not have ac-\ncess to the fully labelled datasets, but it does provide an\ninteresting upper bound on the possible performance of\nATAL.\n5.3. Results and Analysis\nFigure 5 illustrate the comparison results of our pro-\nposed ATAL algorithm with the traditional active learn-\ning process. The orange dotted line denotes the Roberta-\nmodel ﬁne-tuned with the fully labelled dataset, which\ngives us an upper bound on performance for each dataset.\nThe performance of fully ﬁne-tuned Roberta is always close\nto 1 which is corresponding to our expectation, since we\nused the whole fully labelled dataset to ﬁne-tune it. The\nblue and green dotted lines indicate the learning curve of\nthe normal Roberta model and ATAL Roberta model re-\nspectively. Notably, in ﬁve of eight dataset, the ATAL ap-\nproach consistently outperforms the more basic approach\nby a big margin, especially on the Multi-domain Customer\nReview and Additional Customer Review datasets. The\ndiﬀerences appeared when 200 instanced labelled (the step\nof 20) where our ATAL does its ﬁrst ﬁne tuning, which\nproves the eﬀectiveness of our algorithm. In Guardian\n2013, AG News and Dbpedia, the performances of the two\nmethods show no diﬀerence which is because these three\ndatasets are so easy for the pre-trained Roberta model\nthat the accuracy+ score reaches 1 before ATAL performs\nany ﬁne tuning. However, this also demonstrates that, at\nleast, the ATAL algorithm does not hurt the performance.\n11\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_random\nxlnet_random\ngpt2_random\nroberta_random\nglove_random\nw2v_random\nfasttext_random\ntfidf_random\nlda_random\n(a) Random\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_uncertainty\nxlnet_uncertainty\ngpt2_uncertainty\nroberta_uncertainty\nglove_uncertainty\nw2v_uncertainty\nfasttext_uncertainty\ntfidf_uncertainty\nlda_uncertainty (b) Uncertainty\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_ID\nxlnet_ID\ngpt2_ID\nroberta_ID\nglove_ID\nw2v_ID\nfasttext_ID\ntfidf_ID\nlda_ID (c) Information Density\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_QBC\nxlnet_QBC\ngpt2_QBC\nroberta_QBC\nglove_QBC\nw2v_QBC\nfasttext_QBC\ntfidf_QBC\nlda_QBC\n(d) QBC\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_EGAL\nxlnet_EGAL\ngpt2_EGAL\nroberta_EGAL\nglove_EGAL\nw2v_EGAL\nfasttext_EGAL\ntfidf_EGAL\nlda_EGAL (e) EGAL\nFigure 1: Results over Multi-Domain Customer Review (MDCR) dataset regarding diﬀerent representation techniques. X-axis represents the\nnumber of documents that have been manually annotated and Y-axis denotes accuracy+. Each curve starts with 10 along X-axis.\n6. Conclusions\nActive learning processes used with text data rely heav-\nily on the document representation mechanism used. This\npaper presented an evaluation experiment which explored\nthe eﬀectiveness of diﬀerent text representations in an ac-\ntive learning context. The performance of diﬀerent text\nrepresentation techniques combined with popular selec-\ntion strategies was compared over datasets from diﬀerent\ndomains. The comparison showed that the transformer-\nbased-embeddings, which are rarely used in active learn-\ning, lead to better performance compared to vector based\nrepresentations such as BOW or simpler word embeddings.\nSeveral of the most commonly used selection strategies\nhave been applied in experiments to mitigate the impact\nof speciﬁc selection strategies on the eﬀectiveness of diﬀer-\nent text representations. Notably, Roberta combined with\nuncertainty sampling greatly facilitates the application of\nactive learning for text labelling.\nSince several BERT-like models has shown its great\nperformance in active learning a study is carried out to\ninvestigate the eﬀectiveness of various BERT-like mod-\nels and identify the appropriate representation method for\npre-trained language models. Our experiments show that\nrepresentations based on Roberta seem to be best, while\nDistilBert oﬀers competitive performance with a much\nlower computational burden. In addition, the averaged\nword representations is the most eﬀective compared with\n“[CLS]” token representations.\nLastly, we proposed the Adaptive Tuning Active Learn-\ning (ATAL) algorithm where labelled information is fully\nutilized not only for training classiﬁers but also for further\nimproving the eﬀectiveness of embeddings produced. We\nsuggest that Robert + averaged representations + uncer-\ntainty as the default setting of active learning. If the user\nhas problem with GPU memory, DistilBert or Albert could\nbe another option. Besides, ATAL based on Roberta + un-\ncertainty can be considered due to its great performance\nshown in the experiment.\nWhile some of the ﬁndings in this study may not be\nthat surprising (the fact that representations based on\ntransformer-based models are very eﬀective), this paper\nmakes an important contribution as a comprehensive eval-\nuation experiment showing the eﬀectiveness of diﬀerent\nrepresentations.\nAn important application of active learning is to la-\nbelling the included/excluded studies in literature review\n(Wallace et al., 2010; Hashimoto et al., 2016; Miwa et al.,\n2014) which is usually an imbalanced dataset. So it leads\nto more exploration of the active learning framework over\nan imbalanced dataset in future work.\n12\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_random\nxlnet_random\ngpt2_random\nroberta_random\nglove_random\nw2v_random\nfasttext_random\ntfidf_random\nlda_random\n(a) Random\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_uncertainty\nxlnet_uncertainty\ngpt2_uncertainty\nroberta_uncertainty\nglove_uncertainty\nw2v_uncertainty\nfasttext_uncertainty\ntfidf_uncertainty\nlda_uncertainty (b) Uncertainty\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_ID\nxlnet_ID\ngpt2_ID\nroberta_ID\nglove_ID\nw2v_ID\nfasttext_ID\ntfidf_ID\nlda_ID (c) Information Density\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_QBC\nxlnet_QBC\ngpt2_QBC\nroberta_QBC\nglove_QBC\nw2v_QBC\nfasttext_QBC\ntfidf_QBC\nlda_QBC\n(d) QBC\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nbert_EGAL\nxlnet_EGAL\ngpt2_EGAL\nroberta_EGAL\nglove_EGAL\nw2v_EGAL\nfasttext_EGAL\ntfidf_EGAL\nlda_EGAL (e) EGAL\nFigure 2: Results over Movie Review Subjectivity (MRS) dataset regarding various representation techniques. X-axis represents the number\nof documents that have been manually annotated and Y-axis denotes accuracy+. Each curve starts with 10 along X-axis.\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_random\nbert_cls_random\nroberta_random\nroberta_cls_random\ndistilbert_random\ndistilbert_cls_random\nalbert_random\nalbert_cls_random\n(a) Random\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_uncertainty\nbert_cls_uncertainty\nroberta_uncertainty\nroberta_cls_uncertainty\ndistilbert_uncertainty\ndistilbert_cls_uncertainty\nalbert_uncertainty\nalbert_cls_uncertainty (b) Uncertainty\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_ID\nbert_cls_ID\nroberta_ID\nroberta_cls_ID\ndistilbert_ID\ndistilbert_cls_ID\nalbert_ID\nalbert_cls_ID (c) Information Density\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_QBC\nbert_cls_QBC\nroberta_QBC\nroberta_cls_QBC\ndistilbert_QBC\ndistilbert_cls_QBC\nalbert_QBC\nalbert_cls_QBC\n(d) QBC\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_EGAL\nbert_cls_EGAL\nroberta_EGAL\nroberta_cls_EGAL\ndistilbert_EGAL\ndistilbert_cls_EGAL\nalbert_EGAL\nalbert_cls_EGAL (e) EGAL\nFigure 3: Results over Multi-Domain Customer Review (MDCR) dataset regarding diﬀerent variants of BERT. X-axis represents the number\nof documents that have been manually annotated and Y-axis denotes accuracy+. Each curve starts with 10 along X-axis.\n13\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_random\nbert_cls_random\nroberta_random\nroberta_cls_random\ndistilbert_random\ndistilbert_cls_random\nalbert_random\nalbert_cls_random\n(a) Random\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_uncertainty\nbert_cls_uncertainty\nroberta_uncertainty\nroberta_cls_uncertainty\ndistilbert_uncertainty\ndistilbert_cls_uncertainty\nalbert_uncertainty\nalbert_cls_uncertainty (b) Uncertainty\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_ID\nbert_cls_ID\nroberta_ID\nroberta_cls_ID\ndistilbert_ID\ndistilbert_cls_ID\nalbert_ID\nalbert_cls_ID (c) Information Density\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_QBC\nbert_cls_QBC\nroberta_QBC\nroberta_cls_QBC\ndistilbert_QBC\ndistilbert_cls_QBC\nalbert_QBC\nalbert_cls_QBC\n(d) QBC\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+ bert_EGAL\nbert_cls_EGAL\nroberta_EGAL\nroberta_cls_EGAL\ndistilbert_EGAL\ndistilbert_cls_EGAL\nalbert_EGAL\nalbert_cls_EGAL (e) EGAL\nFigure 4: Results over Movie Review Subjectivity (MRS) dataset regarding diﬀerent variants of BERT. X-axis represents the number of\ndocuments that have been manually annotated and Y-axis denotes accuracy+. Each curve starts with 10 along X-axis.\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty\n(a) Movie Review\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty (b) Multi-domain Customer Re-\nview\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty\n(c) Blog Author Gender\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty (d) Guardian 2013\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty\n(e) Additional Customer Review\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty (f) Movie Review Subjectivity\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty (g) Ag News\n10 200 400 600 800 1000\nnumber of labelled samples\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0ACC+\nroberta_uncertainty\nroberta_tuned_uncertainty\nroberta_adaptive_uncertainty (h) Dbpedia\nFigure 5: Comparisons of performance of active learning process with plain pre-trained Roberta, Adaptive Tuning Roberta and fully ﬁne-tuned\nRoberta. The X-axis represents the number of documents that have been manually annotated and the Y-axis denotes accuracy+. Each curve\nstarts with 10 along the X-axis because of the seeded instances.\n14\nAlgorithm 2: Pseudo Code for ATAL.\nInput: T, set of all corpus\nP, index of all ground truth positive documents\nN, index of all ground truth negative documents\nLM, pre-trained language model\nOutput: S, set of accuracy+ scores of each loop\nL, set of documents pseudo labelled by the oracle\nR, set of documents labelled by the classiﬁer\n1 Initialization\n2 // Infer document vectors\n3 E ←Inference (T, LM);\n4 // Random sampling 5 neg and 5 pos\n5 Lt ←Random(T, P,5) ∪Random(T, N,5);\n6 L ←Inference (Lt, LM);\n7 ¬L ←E \\L;\n8 ¬Lt ←T \\Lt;\n9 R ←∅;\n10 loop ←0;\n11 Params ←∅;\n12 S ←∅;\n13 while |L|≤ 1000 do\n14 if loop mod 20 == 0 and loop >0 then\n15 // Fine-tuning model\n16 LM ←Fine tune(Lt, LM, P, N);\n17 // Re-infer all document vectors\n18 L ←Inference (Lt, LM);\n19 ¬L ←Inference (¬Lt, LM);\n20 CL, Params←Train (loop, L, P, N, Params);\n21 X ←Query(CL, ¬L);\n22 L, ¬L, R←Assign(CL, L, X);\n23 // Compute accuracy+ score\n24 S ←S ∪Eval(L, R, P, N);\n25 loop ←loop + 1;\n26 Function Fine tune(Lt, LM, P, N)\n27 LM candidate ←LM;\n28 // Validate the model with labelled examples\n29 acc candidate ←V alidate(Lt, LM, P, N);\n30 epoch ←0;\n31 while epoch <15 do\n32 // Update model weights\n33 LM ←Forward backprop(Lt, LM, P, N);\n34 acc ←V alidate(Lt, LM, P, N);\n35 if acc candidate < accthen\n36 acc candidate ←acc;\n37 LM candidate ←LM;\n38 epoch ←epoch + 1;\n39 return LM candidate\nAcknowledgements\nThis work was supported by the Teagasc Walsh Schol-\narship Programme Reference Number [201603]; and the\nScience Foundation Ireland (SFI) under Grant Number\n[SFI/12/RC/2289].\nReferences\nBelford, M., Mac Namee, B., and Greene, D. (2018). Stability of\ntopic modeling via matrix factorization. Expert Systems with\nApplications, 91:159–169.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet\nallocation. Journal of machine Learning research , 3(Jan):993–\n1022.\nBlitzer, J., Dredze, M., and Pereira, F. (2007). Biographies, bol-\nlywood, boom-boxes and blenders: Domain adaptation for senti-\nment classiﬁcation. In Proceedings of the 45th annual meeting of\nthe association of computational linguistics , pages 440–447.\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). En-\nriching word vectors with subword information. Transactions of\nthe Association for Computational Linguistics , 5:135–146.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert:\nPre-training of deep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805 .\nDing, X., Liu, B., and Yu, P. S. (2008). A holistic lexicon-based\napproach to opinion mining. In Proceedings of the 2008 interna-\ntional conference on web search and data mining , pages 231–240.\nACM.\nHashimoto, K., Kontonatsios, G., Miwa, M., and Ananiadou, S.\n(2016). Topic detection using paragraph vectors to support active\nlearning in systematic reviews. Journal of biomedical informatics,\n62:59–65.\nHoi, S. C., Jin, R., and Lyu, M. R. (2006). Large-scale text cate-\ngorization by batch mode active learning. In Proceedings of the\n15th international conference on World Wide Web , pages 633–\n642. ACM.\nHoward, J. and Ruder, S. (2018). Universal language model ﬁne-\ntuning for text classiﬁcation. arXiv preprint arXiv:1801.06146 .\nHsu, C.-W., Chang, C.-C., Lin, C.-J., et al. (2003). A practical guide\nto support vector classiﬁcation.\nHu, R., Delany, S. J., and Mac Namee, B. (2010). Egal: Exploration\nguided active learning for tcbr. In International Conference on\nCase-Based Reasoning, pages 156–170. Springer.\nHu, R., Mac Namee, B., and Delany, S. J. (2008). Sweetening the\ndataset: Using active learning to label unlabelled datasets. Proc.\nof AICS, 8:53–62.\nJoulin, A., Grave, E., Bojanowski, P., and Mikolov, T. (2016).\nBag of tricks for eﬃcient text classiﬁcation. arXiv preprint\narXiv:1607.01759.\nKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980 .\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Sori-\ncut, R. (2019). Albert: A lite bert for self-supervised learning of\nlanguage representations. arXiv preprint arXiv:1909.11942 .\nLe, Q. and Mikolov, T. (2014). Distributed representations of sen-\ntences and documents. In International conference on machine\nlearning, pages 1188–1196.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. na-\nture, 521(7553):436.\nLewis, D. D. and Gale, W. A. (1994). A sequential algorithm for\ntraining text classiﬁers. In SIGIR94, pages 3–12. Springer.\nLiere, R. and Tadepalli, P. (1997). Active learning with committees\nfor text categorization. In AAAI/IAAI, pages 591–596.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O.,\nLewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta:\nA robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\nMamitsuka, N. A. H. et al. (1998). Query learning strategies using\nboosting and bagging. In Machine learning: proceedings of the\nﬁfteenth international conference (ICML98) , volume 1. Morgan\nKaufmann Pub.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. (2017). Learned\nin translation: Contextualized word vectors. In Advances in\nNeural Information Processing Systems, pages 6294–6305.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Eﬃcient\nestimation of word representations in vector space. arXiv preprint\narXiv:1301.3781.\n15\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J.\n(2013b). Distributed representations of words and phrases and\ntheir compositionality. In Advances in neural information pro-\ncessing systems, pages 3111–3119.\nMiwa, M., Thomas, J., OMara-Eves, A., and Ananiadou, S. (2014).\nReducing systematic review workload through certainty-based\nscreening. Journal of biomedical informatics , 51:242–253.\nMo, Y., Kontonatsios, G., and Ananiadou, S. (2015). Supporting sys-\ntematic reviews using lda-based document representations. Sys-\ntematic reviews, 4(1):172.\nMukherjee, A. and Liu, B. (2010). Improving gender classiﬁcation\nof blog authors. In Proceedings of the 2010 conference on Em-\npirical Methods in natural Language Processing , pages 207–217.\nAssociation for Computational Linguistics.\nPang, B. and Lee, L. (2004). A sentimental education: Senti-\nment analysis using subjectivity summarization based on mini-\nmum cuts. In Proceedings of the 42nd annual meeting on Asso-\nciation for Computational Linguistics , page 271. Association for\nComputational Linguistics.\nPennington, J., Socher, R., and Manning, C. (2014). Glove: Global\nvectors for word representation. In Proceedings of the 2014\nconference on empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. (2018). Deep contextualized word\nrepresentations. arXiv preprint arXiv:1802.05365 .\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I.\n(2018). Improving language understanding by generative pre-\ntraining. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language under-\nstanding paper. pdf.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever,\nI. (2019). Language models are unsupervised multitask learners.\nOpenAI Blog, 1(8):9.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert,\na distilled version of bert: smaller, faster, cheaper and lighter.\narXiv preprint arXiv:1910.01108 .\nSettles, B. (2009). Active learning literature survey. Tech. rep., Uni-\nversity of Wisconsin-Madison Department of Computer Sciences.\nSettles, B. and Craven, M. (2008). An analysis of active learn-\ning strategies for sequence labeling tasks. In Proceedings of the\nconference on empirical methods in natural language processing ,\npages 1070–1079. Association for Computational Linguistics.\nSeung, H. S., Opper, M., and Sompolinsky, H. (1992). Query by\ncommittee. In Proceedings of the ﬁfth annual workshop on Com-\nputational learning theory, pages 287–294. ACM.\nSiddhant, A. and Lipton, Z. C. (2018). Deep bayesian active learning\nfor natural language processing: Results of a large-scale empirical\nstudy. arXiv preprint arXiv:1808.05697 .\nSingh, G., Thomas, J., and Shawe-Taylor, J. (2018). Improv-\ning active learning in systematic reviews. arXiv preprint\narXiv:1801.09496.\nSparck Jones, K. (1972). A statistical interpretation of term speci-\nﬁcity and its application in retrieval. Journal of documentation ,\n28(1):11–21.\nTong, S. and Chang, E. (2001). Support vector machine active learn-\ning for image retrieval. In Proceedings of the ninth ACM inter-\nnational conference on Multimedia, pages 107–118. ACM.\nTong, S. and Koller, D. (2001). Support vector machine active learn-\ning with applications to text classiﬁcation. Journal of machine\nlearning research, 2(Nov):45–66.\nTur, G., Hakkani-T¨ ur, D., and Schapire, R. E. (2005). Combining\nactive and semi-supervised learning for spoken language under-\nstanding. Speech Communication, 45(2):171–186.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser,  L., and Polosukhin, I. (2017). Attention\nis all you need. In Advances in neural information processing\nsystems, pages 5998–6008.\nWallace, B. C., Small, K., Brodley, C. E., and Trikalinos, T. A.\n(2010). Active learning for biomedical citation screening. In Pro-\nceedings of the 16th ACM SIGKDD international conference on\nKnowledge discovery and data mining , pages 173–182. ACM.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and\nLe, Q. V. (2019). Xlnet: Generalized autoregressive pretraining\nfor language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764.\nZhang, C. and Chen, T. (2002). An active learning framework for\ncontent-based information retrieval. IEEE transactions on mul-\ntimedia, 4(2):260–268.\nZhang, X., Zhao, J., and LeCun, Y. (2015). Character-level convo-\nlutional networks for text classiﬁcation. In Advances in neural\ninformation processing systems, pages 649–657.\nZhang, Y. (2019). Neural nlp models under low-supervision scenarios.\nZhang, Y., Lease, M., and Wallace, B. C. (2017). Active discrimina-\ntive text representation learning. In Thirty-First AAAI Confer-\nence on Artiﬁcial Intelligence .\nZhao, W. (2017). Deep active learning for short-text classiﬁcation.\n16",
  "topic": "Labelling",
  "concepts": [
    {
      "name": "Labelling",
      "score": 0.8222634196281433
    },
    {
      "name": "Transformer",
      "score": 0.7417916655540466
    },
    {
      "name": "Computer science",
      "score": 0.705720067024231
    },
    {
      "name": "Natural language processing",
      "score": 0.5602371692657471
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5357785224914551
    },
    {
      "name": "Psychology",
      "score": 0.11196821928024292
    },
    {
      "name": "Engineering",
      "score": 0.099555104970932
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Criminology",
      "score": 0.0
    }
  ]
}