{
  "title": "The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives",
  "url": "https://openalex.org/W2970820321",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2798548111",
      "name": "Elena Voita",
      "affiliations": [
        "Yandex (Russia)",
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A93200637",
      "name": "Rico Sennrich",
      "affiliations": [
        "University of Edinburgh",
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2127391507",
      "name": "Ivan Titov",
      "affiliations": [
        "University of Amsterdam",
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2025341678",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W3015703505",
    "https://openalex.org/W2963503967",
    "https://openalex.org/W2773621464",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W4297749952",
    "https://openalex.org/W2610120229",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W2951984970",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962777840",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W1686946872",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963261262",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2767204723",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2114771311",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2963759780",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W2955541912",
    "https://openalex.org/W2096516049",
    "https://openalex.org/W2892205701",
    "https://openalex.org/W4237723258"
  ],
  "abstract": "We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 4396–4406,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n4396\nThe Bottom-up Evolution of Representations in the Transformer:\nA Study with Machine Translation and Language Modeling Objectives\nElena Voita1,2 Rico Sennrich4,3 Ivan Titov3,2\n1Yandex, Russia 2University of Amsterdam, Netherlands\n3University of Edinburgh, Scotland 4University of Zurich, Switzerland\nlena-voita@yandex-team.ru\nsennrich@cl.uzh.ch ititov@inf.ed.ac.uk\nAbstract\nWe seek to understand how the representations\nof individual tokens and the structure of the\nlearned feature space evolve between layers in\ndeep neural networks under different learning\nobjectives. We focus on the Transformers for\nour analysis as they have been shown effec-\ntive on various tasks, including machine trans-\nlation (MT), standard left-to-right language\nmodels (LM) and masked language model-\ning (MLM). Previous work used black-box\nprobing tasks to show that the representations\nlearned by the Transformer differ signiﬁcantly\ndepending on the objective. In this work, we\nuse canonical correlation analysis and mutual\ninformation estimators to study how informa-\ntion ﬂows across Transformer layers and how\nthis process depends on the choice of learning\nobjective. For example, as you go from bot-\ntom to top layers, information about the past\nin left-to-right language models gets vanished\nand predictions about the future get formed.\nIn contrast, for MLM, representations initially\nacquire information about the context around\nthe token, partially forgetting the token iden-\ntity and producing a more generalized token\nrepresentation. The token identity then gets\nrecreated at the top MLM layers.\n1 Introduction\nDeep (i.e. multi-layered) neural networks have be-\ncome the standard approach for many natural lan-\nguage processing (NLP) tasks, and their analysis\nhas been an active topic of research. One popular\napproach for analyzing representations of neural\nmodels is to evaluate how informative they are for\nvarious linguistic tasks, so-called “probing tasks”.\nPrevious work has made some interesting observa-\ntions regarding these representations; for example,\nZhang and Bowman (2018) show that untrained\nLSTMs outperform trained ones on a word iden-\ntity prediction task; and Blevins et al. (2018) show\nthat up to a certain layer performance of represen-\ntations obtained from a deep LM improves on a\nconstituent labeling task, but then decreases, while\nwith representations obtained from an MT encoder\nperformance continues to improve up to the high-\nest layer. These observations have, however, been\nsomewhat anecdotal and an explanation of the pro-\ncess behind such behavior has been lacking.\nIn this paper, we attempt to explain more gen-\nerally why such behavior is observed. Rather than\nmeasuring the quality of representations obtained\nfrom a particular model on some auxiliary task, we\ncharacterize how the learning objective determines\nthe information ﬂow in the model. In particular,\nwe consider how the representations of individual\ntokens in the Transformer evolve between layers\nunder different learning objectives.\nWe look at this task from the information bottle-\nneck perspective on learning in neural networks.\nTishby and Zaslavsky (2015) state that “the goal\nof any supervised learning is to capture and ef-\nﬁciently represent the relevant information in the\ninput variable about the output-label variable” and\nhence the representations undergo transformations\nwhich aim to encode as much information about\nthe output label as possible, while ‘forgetting’ ir-\nrelevant details about the input. As we study se-\nquence encoders and look into representations of\nindividual tokens rather than the entire input, our\nsituation is more complex. In our model, the infor-\nmation preserved in a representation of a token is\ninduced due to two roles it plays: (i) predicting the\noutput label from a current token representation; 1\n(ii) preserving information necessary to build rep-\nresentations of other tokens. For example, a lan-\nguage model constructs a representation which is\nnot only useful for predicting an output label (in\nthis case, the next token), but also informative for\n1We will clarify how we deﬁne output labels for LM,\nMLM and MT objectives in Section 2.\n4397\nproducing representations of subsequent tokens in\na sentence. This is different from the MT set-\nting, where there is no single encoder state from\nwhich an output label is predicted. We hypothe-\nsize that the training procedure (or, in our notation,\nthe task) deﬁnes\n1. the nature of changes a token representation\nundergoes, from layer to layer;\n2. the process of interactions and relationships\nbetween tokens;\n3. the type of information which gets lost and\nacquired by a token representation in these\ninteractions.\nIn this work, we study how the choice of ob-\njective affects the process by which information\nis encoded in token representations of the Trans-\nformer (Vaswani et al., 2017), as this architec-\nture achieves state-of-the-art results on tasks with\nvery different objectives such as machine trans-\nlation (MT) (Bojar et al., 2018; Niehues et al.,\n2018), standard left-to-right language modeling\n(LM) (Radford et al., 2018) and masked language\nmodeling (MLM) (Devlin et al., 2018). The Trans-\nformer encodes a sentence by iteratively updating\nrepresentations associated with each token start-\ning from a context-agnostic representation consist-\ning of a positional and a token embedding. At\neach layer token representations exchange infor-\nmation among themselves via multi-head attention\nand then this information is propagated to the next\nlayer via a feed-forward transformation. We inves-\ntigate how this process depends on the choice of\nobjective function (LM, MLM or MT) while keep-\ning the data and model architecture ﬁxed.\nWe start with illustrating the process of infor-\nmation loss and gain in representations of individ-\nual tokens by estimating the mutual information\nbetween token representations at each layer and\nthe input token identity (i.e. the word type) or the\noutput label (e.g., the next word for LM).\nThen, we investigate behavior of token repre-\nsentations from two perspectives: how they inﬂu-\nence and are inﬂuenced by other tokens. Using\ncanonical correlation analysis, we evaluate the ex-\ntent of change the representation undergoes and\nthe degree of inﬂuence. We reveal differences in\nthe patterns of this behavior for different tasks.\nFinally, we study which type of information\ngets lost and gained in the interactions between\ntokens and to what extent a certain property is\nimportant for deﬁning a token representation at\neach layer and for each task. As the properties,\nwe consider token identities (‘word type’), posi-\ntions, identities of surrounding tokens and CCG\nsupertags. In these analyses we rely on similarity\ncomputations.\nWe ﬁnd, that (1) with the LM objective, as\nyou go from bottom to top layers, information\nabout the past gets lost and predictions about the\nfuture get formed; (2) for MLMs, representa-\ntions initially acquire information about the con-\ntext around the token, partially forgetting the token\nidentity and producing a more generalized token\nrepresentation; the token identity then gets recre-\nated at the top layer; (3) for MT, though repre-\nsentations get reﬁned with context, less process-\ning is happening and most information about the\nword type does not get lost. This provides us with\na hypothesis for why the MLM objective may be\npreferable in the pretraining context to LM. LMs\nmay not be the best choice, because neither infor-\nmation about current token and its past nor future\nis represented well: the former since this informa-\ntion gets discarded, the latter since the model does\nnot have access to the future.\nOur key contributions are as follows:\n•we propose to view the evolution of a token\nrepresentation between layers from the com-\npression/prediction trade-off perspective;\n•we conduct a series of experiments support-\ning this view and showing that the two pro-\ncesses, losing information about input and\naccumulating information about output, take\nplace in the evolution of representations (for\nMLM, these processes are clearly distin-\nguished and can be viewed as two stages,\n‘context encoding’ and ‘token prediction’);\n•we relate to some ﬁndings from previous\nwork, putting them in the proposed perspec-\ntive, and provide insights into the internal\nworkings of Transformer trained with differ-\nent objectives;\n•we propose an explanation for superior per-\nformance of the MLM objective over the LM\none for pretraining.\nAll analysis is done in a model-agnostic man-\nner by investigating properties of token represen-\n4398\ntations at each layer, and can, in principle, be ap-\nplied to other multi-layer deep models (e.g., multi-\nlayer RNN encoders).\n2 Tasks\nIn this section, we describe the tasks we consider.\nFor each task, we deﬁne input X and output Y.\n2.1 Machine translation\nGiven source sentence X= (x1,x2,...,x S) and a\ntarget sentence Y = (y1,y2,...,y T ), NMT mod-\nels predict words in the target sentence, word by\nword, i.e. provide estimates of the conditional dis-\ntribution p(yi|X,y1,i−1,θ).\nWe train a standard Transformer for the transla-\ntion task and then analyze its encoder. In contrast\nto the other two tasks we describe below, represen-\ntations from the top layers are not directly used to\npredict output labels but to encode the information\nwhich is then used by the decoder.\n2.2 Language modeling\nLMs estimate the probability of a word\ngiven the previous words in a sen-\ntence P(xt|x1,...,x t−1,θ). More formally, the\nmodel is trained with inputs X = (x1,...,x t−1)\nand outputs Y = (xt), where xt is the output label\npredicted from the ﬁnal (i.e. top-layer) representa-\ntion of a token xt−1. It is straightforward to apply\nthe Transformer to this task (Radford et al., 2018;\nLample and Conneau, 2019).\n2.3 Masked language modeling\nWe also consider the MLM objective (Devlin\net al., 2018), randomly sampling 15% of the to-\nkens to be predicted. We replace the correspond-\ning input token by [MASK] or a random token in\n80% and 10% of the time, respectively, keeping it\nunchanged otherwise.\nFor a sentence (x1,x2,...,x S), where token\nxi is replaced with ˜xi, the model receives\nX= (x1,...,x i−1, ˜xi,xi+1,...,x S) as input and\nneeds to predictY =(xi). The label xi is predicted\nfrom the ﬁnal representation of the token ˜xi.\n3 Data and Setting\nAs described below, for a fair comparison, we use\nthe same training data, model architecture and pa-\nrameter initialization across all the tasks. In order\nto make sure that our ﬁndings are reliable, we also\nuse multiple datasets and repeat experiments with\ndifferent random initializations for each task.\nWe train all models on the data from the\nWMT news translation shared task. We con-\nduct separate series of experiments using two lan-\nguage pairs: WMT 2017 English-German (5.8m\nsentence pairs) and WMT 2014 English-French\n(40.8m sentence pairs). For language model-\ning, we use only the source side of the parallel\ndata. We remove randomly chosen 2.8m sentence\npairs from the English-French dataset and use the\nsource side for analysis. English-French models\nare trained on the remaining 38m sentence pairs.\nWe consider different dataset sizes (2.5m and 5.8m\nfor English-German, 2.5m, 5.8m and 38m for\nEnglish-French). We ﬁnd that our ﬁndings are true\nfor all languages, dataset sizes and initializations.\nIn the following, all the illustrations are provided\nfor the models trained on the full English-German\ndataset (5.8m sentence pairs).\nWe follow the setup and training procedure\nof the Transformer base model (Vaswani et al.,\n2017). For details, see the appendix.\n4 The Information-Bottleneck Viewpoint\nIn this section, we give an intuitive explanation of\nthe Information Bottleneck (IB) principle (Tishby\net al., 1999) and consider a direct application of\nthis principle to our analysis.\n4.1 Background\nThe IB method (Tishby et al., 1999) considers\na joint distribution of input-output pairs p(X,Y )\nand aims to extract a compressed representation\n˜X for an input X such that ˜X retains as much\nas possible information about the output Y. More\nformally, the IB method maximizes the mutual in-\nformation (MI) with the output I( ˜X; Y), while\npenalizing for MI with the input I( ˜X; X). The\nlatter term in the objective ensures that the rep-\nresentation is indeed a compression. Intuitively,\nthe choice of the output variable Y determines the\nsplit of Xinto irrelevant and relevant features. The\nrelevant features need to be retained while irrele-\nvant ones should be dropped.\nTishby and Zaslavsky (2015) argue that compu-\ntation in a multi-layered neural model can be re-\ngarded as an evolution towards the theoretical op-\ntimum of the IB objective. A sequence of layers\nis viewed as a Markov chain, and the process of\nobtaining Y corresponds to compressing the rep-\nresentation as it ﬂows across layers and retaining\nonly information relevant to predicting Y. This\nimplies that Y deﬁnes the information ﬂow in the\n4399\nmodel. Since Y is different for each model, we ex-\npect to see different patterns of information ﬂow in\nmodels, and this is the focus of our study.\n4.2 IB for token representations\nIn this work, we view every sequence model (MT,\nLM and MLM) as learning a function from input\nX to output Y. The input is a sequence of tokens\nX = (x1,x2,...,x n) and the output Y is deﬁned\nin Section 2. Recall that we focus on representa-\ntions of individual tokens in every layer rather than\nthe representation of the entire sequence.\nWe start off our analysis of divergences in the\ninformation ﬂow for different objectives by esti-\nmating the amount of information about input or\noutput tokens retained in the token representation\nat each layer.\n4.2.1 Estimating mutual information\nInspired by Tishby and Zaslavsky (2015), we es-\ntimate MI between token representations at a cer-\ntain layer and an input token. To estimate MI, we\nneed to consider token representations at a layer\nas samples from a discrete distribution. To get\nsuch distribution, in the original works (Shwartz-\nZiv and Tishby, 2017), the authors binned the neu-\nron’sarctan activations. Using these discretized\nvalues for each neuron in a layer, they were able\nto treat a layer representation as a discrete vari-\nable. They considered neural networks with maxi-\nmum 12 neurons at a layer, but in practical scenar-\nios (e.g. we have 512 neurons in each layer) this\napproach is not feasible. Instead, similarly to Saj-\njadi et al. (2018), we discretize the representations\nby clustering them into a large number of clusters.\nThen we use cluster labels instead of the continu-\nous representations in the MI estimator.\nSpeciﬁcally, we take only representations of the\n1000 most frequent (sub)words. We gather rep-\nresentations for 5 million occurrences of these at\neach layer for each of the three models. We then\ncluster the representations into N = 10000 clus-\nters using mini-batch k-means with k = 100. In\nthe experiments studying the mutual information\nbetween a layer and source (or target) labels we\nfurther ﬁlter occurrences. Namely, we take only\noccurrences where the source and target labels are\namong the top 1000 most frequent (sub)words.\n4.2.2 Results\nFirst, we estimate the MI between an input token\nand a representation of this token at each layer. In\nFigure 1: The mutual information between an input to-\nken and a representation of this token at each layer.\n(a) LM\n (b) MLM\nFigure 2: The mutual information of token representa-\ntions at a layer and source (or target) tokens. For MLM,\nonly tokens replaced at random are considered to get\nexamples where input and output are different.\nthis experiment, we form data for MLM as in the\ntest regime; in other words, the input token is al-\nways the same as the output token. Results are\nshown in Figure 1. For LM, the amount of rele-\nvant information about the current input token de-\ncreases. This agrees with our expectations: some\nof the information about the history is intuitively\nnot relevant for predicting the future. MT shows\na similar behavior, but the decrease is much less\nsharp. This is again intuitive: the information\nabout the exact identity is likely to be useful for\nthe decoder. The most interesting and surprising\ngraph is for MLM: ﬁrst, similarly to other mod-\nels, the information about the input token is getting\nlost but then, at two top layers, it gets recovered.\nWe will refer to these phases in further discussion\nas context encoding and token reconstruction, re-\nspectively. Whereas such non-monotonic behavior\nis impossible when analyzing entire layers, as in\nTishby and Zaslavsky (2015), in our case, it sug-\ngests that this extra information is obtained from\nother tokens in the context.\nWe perform the same analysis but now measur-\ning MI with the output label for LM and MLM.\nIn this experiment, we form data for MLM as in\ntraining, masking or replacing a fraction of tokens.\nWe then take only tokens replaced with a random\none to get examples where input and output to-\nkens are different. Results are shown in Figure 2.\nWe can see that, as expected, MI with input tokens\n4400\ndecreases while MI with output tokens increases.\nBoth LM and MLM are trained to predict a token\n(next for LM and current for MLM) by encoding\ninput and context information. While in Figure 1\nwe observed monotonic behavior of LM, when\nlooking at the information with both input and out-\nput tokens, we can see the two processes, losing\ninformation about input and accumulating infor-\nmation about output, for both LM and MLM mod-\nels. For MLM these processes are more distinct\nand can be thought of as the context encoding and\ntoken prediction (compression/prediction) stages.\nFor MT, since nothing is predicted directly, we see\nonly the encoding stage of this process. This ob-\nservation relates also to the ﬁndings by Blevins\net al. (2018). They show that up to a certain layer\nthe performance of representations obtained from\na deep multi-layer RNN LM improves on a con-\nstituent labeling task, but then decreases, while for\nrepresentations obtained from an MT encoder per-\nformance continues to improve up to the highest\nlayer. We further support this view with other ex-\nperiments in Section 6.3.\nEven though the information-theoretic view\nprovides insights into processes shaping the rep-\nresentations, direct MI estimation from ﬁnite sam-\nples for densities on multi-dimensional spaces is\nchallenging (Paninski, 2003). For this reason\nin the subsequent analysis we use more well-\nestablished frameworks such as canonical corre-\nlation analysis to provide new insights and also to\ncorroborate ﬁndings we made in this section (e.g.,\nthe presence of two phases in MLM encoding).\nEven though we will be using different machin-\nery, we will focus on the same two IB-inspired\nquestions: (1) how does information ﬂow across\nlayers? and (2) what information does a layer rep-\nresent?\n5 Analyzing Changes and Inﬂuences\nIn this section, we analyze the ﬂow of informa-\ntion. The questions we ask include: how much\nprocessing is happening in a given layer; which\ntokens inﬂuence other tokens most; which tokens\ngain most information from other tokens. As we\nwill see, these questions can be reduced to a com-\nparison between network representations. We start\nby describing the tool we use.\n5.1 Canonical Correlation Analysis\nWe rely on recently introduced projection\nweighted Canonical Correlation Analysis\n(PWCCA) (Morcos et al., 2018), which is an\nimproved version of SVCCA (Raghu et al., 2017).\nBoth approaches are based on classic Canonical\nCorrelation Analysis (CCA) (Hotelling, 1936).\nCCA is a multivariate statistical method for re-\nlating two sets of observations arising from an un-\nderlying process. In our setting, the underlying\nprocess is a neural network trained on some task.\nThe two sets of observations can be seen as ‘two\nviews’ on the data. Intuitively, we look at the same\ndata (tokens in a sentence) from two standpoints.\nFor example, one view is one layer and another\nview is another layer. Alternatively, one view can\nbe l-th layer in one model, whereas another view\ncan be the same l-th layer in another model. CCA\nlets us measure similarity between pairs of views.\nFormally, given a set of tokens(x1,x2,...,x N )\n(with the sentences they occur in), we gather their\nrepresentations produced by two models ( m1 and\nm2) at layers l1 and l2, respectively. To achieve\nthis, we encode the whole sentences and take rep-\nresentations of tokens we are interested in. We\nget two views of these tokens by the models:\nvm1,l1 = (xm1,l1\n1 ,xm1,l1\n2 ,...,x m1,l1\nN ) and vm2,l2 =\n(xm2,l2\n1 ,xm2,l2\n2 ,...,x m2,l2\nN ). The representations\nare gathered in two matrices X1 ∈ Ma,N and\nX2 ∈Mb,N , where a and b are the numbers of\nneurons in the models. 2 These matrices are then\ngiven to CCA (speciﬁcally, PWCCA). CCA iden-\ntiﬁes a linear relation maximizing the correlation\nbetween the matrices and computes the similarity.\nThe values of PWCCA range from 0 to 1, with 1\nindicating that the observations are linear transfor-\nmations of each other, 0 indicating no correlation.\nIn the next sections, we vary two aspects of this\nprocess: tokens and the ‘points of view’.\n5.2 A coarse-grained view\nWe start with the analysis where we do not attempt\nto distinguish between different token types.\n5.2.1 Distances between tasks\nAs the ﬁrst step in our analysis, we measure the\ndifference between representations learned for dif-\nferent tasks. In other words, we compare repre-\nsentations for vm1,l and vm2,l at different layers l.\nHere the data is all tokens of 5000 sentences. We\nalso quantify differences between representations\nof models trained with the same objective but dif-\nferent random initializations. The results are pro-\n2In our experiments, N >100000.\n4401\n(a)\n (b)\nFigure 3: PWCCA distance (a) between representa-\ntions of different models at each layer (“init.” indicates\ndifferent initializations), (b) between consecutive lay-\ners of the same model.\nvided in Figure 3a.\nFirst, differences due to training objective are\nmuch larger than the ones due to random initial-\nization of a model. This indicates that PWCCA\ncaptures underlying differences in the types of in-\nformation learned by a model rather than those due\nto randomness in the training process.\nMT and MLM objectives produce representa-\ntions that are closer to each other than to LM’s\nrepresentations. The reason for this might be two-\nfold. First, for LM only preceding tokens are in the\ncontext, whereas for MT and MLM it is the entire\nsentence. Second, both MT and MLM focus on a\ngiven token, as it either needs to be reconstructed\nor translated. In contrast, LM produces a repre-\nsentation needed for predicting the next token.\n5.2.2 Changes between layers\nIn a similar manner, we measure the difference\nbetween representations of consecutive layers in\neach model (Figure 3b). In this case we take views\nvm,l and vm,l+1 and vary layers land tasks m.\nFor MT, the extent of change monotonically de-\ncreases when going from the bottom to top lay-\ners, whereas there is no such monotonicity for LM\nand MLM. This mirrors our view of LM and es-\npecially MLM as undergoing phases of encoding\nand reconstruction (see Section 4.2), thus requir-\ning a stage of dismissing information irrelevant to\nthe output, which, in turn, is accompanied by large\nchanges in the representations between layers.\n5.3 Fine-grained analysis\nIn this section, we select tokens with some pre-\ndeﬁned property (e.g., frequency) and investigate\nhow much the tokens are inﬂuenced by other to-\nkens or how much they inﬂuence other tokens.\nAmount of change. We measure the extent of\nchange for a group of tokens as the PWCCA dis-\ntance between the representations of these tokens\n(a) MT\n (b) LM\n(c) MLM\nFigure 4: Token change vs its frequency.\nfor a pair of adjacent layers (l,l + 1). This quan-\ntiﬁes the amount of information the tokens receive\nin this layer.\nInﬂuence. To measure the inﬂuence of a token\nat lth layer on other tokens, we measure PWCCA\ndistance between two versions of representations\nof other tokens in a sentence: ﬁrst after encoding\nas usual, second when encoding ﬁrst l−1 layers\nas usual and masking out the inﬂuencing token at\nthe lth layer.3\n5.3.1 Varying token frequency\nFigure 4 shows a clear dependence of the amount\nof change on token frequency. Frequent tokens\nchange more than rare ones in all layers in both\nLM and MT. Interestingly, unlike MT, for LM\nthis dependence dissipates as we move towards top\nlayers. We can speculate that top layers focus on\npredicting the future rather than incorporating the\npast, and, at that stage, token frequency of the last\nobserved token becomes less important.\nThe behavior for MLM is quite different. The\ntwo stages for MLMs could already be seen in Fig-\nures 1 and 3b. They are even more pronounced\nhere. The transition from a generalized token\nrepresentation, formed at the encoding stage, to\nrecreating token identity apparently requires more\nchanges for rare tokens.\nWhen measuring inﬂuence, we ﬁnd that rare to-\nkens generally inﬂuence more than frequent ones\n(Figure 5). We notice an extreme inﬂuence of rare\n3By masking out we mean that other tokens are forbidden\nto attend to the chosen one.\n4402\n(a) MT\n (b) LM\n(c) MLM\nFigure 5: Token inﬂuence vs its frequency.\ntokens at the ﬁrst MT layer and at all LM layers. In\ncontrast, rare tokens are not the most inﬂuencing\nones at the lower layers of MLM. We hypothesize\nthat the training procedure of MLM, with masking\nout some tokens or replacing them with random\nones, teaches the model not to over-rely on these\ntokens before their context is well understood. To\ntest our hypothesis, we additionally trained MT\nand LM models with token dropout on the input\nside (Figure 6). As we expected, there is no ex-\ntreme inﬂuence of rare tokens when using this reg-\nularization, supporting the above interpretation.\nInterestingly, our earlier study of the MT Trans-\nformer (V oita et al., 2019) shows how this inﬂu-\nence of rare tokens is implemented by the model.\nIn that work, we observed that, for any consid-\nered language pair, there is one dedicated atten-\ntion head in the ﬁrst encoder layer which tends to\npoint to the least frequent tokens in every sentence.\nThe above analysis suggest that this phenomenon\nis likely due to overﬁtting.\nWe also analyzed the extent of change and in-\nﬂuence splitting tokens according to their part of\nspeech; see appendix for details.\n6 What does a layer represent?\nWhereas in the previous section we were inter-\nested in quantifying the amount of information ex-\nchanged between tokens, here we primarily want\nto understand what representation in each layer\n‘focuses’ on. We evaluate to what extent a cer-\ntain property is important for deﬁning a token rep-\n(a) MT\n (b) LM\nFigure 6: Token inﬂuence vs its frequency for models\ntrained with word dropout (in training, each input token\nis replaced with a random with the probability 10%).\nresentation at each layer by (1) selecting a large\nnumber of token occurrences and taking their rep-\nresentations; (2) validating if a value of the prop-\nerty is the same for token occurrences correspond-\ning to the closest representations. Though our ap-\nproach is different from probing tasks, we choose\nthe properties which will enable us to relate to\nother works reporting similar behaviour (Zhang\nand Bowman, 2018; Blevins et al., 2018; Tenney\net al., 2019a). The properties we consider are to-\nken identity, position in a sentence, neighboring\ntokens and CCG supertags.\n6.1 Methodology\nFor our analysis, we take 100 random word types\nfrom the top 5,000 in our vocabulary. For each\nword type, we gather 1,000 different occurrences\nalong with the representations from all three mod-\nels. For each representation, we take the closest\nneighbors among representations at each layer and\nevaluate the percentage of neighbors with the same\nvalue of the property.\n6.2 Preserving token identity and position\nIn this section, we track the loss of information\nabout token identity (i.e., word type) and position.\nOur motivation is three-fold. First, this will help\nus to conﬁrm the results provided on Figure 1;\nsecond, to relate to the work reporting results for\nprobing tasks predicting token identity. Finally,\nthe Transformer starts encoding a sentence from a\npositional and a word embedding, thus it is natural\nto look at how this information is preserved.\n6.2.1 Preserving token identity\nWe want to check to what extent a model confuses\nrepresentations of different words. For each of the\nconsidered words we add 9000 representations of\nwords which potentially could be confused with\n4403\n(a)\n (b)\nFigure 7: Preserving (a) token identity, (b) position\nFigure 8: t-SNE of different occurrences of the tokens\n“is” (red), “are” (orange), “was” (blue), “were” (light-\nblue). On the x-axis are layers.\nit.4 For this extended set of representations, we\nfollow the methodology described above.\nResults are presented in Figure 7a. Reassur-\ningly, the plot is very similar to the one computed\nwith MI estimators (Figure 1), further supporting\nthe interpretations we gave previously (Section 4).\nNow, let us recall the ﬁndings by Zhang and Bow-\nman (2018) regarding the superior performance of\nuntrained LSTMs over trained ones on the task of\ntoken identity prediction. They mirror our view\nof the evolution of a token representation as going\nthrough compression and prediction stages, where\nthe learning objective deﬁnes the process of for-\ngetting information. If a network is not trained, it\nis not forced to forget input information.\nFigure 8 shows how representations of differ-\nent occurrences of the words “is”, “are”, “was”,\n“were” get mixed in MT and LM layers and dis-\nambiguated for MLM. For MLM, 15% of tokens\nwere masked as in training. In the ﬁrst layer, these\nmasked states form a cluster separate from the oth-\ners, and then they get disambiguated as we move\nbottom-up across the layers.\n6.2.2 Preserving token position\nWe evaluate the average distance of position of the\ncurrent occurrence and the top 5 closest represen-\ntations. The results are provided in Figure 7b. This\n4See appendix for the details.\nFigure 9: t-SNE of different occurrences of the token\n“it”, position is in color (the larger the word index the\ndarker its color). On the x-axis are layers.\n(a) left\n (b) right\nFigure 10: Preserving immediate neighbors\nillustrates how the information about input (in this\ncase, position), potentially not so relevant to the\noutput (e.g., next word for LM), gets gradually dis-\nmissed. As expected, encoding input positions is\nmore important for MT, so this effect is more pro-\nnounced for LM and MLM. An illustration is in\nFigure 9. For MT, even on the last layer ordering\nby position is noticeable.\n6.3 Lexical and syntactic context\nIn this section, we will look at the two properties:\nidentities of immediate neighbors of a current to-\nken and CCG supertag of a current token. On the\none hand, these properties represent a model’s un-\nderstanding of different types of context: lexical\n(neighboring tokens identity) and syntactic. On\nthe other, they are especially useful for our anal-\nysis since they can be split into information about\n‘past’ and ‘future’ by taking either left or right\nneighbor or part of a CCG tag.\n6.3.1 The importance of neighboring tokens\nFigure 10 supports our previous expectation that\nfor LM the importance of a previous token de-\ncreases, while information about future token is\nbeing formed. For MLM, the importance of neigh-\nbors gets higher until the second layer and de-\ncreases after. This may reﬂect stages of context\nencoding and token reconstruction.\n4404\n(a)\n (b)\n (c)\nFigure 11: Preserving CCG supertag.\n6.3.2 The importance of CCG tags\nResults are provided in Figure 11a. 5 As in pre-\nvious experiments, importance of CCG tag for\nMLM degrades at higher layers. This agrees with\nthe work by Tenney et al. (2019a). The authors ob-\nserve that for different tasks (e.g., part-of-speech,\nconstituents, dependencies, semantic role label-\ning, coreference) the contribution 6 of a layer to a\ntask increases up to a certain layer, but then de-\ncreases at the top layers. Our work gives insights\ninto the underlying process deﬁning this behavior.\nFor LM these results are not really informative\nsince it does not have access to the future. We\ngo further and measure importance of parts of a\nCCG tag corresponding to previous (Figure 11b)\nand next (Figure 11c) parts of a sentence. It can\nbe clearly seen that LM ﬁrst accumulates infor-\nmation about the left part of CCG, understanding\nthe syntactic structure of the past. Then this infor-\nmation gets dismissed while forming information\nabout future.\nFigure 12 shows how representations of differ-\nent occurrences of the token “is” get reordered in\nthe space according to CCG tags (colors corre-\nspond to tags).\n7 Additional related work\nPrevious work analyzed representations of MT\nand/or LM models by using probing tasks. Dif-\nferent levels of linguistic analysis have been con-\nsidered including morphology (Belinkov et al.,\n2017a; Dalvi et al., 2017; Bisazza and Tump,\n2018), syntax (Shi et al., 2016; Tenney et al.,\n2019b) and semantics (Hill et al., 2017; Belinkov\net al., 2017b; Raganato and Tiedemann, 2018;\nTenney et al., 2019b). Our work complements this\n5To derive CCG supertags, we use Yoshikawa et al. (2017)\ntagger, the latest version with ELMO: https://github.\ncom/masashi-y/depccg.\n6In their experiments, representations are pooled across\nlayers with the scalar mixing technique similar to the one\nused in the ELMO model (Peters et al., 2018). The prob-\ning classiﬁer is trained jointly with the mixing weights, and\nthe learned coefﬁcients are used to estimate the contribution\nof different layers to a particular task.\nFigure 12: t-SNE of different occurrences of the token\n“is”, CCG tag is in color (intensity of a color is a token\nposition). On the x-axis are layers.\nline of research by analyzing how word represen-\ntations evolve between layers and gives insights\ninto how models trained on different tasks come\nto represent different information.\nCanonical correlation analysis has been previ-\nously used to investigate learning dynamics of\nCNNs and RNNs, to measure the intrinsic dimen-\nsionality of layers in CNNs and compare represen-\ntations of networks which memorize and general-\nize (Raghu et al., 2017; Morcos et al., 2018). Bau\net al. (2019) used SVCCA as one of the methods\nused for identifying important individual neurons\nin NMT models. Saphra and Lopez (2019) used\nSVCCA to investigate how representations of lin-\nguistic structure are learned over time in LMs.\n8 Conclusions\nIn this work, we analyze how the learning objec-\ntive determines the information ﬂow in the model.\nWe propose to view the evolution of a token\nrepresentation between layers from the compres-\nsion/prediction trade-off perspective. We conduct\na series of experiments supporting this view and\npropose a possible explanation for superior perfor-\nmance of MLM over LM for pretraining. We re-\nlate our ﬁndings to observations previously made\nin the context of probing tasks.\nAcknowledgments\nWe would like to thank the anonymous review-\ners for their comments. The authors also thank\nArtem Babenko, David Talbot and Yandex Ma-\nchine Translation team for helpful discussions and\ninspiration. Ivan Titov acknowledges support of\nthe European Research Council (ERC StG Broad-\nSem 678254) and the Dutch National Science\nFoundation (NWO VIDI 639.022.518). Rico Sen-\nnrich acknowledges support from the Swiss Na-\ntional Science Foundation (PP00P1_176727).\n4405\nReferences\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In International Conference on\nLearning Representations, New Orleans.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017a. What do neu-\nral machine translation models learn about morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 861–872. Association\nfor Computational Linguistics.\nYonatan Belinkov, Lluís Màrquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass.\n2017b. Evaluating layers of representation in neural\nmachine translation on part-of-speech and semantic\ntagging tasks. In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 1–10. Asian\nFederation of Natural Language Processing.\nArianna Bisazza and Clara Tump. 2018. The lazy en-\ncoder: A ﬁne-grained analysis of the role of mor-\nphology in neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2871–2876,\nBrussels, Belgium. Association for Computational\nLinguistics.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep RNNs encode soft hierarchical syntax.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 14–19, Melbourne, Australia.\nAssociation for Computational Linguistics.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Matthias Huck,\nPhilipp Koehn, and Christof Monz. 2018. Find-\nings of the 2018 conference on machine translation\n(wmt18). In Proceedings of the Third Conference\non Machine Translation, Volume 2: Shared Task Pa-\npers, pages 272–307, Belgium, Brussels. Associa-\ntion for Computational Linguistics.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, and Stephan V ogel. 2017. Understanding\nand improving morphological learning in the neu-\nral machine translation decoder. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 142–151. Asian Federation of Natural Lan-\nguage Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nFelix Hill, Kyunghyun Cho, Sébastien Jean, and\nY Bengio. 2017. The representational geometry of\nword meanings acquired by neural machine transla-\ntion models. Machine Translation, 31.\nHarold Hotelling. 1936. Relations between two sets of\nvariates. Biometrika, 28:321–337.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit. In Proceedings of 52nd\nAnnual Meeting of the Association for Computa-\ntional Linguistics: System Demonstrations, pages\n55–60, Baltimore, Maryland. Association for Com-\nputational Linguistics.\nAri S. Morcos, Maithra Raghu, and Samy Bengio.\n2018. Insights on representational similarity in neu-\nral networks with canonical correlation. InNeurIPS,\nMontreal, Canada.\nJan Niehues, Ronaldo Cattoni, Sebastian Stüker,\nMauro Cettolo, Marco Turchi, and Marcello Fed-\nerico. 2018. The IWSLT 2018 Evaluation Cam-\npaign. In Proceedings of the 15th International\nWorkshop on Spoken Language Translation, pages\n118–123, Bruges, Belgium.\nLiam Paninski. 2003. Estimation of entropy and mu-\ntual information. Neural computation, 15(6):1191–\n1253.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In NeurIPS, Los An-\ngeles.\nMehdi S. M. Sajjadi, Olivier Bachem, Mario Lucic,\nOlivier Bousquet, and Sylvain Gelly. 2018. Assess-\ning generative models via precision and recall. In\n4406\nAdvances in Neural Information Processing Systems\n31, pages 5228–5237.\nNaomi Saphra and Adam Lopez. 2019. Understand-\ning learning dynamics of language models with\nSVCCA. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 3257–3267, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural mt learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1526–\n1534. Association for Computational Linguistics.\nRavid Shwartz-Ziv and Naftali Tishby. 2017. Opening\nthe black box of deep neural networks via informa-\ntion. arXiv preprint arXiv:1703.00810.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computa-\ntional Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2019b. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In International Confer-\nence on Learning Representations.\nNaftali Tishby, Fernando C Pereira, and William\nBialek. 1999. The information bottleneck method.\nIn Proceedings of the 37-th Annual Allerton Con-\nference on Communication, Control and Computing,\npages 368–377.\nNaftali Tishby and Noga Zaslavsky. 2015. Deep learn-\ning and the information bottleneck principle. 2015\nIEEE Information Theory Workshop (ITW), pages 1–\n5.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008, Los Angeles.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n5797–5808, Florence, Italy. Association for Compu-\ntational Linguistics.\nMasashi Yoshikawa, Hiroshi Noji, and Yuji Mat-\nsumoto. 2017. A* CCG parsing with a supertag and\ndependency factored model. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n277–287, Vancouver, Canada. Association for Com-\nputational Linguistics.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task\nanalysis. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP, pages 359–361, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7663974761962891
    },
    {
      "name": "Computer science",
      "score": 0.7219892144203186
    },
    {
      "name": "Transformer",
      "score": 0.6404680609703064
    },
    {
      "name": "Natural language processing",
      "score": 0.5112438201904297
    },
    {
      "name": "Joint (building)",
      "score": 0.46051302552223206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43661266565322876
    },
    {
      "name": "Programming language",
      "score": 0.3258870840072632
    },
    {
      "name": "Engineering",
      "score": 0.15268954634666443
    },
    {
      "name": "Electrical engineering",
      "score": 0.09798130393028259
    },
    {
      "name": "Voltage",
      "score": 0.061826348304748535
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    }
  ]
}