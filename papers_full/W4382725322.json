{
    "title": "Fruit ripeness identification using transformers",
    "url": "https://openalex.org/W4382725322",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2805700701",
            "name": "Bingjie Xiao",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2101202397",
            "name": "Minh Nguyen",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2113680690",
            "name": "Wei Qi Yan",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2805700701",
            "name": "Bingjie Xiao",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2101202397",
            "name": "Minh Nguyen",
            "affiliations": [
                "Auckland University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2113680690",
            "name": "Wei Qi Yan",
            "affiliations": [
                "Auckland University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3110932043",
        "https://openalex.org/W3210586215",
        "https://openalex.org/W3118546327",
        "https://openalex.org/W4319068579",
        "https://openalex.org/W3150889303",
        "https://openalex.org/W4225271216",
        "https://openalex.org/W4250174831",
        "https://openalex.org/W2963651088",
        "https://openalex.org/W3042556338",
        "https://openalex.org/W3042421576",
        "https://openalex.org/W3001168107",
        "https://openalex.org/W4312881242",
        "https://openalex.org/W3038668823",
        "https://openalex.org/W4223970271",
        "https://openalex.org/W3083948783",
        "https://openalex.org/W1983016117",
        "https://openalex.org/W3215638759",
        "https://openalex.org/W3003732786",
        "https://openalex.org/W3132252581",
        "https://openalex.org/W3095247428",
        "https://openalex.org/W4210839135",
        "https://openalex.org/W3013954566",
        "https://openalex.org/W4214756052",
        "https://openalex.org/W6600783858",
        "https://openalex.org/W3217153199",
        "https://openalex.org/W3163465952",
        "https://openalex.org/W3028960437",
        "https://openalex.org/W2997004889",
        "https://openalex.org/W3197486099",
        "https://openalex.org/W3216777370",
        "https://openalex.org/W3139633126",
        "https://openalex.org/W4225295778",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W4200079737",
        "https://openalex.org/W3206529494",
        "https://openalex.org/W4214520160",
        "https://openalex.org/W3211328899",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W3004155203",
        "https://openalex.org/W4312872680",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3165924482",
        "https://openalex.org/W4212819272",
        "https://openalex.org/W4288834324"
    ],
    "abstract": "Abstract Pattern classification has always been essential in computer vision. Transformer paradigm having attention mechanism with global receptive field in computer vision improves the efficiency and effectiveness of visual object detection and recognition. The primary purpose of this article is to achieve the accurate ripeness classification of various types of fruits. We create fruit datasets to train, test, and evaluate multiple Transformer models. Transformers are fundamentally composed of encoding and decoding procedures. The encoder is to stack the blocks, like convolutional neural networks (CNN or ConvNet). Vision Transformer (ViT), Swin Transformer, and multilayer perceptron (MLP) are considered in this paper. We examine the advantages of these three models for accurately analyzing fruit ripeness. We find that Swin Transformer achieves more significant outcomes than ViT Transformer for both pears and apples from our dataset.",
    "full_text": "123\nApplied Intelligence (2023) 53:22488–22499\nhttps://doi.org/10.1007/s10489-023-04799-8\nFruit ripeness identification using transformers\nBingjie Xiao1 · Minh Nguyen1 · Wei Qi Yan1\nAccepted: 14 June 2023 \n© The Author(s) 2023\nAbstract\nPattern classification has always been essential in computer vision. Transformer paradigm having attention mechanism with \nglobal receptive field in computer vision improves the efficiency and effectiveness of visual object detection and recognition. \nThe primary purpose of this article is to achieve the accurate ripeness classification of various types of fruits. We create \nfruit datasets to train, test, and evaluate multiple Transformer models. Transformers are fundamentally composed of encod-\ning and decoding procedures. The encoder is to stack the blocks, like convolutional neural networks (CNN or ConvNet). \nVision Transformer (ViT), Swin Transformer, and multilayer perceptron (MLP) are considered in this paper. We examine \nthe advantages of these three models for accurately analyzing fruit ripeness. We find that Swin Transformer achieves more \nsignificant outcomes than ViT Transformer for both pears and apples from our dataset.\nKeywords Visual Object Detection · Vision Transformer · Swin Transformer · Mask R-CNN · MLP\n1 Introduction\nIn recent years, deep learning has been increased expo -\nnentially, with a profession of breakthroughs in theory and \narchitecture [1]. As a branch of deep learning, visual object \ndetection from digital images has also achieved great out -\ncomes in development. Visual object detection is essentially \nwith classification problem [ 2]. So far, visual object detec -\ntion has been able to accurately locate and identify multiple \ntargets. Before using deep learning for visual object detec -\ntion, conventional machine learning algorithms usually have \nthree stages: Region selection, feature extraction, and clas -\nsification. Traditional algorithms usually take use of sliding \nwindow algorithms, but the algorithms have a huge number \nof redundant bounding boxes, correspondingly computa -\ntional complexity is high.\nVisual object detection [3, 4] usually refers to detect the \nlocation of a visual object in an image and assign the label \nof corresponding class. The detector is required to output \n5-tuple: Label of object class, the coordinates of the four \ncorners of the bounding box [5].\nThe motivation of this article stems from the news that we \nusually lack professional labors picking up fruits in mature \nseason. In absence of workers, how to efficiently complete \nfruit selection and pickup is a problem. In this paper, we \nchose apples and pears as the representatives of fruits, \nimplemented the classification of fruits by classifying the \nmaturity of different fruits [ 6].\nThe purpose of this paper is to locate and classify fruits \nin the given images [ 7]. As the name implies, the model is \nrequired to accurately locate fruits in the given image and \nidentify whether the fruit is ripe or overripe [ 8, 9].\nCompared with Region Proposal Network (RPN) of \nFaster R-CNN (region-based CNN) [ 10, 11], Transformer \nis completely based on self-attention mechanism [ 12, 13]. \nThe complexity of a Transformer model ensures that the \naccuracy of the model is higher than that of R-CNN net. \nTherefore, we employed Transformer model to conduct our \nexperiments. According to the characteristics of Transformer \nmodel in visual object detection, we select Swin Trans -\nformer [14] and Vision Transformer (ViT) for experimental \nevaluations. During our experimenting, we found that the \nMLP block is an integral part of the Transformer model. \nTherefore, in this paper, MLP-based object detection model \nis implemented with axial displacement for comparative \nexperiments.\nAs shown in Fig. 1, visual object detection using a bound-\ning box to mark fruits and label the classes is implemented, \n * Wei Qi Yan \n wyan@aut.ac.nz\n1 Auckland University of Technology, Auckland 1010, \nNew Zealand\n/ Published online: 29 June 2023\n123\nFruit ripeness identification using transformers\nthe coordinates are sent to Transformer model for the pro -\ncess of encoding and decoding [15]. In the experiments, we \nanalyze the difference between attention mechanisms of ViT \nand Swin Transformer, introduce Mask R-CNN weights to \nget the best accuracy. Overall, the contributions of this paper \nare:\n(1) We created our own dataset and adopted Swin Trans -\nformer model to achieve fruit object detection and \nachieve 87.43% precision.\n(2) We combined Transformer module with YOLO module \ntogether to achieve accurate classification of fruits, so \nthat the model can distinguish the maturity of apples \nor pears.\nIn the second part of this paper, we depict previous \nresearch work related to Transformers including embedding, \nattention mechanism, and MLP. In the third part, we will \ndetail our proposed models and related work. The results are \nthen presented in the fourth section. Our conclusion regard-\ning the experiments and inspiration for future work will be \ndrawn in Section 5.\n2  Literature review\n2.1  Fruit recognition\nFruit recognition in deep learning is use of mathemati -\ncal models to detect the position and class label of fruits \nbased on given digital images. Fruit recognition based \non pixel intensities is an initial idea [ 16]. R-CNN model \nextracts region of interest for locating fruits [ 17]. Seg -\nmentation provides image regions of interest for wide \nselection [ 18].\nIn practical applications of fruit recognition, visual \nobject has a small area in the image, or mutual occlu -\nsion between the targets [ 19, 20]. For example, in fruit \nimages we have collected, we have clearly separated \npears from the images, there may also be dense apples \npiled on the trees that are difficult to be distinguished. \nFaster R-CNN takes use of the overlapping ground truth \nand predicted bounding boxes to achieve the detection of \nsmall objects [ 21]. Fruit surface disease detection [ 22] is \nassociate with fruit ripeness detection. In actual experi -\nment, we locate the position of fruits in the input image \nand determine the located fruit class. Finally, we analyze \nthe ripeness of fruits by using the fruit appearance in the \ninput images.\nTraditional feature extraction methods were applied to \nidentify diseases of fruits such as tomato [ 23]. Wu et al. \nexperimented two Transformers to obtain feature informa -\ntion, and took advantage of patches of multiple resolutions \nfor multi-granularity feature extraction. Jia et al. improved \nDenseNet by using residual network (ResNet), optimized \nthe training parameters, and made the model identify \napples with an accuracy rate of 97.31% [ 24]. Regarding \nthe Transformers, more encoding modules are encapsu -\nlated to extract effective feature information. Therefore, in \nthis paper, a multi-level attention feature extraction mod -\nule was created. Compared with visual features that CNN \ncan capture, the Transformers can identify details.\nTransformer’s multi-head self-attention.\nEncoder\nLinear Projection of Flattened Patches\n0 1 2 3 4 5 6 7 8 9\nExtra learnable\nclass embedding\nMLP Head\nRi pe a ppl e\nOutput\nInput fruit imageLabelled data\nFig. 1  The flowchart of object detection using Vision Transformer\n22489\n123\nB. Xiao et al.\n2.2  T ransformer and mask R‑CNN model\nTransformer is based on self-attention mechanism, which \nhas virally spanned in the field of Natural Language Process-\ning (NLP). Multilayer Perceptron (MLP) is the earliest and \nsimplest neural network in the NLP. In order to handle more \ncomplex problems, the mainstream architecture of artificial \nneural networks has undergone the evolution of MLP-CNN \n[25, 26] and recurrent neural network (RNN).\nSimilar to the research work in fruit recognition, the pro-\nject was initialized with Swin Transformer [ 27]. Similar to \nthe models that automatically recognize pests encountered \nin rice growth, an experiment essentially was conducted that \nthe model can identify the maturity of fruits, and ultimately \nachieved the goal of agricultural automation. Sliding win -\ndows of Swin Transformer model were taken advantage for \nhierarchical design, which achieved the accuracy 93.4%.\nHan, et al. also studied the use of Transformer model \nto realize the control of machines [ 28]. The robot grasping \nframe takes use of tactile and visual information to achieve \nsafe grasping of visual objects. Similar to our experiments, \nHan's team also made use of the characteristics of predefined \nobjects to perform training on the Transformer model by \ncomparing with CNN + LSTM model.\nSmall object detection by using deep learning has been \ntaken into account in practice [ 29]. Transformer and CNN \nmodels were employed for local perception network of Swin \nTransformer, a Spatial Attention Interleaved Execution Cas-\ncade (SAIEC) network was designed to enhance the segmen-\ntation of digital images. The final model is 1.7% more than \nthe base Swin Transformer network. The multi-perceptual \ndesign of Transformer model outperforms residual network \nto realize the cross-channel transfer of each feature of visual \nobject [30]. If the data is massively enhanced or distillable, \nthe Transformer model does not need to make global adjust-\nments to the convolutional layer, maximum pooling layer, \nand global average pooling layer (GAP) like the CNN model \n[31] or Mask R-CNN model [32].\nThe mainstream algorithms [33] of visual object detection \nwere explored. Unlike CNN, which completes the extraction \nof local image information and constructs global informa -\ntion by stacking convolutional layers, Transformer models \nobtain complete global information from the beginning, it \nhas stronger long-term dependence. In the ViT model, the \naverage attention distance increases from small to large with \nthe deepening increases of the layers, which has a similar \nparadigm of CNN [ 34]. In ViT, if the scaling ability of the \nTransformer is stronger, the transmission effect will be bet-\nter [35]. However, because the Transformer does not have \nbionic characteristics like CNN, in the learning process, the \ntraining set of the Transformer model needs to be enhanced \nor the number of datasets can be increased so as to acquire \nbetter results.\nMask R-CNN was combined with ResNet-50 to detect \nwheat diseases and achieved an accuracy 88.19% [ 36]. In \nMask R-CNN model, ResNet-50 was employed to extract \nRPN and generate various anchors. During anchor box \nextraction, mask loss and bounding box loss are taken into \nconsideration. RPN generated a binary mask for each visual \nobject. Anchor box regions were applied to ROI alignment \nfeatures [37]. After ROI alignment, fully-connected layers \nare employed for bounding boxes regression and classifica-\ntion, each object is detected by using a mask-form convo -\nlutional layer.\nAn axially displaced AS-MLP architecture [ 38] was \nemployed to encode global spatial features. In the experi -\nments, the axial displacement of the feature maps enabled \nMLP model to achieve the same function of local feature \nextraction as CNN architecture. The MLP-Mixer [ 39] pays \nmuch attention to the changes of the ViT based on the MLP \narchitecture. MLP-Mixer splits the image into multiple non-\noverlapping patches, and then takes use of the fully con -\nnected layer to convert each patch into feature embedding \nand sends it to the mixer layer. The MLP-Mixer model can \nbe understood to replace the blocks of ViT with the Mixer \nlayer.\nInspired by the previous work, we adopted Swin Trans -\nformer combined with Mask R-CNN [40] and ViT model to \nachieve fruit ripeness classification [ 41–43]. We also have \nthe MLP object detection model to compare with MLP block \nin the Transformer model.\n3  Methodology\n3.1  F ruit ripeness identification\nFruit ripeness identification is essentially an object detection \ntask. The object detection task is to describe the whole input \nimage as the content, and then detect the specified object. \nAs shown in Fig.  2, the input is a given image. We are use \nof bounding boxes to segment the fruit of interest from the \nbackground and determine the class and location. After the \nmodel has been trained along with the Transformer model, \nthe output is a list, each item in the list includes the fruit \nclass and location of the detected object.\n3.2  Swin transformer\nSwin Transformer has four stages in Fig.  2, each of which \nis similar. As shown in Fig.  3, the red box represents a \nwindow to perform self-attention, and the black box shows \neach patch. The input size of the image for the Swin Trans-\nformer is W × H × C , and t he image is grouped into a patch \ncollection of H\n4 × W\n4  by using 4 × 4 patch. The first stage \nof Swin model is to use a linear embedding and convert \n22490\n123\nFruit ripeness identification using transformers\nthe input patch features into C, then send them to a Swin \nTransformer block. Stages 2 to 4 are the same, using a \npatch merging to merge adjacent patches and feed them \ninto the next Swin Transformer block. As shown in Fig.  2, \nthe role of patch merging is to complete the down sam -\npling of features.\nW\nH Patch\nPartition\nLinear\nEmbe dding\nSwin\nTransformer\nBlock\nStage1\nPatch\nMerging\nSwin\nTransformer\nBlock\nStage2\nPatch\nMerging\nSwin\nTransformer\nBlock\nStage3\nPatch\nMerging\nSwin\nTransformer\nBlock\nStage4\nW-MSA\nLN\nLN\nMLP\nW-MSA\nLN\nLN\nMLP\nArchitectureTwoS ucce ssive\nTransformerB locks\nWindow partition\nFig. 2  The architecture of Swin Transformer\nWindow partition\nMasked\nMSA\nMasked\nMSA\nW\nH\nPatchs ize\nWindow partition\nCyclic shift\nA\nB\nC\nA\nB\nC\nReversec yclic shift\nA\nB\nC\nA\nB\nC\nFig. 3  The shift window of Swin Transformer based on MSA\n22491\n123\nB. Xiao et al.\nThe original feature size of Swin Transformer is \n[H1 ,W1 ,C1 ] . W indow partition progress is based on the \noriginal size of the reshape, the size is,\nThe Swin Transformer Block is characterized by using a \nshift window to replace the standard Multi-head Self-Atten-\ntion (MSA) module. The attention of Swin Transformer is,\nwhere B stands for position code, Q means query vector, K \nrepresents a vector representing the queried information, V \nshows queried information of vector. The variance is d.\nMask R-CNN is a two-stage framework [ 44, 45]. In the \nfirst stage, the proposals are generated. In the second stage, \nthe proposals are classified, bounding boxes and masks are \ngenerated. Mask R-CNN includes FPN to solve the degrade \nof training process. FPN adopts the top-down structure and \nhorizontal connection to conduct the fusion of the feature \nmap from the bottom to the top, which can implement fast \nconnection and extraction of all scales. FPN is also a sliding \n(1)\nReshape Size=[ H 1 × W 1\nwindowsize× windowsize\n,windowsize,windowsize,C 1 ]\n(2)Attention(Q , K , V )= SoftMax\n�\nQK T\n√\nd\n+ B\n�\nV\nwindow with a fixed window size. Feature extraction is con-\nducted through the backbone network, the generated feature \nmap is input into the Region Proposal Network (RPN) for \nsub-network selection.\nIn Fig. 4, in order to implement the intersection of the upper \nwindow partition of each block, Swin Transformer adds 3 × 3 \nshift window to 2 × 2 window to improve feature transfer. \nSince the size of shift window is not the same, the shift pro-\ncessing is fulfilled. The cyclic shift modifies the size from 3 × 3 \nto 2 × 2 , t hen the reverse cyclic shift is conducted according to \nthe attention model so as to obtain the shift window attention \n[46, 47]. The 3 × 3 window feature map is shifted and becomes \na 2 × 2 window, but the actual calculation is still expected to be \ncarried out in 3 × 3 windows, that is, the results of 9 attentions \nare implemented with the help of masks.\nTransformer self-attention is set up by using a specific mask. \nWhile performing attention analysis, only the effective part in \none window is calculated, and the rest is masked. The original \ncalculation method of attention can be changed. The shaded \narea B shown in Fig. 3 is the part that needs to be masked out.\nWindow-based local self-attention (W-MSA) segments \nthe input image into non-overlapping windows, and conducts \nself-attention calculations in different windows. Assume that \nan image has h× w patches, each window contains M × M \nInput\nEmbedding\nMulti-Head\nAttention\nAdd&Norm\nInputs\nFeed\nForward\nAdd&Norm\nNorm\nMulti-Head\nAttention\nEmbedded\nPatches\nNorm\nMLP\nMulti-Head\nAttention\nVision \nTransformer \nEncoder\nTransformer \nEncoder\nNorm\nNorm\nMLP\nChannel\nprojection\nMLP\nNetwork\nArchitecture\nVertical shift Horizontal shift\nChannel\nprojection\nChannel\nprojection\nChannel\nprojection Axial Shift\nFig. 4  The difference between Transformer and MLP methods\n22492\n123\nFruit ripeness identification using transformers\nregions, then the computational complexities of MSA and \nW-MSA are respectively shown as:\n3.3  Vision transformer\nTransformer is employed in natural language processing [48, \n49]. The attention mechanism of Transformer is also broadly \nemployed, such as Se module, CBAM module and other \nattention modules, these attention modules can improve \nnetwork performance. The ViT model demonstrates that a \nstructure that does not rely on CNNs can achieve perfect \nresults for image classification, which is also very suitable \nfor transfer learning. The ViT blocks of the original image \nare input into the encoder of the original Transformer model, \nand finally a fully-connected layer is applied to classify the \nimage.\nAs shown in Fig.  1, ViT model is mainly composed of \nthree modules: 1) Linear projection (i.e., Embedding layer \nof patch + position); 2) Transformer encoder; 3) MLP head \n(i.e., classification layer). The Transformer encoder module \ninputs the patch shown in the black box in Fig.  4. In ViT \nTransformer, each small image is regarded as a token (rep -\nresenting a word in NLP), and the correlation between each \ntoken is calculated in the model.\nIn Fig.  5, the relative encoding of Swim Transformer \nmodel is mainly to solve the problem of arrangement invari-\nance in self-attention, that is, tokens input in different orders \nwill get the same result. In ViT, it is not enough to just split \nthe image into small patches. What the encoder module \nneeds is a vector with a shape as [num _ token,token_ dim] . \nFor the input image data, the shape [H, W, C] does not meet \nthe input requirements, so it is necessary to convert the \nimage data into tokens through the embedding layer. Trans-\nformer encoder module is to stack the encoder block several \ntimes, mainly composed of the following parts:\n(3)Ω(MSA)= 4whC2 + 2(hw)2C\n(4)Ω(W − MSA)= 4whC2 + 2M 2hwC\n1) La yer normalization\nLayer normalization is to calculate the mean and vari -\nance of all feature maps of the sample, and then normalize \nit. ViT also splits the input image into patches. The process \nof using patch embedding is to compress each patch into a \nvector with a dimension through a fully-connected network.\n2) Multi-head attention\nIn Fig. 4, ViT takes use of self-attention to express the rela-\ntionship between each patch and other patches. ViT generates q, \n(5)\nMultiHead(Q , K , V )= Attention(QW Q\ni , KW K\ni , VW V\ni )\nW-MSA\nClassification\nDetection\nClassification\nVision \nTransformerSwin \nTransformer\nMSA\nFig. 5  Swin Transformer and Vision Transformer calculate the self-\nattention of regions in non-overlapping windows\nFig. 6  Horizontal displacement \nprocess of MLP object detection\nPatch\npartition\nLinear\nembedding\nAxials hift\nMLPb lock\nStage1\nPatch\nmerging\nAxials hift\nMLPb lock\nStage2\nPatch\nmerging\nAxials hift\nMLPb lock\nStage4\nPatch\nmerging\nAxials hift\nMLPb lock\nStage3\nInputi mage\n22493\n123\nB. Xiao et al.\nk, and v, it integrates q, k, and v into num_heads, and then per-\nforms self-attention operations on each of them, finally merges \nthem together. Multi-head self-attention isolates parameters and \ncan better focus associated features together for training.\n3) MLP block\nA MLP block is an inverted bottleneck structure consist-\ning of connected layer, GELU activation function and Drop-\nOut. It should be noted that there is no decoder module in \nViT Transformer. Therefore, there is no need to calculate the \ncross-attention value of encoder and decoder.\n3.4  MLP object detection mechanism\nMulti-layer perceptron (MLP) neural network is a kind of \nneural networks that are use of a combination of multiple \nperceptron to implement the segmentation and transmission \nof feature information. MLP neural network consists of an \ninput layer, multiple hidden layers and an output layer. All \nneurons in an MLP are similar, each neuron has a number of \ninputs that connect to the previous layer and output neurons \nthat connect to the next layer. Each neuron will pass the \nsame value to multiple connected output neurons.\nMLP network has less inductive bias, so MLP-based \nbackbone can achieve visual object detection [ 50, 51]. The \nstructures shown in Fig.  7 and Fig.  8 indicate the process \nof axial displacement using MLP object detection method.\nFigure 6 shows the four stages of MLP. Similar to the Trans-\nformer model shown in Fig. 3, the MLP object detection model \nalso splits the original image into multiple 4 × 4 patches.\nAs shown in the Fig.  7, MLP is an operation of a local \nreceptive field, which is more suitable for extracting fea -\ntures with local dependencies. For the fruit image in the \nexperiment, the position of the apple is based on the posi -\ntion of the given output, so MLP relies on the weighting \nof local features to better extract local features.\n4  Results\n4.1  Da taset and parameter settings\nWe collected apple and pear datasets by using our phone \ncameras, we intuitively compare the learned parameters \nwith the parameters of real models. Figure  8 is the input \nimage from our dataset. We labelled our samples with soft-\nware tool LabeleMe. We assign the number of samples in \nPadding\nPadding\nHorizontal shift\nPadding\nPadding\nHorizontal shiftp rocess\nFig. 7  Hor izontal displacement process of MLP object detection\nFig. 8  Samples of the image \ndataset. (a) A sample of input \nimages, (b) A sample of output \nimages\n(a) A sample of input images.  (b) A sample of output images.\n(x,y)\n22494\n123\nFruit ripeness identification using transformers\nthe training set as 2,000, set the conventional parameter \nbatch size to 1.0 and the learning rate to 0.0001 accord -\ning to the computer configuration for training. The size of \nimages is 1920  × 1080.\nIn Fig. 8 (a), the green square box is bounding box, the \nfour points represent the coordinates of the bounding box \n( x, y, w , h ). Figure 8 (b) shows the predicted result by using \nthe trained detector. The experiment is analyzed with the \naccuracy of the model through observing the changes of \niteration parameters. A diversity of fruits is defined as differ-\nent classes, and the same fruit defines ripeness according to \nthe smoothness of the skin. A smooth peel is defined as the \nclass “Ripe”, a folded or decayed surface is defined as the \nclass “Overripe”. In multiclass classification, each class can \nbe drawn as a curve according to recall and precision rate. \nThe average precision is the area under the curve, the mean \naverage precision refers to average the AP of each category. \nWe take use of the value of mean average precision (mAP) \nto evaluate the quality of the proposed model.\n4.2  Result analyze\nWe took use of two scales of ViT models: Base and large. \nvit_base_patch_16 represents the ViT base model; the size \nof image patch is 16 × 16 . vit_lar ge_patch_32  means that \nthe ViT large model is applied, and the image patch size \nis 32 × 32 . In T able  1, the ViT model does not show better \nperformance. More iterations did not achieve better results, \nViT model did not perform well in the trade-off between \nsmall datasets and large datasets. The ViT model is usually \npre-trained based on large datasets. Compared with the ViT \nmodel, CNN can perform better in small datasets.\nAlthough the accuracy of the ViT model in Table  1 is \nnot high, in the training process, the model takes use of less \ncomputing resources that can better allocate computing \nresources. ViT performs structural pruning on the Trans -\nformer model, and then quantizes the pruned model to obtain \nthe final optimized model. However, during the pruning pro-\ncess, the ViT model requires an additional training process, \nwhich limits the practicability of the model. Although the \nmemory usage and execution time are reduced in the process \nof ViT model pruning, it cuts off the accuracy of this model.\nIn Table 2, MLP object detection model has very strong \nperformance in small models. MLP model pays much \nattention to local feature extraction, but if the model \ncapacity is expanded, there will be overfitting problems. \nThe overfitting problems will lead to a roadblock to the \nsuccess of MLP. The self-attention structure of the ViT \nmodel also includes the MLP block. Self-attention is \nrelated to a sequence, which mainly emphasizes that each \nposition of the sequence has the same set of MLP param -\neters, and then conducts a weighted average operation in \nthe new space. The MLP model is a nonlinear mapping. \nWe see from Table 1 and Table 2 that the MLP model can \nbetter capture the features of the model.\nDifferent from ViT and MLP, Swin Transformer's self-\nattention calculation based on moving window ensures that \nthe model can extract more features of visual objects.\nWe chose three weights to train the Swin Transformer \nmodel. In Table 3 and Table 4, the patch can make up four \nwindows after moving, it is impossible for the patch to slide \nTable 1  The results of precisions by training ViT model\nModel Epoch Weights AP@0.5:0.95\nVision Transformer 10 vit_base_patch_16 0.4560\nvit_base_patch_32 0.4060\nvit_large_patch_16 0.4560\nvit_large_patch_32 0.4120\n20 vit_base_patch_16 0.4310\nvit_base_patch_32 0.4310\nvit_large_patch_16 0.3560\nvit_large_patch_32 0.4250\n30 vit_base_patch_16 0.4000\nvit_base_patch_32 0.4190\nvit_large_patch_16 0.3880\nvit_large_patch_32 0.3880\n50 vit_base_patch_16 0.3810\nvit_base_patch_32 0.3940\nvit_large_patch_16 0.4060\nvit_large_patch_32 0.4000\nTable 2  The results of MLP by training Mask R-CNN small weights\nModel Epoch Weights AP50 AP@0.5:0.95 Average \ninference \ntime(seconds)\nMLP using Axial shift MLP block 10 mask_rcnn_small_patch4_1x 0.9450 0.8310 0.5850\n30 0.9560 0.8430 0.5370\n50 0.9600 0.8470 0.5400\n10 mask_rcnn_tiny_patch4_1x 0.9330 0.8270 0.3820\n30 0.9550 0.8440 0.3850\n50 0.9580 0.8440 0.3760\n22495\n123\nB. Xiao et al.\nthrough each window, the mask is employed to contact and \ncalculate the attention in each window. Therefore, Swin \nTransformer is a hierarchical representation that has the \nability to perform complex linear calculations.\nIn Swin Transformer, shifting the window segmenta -\ntion results in more windows, and leads to a large num -\nber of computations while filling smaller windows into \nlarger ones. By setting a reasonable mask, shifted windows \nachieve equivalent calculation results under the same num -\nber of windows as window attention. We observe that Swin \nTransformer achieves better training results. In contrast, \nlarger weights and more iterations allow the model to achieve \nbetter results.\nMLP pays attention to feature transfer. The MLP model \nis use of axial displacement to arrange the features of spatial \npositions in the same position, so that the model can obtain \nlocal dependencies, the model can achieve performance \ncomparable to that of the Transformer model. However, \nEq. (3) and (4) show how many computations are required \nfor MSA and WMSA. The complexity of MSA is related to \n(h× w)2 , and t he complexity of W-MSA is related to (h× w) . \nTherefore, the amount of W-MSA calculation will be small. \nIf the original image is large, the amount of W-MSA calcula-\ntion has obvious advantages. Hence, in Table 2, Table 5, and \nTable 6, we observe more intuitively that experiments with \nthe Swin Transformer module can achieve faster speeds.\nAs shown in Table 5 and Table 6, we harnessed different \nframeworks to implement the Swin Transformer. We see \nthat with the increase of iterations, the results of classifica -\ntion training gradually become better and tend to be stable. \nConventional Transformers take use of pre-normalization at \nthe beginning of each residual branch, which normalizes the \nmagnitude of the input and has not restrictions on the out -\nput. Under pre-normalization, the output activation values   of \neach residual branch are directly merged back into the main \nbranch and accumulated layer by layer, so the amplitude of \nthe main branch increases with depth.\nSwin Transformer takes use of residual-post-normaliza -\ntion. The normalization layer was moved from the begin -\nning to the end of each residual branch, so that the output \nof each residual branch is normalized before being merged \nback into the main branch, as the number of layers deepens, \nthe magnitude of the main branch will not be accumulated.\n4.3  Discussion\nThe advantage of our experiment lies in the better accuracy \nachieved. The Swin Transformer model reached an average \nprecision of 87.43%. Our model is able to accurately locate \nan apple or pear in the input image and tell us whether the \ncurrent fruit belongs to the class “Ripe” or “Overripe”. At \nthe same time, the model also achieves fast and accurate \nrecognition within 0.13 s.\nSimilar to our experiments, an average accuracy of 89.3% \nwas achieved for Kiwifruit detection [ 7], while an average \naccuracy of 88.45% was obtained for banana detection, respec-\ntively. Compared with previous experiments, the weakness of \nour experiment lies in the fact that in practical applications, the \ninfluence of the noise generated by the environment of differ-\nent fruits on the collection of data sets should be more con -\nsidered. At the same time, we should consider changing more \nkinds of pixels in the dataset to simulate the actual growth \nenvironment of the fruit during the fruit picking process.\n5  C onclusion and future work\nIn our experiments related to fruit ripeness classifica -\ntion, we found that for small targets and small datasets, \nthe Swin Transformer model showed its advantages and \nTable 3  The results of Swin \nTransformer by training Mask \nR-CNN small weights\nModel Epoch Weights AP50 AP@0.5:0.95\nSwin Transformer 10 mask_rcnn_small_patch4_1x 0.9390 0.8210\n20 0.9400 0.8230\n30 0.9480 0.8300\n50 0.9510 0.8390\n10 mask_rcnn_small_patch4_3x 0.8340 0.6810\n50 0.9460 0.8350\nTable 4  The results of Swin \nTransformer by training Mask \nR-CNN tiny weights\nModel Epoch Weights AP50 AP@0.5:0.95\nSwin Transformer 10 mask_rcnn_tiny_patch4_1x 0.9360 0.8090\n20 0.9380 0.8230\n30 0.9450 0.8220\n50 0.9450 0.8230\n22496\n123\nFruit ripeness identification using transformers\naccuracy. We have implemented the classification of fruits \nof different maturity, and the model can be practically \napplied in warehouse management, agricultural automatic \npicking, etc.\nThe actual results show that in the ViT Transformer, the \nCNN responding to edges is weak. CNN can only compute \ncorrelations with adjacent pixels. Due to the characteristics \nof sliding window convolution, non-domain pixels cannot \nbe jointly calculated, which makes spatial information unus-\nable. Swin Transformer can provide hierarchical feature \nrepresentation, self-attention based on moving window can \neffectively achieve feature extraction.\nIn future, we will further utilize the unique self-attention \nmechanism of Vision Transformer to capture the pixel infor-\nmation between tokens to ensure that ViT model can obtain \npretty rich features with the same parameters [1, 52].\nTable 5  The results of each class by training Swin Transformer and YOLO module with small Mask R-CNN weights\nModel Weights Epoch Class mAP Average \ninference \ntime(seconds)\nYOLOX + Swin Transformer mask_rcnn_small_patch4_3x 10 Ripe apple 0.0000 0.1217\nOver apple 0.2200\nRipe pear 0.0200\nOverripe pear 0.4600\n20 Ripe apple 0.0000 0.1210\nOver apple 0.0060\nRipe pear 0.0860\nOverripe pear 0.4340\n30 Ripe apple 0.7867 0.1205\nOver apple 0.4687\nRipe pear 0.8404\nOverripe pear 0.8416\n50 Ripe apple 0.8889 0.1212\nOver apple 0.6695\nRipe pear 0.8856\nOverripe pear 0.9127\nTable 6  The results of each class by training Swin Transformer moudel and tiny Mask R-CNN weights\nModel Weights Epoch Class mAP Average inference \ntime(seconds)\nYOLOX + Swin Transformer mask_rcnn_tiny_patch4_3x 10 Ripe apple 0.1978 0.1288\nOver apple 0.0100\nRipe pear 0.0000\nOverripe pear 0.0061\n20 Ripe apple 0.4142 0.1300\nOver apple 0.1392\nRipe pear 0.1833\nOverripe pear 0.6463\n30 Ripe apple 0.8702 0.1320\nOver apple 0.8270\nRipe pear 0.8322\nOverripe pear 0.8426\n50 Ripe apple 0.8791 0.1334\nOver apple 0.8292\nRipe pear 0.8909\nOverripe pear 0.8981\n22497\n123\nB. Xiao et al.\nAuthors contribution statement All authors contributed to this paper \nequally.\nFunding and/or Conflicts of interests/Competing interests   Open  \nAccess funding enabled and organized by CAUL and its Member \nInstitutions All authors agreed with the content to submit. This paper \nhas not relevant information regarding sources of funding, financial or \nnon-financial interests.\nData availability and access  The data appeared in this paper is avail -\nable upon request.\nDeclarations \nEthical and informed consent for data used  No ethical data in this \npaper.\nCompeting interests  No conflict of interests in this paper that are \ndirectly or indirectly related to the work submitted for publication.\nOpen Access  This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta -\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1 .  Y an W (2021) Computational methods for deep learning: theo -\nretic, practice and applications. Springer Cham\n 2. Zhu X, Lyu S, Wang X, Zhao Q (2021) TPH-YOLOv5: Improved \nYOLOv5 based on transformer prediction head for object detec -\ntion on drone-captured scenarios. In: IEEE/CVF International \nConference on Computer Vision, pp 2778–2788\n 3. Lee D, Kim J, Jung K (2021) Improving object detection quality by \nincorporating global contexts via self-attention. Electronics 10(1):90\n 4. Qi J, Nguyen M, Yan W (2022) Small visual object detection in smart \nwaste classification using transformers with deep learning. Interna-\ntional Conference on Image and Vision Computing (IVCNZ), Auck-\nland.  https:// link. sprin ger. com/ chapt er/ 10. 1007/ 978-3- 031- 25825-1_ \n22\n 5. Zhang R, Li X, Zhu L, Zhong M, Gao Y (2021) Target detection \nof banana string and fruit stalk based on YOLOv3 deep learning \nnetwork. In: IEEE International Conference on Big Data, Artifi -\ncial Intelligence and Internet of Things Engineering (ICBAIE), \nIEEE, pp 346–349\n 6. Fu Y, Nguyen M, Yan W (2022) Grading methods for fruit fresh-\nness based on deep learning. Springer Nature Computer Science\n 7. Fu L, Feng Y, Majeed Y, Zhang X, Zhang J, Karkee M, Zhang Q \n(2018) Kiwifruit detection in field images using Faster R-CNN \nwith ZFNet. IFAC-Papers OnLine 51(17):45–50\n 8. Femling F, Olsson A, Alonso-Fernandez F (2018) Fruit and veg-\netable identification using machine learning for retail applica -\ntions. In: International Conference on Signal-Image Technology \n& Internet-Based Systems (SITIS), pp 9–15\n 9. Kuznetsova A, Maleva T, Soloviev V (2020) Using YOLOv3 algo-\nrithm with pre-and post-processing for apple detection in fruit-\nharvesting robot. Agronomy 10(7):1016\n 10. Gao F, Fu L, Zhang X, Majeed Y, Li R, Karkee M, Zhang Q \n(2020) Multi-class fruit-on-plant detection for apple in SNAP \nsystem using Faster R-CNN. Comput Electron Agric 176:105634\n 11. Wang Q, Qi F (2019) Tomato diseases recognition based on Faster \nR-CNN. In: International Conference on Information Technology \nin Medicine and Education (ITME), pp 772–776\n 12. Ding M, Xiao B, Codella N , Luo P, Wang J, Yuan L (2022) \nDaViT: Dual attention Vision Transformers. ECCV\n 13. Hua X, W ang X, Rui T, Zhang H, Wang D (2020) A fast self-\nattention cascaded network for object detection in large scene \nremote sensing images. Appl Soft Comput 94:106495\n 14. Zheng H, Wang G, Li X (2022) Swin-MLP: A strawberry appear-\nance quality identification method by Swin transformer and multi-\nlayer perceptron. J Food Meas Charact:1–12\n 15. Ji Y, Zhang H, Zhang Z, Liu M (2021) CNN-based encoder-\ndecoder networks for salient object detection: A comprehensive \nreview and recent advances. Inform Sci 546:835–857\n 16. Jimenez AR, Ceres R, Pons JL (2000) A survey of computer vision \nmethods for locating fruit on trees. Transact ASAE 43(6):1911\n 17. Shalini K, Srivastava AK, Allam S, Lilaramani D (2021) Compar-\native analysis on deep convolutional neural network models using \nPyTorch and OpenCV DNN frameworks for identifying optimum \nfruit detection solution on RISC-V architecture. In: IEEE Mysore \nSub Section International Conference (MysuruCon), pp 738–743\n 18. Hameed K, Chai D, R assau A (2022) Score-based mask edge \nimprovement of Mask R-CNN for segmentation of fruit and veg-\netables. Expert Syst Appl 190:116205\n 19. Song H, Sun D, Chun S, Jampani V, Han D, Heo B, Yang MH \n(2022) ViDT: an efficient and effective fully Transformer-based \nobject detector. ICLR\n 20. Tu S, Pang J, Liu H, Zhuang N, Chen Y, Zheng C, Xue Y (2020) \nPassion fruit detection and counting based on multiple scale Faster \nR-CNN using RGB-D images. Precis Agricult 21(5):1072–1091\n 21. Behera SK, Rath AK, Sethy PK (2021) Fruits yield estima -\ntion using Faster R-CNN with MIoU. Multimed Tools Appl \n80(12):19043–19056\n 22. Wang H, Mou Q, Yue Y, Zhao H (2020) Research on detection \ntechnology of various fruit disease spots based on Mask R-CNN. \nIn IEEE International Conference on Mechatronics and Automa-\ntion (ICMA), pp 1083–1087\n 23. Wu S, Sun Y, Huang H (2021) Multi-granularity feature extraction \nbased on vision transformer for tomato leaf disease recognition. \nIn International Academic Exchange Conference on Science and \nTechnology Innovation (IAECST), pp 387–390. IEEE\n 24. Jia W, Tian Y, Luo R, Zhang Z, Lian J, Zheng Y (2020) Detection \nand segmentation of overlapped fruits based on optimized mask \nR-CNN application in apple harvesting robot. Comput Electron Agric \n172:105380\n 25. Benz P, Ham S, Zhang C, Karjauv A, Kweon I (2021) Adversarial \nrobustness comparison of vision transformer and MLP-mixer to \nCNNs. BMVC\n 26. Yu T, Li X, Cai Y, Sun M, Li P (2021) Rethinking token-mixing \nMLP for MLP-based vision backbone. BMVC\n 27. Zhang Z, Gong Z, Hong Q, Jiang L (2021) Swin Transformer \nbased classification for rice diseases recognition. In: EEE Interna-\ntional Conference on Computer Information Science and Artificial \nIntelligence (CISAI), pp 153–156\n 28. Han Y, Yu K, Batra R, Boyd N, Zhao T, She Y, Hutchinson S, Zhao \nY (2021) Learning generalizable vision-tactile robotic grasping \nstrategy for deformable objects via transformer. https:// arxiv. org/ \nabs/ 2112. 06374\n22498\n123\nFruit ripeness identification using transformers\n 29. X u X, Feng Z, Cao C, Li M, Wu J, Wu Z, Ye S (2021) An improved \nSwin Transformer-based model for remote sensing object detection \nand instance segmentation. Remote Sens 13(23):4779\n 30. Touvron H, Bojanowski P, Caron M, Cord M, El-Nouby A, Grave  \nE, Jégou H (2023)  ResMLP: Feedforward Networks for image \nclassification with data-efficient training. IEEE Transactions on \nPattern Analysis and Machine Intelligence 45:5314–5321. https:// \ndoi. org/ 10. 1109/ TPAMI. 2022. 32061 48\n 31. Saedi SI, Khosravi H (2020) A deep neural network approach \ntowards real-time on-branch fruit recognition for precision horti-\nculture. Expert Syst Appl 159:113594\n 32. Ganesh P, Volle K, Burks TF, Mehta S (2019) Deep orange: mask \nR-CNN based orange detection and segmentation. IFAC-Paper -\nsOnLine 52(30):70–75\n 33. Arkin E, Yadikar N, Muhtar Y, Ubul K (2021) A survey of object \ndetection based on CNN and transformer. In: IEEE Interna -\ntional Conference on Pattern Recognition and Machine Learning \n(PRML, pp 99–108\n 34. Xiang AJ, Huddin AB, Ibr ahim MF, Hashim FH (2021) An oil \npalm loose fruits image detection system using Faster R-CNN and \nJetson TX2. In International Conference on Electrical Engineering \nand Informatics (ICEEI), pp 1–6\n 35. Zhang P, Dai X, Yang J, Xiao B, Yuan L, Zhang L, Gao J (2021) \nMulti-scale vision longformer: A new vision transformer for high-\nresolution image encoding. In: IEEE/CVF International Confer -\nence on Computer Vision, pp 2998–3008\n 36. Kumar D, Kukreja V (2022) Image-based wheat mosaic virus \ndetection with Mask R-CNN model. In: International Conference \non Decision Aid Sciences and Applications (DASA), pp 178–182\n 37. Chen X, Hsieh CJ, Gong B (2022) When vision transformers out-\nperform ResNets without pre-training or strong data augmenta -\ntions. CLR\n 38. Lian D, Yu Z, Sun X, Gao S (2022) As-MLP: An axial shifted \nMLP architecture for vision. ICLR\n 39. Tolstikhin IO, Houlsby N, Kolesnikov A, Beyer L, Zhai X, Unter-\nthiner T, Dosovitskiy A (2021) MLP-mixer: An all-MLP archi -\ntecture for vision. In: Advances in Neural Information Processing \nSystems 34:24261–24272\n 40. Liu Z, Deng Y, Ma F, Du J, Xiong C, Hu M, Ji X (2021) Target \ndetection and tracking algorithm based on improved Mask R-CNN \nand LMB. In: International Conference on Control, Automation \nand Information Sciences (ICCAIS), pp 1037–1041\n 41. Pannerselvam K (2021) Adaptive parking slot occupancy detec -\ntion using vision transformer and LLIE. In: IEEE International \nSmart Cities Conference (ISC2), pp 1–7\n 42. Ranftl R, Bochkovskiy A, Koltun V (2021) Vision transformers \nfor dense prediction. In IEEE/CVF International Conference on \nComputer Vision, pp 12179–12188\n 43.  Zhang Z, Lu X, Cao G, Yang Y, Jiao L, Liu F (2021) ViT-\nYOLO: Transformer-based YOLO for object detection. In: IEEE/\nCVF International Conference on Computer Vision Workshops \n(ICCVW), pp 2799–2808. https:// doi. org/ 10. 1109/ ICCVW 54120. \n2021. 00314\n 44. He K, Gki oxari G, Dollár P, Girshick R (2017) Mask R-CNN. \nIn: IEEE International Conference on Computer Vision, pp \n2961–2969\n 45. Mai X, Zhang H, Jia X, Meng MQH (2020) F aster R-CNN with \nclassifier fusion for automatic detection of small fruits. IEEE  \nTrans Autom Sci Eng 17(3):1555–1569. https:// doi. org/ 10. 1109/ \nTASE. 2020. 29642 89\n 46. Luo Z, Nguyen M, Yan W (2022) Kayak and sailboat detection \nbased on the improved YOLO with Transformer. In: International \nConference on Control, Automation and Robotics (ICCAR)\n 47. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Guo B (2021) Swin \nTransformer: Hierarchical vision transformer using shifted win -\ndows. In: IEEE/CVF International Conference on Computer \nVision, pp 10012–10022\n 48. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko \nS (2020) End-to-end object detection with transformers. In: Euro-\npean Conference on Computer vision, Springer, pp 213–229\n 49. Dai Z, Cai B, Lin Y , Chen J (2021) Up-DETR: Unsupervised \npre-training for object detection with transformers. In: IEEE/\nCVF Conference on Computer Vision and Pattern Recognition, \npp 1601–1610 \n 50. Chen S,  Chen S, Xie E, Chongjian GE,  Chen R, Liang D, Ping \nD, Luo P (2021) CycleMLP: A MLPlike architecture for dense \nprediction. ICLR 2022. https:// openr eview. net/ forum? id= NMEce \nG4v69Y\n 51. Yu T, Li X, Cai Y, Sun M, Li P (2022) S2-MLP: spatial-shift \nMLP architecture for vision. In IEEE/CVF Winter Conference on \nApplications of Computer Vision, pp 297–306\n 52. Yan W (2019) Introduction to intelligent surveillance: surveillance \ndata capture, transmission, and analytics. Springer Cham. https:// \ndoi. org/ 10. 1007/ 978-3- 030- 10713-0\nPublisher's note  Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nBingjie Xiao is a PhD student with the Auckland University of Technol-\nogy, Auckland New Zealand, her research interests are deep learning \nand computer vision.\nDr. Minh Nguyen is the head of the department within the School of  \nEngineering, Computer & Mathematical Sciences at Auckland Univer-\nsity of Technology, New Zealand. His research is primarily focused on \nthe intersection of Computer Vision, Artificial Intelligence, and Virtual/\nAugmented Reality with an emphasis on their industrial applications.\nDr. Wei Qi Yan research interests include deep learning, intelligent surveil-\nlance, computer vision, multimedia computing, etc. Dr. Yan’s expertise is \ncomputational mathematics and applied mathematics, computer science \nand computer engineering. Dr. Yan was a world’s top 2% cited scientist \nlisted by Stanford University in 2022.\n22499"
}