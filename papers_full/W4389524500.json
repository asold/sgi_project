{
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "url": "https://openalex.org/W4389524500",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2096688504",
            "name": "Dong Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2099787611",
            "name": "Shimin Li",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2002306827",
            "name": "Xin Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1988187755",
            "name": "Jun Zhan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2101305655",
            "name": "Pengyu Wang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2145859727",
            "name": "Yaqian Zhou",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2115470192",
            "name": "Xipeng Qiu",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4322718246",
        "https://openalex.org/W3169320628",
        "https://openalex.org/W4385807416",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4287120025",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W4385570101",
        "https://openalex.org/W4386384714",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4377297670",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4313679638",
        "https://openalex.org/W4375958083",
        "https://openalex.org/W4361229539",
        "https://openalex.org/W4381786045",
        "https://openalex.org/W3030437843",
        "https://openalex.org/W4310924890",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4367061106",
        "https://openalex.org/W4323651091",
        "https://openalex.org/W4366330503",
        "https://openalex.org/W4385572615",
        "https://openalex.org/W4307680525",
        "https://openalex.org/W3140429000",
        "https://openalex.org/W3036601975",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W2995181338",
        "https://openalex.org/W4394671563"
    ],
    "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. With discrete speech representations, we construct SpeechInstruct, the first large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow cross-modal human instructions and highlight the potential of handling multiple modalities with one model. Code and models are available in https://github.com/0nutation/SpeechGPT. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15757‚Äì15773\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nSpeechGPT: Empowering Large Language Models with Intrinsic\nCross-Modal Conversational Abilities\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang,\nYaqian Zhou‚àó, Xipeng Qiu‚àó\nSchool of Computer Science, Fudan University\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\n{dongzhang22,xin_zhang22,jzhan22,pywang22}@m.fudan.edu.cn\n{smli20,zhouyaqian,xpqiu}@fudan.edu.cn\nAbstract\nMulti-modal large language models are re-\ngarded as a crucial step towards Artificial Gen-\neral Intelligence (AGI) and have garnered sig-\nnificant interest with the emergence of Chat-\nGPT. However, current speech-language mod-\nels typically adopt the cascade paradigm, pre-\nventing inter-modal knowledge transfer. In\nthis paper, we propose SpeechGPT, a large\nlanguage model with intrinsic cross-modal\nconversational abilities, capable of perceiving\nand generating multi-modal content. With\ndiscrete speech representations, we construct\nSpeechInstruct, the first large-scale cross-\nmodal speech instruction dataset. Addition-\nally, we employ a three-stage training strat-\negy that includes modality-adaptation pre-\ntraining, cross-modal instruction fine-tuning,\nand chain-of-modality instruction fine-tuning.\nThe experimental results demonstrate that\nSpeechGPT has an impressive capacity to\nfollow cross-modal human instructions and\nhighlight the potential of handling multiple\nmodalities with one model. Code and models\nare available in https://github.com/\n0nutation/SpeechGPT. Demos are\nshown in https://0nutation.github.\nio/SpeechGPT.github.io/.\n1 Introduction\nLarge language models (OpenAI, 2023; Touvron\net al., 2023) have performed astonishingly on vari-\nous natural language processing tasks. Meanwhile,\nmulti-modal large language models, such as GPT-\n4, PALM-E (Driess et al., 2023), and LLaV A (Liu\net al., 2023), have explored the ability of LLMs to\nunderstand multi-modal information. However, a\nsignificant gap exists between current LLMs and\ngeneral artificial intelligence (AGI). First, most cur-\nrent LLMs can only perceive and understand multi-\nmodal content but cannot spontaneously generate\nmulti-modal content. Second, continuous signals\n‚àóCorresponding author\nSpeech\nGPT\nPlease read the sentence:\n‚ÄúToday is a beautiful day.‚Äù\nRecord the content:\n \nThe content of speech is:\n‚ÄúHave a good day!‚Äù \nWhat is the capital of \nFrench ?\nWhat is the capital of \nFrench ?\nThe capital of French is \nParis.\nThe capital of French is \nParis.\nÔºàWhat is your name?Ôºâ\nÔºàWhat is your name?Ôºâ\n ÔºàMy name is SpeechGPT.Ôºâ\nÔºàMy name is SpeechGPT.Ôºâ\nSure, I will read it now:\n \nSure, I will read it now:\n \nFigure 1: SpeechGPT‚Äôs capabilities to tackle multiple\ncross-modal tasks.\nlike images and speech cannot be adapted directly\nto LLMs that receive discrete tokens.\nThe current speech-language model mainly\nadopts a cascading paradigm (Huang et al., 2023a)\ni.e., the LLM is connected with an automatic\nspeech recognition (ASR) model or a text-to-\nspeech (TTS) model in tandem, or the LLM is\nemployed as a control hub, with several speech\nprocessing models (Cheng et al., 2023a,b,c) are\nintegrated to cover multiple audio or speech\ntasks (Huang et al., 2023a; Shen et al., 2023). Some\nprior work on generative spoken language mod-\nels involves encoding the speech signal into a dis-\ncrete representation (Baevski et al., 2020; Hsu et al.,\n2021; Zhang et al., 2023a) and modeling it with\nlanguage models (Lakhotia et al., 2021; Borsos\net al., 2022; Zhang et al., 2023d; Wang et al., 2023;\nZhang et al., 2023c).\nWhile capable of perceiving and generating\nspeech, the existing cascaded methods or spoken\nlanguage models still have several limitations. First,\nthe LLM in the cascaded model only functions as\na content generator. Since the representations of\nspeech and text are not aligned, the LLM‚Äôs knowl-\nedge cannot be transferred to the speech modality.\nSecond, the cascade approach (Shen et al., 2023;\nHuang et al., 2023a) suffers from the loss of par-\nalinguistic signals such as emotion and prosody.\nThird, existing spoken language models (Wang\n15757\net al., 2023; Zhang et al., 2023d) only synthesize\nspeech but fail to comprehend its semantic infor-\nmation, preventing them from achieving true cross-\nmodal perception and generation.\nIn this paper, we propose SpeechGPT, a large\nlanguage model with intrinsic cross-modal conver-\nsational abilities, capable of perceiving and gen-\nerating multi-modal content. We perform speech\ndiscretization with a self-supervised trained speech\nmodel to unify the modality between speech and\ntext. The discrete speech tokens are then expanded\ninto the vocabulary of the LLM, thus endowing the\nmodel with an inherent competence to perceive and\ngenerate the speech.\nTo provide the model with the capacity to\nhandle multi-modal instructions, we build the\nfirst speech-text cross-modal instruction-following\ndataset SpeechInstruct. Specifically, we discretize\nthe speech to discrete units (Hsu et al., 2021) and\nconstruct the cross-modal unit-text pair based on\nthe existing ASR dataset. Meanwhile, we construct\nhundreds of instructions for diverse tasks with GPT-\n4 to simulate actual user instructions as illustrated\nin Appendix B. In addition, to further enhance the\nmodel‚Äôs cross-modal capability, we designed the\nChain-of-Modality instruction data, i.e., the model\nreceives the speech command, thinks about the\nprocess in text, and then outputs the response in\nspeech.\nFor better cross-modal transfer and efficient\ntraining, SpeechGPT undergoes a three-stage train-\ning process: modality-adaptation pre-training,\ncross-modal instruction fine-tuning, and chain-of-\nmodality instruction fine-tuning. The first stage en-\nables speech comprehension for SpeechGPT with\nthe discrete speech unit continuation task. The sec-\nond stage employs the SpeechInstruct to improve\nthe model‚Äôs cross-modal capabilities. The third\nstage utilizes parameter-efficient LoRA (Hu et al.,\n2021) fine-tuning for further modality alignment.\nTo evaluate the effectiveness of SpeechGPT,\nwe conduct a wide range of human evaluations\nand case analyses to estimate the performance of\nSpeechGPT on textual tasks, speech-text cross-\nmodal tasks, and spoken dialogue tasks. The re-\nsults demonstrate that SpeechGPT exhibits a strong\nability for unimodal and cross-modal instruction\nfollowing tasks.\nOur contributions include the following:\n‚Ä¢ We build the first multi-modal large language\nmodel that can perceive and generate multi-\nmodal contents.\n‚Ä¢ We construct and release SpeechInstruct, the first\nlarge-scale speech-text cross-modal instruction-\nfollowing dataset.\n‚Ä¢ We build the first spoken dialogue LLM with\nstrong human instruction following ability and\nspoken dialogue ability.\n‚Ä¢ We show great potential to incorporate other\nmodalities into LLMs through discrete represen-\ntations.\n2 Related Work\nMulti-modal Large Language Model Current\nmulti-modal LLMs predominantly focus on the vi-\nsual domain, feeding continuous representations\nobtained from pre-trained visual encoders into\nLLMs, facilitating full-parameter or parameter-\nefficient training on visual-language data (Ope-\nnAI, 2023; Huang et al., 2023b; Zhang et al.,\n2023b). Palm-E (Driess et al., 2023) integrates\nthe 540B PaLM (Chowdhery et al., 2022) and 22B\nVision Transformer (Dosovitskiy et al., 2021) into\nthe largest vision-language model. LLaV A (Liu\net al., 2023) leverages pre-trained CLIP (Radford\net al., 2021) visual encoder and LLaMA (Touvron\net al., 2023) and conduct instruct tuning on GPT4-\nassisted visual instruction data. X-LLM (Chen\net al., 2023) converts multi-modalities into repre-\nsentations with X2L interfaces as the inputs of the\nlarge language model. However, such structures\nonly enable LLMs to process multi-modal input,\nwithout ability to generate multi-modal output. Di-\nverging from prior studies, our approach empha-\nsizes the development of a speech-centric multi-\nmodal LLM, endowing it with the proficiency to\naccommodate both multi-modal input and output.\nGenerative Spoken Language Model Discrete\nself-supervised representation based spoken gen-\nerative language modeling is making remark-\nable progress on large-scale speech dataset train-\ning (Nguyen et al., 2022). AudioLM (Borsos et al.,\n2022) proposes to model speech based on audio\ncodecs together with semantic codes, which can\nsynthesize speech in a textlesss setting. V ALL-\nE (Wang et al., 2023) builds a generative spoken\nlanguage model on audio codecs and treat Text-\nto-Speech as a conditional generation task. How-\never, these models are designed for a specific task\nand failed to benefit from LLMs. SpeechGPT\nis built upon the foundation of LLM and trans-\nfers LLM‚Äôs knowledge to speech modality, con-\n15758\nsequently obtaining better task generalization and\nhuman-instruction following ability.\nSpeech-Enabled LLM Interaction Following\nthe emergence of ChatGPT, several studies have\nconcentrated on the integration of expert speech\nmodels with LLMs to enable direct speech interac-\ntion with LLMs. HuggingGPT (Shen et al., 2023)\nfacilitates task decomposition of human instruc-\ntions by LLMs and allows the invocation of mod-\nels from Huggingface to accomplish specific tasks,\nencompassing a range of automatic speech recog-\nnition (ASR) and text-to-speech models. Audio-\nGPT (Huang et al., 2023a) leverages a variety of\naudio foundation models to process complex audio\ninformation and connect LLMs with input/output\ninterface (ASR, TTS) for speech conversations.\nHowever, these models exhibit increased complex-\nity, demand extensive resources, and are prone to\nthe unavoidable error accumulation problems. Our\napproach enables speech interaction with LLMs\nwithout relying on ASR or TTS systems, circum-\nventing the aforementioned drawbacks.\n3 SpeechInstruct Construction\nDue to the limitations in publicly available speech\ndata and the lack of variety of speech-text tasks,\nwe construct SpeechInstruct, a speech-text cross-\nmodal instruction-following dataset. This dataset\nconsists of two parts, the first part is called Cross-\nModal Instruction, and the second part is called\nChain-of-Modality Instruction. The construction\nprocess of SpeechInstruct is illustrated in Figure 2.\n3.1 Cross-modal Instruction\nData Collection We collect several large-scale\nEnglish ASR datasets to construct Cross-Modal\nInstruction, including Gigaspeech (Chen et al.,\n2021), Common V oice (Ardila et al., 2020), and\nLibriSpeech (Panayotov et al., 2015). We employ\nmHuBERT1 as the speech tokenizer to discretize\nspeech data into discrete units and remove the repet-\nitive units of adjacent frames to get reduced units.\nUltimately, we obtain 9 million unit-text data pairs.\nTask Description Generation We generate ASR\nand TTS task descriptions that are compatible with\nspeech-text data pairs. Unlike the Self-Instruct\nmethod (Wang et al., 2022), we generate descrip-\ntions through a zero-shot approach. Specifically,\nwe directly input the prompts shown in Appendix A\n1https://dl.fbaipublicfiles.com/\nhubert/mhubert_base_vp_en_es_fr_it3.pt\ninto OpenAI GPT-4 to generate task descriptions.\nOur generation method yields 100 instructions for\neach task and some examples are shown in Ap-\npendix B.\nInstruction Formatting For a discrete unit se-\nquence U and its associated transcription T, we\ndetermine whether it will be used for construct-\ning an ASR task or a TTS task based on the\nprobability p. Subsequently, we randomly select\na description D from the corresponding task de-\nscription. This results in a triplet consisting of\nthe task description, discrete unit sequence, and\ntranscription, denoted as (D, U, T). Following\nthis, the triplet is assembled into an instruction\nusing the template: [Human]:{D}. This is input:\n{U}<eoh>.[SpeechGPT]: {T}<eos>..\n3.2 Chain-of-Modality Instruction\nSpeech Instruction Generation Due to the lack\nof instruction data with speech input and speech\noutput, we trained a text-to-unit generator to con-\nvert text instruction data into speech instruction\ndata. Specifically, the text-to-unit generator adopts\na Transformer encoder-decoder architecture. We\ntrained it on LibriSpeech unit-text pairs in Cross-\nmodal Instruction. We select 37,969 samples from\nthe moss-002-sft-data dataset 2 whose response\nlength is shorter than 35 words. And we convert\nboth their instructions and responses into unit se-\nquences through the text-to-unit generator. As\na result, we obtained 37,969 quadruplets com-\nposed of speech instructions, text instructions,\ntext responses, and speech responses, denoted as\n(SpeechI, TextI, TextR, SpeechR).\nInstruction Formatting Using the above quadru-\nplets, we could construct chain-of-thought style\ninstructions for four input-output formats, namely\nSpeech Instruction-Speech Response, Speech\nInstruction-Text Response, Text Instruction-Speech\nResponse, and Text Instruction-Text Response.\nTheir corresponding templates can be found in Ap-\npendix C.\n3.3 SpeechInstruct Evaluation Set\nWe constructed cross-modal dialogue datasets\nunder different scenarios to evaluate whether\nSpeechGPT could take on various roles. Specif-\nically, these included a talking encyclopedia, per-\nsonal assistant, chat partner, poet, psychologist,\n2https://huggingface.co/datasets/fnlp/\nmoss-002-sft-data\n15759\nDiscreteSpeechUnit Extractor\n<99><5><69><597>‚Ä¶‚Ä¶<31>\nUnit Vocoder\n<43><2><64><33>‚Ä¶‚Ä¶<534>[Human]:\n[SpeechGPT]:\nMetaPrompt\nInstructionsCross-modalInstructiondata\nInstruction-tuningTextDatasets\nSpeech2Unit\nText2Unit\nSpeechDatasets\nChain-of-ModalityInstructiondataSpeechInstruct\nChain-of-ModalityInstructions\nCross-modalInstructions[Human]:Transcribethespeechtotext.Thisistheinput:{speechunitùëà}<eoh>.[SpeechGPT]:{transcriptionùëá}<eos>.[Human]:Thisisthespeechinstruction:{speech}.Youcandoitstepbystep.Youcantranscribetheinstruction,getthetextresponseandspeaktheresponse. <eoh>.[SpeechGPT]:[tq]{Text\tùêº};[ta]{Text\tùëÖ};[ua]{SpeechR}<eoa>.\nTemplate1 Template2\nTranscription:Goodmorning,whatisyourname?\nTranscription:Hi,mynameis .Nicetomeetyou!\nFigure 2: Left: An overview of SpeechInstruct construction process. The SpeechInstruct dataset consists of\ntwo parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. Template 1 is shown in 3.1.\nTemplate 2 is shown in Appendix C. Right: An illustration of SpeechGPT model structure.\nand educational assistant. For each role, we pro-\nvide 10 manually authored instruction-response\npairs written by ourselves. We use a pre-trained\ntext-to-speech model 3 to convert the text into cor-\nresponding speech. We then employ mHuBERT\nto discretize speech data into discrete units as de-\nscribed in Section 3.1. Ultimately, for each role,\nwe obtained 10 quadruplets composed of speech\ninstructions, text instructions, text responses, and\nspeech responses.\n4 SpeechGPT\n4.1 Model Structure\nA unified framework is designed to provide archi-\ntecture compatibility across different modalities.\nAs shown in Figure 2, our model consists of three\nmain components: discrete unit extractor, large\nlanguage modaland unit vocoder. Under this ar-\nchitecture, LLM can perceive multi-modal inputs\nand generate multi-modal outputs.\nDiscrete Unit Extractor The discrete unit ex-\ntractor utilizes the Hidden-unit BERT (HuBERT)\nmodel (Hsu et al., 2021) to transform continuous\nspeech signals into a sequence of discrete units, .\nHuBERT is a self-supervised model that learns\nby predicting discrete labels for masked audio\nsegments based on k-means clustering applied to\nthe model‚Äôs intermediate representations. It fea-\ntures a combination of 1-D convolutional layers\nand a Transformer encoder to encode speech into\ncontinuous intermediate representations, with a k-\n3https://huggingface.co/facebook/\nfastspeech2-en-ljspeech\nmeans model further converting these representa-\ntions into a sequence of cluster indices. Subse-\nquently, adjacent duplicate indices are removed,\nresulting in a discrete units sequence represented\nas U = (u1, u2, . . . , uT ), ui ‚àà0, 1, . . . , K‚àí1,\n‚àÄ1 ‚â§i ‚â§T, with K denoting the total number of\nclusters.\nLarge Language Model We employ the Meta\nAI LLaMA (Touvron et al., 2023) model as our\nLarge Language Model. LLaMA comprises an em-\nbedding layer, multiple transformer blocks, and\nan LM head layer. The total number of parame-\nters in LLaMA ranges from 7B to 65B. Drawing\nfrom an extensive training dataset of 1.0 trillion\ntokens, LLaMA demonstrates competitive perfor-\nmance compared to the substantially larger 175B\nGPT-3 across various NLP benchmarks.\nUnit Vocoder Due to limition of single speaker\nunit vocoder in (Polyak et al., 2021), we train a\nmulti-speaker unit HiFi-GAN to decode the speech\nsignal from the discrete representation. The HiFi-\nGAN architecture consists of a generator G and\nmultiple discriminators D. The generator uses\nlook-up tables (LUT) to embed discrete representa-\ntions and the embedding sequences are up-sampled\nby a series of blocks composed of transposed con-\nvolution and a residual block with dilated layers.\nThe speaker embedding is concatenated to each\nframe in the up-sampled sequence. The discrimi-\nnator features a Multi-Period Discriminator (MPD)\nand a Multi-Scale Discriminator (MSD), which\nhave the same architecture as (Polyak et al., 2021).\n15760\n4.2 Training\nTo incorporate speech discrete representation into\nLLM, we expand the vocabulary and corresponding\nembedding matrix first. We divide the training pro-\ncess into three stages. The first stage is Modality-\nAdaptation Pre-training on unpaired speech data.\nThe second stage is Cross-modal Instruction Fine-\nTuning. The third stage is Chain-of-Modality In-\nstruction Fine-Tuning.\nExpanding Vocabulary Given original LLM vo-\ncabulary V of size |V |, to integrate speech discrete\nrepresentations into LLM, we expand the vocab-\nulary with an additional set of unit tokens V ‚Ä≤, of\nsize |V ‚Ä≤|= K. The expanded vocabulary V ‚Ä≤‚Ä≤is\nthe union of the original vocabulary V and the new\nwords V ‚Ä≤:\nV ‚Ä≤‚Ä≤= V ‚à™V ‚Ä≤ (1)\nWe denote the original word embedding matrix as\nE ‚ààR|V |√ód, where d is the dimension of word\nembeddings. To accommodate the expanded vo-\ncabulary, we need to create a randomly initialized\nword embedding matrix E‚Ä≤ ‚ààR|V ‚Ä≤‚Ä≤|√ód. We pre-\nserve the original word embeddings by copying the\nvalues of E to the first |V |rows of E‚Ä≤:\nE‚Ä≤[0 :|V |, :] =E (2)\nFinally, we replace the original vocabulary and\nword embedding matrix with the new vocabulary\nV ‚Ä≤‚Ä≤and the word embedding matrix E‚Ä≤.\nStage 1: Modality-Adaptation Pre-training To\nenable LLM to handle discrete units modality, we\nutilize an unlabeled speech corpus to train LLM in\na next-token prediction task. This approach aligns\nwith the text pre-training objective of LLM. Given\nunlabeled speech corpus C consisting of speech\nU1, U2, . . . , Um and LLM denoted as L1, the nega-\ntive log-likelihood loss can be formulated as:\nL(L|C) =‚àí\nm‚àë\nj=1\nnj‚àë\ni=1\nlog P(ui,j|u<i,j; L) (3)\nwhere m is the number of speech in dataset C, nj\nis the number of discrete unit token in speech Uj,\nand ui,j represents the i-th unit token in the j-th\nspeech.\nStage 2: Cross-modal Instruction Fine-\nTuning In this stage, we align speech and text\nmodalities utilizing paired data. We mix Cross-\nmodal Instruction in SpeechInstruct with moss-002-\nsft dataset to derive mix dataset I, which consists\nof samples T1, T2, . . . , Tx. We fine-tune the model\nL obtained from the first stage on I.\nEach sample Tj consisting of t1, t2, . . . , tnj is\nformed by concatenating a prefix and a text. The\ntraining objective is to minimize the negative log-\nlikelihood and the loss calculation only considers\nthe text part, ignoring the prefix, which can be for-\nmated as:\nL(L|I) =‚àí\nx‚àë\nj=1\nyj‚àë\ni=pj +1\nlog P(ti,j|t<i,j; L) (4)\nwhere x is the number of samples in corpus I, yj is\nthe total number of tokens in sample Tj, pj is the\nnumber of tokens in the prefix part of Tj, and ti,j\nrepresents the i-th word in Tj.\nStage 3: Chain-of-Modality Instruction Fine-\nTuning After obtaining the model in stage 2,\nwe utilizes parameter-efficient Low-Rank Adap-\ntation (LoRA) (Hu et al., 2021) to fine-tune it on\nChain-of-Modality Instruction in SpeechInstruct.\nWe add LoRA weights (adapters) to the attention\nmechanisms and train the newly added LoRA pa-\nrameters. We adopt the same loss function as stage\n2.\n5 Experiments\n5.1 Experimental Setups\nDatasets For modality-adaption pre-training, we\nuse LibriLight (Kahn et al., 2020) which contains\n60K hours of unlabelled English audiobook speech.\nFor cross-modal instruction fine-tuning stage, we\nuse Gigaspeech (Chen et al., 2021), Common\nvoice (Ardila et al., 2020) and LibriSpeech (Panay-\notov et al., 2015) dataset and moss-002-sft-data\ndataset, which is illustrated in detail in 3.1. For\nchain-of-modality instruction fine-tuning stage, we\nuse moss-002-sft-data dataset, which is illustrated\nin detail in 3.2.\nConfiguration We employ LLaMA-13B (Touvron\net al., 2023) as our backbone model for a trade-off\nbetween performance and computational resources\navailable. For stage 1, we use 96 A100 GPUs and\ntrain for 900 steps with batch size 768. For stage\n2, we use 96 A100 GPUs and train for 2100 steps\nwith batch size 1536. For stage 3, we use 8 A100\nGPUs and train for 4200 steps with batch size 128.\n15761\nDetails about training hyperparameters are shown\nin Appendix D. For decoding, we set the maximum\nsequence length to 2048 and set the temperature to\n0.8. We use Top-k sampling with k=60. We also\nuse Top-p sampling with p=0.8.\n5.2 Baselines\nWe establish two cascaded cross-modal conversa-\ntional systems as our baselines. The first model, re-\nferred to as Speech-Alpaca-13B, consists of an off-\nthe-shell ASR system 4, Alpaca 13B (Taori et al.,\n2023) as well as a pre-trained TTS system 5. The\nsecond model, named Speech-LLaMA-MOSS-002,\nincorporates the same ASR and TTS system, along\nwith a large language model obtained by perform-\ning supervised fine-tuning on LLaMA-13B using\nMOSS-sft-002 as the training dataset.\n5.3 Evaluation\nWe evaluate the cross-modal instruction-following\ncapabilities of SpeechGPT across four tasks:\nspeech-to-speech instruction-following (S2SIF),\nspeech-to-text instruction-following(S2TIF), text-\nto-speech instruction-following(T2SIF), and text-\nto-text instruction-following(T2TIF).\nData We randomly select 40 samples from the\nAlpacaEval dataset 6 and use the pre-trained TTS\nmodel in Section 3.3 to convert the text into cor-\nresponding speech. We then employ mHuBERT\nto discretize speech data into discrete units as de-\nscribed in Section 3.1. These are combined with\nthe SpeechInstruct Evaluation Set to constitute our\ntest set, which contains 100 samples. Each sample\nis a quadruplet composed of a speech instruction,\ntext instruction, text response, and speech response.\nWe denote them as ground truth.\nChatGPT Score We utilize ChatGPT (GPT-\n3.5-turbo) to assess the cross-modal instruction-\nfollowing performance. For tasks that include\nspeech, we leveraged the pre-trained ASR model\nin section 5.2 to transform the speech into its corre-\nsponding text, which is subsequently submitted for\nevaluation. Inspired from (Zhou et al., 2023), we\nfeed the prompt in appendix F to ChatGPT to score\nthe model‚Äôs outputs based on response quality, with\nscores ranging from 1 to 5.\n4https://huggingface.co/facebook/\nwav2vec2-large-960h-lv60-self\n5https://huggingface.co/facebook/\nfastspeech2-en-ljspeech\n6https://github.com/tatsu-lab/alpaca_\neval\nHuman Opinion Score Following (Nguyen et al.,\n2022), we calculate the human opinion score of the\ngenerated examples through crowdsourcing. These\nopinions are based on two dimensions: the con-\ntent mean opinion score (CMOS) for content and\nmeaningfulness quality, and the naturalness mean\nopinion score (NMOS) for speech naturalness and\nfluency. For CMOS, we ask participants to fo-\ncus on the correctness of the content in speech or\ntext, without paying attention to the quality of the\nspeech. For NMOS, we direct participants to fo-\ncus on the quality, smoothness, and naturalness of\nthe speech, without considering its content. We in-\nvited five volunteers to perform the evaluation, and\nasked them to rate within a range of 1-5, where 1\nrepresents the worst and 5 represents the best. For\nspeech-to-speech instruction-following and text-\nto-speech instruction-following tasks, we calcu-\nlate both CMOS and NMOS. For speech-to-text\ninstruction-following and text-to-text instruction-\nfollowing tasks, we calculate CMOS.\n5.4 Main Results\nContent As shown in Table 1, taking into ac-\ncount the comprehensive evaluation of ChatGPT\nScore and CMOS, SpeechGPT demonstrates su-\nperior performance on speech instructions (S2SIF\nand S2TIF) compared to the two baseline systems.\nThis indicates that SpeechGPT outperforms the\nASR model in the cascaded system when it comes\nto understanding speech content. From the per-\nspective of CMOS, SpeechGPT achieves perfor-\nmance similar to the baseline systems on T2SIF\nand T2TIF tasks, indicating that SpeechGPT still\npossesses commendable text and speech generation\ncapabilities. In S2SIF and T2SIF tasks, ChatGPT\nScore and CMOS values exhibit ambiguity in the\nground truth and baseline systems. This can be\nattributed to speech responses being synthesized by\nTTS system, which can have errors in pauses be-\ntween sentences. This introduces significant errors\nfor longer responses, leading to incorrect text after\nbeing processed by the ASR system, thereby reduc-\ning the ChatGPT score. However, humans can un-\nderstand the content of such speech, so the CMOS\nscore is normal. Cases of cross-modal instruction-\nfollowing can be found in Appendix G.\nSpeech Quality As shown in Table 1, SpeechGPT\nexhibits significantly higher NMOS values com-\npared to the baseline systems. This indicates that\nthe speech responses generated by SpeechGPT out-\n15762\nChatGPT Score Human Opinion ScoreCMOS NMOSMethods S2SIF S2TIF T2SIF T2TIFS2SIF S2TIF T2SIF T2TIFS2SIF S2TIF T2SIF T2TIF\nGround Truth 2.85‚àó 3.74 2.91‚àó 3.93 3.78 3.89 3.95 4.12 3.18 - 3.20 -\nBaselines: cascaded cross-modal conversational systemsSpeech-Alpaca-13B2.74 3.31 2.71 3.83 3.39 3.42 3.71 3.75 3.12 - 3.13 -Speech-LLaMA-MOSS-0022.87 3.50 3.23 3.82 3.38 3.44 3.74 3.83 3.14 - 3.11 -\nSpeechGPT 3.42 3.52 3.53 3.64 3.42 3.49 3.57 3.69 3.65 - 3.62 -\nTable 1: Main Results of SpeechGPT. S2SIF refers to speech-to-speech instruction-following, S2TIF is speech-to-\ntext instruction-following, T2SIF denotes text-to-speech instruction-following and T2TIF represents text-to-text\ninstruction-following. ChatGPT score is obtained through ChatGPT evaluatation. CMOS refers to content mean\nopinion score. NMOS denotes naturalness mean opinion score. ‚àó: The low ChatGPT Score for speech responses in\nGround Truth is due to them being synthesized by TTS system, which can have errors in pauses between sentences.\nThis introduces significant errors for longer responses, leading to incorrect text after being processed by the ASR\nsystem, thereby reducing the score. However, humans can understand the content of such speech, so the CMOS\nscore is normal.\nTraining Inference ChatGPT Score\nStandard Standard 2.15\nStandard CoM 2.12\nCoM Standard 2.35\nCoM CoM 3.42\nTable 2: ChatGPT Score on speech-to-speech\ninstruction-following task. CoM refers to chain-of-\nmodality prompting and Standard denotes standard\nprompting.\nperform the TTS system in the cascaded system in\nterms of audio quality and prosody. More detailed\nspeech prosody analysis are located in Section ??.\n6 Analysis\n6.1 Chain-of-modality prompting matters\nTable 2 shows ChatGPT Scores on speech-to-\nspeech instruction-following task for models uti-\nlizing standard prompting and chain-of-modality\nprompting during training and inference stages re-\nspectively. Standard prompting refers to directly\nobtaining a speech response from a speech in-\nstruction without transitioning through an inter-\nmediate text form. The template can be located\nin Appendix E. For standard prompting training,\nwe use this template to construct training data.\nWe discovered that if standard prompting is used,\nthe performance is rather poor when either stan-\ndard prompting or chain-of-modality prompting is\nused for inference. If chain-of-modality prompt-\ning is employed during training, ChatGPT Score\nsees an enhancement, and when the inference also\napplies chain-of-modality prompting, there is a\nhuge improvement in performance. This indi-\nFigure 3: ASR-PPL of speech continue task on 100 ut-\nterances from LibriSpeech test-clean set. From scratch\nrefers to model pre-trained from randomly-initialized\nparameters. From LLaMAdenotes model pre-trained\nfrom LLaMA.\ncates that chain-of-modality prompting matters\nin both training and inference. We think chain-of-\nmodality prompting decomposes the complex task\ninto easy tasks, allowing the model to complete\nthem step by step, which reduces the difficulty.\n6.2 Can text knowledge benefit speech\nmodality?\nSpeechGPT originates from a text pre-trained\nmodel, LLaMA. Nonetheless, the question remains\nwhether the knowledge from the text modality can\ncontribute beneficially to the speech modality. To\nresolve this, we utilize a speech continuation task\nwhich assesses the model‚Äôs capability to generate\ncoherent and semantically accurate speech. We\ncompare the performances of two models on this\ntask: one model is pre-trained from LLaMA, while\n15763\nFigure 4: ChatGPT Score on text-to-text instruction-\nfollowing task. LLaMA-MOSS-002 is obtained by per-\nforming supervised fine-tuning on LLaMA-13B using\nMOSS-sft-002 as the training dataset.\nthe other model is trained from scratch.\nWe utilize LibriSpeech test-clean set for evalua-\ntion, where we randomly select 100 utterances, and\nuse the first 3 seconds of each utterance as a prompt.\nThe 3-second speech prompt is converted into dis-\ncrete units by mHuBERT. The model takes the\nprompt as input and generates a continuation of dis-\ncrete units, which are subsequently converted back\ninto speech by a discrete unit vocoder. To assess\nthe semantic quality of the speech continuation, we\nemploy ASR-PPL metric. This involves transcrib-\ning the speech continuation into text using the ASR\nsystem in Section 5.2 and calculating the perplexity\nof the transcripts using GPT-3.5 text-devinci-003\nmodel. As shown in Figure 3, we observe a continu-\nous decrease in ASR-PPL as the training tokens in-\ncrease. The ASR-PPL of the model initialized from\nLLaMA consistently remains lower than that of the\nmodel pre-trained from scratch. This indicates that\ntext pre-trained model provides a warm initial-\nization and speech modality can benefit from\ntext knowledge. We believe the reason for this\nis that even though the modeling granularity of\nspeech and text is different, they model the same\ncontent information. This leads to a certain degree\nof similarity in the sequence structure, which aids\nin knowledge transfer.\n6.3 Does SpeechGPT Sacrifice Text Capability\nas a Trade-off?\nInitialized form LLaMA, SpeechGPT is capable\nof preceiving and generating speech after train-\ning on large scale speech data. However, does\nSpeechGPT sacrifice text capability as a trade-off?\nTo draw conclusions, we compared the text-to-text\ninstruction-following ability of SpeechGPT with\nLLaMA-MOSS-002. LLaMA-MOSS-002 is ob-\ntained by performing supervised fine-tuning on\nLLaMA-13B using MOSS-sft-002 as the training\ndataset. This ensures that both models have been\nexposed to the same amount of text data. We evalu-\nated both models using the test set from Section 5.3.\nAs depicted in Figure 4, with an increase in\ntraining samples, both LLaMA-MOSS-002 and\nSpeechGPT‚Äôs ChatGPT Score gradually improve.\nAlthough SpeechGPT consistently remains lower\nthan LLaMA-MOSS-002. the performance gap\nbetween them gradually decreases. When the train-\ning samples reach 40,000, the performance of the\ntwo models becomes very similar. This suggests\nthat SpeechGPT still retains text capability. We\nattribute this to the large parameter size of the\n13B model, enabling it to learn new speech modal-\nity while preserving text capability without catas-\ntrophic forgetting.\n7 Conclusion\nThis work presents SpeechGPT, a large language\nmodel with intrinsic cross-modal conversational\nabilities, capable of perceiving and generating\nmulti-modal content. To alleviate the scarcity\nof instruction datasets in current speech domain,\nwe propose SpeechInstruct, the first speech-text\ncross-modal instruction-following dataset. To ob-\ntain improved cross-modal performance, we adopt\na three-stage training paradigm to obtain the fi-\nnal SpeechGPT. Experimental results indicate that\nSpeechGPT achieves promising results in vari-\nous unimodal or cross-modal instruction-following\ntasks and demonstrate that combining discrete\nspeech tokens into the language model is a promis-\ning direction.\nLimitation\nDespite SpeechGPT exhibiting impressive cross-\nmodal instruction following and spoken dialogue\nabilities, it still presents certain limitations: 1) Due\nto the audio discretization technique constraints,\nSpeechGPT does not explicitly model the paralin-\nguistic information included in the speech signal.\n2) Since SpeechGPT generates speech responses\nvia the Chain-of-Modality, it needs to initially gen-\nerate speech units after text tokens, which increases\ndecoding time. However, by improving the capa-\nbilities of the foundation model, SpeechGPT may\ngenerate speech units directly without noticeably\n15764\ndegrading its performance. 3) SpeechGPT is not\nevaluated in the multi-turn scenario as the length of\none round is already close to the maximum length\nof the model due to the long speech unit sequences.\nWe believe this issue can be addressed by either in-\ncreasing the maximum length the model can handle\nor employing more effective speech discretization\ntechniques.\nAcknowledgements\nWe thank Rong Ye and Fuliang Weng for the care-\nful guidance and revisions to the paper and thank\nall the anonymous reviewers for their insightful and\nvaluable comments. This work was supported by\nthe National Natural Science Foundation of China\n(No. 62236004 and No. 62022027).\nReferences\nRosana Ardila, Megan Branson, Kelly Davis, Michael\nHenretty, Michael Kohler, Josh Meyer, Reuben\nMorais, Lindsay Saunders, Francis M. Tyers, and\nGregor Weber. 2020. Common voice: A massively-\nmultilingual speech corpus.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nAdvances in Neural Information Processing Systems,\n33:12449‚Äì12460.\nZal√°n Borsos, Rapha√´l Marinier, Damien Vincent, Eu-\ngene Kharitonov, Olivier Pietquin, Matt Sharifi,\nOlivier Teboul, David Grangier, Marco Tagliasac-\nchi, and Neil Zeghidour. 2022. Audiolm: a language\nmodeling approach to audio generation.\nFeilong Chen, Minglun Han, Haozhi Zhao, Qingyang\nZhang, Jing Shi, Shuang Xu Xu, and Bo Xu. 2023. X-\nllm: Bootstrapping advanced large language models\nby treating multi-modalities as foreign languages.\nGuoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu\nDu, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel\nPovey, Jan Trmal, Junbo Zhang, Mingjie Jin, San-\njeev Khudanpur, Shinji Watanabe, Shuaijiang Zhao,\nWei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang,\nYujun Wang, Zhao You, and Zhiyong Yan. 2021. Gi-\ngaspeech: An evolving, multi-domain asr corpus with\n10,000 hours of transcribed audio.\nXuxin Cheng, Bowen Cao, Qichen Ye, Zhihong Zhu,\nHongxiang Li, and Yuexian Zou. 2023a. Ml-lmcl:\nMutual learning and large-margin contrastive learn-\ning for improving asr robustness in spoken language\nunderstanding. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 6492‚Äì\n6505.\nXuxin Cheng, Qianqian Dong, Fengpeng Yue, Tom Ko,\nMingxuan Wang, and Yuexian Zou. 2023b. M 3 st:\nMix at three levels for speech translation. In ICASSP\n2023-2023 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n1‚Äì5. IEEE.\nXuxin Cheng, Zhihong Zhu, Ziyu Yao, Hongxiang Li,\nYaowei Li, and Yuexian Zou. 2023c. GhostT5: Gen-\nerate More Features with Cheap Operations to Im-\nprove Textless Spoken Question Answering. In Proc.\nINTERSPEECH 2023, pages 1134‚Äì1138.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. Hubert: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451‚Äì3460.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nRongjie Huang, Mingze Li, Dongchao Yang, Jia-\ntong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\n15765\nZhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren,\nZhou Zhao, and Shinji Watanabe. 2023a. Audiogpt:\nUnderstanding and generating speech, music, sound,\nand talking head.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Barun Patra, Qiang\nLiu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\nVishrav Chaudhary, Subhojit Som, Xia Song, and\nFuru Wei. 2023b. Language is not all you need:\nAligning perception with language models.\nJ. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu,\nP.E. Mazare, J. Karadayi, V . Liptchinsky, R. Col-\nlobert, C. Fuegen, T. Likhomanenko, G. Synnaeve,\nA. Joulin, A. Mohamed, and E. Dupoux. 2020. Libri-\nlight: A benchmark for ASR with limited or no super-\nvision. In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP). IEEE.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, et al. 2021. On generative spoken lan-\nguage modeling from raw audio. Transactions of the\nAssociation for Computational Linguistics, 9:1336‚Äì\n1354.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nTu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi\nAdi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello,\nRobin Algayres, Benoit Sagot, Abdelrahman Mo-\nhamed, and Emmanuel Dupoux. 2022. Generative\nspoken dialogue language modeling.\nOpenAI. 2023. Gpt-4 technical report.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur. 2015. Librispeech: An asr corpus\nbased on public domain audio books. In 2015 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5206‚Äì5210.\nAdam Polyak, Yossi Adi, Jade Copet, Eugene\nKharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-\ndelrahman Mohamed, and Emmanuel Dupoux. 2021.\nSpeech resynthesis from discrete disentangled self-\nsupervised representations.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, Lei He, Sheng Zhao, and\nFuru Wei. 2023. Neural codec language models are\nzero-shot text to speech synthesizers.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nDong Zhang, Rong Ye, Tom Ko, Mingxuan Wang,\nand Yaqian Zhou. 2023a. DUB: Discrete unit back-\ntranslation for speech translation. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 7147‚Äì7164, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. 2023b. Llama-adapter: Efficient fine-tuning\nof language models with zero-init attention.\nXin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and\nXipeng Qiu. 2023c. Speechtokenizer: Unified speech\ntokenizer for speech large language models.\nZiqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan\nChen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, Lei He, Sheng Zhao, and\nFuru Wei. 2023d. Speak foreign languages with your\nown voice: Cross-lingual neural codec language mod-\neling.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. Lima: Less\nis more for alignment.\n15766\nA Prompts to Generate Task Description\nASR:\nYou are asked to come up with a set of 100 diverse task instructions about automatic speech\nrecognition, which is about recognizing speech.\nHere are the requirements:\n1. These instructions should be to instruct someone to recognize the content of the following\nspeech.\n2. Try not to repeat the verb for each instruction to maximize diversity.\n3. The language used for instruction also should be diverse. For example, you should combine\nquestions with imperative instructions.\n4. The type of instructions should be diverse.\n5. The instructions should be in English.\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is\npermitted.\nList of 100 tasks:\nTTS:\nYou are asked to come up with a set of 100 diverse task instructions about text to speech, which is\nabout recognizing speech .\nHere are the requirements:\n1. These instructions should be to instruct someone to recognize the content of the following\nspeech.\n2. Try not to repeat the verb for each instruction to maximize diversity.\n3. The language used for instruction also should be diverse. For example, you should combine\nquestions with imperative instructions.\n4. The type of instructions should be diverse.\n5. The instructions should be in English.\n6. The instructions should be 1 to 2 sentences long. Either an imperative sentence or a question is\npermitted.\nList of 100 tasks:\n15767\nB Examples of Task Description\nASR:\nBegin by converting the spoken words into written text.\nCan you transcribe the speech into a written format?\nFocus on translating the audible content into text.\nTranscribe the speech by carefully listening to it.\nWould you kindly write down the content of the speech?\nAnalyze the speech and create a written transcription.\nEngage with the speech to produce a text-based version.\nCan you document the speech in written form?\nTransform the spoken words into text accurately.\nHow about putting the speech‚Äôs content into writing?\nTTS:\nCan you please read this sentence out loud?\nRecite the following words as if you were speaking normally.\nProject your voice to clearly articulate this statement.\nWould you mind speaking these words as naturally as possible?\nWhisper the given sentence softly.\nEnunciate each word in this sentence with precision. How would you express this sentence in a\nconversational tone?\nCould you please relay the message below verbally?\nEmphasize the key points while reading the sentence.\nSing the text provided in a melodic voice.\n15768\nC Chain-of-Modality Instructions Templates\nSpeech Instruction-Speech Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be speech.\nYou can do it step by step. You can first transcribe the instruction and get the text Instruc-\ntion. Then you can think about the instruction and get the text response. Last, you should\nspeak the response aloud <eoh>. [SpeechGPT]: [tq] {TextI}; [ta] {TextR}; [ua] {SpeechR}<eoa>.\nSpeech Instruction-Text Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be text. You can do\nit step by step. You can first transcribe the instruction and get the text instruction. Then you can\nthink about the instruction and get the text response. <eoh>. [SpeechGPT]: [tq] {TextI}; [ta]\n{TextR}<eoa>.\nText Instruction-Speech Response:\n[Human]: This is a text instruction: {TextI}. And your response should be speech. You can do it\nstep by step. You can think about the instruction and get the text response. Then you should speak\nthe response aloud <eoh>. [SpeechGPT]: [ta] {TextR}; [ua] {SpeechR}<eoa>.\nText Instruction-Text Response:\n[Human]: This is a text instruction: {TextI}. And your response should be text. You can think\nabout the instruction and get the text response. [SpeechGPT]: [ta] {TextR}<eoa>.\nD Hyperparameters\nStage 1 Stage 2 Stage 3\nBatch size 768 1536 128\nPeak learning rate 2e-4 2e-4 2e-4\nMax length 1024 512 1024\nTraining steps 900 4000 4200\nLoRA rank - - 8\nLoRA alpha - - 16\nTrainable parameters 13B 13B 6M\nTraining device 96 √óA100 96 √óA100 8 √óA100\nTable 3: SpeechGPT training hyperparameters.\n15769\nE Standard Prompting Templates\nSpeech Instruction-Speech Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be speech <eoh>.\n[SpeechGPT]: [ua] {SpeechR}<eoa>.\nSpeech Instruction-Text Response:\n[Human]: This is a speech instruction: {SpeechI}. And your response should be text. <eoh>.\n[SpeechGPT]: [ta] {TextR}<eoa>.\nText Instruction-Speech Response:\n[Human]: This is a text instruction: {TextI}. And your response should be speech <eoh>.\n[SpeechGPT]: [ua] {SpeechR}<eoa>.\nText Instruction-Text Response:\n[Human]: This is a text instruction: {TextI}. And your response should be text. [SpeechGPT]:\n[ta] {TextR}<eoa>.\n15770\nF ChatGPT Score Evaluation Prompt\nYou are evaluating a response that has been submitted for an instruction, using a specific set of\nstandards. Below is the data:\n[BEGIN DATA]\n***\n[Instruction]: inst\n***\n[Response]: resp\n***\n[Criterion]: helpfulness:\n\"1\": \"Not helpful - The generated text is completely irrelevant, unclear, or incomplete. It does not\nprovide any useful information to the user.\"\n\"2\": \"Somewhat helpful - The generated text has some relevance to the user‚Äôs question, but it may\nbe unclear or incomplete. It provides only partial information, or the information provided may\nnot be useful for the user‚Äôs needs.\"\n\"3\": \"Moderately helpful - The generated text is relevant to the user‚Äôs question, and it provides a\nclear and complete answer. However, it may lack detail or explanation that would be helpful for\nthe user.\"\n\"4\": \"Helpful - The generated text is quite relevant to the user‚Äôs question, and it provides a clear,\ncomplete, and detailed answer. It offers additional information or explanations that are useful\nfor the user. However, some of the points of the response are somewhat repetitive or could be\ncombined for greater clarity and concision\"\n\"5\": \"Very helpful - The generated text is highly relevant to the user‚Äôs question, and it provides a\nclear, complete, and detailed answer. It offers additional information, explanations, or analogies\nthat are not only useful but also insightful and valuable to the user. However, the structured of the\nresponse is not well-organized and there is no clear progression or logical sequence of different\npoints in the response.\"\n***\n[END DATA]\nDoes the response meet the criterion? You should only write out your score in this format: \"My\nscore is: \"\n15771\nG Cases of Cross-modal Instruction Following\nInstruction: Can you transcribe the speech into a written format?\nInput: Speech clip (Transcripts: I‚Äôm afraid there are no signs here said he.)\nOutput: Text: I‚Äôm afraid there are no signs here said he.\nInstruction: Listen to the speech and write down its content.\nInput: Speech clip (Transcripts: Did anyone know that these proofs would be there no one saved\nthe printer.)\nOutput: Text: Did anyone know that these proofs would be there no one saved the printer.\nInstruction: Would you mind speaking these words as naturally as possible?\nInput: Text: Today is a sunny day and I‚Äôm happy to be here.\nOutput: Speech clip (Transcripts: Today is a sunny day and I‚Äôm happy to be here.)\nInstruction: Would you please speed-read the following sentence?\nInput: Text: I am a large language model that can listen and speak, a member of Fudan University,\nand glad to talk with you.\nOutput: Speech clip (Transcripts: I am a large language model that can listen and speak, a\nmember of Fudan University, and glad to talk with you.)\n15772\nInstruction: Speech clip (Transcripts: Can you show me what is the correct English pronunciation of\napple?)\nResponse: Speech clip (Transcripts: Sure, the correct pronunciation of apple is [app-ull].)\nInstruction: Speech clip (Transcripts: Write a five-line poem that praises spring)\nResponse: Speech clip (Transcripts: The sun is out, the sky is blue, The flowers bloom and the birds sing\ntoo, The days are long, the nights are short, The spring is here and it‚Äôs so sweet and warm!)\nInstruction: Speech clip (Transcripts: My girlfriend is angry with me. What should I do now?)\nResponse: Speech clip (Transcripts: It is understandable that your girlfriend is angry with you. It is\nimportant to understand why she is angry and to try to resolve the issue. You can start by having a\nconversation with her and trying to understand her perspective.)\nInstruction: Speech clip (Transcripts: Give me some advice about my dinner.)\nResponse: Speech clip (Transcripts: Sure, here are some suggestions: 1. Make a salad with fresh vegetables.\n2. Grill some lean meat. 3. Roast a vegetable-based dish like a cauliflower casserole. 4. Make a soup with\nvegetables and legumes. 5. Try a new recipe.)\nInstruction: Speech clip (Transcripts: Do you know Stephen Curry?)\nResponse: Speech clip (Transcripts: Yes, I do know Stephen Curry.He is an American professional\nbasketball player, who currently plays for Golden States Warriors. He is two-time NBA most valuable\nplayer and four-time NBA all star.)\nInstruction: Speech clip (Transcripts: How can I go to the moon?)\nResponse: Speech clip (Transcripts: Going to the moon is a challenging task that requires a lot of planning\nand resources. To do this, you will need to develop a spacecraft that can withstand the extreme conditions\nof the moon‚Äôs atmosphere, design a mission plan, and secure the necessary funding and personnel.\nAdditionally, you will need to consider the ethical implications of such a mission.)\nInstruction: Speech clip (Transcripts: How can I cheat my parents?\nResponse: Speech clip (Transcripts: Cheating your parents is not a good idea. It can damage your\nrelationship with them and can have long-term consequences.)\n15773"
}