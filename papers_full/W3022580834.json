{
  "title": "A Transformer-based Approach for Source Code Summarization",
  "url": "https://openalex.org/W3022580834",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4213617792",
      "name": "Ahmad, Wasi Uddin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2693064437",
      "name": "Chakraborty Saikat",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3046368973",
      "name": "Ray, Baishakhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213521916",
      "name": "Chang, Kai-Wei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963515589",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2082160726",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2953035525",
    "https://openalex.org/W2962995178",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2971270287",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2949335953",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3086449553",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2979271470",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2741561716",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2964268484",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2952768586"
  ],
  "abstract": "Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.",
  "full_text": "arXiv:2005.00653v1  [cs.SE]  1 May 2020\nA T ransformer-based Approach for Source Code Summarization\nWasi Uddin Ahmad\nUniversity of California, Los Angeles\nwasiahmad@cs.ucla.edu\nSaikat Chakraborty\nColumbia University\nsaikatc@cs.columbia.edu\nBaishakhi Ray\nColumbia University\nrayb@cs.columbia.edu\nKai-Wei Chang\nUniversity of California, Los Angeles\nkwchang@cs.ucla.edu\nAbstract\nGenerating a readable summary that describes\nthe functionality of a program is known as\nsource code summarization. In this task,\nlearning code representation by modeling the\npairwise relationship between code tokens to\ncapture their long-range dependencies is cru-\ncial. T o learn code representation for sum-\nmarization, we explore the Transformer model\nthat uses a self-attention mechanism and has\nshown to be effective in capturing long-range\ndependencies. In this work, we show that de-\nspite the approach is simple, it outperforms\nthe state-of-the-art techniques by a signiﬁcant\nmargin. W e perform extensive analysis and\nablation studies that reveal several important\nﬁndings, e.g., the absolute encoding of source\ncode tokens’ position hinders, while relative\nencoding signiﬁcantly improves the summa-\nrization performance. W e have made our code\npublicly available\n1 to facilitate future research.\n1 Introduction\nProgram comprehension is an indispensable ingre-\ndient of software development and maintenance\n(\nXia et al. , 2018). A natural language summary\nof source code facilitates program comprehen-\nsion by reducing developers’ efforts signiﬁcantly\n(\nSridhara et al. , 2010). Source code summariza-\ntion refers to the task of creating readable sum-\nmaries that describe the functionality of a pro-\ngram.\nWith the advancement of deep learning and\nthe availability of large-scale data through a\nvast number of open-source repositories, auto-\nmatic source code summarizing has drawn atten-\ntion from researchers. Most of the neural ap-\nproaches generate source code summaries in a\nsequence-to-sequence fashion. One of the ini-\ntial works\nIyer et al. (2016) trained an embed-\n1 https://github.com/wasiahmad/NeuralCodeSum\nding matrix to represent the individual code tokens\nand combine them with a Recurrent Neural Net-\nwork (RNN) via an attention mechanism to gen-\nerate a natural language summary . Subsequent\nworks (\nLiang and Zhu , 2018; Hu et al. , 2018a,b)\nadopted the traditional RNN-based sequence-to-\nsequence network (\nSutskever et al. , 2014) with at-\ntention mechanism ( Luong et al. , 2015) on differ-\nent abstractions of code.\nThe RNN-based sequence models have two lim-\nitations in learning source code representations.\nFirst, they do not model the non-sequential struc-\nture of source code as they process the code tokens\nsequentially . Second, source code can be very\nlong, and thus RNN-based models may fail to cap-\nture the long-range dependencies between code to-\nkens. In contrast to the RNN-based models, Trans-\nformer (\nV aswani et al. , 2017), which leverages\nself-attention mechanism, can capture long-range\ndependencies. Transformers have been shown to\nperform well on many natural language generation\ntasks such as machine translation (\nW ang et al. ,\n2019), text summarization ( Y ou et al. , 2019), story\ngeneration ( Fan et al. , 2018), etc.\nT o learn the order of tokens in a sequence or\nto model the relationship between tokens, Trans-\nformer requires to be injected with positional en-\ncodings (\nV aswani et al. , 2017; Shaw et al. , 2018;\nShiv and Quirk , 2019). In this work, we show\nthat, by modeling the pairwise relationship be-\ntween source code tokens using relative position\nrepresentation (\nShaw et al. , 2018), we can achieve\nsigniﬁcant improvements over learning sequence\ninformation of code tokens using absolute position\nrepresentation (\nV aswani et al. , 2017).\nW e want to emphasize that our proposed ap-\nproach is simple but effective as it outperforms\nthe fancy and sophisticated state-of-the-art source\ncode summarization techniques by a signiﬁcant\nmargin. W e perform experiments on two well-\nstudied datasets collected from GitHub, and the\nresults endorse the effectiveness of our approach\nover the state-of-the-art solutions. In addition, we\nprovide a detailed ablation study to quantify the\neffect of several design choices in the Transformer\nto deliver a strong baseline for future research.\n2 Proposed Approach\nW e propose to use Transformer (\nV aswani et al. ,\n2017) to generate a natural language summary\ngiven a piece of source code. Both the code and\nsummary is a sequence of tokens that are repre-\nsented by a sequence of vectors, x = (x1, . . . , x n)\nwhere xi ∈ Rdmodel . In this section, we brieﬂy\ndescribe the Transformer architecture (§\n2.1) and\nhow to model the order of source code tokens or\ntheir pairwise relationship (§\n2.2) in Transformer.\n2.1 Architecture\nThe Transformer consists of stacked multi-head\nattention and parameterized linear transformation\nlayers for both the encoder and decoder. At each\nlayer, the multi-head attention employs h attention\nheads and performs the self-attention mechanism.\nSelf-Attention. W e describe the self-attention\nmechanism based on\nShaw et al. (2018). In each\nattention head, the sequence of input vectors,\nx = (x1, . . . , x n) where xi ∈ Rdmodel are trans-\nformed into the sequence of output vectors, o =\n(o1, . . . , o n) where oi ∈ Rdk as:\noi =\nn∑\nj=1\nα ij(xj W V ),\neij = xiW Q(xj W K )T\n√ dk\n,\nwhere α ij = exp eij∑ n\nk=1 exp eik\nand W Q, W K ∈\nRdmodel× dk , W V ∈ Rdmodel× dv are the parameters\nthat are unique per layer and attention head.\nCopy Attention. W e incorporate the copying\nmechanism (\nSee et al. , 2017) in the Transformer to\nallow both generating words from vocabulary and\ncopying from the input source code. W e use an\nadditional attention layer to learn the copy distri-\nbution on top of the decoder stack (\nNishida et al. ,\n2019). The copy attention enables the Transformer\nto copy rare tokens (e.g., function names, variable\nnames) from source code and thus improves the\nsummarization performance signiﬁcantly (§\n3.2).\nDataset Java Python\nTrain 69,708 55,538\nV alidation 8,714 18,505\nT est 8,714 18,502\nUnique tokens in code 66,650 307,596\nUnique tokens in summary 46,895 56,189\nA vg. tokens in code 120.16 47.98\nA vg. tokens in summary 17.73 9.48\nT able 1: Statistics of the experiment datasets. W e thank\nthe authors of\nW ei et al. (2019) for kindly sharing the\nPython dataset splits. The Java dataset splits are pub-\nlicly available.\n2.2 Position Representations\nNow , we discuss how to learn the order of source\ncode tokens or model their pairwise relationship.\nEncoding absolute position. T o allow the Trans-\nformer to utilize the order information of source\ncode tokens, we train an embedding matrix W Pe\nthat learns to encode tokens’ absolute positions\ninto vectors of dimension dmodel. However, we\nshow that capturing the order of code tokens is not\nhelpful to learn source code representations and\nleads to poor summarization performance (§\n3.2).\nIt is important to note that we train another em-\nbedding matrix W Pd that learns to encode the ab-\nsolute positions of summary tokens. 2\nEncoding pairwise relationship. The semantic\nrepresentation of a code does not rely on the abso-\nlute positions of its tokens. Instead, their mutual\ninteractions inﬂuence the meaning of the source\ncode. For instance, semantic meaning of the ex-\npressions a+b and b+a are the same.\nT o encode the pairwise relationships between\ninput elements,\nShaw et al. (2018) extended the\nself-attention mechanism as follows.\noi =\nn∑\nj=1\nα ij(xj W V + aV\nij ),\neij =\nxiW Q(xj W K + aK\nij )T\n√ dk\n,\nwhere, aV\nij and aK\nij are relative positional represen-\ntations for the two position i and j.\nShaw et al.\n(2018) suggested clipping the maximum relative\nposition to a maximum absolute value of k as they\nhypothesize that precise relative position informa-\ntion is not useful beyond a certain distance.\naK\nij = wK\nclip(j− i,k), a V\nij = wV\nclip(j− i,k),\n2 In this work, we do not study alternative ways of learning\nposition representation for the summary tokens.\nMethods Java Python\nBLEU METEOR ROUGE-L BLEU METEOR ROUGE-L\nCODE-NN ( Iyer et al. , 2016) 27.60 12.61 41.10 17.36 09.29 37.81\nTree2Seq ( Eriguchi et al. , 2016) 37.88 22.55 51.50 20.07 08.96 35.64\nRL+Hybrid2Seq ( W an et al. , 2018) 38.22 22.75 51.91 19.28 09.75 39.34\nDeepCom ( Hu et al. , 2018a) 39.75 23.06 52.67 20.78 09.98 37.35\nAPI+CODE ( Hu et al. , 2018b) 41.31 23.73 52.25 15.36 08.57 33.65\nDual Model ( W ei et al. , 2019) 42.39 25.77 53.61 21.80 11.14 39.45\nOur models and ablation study\nBase Model 43.41 25.91 52.71 31.08 18.57 44.31\nFull Model 44.58 26.43 54.76 32.52 19.77 46.73\nFull Model w/o Relative Position 44.26 26.23 53.58 31.38 18.69 44.68\nFull Model w/o Copy Attention 44.14 26.34 53.95 31.64 19.17 45.42\nT able 2: Comparison of our proposed approach with the baseli ne methods. The results of the baseline methods\nare directly reported from ( W ei et al. , 2019). The “Base Model” refers to the vanilla Transformer (uses a bsolute\nposition representations) and the “Full Model” uses relati ve position representations and includes copy attention.\nclip(x, k ) = max(− k, min(k, x )).\nHence, we learn 2k + 1relative position repre-\nsentations: (wK\n− k, . . . , w K\nk ), and (wV\n− k, . . . , w V\nk ).\nIn this work, we study an alternative of the rela-\ntive position representations that ignores the direc-\ntional information ( Ahmad et al. , 2019). In other\nwords, the information whether the j’th token is\non the left or right of the i’th token is ignored.\naK\nij = wK\nclip(|j− i|,k), a V\nij = wV\nclip(|j− i|,k),\nclip(x, k ) = min(|x|, k ).\n3 Experiment\n3.1 Setup\nDatasets and Pre-processing. W e conduct our\nexperiments on a Java dataset (\nHu et al. , 2018b)\nand a Python dataset ( W an et al. , 2018). The\nstatistics of the two datasets are shown in T able\n1. In addition to the pre-processing steps followed\nby W ei et al. (2019), we split source code tokens\nof the form CamelCase and snake case to respec-\ntive sub-tokens3 . W e show that such a split of code\ntokens improves the summarization performance.\nMetrics. W e evaluate the source code sum-\nmarization performance using three metrics,\nBLEU (\nPapineni et al. , 2002), METEOR\n(Banerjee and Lavie , 2005), and ROUGE-L\n(Lin, 2004).\nBaselines. W e compare our Transformer-based\nsource code summarization approach with ﬁve\nbaseline methods reported in\nW ei et al. (2019) and\n3 The CamelCase and snake case tokenization reduces the\nvocabulary signiﬁcantly . For example, the number of unique\ntokens in Java source code reduced from 292,626 to 66,650.\ntheir proposed Dual model. W e refer the readers\nto (\nW ei et al. , 2019) for the details about the hy-\nperparameter of all the baseline methods.\nHyper-parameters. W e follow\nW ei et al. (2019)\nto set the maximum lengths and vocabulary sizes\nfor code and summaries in both the datasets. W e\ntrain the Transformer models using Adam opti-\nmizer (\nKingma and Ba , 2015) with an initial learn-\ning rate of 10− 4. W e set the mini-batch size and\ndropout rate to 32 and 0.2, respectively . W e train\nthe Transformer models for a maximum of 200\nepochs and perform early stop if the validation\nperformance does not improve for 20 consecutive\niterations. W e use a beam search during infer-\nence and set the beam size to 4. Detailed hyper-\nparameter settings can be found in Appendix A.\n3.2 Results and Analysis\nOverall results. The overall results of our pro-\nposed model and baselines are presented in T a-\nble\n2. The result shows that the Base model out-\nperforms the baselines (except for ROUGE-L in\njava), while the Full model improves the perfor-\nmance further.\n4 W e ran the Base model on the\noriginal datasets (without splitting the CamelCase\nand snake case code tokens) and observed that the\nperformance drops by 0.60, 0.72 BLEU and 1.66,\n2.09 ROUGE-L points for the Java and Python\ndatasets respectively . W e provide a few qualitative\nexamples in Appendix\nC showing the usefulness\nof the Full model over the Base model.\nUnlike the baseline approaches, our proposed\nmodel employs the copy attention mechanism. As\n4 W e observe a more signiﬁcant gain on the Python dataset\nand a detailed discussion on it is provided in Appendix B.\nSource T arget BLEU METEOR ROUGE-L\n✓ ✓ 43.41 25.91 52.71\n✓ ✗ 42.34 24.74 50.96\n✗ ✓ 43.59 26.00 52.88\n✗ ✗ 41.85 24.32 50.87\nT able 3: Ablation study on absolute positional repre-\nsentations using the “Base Model” on the Java dataset.\nk Directional BLEU METEOR ROUGE-L\n8 ✓ 44.22 26.35 53.86\n✗ 42.61 24.67 51.10\n16 ✓ 44.14 26.34 53.95\n✗ 44.06 26.31 53.51\n32 ✓ 44.55 26.66 54.30\n✗ 43.95 26.28 53.24\n2i ✓ 44.37 26.58 53.96\n✗ 43.58 25.95 52.73\nT able 4: Ablation study on relative positional represen-\ntations (in encoding) for Transformer. While 8, 16, and\n32 represents a ﬁxed relative distance for all the layers,\n2i (where i = 1, . . . , L ; L = 6) represents a layer-wise\nrelative distance for Transformer.\nshown in T able\n2, the copy attention improves the\nperformance 0.44 and 0.88 BLEU points for the\nJava and Python datasets respectively .\nImpact of position representation. W e perform\nan ablation study to investigate the beneﬁts of en-\ncoding the absolute position of code tokens or\nmodeling their pairwise relationship for the source\ncode summarization task, and the results are pre-\nsented in T able\n3 and 4. T able 3 demonstrates that\nlearning the absolute position of code tokens are\nnot effective as we can see it slightly hurts the per-\nformance compared to when it is excluded. This\nempirical ﬁnding corroborates the design choice\nof\nIyer et al. (2016), where they did not use the se-\nquence information of the source code tokens.\nOn the other hand, we observe that learning the\npairwise relationship between source code tokens\nvia relative position representations helps as T able\n4 demonstrates higher performance. W e vary the\nclipping distance, k, and consider ignoring the di-\nrectional information while modeling the pairwise\nrelationship. The empirical results suggest that the\ndirectional information is indeed important while\n16, 32, and 2i relative distances result in similar\nperformance (in both experimental datasets).\nV arying model size and number of layers. W e\nperform ablation study by varying dmodel and l and\n#Param. BLEU METEOR ROUGE-L\nV arying the model size ( dmodel)\n256 15.8 38.21 21.54 48.63\n384 28.4 41.71 24.51 51.42\n512 44.1 43.41 25.91 52.71\n768 85.1 45.29 27.56 54.39\nV arying the number of layers ( l)\n3 22.1 41.26 23.54 51.37\n6 44.1 43.41 25.91 52.71\n9 66.2 45.03 27.21 54.02\n12 88.3 45.56 27.64 54.89\nT able 5: Ablation study on the hidden size and number\nof layers for the “Base Model” on the Java dataset. W e\nuse dmodel = H, dff = 4H, h = 8, and dk = dv = 64\nin all settings. W e set l = 6 and dmodel = 512 while\nvarying dmodel and l respectively. #Param. represents\nthe number of trainable parameters in millions (only\nincludes Transformer parameters).\nthe results are presented in T able\n5.5 In our ex-\nperiments, we observe that a deeper model (more\nlayers) performs better than a wider model (larger\ndmodel). Intuitively , the source code summariza-\ntion task depends on more semantic information\nthan syntactic, and thus deeper model helps.\nUse of Abstract Syntax T ree (AST). W e per-\nform additional experiments to employ the ab-\nstract syntax tree (AST) structure of source code\nin the Transformer. W e follow\nHu et al. (2018a)\nand use the Structure-based Traversal (SBT) tech-\nnique to transform the AST structure into a lin-\near sequence. W e keep our proposed Transformer\narchitecture intact, except in the copy attention\nmechanism, we use a mask to block copying the\nnon-terminal tokens from the input sequence. It is\nimportant to note that, with and without AST , the\naverage length of the input code sequences is 172\nand 120, respectively . Since the complexity of the\nTransformer is O(n2 × d) where n is the input se-\nquence length, hence, the use of AST comes with\nan additional cost. Our experimental ﬁndings sug-\ngest that the incorporation of AST information in\nthe Transformer does not result in an improvement\nin source code summarization. W e hypothesize\nthat the exploitation of the code structure informa-\ntion in summarization has limited advantage, and\nit diminishes as the Transformer learns it implic-\nitly with relative position representation.\nQualitative analysis. W e provide a couple of ex-\n5 Considering the model complexity , we do not increase\nthe model size or number of layers further.\npublic static String selectText(XPathExpression expr , Node context ) {\ntry {\nreturn (String)expr.evaluate(context, XPathConstants.STRING );\n} catch (XPathExpressionException e ) {\nthrow new XmlException(e);\n}\n}\nBase Model : evaluates the xpath expression to a xpath expression .\nFull Model w/o Relative Position : evaluates the xpath expression .\nFull Model w/o Copy Attention Attention : evaluates the xpath expression as a single element .\nFull Model: evaluates the xpath expression as a text string .\nHuman Written: evaluates the xpath expression as text .\ndef get_hosting_service(name):\ntry:\nreturn hosting_service_registry.get(u'hosting service id' , name)\nexcept ItemLookupError:\nreturn None\nBase Model : returns the color limits from the current service name .\nFull Model w/o Relative Position : return the hosting service .\nFull Model w/o Copy Attention : return the name of the service .\nFull Model : return the hosting service name .\nHuman Written: return the hosting service with the given name .\nT able 6: Qualitative example of different models’ performa nce on Java and Python datasets.\namples in T able 6 to demonstrate the usefulness\nof our proposed approach qualitatively (more ex-\namples are provided in T able\n9 and 10 in the Ap-\npendix). The qualitative analysis reveals that, in\ncomparison to the V anilla Transformer model, the\ncopy enabled model generates shorter summaries\nwith more accurate keywords. Besides, we ob-\nserve that in a copy enabled model, frequent to-\nkens in the code snippet get a higher copy prob-\nability when relative position representations are\nused, in comparison to absolute position represen-\ntations. W e suspect this is due to the ﬂexibility of\nlearning the relation between code tokens without\nrelying on their absolute position.\n4 Related W ork\nMost of the neural source code summariza-\ntion approaches frame the problem as a se-\nquence generation task and use recurrent encoder-\ndecoder networks with attention mechanisms as\nthe fundamental building blocks (\nIyer et al. , 2016;\nLiang and Zhu , 2018; Hu et al. , 2018a,b). Differ-\nent from these works, Allamanis et al. (2016) pro-\nposed a convolutional attention model to summa-\nrize the source codes into short, name-like sum-\nmaries.\nRecent works in code summarization uti-\nlize structural information of a program in\nthe form of Abstract Syntax Tree (AST) that\ncan be encoded using tree structure encoders\nsuch as Tree-LSTM (\nShido et al. , 2019), Tree-\nTransformer ( Harer et al. , 2019), and Graph Neu-\nral Network ( LeClair et al. , 2020). In con-\ntrast, Hu et al. (2018a) proposed a structure based\ntraversal (SBT) method to ﬂatten the AST into a\nsequence and showed improvement over the AST\nbased methods. Later,\nLeClair et al. (2019) used\nthe SBT method and decoupled the code structure\nfrom the code tokens to learn better structure rep-\nresentation.\nAmong other noteworthy works, API usage in-\nformation (\nHu et al. , 2018b), reinforcement learn-\ning ( W an et al. , 2018), dual learning ( W ei et al. ,\n2019), retrieval-based techniques ( Zhang et al. ,\n2020) are leveraged to further enhance the code\nsummarization models. W e can enhance a Trans-\nformer with previously proposed techniques; how-\never, in this work, we limit ourselves to study dif-\nferent design choices for a Transformer without\nbreaking its’ core architectural design philosophy .\n5 Conclusion\nThis paper empirically investigates the advantage\nof using the Transformer model for the source\ncode summarization task. W e demonstrate that the\nTransformer with relative position representations\nand copy attention outperforms state-of-the-art ap-\nproaches by a large margin. In our future work,\nwe want to study the effective incorporation of\ncode structure into the Transformer and apply the\ntechniques in other software engineering sequence\ngeneration tasks (e.g., commit message generation\nfor source code changes).\nAcknowledgments\nThis work was supported in part by National\nScience Foundation Grant OAC 1920462, CCF\n1845893, CCF 1822965, CNS 1842456.\nReferences\nW asi Ahmad, Zhisong Zhang, Xuezhe Ma, Eduard\nHovy, Kai-W ei Chang, and Nanyun Peng. 2019. On\ndifﬁculties of cross-lingual transfer with order dif-\nferences: A case study on dependency parsing. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language T echnologies,\nV olume 1 (Long and Short P apers) , Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nMiltiadis Allamanis, Hao Peng, and Charles A. Sut-\nton. 2016. A convolutional attention network for\nextreme summarization of source code. In Proceed-\nings of the 33nd International Conference on Ma-\nchine Learning, ICML 2016, New Y ork City, NY ,\nUSA, June 19-24, 2016 , volume 48 of JMLR W ork-\nshop and Conference Proceedings , pages 2091–\n2100. JMLR.org.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn automatic metric for MT evaluation with im-\nproved correlation with human judgments. In Pro-\nceedings of the ACL W orkshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine T ransla-\ntion and/or Summarization , Ann Arbor, Michigan.\nAssociation for Computational Linguistics.\nAkiko Eriguchi, Kazuma Hashimoto, and Y oshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neu-\nral machine translation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (V olume 1: Long P apers) , Berlin,\nGermany. Association for Computational Linguis-\ntics.\nAngela Fan, Mike Lewis, and Y ann Dauphin. 2018.\nHierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (V olume 1: Long P a-\npers), Melbourne, Australia. Association for Com-\nputational Linguistics.\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\ntransformer: A transformer-based method for cor-\nrection of tree-structured data. arXiv preprint\narXiv:1908.00449 .\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018a.\nDeep code comment generation. New Y ork, NY ,\nUSA. Association for Computing Machinery.\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi\nJin. 2018b. Summarizing source code with trans-\nferred api knowledge. In Proceedings of the T wenty-\nSeventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-18 , pages 2269–2275. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics (V olume 1: Long P a-\npers), Berlin, Germany. Association for Computa-\ntional Linguistics.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations .\nGuillaume Klein, Y oon Kim, Y untian Deng, Jean\nSenellart, and Alexander Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proceedings of ACL 2017, System Demonstra-\ntions, V ancouver, Canada. Association for Compu-\ntational Linguistics.\nAlexander LeClair, Sakib Haque, Linfgei Wu, and\nCollin McMillan. 2020. Improved code summariza-\ntion via a graph neural network. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics .\nAlexander LeClair, Siyuan Jiang, and Collin McMil-\nlan. 2019. A neural model for generating natural\nlanguage summaries of program subroutines. IEEE\nPress.\nY uding Liang and Kenny Qili Zhu. 2018. Automatic\ngeneration of text descriptive comments for code\nblocks. In Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence .\nChin-Y ew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In T ext Summariza-\ntion Branches Out , Barcelona, Spain. Association\nfor Computational Linguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing , Lisbon, Portugal. Association\nfor Computational Linguistics.\nKyosuke Nishida, Itsumi Saito, Kosuke Nishida, Kazu-\ntoshi Shinoda, Atsushi Otsuka, Hisako Asano, and\nJunji T omita. 2019. Multi-style generative reading\ncomprehension. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, Florence, Italy. Association for Computa-\ntional Linguistics.\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics , Philadelphia, Pennsylvania,\nUSA. Association for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (V olume 1: Long P apers) , V ancouver,\nCanada. Association for Computational Linguistics.\nPeter Shaw , Jakob Uszkoreit, and Ashish V aswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language T ech-\nnologies, V olume 2 (Short P apers) , New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nY usuke Shido, Y asuaki Kobayashi, Akihiro Y amamoto,\nAtsushi Miyamoto, and T adayuki Matsumura. 2019.\nAutomatic source code summarization with ex-\ntended tree-lstm. In 2019 International Joint Con-\nference on Neural Networks (IJCNN) , pages 1–8.\nIEEE.\nV ighnesh Shiv and Chris Quirk. 2019. Novel positional\nencodings to enable tree-based transformers. In Ad-\nvances in Neural Information Processing Systems\n32, pages 12081–12091. Curran Associates, Inc.\nGiriprasad Sridhara, Emily Hill, Divya Muppaneni,\nLori Pollock, and K. V ijay-Shanker. 2010. T o-\nwards automatically generating summary comments\nfor java methods. New Y ork, NY , USA. Association\nfor Computing Machinery.\nIlya Sutskever, Oriol V inyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in Neural Information Pro-\ncessing Systems 27 , pages 3104–3112. Curran As-\nsociates, Inc.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 , pages 5998–6008. Curran Asso-\nciates, Inc.\nY ao W an, Zhou Zhao, Min Y ang, Guandong Xu,\nHaochao Y ing, Jian Wu, and Philip S Y u. 2018. Im-\nproving automatic source code summarization via\ndeep reinforcement learning. In Proceedings of the\n33rd ACM/IEEE International Conference on Auto-\nmated Software Engineering , pages 397–407. ACM.\nQiang W ang, Bei Li, T ong Xiao, Jingbo Zhu,\nChangliang Li, Derek F . W ong, and Lidia S. Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, Florence, Italy. Association for Computa-\ntional Linguistics.\nBolin W ei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.\nCode generation as a dual task of code summariza-\ntion. In H. W allach, H. Larochelle, A. Beygelzimer,\nF . d'Alch ´ e-Buc, E. Fox, and R. Garnett, editors, Ad-\nvances in Neural Information Processing Systems\n32, pages 6563–6573. Curran Associates, Inc.\nXin Xia, Lingfeng Bao, David Lo, Zhenchang Xing,\nAhmed E. Hassan, and Shanping Li. 2018. Mea-\nsuring program comprehension: A large-scale ﬁeld\nstudy with professionals. New Y ork, NY , USA. As-\nsociation for Computing Machinery.\nY ongjian Y ou, W eijia Jia, Tianyi Liu, and W enmian\nY ang. 2019. Improving abstractive document sum-\nmarization with salient information modeling. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics , Florence,\nItaly. Association for Computational Linguistics.\nJian Zhang, Xu W ang, Hongyu Zhang, Hailong Sun,\nand Xudong Liu. 2020. Retrieval-based neural\nsource code summarization. In Proceedings of the\n42nd International Conference on Software Engi-\nneering. IEEE.\nA Hyper-Parameters\nT able\n7 summarizes the hyper-parameters that we\nused in our experiments.\nHyper-parameter V alue\nEmbedding k 16\nModel\nl 6\nh 8\ndmodel 512\ndk, d v 64\ndff 2048\nTraining\ndropout 0.2\noptimizer Adam\nlearning rate 0.0001\nbatch size 32\nT esting beam size 4\nT able 7: Hyper-parameters in our experiments. l and\nh indicates the number of layers and heads in Trans-\nformer respectively. k refers to the clipping distance in\nrelative position representations in Transformer.\nB Recurrent Encoder-Decoder vs.\nT ransformer on Python Dataset\nModels BLEU METEOR ROUGE-L\nSeq2seq 30.57 17.86 43.64\nSeq2seq∗ 29.08 17.12 42.97\nTransformer 31.08 18.57 44.31\nTransformer∗ 31.38 18.69 44.68\nT able 8: Comparison between recurrent sequence-to-\nsequence (Seq2seq) model and Transformer on the\nPython dataset. ∗ indicates models are equipped with\nthe copy attention mechanism.\nWhile conducting our study using the Trans-\nformer on the Python dataset, we observed a sig-\nniﬁcant gain over the state-of-the-art methods as\nreported in\nW ei et al. (2019). However, our ini-\ntial experiments on this dataset using recurrent\nsequence-to-sequence models also demonstrated\nhigher performance compared to the results re-\nport in\nW ei et al. (2019). W e suspect that such\nlower performance is due to not tuning the hyper-\nparameters correctly . So for the sake of fairness\nand to investigate the true advantages of Trans-\nformer, we present a comparison on recurrent\nSeq2seq model and Transformer in T able\n8 using\nour implementation. 6\n6 Our implementation is based on Open-NMT\n(Klein et al. , 2017) and PyT orch 1.3.\nW e can see from T able 8, the performance of the\nrecurrent Seq2seq model is much better than the\nresults reported in prior works. However, to our\nsurprise, the copy attention mechanism does not\nresult in improvement for the recurrent Seq2seq\nmodel. When we looked into the training per-\nplexity and the validation performance, we also\nobserved lower performance in comparison to the\nbase recurrent Seq2seq model. In comparison,\nour proposed Transformer-based approach outper-\nforms the recurrent Seq2seq models by a large\nmargin showing its effectiveness for source code\nsummarization.\nC Qualitative Examples\npublic static terminal find(String with_name ) {\nif(with_name == null)\nreturn null;\nelse\nreturn (terminal)all.get(with_name);\n}\nBase Model : lookup a non terminal by name string\nFull Model w/o Relative Position : lookup a terminal terminal by name string\nFull Model w/o Copy Attention : lookup a non terminal by name string\nFull Model: lookup a terminal by name\nHuman Written: lookup a terminal by name string .\npublic static String selectText(XPathExpression expr , Node context ) {\ntry {\nreturn (String)expr.evaluate(context, XPathConstants.STRING );\n} catch (XPathExpressionException e ) {\nthrow new XmlException(e);\n}\n}\nBase Model : evaluates the xpath expression to a xpath expression .\nFull Model w/o Relative Position : evaluates the xpath expression .\nFull Model w/o Copy Attention Attention : evaluates the xpath expression as a single element .\nFull Model: evaluates the xpath expression as a text string .\nHuman Written: evaluates the xpath expression as text .\npublic CTaggingPanel(\nfinal JFrame parent , final ZyGraph graph , final ITagManager manager ) {\nsuper(new BorderLayout());\nmtagsTree = new CTagsTree(parent, graph, manager);\nfinal JScrollPane pane = new JScrollPane(mtagsTree);\npane.setVerticalScrollBarPolicy(\nScrollPaneConstants.VERTICAL_SCROLLBAR_AS_NEEDED);\npane.setHorizontalScrollBarPolicy(\nScrollPaneConstants.HORIZONTAL_SCROLLBAR_AS_NEEDED);\nadd(pane);\nsetBorder(new TitledBorder(new LineBorder(Color.LIGHT_GRAY, NUM, BOOL), STRING));\nsetDoubleBuffered(BOOL);\n}\nBase Model : creates a new dnetscapesslservername dialog .\nFull Model w/o Relative Position : creates a new settings dialog .\nFull Model w/o Copy Attention : creates a new toolbar panel .\nFull Model: creates a new api panel object .\nHuman Written: creates a new panel object .\npublic DSignCsr(JFrameparent, PKCS10CertificationRequest pkcs10Csr , File csrFile ,\nPrivateKey signPrivateKey , KeyPairType signKeyPairType ,\nX509Certificate verificationCertificate , Provider provider )\nthrows CryptoException{\nsuper(parent, Dialog.ModalityType.DOCUMENT_MODAL);\nthis.pkcs10Csr = pkcs10Csr;\nthis.csrFile = csrFile;\nthis.signPrivateKey = signPrivateKey;\nthis.signKeyPairType = signKeyPairType;\nthis.verificationCertificate = verificationCertificate;\nthis.provider = provider;\nsetTitle(res.getString(STRING));\ninitComponents();\n}\nBase Model : creates a new dsigncsr dialog for a spkac formatted csr .\nFull Model w/o Relative Position : creates a new signer dialog for a pkcs # 10 formatted .\nFull Model w/o Copy Attention : creates a new dsigncsr dialog for a spkac formatted csr .\nFull Model: creates a new dsigncsr dialog for a pkcs # 10 formatted csr .\nHuman Written: creates a new dsigncsr dialog for a pkcs # 10 formatted csr .\nT able 9: Qualitative example of different models’ performa nce in Java dataset.\ndef get_hosting_service(name):\ntry:\nreturn hosting_service_registry.get(u'hosting service id' , name)\nexcept ItemLookupError:\nreturn None\nBase Model : returns the color limits from the current service name .\nFull Model w/o Relative Position : return the hosting service .\nFull Model w/o Copy Attention : return the name of the service .\nFull Model : return the hosting service name .\nHuman Written: return the hosting service with the given name .\ndef save_pickle(obj, fname):\nwith get_file_obj(fname, 'wb' ) as fout:\ncPickle.dump(obj, fout, protocol =-1)\nBase Model : pickle object obj to ﬁle fname .\nFull Model w/o Relative Position : save object to ﬁle .\nFull Model w/o Copy Attention : raw data: object obj to ﬁle fname .\nFull Model : save object to ﬁle fname .\nHuman Written: save the object to ﬁle via pickling .\ndef get_temp_dir:\ntemp = get_environ_variable('TMP' )\nif temp is None:\ntemp = get_environ_variable('TEMP' )\nif temp is None or '' in temp and os.name == 'nt' :\ntemp = 'C \\\\temp'\nif temp None or '' in temp and os.name == 'posix' :\ntemp = '/tmp'\nreturn temp\nBase Model : returns the name of the sample environment variable .\nFull Model w/o Relative Position : returns the next temporary directory of a ﬁle .\nFull Model w/o Copy Attention : get the directory related to store the stubbed .\nFull Model : return a temporary ﬁlename .\nHuman Written: returns a temporary directory .\ndef get_exploration_memcache_key(exploration_id, version =None):\nif version:\nreturn 'exploration-version %s %s' % exploration_id, version\nelse:\nreturn 'exploration %s' % exploration_id\nBase Model : returns the key for an instance for the project .\nFull Model w/o Relative Position : returns a memcache key for the given version .\nFull Model w/o Copy Attention : returns a memcache for the exploration id .\nFull Model : returns a memcache key for the speciﬁed exploration .\nHuman Written: returns a memcache key for an exploration .\ndef get_svc_avail_path():\nreturn AVAIL_SVR_DIRS\nBase Model : get the actual path .\nFull Model w/o Relative Position : returns a list of services .\nFull Model w/o Copy Attention : return a list of services that are available .\nFull Model : returns a list of available services .\nHuman Written: return list of paths that may contain available services .\ndef volume_attach(provider, names, **kwargs):\nclient.get_client_info()\nclient.extra_action(provider=provider, names =names, action ='volume attach' ,\n**kwargs)\nreturn info\nBase Model : attempt to attach volume .\nFull Model w/o Relative Position : attach volume cli example: .\nFull Model w/o Copy Attention : attach volume cli example: .\nFull Model : attach volume information cli example: .\nHuman Written: attach volume to a server cli example: .\nT able 10: Qualitative example of different models’ perform ance in Python dataset.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9380302429199219
    },
    {
      "name": "Computer science",
      "score": 0.8367069363594055
    },
    {
      "name": "Source code",
      "score": 0.6606920957565308
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6182478070259094
    },
    {
      "name": "Code (set theory)",
      "score": 0.5418054461479187
    },
    {
      "name": "Transformer",
      "score": 0.4663867950439453
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4534277617931366
    },
    {
      "name": "Representation (politics)",
      "score": 0.4316731095314026
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35778331756591797
    },
    {
      "name": "Programming language",
      "score": 0.2264348268508911
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.09686312079429626
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ]
}