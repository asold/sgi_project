{
  "title": "Tool Use Learning for a Real Robot",
  "url": "https://openalex.org/W2795246421",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2154974597",
      "name": "Handy Wicaksono",
      "affiliations": [
        "Petra Christian University",
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2081312962",
      "name": "Claude Sammut",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2154974597",
      "name": "Handy Wicaksono",
      "affiliations": [
        "Petra Christian University"
      ]
    },
    {
      "id": "https://openalex.org/A2081312962",
      "name": "Claude Sammut",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1410837107",
    "https://openalex.org/W6964514380",
    "https://openalex.org/W2085978998",
    "https://openalex.org/W6666083373",
    "https://openalex.org/W2125027611",
    "https://openalex.org/W1492570646",
    "https://openalex.org/W2101286551",
    "https://openalex.org/W6660438065",
    "https://openalex.org/W2134831918",
    "https://openalex.org/W6671349736",
    "https://openalex.org/W1574909006",
    "https://openalex.org/W2337392266",
    "https://openalex.org/W6684467955",
    "https://openalex.org/W206794860",
    "https://openalex.org/W1565236324",
    "https://openalex.org/W2520858206",
    "https://openalex.org/W2039513385",
    "https://openalex.org/W2074660036",
    "https://openalex.org/W2082711337",
    "https://openalex.org/W2167501464"
  ],
  "abstract": "&lt;p&gt;A robot may need to use a tool to solve a complex problem. Currently, tool use must be pre-programmed by a human. However, this is a difficult task and can be helped if the robot is able to learn how to use a tool by itself. Most of the work in tool use learning by a robot is done using a feature-based representation. Despite many successful results, this representation is limited in the types of tools and tasks that can be handled. Furthermore, the complex relationship between a tool and other world objects cannot be captured easily. Relational learning methods have been proposed to overcome these weaknesses [1, 2]. However, they have only been evaluated in a sensor-less simulation to avoid the complexities and uncertainties of the real world. We present a real world implementation of a relational tool use learning system for a robot. In our experiment, a robot requires around ten examples to learn to use a hook-like tool to pull a cube from a narrow tube.&lt;/p&gt;",
  "full_text": "International Journal of Electrical and Computer Engineering (IJECE)\nV ol. 8, No. 2, April 2018, pp. 1230 – 1237\nISSN: 2088-8708, DOI: 10.11591/ijece.v8i2.pp1230-1237 1230\nTool Use Learning for a Real Robot\nHandy Wicaksono1,2 and Claude Sammut1\n1School of Computer Science and Engineering, University of New South Wales; Sydney, Australia\n2Electrical Engineering Department, Petra Christian University; Surabaya, Indonesia\nArticle Info\nArticle history:\nReceived July 24, 2017\nRevised December 16, 2017\nAccepted December 27, 2017\nKeyword:\ntool use by a robot\ntool use learning\naction learning\ninductive logic programming\nrobot software architecture\nABSTRACT\nA robot may need to use a tool to solve a complex problem. Currently, tool use must be\npre-programmed by a human. However, this is a difﬁcult task and can be helped if the robot\nis able to learn how to use a tool by itself. Most of the work in tool use learning by a robot is\ndone using a feature-based representation. Despite many successful results, this representa-\ntion is limited in the types of tools and tasks that can be handled. Furthermore, the complex\nrelationship between a tool and other world objects cannot be captured easily. Relational\nlearning methods have been proposed to overcome these weaknesses [1, 2]. However, they\nhave only been evaluated in a sensor-less simulation to avoid the complexities and uncer-\ntainties of the real world. We present a real world implementation of a relational tool use\nlearning system for a robot. In our experiment, a robot requires around ten examples to learn\nto use a hook-like tool to pull a cube from a narrow tube.\nCopyright c⃝2018 Institute of Advanced Engineering and Science.\nAll rights reserved.\nCorresponding Author:\nHandy Wicaksono\nSchool of Computer Science and Engineering, University of New South Wales\nhandyw@cse.unsw.edu.au\n1. INTRODUCTION\nHumans use tools to help them complete everyday tasks. The ability to use tools is a feature of humans\nintelligence [3]. Like humans, a robot also needs to be able to use a tool to solve a complex task. As an example, in the\nRoboCup@Home competition, a robot is asked to demonstrate several tool use abilities such as opening a bottle by\nusing a bottle opener or watering a plant with a watering can [4]. The robot is given complete knowledge of the tools\nand how to use them. When such knowledge is not available, a robot must learn it. Most work in tool use learning has\nused a feature-based representation that is dependent on an object’s primitive features. Thus, it is not ﬂexible enough\nto be applied to different tools and environments. Learning is also limited to tool selection only. Brown proposed a\nrelational approach which can overcome these limitations. Furthermore, Wicaksono & Sammut ([5]) have suggested\nthat this representation has potential to solve a more complex problem, such as tool creation.\nWe deﬁne a tool as an object that is deliberately employed by an agent to help it achieve a goal, which would\nbe too difﬁcult to achieve without the tool. We want to learn a tool action model that explains changes in the properties\nof one or more objects affected by the tool, given that certain preconditions are met. Following Brown [1], learning\nis performed by trial and error in an Inductive Logic Programming (ILP) setting [6]. Brown [1] carried out tool use\nlearning in a sensor-less simulation. This means that a robot has perfect knowledge of the world and uncertainties in\nthe sensors’ readings are eliminated. In this paper, we would present a complete robotic system for tool use learning\nfollowing a relational learning approach. This includes three components:\n1. Developing a robot software architecture that consists of primitive and abstract layers and facilitates communi-\ncation between them.\n2. Creating a mechanism to detect objects and generating primitive behaviors for tool use.\n3. Extending relational learning methods and conducting tool use learning experiments using a real robot.\nIn the following sections, we describe relevant previous work, the knowledge representation formalism used,\nthe software architecture, and the tool use learning mechanism. Finally, we perform several real world experiments\nand draw conclusions.\nJournal Homepage:http://iaesjournal.com/online/index.php/IJECE\nInt J Elec & Comp Eng ISSN: 2088-8708 1231\n2. RELATED WORK\nAn intelligent robot is identiﬁed by its ability to make a decision and learn autonomously in an unstructured\nenvironment. Machine learning techniques could be useful as they can compensate for the imperfect model of a robot\nin a real world. For example, control of a robot manipulator can be improved by equipping a PD torque controller\nwith Adaptive Neuro-Fuzzy Inference System [7] or combining sliding mode control with genetic algorithm [8]. In a\nbehavior-based robotics approach, reinforcement learning can be used to learn the robot’s behavior [9].\nHere we describe the previous work in tool use learning by a robot. Wood et al. [10] use a neural network to\nlearn appropriate posture of a Sony Aibo robot so it can reach an object by using a tool placed on its back. Stoytchev\n[11] learns to select a correct tool via its affordances which are grounded in robot behaviors. However, its result can\nnot be generalized to other new tools. More recent work by Tikhanoff et al. [12]) combine exploratory behaviors and a\ngeometrical feature extraction to learn affordances and tool use. Mar et al. [13] extend their work by learning a grasp\nconﬁguration which inﬂuences the outcome of a tool use action.\nThe limitations of feature-based representations were mentioned above. Only recently have such representa-\ntions been extended so that a robot can learn to grasp a tool [13]. An ILP system can overcome these limitations, as\nit represents the tools and other objects in a relational representation [1, 2]. However, previous work has been done\npreviously in sensor-less simulation only. This means the complexities of acquiring perceptions, generating precise\nmovements, and dealing with world uncertainties, are avoided. We aim to develop a complete robotic system that\nfacilitates this learning in a real world.\n3. REPRESENTING STATES AND ACTIONS\nWe maintain two representations of states and actions in primitive and abstract form. Primitive states are the\npositions of all objects in the world that are captured by vision sensors using a mechanism described in section 4.2..\nAs we only use simple objects, they can be detected by their two-dimensional appearance only.\nAbstract states are represented as expressions in ﬁrst-order logic, to be more speciﬁc as Horn clauses. Primi-\ntive states are translated to an abstract state by calling relevant Prolog programs with the primitive states as their bound\nvalues. To be classiﬁed as a tool, an object must possess particular structural properties (e.g. a particular side where\na hook is attached to the handle) and spatial properties (e.g. a hook is touching the back side of a cube). We collect\nthese properties in a hypothesis, namely tool pose, which is shown below in simpliﬁed form.\nt o o l p o s e ( Handle , Hook , Box , Tube ): −\na t t a c h e de n d ( Handle , Hook , back ) , % a s t r u c t u r a l p r o p e r t y\na t t a c h e d s i d e ( Handle , Hook , S i d e ) , % a s t r u c t u r a l p r o p e r t y\nt o u c h i n g ( Hook , Box , back ) . % a s p a t i a l p r o p e r t y\nWe use an extended STRIPS action model [14] to describe an abstract action and how it affects the world. A\nprimitive goal that has to be achieved by a robot is added at the end of the model.\nAction name\nPRE : states that must be true so that an action can be performed\nADD : conditions that become true as a result of the action\nDELETE : conditions that are no longer true following the action\nCONSTRAINTS : the physical limits of actuators that constrain the action\nThis action model is also a planning operator. Thus, action learning does not happen in isolation, as it is a part of a\nproblem solving scenario. Every abstract action is linked to a set of primitive behaviors. This will be discussed later\nin section 4.3.\n4. ROBOTIC SYSTEM\nIn this section, we explain our software architecture, objects detection method, and behavior generation\nmechanism.\n4.1. Software Architecture\nWe use a relational representation in the high-level layer to take advantage of the expressiveness of ﬁrst-order\nlogic (FOL) clauses, especially Horn clauses, and equip a robot with useful symbolic techniques. We implement this\nlayer in SWI Prolog. Most modern Prolog implementations, including SWI, incorporate a constraint solver which\nprovide advanced numerical reasoning capabilities.\nTool Use Learning for a Real Robot(Handy Wicaksono)\n1232 ISSN: 2088-8708\nIn the low-level layer, a robot operates in a world of continuous, noisy and uncertain measurements. Its\nsensors return readings in numeric ranges, and its actuators operate based on numerical set points. As we aim to use\nRobot Operating System (ROS1) as our middleware, we implement this layer in Python. The Baxter robot, from Re-\nthink Robotics, has its own ROS-Python API, a set of classes that provides wrappers around the ROS communication.\nAnother layer, namely the translator, is needed to map primitive to abstract states and to link an abstract\naction to corresponding primitive behaviors. It is written in C++ which has an interface to SWI Prolog as the language\nfor the abstract layer. Communication to the primitive layer is done via ROS using a simple publish and subscribe\nmechanism. Our robot software architecture is shown in Fig. 1a.\nWe give a simple illustration of action execution, namely find tool, which involves communication be-\ntween layers. In the primitive layer, a cube and a tube are detected, their corners are acquired and published to-\ngether by a ROS node, namely /translator, as a ROS topic, /pri states. The translator also has a ROS\nnode, /translator, that subscribes to that topic. Then it evaluates the status of the abstract state, in tube\n(Cube,Tube), by calling the relevant Prolog predicate. In the abstract layer, in tube(Cube,Tube) is called.\nIf it is true, as it is the only the precondition of the find tool action, then the translator will publish it as the ROS\ntopic, /abs action that represents the currently active action. The primitive layer subscribes to this topic. If it\nrecognizes find tool as an active action, then it triggers the corresponding primitive behaviors. See Fig. 1b for\ndetail.\nActions translator\nAbstract Action 1 Constraint\nSolver /\nPose Selector\nAbstract layer\nPerception\nObject \nDetection\nBehaviors Generation\nPID Controller\nILP Learner\nStates\ntranslator\nLogical\nState\nPlanner\nAbstract Action n\nTranslator\nPrimitive layer\nTool Selector\nTool\nidentity\nAbstract \nactions\nTool \npose\nAbstract \nstates\nPrimitive\nstates\nPixels\nPrimitive action +\nparameters\nMotors \ncommands\nPrimitive\nstates\nReal World\n(a) Software architecture\nobjects detection\nROS node: /translator\ncube(12,6,15,9) \ntube(0,0,15,20)\nROS topic: /pri_states\ncall Prolog predicate:\nin_tube(Cube,Tube)\nPRE:\nin_tube(Cube,Tube)\nSTRIPS action model\nNAME:\nfind_tool\nEFF:\ntool_found(Tool)\nROS node: /primitive\nROS topic: /abs_action\npublish \"find_tool\" \nabstract action\nbehaviors generation\nAbstract layer\nTranslator\nPrimitive layer\ngo to tool area,\nfind correct tool (b) Simpliﬁed example\nFigure 1. Robot software architecture and its example\n4.2. Object Detection\nWe detect all objects (i.e. different tools, a cube, and a tube) by their two-dimensional appearances only.\nWe use the Baxter camera to detect objects locally, while an external web camera provides the global world state.\nWe combine local and global images from both cameras to achieve our goal. The OpenCV library is used for object\ndetection.\nIn pre-processing, Gaussian smoothing is applied. Later, the edges of objects in an image are detected with\na Canny Edge detector, and their contours are found. Each contour is tested to check whether it has the properties of\na particular object. The contour properties include the number of edges, the area, the angle formed by two adjacent\nparts, the convexity, and the perimeter. Tools are treated specially as they have more than one shape. After being\ndetected as a tool, the object’s type is determined, and its handle and hook(s) are acquired. All objects are represented\n1http://www.ros.org\nInt J Elec & Comp Eng V ol. 8, No. 2, April 2018: 1230 – 1237\nInt J Elec & Comp Eng ISSN: 2088-8708 1233\nby their minimum and maximum corner points in Cartesian coordinates ( Xmin, Ymin, Xmax, Ymax ). The complete\nmechanism is shown in Algorithm 1.\nAlgorithm 1Objects detection\n1: Capture an image\n2: Perform Gaussian smoothing\n3: Detect the edges of objects using Canny detector\n4: Find the contours of the objects\n5: for each contour do\n6: Classify the contour\n7: if The contour is either cube, tube, or tool then\n8: Find its minimum & maximum corner points\n9: return all minimum & maximum corner points\n4.3. Behavior Generation\nIn our tool use application, a sequence of actions, or a plan, must be performed to achieve the goal, pulling\na cube from a narrow tube. Each action uses different strategies (inverse kinematics solving, visual servoing, gripper\nactivation, constraints solving) depending on whether the primitive goal is known or not, and whether the visual\ninformation is needed or not. We show an example of the plan in Table 1.\nTable 1. The actions sequence for pulling a cube from a narrow tube\nNo Action Technique Goal Visual\n1 Find the tool Inverse Kinematics (IK) solving, visual servoing Known Yes\n2 Grip the tool Close the gripper Known No\n3 Position the tool Constraints solving, IK solving Unknown Yes\n4 Pull the cube IK solving Known No\n5 Move the tool away IK solving Known No\n6 Ungrip the tool Open the gripper Known No\nAn abstract action is directly linked to corresponding primitive actions. For simplicity, we only consider\nmovement in two-dimensional Cartesian coordinates. A primitive goal is a target location in that coordinate system.\nWhen a goal is known, we use an inverse kinematics (IK) solver to compute the angles of all joints and move the arm\ntowards that goal. However, if it is unknown, we use a constraint solver to create a numerical goal from abstract states\nwhich are the preconditions of an abstract action model (see Fig. 2a). To perform an accurate tool picking action,\nwe use a simple version of image-based visual servoing [15], where an error signal is determined directly in terms\nof image feature parameters. We locate the robot arm in a position where a chosen tool is located in the center of an\nimage captured by Baxter’s wrist camera, so the arm can move downward perpendicularly to take the tool. See Fig.\n2b for the detail.\nAction 1\nAction 2\nAction n\nPlan Action Model\nConstraint Solver\nIK Solver\nController\nAbstract goal\nPrimitive goal\nGoal in joints movement\nMotor commands\n(a) Constraints solving to get a primitive goal\nDetect a chosen tool in the \nimage (see Alg. 1)\nIs the tool on \nthe center of image?\nMove arm towards center Go down to pick the tool\nYes\nNo\nStart\nFinish (b) A visual servoing process\nFigure 2. behaviors generation\nTool Use Learning for a Real Robot(Handy Wicaksono)\n1234 ISSN: 2088-8708\n5. TOOL USE LEARNING\nIn this section, we describe relational tool use learning [1, 2]. The robot must learn a tool action model\nthat describes the properties of a tool and other objects in the world that enable a robot to use a tool successfully.\nSpeciﬁcally, the robot learns a tool pose predicate, which describes the structural and spatial conditions that must\nbe met for the tool to be used. We maintain a version space of hypotheses, following Mitchell [16]. A version space is\nbounded by the most general hypothesis (hg) and the most speciﬁc hypothesis (hs). As we explain below, the version\nspace is used to determine what experiments the robot should perform to generalize or specialize the tool action model.\nInitially, a robot may not have a complete action model for a tool, so it cannot construct a plan. After a tutor\ngives a tool use example, the robot segments the observed behavior into discrete actions, matches them to actions\nalready in its background knowledge, and constructs a new action if there is no match for a particular behavior. This\nconstruction may not be sufﬁcient to describe the tool action in a general enough form that it can be applied in different\nsituations. Therefore, trial and error learning is performed to reﬁne the initial action model.\nTrial and error involved performing experiments in which a different tool is selected or its pose is changed to\ntest the current hypothesis for the tool action. In tool selection, an object that has properties that are most similar to a\npreviously useful tool is chosen. More speciﬁcally, we test whether an object satisﬁes all primary structural properties,\nstored in hg, and most of the secondary ones, stored in hs, given its primitive states.\nHaving a correct tool is useless when it is located in an incorrect position before it trying to pull the target\nobject. We select the pose by solving the constraints of the spatial literals in the tool pose predicate. We check whether\nthe spatial constraints of an object can be solved or not. The unsolved constraints are ignored, and the ﬁnal result is\nused as the numerical goal for the robot.\nAfter a tool use learning experiment is performed, its result, whether successful or not, is passed to the ILP\nlearner so the hypothesis can be reﬁned. Our learning is derived from Golem [17], an ILP algorithm. Reﬁnement ofhs\nis done by ﬁnding the constrained Least General Generalization (LGG) of a positive examples pair, and hg is reﬁned\nby performing the negative-based reduction. The learning algorithm is a modiﬁcation of Haber [2] and is shown in\nAlgorithm 2.\nAlgorithm 2Tool use learning\n1: input: new action model M, hg = true, hs = preconditions of M,N trials, K consecutive success\n2: while success < Kor index < Ndo\n3: e = generate experiment(hs, hg)\n4: tool = select tool(hs, hg) // select a tool with highest rank\n5: for each ei in e do\n6: pose = select pose(ei) // performing constraint solving on preconditions of the relevant action model\n7: if pose = null then\n8: break\n9: success = execute exp(tool, pose) // performing a tool use experiment\n10: if success then\n11: label pose positive\n12: increment cons success\n13: hs = generalise hs // via Least General Generalisation\n14: else\n15: label pose negative\n16: cons success ←0\n17: hg = specialise hg // via negative based reduction\n18: add ei to training data\n19: increment index\n6. RESULTS AND DISCUSSIONS\nThe Baxter research robot is used in this experiment (see Fig. 3a). The objects in the scene include a cube, a\ntube, and ﬁve objects of a different shape that potential tools. We also have another set of tool candidates whose hooks\nare narrower than their handles (see Fig. 3b). We use Baxter’s wrist camera for visual servoing. We add an external\nweb camera facing the tube to provide the global state. We divide the experiments into two stages. Initially, we\nevaluate the performance of our object detection algorithm and action execution. Later, we conduct tool use learning\nexperiments.\nInt J Elec & Comp Eng V ol. 8, No. 2, April 2018: 1230 – 1237\nInt J Elec & Comp Eng ISSN: 2088-8708 1235\nweb camera\nBaxter hand's \ncamera\n5 tools\ncube\ntube\n(a) Scene details\n (b) All set of tools\nFigure 3. Experimental scene\n6.1. Object Detection and Execution Experiments\nThe contours of all objects (cube, tube, and tools) are detected, using Baxter’s wrist camera, and their min-\nimum and maximum points are acquired. These are marked with colored dots in the object corners in Fig. 4. Even\nwhen the tool is held by the gripper, our algorithm is still able to detect the object. However, the vision system may\nfail in a low-light environment. It also assumes that all objects are aligned perpendicular to each other and the camera.\nbaxter_cam: cube detection web_cam: cube detection baxter_cam: tube detection web_cam: tube detection\nbaxter_cam: tools detection web_cam: tools detection 1 web_cam: tools detection 2 web_cam: selected tool detection\nFigure 4. Objects detection in images captured by the Baxter hand camera and the web camera\nWe also evaluate the capability of the Baxter to move accurately to accomplish a tool use task based on the\nactions sequence in Table 1. In Fig. 5 we illustrate these actions: find tool action which uses visual servoing,\ngrip tool action, which activates the robot’s gripper, andposition tool actions, which tries to satisfy a prim-\nitive goal given by a constraint solver. From these trials, it was determined that the Baxter can move smoothly to\ncomplete the task. We observe that most errors are caused by the perception system, not the action mechanism.\nfind_tool grip_tool position_tool\nFigure 5. Sample of the robot’s actions\n6.2. Learning Experiments\nIn this experiment, we use ﬁve different candidate tools with narrow and wide hooks. The cube, as a target\nobject, is located at a particular position inside the tube, which is changed after the Baxter can withdraw it from the\ntube successfully. We adopt an initial action model which was created by Brown ([1]). Learning by trial and error is\nthen performed to reﬁne the tool pose hypothesis. We stop learning if the robot accomplishes the task three times\nconsecutively. The complete learning sequence is shown in Fig. 6.\nWe need at least ten examples to learn to perform this task. However, more experiments may be needed if\nany errors occur (e.g. a change in lighting or an invalid move produced by the IK solver). The ﬁrst positive example\nis given by a teacher. In the second example, the robot attempts to imitate the tool and location of the ﬁrst one, but\nit fails as the cube location is changed. In the third example, the robot ﬁnds that the attachment side of the hook\nshould be the same as the location of the cube inside the tube. Later on, the robot still makes mistakes as it tends to\nchoose a tool with a narrow hook. This narrower property is eliminated from hs in the generalization process in\nTool Use Learning for a Real Robot(Handy Wicaksono)\n1236 ISSN: 2088-8708\nexample 2: - example 3: + example 4: - example 5: -example 1: +\nexample 7: - example 8: + example 9: + example 10: +example 6: +\nFigure 6. Tool use learning experiments in a real world\nthe eighth example. Finally, learning is completed in the tenth example, as it performs three consecutive successful\nexperiments. The ﬁnal hypothesis is shorter ( hs is reduced from 14 literals to 11 literals) and more general than the\ninitial hypothesis.\n6.3. Discussions\nOur result is similar to the previous relational learning approach [1], they learn tool-use in 12 experiments,\nas we use the relatively same mechanism. However, we perform our experiment on a real robot, while the former only\ndo it on a sensor-less simulation. This includes bridging the gap between low-level layer (in ROS environment) and\nhigh-level layer (in SWI Prolog). We also build objects detection system that has to deal with the noisy environment.\nPrevious work did not do any detections, as it acquires perfect data from a simulator.\nCompared to the line of work in feature-based representation, such as work of Tikhanoff et al. [12], our\napproach can learn faster (theirs needs at least 120 trials in various learning stages) as we can easily incorporate\nhuman expertise in the background knowledge. Our experiment scenario is also more complicated, as we locate the\ntarget object inside a tube. We exploit the ability of the relational representation to represent a complex relationship\nbetween objects compactly. We also learn the tool-pose, where the tool should be located to be able to pull the tool\nsuccessfully, while it is predeﬁned in their work.\nThere are limitations in our work, especially in the perception system which can only handle 2D images and\nnot robust in changing environments. In this area, previous work [12, 13] is better than ours. Despite these limitations,\nthe experiment demonstrates that the learning system is capable of relational tool use learning in the real world. In\nother work of us, we also use a physics simulator to assist a real robot performs tool use learning [18].\n7. CONCLUSIONS AND FUTURE WORK\nWe have developed a complete robot system for relational tool use learning in the real world. The primitive,\ntranslator and abstract layers have been built, along with with their interfaces. We have also implemented a system\nto detect objects and generate primitive behaviors by inverse kinematics, a constraint solver and a visual servoing\nmechanism. Finally, we have extended a tool use learning system, which has been tested in experiments in the real\nworld. For a relatively simple tool use scenario, the robot at needs at least ten examples to learn the tool’s properties.\nIn the future, we want our robot to do tool creation when the available tools cannot be used to solve the\ncurrent problem. Those tools can then be tested in the simulator, to save time and materials, and manufactured by\nusing a 3D printer. The development of a more robust perception and a more accurate movement systems will improve\nour system performance. The system will be tested on a wider variety of tool use scenarios, with different tools and\ndifferent tasks.\nACKNOWLEDGEMENT\nHandy Wicaksono is supported by The Directorate General of Resources for Science, Technology and Higher\nEducation (DG-RSTHE), Ministry of Research, Technology, and Higher Education of the Republic of Indonesia.\nREFERENCES\n[1] S. Brown and C. Sammut, “A relational approach to tool-use learning in robots,” inInductive Logic Programming.\nSpringer, 2012, pp. 1–15.\n[2] A. Haber, “A system architecture for learning robots,” Ph.D. dissertation, School of Computer Science and\nEngineering, UNSW Australia, 2015.\nInt J Elec & Comp Eng V ol. 8, No. 2, April 2018: 1230 – 1237\nInt J Elec & Comp Eng ISSN: 2088-8708 1237\n[3] S. H. Johnson-Frey, “What’s so special about human tool use?” Neuron, vol. 39, no. 2, pp. 201 – 204, 2003.\n[Online]. Available: http://www.sciencedirect.com/science/article/pii/S0896627303004240\n[4] J. Stckler and S. Behnke, “Adaptive tool-use strategies for anthropomorphic service robots,” in 2014 IEEE-RAS\nInternational Conference on Humanoid Robots, Nov 2014, pp. 755–760.\n[5] H. Wicaksono and C. Sammut, “A learning framework for tool creation by a robot,” in Proceedings of ACRA,\n2015.\n[6] S. Muggleton, “Inductive logic programming,” New Generation Computing, vol. 8, no. 4, pp. 295–318, 1991.\n[7] O. Bachir and A.-f. Zoubir, “Adaptive neuro-fuzzy inference system based control of puma 600 robot manipula-\ntor,”International Journal of Electrical and Computer Engineering, vol. 2, no. 1, p. 90, 2012.\n[8] A. R. Firdaus and A. S. Rahman, “Genetic algorithm of sliding mode control design for manipulator robot,”\nTELKOMNIKA (Telecommunication Computing Electronics and Control), vol. 10, no. 4, pp. 645–654, 2012.\n[9] H. Wicaksono, H. Khoswanto, and S. Kuswadi, “Behaviors coordination and learning on autonomous navigation\nof physical robot,” TELKOMNIKA (Telecommunication Computing Electronics and Control), vol. 9, no. 3, pp.\n473–482, 2013.\n[10] A. Wood, T. Horton, and R. Amant, “Effective tool use in a habile agent,” inSystems and Information Engineer-\ning Design Symposium, 2005 IEEE, April 2005, pp. 75–81.\n[11] A. Stoytchev, “Behavior-grounded representation of tool affordances,” inProceedings of the 2005 IEEE Interna-\ntional Conference on Robotics and Automation, April 2005, pp. 3060–3065.\n[12] V . Tikhanoff, U. Pattacini, L. Natale, and G. Metta, “Exploring affordances and tool use on the icub,” in 2013\n13th IEEE-RAS International Conference on Humanoid Robots (Humanoids), Oct 2013, pp. 130–137.\n[13] T. Mar, V . Tikhanoff, G. Metta, and L. Natale, “Self-supervised learning of grasp dependent tool affordances on\nthe icub humanoid robot,” in 2015 IEEE International Conference on Robotics and Automation (ICRA), May\n2015, pp. 3200–3206.\n[14] R. Fikes and N. J. Nilsson, “STRIPS: A new approach to the application of theorem proving to problem solving,”\nArtif. Intell., vol. 2, no. 3/4, pp. 189–208, 1971.\n[15] S. Hutchinson, G. D. Hager, and P. I. Corke, “A tutorial on visual servo control,” IEEE transactions on robotics\nand automation, vol. 12, no. 5, pp. 651–670, 1996.\n[16] T. M. Mitchell, “Version spaces: A candidate elimination approach to rule learning,” in Proceedings of the 5th\ninternational joint conference on Artiﬁcial intelligence-Volume 1. Morgan Kaufmann Publishers Inc., 1977, pp.\n305–310.\n[17] S. Muggleton and C. Feng, “Efﬁcient induction in logic programs,” in Inductive Logic Programming, S. Mug-\ngleton, Ed. Academic Press, 1992, pp. 281–298.\n[18] H. Wicaksono and C. Sammut, “Relational tool use learning by a robot in a real and simulated world,” in Pro-\nceedings of ACRA, 2016.\nBIOGRAPHY OF AUTHORS\nHandy Wicaksonois a Ph.D. student at Artiﬁcial Intelligence Group, School of Computer Science\nand Engineering, UNSW Australia, with bachelor and master degree in Electrical Engineering from\nInstitut Teknologi Sepuluh Nopember, Indonesia. He is also a lecturer in Electrical Engineering De-\npartment, Petra Christian University, Indonesia. His research is in the area of artiﬁcial intelligence,\nintelligent robot, and industrial automation.\nClaude Sammut is a Professor of Computer Science and Engineering at the University of New\nSouth Wales, Head of the Artiﬁcial Intelligence Research Group and Deputy Director of the iCin-\nema Centre for Interactive Cinema Research. Previously, he was a program manager for the Smart\nInternet Technology Cooperative Research Centre, the UNSW node Director of the ARC Centre\nof Excellence for Autonomous Systems and a member of the joint ARC/NH&MRC project on\nThinking Systems. His early work on relational learning helped to the lay the foundations for the\nﬁeld of Inductive Logic Programming (ILP). With Donald Michie, he also did pioneering work in\nBehavioral Cloning. His current interests include Conversational Agents and Robotics.\nTool Use Learning for a Real Robot(Handy Wicaksono)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7315528988838196
    },
    {
      "name": "Robot",
      "score": 0.7154307961463928
    },
    {
      "name": "Task (project management)",
      "score": 0.6393022537231445
    },
    {
      "name": "Representation (politics)",
      "score": 0.5964056253433228
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5730277299880981
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5336720943450928
    },
    {
      "name": "Robot learning",
      "score": 0.4544113874435425
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.42401832342147827
    },
    {
      "name": "Machine learning",
      "score": 0.32931336760520935
    },
    {
      "name": "Mobile robot",
      "score": 0.22052812576293945
    },
    {
      "name": "Engineering",
      "score": 0.08193883299827576
    },
    {
      "name": "Systems engineering",
      "score": 0.06974992156028748
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I114202661",
      "name": "Petra Christian University",
      "country": "ID"
    }
  ],
  "cited_by": 4
}