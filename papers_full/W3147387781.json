{
    "title": "ViViT: A Video Vision Transformer",
    "url": "https://openalex.org/W3147387781",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2173162721",
            "name": "Anurag Arnab",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2112331270",
            "name": "Mostafa Dehghani",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2407094528",
            "name": "Georg Heigold",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2102541890",
            "name": "Chen Sun",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2147033837",
            "name": "Mario Lucic",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2111851554",
            "name": "Cordelia Schmid",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6779163297",
        "https://openalex.org/W6724944384",
        "https://openalex.org/W6785727093",
        "https://openalex.org/W6786708909",
        "https://openalex.org/W6763239785",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W6768021236",
        "https://openalex.org/W3109304426",
        "https://openalex.org/W4288083516",
        "https://openalex.org/W2955874753",
        "https://openalex.org/W3035104321",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W2981385151",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W6701947533",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W6771626834",
        "https://openalex.org/W2971680695",
        "https://openalex.org/W6955071965",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W6783267081",
        "https://openalex.org/W6785783668",
        "https://openalex.org/W6788135285",
        "https://openalex.org/W6772853553",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W6755977528",
        "https://openalex.org/W2984287396",
        "https://openalex.org/W2963155035",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2068611653",
        "https://openalex.org/W3035619757",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W4214482673",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W6729814214",
        "https://openalex.org/W6631943919",
        "https://openalex.org/W6743837088",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2625366777",
        "https://openalex.org/W2948048211",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2964191259",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W6682864246",
        "https://openalex.org/W6784492024",
        "https://openalex.org/W6762876521",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W6761628794",
        "https://openalex.org/W6783944145",
        "https://openalex.org/W2020163092",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W6780677595",
        "https://openalex.org/W6753640285",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3034345703",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6745136726",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6788620109",
        "https://openalex.org/W6770390784",
        "https://openalex.org/W6788305448",
        "https://openalex.org/W3034572008",
        "https://openalex.org/W6752764193",
        "https://openalex.org/W6790307280",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6746798562",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W6766470091",
        "https://openalex.org/W6640257725",
        "https://openalex.org/W3210279979",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6791821072",
        "https://openalex.org/W2990152177",
        "https://openalex.org/W3035303837",
        "https://openalex.org/W2962711930",
        "https://openalex.org/W6751037545",
        "https://openalex.org/W3120857301",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3108995912",
        "https://openalex.org/W3125056032",
        "https://openalex.org/W2783139164",
        "https://openalex.org/W2950870964",
        "https://openalex.org/W3040304705",
        "https://openalex.org/W2553594924",
        "https://openalex.org/W2963399829",
        "https://openalex.org/W2966019198",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W3203247536",
        "https://openalex.org/W2989728968",
        "https://openalex.org/W3092354667",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W2994760783",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2507009361",
        "https://openalex.org/W2994673210",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2995684093",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3037916678",
        "https://openalex.org/W2964233791",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3102631365",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2899505139",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2883429621",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2156303437",
        "https://openalex.org/W2964091144",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W1923404803",
        "https://openalex.org/W3103682594",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2970389371",
        "https://openalex.org/W3107036272",
        "https://openalex.org/W3126337037",
        "https://openalex.org/W2941956444",
        "https://openalex.org/W3138796575",
        "https://openalex.org/W3209243117",
        "https://openalex.org/W2950739196",
        "https://openalex.org/W3112996878"
    ],
    "abstract": "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit",
    "full_text": "ViViT: A Video Vision Transformer\nAnurag Arnab* Mostafa Dehghani* Georg Heigold Chen Sun Mario Lu ˇci´c† Cordelia Schmid†\nGoogle Research\n{aarnab, dehghani, heigold, chensun, lucic, cordelias}@google.com\nAbstract\nWe present pure-transformer based models for video\nclassiﬁcation, drawing upon the recent success of such mod-\nels in image classiﬁcation. Our model extracts spatio-\ntemporal tokens from the input video, which are then en-\ncoded by a series of transformer layers. In order to han-\ndle the long sequences of tokens encountered in video, we\npropose several, efﬁcient variants of our model which fac-\ntorise the spatial- and temporal-dimensions of the input. Al-\nthough transformer-based models are known to only be ef-\nfective when large training datasets are available, we show\nhow we can effectively regularise the model during training\nand leverage pretrained image models to be able to train on\ncomparatively small datasets. We conduct thorough abla-\ntion studies, and achieve state-of-the-art results on multiple\nvideo classiﬁcation benchmarks including Kinetics 400 and\n600, Epic Kitchens, Something-Something v2 and Moments\nin Time, outperforming prior methods based on deep 3D\nconvolutional networks. To facilitate further research, we\nrelease code at https://github.com/google-research/scenic.\n1. Introduction\nApproaches based on deep convolutional neural net-\nworks have advanced the state-of-the-art across many stan-\ndard datasets for vision problems since AlexNet [38]. At\nthe same time, the most prominent architecture of choice in\nsequence-to-sequence modelling (e.g. in natural language\nprocessing) is the transformer [68], which does not use con-\nvolutions, but is based on multi-headed self-attention. This\noperation is particularly effective at modelling long-range\ndependencies and allows the model to attend over all ele-\nments in the input sequence. This is in stark contrast to\nconvolutions where the corresponding “receptive ﬁeld” is\nlimited, and grows linearly with the depth of the network.\nThe success of attention-based models in NLP has re-\ncently inspired approaches in computer vision to integrate\ntransformers into CNNs [75, 7], as well as some attempts to\nreplace convolutions completely [49, 3, 53]. However, it is\n*Equal contribution\n†Equal advising\nonly very recently with the Vision Transformer (ViT) [18],\nthat a pure-transformer based architecture has outperformed\nits convolutional counterparts in image classiﬁcation. Doso-\nvitskiy et al. [18] closely followed the original transformer\narchitecture of [68], and noticed that its main beneﬁts\nwere observed at large scale – as transformers lack some\nof the inductive biases of convolutions (such as transla-\ntional equivariance), they seem to require more data [18]\nor stronger regularisation [64].\nInspired by ViT, and the fact that attention-based ar-\nchitectures are an intuitive choice for modelling long-\nrange contextual relationships in video, we develop sev-\neral transformer-based models for video classiﬁcation. Cur-\nrently, the most performant models are based on deep 3D\nconvolutional architectures [8, 20, 21] which were a natu-\nral extension of image classiﬁcation CNNs [27, 60]. Re-\ncently, these models were augmented by incorporating self-\nattention into their later layers to better capture long-range\ndependencies [75, 23, 79, 1].\nAs shown in Fig. 1, we propose pure-transformer mod-\nels for video classiﬁcation. The main operation performed\nin this architecture is self-attention, and it is computed on\na sequence of spatio-temporal tokens that we extract from\nthe input video. To effectively process the large number of\nspatio-temporal tokens that may be encountered in video,\nwe present several methods of factorising our model along\nspatial and temporal dimensions to increase efﬁciency and\nscalability. Furthermore, to train our model effectively on\nsmaller datasets, we show how to reguliarise our model dur-\ning training and leverage pretrained image models.\nWe also note that convolutional models have been de-\nveloped by the community for several years, and there are\nthus many “best practices” associated with such models.\nAs pure-transformer models present different characteris-\ntics, we need to determine the best design choices for such\narchitectures. We conduct a thorough ablation analysis of\ntokenisation strategies, model architecture and regularisa-\ntion methods. Informed by this analysis, we achieve state-\nof-the-art results on multiple standard video classiﬁcation\nbenchmarks, including Kinetics 400 and 600 [35], Epic\nKitchens 100 [13], Something-Something v2 [26] and Mo-\nments in Time [45].\narXiv:2103.15691v2  [cs.CV]  1 Nov 2021\n…\n1CLS0\n32\nN\nPosition + Token Embedding\nMLP HeadClassFactorisedEncoder\nL×\nK         V         Q \nSelf-Attention\nTransformer  Encoder MLP\nLayer Norm\nLayer Norm\nMulti-HeadDot-Product Attention\nEmbed to tokens\nFactorisedSelf-Attention\n21 N\nFactorisedDot-Product\n●●●\nSpatial\nSpatial\n●●●\nTemporal\nTemporal\nSpatial\nTemporal\nSpatial\nTemporal●●●\nSpatialTemporal\n●●●\nFuse\nSpatialTemporalFuse\n●●●21 N●●●21 N●●●\nFigure 1: We propose a pure-transformer architecture for video classiﬁcation, inspired by the recent success of such models for images [18].\nTo effectively process a large number of spatio-temporal tokens, we develop several model variants which factorise different components\nof the transformer encoder over the spatial- and temporal-dimensions. As shown on the right, these factorisations correspond to different\nattention patterns over space and time.\n2. Related Work\nArchitectures for video understanding have mirrored ad-\nvances in image recognition. Early video research used\nhand-crafted features to encode appearance and motion\ninformation [41, 69]. The success of AlexNet on Ima-\ngeNet [38, 16] initially led to the repurposing of 2D im-\nage convolutional networks (CNNs) for video as “two-\nstream” networks [34, 56, 47]. These models processed\nRGB frames and optical ﬂow images independently before\nfusing them at the end. Availability of larger video classi-\nﬁcation datasets such as Kinetics [35] subsequently facili-\ntated the training of spatio-temporal 3D CNNs [8, 22, 65]\nwhich have signiﬁcantly more parameters and thus require\nlarger training datasets. As 3D convolutional networks re-\nquire signiﬁcantly more computation than their image coun-\nterparts, many architectures factorise convolutions across\nspatial and temporal dimensions and/or use grouped convo-\nlutions [59, 66, 67, 81, 20]. We also leverage factorisation\nof the spatial and temporal dimensions of videos to increase\nefﬁciency, but in the context of transformer-based models.\nConcurrently, in natural language processing (NLP),\nVaswani et al. [68] achieved state-of-the-art results by re-\nplacing convolutions and recurrent networks with the trans-\nformer network that consisted only of self-attention, layer\nnormalisation and multilayer perceptron (MLP) operations.\nCurrent state-of-the-art architectures in NLP [17, 52] re-\nmain transformer-based, and have been scaled to web-scale\ndatasets [5]. Many variants of the transformer have also\nbeen proposed to reduce the computational cost of self-\nattention when processing longer sequences [10, 11, 37,\n62, 63, 73] and to improve parameter efﬁciency [40, 14].\nAlthough self-attention has been employed extensively in\ncomputer vision, it has, in contrast, been typically incor-\nporated as a layer at the end or in the later stages of\nthe network [75, 7, 32, 77, 83] or to augment residual\nblocks [30, 6, 9, 57] within a ResNet architecture [27].\nAlthough previous works attempted to replace convolu-\ntions in vision architectures [49, 53, 55], it is only very re-\ncently that Dosovitisky et al. [18] showed with their ViT ar-\nchitecture that pure-transformer networks, similar to those\nemployed in NLP, can achieve state-of-the-art results for\nimage classiﬁcation too. The authors showed that such\nmodels are only effective at large scale, as transformers lack\nsome of inductive biases of convolutional networks (such\nas translational equivariance), and thus require datasets\nlarger than the common ImageNet ILSRVC dataset [16] to\ntrain. ViT has inspired a large amount of follow-up work\nin the community, and we note that there are a number\nof concurrent approaches on extending it to other tasks in\ncomputer vision [71, 74, 84, 85] and improving its data-\nefﬁciency [64, 48]. In particular, [4, 46] have also proposed\ntransformer-based models for video.\nIn this paper, we develop pure-transformer architectures\nfor video classiﬁcation. We propose several variants of our\nmodel, including those that are more efﬁcient by factoris-\ning the spatial and temporal dimensions of the input video.\nWe also show how additional regularisation and pretrained\nmodels can be used to combat the fact that video datasets\nare not as large as their image counterparts that ViT was\noriginally trained on. Furthermore, we outperform the state-\nof-the-art across ﬁve popular datasets.\n3. Video Vision Transformers\nWe start by summarising the recently proposed Vision\nTransformer [18] in Sec. 3.1, and then discuss two ap-\nproaches for extracting tokens from video in Sec. 3.2. Fi-\nnally, we develop several transformer-based architectures\nfor video classiﬁcation in Sec. 3.3 and 3.4.\n3.1. Overview of Vision Transformers (ViT)\nVision Transformer (ViT) [18] adapts the transformer\narchitecture of [68] to process 2D images with minimal\nchanges. In particular, ViT extracts N non-overlapping im-\nage patches, xi ∈Rh×w, performs a linear projection and\nthen rasterises them into 1D tokens zi ∈Rd. The sequence\nof tokens input to the following transformer encoder is\nz = [zcls,Ex1,Ex2,..., ExN ] +p, (1)\nwhere the projection byE is equivalent to a 2D convolution.\nAs shown in Fig. 1, an optional learned classiﬁcation token\nzcls is prepended to this sequence, and its representation at\nthe ﬁnal layer of the encoder serves as the ﬁnal represen-\ntation used by the classiﬁcation layer [17]. In addition, a\nlearned positional embedding, p ∈RN×d, is added to the\ntokens to retain positional information, as the subsequent\nself-attention operations in the transformer are permutation\ninvariant. The tokens are then passed through an encoder\nconsisting of a sequence ofLtransformer layers. Each layer\nℓcomprises of Multi-Headed Self-Attention [68], layer nor-\nmalisation (LN) [2], and MLP blocks as follows:\nyℓ = MSA(LN(zℓ)) +zℓ (2)\nzℓ+1 = MLP(LN(yℓ)) +yℓ. (3)\nThe MLP consists of two linear projections separated by a\nGELU non-linearity [28] and the token-dimensionality, d,\nremains ﬁxed throughout all layers. Finally, a linear classi-\nﬁer is used to classify the encoded input based onzL\ncls ∈Rd,\nif it was prepended to the input, or a global average pooling\nof all the tokens, zL, otherwise.\nAs the transformer [68], which forms the basis of\nViT [18], is a ﬂexible architecture that can operate on any\nsequence of input tokens z ∈RN×d, we describe strategies\nfor tokenising videos next.\n3.2. Embedding video clips\nWe consider two simple methods for mapping a video\nV ∈ RT×H×W×C to a sequence of tokens ˜ z∈\nRnt×nh×nw×d. We then add the positional embedding and\nreshape into RN×d to obtain z, the input to the transformer.\nUniform frame sampling As illustrated in Fig. 2, a\nstraightforward method of tokenising the input video is to\nuniformly sample nt frames from the input video clip, em-\nbed each 2D frame independently using the same method\nas ViT [18], and concatenate all these tokens together. Con-\ncretely, if nh ·nw non-overlapping image patches are ex-\ntracted from each frame, as in [18], then a total ofnt·nh·nw\ntokens will be forwarded through the transformer encoder.\nIntuitively, this process may be seen as simply constructing\na large 2D image to be tokenised following ViT. We note\nthat this is the input embedding method employed by the\nconcurrent work of [4].\n#\n!\n\"\nFigure 2: Uniform frame sampling: We simply sample nt frames,\nand embed each 2D frame independently following ViT [18].\n!\n\"\n#\nFigure 3: Tubelet embedding. We extract and linearly embed non-\noverlapping tubelets that span the spatio-temporal input volume.\nTubelet embedding An alternate method, as shown in\nFig. 3, is to extract non-overlapping, spatio-temporal\n“tubes” from the input volume, and to linearly project this to\nRd. This method is an extension of ViT’s embedding to 3D,\nand corresponds to a 3D convolution. For a tubelet of di-\nmension t×h×w, nt = ⌊T\nt ⌋, nh = ⌊H\nh ⌋and nw = ⌊W\nw ⌋,\ntokens are extracted from the temporal, height, and width\ndimensions respectively. Smaller tubelet dimensions thus\nresult in more tokens which increases the computation.\nIntuitively, this method fuses spatio-temporal information\nduring tokenisation, in contrast to “Uniform frame sam-\npling” where temporal information from different frames is\nfused by the transformer.\n3.3. Transformer Models for Video\nAs illustrated in Fig. 1, we propose multiple transformer-\nbased architectures. We begin with a straightforward ex-\ntension of ViT [18] that models pairwise interactions be-\ntween all spatio-temporal tokens, and then develop more\nefﬁcient variants which factorise the spatial and temporal\ndimensions of the input video at various levels of the trans-\nformer architecture.\nModel 1: Spatio-temporal attention This model sim-\nply forwards all spatio-temporal tokens extracted from the\nvideo, z0, through the transformer encoder. We note that\nthis has also been explored concurrently by [4] in their\n“Joint Space-Time” model. In contrast to CNN architec-\ntures, where the receptive ﬁeld grows linearly with the\nnumber of layers, each transformer layer models all pair-\n…1CLS N\nPositional + Token Embedding\nTemporal + Token Embedding\nEmbed to tokens\n…1 N\n2\n…1 N…\nT\nTemporal Transformer EncoderMLP Head\nClass…CLS10\n0 CLS0 CLS0\nSpatial Transformer EncoderSpatial Transformer EncoderSpatial Transformer Encoder\nFigure 4: Factorised encoder (Model 2). This model consists of\ntwo transformer encoders in series: the ﬁrst models interactions\nbetween tokens extracted from the same temporal index to produce\na latent representation per time-index. The second transformer\nmodels interactions between time steps. It thus corresponds to a\n“late fusion” of spatial- and temporal information.\nwise interactions between all spatio-temporal tokens, and it\nthus models long-range interactions across the video from\nthe ﬁrst layer. However, as it models all pairwise in-\nteractions, Multi-Headed Self Attention (MSA) [68] has\nquadratic complexity with respect to the number of tokens.\nThis complexity is pertinent for video, as the number of to-\nkens increases linearly with the number of input frames, and\nmotivates the development of more efﬁcient architectures\nnext.\nModel 2: Factorised encoder As shown in Fig. 4, this\nmodel consists of two separate transformer encoders. The\nﬁrst, spatial encoder, only models interactions between to-\nkens extracted from the same temporal index. A representa-\ntion for each temporal index, hi ∈Rd, is obtained after Ls\nlayers: This is the encoded classiﬁcation token,zLs\ncls if it was\nprepended to the input (Eq. 1), or a global average pooling\nfrom the tokens output by the spatial encoder, zLs , other-\nwise. The frame-level representations, hi, are concatenated\ninto H ∈Rnt×d, and then forwarded through a temporal\nencoder consisting of Lt transformer layers to model in-\nteractions between tokens from different temporal indices.\nThe output token of this encoder is then ﬁnally classiﬁed.\nThis architecture corresponds to a “late fusion” [34,\n56, 72, 46] of temporal information, and the initial spa-\ntial encoder is identical to the one used for image classi-\nﬁcation. It is thus analogous to CNN architectures such\nas [24, 34, 72, 86] which ﬁrst extract per-frame fea-\ntures, and then aggregate them into a ﬁnal representation\nbefore classifying them. Although this model has more\ntransformer layers than Model 1 (and thus more parame-\nters), it requires fewer ﬂoating point operations (FLOPs),\nas the two separate transformer blocks have a complexity\nof O((nh ·nw)2 + n2\nt ) compared to O((nt ·nh ·nw)2) of\nModel 1.\nTransformer Block xLK         V       Q \nMLPLayer Norm\nLayer Norm\nMulti-HeadAttention\nK         V       Q \nLayer Norm\nMulti-HeadAttention\nTemporal Self-Attention BlockSpatial Self-Attention Block\nToken embeddingPositional embedding\nFigure 5: Factorised self-attention (Model 3). Within each trans-\nformer block, the multi-headed self-attention operation is fac-\ntorised into two operations (indicated by striped boxes) that ﬁrst\nonly compute self-attention spatially, and then temporally.\nModel 3: Factorised self-attention This model, in con-\ntrast, contains the same number of transformer layers as\nModel 1. However, instead of computing multi-headed\nself-attention across all pairs of tokens, zℓ, at layer l, we\nfactorise the operation to ﬁrst only compute self-attention\nspatially (among all tokens extracted from the same tem-\nporal index), and then temporally (among all tokens ex-\ntracted from the same spatial index) as shown in Fig. 5.\nEach self-attention block in the transformer thus models\nspatio-temporal interactions, but does so more efﬁciently\nthan Model 1 by factorising the operation over two smaller\nsets of elements, thus achieving the same computational\ncomplexity as Model 2. We note that factorising attention\nover input dimensions has also been explored in [29, 78],\nand concurrently in the context of video by [4] in their “Di-\nvided Space-Time” model.\nThis operation can be performed efﬁciently by reshaping\nthe tokens z from R1×nt·nh·nw·d to Rnt×nh·nw·d (denoted\nby zs) to compute spatial self-attention. Similarly, the input\nto temporal self-attention, zt is reshaped to Rnh·nw×nt·d.\nHere we assume the leading dimension is the “batch dimen-\nsion”. Our factorised self-attention is deﬁned as\nyℓ\ns = MSA(LN(zℓ\ns)) +zℓ\ns (4)\nyℓ\nt = MSA(LN(yℓ\ns)) +yℓ\ns (5)\nzℓ+1 = MLP(LN(yℓ\nt)) +yℓ\nt. (6)\nWe observed that the order of spatial-then-temporal self-\nattention or temporal-then-spatial self-attention does not\nmake a difference, provided that the model parameters are\ninitialised as described in Sec. 3.4. Note that the number\nof parameters, however, increases compared to Model 1, as\nthere is an additional self-attention layer (cf. Eq. 7). We do\nnot use a classiﬁcation token in this model, to avoid ambi-\nguities when reshaping the input tokens between spatial and\ntemporal dimensions.\nModel 4: Factorised dot-product attentionFinally, we\ndevelop a model which has the same computational com-\nplexity as Models 2 and 3, while retaining the same number\nof parameters as the unfactorised Model 1. The factorisa-\ntion of spatial- and temporal dimensions is similar in spirit\nK         V       Q \nSelf-Attention BlockLayer Norm\nMulti-HeadDot-product AttentionConcatenateLinear\nK             V                Q \nScaled Dot-Product Attention\nLinearLinearLinear\nSpatial HeadsK             V                Q \nScaled Dot-Product Attention\nLinearLinearLinear\nTemporal Heads\nFigure 6: Factorised dot-product attention (Model 4). For half of\nthe heads, we compute dot-product attention over only the spatial\naxes, and for the other half, over only the temporal axis.\nto Model 3, but we factorise the multi-head dot-product at-\ntention operation instead (Fig. 6). Concretely, we compute\nattention weights for each token separately over the spatial-\nand temporal-dimensions using different heads. First, we\nnote that the attention operation for each head is deﬁned as\nAttention(Q,K,V) = Softmax\n(QK⊤\n√dk\n)\nV. (7)\nIn self-attention, the queries Q = XWq, keys K = XWk,\nand values V = XWv are linear projections of the input X\nwith X,Q,K,V ∈RN×d. Note that in the unfactorised\ncase (Model 1), the spatial and temporal dimensions are\nmerged as N = nt ·nh ·nw.\nThe main idea here is to modify the keys and values for\neach query to only attend over tokens from the same spatial-\nand temporal index by constructing Ks,Vs ∈Rnh·nw×d\nand Kt,Vt ∈Rnt×d, namely the keys and values corre-\nsponding to these dimensions. Then, for half of the atten-\ntion heads, we attend over tokens from the spatial dimen-\nsion by computing Ys = Attention(Q,Ks,Vs), and for\nthe rest we attend over the temporal dimension by comput-\ning Yt = Attention(Q,Kt,Vt). Given that we are only\nchanging the attention neighbourhood for each query, the\nattention operation has the same dimension as in the unfac-\ntorised case, namely Ys,Yt ∈RN×d. We then combine\nthe outputs of multiple heads by concatenating them and\nusing a linear projection [68], Y = Concat(Ys,Yt)WO.\n3.4. Initialisation by leveraging pretrained models\nViT [18] has been shown to only be effective when\ntrained on large-scale datasets, as transformers lack some of\nthe inductive biases of convolutional networks [18]. How-\never, even the largest video datasets such as Kinetics [35],\nhave several orders of magnitude less labelled examples\nwhen compared to their image counterparts [16, 39, 58]. As\na result, training large models from scratch to high accuracy\nis extremely challenging. To sidestep this issue, and enable\nmore efﬁcient training we initialise our video models from\npretrained image models. However, this raises several prac-\ntical questions, speciﬁcally on how to initialise parameters\nnot present or incompatible with image models. We now\ndiscuss several effective strategies to initialise these large-\nscale video classiﬁcation models.\nPositional embeddings A positional embedding p is\nadded to each input token (Eq. 1). However, our video\nmodels have nt times more tokens than the pretrained im-\nage model. As a result, we initialise the positional embed-\ndings by “repeating” them temporally from Rnw·nh×d to\nRnt·nh·nw×d. Therefore, at initialisation, all tokens with\nthe same spatial index have the same embedding which is\nthen ﬁne-tuned.\nEmbedding weights,E When using the “tubelet embed-\nding” tokenisation method (Sec. 3.2), the embedding ﬁlter\nE is a 3D tensor, compared to the 2D tensor in the pre-\ntrained model, Eimage. A common approach for initialising\n3D convolutional ﬁlters from 2D ﬁlters for video classiﬁca-\ntion is to “inﬂate” them by replicating the ﬁlters along the\ntemporal dimension and averaging them [8, 22] as\nE = 1\nt[Eimage,..., Eimage,..., Eimage]. (8)\nWe consider an additional strategy, which we denote as\n“central frame initialisation”, whereE is initialised with ze-\nroes along all temporal positions, except at the centre ⌊t\n2 ⌋,\nE = [0,..., Eimage,..., 0]. (9)\nTherefore, the 3D convolutional ﬁlter effectively behaves\nlike “Uniform frame sampling” (Sec. 3.2) at initialisation,\nwhile also enabling the model to learn to aggregate temporal\ninformation from multiple frames as training progresses.\nTransformer weights for Model 3 The transformer\nblock in Model 3 (Fig. 5) differs from the pretrained ViT\nmodel [18], in that it contains two multi-headed self atten-\ntion (MSA) modules. In this case, we initialise the spatial\nMSA module from the pretrained module, and initialise all\nweights of the temporal MSA with zeroes, such that Eq. 5\nbehaves as a residual connection [27] at initialisation.\n4. Empirical evaluation\nWe ﬁrst present our experimental setup and implementa-\ntion details in Sec. 4.1, before ablating various components\nof our model in Sec. 4.2. We then present state-of-the-art\nresults on ﬁve datasets in Sec. 4.3.\n4.1. Experimental Setup\nNetwork architecture and trainingOur backbone archi-\ntecture follows that of ViT [18] and BERT [17]. We con-\nsider ViT-Base (ViT-B,L=12, NH=12, d=768), ViT-Large\n(ViT-L, L=24, NH=16, d=1024), and ViT-Huge (ViT-H,\nL=32, NH=16, d=1280), where L is the number of trans-\nformer layers, each with a self-attention block of NH heads\nTable 1: Comparison of input encoding methods using ViViT-B\nand spatio-temporal attention on Kinetics. Further details in text.\nTop-1 accuracy\nUniform frame sampling 78.5\nTubelet embedding\nRandom initialisation [25] 73.2\nFilter inﬂation [8] 77.6\nCentral frame 79.2\nand hidden dimension d. We also apply the same naming\nscheme to our models (e.g., ViViT-B/16x2 denotes a ViT-\nBase backbone with a tubelet size ofh×w×t= 16×16×2).\nIn all experiments, the tubelet height and width are equal.\nNote that smaller tubelet sizes correspond to more tokens at\nthe input, and thus more computation.\nWe train our models using synchronous SGD and mo-\nmentum, a cosine learning rate schedule and TPU-v3 ac-\ncelerators. We initialise our models from a ViT image\nmodel trained either on ImageNet-21K [16] (unless other-\nwise speciﬁed) or the larger JFT [58] dataset. We imple-\nment our method using the Scenic library [15] and have re-\nleased our code and models.\nDatasets We evaluate the performance of our proposed\nmodels on a diverse set of video classiﬁcation datasets:\nKinetics [35] consists of 10-second videos sampled at\n25fps from YouTube. We evaluate on both Kinetics 400\nand 600, containing 400 and 600 classes respectively. As\nthese are dynamic datasets (videos may be removed from\nYouTube), we note our dataset sizes are approximately 267\n000 and 446 000 respectively.\nEpic Kitchens-100 consists of egocentric videos captur-\ning daily kitchen activities spanning 100 hours and 90 000\nclips [13]. We report results following the standard “action\nrecognition” protocol. Here, each video is labelled with a\n“verb” and a “noun” and we therefore predict both cate-\ngories using a single network with two “heads”. The top-\nscoring verb and action pair predicted by the network form\nan “action”, and action accuracy is the primary metric.\nMoments in Time [45] consists of 800 000, 3-second\nYouTube clips that capture the gist of a dynamic scene in-\nvolving animals, objects, people, or natural phenomena.\nSomething-Something v2 (SSv2) [26] contains 220 000\nvideos, with durations ranging from 2 to 6 seconds. In con-\ntrast to the other datasets, the objects and backgrounds in\nthe videos are consistent across different action classes, and\nthis dataset thus places more emphasis on a model’s ability\nto recognise ﬁne-grained motion cues.\nInference The input to our network is a video clip of 32\nframes using a stride of 2, unless otherwise mentioned, sim-\nilar to [21, 20]. Following common practice, at inference\ntime, we process multiple views of a longer video and aver-\nTable 2: Comparison of model architectures using ViViT-B as the\nbackbone, and tubelet size of16×2. We report Top-1 accuracy on\nKinetics 400 (K400) and action accuracy on Epic Kitchens (EK).\nRuntime is during inference on a TPU-v3.\nK400 EK FLOPs\n(×109)\nParams\n(×106)\nRuntime\n(ms)\nModel 1: Spatio-temporal 80.0 43.1 455.2 88.9 58.9\nModel 2: Fact. encoder 78.8 43.7 284.4 115.1 17.4\nModel 3: Fact. self-attention 77.4 39.1 372.3 117.3 31.7\nModel 4: Fact. dot product 76.3 39.5 277.1 88.9 22.9\nModel 2: Ave. pool baseline 75.8 38.8 283.9 86.7 17.3\nTable 3: The effect of varying the number of temporal transform-\ners, Lt, in the Factorised encoder model (Model 2). We report the\nTop-1 accuracy on Kinetics 400. Note that Lt = 0corresponds to\nthe “average pooling baseline”.\nLt 0 1 4 8 12\nTop-1 75.8 78.6 78.8 78.8 78.9\nage per-view logits to obtain the ﬁnal result. Unless other-\nwise speciﬁed, we use a total of 4 views per video (as this\nis sufﬁcient to “see” the entire video clip across the various\ndatasets), and ablate these and other design choices next.\n4.2. Ablation study\nInput encoding We ﬁrst consider the effect of different\ninput encoding methods (Sec. 3.2) using our unfactorised\nmodel (Model 1) and ViViT-B on Kinetics 400. As we pass\n32-frame inputs to the network, sampling 8 frames and ex-\ntracting tubelets of length t = 4 correspond to the same\nnumber of tokens in both cases. Table 1 shows that tubelet\nembedding initialised using the “central frame” method\n(Eq. 9) performs well, outperforming the commonly-used\n“ﬁlter inﬂation” initialisation method [8, 22] by 1.6%, and\n“uniform frame sampling” by 0.7%. We therefore use this\nencoding method for all subsequent experiments.\nModel variants We compare our proposed model vari-\nants (Sec. 3.3) across the Kinetics 400 and Epic Kitchens\ndatasets, both in terms of accuracy and efﬁciency, in Tab. 2.\nIn all cases, we use the “Base” backbone and tubelet size of\n16 ×2. Model 2 (“Factorised Encoder”) has an additional\nhyperparameter, the number of temporal transformers, Lt.\nWe set Lt = 4for all experiments and show in Tab. 3 that\nthe model is not sensitive to this choice.\nThe unfactorised model (Model 1) performs the best\non Kinetics 400. However, it can also overﬁt on smaller\ndatasets such as Epic Kitchens, where we ﬁnd our “Fac-\ntorised Encoder” (Model 2) to perform the best. We also\nconsider an additional baseline (last row), based on Model\n2, where we do not use any temporal transformer, and sim-\nply average pool the frame-level representations from the\nspatial encoder before classifying. This average pooling\nbaseline performs the worst, and has a larger accuracy drop\nTable 4: The effect of progressively adding regularisation (each\nrow includes all methods above it) on Top-1 action accuracy on\nEpic Kitchens. We use a Factorised encoder model with tubelet\nsize 16 ×2.\nTop-1 accuracy\nRandom crop, ﬂip, colour jitter 38.4\n+ Kinetics 400 initialisation 39.6\n+ Stochastic depth [31] 40.2\n+ Random augment [12] 41.1\n+ Label smoothing [61] 43.1\n+ Mixup [82] 43.7\n16x8 16x4 16x2\nInput tubelet size\n78\n79\n80Top-1 Accuracy\n16x8 16x4 16x2\nInput tubelet size\n0.5\n1.0\n1.5TFLOPs\nViViT-B ViViT-L\n(a) Accuracy (b) Compute\nFigure 7: The effect of the backbone architecture on (a) accuracy\nand (b) computation on Kinetics 400, for the spatio-temporal at-\ntention model (Model 1).\n16x8 16x4 16x2\nInput tubelet size\n72.5\n75.0\n77.5\n80.0Top-1 Accuracy\n16x8 16x4 16x2\nInput tubelet size\n0.2\n0.4TFLOPs\nSpatio-temporal Factorised encoder Factorised self-attention Factorised dot-product\n(a) Accuracy (b) Compute\nFigure 8: The effect of varying the number of temporal tokens on\n(a) accuracy and (b) computation on Kinetics 400, for different\nvariants of our model with a ViViT-B backbone.\non Epic Kitchens, suggesting that this dataset requires more\ndetailed modelling of temporal relations.\nAs described in Sec. 3.3, all factorised variants of our\nmodel use signiﬁcantly fewer FLOPs than the unfactorised\nModel 1, as the attention is computed separately over\nspatial- and temporal-dimensions. Model 4 adds no addi-\ntional parameters to the unfactorised Model 1, and uses the\nleast compute. The temporal transformer encoder in Model\n2 operates on only nt tokens, which is why there is a barely\na change in compute and runtime over the average pool-\ning baseline, even though it improves the accuracy substan-\ntially (3% on Kinetics and 4.9% on Epic Kitchens). Fi-\nnally, Model 3 requires more compute and parameters than\nthe other factorised models, as its additional self-attention\nblock means that it performs another query-, key-, value-\nand output-projection in each transformer layer [68].\nModel regularisation Pure-transformer architectures\nsuch as ViT [18] are known to require large training\ndatasets, and we observed overﬁtting on smaller datasets\nlike Epic Kitchens and SSv2, even when using an ImageNet\npretrained model. In order to effectively train our models\nTable 5: The effect of spatial resolution on the performance of\nViViT-L/16x2 and spatio-temporal attention on Kinetics 400.\nCrop size 224 288 320\nAccuracy 80.3 80.7 81.0\nGFLOPs 1446 2919 3992\nRuntime 58.9 147.6 238.8\non such datasets, we employed several regularisation strate-\ngies that we ablate using our “Factorised encoder” model\nin Tab. 4. We note that these regularisers were originally\nproposed for training CNNs, and that [64] have recently\nexplored them for training ViT for image classiﬁcation.\nEach row of Tab. 4 includes all the methods from the\nrows above it, and we observe progressive improvements\nfrom adding each regulariser. Overall, we obtain a substan-\ntial overall improvement of 5.3% on Epic Kitchens. We\nalso achieve a similar improvement of 5% on SSv2 by us-\ning all the regularisation in Tab. 4. Note that the Kinetics-\npretrained models that we initialise from are from Tab. 2,\nand that all Epic Kitchens models in Tab. 2 were trained\nwith all the regularisers in Tab. 4. For larger datasets like\nKinetics and Moments in Time, we do not use these ad-\nditional regularisers (we use only the ﬁrst row of Tab. 4),\nas we obtain state-of-the-art results without them. The ap-\npendix contains hyperparameter values and additional de-\ntails for all regularisers.\nVarying the backbone Figure 7 compares the ViViT-\nB and ViViT-L backbones for the unfactorised spatio-\ntemporal model. We observe consistent improvements in\naccuracy as the backbone capacity increases. As expected,\nthe compute also grows as a function of the backbone size.\nVarying the number of tokensWe ﬁrst analyse the per-\nformance as a function of the number of tokens along the\ntemporal dimension in Fig. 8. We observe that using smaller\ninput tubelet sizes (and therefore more tokens) leads to con-\nsistent accuracy improvements across all of our model ar-\nchitectures. At the same time, computation in terms of\nFLOPs increases accordingly, and the unfactorised model\n(Model 1) is impacted the most.\nWe then vary the number of tokens fed into the model\nby increasing the spatial crop-size from the default of 224\nto 320 in Tab. 5. As expected, there is a consistent increase\nin both accuracy and computation. We note that when com-\nparing to prior work we consistently obtain state-of-the-art\nresults (Sec. 4.3) using a spatial resolution of 224, but we\nalso highlight that further improvements can be obtained at\nhigher spatial resolutions.\nVarying the number of input framesIn our experiments\nso far, we have kept the number of input frames ﬁxed at 32.\nWe now increase the number of frames input to the model,\nthereby increasing the number of tokens proportionally.\nTable 6: Comparisons to state-of-the-art across multiple datasets. For “views”, x ×y denotes x temporal crops and y spatial crops. We\nreport the TFLOPs to process all spatio-temporal views. “FE” denotes our Factorised Encoder model.\n(a) Kinetics 400\nMethod Top 1 Top 5 Views TFLOPs\nblVNet [19] 73.5 91.2 – –\nSTM [33] 73.7 91.6 – –\nTEA [42] 76.1 92.5 10 ×3 2.10\nTSM-ResNeXt-101 [43] 76.3 – – –\nI3D NL [75] 77.7 93.3 10 ×3 10.77\nCorrNet-101 [70] 79.2 – 10 ×3 6.72\nip-CSN-152 [66] 79.2 93.8 10 ×3 3.27\nLGD-3D R101 [51] 79.4 94.4 – –\nSlowFast R101-NL [21] 79.8 93.9 10 ×3 7.02\nX3D-XXL [20] 80.4 94.6 10 ×3 5.82\nTimeSformer-L [4] 80.7 94.7 1 ×3 7.14\nViViT-L/16x2 FE 80.6 92.7 1 ×1 3.98\nViViT-L/16x2 FE 81.7 93.8 1 ×3 11.94\nMethods with large-scale pretraining\nip-CSN-152 [66] (IG [44]) 82.5 95.3 10 ×3 3.27\nViViT-L/16x2 FE (JFT) 83.5 94.3 1 ×3 11.94\nViViT-H/14x2 (JFT) 84.9 95.8 4 ×3 47.77\n(b) Kinetics 600\nMethod Top 1 Top 5\nAttentionNAS [76] 79.8 94.4\nLGD-3D R101 [51] 81.5 95.6\nSlowFast R101-NL [21] 81.8 95.1\nX3D-XL [20] 81.9 95.5\nTimeSformer-L [4] 82.2 95.6\nViViT-L/16x2 FE 82.9 94.6\nViViT-L/16x2 FE (JFT) 84.3 94.9\nViViT-H/14x2 (JFT) 85.8 96.5\n(c) Moments in Time\nTop 1 Top 5\nTSN [72] 25.3 50.1\nTRN [86] 28.3 53.4\nI3D [8] 29.5 56.1\nblVNet [19] 31.4 59.3\nAssembleNet-101 [54] 34.3 62.7\nViViT-L/16x2 FE 38.5 64.1\n(d) Epic Kitchens 100 Top 1 accuracy\nMethod Action Verb Noun\nTSN [72] 33.2 60.2 46.0\nTRN [86] 35.3 65.9 45.4\nTBN [36] 36.7 66.0 47.2\nTSM [43] 38.3 67.9 49.0\nSlowFast [21] 38.5 65.6 50.0\nViViT-L/16x2 FE 44.0 66.4 56.8\n(e) Something-Something v2\nMethod Top 1 Top 5\nTRN [86] 48.8 77.6\nSlowFast [20, 80] 61.7 –\nTimeSformer-HR [4] 62.5 –\nTSM [43] 63.4 88.5\nSTM [33] 64.2 89.8\nTEA [42] 65.1 –\nblVNet [19] 65.2 90.3\nViVIT-L/16x2 FE 65.9 89.9\n1 2 3 4 5 6 7\nNumber of views\n76\n78\n80Top-1 Accuracy\n32 stride 2 64 stride 2 128 stride 2\nFigure 9: The effect of varying the number of frames input to the\nnetwork and increasing the number of tokens proportionally. We\nuse ViViT-L/16x2 Factorised Encoder on Kinetics 400. A Kinetics\nvideo contains 250 frames (10 seconds sampled at 25 fps) and the\naccuracy for each model saturates once the number of equidistant\ntemporal views is sufﬁcient to “see” the whole video clip. Ob-\nserve how models processing more frames (and thus more tokens)\nachieve higher single- and multi-view accuracy.\nFigure 9 shows that as we increase the number of frames\ninput to the network, the accuracy from processing a sin-\ngle view increases, since the network incorporates longer\ntemporal context. However, common practice on datasets\nsuch as Kinetics [21, 75, 42] is to average results over mul-\ntiple, shorter “views” of the same video clip. Figure 9 also\nshows that the accuracy saturates once the number of views\nis sufﬁcient to cover the whole video. As a Kinetics video\nconsists of 250 frames, and we sample frames with a stride\nof 2, our model which processes 128 frames requires just a\nsingle view to “see” the whole video and achieve its maxi-\nmum accuarcy.\nNote that we used ViViT-L/16x2 Factorised Encoder\n(Model 2) here. As this model is more efﬁcient it can pro-\ncess more tokens, compared to the unfactorised Model 1\nwhich runs out of memory after 48 frames using tubelet\nlength t = 2 and a “Large” backbone. Models processing\nmore frames (and thus more tokens) consistently achieve\nhigher single- and multi-view accuracy, in line with our ob-\nservations in previous experiments (Tab. 5, Fig. 8). Mo-\nroever, observe that by processing more frames (and thus\nmore tokens) with Model 2, we are able to achieve higher\naccuracy than Model 1 (with fewer total FLOPs as well).\nFinally, we observed that for Model 2, the number of\nFLOPs effectively increases linearly with the number of in-\nput frames as the overall computation is dominated by the\ninitial Spatial Transformer. As a result, the total number\nof FLOPs for the number of temporal views required to\nachieve maximum accuracy is constant across the models.\nIn other words, ViViT-L/16x2 FE with 32 frames requires\n995.3 GFLOPs per view, and 4 views to saturate multi-view\naccuracy. The 128-frame model requires 3980.4 GFLOPs\nbut only a single view. As shown by Fig. 9, the latter model\nachieves the highest accuracy.\n4.3. Comparison to state-of-the-art\nBased on our ablation studies in the previous section,\nwe compare to the current state-of-the-art using two of our\nmodel variants. We primarily use our Factorised Encoder\nmodel (Model 2), as it can process more tokens than Model\n1 to achieve higher accuracy.\nKinetics Tables 6a and 6b show that our spatio-temporal\nattention models outperform the state-of-the-art on Kinetics\n400 and 600 respectively. Following standard practice, we\ntake 3 spatial crops (left, centre and right) [21, 20, 66, 75]\nfor each temporal view, and notably, we require signiﬁ-\ncantly fewer views than previous CNN-based methods.\nWe surpass the previous CNN-based state-of-the-art us-\ning ViViT-L/16x2 Factorised Encoder (FE) pretrained on\nImageNet, and also outperform [4] who concurrently pro-\nposed a pure-transformer architecture. Moreover, by initial-\nising our backbones from models pretrained on the larger\nJFT dataset [58], we obtain further improvements. Al-\nthough these models are not directly comparable to previ-\nous work, we do also outperform [66] who pretrained on\nthe large-scale, Instagram dataset [44]. Our best model uses\na ViViT-H backbone pretrained on JFT and signiﬁcantly ad-\nvances the best reported results on Kinetics 400 and 600 to\n84.9% and 85.8%, respectively.\nMoments in Time We surpass the state-of-the-art by a\nsigniﬁcant margin as shown in Tab. 6c. We note that the\nvideos in this dataset are diverse and contain signiﬁcant la-\nbel noise, making this task challenging and leading to lower\naccuracies than on other datasets.\nEpic Kitchens 100 Table 6d shows that our Factorised\nEncoder model outperforms previous methods by a signiﬁ-\ncant margin. In addition, our model obtains substantial im-\nprovements for Top-1 accuracy of “noun” classes, and the\nonly method which achieves higher “verb” accuracy used\noptical ﬂow as an additional input modality [43, 50]. Fur-\nthermore, all variants of our model presented in Tab. 2 out-\nperformed the existing state-of-the-art on action accuracy.\nWe note that we use the same model to predict verbs and\nnouns using two separate “heads”, and for simplicity, we do\nnot use separate loss weights for each head.\nSomething-Something v2 (SSv2) Finally, Tab. 6e shows\nthat we achieve state-of-the-art Top-1 accuracy with our\nFactorised encoder model (Model 2), albeit with a smaller\nmargin compared to previous methods. Notably, our Fac-\ntorised encoder model signiﬁcantly outperforms the concur-\nrent TimeSformer [4] method by 2.9%, which also proposes\na pure-transformer model, but does not consider our Fac-\ntorised encoder variant or our additional regularisation.\nSSv2 differs from other datasets in that the backgrounds\nand objects are quite similar across different classes, mean-\ning that recognising ﬁne-grained motion patterns is neces-\nsary to distinguish classes from each other. Our results sug-\ngest that capturing these ﬁne-grained motions is an area of\nimprovement and future work for our model. We also note\nan inverse correlation between the relative performance of\nprevious methods on SSv2 (Tab. 6e) and Kinetics (Tab. 6a)\nsuggesting that these two datasets evaluate complementary\ncharacteristics of a model.\n5. Conclusion and Future Work\nWe have presented four pure-transformer models for\nvideo classiﬁcation, with different accuracy and efﬁciency\nproﬁles, achieving state-of-the-art results across ﬁve pop-\nular datasets. Furthermore, we have shown how to ef-\nfectively regularise such high-capacity models for training\non smaller datasets and thoroughly ablated our main de-\nsign choices. Future work is to remove our dependence on\nimage-pretrained models. Finally, going beyond video clas-\nsiﬁcation towards more complex tasks is a clear next step.\nReferences\n[1] Anurag Arnab, Chen Sun, and Cordelia Schmid. Uniﬁed\ngraph structured models for video understanding. In ICCV,\n2021. 1\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. In arXiv preprint arXiv:1607.06450 ,\n2016. 3\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, 2019. 1\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\nIn arXiv preprint arXiv:2102.05095, 2021. 2, 3, 4, 8, 9\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, et al. Language models are few-shot learners. In\nNeurIPS, 2020. 2\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In CVPR Workshops, 2019. 2\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1,\n2\n[8] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017. 1, 2, 5, 6, 8\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\nYan, and Jiashi Feng. A2-nets: Double attention networks.\nIn NeurIPS, 2018. 2\n[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. In\narXiv preprint arXiv:1904.10509, 2019. 2\n[11] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. In ICLR, 2021.\n2\n[12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V .\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In NeurIPS, 2020. 7, 13, 14\n[13] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,\nAntonino Furnari, Jian Ma, Evangelos Kazakos, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and\nMichael Wray. Rescaling egocentric vision. In arXiv\npreprint arXiv:2006.13256, 2020. 1, 6\n[14] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\nUszkoreit, and Łukasz Kaiser. Universal transformers. In\nICLR, 2019. 2\n[15] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab,\nMatthias Minderer, and Yi Tay. Scenic: A JAX library\nfor computer vision research and beyond. arXiv preprint\narXiv:2110.11403, 2021. 6\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 5, 6\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019. 2, 3,\n5\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 1, 2,\n3, 5, 7\n[19] Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia,\nand David Cox. More is less: Learning efﬁcient video repre-\nsentations by big-little network and depthwise temporal ag-\ngregation. In NeurIPS, 2019. 8\n[20] Christoph Feichtenhofer. X3d: Expanding architectures for\nefﬁcient video recognition. In CVPR, 2020. 1, 2, 6, 8\n[21] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nICCV, 2019. 1, 6, 8\n[22] Christoph Feichtenhofer, Axel Pinz, and Richard Wildes.\nSpatiotemporal residual networks for video action recogni-\ntion. In NeurIPS, 2016. 2, 5, 6\n[23] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n1\n[24] Rohit Girdhar and Deva Ramanan. Attentional pooling for\naction recognition. In NeurIPS, 2017. 4\n[25] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In AIS-\nTATS, 2010. 6\n[26] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al. The” something something” video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 1, 6\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 1, 2, 5\n[28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). In arXiv preprint arXiv:1606.08415, 2016. 3\n[29] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\nIn arXiv preprint arXiv:1912.12180, 2019. 4\n[30] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In CVPR, 2018. 2\n[31] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian\nWeinberger. Deep networks with stochastic depth. InECCV,\n2016. 7, 13, 14\n[32] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 2\n[33] Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and\nJunjie Yan. Stm: Spatiotemporal and motion encoding for\naction recognition. In ICCV, 2019. 8\n[34] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video\nclassiﬁcation with convolutional neural networks. In CVPR,\n2014. 2, 4\n[35] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The ki-\nnetics human action video dataset. In arXiv preprint\narXiv:1705.06950, 2017. 1, 2, 5, 6\n[36] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and\nDima Damen. Epic-fusion: Audio-visual temporal binding\nfor egocentric action recognition. In ICCV, 2019. 8\n[37] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efﬁcient transformer. In ICLR, 2020. 2\n[38] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS, volume 25, 2012. 1, 2\n[39] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Tom Duerig, et al. The open im-\nages dataset v4: Uniﬁed image classiﬁcation, object detec-\ntion, and visual relationship detection at scale. IJCV, 2020.\n5\n[40] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\nIn ICLR, 2020. 2\n[41] Ivan Laptev. On space-time interest points. IJCV, 64(2-3),\n2005. 2\n[42] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and\nLimin Wang. Tea: Temporal excitation and aggregation for\naction recognition. In CVPR, 2020. 8\n[43] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift\nmodule for efﬁcient video understanding. In ICCV, 2019. 8,\n9\n[44] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,\nKaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\nand Laurens Van Der Maaten. Exploring the limits of weakly\nsupervised pretraining. In ECCV, 2018. 8, 9\n[45] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ra-\nmakrishnan, Sarah Adel Bargal, Tom Yan, Lisa Brown,\nQuanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments\nin time dataset: one million videos for event understanding.\nPAMI, 42(2):502–508, 2019. 1, 6\n[46] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-\nselmann. Video transformer network. In arXiv preprint\narXiv:2102.00719, 2021. 2, 4\n[47] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vi-\njayanarasimhan, Oriol Vinyals, Rajat Monga, and George\nToderici. Beyond short snippets: Deep networks for video\nclassiﬁcation. In CVPR, 2015. 2\n[48] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jian-\nfei Cai. Scalable visual transformers with hierarchical pool-\ning. In arXiv preprint arXiv:2103.10619, 2021. 2\n[49] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 1, 2\n[50] Will Price and Dima Damen. An evaluation of action\nrecognition models on epic-kitchens. In arXiv preprint\narXiv:1908.00867, 2019. 9\n[51] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and\nTao Mei. Learning spatio-temporal representation with local\nand global diffusion. In CVPR, 2019. 8\n[52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. JMLR, 2020. 2\n[53] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019. 1, 2\n[54] Michael S Ryoo, AJ Piergiovanni, Mingxing Tan, and Anelia\nAngelova. Assemblenet: Searching for multi-stream neural\nconnectivity in video architectures. In ICLR, 2020. 8\n[55] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia,\nand Ching-Hui Chen. Global self-attention networks for im-\nage recognition. In arXiv preprint arXiv:2010.03019, 2021.\n2\n[56] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In\nNeurIPS, 2014. 2, 4\n[57] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck\ntransformers for visual recognition. In CVPR, 2021. 2\n[58] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In ICCV, 2017. 5, 6, 9\n[59] Lin Sun, Kui Jia, Dit-Yan Yeung, and Bertram E Shi. Human\naction recognition using factorized spatio-temporal convolu-\ntional networks. In ICCV, 2015. 2\n[60] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR, 2015. 1\n[61] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, 2016. 7, 13, 14\n[62] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebas-\ntian Ruder, and Donald Metzler. Long range arena: A\nbenchmark for efﬁcient transformers. In arXiv preprint\narXiv:2011.04006, 2020. 2\n[63] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. Efﬁcient transformers: A survey. In arXiv preprint\narXiv:2009.06732, 2020. 2\n[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In arXiv preprint arXiv:2012.12877, 2020. 1, 2, 7\n[65] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,\nand Manohar Paluri. Learning spatiotemporal features with\n3d convolutional networks. In ICCV, 2015. 2\n[66] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feis-\nzli. Video classiﬁcation with channel-separated convolu-\ntional networks. In ICCV, 2019. 2, 8, 9\n[67] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann\nLeCun, and Manohar Paluri. A closer look at spatiotemporal\nconvolutions for action recognition. In CVPR, 2018. 2\n[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2, 3, 4, 5, 7\n[69] Heng Wang, Alexander Kl ¨aser, Cordelia Schmid, and\nCheng-Lin Liu. Dense trajectories and motion boundary de-\nscriptors for action recognition. IJCV, 103(1), 2013. 2\n[70] Heng Wang, Du Tran, Lorenzo Torresani, and Matt Feiszli.\nVideo modeling with correlation networks. In CVPR, 2020.\n8\n[71] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. Max-deeplab: End-to-end panoptic\nsegmentation with mask transformers. In arXiv preprint\narXiv:2012.00759, 2020. 2\n[72] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In ECCV, 2016. 4, 8\n[73] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\nIn arXiv preprint arXiv:2006.04768, 2020. 2\n[74] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. In arXiv preprint\narXiv:2102.12122, 2021. 2\n[75] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1, 2,\n8\n[76] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Pier-\ngiovanni, Michael S Ryoo, Anelia Angelova, Kris M Kitani,\nand Wei Hua. Attentionnas: Spatiotemporal attention cell\nsearch for video classiﬁcation. In ECCV, 2020. 8\n[77] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In arXiv\npreprint arXiv:2011.14503, 2020. 2\n[78] Dirk Weissenborn, Oscar T ¨ackstr¨om, and Jakob Uszkoreit.\nScaling autoregressive video models. In ICLR, 2020. 4\n[79] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-\ning He, Philipp Krahenbuhl, and Ross Girshick. Long-term\nfeature banks for detailed video understanding. In CVPR,\n2019. 1\n[80] Chao-Yuan Wu, Ross Girshick, Kaiming He, Christoph Fe-\nichtenhofer, and Philipp Krahenbuhl. A multigrid method\nfor efﬁciently training video models. In CVPR, 2020. 8\n[81] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning:\nSpeed-accuracy trade-offs in video classiﬁcation. In ECCV,\n2018. 2\n[82] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. Mixup: Beyond empirical risk minimiza-\ntion. In ICLR, 2018. 7, 13, 14\n[83] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dy-\nnamic graph message passing networks. In CVPR, 2020. 2\n[84] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. In arXiv preprint\narXiv:2012.09164, 2020. 2\n[85] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In arXiv preprint arXiv:2012.15840, 2020. 2\n[86] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-\nralba. Temporal relational reasoning in videos. In ECCV,\n2018. 4, 8\nAppendix\nA. Additional experimental details\nIn this appendix, we provide additional experimental de-\ntails. Section A.1 provides additional details about the reg-\nularisers we used and Sec. A.2 details the training hyper-\nparamters used for our experiments.\nA.1. Further details about regularisers\nIn this section, we provide additional details and list the\nhyperparameters of the additional regularisers that we em-\nployed in Tab. 4. Hyperparameter values for all our experi-\nments are listed in Tab. 7.\nStochastic depth Stochastic depth regularisation was\noriginally proposed for training very deep residual net-\nworks [31]. Intuitively, the outputs of a layer, ℓ, are\n“dropped out” with probability, pdrop(ℓ) during training, by\nsetting the output of the layer to be equal to its input.\nFollowing [31], we linearly increase the probability of\ndropping a layer according to its depth within the network,\npdrop(ℓ) = ℓ\nLpdrop, (10)\nwhere ℓis the index of the layer in the network, andLis the\ntotal number of layers.\nRandom augment Random augment [12] randomly ap-\nplies data augmentation transformations sequentially to an\ninput example. We follow the public implementation 1, but\nmodify the data augmentation operations to be temporally\nconsistent throughout the video (in other words, the same\ntransformation is applied on each frame of the video).\nThe authors deﬁne two hyperparameters for Random\naugment, “number of layers” , the number of augmentation\ntransformations to apply sequentially to a video and “mag-\nnitude”, the strength of the transformation that is shared\nacross all augmentation operations. Our values for these\nparameters are shown in Tab. 7.\nLabel smoothing Label smoothing was proposed by [61]\noriginally to regularise training Inception-v3. Concretely,\nthe label distribution used during training, ˜y, is a mixture\nof the one-hot ground-truth label, y, and a uniform distribu-\ntion, u, to encourage the network to produce less conﬁdent\npredictions during training:\n˜y= (1−λ)y+ λu. (11)\nThere is therefore one scalar hyperparamter, λ∈[0,1].\n1https://github.com/tensorflow/models/blob/\nmaster/official/vision/beta/ops/augment.py\nMixup Mixup [82] constructs virtual training examples\nwhich are a convex combination of pairs of training exam-\nples and their labels. Concretely, given (xi,yi) and (xj,yj)\nwhere xi denotes an input vector and yi a one-hot input la-\nbel, mixup constructs the virtual training example,\n˜x= λxi + (1−λ)xj\n˜y= λyi + (1−λ)yj. (12)\nλ ∈ [0,1], and is sampled from a Beta distribution,\nBeta(α,α). Our choice of the hyperparameter αis detailed\nin Tab. 7.\nA.2. Training hyperparameters\nTable 7 details the hyperparamters for all of our ex-\nperiments. We use synchronous SGD with momentum, a\ncosine learning rate schedule with linear warmup, and a\nbatch size of 64 for all experiments. As aforementioned,\nwe only employed additional regularisation when training\non the smaller Epic Kitchens and Something-Something v2\ndatasets.\nTable 7: Training hyperparamters for experiments in the main paper. “–” indicates that the regularisation method was not used at all. Values\nwhich are constant across all columns are listed once. Datasets are denoted as follows: K400: Kinetics 400. K600: Kinetics 600. MiT:\nMoments in Time. EK: Epic Kitchens. SSv2: Something-Something v2.\nK400 K600 MiT EK SSv2\nOptimisation\nOptimiser Synchronous SGD\nMomentum 0.9\nBatch size 64\nLearning rate schedule cosine with linear warmup\nLinear warmup epochs 2.5\nBase learning rate 0.1 0.1 0.25 0.5 0.5\nEpochs 30 30 10 50 35\nData augmentation\nRandom crop probability 1.0\nRandom ﬂip probability 0.5\nScale jitter probability 1.0\nMaximum scale 1.33\nMinimum scale 0.9\nColour jitter probability 0.8 0.8 0.8 – –\nRand augment number of layers [12] – – – 2 2\nRand augment magnitude [12] – – – 15 20\nOther regularisation\nStochastic droplayer rate, pdrop [31] – – – 0.2 0.3\nLabel smoothing λ[61] – – – 0.2 0.3\nMixup α[82] – – – 0.1 0.3"
}