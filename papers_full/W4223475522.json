{
  "title": "A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis",
  "url": "https://openalex.org/W4223475522",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3080521052",
      "name": "Ehsan Hosseini Asl",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2115644308",
      "name": "Wenhao Liu",
      "affiliations": [
        "Salesforce (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2095665791",
      "name": "Caiming Xiong",
      "affiliations": [
        "Salesforce (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962741379",
    "https://openalex.org/W2251294039",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3206646281",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4287815000",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W2949998441",
    "https://openalex.org/W3114242174",
    "https://openalex.org/W66373487",
    "https://openalex.org/W4287591332",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W4385414156",
    "https://openalex.org/W3196692796",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W3093495200",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2951278869",
    "https://openalex.org/W2962990575",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W156016533",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W2903110172",
    "https://openalex.org/W4288029087",
    "https://openalex.org/W3123091093"
  ],
  "abstract": "Sentiment analysis is an important task in natural language processing. In recent works, pre-trained language models are often used to achieve state-of-the-art results, especially when training data is scarce. It is common to fine-tune on the downstream task, usually by adding task-specific layers on top of the model. In this paper, we focus on aspect-based sentiment analysis, which involves extracting aspect term, category, and predicting their corresponding polarities. In particular, we are interested in few-shot settings. We propose to reformulate the extraction and prediction tasks into the sequence generation task, using a generative language model with unidirectional attention (GPT2 is used unless stated otherwise). This way, the model learns to accomplish the tasks via language generation without the need of training task-specific layers. Our evaluation results on the single-task polarity prediction show that our approach outperforms the previous state-of-the-art (based on BERT) on average performance by a large margins in few-shot and full-shot settings. More importantly, our generative approach significantly reduces the model variance caused by low-resource data. We further demonstrate that the proposed generative language model can handle joint and multi-task settings, unlike previous work. We observe that the proposed sequence generation method achieves further improved performances on polarity prediction when the model is trained via joint and multi-task settings. Further evaluation on similar sentiment analysis datasets, SST-2, SST-5 and OOS intent detection validates the superiority and noise robustness of generative language model in few-shot settings.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 770 - 787\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nA Generative Language Model for Few-shot Aspect-Based Sentiment\nAnalysis\nEhsan Hosseini-Asl, Wenhao Liu, Caiming Xiong\nSalesforce AI Research\n{ehosseiniasl,wenhao.liu,cxiong}@salesforce.com\nAbstract\nSentiment analysis is an important task in nat-\nural language processing. In recent works,\npre-trained language models are often used to\nachieve state-of-the-art results, especially when\ntraining data is scarce. It is common to fine-\ntune on the downstream task, usually by adding\ntask-specific layers on top of the model. In\nthis paper, we focus on aspect-based sentiment\nanalysis, which involves extracting aspect term,\ncategory, and predicting their corresponding po-\nlarities. In particular, we are interested in few-\nshot settings. We propose to reformulate the ex-\ntraction and prediction tasks into the sequence\ngeneration task, using a generative language\nmodel with unidirectional attention (GPT2 is\nused unless stated otherwise). This way, the\nmodel learns to accomplish the tasks via lan-\nguage generation without the need of training\ntask-specific layers. Our evaluation results on\nthe single-task polarity prediction show that\nour approach outperforms the previous state-\nof-the-art (based on BERT) on average perfor-\nmance by a large margins in few-shot and full-\nshot settings. More importantly, our generative\napproach significantly reduces the model vari-\nance caused by low-resource data. We further\ndemonstrate that the proposed generative lan-\nguage model can handle joint and multi-task\nsettings, unlike previous work. We observe\nthat the proposed sequence generation method\nachieves further improved performances on po-\nlarity prediction when the model is trained via\njoint and multi-task settings. Further evaluation\non similar sentiment analysis datasets, SST-2,\nSST-5 and OOS intent detection validates the\nsuperiority and noise robustness of generative\nlanguage model in few-shot settings.\n1 Introduction\nSentiment analysis (Pang et al., 2002; Turney,\n2002; Chevalier and Mayzlin, 2006; Bastan et al.,\n2020) aims at detecting the overall polarity of a user\ngenerated text, which describes the user opinion\nfor an entity. However, user may express opin-\nions about an entity at different granularity. For\nexample, a user may give an overall rate about a\nrestaurant service, and then explains fine-grained\nreview about specific aspects, such as food qual-\nity, waiting time, waitress service, environment,\netc. Aspect-based sentiment analysis task (Pontiki\net al., 2014, 2016) aims at addressing this prob-\nlem, where user sentiment is annotated at coarse\nand fine-grained levels. Moreover, user can ex-\npress conflicting opinions for different aspects of\nan entity.\nTraditionally, neural-based models are employed\nas a single-task model for aspect-based sentiment\nanalysis (ABSA) task, similar to Machine Read-\ning Comprehension task (MRC) (Rajpurkar et al.,\n2016). For example, a pre-trained BERT language\nmodel is fine-tuned for ABSA term polarity predic-\ntion (single-task) as a classifier. In this approach,\na task-specific layer is fine-tuned for each down-\nstream task, such as a layer for aspect term polarity\nclassification, and a different layer for aspect term\nspan extraction (Xu et al., 2019).\nRecently, generative language models with uni-\ndirectional self-attention, which are pre-trained by\ncausal language modeling loss (predicting next\nword given the history), have shown promising per-\nformance when fine-tuned on the downstream tasks\n(GPT2) (Radford et al., 2018). Using this approach,\nthe language model learns the downstream task as\nlanguage generation, where the task is represented\nas a serialized text. Moreover, Brown et al. (2020)\nproposed GPT3, a large-scale generative language\nmodel with few-shot ability. GPT3 learns to solve\nthe downstream task by conditioning on few exam-\nples in the prompt, without any parameter update\n(in-context learning).\nMotivated by the ability of the pre-trained gener-\native language model (GPT2) for solving the down-\nstream tasks in a generative manner, we propose\na generative language model for ABSA task. The\n770\nevaluation results indicate that the proposed ap-\nproach achieves better performance with signifi-\ncantly lower variance compared to the previous\nstate-of-the-art models (which are based on BERT\npre-trained model) on few-shot and full-shot set-\ntings, for single-task polarity prediction of aspect\nterm and aspect category. For example, using 1%\n(20 examples) of training data on restaurant do-\nmain for aspect term polarity prediction task, our\nproposed GPT2 model outperforms BERT-PT (Xu\net al., 2019) by 9 points on average accuracy and\nreduced standard deviation by 6.2 points, as shown\nin Figure 1(a). Moreover, when fine-tuned on mul-\ntiple tasks, such as aspect term extraction, term\npolarity, aspect category detection, and category\npolarity, the proposed model improved single-task\nperformance, such as aspect term extraction (mea-\nsured by F1 score). 1\nThe contributions of our proposed generative\nlanguage model are,\n• A robust generative model on few-shot aspect-\nbased sentiment analysis by reformulating the\ntask as language generation. This allows us\nto use uni-directional language model with\nno additional head for the downstream tasks,\nwhich outperforms the previous state-of-the-\narts on average performance by a large mar-\ngin, with no additional pretraining on out-of-\ndomain data (such as BERT-PT (Xu et al.,\n2019)).\n• Our proposed generative model reduces vari-\nance in polarity prediction, caused by low re-\nsource data and random noise, in all few and\nfull-shot settings by large value.\n• Joint and multi-task training can further im-\nprove the single-task few-shot performances,\nsuch as aspect term extraction.\n• More evaluation on similar sentiment analysis\ntasks (SST-2, SST-5, OOS intent detection)\nprovides further evidence of the superiority\nand robustness of generative language model.\nIn the next sections, we discuss the proposed\nmodel and presents the evaluation results. In sec-\ntion 2, the previous state-of-the-arts are described.\nSection 3 explains the task of aspect-based sen-\ntiment analysis (ABSA) (section 3.1) followed\n1Code is available at https://github.com/\nsalesforce/fewshot_absa\nby reformulating ABSA task as language gener-\nation (section 3.2). In section 4, the evaluation\nresults for single, joint and multi-task settings are\npresented for SemEval14 (Pontiki et al., 2014) and\nSemEval16 (Pontiki et al., 2016) and SST-2, SST-5\nand OOS intent detection datasets.\n2 Related Works\nSentiment analysis is characterized by three cat-\negorizes, i.e. document, sentence, and aspect\nlevel (Liu, 2012; Liu and Zhang, 2012; Cambria\nand Hussain, 2012). In this section, we review\nthe previous models developed for aspect-based\nsentiment analysis (ABSA) (Hu and Liu, 2004).\nEarlier works on ABSA task focused on devel-\noping feature engineered models (Samha et al.,\n2014). Xu et al. (2018) proposed a model based\non using convolutional neural network (CNN) for\naspect term extraction task only. The approach uses\ntwo types of pre-trained embeddings, a general-\npurpose embedding and a domain-specific one.\nThen, a softmax classification layer is used to clas-\nsify each word to identify aspect term start and end\npositions, or non-related words.\nLi et al. (2019) proposed Multi-granularity\nAlignment Network (MGAN), a coarse-to-fine\napproach for single-task aspect term polar-\nity prediction using recurrent neural network\n(RNN) (Hochreiter and Schmidhuber, 1997). They\ndefined aspect category as coarse-level and aspect\nterm as fine-level sentiments, and further leveraged\nhigh-resource out-of-domain data for pre-training.\nThis way, the knowledge is transferred from coarse-\ngrain domains (single-opinion prediction) to multi-\ngrain domains (ABSA task).\nWith the advent of BERT (Devlin et al., 2018) as\na pre-trained bidirectional language model, which\npresents a powerful contextualized word represen-\ntation for the language understanding downstream\ntasks, several models are proposed for ABSA task\nusing BERT as feature extraction. Xu et al. (2019)\ndefined ABSA task as question answering (Ra-\njpurkar et al., 2016), named Review Reading Com-\nprehension (RRC), and used BERT as the base\nmodel, with separate heads for aspect term extrac-\ntion (as span extraction) and term polarity predic-\ntion. To enhance RRC performance, they intro-\nduced a post-training algorithm, which additionally\npre-train the model on out-of-domain data from\nAmazon and Yelp review datasets, and additionally\non MRC question answering dataset (Rajpurkar\n771\net al., 2016). These result in additional training\nset of 1,151,863 for laptop domain, 2,677,025\nmore examples for restaurant domain, and 87,599\ntraining examples from MRC dataset.\nKarimi et al. (2020) proposed an approach based\non conditional random field (CRF) (Lafferty et al.,\n2001), combined with BERT for aspect term ex-\ntraction and term polarity prediction tasks. Two\nmodules are employed for improving aspect term\nextraction and term polarity prediction of BERT\nmodel. First, a parallel approach is used which\ncombines predictions for aspect term and polarity\nfrom last four layers of BERT in parallel. Moreover,\na hierarchical aggregation module is also examined,\nwhere predictions of previous layers of BERT are\nfed into the next layer. Reddy et al. (2020) com-\nbines GLOVE pre-trained embedding (Pennington\net al., 2014) with deep contextualized representa-\ntion of BERT to enhance the representation of word\nvectors for predicting aspect term polarity. The pro-\nposed BERT-IL model predicts aspect term polarity\nby learning a similarity between GLOVE vector of\naspect term and its contextualized representation\nextracted from BERT. First, the aspect term rep-\nresentations are extracted from multiple layers of\nBERT, and fed into a self-attention layer. Finally, it\nis further fine-tuned on ABSA task for performance\nimprovement. Liu et al. (2021) proposed a model\nbased on BART (Lewis et al., 2020) for aspect cat-\negory detection. They rank all aspect categories\nwith different polarities and select the pair with\nhighest score. Seoh et al. (2021) proposed an NLI\napproach based on BERT for single task of polarity\nprediction only, using extra pretraining on review\ndatasets. In section 4, evaluation of our proposed\ngenerative language model are compared with the\nrecent BERT-based models.\n3 Model\nThis section describes aspect-based sentiment anal-\nysis task (ABSA), the proposed generative lan-\nguage model approach, details of the datasets,\nmodel training, and evaluation metrics.\n3.1 Aspect Based Sentiment Analysis\nAspect-based sentiment analysis (ABSA) is sim-\nilar to sentiment analysis, in the sense that the\ntask is to predict the polarity of an entity in a sen-\ntence. However, it is different, since the goal is to\npredict fine-grained sentiment of multiple aspect\nterms and categories of an entity. The task was\nfirst introduced in Semantic Evaluation Challenge\n(SemEval14) (Pontiki et al., 2014). It was then\nextended in SemEval16 challenge (Pontiki et al.,\n2016). The challenges comprise of two domains,\nrestaurant and laptop, where each domain spans\nover four sub-tasks (SB1-4).\nAspect Term Extraction (SB1)For a given re-\nview sentence, this sub-task is about predicting\nall aspects terms (word span) that opinions are\nexpressed. It requires that all aspect terms to be\npredicted, including those which no opinion is ex-\npressed (neutral sentiment). This sub-task (AE)\ncorresponds to sub-task 1 (SB1) - single sentence –\nslot 2 in SemEval16 challenge, named as opinion\ntarget expression (OTE) (Pontiki et al., 2016).\nAspect Term Polarity (SB2)For a given review\nsentence and an aspect term, the goal is to predict\nthe polarity of the expressed opinion (positive,\nnegative, neutral, conflict). This sub-\ntask corresponds to SB1-Slot3 in SemEval16 chal-\nlenge.\nAspect Category Detection (SB3) Given\na set of pre-defined aspect categories (e.g.\nPRICE, FOOD, SERVICE, AMBIENCE,\nANECDOTE/MISCELLANEOUS), the goal\nis to predict all categories that an opinion\nis expressed about. This sub-task corre-\nsponds to SB1-Slot1 (single-sentence) in\nSemEval16 challenge, where the category is\ndefined as the pair of entity and attribute, e.g.\nRESTAURANT#PRICE, FOOD#QUALITY,\nLAPTOP#GENERAL, LAPTOP#PRICE. Please\nrefer to Table 4 in the appendix for the full list of\ncategories for laptop and restaurant domains.\nAspect Category Polarity (SB4) Given a re-\nview sentence and a category, the goal is to pre-\ndict the sentiment of the category ( positive,\nnegative, neutral, conflict). This sub-\ntask corresponds to SB1-Slot3 in SemEval16 (Pon-\ntiki et al., 2016).\n3.2 Generative Language Modeling\nABSA task comprises of four sub-tasks: aspect\nterm extraction, aspect category detection, and as-\npect term and category polarity predictions. The\ndominant approach for solving ABSA task is to\ntrain separate classifiers for each sub-task (Xu et al.,\n2019). In this paper, we propose to solve all sub-\ntasks using a single auto-regressive (generative)\nlanguage model, either using single-task or joint-\ntask training.\n772\n3.2.1 Language model\nThe goal of generative language modeling is\nto learn data distribution p(x), where x =\n(x1,...,x n) is a sequence of n symbols. In or-\nder to model p(x), the language model factorizes\nthe distribution of a single sequence p(x) using the\nchain rule of probability (Bengio et al., 2003), and\ntraining a neural network, which is parameterized\nby θ, by minimizing the negative log-likelihood,\npθ(x) =\nn∏\nt=0\npθ(xt|x<t) (1)\nLD = −\nK∑\nk=1\nn∑\nt=1\nlog pθ(xk\nt|xk\n<t) (2)\nDuring inference, the generative model sequen-\ntially generates tokens by conditioning on the input\nexample xk, and the past generated tokens.\n3.3 ABSA task as generative language\nmodeling\nEach ABSA task training example, xk, contains a\nsentence Sk, I pairs of aspect term and term po-\nlarity, and J pairs of aspect category and category\npolarity,\nTk = {TPk\ni = (tk\ni,ptk\ni); i∈I} (3)\nCk = {CPk\nj = (ck\nj,pck\nj); j ∈J} (4)\nwhere tk\ni, ptk\ni, and TPk\ni are i-th aspect term, term\npolarity, and their pair. Moreover,ck\nj and pck\nj, and\nCPk\nj are j-th aspect category, category polarity,\nand their pair of k-th sentence.\n3.3.1 Single-Task Polarity Prediction\nThis task consists of predicting the polarity of as-\npect terms or aspect categories only (named as SB2\nand SB4 in section 3.1). To generate polarity during\nthe inference, the input to the generative language\nmodel (LM) comprises of k-th sentence and the\ncorresponding aspect term or category,\nptk\ni = LMterm(Sk,tk\ni) (5)\npck\nj = LMcategory(Sk,ck\nj) (6)\nwhere LMterm refers to a model that trained on as-\npect term dataset, and LMcategory refers to aspect\ncategory dataset, respectively. The details of train-\ning language model are described in section 3.3.3.\nMoreover, the details of input sequence formula-\ntion during training and inference are presented in\nAppendix A and Tables 3 and 5.\n3.3.2 Joint and Multi-Task Prediction\nThis task includes generating pairs of aspect term\nand term polarity, or pairs of aspect category and\ntheir polarity. To jointly generate aspect terms and\ntheir polarities, the model input relies on the review\nsentence Sk only, and the model outputs all aspect\nterm and polarity pairs in token-by-token (auto-\nregressive) generation,\nTk = LMterm(Sk) (7)\nCk = LMcategory(Sk) (8)\nwhere Tk is the set of aspect term and polarity pairs,\nEq. (3), and Ck is the set of aspect category and\npolarity pairs, Eq. (4). The same method in joint-\ntask prediction can be used to generate all pairs\nof aspect term and aspect category, i.e. multi-task\nprediction,\n[Tk; Ck] =LMmulti(SK) (9)\nIn this case, during training, the model learns to\ngenerate Ipairs of aspect term andJpairs of aspect\ncategory via language model training, Eq. (1).\n3.3.3 Training\nA training sequence for solving each sub-tasks\n(SB1-4) of section 3.1, consists of the review sen-\ntence, concatenated by the corresponding aspect\nterm/category and its polarity. For example, in\ntraining LMterm for predicting aspect term polarity\n(Eq. 5) and joint-task prediction of aspect term and\npolarity (Eq. 7), the training sequence comprises of\nthe review sentence concatenated by aspect terms\nand their polarities, xk = [Sk; Tk]. Respectively,\nxk = [Sk; Ck] is used for training LMcategory, as\nmentioned in Eq. (6) and (8). For more details\non input sequence representation, see Appendix A,\nTables 3 and 5.\nIn order to train LMterm, the model can be\ntrained on different training sequences, where the\nreview sentence Sk needs to only be concatenated\nwith a single pair of aspect term and polarity. In this\ncase, multiple training sequences are created for the\nk-th sentence, i.e. {xk\ni = [Sk; TPk\ni ]; i ∈I}. We\nwill present an ablation study on these two meth-\nods of sequence creation for the language model\ntraining, and its effect on few-shot and full-shot\nperformances, are presented in Appendix C and\nFigure 4.\n773\n(a)\n (b)\n (c)\n(d)\n (e)\n (f)\nFigure 1: Single-Task polarity prediction (SB2 and SB4 sub-tasks), in few and full-shot settings. Note: 1-shot refers\nto one example per class, for aspect category, and1% is percentage of training data for aspect term. Lines represents\nmean accuracy, and shaded area are standard deviation of experiments with 4 different random seeds. (best viewed\nin color)\nDataset Domain Train Dev Test\nSemEval 14 Restaurant 3041 - 800\nLaptop 3045 - 800\nSemEval 16 Restaurant 2000 - 676\nLaptop 2500 - 808\nSST-2 Movie 66749 872 1821\nSST-5 Movie 8544 1101 2210\nOOS Misc. 15100 3100 4500\nTable 1: Dataset distribution\n3.4 Dataset\nThe proposed generative language model is evalu-\nated on the two datasets proposed for ABSA task.\nSemEval14 challenge (Pontiki et al., 2014) consists\nof four sub-tasks as described in section 3.1. We\nalso evaluate the proposed model on task 5 of Se-\nmEval16 (Pontiki et al., 2016), which contains two\nsub-tasks for sentence and text level review data in\nmultiple languages. In this paper, we only focus on\nthe English language of sub-task 1 (sentence level)\nto be able to compare with the prior arts.\nMoreover, we evaluate on Stanford Sentiment\nTreebank (SST) dataset (Socher et al., 2013) for\nbinary (SST-2) and fine-grained (SST-5) sentiment\nclassification of movie reviews domain. Since in-\ntent detection is a similar task to sentiment analysis,\nthe evaluation is also performed on out-of-scope\n(OOS) intent detection dataset (Larson et al., 2019)\nwhich created for chatbot systems.\nTo evaluate the performance on few-shot setting,\nwe sub-sample training set for aspect term and as-\npect category domains. For aspect term, the train\nset is randomly sub-sampled to the smaller sizes,\n[1%,5%,10%,20%]. For example, 1% few-shot\ntrain set contains only about ≈20 sentences. For\naspect category, since there is the predefined set of\ncategories, we randomly sub-sample examples for\neach category, with different number of examples\nof [1,5,10,20].\nThe distribution of the train, dev and test splits\nfor each domain are shown in Table 1. It is note-\nworthy that the previous baselines have created\ncustomized validation set from train set. Since no\nofficial validation set is released for SemEval14\nand SemEval16, and in order to have a unified\nevaluation, we used the official trial set (part\nof train set) for validation, and exclude those ex-\namples from the train set. Moreover, prior works\nexcluded examples with conflict polarity from\ntheir evaluations, since it is considered a difficult\nprediction task. However, for more accurate evalua-\ntion, these examples are retained in our evaluation.\n3.5 Evaluation\nPerformance evaluation of aspect term polarity\n(SB2) and aspect category polarity (SB4) single-\ntasks in Eq. (5) and Eq. (6) are based on accuracy\nmetric. It is measured by counting the number of\naspect term and aspect category polarities which\nare correctly predicted. The evaluation of aspect\nterm extraction (SB1) and aspect category detection\n(SB3) are measured by F1 metric (Pontiki et al.,\n774\n2014) computed on the overlap of the ground-truth\nand generated sequences. The evaluation of SST-2,\nSST-5 and OOS datasets are measured by accu-\nracy metric. On OOS dataset, full accuracy on in-\ndomain and out-of-scope examples are measured.\nEvaluation of joint and multi-task models in\nEq. (7)(8)(9) are measured by joint accuracy. This\nmeans that for an example sentenceSk, if all the as-\npect term and term polarity predictions are correct,\nit is assumed as a correct prediction.\nThe restaurant domain contains both aspect term\nand aspect category annotations for SemEval14 and\nSemEval16. However, the laptop domain only con-\ntains aspect term annotation for SemEval14, and\naspect category annotation for SemEval16. There-\nfore, single-task evaluation on laptop domain is\nconstrained and multi-task prediction performance\ncan only be evaluated on restaurant domain.\n4 Experiments\nThe proposed generative language model is eval-\nuated on five tasks. Single-task setting includes\naspect term polarity and aspect category polarity\nprediction, Eq. (5)(6), for restaurant and laptop do-\nmains. Joint-task includes a) aspect term extraction\nand polarity Eq. (7) and b) aspect category detec-\ntion and polarity Eq. (8). Finally, multi-task setting\ncomprises all sub-tasks, i.e. aspect term extraction\n(SB1), aspect category detection (SB3), and their\npolarity predictions (SB2 and SB4), Eq. (9).\nThe evaluation of our proposed generative lan-\nguage model is compared with recent BERT-\nPT (Xu et al., 2019) model. We have reproduced\nresults of BERT-PT on full-shot settings, since we\ninclude examples with conflict polarity. Other\nBERT-based models such as BERT-IL (Reddy et al.,\n2020) has not open-sourced code, and therefore\nthey are not included in few-shot evaluation.\n4.1 Single-Task Polarity evaluation\nIn this section, the proposed generative language\nmodel is evaluated on aspect term and aspect cate-\ngory polarity prediction for both restaurant and lap-\ntop domains. As shown in Figure 1, the proposed\nmodel, based on GPT2-base, outperforms BERT\non few- and full-shot settings on all sub-tasks (SB2\nand SB4) for SemEval14 and SemEval16. More\nimportantly, GPT2 model has lower variance than\nBERT, especially in1% or 1-shot setting.\nIt is shown that BERT average performance\ndrops by a large margin on low-resource regimes\n(< 5% or < 5 shot) and with increased variance,\nwhereas our proposed generative model shows ro-\nbust performance on few-shot setting with small\nvariance. Compared to BERT-PT (Xu et al., 2019),\nwhich exploits additional pre-training on review\ndata from Amazon and Yelp datasets, and using\nauxiliary tasks of MRC, generative model with\nmore layers (GPT2-medium) and no additional pre-\ntraining matches or outperforms BERT-PT aver-\nage performance in few-shot setting with smaller\nvariance. Interestingly, GPT2-base model (12 lay-\ners) outperforms BERT-PT average performance in\nsome cases, including all 1% and 1-shot settings\nwith reduced variance. For example, GPT2-base\noutperforms by a large margin, 16.75 points on av-\nerage accuracy and reduces standard deviation by\n8.8 points on 1%-shot setting of category polarity\nprediction in restaurant domain of SemEval16, Fig-\nure 1(e). Moreover, GPT2-base outperforms BERT-\nPT in all few- and full-shot settings on aspect cat-\negory polarity prediction task (SB4) of restaurant\ndomains in SemEval16 dataset, Figure 1(f).\nAlthough GPT2-medium average performance\nmostly outperforms BERT-PT, there are some ex-\nceptions, such as Figure 1(a) for full-shot, Fig-\nure 1(c) for 5%-shot, Figure 1(d) for 20% and full-\nshot. On the other hand, BERT-PT has much larger\nvariance and less robustness in all few- and full-\nshot settings. This is perhaps due to the use of\nout-of-domain data in additional pre-training of\nBERT-PT which results in higher variance, even\nthan BERT baseline, when finetuned on few-shot\ndownstream tasks. The goal of our proposed model\nis not to simply outperforms BERT-PT by addi-\ntional pre-training, but to provide a robust model\nfor few-shot setting.\nMore evaluation on sentiment polarity predic-\ntion on SST5, SST2 and OOS intent detection\ndatasets are presented in Figure 2, Appendix G and\nFigure 8. They indicate that generative language\nmodel outperforms BERT-based classifier models.\nOverall, the results of single-task polarity predic-\ntion indicate that our proposed generative model\nbased on language generation (uni-directional self-\nattention) have better performance than the discrim-\ninative models which uses BERT (bi-directional\nself-attention) as encoder.\n4.2 Joint and Multi-Task evaluation\nIn this section, the proposed generative model is\nevaluated for joint and multi-task prediction. It\nincludes solving two sub-tasks jointly, e.g. aspect\nterm extraction and term polarity prediction, or\n775\nMethod Training Task Model Restaurant Laptop\nJoint Accuracy SB1 (F1) Joint Accuracy SB1 (F1)\nDiscriminative Single (SB1)\nMGAN - 71.48 - 71.42\nBERT - 74.1 - 79.28\nBERT-DK - 77.02 - 83.55\nBERT-MRC - 74.21 - 81.06\nBERT-PT - 77.97 - 84.26\nBERT-PSUM - - - 85.94\nBERT-HSUM - - - 86.09\nGenerative\nJoint (SB1&2) GPT2 (base) 56.47±0.82 77.59±0.32 50.65±1.04 72.61±1.03\nGPT2 (medium) 60.07±0.52 81.52±0.8 53.55±0.43 75.94±0.17\nMulti (SB1-4) GPT2 (base) 49.84±1.03 77.92±0.53 - -\nGPT2 (medium) 54.43±0.47 82.04±0.21 - -\nTable 2: SemEval14 SB1 and SB2 sub-tasks for restaurant and laptop domains. Comparing joint and multi-task\ngenerative model with single-task BERT baselines for full-shot setting.\naspect category detection and category polarity pre-\ndiction, Eqs. (7)(8), or predicting all Eqs. (9). Since\nBERT and BERT-PT are single-task models, which\nrequired to use different heads for each sub-task,\nwe can not directly compare our joint-task model\nwith these baselines on joint-accuracy metric. For\nexample, BERT-PT uses groundtruth aspect term\nto evaluate on polarity prediction (SB2), which\nis not comparable to our joint-task model which\ngenerates aspect term and polarity jointly.\nResults in Table 2 indicate that although gen-\nerative model is trained in joint-task manner, for\npredicting aspect term extraction and term polar-\nity, it still outperforms BERT-PT and other BERT\nbaselines which are trained to solve single-task\naspect term extraction only, on aspect term ex-\ntraction (SB1) metric, in restaurant domain. How-\never, in laptop domain, the generative model under-\nperforms BERT-based models on aspect term ex-\ntraction (SB1) metric, perhaps due to less training\ndata in laptop domain for joint-task loss.\nAspect category sub-tasks improve aspect term\nextraction: In multi-task setting, where genera-\ntive model is trained on all sub-tasks (SB1-4), the\naspect term extraction (SB1) F1 metric is improved\nmore, compared to when trained as a single-task\nmodel. This indicates that training the generative\nmodel using extra supervision (from aspect cate-\ngory) helps to extract multiple aspect terms in the\nreview sentence more accurately.\nGenerative language modeling is better for\nmulti-task learning: Evaluation results on Se-\nmEval14 restaurant domain are shown in Ap-\npendix B Table 6. Combined with the results from\nTable 2, it indicates that the proposed generative\nlanguage model performs well on solving all sub-\ntasks (SB1-4) using language generation. For ex-\nample, compared to joint-task setting (Table 2),\naspect term extraction (SB1) F1 metric improves\nmore for restaurant domain. Multi-task evaluation\nresults on SemEval16 restaurant domain are shown\nin Appendix B Table 7 for reference.\nFigure 2: Few-shot evaluation on SST5 dataset. Note:\n1-shot refers to one example per class. (best viewed in\ncolor)\n4.3 Ablation\nIn this section, the ablation study of proposed gen-\nerative language model is studied on two aspects.\nFirst, using the language model (GPT2) as a dis-\ncriminative classifier vs. for language generation.\nSecond, we study the training convergence of gen-\nerative model with two discriminative baselines, i.e.\nBERT and GPT2 as classifier to better understand\nfew-shot performance.\nGenerative vs. Discriminative training of unidi-\nrectional language model: To analyze the bene-\nfit of fine-tuning GPT2 using language modeling\nloss, we also fine-tune it as a classifier. In the latter\ncase, a classification layer is added, which uses\nthe output of the last token of the input sequence\nfor polarity prediction. As shown in Figure 3(c),\nGPT2-classifier under-performs BERT, when only\ntrained with discriminative loss. We conjecture that\nsince GPT2 uses uni-directional self-attention (left-\nto-right), it captures less contextualized represen-\ntation, compared to bidirectional self-attention in\nBERT. On the other hand, when fine-tuning GPT2\nusing generative loss (next word prediction), uni-\ndirectional self-attention learns a better representa-\n776\n(a)\n (b)\n (c)\nFigure 3: Analysis of few-shot training convergence, evaluated on SemEval14 aspect term polarity prediction (SB2)\non restaurant domain for 1% training data. GPT2-classifier model uses a classification layer on the output of last\ninput token without using language modeling loss for training. Note: Lines represents mean value, and shaded area\nare standard deviation of experiments with 4 random seeds. (best viewed in color)\ntion, which improves few-shot performance. Abla-\ntion analysis on laptop domain and aspect category\npolarity predictions for both domains are shown in\nAppendix D and Figures 5 and 6.\nGPT2 language model exploits more supervi-\nsion than BERT in few-shot setting: To un-\nderstand the training dynamics of generative lan-\nguage model and its relation to few-shot perfor-\nmance, we investigate the training convergence for\nGPT2, BERT, and GPT2-classifier. Results for\nSemEval14 restaurant aspect term polarity predic-\ntion are shown in Figure 3. It is indicated that\nBERT model converges faster than GPT2 in 1%\nfew-shot settings, due to using a small classifica-\ntion head (fully-connected layer with 4 outputs)\nfor the downstream task, which perhaps makes the\nmodel to overfits quickly to few-shot training data.\nOn the other hand, GPT2 converges more slowly,\nperhaps due to using language modeling loss, i.e.\ncross-entropy loss across all tokens of the input\nsequence, and also using output layer with size of\nthe vocabulary. However, the cross-entropy loss\non the position corresponding to predicting label,\ngpt2-generative (label position), converges faster\nthan BERT, early in training, and the loss value is\nsmaller than BERT between 40-90 steps, where the\nmodel has better validation accuracy than BERT.\nLater during the training, BERT training loss con-\nverges to smaller values, but its performance does\nnot outperform GPT2. This is perhaps an evidence\nof BERT model overfitting due to using a small\nclassification head which is specifically designed\nfor the downstream task (4 output nodes).\nSince the language modeling loss benefits GPT2\nmodel to exploit more supervision during training\n(predicting next token for all input tokens), perhaps\nthis helps GPT2 to be less prune to overfitting, and\noutperforms BERT in few-shot setting. Addition-\nally, reformulating the task as natural text might\nbenefits GPT2 to infer the sentiment polarity easier\nthan BERT. Overall, GPT2 validation and test ac-\ncuracy achieves higher performance. Analysis of\ntraining convergence on other tasks and domains\nare presented in Appendix E, Figures 5 and 6.\nWe also investigates model weights change dur-\ning fine-tuning by measuring the average of the\nnormalized weight update, Eq. (10), for each layer\n(more details are presented in Appendix F and Fig-\nure 7). It is shown that gpt2-generative model has\nhigher weight update in all layers at the end of\ntraining, and overall higher update in embedding\nlayer (by one to two order of magnitude). This ob-\nservation perhaps indicates that standard language\nmodeling loss provides more supervision to GPT2\nmodel, when finetuned on few-shot data.\n5 Conclusion\nIn this paper, we proposed a generative language\nmodel for aspect based sentiment analysis (ABSA).\nBy reformulating the task as language generation,\nthe model learns to predict aspects and their polari-\nties via language generation. Evaluation results on\nsingle-task polarity prediction on few and full shot\nsetting indicate that the proposed approach outper-\nforms prior arts, which are based on discriminative\nclassification using BERT as encoder, with higher\naverage performance and lower variance. On join-\ntask and multi-task settings, the proposed model\nshows better performance on single-task polarity\nprediction metrics. Additionally, evaluation results\non coarse-grained (SST2), fine-grained (SST5) sen-\ntiment analysis datasets, and OOS intent detection\ndataset indicate the better and more robust few-shot\nperformance of generative language model. Fur-\nthermore, qualitative analysis indicates that using\nmulti-task setting improves model prediction via\nsupervision across aspect term and category.\n777\n6 Broader Impact\nThis work may have implications for the simplifi-\ncation of sentiment analysis using neural text gen-\neration. In the narrow sense, this work addresses\naspect-based sentiment analysis. If so, the improve-\nment of neural text generation systems and eas-\nier deployment would amplify both the positive\nand negative aspects of sentiment analysis. On\nthe positive side, neural text generation models\nmight play a role in automating user opinion min-\ning, and thereby increasing efficiency of currently\nmodular systems. On the negative side, it can de-\nhumanize current systems, by automating systems\ntowards multi-tasking, and reducing the level of\nhuman control on language generation. Moreover,\nthis approach can introduce toxicity and biases into\nsentiment polarity predictions, such as gender, race,\nreligious, and ethics (Kiritchenko and Mohammad,\n2018; Park et al., 2018). This is due to biases which\nare learned during pretraining of neural text mod-\nels on internet data (Sheng et al., 2019; Tan and\nCelis, 2019). These consequences are not specific\nto this work, but should be considered by the field\nof natural language processing more broadly.\nReferences\nMohaddeseh Bastan, Mahnaz Koupaee, Youngseo Son,\nRichard Sicoli, and Niranjan Balasubramanian. 2020.\nAuthor’s sentiment prediction. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 604–615, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nErik Cambria and Amir Hussain. 2012. Sentic comput-\ning. marketing, 59(2):557–577.\nJudith A Chevalier and Dina Mayzlin. 2006. The effect\nof word of mouth on sales: Online book reviews.\nJournal of marketing research, 43(3):345–354.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9(8):1735–\n1780.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168–177.\nAkbar Karimi, Leonardo Rossi, and Andrea Prati. 2020.\nImproving bert performance for aspect-based senti-\nment analysis. arXiv preprint arXiv:2010.11731.\nSvetlana Kiritchenko and Saif M Mohammad. 2018.\nExamining gender and race bias in two hun-\ndred sentiment analysis systems. arXiv preprint\narXiv:1805.04508.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random fields: Proba-\nbilistic models for segmenting and labeling sequence\ndata.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nevaluation dataset for intent classification and out-of-\nscope prediction. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1311–1316.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880. Association for Computational\nLinguistics.\nZheng Li, Ying Wei, Yu Zhang, Xiang Zhang, and Xin\nLi. 2019. Exploiting coarse-to-fine task transfer for\naspect-level sentiment classification. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 33, pages 4253–4260.\nBing Liu. 2012. Sentiment analysis and opinion mining.\nSynthesis lectures on human language technologies,\n5(1):1–167.\nBing Liu and Lei Zhang. 2012. A survey of opinion\nmining and sentiment analysis. In Mining text data,\npages 415–463. Springer.\nJian Liu, Zhiyang Teng, Leyang Cui, Hanmeng Liu, and\nYue Zhang. 2021. Solving aspect category sentiment\nanalysis as a text generation task. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 4406–4416. As-\nsociation for Computational Linguistics.\n778\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan.\n2002. Thumbs up? sentiment classification us-\ning machine learning techniques. arXiv preprint\ncs/0205070.\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Reduc-\ning gender bias in abusive language detection. arXiv\npreprint arXiv:1808.07231.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nMaria Pontiki, Dimitrios Galanis, Haris Papageorgiou,\nIon Androutsopoulos, Suresh Manandhar, Moham-\nmad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao,\nBing Qin, Orphée De Clercq, et al. 2016. Semeval-\n2016 task 5: Aspect based sentiment analysis. In In-\nternational workshop on semantic evaluation, pages\n19–30.\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-\nris Papageorgiou, Ion Androutsopoulos, and Suresh\nManandhar. 2014. SemEval-2014 task 4: Aspect\nbased sentiment analysis. In Proceedings of the 8th\nInternational Workshop on Semantic Evaluation (Se-\nmEval 2014), pages 27–35.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2.amazonaws.com/\nopenai-assets/research-covers/langu\nageunsupervised/language_understand\ning_paper.pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nNatesh Reddy, Pranaydeep Singh, and Muktabh Mayank\nSrivastava. 2020. Does bert understand sentiment?\nleveraging comparisons between contextual and non-\ncontextual embeddings to improve aspect-based sen-\ntiment models. arXiv preprint arXiv:2011.11673.\nAmani K Samha, Yuefeng Li, and Jinglan Zhang. 2014.\nAspect-based opinion extraction from customer re-\nviews. arXiv preprint arXiv:1404.1982.\nRonald Seoh, Ian Birle, Mrinal Tak, Haw-Shiuan Chang,\nBrian Pinette, and Alfred Hough. 2021. Open aspect\ntarget sentiment classification with natural language\nprompts. arXiv preprint arXiv:2109.03685.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642.\nYi Chern Tan and L Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. arXiv preprint arXiv:1911.01485.\nPeter D Turney. 2002. Thumbs up or thumbs down?\nsemantic orientation applied to unsupervised classifi-\ncation of reviews. arXiv preprint cs/0212032.\nChien-Sheng Wu, Steven C.H. Hoi, Richard Socher,\nand Caiming Xiong. 2020. TOD-BERT: Pre-trained\nnatural language understanding for task-oriented di-\nalogue. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 917–929.\nHu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2018. Dou-\nble embeddings and cnn-based sequence labeling for\naspect extraction. arXiv preprint arXiv:1805.04601.\nHu Xu, Bing Liu, Lei Shu, and Philip S Yu. 2019.\nBert post-training for review reading comprehension\nand aspect-based sentiment analysis. arXiv preprint\narXiv:1904.02232.\n779\nA Input Representation and Method\nOverview\nAs described in Section 3.3.3, a single training se-\nquence consists of the concatenation of review sen-\ntence Sk with the corresponding aspect terms and\ntheir polarities xk =\n[\nSk; Tk]\n, or aspect categories\nand their polarities xk =\n[\nSk; Ck]\n.\nA schematic overview of each segment is shown\nin Table 3 together with special tokens marking\ntransition points. The generative language model\nis optimized by minimizing the negative likelihood\nover the joint sequence. The output state associ-\nated with each input token is used to predict the\nnext token. During inference, for single task polar-\nity prediction of each aspect term (sub-task SB1),\nthe language model input comprises the review\nsentence concatenated by the corresponding as-\npect term. The the model generates a single token,\nwhich assumed as predicted polarity. Same method\nis used for sub-task SB4 for aspect category polar-\nity prediction. For joint- and multi-task prediction,\nthe input sequence contains only the review sen-\ntence. The language model then generates aspect\nterms and aspect categories along with their polar-\nities in single toke-by-token generation, until the\nend-of-sentence special token is generated.\nExamples of different input sequence formatting\nfor different datasets evaluated in the paper are\npresented in Table 5. We are using identifiers to\nseparate different segments of the input sequence.\nFor example, to separate review sentence from as-\npect term, we introduced identifiers<|review|> and\n<|term|> to separate them. each segment also ends\nwith an end-of-segment identifier, such as <|end-\nofreview|> and <|endofterm|> identifiers. It is note-\nworthy that these identifiers are not special token,\nsimilar to BERT, which introduces new embed-\ndings into vocabulary. We have noticed that defin-\ning identifiers as special token will decrease the per-\nformance of generative language model, perhaps\ndue to introducing randomly-initialized embedding\nvectors into vocabulary, which requires more train-\ning data to finetune them. However, since GPT2\ndid not use special tokens during pretraining, using\nidentifiers which are combination of pretrained vo-\ncabulary tokens and special characters, such as {<,\n|, ,|, >}, helps GPT2 to understand different seg-\nments in the input sequence, to infer the sentiment\npolarity more accurately.\nB Multi-task prediction\nIn this section, evaluation results on SemEval 14\nand SemEval16 restaurant domain are presented\nfor multi-task learning using our proposed genera-\ntive language model, based GPT2-base model, in\nTables 6 and 7. For more details, please refer to\nsection 4.2.\nC Ablation: Model input sequence\nformatting\nFor a single review sentence with multiple aspect\nterms or categories, there are two ways to create\ninput sequence for language model training, as de-\nscribed in section 3.3.3. First, the review sentence\ncan be concatenated with each aspect terms sepa-\nrately ( GPT2-Split), which results in better per-\nformance for few-shot setting (Figure 4) There\nare very few example in few-shot setting, such\nas 20 unique examples in 1% setting, and using\nsplit method increases training data and perhaps\nmitigates model over-fitting. However, when the\nreview sentence is concatenated with all pairs of\naspect terms or categories in a single sequence, per-\nformance is better for full-shot setting. There are\nfew exceptions in Figure 4(a) for 1% and 5% shot\nsettings. We observe that 1% few-shot contains 20,\n14, 12 input sequences in Figure 4(a), (b), and (c),\nrespectively, for the regular method. However, the\nsplit method increases input training sequences to\n36, 23, 17. It means that when the number of train-\ning sequences are high enough, increasing number\nof training examples using split methods might\ndeteriorates the few-shot performance, as shown\nin Figure 4(a). We guess that the better few-shot\nperformance of the GPT2-Split method possibly de-\npends on the number of unique training sequences\nwhen comparing to the regular method. In other\nwords, the GPT2-Split methods might outperforms\nthe regular method when the number of training\nsequences is very low.\nD Ablation: Generative vs.\nDiscriminative language model\nIn this section, ablation analysis on using genera-\ntive language model as a classifier are presented in\nFigures 5 and 6. It is shown that when fine-tuning\nGPT2 model as a classifier on the downstream\ntask using an classification layer, it under-performs\nBERT model on few and full-shot settings. For\nmore details, please refer to section 4.3.\n780\nSentence Sk [review] review sentence [endofreview]\nAspect term Tk [term] term1 polarity1, term2 polarity2, . . . termI polarityI [end-\nofterm]\nAspect category Ck [category] category1 polarity1, category2 polarity2, . . . categoryJ\npolarityJ [endofcategory]\nAspect term single and\njoint task training se-\nquence (LMterm)\n[review] review sentence [endofreview] [term] term1 polarity1, . . .[end-\nofterm]\nAspect category single\nand joint task training se-\nquence (LMcategory )\n[review] review sentence [endofreview] [category] category1 polarity1,\n. . .[endofcategory]\nMulti-task training se-\nquence (LMmulti)\n[review] review sentence [endofreview] [term] term1 polarity1, . . .[end-\nofterm] [category] category1 polarity1, . . .[endofcategory]\nTable 3: A schematic representation of the different components of inputs/outputs in aspect-based sentiment analysis.\nWhen training generative language model, these are concatenated together into a single sequence, as shown in last\nthree rows.\nAspect Category\nDataset Domain Entity Attribute\nSemEval 14 Restaurant ambience, anecdotes miscellaneous,\nfood, price, service\nN/A\nLaptop N/A N/A\nSemEval 16\nRestaurant ambience, drinks, food, location, restau-\nrant, service\ngeneral, price, style, quality\nLaptop\nbattery, company, cpu, display, fans cool-\ning, graphics, hard disc, hardware, key-\nboard, laptop, memory, motherboard,\nmouse, multimedia devices, optical\ndrives, os, ports, power supply, shipping,\nsoftware, support, warranty\nmiscellaneous, operation performance,\nquality, general, design features, usabil-\nity, connectivity, portability, price\nTable 4: Ascpet category definition for SemEval14 and SemEval16 datasets. In Semeval14, each unique aspect\ncategory is defined as entity. For SemEval16, aspect category is defined as combination of entity and attribute.\nLaptop domain does not have annotation in SemEval14 dataset.\n(a)\n (b)\n (c)\nFigure 4: Ablation analysis on model input sequence formatting.GPT2 (split) means review sentence is concatenated\nwith each aspect terms separately. (best viewed in color)\nE Ablation: Training convergence\nIn this section, training convergence of GPT2\nmodel is compared with BERT and GPT2-classifier\nmodel in varios tasks of aspect-based sentiment\nanalysis. As shown in Figures 5 and 6, GPT2\nachieves higher validation accuracy, when its train-\ning losses, standard language modeling and loss\ncorresponding to label position, have higher value\n781\nDataset Task Type Input sequence\ntrain inference\nSemEval14 Single task aspect term\npolarity predic-\ntion\n<|review|> once we sailed, the top-notch\nfood and live entertainment sold us on a\nunforgettable evening. <|endofreview|>\n<|term|> food positive , live entertainment\npositive <|endofterm|>\n<|review|> once we sailed, the top-notch\nfood and live entertainment sold us on a\nunforgettable evening. <|endofreview|>\n<|term|> food\nSemEval14 Joint task aspect term <|review|> once we sailed, the top-notch\nfood and live entertainment sold us on a\nunforgettable evening. <|endofreview|>\n<|term|> food positive , live entertainment\npositive <|endofterm|>\n<|review|> once we sailed, the top-notch\nfood and live entertainment sold us on a\nunforgettable evening. <|endofreview|>\nSemEval14 Multi-task aspect term &\naspect category\n<|review|> the service was attentive with-\nout being overbearing and each dish we\ntried was wonderful from the spring rolls\nto the cod with pineapple tempura. <|end-\nofreview|> <|term|> service positive , dish\npositive , spring rolls positive , cod with\npineapple tempura positive <|endofterm|>\n<|category|> food positive , service posi-\ntive <|endofcategory|>\n<|review|> the service was attentive with-\nout being overbearing and each dish we\ntried was wonderful from the spring rolls\nto the cod with pineapple tempura. <|end-\nofreview|>\nSST-2 Single-task polarity predic-\ntion\n<|review|> does n’t try to surprise us with\nplot twists , but rather seems to enjoy its\nown transparency <|endofreview|> <|sen-\ntiment|> positive <|endofsentiment|>\n<|review|> does n’t try to surprise us with\nplot twists , but rather seems to enjoy its\nown transparency <|endofreview|> <|sen-\ntiment|>\nSST-5 Single-task polarity predic-\ntion\n<|review|> it ’s a lovely film with lovely\nperformances by buy and accorsi . <|end-\nofreview|> <|sentiment|> somewhat posi-\ntive <|endofsentiment|>\n<|review|> it ’s a lovely film with lovely\nperformances by buy and accorsi . <|end-\nofreview|> <|sentiment|>\nOOS Single-task intent predic-\ntion\n<|user|> how would you say fly in italian\n<|endofuser|> <|intent|> translate <|end-\nofintent|>\n<|user|> how would you say fly in italian\n<|endofuser|> <|intent|>\nTable 5: Examples of input sequence during training and inference of generative language model for different\ndatasets.\nShot Layers Joint Accuracy Term Category\nSB1 (F1) SB2 (Acc) SB3 (F1) SB4 (Acc)\n1% 12 20.75 39.26 19.69 62.82 43.4\n24 20.62 37.87 18.99 61.79 41.51\n5% 12 31 44.35 32.38 74.46 56.51\n24 34.87 60.4 35.18 75.39 59.06\n10% 12 38.37 62.47 35.98 77.43 61.32\n24 41.75 65.9 40.06 79.27 62.92\n20% 12 42.88 66.82 39.91 79.39 62.36\n24 45 72.73 45.31 80.79 65.28\n100% 12 51.63 77.43 49.71 85.34 70.57\n24 55.62 81.53 57.92 82.4 70.38\nTable 6: Multi-task evaluation on SemEval14 restaurant domain (SB1-4) on few-shot settings using generative\nlanguage model (GPT2).\nShot Layers Joint Accuracy Term Category\nSB1 (F1) SB2 (Acc) SB3 (F1) SB4 (Acc)\n1% 12 11.6 28.68 13.38 46.36 38.31\n24 9.04 24.87 11.36 44.32 35.63\n5% 12 18.43 33.81 16.74 56.85 50.06\n24 20.48 34.99 18.88 61.09 54.66\n10% 12 21.16 33.48 16.74 63.11 50.45\n24 22.18 37.13 19.64 67.12 55.43\n20% 12 25.77 37.74 20.63 69.39 62.07\n24 26.96 40.6 22.15 72.9 65.39\n100% 12 32.42 48.48 27.67 76.51 66.41\n24 43 50.27 30.15 76.78 69.6\nTable 7: Multi-task evaluation on SemEval16 restaurant domain (SB1-4) on few-shot settings using generative\nlanguage model (GPT2).\nthan BERT and GPT2-classifier. This indicates\nthat perhaps BERT and GPT2-classifier overfitted\nto the few-shot training data. On the other hand,\nGPT2 language model achieves more supervision\n782\nvia standard language modeling loss, which results\nin higher training loss, but better validation perfor-\nmance.\nF Ablation: Model weights update during\ntraining\nIn order to understand models behavior during\ntraining on few-shot data, we study the weight up-\ndate at each layer of GPT2 and BERT models, dur-\ning training on 1% few-shot data. For each layer,\nthe mean normalized weight update is defined as,\nk∑\ni=0\n(wl\ni −wl\ni−1)\nwl\n0\n(10)\nwhere lindicate the layer index, iindicates training\nstep, and wl\n0 refers to initial weight value before\ntraining. The comparison between GPT2 as gener-\native gpt2-generative, GPT2 as an ecoder for clas-\nsification gpt2-classifier and BERT model when\ntrained on 1% few shot data of SemEval14 restau-\nrant domain are shown in Figure 7. The results\nindicate that Bert model has higher variance for\nall layers, especially for the randomly-initialized\nclassification layer. Moreover, the mean normal-\nized update of BERT model is larger that gpt2-\ngenerative early during training, but is smaller at\nthe end of training, where gpt2-generative achieves\nhigher validation performance, as shown in Fig-\nure 3. Furthermore, the mean normalized update in\nembedding layer of gpt2-generative is significantly\nlarger than BERT and gpt2-classifier by one order\nof magnitude early at training, which increased to\ntwo order of magnitude at the end. We conjecture\nthat higher value in layer weights update at em-\nbedding layers, and at the end of training for other\nlayers is perhaps due to using standard language\nmodeling loss, which may provide more supervi-\nsion signal for GPT2, compared to cross-entropy\nloss in BERT and gpt2-classifier models.\nG Ablation: Other Sentiment Analysis\nTasks\nIn order to extend the investigate the performance\nof our proposed generative language model to\nother sentiment analysis tasks, we also evaluate\nfew-shot performance on SST-5 sentiment analy-\nsis dataset (Socher et al., 2013) (binary and fine-\ngrained sentiment classification), and OOS (Larson\net al., 2019) intent detection dataset. The results\nare shown in Figure 8, which indicate the superior-\nity of generative model (GPT2) over discriminative\nBERT. On intent detection, Figure 8(c), GPT2 also\noutperforms TOD-BERT (Wu et al., 2020) which\nexploits extra pretraining on dialogue datasets to\nincrease its few-shot performance.\nH Qualitative Analysis\nAs described in section 4.2 and Table 2, aspect\nterm extraction on restaurant domain (SemEval14)\nis improved in multi-task learning. To better under-\nstand model behavior, some examples are shown\nin Table 8. Using aspect category as supervision in\nmulti-task learning helps the model to more accu-\nrately generates the aspect terms, reduces false pos-\nitive aspect terms and wrong polarity predictions.\nMoreover, multi-tasking helps to better predict cat-\negory polarity, using supervision from aspect term\nduring training. Some examples of wrong predic-\ntion are shown in Table 9. It indicates that when\nthere are negative or conflict polarity, the model\nstruggles to correctly predict everything correctly.\nThis often happens when there are opposite opin-\nions for different aspect terms or categories.\n783\n(a) SemEval14 Laptop Aspect Term Polarity (SB2)\n(b) SemEval14 Restaurant Aspect Category Polarity (SB4)\nFigure 5: Analysis of few-shot training convergence, evaluated on SemEval14 for 1% and 1-shot training data, and\nfew-shot performance on all settings (right). GPT2-classifier model uses a classification layer on the output of last\ninput token without using language modeling loss for training. (best viewed in color)\nSentence Task Model Output\nthe sangria’s - watered down. aspect term <|term|> sangria negative\naspect category <|category|> food neutral\naspect term & category <|term|> sangria negative <|category|> food negative\ngroundtruth <|term|> sangria negative <|category|> food negative\neveryone who works there (the host, the bartender, the servers)\nis so helpful.\naspect term host positive, bartender neutral, servers positive\naspect category <|category|> service positive\naspect term & category <|term|> host positive, bartender positive, servers\npositive <|category|> service positive\ngroundtruth <|term|> bartender positive, host positive, servers\npositive <|category|> service positive\nin mi burrito, here was nothing but dark chicken that had that\ncooked last week and just warmed up in a microwave taste.\naspect term <|term|> dark chicken negative, microwave taste\nneutral\naspect category <|category|> food negative\naspect term & category <|term|> chicken negative, taste negative <|cate-\ngory|> food negative\nif you like seafood and or greek food you will love this place\nthough it is not limited to just these things.\naspect term <|term|> seafood positive, greek food positive, place\nnegative\naspect category <|category|> food positive\naspect term & category <|term|> seafood positive, greek food positive <|cat-\negory|> food positive\ngroundtruth <|term|> greek food positive, seafood positive <|cat-\negory|> food positive\nTable 8: Examples of correct predictions in multi-task learning.\n784\n(a) SemEval16 Restaurant Aspect Term Polarity (SB2)\n(b) SemEval16 Restaurant Aspect Category Polarity (SB4)\n(c) SemEval16 Laptop Aspect Category Polarity (SB4)\nFigure 6: Analysis of few-shot training convergence, evaluated on SemEval16 for 1% and 1-shot training data, and\nfew-shot performance on all settings (right). GPT2-classifier model uses a classification layer on the output of last\ninput token without using language modeling loss for training. (best viewed in color)\nSentence Task Model Output\ncertainly not the best sushi in new york, however, it is always\nfresh, and the place is very clean, sterile.\naspect term <|term|> sushi negative, place positive\naspect category <|category|> ambience positive, food positive\naspect term & category <|term|> sushi positive, place positive <|category|>\nfood positive, ambience positive\ngroundtruth <|term|> place positive, sushi conflict <|category|>\nambience positive, food conflict\nwhile there’s a decent menu, it shouldn’t take ten minutes to\nget your drinks and 45 for a dessert pizza.\naspect term menu positive, drinks positive, dessert pizza positive\naspect category food conflict\naspect term & category <|term|> menu positive, drinks positive, dessert\npizza positive <|category|> food positive\ngroundtruth <|term|> dessert pizza neutral, drinks neutral, menu\npositive <|category|> food positive, service negative\nthe portions of the food that came out were mediocre. aspect term portions negative, food neutral\naspect category food negative\naspect term & category <|term|> portions negative, food negative <|cate-\ngory|> food negative\ngroundtruth <|term|> portions of the food neutral <|category|>\nfood neutral\nTable 9: Examples of wrong prediction for joint and multi-task generative language model.\n785\n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 7: Model Layers mean normalized update, Eq. ( 10) , during training. Normalized update of the weight wat\ntraining step iis defined as (wi −wi−1)/w0. Results are for training on 1% few-shot data on SemEval14 restaurant\naspect term polarity (SB2) prediction task for4 random seed. Shaded area indicates standard deviation.\n786\n(a)\n(b)\nFigure 8: Few-shot evaluation of GPT2 and BERT models on SST2 dev set and OOS intent detection datasets. Note:\n1-shot refers to one example per class. (best viewed in color)\n787",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8661152124404907
    },
    {
      "name": "Generative model",
      "score": 0.6929287910461426
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6663309931755066
    },
    {
      "name": "Generative grammar",
      "score": 0.6596366167068481
    },
    {
      "name": "Language model",
      "score": 0.6140850782394409
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5970128774642944
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5627972483634949
    },
    {
      "name": "Task (project management)",
      "score": 0.5476736426353455
    },
    {
      "name": "Natural language processing",
      "score": 0.4758060872554779
    },
    {
      "name": "Machine learning",
      "score": 0.3972422182559967
    },
    {
      "name": "Speech recognition",
      "score": 0.37855759263038635
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}