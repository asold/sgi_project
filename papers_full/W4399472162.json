{
    "title": "Large Language Models Amplify Human Biases in Moral Decision-Making",
    "url": "https://openalex.org/W4399472162",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2152274041",
            "name": "Vanessa Cheung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2400799195",
            "name": "Maximilian Maier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142775165",
            "name": "Falk Lieder",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4388067858",
        "https://openalex.org/W2119659537",
        "https://openalex.org/W4392504765",
        "https://openalex.org/W4388798177",
        "https://openalex.org/W2226396244",
        "https://openalex.org/W2746351266",
        "https://openalex.org/W4283525019",
        "https://openalex.org/W4385573216",
        "https://openalex.org/W4396833703",
        "https://openalex.org/W4387378202",
        "https://openalex.org/W4378976798",
        "https://openalex.org/W2122253967",
        "https://openalex.org/W3022851539",
        "https://openalex.org/W4398143508",
        "https://openalex.org/W4378474033",
        "https://openalex.org/W4367694135",
        "https://openalex.org/W4399117902",
        "https://openalex.org/W3088399774",
        "https://openalex.org/W2097888778",
        "https://openalex.org/W2911589237",
        "https://openalex.org/W4378383736",
        "https://openalex.org/W3210943603",
        "https://openalex.org/W1560460460",
        "https://openalex.org/W4386721654",
        "https://openalex.org/W4391591806",
        "https://openalex.org/W4255307971",
        "https://openalex.org/W2520323219",
        "https://openalex.org/W3194049838",
        "https://openalex.org/W4323338447",
        "https://openalex.org/W4312236423",
        "https://openalex.org/W582220661",
        "https://openalex.org/W4292121845",
        "https://openalex.org/W4362657806",
        "https://openalex.org/W2269483693",
        "https://openalex.org/W4385572854",
        "https://openalex.org/W4362673335",
        "https://openalex.org/W2089012547",
        "https://openalex.org/W4390723916",
        "https://openalex.org/W4386421934",
        "https://openalex.org/W4396608701",
        "https://openalex.org/W4243106557",
        "https://openalex.org/W2600886565",
        "https://openalex.org/W4253725888",
        "https://openalex.org/W2009542038",
        "https://openalex.org/W3046000984",
        "https://openalex.org/W4392490008",
        "https://openalex.org/W4386501849",
        "https://openalex.org/W4318318155",
        "https://openalex.org/W2090025733",
        "https://openalex.org/W2530269857",
        "https://openalex.org/W1566702655",
        "https://openalex.org/W3007807084",
        "https://openalex.org/W4289527538",
        "https://openalex.org/W3136913552",
        "https://openalex.org/W4320476029",
        "https://openalex.org/W4396577456",
        "https://openalex.org/W4283263983",
        "https://openalex.org/W4366769338",
        "https://openalex.org/W2157581667",
        "https://openalex.org/W2167052337",
        "https://openalex.org/W3198909097"
    ],
    "abstract": "As large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is, therefore, important to understand how well LLMs make moral decisions and how they compare to humans. We investigated these questions by asking a range of LLMs to emulate or advise people's decisions in realistic moral dilemmas. In Study 1, we compared responses from LLMs to a representative U.S. sample (N = 285) for 22 dilemmas: collective action problems that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost-benefit reasoning against deontological rules. In the collective action problems, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: they usually endorsed inaction over action. In Study 2 (N = 490, preregistered), we replicated this omission bias and documented an additional bias: unlike humans, most LLMs were biased toward answering \"no\" in moral dilemmas, thus flipping their decision/advice depending on how the question is worded. In Study 3 (N=493, preregistered), we replicated these biases in LLMs using everyday moral dilemmas adapted from forum posts on Reddit. In Study 4, we investigated the sources of these biases by comparing models with and without fine-tuning, showing that they likely arise from fine-tuning models for chatbot applications. Our findings suggest that LLMs' moral decisions and advice amplify human biases and introduce novel potentially problematic biases.",
    "full_text": null
}