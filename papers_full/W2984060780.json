{
  "title": "Contextual Text Denoising with Masked Language Model",
  "url": "https://openalex.org/W2984060780",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2314199375",
      "name": "Yifu Sun",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2472586093",
      "name": "Haoming Jiang",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963881719",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3101767350",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4241454642",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W4381618495",
    "https://openalex.org/W1979822206",
    "https://openalex.org/W2914361693",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2890230387",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.",
  "full_text": "Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 286–290\nHong Kong, Nov 4, 2019.c⃝2019 Association for Computational Linguistics\n286\nContextual Text Denoising with Masked Language Models\nYifu Sun∗\nTencent\nyifusun2016@outlook.com\nHaoming Jiang\nGeorgia Tech\njianghm@gatech.edu\nAbstract\nRecently, with the help of deep learning mod-\nels, signiﬁcant advances have been made in\ndifferent Natural Language Processing (NLP)\ntasks. Unfortunately, state-of-the-art models\nare vulnerable to noisy texts. We propose a\nnew contextual text denoising algorithm based\non the ready-to-use masked language model.\nThe proposed algorithm does not require re-\ntraining of the model and can be integrated\ninto any NLP system without additional train-\ning on paired cleaning training data. We evalu-\nate our method under synthetic noise and natu-\nral noise and show that the proposed algorithm\ncan use context information to correct noise\ntext and improve the performance of noisy in-\nputs in several downstream tasks.\n1 Introduction\nBased on our prior knowledge and contextual in-\nformation in sentences, humans can understand\nnoisy texts like misspelled words without difﬁ-\nculty. However, NLP systems break down for\nnoisy text. For example, Belinkov and Bisk\n(2017) showed that modern neural machine trans-\nlation (NMT) system could not even translate texts\nwith moderate noise. An illustrative example\nof English-to-Chinese translation using Google\nTranslate 1 is presented in Table 1.\nText correction systems are widely used in\nreal-world scenarios to address noisy text inputs\nproblem. Simple rule-based and frequency-based\nspell-checker are limited to complex language sys-\ntems. More recently, modern neural Grammati-\ncal Error Correction (GEC) systems are developed\nwith the help of deep learning (Zhao et al., 2019;\nChollampatt and Ng, 2018). These GEC systems\nheavily rely on annotated GEC corpora, such as\nCoNLL-2014 (Ng et al., 2014). The parallel GEC\n∗ Work done at Georgia Tech.\n1https://translate.google.com; Access Date:\n08/09/2019\ncorpora, however, are expansive, limited, and even\nunavailable for many languages. Another line of\nresearches focuses on training a robust model that\ninherently deals with noise. For example, Be-\nlinkov and Bisk (2017) train robust character-level\nNMT models using noisy training datasets, in-\ncluding both synthetic and natural noise. On the\nother hand, Malykh et al. (2018) consider robust\nword vectors. These methods require retraining\nthe model based on new word vectors or noise\ndata. Retraining is expensive and will affect the\nperformance of clean text. For example, in Be-\nlinkov and Bisk (2017), the robustness scariﬁes the\nperformance of the clean text by about 7 BLEU\nscore on the EN-FR translation task.\nIn this paper, we propose a novel text denois-\ning algorithm based on the ready-to-use masked\nlanguage model (MLM, Devlin et al. (2018)). No-\ntice that we are using English Bert. For other lan-\nguages, We need to use MLM model pre-trained\non that speciﬁc language. The design follows the\nhuman cognitive process that humans can utilize\nthe context, the spell of the wrong word (Mayall\net al., 1997), and even the location of the letters\non the keyboard to correct noisy text. The MLM\nessentially mimics the process that the model pre-\ndicts the masked words based on their context.\nThere are several beneﬁts of the proposed method:\n•Our method can make accurate corrections\nbased on the context and semantic meaning\nof the whole sentence as Table 1 shows.\n•The pre-trained masked language model is\nready-to-use (Devlin et al., 2018; Liu et al.,\n2019). No extra training or data is required.\n•Our method makes use of Word Piece embed-\ndings (Wu et al., 2016) to alleviate the out-of-\nvocabulary problem.\n2 Method\nOur denoising algorithm cleans the words in the\nsentence in sequential order. Given a word, the\n287\nMethod Input Text Google Translate\nClean Input there is a fat duck swimming in the lake 湖里有一只胖鸭子在游泳\nNoisy Input there is a fat dack swimming in the leake 在 leake 里游泳时有一个 胖子\nSpell-Checker there is a fat sack swimming in the leak 在泄露处有一个肥胖 袋在游泳\nGrammaly2 there is a fat dack swimming in the lake 湖里 游泳很胖\nOurs there is a fat duck swimming in the lake 湖里有一只胖鸭子在游泳\nTable 1: Illustrative example of spell-checker and contextual denoising.\nalgorithm ﬁrst generates a candidate list using the\nMLM and then further ﬁlter the list to select a can-\ndidate from the list. In this section, we ﬁrst brieﬂy\nintroduce the masked language model, and then\ndescribe the proposed denoising algorithm.\n2.1 Masked Language Model\nMasked language model (MLM) masks some\nwords from a sentence and then predicts the\nmasked words based on the contextual informa-\ntion. Speciﬁcally, given a sentence x = {xi}L\ni=1\nwith L words, a MLM models\np(xj|x1, ..., xj−1, [MASK ], xj+1, ..., xL),\nwhere [MASK ] is a masking token over the j-th\nword. Actually, MLM can recover multiple masks\ntogether, here we only present the case with one\nmask for notation simplicity. In this way, unlike\ntraditional language model that is in left-to-right\norder (i.e., p(xj|x1, ..., xj−1)), MLM is able to\nuse both the left and right context. As a result,\na more accurate prediction can be made by MLM.\nIn the following, we use the pre-trained masked\nlanguage model, BERT (Devlin et al., 2018). So\nno training process is involved in developing our\nalgorithm.\n2.2 Denoising Algorithm\nThe algorithm cleans every word in the sentence\nwith left-to-right order except for the punctuation\nand numbers by masking them in order. For each\nword, MLM ﬁrst provide a candidate list using a\ntransformed sentence. Then the cleaned word is\nselected from the list. The whole process is sum-\nmarized in Algorithm 1.\nText MaskingThe ﬁrst step is to convert the sen-\ntence x into a masked form x′. With the use of\nWord Piece tokens, each word can be represented\nby several different tokens. Suppose the j-th word\n(that needs to be cleaned) is represented by the js-\nth token to the je-th token, we need to mask them\nout together. For the same reason, the number of\ntokens of the expected cleaned word is unknown.\n2https://app.grammarly.com; Access Date:\n08/09/2019\nSo we use different number of masks to create the\nmasked sentence {x′\nn}N\nn=1, where x′\nn denotes the\nmasked sentence with n-gram mask. Speciﬁcally,\ngiven x = x1, ..., xjs , ..., xje , ..., xL, the masked\nform is x′\nn = x1, ...,[MASK ] ×n, ..., xL. We\nmask each word in the noisy sentence by order.\nThe number of masks N can not be too small or\ntoo large. The candidate list will fail to capture the\nright answer with a small N. However, the opti-\nmal answer would ﬁt the noisy text perfectly with\na large enough N. Empirically, we ﬁnd out N = 4\nis sufﬁciently large to obtain decent performance\nwithout too much overﬁtting.\nText AugmentationSince the wrong word is also\ninformative, so we augment each masked text x′\nn\nby concatenating the original text x. Speciﬁcally,\nthe augmented text is ˜xn = x′\nn[SEP ]x, where\n[SEP ] is a separation token.3\nCompared with directly leaving the noisy word\nin the original sentence, the masking and augmen-\ntation strategy are more ﬂexible. It is beneﬁted\nfrom that the number of tokens of the expected\nword does not necessarily equal to the noisy word.\nBesides, the model pays less attention to the noisy\nwords, which may induce bias to the prediction of\nthe clean word.\nCandidate Selection The algorithm then con-\nstructs a candidate list using the MLM, which is\nsemantically suitable for the masked position in\nthe sentence. We ﬁrst construct candidate list V n\nc\nfor each ˜xn, and then combine them to obtained\nthe ﬁnal candidate list Vc = V 1\nc ∪···∪ V N\nc .\nNote that we need to handle multiple masks when\nn > 1. So we ﬁrst ﬁnd k most possible word\npieces for each mask and then enumerate all pos-\nsible combinations to construct the ﬁnal candidate\nlist. Speciﬁcally,\nV n\nc = Top-k{p([MASK ]1 = w|˜xn)}w∈V\n×···× Top-k{p([MASK ]n = w|˜xn)}w∈V ,\nwhere V is the whole vocabulary and×means the\nCartesian product.\n3In BERT convention, the input also needs to be embraced\nwith a [CLS] and a [SEP ] token.\n288\nThere may be multiple words that make sense\nfor the replacement. In this case, the spelling\nof the wrong word is useful for ﬁnding the most\nlikely correct word. We use the edit distance to\nselect the most likely correct word further.\nwc = arg min\nw∈Vc\nE(w, xj),\nwhere E(w, xj) represent the edit distance be-\ntween w and the noisy word xj.\nAlgorithm 1:Denoising with MLM\nInput: Noisy sentence x = {xi}L\ni=1\nOutput: Denoised sentence x = {xi}L\ni=1\nfor i = 1, 2, ..., Ldo\n{x′\nn}N\nn=1 = Masking(x) ;\n{˜xn}N\nn=1 = {Augment(x′\nn, x)}N\nn=1 ;\nfor n = 1, 2, ..., Ndo\nV n\nc = Candidate(˜xn) ;\nend\nVc = V 1\nc ∪···∪ V N\nc ;\nwc = arg minw∈Vc E(w, xj) ;\nxi = wc;\nend\n3 Experiment\nWe test the performance of the proposed text de-\nnoising method on three downstream tasks: neural\nmachine translation, natural language inference,\nand paraphrase detection. All experiments are\nconducted with NVIDIA Tesla V100 GPUs. We\nuse the pretrained pytorch Bert-large (with whole\nword masking) as the masked language model 4.\nFor the denoising algorithm, we use at most N =\n4 masks for each word, and the detailed conﬁgu-\nration of the size of the candidate list is shown in\nTable 2. We use a large candidate list for one word\npiece which covers the most cases. For multiple\nmasks, a smaller list would be good enough.\nFor all tasks, we train the task-speciﬁc model\non the original clean training set. Then we com-\npare the model performance on the different test\nsets, including original test data, noise test data,\nand cleaned noise test data. We use a commercial-\nlevel spell-checker api 5 as our baseline method.\nIn this section, we ﬁrst introduce how the noise\nis generated, and then present experimental results\nof three NLP tasks.\n4https://github.com/huggingface/\npytorch-pretrained-BERT\n5https://rapidapi.com/montanaflynn/\napi/spellcheck; Access Date: 08/09/2019\nNo. of [MASK ] (n) Top k Size\n1 3000 3000\n2 5 25\n3 3 27\n4 2 16\nTotal: 3068\nTable 2: Size of the candidate list\n3.1 Noise\nTo control the noise level, we randomly pick\nwords from the testing data to be perturbed with\na certain probability. For each selected word, we\nconsider two perturbation setting: artiﬁcial noise\nand natural noise. Under artiﬁcial noise setting,\nwe separately apply four kinds of noise: Swap,\nDelete, Replace, Insert with certain probability.\nSpeciﬁcally,\n•Swap: We swap two letters per word.\n•Delete: We randomly delete a letter in the\nmiddle of the word.\n•Replace: We randomly replace a letter in a\nword with another.\n•Insert: We randomly insert a letter in the mid-\ndle of the word.\nFollowing the setting in (Belinkov and Bisk,\n2017), the ﬁrst and the last character remains un-\nchanged.\nFor the artiﬁcial noise , we follow the experi-\nment of Belinkov and Bisk (2017) that harvest nat-\nurally occurring errors (typos, misspellings, etc.)\nfrom the edit histories of available corpora. It gen-\nerates a lookup table of all possible errors for each\nword. We replace the selected words with the cor-\nresponding noise in the lookup table according to\ntheir settings.\n3.2 Neural Machine Translation\nWe conduct the English-to-German translation ex-\nperiments on the TED talks corpus from IWSLT\n2014 dataset 6. The data contains about 160, 000\nsentence pairs for training, 6, 750 pairs for testing.\nWe ﬁrst evaluate the performance using a 12-\nlayer transformer implemented by fairseq (Ott\net al., 2019). For all implementation details, we\nfollow the training recipe given by fairseq 7. We\nalso evaluate the performance of Google Trans-\nlate.\n6https://wit3.fbk.eu/archive/2014-01/\ntexts/en/de/en-de.tgz\n7https://github.com/pytorch/fairseq/\ntree/master/examples/translation\n289\nFor the artiﬁcial noise setting, we perturb 20%\nwords and apply each noise with probability 25%.\nFor that natural noise setting, we also perturb20%\nwords. All experiment results is summarized in\nTable 3, where we use BLEU score (Papineni\net al., 2002) to evaluate the translation result.\nText Source Google Fairseq\nOriginal 31.49 28 .06\nArtiﬁcial Noise 28.11 22 .27\n+ Spell-Checker 26.28 21 .15\n+ Ours 28.96 25 .80\nNatural Noise 25.22 17 .29\n+ Spell-Checker 20.90 15 .04\n+ Ours 25.49 21 .40\nTable 3: BLEU scores of EN-to-DE tranlsation\nAs can be seen, both fairseq model and Google\nTranslate suffer from a signiﬁcant performance\ndrop on the noisy texts with both natural and syn-\nthetic noise. When using the spell-checker, the\nperformance even drops more. Moreover, our pur-\nposed method can alleviate the performance drop.\n3.3 Natural Language Inference\nWe test the algorithm on Natural Language Infer-\nence (NLI) task, which is one of the most chal-\nlenge tasks related to the semantics of sentences.\nWe establish our experiment based on the SNLI\n(the Stanford Natural Language Inference, Bow-\nman et al. (2015)) corpus. Here we use accuracy\nas the evaluation metric for SNLI.\nHere we use state-of-the-art 400 dimensional\nHierarchical BiLSTM with Max Pooling (HBMP)\n(Talman et al., 2019). The implementation follows\nthe publicly released code 8. We use the same\nnoise setting as the NMT experiments. All results\nare presented in Table 4. We observe performance\nimprovement with our method. To see if the de-\nnoising algorithm would induce noises to the clean\ntexts, we also apply the algorithm to the original\nsentence and check if performance will degrade.\nIt can be seen that, unlike the traditional robust\nmodel approach, applying a denoising algorithm\non a clean sample has little inﬂuence on perfor-\nmance.\nAs shown in the Table4, the accuracy is very\nclose to the original one under the artiﬁcial noise.\nNatural noises contain punctuations and are more\ncomplicated than artiﬁcial ones. As a result, infer-\nence becomes much harder in this way.\n8https://github.com/Helsinki-NLP/HBMP\nMethod Original Artiﬁcial Natural\nNoise Noise\nHBMP 84.0 75 .0 74 .0\n+Spell-Checker 84.0∗ 63.0 68 .0\n+Ours 83.0∗ 81.0 77 .0\nTable 4: SNLI classiﬁcation accuracy with artiﬁcial\nnoise and natural noise. ∗: Applying denoising algo-\nrithm on original texts.\n3.4 Paraphrase Detection\nWe conducted Paraphrase detection experiments\non the Microsoft Research Paraphrase Corpus\n(MRPC, Dolan and Brockett (2005)) consisting of\n5800 sentence pairs extracted from news sources\non the web. It is manually labelled for pres-\nence/absence of semantic equivalence.\nWe evaluate the performance using the state-of-\nthe-art model: ﬁne-tuned RoBERTa (Liu et al.,\n2019). For all implemented details follows the\npublicly released code 9. All experiment results\nis summarized in Table 5. We increase the size of\nthe candidate list to10000+25+27+16 = 10068\nbecause there are a lot of proper nouns, which are\nhard to predict.\nMethod Original Artiﬁcial Natural\nNoise Noise\nRoBERTa 84.3 81.9 75.2\n+Spell-Checker 82.6 81.3 75.4\n+Ours 83.6 82.7 76.4\nTable 5: Classiﬁcation F1 score on MRPC\n4 Conclusion and Future Work\nIn this paper, we present a novel text denois-\ning algorithm using ready-to-use masked language\nmodel. We show that the proposed method can re-\ncover the noisy text by the contextual information\nwithout any training or data. We further demon-\nstrate the effectiveness of the proposed method on\nthree downstream tasks, where the performance\ndrop is alleviated by our method. A promising fu-\nture research topic is how to design a better can-\ndidate selection rule rather than merely using the\nedit distance. We can also try to use GEC corpora,\nsuch as CoNLL-2014, to further ﬁne-tune the de-\nnoising model in a supervised way to improve the\nperformance.\n9https://github.com/pytorch/fairseq/\ntree/master/examples/roberta\n290\nReferences\nYonatan Belinkov and Yonatan Bisk. 2017. Synthetic\nand natural noise both break neural machine transla-\ntion. arXiv preprint arXiv:1711.02173.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\narXiv preprint arXiv:1508.05326.\nShamil Chollampatt and Hwee Tou Ng. 2018. Neural\nquality estimation of grammatical error correction.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2528–2539.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nValentin Malykh, Varvara Logacheva, and Taras\nKhakhulin. 2018. Robust word vectors: Context-\ninformed embeddings for noisy texts. In Proceed-\nings of the 2018 EMNLP Workshop W-NUT: The 4th\nWorkshop on Noisy User-generated Text, pages 54–\n63.\nKate Mayall, Glyn W Humphreys, and Andrew Olson.\n1997. Disruption to word or letter processing? the\norigins of case-mixing effects. Journal of Experi-\nmental Psychology: Learning, Memory, and Cogni-\ntion, 23(5):1275.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The conll-2014 shared task on\ngrammatical error correction. In Proceedings of the\nEighteenth Conference on Computational Natural\nLanguage Learning: Shared Task, pages 1–14.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nAarne Talman, Anssi Yli-Jyr ¨a, and J ¨org Tiedemann.\n2019. Sentence embeddings in nli with iterative re-\nﬁnement encoders. Natural Language Engineering,\n25(4):467–482.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nWei Zhao, Liang Wang, Kewei Shen, Ruoyu Jia, and\nJingming Liu. 2019. Improving grammatical er-\nror correction via pre-training a copy-augmented\narchitecture with unlabeled data. arXiv preprint\narXiv:1903.00138.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8382810354232788
    },
    {
      "name": "Retraining",
      "score": 0.6770827174186707
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6615068316459656
    },
    {
      "name": "Language model",
      "score": 0.6476597785949707
    },
    {
      "name": "Noise reduction",
      "score": 0.6332060098648071
    },
    {
      "name": "Noise (video)",
      "score": 0.6161024570465088
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5644243955612183
    },
    {
      "name": "Natural language processing",
      "score": 0.5188835263252258
    },
    {
      "name": "Natural language",
      "score": 0.5032503008842468
    },
    {
      "name": "Machine learning",
      "score": 0.4502803087234497
    },
    {
      "name": "Speech recognition",
      "score": 0.3720325231552124
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "International trade",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}