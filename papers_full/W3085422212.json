{
  "title": "Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding",
  "url": "https://openalex.org/W3085422212",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5034826937",
      "name": "Shuohang Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084879213",
      "name": "Luowei Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066666034",
      "name": "Zhe Gan",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101877289",
      "name": "Yen-Chun Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102952667",
      "name": "Yuwei Fang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5023777406",
      "name": "Siqi Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101580521",
      "name": "Yu Cheng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100442542",
      "name": "Jingjing Liu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3166398787",
    "https://openalex.org/W2963021447",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W3033943443",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2799081691",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W3034987253",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2741263286",
    "https://openalex.org/W3102129360",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2891850907",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2734823783",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "Transformer has become ubiquitous in the deep learning field. One of the key ingredients that destined its success is the self-attention mechanism, which allows fully-connected contextual encoding over input tokens. However, despite its effectiveness in modeling short sequences, self-attention suffers when handling inputs with extreme long-range dependencies, as its complexity grows quadratically with respect to the sequence length. Therefore, long sequences are often encoded by Transformer in chunks using a sliding window. In this paper, we propose Cluster-Former, a novel clustering-based sparse Transformer to perform attention across chunked sequences. The proposed framework is pivoted on two unique types of Transformer layer: Sliding-Window Layer and Cluster-Former Layer, which encode local sequence information and global context jointly and iteratively. This new design allows information integration beyond local windows, which is especially beneficial for question answering (QA) tasks that rely on long-range dependencies. Experiments show that Cluster-Former achieves state-of-the-art performance on several major QA benchmarks.",
  "full_text": "Cluster-Former: Clustering-based Sparse Transformer\nfor Question Answering\nShuohang Wang Luowei Zhou Zhe Gan Yen-Chun Chen\nSiqi Sun Yuwei Fang Yu Cheng Jingjing Liu\nMicrosoft Corporation\n{shuowa, luowei.zhou, zhe.gan, yen-chun.chen}@microsoft.com\n{siqi.sun, yuwfan, yu.cheng, jingjl}@microsoft.com\nAbstract\nTransformer has become ubiquitous in the\ndeep learning ﬁeld. One of the key ingredients\nthat destined its success is the self-attention\nmechanism, which allows fully-connected con-\ntextual encoding over input tokens. However,\ndespite its effectiveness in modeling short se-\nquences, self-attention suffers when handling\ninputs with extreme long-range dependencies,\nas its complexity grows quadratically w.r.t.the\nsequence length. Therefore, long sequences\nare often encoded by Transformer in chunks\nusing a sliding window. In this paper, we\npropose Cluster-Former, a novel clustering-\nbased sparse Transformer to perform atten-\ntion across chunked sequences. The proposed\nframework is pivoted on two unique types of\nTransformer layer: Sliding-Window Layer and\nCluster-Former Layer, which encode local se-\nquence information and global context jointly\nand iteratively. This new design allows in-\nformation integration beyond local windows,\nwhich is especially beneﬁcial for question an-\nswering (QA) tasks that rely on long-range de-\npendencies. Experiments show that Cluster-\nFormer achieves state-of-the-art performance\non several major QA benchmarks.\n1 Introduction\nLong-range contextual understanding has proven\ncritical in many natural language processing (NLP)\ntasks. For example, the relevant context for cor-\nrectly answering an open-domain question can arch\nover thousands of words (Chen et al., 2017). En-\ncoding long sequences via deep neural networks,\nhowever, has remained an expensive and challeng-\ning task due to high demand on training time\nand GPU memory. Traditional sequence model-\ning methods (Hochreiter and Schmidhuber, 1997)\nencode long sequences in a chronological order,\nwhich suffers high latency. In the place of se-\nquential encoding, recent models such as Trans-\nformer (Vaswani et al., 2017) use simultaneous self-\nattention over the entire input instead, which has\nbeen successfully adopted in many NLP tasks such\nas textual entailment (Devlin et al., 2019), depen-\ndency parsing (Zhou and Zhao, 2019), and summa-\nrization (Lewis et al., 2019). A caveat with Trans-\nformer though is that building full connections over\nlong sequences translates to quadratic growth on\nmemory demand and computational complexity\nw.r.t.sequence length.\nOne way to efﬁciently encode long sequences is\nto ﬁrst chunk a sequence into much shorter ones\nwith a sliding window, then build connections be-\ntween the shorter sequences (Figure 1(a)). For ex-\nample, Child et al. (2019), Beltagy et al. (2020) and\nZaheer et al. (2020) apply sparse attention to chun-\nked sequences in hand-designed patterns in order to\ngather information from the chunks (Figure 1(b)).\nChoi et al. (2017) and Wang et al. (2019) ﬁrst use\na simpler model to ﬁlter chunked sequences, then\nprocess selected sequences with fully-connected\nself-attention. Rae et al. (2019) makes use of the\nshared memory of chunked sequences to build con-\nnections between them. However, these methods\ncannot encode long-range dependencies with as\nmuch ﬂexibility or accuracy as fully-connected\nself-attention, due to their dependency on hand-\ndesigned patterns.\nRecently, several studies (Kitaev et al., 2020;\nTay et al., 2020a) propose to further improve the\nsparse attention mechanism by hashing or sort-\ning the hidden states into different buckets (Fig-\nure 1(c)). These works mainly explore tasks with\nrelatively short sequences, such as sentence-level\nmachine translation, where the number of hash-\ning vectors is relatively small (less than 16 in Ki-\ntaev et al. (2020)), allowing randomly initialized\nhashing vectors to hash hidden states into correct\nbuckets. However, how to use hashing-based atten-\ntion in the context of long sequences ( e.g.,, up to\narXiv:2009.06097v2  [cs.CL]  7 Jun 2021\nFigure 1: Illustration of different methods for processing long sequences. Each square represents a hidden state.\nThe black-dotted boxes are Transformer layers. (a) is the sliding-window-based method to chunk a long sequence\ninto short ones with window size 3 and stride 2. (b) builds cross-sequence attention based on sliding window\nover pre-selected positions (red-dotted boxes). (c) hashes the hidden states into different buckets by randomly-\ninitialized vectors. (d) is our proposed approach to cluster the hidden states. Our ﬁnal model is a combination of\n(a) and (d) that processes both local and global context.\nthousands of words) is still an unexplored territory.\nOur proposed framework for efﬁcient long se-\nquence encoding, Cluster-Former, marries both\nsliding-window and hashing-based methods to\nachieve effective local and long-range dependency\nencoding. Cluster-Former consists of two types\nof encoding layer. The ﬁrst one (noted as Sliding-\nWindow Layer) focuses on extracting local infor-\nmation within a sliding window. It applies Trans-\nformer to the hidden states of each chunked se-\nquence independently, as shown in Figure 1(a). The\nother one (noted asCluster-Former Layer) learns to\nencode global information beyond the initial chun-\nked sequences. Speciﬁcally, we ﬁrst apply cluster-\ning to the input hidden states so that similar hidden\nstates are assigned to the same cluster, as shown\nin Figure 1(d). The clustered and sorted input is\nthen divided uniformly into chunks, each encoded\nby a Transformer layer. Note that to make model\ntraining more efﬁcient, the cluster centroids are not\ncomputed online but updated periodically (every\nepoch or a few epochs). We accumulate the hidden\nstates from the layer prior to the Cluster-Former\nlayer in a memory bank, and apply the K-Means\nalgorithm to form cluster centroids during each\nupdate cycle. Compared to previously discussed\nsparse attention based on pre-selected positions\n(Figure 1(b)) or randomly-initialized hashing vec-\ntors (Figure 1(c)), experimental results show that\nour method can encode dependency across chunked\nsequences more effectively.\nOur contributions can be summarized as follows.\n(i) We propose Cluster-Former, a novel approach\nto capturing long-range dependencies more effec-\ntively than locality-sensitive hashing method. (ii)\nWe propose a new Transformer-based framework\nto process long sequences by combining Sliding-\nWindow and Cluster-Former layers to extract both\nlocal and global contextual information. (iii) Our\nmodel achieves the best performance on question\nanswering datasets of Natural Questions (long an-\nswer), SearchQA, and Quasar-T.\n2 Related Work\nEfﬁcient Transformers With Transformer mod-\nels growing larger and larger, how to handle\nlonger sequences arises as a critical challenge.\nMany works have been proposed to improve the\ncomputational and memory efﬁciency of Trans-\nformers, including Sparse Transformer (Child\net al., 2019), Set Transformer (Lee et al., 2019),\nRouting Transformer (Roy et al., 2020), Fast\nTransformer (Vyas et al., 2020), Reformer (Ki-\ntaev et al., 2020), Sinkhorn Transformer (Tay\net al., 2020a), Longformer (Beltagy et al., 2020),\nETC (Ainslie et al., 2020), Synthesizer (Tay et al.,\n2021), Performer (Choromanski et al., 2020),\nLinformer (Wang et al., 2020), Linear Trans-\nformer (Katharopoulos et al., 2020), and Big-\nBird (Zaheer et al., 2020). Tay et al. (2020b) pro-\nvided an excellent literature survey on this emerg-\ning topic. Our method falls into the setting of learn-\nable sparse-attention patterns.\nAmong all these works, our method is closer\nto Set Transformer (Lee et al., 2019), Routing\nTransformer (Roy et al., 2020), and Fast Trans-\nFigure 2: An overview of the proposed Transformer layer. (a) Sliding-Window layer over a sequence. (b) Cluster-\nFormer layer over clustered hidden states from the output of (a). Cluster centroids are periodically updated based\non the memory bank of the hidden states in the corresponding layer.\nformer (Vyas et al., 2020), which all use cluster\ncentroids to learn patterns. However, we target at\nsolving a different task, question answering. And\nit also leads to a signiﬁcant different framework\nto encode a short question with a long context,\nother than a single long sequence, such as language\nmodeling task. Moreover, our cluster centroids are\nupdated in a very different way by periodical cen-\ntroids update with K-Means on memory bank, other\nthan memory-based centroids (Lee et al., 2019), ex-\nponentially moving centroids (Roy et al., 2020), or\nonline clustering (Vyas et al., 2020).\nLong Sequence in Question Answering For\ntasks such as open-domain question answer-\ning (Chen et al., 2017), a large volume of docu-\nments or paragraphs is usually retrieved to infer the\nanswer, yielding extremely long context content.\nDespite the fact that state-of-the-art NLP models\nare capable of extracting answers amid complex\ncontext, they still struggle with extremely long in-\nput sequences. Recent advances that advocate the\nuse of large-scale pre-trained models (Lewis et al.,\n2019; Liu et al., 2019; Lan et al., 2020) for question\nanswering make this problem more prominent, due\nto tremendous memory consumption. To process\nlong sequences, the most widely-used method is to\nﬁrst use a lightweight model to ﬁlter out redundant\ntext, then use sliding-window-based approaches to\nencode the remaining sequences with a more so-\nphisticated model. Chen et al. (2017) integrated\nbi-gram features into Information Retrieval (IR)\nmethods to retrieve related documents more ac-\ncurately. Wang et al. (2018) trained a paragraph\nselector using as the reward whether the entire sys-\ntem can obtain the correct answer or not. Asai et al.\n(2020) trained a recurrent retriever to select para-\ngraphs for multi-hop question answering. Izacard\nand Grave (2021) proposed to fuse local encoded\ninformation into a decoder for answer generation.\nBesides the above methods, directly applying Ef-\nﬁcient Transformers to process long sequences in\nquestion answering is another option. In this paper,\nwe focus on this direction by directly training our\nCluster-Former on the long context without using\nlightweight model for context ﬁltering.\n3 Proposed Approach\nThe proposed framework to handle long sequences\nis pivoted on two types of Transformer layer: ( i)\nSliding-Window Layer; and ( ii) Cluster-Former\nLayer. The former focuses on encoding local se-\nquence information, while the latter is on encoding\nglobal context and always built on top of the former\nlayer. An overview of the two layers is illustrated\nin Figure 2.\n3.1 Sliding-Window Layer\nDespite that our focus is on capturing long-range\ndependencies for global context, local information\nalso plays a critical role for knowledge propaga-\ntion. Therefore, in the lower section of our net-\nwork, we adopt the traditional sliding-window en-\ncoding mechanism. A sliding window segments\na long sequence X into short, overlapping ones\nwith window size l and stride m, as illustrated in\nFigure 2(a). Note that in this paper, we focus on\nquestion answering tasks, for which we concate-\nnate the question Q with each sequence chunked\nfrom the document:\nH0\nk = [Q; X [m ×k : (m ×k + l)]] , (1)\nwhere Q ∈Rq×d denotes question embeddings\ngiven a QA task, q is the number of tokens in the\nquestion, and X ∈Rx×d is the embeddings for all\ncontext, x is the number of tokens in context. k is\nthe ID of the chunked sequence, l is the window\nsize, and m is the stride of the sliding window.\n[idx1 : idx2] indicates selecting rows between the\nindex of idx1 and idx2 of the matrix. [·; ·] means\nconcatenating the matrices along the row. We ﬁrst\nuse Transformer to encode each sequence in sliding\nwindow as follows:\nHn+1\nk = Transformer(Hn\nk), (2)\nwhere Hn+1\nk ∈R(q+l)×d is the output of Trans-\nformer on the k-th sequence in the n-th layer, while\nit is not the ﬁnal output of the n-th layer. As we\nexpect the neighbouring sequences to share useful\ninformation in hidden states as well, we always set\nm < lto allow overlapping between sequences.\nWe use the mean values of the Transformer hidden\nstates at the overlapped tokens between windows\nas ﬁnal outputs. To merge the representations from\nthe (k −1)-th sequence:\nHn+1\nk [q : q + l −m] + = Hn+1\nk−1 [q + m : end],\nHn+1\nk [q : q + l −m] / = 2 ,\nand merge representations from (k + 1)-th se-\nquence:\nHn+1\nk [q + m : end] + = Hn+1\nk+1 [q : q + l −m],\nHn+1\nk [q + m : end] / = 2 , (3)\nwhere + = is to add matrices in-place and / = is\nto divide a matrix by a scalar value in-place. The\nmerged hidden states Hn+1\nk ∈R(q+l)×d are the\nﬁnal outputs of the n-th layer. If the next layer\nis Cluster-Former, the output hidden states in this\nlayer Hn+1\nk will be saved into memory bank for\ncomputing the cluster centroids.\nAlgorithm 1 Cluster Centroids Update\n1: Initialize Memory = Queue()\n2: Centroids = GETCENTROIDS (RandomVector)\n3:\n4: function TRAIN (Inputs)\n5: for i = 1, 2,. . . , IterationNumdo\n6: States = Sliding-Transformer(Inputs[i])\n7: Memory.add(States)\n8: while len(Memory) > M do\n9: Memory.pop()\n10: end while\n11: if i % ClusterUpdateFrequency == 0 then\n12: Centroids = GETCENTROIDS (Memory)\n13: end if\n14: Clusters = cluster States by Centroids\n15: States = Cluster-Former(Clusters)\n16: end for\n17: end function\n18:\n19: function GETCENTROIDS (HiddenStates)\n20: Centroids = K-Means(HiddenStates)\n21: Outputs = List()\n22: Outputs[1] = Centroids[1]\n23: for i = 2, 3,. . . , ClusterNumdo\n24:\nOutputs[i] =centroid from Centroids\nthat is closest to Outputs[i −1]\nbut not in Outputs\n25: end for\n26: return Outputs\n27: end function\n3.2 Cluster-Former Layer\nWe introduce a Cluster-Former layer to add global\nrepresentational power to Transformer beyond slid-\ning windows. An in-depth visualization of the layer\nis illustrated in Figure 2(b).\nThe input of the Cluster-Former layer comes\nfrom the hidden states of the prior layer (in our\ncase a Sliding-Window layer). After merging the\noverlaps between sequence chunks, the input of\nthis layer is deﬁned as:\n¯Hn = [Hn\n0 [0 : q + m]; ...; Hn\nk[0 : q + m]] , (4)\nwhere ¯Hn ∈R(q⌈x/m⌉+x)×d is the hidden states to\ncluster, x is the number of tokens in the context.\nAs the hidden states with larger cosine similarity\nare more likely to have higher attention weights,\nwe build sparse self-attention only on the hidden\nstates in the same cluster. In this work, we use\nK-Means as the chosen clustering method for sim-\nplicity. More advanced clustering algorithms have\nthe potential of yielding better performance. Since\nrunning K-Means on the ﬂy in each training itera-\ntion is computationally expensive, we decide to re-\ncompute the cluster centroids with low frequency\n(every epoch or a few epochs).\nIn addition, to avoid dramatic changes in the\ncluster centroids due to limited hidden state inputs,\nwe maintain a memory bank for the most recent\nhidden states. The entire procedure is depicted in\nAlgorithm 1. Once we compute the cluster cen-\ntroids, we can directly use them for hidden state\nclustering as follows:\nvn = argmax\n( Hn(Cn)T\n||Hn||2||Cn||2\n)\n, (5)\nwhere Cn ∈ Rp×d are the cluster centroids for\nlayer n, and p is the pre-deﬁned number of clusters.\nThe function argmax(·) performs on the last dimen-\nsion and assigns all the input hidden states into\ndifferent clusters based on the max value of cosine\nsimilarity between the hidden states and cluster cen-\ntroids. vn ∈R(q⌈x/m⌉+x) is the assigned cluster\nIDs of all the input hidden states.\nSince the number of hidden states in different\nclusters can vary substantially, padding them to\nthe maximum length for Transformer training will\nsigniﬁcantly increase the computational time. To\nmake the extraction of global context more efﬁ-\ncient, we greedily pick the cluster centroids based\non the nearest neighbour (measured by cosine simi-\nlarity) as shown in the functionGETCENTROIDS in\nAlgorithm 1. Thus, the hidden states with similar\ncluster IDs are also close to each other. Then, we\ncan directly sort the cluster IDs of hidden states and\nuniformly chunk the hidden states (same window\nsize and stride m):\nun = argsort(vn),\nan\nk = un[mk : m(k + 1)],\nEn\nk = Hn[an\nk], (6)\nwhere the function argsort(·) is to obtain the indexes\nof input values sorted in order (same values sorted\nby the corresponding position of hidden states).\nan\nk ∈Rm is the chunked indexes of the hidden\nstates. En\nk ∈Rm×d is the k-th clustered hidden\nstates, and we will run Transformer on top of it to\nbuild the connection beyond the words in the initial\nsliding window as follows:\nEn+1\nk = Transformer(En\nk). (7)\nAfter updating the hidden states, we map them back\nto the order before clustering:\n¯Hn+1 = [ En+1\n0 ; En+1\n1 ; ...; En+1\nK ],\n¯an = [ an\n0 ; an\n1 ; ...; an\nK], (8)\n¯Hn+1[¯an] = clone( ¯Hn+1), (9)\n#train #test med max\nQuasar-T 29k 3k 2.8k 8.2k\nSearchQA 100k 27k 2.5k 4.9k\nNQ 292k 8k 6.3k 128k\nTable 1: Statistics of Question Answering datasets.\n#train: number of questions in the training set. #test:\nnumber of questions in the test set. med: median length\nof the context. max: max length of the context.\nwhere ¯Hn+1 is the ﬁnal output hidden state of this\nlayer and has the same word order as the input ¯Hn.\nIn experiments, we stack these two types of layer\ninterchangeably to capture both global and local\ncontext efﬁciently.\n4 Experiments\n4.1 Datasets\nWe evaluate our proposed approach on multiple\nquestion answering benchmarks. The statistics of\nall the datasets are summarized in Table 1.\n• Quasar-T1 (Dhingra et al., 2017): The goal of\nthis task is to answer open-domain questions\nfrom Trivia Challenge. All the passages har-\nvested through information retrieval can be used\nto answer questions. The task requires the model\nto generate answers in phrases. The evaluation\nmetric on this dataset is based on Exact Match\nand F1 score of the bag-of-words matching. Our\nevaluation tool2 comes from the SQuAD dataset.\n• SearchQA3 (Dunn et al., 2017): The setting\nof this dataset is the same as Quasar-T, except\nthat the questions are sourced from Jeopardy!\ninstead.\n• Natural Questions4 (Kwiatkowski et al., 2019):\nThis task aims to answer questions based on a\ngiven Wikipedia document, and has two settings.\n(i) Long answer: select a paragraph that can an-\nswer the question based on the Wikipedia docu-\nment if any. (ii) Short answer: extract an answer\nphrase from the document if the document con-\ntains the answer. As the given document may not\ncontain answer, we can either predict an answer\nor predict no answer. The evaluation metric on\nthis dataset is the F1 score, where true positives\nare exactly correct answers, false positives are\n1https://github.com/bdhingra/quasar\n2https://rajpurkar.github.io/SQuAD-explorer/\n3https://github.com/nyu-dl/dl4ir-searchQA\n4https://ai.google.com/research/NaturalQuestions\nQuasar-T SearchQA NQ(long) NQ(short)\nEM/F1 EM/F1 F1 F1\nR3 (Wang et al., 2018) 35.3/41.7 49.0/55.3 - -\nDECAPROP (Tay et al., 2018) 38.6/46.9 62.2/70.8 - -\nDS-QA (Lin et al., 2018) 42.2/49.3 58.8/64.5 - -\nMulti-passage BERT (Wang et al., 2019) 51.1/59.1 65.1/70.7 - -\nDrQA (Chen et al., 2017) 37.7/44.5 41.9/48.7 46.1 35.7\nDecAtt + DocReader (Kwiatkowski et al., 2019) - - 54.8 31.4\nBERTjoint (Alberti et al., 2019) - - 64.7 52.7\nBERTwwm + SQuAD2 (Pan et al., 2019) - - 68.2 57.2\nRikiNet-RoBERTa (Liu et al., 2020) - - 75.3 59.3\nSliding Window 52.9/62.8 65.8/73.2 75.3 56.4\nSparse Attention (Child et al., 2019) 52.1/62.0 64.7/71.7 74.5 56.1\nLocality-Sensitive Hashing (Kitaev et al., 2020) 53.2/62.9 66.0/73.5 75.5 56.4\nCluster-Former (#C=64) 53.3/63.3 67.0/74.2 76.3 56.7\nCluster-Former (#C=256) 53.6/63.5 67.5/74.5 76.3 56.7\nCluster-Former (#C=512) 54.0/63.9 68.0 /75.1 76.5 57.1\nTable 2: Results on Quasar-T, SearchQA test sets and NQ dev set. #C: number of clusters.\nLong Answer Short Answer\nF1 Precision Recall F1 Precision Recall\nBigBird-ETC-large (Zaheer et al., 2020) 77.8 77.5 78.1 57.9 63.7 53.0\nRikiNet (Liu et al., 2020) 76.1 78.1 74.2 61.3 67.6 56.1\nCluster-Former (Ours) 78.0 78.5 77.5 60.9 62.1 59.8\nTable 3: Results on Natural Questions (NQ) leaderboard (test set). We show two published results here from over\n40 submissions. Our model achieves No.1 for long answer and No.4 for short answer.\nincorrect answer predictions, and false negatives\nare incorrect “no answer” predictions. As the\ntest set is hidden, we split 5% of the training\nset for validation, and use the original validation\nset for testing. We use the ofﬁcial tool from the\ndataset to evaluate our models. We also submit\nour best model to the leaderboard.\n4.2 Implementation Details\nAll the models are trained on 8 Nvidia V100 GPUs.\nFor clustering, we adopt “Yinyang kmeans” (Ding\net al., 2015)5 which takes less than 5 seconds for\nclustering in all our experiment settings. We set\nthe memory size for clustering M = 100, 000 in\nAlgorithm 1. Based on our experiments, it makes\nlittle difference for memory banks with 50k and\n100k, update cycles with 1 iteration or half itera-\ntion. We use cluster centroids that perform the best\non the validation set for test set experiments. As\n5https://github.com/src-d/kmcuda\nthe cluster-centroid is ofﬂine computed, the infer-\nence time is the same as the sliding-window-based\nmethod. We initialize our models with RoBERTa-\nlarge (Liu et al., 2019). As the number of posi-\ntion embeddings of RoBERTa is limited to 512,\nwe cannot assign different position embeddings to\nall tokens. Instead, we assign the same position\nembeddings to each chunked sequence.\nThe majority of our model is made up of Sliding-\nWindow Layers, as the local information is essen-\ntial for QA tasks. We adopt the proposed Cluster-\nFormer Layer in layers 15 and 20 to further capture\nlong-range information. We set the sliding win-\ndow size l to 256, stride m to 224, and change the\nnumber of clusters in {64, 256, 512}to analyze its\nimpact on the ﬁnal performance. We prepend a spe-\ncial token to the beginning of all the given/retrieved\nparagraphs and directly concatenate all the para-\ngraphs as the ﬁnal context sequence. Due to mem-\nory constraints, we set the max length to be 5000\n3 4 5 6\n8 55.7/65.0 55.6/64.4 54.7/64.3 55.4/64.6\n12 55.1/64.9 55.8/65.0 56.1/65.4 55.4/64.6\n16 55.6/65.0 55.2/64.7 55.1/64.6 54.8/64.1\n20 54.8/64.2 55.4/64.8 55.1/64.6 -\nTable 4: Experiments on Quasar-T dev dataset. a ∈\n{3, 4, 5, 6}and b ∈{8, 12, 16, 20}, if the layer number\nl % a == 0 and l >= b, we set it as Cluster-Former\nLayer, otherwise Sliding Window Layer.\nduring training and 10000 during inference. Dur-\ning dataset ﬁnetuning, we use Adam (Kingma and\nBa, 2015) to optimize the model. We set warm-up\nupdates to 2,220, maximal updates to 22,200, learn-\ning rate to 5 ×10−5, and batch size to 160. We\ntune the dropout rate from {0.1, 0.15, 0.2}for all\nthe methods including baselines and report the best\nresults. The model converges in one day for all the\nQA datasets.\nFor Quasar-T and SearchQA, we predict the\nstart and end positions of the answer. For Natu-\nral Question, we ﬁrst identify whether the question\nhas short/long answers or not based on the mean\nvalues of the ﬁrst hidden state of all the chunked\nsequences, 1\nK\n∑K\nk=1 HN\nk [0], where K is the num-\nber of chunks and N is the number of layers. If\nanswerable, we rank all the candidates for long\nanswer selection, and predict the start and end po-\nsitions of short answers. Our model submitted to\nNatural Question Leaderboard ensembled 3 mod-\nels with 512 clusters, and only these models are\nﬁrstly trained on SQuAD2.0 and then ﬁnetuned on\nNatural Question dataset.\n4.3 Baselines\nWe compare our models with several strong base-\nlines, including:\nR3 (Wang et al., 2018) proposes to use rein-\nforcement learning to jointly train passage ranker\nand reader. DS-QA (Lin et al., 2018) proposes to\nﬁrst use paragraph selection to ﬁlter the noisy data\nand then trained model on denoised data. Multi-\npassage BERT (Wang et al., 2019) proposes to ﬁl-\nter the passages and then merge multiple useful pas-\nsages into one sequence, which can be encoded by\nBERT.DrQA (Chen et al., 2017) makes use of at-\ntention mechanism across the question and the doc-\nument for answer phrase extraction. DecAtt and\nDocReader (Kwiatkowski et al., 2019) is based on\na pipeline approach that ﬁrst uses a simpler model\nWikitext Enwik8\nppl bpc\nSliding window 20.8 1.34\nSparse Attention 20.5 1.29\nLocality-Sensitive Hashing 20.8 1.33\nCluster-Former (#C=64) 20.5 1.28\nCluster-Former (#C=256) 20.3 1.24\nCluster-Former (#C=512) 20.2 1.22\nTable 5: Results on Language Modeling. #C: number\nof clusters; Wikitext: Wikitext-103.\nto select long answers and then a reading com-\nprehension model to extract short answers from\nthe long answers. BERTjoint (Alberti et al., 2019)\njointly trains short and long answer extraction in a\nsingle model rather than using a pipeline approach.\nBERTwwm+SQuAD2 (Pan et al., 2019) makes use\nof multi-task learning to further boost performance.\nRikiNet-RoBERTa (Liu et al., 2020) proposes a\ndynamic paragraph dual-attention reader and a\nmulti-level cascaded answer predictor. BigBird-\nETC (Zaheer et al., 2020) makes use of a sparse\nattention mechanism to encode long sequences.\nWe also re-implement several strong baselines\nwhich have not been applied to process long context\nin question answering tasks:\n• Sliding Window: The original method is fully\nmade up of Sliding-Window Layers and can only\nattend to local information. To make a fair com-\nparison among different methods on long-range\ninformation collection, we replace several layers\nof this sliding window baseline with Sparse At-\ntention, Locality-Sensitive Hashing, and Cluster-\nFormer.\n• Sparse Attention (Child et al., 2019): This\nmethod replaces several layers in the previous\nbaseline by training a Transformer layer across\nsequences on pre-selected positions. We run this\nsparse Transformer on all the hidden states in\nthe same position across sequences, so that the\noutput of sparse Transformer can merge the in-\nformation from different sequences.\n• Locality-Sensitive Hashing (Kitaev et al.,\n2020): This method hashes hidden states\ninto different buckets determined by randomly-\ninitialized hashing vectors. A Transformer layer\nis then applied across buckets to build Sparse\nQuestion Where did the underground railroad start and ﬁnish ?\nContext The Underground Railroad by artist Charles T. Webber , 1893 Date Late 1700s - 1865\nLocation Northern United States with routes to Canada , Mexico ...\nSpecial token <s><s><s>Island island in the colonies city<s><s><s>With in the in .\nTime did start and ﬁnish 1893 Date 1700 1865 Location Participants Outcome Deaths 19\n1763\nStopwords the the , the , , , , to , , , , the American runaway slaves of free states the , , , it to , a the\nEntity Canada Mexico Canada is applied Florida Spanish Railroad Railroad Railroad\nPositions 49, 50, 51, 52, 53, 54, 55, 115, 116, 168, 273, 394, ..., 6022, 6040, 6042, 6060, 6094\nTable 6: An example from Natural Question dataset. The rows in the middle section show the corresponding words\nof the clustered hidden states, and the bottom row shows the positions of the clustered hidden states. “<s>” refers\nto start token of long answer candidate.\nAttention across the whole sequence. Note that\nthis method cannot be directly used for question\nanswering without adding Sliding-Window layer,\nas our QA model is initialized by RoBERTa that\nonly has 512 position embeddings.\n4.4 Experimental Results\nState-of-the-Art Results on QA Table 2 and 3\nshow that our proposed method outperforms sev-\neral strong baselines, thanks to its ability to encode\nboth local and global information. Cluster-Former\nwith 512 clusters achieves new state-of-the-art re-\nsults on Quasar-T, SearchQA and Natural Question\n(long answer).\nEffect of Cluster-Former We also test the abil-\nity of Cluster-Former on modeling long-range de-\npendencies. Note that Sparse Attention (Child et al.,\n2019) and Locality-Sensitive Hashing (Kitaev et al.,\n2020) have never been tested on question answer-\ning tasks with long context. For fair comparison,\nwe set the layers 15 and 20 as either Sparse At-\ntention, Locality-Sensitive Hashing or our Cluster-\nFormer, and the left layers are Sliding Window\nlayers.\nAs shown, Sparse Attention performs worse than\nour Cluster-Former. The loss may come from the\nnoise introduced by pre-selected positions, the cor-\nresponding words of which may not be related.\nWe set the number of hashing vectors in Locality-\nSensitive Hashing (LSH) to 64, the same as the\nnumber of clusters in Cluster-Former. LSH outper-\nforms the baseline slightly on QA and consistently\nunderperforms our Cluster-Former (#C=64). Over-\nall, our Cluster-Former performs the best.\nEffect of Number of Cluster Centroids We\nalso test the effect of different numbers of cluster\ncentroids (C) on model performance. We observe\nthat the model with 512 clusters works signiﬁcantly\nbetter than the model with 64 clusters on most of\nthe tasks. However, for Natural Questions Long\nAnswer setting, the improvement is marginal. As\nwe mainly rely on the hidden state of special tokens\n“<s>” for long answer selection, and the same to-\nkens can be assigned into same chunk more easily\neven with a smaller number of clusters.\nSelection of Cluster-Former Layers We also\nhave an analysis on which layers are better used\nfor Cluster-Former layer. As shown in Table 4, we\nconduct a hyper-parameter search. And ﬁnd that it\ncan get better performance with at least one Cluster-\nFormer layers in the middle layer (8-16). The worst\nresults come from only one Cluster-Former layer\nin the layer of 22 or 23.\nLanguage Modeling Although we focus on QA\ntasks, to demonstrate the versatility of Cluster-\nFormer, we conduct additional experiments on lan-\nguage modeling using the Wikitext-103 (Merity\net al., 2017) and Enwik8 (Mahoney, 2011) bench-\nmarks. All the models are trained from scratch.\nWe set the number of layers to 16, with 8 heads\nper layer. Our Cluster-Former Layer is used in\nlayers 11 and 15 as in QA models. We segment\nlong input into short sequences of 3072 tokens, set\nsliding window size l to 256, and stride m to 128.\nSGD is used for optimizing the models. We set\nclip threshold of gradients to 0.1, warm-up updates\nto 16,000, maximal updates to 286,000, dropout\nrate to 0.3, learning rate to 0.1, and batch size to\n16. The model will converge in 3 days for all the\nLM datasets. As shown in Table 5, Cluster-Former\noutperforms strong state-of-the-art baselines.\n4.5 Qualitative Analysis\nWe perform qualitative analysis on how the hidden\nstates are clustered, by visualizing the correspond-\ning words and positions of the hidden states in Ta-\nble 6. From the ﬁrst row, we can see that the special\ntokens “<s>” tend to belong to the same cluster.\nNote that “<s>” is the start token of each long an-\nswer candidate, and its hidden state is used for ﬁnal\nlong answer selection. Therefore, Transformer on\nthis cluster can compare across the candidates to\nmake the ﬁnal prediction.\nWe further observe that the same types of to-\nken are more likely to appear in the same cluster.\nFor example, words from the second row to the\nforth row cover the topics of time, stopwords, and\norganization & geopolitical entities.\nFinally, we randomly sample a cluster and list\nthe positions of clustered hidden states in the last\nrow of the table. We ﬁnd that states in long dis-\ntance, such as the 50-th and 6060-th states (over\n6000 tokens apart), can be in one cluster, which\ndemonstrates the ability of Cluster-Former in de-\ntecting long-range dependencies. Further, we ob-\nserve that states tend to cluster in phrases. For\nexample, we see consecutive positions such as “49,\n50, 51, 52, 53, 54, 55”, which likely results from\nthe sliding-window encoding.\n5 Conclusion\nIn this paper, we present Cluster-Former, a new\nmethod to encode global information for long se-\nquences. We achieve new state of the art on three\nquestion answering datasets: Quasar-T, SearchQA,\nand Natural Questions. Further, we observe that\na larger number of clusters in Cluster-Former can\nlead to better performance on question answering\ntasks. Cluster-Former is a generic approach, and\nwe believe that it can beneﬁt other NLP tasks that\nrely on long-range dependencies as well.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Philip\nPham, Anirudh Ravula, and Sumit Sanghai. 2020.\nEtc: Encoding long and structured data in transform-\ners. In Empirical Methods in Natural Language Pro-\ncessing (EMNLP).\nChris Alberti, Kenton Lee, and Michael Collins. 2019.\nA bert baseline for the natural questions. arXiv\npreprint arXiv:1901.08634.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In International Conference\non Learning Representations (ICLR).\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Association for Computa-\ntional Linguistics (ACL).\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nEunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia\nPolosukhin, Alexandre Lacoste, and Jonathan Be-\nrant. 2017. Coarse-to-ﬁne question answering for\nlong documents. In Association for Computational\nLinguistics (ACL).\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Jared Davis, Tamas Sarlos,\nDavid Belanger, Lucy Colwell, and Adrian Weller.\n2020. Masked language modeling for proteins via\nlinearly scalable long-context transformers. arXiv\npreprint arXiv:2006.03555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nBhuwan Dhingra, Kathryn Mazaitis, and William W\nCohen. 2017. Quasar: Datasets for question an-\nswering by search and reading. arXiv preprint\narXiv:1707.03904.\nYufei Ding, Yue Zhao, Xipeng Shen, Madanlal Musu-\nvathi, and Todd Mytkowicz. 2015. Yinyang k-\nmeans: A drop-in replacement of the classic k-\nmeans with consistent speedup. In International\nconference on machine learning (ICML).\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur\nGuney, V olkan Cirik, and Kyunghyun Cho. 2017.\nSearchqa: A new q&a dataset augmented with\ncontext from a search engine. arXiv preprint\narXiv:1704.05179.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In European Chap-\nter of the Association for Computational Linguistics\n(EACL).\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Franc ¸ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. arXiv preprint arXiv:2006.16236.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations (ICLR).\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. 2019.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Interna-\ntional Conference on Machine Learning (ICML).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. In Association for Computational\nLinguistics (ACL).\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun.\n2018. Denoising distantly supervised open-domain\nquestion answering. In Association for Computa-\ntional Linguistics (ACL).\nDayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng\nChen, Daxin Jiang, Jiancheng Lv, and Nan Duan.\n2020. Rikinet: Reading wikipedia pages for natu-\nral question answering. In Association for Computa-\ntional Linguistics (ACL).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMatt Mahoney. 2011. Large text compression bench-\nmark. URL: http://www. mattmahoney. net/text/text.\nhtml.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In International Conference on Learning Repre-\nsentations (ICLR).\nLin Pan, Rishav Chakravarti, Anthony Ferritto,\nMichael Glass, Alﬁo Gliozzo, Salim Roukos, Radu\nFlorian, and Avirup Sil. 2019. Frustratingly\neasy natural question answering. arXiv preprint\narXiv:1909.05286.\nJack W Rae, Anna Potapenko, Siddhant M Jayaku-\nmar, and Timothy P Lillicrap. 2019. Compressive\ntransformers for long-range sequence modelling. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2021. Synthesizer:\nRethinking self-attention in transformer models.\nIn International Conference on Machine Learning\n(ICML).\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and\nDa-Cheng Juan. 2020a. Sparse sinkhorn attention.\nIn International Conference on Machine Learning\n(ICML).\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nYi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su.\n2018. Densely connected attention propagation for\nreading comprehension. In Advances in Neural In-\nformation Processing Systems (NeurIPS).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nApoorv Vyas, Angelos Katharopoulos, and Franc ¸ois\nFleuret. 2020. Fast transformers with clustered at-\ntention. Advances in Neural Information Processing\nSystems (NeurIPS).\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. R3:\nReinforced ranker-reader for open-domain question\nanswering. In AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI).\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nal-\nlapati, and Bing Xiang. 2019. Multi-passage bert:\nA globally normalized bert model for open-domain\nquestion answering. In Empirical Methods in Natu-\nral Language Processing (EMNLP).\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS).\nJunru Zhou and Hai Zhao. 2019. Head-driven phrase\nstructure grammar parsing on penn treebank. In As-\nsociation for Computational Linguistics (ACL).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7079789638519287
    },
    {
      "name": "Cluster analysis",
      "score": 0.6621308922767639
    },
    {
      "name": "Sliding window protocol",
      "score": 0.6286013722419739
    },
    {
      "name": "Transformer",
      "score": 0.5997459292411804
    },
    {
      "name": "ENCODE",
      "score": 0.5555221438407898
    },
    {
      "name": "Quadratic growth",
      "score": 0.4631001651287079
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4518170654773712
    },
    {
      "name": "Cluster (spacecraft)",
      "score": 0.41288405656814575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4087035357952118
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35265734791755676
    },
    {
      "name": "Algorithm",
      "score": 0.24794137477874756
    },
    {
      "name": "Window (computing)",
      "score": 0.11024567484855652
    },
    {
      "name": "Engineering",
      "score": 0.10431772470474243
    },
    {
      "name": "Computer network",
      "score": 0.09694433212280273
    },
    {
      "name": "Voltage",
      "score": 0.09634828567504883
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}