{
  "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
  "url": "https://openalex.org/W4389524398",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2980499624",
      "name": "Gangwoo Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2166198374",
      "name": "Sung-Dong Kim",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "University of Richmond",
        "Naver (South Korea)",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2398026421",
      "name": "Byeongguk Jeon",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2112029105",
      "name": "Joon-Suk Park",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Naver (South Korea)",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2181668919",
      "name": "Jaewoo Kang",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100292568",
    "https://openalex.org/W3142869857",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3204850704",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4389524305",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285307817",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4307937379",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4389523719",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287208383",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4311726857",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4385573898",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3173947338"
  ],
  "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQâ€”via few-shot prompting leveraging external knowledgeâ€”and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 996â€“1009\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nTree of Clarifications: Answering Ambiguous Questions\nwith Retrieval-Augmented Large Language Models\nGangwoo Kim1 Sungdong Kim2,3,4 Byeongguk Jeon1 Joonsuk Park2,3,5 Jaewoo Kang1â€ \nKorea University1 NA VER Cloud2 NA VER AI Lab3\nKAIST AI4 University of Richmond5\n{gangwoo_kim, bkjeon1211, kangj}@korea.ac.kr\nsungdong.kim@navercorp.com park@joonsuk.org\nAbstract\nQuestions in open-domain question answering\nare often ambiguous, allowing multiple inter-\npretations. One approach to handling them is\nto identify all possible interpretations of the\nambiguous question (AQ) and to generate a\nlong-form answer addressing them all, as sug-\ngested by Stelmakh et al. (2022). While it\nprovides a comprehensive response without\nbothering the user for clarification, considering\nmultiple dimensions of ambiguity and gather-\ning corresponding knowledge remains a chal-\nlenge. To cope with the challenge, we propose a\nnovel framework, TREE OF CLARIFICATIONS\n(TOC): It recursively constructs a tree of disam-\nbiguations for the AQâ€”via few-shot prompt-\ning leveraging external knowledgeâ€”and uses\nit to generate a long-form answer. TOC out-\nperforms existing baselines on ASQA in a few-\nshot setup across all metrics, while surpass-\ning fully-supervised baselines trained on the\nwhole training set in terms of Disambig-F1\nand Disambig-ROUGE. Code is available at\ngithub.com/gankim/tree-of-clarifications.\n1 Introduction\nIn open-domain question answering (ODQA),\nusers often ask ambiguous questions (AQs), which\ncan be interpreted in multiple ways. To handle\nAQs, several approaches have been proposed, such\nas providing individual answers to disambiguated\nquestions (DQs) for all plausible interpretations of\nthe given AQ (Min et al., 2020) or asking a clarifi-\ncation question (Guo et al., 2021). Among them,\nwe adopt that of Stelmakh et al. (2022), which pro-\nvides a comprehensive response without bothering\nthe user for clarification: The task is to identify\nall DQs of the given AQ and generate a long-form\nanswer addressing all the DQs (See Figure 1).\nThere are two main challenges to this task: (1)\nthe AQ may need to be clarified by considering mul-\nâ€ Corresponding author\nğ‘«ğ‘¸ğŸğ‘«ğ‘¸ğŸ ğ‘«ğ‘¸ğŸ‘\nQuestion \nClarification\nQuestion \nClarification\nğ‘«ğ‘¸ğŸğŸğ‘«ğ‘¸ğŸğŸ ğ‘«ğ‘¸ğŸğŸ‘ ğ‘«ğ‘¸ğŸğŸ’ğ‘«ğ‘¸ğŸğŸ ğ‘«ğ‘¸ğŸğŸ‘ğ‘«ğ‘¸ğŸğŸ\nğ‘¨ğ’ğ’ƒğ’Šğ’ˆğ’–ğ’ğ’–ğ’” ğ‘¸ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’ (ğ‘¨ğ‘¸)\nğ‘·ğ’‚ğ’”ğ’”ğ’‚ğ’ˆğ’†ğ’”\nAnswer\nGeneration\nâ€œThe United States has the \nmost total medals . . . \nNorway has won most \nmedals  in winter Olympic.â€\nğ‘³ğ’ğ’ğ’ˆ ğ‘­ğ’ğ’“ğ’ ğ‘¨ğ’ğ’”ğ’˜ğ’†ğ’“\nPruned\nâ€¦\nâ€œWhat country has the most medals \nin Olympic history?â€\nâ€œWhat country has \nthe most gold medals\nin Olympic history?â€\nQuestion \nClarification\nInformation\nRetrieval\nâ€œWhat country has the \nmost  medals in winter \nOlympic history?â€\nâ€œWhat  country has \nthe most total medals\nin Olympic history?â€ \nğ‘»ğ’“ğ’†ğ’† ğ’ğ’‡ ğ‘ªğ’ğ’‚ğ’“ğ’Šğ’‡ğ’Šğ’„ğ’‚ğ’•ğ’Šğ’ğ’ğ’”\nğ‘«ğ‘¸ğŸğŸ’\nFigure 1: Overview of TREE OF CLARIFICATIONS . (1)\nrelevant passages for the ambiguous question (AQ) are\nretrieved. (2) leveraging the passages, disambiguated\nquestions (DQs) for the AQ are recursively generated\nvia few-shot prompting and pruned as necessary. (3) a\nlong-form answer addressing all DQs is generated.\ntiple dimensions of ambiguity. For example, the\nAQ â€œwhat country has the most medals in Olympic\nhistoryâ€ in Figure 1 can be clarified with respect\nto the type of medalsâ€”gold, silver, or bronzeâ€”or\nOlympicsâ€”summer or winter; and (2) substantial\nknowledge is required to identify DQs and respec-\ntive answers. For example, it requires knowledge\nto be aware of the existence of different types of\nmedals and the exact counts for each country.\nTo address the challenges and provide a long-\nform answer to AQ, we propose a novel framework,\nTREE OF CLARIFICATIONS (TOC): It recursively\nconstructs a tree of DQs for the AQâ€”via few-shot\n996\nprompting leveraging external knowledgeâ€”and\nuses it to generate a long-form answer. More specif-\nically, first, relevant passages for the AQ are re-\ntrieved. Then, leveraging the passages, DQs for the\nAQ are recursively generated via few-shot prompt-\ning and pruned as necessary. Lastly, a long-form\nanswer addressing all DQs is generated. The tree\nstructure promotes exploring DQs in targeting par-\nticular dimensions of clarification, addressing the\nfirst challenge, and the external sources offer ad-\nditional knowledge to cope with the second chal-\nlenge.\nExperiments demonstrate that our proposed use\nof LLMs with retrieval-augmentation and guid-\nance to pursue diverse paths of clarification results\nin the new state-of-the-art on ASQA (Stelmakh\net al., 2022)â€”a long-form QA benchmark for AQs.\nTOC outperforms existing baselines on ASQA in\na few-shot setup across all metrics. In addition,\nthis 5-shot performance surpasses that of the fully-\nsupervised baselines trained on the whole training\nset by 7.3 and 2.9 in terms of Disambig-F1 and\nDisambig-ROUGE, respectively.\nThe main contribution of this work is proposing\na novel framework, TREE OF CLARIFICATIONS\n(TOC), for generating long-form answers to AQs\nin ODQA, advancing the state-of-the-art on the\nASQA benchmark. TOC introduces two main in-\nnovations:\nâ€¢ It guides LLMs to explore diverse paths of\nclarification of the given AQ in a tree structure\nwith the ability to prune unhelpful DQs.\nâ€¢ To the best of our knowledge, it is the first\nto combine retrieval systems with LLM for\ngenerating long-form answers to AQs.\n2 Related Work\nA line of studies (Min et al., 2020, 2021; Gao et al.,\n2021; Shao and Huang, 2022) extends retrieve-and-\nread frameworks dominant in ODQA task (Chen\net al., 2017; Karpukhin et al., 2020; Lewis et al.,\n2020; Izacard and Grave, 2021) to clarify AQ and\ngenerate DQs with corresponding answers to them.\nHowever, their approaches require fine-tuning mod-\nels on the large-scale train set. On the other hand,\nour framework enables LLM to generate a compre-\nhensive response addressing all DQs via few-shot\nprompting.\nRecent studies introduce LLM-based methods to\ngenerate a long-form answer to the AQ. Amplayo\net al. (2023) suggest optimal prompts specifically\nengineered for the task. Kuhn et al. (2022) prompt\nLLMs to clarify ambiguous questions selectively.\nHowever, the studies do not utilize external infor-\nmation to ensure the factual correctness of the dis-\nambiguations, thereby potentially increasing the\nrisk of hallucinations from LLMs. Moreover, the\nresults could be bounded by inherent parametric\nknowledge of LLM. Concurrently, Lee et al. (2023)\nautomatically generate clarifying questions to re-\nsolve ambiguity.\nOur framework involves the recursive tree archi-\ntecture, inspired by several prior studies. Min et al.\n(2021) propose the tree-decoding algorithm to au-\ntoregressively rerank passages in ambiguous QA.\nGao et al. (2021) iteratively explore additional inter-\npretations and verify them in a round-trip manner.\nConcurrently, extending chain of thoughts (Wei\net al., 2022) prompting, Yao et al. (2023) apply\nthe tree architecture to reasoning tasks for deduc-\ntive or mathematical problems. On the contrary,\nTOC recursively clarifies questions and introduces\na self-verification method to prune unhelpful DQs.\n3 Tree of Clarifications\nWe introduce a novel framework,TREE OF CLARI -\nFICATIONS (TOC), as illustrated in Figure 1. We\nfirst devise retrieval-augmented clarification (RAC;\nSec. 3.1), a basic component that clarifies AQ and\ngenerates DQs based on relevant passages. TOC\nexplores various fine-grained interpretations, rep-\nresented as a tree structure (TS; Sec. 3.2) by re-\ncursively performing RAC and pruning unhelpful\nDQs. Lastly, it aggregates the tree and generates\na long-form answer addressing all valid interpreta-\ntions.\n3.1 Retrieval-Augmented Clarification (RAC)\nWe first retrieve relevant Wikipedia documents\nfor the AQ by using two retrieval systems, Col-\nBERT (Khattab and Zaharia, 2020) and Bing search\nengine1. ColBERT is a recent dense retriever that\nhas effective and efficient zero-shot search qual-\nity. Following Khattab et al. (2022), we use the\noff-the-shelf model pre-trained on MS-Marco (Ba-\njaj et al., 2016). We additionally include the Bing\nsearch engine to promote the diversity of retrieved\nWikipedia passages. Finally, we obtain over 200\npassages by combining passages retrieved by each\nsystem.\n1https://www.microsoft.com/bing\n997\nAfter collecting a passage set for the AQ, we\nrerank and choose top- k passages and augment\nthem to a prompt. We use SentenceBERT (Reimers\nand Gurevych, 2019) pre-trained on MS-Marco\nas the reranker backbone. For in-context learn-\ning setup, we dynamically choose k-shot examples\nwith the nearest neighbor search2 and add them to\nthe prompt. We initiate with the instruction of Am-\nplayo et al. (2023) and revise it for our setup. Given\nthe prompt with relevant passages and AQs, LLM\ngenerates all possible DQs and their corresponding\nanswers3.\n3.2 Tree Structure (TS)\nTo effectively explore the diverse dimensions of\nambiguity, we introduce a recursive tree structure\nof clarifications. Starting from the root node with\nAQ, it progressively inserts child nodes by recur-\nsively performing RAC, each of which contains a\ndisambiguated question-answer pair. In each ex-\npansion step, passages are reranked again regarding\nthe current query. It allows each step to focus on its\nown DQ, encouraging TOC to comprehend a wider\nrange of knowledge. Exploration of a tree ends\nwhen it satisfies termination conditions; it reaches\nthe maximum number of valid nodes or the max-\nimum depth. We choose the breadth-first search\n(BFS) by default, hence the resulting tree could\ncover the broader interpretations4.\nPruning with Self-Verification To remove un-\nhelpful nodes, we design a pruning method, in-\nspired by current studies for self-verification (Ka-\ndavath et al., 2022; Cole et al., 2023). Specifically,\nwe check the factual coherency between the an-\nswers in a target node and the AQ in the root node.\nBy doing so, we discard the generated DQs that\nask different or irrelevant facts from the original\none. For example, given an AQ â€œWho will host the\nnext world cup 2022?â€, a generated disambiguation\nâ€œDQ: Who hosted the world cup 2018? A: Russiaâ€\nis a factually consistent question-answer pair but it\nchanges the original scope of the AQ5. We perform\nself-verification by prompting LLMs to determine\nwhether the current node would be pruned or not.\nPrompted with AQ, the answer to the target DQ,\nand the answer-containing passage, LLM identifies\n2See Appendix A.3 for detailed implementation\n3See Appendix C.2 for example prompts\n4It is suboptimal to adopt the depth-first search since it\nwould encounter unambiguous questions more frequently. See\nAppendix 7 for failure cases.\n5See Appendix C.3 for more detailed case studies\nModel D-F1 R-L DR\nFully-supervised\nT5-Large Closed-Book 7.4 33.5 15.7\nT5-Large w/ JPR 26.4 43.0 33.7\nPaLM w/ Soft Prompt Tuningâˆ— 27.8 37.4 32.1\nFew-shot Prompting (5-shot)\nPaLMâˆ— 25.3 34.5 29.6\nGPT-3âˆ— 25.0 31.8 28.2\nTree of Clarifications (ToC; Ours)\nGPT-3 + RAC 31.1 39.6 35.1\nGPT-3 + RAC + TS 32.4 40.0 36.0\nGPT-3 + RAC + TS w/ Pruning 33.7 39.7 36.6\nâˆ—from Amplayo et al. (2023)\nTable 1: Evaluation results for long-form QA on ambigu-\nous questions from the development set of ASQA (Stel-\nmakh et al., 2022). Baselines are either fully-supervised\nor 5-shot prompted. Note, TOC framework consists of\nretrieval-augmented clarification (RAC) and tree struc-\nture (TS).\nif the given answer could be a correct answer to\nAQ.\nAnswer Generation Once constructing the tree\nof clarifications, TOC aggregates all valid nodes\nand generates a comprehensive long-form answer\nto AQ. It selects the disambiguations in retained\nnodes of the resulting tree with the relevant pas-\nsages. If the number of nodes is insufficient, we\nundo the pruning steps from closer nodes to the\nroot node in BFS order. Passages that contain the\nanswers of valid nodes are prioritized. It finally\ngenerates a long-form answer, encoding AQ, se-\nlected disambiguations, and relevant passages6.\n4 Experiment\n4.1 Experimental Setup\nDatasets All baselines and our framework are\nevaluated on ASQA (Stelmakh et al., 2022). It is a\nlong-form QA dataset built upon the 6K ambiguous\nquestions identified from AmbigNQ (Min et al.,\n2020). More details are in Appendix A.1\nEvaluation Metrics We use three evaluation\nmetrics, following Stelmakh et al. (2022). (1)\nDisambig-F1 (D-F1) measures the factual correct-\nness of generated predictions. It extracts short an-\nswers to each DQ and computes their F1 accuracy.\n(2) ROUGE-L (R-L) measures the lexical overlap\nbetween long-form answers from references and\npredictions. (3) DR score is the geometric mean of\n6See Appendix C.4 for an example prompt\n998\nModel D-F1 R-L DR\nGPT-3 (Baseline) 24.2 36.0 29.5\nGPT-3 w/ RAC 31.1 39.6 35.1\nâˆ’ Disambiguations 30.5 37.3 33.7\nâˆ’ Bing Search Engine 28.5 37.4 32.7\nâˆ’ Retrieval Systems 25.6 35.1 30.0\nTable 2: Ablation study on all components of retrieval-\naugmented clarification (RAC).\ntwo scores, which assesses the overall performance.\nFor validating intermediate nodes, we additionally\nuse Answer-F1 that measures the accuracy of gen-\nerated short answers in disambiguation. Further\ndetails are in Appendix A.2.\nBaselines Stelmakh et al. (2022) propose fine-\ntuned baselines. They fine-tune T5-large (Raf-\nfel et al., 2020) to generate long-form answers\non the whole train set. Models are evaluated in\nthe closed-book setup or combined with JPR (Min\net al., 2021), task-specific dense retriever for am-\nbiguous QA by enhancing DPR (Karpukhin et al.,\n2020). On the other hand, Amplayo et al. (2023)\npropose a prompt engineering method to adapt\nLLMs to the ASQA benchmark. They employ\nPaLM (Chowdhery et al., 2022) and Instruct-\nGPT (Ouyang et al., 2022) that learn the soft\nprompts or adopt in-context learning with few-shot\nexamples. They conduct experiments in the closed-\nbook setup. Note that they share the same back-\nbone with our models, GPT-3 with 175B parame-\nters (text-davinci-002).\n4.2 Experimental Results\nTOC outperforms fully-supervised and few-shot\nprompting baselines. Table 1 shows the long-form\nQA performance of baselines and TOC on the de-\nvelopment set of ASQA. Among baselines, using\nthe whole training set (Fully-supervised) achieves\ngreater performances than Few-shot Prompting\nin all metrics. It implies that long-form QA\ntask is challenging in the few-shot setup. In the\nclosed-book setup, GPT-3 shows competitive per-\nformances with T5-large with JPR in D-F1 score,\nshowing LLMâ€™s strong reasoning ability over its\ninherent knowledge.\nAmong our models, LLM with RAC outper-\nforms all other baselines in D-F1 and DR scores.\nIt indicates the importance of leveraging external\nknowledge in clarifying AQs. Employing the tree\nstructure (TS) helps the model to explore diverse\ninterpretations, improving D-F1 and DR scores by\nFiltration #(DQs) Answer-F1\nw/o Pruning (None) 12,838 40.9\nw Pruning\n+ Deduplication 10,598 40.1\n+ Self-Verification 4,239 59.3\nTable 3: Ablated results with and without pruning meth-\nods. The number of retained DQs after pruning and\nAnswer-F1 are reported.\n1.3 and 0.9. When pruning the tree with our pro-\nposed self-verification (TS w/ Pruning), the model\nachieves state-of-the-art performance in D-F1 and\nDR score, surpassing the previous few-shot base-\nline by 8.4 and 7.0. Notably, it outperforms the best\nmodel in a fully-supervised setup (T5-large with\nJPR) by 7.3 and 2.9. In the experiment, T5-Large\nin a closed-book setup achieves comparable per-\nformance with LLM baselines in ROUGE-L score\ndespite its poor D-F1 scores. It reconfirms the ob-\nservation from Krishna et al. (2021) that shows the\nlimitations of the ROUGE-L metric.\nIntegrating retrieval systems largely con-\ntributes to accurate and diverse disambigua-\ntions. Table 2 displays the ablation study for mea-\nsuring the contributions of each proposed compo-\nnent. When removing disambiguations from few-\nshot training examples, the ROUGE-L score is sig-\nnificantly degraded, which shows the importance\nof the intermediate step to provide the complete\nanswer. Integrating retrieval systems (i.e., Bing\nsearch engine and ColBERT) largely improves the\nmodel performance, especially in the D-F1 score.\nIt indicates using external knowledge is key to en-\nhancing the factual correctness of clarification. We\nreport intrinsic evaluation for each retrieval system\nin Appendix B.\nOur pruning method precisely identifies help-\nful disambiguations from the tree. Table 3 shows\nintrinsic evaluation for generated disambiguations,\nwhere all baselines are evaluated with Answer-F1\nscore that measures the F1 accuracy of the answer\nto the target DQ. Compared to the baseline, the\nvalid nodes that pass self-verification contain more\naccurate disambiguations, achieving much higher\nAnswer-F1 score ( +18.4). On the other hand,\nsolely using deduplication does not advance the\naccuracy, indicating the efficacy of our proposed\nself-verification method.\n999\n5 Discussion\nAmbiguity Detection TOC is designed to clar-\nify AQs without bothering users; hence does not\nexplicitly identify whether the given question is am-\nbiguous or not. It tries to perform clarification even\nif the question cannot be disambiguated anymore,\noften resulting in generating duplicate or irrele-\nvant DQs7. However, we could presume a question\nto be unambiguous if it can no longer be disam-\nbiguated8. In TOC, when it fails to disambiguate\nthe given question or all generated disambiguations\nare pruned, the question could be regarded as un-\nambiguous.\nComputational Complexity Although TOC re-\nquires multiple LLM calls, its maximum number\nis less than 20 times per question. Exploration of\nthe tree ends when it obtains the pre-defined num-\nber of valid nodes (10 in our experiments). Since\nthe clarification process generates from two to five\ndisambiguations for each question, it satisfies the\ntermination condition in a few steps without the\npruning method. Failing to expand three times in a\nrow also terminates the exploration. Pruning steps\nconsume a smaller amount of tokens since they\nencode a single passage without few-shot exem-\nplars. Compared to the existing ensemble methods\nsuch as self-consistency (Wei et al., 2022) which\ncannot be directly adopted to the generative task,\nToC achieves a state-of-the-art performance with a\ncomparable number of LLM calls.\nGeneralizability The key idea of ToC could be\npotentially generalized to other tasks and model\narchitectures. It has a model-agnostic structure that\ncould effectively explore diverse paths of recursive\nreasoning, which would be helpful for tasks that re-\nquire multi-step reasoning, such as multi-hop QA.\nFuture work might investigate the generalizability\nof TOC to diverse tasks, datasets, and LM architec-\ntures.\n6 Conclusion\nIn this work, we propose a novel framework, TREE\nOF CLARIFICATIONS . It recursively builds a tree of\ndisambiguations for the AQ via few-shot prompting\nwith external knowledge and utilizes it to generate a\n7See Appendix 7 for failure cases\n8The idea is aligned with the annotation process of Am-\nbigQA (Min et al., 2020), in which the target question is\nclassified as ambiguous if multiple distinct answers to it were\nobserved.\nlong-form answer. Our framework explores diverse\ndimensions of interpretations of ambiguity. Experi-\nmental results demonstrate TOC successfully guide\nLLMs to traverse diverse paths of clarification for\na given AQ within tree structure and generate com-\nprehensive answers. We hope this work could shed\nlight on building robust clarification models, which\ncan be generalized toward real-world scenarios.\nLimitations\nAlthough TOC is a model-agnostic framework that\ncould be combined with other components, our\nstudy is limited in demonstrating the generalizabil-\nity of different kinds or sizes of LLMs. In addition,\nthe experiments are only conducted on a bench-\nmark, ASQA (Stelmakh et al., 2022). Although\nTOC enables LLM to explore diverse reasoning\npaths by iteratively prompting LLM, the cost of\nmultiple prompting is not negligible.\nWe tried the recent prompting method, chain\nof thoughts (Wei et al., 2022), but failed to en-\nhance the performance in our pilot experiments. It\nmight indicate the disambiguation process requires\nexternal knowledge, which shows the importance\nof document-grounded or retrieval-augmented sys-\ntems. Future work could suggest other pruning\nmethods that identify unhelpful DQs more effec-\ntively. The performance could be further enhanced\nby using the state-of-the-art reranker in the an-\nswer sentence selection task, as proposed by recent\nworks (Garg et al., 2020; Lauriola and Moschitti,\n2021).\nAcknowledgements\nThe first author, Gangwoo Kim, has been sup-\nported by the Hyundai Motor Chung Mong-Koo\nFoundation. This research was supported by the\nNational Research Foundation of Korea (NRF-\n2023R1A2C3004176, RS-2023-00262002), the\nMSIT (Ministry of Science and ICT), Korea, under\nthe ICT Creative Consilience program (IITP-2022-\n2020-0-01819) supervised by the IITP (Institute\nfor Information & communications Technology\nPlanning & Evaluation), and the Electronics and\nTelecommunications Research Institute (RS-2023-\n00220195).\n1000\nReferences\nReinald Kim Amplayo, Kellie Webster, Michael Collins,\nDipanjan Das, and Shashi Narayan. 2023. Query\nrefinement prompts for closed-book long-form ques-\ntion answering. Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. 30th Conference on Neu-\nral Information Processing Systems (NIPS 2016),\nBarcelona, Spain.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870â€“1879.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJeremy R Cole, Michael JQ Zhang, Daniel Gillick, Ju-\nlian Martin Eisenschlos, Bhuwan Dhingra, and Jacob\nEisenstein. 2023. Selectively answering ambiguous\nquestions. arXiv preprint arXiv:2305.14613.\nYifan Gao, Henghui Zhu, Patrick Ng, Cicero dos San-\ntos, Zhiguo Wang, Feng Nan, Dejiao Zhang, Ramesh\nNallapati, Andrew O Arnold, and Bing Xiang. 2021.\nAnswering ambiguous questions through generative\nevidence fusion and round-trip prediction. In Pro-\nceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 3263â€“\n3276.\nSiddhant Garg, Thuy Vu, and Alessandro Moschitti.\n2020. Tanda: Transfer and adapt pre-trained trans-\nformer models for answer sentence selection. In\nProceedings of the AAAI conference on artificial in-\ntelligence, volume 34, pages 7780â€“7788.\nMeiqi Guo, Mingda Zhang, Siva Reddy, and Malihe\nAlikhani. 2021. Abg-coqa: Clarifying ambiguity in\nconversational question answering. In 3rd Confer-\nence on Automated Knowledge Base Construction.\nGautier Izacard and Ã‰douard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874â€“880.\nJeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data, 7(3):535â€“547.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769â€“6781.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Effi-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research\nand development in Information Retrieval, pages 39â€“\n48.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4940â€“4957.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2022.\nClam: Selective clarification for ambiguous ques-\ntions with large language models. arXiv preprint\narXiv:2212.07769.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: A benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:452â€“\n466.\nIvano Lauriola and Alessandro Moschitti. 2021. An-\nswer sentence selection using local and global context\nin transformer models. In European Conference on\nInformation Retrieval, pages 298â€“312. Springer.\nDongryeol Lee, Segwang Kim, Minwoo Lee, Hwan-\nhee Lee, Joonsuk Park, Sang-Woo Lee, and Kyomin\nJung. 2023. Asking clarification questions to han-\ndle ambiguity in open-domain qa. arXiv preprint\narXiv:2305.13808.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntÃ¤schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459â€“9474.\n1001\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina\nToutanova, and Hannaneh Hajishirzi. 2021. Joint\npassage ranking for diverse multi-answer retrieval.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6997â€“7008.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. Ambigqa: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783â€“\n5797.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730â€“27744.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485â€“5551.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you donâ€™t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784â€“789.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 3982â€“3992.\nZhihong Shao and Minlie Huang. 2022. Answering\nopen-domain multi-answer questions via a recall-\nthen-verify framework. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1825â€“\n1838.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8273â€“8288, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776â€“5788.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824â€“24837.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\n1002\nA Experimental Setup Details\nA.1 Ambiguous QA Datasets\nAll baselines and our framework are evaluated\non ASQA benchmark (Stelmakh et al., 2022).\nIt is a long-form QA dataset built on the sub-\nset of ambiguous questions identified from Am-\nbigNQ dataset (Min et al., 2020). It contains open-\ndomain questions collected from Natural Ques-\ntions (Kwiatkowski et al., 2019). ASQA consists of\n6,316 ambiguous questions and their long-form an-\nswers with disambiguations, split into 4,353, 948,\nand 1,015 train, development, and test set, respec-\ntively.\nA.2 Evaluation Metrics\nFollowing Stelmakh et al. (2022), we use three eval-\nuation metrics on ASQA. First, ROUGE-L (R-L)\nmeasures the lexical overlap between long-form an-\nswers from references and system-generated predic-\ntions. Since the benchmark provides two ground-\ntruth answers, we report the maximum ROUGE-L\nscore. Disambig-F1 (D-F1) measures the factual\ncorrectness of generated predictions. A reading\ncomprehension model, RoBERTa (Liu et al., 2019)\ntrained on SQuADv2 (Rajpurkar et al., 2018), finds\nshort answers to the ground-truth DQs from the\ngenerated long-form response. Then, F1 accuracy\nof the detected answer is calculated to check if the\nlong-form answer contains accurate information.\nDisambiguation-ROUGE (DR) score is computed\nas the geometric mean of ROUGE-L and Disambig-\nF1 to measure the overall performance. We addi-\ntionally use Answer-F1 to validate the disambigua-\ntions. It computes the maximum F1 accuracy of\nanswers to a single DQ. We use ground-truth dis-\nambiguations provided by AmbigNQ (Min et al.,\n2020).\nA.3 Implementation Details\nA large portion of our implementation is based on\nthe DSP library (Khattab et al., 2022). To dynami-\ncally find few-shot examples with the nearest neigh-\nbor search, we use pre-trained MiniLM (Wang\net al., 2020) to obtain hidden representations of\nquestions and compute similarity scores with Faiss\nlibrary (Johnson et al., 2019). We add 5-shot train-\ning examples to the prompt, following Amplayo\net al. (2023). It was the optimal number in our pilot\nexperiment.\nFor prompting LLM to perform RAC, we use\ntop-5 relevant passages. To determine whether to\nRetrieval System AC@10 AC@30 AC@100\nColBERTv2 56.4 68.4 73.4\nw/ Reranker 56.8 69.0 73.4\nBing Search Engine 43.3 58.3 73.5\nw/ Reranker 62.7 68.0 72.8\nCombined w/ Reranker 64.2 77.4 80.1\nTable 4: Intrinsic evaluation of retrieval systems. An-\nswer coverage at k (AC@k) measures the proportion\nof disambiguated answers that are covered by top-k re-\ntrieved passages.\nprune the target node or not, we rerank and pick the\nmost relevant passage among those containing the\nanswer in the target node. In the answer generation\nprocess, we took ten valid disambiguations in BFS\norder and five answer-containing passages. We use\nAPI served by OpenAI9 to employ GPT-3 as our\nbackbone. We set max tokens as 300 and top-p as\n1.0.\nB Additional Experiment\nB.1 Intrinsic Evaluation for Retrieval Systems\nWe randomly sample 100 examples from ASQA\ndataset and report intrinsic evaluation results for\nretrieval systems. Since a single AQ has multi-\nple ground-truth DQs and their answers, it is not\ntrivial to check how many answers are covered by\nretrieved passages. Inspired by Min et al. (2021),\nwe devise an evaluation proxy, answer coverage,\nfor measuring the quality of retrieved passages in\nambiguous QA tasks. We consider the retrieval\nas successful if the retrieved passages contain one\nof the answers to the target DQ. We calculate the\nproportion of success among DQs for a single AQ\nto check overall answer coverage.\nTable 4 compares retrieval systems in answer\ncoverage (AC@k) of top-k passages. Bing search\nengine without reranker performs worst among\nbaselines in AC@10 and @30. However, with\nreranker, its performances are greatly enhanced,\noutperforming ColBERT baselines. When combin-\ning two retrieval systems, it shows the best perfor-\nmances across all evaluation metrics; hence two\nresults are complementary. It achieves 80.1 in\nAC@100 scores, which indicates the passage set\nhas sufficient information if properly explored.\n9https://platform.openai.com/docs/api-reference\n1003\nC Qualitative Analysis\nC.1 Prompt Format\nWe add format descriptions to our prompt follow-\ning Khattab et al. (2022). Table 5 displays the for-\nmat specifically designed to generate disambigua-\ntions for a given question based on external doc-\numents. The format description is augmented to\nprompts of both RAC and the answer generation.\nBy using it, we encouarge LLM to comply with the\nformat.\nC.2 Question Clarification\nTable 6 shows an example of RAC for the AQ.\nRetrieval systems provide the external knowl-\nedge. Leveraging it, LLM generates disambiguated\nquestion-answer pairs. In RAC, long-form answers\nare also generated to follow the format but we do\nnot use them in the later steps.\nIn Table 7, we observe the cases where TOC en-\ncounters unambiguous questions and fails to clarify\nthem. It often asks different or irrelevant facts from\nthem of original AQ.\nC.3 Self Verification\nTable 8, 9 show examples of self-verification\nprompt. We prompt LLM to verify the current\nanswer is factually coherent with AQ based on the\nrelevant passage. It generates â€˜Trueâ€™ or â€˜Falseâ€™ to\ndetermine whether the node would be discarded or\nnot. We do not provide few-shot training examples\nor formats.\nC.4 Answer Generation\nTable 10 depicts an example of answer generation\nprompt. We use a similar prompt to that of RAC\nexcept disambiguations are given as inputs. It en-\ncodes up to ten disambiguations and five relevant\npassages.\n1004\nFollow the following format.\nContext:\n${sources that may contain relevant content}\nQuestion: ${ambiguous question to be disambiguated}\nDisambiguations: ${the disambiguated pairs of questions and answers, each is separated by a new\nline.}\nDQ i: ${(i)-th disambiguated question that clarifies the ambiguous question}\nDA i: ${short factoid answers separated by semi-colon (;) to (i)-th disambiguated question, often\nbetween 1 and 5 words}\nAnswer: ${a thorough, detailed answer that explains the multiple interpretations of the original\nquestion and includes the appropriate disambiguations, at least three sentences.}\nTable 5: Format description for both RAC and the answer generation.\nI will provide ambiguous questions that can have multiple answers based on their different possible\ninterpretations. Clarify the given question into several disambiguated questions and provide short\nfactoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at\nleast three sentences. Here are some examples.\nContext:\n[1] Fred and George Weasley | Fred and George Weasley are fictional characters in the \"Harry Potter\"\nbook series written by J. K. Rowling. The characters are the identical twin brothers ... The twins were\nplayed by identical twin brothers James and Oliver Phelps in the film adaptations\n[2] James and Oliver Phelps | James Andrew Eric Phelps and Oliver Martyn John Phelps (born 25\nFebruary 1986) are English actors and identical twin brothers. They are known for playing Fred and\nGeorge Weasley in the \"Harry Potter\" film series from 2001 to 2011 ...\nÂ· Â· Â·\n[5] Chris Rankin | plays of \"Bugsy Malone\" and \"The Lion, The Witch and The Wardrobe\". His\nprofessional acting career began when he won the role of Percy Weasley in September 2000 ... after his\ncharacterâ€™s absence from \"Harry Potter and the Goblet\"\nQuestion: Who played the weasley brothers in harry potter?\nDisambiguations:\nDQ 1: Who played the fictional characters Fred and George Weasley in the \"Harry\nPotter\" book series?\nDA 1: James and Oliver Phelps\nDQ 2: Who are the English actors and identical twin brothers known for playing\nFred and George Weasley in the \"Harry Potter\" film series?\nDA 2: James and Oliver Phelps\nDQ 3: Who is the actor that plays Percy Weasley in the Harry Potter series?\nDA 3: Chris Rankin\nAnswer: The Weasley brothers in the Harry Potter series were played by identical\ntwin brothers James and other Oliver Phelps. The Phelps brothers are English actors\nwho have appeared in films and TV shows together as a duo. Chris Rankin is the\nactor who played Percy Weasley in the Harry Potter series. Rankin is a British\nactor who has appeared in theatre, film, and TV.\nTable 6: Example prompt and output in RAC. Few-shot training examples and format descriptions are omitted for\nsimplicity.\n1005\nI will provide ambiguous questions that can have multiple answers based on their different possible\ninterpretations. Clarify the given question into several disambiguated questions and provide short\nfactoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at\nleast three sentences. Here are some examples.\nFollow the following format.\nContext:\n[1] 1991 Major League Baseball All-Star Game | The 1991 Major League Baseball All-Star Game ...\nÂ· Â· Â·\n[5] Venues of the 1996 Summer Olympics | would serve as host to the Peach Bowl from ...\nQuestion: When did Toronto host the MLB All-Star Game in 1991?\nDisambiguations:\nDQ 1: When was the 1991 Major League Baseball All-Star Game played?\nDA 1: July 9, 1991\nDQ 2: What was the outcome of the 1991 Major League Baseball All-Star Game?\nDA 2: American League defeated the National League\nAnswer: The 1991 Major League Baseball All-Star Game was ...\nI will provide ambiguous questions that can have multiple answers based on their different possible\ninterpretations. Clarify the given question into several disambiguated questions and provide short\nfactoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at\nleast three sentences. Here are some examples.\nFollow the following format.\nContext:\n[1] Highest-paid NBA players by season | Highest-paid NBA players by season The highest-paid NBA\nplayers by season over ...\nÂ· Â· Â·\n[5] Highest-paid NBA players by season | Highest-paid NBA players ...\nQuestion: Who was the highest-paid NBA player in the 2017-2018 season?\nDisambiguations:\nDQ 1: Who was the highest-paid NBA player in the 2017-2018 season by salary?\nDA 1: LeBron James\nDQ 2: Who was the highest-paid NBA player in the 2017-2018 season by total earnings?\nDA 2: LeBron James\nAnswer: LeBron James was the highest-paid NBA player in the 2017-2018 season ...\nTable 7: Failure case where the model encounters and clarifies unambiguous questions. Few-shot training examples\nand format descriptions are omitted for simplicity.\n1006\nCorrect Case 1\nDQ: Who was selected to host the 2018 FIFA World Cup?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed\nanswer could be correct answers or not with only â€˜Trueâ€™ or â€˜Falseâ€™\nContext:\n2018 and 2022 FIFA World Cup bids | FIFAâ€™s headquarters in Zurich. Russia was chosen to host the\n2018 World Cup, and Qatar was chosen to host the 2022 World Cup. This made Russia the first Eastern\nEuropean country to host the World Cup, while Qatar would be the first Middle Eastern country to\nhost the World Cup. Blatter noted that the committee had decided to â€œgo to new landsâ€ and reflected a\ndesire to â€œdevelop footballâ€ by bringing it to more countries. In each round a majority of twelve votes\nwas needed. If no bid received 12 votes in a round, the bid with the fewest votes\nQuestion: Who is hosting the next world cup 2022?\nProposed Answer: Russia\nFalse\nCorrect Case 2\nDQ: Which player has won the most World Series in baseball?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed\nanswer could be correct answers or not with only â€˜Trueâ€™ or â€˜Falseâ€™\nContext:\nWorld Series ring | on World Series rings. The New York Yankees Museum, located in Yankee Stadium,\nhas an exhibit with replicas of all Yankeesâ€™ World Series rings, including the pocket watch given after\nthe 1923 World Series. Yogi Berra won the most World Series rings with 10, as a player. Frankie\nCrosetti won 17 as a player and as a coach. Yogi Berra Museum and Learning Center. World Series\nring A World Series ring is an award given to Major League Baseball players who win the World Series.\nSince only one Commissionerâ€™s Trophy is awarded to the team, a World Series ring is\nQuestion: Whoâ€™s won the most world series in baseball?\nProposed Answer: Yogi Berra\nTrue\nTable 8: Correct cases of pruning method. Few-shot training examples or formats are not augmented to the prompt.\nGenerated texts are colored green.\n1007\nIncorrect Case 1\nDQ: Who is the highest goalscorer in world football in a single game?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed\nanswer could be correct answers or not with only â€˜Trueâ€™ or â€˜Falseâ€™\nContext:\nList of footballers with the most goals in a single game | This is a list of players with the most goals in\na football game. The list only includes players who have scored the most multiple goals in first class or\nfully professional matches for country or club.The current world record for an international is held by\nArchie Thompson, who scored 13 goals against American Samoa in Australiaâ€™s 31â€“0 victory during\nthe 2002 FIFA World Cup qualification. David Zdrilic scored 8 goals.In November 2022, Shokhan\nNooraldin Salihi scored 15 goals in the match of Al-Hilal against Sama in the 2022â€“23 Saudi Womenâ€™s\nPremier League. In this match, Al-Hilal beat Sama 18-0.\nQuestion: Who has the highest goals in world football?\nProposed Answer: Archie Thompson\nFalse\nIncorrect Case 2\nDQ: When was episode 113 of Dragon Ball Super released in the US?\nI will provide a question, relevant context, and proposed answer to it. Identify whether the proposed\nanswer could be correct answers or not with only â€˜Trueâ€™ or â€˜Falseâ€™\nContext:\nDragon Ball Super | would be available in the United States in summer 2017. Bandai has also\nannounced the updated â€œDragon Ball Super Card Gameâ€ that starts with one starter deck, one special\npack containing 4 booster packs and a promotional Vegeta card and a booster box with 24 packs. It was\nreleased on July 28, 2017. A line of six â€œDragon Ball Superâ€ Happy Meal toys were made available at\nJapanese McDonaldâ€™s restaurants in May 2017. The average audience TV rating in Japan was 5.6%\n(Kanto region). The maximum audience rating was 8.4% (Episode 47) and the lowest rating was 3.5%\n(Episodes 109-110).\nQuestion: When is episode 113 of dragon ball super coming out?\nProposed Answer: November 5, 2017\nFalse\nTable 9: Incorrect cases of self-verification. Generated texts are colored green.\n1008\nI will provide ambiguous questions that can have multiple answers based on their different possible\ninterpretations. Clarify the given question into several disambiguated questions and provide short\nfactoid answers to each question. Subsequently, summarize them into a detailed long-form answer of at\nleast three sentences. Here are some examples.\nContext:\n[1] Game of Thrones | Game of Thrones Game of Thrones is an American fantasy drama television\nseries created by David Benioff and D. B. Weiss. ... and its seventh season ended on August 27, 2017.\nThe series will conclude with its eighth season\n[2] Game of Thrones | Game of Thrones is an American fantasy drama television series created by\nDavid Benioff and for HBO. It is an adaptation of \"A Song of Ice and Fire\", ... Set on the fictional\ncontinents of Westeros and Essos, \"Game of Thrones\" has a large ensemble cast\nÂ· Â· Â·\n[5] A Game of Thrones (comics) | A Game of Thrones (comics) A Game of Thrones is the comic book\nadaptation of George R. R. Martinâ€™s fantasy novel \"A Game of Thrones\", . . . It is intended to follow\nthe story and atmosphere of the novel closely, at a rate of about a page of art for each page of text, and\nQuestion: What kind of series is game of thrones?\nDisambiguations:\nDQ 1: What is the genre of the American television series Game of Thrones?\nDA 1: fantasy drama\nDQ 2: What is the genre of the comic book series A Game of Thrones?\nDA 2: fantasy\nÂ· Â· Â·\nDQ 10: What is the genre of the board game A Game of Thrones?\nDA 10: strategy\nAnswer: There are multiple works that share the title Game of Thrones. The first\nis a television series that is a fantasy drama, the second is a comic book series\nthat is fantasy, the third is a book series that is fantasy, and the fourth is a\nboard game that is a strategy game.\nTable 10: Example prompt for the answer generation process. Generated texts are colored green.\n1009",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.846920907497406
    },
    {
      "name": "Question answering",
      "score": 0.8214868903160095
    },
    {
      "name": "Ambiguity",
      "score": 0.8126087188720703
    },
    {
      "name": "Tree (set theory)",
      "score": 0.6910257339477539
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.608249843120575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5330789089202881
    },
    {
      "name": "Code (set theory)",
      "score": 0.5082049369812012
    },
    {
      "name": "Natural language processing",
      "score": 0.4885624349117279
    },
    {
      "name": "Language model",
      "score": 0.48736220598220825
    },
    {
      "name": "Information retrieval",
      "score": 0.4750635325908661
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.42269638180732727
    },
    {
      "name": "Machine learning",
      "score": 0.3911476731300354
    },
    {
      "name": "Mathematics",
      "score": 0.06924265623092651
    },
    {
      "name": "Programming language",
      "score": 0.06461203098297119
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}