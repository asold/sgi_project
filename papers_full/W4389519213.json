{
  "title": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs",
  "url": "https://openalex.org/W4389519213",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5050600731",
      "name": "Yichuan Li",
      "affiliations": [
        null,
        "Worcester Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5044455276",
      "name": "Kaize Ding",
      "affiliations": [
        null,
        "Worcester Polytechnic Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5103224637",
      "name": "Kyumin Lee",
      "affiliations": [
        null,
        "Worcester Polytechnic Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2987283559",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4377864091",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W4386804245",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W3035324702",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2130354913",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W3114632476",
    "https://openalex.org/W4283796586",
    "https://openalex.org/W3171592446",
    "https://openalex.org/W4306175892",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3212640459",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3155224787",
    "https://openalex.org/W4307477897",
    "https://openalex.org/W4285723986",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4225657277",
    "https://openalex.org/W4286889809",
    "https://openalex.org/W4382239158",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4221145116",
    "https://openalex.org/W4385567522",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3022061250",
    "https://openalex.org/W3100078588",
    "https://openalex.org/W3154091824",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3023960840",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "Self-supervised representation learning on text-attributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice. To solve the problem of self-supervised representation learning on text-attributed graphs, we develop a novel Graph-Centric Language model ‚Äì GRENADE. Specifically, GRENADE harnesses the synergy of both pre-trained language model and graph neural network by optimizing with two specialized self-supervised learning algorithms: graph-centric contrastive learning and graph-centric knowledge alignment. The proposed graph-centric self-supervised learning algorithms effectively help GRENADE to capture informative textual semantics as well as structural context information on text-attributed graphs. Through extensive experiments, GRENADE shows its superiority over state-of-the-art methods.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2745‚Äì2757\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nGrenade: Graph-Centric Language Model for Self-Supervised\nRepresentation Learning on Text-Attributed Graphs\nYichuan Li\nWorcester Polytechnic Institute\nyli29@wpi.edu\nKaize Ding‚àó\nNorthwestern University\nkaize.ding@northwestern.edu\nKyumin Lee\nWorcester Polytechnic Institute\nkmlee@wpi.edu\nAbstract\nSelf-supervised representation learning on text-\nattributed graphs, which aims to create expres-\nsive and generalizable representations for vari-\nous downstream tasks, has received increas-\ning research attention lately. However, ex-\nisting methods either struggle to capture the\nfull extent of structural context information\nor rely on task-specific training labels, which\nlargely hampers their effectiveness and gener-\nalizability in practice. To solve the problem of\nself-supervised representation learning on text-\nattributed graphs, we develop a novelGraph-\nCentric Language model ‚ÄìGrenade. Specifi-\ncally,Grenade exploits the synergistic effect\nof both pre-trained language model and graph\nneural network by optimizing with two special-\nizedself-supervisedlearningalgorithms: graph-\ncentric contrastive learning and graph-centric\nknowledge alignment. The proposed graph-\ncentric self-supervised learning algorithms ef-\nfectively helpGrenade to capture informative\ntextual semantics as well as structural context\ninformation on text-attributed graphs. Through\nextensive experiments,Grenade shows its su-\nperiority over state-of-the-art methods. Im-\nplementation is available at https://github.com/\nbigheiniu/GRENADE.\n1 Introduction\nText-Attributed Graph (TAG) (Yang et al., 2021)\n(a.k.a., Textual Graph) has been widely used for\nmodeling a variety of real-world applications, such\nas information retrieval (Cohan et al., 2020; Yang\net al., 2021), product recommendation (Zhu et al.,\n2021) and many more. In TAG, each node rep-\nresents a text document, while the relationships\namong these text nodes are depicted by the edges.\nFor instance, in citation networks, text nodes rep-\nresent academic papers, and edges are the citation\nrelationship between different papers. To conduct\ndifferent analytics tasks onTAG, the key is to learn\nexpressive node representations for the text nodes.\n‚àóCorresponding Author.\nRecent research has demonstrated that self-\nsupervised learning (SSL) can substantially im-\nprove the effectiveness of representation learning\non text data (Reimers and Gurevych, 2019; Gao\net al., 2021; Wu et al., 2020) without using human\nsupervision. Thosemethodsarecommonlylearned\nunder the assumption that text documents are inde-\npendently and identically distributed (i.i.d.), which\nneglects the structural interdependencies among\ntext nodes onTAG. However, the interdependen-\ncies between different text documents can provide\nvaluable insights for understanding their semantic\nrelationships. Takecitationnetworksasanexample,\nthose academic papers (text nodes) that have cita-\ntionrelationshipsoftensharesimilartopics. Hence,\nit is necessary forSSL models to account for not\nonly textual semantics but also structural context\ninformation.\nIn fact, self-supervised representation learning\non TAG remains in its infancy:(i) Though recent\nresearch efforts (Zhao et al., 2022; Chien et al.,\n2021; Cohan et al., 2020; Yasunaga et al., 2022)\ntry to empower pre-trained language models (PLM)\nwith structural context information, most of them\nstill stay superficial by designing local structure-\ndependent SSL objectives. For example, both\nGIANT (Chien et al., 2021) andSPECTER (Cohan\net al., 2020) train the language model by inferring\nthe local neighborhood based on representations of\ntext nodes. However, simply relying on thoseSSL\nobjectives cannot help thePLM fully understand\ncomplex graph structures, especially compared to\nmodels like graph neural networks (GNN) (Kipf and\nWelling, 2017; Velickovic et al., 2018; Hamilton\net al., 2017; Ding et al., 2022a);(ii) Meanwhile,\nanother line of research (Mavromatis et al., 2023;\nZhao et al., 2022) try to combine the advantages\nof bothPLM and GNN by distilling the knowledge\nfromonetotheother(Hintonetal.,2015)andhave\nshown promising results. Nonetheless, one major\nissue is that those methods are task-specific (e.g.,\n2745\nsemi-supervised node classification) and require\nhuman-annotated labels to enable knowledge distil-\nlation. Such an inherent limitation jeopardizes the\nversatilityoftheirmodelsforhandlingdifferentand\neven unseen downstream tasks, which runs counter\nto the goal ofSSL.\nTo go beyond the existing learning paradigms\nand capture informative textual semantic and graph\nstructure information, we develop a new model for\nself-supervised learning onTAG, namelyGrenade\n(Graph-Centric Language Model). Grenade is\nbuilt with a PLM encoder along with an adju-\nvant GNN encoder that provides complementary\nknowledge for it. More importantly,Grenade is\nlearned through two new self-supervised learning\nalgorithms: Graph-Centric Contrastive Learning\n(GC-CL), astructure-awareand augmentation-free\ncontrastive learning algorithm that improves the\nrepresentation expressiveness by leveraging the in-\nherentgraphneighborhoodinformation;andGraph-\nCentric Knowledge Alignment (GC-KA), which en-\nables thePLM and GNN modules to reinforce each\nother by aligning their learned knowledge encoded\nin the text node representations.\nSpecifically,GC-CL enforces neighboring nodes\nto share similar semantics in the latent space by\nconsideringthemaspositivepairs. Evenwithoutus-\ning data augmentation,GC-CL performs node-wise\ncontrastive learning to elicit the structural context\ninformation from TAG. In the meantime,GC-KA\nbridges the knowledge gap betweenPLM and GNN\nby performing dual-level knowledge alignment on\nthecomputedrepresentations: atthenodelevel, we\nminimize the distance between the representations\nlearned from two encoders that focus on different\nmodalities. Attheneighborhoodlevel,weminimize\nthe discrepancy between two neighborhood simi-\nlarity distributions computed fromPLM and GNN.\nBy virtue of the two proposed graph-centric self-\nsupervisedlearningalgorithms,weareabletolearn\nGrenade that can generate expressive and general-\nizablerepresentationsforvariousdownstreamtasks\nwithout using any human supervision. In summary,\nour work has the following contributions:\n‚Ä¢ We developGrenade, which is a graph-centric\nlanguage model that addresses the underexplored\nproblem of self-supervised learning onTAG.\n‚Ä¢ We propose two new self-supervised learning\nalgorithms forTAG, which allow us to perform\ncontrastive learning and knowledge alignment in\na graph-centric way.\n‚Ä¢ We conduct extensive experiments to show that\nourmodel Grenade significantlyandconsistently\noutperforms state-of-the-art methods on a wide\nspectrum of downstream tasks.\n2 Problem Definition\nNotations. We utilize bold lowercase letters such\nasd torepresentvectors,boldcapitalletterslike W\nto denote matrices and calligraphic capital letters\nlikeWto represent sets. LetG= (A,D) denote\na text-attributed graph with adjacency matrixA ‚àà\n{0,1}|D|√ó|D|and text setD. The Aij = 1when\nthere is a connection between nodeiand j. Each\nnode irepresents a text document which consists\nof a sequence oftokensDi = {wv}|Di|\nv=0.\nProblem 1Given an input text-attributed graph\n(TAG) denoted asG=(A,D), our goal is to learn\na graph-centric language modelPLM(¬∑), that can\ngenerate expressive and generalizable representa-\ntion for an arbitray node i onG: di=PLM(Di).\nNote that the whole learning process is performed\nsolely on the input graphGwithout the utilization\nof human-annotated labels.\n3 Proposed Approach: Graph-Centric\nLanguage Model (Grenade)\nTo learn expressive representations fromTAG in a\nself-supervised learning manner, we propose our\nGraph-Centric Language ModelGrenade, which\nbridges the knowledge gap between Pre-trained\nLanguage Model (PLM) and Graph Neural Network\n(GNN). By optimizing two distinct encoders with a\nsetofnovelself-supervisedlearningalgorithms,the\nPLM encoder andGNN encoder mutually reinforce\neach other, and we can finally derive our Graph-\nCentric Language Model (Grenade). GNN. The\noverall framework is shown in Fig. 1.\n3.1 Model Architecture\nOurproposedmodel Grenade iscomposedofaPre-\ntrained Language Model (PLM) along with a Graph\nNeuralNetwork( GNN),whichareoptimizedbyaset\nof novel self-supervised learning algorithms. We\nfirst introduce the details about those two essential\ncomponents as follows:\nPLM Encoder. The primary componentPLM(¬∑) is\naBERT (Devlin et al., 2018) based text encoder that\nprojects a sequence of tokensDi into a vectorized\ntext node representationdi:\ndi=PLM(Di), (1)\n2746\nGraph-Centric Knowledge Alignment\nNBH-KA \tùëñ 236 45\tùëñ6243\n5\nND-KA\tùëñ6243\n5 \tùëñ 236 45Graph-Centric CL\nGC-CL \tùëñ 236 45\tùëñ6243\n5\nPLMEncoderGNNEncoder\tùëñ6243\n5\nKL\nPositive InstanceNegative InstanceTAG\tùëñ6243\n5\nFigure 1: Illustration ofGrenade. Given a text-attributed graph (TAG), Grenade is jointly optimized by two graph-\ncentric self-supervised learning algorithms: graph-centric contrastive learningGC-CL and a dual-level graph-centric\nknowledge alignment, which comprises node-level alignment (ND-KA) and neighborhood-level alignment (NBH-KA).\nwhere di is the hidden representation of the[CLS]\ntoken computed from the last layer of thePLM\nencoder.\nGNN Encoder. As an adjuvant component, the\nGNN encoderGNN(¬∑) isbuiltwithastackofmessage-\npassing basedGNN layers, which compute the node\ni‚Äôs representation by iteratively aggregating and\ntransformingthefeatureinformationfromitsneigh-\nborhood (Hamilton et al., 2017). For each node\ni, its representation learned from aL-layer GNN\nencoder can be denoted as:\nei=E[i,:],E=GNN(E0,A) (2)\nwhere the input node feature matrixE0 is obtained\nfrom the hidden representations of[CLS] token\nfrom the last layer of a pre-trainedBERT model.\n3.2 Graph-Centric Contrastive Learning\nIn order to improve the learning capability of those\ntwo encoders without using any human-annotated\nlabels, one prevailing way is to conduct contrastive\nlearningfromeitherthetextperspective(Gaoetal.,\n2021) or graph perspective (Ding et al., 2022c).\nHowever, most of the existing contrastive learn-\ning methods have the following two limitations:\n(1) conventional instance-level contrastive learning\nmethods merely encourage instance-wise discrimi-\nnation (Li et al., 2021b; Ding et al., 2023), which\nneglects the property ofTAG, i.e., the relational\ninformation among text nodes. Hence, those in-\nstances that share similar semantics may be unde-\nsirablypushedawayinthelatentspace; (2)existing\nmethods commonly rely on arbitrary augmentation\nfunctions to generate different augmented views\nfor applying contrastive learning, while those aug-\nmentations may unexpectedly disturb the semantic\nmeaning of the original instance (Lee et al., 2022).\nTo counter the aforementioned issues, we pro-\npose a new graph-centric contrastive learning\n(GC-CL) algorithm, which isstructure-awareand\naugmentation-free. GC-CL exploits inherent graph\nknowledge fromTAG and can be applied to both\nthePLM encoder andGNN encoder. As suggested by\nthe Homophily principle (McPherson et al., 2001),\nneighboring nodes commonly share similar seman-\ntics, meaning that their representations should also\nbe close to each other in the latent space. Based\non the PLM representation of nodei, its K-hop\nneighboring nodesN(i), and the nodeiexcluded\nmini-batch instancesB(i), theGC-CL objective for\nPLM can be defined as follows:\nLGC-CL1 = ‚àí1\n|N(i)|\n‚àë\np‚ààN(i)\nlog esim(di,dp)/œÑ\n‚àë\nj‚ààC(i) esim(di,dj)/œÑ,\n(3)\nwhere œÑ denotes the temperature and sim(¬∑,¬∑)\nrepresents the cosine similarity function. Here\nC(i) =N(i) ‚à™B(i). Note that for nodei, we con-\nsiderits PLM representationdiasthequeryinstance.\nThe positive instances are the representations of\nnodei‚ÄôsK-hopneighboringnodes {dp|p‚ààN(i)}.\nMeanwhile, the negative instances are the represen-\ntations of other text nodes excludingiwithin the\nsame mini-batch{dj|j ‚ààB(i)}.\nSimilar to thePLM encoder, we also apply our\nGC-CL algorithmtothe GNN encoderGNN(¬∑). Specif-\n2747\nically, the objective function is defined as follows:\nLGC-CL2 = ‚àí1\n|N(i)|\n‚àë\np‚ààN(i)\nlog esim(ei,ep)/œÑ\n‚àë\nj‚ààC(i) esim(ei,ej)/œÑ,\n(4)\nwhere ei is the query instance. The positive in-\nstances are{ep|p ‚ààN (i)}and the negative in-\nstances are{ej|j ‚ààB(i)}.\nApart from the conventional instance-level con-\ntrastive learning counterparts, our graph-centric\ncontrastive learning also enforces neighboring\nnodes to share similar representations. In a sense,\nthisself-supervisedlearningalgorithmisanalogous\nto performing link prediction task based on the rep-\nresentations learned from thePLM encoder, which\ninherently elicits informative graph knowledge dur-\ning the learning process.\n3.3 Graph-Centric Knowledge Alignment\nIn this work, our ultimate goal is to learn expres-\nsive and generalizable representations that encode\ninformativetextualsemanticswithineachtextnode\nas well as the relational information among nodes.\nHowever,individuallyconductingthegraph-centric\ncontrastive learning on eitherPLM or GNN is not\nenough due to the lack of knowledge exchange be-\ntweenthem. Tobetteralignandenhancetheknowl-\nedge captured by thePLM and GNN encoders, we\nproposeadual-levelgraph-centricknowledgealign-\nmentalgorithmfor TAG,whichincludesNode-Level\nKnowledgeAlignment( ND-KA)andNeighborhood-\nLevel Knowledge Alignment (NBH-KA).\nNode-Level Knowledge Alignment. Different\nfrom the previously introduced graph-centric con-\ntrastive learning, which only focuses on single-\nmodal contrasting,ND-KA tries to align the knowl-\nedge across the two encoders by performing graph-\ncentric contrastive learning in a cross-modal form.\nForeachnode i,basedonitsrepresentationslearned\nfromthe PLM encoderand GNN encoder(i.e., di and\nei, respectively), we formulate the objective of\nND-KA as follows:\nLND-KA= ‚àí1\n|ÀúN(i)|\n‚àë\np‚ààÀúN(i)\n(\nlog esim(ei,dp)/œÑ\n‚àë\nj‚ààÀúC(i) esim(ei,dj)/œÑ\n+ log esim(di,ep)/œÑ\n‚àë\nj‚ààÀúC(i) esim(di,ej)/œÑ\n)\n/2,\n(5)\nwhere ÀúN(i) = {i}‚à™N (i) and ÀúC(i) = ÀúN(i) ‚à™\nB(i). Note that for nodei, we first considerei\nthat is learned from theGNN encoder as the query,\nthen construct the positive and negative instances\nbased on the representations learned from thePLM\nencoder. Specifically,thepositiveinstancesinclude\nboth the representation of nodei as well as the\nrepresentations ofi‚Äôs K-hop neighboring nodes\n(i.e., {dp|p ‚àà ÀúN(i)}), and the negative instances\naretherepresentationsofotherinstanceswithinthe\nsame mini-batch{dj|j ‚ààB(i)}. In the meantime,\nwe also consider thedi as the query and construct\nits corresponding positive and negative instances\nin the same way. Here we omit the illustration for\nsimplicity.\nBy virtue of the proposedND-KA algorithm, the\nrepresentations of the same node learned from two\nseparate encoders will be pulled together in the la-\ntentspace. Inthemeantime, ND-KA alsoencourages\nneighboring nodes to have similar representations\nacross different modalities.\nNeighborhood-Level Knowledge Alignment.\nTo further facilitate knowledge alignment between\nPLM and GNN, we propose Neighborhood-Level\nKnowledgeAlignment( NBH-KA)toaligntheneigh-\nborhood similarity distributions learned from the\ntwo encoders. Specifically,NBH-KA first computes\nthe neighborhood similarity distribution between\nthe query nodeiand itsK-hop neighboring nodes\nN(i) aswellastherestnodeswithinthesamemini-\nbatch B(i) for each encoder. Then we minimize\nthe KL-divergence between the two distributions\nto align the knowledge between two encoders. The\ncorresponding learning objective is:\nLNBH-KA=\n(\nKL(PPLM(i)||PŒ≥(i))\n+ KL(PGNN(i)||PPLM(i))\n)\n/2,\nPPLM(i)=softmaxj‚ààC(i)(sim(di,dj)/œÑ),\nPGNN(i)=softmaxj‚ààC(i)(sim(ei,ej)/œÑ),\n(6)\nwhere PPLM(i) and PGNN(i) are the neighborhood\nsimilarity distributions forPLM encoder andGNN\nencoder respectively. From a certain perspective,\nour NBH-KA algorithm can be considered a self-\nsupervised form of knowledge distillation. Specif-\nically, NBH-KA leverages the neighborhood infor-\nmation as self-supervision to guide the knowledge\nalignment process. Moreover, we conduct two-way\nknowledge alignment across two encoders, which\nis different from original knowledge distillation.\n2748\n3.4 Model Learning\nIn order to learn our graph-centric language model\nGrenade onTAG without using human-annotated\nlabels, we jointly optimize the proposed graph-\ncentric contrastive learning and knowledge align-\nment algorithms. For the sake of simplicity, we\ndefine the overall training loss as follows:\nL=LGC-CL1 + LGC-CL2 + LND-KA + LNBH-KA. (7)\nOnce the training is finished, we can freeze the\nparametersofthe PLM encoderanduseittocompute\ntherepresentationsofeachtextnodewithaforward\npass. The computed representations can be further\nused for different downstream tasks.\n4 Experiment\nTo evaluate the effectiveness of our approach\nGrenade, we conduct comprehensive experiments\non different datasets and various downstream tasks.\n4.1 Experimental Setup\nEvaluation Datasets. We evaluate the general-\nizability of the representations computed from\ndifferent methods on three Open Graph Bench-\nmark (OGB) (Hu et al., 2020) datasets: ogbn-arxiv,\nogbn-products, ogbl-citation2. These datasets are\nutilizedtoevaluatetheperformanceoffew-shotand\nfull-shot node classification, node clustering, and\nlink prediction tasks. It should be noted that ogbn-\narxiv and ogbn-products datasets are not originally\ndesigned for link prediction evaluation. There-\nfore, we create two link prediction tasks based on\nthese two datasets, respectively. Furthermore, we\nincorporate obgl-citation2 into our node classifica-\ntion experiment. The statistical information of the\ndatasets is shown in Tab. 1. More comprehensive\ninformation regarding the dataset extension can be\nfound in Appendix A.\nDataset #Nodes #Edges #Classes\nogbn-arxiv 169,343 1,166,243 40\nogbn-products 2,449,029 61,859,140 47\nogbl-citations2 2,927,963 30,387,995 172\nTable 1: Statistical information of the datasets.\nBaseline Methods. The baseline methods in-\ncluded in our study encompass three categories:\n(1) Untuned representations:BERT (Devlin et al.,\n2018)and OGB representations(Mikolovetal.,2013;\nHu et al., 2020). ForBERT, we extract the final\nlayer‚Äôs hidden state of[CLS] token from frozen\nbert-base-uncased as the text node representa-\ntion. As for OGB, we utilize the default features\nfrom benchmark datasets, such as averaged word\nembeddings and bag-of-words representations.(2)\nText-only self-supervised representation learning\nmodels: BERT+MLM andSimCSE (Gao et al., 2021);\nIn BERT+MLM, we apply masked language model-\ning toBERT for the targetTAG. SimCSE employs\ninstance-wisecontrastivelearningtolearntextnode\nrepresentation. (3) Structure-augmented language\nmodels: This category includes SPECTER (Co-\nhan et al., 2020),GIANT (Chien et al., 2021) and\nGLEM (Zhao et al., 2022).SPECTER applies graph-\ncentric contrastive learning on the language model,\nandGIANT employstheextrememulti-labelclassifi-\ncationtotrainthelanguagemodelforneighborhood\nprediction. It is noteworthy thatGLEM utilizes task-\nspecific labels to alternatively guide the pre-trained\nlanguage modelPLM and graph neural networks\nGNN through self-knowledge distillation. Com-\npared withGLEM, our proposed methodGrenade\nis fully self-supervised and does not rely on any\nhuman-annotated labels. The learned text node\nrepresentations can be efficiently and effectively\ngeneralized to downstream tasks.\nImplementation Details. To ensure a fair com-\nparison, we implemented all baseline methods and\nGrenade using the same language model, specif-\nically bert-base-uncased. For our proposed\nmethod,Grenade, we set theK-hop neighbor as\n1, set the temperature parameterœÑ to 0.05 in all the\nlossfunctions. Theoptimalhyperparameter |N(i)|\nis discussed in ¬ß 4.5. Please refer to Appendix B\nfor additional implementation details.\n4.2 Experimental Results\nFew-shotNodeClassification. Toassessthegen-\neralizability of learned representation to new tasks\nunder low-data scenarios, we conduct experiments\non few-shot node classification. Under this set-\nting, the classification models are trained with\nvarying numbers of labeled instances per class\n(k = {2,4,8,16}). We repeat the experiment 10\ntimesandreportedtheaverageresultsalongwiththe\nstandard deviation. The classification models uti-\nlizedinthisevaluationarethemultilayerperceptron\n(MLP)and GraphSAGE (Hamiltonetal.,2017). The\nhyperparametersfortheclassificationmodelscanbe\nfoundinAppendixB. AstheresultshowedinTab.2,\nseveralobservationscanbemade: (1)Inmostcases,\n2749\nMethods MLP GraphSAGE\nk = 2 k = 4 k = 8 k = 16 k = 2 k = 4 k = 8 k = 16\nogbn-arxiv\nOGB‚ãÜ 32.16¬±1.96 37.81¬±1.62 42.33¬±1.07 45.84¬±0.47 49.91¬±3.46 55.52¬±1.42 59.30¬±1.14 62.21¬±0.58\nBERT 34.06¬±2.73 40.29¬±2.16 46.59¬±0.88 50.17¬±1.02 52.92¬±3.21 57.11¬±1.06 60.36¬±1.17 64.10¬±0.61\nBERT+MLM 38.41¬±2.11 46.72¬±1.72 51.89¬±0.98 55.87¬±1.23 53.02¬±1.26 57.76¬±1.41 61.94¬±0.77 65.22¬±0.49\nSimCSE 28.83¬±1.68 32.65¬±1.46 37.78¬±1.26 43.25¬±0.69 46.61¬±2.97 53.86¬±1.20 57.75¬±0.89 62.39¬±0.61\nSPECTER 50.15¬±2.21 54.46¬±0.96 58.74¬±0.63 61.63¬±0.78 53.85¬±2.27 59.46¬±1.63 63.43¬±0.61 66.41¬±0.45\nGIANT‚ãÜ 48.50¬±2.30 55.72¬±1.90 59.80¬±1.03 64.15¬±0.87 50.18¬±2.46 55.30¬±1.69 59.24¬±1.33 63.48¬±0.77\nGLEM - - - - 27.14¬±2.31 45.52¬±1.27 53.37¬±1.08 55.39¬±0.89\nGrenade 55.85¬±2.34 61.10¬±1.59 63.95¬±0.89 66.62¬±0.47 57.17¬±3.54 60.49¬±0.93 64.65¬±1.08 67.50¬±0.41\nogbn-products\nOGB‚ãÜ 9.47¬±1.51 13.54¬±1.42 16.83¬±2.63 20.71¬±1.32 24.74¬±3.01 33.14¬±2.58 38.38¬±1.18 44.19¬±2.61\nBERT 20.53¬±4.03 30.91¬±2.64 40.82¬±1.52 47.77¬±1.07 40.53¬±2.87 50.43¬±1.73 57.29¬±1.48 59.03¬±0.48\nBERT+MLM 38.54¬±3.35 47.15¬±3.41 55.95¬±0.89 59.57¬±1.53 54.73¬±4.35 60.80¬±2.65 64.63¬±1.94 67.75¬±1.48\nSimCSE 7.76¬±1.22 11.30¬±1.30 17.94¬±1.91 25.61¬±0.72 24.99¬±5.01 37.05¬±1.73 44.74¬±1.96 50.13¬±2.94\nSPECTER 27.86¬±4.14 43.70¬±2.70 51.51¬±1.54 57.63¬±1.45 42.74¬±4.58 51.91¬±2.31 57.72¬±2.52 60.77¬±1.36\nGIANT‚ãÜ 20.83¬±2.68 31.37¬±1.95 42.84¬±2.72 51.36¬±2.19 30.90¬±4.63 40.80¬±4.01 52.34¬±2.20 58.58¬±2.36\nGLEM - - - - 41.72¬±4.62 42.50¬±3.45 43.05¬±2.78 48.64¬±2.01\nGrenade 38.60¬±4.51 49.64¬±1.39 59.34¬±2.38 65.05¬±1.00 59.63¬±2.80 64.95¬±1.63 67.38¬±2.17 70.92¬±1.18\nogbl-citation2\nOGB‚ãÜ 21.78¬±1.55 25.54¬±1.55 27.39¬±0.69 29.53¬±0.70 18.83¬±3.12 22.49¬±1.84 31.38¬±1.89 35.19¬±0.89\nBERT 17.95¬±2.34 20.84¬±2.17 23.84¬±1.36 26.77¬±1.38 21.05¬±2.49 25.65¬±1.58 28.06¬±2.03 35.84¬±1.48\nBERT+MLM 32.03¬±1.84 34.39¬±2.61 38.53¬±1.82 41.66¬±0.98 26.28¬±2.35 28.10¬±2.47 37.69¬±1.39 42.63¬±1.21\nSimCSE 13.27¬±1.08 16.68¬±0.62 20.01¬±1.03 23.57¬±0.77 20.01¬±2.04 28.11¬±2.90 28.17¬±1.06 31.71¬±1.28\nSPECTER 30.81¬±2.94 35.31¬±1.92 39.74¬±1.33 42.31¬±0.64 31.57¬±2.04 35.66¬±2.18 37.23¬±2.70 45.59¬±1.82\nGIANT‚ãÜ 38.93¬±1.81 43.74¬±1.93 48.81¬±1.25 52.05¬±0.72 35.75¬±2.72 38.43¬±3.43 40.99¬±3.42 49.44¬±1.68\nGLEM - - - - 30.86¬±4.62 33.78¬±1.88 47.36¬±2.73 51.42¬±1.41\nGrenade 46.40¬±2.00 47.93¬±1.34 50.61¬±0.71 53.75¬±0.82 40.63¬±3.77 44.44¬±2.63 49.15¬±1.73 52.41¬±1.94\nTable2: Experiment resultsoffew-shotnode classification.‚ãÜ indicatesthatthe textnoderepresentationsareobtained\nfrom their official release.‚àíindicates no result forGLEM. This is because in the representation learning stage,GLEM\nwill utilize the labeled dataset to trainGNN.\nMethods ogbn-arxiv ogbn-products ogbl-citation2\nACC ARI NMI ACC ARI NMI ACC ARI NMI\nOGB 39.57¬±0.39 12.76¬±1.22 17.43¬±0.79 38.53¬±1.43 16.46¬±4.24 20.21¬±1.84 40.31¬±0.40 22.82¬±0.34 40.57¬±0.38\nBERT 35.65¬±1.21 8.41¬±1.01 12.78¬±1.20 48.72¬±0.57 52.17¬±1.81 35.76¬±0.82 40.57¬±0.31 22.58¬±0.84 33.38¬±0.44\nBERT+MLM40.07¬±0.60 15.05¬±0.17 19.24¬±0.51 64.35¬±0.81 69.58¬±1.31 54.64¬±0.31 49.13¬±0.46 31.91¬±0.46 44.44¬±0.34\nSimCSE 33.42¬±0.74 5.80¬±0.40 9.62¬±0.74 39.40¬±1.02 29.00¬±2.22 19.88¬±1.01 26.67¬±0.64 8.49¬±0.41 14.26¬±0.65\nSPECTER 58.00¬±0.66 40.44¬±1.17 42.81¬±0.53 70.15¬±0.67 69.82¬±1.25 58.99¬±0.82 63.36¬±0.36 48.66¬±0.14 58.96¬±0.23\nGIANT 58.00¬±0.82 39.69¬±1.22 43.73¬±0.68 61.99¬±0.78 47.60¬±4.18 47.51¬±1.14 63.06¬±0.53 48.57¬±0.89 58.68¬±0.25\nGrenade 61.96¬±0.79 44.98¬±1.85 49.19¬±0.63 73.54¬±0.75 69.64¬±1.64 64.14¬±0.71 64.89¬±0.30 50.22¬±0.42 59.68¬±0.23\nTable 3: Experiment results of node clustering.\nSSL basedmethodsachievebetterperformancethan\nnon-SSL methods(BERT+MLM,SPECTER andGIANT\n> GLEM), this indicates the significance ofSSL in\nenhancing model transferability to new tasks with\nlimited labels. (2) Among state-of-the-art TAG\nrepresentation models,Grenade achieves the best\nperformance on these datasets. This indicates the\nsuperior generalization ability of representations\nextracted byGrenade. The designed knowledge\nalignment allows theGrenade to integrate the pre-\ntrained knowledge fromPLM encoder and structure\ninductive bias learned byGNN encoder. These ex-\npressiverepresentationscanbeeasilyandefficiently\ngeneralized to few-shot learning tasks.\nFull Data Node Classification. We also conduct\nthenodeclassificationexperimentwithfulltraining\ndataset underMLP, GraphSAGE (Hamilton et al.,\n2017) andRevGAT-KD (Li et al., 2021a). As the\nresult shown in Tab. 4, we can observe that:(1)\nGrenade achieves the best performance across all\nthe baseline methods. (2) The performance gap\nbetweenGrenade and some baseline methods like\nGIANT and GLEM becomes smaller as more labeled\ndata provided, butGrenade is consistently better\nthan these methods.\nNode Clustering. In the node clustering task, we\nutilizethelearnedtextnoderepresentationstotrain\na K-means++ model for clustering instances. We\napply the default hyperparameters ofK-means++\nas provided by scikit-learn (Pedregosa et al., 2011).\nThe number of clusters is set to the number of\nclassesinthedataset,andweassigntheclusterlabel\n2750\nMethods arxiv products citation2\nMLP GraphSAGE RevGAT-KD MLP GraphSAGE MLP GraphSAGE\nOGB 55.50¬±0.23‚ãÜ 71.49¬±0.27‚ãÜ 74.26¬±0.17‚ãÜ 61.06¬±0.08‚ãÜ 75.81¬±0.46 48.98¬±1.74 62.10¬±0.25\nBERT 62.91¬±0.60‚ãÜ 70.97¬±0.33‚ãÜ 73.59¬±0.10‚ãÜ 60.90¬±1.09‚ãÜ 80.70¬±0.50 56.75¬±0.33 60.38¬±0.67\nBERT+MLM 67.71¬±0.22 73.65¬±0.14 75.39¬±0.16 76.86¬±0.24 80.90¬±0.11 66.11¬±0.35 64.57¬±0.50\nSimCSE 60.58¬±0.13 67.26¬±0.24 73.69¬±0.15 67.69¬±0.06 80.41¬±0.44 53.91¬±0.36 58.75¬±0.24\nSPECTER 70.40¬±0.25 74.19¬±0.18 75.61¬±0.67 75.38¬±0.22 79.42¬±0.34 54.20¬±0.67 66.58¬±0.23\nGIANT 73.08¬±0.06‚ãÜ 74.59¬±0.28‚ãÜ 76.12¬±0.16‚ãÜ 79.82¬±0.07‚ãÜ 82.03¬±0.65 67.24¬±0.27 70.32¬±0.27\nGLEM - 73.59¬±0.40 - - 82.02¬±0.62 - 68.25¬±0.18\nGrenade 73.16¬±0.12 75.00¬±0.19 76.21¬±0.17 81.58¬±0.18 83.11¬±0.56 68.11¬±0.34 70.89¬±0.34\nTable 4: Supervised node classification performance comparison on benchmark datasets. Boldfaced numbers\nindicate the best performance of downstream models. The‚ãÜ represents the experiment results adopted from (Chien\net al., 2021), while‚Ä† denotes the experiment results adopted from (Zhao et al., 2022).\nbased on the most common label within each clus-\nter. Following the evaluation protocol described in\n(Dingetal.,2022b),wereportthreeclusteringeval-\nuationmetrics: accuracy( ACC),normalizedmutual\ninformation (NMI), and adjusted rand index (ARI).\nWe exclude theGLEM model from this evaluation\nsince it requires the labels during representation\nlearning. To ensure robustness, we perform 10\nruns ofK-means++ with different random seeds\nand report the average results. As shown in Table\n3, we observe that the structure augmentedSSL\nmethods outperform the text-only self-supervised\nrepresentationlearningmethods( Grenade,GIANT,\nSPECTER > BERT+MLM, SimCSE). This indicates\nstructure-augmented SSL methods can understand\nthe context within graph structure that can lead\nto more accurate node representations, which in\nturn can lead to better clustering. Additionally,\nour proposed methodGrenade consistently out-\nperforms all baseline methods. The improvement\ndemonstrates that Grenade can better preserve\nneighborhood information which will inform the\nclustering methods of how data points are intercon-\nnected or related to each other.\nogbn-arxiv ogbn-products ogbl-citation2\n0\n20\n40\n60\n80MRR (%)27.6\n40.9\n24.123.6\n28.2 26.426.4\n42.5\n22.9\n31.6\n44.2\n16.5\n48.8\n68.8\n39.437.2\n67.1\n43.8\n35.5 38.8\n21.1\n52.7\n72.4\n44.7\nOGB\nBERT\nBERT+MLM\nSimCSE\nSPECTER\nGIANT\nGLEM\nGRENADE\nFigure 2: Experiment results of link prediction.\nLink Prediction. Next, we evaluate the learned\nrepresentation in predicting missing connections\ngiven existing connections fromTAG. We aim to\nrank the positive candidates (1 or 2 positive in-\nstances) higher than the negative candidates (1,000\nnegative instances) for each query node. The eval-\nuation metric used for this task is the mean recip-\nrocal rank (MRR), which measures the reciprocal\nrank of the positive instance among the negative\ninstances for each query instance and takes the av-\nerage over all query instances. As shown in Fig. 2,\nweobservethat Grenade significantlyoutperforms\nother approaches. In fact,Grenade achieves at\nleast a4% performance improvement compared\nto methods that utilize structure-augmented self-\nsupervised learning loss (SPECTER and GIANT)\nacrossalldatasets. Thisdemonstratesthat Grenade\ncan better preserve the neighborhood information,\nwhich is consistent with the findings from ¬ß 4.2.\n4.3 Representation Visualization\nTo visually demonstrate the quality of the learned\nrepresentations, we apply t-distributed stochastic\nneighbor embedding (t-SNE) (Van der Maaten and\nHinton, 2008) to for representation visualization.\nWe compareGrenade with two best-performing\nbaseline methods, includingSPECTER and GIANT\nonthearxivdataset. InFig.3,wepresentthe t-SNE\nvisualization of the embeddings for 10 randomly\nsampled classes comprising 5,000 subsampled in-\nstances. The colors in the visualization correspond\nto the labels of these subsampled instances. From\nFig. 3, we observeGrenade exhibits denser clus-\nters and more explicit boundaries among different\nclasses compared to the baseline methods. This ob-\nservationconfirmsthat Grenade canlearncompact\nintra-class and distinct inter-class representations.\n(a) SPECTER\n (b) GIANT\n (c)Grenade\nFigure 3: Representation visualization on arxiv dataset.\n2751\n4.4 Ablation Study\nTo validate the effectiveness of graph-centric con-\ntrastivelearningandgraph-centricknowledgealign-\nment, we conducted an ablation study onGrenade.\nInthisstudy,werespectivelyremove GC-CL,ND-KA,\nand NBH-KA from the full model and report these\nmodel variants‚Äô performance in Tab. 5. In general,\nthe full modelGrenade has the best performance\nin most cases, and we notice a performance de-\ncline when any of the components is removed or\nreplaced, underscoring the significance of each\ncomponent inGrenade. Remarkably, we observe\na performance improvement in link prediction after\nremoving graph-centric contrastive learning (w/o\nGC-CL > Grenade in terms of MRR). Considering\nthe task similarity betweenGC-CL and the link pre-\ndiction, one possible explanation is that removing\nGC-CL couldhelpthemodelmitigateoverfittingand\nfurtherimproveperformanceforthelinkprediction\ntask. Meanwhile, this observation, in turn shows\nthat the dual-level graph-centric knowledge align-\nment(ND-KA andNBH-KA)iseffectiveforcapturing\nstructural context information from theTAG.\nMethods MLP(ACC) K-means++(ACC) MRR Avg. Rank‚Üì\nGrenade 73.16¬±0.12 61.96¬±0.79 52.73 1.33\nw/oGC-CL 72.83¬±0.08 58.75¬±0.99 55.87 2.67\nw/oND-KA 72.65¬±0.16 60.69¬±0.72 49.15 3 .33\nw/oNBH-KA 73.05¬±0.17 60.50¬±0.86 52.56 2 .67\nTable 5: Ablation study of graph-centric contrastive\nlearning (GC-CL) and knowledge-alignment on ogbn-\narxiv datasets. ‚Äúw/o‚Äù is the abbreviation of ‚Äúwithout‚Äù.\n4.5 Hyperparameter Analysis\nK-hop Neighbors. We delved into understand-\ning the impact of theK-hop neighbor selection on\nGrenade‚Äôs efficiency. The choice of differentK\nvalues directly affects the formulation of positive\npairs in graph-centric contrastive learning (Eq. 3\nandEq.4),andthealignmentofknowledgebetween\nthe graph neural network and the language model\n(Eq.5andEq.6). Basedontheresultspresentedin\nFig.4,itisevidentthataugmentingthehopdistance\nadversely affects performance metrics in full data\nnode classification (ACC ofMLP), node clustering\n(ACC), and link prediction (MRR). This suggests\nthat 1-hop neighbors optimally capture structural\nknowledge within our algorithm. However, when\nextending to2-hop or3-hop neighbors, there‚Äôs a\nheightened risk of integrating noisy data. This in-\nsightalignswiththeconclusionsdrawnfromrelated\nresearch,specifically SPECTER (Cohanetal.,2020).\nWe contend that our methodology strikes a harmo-\nnious balance between assimilating structural data\nandfilteringoutextraneousnoise, therebyensuring\nconsistent performance in our assessments.\n1 2 3\nNode Classification\n71.7\n72.1\n72.5\n72.9\n73.4ACC (%)\n1 2 3\nNode Clustering\n59.6\n60.3\n61.0\n61.6\n62.3\n1 2 3\nLink Prediction\n47.2\n48.7\n50.1\n51.5\n53.0\nMRR (%)\nFigure 4: Hyperparameter evaluation forK-hop neigh-\nbors in the ogbn-arxiv dataset. Dashed lines indicate\nthe peak performance ofGrenade with K = 1.\n1-Hop Neighbor Size. One crucial aspect of\nGrenade‚ÄôsSSL objectives is the hyperparameter\n|N(i)|, which controls the number of 1-hop neigh-\nbors considered for representation learning. To\ninvestigate the impact of subsampled neighbor size\ninGrenade,weconductahyperparameteranalysis\nonthefulltrainingdatasetnodeclassification,node\nclustering, and link prediction tasks. As shown in\nFig. 5, we observe thatGrenade achieves its best\nperformance with a practical number of neighbors\n(|N(i)|= 2 for ogbn-arxiv and|N(i)|= 1 for\nogbn-products). This finding is particularly advan-\ntageous as it reduces the computational burden of\nthe PLM encoder in graph-centric contrastive learn-\ning and knowledge alignment between thePLM and\nGNN encoders.\n1 2 3 4 5\nNode Classification\n72.3\n72.6\n72.8\n73.1\n73.4ACC (%)\n1 2 3 4 5\nNode Clustering\n59.0\n59.9\n60.8\n61.6\n62.5\n1 2 3 4 5\nLink Prediction\n50.4\n51.0\n51.6\n52.2\n52.8\nMRR (%)\n(a) ogbn-arxiv\n1 2 3 4 5\nNode Classification\n79.3\n79.9\n80.6\n81.2\n81.9ACC (%)\n1 2 3 4 5\nNode Clustering\n71.6\n72.4\n73.3\n74.2\n75.0\n1 2 3 4 5\nLink Prediction\n66.9\n68.3\n69.8\n71.2\n72.7\nMRR (%)\n(b) ogbn-products\nFigure 5: Analysis of hyperparameters on both ogbn-\narxiv and ogbn-products datasets. The dashed line\nrepresentsGrenade‚Äôs optimal result when|N(i)|= 2\nfor ogbn-arxiv and|N(i)|= 1for ogbn-products.\n5 Related Work\nLearningwith TAG. Thisprobleminvolveslearn-\ning text node representations that encode both tex-\ntual semantics and structural context information.\n2752\nA prominent approach uses structure-augmented\nSSL objectives to incorporate structure informa-\ntion into language models (Chien et al., 2021;\nCohan et al., 2020; Yasunaga et al., 2022; Os-\ntendorff et al., 2022). For example,SPECTER and\nLinkBERT focus on node-pair relationships (Co-\nhan et al., 2020; Yasunaga et al., 2022), while\nGIANT targets extreme multiclass neighborhood\nprediction (Chien et al., 2021). A concurrent\nwork PATTON (Jin et al., 2023) trains a language\nmodel through network-contextualized masked lan-\nguage modeling and link prediction. A parallel\nline of research tries to integrate pre-trained lan-\nguage models (PLMs) with graph neural networks\n(GNN) (Yang et al., 2021; Zhao et al., 2022; Mavro-\nmatis et al., 2023; Shang et al., 2020; Zhang et al.,\n2021). GraphFormers interweavesGNN withLMs‚Äô\nself-attentionlayers(Yangetal.,2021). GLEM (Zhao\net al., 2022),GraDBERT (Mavromatis et al., 2023)\nand LRTN (Zhang et al., 2021) leverage labeled\ndatasetsforco-trainingbetween PLM andGNN.While\nthey excel in node classification benchmarks, their\nreliance on labeled datasets limits the representa-\ntion‚Äôs generalization ability to new tasks.\nContrastive Learning. Contrastive learning is a\nself-supervisedlearningparadigmthataimstolearn\nrepresentations by distinguishing between positive\nand negative instances (Jaiswal et al., 2020). A key\npractice in contrastive learning is to use augmented\nversions of the same instance as positive instances\nandotherinstancesasnegativeinstances(Gaoetal.,\n2021; He et al.; Radford et al., 2021). For ex-\nample,SimCSE creates augmented views for each\ninstance based on dropout (Srivastava et al., 2014).\nHowever, conventional instance-level contrastive\nlearning only encourages instance-wise discrimi-\nnation (Li et al., 2021b) and commonly assumes\ndifferentinstancesare i.i.d.,whichneglectstherela-\ntionships among different instances onTAG. Hence,\nconventional contrastive learning methods are inef-\nfective to learn expressive representation learning\nonTAG. To address those limitations, many recent\nmethods seek to extend the design of positive pair\nconstruction by considering local neighborhood\ninformation (Cohan et al., 2020; Ostendorff et al.,\n2022). However, those methods cannot fully cap-\nture complex graph structures. In contrast, our pro-\nposed method,Grenade, leverages graph-centric\ncontrastive learning and graph-centric knowledge\nalignment to fully exploit the structural context\ninformation fromTAG.\n6 Conclusion\nIn this paper, we introduce a self-supervised graph-\ncentric language model: Grenade, for learning\nexpressive and generalized representation from tex-\ntual attributed graphs (TAG). Grenade is learned\nthrough two self-supervised learning algorithms:\n(1) Graph-Centric Contrastive Learning, which\nenables Grenade to harness intrinsic graph knowl-\nedge through relational-aware and augmentation-\nfree contrastive learning; and(2) Graph-Centric\nKnowledge Alignment, which facilitates the ex-\nchange and strengthening of knowledge derived\nfrom the pre-trained language model encoder and\nthe graph neural network encoder, thereby en-\nhancing the capture of relational information from\nTAG. We conduct experiments on four benchmark\ndatasetsunderfew-shotandfulldatanodeclassifica-\ntion, node clustering, and link prediction tasks, and\nfind thatGrenade significantly and consistently\noutperforms baseline methods.\n7 Limitations\nIn this section, we acknowledge the following\nconstraints in our study:(1) Constraints on the\nChoice of Backbone Model. Our choice of the\nbackbone model was restricted to the initialization\nof ‚Äúbert-base-uncased‚Äù in trainingGrenade.\nThis choice was necessitated by the limitations\nin computational resources available to us. Ex-\nploration with alternativePLM backbones such as\nGPT2(Radford et al., 2019) andRoBERTa(Liu et al.,\n2019) has not been carried out and represents a\npromising direction for subsequent studies. (2)\nComparisonwithLargeLanguageModels. Thenat-\nural language processing domain has recently seen\nbreakthroughs with state-of-the-art large language\nmodels likeLLaMA(Touvron et al., 2023) and Chat-\nGPT (OpenAI, 2023), which have demonstrated\nexceptionalperformanceinlanguageunderstanding\ntasks. Our experiment confirms thatGrenade sur-\npasses existing representation learning techniques\nin TAG, but the performance ofGrenade relative\nto these cutting-edge language models remains to\nbe ascertained.(3) Breadth of Evaluation.In this\nwork,weevaluate Grenade primarilythroughnode\nclassification, node clustering, and link prediction\ntasks. However, there are other relevant evaluation\ndimensions, such as retrieval, reranking, co-view\nandothers(Cohanetal.,2020). Futureworkwillin-\nvestigatetheapplicabilityandcapacityof Grenade\nto broader tasks.\n2753\nReferences\nJinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He.\n2023. Nagphormer: A tokenized graph transformer\nfor node classification in large graphs.\nEli Chien, Wei-Cheng Chang, Cho-Jui Hsieh, Hsiang-\nFu Yu, Jiong Zhang, Olgica Milenkovic, and In-\nderjit S Dhillon. 2021. Node feature extraction by\nself-supervised multi-scale neighborhood prediction.\narXiv preprint arXiv:2111.00064.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel S Weld. 2020. Specter:\nDocument-level representation learning using\ncitation-informed transformers. arXiv preprint\narXiv:2004.07180.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectionaltransformersforlanguageunderstanding.\narXiv preprint arXiv:1810.04805.\nKaize Ding, Jianling Wang, James Caverlee, and Huan\nLiu. 2022a. Meta propagation networks for graph\nfew-shot semi-supervised learning. InProceedings\nof the AAAI Conference on Artificial Intelligence.\nKaizeDing,YanchengWang,YingzhenYang,andHuan\nLiu. 2022b. Eliciting structural and semantic global\nknowledgeinunsupervisedgraphcontrastivelearning.\narXiv preprint arXiv:2202.08480.\nKaizeDing,YanchengWang,YingzhenYang,andHuan\nLiu. 2023. Eliciting structural and semantic global\nknowledgeinunsupervisedgraphcontrastivelearning.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, pages 7378‚Äì7386.\nKaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu.\n2022c. Data augmentation for deep graph learning:\nA survey.ACM SIGKDD Explorations Newsletter.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs.\nAdvances in neural information processing systems,\n30.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss B. Girshick. Momentum contrast for unsu-\npervised visual representation learning. In 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.arXiv\npreprint arXiv:1503.02531.\nWeihuaHu,MatthiasFey,MarinkaZitnik,YuxiaoDong,\nHongyu Ren, Bowen Liu, Michele Catasta, and Jure\nLeskovec.2020. Opengraphbenchmark: Datasetsfor\nmachine learning on graphs. InAdvances in Neural\nInformation Processing Systems 33: Annual Confer-\nenceonNeuralInformationProcessingSystems2020,\nNeurIPS 2020.\nAshishJaiswal,AshwinRameshBabu,MohammadZaki\nZadeh, Debapriya Banerjee, and Fillia Makedon.\n2020. A survey on contrastive self-supervised learn-\ning. Technologies, 9(1):2.\nBowen Jin, Wentao Zhang, Yu Zhang, Yu Meng,\nXinyang Zhang, Qi Zhu, and Jiawei Han. 2023. Pat-\nton: Language model pretraining on text-rich net-\nworks. CoRR, abs/2305.12268.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. InICLR.\nNamkyeong Lee, Junseok Lee, and Chanyoung Park.\n2022. Augmentation-freeself-supervisedlearningon\ngraphs. InAAAI.\nGuohao Li, Matthias M√ºller, Bernard Ghanem, and\nVladlen Koltun. 2021a. Training graph neural net-\nworks with 1000 layers. InICML.\nJunnan Li, Pan Zhou, Caiming Xiong, and Steven C. H.\nHoi. 2021b. Prototypical contrastive learning of\nunsupervised representations. In9th International\nConferenceonLearningRepresentations,ICLR2021 .\nYinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer,andVeselinStoyanov.2019. Roberta: A\nrobustly optimized bert pretraining approach.arXiv\npreprint arXiv:1907.11692.\nCostas Mavromatis, Vassilis N Ioannidis, Shen Wang,\nDaZheng,SojiAdeshina,JunMa,HanZhao,Christos\nFaloutsos, and George Karypis. 2023. Train your\nown gnn teacher: Graph-aware distillation on textual\ngraphs. arXiv preprint arXiv:2304.10668.\nMiller McPherson, Lynn Smith-Lovin, and James M\nCook. 2001. Birds of a feather: Homophily in social\nnetworks. Annual Review of Sociology, (1).\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntionsinvectorspace. arXivpreprintarXiv:1301.3781 .\nOpenAI. 2023. Gpt-4 technical report.arXiv preprint\narXiv:2303.08774.\nMalte Ostendorff, Nils Rethmeier, Isabelle Augenstein,\nBela Gipp, and Georg Rehm. 2022. Neighborhood\ncontrastive learning for scientific document represen-\ntations with citation embeddings. InProceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing.\nAdamPaszke,SamGross,FranciscoMassa,AdamLerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen,\nZemingLin,NataliaGimelshein,LucaAntiga,Alban\nDesmaison, Andreas K√∂pf, Edward Yang, Zachary\n2754\nDeVito, Martin Raison, Alykhan Tejani, Sasank Chil-\namkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. 2019. Pytorch: An imperative\nstyle, high-performance deep learning library. In\nNeurIPS.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD.Cournapeau,M.Brucher,M.Perrot,andE.Duches-\nnay. 2011. Scikit-learn: Machine learning in Python.\nJournal of Machine Learning Research.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th Interna-\ntional Conference on Machine Learning, ICML 2021,\nProceedings of Machine Learning Research.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners.OpenAI\nblog.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nJingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li,\nand Jiawei Han. 2020. Nettaxo: Automated topic\ntaxonomy construction from text-rich network. In\nTheWebConf.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting.Journal of Machine Learning Re-\nsearch.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models.arXiv preprint\narXiv:2302.13971.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, (11).\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li√≤, and Yoshua Bengio.\n2018. Graph attention networks. In6th International\nConferenceonLearningRepresentations,ICLR2018 .\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond,ClementDelangue,AnthonyMoi,Pierric\nCistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface‚Äôs transformers: State-of-\nthe-art natural language processing.arXiv preprint\narXiv:1910.03771.\nZhuofengWu,SinongWang,JiataoGu,MadianKhabsa,\nFei Sun, and Hao Ma. 2020. Clear: Contrastive\nlearning for sentence representation.arXiv preprint\narXiv:2012.15466.\nJunhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li,\nDefuLian,SanjayAgrawal,AmitSingh,Guangzhong\nSun,andXingXie.2021. Graphformers: Gnn-nested\ntransformers for representation learning on textual\ngraph. Advances in Neural Information Processing\nSystems.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. Linkbert: Pretraining language models with\ndocument links.arXiv preprint arXiv:2203.15827.\nXinyangZhang,ChenweiZhang,XinLunaDong,Jingbo\nShang, and Jiawei Han. 2021. Minimally-supervised\nstructure-rich text categorization via learning on text-\nrich networks. InTheWebConf.\nJianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian\nLiu,RuiLi,XingXie,andJianTang.2022. Learning\non large-scale text-attributed graphs via variational\ninference. arXiv preprint arXiv:2210.14709.\nJason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li,\nMarkus Pelger, Tianqi Yang, Liangjie Zhang, Ruofei\nZhang, and Huasha Zhao. 2021. Textgnn: Improving\ntext encoder via graph neural network in sponsored\nsearch. InWWW.\n2755\nA Dataset Details\nWe extend ogbn-arxiv and ogbn-products for link\nprediction and we evaluate models on the test-split\nfromthesetwonodedatasets. Specifically, foreach\nsourcenode i,werandomlychooseitsoneneighbor\nas the positive candidate and 1,000 negative candi-\ndates and would like the model to rank the positive\ncandidate over the negative candidates. The nega-\ntive references are randomly-sampled from all the\nnodes fromTAG that are not connected byi.\nFor ogbl-citation2, we also extend it for node\nclassification. Herethetaskistopredictthesubject\nareas of the subset of the nodes/papers that pub-\nlished in arxiv like ogbn-papers100M. We borrow\nthelabelsforogbl-citation2fromogbn-papers100M.\nWe align the nodes from ogbl-citation2 and ogbn-\npapers100M through Microsoft academic graph pa-\nperID.Wesplitthedataintotraining/validation/test\nby year thatthe papers publishedbefore 2017 isthe\ntraining dataset, between 2017-2018 is the valida-\ntion dataset and after 2018 is the test dataset.\nB Implementation Details\nOurproposedmethodologywasimplementedusing\nPyTorch version 1.13.1 (Paszke et al., 2019) and\nHuggingFace Transformers version 4.24.0 (Wolf\net al., 2019). The experiments were executed\non A5000, A6000, and RTX 4090 GPUs. For\nGrenade, the learning rate is configured at5e‚àí5,\nandAdamWoptimizerisemployedfortrainingover\nthe course of 3 epochs. The search space for the\nnumber ofGNN layers,L, ranges from{1,2,3,4},\nand further hyperparameter analysis is performed\nas detailed in Fig. 6. The hyperparameters for few-\nshot node classification are shown in Tab. 6 and\nTab. 7, respectively. It should be noticed that ‚Äú‚àí1‚Äù\nmeans utilize all the training data for batch_size\nand all the neighbors for neighbor sampling.\nHyperparametersValue\nhidden_size 256\ndropout 0.5\nlr 1e-4\nepochs 300\nbatch_size -1\nTable 6: Hyperparameter setting forMLP model in node\nclassification.\nC Additional Experimental Results\nFull Data Node Classification.\nDatasets ogbn-arxiv ogbn-products ogbl-citation2\nhidden_size 256 256 256\ndropout 0.5 0.5 0.5\nlr 1e-3 1e-3 1e-3\nepochs 500 50 50\nbatch_size -1 1024 1024\nneighbors -1‚Äì -1 5‚Äì10 5‚Äì10\nTable 7: Hyperparameter setting forGraphSAGE model\nin node classification.\nEnhanced Node Classification Model.Besides\nMLP and GraphSage, we incorporated the recent\nGraph Transformer Network,NAGphormer (Chen\net al., 2023), into our node classification evalu-\nation. As evidenced by the data in Tab. 8, our\nproposed approach consistently surpasses the base-\nline techniques (BERT+MLM, SPECTER, GIANT) in\nthe few-shot and full-data node classification. This\nperformance is consistent with the observations of\nusingMLP andGraphSage.\nMethods k = 2 k = 4 k = 8 k = 16 All\nBERT+MLM 42.47 49 .29 56 .78 60 .52 66 .37\nSPECTER 49.89 54 .79 59 .66 63 .26 69 .11\nGIANT 40.57 44 .81 54 .27 58 .93 65 .82\nGrenade 52.73 58 .28 62 .53 64 .97 70 .85\nTable 8: ogbn-arxiv node classification result on\nNAGphormer.\nAdditionalAblationStudy. Wehavetheablation\nstudy on ogbn-arxiv and ogbn-products dataset un-\nder 7 different model variations. In the second row\nofTab.9,theterm ICL referstoinstance-wisecross-\nmodality contrastive learning. It indicates that the\npositive pairs are formed between identically in-\ndexed document and node representations, while\nthe negative pairs consist of other document and\nnode representations within the minibatch. From\nTable 9, have the consistent observation as 5 that\neach component ofGrenade contributes the per-\nformance improvement.\nIn-Depth Hyperparameter Analysis. To evalu-\nate the performance of the graph neural network\nencoder(GNN),weconductananalysisofthehyper-\nparameterLasdepictedinFig.6. Ourobservations\nindicate that an optimal performance is achieved\nwhenL= 2.\nInferenceTimeComplexityAnalysis Whenpro-\nvided with an arbitrary node,Grenade is capable\nof generating the textual representation of the node\nwithout requiring graph information. This leads\n2756\nMethods ogbn-arxiv ogbn-products\nMLP(ACC) K-means++(ACC) MRR Avg. Rank‚Üì MLP(ACC) K-means++(ACC) MRR Avg Rank.‚Üì\nGrenade 73.16¬±0.12 61.96¬±0.79 52.73 1.67 81.58¬±0.18 73.54¬±1.03 72.39 2.00\nGC-CL+ICL+NBH-KA 72.20¬±0.20 60.67¬±0.71 47.74 4 .33 80.90¬±0.24 72.48¬±1.42 65.72 5 .67\nGrenadew/oGC-CL 72.83¬±0.08 58.75¬±0.99 55.87 3.33 80.52¬±0.15 71.25¬±0.98 74.51 4.33\nGrenadew/oND-KA 72.65¬±0.16 60.69¬±0.72 49.15 4 .33 80.96¬±0.29 73.37¬±0.71 68.47 4 .33\nGrenadew/oNBH-KA 73.05¬±0.17 60.50¬±0.86 52.56 3 .33 81.36¬±0.30 72.77¬±1.42 72.08 3 .33\nGrenadew/oND-KA+NBH-KA72.71¬±0.21 60.34¬±1.00 49.67 4 .67 81.14¬±0.17 73.59¬±0.82 68.71 3 .00\nGrenadew/oGC-CL+NBH-KA72.63¬±0.09 58.66¬±0.77 55.46 5 .00 80.35¬±0.20 71.10¬±1.20 74.27 5 .33\nGrenadew/oGC-CL+ND-KA17.97¬±6.48 22.49¬±0.17 0.55 8 .00 66.76¬±0.62 44.83¬±0.26 21.08 8 .00\nTable 9: Ablation study of graph-centric contrastive learning (CL) and knowledge-alignment betweenPLM encoder\nand GNN encoder (ND-KA and NBH-KA) on ogbn-arxiv and ogbn-products datasets.\n1 2 3 4\nNode Classification\n72.5\n72.7\n72.9\n73.1\n73.3ACC (%)\n1 2 3 4\nNode Clustering\n59.1\n60.2\n61.3\n62.4\n63.5\n1 2 3 4\nLink Prediction\n51.2\n51.6\n52.0\n52.4\n52.8\nMRR (%)\n(a) ogbn-arxiv.\n1 2 3 4\nNode Classification\n80.8\n81.1\n81.4\n81.7\n82.0ACC (%)\n1 2 3 4\nNode Clustering\n70.8\n71.7\n72.6\n73.5\n74.5\n1 2 3 4\nLink Prediction\n71.5\n71.7\n71.9\n72.2\n72.4\nMRR (%)\n(b) ogbn-products.\nFigure 6: Hyperparameter analysis onL. Dashed hori-\nzontallinesaretheperformanceofchosen linthispaper.\nto an efficiency that is on par withBERT (Devlin\net al., 2018), while achieving approximately a10%\nenhancement in performance for full data node\nclassification.\nRegarding the training efficiency, though our\nmodel requires longer training time compared\nto text-only PLM/SSL methods likeBERT+MLM\n(Àú1h24m) and SimCSE (Gao et al., 2021)(around\n41m), we are able to achieve a great margin of per-\nformance improvement with reasonable additional\ntrainingtime(around2h50m). Also,ourmodelcan\nachieve stable performance with 3-epoch training,\nwhich is more efficient than structure-based con-\ntrastivelearningmethodsuchasSPECTER(Cohan\net al., 2020) (around 3h19m for 3 epochs).\n2757",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7843232154846191
    },
    {
      "name": "Generalizability theory",
      "score": 0.7507708668708801
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6388931274414062
    },
    {
      "name": "Natural language processing",
      "score": 0.553936779499054
    },
    {
      "name": "Graph",
      "score": 0.537236750125885
    },
    {
      "name": "Machine learning",
      "score": 0.4822019338607788
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.46379926800727844
    },
    {
      "name": "Representation (politics)",
      "score": 0.4222373068332672
    },
    {
      "name": "Feature learning",
      "score": 0.4155117869377136
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3042352497577667
    },
    {
      "name": "Mathematics",
      "score": 0.08462324738502502
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I107077323",
      "name": "Worcester Polytechnic Institute",
      "country": "US"
    }
  ],
  "cited_by": 7
}