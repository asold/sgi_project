{
  "title": "Analysis of the evolution of advanced transformer-based language models: experiments on opinion mining",
  "url": "https://openalex.org/W4385505449",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Nour Eddine Zekaouiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230803694",
      "name": "Siham Yousfi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A8977801",
      "name": "Maryem Rhanoui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1682363516",
      "name": "Mounia Mikram",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4381304672",
    "https://openalex.org/W3097818286",
    "https://openalex.org/W3159729103",
    "https://openalex.org/W3127800866",
    "https://openalex.org/W3208182228",
    "https://openalex.org/W2019759670",
    "https://openalex.org/W2548006041",
    "https://openalex.org/W4241652516",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3047171714",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3163832451",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6676984168",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W6890311436",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W3144293453",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2803686962"
  ],
  "abstract": "&lt;p&gt;Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.&lt;/p&gt;",
  "full_text": "IAES International Journal of Artiﬁcial Intelligence (IJ-AI)\nV ol. 12, No. 4, December 2023, pp. 1995∼2010\nISSN: 2252-8938, DOI: 10.11591/ijai.v12.i4.pp1995-2010 Ì 1995\nAnalysis of the evolution of advanced transformer-based\nlanguage models: experiments on opinion mining\nNour Eddine Zekaoui, Siham Yousﬁ, Maryem Rhanoui, Mounia Mikram\nMeridian Team, LYRICA Laboratory, School of Information Sciences, Rabat, Morocco\nArticle Info\nArticle history:\nReceived Jan 5, 2023\nRevised Jan 16, 2023\nAccepted Mar 10, 2023\nKeywords:\nNatural language processing\nOpinion mining\nTransformer-based models\nABSTRACT\nOpinion mining, also known as sentiment analysis, is a subﬁeld of natural lan-\nguage processing (NLP) that focuses on identifying and extracting subjective\ninformation in textual material. This can include determining the overall senti-\nment of a piece of text (e.g., positive or negative), as well as identifying speciﬁc\nemotions or opinions expressed in the text, that involves the use of advanced\nmachine and deep learning techniques. Recently, transformer-based language\nmodels make this task of human emotion analysis intuitive, thanks to the atten-\ntion mechanism and parallel computation. These advantages make such models\nvery powerful on linguistic tasks, unlike recurrent neural networks that spend a\nlot of time on sequential processing, making them prone to fail when it comes to\nprocessing long text. The scope of our paper aims to study the behaviour of the\ncutting-edge Transformer-based language models on opinion mining and pro-\nvide a high-level comparison between them to highlight their key particularities.\nAdditionally, our comparative study shows leads and paves the way for produc-\ntion engineers regarding the approach to focus on and is useful for researchers\nas it provides guidelines for future research subjects.\nThis is an open access article under the CC BY-SA license.\nCorresponding Author:\nNour Eddine Zekaoui\nMeridian Team, LYRICA Laboratory, School of Information Sciences\nRabat, Morocco\nEmail: noureddinezekaoui@gmail.com, nour-eddine.zekaoui@esi.ac.ma\n1. INTRODUCTION\nOver the past few years, interest in natural language processing (NLP) [1] has increased signiﬁcantly.\nToday, several applications are investing massively in this new technology, such as extending recommender\nsystems [2], [3], uncovering new insights in the health industry [4], [5], and unraveling e-reputation and opin-\nion mining [6], [7]. Opinion mining is an approach to computational linguistics and NLP that automatically\nidentiﬁes the emotional tone, sentiment, or thoughts behind a body of text. As a result, it plays a vital role\nin driving business decisions in many industries. However, seeking customer satisfaction is costly expensive.\nIndeed, mining user feedback regarding the products offered, is the most accurate way to adapt strategies and\nfuture business plans. In recent years, opinion mining has seen considerable progress, with applications in\nsocial media and review websites. Recommendation may be staff-oriented [2] or user-oriented [8] and should\nbe tailored to meet customer needs and behaviors.\nNowadays, analyzing people’s emotions has become more intuitive thanks to the availability of many\nlarge pre-trained language models such as bidirectional encoder representations from transformers (BERT) [9]\nand its variants. These models use the seminal transformer architecture [10], which is based solely on attention\nmechanisms, to build robust language models for a variety of semantic tasks, including text classiﬁcation.\nJournal homepage:http://ijai.iaescore.com\n1996 Ì ISSN: 2252-8938\nMoreover, there has been a surge in opinion mining text datasets, speciﬁcally designed to challenge NLP\nmodels and enhance their performance. These datasets are aimed at enabling models to imitate or even exceed\nhuman level performance, while introducing more complex features.\nEven though many papers have addressed NLP topics for opinion mining using high-performance\ndeep learning models, it is still challenging to determine their performance concretely and accurately due to\nvariations in technical environments and datasets. Therefore, to address these issues, our paper aims to study\nthe behaviour of the cutting-edge transformer-based models on textual material and reveal their differences.\nAlthough, it focuses on applying both transformer encoders and decoders, such as BERT [9] and generative\npre-trained transformer (GPT) [11], respectively, and their improvements on a benchmark dataset. This enable\na credible assessment of their performance and understanding their advantages, allowing subject matter experts\nto clearly rank the models. Furthermore, through ablations, we show the impact of conﬁguration choices on\nthe ﬁnal results.\n2. BACKGROUND\n2.1. Transformer\nThe transformer [10], as illustrated in Figure 1, is an encoder-decoder model dispensing entirely with\nrecurrence and convolutions. Instead, it leverages the attention mechanism to compute high-level contextual-\nized embeddings. Being the ﬁrst model to rely solely on attention mechanisms, it is able to address the issues\ncommonly associated with recurrent neural networks, which factor computation along symbol positions of in-\nput and output sequences, and then precludes parallelization within samples. Despite this, the transformer is\nhighly parallelizable and requires signiﬁcantly less time to train. In the upcoming sections, we will highlight the\nrecent breakthroughs in NLP involving transformer that changed the ﬁeld overnight by introducing its designs,\nsuch as BERT [9] and its improvements.\nFigure 1. The transformer model architecture [10]\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 1997\n2.2. BERT\nBERT [9] is pre-trained using a combination of masked language modeling (MLM) and next sentence\nprediction (NSP) objectives. It provides high-level contextualized embeddings grasping the meaning of words\nin different contexts through global attention. As a result, the pre-trained BERT model can be ﬁne-tuned for a\nwide range of downstream tasks, such as question answering and text classiﬁcation, without substantial task-\nspeciﬁc architecture modiﬁcations.\nBERT and its variants allow the training of modern data-intensive models. Moreover, they are able to\ncapture the contextual meaning of each piece of text in a way that traditional language models are unﬁt to do,\nwhile being quicker to develop and yielding better results with less data. On the other hand, BERT and other\nlarge neural language models are very expensive and computationally intensive to train/ﬁne-tune and make\ninference.\n2.3. GPT-I, II, III\nGPT [11] is the ﬁrst causal or autoregressive transformer-based model pre-trained using language\nmodeling on a large corpus with long-range dependencies. However, its bigger an optimized version called\nGPT-2 [12], was pre-trained on WebText. Likewise, GPT-3 [13] is architecturally similar to its predecessors. Its\nhigher level of accuracy is attributed to its increased capacity and greater number of parameters, and it was pre-\ntrained on Common Crawl. The OpenAI GPT family models has taken pre-trained language models by storm,\nthey are very powerful on realistic human text generation and many other miscellaneous NLP tasks. Therefore,\na small amount of input text can be used to generate large amount of high-quality text, while maintaining\nsemantic and syntactic understanding of each word.\n2.4. ALBERT\nA lite BERT (ALBERT) [14] was proposed to address the problems associated with large models. It\nwas speciﬁcally designed to provide contextualized natural language representations to improve the results on\ndownstream tasks. However, increasing the model size to pre-train embeddings becomes harder due to memory\nlimitations and longer training time. For this reason, this model arose.\nALBERT is a lighter version of BERT, in which next sentence prediction (NSP) is replaced by sentence\norder prediction (SOP). In addition to that, it employs two parameter-reduction techniques to reduce memory\nconsumption and improve training time of BERT without hurting performance:\n− Splitting the embedding matrix into two smaller matrices to easily grow the hidden size with fewer\nparameters, ALBERT separates the hidden layers size from the size of the vocabulary embedding by\ndecomposing the embedding matrix of the vocabulary.\n− Repeating layers split among groups to prevent the parameter from growing with the depth of the net-\nwork.\n2.5. RoBERTa\nThe choice of language model hyper-parameters has a substantial impact on the ﬁnal results. Hence,\nrobustly optimized BERT pre-training approach (RoBERTa) [15] is introduced to investigate the impact of\nmany key hyper-parameters along with data size on model performance. RoBERTa is based on Google’s BERT\n[9] model and modiﬁes key hyper-parameters, where the masked language modeling objective is dynamic and\nthe NSP objective is removed. It is an improved version of BERT, pre-trained with much larger mini-batches\nand learning rates on a large corpus using self-supervised learning.\n2.6. XLNet\nThe bidirectional property of transformer encoders, such as BERT [9], help them achieve better per-\nformance than autoregressive language modeling based approaches. Nevertheless, BERT ignores dependency\nbetween the positions masked, and suffers from a pretrain-ﬁnetune discrepancy when relying on corrupting the\ninput with masks. In view of these pros and cons, XLNet [16] has been proposed. XLNet is a generalized\nautoregressive pretraining approach that allows learning bidirectional dependencies by maximizing the antici-\npated likelihood over all permutations of the factorization order. Furthermore, it overcomes the drawbacks of\nBERT [9] due to its casual or autoregressive formulation, inspired from the transformer-XL [17].\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n1998 Ì ISSN: 2252-8938\n2.7. DistilBERT\nUnfortunately, the outstanding performance that comes with large-scale pretrained models is not\ncheap. In fact, operating them on edge devices under constrained computational training or inference bud-\ngets remains challenging. Against this backdrop, DistilBERT [18] (or Distilled BERT) has seen the light to\naddress the cited issues by leveraging knowledge distillation [19].\nDistilBERT is similar to BERT, but it is smaller, faster, and cheaper. It has 40% less parameters than\nBERT base, runs 40% faster, while preserving over 95% of BERT’s performance. It is trained using distillation\nof the pretrained BERT base model.\n2.8. XLM-RoBERTa\nPre-trained multilingual models at scale, such as multilingual BERT (mBERT) [9] and cross-lingual\nlanguage models (XLMs) [20], have led to considerable performance improvements for a wide variety of\ncross-lingual transfer tasks, including question answering, sequence labeling, and classiﬁcation. However, the\nmultilingual version of RoBERTa [15] called XLM-RoBERTa [21], pre-trained on the newly created 2.5TB\nmultilingual CommonCrawl corpus containing 100 different languages, has further pushed the performance. It\nhas shown strong improvements on low-resource languages compared to previous multilingual models.\n2.9. BART\nBidirectional and auto-regressive transformer (BART) [22] is a generalization of BERT [9] and GPT\n[11], it takes advantage of the standard transformer [10]. Concretely, it uses a bidirectional encoder and a\nleft-to-right decoder. It is trained by corrupting text with an arbitrary noising function and learning a model to\nreconstruct the original text. BART has shown phenomenal success when ﬁne-tuned on text generation tasks\nsuch as translation, but also performs well for comprehension tasks like question answering and classiﬁcation.\n2.10. ConvBERT\nWhile BERT [9] and its variants have recently achieved incredible performance gains in many NLP\ntasks compared to previous models, BERT suffers from large computation cost and memory footprint due to\nreliance on the global self-attention block. Although all its attention heads, BERT was found to be compu-\ntationally redundant, since some heads simply need to learn local dependencies. Therefore, ConvBERT [23]\nis a better version of BERT [9], where self-attention blocks are replaced with new mixed ones that leverage\nconvolutions to better model global and local context.\n2.11. Reformer\nConsistently, large transformer [10] models achieve state-of-the-art results in a large variety of linguis-\ntic tasks, but training them on long sequences is costly challenging. To address this issue, the Reformer [24]\nwas introduced to improve the efﬁciency of transformers while holding the high performance and the smooth\ntraining. Reformer is more efﬁcient than transformer [10] thanks to locality-sensitive hashing attention and\nreversible residual layers instead of the standard residuals, and axial position encoding and other optimizations.\n2.12. T5\nTransfer learning has emerged as one of the most inﬂuential techniques in NLP. Its efﬁciency in trans-\nferring knowledge to downstream tasks through ﬁne-tuning has given birth to a range of innovative approaches.\nOne of these approaches is transfer learning with a uniﬁed text-to-text transformer (T5) [25], which consists\nof a bidirectional encoder and a left-to-right decoder. This approach is reshaping the transfer learning land-\nscape by leveraging the power of being pre-trained on a combination of unsupervised and supervised tasks and\nreframing every NLP task into text-to-text format.\n2.13. ELECTRA\nMasked language modeling (MLM) approaches like BERT [9] have proven to be effective when trans-\nferred to downstream NLP tasks, although, they are expensive and require large amounts of compute. Efﬁ-\nciently learn an encoder that classiﬁes token replacements accurately (ELECTRA) [26] is a new pre-training\napproach that aims to overcome these computation problems by training two Transformer models: the gener-\nator and the discriminator. ELECTRA trains on a replaced token detection objective, using the discriminator\nto identify which tokens were replaced by the generator in the sequences. Unlike MLM-based models, ELEC-\nTRA is deﬁned over all input tokens rather than just a small subset that was masked, making it a more efﬁcient\npre-training approach.\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 1999\n2.14. Longformer\nWhile previous transformers were focusing on making changes to the pre-training methods, the long-\ndocument transformer (Longformer) [27] comes to change the transformer’s self-attention mechanism. It has\nbecame the de facto standard for tackling a wide range of complex NLP tasks, with an new attention mecha-\nnism that scales linearly with sequence length, and then being able to easily process longer sequences. Long-\nformer’s new attention mechanism is a drop-in replacement for the standard self-attention and combines a local\nwindowed attention with a task motivated global attention. Simply, it replaces the transformer [10] attention\nmatrices with sparse matrices for higher training efﬁciency.\n2.15. DeBERTa\nDeBERTa [28] stands for decoding-enhanced BERT with disentangled attention. It is a pre-training\napproach that extends Google’s BERT [9] and builds on the RoBERTa [15]. Despite being trained on only half\nof the data used for RoBERTa, DeBERTa has been able to improve the efﬁciency of pre-trained models through\nthe use of two novel techniques:\n− Disentangled attention (DA): an attention mechanism that computes the attention weights among words\nusing disentangled matrices based on two vectors that encode the content and the relative position of\neach word respectively.\n− Enhanced mask decoder (EMD): a pre-trained technique used to replace the output softmax layer. Thus,\nincorporate absolute positions in the decoding layer to predict masked tokens for model pre-training.\n3. APPROACH\nTransformer-based pre-trained language models have led to substantial performance gains, but careful\ncomparison between different approaches is challenging. Therefore, we extend our study to uncover insights\nregarding their ﬁne-tuning process and main characteristics. Our paper ﬁrst aims to study the behavior of\nthese models, following two approaches: a data-centric view focusing on the data state and quality, and a\nmodel-centric view giving more attention to the models tweaks. Indeed, we will see how data processing\naffects their performance and how adjustments and improvements made to the model over time is changing\nits performance. Thus, we seek to end with some takeaways regarding the optimal setup that aids in cross-\nvalidating a Transformer-based model, speciﬁcally model tuning hyper-parameters and data quality.\n3.1. Models summary\nIn this section, we present the base versions’ details of the models introduced previously as shown\nin Table A1. We aim to provide a fair comparison based on the following criteria: L-Number of transformer\nlayers, H-Hidden state size or model dimension, A-Number of attention heads, number of total parameters,\ntokenization algorithm, data used for pre-training, training devices and computational cost, training objectives,\ngood performance tasks, and a short description regarding the model key points [29]. All these information\nwill help to understand the performance and behaviors of different transformer-based models and aid to make\nthe appropriate choice depending on the task and resources.\n3.2. Conﬁguration\nIt should be noted that we have used almost the same architecture building blocks for all our imple-\nmented models as shown in Figure 2 and Figure 3 for both encoder and decoder based models, respectively.\nIn contrast, seq2seq models like BART are merely a bidirectional encoder pursued by an autoregressive de-\ncoder. Each model is fed with the three required inputs, namely input ids, token type ids, and attention mask.\nHowever, for some models, the position embeddings are optional and can sometimes be completely ignored\n(e.g RoBERTa), for this reason we have blurred them a bit in the ﬁgures. Furthermore, it is important to note\nthat we uniformed the dataset in lower cases, and we tokenized it with tokenizers based on WordPiece [30],\nSentencePiece [31], and Byte-pair-encoding [32] algorithms.\nIn our experiments, we used a highly optimized setup using only the base version of each pre-trained\nlanguage model. For training and validation, we set a batch size of 8 and 4, respectively, and ﬁne-tuned the\nmodels for 4 epochs over the data with maximum sequence length of 384 for the intent of correspondence to\nthe majority of reviews’ lengths and computational capabilities. The AdamW optimizer is utilized to optimize\nthe models with a learning rate of 3e-5 and the epsilon (eps) used to improve numerical stability is set to 1e-6,\nwhich is the default value. Furthermore, the weight decay is set to 0.001, while excluding bias, LayerNorm.bias,\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2000 Ì ISSN: 2252-8938\nand LayerNorm.weight from the decay weight when ﬁne-tuning, and not decaying them when it is set to 0.000.\nWe implemented all of our models using PyTorch and transformers library from Hugging Face, and ran them\non an NVIDIA Tesla P100-PCIE GPU-Persistence-M (51G) GPU RAM.\nFigure 2. The architecture of the transformer encoder-based models\n3.3. Evaluation\nDataset to ﬁne-tune our models, we used the IMDb movie review dataset [33]. A binary sentiment\nclassiﬁcation dataset having 50K highly polar movie reviews labelled in a balanced way between positive\nand negative. We chose it for our study because it is often used in research studies and is a very popular\nresource for researchers working on NLP and ML tasks, particularly those related to sentiment analysis and text\nclassiﬁcation due to its accessibility, size, balance and pre-processing. In other words, it is easily accessible\nand widely available, with over 50K reviews well-balanced, with an equal number of positive and negative\nreviews as shown in Figure 4. This helps prevent biases in the trained model. Additionally, it has already been\npre-processed with the text of each review cleaned and normalized.\nMetrics to assess the performance of the ﬁne-tuned transformers on the IMDb movie reviews dataset,\ntracking the loss and accuracy learning curves for each model is an effective method. These curves can help\ndetect incorrect predictions and potential overﬁtting, which are crucial factors to consider in the evaluation\nprocess. Moreover, widely-used metrics, namely accuracy, recall, precision, and F1-score are valuable to\nconsider when dealing with classiﬁcation problems. These metrics can be deﬁned as:\nPrecision = TP\nTP + FP , Recall = TP\nTP + FN , and F1 = 2× Precision × Recall\nPrecision + Recall (1)\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 2001\nFigure 3. The architecture of the transformer decoder-based models\nFigure 4. Positive and negative reviews distribution\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2002 Ì ISSN: 2252-8938\n4. RESULTS\nIn this section, we present the ﬁne-tuning main results of our implemented transformer-based lan-\nguage models on the opinion mining task on the IMDb movie reviews dataset. Typically, all the ﬁne-tuned\nmodels perform well with fairly high performance, except the three autoregressive models: GPT, GPT-2,\nand Reformer, as shown in Table 1. The best model, ELECTRA, provides an F1-score of 95.6 points, fol-\nlowed by RoBERTa, Longformer, and DeBERTa, with an F1-score of 95.3, 95.1, and 95.1 points, respectively.\nOn the other hand, the worst model, GPT-2 provide an F1-score of 52.9 points as shown in Figure 5 and\nFigure 6. From the results, it is clear that purely autoregressive models do not perform well on comprehension\ntasks like sentiment classiﬁcation, where sequences may require access to bidirectional contexts for better word\nrepresentation, therefore, good classiﬁcation accuracy. Whereas, with autoencoding models taking advantage\nof left and right contexts, we saw good performance gains. For instance, the autoregressive XLNet model is our\nfourth best model in Table 1 with an F1 score of 94.9%, it incorporates modelling techniques from autoencod-\ning models into autoregressive models while avoiding and addressing limitations of encoders. The code and\nﬁne-tuned models are available at [34].\nTable 1. Transformer-based language models validation performance on the opinion mining IMDb dataset\nModel Recall Precision F1 Accuracy\nBERT 93.9 94.3 94.1 94.0\nGPT 92.4 51.8 66.4 53.2\nGPT-2 51.1 54.8 52.9 54.5\nALBERT 94.1 91.9 93.0 93.0\nRoBERTa 96.0 94.6 95.3 95.3\nXLNet 94.7 95.1 94.9 94.8\nDistilBERT 94.3 92.7 93.5 93.4\nXLM-RoBERTA 83.1 71.7 77.0 75.2\nBART 96.0 93.3 94.6 94.6\nConvBERT 95.5 93.7 94.6 94.5\nDeBERTa 95.2 95.0 95.1 95.1\nELECTRA 95.8 95.4 95.6 95.6\nLongformer 95.9 94.3 95.1 95.0\nReformer 54.6 52.1 53.3 52.2\nT5 94.8 93.4 94.0 93.9\nFigure 5. Worst model: GPT-2 loss learning curve\n5. ABLATION STUDY\nIn Table 2 and Figure 7, we demonstrate the importance of conﬁguration choices through controlled\ntrials and ablation experiments. Indeed, the maximum length of the sequence and data cleaning are particularly\ncrucial. Thus, to make our ablation study credible, we ﬁne-tuned our BERT model with the same setup,\nchanging only the sequence length (max-len) initially and cleaning the data (cd) at another time to observe how\nthey affect the performance of the model.\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 2003\nFigure 6. Worst model: GPT-2 acc learning curve\nTable 2. Validation results of the BERT model based on different conﬁgurations, where cd stands for cleaned\ndata, meaning that the latest model (BERTmax-len=384, cd) is trained on an exhaustively cleaned text\nModel Recall Precision F1 Accuracy\nBERTmax-len=64 86.8% 84.7% 85.8% 85.6%\nBERTmax-len=384 93.9% 94.3% 94.1% 94.0%\nBERTmax-len=384, cd 92.6% 91.6% 92.1% 92.2%\nFigure 7. Validation accuracy history of BERT model based on different conﬁgurations\n5.1. Effects of hyper-parameters\nThe gap between the performance of BERT max-len=64 and BERTmax-len=384 on the IMDb dataset is an\nastounding 8.3 F1 points, as in Table 2, demonstrating how important this parameter is. Thereby, visualizing the\ndistribution of tokens or words count is the ultimate solution for deﬁning the optimal and correct value of the\nmaximum length parameter that corresponds to all the training data points. Figure 8 illustrates the distribution\nof the number of tokens in the IMDb movie reviews dataset, it shows that the majority of reviews are between\n100 and 400 tokens in length. In this context, we chose 384 as the maximum length reference to study the effect\nof the maximum length parameter, because it covers the majority of review lengths while conserving memory\nand saving computational resources. It should be noted that the BERT model can process texts up to 512 tokens\nin length. It is a consequence of the model architecture and can not be adjusted directly.\n5.2. Effects of data cleaning\nTraditional machine learning algorithms require extensive data cleaning before vectorizing the input\nsequence and then feeding it to the model, with the aim of improving both reliability and quality of the data.\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2004 Ì ISSN: 2252-8938\nTherefore, the model can only focus on important features during training. Contrarily, the performance dropped\ndown dramatically by 2 F1 points when we cleaned the data for the BERT model. Indeed, the cleaning carried\nout aims to normalize the words of each review. It includes lemmatization to group together the different forms\nof the same word, stemming to reduce a word to its root, which is afﬁxed to sufﬁxes and preﬁxes, deletion of\nURLs, punctuations, and patterns that do not contribute to the sentiment, as well as the elimination of all stop\nwords, except the words “no”, “nor”, and “not”, because their contribution to the sentiment can be tricky. For\ninstance, “Black Panther is boring” is a negative review, but “Black Panther is not boring” is a positive review.\nThis drop can be justiﬁed by the fact that BERT model and attention-based models need all the sequence words\nto better capture the meaning of words’ contexts. However, with cleaning, words may be represented differently\nfrom their meaning in the original sequence. Note that “not boring” and “boring” are completely different in\nmeaning, but if the stop word “not” is removed, we end up with two similar sequences, which is not good in\nsentiment analysis context.\nFigure 8. Distribution of the number of tokens for a better selection of the maximum sequence length\n5.3. Effects of bias and training data\nCarefully observing the accuracy and the loss learning curves in Figure 9 and Figure 10, we notice that\nthe validation loss starts to creep upward and the validation accuracy starts to go down. In this perspective, the\nmodel in question continues to lose its ability to generalize well on unseen data. In fact, the model is relatively\nbiased due to the effect of the training data and data-drift issues related to the ﬁne-tuning data. In this context,\nwe assume that the model starts to overﬁt. However, setting different dropouts, reducing the learning rate, or\neven trying larger batches will not work. On the other hand, these strategies sometimes give worst results,\nthen a more critical overﬁtting problem. For this reason, pretraining these models on your industry data and\nvocabulary and then ﬁne-tuning them may be the best solution.\nFigure 9. Best model: ELECTRA loss learning curve\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 2005\nFigure 10. Best model: ELECTRA acc. learning curve\n6. CONCLUSION\nIn this paper, we presented a detailed comparison to highlight the main characteristics of transformer-\nbased pre-trained language models and what differentiates them from each other. Then, we studied their perfor-\nmance on the opinion mining task. Thereby, we deduce the power of ﬁne-tuning and how it helps in leveraging\nthe pre-trained models’ knowledge to achieve high accuracy on downstream tasks, even with the bias they came\nwith due to the pre-training data. Experimental results show how performant these models are. We have seen\nthe highest F1-score with the ELECTRA model with 95.6 points, across the IMDb dataset. Similarly, we found\nthat access to both left and right contexts is necessary when it comes to comprehension tasks like sentiment\nclassiﬁcation. We have seen that autoregressive models like GPT, GPT-2, and Reformer perform poorly and\nfail to achieve high accuracy. Nevertheless, XLNet has reached good results even though it is an autoregressive\nmodel because it incorporates ideas taken from encoders characterized by their bidirectional property. Indeed,\nall performances were nearby, including DistilBERT, which helps to gain incredible performance in less train-\ning time thanks to knowledge distillation. For example, for 4 epochs, BERT took 70 minutes to train, while\nDistilBERT took 35 minutes, losing only 0.6 F1 points, but saving half the time taken by BERT. Moreover, our\nablation study shows that the maximum length of the sequence is one of the parameters having a signiﬁcant im-\npact on the ﬁnal results and must be carefully analyzed and adjusted. Likewise, data quality is a must for good\nperformance, data that will do not need to be processed, since extensive data cleaning processes may not help\nthe model capture local and global contexts in sequences, distilled sometimes with words removed or trimmed\nduring cleaning. Besides, we notice, that the majority of the models we ﬁne-tuned on the IMDb dataset start\nto overﬁt at a certain number of epochs, which can lead to biased models. However, good quality data is not\neven enough, but pre-training a model on large amounts of business problem data and vocabulary may help on\npreventing it from making wrong predictions and may help on reaching a high level of generalization.\nACKNOWLEDGMENTS\nWe are grateful to the Hugging Face team for their role in democratizing state-of-the-art machine\nlearning and natural language processing technology through open-source tools. Their commitment to pro-\nviding valuable resources to the research community is highly appreciated, and we acknowledge their vital\ncontribution to the development of our article.\nAPPENDIX\nAppendix for ”Analysis of the evolution of advanced transformer-based language models: experi-\nments on opining mining”.\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2006 Ì ISSN: 2252-8938\nTable A1. Summary and comparison of transformer-based models\nModel L H A Att.\ntype\nTotal\nparams\nTokenization Training data Computational\ncost\nTraining objec-\ntives\nPerformance tasks Short description\nGPT 12 512 12 Global 110M Byte-pair-\nencoding\n[32]\nBooks Corpus (800M\nwords)\n- Autoregressive,\ndecoder\nZero-shot, text sum-\nmarization, question\nanswering, translation.\nThe ﬁrst transformer-\nbased autoregressive and\ncausal masking model.\nBERT 12 768 12 Global 110M WordPiece\n[30]\nBooks Corpus (800M\nwords) and English\nWikipedia (2,500M\nwords)\n4 days on 4\nCloud TPUs in\nPod conﬁgura-\ntion.\nAutoencoding,\nencoder (MLM -\nNSP)\nText classiﬁcation, nat-\nural language inference,\nquestion answering.\nThe ﬁrst transformer-\nbased autoencoding\nmodel, that uses global\nattention to provide\nhigh-level bidirectional\ncontextualization.\nGPT-2 12 1600 12 Global 117M Byte-pair-\nencoding\nWebText (10B words) - Autoregressive,\ndecoder\nZero-shot, text sum-\nmarization, question\nanswering, translation.\nOptimized and bigger than\nGPT and performs well on\nzero-shot settings.\nGPT-3 96 12288 96 Global 175B Byte-pair-\nencoding\nFiltered Common\nCrawl, WebText2,\nBooks1, Books2, and\nWikipedia for 300B\nwords.\n- Autoregressive,\ndecoder\nText summarization,\nquestion answering,\ntranslation, zero-shot,\none-shot, few-shot.\nBigger that its predeces-\nsors.\nALBERT 12 768 12 Global 11M SentencePiece\n[31]\nBooks Corpus\n[35] and English\nWikipedia.\nCloud TPU V3\nTPUs number\nranges from\n64 to 512 (32h\nALBERT-\nxxlarge).\nAutoencoding,\nencoder,\nsentence-\nordering predic-\ntion (SOP)\nSemantic similarity, se-\nmantic relevance, ques-\ntion answering, reading\ncomprehension.\nSmaller and similar to\nBERT with minimal\ntweaks including the split-\nting of layers into groups\nvia cross-layer parameter\nsharing, making it faster\nand reducing memory\nfootprint.\nDistilBERT 6 768 12 Global 66M WordPiece English Wikipedia\nand Toront Book\nCorpus.\n90 hours on 8\n16GB V100\nGPUs.\nAutoencoding\n(MLM), encoder\nSemantic similarity, se-\nmantic relevance, ques-\ntion answering, textual\nentailment.\nPre-training leveraging\nknowledge distillation to\ndeliver great results as\nBERT with lower latency.\nSimilar to BERT model\nbut smaller.\nRoBERTa 12 1024 12 Global 125M Byte-pair-\nencoding\nBook Corpus [35],\nCC-News, Open Web\nText, and Stories [36].\n8 32GB Nvidia\nV100 GPUs.\nAutoencoding\n(Dynamic\nMLM, No NSP),\nencoder\nText classiﬁcation, lan-\nguage inference, ques-\ntion answering.\nPre-trained with large\nbatches using some tricks\nfor diverse learning like\ndynamic masking, where\ntokens are differently\nmasked for each epoch.\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 2007\nXLM 12 2048 8 Global - Byte-pair en-\ncoding\nWikipedias of the\nXNLI languages.\n64 V olta GPUs\nfor the language\nmodeling tasks\nand 8 GPUs for\nthe MT tasks.\nAutoencoding,\nencoder, causal\nlanguage mod-\neling (CLM),\nmasked lan-\nguage modeling\n(MLM), and\ntranslation lan-\nguage modeling\n(TLM).\nTranslation tasks and\nNLU cross-lingual\nbenchmarks.\nBy being trained on sev-\neral pre-training objectives\non a multilingual corpus,\nXLM proves that multilin-\ngual pre-training methods\nhave a strong impact, espe-\ncially on the performance\nof multilingual tasks.\nXLM-\nRoBERTa\n12 768 8 Global 270M SentencePiece CommonCrawl Cor-\npus in 100 languages.\n100 32GB\nNvidia V100\nGPUs.\nAutoencoding,\nencoder, MLM.\nTranslation tasks and\nNLU cross-lingual\nbenchmarks.\nUsing only the masked\nlanguage modeling objec-\ntive, XLM-RoBERTa uses\nRoBERTa tricks on XLM\napproaches. it is able to\ndetect the input language\nby itself (100 languages).\nELECTRA 12 768 12 Global 110M WordPiece Wikipedia,\nBooksCorpus, Gi-\ngas5 [37], ClueWeb\n2012-B, and Common\nCrawl.\n4 days on 1\nGPU.\nGenerator (au-\ntoregressive,\nreplaced token\ndetection) and\ndiscriminator\n(Electra: pre-\ndicting masked\ntokens).\nSentiment analysis, lan-\nguage inference tasks.\nReplaced token detection\nis a pre-training objective\nthat addresses MLM is-\nsues and it resutls in efﬁ-\ncient performance.\nDeBERTa 12 768 12 Global\n(Disen-\ntangled\natten-\ntion)\n125M Byte-pair en-\ncoding\nWikipedia,\nBooksCorpus, Red-\ndit content, Stories,\nSTORIES.\n10 days 64 V100\nGPUs.\nAutoencoding,\ndisentangled\nattention mech-\nanism, and\nenhanced mask\ndecoder.\nDeBERTa was the ﬁrst\npretrained model to beat\nHLP on the SuperGLUE\nbenchmark [38].\nDeBERTa uses RoBERTa\nwith disentangled atten-\ntion and an enhanced\nmask decoder to signif-\nicantly improve model\nperformance on many\ndownstream tasks while\nbeing trained only on\nhalf of the data used in\nRoBERTa large version.\nXLNet 12 768 12 Global 110M SentencePiece Wikipedia,\nBooksCorpus, Gi-\ngas5 [37], ClueWeb\n2012-B, and Common\nCrawl.\n5.5 days on 512\nTPU v3 chips.\nAutoregressive,\ndecoder\nXLNet achieved state-\nof-the-art results and\noutperformed BERT\non 20 downstream\ntask inlcuding senti-\nment analysis, question\nanswering, reading com-\nprehension, document\nranking.\nXLNet incorporates ideas\nfrom transformer-XL [17]\nand addresses the pretrain-\nﬁnetune BERT’s discrep-\nancy being more capable\nto grasp dependencies be-\ntween masked tokens.\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2008 Ì ISSN: 2252-8938\nBART 12 768 16 Global 139M Byte-pair en-\ncoding\nWikipedia,\nBooksCorpus.\n- Generative\nsequence to se-\nquence, encoder\ndecoder, token\nmasking, token\ndeletion, text in-\nﬁlling, sentence\npermutation,\nand document\nrotation.\nBART beats its prede-\ncessors on generation\ntasks such as translation\nand achieved state-of-\nthe-art results, while\nperforming similarly to\nRoBERTa on discrim-\ninative tasks including\nquestion answering and\nclassiﬁcation.\nTrained to map corrupted\ntext to the original using an\narbitrary noising function.\nConvBERT 12 768 12 Global 124M WordPiece OpenWebText [39] GPU and TPU Autoencoding,\nencoder\nWith fewer parameters\nand lower costs Con-\nvBERT consistently out-\nperforms BERT on var-\nious downstream tasks\nwith less training cost.\nFor reduced redundancy\nand better modeling of\nglobal and local context,\nBERT’s self-attention\nblocks are replaced by\nmixed-attention blocks in-\ncorporating self-attention\nand span-based dynamic\nconvolutions.\nReformer 12 1024 8 Attenion\nwith\nlocal\nsensitive\nhashing\n149M SentencePiece OpenWebText [39] Parallelization\nacross 8 GPUs\nor 8 TPU v3\ncores.\nAutoregressive,\ndecoder.\nPerforms well with\nparagmatic require-\nments, thanks to re-\nduction of the attention\ncomplexity.\nAn efﬁcient and faster\ntransformer that costs less\ntime on long sequences\nthanks to two optimization\ntechniques, local-sensitive\nhashing attention and axial\nposition encoding.\nT5 12 768 12 Global 220M SentencePiece The Colossal Clean\nCrawled Corpus (C4)\nCloud TPU\nPods.\nGenerative\nsequence to\nsequence,\nencoder-\ndecoder.\nEntailement, coreference\nchallenges, question an-\nswering tasks via Super-\nGLUE benchmark\nTo incorporate the vari-\neties of most linguistic\ntasks, T5 pre-trained on a\nmix of supervised and un-\nsupervised tasks in a text-\nto-text format.\nLongformer 12 768 12\nLocal +\nGlobal.\n149M Byte-pair-\nencoding\nBooks corpus, En-\nglish Wikipedia, and\nRealnews dataset [40]\n. Autoregressive,\ndecoder\nLongformer achieved\nstate-of-the-art results\non two benchmark\ndatasets WikiHop and\nTriviaQA.\nFor heigher training efﬁ-\nciency on long documents,\nLongformer uses sparse\nmatrices instead of atten-\ntion matrices to linearly\nscale with sequences of\nlength up to 4 096.\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010\nInt J Artif Intell ISSN: 2252-8938 Ì 2009\nREFERENCES\n[1] K. R. Chowdhary, “Natural language processing,” in Fundamentals of artiﬁcial intelligence, 2020, pp. 603–649, doi: 10.1007/978-\n81-322-3972-7 19.\n[2] M. Rhanoui, M. Mikram, S. Yousﬁ, A. Kasmi, and N. Zoubeidi, “A hybrid recommender system for patron driven library acquisition\nand weeding,” inJournal of King Saud University-Computer and Information Sciences, 2020, vol. 34, no. 6, Part A, pp. 2809–2819,\ndoi: 10.1016/j.jksuci.2020.10.017.\n[3] F. Z. Trabelsi, A. Khtira, and B. El Asri, “Hybrid recommendation systems: a state of art.,” in Proceedings of the 16th\nInternational Conference on Evaluation of Novel Approaches to Software Engineering (ENASE) , 2021, pp. 281–288, doi:\n10.5220/0010452202810288.\n[4] B. Pandey, D. K. Pandey, B. P. Mishra, and W. Rhmann, “A comprehensive survey of deep learning in the ﬁeld of medical imaging\nand medical natural language processing: challenges and research directions,” in Journal of King Saud University-Computer and\nInformation Sciences, 2021, vol. 34, no. 8, Part A, pp. 5083–5099, doi: 10.1016/j.jksuci.2021.01.007.\n[5] A. Harnoune, M. Rhanoui, M. Mikram, S. Yousﬁ, Z. Elkaimbillah, and B. El Asri, “BERT based clinical knowledge extraction for\nbiomedical knowledge graph construction and analysis,” inComputer Methods and Programs in Biomedicine Update, 2021, vol. 1,\np. 100042, doi: 10.1016/j.cmpbup.2021.100042.\n[6] W. Medhat, A. Hassan, and H. Korashy, “Sentiment analysis algorithms and applications: a survey,” in Ain Shams engineering\njournal, 2014, vol. 5, no. 4, pp. 1093–1113, doi: 10.1016/j.asej.2014.04.011.\n[7] S. Sun, C. Luo, and J. Chen, “A review of natural language processing techniques for opinion mining systems,” in Information\nfusion, 2017, vol. 36, pp. 10–25, doi: 10.1016/j.inffus.2016.10.004.\n[8] S. Yousﬁ, M. Rhanoui, and D. Chiadmi, “Mixed-proﬁling recommender systems for big data environment,” in First International\nConference on Real Time Intelligent Systems, 2017, pp. 79–89, doi: 10.1007/978-3-319-91337-7 8.\n[9] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: pre-training of deep bidirectional transformers for language understand-\ning,” in NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies - Proceedings of the Conference, 2019, vol. 1, pp. 4171–4186, doi: 10.18653/v1/N19-1423.\n[10] A. Vaswani et al., “Attention is all you need,” in Proceedings of the 31st Conference on Neural Information Processing Systems ,\nDec. 2017, pp. 5998–6008, doi: 10.48550/arXiv.1706.03762.\n[11] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding with unsupervised learning,” Pro-\nceedings of the 2018 Conference on Neural Information Processing Systems, 2018.\n[12] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,”OpenAI\nblog, vol. 1, no. 8, p. 9, 2019.\n[13] T. B. Brown et al. , “Language models are few-shot learners,” in Proceedings of the 34th International Conference on Neural\nInformation Processing Systems, 2020, vol. 33, pp. 1877–1901, doi: 10.48550/arXiv.2005.14165.\n[14] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert: A lite bert for self-supervised learning of language\nrepresentations,”International Conference on Learning Representations, 2019, doi: 10.48550/arXiv.1909.11942.\n[15] Y . Liu et al. , “RoBERTa: a robustly optimized BERT pretraining approach,” arXiv:1907.11692, 2019, doi:\n10.48550/arXiv.1907.11692.\n[16] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V Le, “XlNet: generalized autoregressive pre-\ntraining for language understanding,” in Advances in neural information processing systems , 2019, pp. 5753–5763, doi:\n10.48550/arXiv.1906.08237.\n[17] Z. Yang, Y . Yang, J. Carbonell, Q. Le, and R. Salakhutdinov, “Transformer-XL: attentive language models beyond a ﬁxed-length\ncontext,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , Jul. 2019, pp. 2978–2988,\ndoi: 10.18653/v1/P19-1285.\n[18] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv\n2019,” inarXiv preprint arXiv:1910.01108, 2019, doi: 10.48550/arXiv.1910.01108.\n[19] G. E. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” in arXiv preprint arXiv:1503.02531, vol. 2,\nno. 7, Mar. 2015, doi: 10.48550/arXiv.1503.02531.\n[20] G. Lample and A. Conneau, “Cross-lingual language model pretraining,” arXiv:1901.07291, 2019, doi:\n10.48550/arXiv.1901.07291.\n[21] A. Conneau et al., “Unsupervised cross-lingual representation learning at scale,” in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 2020, pp. 8440–8451, doi: 10.18653/v1/2020.acl-main.747.\n[22] M. Lewis et al., “Bart: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehen-\nsion,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , 2020, pp. 7871–7880, doi:\n10.18653/v1/2020.acl-main.703..\n[23] Z. Jiang, W. Yu, D. Zhou, Y . Chen, J. Feng, and S. Yan, “ConvBERT: Improving BERT with span-based dynamic convo-\nlution,” in Proceedings of the 34th International Conference on Neural Information Processing Systems , 2020, p. 12, doi:\n10.48550/arXiv.2008.02496.\n[24] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: the efﬁcient transformer,” arXiv:2001.04451, 2020, doi:\n10.48550/arXiv.2001.04451.\n[25] C. Raffel et al., “Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.,” in Journal of Machine Learning\nResearch, 2020, vol. 21, no. 140, pp. 1–67, doi: 10.48550/arXiv.1910.10683.\n[26] K. Clark, M.-T. Luong, Q. V Le, and C. D. Manning, “Electra: pre-training text encoders as discriminators rather than generators,”\narXiv:2003.10555, p. 18, 2020, doi: 10.48550/arXiv.2003.10555.\n[27] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: the long-document transformer,” arXiv:2004.05150, 2020, doi:\n10.48550/arXiv.2004.05150.\n[28] P. He, X. Liu, J. Gao, and W. Chen, “DeBERTa: decoding-enhanced BERT with disentangled attention,” arXiv:2006.03654, 2020,\ndoi: 10.48550/arXiv.2006.03654.\n[29] S. Singh and A. Mahmood, “The NLP cookbook: modern recipes for transformer based deep learning architectures,” in IEEE\nAccess, 2021, vol. 9, pp. 68675–68702, doi: 10.1109/access.2021.3077350.\nAnalysis of the evolution of advanced transformer-based language models: ... (Nour Eddine Zekaoui)\n2010 Ì ISSN: 2252-8938\n[30] Y . Wu et al., “Google’s neural machine translation system: Bridging the gap between human and machine translation,” in arXiv\npreprint arXiv:1609.08144, 2016, doi: 10.48550/arXiv.1609.08144.\n[31] T. Kudo and J. Richardson, “Sentence Piece: A simple and language independent subword tokenizer and detokenizer for Neural\nText Processing,” inProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demon-\nstrations, 2018, pp. 66–71, doi: 10.18653/v1/D18-2012.\n[32] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , 2016, vol. 1, pp. 1715–1725, doi:\n10.18653/v1/P16-1162.\n[33] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts, “Learning word vectors for sentiment analysis,” inProceedings\nof the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , Jun. 2011, vol. 1,\npp. 142–150.\n[34] N. E. Zekaoui, “Opinion transformers.” 2023, [Online]. Available: https://github.com/zekaouinoureddine/Opinion-Transformers\n(Accessed Jan. 2, 2023).\n[35] Y . Zhu et al., “Aligning books and movies: towards story-like visual explanations by watching movies and reading books,” in Pro-\nceedings of the IEEE International Conference on Computer Vision, 2015, vol. 2015 Inter, pp. 19–27, doi: 10.1109/ICCV .2015.11.\n[36] T. H. Trinh and Q. V Le, “A simple method for commonsense reasoning,” in arXiv:1806.02847, 2018, doi:\n10.48550/arXiv.1806.02847.\n[37] R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda, “English gigaword ﬁfth edition, linguistic data consortium,” 2011, doi:\n10.35111/wk4f-qt80.\n[38] A. Wang et al., “SuperGLUE: A stickier benchmark for general-purpose language understanding systems,” in Advances in neural\ninformation processing systems, 2019, vol. 32, doi: 10.48550/arXiv.1905.00537.\n[39] A. Gokaslan and V . Cohen, “OpenWebText Corpus,” 2019. http://skylion007.github.io/OpenWebTextCorpus (Accessed Jan. 2,\n2023).\n[40] R. Zellers et al., “Defending against neural fake news,” Advances in Neural Information Processing Systems, vol. 32, p. 12, 2019,\ndoi: 10.48550/arXiv.1905.12616.\nBIOGRAPHIES OF AUTHORS\nNour Eddine Zekaoui\n holds an Engineering degree in Knowledge and Data Engineer-\ning from School of Information Sciences, Morocco in 2021. He is currently a Machine Learn-\ning Engineer in a tech company. His research focuses on the areas of natural language process-\ning and artiﬁcial intelligence, including information retrieval, question answering, semantic simi-\nlarity, and bioinformatics. He can be contacted at email: noureddinezekaoui@gmail.com or nour-\neddine.zekaoui@esi.ac.ma.\nSiham Yousﬁ\n is a Professor of Computer Sciences and Big Data at the School of Informa-\ntion Sciences, Rabat since 2011. She is a PhD holder from Mohammadia School of engineering of\nMohammed V University in Rabat (2019). Her research interests include big data, natural language\nprocessing and artiﬁcial intelligence. She can be contacted at email: syousﬁ@esi.ac.ma.\nMaryem Rhanoui\n is an Associate Professor of Computer Sciences and Data Engineering at\nthe School of Information Sciences, Rabat. She received an engineering degree in computer science\nthen a PhD degree from ENSIAS, Mohammed V University, Rabat 2015). Her research interests\ninclude pattern recognition, computer vision, cybersecurity and medical data analysis. She can be\ncontacted at email: mrhanoui@esi.ac.ma.\nMounia Mikram\n is an Associate Professor of Computer Sciences and Mathematics at the\nSchool of Information Sciences, Rabat since 2010. She received her master degree from Mohammed\nV University Rabat (2003) and her PhD degree from Mohammed V University, Rabat, and Bordeaux\nI University (2008). Her research interests include pattern recognition, computer vision, biometrics\nsecurity systems and artiﬁcial intelligence. She can be contacted at email: mmikram@esi.ac.ma.\nInt J Artif Intell, V ol. 12, No. 4, December 2023: 1995–2010",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8426617383956909
    },
    {
      "name": "Sentiment analysis",
      "score": 0.8376342058181763
    },
    {
      "name": "Transformer",
      "score": 0.7209135293960571
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6142669916152954
    },
    {
      "name": "Natural language processing",
      "score": 0.5744943022727966
    },
    {
      "name": "Scope (computer science)",
      "score": 0.534487247467041
    },
    {
      "name": "Language model",
      "score": 0.504327654838562
    },
    {
      "name": "Focus (optics)",
      "score": 0.4417755603790283
    },
    {
      "name": "Language understanding",
      "score": 0.4293366074562073
    },
    {
      "name": "Deep learning",
      "score": 0.4130743145942688
    },
    {
      "name": "Machine learning",
      "score": 0.4119374752044678
    },
    {
      "name": "Data science",
      "score": 0.34151774644851685
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}