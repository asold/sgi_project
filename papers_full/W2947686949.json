{
  "title": "Efficient Adaptation of Pretrained Transformers for Abstractive Summarization",
  "url": "https://openalex.org/W2947686949",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Hoang, Andrew",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227134328",
      "name": "Bosselut, Antoine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223995598",
      "name": "Celikyilmaz, Asli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2149953915",
      "name": "Choi, Yejin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2574535369",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2897139265",
    "https://openalex.org/W2902744721",
    "https://openalex.org/W2741375528",
    "https://openalex.org/W2899386490",
    "https://openalex.org/W2794945088",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2797269287",
    "https://openalex.org/W2799064010",
    "https://openalex.org/W2740711318",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964237709",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2182959134",
    "https://openalex.org/W2952942550",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2951777553"
  ],
  "abstract": "Large-scale learning of transformer language models has yielded improvements on a variety of natural language understanding tasks. Whether they can be effectively adapted for summarization, however, has been less explored, as the learned representations are less seamlessly integrated into existing neural text production architectures. In this work, we propose two solutions for efficiently adapting pretrained transformer language models as text summarizers: source embeddings and domain-adaptive training. We test these solutions on three abstractive summarization datasets, achieving new state of the art performance on two of them. Finally, we show that these improvements are achieved by producing more focused summaries with fewer superfluous and that performance improvements are more pronounced on more abstractive datasets.",
  "full_text": "Efﬁcient Adaptation of Pretrained Transformers for\nAbstractive Summarization\nAndrew Hoang♥, Antoine Bosselut♥♠, Asli Celikyilmaz♣, Yejin Choi♥♠\n♥Allen School of Computer Science & Engineering, University of Washington, Seattle, W A\n♠Allen Institute for Artiﬁcial Intelligence, Seattle, W A\n♣Microsoft Research, Redmond, W A\n{antoineb, yejin}@cs.washington.edu {asli}@ieee.org\nAbstract\nLarge-scale learning of transformer language models has yielded improvements\non a variety of natural language understanding tasks. Whether they can be effec-\ntively adapted for summarization, however, has been less explored, as the learned\nrepresentations are less seamlessly integrated into existing neural text production\narchitectures. In this work, we propose two solutions for efﬁciently adapting\npretrained transformer language models as text summarizers: source embeddings\nand domain-adaptive training. We test these solutions on three abstractive sum-\nmarization datasets, achieving new state of the art performance on two of them.\nFinally, we show that these improvements are achieved by producing more focused\nsummaries with fewer superﬂuous and that performance improvements are more\npronounced on more abstractive datasets.\n1 Introduction\nRecent work in large-scale language models [ 19; 20; 5] has allowed pretrained contextual repre-\nsentations to be easily adapted for a variety of downstream tasks, yielding improvements on many\nbenchmarks evaluating natural language understanding [27]. Less explored, however, has been the\neffect of these pretrained representations on text production tasks, such as abstractive summarization,\nwhere state of the art performance is still achieved with sequence to sequence (seq2seq) models [1; 6].\nThese sequence-to-sequence methods typically use an encoder and decoder model with separate\nparameters to represent the input article and produce the output summary, and the most successful\nsolutions [1; 6; 22] use attention mechanisms that learn an alignment between encoder and decoder\nstates. Pretrained language models, however, do not learn the parameters for such a task-speciﬁc\nalignment, making it challenging to integrate their learned representations into a summarization\narchitecture at a higher level of abstraction than the word embedding.\nIn this work, we adapt full transformer language models for abstractive summarization. Building off\nthe work of Liu et al. [12], who ﬁrst proposed concatenating input and output text to a joint sequence\nand using a common transformer to encode both, we use a language model as a summarizer (rather\nthan an encoder-decoder). With this approach, representations from a pretrained transformer language\nmodel (in this case, GPT [20]) can be used to fully initialize the parameters of the summarization\nmodel, allowing it to leverage the representational power of a model trained at much larger scale.\nTo accomplish this effectively, we outline two strategies for adapting pretrained representations for\nabstractive summarization. In the ﬁrst, we augment the input representation of the summarization\nmodel by instantiating source embeddings that encode the token type of the text being read. This\nchange allows the model to recognize whether a given token belongs to the input article or the output\nsummary, thereby learning how to distinguish both types of text when encoding. In the second, we\nPreprint. Under review.\narXiv:1906.00138v1  [cs.CL]  1 Jun 2019\n, ...,xa1 xaM _delim_ , ...,xs1 xsN _end_ \ne[ ], ..., e[ ]xa1 xaM e[_delim_] e[ ], ..., e[ ]xs1 xsN e[_end_]Word \nEmbeddings \nInput \nSequence \n, ...,p1 pM p0 , ...,p1 pN pN+1Position \nEmbeddings \n, ...,da da ds , ...,ds ds dsSource \nEmbeddings \nFigure 1: The embedding process for inputs to the Transformer-SM model.\nintroduce a domain-adaptive training procedure that ﬁne-tunes the transformer toward understanding\ngeneral newswire text before training on the summarization end task directly, allowing the model\nto learn the general structure and language distribution of newswire text before being ﬁne-tuned to\nproduce summaries.\nA comprehensive empirical study across three datasets, CNN/DailyMail [8], XSum [16], and News-\nroom [7], shows that transformer language models can be used to train abstractive summarizers,\nproducing summaries that are more concise and focused than state of the art baselines. Our investiga-\ntion also empirically validates several observations about the abstractive summarization task. First,\nechoing the results of Sun et al. [24], the most common summarization evaluation metric, ROUGE\n[10], is highly sensitive to summary length, providing an advantage to methods that produce longer\nsummaries, either through learning or with minimum summary length constraints. Second, achieving\nhigher ROUGE scores is not strongly consistent with human assessments of abstractive summary\nquality. Finally, despite being conceived as abstractive summarizers, most current state of the art\nmodels are highly extractive, copying phrases and even sentences verbatim from the document.\n2 Model\nIn this paper, we focus on a variant of the Transformer [26] that has been pretrained on a large corpus\nof natural language stories: the GPT model [20]. As our architecture is practically identical to the\none proposed in Radford et al. [20], we point readers to that work for background on the architecture\nof the model, and focus below on the enhancements to the input representation made in our approach.\n2.1 Input Representation\nEach article is represented as a sequence of M tokens Xa = {xa}M\nm=1 = xa\n1, ..., xa\nM and its\ncorresponding summary is a sequence of N tokens Xs = {xs}N\nn=1 = xs\n1, ..., xs\nN . As outlined\nin Figure 1, the input structure of the training set is a pair of article and corresponding summary\nconcatenated into two sequences similar to [12]:\nX = {x}T\nt=1 = [Xa; <D>; Xs, <E>] (1)\nwhere T = M + N + 2, and <D> and <E> are special tokens identifying the delimitation and end of\nthe sequence. Below, we deﬁne the process of encoding these sequences as inputs to the transformer.\nWord Embedding First, each token xt in the concatenated sequence X indexes a word embedding\ne[xt] ∈Rh from a joint vocabulary for the article and summary (and special tokens).\nPosition Embedding Second, since the transformer (a self-attention model) has no concept of\nordering of tokens, a position embedding pt ∈Rh is initialized for each absolute position in the\nsequence [26]. The embedding for each position in the sequence is added to the word embedding of\nthe token occupying that position, augmenting the ﬁnal representation of the input. For example, each\ntoken in the article would be represented as: wa\nm = e[xa\nm] +pm. Once the delimitation token <D> is\n2\nreached, the position counter is reset. For example, the ﬁrst token of the article, xa\n1, and the ﬁrst token\nof the summary, xs\n1, both receive p1 as a positional embedding to augment their representations.\nSource Embedding Finally, because the transformer must recognize pragmatic differences between\nthe text of the article it reads and the text of the summary it learns to produce, an additional, source-\nspeciﬁc embedding is initialized, d ∈Rh. The source embedding encodes whether a token is from\nthe article portion da of the concatenated input, or the summary portion ds. For any article token\n(Eq. 2) or summary token (Eq. 3) then, the ﬁnal encoding is:\n∀\nm∈[1,M]\nwa\nm = e[xa\nm] +pm + da (2) ∀\nn∈[1,N]\nws\nn = e[xs\nn] +pn + ds (3)\nIn contrast to the other embeddings in the model, the source embeddings are not pretrained, intro-\nducing the potential that they could dominate pretrained representations for the word and position\nembeddings when summed (Eq. 2, 3). To avoid this, we normalize the random initialization of the\nsource embeddings to have norm equal to half of the average norm of the word embeddings.\n3 Training\nThe model is initialized with pretrained parameters from the GPT model [ 20] that was trained on\nthe BooksCorpus [28]. Following this initialization, we pursue two additional training procedures:\ndomain-adaptive training and end task training.\n3.1 Domain-adapative Training\nDespite the beneﬁt of using pretrained representations from the GPT model to initialize a summarizer,\nthere is a language shift between the storybooks data on which the GPT model was trained and\nthe type of language found in newswire summarization datasets [8; 16; 7]. Additionally, there are\nstructural differences between how articles are written (usually expressing salient points early on,\nfollowed by details later) and how stories unfold (less front-loading of key information).\nTo address this discrepancy, we propose domain-adaptive training (DAT) to adapt the transformer\nsummarization model to the language distribution of newswire text by maximizing the conditional\nloglikelihood of the article tokens and summary tokens given all previous tokens in their concatenated\ninput representation (see Figure 1):\nLdat = −\nM∑\nm=1\nlog P(xa\nm|{xa}m\n1 ) −\nN∑\nn=1\nlog P(xs\nn|{xs}n\n1 , {xa}M\n1 ) (4)\nwhere M is length of the article,N is the length of the summary,{xa}<m is the set of all tokens in the\narticle that precede xa\nm, {xs}<n is the set of all tokens in the summary that precede xs\nn, and {xa}M\nis the set of all article tokens. In this framework, the model is adapted to produce newswire-like\nlanguage before being trained on the summarization end task, which only focuses on learning for\nsummary production.\n3.2 End Task Training\nDuring end task training (ETT), the model is trained speciﬁcally to be able to produce a summary\ngiven a document, constraining the loss function toward maximizing the conditional loglikelihood of\nproducing only the correct summary tokens given the set of article tokens {xa}M :\nLett = −\nN∑\nn=1\nlog P(xs\nn|{xs}<n, {xa}M ) (5)\nwhere {xs}<n is the set of tokens in the summary that precede xs\nn.\n3\nTable 1: Comparison of summarization datasets with respect to dataset size, proportion of unique\nn-grams, mean article length in words, and mean summary length in words.\nDataset Split Size % Novel n-grams in Gold Summary Mean # Words\nTrain Validation Test unigrams bigrams trigrams 4-grams Article Summary\nNewsroom 993,746 108,590 108,636 17.40 44.05 55.38 61.21 658.6 26.7\nXSum 204,045 11,332 11,334 34.88 78.78 92.03 96.80 431.1 23.3\nCNN/DailyMail287,227 13,368 11,490 12.70 46.29 65.04 75.56 685.2 52.0\n4 Experimental Setup\nDatasets The CNN/Daily Mail dataset [8] consists of articles from CNN and Daily Mail. Each\narticle is associated with several descriptive bullet point highlights. Similar to previous work [15],\nwe concatenate the highlights to create a target summary for each article in the dataset and use the\nsame dataset splits. The Extreme Summarization (XSum) dataset [ 16] consists of ∼230k article\nsummary pairs taken from the BBC. Each summary is a single sentence long and is professionally\nwritten (usually by the author), making the dataset exhibit more abstractive content than typical\nsummarization datasets, such as CNN/DailyMail [8]. The Newsroom dataset [7] consists of ∼1.2M\narticle summary pairs scraped from the Internet Archive. The articles come from a set of 38 publishers\nand cover diverse topics. We provide statistics about each dataset in Table 1.\nData Preprocessing We used a bytepair encoding (BPE) for tokenization. For each summarization\ndataset, we use the BPE to tokenize each article and summary, and then truncate the articles to a\nmaximum length of 512 tokens and each summary to a maximum length of 110 tokens. We then\nformat each article summary pair into the format outlined in Figure 1.\nModel Speciﬁcations We used a transformer decoder with N = 12blocks and h = 12masked\nself-attention heads in each block. We set the dimensionality of each self-attention head to be\ndmodel = 768. Unless stated otherwise, we use the pretrained weights of Radford et al. [20] to\ninitialize the parameters of the model. Special tokens that are added to the vocabulary (i.e. the\nend token, start token, and delimiter token) are initialized by sampling from the standard normal\ndistribution. Our full model with source embeddings (§2.1) is denoted as as Transformer-SM and we\nalso train an ablation, Transformer-LM, that does not use source embeddings.\nTraining Details All models were trained with a learning rate of 6.25 ×10−5 and a minibatch\nsize of 64. When domain-adaptive training (DAT) is used, we train for 10 epochs using DAT and\nthen for an additional 10 epochs using end task training (ETT). Without DAT, we train on the end\ntask for 20 epochs. Unless speciﬁed otherwise, the ﬁnal model trained for each dataset uses both\ndomain-adaptive training and end task training. We did not tune hyperparameters. All models were\ntrained using the PyTorch package1 and the HuggingFace implementation of GPT.2 We trained each\nmodel on 8 Tesla V100-SMX2. Training for a total of 20 epochs took approximately 1 day of clock\ntime for the XSum and CNN/Daily Mail datasets, and 3 days for the Newsroom dataset. Our source\ncode is publicly available.3\nGeneration We perform generation by using beam search with a beam size of 3. We use the trigram\ntrick [18] during beam search. Each summary token is generated by decoding from the distribution\nyielded by the model from processing an input tensor that is the concatenation of the article tokens,\nthe delimiter token, and any previously generated summary tokens.\nEvaluation We evaluate our system with common summarization metrics: ROUGE-1 (R-1), a\nmeasure of unigram recall between the summary and document, ROUGE-2 (R-2), a similar measure\nof bigram recall, and ROUGE-L (R-L), a measure of the longest common subsequence between the\nsummary and document [11]. We also report the length of the summary in terms of tokens produced.\nFor each dataset, for evaluation on the test set, we selected models with the largest ROUGE-1 score\non a subset of 500 samples from the validation set.\n1https://pytorch.org/\n2https://github.com/huggingface/pytorch-openai-transformer-lm\n3https://github.com/Andrew03/transformer-abstractive-summarization\n4\n5 Experiments\n5.1 CNN/Daily Mail\nBaselines We report the results from various models previously trained and evaluated on the\nCNN/Daily Mail dataset. The PGen and PGen + Coverage models [ 22], consist of attentive RNN\nencoder-decoders that integrate the ability to directly copy from the article when generating tokens.\nPasunuru and Bansal [17] extend this work by adding policy gradient training with a mixture of\nrewards that promote saliency and entailment. Bottom-up summarization and the Copy Transformer\n[6] also extend See et al. [22] by using the copy mechanism to compress the article to only relevant\ncontent before summarizing it. Chen and Bansal [2] also look at performing content selection, but\nextract full sentences from the document with a novel extractor model. Finally, the DCA model\n[1] uses multiple separate communicating encoders over different parts of the document to produce\nrepresentations that are more focused on salient details.\nTable 2: ROUGE F1 results on the test set of CNN/Daily Mail. Best model results are bolded.\nModel R-1 R-2 R-L Length ( L)\nPGen [22] 36.44 15.66 33.42 53.69\nPGen + Coverage [22] 39.53 17.28 36.38 59.75\nRougeSal + Ent RL [17] 40.43 18.00 37.10 -\nBottom-Up Summ [6] 41.22 18.68 38.34 55.25\nCopyTransformer [6] 40.96 18.38 38.16 -\nrnn-ext + RL [2] 41.47 18.72 37.76 77.44\nDCA [1] 41.67 19.47 37.92 51.01\nTransformer-LM 38.67 17.47 35.79 43.40\nTransformer-SM 37.96 17.36 35.12 42.42\nAutomatic Metrics We report our results using automatic metrics in Table 2. On this dataset, our\nmain model, Transformer-SM, performs slightly worse than other state of the art models. We note\nthat our model tends to generate shorter summaries than the gold summaries (∼20% shorter), which\ncould lower ROUGE recall performance.\nIn Figure 2, we investigate the correlation of ROUGE-L scores with summary length, and note that a\nminimum decoding length used by state-of-the-art algorithms places baseline generated summaries in\nlength bins of higher average ROUGE-L performance. When Transformer-SM produces summaries\nin these same length bins (i.e., more than 30 tokens), its performance is only consistently beaten by\nthe DCA model, which was ﬁne-tuned with RL.\nFigure 2: Average ROUGE-L for summaries in different length bins. Scatter plots correspond to\nROUGE-L scores for each bin, while solid lines correspond to the number of summaries in each bin\n5\nTable 3: Head-to-head comparison between test set outputs of (Left) DCA [ 1] and Transformer-\nSM (Right) PGen+Cov [22] and Transformer-SM. Analyses done on summaries for CNN/DailyMail.\nModel DCA Same T-SM PGen+Cov Same T-SM\nNon-redundancy 96 116 163 77 85 213\nCoherence 136 60 179 160 35 180\nFocus 136 36 203 115 33 227\nOverall 138 36 201 150 39 186\nHuman Evaluation While ROUGE scores are negatively inﬂuenced by the shorter average length\nof the summaries produced by our model, it is not clear that shorter summaries are correlated with\nworse quality. To evaluate this hypothesis, we perform a human evaluation on 125 (article, summary)\npairs randomly sampled from the test set. The article and model-generated summaries were presented\nto three workers from Amazon Mechanical Turk (AMT).\nEach worker was presented two model-generated summaries, one produced by the Transformer-\nSM model, and one from the DCA model [ 1] or the PGen+Cov model [22]. Workers were asked\nto select the better summary for four different quality metrics from Celikyilmaz et al. [1]: non-\nredundancy (fewer of the same ideas are repeated), coherence (ideas are expressed clearly), focus\n(the main ideas of the document are shared while avoiding superﬂuous details), and overall.\nThe results are presented in Table 3. Interestingly, the summaries from Transformer-SM are consis-\ntently preferred by humans across all 4 evaluations dimensions compared to those from the DCA and\nPGen+Coverage models, indicating that the Transformer-SM’s lower ROUGE scores observed in\nTable 2 are not necessarily correlated with human judgments of quality.\nTable 4: ROUGE-L precision (R-L P), recall (R-\nL R), and F1 (R-L F1) scores computed between\ngenerated summaries and input CNN/DailyMail\narticles after removing stop words\nModel Name R-L P R-L R L\nPGen [22] 95.22 8.22 53.69\nPGen+Cov [22] 99.83 10.74 59.75\nBottom-Up [6] 98.93 9.35 55.25\nrnn-ext + RL [2] 99.05 12.77 77.44\nDCA [1] 97.31 8.24 51.01\nTransformer-LM 96.66 9.95 43.40\nTransformer-SM 97.16 9.78 42.42\nGold Summary 79.88 11.13 52.02\nTable 5: Ablation study of training schedules\non CNN/DailyMail. (PT) Model initialized with\npretrained weights; (DAT) Model uses domain-\nadaptive training; (ETT) trained on end task.\nModel R-1 R-2 R-L\nT-LM (ETT) 36.82 16.04 34.03\nT-LM (DAT+ETT) 38.00 17.13 35.20\nT-LM (PT+ETT) 38.20 17.39 35.40\nT-LM (PT+DAT+ETT) 39.01 17.87 36.17\nT-SM (ETT) 37.81 16.82 34.87\nT-SM (DAT+ETT) 38.34 17.34 35.44\nT-SM (PT+ETT) 38.71 17.53 35.90\nT-SM (PT+DAT+ETT) 38.33 17.79 35.56\nEfﬁciency Due to the large improvements over the baseline models in the human evaluation cate-\ngories of non-redundancy and focus, and the generally shorter summaries produced by Transformer-\nSM, we investigate whether Transformer-SM is able to more efﬁciently express key ideas of the\ndocument. To evaluate the efﬁciency of each model, we remove non-content words from the model-\ngenerated summaries and articles, and compute the ROUGE score between them. This measure\nserves as a proxy for the rate at which ideas expressed in the summary can be found in the document.\nWe report these results in Table 4 and observe that Transformer-SM reports comparable ROUGE-\nL recall scores to other baselines when evaluated with respect to the article, despite producing\nsummaries that, on average, 27% shorter. Meanwhile, ROUGE-L precision is also very similar to the\nbaseline models, indicating that the summaries of all models indicate a similar degree of information\nrelevance.4 Combined with the results from Table 3, we conjecture that Transformer-SM is able\n4The high precision scores across all models conﬁrm that despite being conceived as abstractive generators,\nthese models display highly extractive behavior.\n6\nto more efﬁciently express key ideas from the document. While other models may be producing\nlonger summaries that yield higher ROUGE performance (Table 2), the additional tokens may reﬂect\nredundant and unsalient information, which human evaluators penalize.\nAnalysis of domain-adaptive training and source embeddings Our approach involved two strate-\ngies for efﬁciently using transformer language models for abstractive summarization: domain-adaptive\ntraining and source embeddings. To assess their individual impact, we evaluate multiple training\nschedule permutations (e.g., various combinations of using pretrained representations from the GPT\nmodel and using domain-adaptive training), as well as the impact of source embeddings. Our results\nin Table 5 yield multiple interesting conclusions. First, in general, domain-adaptive training (+DAT in\nTable 5) provides a clear improvement over training directly on the end task, irrespective of whether\npretrained representations are used. Similarly, using source embeddings (T-SM in Table 5) provides a\nrepeated improvement over the T-LM ablation. Surprisingly, when pretrained initializations, DAT,\nand source embeddings are used in tandem, performance drops slightly compared to not using DAT\nor not using source embeddings. We note, however, that this observation does not hold true for the\nXSum dataset (§5.2), and conjecture that the extractive nature of the CNN/DailyMail dataset may\nmake these approaches have redundant effects in this setting.\n5.2 XSum\nA study on the quality of abstractive summaries is best performed on the XSum dataset [16], which is\nspeciﬁcally designed with gold summaries that are less extractive than the other datasets (Table 1).\nBaselines We report the performance of Transformer-SM on this dataset in comparison to baselines\noriginally reported in Narayan et al. [16]: an attention-based sequence to sequence model (AttnS2S),\na pointer-generator model capable of generating words and copying directly from the input (PGen), a\nsecond pointer-generator model with a coverage mechanism to prevent repetition (PGen+Cov), and\nthe top performing variant of the topic aware convolutional sequence to sequence model (T-ConvS2S),\nin which the encoder and decoder are provided with word topic and document topic distributions\nobtained using LDA as additional inputs. Our ﬁnal baseline is the Multi-Level Memory Network\n(MMN) [9], which applies attention over multiple memory layers for varying levels of abstraction.\nTable 6: Comparison results on the XSum test\nset using the F1 variants of ROUGE\nModel R-1 R-2 R-L\nAttnS2S [16] 28.42 8.77 22.48\nPGen [16] 29.70 9.21 23.24\nPGen+Cov [16] 28.10 8.02 21.72\nT-ConvS2S [16] 31.89 11.54 25.75\nMMN [9] 32.00 12.10 26.00\nTransformer-LM 36.03 14.57 29.20\nTransformer-SM 36.73 14.93 29.66\nResults We report our results in Table 6. Our mod-\nels signiﬁcantly outperform the comparison base-\nlines across all three variants of the ROUGE metric.\nInterestingly, the Transformer-SM achieves notice-\nable improvement over the Transformer-LM model,\nsuggesting that both source embeddings and domain\nadaptive training are helpful when target summaries\nare more abstractive. Examples of model-generated\nsummaries from the XSum dataset illustrate the im-\nprovement over baselines qualitatively in Table 7. In\nsupport of results presented earlier, the model pro-\nduces abstractive summaries that provide focused\ninformation about the main points of the articles.\n5.3 Newsroom\nFinally, we report the performance of our model on the Newsroom dataset [ 7], the largest of the\nevaluation datasets. Due to the large cost of training, only the Transformer-SM model was evaluated.\nBaselines As baselines, we report the performance of models released by the authors of the\nNewsroom dataset [7]. These models included an attentive encoder-decoder (Attn-S2S) and a pointer-\ngenerator network (PGen). We also compared against C10110 [23], a complex encoder-decoder that\nuses LSTMs, encoder attention, intra-decoder attention, and pointer-generation to produce summaries.\nWe also compare against the Multi-Level Memory Network (MMN) [ 9] mentioned earlier. The\nauthors of this baseline only evaluated on the abstractive subset of the Newsroom dataset.\n7\nTable 7: XSum samples for the baseline T-ConvS2S model and Transformer-SM along with the gold\nsummary. Articles are shortened for brevity. Capitalization was manually added for ease of reading.\nSource Source Text\nArticle snippet\nOfﬁcials said the attack happened at the Europa shopping centre in the capital Minsk. ... Police\nlater arrested the 18-year-old suspect. ... \"He cut one woman with the chainsaw and hit her with\na hammer. She died. He also attacked others.\" The injured woman was taken to a local hospital.\nThe attacker had brought the chainsaw and the axe to the shopping centre ...\nT-ConvS2S A man has been arrested on suspicion of attempted murder by after a knife attack on a shopping\ncentre in central London.\nTransformer-SM A teenage girl has been killed by a chainsaw attack at a shopping centre in central Russia, police\nsay.\nGold A young man has attacked people with a chainsaw and an axe at a shopping centre in Belarus,\nkilling one woman and injuring another.\nArticle snippet\nThe 34-year-old Sweden striker’s contract with the french champions expires in the summer, and\nhe has been linked with Manchester United, LA Galaxy and AC Milan. ... PSG said Ibrahimovic\nleaves as \"the greatest striker and one of the very best players in the club’s history\" . ...\nT-ConvS2S Paris St-Germain have completed the signing of Zlatan Ibrahimovic from Paris St-Germain for\nan undisclosed fee.\nTransformer-SM Zlatan Ibrahimovic says he will leave Paris St-Germain at the end of the season to return to the\nclub.\nGold Zlatan Ibrahimovic will leave Paris St-Germain at the end of the season.\nArticle snippet\n... The animal was taken from Lathom pets and aquatics in Ormskirk on Tuesday afternoon,\nLancashire police said. The shop’s owner said CCTV showed a man taking the tortoise - which\nneeds calcium supplements - out of the tank. ...\nT-ConvS2S A puppy’s pet shop has been stolen from a shop in Lancashire.\nTransformer-SM A tortoise has been stolen from a pet shop.\nGold A baby tortoise has been stolen from a pet shop in Lancashire.\nTable 9: ROUGE F1 results on validation subsets and full validation set for Newsroom\nModel Name Extractive Mixed Abstractive Newsroom-D\nR-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nPGen [7] 39.11 27.95 36.17 25.48 11.04 21.06 14.66 2.26 11.44 26.27 13.55 22.72\nMMN [9] - - - - - - 17.5 4.7 14.2 - - -\nTransformer-SM64.69 59.72 63.56 35.75 18.83 30.63 22.44 7.39 18.75 41.42 29.51 38.26\nTable 8: Comparison results on the News-\nroom test set using ROUGE F1\nModel R-1 R-2 R-L\nAttn-S2S [16] 5.88 0.39 5.32\nPGen [16] 26.02 13.25 22.43\nC10110 [23] 39.36 27.86 36.35\nT-SM 40.87 28.59 37.62\nResults We report our results with ROUGE-style au-\ntomatic metrics in Table 8, showing that Transformer-\nSM outperforms the previous best model, C10110 [23],\nacross all metrics. Interestingly, our model achieves\nits highest performance increase over baseline models\non Rouge-L, the metric usually considered as being\nmost strongly correlated with strong summaries. Fur-\nthermore, an analysis of different validation subsets\nof the Newsroom dataset in Table 9 (split on the level\nof extractiveness of the gold summaries) shows that\nTransformer-SM performs better than baselines ap-\nproaches on all varieties of summary types.\n6 Related Work\nAbstractive Summarization There has been a large variety of work exploring different methods\nfor neural abstractive document summarization. Attention mechanisms have been shown to improve\na variety of models [14; 25; 3], and is one of the motivating factors for this work. Pointer generator\nnetworks introduced in See et al. [22] have been shown to increase summary veracity, and inspired\n8\nthe tangential usage of copy mechanisms in Transformers for document summarization Gehrmann\net al. [6]. Other works have also explored the use of reinforcement learning to directly optimize\nsummarization models on the ROUGE metric [17; 18; 2].\nContextualized Representations Our approach is also relevant to recent work on contextualized\nlanguage representations that are pretrained on large-scale language corpora. These representations\ncan then be simply integrated or ﬁne-tuned for improved performance on many downstream tasks\n[27]. SSL [4], CoVe [13], and ELMo [19] all learned contextualized representations through training\nRNN language models and encoder-decoders. Follow-up work extended these ideas, but replaced the\nRNN with a deep transformer [20] that was trained to learn language patterns on a large story dataset.\nBERT [5] more clearly extended the idea of using Transformers for language modeling by making the\nencoded representations bidirectional and adding two new loss functions: a masked token loss and\nnext sentence prediction loss for more accurate discourse representations. More recently, GPT2 [21]\nexpanded the scale of pretrained language models, and showed promising results on zero-shot tasks.\n7 Conclusion\nIn this work, we introduce two approaches for effectively adapting pretrained language model repre-\nsentations to abstractive summarization: domain-adaptive training, and source embeddings. We eval-\nuate the effect of both approaches across three abstractive summarization testbeds: CNN/DailyMail,\nXSum, and Newsroom, and achieve state of the art ROUGE-L results on two of them, while showing\nsuperior human evaluation performance on the third. In the process, we show that the ROUGE-L\nmetric often used for abstractive summarization evaluation is quite sensitive to summary length,\nallowing it to be exploitable by approaches that use heuristics to control summary length.\nReferences\n[1] Asli Celikyilmaz, Antoine Bosselut, Xiaodong He, and Yejin Choi. 2018. Deep communicating\nagents for abstractive summarization. In NAACL.\n[2] Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected\nsentence rewriting. In ACL.\n[3] Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and\nwords. arXiv preprint arXiv:1603.07252.\n[4] Andrew M. Dai and Quoc V . Le. 2015. Semi-supervised sequence learning. In NIPS.\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language understanding. In NAACL.\n[6] Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-up abstractive\nsummarization. In EMNLP.\n[7] Max Grusky, Mor Naaman, and Yoav Artzi. 2019. Newsroom: A dataset of 1.3 million\nsummaries with diverse extractive strategies. In NAACL.\n[8] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances\nin Neural Information Processing Systems, pages 1693–1701.\n[9] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2018. Abstractive summarization of\nreddit posts with multi-level memory networks. arXiv preprint arXiv:1811.00783.\n[10] Chin-Yew Lin. 2004. Looking for a few good metrics: Automatic summarization evaluation-how\nmany samples are enough? In NTCIR.\n[11] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Text Summa-\nrization Branches Out.\n[12] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by summarizing long sequences. In ICLR.\n9\n[13] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in\ntranslation: Contextualized word vectors. In NIPS.\n[14] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural\nnetwork based sequence model for extractive summarization of documents. In Thirty-First\nAAAI Conference on Artiﬁcial Intelligence.\n[15] Ramesh Nallapati, Bowen Zhou, Cícero Nogueira dos Santos, Çaglar Gülçehre, and Bing Xiang.\n2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In CoNLL.\n[16] Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don’t give me the details, just the\nsummary! topic-aware convolutional neural networks for extreme summarization. In EMNLP.\n[17] Ramakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with\nsaliency and entailment. In ACL.\n[18] Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for\nabstractive summarization. In ICLR.\n[19] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.\n[20] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training. 2018. URL https://s3-us-west-2. amazonaws.\ncom/openai-assets/research-covers/language-unsupervised/language_understanding_paper.\npdf.\n[21] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask learners.\n[22] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization\nwith pointer-generator networks. In ACL.\n[23] Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, and Chandan K Reddy. 2018. Neural abstrac-\ntive text summarization with sequence-to-sequence models. arXiv preprint arXiv:1812.02303.\n[24] Simeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova. 2019. How to compare summarizers\nwithout target length? pitfalls, solutions and re-examination of the neural summarization\nliterature. In Proceedings of NAACL 2019 Workshop on Optimizing and Evaluating Neural\nLanguage Generation (NeuralGen).\n[25] Jiwei Tan, Xiaojun Wan, and Jianguo Xiao. 2017. Abstractive document summarization with\na graph-based attentional neural model. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1171–\n1181.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998–6008.\n[27] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\n2018. Glue: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint 1804.07461.\n[28] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan R. Salakhutdinov, Raquel Urtasun, Antonio\nTorralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books. 2015 IEEE International Conference on\nComputer Vision (ICCV), pages 19–27.\n10\nA Reproducibility\nWe provide additional details relevant to the experimental environment here.\nData Sources The CNN/Daily Mail dataset [8] consists of articles from CNN and Daily Mail.5 Each\narticle is associated with several descriptive bullet point highlights. Similar to previous work [15], we\nconcatenate the highlights to create a target summary for each article in the dataset. The Newsroom\ndataset [7] consists of ∼1.2M article summary pairs scraped from the Internet Archive.6 The articles\ncome from a set of 38 publishers and cover diverse topics. Finally, the Extreme Summarization\n(XSum) dataset [16] consists of ∼230k article summary pairs taken from the BBC. 7 Each summary\nis a single sentence long and is professionally written (usually by the author). For all datasets, we\nuse the splits deﬁned in the original works that proposed them. Because the datasets are too large to\nprovide as supplementary material, we provide pointers in the source code README for acquiring\nthem.\nHyperparameters Details about important hyperparameters can be found in Section 4 of the paper.\nAdditional training hyperparameters can be found as the default parameters in the training script of the\nsource code.8 Most hyperparameter values selected were the same ones suggested by previous work\non transformer language models [20]. The only hyperparameter we varied that is not measured as an\nablation (i.e., training schedules and whether to include source embeddings) was the initialization\nof source embeddings (if they were included). For this hyperparameter, we explored three different\ninitializations: 1) initializing both source embeddings with zero vectors, 2) initializing both source\nembeddings with values sampled from the standard normal distribution, and 3) initializing both source\nembeddings with values sampled from a normal distribution with mean 0 and standard deviation\nequal to half the norm of the average norm among pretrained embeddings from the GPT language\nmodel. This last one is the one we report in all experiments.\nExperimental Process Each experiment was run as follows for any given model and dataset. First,\nwe trained the model as described in the paper. After every 1000 minibatches, we compute ROUGE\nfor a random, but persistent 500 example subset of the validation set. When the ROUGE-1 score of\nthe model stopped rising, we used the previous checkpoint as a model to generate summaries for all\narticles in the test set. We used beam search to decode summaries using a beam with of 3. We ran\nexactly one evaluation run for each result we include in our paper.\n5https://www.cnn.com; https://www.dailymail.co.uk\n6https://archive.org/\n7https://www.bbc.com/\n8https://github.com/Andrew03/transformer-abstractive-summarization\n11",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9590722322463989
    },
    {
      "name": "Transformer",
      "score": 0.8636003732681274
    },
    {
      "name": "Computer science",
      "score": 0.8423616886138916
    },
    {
      "name": "Natural language processing",
      "score": 0.6205102205276489
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5650083422660828
    },
    {
      "name": "Language model",
      "score": 0.5254423022270203
    },
    {
      "name": "Domain adaptation",
      "score": 0.5068801045417786
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4479852318763733
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 39
}