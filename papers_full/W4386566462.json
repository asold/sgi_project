{
  "title": "ViDeBERTa: A powerful pre-trained language model for Vietnamese",
  "url": "https://openalex.org/W4386566462",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5078542204",
      "name": "Cong Dao Tran",
      "affiliations": [
        "University of California, San Diego",
        "University of Massachusetts Amherst",
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A5028544296",
      "name": "Nhut Huy Pham",
      "affiliations": [
        "University of California, San Diego",
        "University of Massachusetts Amherst",
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A5101848312",
      "name": "Anh Tuan Nguyen",
      "affiliations": [
        "University of California, San Diego",
        "University of Massachusetts Amherst",
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A5073178563",
      "name": "Truong Son Hy",
      "affiliations": [
        "University of California, San Diego",
        "University of Massachusetts Amherst",
        "FPT University"
      ]
    },
    {
      "id": "https://openalex.org/A5110616429",
      "name": "Tu Vu",
      "affiliations": [
        "University of California, San Diego",
        "University of Massachusetts Amherst",
        "FPT University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3153642904",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3037205571",
    "https://openalex.org/W4286970289",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4313015652",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4287393888",
    "https://openalex.org/W2171278097",
    "https://openalex.org/W4280557512",
    "https://openalex.org/W3121694563",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3169653581",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3113790969",
    "https://openalex.org/W2086511124",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2115758952",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3098637735"
  ],
  "abstract": "This paper presents ViDeBERTa, a new pre-trained monolingual language model for Vietnamese, with three versions - ViDeBERTa_xsmall, ViDeBERTa_base, and ViDeBERTa_large, which are pre-trained on a large-scale corpus of high-quality and diverse Vietnamese texts using DeBERTa architecture. Although many successful pre-trained language models based on Transformer have been widely proposed for the English language, there are still few pre-trained models for Vietnamese, a low-resource language, that perform good results on downstream tasks, especially Question answering. We fine-tune and evaluate our model on three important natural language downstream tasks, Part-of-speech tagging, Named-entity recognition, and Question answering. The empirical results demonstrate that ViDeBERTa with far fewer parameters surpasses the previous state-of-the-art models on multiple Vietnamese-specific natural language understanding tasks. Notably, ViDeBERTa_base with 86M parameters, which is only about 23% of PhoBERT_large with 370M parameters, still performs the same or better results than the previous state-of-the-art model. Our ViDeBERTa models are available at: https://github.com/HySonLab/ViDeBERTa.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1071–1078\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nViDeBERTa: A powerful pre-trained language model for Vietnamese\nCong Dao Tran∗\nFPT Software AI Center\ndaotc2@fsoft.com.vn\nNhut Huy Pham∗\nFPT Software AI Center\nhuypn10@fsoft.com.vn\nAnh Nguyen\nMicrosoft\nanhnguyen@microsoft.com\nTruong Son Hy†\nUniversity of California San Diego\ntshy@ucsd.edu\nTu Vu\nUniversity of Massachusetts Amherst\ntuvu@cs.umass.edu\nAbstract\nThis paper presents ViDeBERTa, a new\npre-trained monolingual language model\nfor Vietnamese, with three versions -\nViDeBERTaxsmall, ViDeBERTabase, and\nViDeBERTalarge, which are pre-trained on a\nlarge-scale corpus of high-quality and diverse\nVietnamese texts using DeBERTa architec-\nture. Although many successful pre-trained\nlanguage models based on Transformer have\nbeen widely proposed for the English language,\nthere are still few pre-trained models for Viet-\nnamese, a low-resource language, that perform\ngood results on downstream tasks, especially\nQuestion answering. We fine-tune and evaluate\nour model on three important natural language\ndownstream tasks, Part-of-speech tagging,\nNamed-entity recognition, and Question\nanswering. The empirical results demonstrate\nthat ViDeBERTa with far fewer parameters\nsurpasses the previous state-of-the-art models\non multiple Vietnamese-specific natural\nlanguage understanding tasks. Notably,\nViDeBERTabase with 86M parameters, which\nis only about 23% of PhoBERTlarge with\n370M parameters, still performs the same or\nbetter results than the previous state-of-the-art\nmodel. Our ViDeBERTa models are available\nat: https://github.com/HySonLab/ViDeBERTa.\n1 Introduction\nIn recent years, pre-trained language models\n(PLMs) and Transformer-based architecture mod-\nels have been essential in the advancement of\nNatural Language Processing (NLP). Large-scale\nTransformer-based pre-trained models with the ca-\npacity to derive a contextual representation of the\nlanguages in the training data include GPT (Rad-\nford et al., 2019; Brown et al., 2020), BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019), XL-\nNet (Yang et al., 2019b), ELECTRA (Clark et al.,\n2020), T5 (Raffel et al., 2020), and DeBERTa (He\n∗ ∗: Co-first authors. †: Correspondent author.\net al., 2020, 2021). Following pre-training, these\nmodels performed at the cutting edge on various\ndownstream NLP tasks (Devlin et al., 2019). The\ndevelopment of pre-trained models in other lan-\nguages, including Vietnamese (PhoBERT (Nguyen\nand Nguyen, 2020); ViBERT (Tran et al., 2020);\nViT5 (Phan et al., 2022)), and Arabic (Antoun et al.,\n2021), has been spurred on by the success of pre-\ntrained models in English. In order to enhance per-\nformance across several languages by learning both\ngeneral and language-specific representations, mul-\ntilingual pre-trained models ( XLM-R (Conneau\net al., 2020), mT5 (Xue et al., 2021), and mBART\n(Liu et al., 2020) are also being developed.\nMost recently, PhoBERT (Nguyen and Nguyen,\n2020), the first large pre-trained model for Viet-\nnamese that inherits the RoBERTa (Liu et al.,\n2019) architecture, has demonstrated the effective-\nness of the trained language model compared with\ncurrent methods modernized in four Vietnamese-\nspecific tasks, including Part of Speech Tagging\n(POS), Dependency Parsing, Named Entity Recog-\nnition (NER), and Natural Language Inference\n(NLI). Nevertheless, there are still rooms to build\nan improved pre-trained language model for Viet-\nnamese. Firstly, PhoBERT was pre-trained on a\nrelatively small Vietnamese dataset of 20GB of un-\ncompressed texts, while pre-trained language mod-\nels can be significantly improved by using more\npre-training data (Liu et al., 2019). Secondly, Ques-\ntion answering (QA) is one of the most impactful\ntasks that has mainly focused on the computational\nlinguistics and artificial intelligence research com-\nmunity within information retrieval and informa-\ntion extraction in recent years. However, there are\na few pre-trained models for Vietnamese that pro-\nduce efficient results in the QA tasks, especially\nPhoBERT (Nguyen and Nguyen, 2020) and ViT5\n(Phan et al., 2022). Last but not least, some pre-\nvious works point to DeBERTa architecture (He\net al., 2020, 2021) using several novel techniques\n1071\nthat can significantly outperform RoBERTa and im-\nprove the efficiency of model pre-training and the\nperformance of downstream tasks in some respects.\nInspired by that, we introduce an improved large-\nscale pre-trained language model, ViDeBERTa,\ntrained on CC100 Vietnamese monolingual, fol-\nlowing the architecture and pre-training methods\nof DeBERTaV3 (He et al., 2021). We comprehen-\nsively evaluate and compare our model with com-\npetitive baselines, i.e., the previous SOTA models\nPhoBERT, ViT5, and the multilingual model XLM-\nR on three Vietnamese downstream tasks, including\nPOS tagging, NER, and QA. In this work, we focus\non two main categories of QA: Machine Reading\nComprehension (MRC) and Open-domain Ques-\ntion Answering (ODQA). The experiment results\nshow the performance of our model surpasses all\nbaselines on all tasks. Our main contributions are\nsummarized as follows:\n• We present and implement ViDeBERTa\nwith three versions: ViDeBERTaxsmall,\nViDeBERTabase, and ViDeBERTalarge which\nare the improved large-scale monolingual\nlanguage models pre-trained for Vietnamese\nbased on the DeBERTa architecture and pre-\ntraining procedure.\n• We also conduct extensive experiments to ver-\nify the performance of our pre-trained models\ncompared to previous strong models in terms\nof Vietnamese language modeling. Our empir-\nical results demonstrated the state-of-the-art\n(SOTA) results on Vietnamese downstream\ntasks: POS tagging, NER, and QA, thus con-\nfirming the effectiveness of our improved pre-\ntrained language model for Vietnamese.\n• Our model, ViDeBERTa, which works with\nhuggingface and transformers, is available to\nthe public. We expect that ViDeBERTa will\nbe an effective pre-trained model for many\nNLP applications and research in Vietnamese\nand other low-resource languages.\n2 Related work\nPre-trained language models for Vietnamese.\nPhoBERT (Nguyen and Nguyen, 2020) is the first\nlarge-scale PLM for Vietnamese, which has the\nsame architecture as BERT (Devlin et al., 2019)\nand the same pre-training approach as RoBERTa\n(Liu et al., 2019) for more robust performance. This\nmodel was trained on a Vietnamese Wikipedia cor-\npus of 20GB word-level texts and produced SOTA\nresults on Vietnamese understanding tasks such as\nPOS, NER, Dependency parsing, and NLI. Follow-\ning PhoBERT, ViBERT (Tran et al., 2020) and Vi-\nELECTRA are public monolingual language mod-\nels for Vietnamese based on BERT and ELECTRA\npre-training techniques (Clark et al., 2020) that are\npre-trained on syllable-level Vietnamese textual\ndata. Recent works such as BARTpho (Tran et al.,\n2021) and ViT5 (Tran et al., 2020) are pre-trained\nfor Vietnamese text summarization.\nFine-tuning tasks.This work utilizes three Viet-\nnamese natural language understanding (NLU)\ntasks, including POS tagging, NER, and QA, for\nfine-tuning and evaluating our model’s perfor-\nmance. For POS tagging and NER, PhoBERT still\nproduces better results than ViELECTRA, PhoNLP,\nand ViT5 (Nguyen and Nguyen, 2020, 2021; Phan\net al., 2022). While early QA (V oorhees et al.,\n1999; Brill et al., 2002; Ferrucci et al., 2010) sys-\ntems were commonly complex and had many parts,\nMRC models have evolved and now suggest a sim-\npler two-stage retriever-reader framework (Chen\net al., 2017). A context retriever first selects a small\nsubset of passages where some of them contain\nthe answer to the question then a machine reader\ncan carefully review the retrieved contexts and de-\ntermine the correct answer. The tasks based on\nQA have gained much attention in recent years in\nthe Vietnamese natural language processing and\ncomputational linguistics community. However, to\nthe best of our knowledge, there is only the work\n(Van Nguyen et al., 2022) that proposes the first\nVietnamese retriever-reader QA system employing\na transformer-based model (XLM-R) evaluated on\nthe ViQuAD corpus (Nguyen et al., 2020).\n3 ViDeBERTa\n3.1 Pre-training data\nIn this work, we use a large corpus CC100\nDataset of 138GB uncompressed texts (Monolin-\ngual Datasets from Web Crawl Data) (Conneau\net al., 2020) as a pre-training dataset. This corpus\nincludes data for romanized languages and mono-\nlingual data for more than 100 languages.\nAccording to Nguyen and Nguyen (2020); Tran\net al. (2021), pre-trained language models trained\non word-level data can perform better than those\ntrained on syllable-level data for word-level Viet-\nnamese NLP tasks. As a result, we perform word\n1072\nand sentence segmentation using a Vietnamese\ntoolkit PyVi 1 on the pre-training dataset. After\nthat, we use a pre-trained SentencePiece tokenizer\nfrom DeBERTaV3 (He et al., 2021) to segment\nthese sentences with sub-word units, which have a\nvocabulary of 128K sub-word types.\n3.2 Model Architecture\nOur model, ViDeBERTa, follows the DeBERTaV3\narchitecture by He et al. (2021), which is trained us-\ning the self-supervise learning objectives of MLM\nand RTD task and a new weight-sharing Gradient-\nDisentangled Embedding Sharing (GDES) to en-\nhance the performance of the model. We present\nthree versions of our model, ViDeBERTaxsmall,\nViDeBERTabase, and ViDeBERTalarge with 22M,\n86M, and 304M backbone parameters, respectively.\nThe details of our model architecture hyper-\nparameters are listed in Table 1.\nTable 1: Statistic of our model hyper-parameters. #layer\nand #heads denote the numbers of layers and attention\nheads of ViDeBERTa model versions, respectively.\nModel #layers #heads hidden size\nViDeBERTaxsmall 6 12 768\nViDeBERTabase 12 12 768\nViDeBERTalarge 24 12 1024\n3.3 Optimization\nWe employ our model based on the DeBERTaV3\nimplementation from (He et al., 2021). We use\nAdam (Kingma and Ba, 2015) as the optimizer\nwith weight decay (Loshchilov and Hutter, 2018)\nand use a global batch size of 8,192 across 32 A100\nGPUs (80GB each) and a peak learning rate of 6e-\n4 for both ViDeBERTaxsmall and ViDeBERTabase,\nwhile peak learning rate of 3e-4 was used for\nViDeBERTalarge. We pre-train ViDeBERTaxsmall\nand ViDeBERTabase for 500k training iterations\nand ViDeBERTalarge for 250k training iterations.\n4 Experiments and Results\n4.1 POS tagging and NER\n4.1.1 Experimental setup\nFor POS tagging and NER tasks, we use standard\nbenchmarks of the VLSP POS tagging dataset 2\nand the PhoNER dataset (Truong et al., 2021).\n1https://pypi.org/project/pyvi/\n2https://vlsp.org.vn/vlsp2013/eval/ws-pos\nWe follow the procedure in Devlin et al. 2019;\nNguyen and Nguyen 2020 to fine-tune our pre-\ntrained model for POS tagging and NER tasks. In\nparticular, a linear layer for prediction is appended\non top of our model architecture (the last Trans-\nformer layer). We then use Adam (Kingma and Ba,\n2015) to optimize our model for fine-tuning with\na fixed learning rate of 1e-5 and batch size of 16\n(He et al., 2021). The final results for each task and\neach dataset are averaged and reported over five\nindependent runs with different random seeds.\nWe compare the performance of ViDeBERTa\nmodels with the solid baselines, including\nPhoBERT, XLM-R, and ViT5, for these tasks.\nHere, XLM-R is a multilingual masked language\nmodel pre-trained on 2.5 TB of CommmonCrawl\ndataset of 100 languages, which includes 137GB\nof Vietnamese texts.\n4.1.2 Main results\nModel\nPOS NER MRC\nAcc. F 1 F1\nXLM-Rbase 96.2† _ 82.0‡\nXLM-Rlarge 96.3† 93.8⋆ 87.0‡\nPhoBERTbase 96.7† 94.2⋆ 80.1\nPhoBERTlarge 96.8† 94.5⋆ 83.5\nViT5base1024−length _ 94.5⋆ _\nViT5large1024−length _ 93.8⋆ _\nViDeBERTaxsmall 96.4 93.6 81.3\nViDeBERTabase 96.8 94.5 85.7\nViDeBERTalarge 97.2 95.3 89.9\nTable 2: Test results (%) for three tasks POS tagging\n(POS for short), NER, and MRC on test sets. Note\nthat “Acc.” abbreviates the accuracy.†, ⋆, and ‡denote\nscores taken from the PhoBERT paper (Nguyen and\nNguyen, 2020), the ViT5 paper (Phan et al., 2022), and\nthe ViQuAD paper (Nguyen et al., 2020), respectively.\nTable 2 shows the obtained scores of ViDe-\nBERTa compared to the baselines with the highest\nreported results. It can be seen clearly that our\nmodel produces significantly better results than the\nbaselines and achieves new SOTA performance on\nboth POS tagging and NER tasks.\nFor POS tagging, ViDeBERTa obtains 0.9% and\n0.4% absolute higher accuracy than the large-scale\nmultilingual model XLM-R (Nguyen et al., 2020)\nand the previous SOTA model PhoBERT (Nguyen\nand Nguyen, 2020), respectively . Table 2 also\nshows our ViDeBERTaxsmall obtains 96.4% accu-\nracy that are better than the baseline XLM-Rlarge\n1073\nand ViDeBERTabase obtains 96.8% that are com-\npetitively the same as the PhoBERTlarge.\nFor NER, our ViDeBERTalarge achieves F1\nscore at 95.3% and improves 0.8% absolute\nhigher score than the previous SOTA models\nViT5base1024−length and PhoBERTlarge. Further-\nmore, ViDeBERTalarge and ViDeBERTabase pre-\nform 1.5% and 0.7% absolute higher scores than\nthe baseline XLM-Rlarge on the PhoNER corpus.\n4.2 Question Answering\n4.2.1 Experimental setup\nFor QA, we evaluate our model on two main tasks:\nMRC and ODQA. For ODQA, we propose a new\nframework ViDeBERTa-QA, that uses a BM25\n(Robertson et al., 2009) as a retriever and ViDe-\nBERTa as a text reader.\nFigure 1 depicts an overview of our ViDeBERTa\nframework for the Vietnamese Open-domain Ques-\ntion answering task. The statistics of the ViQuAD\ndataset used for the task, which is introduced by\nNguyen et al. (2020), are summarized in Table 3.\nCorpus #article #passage #question\nTrain 138 4,101 18,579\nDev 18 515 2,285\nTest 18 493 2,21\nFull 174 5,109 23,074\nTable 3: Statistics of the ViQuAD dataset for QA. “#arti-\ncle”, “#valid”, and “#test” denote the number of articles,\npassages, and questions in the ViQuAD, respectively.\nWe compare ViDeBERTa to the best model\nXLM-R (Nguyen et al., 2020) and PhoBERT 3 for\nVietnamese MRC. We also compare our frame-\nwork, ViDeBERTa-QA, to strong baselines DrQA\n(Chen et al., 2017), BERTserini (Yang et al.,\n2019a), and the first Vietnamese ODQA system\nXLMRQA (Van Nguyen et al., 2022)) that uses\nXLM-Rlarge as a reader. We use the ViQuAD cor-\npus introduced by Nguyen et al. (2020) for assess-\ning these tasks. ViQuAD is a Vietnamese corpus\nthat comprises over 23k triples and each triple in-\ncludes a question, its answer, and a passage con-\ntaining the answer.\nSimilar to POS tagging and NER, we use Adam\n(Kingma and Ba, 2015) as an optimizer with a learn-\ning rate of 2e-5 and a batch size of 16. We report\n3We carefully fine-tune PhoBERT for the MRC task fol-\nlowing the fine-tuning approach that we use for ViDeBERTa.\nthe final results as an average over five independent\nruns with different random seeds.\n4.2.2 Main results\nTable 2 presents the results obtained by ViDe-\nBERTa and two baselines XLM-R (reported by\nNguyen et al. (2020)) and PhoBERT for MRC on\nViQuAD corpus. We find that our ViDeBERTa per-\nformance outperforms both XLM-R and PhoBERT\nin terms of F1 score.\nIn particular, the previous SOTA model\nXLM-Rlarge for Vietnamese MRC obtains 87%.\nClearly, ViDeBERTa helps boost the XLM-R with\nabout 2.9% absolute improvement, obtaining a\nnew SOTA result at 89.9%. In addition, both\nversions ViDeBERTabase and ViDeBERTalarge\nalso outperform PhoBERTbase and PhoBERTlarge\nby large margins, respectively. Especially,\nViDeBERTaxsmall (22M parameters) produces\n1.2% absolute higher score than PhoBERTbase\n(135M parameters) and ViDeBERTabase (86M pa-\nrameters) produces 2.2% absolute higher score\nthan PhoBERTlarge (370M parameters) but uses\nfar fewer parameters than PhoBERT.\nFor ODQA, Table 4 shows the obtained F1\nscores for ViDeBERTa-QA and its baselines on\nthe test set. Obviously, ViDeBERTa-QA achieves\nbetter scores than the previous SOTA XLMRQA,\nBERTsini, and DrQA at the top k passages, se-\nlected by retrievers, is 10 and 20. In particular,\nViDeBERTa-QA performs 0.85% (atk= 20) and\n0.4% (at k = 10) absolute higher scores than the\nprevious SOTA system. At smallerk(= 1, 5), ViDe-\nBERTa performs better BERTserini and DrQA by a\nlarge margin; however, XLMRQA does better than\nViDeBERTa-QA.\nModel\nTopkselected passages\n1 5 10 20\nDrQA [*] 37.86 37.86 37.86 37.86\nBERTserini [*] 55.55 58.30 57.98 58.09\nXLMRQA [*] 61.83 64.99 64.49 64.49\nViDeBERTaxsmall 52.76 56.24 56. 93 57.40\nViDeBERTabase 58.55 61.37 61.89 62.43\nViDeBERTalarge 61.23 63.57 64.89 65.34\nTable 4: Test scores (F1 in %) for ODQA on ViQuAD\ncorpus with different kvalues. Note that [*] indicates\nthe results reported following Van Nguyen et al. (2022).\n4.3 Discussion\nAccording to the results on both downstream tasks\nof POS tagging and NER in Table 2, we find that\n1074\nRetriever  \n(BM25) \nReader  \n(ViDeBERTa) \nCorpus\n Top k  passages\nreader \nscore\nretriever \nscore\nAnswer : 1911\nQuestion:  Chủ tịch Hồ Chí Minh ra đi tìm\nđường cứu nước vào năm nào?\n(Which year did President Ho Chi Minh leave\nthe country to find a way to save the nation?)\nFigure 1: An overview of ViDeBERTa-QA framework for Vietnamese Open-domain Question Answering task.\nViDeBERTaxsmall (86M) with fewer parameters\n(i.e. only about 15% of XLM-Rlarge 560M and\n25% of PhoBERTlarge 370M) but still performs\nslightly better than XLM-Rlarge and competitively\nthe same as the previous SOTA PhoBERTlarge.\nOne possible reason is that our model inherits\nthe robustness of DeBERTaV3 architecture and\npre-training techniques, which are demonstrated\nsuperior performance by He et al. (2020, 2021).\nMoreover, using more high-quality pre-training\ndata (138GB) can help ViDeBERTa significantly\nimprove its performance compared to PhoBERT\n(using 20GB).\nFor Vietnamese QA, the results on the MRC task\nshow that ViDeBERTa outperforms PhoBERT by\na large margin. It is worth noting that PhoBERT\nset a maximum length of 256 subword tokens for\nboth versions while ViDeBERTa set a larger one of\n512. As a result, our models are more scalable than\nPhoBERT for long contexts. The results obtained\nby ViDeBERTa-QA on ODQA also suggest that\nour framework achieves the best performance with\nlarge top kpassages selected by the retriever (i.e.\nk= 10,20).\n5 Conclusion\nIn this paper, we have introduced ViDeBERTa, a\nnew pre-trained large-scale monolingual language\nmodel for Vietnamese. We demonstrate the effec-\ntiveness of our ViDeBERTa by showing that ViDe-\nBERTa with fewer parameters performs better than\nthe recent strong pre-trained language models as\nXLM-R, PhoBERT, and ViT5, and achieves SOTA\nperformances for three downstream Vietnamese\nlanguage understanding tasks, including POS tag-\nging, NER, and especially QA. We hope that our\npublic ViDeBERTa model will boost ongoing NLP\nresearch and applications for Vietnamese and other\nlow-resource languages.\nLimitations\nWhile we have shown that ViDeBERTa can achieve\nstate-of-the-art performance on a variety of NLP\ntasks for Vietnamese, we believe that more analyses\nand ablations are required to better understand what\nfacets of ViDeBERTa contributed to its success and\nwhat knowledge of Vietnamese that ViDeBERTa\ncaptures. We leave these further explorations to\nfuture work.\nReferences\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021.\nAragpt2: Pre-trained transformer for arabic language\ngeneration. In Proceedings of the Sixth Arabic Natu-\nral Language Processing Workshop, pages 196–207.\nEric Brill, Susan Dumais, and Michele Banko. 2002. An\nanalysis of the askmsr question-answering system. In\nProceedings of the 2002 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2002), pages 257–264.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\n1075\nciation for Computational Linguistics, pages 8440–\n8451.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nDavid Ferrucci, Eric Brown, Jennifer Chu-Carroll,\nJames Fan, David Gondek, Aditya A Kalyanpur,\nAdam Lally, J William Murdock, Eric Nyberg, John\nPrager, et al. 2010. Building watson: An overview of\nthe deepqa project. AI magazine, 31(3):59–79.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nDat Quoc Nguyen and Anh-Tuan Nguyen. 2020.\nPhobert: Pre-trained language models for vietnamese.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 1037–1042.\nKiet Nguyen, Vu Nguyen, Anh Nguyen, and Ngan\nNguyen. 2020. A vietnamese dataset for evaluating\nmachine reading comprehension. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 2595–2605.\nLinh The Nguyen and Dat Quoc Nguyen. 2021. Phonlp:\nA joint multi-task learning model for vietnamese part-\nof-speech tagging, named entity recognition and de-\npendency parsing. arXiv preprint arXiv:2101.01476.\nLong Phan, Hieu Tran, Hieu Nguyen, and Trieu H\nTrinh. 2022. Vit5: Pretrained text-to-text transformer\nfor vietnamese language generation. arXiv preprint\narXiv:2205.06457.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nNguyen Luong Tran, Duong Minh Le, and Dat Quoc\nNguyen. 2021. Bartpho: Pre-trained sequence-to-\nsequence models for vietnamese. arXiv preprint\narXiv:2109.09701.\nThi Oanh Tran, Phuong Le Hong, et al. 2020. Im-\nproving sequence tagging for vietnamese text using\ntransformer-based neural models. In Proceedings\nof the 34th Pacific Asia Conference on Language,\nInformation and Computation, pages 13–20.\nThinh Hung Truong, Mai Hoang Dao, and Dat Quoc\nNguyen. 2021. Covid-19 named entity recognition\nfor vietnamese. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2146–2153.\nKiet Van Nguyen, Phong Nguyen-Thuan Do, Nhat Duy\nNguyen, Tin Van Huynh, Anh Gia-Tuan Nguyen,\nand Ngan Luu-Thuy Nguyen. 2022. Xlmrqa: Open-\ndomain question answering on vietnamese wikipedia-\nbased textual knowledge source. arXiv preprint\narXiv:2204.07002.\nEllen M V oorhees et al. 1999. The trec-8 question\nanswering track report. In Trec, volume 99, pages\n77–82.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n72–77.\n1076\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019b.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\n1077\nA Background of DeBERTa\nDeBERTa enhances BERT with disentangled at-\ntention and a more powerful mask decoder. The\ndisentangled attention mechanism is distinct from\nprior methods in that it uses two distinct vectors to\nrepresent each input word: one for the content and\none for the location. The words’ attention weights\nare calculated using disentangled matrices based on\nboth their relative placements and contents. Simi-\nlar to BERT, DeBERTa has been pre-trained using\nmasked language modeling. The disentangled at-\ntention process already accounts for the relative\nlocations and contents of the context words but not\nfor their absolute positions, which are usually cru-\ncial for prediction. DeBERTa improves MLM by\nutilizing a better mask decoder at the MLM decod-\ning layer and absolute position information of the\ncontext words.\nA.1 Masked Language model\nLarge-scale Transformer-based PLMs are often pre-\ntrained using a self-supervision aim called Masked\nLanguage Model (MLM) (Devlin et al., 2019) to\nlearn contextual word representations in enormous\nvolumes of text. In further detail, we corrupt a\ngiven sequence X = {xi}into ˜X by randomly\nmasking 15% of its tokens and train a language\nmodel parameterized by θto reconstruct X by an-\nticipating the masked tokens ˜xconditioned on ˜X:\nmax\nθ\nlog pθ(X|˜X) = max\nθ\n∑\ni∈C\nlog pθ( ˜xi = xi|˜X),\n(1)\nwhere Cis the sequence’s index set for the masked\ntokens. The authors of BERT suggest keeping 10%\nof the masked tokens unchanged, replacing another\n10% with tokens chosen at random, and replacing\nthe remaining tokens with the [MASK] token.\nA.2 Replaced token detection\nLike ELECTRA, which was trained with two trans-\nformer encoders in GAN style, DeBERTaV3 (He\net al., 2021) improves DeBERTa by using the train-\ning loss in the generator is MLM and discriminator\nis Replaced Token Detection (RTD). The loss func-\ntion of the generator can be written as follows:\nLMLM = E\n(\n−\n∑\ni∈C\nlog pθG(˜xi,G = xi|˜XG)\n)\n,\n(2)\nwhere θG and ˜XG are the parameter and the in-\nput of the generator by masking 15% tokens in X,\nrespectively.\nThe discriminator’s input sequence is con-\nstructed by replacing masked tokens with new to-\nkens sampled according to the generator’s output\nprobability:\n˜xi,D =\n{\n˜xi ∼pθG(˜xi,G = xi|˜XG), i ∈C\nxi, i / ∈C (3)\nThe loss function of the discriminator is written as\nfollows:\nLRTD = E\n(\n−\n∑\ni\nlog pθG(1 (˜xi,D = xi)|˜XD)\n)\n,\n(4)\nwhere θD is the parameter of the discriminator,1 (·)\nis the indicator function, and ˜XD is the input to\nthe discriminator constructed by Equation 4. Then\nLMLM and LRTD are optimized jointly by the final\nloss L= LMLM + λLRTD, where λis the weight\nof the discriminator loss.\nBesides using the RTD training loss like ELEC-\nTRA (Clark et al., 2020), DeBERTaV3 improves\nDeBERTa by using a new weight-sharing method\ncalled Gradient-Disentanggled Embedding Sharing\n(GDES) (He et al., 2021). The experimental results\nconducted by He et al. indicate that GDES is an ef-\nfective weight-sharing method for language model\npre-trained with MLM and RTD tasks.\n1078",
  "topic": "Vietnamese",
  "concepts": [
    {
      "name": "Vietnamese",
      "score": 0.9555044770240784
    },
    {
      "name": "Computer science",
      "score": 0.873019814491272
    },
    {
      "name": "Question answering",
      "score": 0.7311781644821167
    },
    {
      "name": "Natural language processing",
      "score": 0.7162572145462036
    },
    {
      "name": "Language model",
      "score": 0.7118876576423645
    },
    {
      "name": "Transformer",
      "score": 0.6700592637062073
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6360614895820618
    },
    {
      "name": "Natural language",
      "score": 0.5395402908325195
    },
    {
      "name": "Natural language understanding",
      "score": 0.5207431316375732
    },
    {
      "name": "Speech recognition",
      "score": 0.32271772623062134
    },
    {
      "name": "Linguistics",
      "score": 0.1724456250667572
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I24603500",
      "name": "University of Massachusetts Amherst",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I109689652",
      "name": "FPT University",
      "country": "VN"
    }
  ],
  "cited_by": 6
}