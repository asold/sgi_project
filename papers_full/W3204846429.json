{
    "title": "Revisiting Self-training for Few-shot Learning of Language Model",
    "url": "https://openalex.org/W3204846429",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2099591825",
            "name": "Yi-Ming Chen",
            "affiliations": [
                "Southern University of Science and Technology",
                "National University of Singapore",
                "Chinese University of Hong Kong, Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2009526818",
            "name": "Yan Zhang",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2096971521",
            "name": "Chen Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2792984118",
            "name": "Grandee Lee",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2067966727",
            "name": "Ran Cheng",
            "affiliations": [
                "Chinese University of Hong Kong, Shenzhen",
                "National University of Singapore",
                "Southern University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2096728398",
            "name": "Haizhou Li",
            "affiliations": [
                "National University of Singapore"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2976223659",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W2432717477",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2983722336",
        "https://openalex.org/W2014902591",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W2964316912",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W3115295967",
        "https://openalex.org/W2963341924",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2048679005",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W2114524997",
        "https://openalex.org/W3092358983",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2804047946",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3001197829",
        "https://openalex.org/W3035003500",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W2163568299",
        "https://openalex.org/W4295727797",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W3099403624",
        "https://openalex.org/W2971296908",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2101210369",
        "https://openalex.org/W4287614078",
        "https://openalex.org/W2968250601",
        "https://openalex.org/W2163455955",
        "https://openalex.org/W2951775809",
        "https://openalex.org/W2963777632",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2136504847",
        "https://openalex.org/W2604763608",
        "https://openalex.org/W2160660844",
        "https://openalex.org/W3120044914",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3168921237",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3173188814",
        "https://openalex.org/W3085177480"
    ],
    "abstract": "As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model. The question is how to effectively make use of such data. In this work, we revisit the self-training technique for language model fine-tuning and present a state-of-the-art prompt-based few-shot learner, SFLM. Given two views of a text sample via weak and strong augmentation techniques, SFLM generates a pseudo label on the weakly augmented version. Then, the model predicts the same pseudo label when fine-tuned with the strongly augmented version. This simple approach is shown to outperform other state-of-the-art supervised and semi-supervised counterparts on six sentence classification and six sentence-pair classification benchmarking tasks. In addition, SFLM only relies on a few in-domain unlabeled data. We conduct a comprehensive analysis to demonstrate the robustness of our proposed approach under various settings, including augmentation techniques, model scale, and few-shot knowledge transfer across tasks.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9125–9135\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n9125\nRevisiting Self-Training for Few-Shot Learning of Language Model\nYiming Chen†,‡ Yan Zhang† Chen Zhang† Grandee Lee†\nRan Cheng‡,∗ Haizhou Li†,⋆,⋆⋆\n†National University of Singapore ‡Southern University of Science and Technology\n⋆The Chinese University of Hong Kong (Shenzhen) ⋆⋆Kriston AI Lab, China\n{yiming.chen,chen_zhang,grandee.lee}@u.nus.edu,\n{haizhou.li,eleyanz}@nus.edu.sg,\nranchengcn@gmail.com\nAbstract\nAs unlabeled data carry rich task-relevant in-\nformation, they are proven useful for few-\nshot learning of language model. The ques-\ntion is how to effectively make use of such\ndata. In this work, we revisit the self-training\ntechnique for language model ﬁne-tuning and\npresent a state-of-the-art prompt-based few-\nshot learner, SFLM. Given two views of a\ntext sample via weak and strong augmentation\ntechniques, SFLM generates a pseudo label\non the weakly augmented version. Then, the\nmodel predicts the same pseudo label when\nﬁne-tuned with the strongly augmented ver-\nsion. This simple approach is shown to out-\nperform other state-of-the-art supervised and\nsemi-supervised counterparts on six sentence\nclassiﬁcation and six sentence-pair classiﬁca-\ntion benchmarking tasks. In addition, SFLM\nonly relies on a few in-domain unlabeled\ndata. We conduct a comprehensive analysis to\ndemonstrate the robustness of our proposed ap-\nproach under various settings, including aug-\nmentation techniques, model scale, and few-\nshot knowledge transfer across tasks. 1\n1 Introduction\nPre-trained language models (Devlin et al., 2019;\nLiu et al., 2019; Radford et al., 2019; Yang et al.,\n2019; Lan et al., 2020; Raffel et al., 2020; Clark\net al., 2020) have set new state-of-the-art perfor-\nmance in many downstream NLP tasks. However,\nsuch performance often relies on large-scale high-\nquality supervision. Unfortunately, labeled data are\nnot always available in practice.\nRecently, Brown et al. (2020) study how to fa-\ncilitate the few-shot learning of language models\nvia the GPT-3 model. It achieves remarkable per-\nformance on many NLP datasets without any gradi-\nent updates, by incorporating task-speciﬁc prompts\n∗ Corresponding author.\n1Our code is publicly available at https://github.\ncom/MatthewCYM/SFLM\ninto the text and reformulating the task as language\nmodeling problems. However, GPT-3 has 175B\nparameters, that has a footprint too large for many\nreal-world applications. Gao et al. (2020) applies\nthe concept of prompt strategies in GPT-3 to small-\nfootprint language models, such as BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019). After\nﬁne-tuned on a few annotated samples, the small-\nfootprint models exhibit comparable performance\nto that of the large-footprint GPT-3 model. How-\never, the performance of these models is still lag-\nging behind those under supervised learning, which\nhave a much smaller footprint. Intuitively, unla-\nbeled data also carry rich information of down-\nstream tasks and are more available than labelled\ndata. In this paper, we focus on the few-shot learn-\ning of language model with a small amount of la-\nbeled and unlabeled data.\nSemi-supervised learning beneﬁts from partially\nlabeled datasets. A common implementation of\nsemi-supervised learning is self-training, which\nleverages supervision signals offered by labeled\ndata to create pseudo-labels for unlabeled data.\nThese pseudo labels serve as additional supervision\nto reﬁne the models (Yarowsky, 1995; Blum and\nMitchell, 1998; Zhu, 2005; Qiu et al., 2019; Zoph\net al., 2020). Recent works (Schick and Schütze,\n2020, 2021) apply self-training to language model\nfew-shot learning in an iterative manner whereby\nmultiple generations of models are trained on data\npseudo-labeled by the ensemble of previous genera-\ntions. However, the amount of in-domain unlabeled\ndata required by these methods is quite large, that\nlimits the scope of the applications, especially for\nlow-resource downstream tasks. Du et al. (2020)\ntry to retrieve more task-relevant unlabeled data\nfrom open-domain corpus, but the method depends\non a quality sentence encoder.\nTo better address the above issue, we re-\nvisit the Self-training techniques and introduce\na data-efﬁcient Few-shot learner of Language\n9126\nModel (SFLM). Inspired by recent advances in\nsemi-supervised representation learning for im-\nages (Sohn et al., 2020), SFLM combines pseudo-\nlabeling with consistency regularization.\nNext, we brieﬂy describe the workﬂow. Given\neach unlabeled sentence, we construct two views\nthrough weak augmentation (random dropout) and\nstrong augmentation (token masking), respectively.\nThe weakly-augmented view is ﬁrst passed to a\nprompt-based language model (Gao et al., 2020)\nto derive the pseudo-label, while the strongly-\naugmented view is passed through the model to\npredict the probability distribution over classes,\nwhich is compared to the pseudo-label to derive\na cross-entropy loss. This learning procedure en-\ncourages the model to capture the information that\nis almost outside of the data distribution, leading\nto effective utilization of data.\nWe evaluate SFLM on two groups of tasks –\nsentence classiﬁcation and sentence-pair classiﬁ-\ncation. Experiments show that our model outper-\nforms other supervised and semi-supervised base-\nlines. We also conduct a detailed analysis of the\ndata efﬁciency of our model by examining its per-\nformance w.r.t various ratios of the amount of the\nunlabeled data to that of the labelled data. We\nﬁnd out that the performance gain diminishes as\nmore unlabeled data are used. We further extend\nour method for a more challenging scenario: few-\nshot transfer across tasks, where the model is ﬁrst\ntrained on the labeled data of a source task and the\nunlabeled data of a target task, then evaluated on\nthe target task. We provide the analysis of the fac-\ntors that affect the model performance to motivate\nfuture research.\n2 Related Work\n2.1 Few-Shot Learning of Language Model\nIt is desirable to reduce the amount of labeled\ndata for language model ﬁne-tuning, a.k.a., lan-\nguage model few-shot learning. The popular\nmethods usually address this problem with meta-\nlearning (Vinyals et al., 2016; Snell et al., 2017;\nFinn et al., 2017), which ﬁrst pre-trains a model on\na set of auxiliary tasks, then ﬁne-tunes on the task\nof interest (Yu et al., 2018; Han et al., 2018; Bao\net al., 2020; Bansal et al., 2020).\nRecently, Brown et al. (2020) proposes GPT-3\nand demonstrates that the language model itself has\na great potential for few-shot learning through task\ndemonstrations and prompts. As GPT-3 (Brown\net al., 2020) has an extremely large footprint, that\nlimits its scope of applications.\nMore recent studies explore few-shot learning\nwith pre-trained language models (Gunel et al.,\n2020; Schick and Schütze, 2020; Gao et al., 2020)\nof smaller size. A representative example is the\nLM-BFF by (Gao et al., 2020), which explores au-\ntomatic prompt generation and prompt-based ﬁne-\ntuning with a RoBERTa-large (Liu et al., 2019)\nlanguage model in a few-shot setup. LM-BFF\nhas achieved comparable results w.r.t methods ﬁne-\ntuned with the full annotated dataset.\nWe are motivated to study few-shot learning\nof language model with prompt-based language\nmodel ﬁne-tuning. We exploit the rich information\nin unlabeled data with semi-supervised learning.\nFurthermore, we adopt an even smaller RoBERTa-\nbase model as the backbone of our framework.\n2.2 Self-Training\nSelf-training refers to the process of creating\npseudo-labels on unlabeled data with a pre-trained\nteacher model, then applying these labeled data\nto train a student model. It is a simple and effec-\ntive semi-supervised approach, which has bene-\nﬁted a wide range of tasks, such as image classi-\nﬁcation (Xie et al., 2020b), neural sequence gen-\neration (He et al., 2019), and parsing (McClosky\net al., 2006). Generally, sophisticated learning al-\ngorithms (Sohn et al., 2020), and a large corpus of\ntask-relevant data (Xie et al., 2020a) are required\nfor self-training to work well.\nFrom the algorithm perspective, FixMatch (Sohn\net al., 2020) is a simple and effective self-training\nframework for image classiﬁcation, which uniﬁes\nconsistency regularization and pseudo-labeling. In\nour work, we transfer this useful framework to lan-\nguage model few-shot learning by exploring vari-\nous text augmentation techniques for ﬁne-tuning\nthe pre-trained language model.\nFrom the data perspective, several recent works\nhave shown the effectiveness of self-training for\nlanguage model ﬁne-tuning (Du et al., 2020; Schick\nand Schütze, 2020, 2021) leveraging a large amount\nof unlabeled data. PET (Schick and Schütze, 2020)\nadopts prompt-based ﬁne-tuning and self-training\nfor language model few-shot learning. This ap-\nproach assumes the presence of a large number of\nunlabeled in-domain data (roughly 10,000 exam-\nples per class). In addition, Du et al. (2020) propose\nto retrieve task-relevant unlabeled data from a large-\n9127\n \nThree kids are laughing. It was [MASK]\nThree kids are laughing. It was [MASK]\nThree kids are [MASK]. It was [MASK]\nLM\nLM\n Strongly-augmented \n Weakly-augmented \nLM\n... \nsleeping\nlaughing \ncoding \nplaying \n...\nA female is having taco. It was [MASK]\nThe man is exercising. It was [MASK]\nUnlabeled samples\nA cute cat. It was [MASK]\nLabeled samples\nLM great  (label: positive) \nterrible (label: negative)\n \nPrediction\nPrediction\nPseudo-label\nLM Language model with MLM head\nWord to label mapping\nTwo different forward passes\nFigure 1: The learning process of SFLM on both labeled and unlabeled samples with three loss terms. For the\nsupervised loss term Ls, in SFLM, a pre-trained language model with a MLM head is used to get the predicted\nword from the template. Then the predicted word is mapped to the corresponding label with manually deﬁned\ntask-speciﬁc word to label mapping M. Two loss terms are computed upon the unlabeled data: (1) We use masked\nlanguage modeling to compute the self-supervised loss. (2) We use a weak augmented (dropout) sentence to get\nthe pseudo-label, then force the prediction given by a strongly-augmented (random mask) view against the pseudo\nlabel via the self-training loss.\nscale open-domain sentence bank. A paraphrase-\nbased universal sentence encoder is designed to\noutput sentence-level vectors for computing cosine\nsimilarity between labeled sentences and unlabeled\nones in the sentence bank.\nUnlike the prior studies, which rely on a large\namount of unlabeled data and expensive computa-\ntion resources, we do not assume the availability\nof abundant in-domain unlabeled data. Instead, we\ntackle the in-domain data constraint via improving\ndata efﬁciency, i.e., proposing a scalable and effec-\ntive self-training framework leveraging only a few\nunlabeled data.\n3 Methodology\nProblem setup: Our goal is to adapt pre-trained\nlanguage models to downstream tasks in a few-shot\nsetting. The model, m should correctly classify\nunseen examples leveraging very few labeled data\npoints from each class. Let Xdenote a small set\nof labeled training data with N samples per class\nand an unlabeled dataset, Ufrom the same task\ndomain as X. Assume that this unlabeled dataset\nhas very limited size µN per class, where µis the\nratio between the size of Xand that of U.\nDuring training, let each batch consist of B\nlabeled data points, XB, and µB unlabeled data\npoints, UB:\nXB= {(xi,yi) :i∈(1,...,B )} (1)\nUB= {ui : i∈(1,...,µB )} (2)\nFigure 1 illustrates the learning process with an\nexample, containing one labeled and three unla-\nbeled data samples. SFLM is optimized with fol-\nlowing loss function:\nL= Ls + λ1Lst + λ2Lssl (3)\nwhere Ls is the prompt-based supervised loss ap-\nplied to the labeled data (Gao et al., 2020), Lst and\nLssl refer to self-training loss and self-supervised\nloss applied to the unlabeled data accordingly,\nwhile λ1 and λ2 are ﬁxed scalar hyper-parameters\ncontrolling the relative weight of the unlabeled loss\nterms.\nPrompt-based supervised loss: The prompt-\nbased supervised loss is motivated by LM-\nBFF (Gao et al., 2020). The classiﬁcation is re-\nformulated as a language modeling task, in which\nthe probability of class prediction yi ∈Y is,\npm(yi|xi) =pm([MASK] =M\n′\n(yi|xprompt\ni )\n(4)\nwhere M\n′\nrefers a mapping from task labels to the\ncorresponding words2, and xprompt\ni is the recon-\nstructed input sentence with task-speciﬁc template.\nFor instance, in a sentence-level binary classiﬁca-\ntion task, the input sentence xi is reconstructed as:\nxprompt\ni = xi ◦It was [MASK]. (5)\nwhere ◦denotes the string concatenation operation.\nInstead of using an additional classiﬁer, the pre-\ntrained masked language modeling head decides\nwhich word to be ﬁlled in the masked position.\n2M\n′\nis the inverse operation of Mas shown in ﬁgure 1\n9128\nThen we could ﬁne-tune the model with the stan-\ndard cross-entropy loss:\nLs = 1\nB\nB∑\ni=1\nH(yi,pm(yi|xi)) (6)\nSelf-training loss: For each unlabeled sentence\nui, we obtain the weakly-augmented version α(ui)\nand the strongly-augmented version A(ui), where\nα and Arefers to different augmentation strate-\ngies. The self-training process consists of two\nstages. Firstly, we assign a pseudo label to each\nunlabeled sentence in the batch by computing the\noutput probability distribution corresponding to the\nweakly-augmented input sentence α(ui), deﬁned\nas qi = pm(yi|α(ui)). The pseudo label, ˆqi, is\nobtained by ˆqi = arg max (qi). Secondly, we com-\npute the prompt-based cross-entropy loss between\nˆqi and the prediction corresponding to the strongly-\naugmented input sentence A(ui). The self-training\nloss is deﬁned as,\nLst = 1\nµB\nµB∑\ni=1\n1 (max(qi) ≥τ) ·\nH(ˆqi,pm(yi|A(ui)))\n(7)\nwhere τ deﬁnes the threshold above which we re-\ntain a pseudo-label.\nSohn et al. (2020) adopt AutoAugment (Cubuk\net al., 2018) for image augmentation, and highlight\nthe importance of applying proper augmentation\ntechniques in self-training. Text augmentation tech-\nniques can be tricky due to the discrete nature of\ntext data. The recent successes in representation\nlearning (Devlin et al., 2019; Gao et al., 2021) mo-\ntivate us to purely rely on dropout for our weak\naugmentation, and random token masking for our\nstrong augmentation.\nSpeciﬁcally, the surface forms of weakly-\naugmented sentences remain unchanged: α(ui) =\nui. For strong augmentation, we randomly re-\nplace 15% of the tokens in A(ui) with the special\nmask token, [MASK]. Then, we input α(ui) and\nA(ui) to the language model separately. Therefore,\nthe two input sentences will undergo independent\ndropout operations (0.1 dropout rate by default),\nwhich can be considered as part of the data augmen-\ntation process (The green arrow in Figure 1). We\nempirically show that the performance of our pro-\nposed augmentation techniques is superior against\nother common text augmentation techniques in Sec-\ntion 4.\nSelf-supervised loss: We also include an auxil-\niary self-supervised loss term, Lssl, for regulariza-\ntion purpose. The masked language model loss is\nused for its simplicity and efﬁciency.\n4 Experiment\n4.1 Setup\nWe evaluate our model on two groups of tasks:\n(1) 6 standard single sentence classiﬁcation tasks\n(SST-2 (Socher et al., 2013), SST-5 (Socher et al.,\n2013), MR (Pang and Lee, 2005), CR (Hu and Liu,\n2004), MPQA (Wiebe et al., 2005), Subj (Pang\nand Lee, 2004)) and (2) 6 sentence pair classiﬁca-\ntion tasks (MNLI (Williams et al., 2018), MNLI-\nmm (Williams et al., 2018), SNLI (Bowman et al.,\n2015), QNLI (Rajpurkar et al., 2016), RTE (Dagan\net al., 2005; Haim et al., 2006; Giampiccolo et al.,\n2007; Bentivogli et al., 2009), MRPC (Dolan and\nBrockett, 2005)). These tasks are adapted from the\nbenchmarks in (Conneau and Kiela, 2018; Wang\net al., 2018).\nWe set N to 16 and µ to 4. Following (Gao\net al., 2020). We randomly sample ﬁve different\nsplits of (Xtrain, Xdev, U) from the original train-\ning set. Five different models are trained with these\nsplits. Then, we report the average performance\nof these ﬁve models on the original development\nset. As in the previous work (Schick and Schütze,\n2020), the sampled unlabeled splits are carefully\nconstructed to account for class balance. As few-\nshot learning can be unstable, and extremely sensi-\ntive to hyper-parameter selection, we also perform\na grid search over several hyper-parameters (learn-\ning rate, batch size B, controlling weight of loss λ\nand conﬁdence threshold τ) across different tasks.\nFinally, Adam (Kingma and Ba, 2015) is used as\nthe optimizer.\n4.2 Baselines\nWe consider three baselines, namely standard ﬁne-\ntuning (FT), supervised learning (Gao et al., 2020)\n(LM-BFF), semi-supervised learning (Schick and\nSchütze, 2021) (PET). We use RoBERTa-base (Liu\net al., 2019), which has 125M parameters, and the\nsame task-speciﬁc manual prompt from (Gao et al.,\n2020), including template and word-to-label map-\nping, for prompt-based ﬁne-tuning.\nFT: We directly ﬁne-tune the language model\nwith the sequence classiﬁcation head on the few-\nshot labeled dataset.\nLM-BFF: We choose current state-of-the-art\n9129\nSST-2\n(acc)\nSST-5\n(acc)\nMR\n(acc)\nCR\n(acc)\nMPQA\n(acc)\nSubj\n(acc)\nFT 78.3 (3.7) 36.4 (2.1) 69.5 (4.3) 78.6 (4.3) 69.2 (7.2) 89.6 (0.8)\nLM-BFF 89.9 (0.5) 45.8 (3.1) 84.1 (1.7) 89.5 (0.6) 84.3 (1.1) 88.3 (3.5)\nPET-few♠ 89.8 (0.9) 46.7 (0.8) 84.2 (1.2) 89.4 (0.7) 84.9 (0.9) 90.0 (1.7)\nPET-full♥ 90.2 (0.8) 46.0 (1.2) 85.0 (1.3) 88.9 (1.0) 84.1 (1.0) 91.4 (0.7)\nSFLM 91.0 (0.7) 47.7 (1.1) 86.6 (0.4) 90.8 (0.6) 86.5 (0.3) 92.4 (0.7)\nMNLI\n(acc)\nMNLI-mm\n(acc)\nSNLI\n(acc)\nQNLI\n(acc)\nRTE\n(acc)\nMRPC\n(F1)\nFT 41.2 (2.2) 42.9 (2.6) 43.9 (3.1) 60.3 (3.7) 50.3 (1.9) 70.8 (19.9)\nLM-BFF 60.2 (1.7) 62.3 (1.4) 65.8 (2.6) 60.6 (2.1) 66.2 (3.4) 77.7 (0.8)\nPET-few♠ 60.0 (1.8) 61.6 (1.4) 66.8 (2.8) 60.8 (2.5) 62.5 (1.7) 77.4 (5.0)\nPET-full♥ 62.6 (2.9) 64.8 (2.2) 67.8 (3.5) 61.3 (4.0) 65.5 (2.3) 77.5 (4.5)\nSFLM 62.6 (1.5) 64.7 (1.3) 67.4 (2.7) 61.0 (4.6) 67.3 (2.7) 81.8 (1.2)\nTable 1: We use RoBERTa-base and report the average scores in all experiments, where acc denotes the accuracy\n(%), and F1 denotes the F1 score. The standard deviation is included in the bracket. We use N = 16(# labeled\nexamples per class), and µ = 4 (ratio of unlabeled data to labeled data) for few-shot experiments. Upper block\nshows the results on single sentence tasks, while lower block shows the results on sentence pair tasks. ♠: we re-\nimplement the PET (Schick and Schütze, 2021) based on LM-BFF (Gao et al., 2020). ♥: We treat the full traning\nset as the unlabeled dataset and ﬁne-tune PET with it. The size of the full training set is kept to 10,000 samples for\nall tasks.\nLM-BFF (Gao et al., 2020) as our supervised base-\nline. We use the prompt with demonstration (Gao\net al., 2020) implementation across all tasks for fair\ncomparison. We retrain the model with the ofﬁcial\ncode 3.\nPET: For fair comparison, We re-implement\nour own version of PET based on LM-BFF (Gao\net al., 2020), since LM-BFF largely beneﬁts from\nprompt-based ﬁne-tuning. Speciﬁcally, we re-\nmove the knowledge distillation on the standard\nsequence classiﬁer in the original implementation.\nInstead, we ﬁne-tune the prompt-based language\nmodel (Gao et al., 2020) with a mixed training\nset of labeled and pseudo-labeled data. We itera-\ntively increase the amount of pseudo-labeled data\nin the training set for model ﬁne-tuning. Through\nour extensive experiments, we ﬁnd that our imple-\nmentation outperforms the ofﬁcial implementation4\nacross various tasks. In addition, we evaluate PET\nunder two different settings: (1) using reduced un-\nlabeled dataset, which is the same as our SFLM; (2)\nusing the full training set for self-training, while we\nlimit the number of unlabeled samples to 10,000\n3https://github.com/princeton-nlp/\nLM-BFF\n4https://github.com/timoschick/pet\nper class such that the amount of unlabeled samples\nacross different tasks are kept in the same range.\n4.3 Main Results\nTable 1 presents the performance of SFLM against\nbaselines across various benchmarking tasks. Over-\nall, our proposed SFLM consistently outperforms\nthe supervised and semi-supervised methods by 2%\non average with the same amount of data, and by\n1.2% with 45 times less unlabeled data. Next, we\nsummarize our observations over the experiment\nresults.\nFirst, we ﬁnd that self-training can greatly im-\nprove the performance of vanilla prompt-based ﬁne-\ntuning under few-shot setting, either using PET\nor SFLM. With a large amount of in-domain un-\nlabeled data, even a simple iterative self-training\napproach can boost the performance by 3.13% on\nSubj, and 0.88% on average (PET-full vs. LM-\nBFF). This demonstrates the effectiveness of ex-\nploiting the rich information carried in the unla-\nbeled data.\nSecond, in-domain unlabeled data is crucial to\nthe success of semi-supervised methods. As we\ndown-sample the unlabeled dataset size to 64 sam-\nples per class, The performance of PET barely has\n9130\nany improvement w.r.t LM-BFF. The performance\neven degrades in some tasks. For example, in tasks5\nsuch as RTE, CR, and MRPC, PET doesn’t perform\nas well as LM-BFF even using the full unlabeled\ndataset.\nThird, unlike PET-few, SFLM can still bring a\nsigniﬁcant improvement w.r.t LM-BFF even when\nthe size of unlabeled data is limited. This observa-\ntion implies that our approach utilizes the unlabeled\ndata in a much more efﬁcient way. SFLM also out-\nperforms PET on 8 tasks out of 12. The exceptions\nare the four natural language inference tasks, of\nwhich the unlabeled datasets contain 10,000 un-\nlabeled data samples. However, the performance\ndifference between PET-full, which uses all the\n10,000 unlabeled data samples, and SFLM in these\nfour tasks is insigniﬁcant (less than 0.4%). In addi-\ntion, SFLM generally has lower variance compared\nto the baselines.\nThe major difference between SFLM and LM-\nBFF is that we utilize the rich information carried\nin the unlabeled data. The experiment results con-\nﬁrm our hypothesis about the usefulness of semi-\nsupervised learning in language model few-shot\nlearning. Furthermore, the major difference be-\ntween SFLM and PET is the self-training algorithm.\nWe include strong data augmentation technique for\nconsistency regularization. This conﬁrms that our\nproposed text augmentation techniques are crucial\nto the success of SFLM.\n4.4 Analysis of Data Efﬁciency\nOne of the key questions in this study is how many\nlabeled and unlabeled data are required. We pro-\nvide an answer in Figure 2, which illustrates the\nperformance of SFLM with different combinations\nof N and µ. It can be observed that the error\nrate reduces generally as µincreases, that suggests\nSFLM beneﬁts from more unlabeled data, with an\nexception in SST-5, where the best performance is\nachieved at µ= 2. a similar trend is also spotted\nfor PET (see Table 1).\nWe note that SST-5 is the most difﬁcult one\namong the six tasks. We observe that, with a rel-\natively weak teacher model, self-training doesn’t\nbeneﬁt from more unlabeled data. We speculate\nthat by increasing unlabeled data, we introduce\nmore noise into the training process when we have\na weak starting point.\n5The full unlabeled datasets for these tasks contain less\nthan 5,000 sentences.\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni0000001b\n/uni0000001c\n/uni00000014/uni00000013\n/uni00000014/uni00000014\n/uni00000014/uni00000015/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000036/uni00000036/uni00000037/uni00000010/uni00000015\nN = 4\nN = 8\nN = 16\nN = 32\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni00000018/uni00000013\n/uni00000018/uni00000014\n/uni00000018/uni00000015\n/uni00000018/uni00000016\n/uni00000018/uni00000017\n/uni00000018/uni00000018\n/uni00000018/uni00000019\n/uni00000018/uni0000001a\n/uni00000018/uni0000001b/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000036/uni00000036/uni00000037/uni00000010/uni00000018\nN = 4\nN = 8\nN = 16\nN = 32\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni00000014/uni00000015\n/uni00000014/uni00000016\n/uni00000014/uni00000017\n/uni00000014/uni00000018\n/uni00000014/uni00000019\n/uni00000014/uni0000001a/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000030/uni00000035\nN = 4\nN = 8\nN = 16\nN = 32\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni0000001c\n/uni00000014/uni00000013\n/uni00000014/uni00000014\n/uni00000014/uni00000015/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000026/uni00000035\nN = 4\nN = 8\nN = 16\nN = 32\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni00000014/uni00000015\n/uni00000014/uni00000016\n/uni00000014/uni00000017\n/uni00000014/uni00000018\n/uni00000014/uni00000019\n/uni00000014/uni0000001a\n/uni00000014/uni0000001b/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000030/uni00000033/uni00000034/uni00000024\nN = 4\nN = 8\nN = 16\nN = 32\n/uni00000013/uni00000015/uni00000017/uni0000001b/uni00000014/uni00000019\n/uni00000019\n/uni0000001b\n/uni00000014/uni00000013\n/uni00000014/uni00000015\n/uni00000014/uni00000017\n/uni00000014/uni00000019/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000003/uni00000055/uni00000044/uni00000057/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000036/uni00000058/uni00000045/uni0000004d\nN = 4\nN = 8\nN = 16\nN = 32\nFigure 2: Error rates of our SFLM under different N\n(# instances per class), and µ(ratio between unlabeled\nand labeled data)6. µ= 0refers to vanilla LM-BFF.\nIn Figure 2, we also observe that, for the simpler\ntasks, e.g., SST-2 and Subj, the gain obtained by\nmore unlabeled data rapidly saturates when four\ntimes of unlabeled data are given, which is con-\nsistent with the ﬁnding in (Sohn et al., 2020). We\nare encouraged to see that SFLM continues to im-\nprove as µincreases for the other three tasks and\nsaturates later. In general, the trend of performance\nimprovement by varying µis consistent across dif-\nferent value of N. For instance, the performance\ngain is ∼0.65% for increasing µfrom 2 to 4 across\ndifferent N.\nTo better understand what SLFM actually im-\nproves over the baselines, we further analyze the\ndistribution of incorrectly-labelled examples cor-\nrected by SFLM, motivated by (Wei et al., 2020).\nFirst, we partition incorrectly-labelled examples\ninto ﬁve bins based on the cosine similarity of their\nsentence embeddings given by SimCSE (Gao et al.,\n6The performance in CR for (N = 32, µ= 16) is put as\nthe same as that of (N = 32, µ= 8) due to limited amount\nof unlabeled data\n9131\n2021) w.r.t. the average embedding of the training\nset. Next, we show the percentage of examples in\neach bin whose labels are corrected by SFLM. As\nshown in Figure 3, SFLM is more likely to correct\nexamples within the gold-labelled data’s neighbor-\nhood (the region surrounding a data point in the\nembedding space) than those away from the neigh-\nborhood. In other words, the performance gain of\nSFLM is not only related to the unlabelled data size\nbut also to the data distribution.\n< 0.1 0.1~0.15 0.15~0.2 0.2~0.25 > 0.25\nCosine similarity to training set\n0\n5\n10\n15\n20\n25% corrected by SFLM\nFigure 3: Percentage of examples with varying cosine\nsimilarities to training set corrected by SFLM.\nSFLM is also scalable with different amounts\nof labeled data, and consistently outperforms LM-\nBFF by 3.2% for N = 4and by 2% for N = 32\non average. Especially, SFLM exhibits signiﬁcant\nimprovement over LM-BFF under the extremely\nfew-shot scenario. When N = 32, the performance\nof LM-BFF saturates in simple tasks, e.g., SST-2,\nCR. However, SFLM continues to narrow the gap\nbetween few-shot learning and ﬁne-tuning with the\nentire labeled dataset, which further validates the\nbeneﬁts of self-training to supervised learning.\n4.5 Augmentation Techniques\nIt has been shown that strong data augmentation\nplays a crucial role in semi-supervised visual repre-\nsentation learning (Zoph et al., 2020; Xie et al.,\n2020b; Sohn et al., 2020). The images can be\naugmented easily by cutout, ﬂipping, and crop-\nping (Zoph et al., 2020). However, very few works\nhave been done on augmentation techniques for\ntext (Xie et al., 2020b)7.\nHere, we study how different augmentation tech-\nniques would affect the model performance. We\n7They adopt a back-translation system, which requires\nexternal parallel corpus. In SFLM, we focus on a few easy\ndata augmentation strategies (Wei and Zou, 2019)\nﬁx Dropout as the weak augmentation, and present\nthe results of another three strong augmentation\napproaches, including Crop, Swap and Deletion\nin Table 2. Speciﬁcally, Dropout is same as the\nweak augmentation, we directly forward the origi-\nnal sentence into the language model. Crop refers\nto randomly cropping the original sentence into\na continuous span of 85% of the original length.\nIn terms of Swap, we randomly swap two tokens\nin the sentence and repeat the same procedure 3\ntimes. For Deletion, we randomly delete 15% of\nthe tokens in a sentence. Mask refers to randomly\nreplacing 15% of tokens in a sentence with the spe-\ncial [MASK] token. In this experiment, we keep\nN = 16and µ= 4.\nWe observe that the SFLM framework can also\nwork with Dropout and Swap, as they still outper-\nform PET on average by 0.4 and 0.2 respectively.\nHowever, they are less effective than Mask. An-\nother interesting ﬁnding is that Deletion, which is\nsimilar to Mask, yields poor performance. We hy-\npothesize that Deletion and Crop may adversely\naffect the original semantics, for example, deleting\nthe word, not, may reverse the meaning of the orig-\ninal sentence. In contrast, Mask keeps the structure\nof sentences, and hence, it is easier to maintain the\nsemantics by adding consistency regularization and\nMLM objectives.\nFurthermore, we empirically study the effect of\ndifferent masking ratio on SST-2. 90.62% of accu-\nracy is obtained for 10% masking, 90.14% accu-\nracy for 20% masking, and the best performance\nof 91.0% for 15% masking.\n4.6 Model Scale\nTo put the SFLM framework under a stress test, we\nfurther apply SFLM on a smaller language model,\ndistilled RoBERTa with 84M parameters, as re-\nported in Table 3. While DistilRoBERTa shows\na similar performance as RoBERTa, it performs\nmuch worse under few-shot learning scenarios, e.g.,\nLM-BFF has a signiﬁcant performance drop of 5%\nin CR and MPQA. We hypothesize that a robust\nlanguage model of reasonable footprint is required\nfor effective self-training.\nWhile SFLM consistently outperforms LM-BFF\nwith a smaller language model, the overall per-\nformance of SFLM based on DistilRoBERTa-base\nalso degrades sharply w.r.t that based on RoBERTa-\nbase. This suggests the crucial role of the language\nmodel in our approach.\n9132\nAugmentation SST-2 SST-5 MR CR MPQA Subj Avg\nDropout 90.0 46.0 84.7 89.7 85.7 90.9 81.2\nCrop 89.6 45.7 83.6 88.4 85.2 88.8 80.2\nSwap 91.1 41.8 86.1 89.2 85.4 92.3 81.0\nDeletion 90.3 44.1 83.7 89.8 86.3 90.0 80.7\nMask 91.0 47.7 86.6 90.8 86.5 92.4 82.5\nTable 2: Comparison of different data augmentation on six single sentence classiﬁcation datasets (accuracy %).\nCR MPQA Subj\nRoBERTa-base (12-layers)\nFT 78.6 69.2 89.6\nLM-BFF 89.5 84.3 88.3\nPET-few♠ 89.4 84.9 90.0\nPET-full♥ 88.9 84.1 91.4\nSFLM 90.8 86.5 92.4\nFT (full) 89.6 87.4 96.9\nDistilRoBERTa-base (6-layers)\nFT 71.0 71.7 86.6\nLM-BFF 84.8 79.8 87.5\nPET-few♠ 86.5 80.6 88.3\nPET-full♥ 87.2 80.6 88.0\nSFLM 87.9 82.0 89.5\nFT (full) 85.5 86.9 96.6\nTable 3: Accuracy (%) for systems with language mod-\nels of different size. We use N = 16(# labeled exam-\nples per class), and µ = 4 (ratio of unlabeled data to\nlabeled data) for few-shot experiments. ♠, ♥follow\nthe same deﬁnition in Table 1. DistilRoBERa-base: 6-\nlayer RoBERTa distilled from RoBERTa-base with 2\ntimes speedup.\nMeanwhile, we ﬁnd that PET better suits the\nsmall language model setting. For instance, the per-\nformance gain of PET-full w.r.t. LM-BFF increas-\ning from 0.8% to 1.1% under the RoBERTa-base\nand DistilRoBERTa-base settings respectively. We\nhypothesize that more unlabeled data beneﬁt few-\nshot learning of smaller language model. However,\nthe performance of PET is still worse than that of\nSFLM under the DistilRoBERTa-base setting.\nOverall, the above observations conﬁrm the ef-\nfectiveness of our approach for language model\nfew-shot learning regardless of the backbone lan-\nguage model.\n4.7 Zero-shot Transfer Across Tasks\nLastly, we show that our proposed method can be\neasily extended to zero-shot transfer across tasks.\nSpeciﬁcally, we assume that before few-shot learn-\ning on the target unlabeled dataset U, the learner\nhas access to an annotated dataset Dknown as the\nbase dataset.8 Accordingly, we modify the learning\nobjective as:\nL= LD\ns + λ1LU\nst + λ2LU\nssl, (8)\nwhere the last two terms are intended to encour-\nage the model to capture knowledge speciﬁc to the\ntarget task.\nWe evaluate SFLM on three binary classiﬁcation\ntasks, including sentiment analysis (MR and SST)\nand product reviews (CR). In the experiments, we\nuse one of the three tasks as the source task and test\non another two target tasks. The SFLM model is\nbased on RoBERTa-base and trained on 64 samples\nper class for Dand U, respectively. We compare\nSFLM with a baseline, which is a language model\nﬁne-tuned directly on the source dataset with a\nsingle prompt-based loss LD\ns .\nThe results are reported in Table 4. SFLM,\nwhich adopts a small number of unlabeled target\ndata, generally outperforms that with supervised\nﬁne-tuning on source datasets. In particular, with\nMR as the source and CR as the target, SFLM\nobtains 1.3 points higher than the baseline model,\nwhich demonstrates that our proposed method has\nthe ﬂexibility to be applied to the cross-task zero-\nshot learning scenario.\n5 Conclusion and Future Work\nIn this paper, we present SFLM, a simple and effec-\ntive self-training framework for few-shot learning\nof language model. Our approach addresses the\n8The concept of task transfer has been successfully applied\nto natural language understanding. For example, pre-training\nlanguage model on MNLI before ﬁne-tuning on RTE (Liu\net al., 2019) yields better performance.\n9133\nTransfer SFLM\nSST MR CR SST MR CR\nSST - 87.3 89.7 - 87.3 90.8\nMR 90.7 - 89.5 91.4 - 90.8\nCR 89.7 84.5 - 90.3 85.5 -\nTable 4: Accuracy (%) for zero-shot task trans-\nfer between three sentiment classiﬁcation tasks with\nRoBERTa-base. Transfer: a language model ﬁne-tuned\non source dataset with a single prompt-based loss; We\nrefer SST to SST-2.\nfew-shot language model ﬁne-tuning problem with\nvery limited labeled and unlabeled data with a self-\ntraining loss term unifying pseudo-labeling and\nconsistency regularization. Through comprehen-\nsive experiments, we show that our approach out-\nperforms previous state-of-the-art methods across\ntasks, data amount, and model scale. Despite its\nefﬁciency, SFLM also has several limitations. Com-\npared to standard ﬁne-tuning, SFLM requires more\ncomputational resources for unlabeled data. In ad-\ndition, the performance gain by self-training is not\nproportional to the amount of unlabeled data. We\nleave it to future study. Namely, how to utilize\nlarge amount of unlabeled data efﬁciently through\nself-training.\nAcknowledgements\nWe would like to thank all the anonymous review-\ners for their constructive comments. This work\nis partly supported by Human-Robot Interaction\nPhase 1 (Grant No. 19225 00054), National Re-\nsearch Foundation (NRF) Singapore under the Na-\ntional Robotics Programme; Human Robot Col-\nlaborative AI for AME (Grant No. A18A2b0046),\nNRF Singapore; National Natural Science Founda-\ntion of China (Grant NO. 61903178, 61906081,\nand U20A20306); Program for Guangdong In-\ntroducing Innovative and Entrepreneurial Teams\n(Grant No. 2017ZT03X386); Program for Univer-\nsity Key Laboratory of Guangdong Province (Grant\nNo. 2017KSYS008).\nReferences\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020. Self-supervised\nmeta-learning for few-shot natural language classiﬁ-\ncation tasks. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 522–534, Online. Association\nfor Computational Linguistics.\nYujia Bao, Menghua Wu, Shiyu Chang, and Regina\nBarzilay. 2020. Few-shot text classiﬁcation with dis-\ntributional signatures. In Proc. of ICLR.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\ntual entailment challenge. In TAC.\nAvrim Blum and Tom Mitchell. 1998. Combining la-\nbeled and unlabeled data with co-training. In Proc.\nof COLT.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nT. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, J. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, T. Henighan, R. Child,\nA. Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess,\nJ. Clark, Christopher Berner, Sam McCandlish,\nA. Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. Proc.\nof NeurIPS.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In Proc. of ICLR.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nE. D. Cubuk, Barret Zoph, Dandelion Mané, Vijay\nVasudevan, and Quoc V . Le. 2018. Autoaugment:\nLearning augmentation policies from data. ArXiv.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. page 177–190.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\n9134\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nJingfei Du, E. Grave, Beliz Gunel, Vishrav Chaud-\nhary, Onur Celebi, Michael Auli, Ves Stoyanov,\nand Alexis Conneau. 2020. Self-training improves\npre-training for natural language understanding. In\nArXiv.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. 2017.\nModel-agnostic meta-learning for fast adaptation of\ndeep networks. In Proc. of ICML.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. ArXiv, abs/2104.08821.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\nand Bill Dolan. 2007. The third PASCAL recogniz-\ning textual entailment challenge. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 1–9, Prague. Association\nfor Computational Linguistics.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves\nStoyanov. 2020. Supervised contrastive learning\nfor pre-trained language model ﬁne-tuning. ArXiv,\nabs/2011.01403.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proc. of the Second PASCAL\nChallenges Workshop on Recognising Textual Entail-\nment.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel:\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 4803–\n4809, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio\nRanzato. 2019. Revisiting self-training for neural\nsequence generation. In Proc. of ICLR.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In Proc. of\nICLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proc. of ICLR.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach. In\nArXiv.\nDavid McClosky, Eugene Charniak, and Mark Johnson.\n2006. Effective self-training for parsing. In Pro-\nceedings of the Human Language Technology Con-\nference of the NAACL, Main Conference, pages 152–\n159, New York City, USA. Association for Compu-\ntational Linguistics.\nBo Pang and Lillian Lee. 2004. A sentimental edu-\ncation: Sentiment analysis using subjectivity sum-\nmarization based on minimum cuts. In Proceed-\nings of the 42nd Annual Meeting of the Association\nfor Computational Linguistics (ACL-04), pages 271–\n278, Barcelona, Spain.\nBo Pang and Lillian Lee. 2005. Seeing stars: Ex-\nploiting class relationships for sentiment categoriza-\ntion with respect to rating scales. In Proceed-\nings of the 43rd Annual Meeting of the Association\nfor Computational Linguistics (ACL’05), pages 115–\n124, Ann Arbor, Michigan. Association for Compu-\ntational Linguistics.\nZimeng Qiu, Eunah Cho, Xiaochun Ma, and William\nCampbell. 2019. Graph-based semi-supervised\nlearning for natural language understanding. In\nProceedings of the Thirteenth Workshop on Graph-\nBased Methods for Natural Language Processing\n(TextGraphs-13), pages 151–158, Hong Kong. Asso-\nciation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam M. Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, W. Li, and Peter J. Liu. 2020. Explor-\ning the limits of transfer learning with a uniﬁed text-\nto-text transformer. J. Mach. Learn. Res., 21:140:1–\n140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. In Proc. of NAACL.\n9135\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nJake Snell, Kevin Swersky, and Richard Zemel. 2017.\nPrototypical networks for few-shot learning. In\nProc. of NeurlPS.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nKihyuk Sohn, David Berthelot, Nicholas Carlini,\nZizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do-\ngus Cubuk, Alexey Kurakin, and Chun-Liang Li.\n2020. Fixmatch: Simplifying semi-supervised learn-\ning with consistency and conﬁdence. In Proc. of\nNeurIPS.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Ko-\nray Kavukcuoglu, and Daan Wierstra. 2016. Match-\ning networks for one shot learning. In Proc. of\nNeurlPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nColin Wei, Kendrick Shen, Yining Chen, and Tengyu\nMa. 2020. Theoretical analysis of self-training with\ndeep networks on unlabeled data. In Proc. of ICLR.\nJason Wei and Kai Zou. 2019. EDA: Easy data aug-\nmentation techniques for boosting performance on\ntext classiﬁcation tasks. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 6382–6388, Hong Kong,\nChina. Association for Computational Linguistics.\nJanyce Wiebe, Theresa Wilson, and Claire Cardie.\n2005. Annotating expressions of opinions and emo-\ntions in language. Language resources and evalua-\ntion, 39(2):165–210.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020a. Unsupervised data augmenta-\ntion for consistency training. In Proc. of NeurlPS.\nQizhe Xie, E. Hovy, Minh-Thang Luong, and Quoc V .\nLe. 2020b. Self-training with noisy student im-\nproves imagenet classiﬁcation. In Proc. of CVPR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Proc. of NeurlPS.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 189–196, Cambridge, Mas-\nsachusetts, USA. Association for Computational\nLinguistics.\nMo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni\nPotdar, Yu Cheng, Gerald Tesauro, Haoyu Wang,\nand Bowen Zhou. 2018. Diverse few-shot text clas-\nsiﬁcation with multiple metrics. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 1206–1215, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nXiaojin Zhu. 2005. Semi-supervised learning literature\nsurvey. world.\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,\nHanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.\n2020. Rethinking pre-training and self-training. In\nProc. of NeurlPS."
}