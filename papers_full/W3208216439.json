{
  "title": "Enhance Text-to-Text Transfer Transformer with Generated Questions for Thai Question Answering",
  "url": "https://openalex.org/W3208216439",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3209714665",
      "name": "Puri Phakmongkol",
      "affiliations": [
        "Chulalongkorn University"
      ]
    },
    {
      "id": "https://openalex.org/A228581428",
      "name": "Peerapon Vateekul",
      "affiliations": [
        "Chulalongkorn University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2791479170",
    "https://openalex.org/W3082352295",
    "https://openalex.org/W3014006655",
    "https://openalex.org/W3101007570",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2963175042",
    "https://openalex.org/W3095789240",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3217257119"
  ],
  "abstract": "Question Answering (QA) is a natural language processing task that enables the machine to understand a given context and answer a given question. There are several QA research trials containing high resources of the English language. However, Thai is one of the languages that have low availability of labeled corpora in QA studies. According to previous studies, while the English QA models could achieve more than 90% of F1 scores, Thai QA models could obtain only 70% in our baseline. In this study, we aim to improve the performance of Thai QA models by generating more question-answer pairs with Multilingual Text-to-Text Transfer Transformer (mT5) along with data preprocessing methods for Thai. With this method, the question-answer pairs can synthesize more than 100 thousand pairs from provided Thai Wikipedia articles. Utilizing our synthesized data, many fine-tuning strategies were investigated to achieve the highest model performance. Furthermore, we have presented that the syllable-level F1 is a more suitable evaluation measure than Exact Match (EM) and the word-level F1 for Thai QA corpora. The experiment was conducted on two Thai QA corpora: Thai Wiki QA and iApp Wiki QA. The results show that our augmented model is the winner on both datasets compared to other modern transformer models: Roberta and mT5.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7486655712127686
    },
    {
      "name": "Computer science",
      "score": 0.7454673051834106
    },
    {
      "name": "Natural language processing",
      "score": 0.7301540374755859
    },
    {
      "name": "Preprocessor",
      "score": 0.6730808615684509
    },
    {
      "name": "Question answering",
      "score": 0.6357811093330383
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6020776033401489
    },
    {
      "name": "Information retrieval",
      "score": 0.3656122088432312
    },
    {
      "name": "Engineering",
      "score": 0.07431367039680481
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158708052",
      "name": "Chulalongkorn University",
      "country": "TH"
    }
  ],
  "cited_by": 10
}