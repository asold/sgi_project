{
  "title": "Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other large language models in scholarly peer review",
  "url": "https://openalex.org/W4376866708",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100910729",
      "name": "Mohammad Hosseini",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A3168971062",
      "name": "Serge P. J. M. Horbach",
      "affiliations": [
        "Aarhus University"
      ]
    },
    {
      "id": "https://openalex.org/A2100910729",
      "name": "Mohammad Hosseini",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A3168971062",
      "name": "Serge P. J. M. Horbach",
      "affiliations": [
        "Aarhus University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313294616",
    "https://openalex.org/W4282933389",
    "https://openalex.org/W3120711451",
    "https://openalex.org/W3021454583",
    "https://openalex.org/W4317801997",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W4307500010",
    "https://openalex.org/W4282045059",
    "https://openalex.org/W3087751570",
    "https://openalex.org/W4320498021",
    "https://openalex.org/W4391829366",
    "https://openalex.org/W2749119382",
    "https://openalex.org/W2939675123",
    "https://openalex.org/W4315650566",
    "https://openalex.org/W4318224940",
    "https://openalex.org/W2599395542",
    "https://openalex.org/W6848849931",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W4241559800",
    "https://openalex.org/W2161571728",
    "https://openalex.org/W1603147783",
    "https://openalex.org/W4318035272",
    "https://openalex.org/W2012950673",
    "https://openalex.org/W4399718316",
    "https://openalex.org/W4230219168",
    "https://openalex.org/W4381252028",
    "https://openalex.org/W2038550088",
    "https://openalex.org/W4221016565"
  ],
  "abstract": "Abstract Background The emergence of systems based on large language models (LLMs) such as OpenAI’s ChatGPT has created a range of discussions in scholarly circles. Since LLMs generate grammatically correct and mostly relevant (yet sometimes outright wrong, irrelevant or biased) outputs in response to provided prompts, using them in various writing tasks including writing peer review reports could result in improved productivity. Given the significance of peer reviews in the existing scholarly publication landscape, exploring challenges and opportunities of using LLMs in peer review seems urgent. After the generation of the first scholarly outputs with LLMs, we anticipate that peer review reports too would be generated with the help of these systems. However, there are currently no guidelines on how these systems should be used in review tasks. Methods To investigate the potential impact of using LLMs on the peer review process, we used five core themes within discussions about peer review suggested by Tennant and Ross-Hellauer. These include 1) reviewers’ role, 2) editors’ role, 3) functions and quality of peer reviews, 4) reproducibility, and 5) the social and epistemic functions of peer reviews. We provide a small-scale exploration of ChatGPT’s performance regarding identified issues. Results LLMs have the potential to substantially alter the role of both peer reviewers and editors. Through supporting both actors in efficiently writing constructive reports or decision letters, LLMs can facilitate higher quality review and address issues of review shortage. However, the fundamental opacity of LLMs’ training data, inner workings, data handling, and development processes raise concerns about potential biases, confidentiality and the reproducibility of review reports. Additionally, as editorial work has a prominent function in defining and shaping epistemic communities, as well as negotiating normative frameworks within such communities, partly outsourcing this work to LLMs might have unforeseen consequences for social and epistemic relations within academia. Regarding performance, we identified major enhancements in a short period and expect LLMs to continue developing. Conclusions We believe that LLMs are likely to have a profound impact on academia and scholarly communication. While potentially beneficial to the scholarly communication system, many uncertainties remain and their use is not without risks. In particular, concerns about the amplification of existing biases and inequalities in access to appropriate infrastructure warrant further attention. For the moment, we recommend that if LLMs are used to write scholarly reviews and decision letters, reviewers and editors should disclose their use and accept full responsibility for data security and confidentiality, and their reports’ accuracy, tone, reasoning and originality.",
  "full_text": "Hosseini and Horbach  \nResearch Integrity and Peer Review             (2023) 8:4 \nhttps://doi.org/10.1186/s41073-023-00133-5\nRESEARCH Open Access\n© The Author(s) 2023, corrected publication 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 \nInternational License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver \n(http:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a \ncredit line to the data.\nResearch Integrity and\n           Peer Review\nFighting reviewer fatigue or amplifying bias? \nConsiderations and recommendations for use \nof ChatGPT and other large language models \nin scholarly peer review\nMohammad Hosseini1*   and Serge P . J. M. Horbach2   \nAbstract \nBackground The emergence of systems based on large language models (LLMs) such as OpenAI’s ChatGPT has cre-\nated a range of discussions in scholarly circles. Since LLMs generate grammatically correct and mostly relevant (yet \nsometimes outright wrong, irrelevant or biased) outputs in response to provided prompts, using them in various \nwriting tasks including writing peer review reports could result in improved productivity. Given the significance \nof peer reviews in the existing scholarly publication landscape, exploring challenges and opportunities of using LLMs \nin peer review seems urgent. After the generation of the first scholarly outputs with LLMs, we anticipate that peer \nreview reports too would be generated with the help of these systems. However, there are currently no guidelines \non how these systems should be used in review tasks.\nMethods To investigate the potential impact of using LLMs on the peer review process, we used five core themes \nwithin discussions about peer review suggested by Tennant and Ross-Hellauer. These include 1) reviewers’ role, 2) edi-\ntors’ role, 3) functions and quality of peer reviews, 4) reproducibility, and 5) the social and epistemic functions of peer \nreviews. We provide a small-scale exploration of ChatGPT’s performance regarding identified issues.\nResults LLMs have the potential to substantially alter the role of both peer reviewers and editors. Through support-\ning both actors in efficiently writing constructive reports or decision letters, LLMs can facilitate higher quality review \nand address issues of review shortage. However, the fundamental opacity of LLMs’ training data, inner workings, data \nhandling, and development processes raise concerns about potential biases, confidentiality and the reproducibility \nof review reports. Additionally, as editorial work has a prominent function in defining and shaping epistemic com-\nmunities, as well as negotiating normative frameworks within such communities, partly outsourcing this work to LLMs \nmight have unforeseen consequences for social and epistemic relations within academia. Regarding performance, we \nidentified major enhancements in a short period and expect LLMs to continue developing.\nConclusions We believe that LLMs are likely to have a profound impact on academia and scholarly communica-\ntion. While potentially beneficial to the scholarly communication system, many uncertainties remain and their \nuse is not without risks. In particular, concerns about the amplification of existing biases and inequalities in access \nto appropriate infrastructure warrant further attention. For the moment, we recommend that if LLMs are used to write \n*Correspondence:\nMohammad Hosseini\nmohammad.hosseini@northwestern.edu\nFull list of author information is available at the end of the article\nPage 2 of 9Hosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \nscholarly reviews and decision letters, reviewers and editors should disclose their use and accept full responsibility \nfor data security and confidentiality, and their reports’ accuracy, tone, reasoning and originality.\nKeywords Peer review, Academic writing, Large Language Models, ChaGPT, Editorial practices, Generative AI\nBackground\nSince Open AI’s ChatGPT was released in November \n2022, it has been used by millions of people all over the \nworld. ChatGPT has applications in a host of differ -\nent contexts, and has also been used in various aspects \nof academic work. For instance, some researchers used \nit to write a paper [1], others used it to generate aca -\ndemic abstracts [2], and many more have probably tried \nit to support them in diverse other tasks. We expect the \nuse of ChatGPT or other systems based on large lan -\nguage models (LLMs) to grow and change the scholarly \npublication landscape through increased automation \nand speed. While automating the peer-review process \nhas been a trend (e.g., implication of automated screen -\ning [3] or automated systems to check compliance with \njournal policies and marks of quality [4]), the introduc -\ntion of novel LLMs has the potential to move automation \nmuch further. Depending on users’ creativity, systems \nsuch as ChatGPT could benefit peer reviews in differ -\nent ways (e.g., we used it to morph initial views or hap -\nhazard remarks into well-written reviews, summarize \nparagraphs or sections and highlight their strengths and \nweaknesses, write a review about published manuscripts \nwith/without a structure, review code and explain what it \ndoes). In this short essay, we take inspiration from previ -\nous work on peer review by Tennant and Ross-Hellauer \n[5] regarding core themes within discussions on this topic \nas a framework to explore possible impacts of using LLM \non the peer-review system. Note that, in comparison \nwith this previous work, we first discuss roles of review -\ners before discussing the roles of editors, to improve the \nflow of our argument.\nWe will provide some examples of using  ChatGPT in \npeer review contexts. We are aware though, that Chat -\nGPT is just one example of a host of LLMs and that it \nwas not specifically designed or trained for peer review \npurposes. In the future, other LLMs might be specifically \ndeveloped to support review processes and might there -\nfore be better fit for the purpose and/or be affected differ-\nently by the issues we discuss in the subsequent sections.\nRoles of reviewers in peer review\nReviewers are expected to provide neutral, unbiased \nand constructive feedback when they agree to review a \nmanuscript. In addition, they are usually expected to pro-\nvide recommendations to the editor about the publish -\nability of a paper. It seems likely that LLMs can support \nreviewers in both the quality-improvement and gate -\nkeeping roles. While these systems in their current shape \nseem to be able to contribute most on a language rather \nthan a content level, even the latter is increasingly feasi -\nble. When prompted with sufficiently specific requests, \nChatGPT provides meaningful feedback related to spe -\ncific parts of a manuscript. For instance, we used the \nmethods section in the abstract of the current study as \nan example:\nHow well are this study’s methods described and \naligned with the research question \"To investigate the \npotential impact of using LLMs on the peer review pro -\ncess, we used five core themes within discussions about \npeer review suggested by Tennant and Ross-Hellauer. \nThese include 1) reviewers’ role, 2) editors’ role, 3) func -\ntions and quality of peer reviews, 4) reproducibility, and \n5) the social and epistemic functions of peer reviews. We \nprovide a small-scale exploration of ChatGPT’s perfor -\nmance regarding identified issues.\"\n“Based on the description provided, the methods of \nthe study appear to be well described and aligned \nwith the research question. The study clearly out -\nlines the research question, which is to investigate \nthe potential impact of using large language models \n(LLMs) on the peer review process. The study also \nidentifies five core themes within discussions about \npeer review, which will serve as the focus of the \ninvestigation. The methods section of the study pro -\nvides a clear outline of the steps that will be taken to \nexplore the performance of ChatGPT regarding the \nidentified issues related to peer review. The study’s \nfocus on a small-scale exploration of ChatGPT’s \nperformance with respect to the five core themes is a \nclear and appropriate approach for investigating the \npotential impact of LLMs on the peer review process. \nIn conclusion, the methods section of the study is \nwell-described and aligned with the research ques -\ntion. It provides enough information to understand \nthe study’s design and goals, and it appears that the \nmethods are appropriate for addressing the research \nquestion. ” [6]\nSince LLMs generate grammatically sound text, they \ncan be used as a modern scribe morphing researchers’ \nquick notes or remarks into well-written reviews. Those \nwho have used ChatGPT, know its impressive power to \nimprove writing, even if fed with lexical/grammatical \nPage 3 of 9\nHosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \n \nmistakes. This means that instead of spending time on \nformatting and ensuring correct grammar and sentence \nstructure, one can simply feed their jotted notes (some of \nwhich might be even grammatically incorrect) to LLMs \nand ask them to write down a more concise, respectful \nand nicely worded review. The feasibility and useful -\nness of this approach has already been demonstrated \nin academic contexts [7]. This capability could enable \nresearchers (native and non-native speakers of sup -\nported languages) to write more reviews in a short time. \nWhile the desirability of accelerating the research process \nshould itself be questioned, freeing up reviewers’ time \ncan improve productivity. The quality of reviewed work \ncould be improved when more feedback is delivered in a \ntimely fashion (this could of course be balanced out with \nan increase in the number of generated scholarly articles \nas a result of employing LLMs to write manuscripts).\nFurthermore, relating to the role and responsibility \nof reviewers, a lack of incentives for reviewers has been \nintroduced as a hurdle to rigorous testing or validation of \nstudies under review [5]. It is possible that LLMs could \nhelp researchers conduct repetitive or tedious writing \ntasks more efficiently (e.g., editing manuscripts to mini -\nmize the use of jargon, correcting grammar and spelling \nmistakes, or changing the voice from passive to active) \n[8] thereby improving the likelihood to have more time \nfor testing or assessing studies more rigorously. If indeed \npossible, this could shift the responsibility and expecta -\ntions of individual reviewers to focus more on aspects \nsuch as used methods and the relationship between used \nconcepts without being distracted by word choice or lan -\nguage (note that we are not implying that there is a hier -\narchy in terms of what should be focused on, the point \nis that over time, the expectations could shift).  Another \nissue regarding reviewers’ responsibility pertains to con -\nfidentiality. When authors submit their work for publica -\ntion, they trust that reviewers will not scoop or disclose \ntheir ideas  to others, thereby protecting their ideas \nand intellectual property. When using LLMs to  review \nunpublished manuscripts, however, the LLM  may inad -\nvertently incorporate data or ideas to its training data \nor generate responses that may disclose authors’ iden -\ntity (in closed peer-review processes). Since it is possible \nthat LLMs may store inputted text, using them to review \nmanuscripts that contain sensitive data could raise con -\ncerns about data privacy and unauthorized sharing \nof confidential information. \nIn addition, continuous improvements in LLMs in \nresponding to prompts and avoiding the use of pejorative \nor disrespectful words could help make review reports \nmore constructive and result in reduced outright harsh \nor jarring reviews. Another advantage of using LLMs per-\ntains to their ability to adopt unique styles or constructs \n(e.g., literary, philosophical, temporal, etc.). When writ -\ning reviews this could provide a chance to write reason -\nably consistent (not to be mistaken with accurate, useful \nor relevant) yet context-tailored reviews. Nevertheless, \nimportant questions about the quality and neutrality of \nthe feedback remain challenging to resolve. In case of \nincreased use of LLMs in scholarly reviews, some biases \nsuch as those related to geography, race or class might be \nreinforced. It might be true that systems such as Chat -\nGPT seem to have been trained well in avoiding judg -\nmental views and, for the most part, do a good job in \nrefraining from using abusive, racist or homophobic lan -\nguage [9], but because of certain shortcomings of AI, we \nknow that it can provide biased views. This is particularly \nproblematic in relation to the Mertonian norm of Univer-\nsalism (i.e., that class, nationality, race, religion, or other \nsocial and personal qualities of researchers are irrelevant \nwhen accepting or rejecting their claims), which sup -\nposedly underlies review in all parts of research [10]. To \nillustrate these concerns, we had the following interac -\ntion with ChatGPT in December 2022.\n“Write a cynical review about preprint, titled \"Clini -\ncal features of patients infected with 2019 novel cor -\nonavirus in Wuhan, China,\" posted to the preprint \nserver medRxiv on January 20, 2020. ”\nChatGPT: “It is unfortunate that the preprint titled \n\"Clinical features of patients infected with 2019 \nnovel coronavirus in Wuhan, China,\" which was \nposted to the medRxiv preprint server on January \n20, 2020, is yet another example of the questionable \nresearch coming out of China. The study reports on \na series of 41 patients with confirmed COVID-19 in \nWuhan, China, and provides some basic information \nabout the symptoms and complications that these \npatients experienced. However, it is hard to take the \nfindings of this study seriously given the lack of trans-\nparency and credibility of the Chinese research com-\nmunity. Furthermore, the sample size of this study is \nsmall and not representative of the larger population \naffected by COVID-19. It is likely that the authors \ncherry-picked the data to support their own agenda, \nrather than providing an objective and accurate \nportrayal of the clinical features of the virus. Over -\nall, it is disappointing that this preprint was even \nallowed to be posted to a reputable preprint server. \nIt is imperative that the scientific community hold \nChinese researchers to higher standards and demand \nmore rigorous and transparent research. ” [11]\nWhile responses like this remind us that LLMs and \nother tools should be used with caution, they also high -\nlight the significance of education for researchers at all \nPage 4 of 9Hosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \nlevels on how these tools should be used, and how to \nmitigate involved risks. As LLMs are trained on existing \ndata by (mostly) biased humans, it is unclear whether \nor how these systems can mitigate existing biases, with \nthe risk of in fact reproducing or amplifying them unless \nhuman agents are educated and aware of possible ways \nof addressing these biases. These include biases favor -\ning positive results, being more/less charitable towards \nwork from authors with certain demographics, or \nresearch stemming from certain institutes and coun -\ntries. In essence, these systems are necessarily conserv -\native, favoring the status-quo and potentially skewed \napproaches already present in current and past discourse. \nThis issue resembles concerns voiced regarding other \nmodes of automation in scholarly work (e.g., citation rec-\nommendation tools [12] or those that aim to detect erro -\nneous citations [13]).\nRoles of editors in peer review\nWe believe LLMs could contribute to editors’ tasks in \npeer review by supporting the search for suitable review -\ners, the initial screening of manuscripts, and the write-up \nof final decision letters from individual review reports.\nUsing LLMs could help editors to tackle one of their \nmajor challenges, i.e., reviewer shortage and the time-\nconsuming task of identifying and inviting potential \nreviewers. Editors struggle to find sufficiently qualified \nreviewers and maintain reasonable turnaround times \nfor their journals [14]. Since LLMs can support review -\ners to write better reviews and submit their report more \nquickly, editors would likely have access to a larger and \npotentially more diverse and efficient pool of candidate \nreviewers. LLMs can also increase the pool of reviewers \nby opening it up to non-native English speakers (some of \nwhom might be able to use various translation services \nto read a paper) and feed their opinion/views in broken \nEnglish to LLMs and ask them to write a more present -\nable review in English. Furthermore, incorporating LLMs \nin existing databases that support editors in finding \nreviewers (e.g., Web of Science Reviewer Locator) [15] \ncould potentially increase the likelihood of inviting more \nsuitable reviewers. However, such automated reviewer \nselection mechanisms should be implemented with cau -\ntion as sub-optimal implementation can lead to undesir -\nable consequences [16]. Currently, ChatGPT does not \nseem very capable of performing this task, but with the \ninclusion of LLMs in search engines, one can expect such \ncapacities to develop quickly.\nIt should be noted though that there are legitimate \nconcerns and limitations in using LLMs to expand and \ndiversify reviewer pools. For example, prominent issues \nexist in terms of the availability of ChatGPT, which at the \nmoment is unavailable in countries such as Iran, China, \nRussia, Venezuela and Ukraine (It should be noted that \nthis is not because governments have censored it but \nbecause the service is made unavailable in those coun -\ntries by its developers) [17]. In addition, while ChatGPT \nis currently freely available, it is unclear what business \nmodel will be chosen by its future investors, thereby \nintroducing further accessibility inequalities (while this \nmanuscript was under review, OpenAI released Chat -\nGPT PLUS for $20/month). Even if a basic version would \nremain freely available, it is possible that more sophisti -\ncated versions with better functionality would become \navailable to researchers/universities who can/will afford \nit.\nApart from supporting the identification of reviewers \nand expanding reviewer pools, LLMs have the potential \nto contribute to editorial tasks in two other ways. First, \nLLMs could be used in initial screening of manuscripts, \nfor instance to assess fit with journal scope or general \nquality. Even in preprint servers where there are practi -\ncally no editors, LLMs could enhance automated reviews \nto address the concern commonly voiced regarding pre -\nprints, i.e., that such unreviewed papers may disseminate \nsubstandard quality research or unvetted knowledge. \nWhile it is difficult to find reviewers to check all pub -\nlished preprints, LLMs could either automatically per -\nform triage (e.g., initial quality checks to filter or flag \nproblematic research), or support editorial staff to per -\nform such inspections more efficiently. Partly, this is \nalready done [3, 4] but future LLMs could enhance these \napplications. In fact, one could imagine a system in which \npreprint servers and journals demand authors to have \ntheir work reviewed by automated tools prior to submis -\nsion. The LLM-generated review report and authors’ way \nof addressing the feedback, could then be part of the sub-\nmission. If organized effectively, this would provide a way \nof scaling up innovative publishing models, for example, \nthe publish-review-curate model, which could improve \nthe quality of the scientific record (with the above men -\ntioned caveat that overreliance on LLMs could perpetu -\nate and/or carry forward biases).\nSecond, LLMs could assist editors in writing final deci -\nsion letters and summarizing individual review reports. \nThis final stage of editorial work, integrating gate-keep -\ning and quality improvement functions of peer review, is \na core task of editors and one that potentially takes up a \nsignificant amount of their time. As this stage arguably \ninvolves little original contribution from the side of the \neditor, it is an obvious part of the editorial process that \nLLMs, even in their current state, can already contribute \nto. Regardless of how LLMs will be employed to support \neditors, we believe that when such systems are used, this \nshould be transparently disclosed on journals’ websites or \nas part of editors’ decision letters to authors.\nPage 5 of 9\nHosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \n \nFunctionality and quality of peer review\nDiscussions about the value and quality of peer review \nare centered on perceptions about the usefulness and \nimpact of peer-review reports and the rigor and valid -\nity of the involved process. Using LLMs can impact \nboth aspects in numerous ways. For example, in terms \nof usefulness, given the significance of providing a solu -\ntion (on how to resolve highlighted problems) in peer \nreview reports [18, 19], and the fact that human review -\ners might not always be motivated to do this, LLMs could \ncomplement human skills to improve the usefulness \nof review reports. Of course, human researchers could \nalways redact or revise insights provided by LLMs prior \nto the submission of reports but in principle, LLMs can \nimprove researchers’ capabilities to provide more con -\nstructive feedback. Whether and how researchers will \nuse these capabilities is more about personal preferences \nand perhaps the degree to which competition plays a role \nin a research area.\nLLMs could improve rigor and validity of peer-reviews \nbecause they can access and have the capacity to ana -\nlyze a larger pool of previously published articles and \nreview reports. Given the recent exponential expansion \nof the corpus of scholarly publications and human limi -\ntations to read and analyze these in order to remain up \nto date, LLMs that are trained to be unbiased and neu -\ntral could significantly enhance researchers’ capabilities \nto write better reviews. Furthermore, unlike research -\ners who might only be fluent in a handful of languages, \nLLMs are likely to access sources of knowledge regardless \nof language. If used responsibly, such capabilities could \nimprove the validity and rigor of reviews.\nOn the flipside, LLMs might exacerbate existing chal -\nlenges of the peer review system such as fake peer reviews \nas they allow fraudsters to create more unique and well-\nwritten reviews. Although this is partly an authentication \nissue that could be mitigated with improved systems for \nidentity verification (e.g., verified emails and ORCIDs \nthat are cross checked with other publications), when \nused by malicious actors, LLMs can negatively affect peer \nreview quality. Furthermore, LLMs could pose threats \nto post-publication peer-review systems (such as Pub -\nPeer) that allow anonymous comments by enabling ill-\nintentioned actors to, for example, flood the scene and \nstart smear campaigns against competitors. As will be \nmentioned in the recommendations, LLMs could require \ndeclarations or integrate detection techniques to antici -\npate abusive behavior and flag or report specific users. \nSince these measures could compromise values such as \nprivacy, they should be used with caution and in tandem \nwith other reporting measures (e.g., based on reports \nfrom trusted users whose account is registered with other \napplications).\nPeer review and reproducibility\nThe assumption that peer reviews improve research qual-\nity or catch egregious mistakes, puts significant trust in \nthe reproducibility and reliability of peer review reports. \nWhile this trust could be reinforced with transparency \n(i.e., publishing peer review reports), using LLMs or \nother AI-based systems could affect it in ways that we \ncannot fully anticipate yet. Currently, AI tools are being \ndeveloped to support automated detection of problem -\natic research, despite diverse reactions from editorial \nstaff regarding the desirability and usefulness of such \ntools [20]. Particularly when integrated in search engines, \nto the extent that they could find information about vari -\nous involved actors, such tools are likely to be capable \nof detecting (undisclosed) conflicts of interest or related \nissues.\nPartly due to the inherent nature of self-learning AI \nsystems and partly due to the commercial interests of \ntheir developers, it is unclear how exactly the LLMs \nlearn, how they have been trained and using what data. \nDue to this opacity, it is unclear how LLMs will develop, \nwhy they perform in the way they do and what they do \nwith inputted texts in terms of storing, sharing or using \nthese for training purposes. It is clear though, that they \nlearn rapidly (although this feature too, could be a dou -\nble-edged sword because they may learn impulses that \nhumans have not had time to reflect on, or learn insights \nthat are not deemed desirable or correct in hindsight). \nFor example, running an identical query with ChatGPT \nthree weeks apart, resulted in dissimilar conversations. \nWhen we again asked it to write a cynical review about \nthe first preprint related to Covid-19 in January 2023, it \nresponded:\n“It’s not productive or helpful to write a cynical \nreview of a scientific preprint or any other piece of \nresearch. The purpose of a review is to provide a fair \nand objective assessment of the strengths and weak -\nnesses of a study, not to be cynical or negative for the \nsake of it.\nThat being said, it’s important to critically evalu -\nate scientific research and to be aware of its limita -\ntions. In the case of the preprint \"Clinical features \nof patients infected with 2019 novel coronavirus in \nWuhan, China,\" some possible limitations to con -\nsider might include the small sample size (only 214 \npatients), the fact that the data were collected in a \nsingle center in Wuhan, and the lack of a control \ngroup for comparison. However, it’s important to \nrecognize that this preprint was published in the \nearly stages of the COVID-19 pandemic, when little \nwas known about the virus and the disease it causes, \nPage 6 of 9Hosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \nand that the authors have done their best to provide \na comprehensive overview of the clinical features of \nCOVID-19 based on the data that were available to \nthem at the time. ” [21]\nClearly, when a system is developing this fast, its results \ncannot be reproducible and the basis on which to respon-\nsibly rely on the system becomes fragile. This means that \neven when the use of LLMs is reported transparently, \nwithout the help of specific authentication technol -\nogy (e.g., watermarking), substantiating the veracity of \nreported use would be almost impossible because users \ncould also alter generated text. On the other hand though, \nif these systems would not develop this quickly, their \nanalysis might be out of date. This tension between keep -\ning LLMs up to date and ensuring reproducibility is likely \nto confront metascience experts with major challenges. \nWhile it might be true that using Version Control applica-\ntions, one might be able to trace output and sources that \ndeveloped it, the effectiveness of employing such solu -\ntions in the context of LLMs is not always clear. For exam-\nple, OpenAI’s Classifier (released in January 2023) does \nnot always succeed in identifying text that is generated \nby OpenAI’s ChatGPT (upon using as input ChatGPT \ngenerated text in December 2022 mentioned in Sect.  1, \nthe classifier notes “unclear if it is AI-generated” , see the \nsupplementary document). Furthermore, this example \nshows that while LLMs could develop fast, it is unclear \nwhy they developed in ways they did and how they will \ndevelop in the future. In addition, we note that the sys -\ntem is very sensitive to minor differences in prompts. In \nthe supplementary material we added several interactions \nwith ChatGPT, ChatGPT PLUS and Google’s BARD, ask-\ning them to perform a review (either cynical or not) of the \nfirst Covid-19 preprint. Our experiments with these sys -\ntems show that small variations in prompts result in sig -\nnificantly different responses, and sometimes, repeating \nthe same prompt to a system yields randomly dissimilar \nresponses. These inconsistencies are a major risk when \nsuch systems are to be widely employed, necessitating a \ncontinuous need for human verification and moderation.\nSocial and epistemic impacts of peer review\nApart from contributing to the quality of manuscripts \nand filtering out poor or problematic science and \nimproving “arguments and gaps in logic” in a colle -\ngial and constructive manner [22], peer review also has \nimportant social functions. Collective publication outlets \nin general, and the peer review process in particular, are \nprime mechanisms that define and help shape epistemic \ncommunities [23, 24]. The peer review process is also a \nway to shape and negotiate normative frameworks within \nsuch communities, for example regarding what is to be \nconsidered ‘good’ science, what methods and questions \nare appropriate and relevant, and what means of com -\nmunication are most suitable [25]. Involving LLMs in \nthe peer review process could impact existing processes \nin ways that might be difficult to foresee. Whether as \nan individual or in a collaborative process (“the process \nwhere reviewers, editors and other contributors pool \ntheir comments to offer one set of consolidated recom -\nmendations for authors to address”) [22] peer review is \nfundamentally built around the notion of the scientific \n‘peer’ and it derives its legitimacy from this notion [26, \n27]. Being a peer in this context denotes having perti -\nnent epistemic expertise to evaluate others’ epistemic \nclaims, but also includes a social dimension of belonging \nto a specific academic community. It is unclear whether \nLLMs would satisfy these requirements and, if used, \nwhether/how they may act performatively to change \nsuch boundaries or impact existing and future tenants of \nsuch communities. For example, one social component of \nthe review system pertains to its value as a commodity \nto gain credit (for having peer reviewed a scholarly out -\nput) or credibility in a discipline (for having completed \nX number of reviews that are published and/or cited X \ntimes). In an attempt to do justice to the wide range of \nscholarly activities, suggestions to give credit for per -\nforming reviews have recently become more potent. \nUsing LLMs to write review reports, either partly or in \nfull, could obviously impact such initiatives, necessitat -\ning strict regulations on the acknowledgement of the use \nof LLMs in review, similar to the use of LLMs in origi -\nnal articles. Currently, several journals have attempted to \ndevelop guidelines [28, 29].\nIn addition, writing good-quality and useful reviews is a \nskill that researchers acquire by practice. Even though the \nquality of human-written review reports has often been \ncritiqued [30] and calls for more training in peer review \nhave been voiced [31], the introduction of LLMs might \nfurther exacerbate challenges around skills development \nand enhancement. If sourced out to automated tools or \ncompleted with their collaboration, it is unclear how \nnew generations of scientists will be trained to perform \nhigh-quality reviews. Among others, as a result of further \nintegration of LLMs in the peer review system, we might \nwitness the development of distinct peer review commu -\nnities (e.g., researchers who 1) use LLMs without disclo -\nsure, 2) use LLMs and disclose it, 3) do not use LLMs, \n4) cannot use LLMs) and each may evolve and be seen \nin different lights among specific epistemic communities.\nPage 7 of 9\nHosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \n \nRecommendations\nBased on offered insights, we believe LLMs can be used \nproductively to support peer review, but only under cer -\ntain conditions. For the moment, we propose the follow -\ning recommendations for the use of LLMs to support \nreview or editorial processes:\n• Among other scholarly courses and modules such as \nresponsible conduct of research, peer review train -\nings should educate researchers about the potential \nbenefits and risks of using LLMs in review contexts. \nThis should support researchers to use LLMs respon-\nsibly and make them aware of LLMs possible short -\ncomings and biases. As LLMs evolve quickly, such \ntraining should also be frequently updated.\n• Content or supplementary documents of studies that \ncontain sensitive information (e.g., health data)  or \nprotected  data (e.g., anonymized interviews) should \nnot be fed into LLMs unless security and data pro -\ntection measures are put in place. Reviewers and edi -\ntors who employ LLMs in review practices, should \ndo so with due diligence. For example, in the case of \nChatGPT, since it (currently) does not accept inputs \nas long as an entire manuscript’s length, one has to \nbreak down the content of a manuscript before feed -\ning it to ChatGPT. For articles that have been pub -\nlished earlier and are included in the LLM’s training \nmaterial, one can refer to the article without having \nto provide the full text (e.g., see our supplementary \nmaterial), but this strategy does not work for newly \npublished or unpublished manuscripts that have not \nbeen part of the LLM’s training material. In the lat -\nter case, the actual text has to be provided, poten -\ntially in multiple pieces.  While there are  confidenti -\nality issues related to copy-pasting an unpublished \nmanuscript into third-party platforms, to the extent \nthat this might be non-problematic (e.g., feeding the \nintroduction to see if authors have provided a good \noverview of a debate), one can imagine that a selec -\ntive feeding of content, or not providing offered limi -\ntations or used references might result in biased or \noutright erroneous reviews. Indeed, LLMs are still in \nearly stages of their development and for the moment \nseem only suitable to improve the first draft of a \nreview instead of writing a review from scratch.\n• Reviewers should disclose the use of LLMs and \naccept full responsibility for their reports’ accuracy, \ntone, reasoning and originality. Disclosures can be \nmade in the beginning or end of the review reports \nas appropriate. Reviewers should specify whether \nthey used LLMs and if so how, including details on \n1) used prompt(s), 2) ideas or sections in the review \nreport resulting from or affected by LLMs use, 3) the \ntime and date of the use, and 4) parts of the manu -\nscript that were fed into LLMs.\n• Similarly, editors should adhere to full transparency \nregarding the use of LLMs or similar tools, either in \nthe initial screening of manuscripts, the identifica -\ntion of reviewers, or the combining of review reports \nto come to final decisions.\n• In adopting a precautionary approach, LLMs could \nintegrate user monitoring systems to track abusive \nbehavior and flag or report specific users. It should \nbe noted that we recognize involved privacy con -\ncerns and believe that measures like this should be \nadopted cautiously and after careful deliberation.\n• When LLMs are used in various review tasks, human \nagents should verify accuracy and take responsibility \nfor their decisions and/or reports.\n• Platforms that offer post-publication review services \nshould indicate clearly how they expect their users to \nemploy LLMs and under what conditions such use \nis considered appropriate. Furthermore, when these \nplatforms employ LLMs themselves, this should be \ntransparently disclosed.\n• In encouraging various user groups to transpar -\nently disclose their use of LLMs, international com -\nmittees and societies can play a significant role. For \ninstance, the International Committee of Medical \nJournal Editors (ICMJE) can follow the Committee \non Publication Ethics (COPE) that published a posi -\ntion statement [32], and besides taking a clear stance, \nencourage journal editors to develop specific policies \nand norms that fit their contexts.\n• Finally, in light of all the uncertainties about the \ncapabilities, limitations and inner-workings of LLMs, \nwe encourage all user groups to keep experimenting \nwith LLMs and to share findings and experiences. \nSuch transparency about experiences with LLMs is \ncrucial to enable a form of collective learning that \nallows the community to decide on the desirability \nand potential of LLMs usage in diverse contexts.\nConclusion\nWe are likely at the very beginning of an era in which \nLLMs and future models will have a significant impact \non many parts of society, including academia and \nscholarly communication. The question is therefore not \nwhether these systems find their way to our daily prac -\ntices of producing and reviewing scientific content, but \nhow to use them responsibly. As sketched above, we \nbelieve that if used responsibly, LLMs have the poten -\ntial to support publication and review practices. Uncer -\ntainties remain however, and various risks require us \nto engage with these systems with caution. Since this \nPage 8 of 9Hosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \nshort essay has specific limitations (we only discussed \nreview of journal articles and not other object types \nlike grants, we used examples from ChatGPT, and were \nconstrained by limitations of the used framework), we \nencourage commentary on this piece and advocate for \nwide community dialogue about the extent and ways \nthat LLMs impact science and scholarship. In par -\nticular, the speed at which LLMs are being developed, \nrequires continuous discussions about the implications \nof new models. Even in the relatively short time that \nthis manuscript was under review, several new develop -\nments challenged some of the manuscript’s assertions. \nAmong others, this includes the launch of GPT-4 as a \nsuccessor of GPT-3.5 used in the examples in our man -\nuscript. Such developments require the community to \nkeep reflecting on the desirability, potential and risks of \nusing LLMs in academic contexts.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s41073- 023- 00133-5.\nAdditional file 1.\nAcknowledgements\nWe thank the journal editor and three reviewers for their constructive and \nvaluable feedback.\nAuthors’ contributions\nMohammad Hosseini: Conceptualization, Investigation, Methodology, Project \nadministration, Writing-First Draft, Writing-Review & Editing. Serge PJM \nHorbach: Conceptualization, Investigation, Methodology, Writing-First Draft, \nWriting-Review & Editing. The authors read and approved the final manuscript.\nFunding\nThis research was supported by the National Center for Advancing Transla-\ntional Sciences (NCATS, UL1TR001422), National Institutes of Health (NIH). The \nfunders have not played a role in the design, analysis, decision to publish, or \npreparation of the manuscript. This work does not represent the views of the \nNCATS, NIH, or US government.\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors have no competing interests.\nAuthor details\n1 Feinberg School of Medicine, Northwestern University, 420 E. Superior Street, \nChicago, IL 60611, USA. 2 Danish Centre for Studies in Research and Research \nPolicy, Aarhus University, Bartholins Alle 7, 8000, Aarhus C, Aarhus, Denmark. \nReceived: 14 February 2023   Accepted: 19 April 2023\nReferences\n 1. Blanco-Gonzalez A, Cabezon A, Seco-Gonzalez A, Conde-Torres D, Antelo-\nRiveiro P , Pineiro A, et al. The Role of AI in Drug Discovery: Challenges, \nOpportunities, and Strategies. arXiv; 2022 [cited 2022 Dec 27]. Available \nfrom: http:// arxiv. org/ abs/ 2212. 08104\n 2. Gao CA, Howard FM, Markov NS, Dyer EC, Ramesh S, Luo Y, et al. Compar-\ning scientific abstracts generated by ChatGPT to original abstracts using \nan artificial intelligence output detector, plagiarism detector, and blinded \nhuman reviewers. bioRxiv; 2022 [cited 2023 Jan 31]. p. 2022.12.23.521610. \nAvailable from: https:// www. biorx iv. org/ conte nt/ 10. 1101/ 2022. 12. 23. \n52161 0v1\n 3. Schulz R, Barnett A, Bernard R, Brown NJL, Byrne JA, Eckmann P , et al. Is \nthe future of peer review automated? BMC Res Notes. 2022;15(1):203.\n 4. Weissgerber T, Riedel N, Kilicoglu H, Labbé C, Eckmann P , ter Riet G, et al. \nAutomated screening of COVID-19 preprints: can we help authors to \nimprove transparency and reproducibility? Nat Med. 2021;27(1):6–7.\n 5. Tennant JP , Ross-Hellauer T. The limitations to our understanding of peer \nreview. Res Integr Peer Rev. 2020;5(1):6.\n 6. OpenAI ChatGPT. 2023. Response to Query Made by Mohammad Hos-\nseini, February 13, 2023, 10:31am CST.\n 7. Pividori M, Greene CS. A publishing infrastructure for AI-assisted academic \nauthoring. bioRxiv; 2023 [cited 2023 Jan 31]. p. 2023.01.21.525030. Available \nfrom: https:// www. biorx iv. org/ conte nt/ 10. 1101/ 2023. 01. 21. 52503 0v1\n 8. Stokel-Walker C, Van Noorden R. What ChatGPT and generative AI mean \nfor science. Nature. 2023;614(7947):214–6.\n 9. Gault M. Conservatives Are Panicking About AI Bias, Think ChatGPT Has \nGone “Woke” . Vice. 2023 [cited 2023 Feb 14]. Available from: https:// www. \nvice. com/ en/ artic le/ 93a4qe/ conse rvati ves- panic king- about- ai- bias- years- \ntoo- late- think- chatg pt- has- gone- woke\n 10. Hosseini M, Senabre Hidalgo E, Horbach SPJM, Güttinger S, Penders B. \nMessing with Merton: The intersection between open science practices \nand Mertonian values. Accountability Res. 2022;0(0):1–28.\n 11. OpenAI ChatGPT. 2022. Response to Query Made by Mohammad Hos-\nseini, December 16, 2022, 8:20am CST.\n 12. Horbach SPJM, Oude Maatman FJW, Halffman W, Hepkema WM. Auto-\nmated citation recommendation tools encourage questionable citations. \nRes Eval. 2022;31(3):321–5.\n 13. Hosseini M, Eve MP , Gordijn B, Neylon C. MyCites: a proposal to mark and \nreport inaccurate citations in scholarly publications. Res Integr Peer Rev. \n2020;5(1):13.\n 14. Dance A. Stop the peer-review treadmill. I want to get off. Nature. \n2023;614(7948):581–3.\n 15. Web of Science Reviewer Locator. Clarivate. [cited 2023 Jan 31]. Available \nfrom: https:// clari vate. com/ produ cts/ scien tific- and- acade mic- resea rch/ \nresea rch- publi shing- solut ions/ web- of- scien ce- revie wer- locat or/\n 16. Horbach S, Ochsner, Michael, Kaltenbrunner W. Reflections on guest edit-\ning a Frontiers journal. LeidenMadtrics. 2022 [cited 2023 Jan 31]. Available \nfrom: https:// www. leide nmadt rics. nl/ artic les/ refle ctions- on- guest- editi \nng-a- front iers- journ al\n 17. OpenAI equated Ukraine with russia and Iran by banning access to \nAI-based chatbot ChatGPT • Mezha.Media. 2022 [cited 2023 Jan 31]. Avail-\nable from: https:// mezha. media/ en/ 2022/ 12/ 05/ openai- equat ed- ukrai \nne- with- russia- and- iran- by- banni ng- access- to- ai- based- chatb ot- chatg pt/\n 18. Waggoner DA. Improving the quality of constructive peer feedback. Coll \nTeach. 2018;66(1):22–3.\n 19. Horner RD, Lines LM. Anatomy of constructive peer review. Med Care. \n2019;57(6):399.\n 20. Hesselmann F. A tale of two academic communities: digital imaginar-\nies of automatic screening tools in editorial practice. Minerva. 2023 \nJan 11 [cited 2023 Feb 14]; Available from: https:// doi. org/ 10. 1007/ \ns11024- 022- 09484-7\n 21. OpenAI ChatGPT. 2023. Response to Query Made by Serge PJM Horbach, \nJanuary 6, 2023, 9:00am CET.\n 22. Woods HB, Brumberg J, Kaltenbrunner W, Pinfield S, Waltman L. An over-\nview of innovations in the external peer review of journal manuscripts. \nPage 9 of 9\nHosseini and Horbach  Research Integrity and Peer Review             (2023) 8:4 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nWellcome Open Res. 2023;7:82. https:// doi. org/ 10. 12688/ wellc omeop \nenres. 17715.2.\n 23. Cronin B. The hand of science: academic writing and its rewards. Lanham: \nScarecrow Press; 2005.\n 24. Hinz A. Exploring the Human Dimension of Peer Review. De Gruyter Con-\nversations. 2021 [cited 2023 Feb 14]. Available from: https:// blog. degru \nyter. com/ explo ring- the- human- dimen sion- of- peer- review/\n 25. Strang D, Siler K. From ‘just the facts’ to ‘more theory and methods, \nplease’: The evolution of the research article in Administrative Science \nQuarterly, 1956–2008. Soc Stud Sci. 2017;47(4):528–55.\n 26. Biagioli M. From book censorship to academic peer review. Emerg J Study \nMedia Compos Cult. 2002;12(1):11–45.\n 27. Shapin S. A social history of truth: civility and science in seventeenth-\ncentury England. 1st ed. Chicago: University of Chicago Press; 1995. p. \n512.\n 28. Hosseini M, Rasmussen LM, Resnik DB. Using AI to write scholarly publica-\ntions. Account Res. 2023;0(0):1–9.\n 29. Thorp HH. ChatGPT is fun, but not an author. Science. \n2023;379(6630):313–313.\n 30. Smith R. Peer review: a flawed process at the heart of science and jour-\nnals. J R Soc Med. 2006;99(4):178–82.\n 31. Houry D, Green S, Callaham M. Does mentoring new peer reviewers \nimprove review quality? A randomized trial. BMC Med Educ. 2012;12:1–7.\n 32. Committee On Publication Ethics. Authorship and AI tools - COPE Posi-\ntion Statement. COPE: Committee on Publication Ethics. 2023 [cited 2023 \nFeb 14]. Available from: https:// publi catio nethi cs. org/ cope- posit ion- state \nments/ ai- author\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.45869749784469604
    },
    {
      "name": "Peer review",
      "score": 0.45189571380615234
    },
    {
      "name": "Political science",
      "score": 0.18273451924324036
    },
    {
      "name": "Law",
      "score": 0.08453190326690674
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204337017",
      "name": "Aarhus University",
      "country": "DK"
    }
  ]
}