{
  "title": "Building Language Models for Text with Named Entities",
  "url": "https://openalex.org/W2798904449",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5104048507",
      "name": "Rizwan Parvez",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074368173",
      "name": "Saikat Chakraborty",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5064541855",
      "name": "Baishakhi Ray",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5087096372",
      "name": "Kai-Wei Chang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2057900969",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2728599219",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2561658355",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2951813108",
    "https://openalex.org/W2735651482",
    "https://openalex.org/W2064580901",
    "https://openalex.org/W1990365157",
    "https://openalex.org/W2740663516",
    "https://openalex.org/W2074529754",
    "https://openalex.org/W2963094819",
    "https://openalex.org/W2143960295",
    "https://openalex.org/W2754629507",
    "https://openalex.org/W2950075229"
  ],
  "abstract": "Text in many domains involves a significant amount of named entities. Predict- ing the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a discriminative language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evalu- ate the proposed model. Experimental re- sults show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than the state-of-the-art language models.",
  "full_text": "Building Language Models for Text with Named Entities\nMd Rizwan Parvez\nUniversity of California Los Angeles\nrizwan@cs.ucla.edu\nBaishakhi Ray\nColumbia University\nrayb@cs.columbia.edu\nSaikat Chakraborty\nUniversity of Virginia\nsaikatc@virginia.edu\nKai-Wei Chang\nUniversity of California Los Angeles\nkwchang@cs.ucla.edu\nAbstract\nText in many domains involves a signif-\nicant amount of named entities. Predict-\ning the entity names is often challenging\nfor a language model as they appear less\nfrequent on the training corpus. In this\npaper, we propose a novel and effective\napproach to building a discriminative lan-\nguage model which can learn the entity\nnames by leveraging their entity type in-\nformation. We also introduce two bench-\nmark datasets based on recipes and Java\nprogramming codes, on which we evalu-\nate the proposed model. Experimental re-\nsults show that our model achieves 52.2%\nbetter perplexity in recipe generation and\n22.06% on code generation than the state-\nof-the-art language models.\n1 Introduction\nLanguage model is a fundamental component in\nNatural Language Processing (NLP) and it sup-\nports various applications, including document\ngeneration (Wiseman et al., 2017), text auto-\ncompletion (Arnold et al., 2017), spelling correc-\ntion (Brill and Moore, 2000), and many others.\nRecently, language models are also successfully\nused to generate software source code written in\nprogramming languages like Java, C, etc. (Hin-\ndle et al., 2016; Yin and Neubig, 2017; Hel-\nlendoorn and Devanbu, 2017; Rabinovich et al.,\n2017). These models have improved the language\ngeneration tasks to a great extent, e.g., (Mikolov\net al., 2010; Galley et al., 2015). However, while\ngenerating text or code with a large number of\nnamed entities (e.g., different variable names in\nsource code), these models often fail to predict the\nentity names properly due to their wide variations.\nFor instance, consider building a language model\nfor generating recipes. There are numerous simi-\nlar, yet slightly different cooking ingredients (e.g.,\nolive oil, canola oil, grape oil, etc. —all are dif-\nferent varieties of oil). Such diverse vocabularies\nof the ingredient names hinder the language model\nfrom predicting them properly.\nTo address this problem, we propose a novel\nlanguage model for texts with many entity names.\nOur model learns the probability distribution over\nall the candidate words by leveraging the en-\ntity type information. For example, oil is the\ntype for named entities like olive oil, canola oil,\ngrape oil, etc. 1 Such type information is even\nmore prevalent for source code corpus written in\nstatically typed programming languages (Bruce,\n1993), since all the variables are by construct as-\nsociated with types like integer, ﬂoat, string, etc.\nOur model exploits such deterministic type in-\nformation of the named entities and learns the\nprobability distribution over the candidate words\nby decomposing it into two sub-components: (i)\nType Model. Instead of distinguishing the individ-\nual names of the same type of entities, we ﬁrst con-\nsider all of them equal and represent them by their\ntype information. This reduces the vocab size to\na great extent and enables to predict the type of\neach entity more accurately. (ii) Entity Composite\nModel. Using the entity type as a prior, we learn\nthe conditional probability distribution of the ac-\ntual entity names at inference time. We depict our\nmodel in Fig. 1.\nTo evaluate our model, we create two bench-\nmark datasets that involve many named entities.\nOne is a cooking recipe corpus2 where each recipe\ncontains a number of ingredients which are cate-\n1Entity type information is often referred as category in-\nformation or group information. In many applications, such\ninformation can be easily obtained by an ontology or by a\npre-constructed entity table.\n2 Data is crawled from http://www.ffts.com/\nrecipes.htm.\narXiv:1805.04836v1  [cs.CL]  13 May 2018\nplace\t\t\t\t\tproteins in\t\t\t\t\t\t\tcenter\t\t\t\tof\t\t\t\t\t\t\t\t\ta\t\t\t\t\t\t\t\t\tdish\t\t\twith\t\tvegetables on\t\t\t\t\teach\t\t\t\t\t\tside\t\t\t\t\t\t\t\t\t\t.\t\t\nplace\t\t\t\tchicken in\t\t\t\t\t\tcenter\t\t\t\t\tof\t\t\t\t\t\t\t\t\t\t\t\ta\t\t\t\t\t\t\t\t\tdish\t\t\t\t\twith\t\t\t\tbroccoli on\t\t\t\t\t\teach\t\t\t\t\t\tside\t\t\t\t\t\t\t\t.\t\t\nentity name\tw P(w|proteins) P(w)\nq chicken 0.43 0.35\tx\t0.43\nq beef 0.19 0.35\tx\t0.19\nq .. .. ..\nLanguage\tModel\t(type\tmodel)\nLanguage\tModel\t(entity\tcomposite\ttype\tmodel)\ntype P(type)\nq proteins 0.35\nq vegetables 0.11\nq .. ..\ntype P(type)\nq vegetables 0.52\nq fruits 0.22\nq .. ..\nentity name\tw P(w|vegetables) P(w)\nq broccoli 0.26 0.52\tx\t0.26\nq potatoes 0.21 0.52\tx\t0.21\nq .. .. ..\nFigure 1: An example illustrates the proposed model.\nFor a given context (i.e., types of context words as input),\nthe type model (in bottom red block) generates the type\nof the next word (i.e., the probability of the type of the\nnext word as output). Further, for a given context and\ntype of each candidate (i.e., context words, correspond-\ning types of the context words, and type of the next word\ngenerated by the type modelas input), the entity compos-\nite model (in upper green block) predicts the next word\n(actual entity name) by estimating the conditional proba-\nbility of the next word as output. The proposed approach\nconducts joint inference over both models to leverage type\ninformation for generating text.\ngorized into 8 super-ingredients (i.e., type); e.g.,\n“proteins”, “vegetables”, “fruits”, “seasonings”,\n“grains”, etc. Our second dataset comprises a\nsource code corpus of 500 open-source Android\nprojects collected from GitHub. We use an Ab-\nstract Syntax Tree (AST) (Parsons, 1992) based\napproach to collect the type information of the\ncode identiﬁers.\nOur experiments show that although state-of-\nthe-art language models are, in general, good to\nlearn the frequent words with enough training in-\nstances, they perform poorly on the entity names.\nA simple addition of type information as an ex-\ntra feature to a neural network does not guarantee\nto improve the performance because more features\nmay overﬁt or need more model parameters on the\nsame data. In contrast, our proposed method sig-\nniﬁcantly outperforms state-of-the-art neural net-\nwork based language models and also the models\nwith type information added as an extra feature.\nOverall, followings are our contributions:\n•We analyze two benchmark language corpora\nwhere each consists of a reasonable number\nof entity names. While we leverage an ex-\nisting corpus for recipe, we curated the code\ncorpus. For both datasets, we created auxil-\niary corpora with entity type information. All\nthe code and datasets are released.3\n•We design a language model for text consist-\ning of many entity names. The model learns\nto mention entities names by leveraging the\nentity type information.\n•We evaluate our model on our benchmark\ndatasets and establish a new baseline perfor-\nmance which signiﬁcantly outperforms state-\nof-the-art language models.\n2 Related Work and Background\nClass Based Language Models. Building lan-\nguage models by leveraging the deterministic\nor probabilistic class properties of the words\n(a.k.a, class-based language models) is an old\nidea (Brown et al., 1992; Goodman, 2001). How-\never, the objective of our model is different from\nthe existing class-based language models. The\nkey differences are two-folds: 1) Most existing\nclass-based language models (Brown et al., 1992;\nPereira et al., 1993; Niesler et al., 1998; Baker and\nMcCallum, 1998; Goodman, 2001; Maltese et al.,\n2001) are generative n-gram models whereas ours\nis a discriminative language model based on neu-\nral networks. The modeling principle and assump-\ntions are very different. For example, we can-\nnot calculate the conditional probability by statis-\ntical occurrence counting as these papers did. 2)\nOur approaches consider building two models and\nperform joint inference which makes our frame-\nwork general and easy to extend. In Section 4,\nwe demonstrate that our model can be easily in-\ncorporated with the state-of-art language model.\nThe closest work in this line is hierarchical neu-\nral language models (Morin and Bengio, 2005),\nwhich model language with word clusters. How-\never, their approaches do not focus on dealing\nwith named entities as our model does. A recent\nwork (Ji et al., 2017) studied the problem of build-\ning up a dynamic representation of named entity\nby updating the representation for every contextu-\nalized mention of that entity. Nonetheless, their\napproach does not deal with the sparsity issue and\ntheir goal is different from ours.\nLanguage Models for Named Entities. In\nsome generation tasks, recently developed lan-\nguage models address the problem of predict-\n3https://github.com/uclanlp/NamedEntityLanguageModel\ning entity names by copying/matching the entity\nnames from the reference corpus. For example,\nVinyals et al. (2015) calculates the conditional\nprobability of discrete output token sequence cor-\nresponding to positions in an input sequence. Gu\net al. (2016) develops a seq2seq alignment mech-\nanism which directly copies entity names or long\nphrases from the input sequence. Wiseman et al.\n(2017) generates document from structured table\nlike basketball statistics using copy and recon-\nstruction method as well. Another related code\ngeneration model (Yin and Neubig, 2017) parses\nnatural language descriptions into source code\nconsidering the grammar and syntax in the tar-\nget programming language (e.g., Python). Kid-\ndon et al. (2016) generates recipe for a given goal,\nand agenda by making use of items on the agenda.\nWhile generating the recipe it continuously moni-\ntors the agenda coverage and focus on increasing\nit. All of them are sequence-to-sequence learning\nor end-to-end systems which differ from our gen-\neral purpose (free form) language generation task\n(e.g., text auto-completion, spelling correction).\nCode Generation. The way developers write\ncodes is not only just writing a bunch of instruc-\ntions to run a machine, but also a form of com-\nmunication to convey their thought. As observed\nby Donald E. Knuth (Knuth, 1992), “The prac-\ntitioner of literate programming can be regarded\nas an essayist, whose main concern is exposition\nand excellence of style. Such an author, with the-\nsaurus in hand, chooses the names of variables\ncarefully and explains what such variable means. ”\nSuch comprehensible software corpora show sur-\nprising regularity (Ray et al., 2015; Gabel and\nSu, 2010) that is quite similar to the statistical\nproperties of natural language corpora and thus,\namenable to large-scale statistical analysis (Hindle\net al., 2012). (Allamanis et al., 2017) presented a\ndetailed survey.\nAlthough similar, source code has some unique\nproperties that differentiate it from natural lan-\nguage. For example, source code often shows\nmore regularities in local context due to common\ndevelopment practices like copy-pasting (Ghare-\nhyazie et al., 2017; Kim et al., 2005). This prop-\nerty is successfully captured by cache based lan-\nguage models (Hellendoorn and Devanbu, 2017;\nTu et al., 2014). Code is also less ambiguous than\nnatural language so that it can be interpreted by\na compiler. The constraints for generating cor-\nrect code is implemented by combining language\nmodel and program analysis technique (Raychev\net al., 2014). Moreover, code contains open vocab-\nulary—developers can coin new variable names\nwithout changing the semantics of the programs.\nOur model aims to addresses this property by\nleveraging variable types and scope.\nLSTM Language Model. In this paper, we use\nLSTM language model as a running example to\ndescribe our approach. Our language model uses\nthe LSTM cells to generate latent states for a\ngiven context which captures the necessary fea-\ntures from the text. At the output layer of our\nmodel, we use Softmax probability distribution to\npredict the next word based on the latent state.\nMerity et al. (2017) is a LSTM-based language\nmodel which achieves the state-of-the-art perfor-\nmance on Penn Treebank (PTB) and WikiText-\n2 (WT2) datasets. To build our recipe language\nmodel we use this as a blackbox and for our code\ngeneration task we use the simple LSTM model\nboth in forward and backward direction. A for-\nward directional LSTM starts from the beginning\nof a sentence and goes from left to right sequen-\ntially until the sentence ends, and vice versa. How-\never, our approach is general and can be applied\nwith other types of language models.\n3 A Probabilistic Model for Text with\nNamed Entities\nIn this section, we present our approach to build a\nlanguage model for text with name entities. Given\nprevious context ¯w = {w1,w2,..,w t−1}, the goal\nof a language model is to predict the probabil-\nity of next word P(wt|¯w) at time step t, where\nwt ∈Vtext and Vtext is a ﬁxed vocabulary set.\nBecause the size of vocabulary for named entities\nis large and named entities often occur less fre-\nquently in the training corpus, the language model\ncannot generate these named entities accurately.\nFor example, in our recipe test corpus the word\n“apple” occurs only 720 times whereas any kind of\n“fruits” occur 27,726 times. Existing approaches\noften either only generate common named entities\nor omit entities when generating text (Jozefowicz\net al., 2016).\nTo overcome this challenge, we propose to\nleverage the entity type information when model-\ning text with many entities. We assume each en-\ntity is associated with an entity type in a ﬁnite set\nof categories S = {s1,s2,..,s i,..,s k}. Given a\nword w, s(w) reﬂects its entity type. If the word\nis a named entity, then we denote s(w) ∈S; oth-\nerwise the type function returns the words itself\n(i.e, s(w) = w). To simplify the notations, we use\ns(w) ̸∈S to represent the case where the word is\nnot an entity. The entity type information given\nby s(w) is an auxiliary information that we can\nuse to improve the language model. We use s( ¯w)\nto represent the entity type information of all the\nwords in context ¯wand use wto represent the cur-\nrent word wt. Below, we show that a language\nmodel for text with typed information can be de-\ncomposed into the following two models: 1) atype\nmodel θt that predicts the entity type of the next\nword and 2) an entity composite model θv that pre-\ndicts the next word based on a given entity type.\nOur goal is to model the probability of next\nword wgiven previous context ¯w:\nP(w|¯w; θt,θv) , (1)\nwhere θt and θv are the parameters of the two\naforementioned models. As we assume the typed\ninformation is given on the data, Eq. (1) is equiv-\nalent to\nP(w,s(w)|¯w,s( ¯w); θt,θv) . (2)\nA word can be either a named entity or not;\ntherefore, we consider the following two cases.\nCase 1: next word is a named entity. In this\ncase, Eq. (2) can be rewritten as\nP(s(w) = s|¯w,s( ¯w); θt,θv) ×\nP(w|¯w,s( ¯w),s(w) = s; θv,θt) (3)\nbased on the rules of conditional probability.\nWe assume the type of the next token s(w) can\nbe predicted by a model θt using information of\ns( ¯w), and we can approximate the ﬁrst term in Eq.\n(3)\nP(s(w)|¯w,s( ¯w); θt,θv) ≈P(s(w)|s( ¯w),θt)\n(4)\nSimilarly, we can make a modeling assumption to\nsimplify the second term as\nP(w|¯w,s( ¯w),s(w),θv,θt)\n≈P(w|¯w,s( ¯w),s(w),θv). (5)\nCase 2: next word is not a named entity. In\nthis case, we can rewrite Eq. (2) to be\nP(s(w) ̸∈S|¯w,s( ¯w),θt) ×\nP(w|¯w,s( ¯w),s(w) ̸∈S,θv) . (6)\nThe ﬁrst term in Eq. (6) can be modeled by\n1 −\n∑\ns∈S\nP(s(w) = s|s( ¯w),θt),\nwhich can be computed by the type model4. The\nsecond term can be again approximated by (5) and\nfurther estimated by an entity composition model.\nTyped Language Model. Combine the afore-\nmentioned equations, the proposed language\nmodel estimates P(w|¯w; θt,θv) by\nP(w|¯w,s( ¯w),s(w),θv)×{\nP(s(w)|s( ¯w),θt) if s(w) ∈S\n(1−∑\ns∈S P(s(w)= s|s( ¯w),θt)) if s(w) ̸∈S\n(7)\nThe ﬁrst term can be estimated by anentity com-\nposite model and the second term can be estimated\nby a type model as discussed below.\n3.1 Type model\nThe type model θt estimates the probability of\nP(s(w)|s( ¯w),θt). It can be viewed as a lan-\nguage model builds on a corpus with all entities\nreplaced by their type. That is, assume the train-\ning corpus consists of x = {w1,w2,..,w n}. Us-\ning the type information provided in the auxiliary\nsource, we can replace each word w with their\ncorresponding type s(w) and generate a corpus of\nT = {s(wi),s(w2),..,s (wn)}. Note that if wi is\nnot an named entity (i.e., s(w) ̸∈S), s(w) = w\nand the vocabulary on T is Vtext ∪S.5 Any lan-\nguage modeling technique can be used in model-\ning the type model on the modiﬁed corpus T. In\nthis paper, we use the state-of-the-art model for\neach individual task. The details will be discussed\nin the experiment section.\n3.2 Entity Composite Model\nThe entity composite model predicts the next word\nbased on modeling the conditional probability\nP(w|¯w,s( ¯w),s(w),θv), which can be derived by\nP(w|¯w,s( ¯w); θv)∑\nws∈Ω(s(w)) P(ws|¯w,s( ¯w); θv), (8)\n4Empirically for the non-entity words, ∑\ns∈S P(s(w) =\ns|s( ¯w) ≈0\n5In a preliminary experiment, we consider putting all\nwords with s(w) ̸∈S in a category “N/A”. However, because\nmost words on the training corpus are not named entities, the\ntype “N/A” dominates others and hinder the type model to\nmake accurate predictions.\nwhere Ω(s(w)) is the set of words of the same type\nwith w.\nTo model the types of context word s( ¯w) in\nP(w|¯w,s( ¯w); θv), we consider learning a type\nembedding along with the word embedding by\naugmenting each word vector with a type vec-\ntor when learning the underlying word representa-\ntion. Speciﬁcally, we represent each word w as a\nvector of [vw(w)T ; vt(s(w))T ]T , where vw(·) and\nvt(·) are the word vectors and type vectors learned\nby the model from the training corpus, respec-\ntively. Finally, to estimate Eq. (8) using θv, when\ncomputing the Softmax layer, we normalize over\nonly words in Ω(s(w)). In this way, the condi-\ntional probability P(w|¯w,s( ¯w),s(w),θv) can be\nderived.\n3.3 Training and Inference Strategies\nWe learn model parameters θt and θv indepen-\ndently by training two language models type\nmodel and entity composite model respectively.\nGiven the context of type, type model predicts the\ntype of the next word. Given the context and the\ntype information of the all candidate words, en-\ntity composite model predicts the conditional ac-\ntual word (e.g., entity name) as depicted in Fig\n1. At inference time the generated probabilities\nfrom these two models are combined according to\nconditional probability (i.e., Eq. (7)) which gives\nthe ﬁnal probability distribution over all candidate\nwords6.\nOur proposed model is ﬂexible to any language\nmodel, training strategy, and optimization. As per\nour experiments, we use ADAM stochastic mini-\nbatch optimization (Kingma and Ba, 2014). In Al-\ngorithm 1, we summarize the language generation\nprocedure.\n4 Experiments\nWe evaluate our proposed model on two different\nlanguage generation tasks where there exist a lot of\nentity names in the text. In this paper, we release\nall the codes and datasets. The ﬁrst task is recipe\ngeneration. For this task, we analyze a cooking\nrecipe corpus. Each instance in this corpus is an\nindividual recipe and consists of many ingredi-\n6While calculating the ﬁnal probability distribution over\nall candidate words, with our joint inference schema, a strong\nstate-of-art language model, without the type information, it-\nself can work sufﬁciently well and replace the entity com-\nposite model. Our experiments using (Merity et al., 2017) in\nSection 4.1 validate this claim.\nAlgorithm 1: Language Generation\nInput: Language corpus\nX= {w1,w2,..,w n}, type s(w) of\nthe words, integer number m.\nOutput: θt, θv, {W1,W2,..,W m}\n1 Training Phase:\n2 Generate T = {s(w1),s(w2),..,s (wn)}\n3 Train type model θt on T\n4 Train entity composite model θv on Xusing\n[wi; s(wi)] as input\n5 Test Phase (Generation Phase):\n6 for i= 1 to mdo\n7 for w∈Vtext do\n8 Compute P(s(w)|s( ¯w),θt)\n9 Compute P(w|¯w,s( ¯w),s(w),θv)\n10 Compute P(w|¯w; θt,θv) using Eq.(7)\n11 end\n12 Wi ←argmaxwP(w|¯w; θt,θv)\n13 end\nents’. Our second task is code generation. We\nconstruct a Java code corpus where each instance\nis a Java method (i.e., function). These tasks are\nchallenging because they have the abundance of\nentity names and state-of-the-art language models\nfail to predict them properly as a result of insufﬁ-\ncient training observations. Although in this paper,\nwe manually annotate the types of the recipe in-\ngredients, in other applications it can be acquired\nautomatically. For example: in our second task of\ncode generation, the types are found using Eclipse\nJDT framework. In general, using DBpedia ontol-\nogy (e.g., “Berlin” has an ontology “Location”),\nWordnet hierarchy (e.g., “Dog” is an “Animal”),\nrole in sports (e.g., “Messi” plays in “Forward”;\nalso available in DBpedia7), Thesaurus (e.g., “re-\nnal cortex”, “renal pelvis”, “renal vein”, all are\nrelated to “kidney”), Medscape (e.g., “Advil” and\n“Motrin” are actually “Ibuprofen”), we can get the\nnecessary type information. As for the applica-\ntions where the entity types cannot be extracted\nautomatically by these frameworks (e.g., recipe in-\ngredients), although there is no exact strategy, any\nreasonable design can work. Heuristically, while\nannotating manually in our ﬁrst task, we choose\nthe total number of types in such a way that each\ntype has somewhat balanced (similar) size.\nWe use the same dimensional word embedding\n7 http://dbpedia.org/page/Lionel Messi\n(400 for recipe corpus, 300 for code corpus) to\nrepresent both of the entity name (e.g., “apple”)\nand their entity type (e.g., “fruits”) in all the mod-\nels. Note that in our approach, the type model\nonly replaces named entities with entity type when\nit generates next word. If next word is not a\nnamed entity, it will behave like a regular language\nmodel. Therefore, we set both models with the\nsame dimensionality. Accordingly, for the entity\ncomposite model which takes the concatenation of\nthe entity name and the entity type, the concate-\nnated input dimension is 800 and 600 respectively\nfor recipe and code corpora.\n4.1 Recipe Generation\nRecipe Corpus Pre-processing: Our recipe cor-\npus collection is inspired by (Kiddon et al., 2016).\nWe crawl the recipes from “Now Youre Cooking!\nRecipe Software” 8. Among more than 150,000\nrecipes in this dataset, we select similarly struc-\ntured/formatted (e.g, title, blank line then ingre-\ndient lists followed by a recipe) 95,786 recipes.\nWe remove all the irrelevant information (e.g., au-\nthor’s name, data source) and keep only two in-\nformation: ingredients and recipes. We set aside\nthe randomly selected 20% of the recipes for test-\ning and from the rest, we keep randomly selected\n80% for the training and 20% for the develop-\nment. Similar to (Kiddon et al., 2016), we pre-\nprocess the dataset and ﬁlter out the numerical\nvalues, special tokens, punctuation, and symbols.9\nQuantitatively, the data we ﬁlter out is negligible;\nin terms of words, we keep 9,994,365 words out\nof 10,231,106 and the number of ﬁlter out words\nis around ∼2%. We release both of the raw and\ncleaned data for future challenges. As the ingredi-\nents are the entity names in our dataset, we process\nit separately to get the type information.\nRetrieving Ingredient Type: As per our type\nmodel, for each word w, we require its type s(w).\nWe only consider ingredient type for our experi-\nment. First, we tokenize the ingredients and con-\nsider each word as an ingredient. We manually\nclassify the ingredients into 8 super-ingredients:\n“fruits”, “proteins”, “sides”, “seasonings”, “veg-\netables”, “dairy”, “drinks”, and “grains”. Some-\n8http://www.ffts.com/recipes.htm\n9For example, in our crawled raw dataset, we ﬁnd that\nsome recipes have lines like “===MMMMM===” which are\ntotally irrelevant to our task. For the words with numerical\nvalues like “100 ml”, we only remove the “100” and keep the\n“ml” since our focus is not to predict the exact number.\ntimes, ingredients are expressed using multiple\nwords; for such ingredient phrase, we classify\neach word in the same group (e.g., for “boneless\nbeef” both “boneless” and “beef” are classiﬁed as\n“proteins”). We classify the most frequent 1,224\nunique ingredients, 10 which cover 944,753 out\nof 1,241,195 mentions (top 76%) in terms of fre-\nquency of the ingredients. In our experiments,\nwe omit the remainder 14,881 unique ingredients\nwhich are less frequent and include some mis-\nspelled words. The number of unique ingredients\nin the 8 super ingredients is 110, 316, 140, 180,\n156, 80, 84, and 158 respectively. We prepare the\nmodiﬁed type corpus by replacing each actual in-\ngredient’s namewin the original recipe corpus by\nthe type (i.e., super ingredients s(w)) to train the\ntype model.\nRecipe Statistics: In our corpus, the total num-\nber of distinct words in vocabulary is 52,468;\nnumber of unique ingredients (considering split-\nting phrasal ingredients also) is 16,105; number\nof tokens is 8,716,664. In number of instances\ntrain/dev/test splits are 61,302/15,326/19,158. The\naverage instance size of a meaningful recipe is 91\non the corpus.\nConﬁguration: We consider the state-of-the art\nLSTM-based language model proposed in (Mer-\nity et al., 2017) as the basic component for build-\ning the type model , and entity composite model .\nWe use 400 dimensional word embedding as de-\nscribed in Section 4. We train the embedding for\nour dataset. We use a minibatch of 20 instances\nwhile training and back-propagation through time\nvalue is set to 70. Inside of this (Merity et al.,\n2017) language model, it uses 3 layered LSTM\narchitecture where the hidden layers are 1150 di-\nmensional and has its own optimization and reg-\nularization mechanism. All the experiments are\ndone using PyTorch and Python 3.5.\nBaselines: Our ﬁrst baseline is ASGD Weight-\nDropped LSTM (AWD LSTM) (Merity et al.,\n2017), which we also use to train our models (see\n’Conﬁguration’ in 4.1). This model achieves the\nstate-of-the-art performance on benchmark Penn\nTreebank (PTB), and WikiText-2 (WT2) language\ncorpus. Our second baseline is the same language\nmodel (AWD LSTM) with the type information\nadded as an additional feature (i.e., same as entity\ncomposite model).\n10We consider both singular and plural forms. The number\nof singular formed annotated ingredients are 797.\nModel Dataset V ocabulary Perplexity\n(Recipe Corpus) Size\nAWD LSTM original 52,472 20.23\nAWD LSTM modiﬁed type 51,675 17.62\ntype model\nAWD LSTM original 52,472 18.23\nwith type feature\nour model original 52,472 9.67\nTable 1: Comparing the performance of recipe gen-\neration task. All the results are on the test set of the\ncorresponding corpus. A WD LSTM (type model) is our\ntype modelimplemented with the baseline language model\nA WDLSTM (Merity et al., 2017). Our second baseline is\nthe same language model (A WDLSTM) with the type in-\nformation added as an additional feature for each word.\nResults of Recipe Generation. We compare\nour model with the baselines usingperplexity met-\nric—lower perplexity means the better prediction.\nTable 1 summarizes the result. The 3rd row shows\nthat adding type as a simple feature does not\nguarantee a signiﬁcant performance improvement\nwhile our proposed method signiﬁcantly outper-\nforms both baselines and achieves 52.2% improve-\nment with respect to baseline in terms of perplex-\nity. To illustrate more, we provide an example\nsnippet of our test corpus: “place onion and gin-\nger inside chicken . allow chicken to marinate for\nhour .”. Here, for the last mention of the word\n“chicken”, the standard language model assigns\nprobability 0.23 to this word, while ours assigns\nprobability 0.81.\n4.2 Code Generation\nCode Corpus Pre-processing. We crawl 500\nAndroid open source projects from GitHub 11.\nGitHub is the largest open source software forge\nwhere anyone can contribute (Ray et al., 2014).\nThus, GitHub also contains trivial projects like\nstudent projects, etc. In our case, we want to study\nthe coding practices of practitioners so that our\nmodel can learn to generate quality code. To en-\nsure this, we choose only those Android projects\nfrom GitHub that are also present in Google Play\nStore12. We download the source code of these\nprojects from GitHub using an off the shelf tool\nGitcProc (Casalnuovo et al., 2017).\nSince real software continuously evolves to\ncater new requirements or bug ﬁxes, to make our\nmodeling task more realistic, we further study dif-\n11https://github.com\n12https://play.google.com/store?hl=en\nferent project versions. We partition the codebase\nof a project into multiple versions based on the\ncode commit history retrieved from GitHub; each\nversion is taken at an interval of 6 months. For\nexample, anything committed within the ﬁrst six\nmonths of a project will be in the ﬁrst version,\nand so on. We then build our code suggestion\ntask mimicking how a developer develops code in\nan evolving software—based on the past project\nhistory, developers add new code. To implement\nthat we train our language model on past project\nversions and test it on the most recent version, at\nmethod granularity. However, it is quite difﬁcult\nfor any language model to generate a method from\nthe scratch if the method is so new that even the\nmethod signature (i.e., method declaration state-\nment consisting of method name and parameters)\nis not known. Thus, during testing, we only fo-\ncus on the methods that the model has seen before\nbut some new tokens are added to it. This is simi-\nlar to the task when a developer edits a method to\nimplement a new feature or bug-ﬁx.\nSince we focus on generating the code for ev-\nery method, we train/test the code prediction task\nat method level—each method is similar to a sen-\ntence and each token in the method is equivalent\nto a word. Thus, we ignore the code outside the\nmethod scope like global variables, class decla-\nrations, etc. We further clean our dataset by re-\nmoving user-deﬁned “String” tokens as they in-\ncrease the diversity of the vocabularies signiﬁ-\ncantly, although having the same type. For ex-\nample, the word sequences “Hello World!” and\n“Good wishes for ACL2018!!” have the same type\njava.lang.String.VAR.\nRetrieving Token Type: For every token win\na method, we extract its type information s(w).\nA token type can be Java built-in data types\n(e.g., int, double, ﬂoat, boolean etc.,) or user or\nframework deﬁned classes (e.g., java.lang.String,\nio.segment.android.ﬂush.FlushThread etc.). We\nextract such type information for each token by\nparsing the Abstract Syntax Tree (AST) of the\nsource code 13. We extract the AST type infor-\nmation of each token using Eclipse JDT frame-\nwork14. Note that, language keywords like for,\nif, etc. are not associated with any type. Next,\nwe prepare the type corpus by replacing the\n13AST represents source code as a tree by capturing its ab-\nstract syntactic structure, where each node represents a con-\nstruct in the source code.\n14https://www.eclipse.org/jdt/\nvariable names with corresponding type informa-\ntion. For instance, if variable var is of type\njava.lang.Integer, in the type corpus we replace\nvar by java.lang.Integer. Since multiple packages\nmight contain classes of the same name, we retain\nthe fully qualiﬁed name for each type15.\nCode Corpus Statistics: In our corpus, the\ntotal number of distinct words in vocabulary is\n38,297; the number of unique AST type (including\nall user-deﬁned classes) is 14,177; the number of\ntokens is 1,440,993. The number of instances used\nfor train and testing is 26,600 and 3,546. Among\nthese 38,297 vocabulary words, 37,411 are seen at\ntraining time while the rests are new.\nConﬁguration: To train both type model and\nentity composite model, we use forward and back-\nward LSTM (See Section 2) and combine them\nat the inference/generation time. We train 300-\ndimensional word embedding for each token as\ndescribed in Section 4 initialized by GLOVE (Pen-\nnington et al., 2014). Our LSTM is single lay-\nered and the hidden size is 300. We implement\nour model on using PyTorch and Python 3.5. Our\ntraining corpus size 26,600 and we do not split\nit further into smaller train and development set;\nrather we use them all to train for one single epoch\nand record the result on the test set.\nBaselines: Our ﬁrst baseline is standard LSTM\nlanguage model which we also use to train our\nmodules (see ‘Conﬁguration’ in 4.2). Similar to\nour second baseline for recipe generation we also\nconsider LSTM with the type information added\nas more features 16 as our another baseline. We\nfurther compare our model with state-of-the-art\ntoken-based language model for source code SLP-\nCore (Hellendoorn and Devanbu, 2017).\nResults of Code Generation: Table 2 shows\nthat adding type as simple features does not\nguarantee a signiﬁcant performance improvement\nwhile our proposed method signiﬁcantly outper-\nforms both forward and backward LSTM base-\nlines. Our approach with backward LSTM has\n40.3% better perplexity than original backward\nLSTM and forward has 63.14% lower (i.e., bet-\nter) perplexity than original forward LSTM. With\nrespect to SLP-Core performance, our model is\n22.06% better in perplexity. We compare our\nmodel with SLP-Core details in case study-2.\n15Also the AST type of a very same variable may differ in\ntwo different methods. Hence, the context is limited to each\nmethod.\n16LSTM with type is same as entity composite model.\nModel Dataset V ocabulary Perplexity\n(Code Corpus) Size\nSLP-Core original 38,297 3.40\nfLSTM original 38,297 21.97\nfLSTM [type model] modiﬁed type 14,177 7.94\nfLSTM with type feature original 38,297 20.05\nour model (fLSTM) original 38,297 12.52\nbLSTM original 38,297 7.19\nbLSTM [type model] modiﬁed type 14,177 2.58\nbLSTM with type feature original 38,297 6.11\nour model (bLSTM) original 38,297 2.65\nTable 2: Comparing the performance of code genera-\ntion task. All the results are on the test set of the corre-\nsponding corpus. fLSTM, bLSTM denotes forward and\nbackward LSTM respectively. SLP-Core refers to (Hel-\nlendoorn and Devanbu, 2017).\n5 Quantitative Error Analysis\nTo understand the generation performance of our\nmodel and interpret the meaning of the numbers\nin Table 1 and 2, we further perform the following\ncase studies.\n5.1 Case Study-1: Recipe Generation\nAs the reduction of the perplexity does not neces-\nsarily mean the improvement of the accuracy, we\ndesign a “ﬁll in the blank task” task to evaluate our\nmodel. A blank place in this task will contain an\ningredient and we check whether our model can\npredict it correctly. In particular, we choose six\ningredients from different frequency range (low,\nmid, high) based on how many times they have\nappeared in the training corpus. Following Table\nshows two examples with four blanks (underlined\nwith the true answer).\nExample ﬁll in the blank task\n1. Sprinkle chicken pieces lightly with salt.\n2. Mix egg and milk and pour over bread.\nWe further evaluate our model on a multiple\nchoice questioning (MCQ) strategy where the ﬁll\nin the blank problem remains same but the options\nfor the correct answers are restricted to the six in-\ngredients. Our intuition behind this case-study is\nto check when there is an ingredient whether our\nmodel can learn it. If yes, we then quantify the\nlearning using standard accuracy metric and com-\npare with the state-of-the-art model to evaluate\nhow much it improves the performance. We also\nmeasure how much the accuracy improvement de-\npends on the training frequency.\nTable 3 shows the result. Our model outper-\nforms the ﬁll in the blank task for both cases,\nAccuracy\nIngredient Train Freq. #Blanks Free-Form MCQ\nAWD LSTM Our AWD LSTM Our\nMilk 14, 136 4,001 26.94 59.34 80.83 94.90\nSalt 33,906 9,888 37.12 62.47 89.29 95.75\nApple 7,205 720 1.94 30.28 37.65 89.86\nBread 11,673 3,074 32.43 52.64 78.85 94.53\nTomato 12,866 1,815 2.20 35.76 43.53 88.76\nChicken 19,875 6,072 22.50 45.24 77.70 94.63\nTable 3: Performance of ﬁll in the blank task.\ni.e., without any options (free-form) and MCQ.\nNote that, the percentage of improvement is in-\nversely proportional to the training frequencies of\nthe ingredients—less-frequent ingredients achieve\na higher accuracy improvement (e.g., “Apple” and\n“Tomato”). This validates our intuition of learning\nto predict the type ﬁrst more accurately with lower\nvocabulary set and then use conditional probabil-\nity to predict the actual entity considering the type\nas a prior.\n5.2 Case Study-2: Code Generation\nProgramming language source code shows regu-\nlarities both in local and global context (e.g., vari-\nables or methods used in one source ﬁle can also\nbe created or referenced from another library ﬁle).\nSLP-Core (Hellendoorn and Devanbu, 2017) is a\nstate-of-the-art code generation model that cap-\ntures this global and local information using a\nnested cache based n-gram language model. They\nfurther show that considering such code structure\ninto account, a simple n-gram based SLP-Core\noutperforms vanilla deep learning based models\nlike RNN, LSTM, etc.\nIn our case, as our example instance is a Java\nmethod, we only have the local context. There-\nfore, to evaluate the efﬁciency of our proposed\nmodel, we further analyze that exploiting only the\ntype information are we even learning any global\ncode pattern? If yes, then how much in compar-\nison to the baseline (SLP-Core)? To investigate\nthese questions, we provide all the full project\ninformation to SLP-Core (Hellendoorn and De-\nvanbu, 2017) corresponding to our train set. How-\never, at test-time, to establish a fair comparison,\nwe consider the perplexity metric for the same\nmethods. SLP-Core achieves a perplexity 3.40\nwhere our backward LSTM achieves 2.65. This\nresult shows that appropriate type information can\nactually capture many inherent attributes which\ncan be exploited to build a good language model\nfor programming language.\n6 Conclusion\nLanguage model often lacks in performance to\npredict entity names correctly. Applications with\nlots of named entities, thus, obviously suffer. In\nthis work, we propose to leverage the type infor-\nmation of such named entities to build an effective\nlanguage model. Since similar entities have the\nsame type, the vocabulary size of a type based lan-\nguage model reduces signiﬁcantly. The prediction\naccuracy of the type model increases signiﬁcantly\nwith such reduced vocabulary size. Then, using\nthe entity type information as prior we build an-\nother language model which predicts the true en-\ntity name according to the conditional probability\ndistribution. Our evaluation and case studies con-\nﬁrm that the type information of the named entities\ncaptures inherent text features too which leads to\nlearn intrinsic text pattern and improve the perfor-\nmance of overall language model.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. We also thank Wasi Ud-\ndin Ahmad, Peter Kim, Shou-De Lin, and Paul\nMineiro for helping us implement, annotate, and\ndesign the experiments. This work was supported\nin part by National Science Foundation Grants IIS-\n1760523, CCF-16-19123, CNS-16-18771 and an\nNVIDIA hardware grant.\nReferences\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu,\nand Charles Sutton. 2017. A survey of machine\nlearning for big code and naturalness.arXiv preprint\narXiv:1709.06182 .\nKenneth C. Arnold, Kai-Wei Chang, and Adam Kalai.\n2017. Counterfactual language model adaptation for\nsuggesting phrases. In Proceedings of the Eighth In-\nternational Joint Conference on Natural Language\nProcessing, IJCNLP 2017. pages 49–54.\nL Douglas Baker and Andrew Kachites McCallum.\n1998. Distributional clustering of words for text\nclassiﬁcation. In Proceedings of the 21st annual in-\nternational ACM SIGIR conference on Research and\ndevelopment in information retrieval . ACM, pages\n96–103.\nEric Brill and Robert C Moore. 2000. An improved er-\nror model for noisy channel spelling correction. In\nProceedings of the 38th Annual Meeting on Associ-\nation for Computational Linguistics. Association for\nComputational Linguistics, pages 286–293.\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics 18(4):467–479.\nKim B Bruce. 1993. Safe type checking in a\nstatically-typed object-oriented programming lan-\nguage. In Proceedings of the 20th ACM SIGPLAN-\nSIGACT symposium on Principles of programming\nlanguages. ACM, pages 285–298.\nCasey Casalnuovo, Yagnik Suchak, Baishakhi Ray, and\nCindy Rubio-Gonz ´alez. 2017. Gitcproc: a tool for\nprocessing and classifying github commits. ACM,\npages 396–399.\nMark Gabel and Zhendong Su. 2010. A study of\nthe uniqueness of source code. In Proceedings of\nthe eighteenth ACM SIGSOFT international sympo-\nsium on Foundations of software engineering. ACM,\npages 147–156.\nMichel Galley, Chris Brockett, Alessandro Sordoni,\nYangfeng Ji, Michael Auli, Chris Quirk, Mar-\ngaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015.\ndeltableu: A discriminative metric for genera-\ntion tasks with intrinsically diverse targets. arXiv\npreprint arXiv:1506.06863 .\nMohammad Gharehyazie, Baishakhi Ray, and\nVladimir Filkov. 2017. Some from here, some\nfrom there: cross-project code reuse in github. In\nProceedings of the 14th International Conference\non Mining Software Repositories. IEEE Press, pages\n291–301.\nJoshua Goodman. 2001. Classes for fast maxi-\nmum entropy training. CoRR cs.CL/0108006.\nhttp://arxiv.org/abs/cs.CL/0108006.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor\nO. K. Li. 2016. Incorporating copying mech-\nanism in sequence-to-sequence learning. CoRR\nabs/1603.06393. http://arxiv.org/abs/1603.06393.\nVincent J. Hellendoorn and Premkumar Devanbu.\n2017. Are deep neural networks the best\nchoice for modeling source code? In Pro-\nceedings of the 2017 11th Joint Meeting on\nFoundations of Software Engineering . ACM, New\nYork, NY , USA, ESEC/FSE 2017, pages 763–773.\nhttps://doi.org/10.1145/3106237.3106290.\nAbram Hindle, Earl T. Barr, Mark Gabel, Zhendong\nSu, and Premkumar Devanbu. 2016. On the natu-\nralness of software. Commun. ACM 59(5):122–131.\nhttps://doi.org/10.1145/2902362.\nAbram Hindle, Earl T Barr, Zhendong Su, Mark Gabel,\nand Premkumar Devanbu. 2012. On the naturalness\nof software. In Software Engineering (ICSE), 2012\n34th International Conference on. IEEE, pages 837–\n847.\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A Smith. 2017. Dynamic entity\nrepresentations in neural language models. arXiv\npreprint arXiv:1708.00781 .\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410 .\nChlo´e Kiddon, Luke Zettlemoyer, and Yejin Choi.\n2016. Globally coherent text generation with neural\nchecklist models. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing. pages 329–339.\nMiryung Kim, Vibha Sazawal, David Notkin, and Gail\nMurphy. 2005. An empirical study of code clone ge-\nnealogies. In ACM SIGSOFT Software Engineering\nNotes. ACM, volume 30, pages 187–196.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR\nabs/1412.6980. http://arxiv.org/abs/1412.6980.\nDonald E Knuth. 1992. Literate programming. CSLI\nLecture Notes, Stanford, CA: Center for the Study of\nLanguage and Information (CSLI), 1992 .\nGiulio Maltese, P Bravetti, Hubert Cr´epy, BJ Grainger,\nM Herzog, and Francisco Palou. 2001. Combining\nword-and class-based language models: A compar-\native study in several languages using automatic and\nmanual word-clustering techniques. In Seventh Eu-\nropean Conference on Speech Communication and\nTechnology.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and Optimiz-\ning LSTM Language Models. arXiv preprint\narXiv:1708.02182 .\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nFrederic Morin and Yoshua Bengio. 2005. Hierarchi-\ncal probabilistic neural network language model. In\nAistats. Citeseer, volume 5, pages 246–252.\nThomas R Niesler, Edward WD Whittaker, and\nPhilip C Woodland. 1998. Comparison of part-\nof-speech and automatically derived category-based\nlanguage models for speech recognition. In Acous-\ntics, Speech and Signal Processing, 1998. Proceed-\nings of the 1998 IEEE International Conference on.\nIEEE, volume 1, pages 177–180.\nThomas W Parsons. 1992. Introduction to compiler\nconstruction. Computer Science Press New York.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing. pages 1532–1543.\nFernando Pereira, Naftali Tishby, and Lillian Lee.\n1993. Distributional clustering of english words. In\nProceedings of the 31st annual meeting on Associa-\ntion for Computational Linguistics . Association for\nComputational Linguistics, pages 183–190.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code genera-\ntion and semantic parsing. CoRR abs/1704.07535.\nhttp://arxiv.org/abs/1704.07535.\nBaishakhi Ray, Meiyappan Nagappan, Christian Bird,\nNachiappan Nagappan, and Thomas Zimmermann.\n2015. The uniqueness of changes: Characteristics\nand applications. ACM, MSR ’15.\nBaishakhi Ray, Daryl Posnett, Vladimir Filkov, and\nPremkumar Devanbu. 2014. A large scale study of\nprogramming languages and code quality in github.\nIn Proceedings of the 22nd ACM SIGSOFT Interna-\ntional Symposium on Foundations of Software Engi-\nneering. ACM, pages 155–165.\nVeselin Raychev, Martin Vechev, and Eran Yahav.\n2014. Code completion with statistical language\nmodels. In Acm Sigplan Notices. ACM, volume 49,\npages 419–428.\nZhaopeng Tu, Zhendong Su, and Premkumar Devanbu.\n2014. On the localness of software. In Proceed-\nings of the 22nd ACM SIGSOFT International Sym-\nposium on Foundations of Software Engineering .\nACM, pages 269–280.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In C. Cortes, N. D.\nLawrence, D. D. Lee, M. Sugiyama, and R. Gar-\nnett, editors, Advances in Neural Information\nProcessing Systems 28 , Curran Associates, Inc.,\npages 2692–2700. http://papers.nips.cc/paper/5866-\npointer-networks.pdf.\nSam Wiseman, Stuart M. Shieber, and Alexan-\nder M. Rush. 2017. Challenges in data-to-\ndocument generation. CoRR abs/1707.08052.\nhttp://arxiv.org/abs/1707.08052.\nPengcheng Yin and Graham Neubig. 2017. A\nsyntactic neural model for general-purpose\ncode generation. CoRR abs/1704.01696.\nhttp://arxiv.org/abs/1704.01696.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9578958749771118
    },
    {
      "name": "Computer science",
      "score": 0.8462650775909424
    },
    {
      "name": "Language model",
      "score": 0.7095636129379272
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6988021731376648
    },
    {
      "name": "Discriminative model",
      "score": 0.6883898377418518
    },
    {
      "name": "Natural language processing",
      "score": 0.6542012095451355
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6156589388847351
    },
    {
      "name": "Code (set theory)",
      "score": 0.5362936854362488
    },
    {
      "name": "Entity linking",
      "score": 0.5058898329734802
    },
    {
      "name": "Java",
      "score": 0.4806581735610962
    },
    {
      "name": "Programming language",
      "score": 0.4251837730407715
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Knowledge base",
      "score": 0.0
    }
  ],
  "institutions": []
}