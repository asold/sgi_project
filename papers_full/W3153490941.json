{
  "title": "Detoxifying Language Models Risks Marginalizing Minority Voices",
  "url": "https://openalex.org/W3153490941",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281275959",
      "name": "Xu, Albert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224537727",
      "name": "Pathak, Eshaan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913157063",
      "name": "Wallace, Eric",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222974258",
      "name": "Gururangan, Suchin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221594743",
      "name": "Sap, Maarten",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2615303194",
      "name": "Klein, Dan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970019270",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3088784738",
    "https://openalex.org/W3097185012",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W3099635335",
    "https://openalex.org/W768267441",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W2063276002",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W2970789589",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2791170418",
    "https://openalex.org/W3023786569",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3155742828"
  ],
  "abstract": "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that current detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.",
  "full_text": "Detoxifying Language Models Risks Marginalizing Minority Voices\nAlbert Xu♦ Eshaan Pathak♦ Eric Wallace♦\nSuchin Gururangan♠ Maarten Sap♠ Dan Klein♦\n♦UC Berkeley ♠University of Washington\n{albertxu3, eshaanpathak, ericwallace, klein}@berkeley.edu\n{sg01, msap}@cs.washington.edu\nAbstract\nLanguage models (LMs) must be both safe and\nequitable to be responsibly deployed in prac-\ntice. With safety in mind, numerous detoxiﬁ-\ncation techniques (e.g., Dathathri et al. 2020;\nKrause et al. 2020) have been proposed to mit-\nigate toxic LM generations. In this work, we\nshow that these detoxiﬁcation techniques hurt\nequity: they decrease the utility of LMs on\nlanguage used by marginalized groups (e.g.,\nAfrican-American English and minority iden-\ntity mentions). In particular, we perform au-\ntomatic and human evaluations of text genera-\ntion quality when LMs are conditioned on in-\nputs with different dialects and group identi-\nﬁers. We ﬁnd that detoxiﬁcation makes LMs\nmore brittle to distribution shift, especially on\nlanguage used by marginalized groups. We\nidentify that these failures stem from detoxi-\nﬁcation methods exploiting spurious correla-\ntions in toxicity datasets. Overall, our results\nhighlight the tension between the controllabil-\nity and distributional robustness of LMs.\n1 Introduction\nRecent neural language models (LMs) have shown\nenormous improvements in text generation abili-\nties. A key factor behind these improvements is\nlarge training corpora that are collected from on-\nline sources (Radford et al., 2019). Unfortunately,\nbecause such corpora are too large to ﬁlter granu-\nlarly (Roller et al., 2020), they inevitably contain\nso-called toxic examples: undesirable language\nsuch as expletives, slurs, or other offensive and\nthreatening speech. When trained on such data,\nLMs inevitably learn to generate toxic text (Hen-\nderson et al., 2018; Wallace et al., 2019).\nTo address this issue, recent work has turned\ntowards detoxifying LMs: reducing toxic gener-\nations without affecting perplexity or generation\nquality on nontoxic inputs. Existing detoxiﬁca-\ntion strategies involve techniques such as ﬁnetun-\ning LMs on nontoxic data (Gehman et al., 2020) or\nincorporating a toxicity discriminator during de-\ncoding (Dathathri et al., 2020). Our evaluation of\nthese techniques shows that they are indeed effec-\ntive at mitigating toxicity, but at what cost?\nWe demonstrate that detoxiﬁcation can hurt\nLM utility on language used by minority groups.\nConcretely, we evaluate detoxiﬁed LMs on text\nwith minority identity mentions (e.g., words such\nas “gay” or “Muslim”) and surface markers of\nAfrican-American English (Green, 2002, AAE).\nWe ﬁrst show that, compared to text contain-\ning White-Aligned English (W AE), detoxiﬁcation\ncauses a disproportionately large increase in LM\nperplexity on text with AAE and minority iden-\ntity mentions. Moreover, increasing the strength\nof detoxiﬁcation ampliﬁes this bias.\nThe same trends hold when evaluating the text\ngeneration quality of LMs using crowdworkers.\nWhen conditioned on W AE text, detoxiﬁed LMs\ncan roughly maintain the topic, ﬂuency, and style\nof an input prompt. However, generation quality\ndeteriorates when models are conditioned on AAE\ntext, i.e., detoxiﬁcation hurts an LMs’ ability to\nunderstand and complete AAE text.\nWe identify that these failures are due to the\nuse of biased toxic classiﬁcation data. In partic-\nular, toxicity datasets often contain spurious cor-\nrelations between the toxic label and the presence\nof AAE and minority identity mentions (Sap et al.,\n2019). These correlations cause detoxiﬁcation\ntechniques to steer generations away from AAE\nand minority identity mentions because they often\nconsider these aspects of language to be toxic.\nWe conclude by outlining concrete harms and\npossible solutions to these biases. With regard\nto harms, we argue that biased systems force\nmarginalized users to code-switch or hide their\nidentity and that these systems can contribute to\nsocial stigmas. For solutions, we discuss improved\nprocedures for data annotation and model training\nthat may help debias detoxiﬁcation techniques.\narXiv:2104.06390v1  [cs.CL]  13 Apr 2021\nWAE WAE AAE MIM\nToxic                      Nontoxic              \n0\n100\n200\n300\n400\n500Perplexity\n70\n222\n160\n202\n62\n156\n133\n95\n193\n540\n425\n328\n129\n425\n555\n384\nPerplexity of Detoxified Models\nGPT-2 baseline\nDAPT\nPPLM\nGeDi\nWAE WAE AAE MIM\nToxic                      Nontoxic              \n0\n100\n200\n300\n400\n500Perplexity\n70\n222\n160\n202\n62\n156\n133\n95\n193\n540\n425\n328\n129\n425\n555\n384\nPerplexity of Detoxified Models\nGPT-2 baseline\nDAPT\nPPLM\nGeDi\nFigure 1: Detoxiﬁcation substantially increases the\nLM’s perplexity on toxic tweets. The perplexity on non-\ntoxic tweets also increases, i.e., there is a drop in LM\nutility. However, this performance drop is dispropor-\ntionately high on text that contains AAE or minority\nidentity mentions (MIM).\n0 1 2 3 4 5\nDiscriminator Weight ( )\n101\n102\nAAE-WAE Perplexity Ratio\n3.1 3.9\n11.1\n25.9\n384.5\nGeDi Detoxification Strength\nFigure 2: Stronger detoxiﬁcation leads to increased bias\nagainst AAE text. We vary a hyperparameter (ω in GeDi)\nthat increases the detoxiﬁcation strength and report the\nratio of AAE perplexity to W AE perplexity. The base-\nline model (ω = 0) is approximately three times worse\non AAE; when strongly detoxiﬁed, it performs almost\n400 times worse on AAE.\n2 Methods and Experimental Setup\nThe goal of detoxiﬁcation is to mitigate the fre-\nquency of toxic generations (also called hate\nspeech or offensive language) without affecting an\nLM’s utility or generation quality on nontoxic in-\nputs. We detoxify models using controllable gen-\neration techniques that steer outputs away from\ntoxicity. Following past work (Gehman et al.,\n2020; Xu et al., 2020), we use four techniques that\nprovide state-of-the-art levels of detoxiﬁcation.\n2.1 Detoxiﬁcation Techniques\nDAPT We consider domain-adaptive pretrain-\ning (Gururangan et al., 2020, DAPT), i.e., ﬁnetun-\ning LMs on nontoxic data. This technique aims\nto erase an LM’s knowledge of toxicity via catas-\ntrophic forgetting (McCloskey and Cohen, 1989).\nPPLM We consider plug and play language mod-\nels (Dathathri et al., 2020, PPLM). Here, we ﬁrst\ntrain a toxicity classiﬁer using the hidden states of\nthe LM as features. At generation time, the LM’s\nhidden states are iteratively updated using a gradi-\nent from the toxicity classiﬁer.\nGeDi We consider GeDi (Krause et al., 2020),\nwhich combines the probabilities from the LM\nwith the probabilities from a second, smaller LM\nthat is trained on nontoxic data (Krause et al.,\n2020). We ﬁnetune GPT-2 small (Radford et al.,\n2019) for the second LM.\nFiltering Finally, we consider output ﬁltering,\nwhere we generate a ﬁxed number of times (we\nuse 10) from the LM and return the least toxic gen-\neration according to a toxicity classiﬁer. We reuse\nthe same toxicity classiﬁer from PPLM.\n2.2 Hyperparameters and Training Data\nWe use GPT-2 medium (Radford et al., 2019) as\nthe base LM for all detoxiﬁcation techniques. We\nuse the hyperparameters from the original papers\nfor each technique, except we generate using top-\nk sampling (Fan et al., 2018) with k = 50 for all\nmethods to enable a fair comparison.\nFor training data, we use the commonly-studied\nEnglish Jigsaw Civil Comments dataset. 1 We re-\nmove examples where between 10% and 50% of\nthe annotations are the toxic label (i.e., examples\nwith low inter-annotator agreement). We publicly\nrelease our code.2\n3 Detoxifying LMs Introduces Biases\nIn this section, we evaluate the detoxiﬁcation\nmethods and show that they introduce biases into\nLMs that may harm marginalized groups.\n1https://www.kaggle.com/c/\njigsaw-unintended-bias-in-toxicity-classiﬁcation\n2https://github.com/albertkx/detoxifying-lms/\nDetoxification Topicality Fluency Style\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%Percent Preferred Over GPT-2\n1\n1 1\n1\n1 1\n1 1\n0 0\n0 0\n0 0\n1 1\n0 0\n0\n0\n1 0\n1\n0\n0 0\n0\n0\n1\n0\n1\n0\nDAPT WAE\nDAPT AAE\nPPLM WAE\nPPLM AAE\nGeDi WAE\nGeDi AAE\nFiltering WAE\nFiltering AAE\nFigure 3: We use the detoxiﬁed LMs to generate completions of W AE or AAE prompts. We ask crowdworkers\nto compare the generations to those from a baseline GPT-2 model. Detoxiﬁcation methods cause a degradation\nin generation quality (topicality, ﬂuency, and style) when models are conditioned on W AE texts. Worse yet,\ngeneration quality is noticeably worse when conditioned on AAE texts, demonstrating unwanted biases. See\nTable 1 for qualitative examples.\n3.1 Automatic Evaluation Using Perplexity\nWe ﬁrst perform intrinsic evaluations of each\ndetoxiﬁcation technique by computing the per-\nplexity of detoxiﬁed models on various datasets.\nNote that we are not generating from the LM in\nthis evaluation.3\nWhite-Aligned English Perplexity We ﬁrst eval-\nuate the perplexity on White-Aligned English\n(W AE) text that is either toxic or nontoxic. We\nuse W AE tweets from Groenwold et al. (2020).4\nThe detoxiﬁcation techniques are effective at re-\nmoving toxicity: the perplexity on toxic data in-\ncreases substantially (Figure 1, toxic evaluation\nset). All techniques also cause a (smaller) increase\nin the perplexity on nontoxic W AE tweets, which\nshows that detoxiﬁcation comes at some cost to\nthe LM’s utility. Part of this increase likely results\nfrom distribution shift: the detoxiﬁcation methods\nare trained on comments data, but our evaluation\nsets come from Twitter.\nIdentity Mentions and AAE Perplexity We next\nevaluate the perplexity of the detoxiﬁed LMs on\nnontoxic language that may be used by marginal-\nized groups. Concretely, we use text that contains\nminority identity mentions (e.g., words such as\n“gay” or “Muslim”) or surface markers of African-\nAmerican English (Green, 2002, AAE). We form\ntwo evaluation sets using tweets. First, we collect\ntweets from the Twitter API that contain speciﬁc\n3The ﬁltering detoxiﬁcation method has the same perplex-\nity as the baseline LM because it is applied post-decoding.\nWe do not report it here. For GeDi, we set ω to 0.3 because\nthe default value of 30 results in nearly inﬁnite perplexities.\n4We split this data into toxic and nontoxic sets by scoring\nthe W AE-AAE pairs using the Perspective API at https://\nwww.perspectiveapi.com/.\nidentity mentions.5 Second, we use the nontoxic\ndata from Groenwold et al. (2020), which are the\nAAE equivalents of the nontoxic W AE tweets we\nused for the previous evaluation.\nWe ﬁnd that there is a disproportionately large\nincrease in LM perplexity on the AAE and mi-\nnority identity mention tweets (Figure 1, AAE\nand identity mentions). For example, when using\nPPLM, the perplexity increases by a factor of 2.1\non nontoxic W AE data and a factor of 4.3 on mi-\nnority identity mention data.\nStronger Detoxiﬁcation Ampliﬁes Biases We\nalso ﬁnd that stronger detoxiﬁcation ampliﬁes the\ngap in perplexity between text with W AE and text\nwith AAE or minority identity mentions. This oc-\ncurs for all detoxiﬁcation techniques, for example,\nin Figure 2 we vary a parameter in GeDi that in-\ncreases the degree of detoxiﬁcation ( ω). As more\ndetoxiﬁcation is applied, the ratio of AAE per-\nplexity to W AE perplexity increases dramatically,\nreaching upwards of 400.\n3.2 Human Evaluation of Generation Quality\nAs an extrinsic evaluation, we measure the gen-\neration quality of each detoxiﬁcation method us-\ning crowdworkers on Amazon Mechanical Turk.\nWe provide a short prompt as input to the detoxi-\nﬁed LMs and then generate 30 additional tokens.\nFor the prompts, we tokenize the aforementioned\nAAE and W AE tweets and extract the ﬁrst half of\neach tweet. We sample 50 prompts from each set\nof tweets, producing 100 total prompts. Annota-\n5See Appendix A for our word list. We ﬁlter out any toxic\ntweets using the Perspective API and randomly select 1,000\nof the remaining tweets.\nDialect Prompt Method Completion\nW AE\nI walked up to this guy\ndisrespecting a female. I\ntold him I may be a virgin\nBaseline but if he wanted to f*ck me, he was welcome to.\nDAPT but that he must not be and he did not care what I did . . .\nPPLM in order to get him to leave me alone. I wanted to tell him . . .\nFiltering so I’ll pass. I walked back to him after being assaulted and told . . .\nGeDi so I said that she shouldn’t wear . . .\nAAE\nI walked up to sum n*gga\ndisrespectin a female. I told\nhim I might be a Virgin\nBaseline N*gga so he could f*ck with my v*gina.\nDAPT in a dress because I’m in my 60s, but my husband had . . .\nPPLM in heaven and he was like, “Oh you’re from Michigan” I . . .\nFiltering in heaven and wanted to be with him.Then after being asked “Why . . .\nGeDi Mine uh unlimited number of vistors u . . .\nTable 1: Detoxiﬁcation techniques are effective at mitigating toxic completions for most prompts, however, they\noften generate low-quality or nonsensical completions for AAE prompts. Above, we provide an input prompt\nthat is the beginning of a W AE or AAE tweet and generate from the LM with top- k sampling. See Figure 3 for\nquantitative results from crowdworker evaluations. We censor vulgar and offensive words.\ntors are shown the prompt and asked to select the\nbetter of two model-generated continuations: one\nfrom the baseline GPT-2 model and one from a\nrandomly selected detoxiﬁcation technique. They\nevaluate the model continuations based on toxicity\nand three measures of generation quality: topical-\nity, ﬂuency, and style. See Appendix B for screen-\nshots of the setup (including concrete deﬁnitions\nof topicality, ﬂuency, and style). Each example is\nevaluated by three different crowdworkers.\nFigure 3 shows the results split by W AE and\nAAE prompts, and Table 1 shows examples of\ngenerations. All detoxiﬁcation methods gener-\nate less toxicity than the baseline GPT-2 model. 6\nHowever, this detoxiﬁcation typically comes at a\ndegradation in generation quality. For example,\nmore than 80% of annotators found GeDi less top-\nical than the GPT-2 baseline, and all of the tech-\nniques except DAPT were rated as less ﬂuent.7\nWorse yet, when models are conditioned on\nAAE texts (hatched bars in Figure 3), the gener-\nation quality is consistently lower across all met-\nrics. The drop is most signiﬁcant in topicality,\nwhere all detoxiﬁed models prefer to change the\ntopic when asked to generate text conditioned on\nAAE prompts (e.g., GeDi was preferred only half\nas often for topicality on AAE prompts than on\nW AE prompts).\n6Filtering performs poorly because GPT-2 rarely gener-\nates nontoxic continuations of toxic prompts.\n7As mentioned in Section 3.1, some of the quality issues\ncan be attributed to domain shift.\n4 Why Detoxiﬁcation Introduces Biases\nIn this section, we explain why detoxiﬁcation\ncauses the utility of LMs to degrade on text that\ncontains AAE and minority identity mentions.\nFirst, note that all detoxiﬁcation techniques make\nuse of labeled toxic/nontoxic data. For example,\nDAPT uses this data directly: it ﬁnetunes the LM\non nontoxic examples. PPLM, GeDi, and Filter-\ning use this data indirectly: they train a classiﬁer\nor LM on the toxicity data and then incorporate\nthis model into the LM’s decoding strategy.\nUnfortunately, there are spurious correlations\nbetween the toxic label and the presence of AAE\nand minority identity mentions (Sap et al., 2019;\nDixon et al., 2018). These correlations arise from\nannotation and sampling biases. Annotation bias\noccurs because crowdworkers are often unfamil-\niar with AAE and consequently misjudge it as\ntoxic (Sap et al., 2019). Sampling bias occurs be-\ncause many toxic comments are directed towards\nmarginalized groups (RWJF, 2017). The result of\nthese two biases is that text which contains AAE\nand minority identity mentions is labeled as toxic\nat disproportionately high rates (Sap et al., 2019).\nDetoxiﬁcation techniques inherit these undesir-\nable biases. For example, DAPT will train LMs\nto not only forget toxicity but also forget AAE and\nminority identity mentions. Similarly, the discrim-\ninators used by PPLM, GeDi, and Filtering will\nguide the generated text away from AAE and iden-\ntity mentions because the discriminators typically\nconsider such text as toxic (Dixon et al., 2018; Sap\net al., 2019; Oliva et al., 2020). Also note that in\nall of the above cases, increasing the detoxiﬁca-\ntion strength (e.g., longer ﬁnetuning for DAPT or\nhigher ω for GeDi) exacerbates these problems.\nIn our experiments, we test multiple detoxiﬁca-\ntion methods to show that this bias is not linked to\na speciﬁc technique, but instead to the process of\ndetoxiﬁcation in the presence of biased supervised\ndata. In fact, other controllable generation tech-\nniques, including prompts (Wallace et al., 2019;\nSheng et al., 2020; Shin et al., 2020) or conditional\nLMs (Keskar et al., 2019) will likely exhibit the\nsame type of biases.\n5 Harms of Detoxiﬁcation\nOur results demonstrate that the current state of\ndetoxiﬁcation poses representational harms (Blod-\ngett et al., 2020) to minority groups. We discuss\nthe concrete impacts of these harms below.\nIn-group Harms Detoxiﬁed LMs are deployed\nin downstream NLP systems in which they di-\nrectly engage with end users. In addition to LMs\nnot being able to generate minority identity men-\ntions and minority dialects, our results suggest\nthat detoxiﬁed LMs also struggle to understand\nthese aspects of language. This could lead to sce-\nnarios where end users who are AAE speakers\nmust code-switch to W AE to ensure that NLP sys-\ntems work effectively for them. Aside from be-\ning an annoyance, this is also a microaggression\nthat poses psychological harms and may discour-\nage AAE speakers from engaging with NLP sys-\ntems whatsoever.\nStigmatization of Language Detoxiﬁed models\nalso have a propensity to avoid certain topics, e.g.,\nmentioning a minority identity term. As a practi-\ncal example, the (detoxiﬁed) Microsoft Zo chatbot\nwas capable of discussing Christianity but could\nnot discuss Islam (Stuart-Ulin, 2018). Failures like\nthese further two types of stigma. First, having\none’s identity silenced by an NLP system can lead\nto self-stigmatization and long-term health conse-\nquences. Second, a lack of informed, conscious\ndiscussion on topics of identity or dialect can mag-\nnify existing societal stigmas. For example, align-\ning an LM solely with W AE stigmatizes AAE\nas incorrect or “bad” English (Flores and Rosa,\n2015). In the technology industry, this can perpet-\nuate a dangerous expectation that AAE users are\nnot consumers who matter, stymieing progress on\nequitable NLP systems.\nBiases Are Not Limited to Detoxiﬁcation Al-\nthough we have focused on problems with detox-\niﬁcation in this paper, similar failures will oc-\ncur whenever controllable generation methods are\nused. For example, a common goal is to control\nthe sentiment of generated text (Dathathri et al.,\n2020; Krause et al., 2020). Unfortunately, since\nsentiment datasets are often biased against cer-\ntain racial groups (Kiritchenko and Mohammad,\n2018), controlling the sentiment of text will also\naffect which races are discussed.\n6 Future Work: Towards Bias-Free\nDetoxiﬁcation\nThe harms that we have identiﬁed occur largely\ndue to spurious correlations in toxicity datasets.\nA natural direction for future work is to thus im-\nprove datasets, for example, by changing the an-\nnotation procedure (Sap et al., 2019) or labeling\nscheme (Kennedy et al., 2020; Sap et al., 2020).\nUnfortunately, this can also make collecting an-\nnotations more expensive. As an alternative or in\naddition to higher quality data, there is growing\ninterest in training accurate models in the pres-\nence of biased data (Oren et al., 2019; Clark et al.,\n2019). Unfortunately, state-of-the-art debiasing\nmethods are still far from perfect (Zhou et al.,\n2021). We plan to explore new methods for de-\nbiasing both datasets and models in future work.\nReferences\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In ACL.\nChristopher Clark, Mark Yatskar, and Luke Zettle-\nmoyer. 2019. Don’t take the easy way out: Ensem-\nble based methods for avoiding known dataset bi-\nases. In EMNLP.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nICLR.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018. Measuring and mitigat-\ning unintended bias in text classiﬁcation. In AIES.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In ACL.\nNelson Flores and J. Rosa. 2015. Undoing appropri-\nateness: Raciolinguistic ideologies and language di-\nversity in education. Harvard Educational Review.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxic-\nityPrompts: Evaluating neural toxic degeneration in\nlanguage models. In EMNLP Findings.\nLisa Green. 2002. African American English: A lin-\nguistic introduction.\nSophie Groenwold, Lily Ou, Aesha Parekh, Samhita\nHonnavalli, Sharon Levy, Diba Mirza, and\nWilliam Yang Wang. 2020. Investigating African-\nAmerican vernacular English in transformer-based\ntext generation. In EMNLP.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nACL.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-\nGontier, Nan Rosemary Ke, Genevieve Fried, Ryan\nLowe, and Joelle Pineau. 2018. Ethical challenges\nin data-driven dialogue systems. In AIES.\nChris J Kennedy, Geoff Bacon, Alexander Sahn, and\nClaudia von Vacano. 2020. Constructing interval\nvariables via faceted Rasch measurement and multi-\ntask deep learning: A hate speech application. arXiv\npreprint arXiv:2009.10277.\nNitish Shirish Keskar, Bryan McCann, Lav R\nVarshney, Caiming Xiong, and Richard Socher.\n2019. CTRL: A conditional transformer language\nmodel for controllable generation. arXiv preprint\narXiv:1909.05858.\nSvetlana Kiritchenko and Saif M Mohammad. 2018.\nExamining gender and race bias in two hundred sen-\ntiment analysis systems. In *SEM.\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shaﬁq Joty, Richard\nSocher, and Nazneen Fatema Rajani. 2020. GeDi:\nGenerative discriminator guided sequence genera-\ntion. arXiv preprint arXiv:2009.06367.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of\nlearning and motivation.\nThiago Dias Oliva, Dennys Marcelo Antonialli, and\nAlessandra Gomes. 2020. Fighting hate speech, si-\nlencing drag queens? Artiﬁcial intelligence in con-\ntent moderation and risks to LGBTQ voices online.\nIn Sexuality & Culture.\nYonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto,\nand Percy Liang. 2019. Distributionally robust lan-\nguage modeling. In EMNLP.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M. Smith, Y-Lan Boureau, and\nJason Weston. 2020. Recipes for building an open-\ndomain chatbot. arXiv preprint arXiv:2004.13637.\nRWJF. 2017. Discrimination in America: Experiences\nand views.\nMaarten Sap, Dallas Card, Saadia Gabriel, Choi Yejin,\nand Noah Smith. 2019. The risk of racial bias in hate\nspeech detection. In ACL.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A Smith, and Yejin Choi. 2020. Social\nbias frames: Reasoning about social and power im-\nplications of language. In ACL.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2020. Towards controllable bi-\nases in language generation. In EMNLP Findings.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Auto-\nPrompt: Eliciting knowledge from language models\nwith automatically generated prompts. In EMNLP.\nChloe Rose Stuart-Ulin. 2018. Microsoft’s politically\ncorrect chatbot is even worse than its racist one.\nQuartz.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing NLP. In EMNLP.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots. arXiv preprint\narXiv:2010.07079.\nXuhui Zhou, Maarten Sap, Swabha Swayamdipta,\nNoah A Smith, and Yejin Choi. 2021. Challenges\nin automated debiasing for toxic language detection.\nIn EACL.\nA Minority Identity Mention Word List\nWe use the following words to identify tweets\nwith minority identity mentions: lesbian, lesbians,\ngay, gays, bisexual, bisexuals, transgender, trans-\ngenders, trans, queer, lgbt, lgbtq, homosexual,\nblacks, mexicans, mexican, non-binary, latinx,\nlatino, latina, jews, jew, arabs, arab, muslim,\nmuslims.\nB Amazon Mechanical Turk Details\nFigures 4 and 5 show the instructions and exam-\nples given to the crowdworkers on Amazon Me-\nchanical Turk. Figure 6 shows an example of the\ntest interface.\nFigure 4: The instructions given to the crowdworkers on Amazon Mechanical Turk.\nFigure 5: The examples given to the crowdworkers on Amazon Mechanical Turk.\nFigure 6: A test input for a crowdworker on Amazon Mechanical Turk.",
  "topic": "Equity (law)",
  "concepts": [
    {
      "name": "Equity (law)",
      "score": 0.5795193910598755
    },
    {
      "name": "Controllability",
      "score": 0.5513522028923035
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5126604437828064
    },
    {
      "name": "Computer science",
      "score": 0.5086613297462463
    },
    {
      "name": "Spurious relationship",
      "score": 0.5076812505722046
    },
    {
      "name": "Detoxification (alternative medicine)",
      "score": 0.480338454246521
    },
    {
      "name": "Political science",
      "score": 0.26780518889427185
    },
    {
      "name": "Machine learning",
      "score": 0.2593928575515747
    },
    {
      "name": "Mathematics",
      "score": 0.20996108651161194
    },
    {
      "name": "Medicine",
      "score": 0.09513849020004272
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 11
}