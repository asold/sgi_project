{
  "title": "Multimodal music emotion recognition in Indonesian songs based on CNN-LSTM, XLNet transformers",
  "url": "https://openalex.org/W4308478537",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4308479242",
      "name": "Andrew Steven Sams",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A2911444256",
      "name": "Amalia Zahra",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A4308479242",
      "name": "Andrew Steven Sams",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A2911444256",
      "name": "Amalia Zahra",
      "affiliations": [
        "Binus University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2439669381",
    "https://openalex.org/W3209211564",
    "https://openalex.org/W2795014578",
    "https://openalex.org/W3183891373",
    "https://openalex.org/W4225620560",
    "https://openalex.org/W3145777983",
    "https://openalex.org/W3098740611",
    "https://openalex.org/W2949191678",
    "https://openalex.org/W3046491228",
    "https://openalex.org/W3023001428",
    "https://openalex.org/W4288633161",
    "https://openalex.org/W3022223935",
    "https://openalex.org/W3125348957",
    "https://openalex.org/W3148772742",
    "https://openalex.org/W4283740381",
    "https://openalex.org/W4225591880",
    "https://openalex.org/W2808471375",
    "https://openalex.org/W2171848217",
    "https://openalex.org/W2948490758",
    "https://openalex.org/W3110420963",
    "https://openalex.org/W2804220026",
    "https://openalex.org/W4225071736",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2534765427",
    "https://openalex.org/W4288050061",
    "https://openalex.org/W3186192207",
    "https://openalex.org/W4283734915",
    "https://openalex.org/W3094524767",
    "https://openalex.org/W3010236298",
    "https://openalex.org/W3019268159",
    "https://openalex.org/W2940096514",
    "https://openalex.org/W3024801493",
    "https://openalex.org/W3214127792",
    "https://openalex.org/W3062715998",
    "https://openalex.org/W1566337733",
    "https://openalex.org/W2806592685"
  ],
  "abstract": "Music carries emotional information and allows the listener to feel the emotions contained in the music. This study proposes a multimodal music emotion recognition (MER) system using Indonesian song and lyrics data. In the proposed multimodal system, the audio data will use the mel spectrogram feature, and the lyrics feature will be extracted by going through the tokenizing process from XLNet. Convolutional long short term memory network (CNN-LSTM) performs the audio classification task, while XLNet transformers performs the lyrics classification task. The outputs of the two classification tasks are probability weight and actual prediction with the value of positive, neutral, and negative emotions, which are then combined using the stacking ensemble method. The combined output will be trained into an artificial neural network (ANN) model to get the best probability weight output. The multimodal system achieves the best performance with an accuracy of 80.56%. The results showed that the multimodal method of recognizing musical emotions gave better performance than the single modal method. In addition, hyperparameter tuning can affect the performance of multimodal systems.",
  "full_text": "Bulletin of Electrical Engineering and Informatics \nVol. 12, No. 1, February 2023, pp. 355~364 \nISSN: 2302-9285, DOI: 10.11591/eei.v12i1.4231      355  \n \nJournal homepage: http://beei.org \nMultimodal music emotion recognition in Indonesian songs \nbased on CNN-LSTM, XLNet transformers \n \n \nAndrew Steven Sams, Amalia Zahra \nDepartment of Computer Science, BINUS Graduate Program, Master of Computer Science, Bina Nusantara University, \nJakarta, Indonesia \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Jun 11, 2022 \nRevised Sep 6, 2022 \nAccepted Oct 6, 2022 \n \n Music carries emotional information and allows the listener to feel the \nemotions contained in the music. This study proposes a multimodal music \nemotion recognition (MER) system using Indonesian song and lyrics data. In \nthe proposed multimodal system, the a udio data will use the mel \nspectrogram feature, a nd the lyrics feature will be extracted by going \nthrough the tokenizing process from XLNet. Convolutional long short term \nmemory network (CNN-LSTM) performs the audio classification task, while \nXLNet transformers performs the lyrics classification task. The outputs of \nthe two classification tasks are probability weight and actual prediction with \nthe value of positive, neutral, and negative emotions, which are then \ncombined using the stacking ensemble method. The combined output will be \ntrained into an artificial neural network  (ANN) model to get the best \nprobability weight output. The multimodal system achieves the best \nperformance with an accuracy of 80.56%. The results showed that the \nmultimodal method of re cognizing musical emotions gave better \nperformance than the single modal method. In addition, hyperparameter \ntuning can affect the performance of multimodal systems. \nKeywords: \nCNN-LSTM \nIndonesian song dataset \nMel spectrogram \nMultimodal \nMusic emotion recognition \nStacking ensemble method \nXLNet transformers \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nAndrew Steven Sams \nDepartment Computer Science, BINUS Graduate Program, Master of Computer Science \nBina Nusantara University \nWest Jakarta, Indonesia \nEmail: andrew.sams@binus.ac.id \n \n \n1. INTRODUCTION \nMusic is a cultural expression that has gone through a long history and is used to express a feeling \nor event. In today's era, music has become a lifestyle that can be listened to anywhere and anytime or a \nhelpful tool to reduce stress after a long day at work. In 2021, the world still suffered from the pandemic \nCOVID-19. Peoples prefer to do their activities at home and work at home to decrease the spread of the \npandemic. These activities boost the music industry because people tend to listen to music to k ill their \nboredom while doing their activities at home. Based on statistical data released by business of apps shows \nhow Spotify performed during the pandemic and how pandemic COVID-19 may affect the music industry . \nThe Spotify perfomance will be divide to  two figures, Figure 1( a) shows Spotify's profit during the  \nCOVID-19 pandemic era, and Figure 1(b) shows the people that start streamed music on Spotify. \nFigure 1(a) shows Spotify's profit rose around 16%, from 2.5 billion to 2.7 billion revenues during \nthe COVID-19 pandemic era, on Figure 1(b) shows up to 400 million people start streamed music on Spotify , \nwhich shows the Spotify peak performance in 2021 . The crisis may accelerate underlying trends in the music \nindustry. This fact shows the need to stream music, which has grown from 9% to 47% in just six years. \nMusic has various genres, including jazz, rock, pop, blues, RnB, metal, classic, country, hip -hop, and many  \n\n                ISSN: 2302-9285 \nBulletin of Electr Eng & Inf, Vol. 12, No. 1, February 2023: 355-364 \n356 \nmore. The exciting thing about music is that it can carry emotions and let the listeners feel the same \nemotions, affecting their mood  [1], involving 2 ,400 people who stated , “even music with sad melodies can \ncalm someone's feeling s”. Also, this study found that people tend to prefer listening to sad songs when they \nfeel a painful event such as a breakup  or are grieving the loss of a loved one. Songs can express our inner \nfeelings, produce goosebumps, bring us to tears, share an emo tional state with a composer or performer, or \ntrigger specific memories. Interest in a deeper understanding of the relationship between music and emotion \nhas motivated researchers from various areas of knowledge for decades  [2]. This proves that music can affect \nor improve a perso n's mood when given to people in the right mood. Music contains much human emotional \ninformation. Also,  Panda et al. [3] gave an example of music containing multiple emotional features. The \nemotional information can be extracted and can help identify basic emotions cont ained in the song through \nthe study of music emotion recognition (MER) [4]. Kumar and Saxena [5], they study the behavior of human \nwith bipolar probability which also count the survivability of the subject, MER study also happ en to study the \nbehavior of human that listen to the music or the creator of the music by studying the emotion that consists \noff the music. \nThe study of MER has recently gained attention, and several studies have served as the foundation \nfor this study. Th e study about lyrics -based MER achieved 94.7% accuracy using a deep neural network  \n(DNN) and the XLNet transformer model [6]. While [7] achieved 100% accuracy using convolutional long-\nshort term memory deep neural network (CLDNN). The architecture consists of convolutional neur al \nnetwork (CNN), long short term memory (LSTM), and DNN layers. With audio modal [8], using architecture \nstacked CNN, bidirectional gated recurrent unit  (BiGRU) achieved root mean square error (RMSE) of 0,01. \nA multimodal study by [9] uses CNN -LSTM to process 2D features, DNN to process 1D features, the \nstacking m ethod to ensemble results, and artificial neural network  ANN as the meta classifiers to achieve \n78.2% average accuracy. With different methods [10], using LSTM on audio, bidirectional encoder \nrepresentations from trans formers (BERT) transformer on lyrics, and using late fusion subtask merging  \n(LFSM) method achieve 79.62%. The problem common in the MER studies is the limitation to achieving \nhigher accuracy in detecting emotion just by using one music information variable  (like audio, lyrics, or \nvideo) [11]. Through the good results of MER, researchers try to achieve more significant results by \ncombining two or more datasets called the multimodal method. \nThe proposed method CNN -LSTM uses the same architecture as the simple LSTM as there are \nLSTM layers and dropout layers respectively throughout the entire architecture [12]. Talafha et al. [13] used \nthe LSTM layer as a decoder on RNN architecture, the proposed method uses the CNN layer and CNN -\nLSTM layer as a decoder on LSTM architecture [14], [15]. After each CNN layer, [16] uses the max pool \nlayer to reduce the overfitting/underfitting. Wi th the same goal, the proposed method CNN -LSTM uses a \ndropout layer after each CNN layer. The order of the layer in the proposed method are CNN layer as the \ninput layer and forget layer on LSTM architecture also, the CNN layer is the state of art in decodi ng images \nand other 2 dimensions or more data and is considered capable to decode the Mel spectrogram data used in \nthis paper. The second layer is the CNN -LSTM layer or known as the ConvLSTM2D layer in the keras \nlibrary, the CNN -LSTM layer is used as the o utput layer on the LSTM architecture. The last layer is the \ndropout layer function to reduce overfitting or underfitting. \nAfter researching journals and papers about MER, this paper was inspired by the [10] method, \nwhich combines the deep learning model for audio and transformers for the lyrics. The model used is the best \nresults study on audio MER and lyrics MER. Hence, the best result model, CNN -LSTM, which is also used \nas the audio classifier on [9], [14], [15] , and the XLNet transformers [6], is expected to achieve a better \nresult. The stacking method for ensembled the results, and simple ANN as the meta-classifier was used for \nthis experiment. This paper proposes a multimodal method for audio and lyrics data. The song dataset \nconsists of 476 audio and lyrics from Indonesian pop songs. The dataset will be grouped by positive, neutral, \nand negative. The other stud ent will share and annotate each song with Google form. The output emotion \nsuch as audio_positive, audio_negative, audio_neutral, text_positive, text_negative, text_neutral. There are \ntwo main contributions this paper can propose such as: \nCombine both lyri cs and audio features from the Indonesian song dataset to fit the lack of \nclassification performance. We still see the potential in this experiment because the previous study still uses \nonly one feature. Combining these two features into one or multimodal method will create opportunities for \nthis experiment to achieve higher results. The model used in this experiment is CNN -LSTM for the audio \nclassifier, XLNet transformers for the lyric classifier, and ANN for the meta-classifier. Lastly, to solve the \nproblem of two heterogeneous outputs [7], we proposed the stacking ensemble method, which fuses emotion \noutput from different m odalities by averaging both outputs and feeding them to the SoftMax layer. Besides \nsolving the heterogeneity problems, the fusion method significantly improves the accuracy of this multimodal \nMER research by achieving both audio and lyrics musical information. \nBulletin of Electr Eng & Inf  ISSN: 2302-9285  \n \nMultimodal music emotion recognition in Indonesian songs based on CNN-LSTM … (Andrew Steven Sams) \n357 \nIn summary, this paper proposes the solution of achieving higher results by using the multimodal \nmethod, the modal used in this paper are audio and lyrics. Different with the model that will be used in this \npaper is the best result achieved model from  the literature. CNN -LSTM will be used as the audio classifier \nand XLNet transformers as the lyric classifier. To ensemble both classifiers, a neural network is  used in this \npaper as the meta-classifier. The flow of this experiment will be shown in chapter 2. \n \n \n \n(a) \n \n \n(b) \n \nFigure 1. Spotify performance report for the 4th quarter of 2021 (a) Spotify profit has risen to approximately \n2.7 billion and (b) 400 million users have used Spotify \n\n                ISSN: 2302-9285 \nBulletin of Electr Eng & Inf, Vol. 12, No. 1, February 2023: 355-364 \n358 \n2. METHOD \nThis section discussed the proposed CNN -LSTM model as the audio classifier, XLNet transformers \nas the text classifier, and the ensemble method. For this experiment, the total audio dataset was 476 \nIndonesian songs in .wav format, while the lyric dataset wa s 476 Indonesian song lyrics. Both datasets have \nbeen annotated  by crowdsourcing method like [17], [18]  and divided into positive, neutral, and negative \nsubgroups. Then the audio data got trimmed and cleaned from the blank sounds, while the text got cleaned \nfrom special characters, punctuati on, and got lowercase. After the data got cleaned, the flow of audio MER \nincludes the data extracted for the Mel spectrogram features. The input data is split into 50% training data, \n25% testing data, and 25% validation data. Then input the audio data to t rain and validate the CNN -LSTM \nmodel, after the train is done the model is tested with the rest of the input data. The output is the matrix of \nemotion labels with the size of 476 ×3 and saved with the pickle library for the next process. While the text \nMER flow includes inputting the text to test the pre -trained model XLNet transformers which has been pre -\ntrained with the Indonesian language. The output is the matrix of emotion labels with the size of 476 ×3 and \nsaved with the pickle library for the next process. \nThe fusion method includes importing the text MER output and audio MER output from the pickle \nlibrary. Next, concatenate both output results to a matrix 476 ×6. After concatenating the data, the data got \nsplit into 70:30 training and testing data. Then  input the 333×6 training data matrix to the ANN networks to \ntrain the model. After training the model, the model gets tested with the rest of the data. The test result is a \n143×6 matrix that shows the result for each label, the last thing to sum up the re sults is to choose the best \nresult for each row of the test matrix, so the result can be concluded. The final result is the concluded \nemotion label for each song, which results in a 143 ×1 matrix. Figure 2 shows the flowchart of how the \nproposed multimodal system works. Figure 2 explains that each data will be trained in a separate classifier \nand concatenated afterward. Then both outputs will be trained and predicted with the meta classifier, and the \nhighest probability value will be picked as the final results. \n \n \n \n \nFigure 2. The multimodal system flowchart \n\nBulletin of Electr Eng & Inf  ISSN: 2302-9285  \n \nMultimodal music emotion recognition in Indonesian songs based on CNN-LSTM … (Andrew Steven Sams) \n359 \n2.1.  CNN-LSTM \nCNN-LSTM is a deep learning model that combines the CNN architecture with the LSTM layer and \nis designed to solve sequential prediction problems using spatial input such as images or videos [19], [20]. \nThe LSTM layer could learn the temporal information in the feature [12], while CNN is useful for analyzing \nimage data. The combination of CNN and LSTM was used to analyze images with time -domain information \n(e.g., songs, and videos) [21]. The Me l-spectrogram is one of the audio features that turns time domain \ninformation into a Mel -scale with the image's shape. The CNN -LSTM model is suitable for this experiment \nbecause the feature that wi ll be used on audio data is Mel -spectrogram, an image. The CNN -LSTM \narchitecture is illustrated in Figure 3. The architecture is made by stacking the convolution and CNN -LSTM \nlayers, respectively. \n \n \n \n \nFigure 2. The architecture of the proposed CNN-LSTM \n \n \nThe architecture consis ts of a convolutional layer, a CNN -LSTM layer, 3 dense layers , and the \noutput layer. A 2 ×3 kernel was used for the convolutional and CNN-LSTM layers. We used rectified linear \nunit (ReLU) for the convolutional, CNN-LSTM layers and the dense layer for the activation function. For the \noutput layer, SoftMax was used as the activation function. The Adam optimizer and the categorical cross -\nentropy loss function were also used. The network was implemented using the Keras framework.  \nTo conduct the audio -based MER experiment, we need to download the dataset from Google drive \nusing the Pydrive library. Then, we need to transfor m the audio signal into the Mel -spectrogram using the \nLibrosa librar y. Several studies also use Mel -spectrogram and achieved great results [4], [10], [22], [23] . \nAfter all the song data was turned into a spectrogram, all the data was grouped into positive, neu tral, and \nnegative and saved to a folder then uploaded to Google drive. Next, split the dataset into 60% training, 20% \ntest, and 20% validation data. The next thing is building the model by using the Keras library. The model was \nbuilt by stacking the CNN l ayer (with 8 and 32 filters) and CNN -LSTM layer (with 16 and 64 filters) \nsequentially. All the layers used ReLU activation and dropout layer 0.8 dropout layer help reduce the \noverfitting. It was then stacked with 128 and 92 filters dense layer as the outpu t layer. The output layer used \nSoftMax activation. An early stopping monitor and model checkpoint was used to save the best result to save \nand stop the model if it achieves the best result. The parameter that used: optimizer=Adam, loss \nfunction=categorical cross-entropy, batch size=4, epoch=11. After achieving the best result from the training \nprocess, the model predicts the test dataset that was prepared  early, consisting of a 109 Mel-spectrogram. The \noutput of model prediction is the probability value and  the true prediction value. The next step is saving the \nresult with the pickle library for later late fusion. \n \n2.2.  XLNet transformers \nXLNet transformers is an auto -regressive language model based on transformer XL and outputs the \ntoken's joint probability [24]. The probability is calculated with the permutation of word tokens in a sentence. \nThis is different from BERT, which uses [mask]  on a random token as the learning method. XLNet predicts \nthe word sequentially by calculating all possible permutations of each token. This way allows the model to \nget more textual information. The XLNet is advanced in any NLP task because these transform ers use the \ntwo-stream attention system [25]. \nThere are several steps for the text-based MER, but before that, the lyrics must be preprocessed. The \npreprocess such: clear the symbol or special characters, inlining the word, lowercase the capital word, and \nannotating the lyrics with positive, neutral, and negative. The n ext thing to do is list the artist, title, \nannotation, and lyrics in the comma separated values ( CSV) file [26]. Afterward, the CSV files were \n\n                ISSN: 2302-9285 \nBulletin of Electr Eng & Inf, Vol. 12, No. 1, February 2023: 355-364 \n360 \nuploaded to Google drive to ease the experiments. Next, download the CSV files from Google drive using the \nPydrive library and read the data using the Pandas library. The featur e that will be used is the emotion label \n(e.g., positive, neutral, and negative). Then, import the XLNet transformer tokenizer and model using the \ntransformer library. The imported XLNet model is from hugging face, titled 'malay -huggingface/xlnet-base-\nbahasa-cased. Next, feed the lyric to the tokenizer, so it returns the encodings, attention mask, and input id. \nAfter that, split the data to train, validate, and test data with the ratio of 50:25:25. The next thing to do was \ncreate the training loop, resize t he input shape, and save the model if it achieved the best accuracy. The \nhyperparameter used in experiment such as: batch size=4, epochs=3, optimizer adamW, learning rate=3e -5. \nAfter achieving the training output, feed the test data to the model for the pr ediction task. The prediction task \nshould be finished in about 10 minutes, and the output of the prediction is a probability and the true \nprediction value. After achieving the great result on prediction, the model prediction and the probability were \nsaved with the pickle library for multimodal fusion usage. \n \n2.3.  Stacking ensemble method \nStacking is one of the ensemble method techniques . These techniques aim to apply the output of a \nmulticlass model to generate a new model, this method is seen in these stu dies [27]–[32]. The ensemble \nmethod acts as a meta or sub -classifier for the basic cla ssifier's output, where the basic classifier's output has \nbeen trained for different tasks or features [33], [34] . The stacking method has achieved excellent \nperformance for image and text classification tasks where these statements also fulfill their task [9]. Also, the \nstacking ensemble method is stable and accurate. The model program is simple to implement, and there is no \nneed to adjust the previously constructed single modal classification model. This pape r used the SoftMax \nlayer as the meta-classifier for the experiment. According to Shinohara et al. [35] study results, LSTM and \nrecurrent neural network (RNN) architectures were functional model dependencies on a short or few -second \nscale. They also used SoftMax in one of the hidden layers as an activation function. In conclusion, the basic \nneuron network layer with SoftMax activation on the  output is suitable as the meta -classifier for our \nmulticlass classifier network. A multimodal stacking ensemble model, as shown in Figure 4.  \n \n \n \n \nFigure 3. The architecture for the meta-learner \n \n \nThe audio and text classifier output will be fused by concatenating both outputs into one i nput \nvector. The fusion vector will be used as input for the meta-classifier using the neuron networks model. The \nfirst step of the stacking method is to download the audio and lyrics output locally by using the pickle library. \nNext, concatenate the probab ility and true prediction value into one vector. This vector will act as the input \nfor the meta -classifier, which uses the neural network. The neural network uses two dense layers and the \noutput layer, which uses SoftMax as the activation function. Next, create the training loop and feed the fused \ndata to the model for the training process. Lastly, predict the data and measure the performance. The output is \nvaried from 0 to 1 for each label. To achieves the linear output, the highest value of the output wil l be present \nas the final output. \n\nBulletin of Electr Eng & Inf  ISSN: 2302-9285  \n \nMultimodal music emotion recognition in Indonesian songs based on CNN-LSTM … (Andrew Steven Sams) \n361 \n3. RESULTS AND DISCUSSION \nFor this research experiment, Google Collab is used as a development environment and is run using \nthe hosted server. The experiment would be divided into the audio -based MER, text -based MER, and fusi on \nmethod. The hyperparameter is tuned to achieve the best result. The batch size, epoch, and learning rate are \nthe hyperparameter used for tuning the text classifier. The batch size, epoch, and the number of channels used \nfor tuning the audio classifier. Lastly, the meta classifier used the number of channels. Tables 1-3 summarize \nthe result of hyperparameter tuning on the audio classifier, text classifier, and fusion method.  \n \n \nTable 1. The result of hyperparameter tuning on text-based MER \nNo. Batch size Learning rate Validation accuracy (%) \n1. 8 3e-5 57.88 \n2. 8 3e-4 52.38 \n3. 8 3e-3 44.76 \n4. 8 3e-2 39.64 \n5. 4 3e-5 58.39 \n6. 4 3e-4 50.17 \n7. 4 3e-3 43.92 \n8. 4 3e-2 51.07 \n \n \nTable 2. The results of hyperparameter tuning on audio-based MER \nNo. Filter Batch size Validation accuracy (%) \n1. 8,16,32,64 1 55.56 \n2. 16,32,64,128 1 48.15 \n3. 16,32,32,64 1 66.67 \n4. 8,16,16,32 1 55.56 \n5. 8,16,32,64 4 51.85 \n6. 16,32,64,128 4 55.56 \n7. 16,32,32,64 4 44.4 \n8. 8,16,16,32 4 44.4 \n \n \nTable 3. The result of hyperparameter tuning on fusion method \nNo. Filter Validation accuracy (%) \n1. 16,8 78 \n2. 32,16 80.56 \n3. 32,64 79.66 \n4. 64,128 75.2 \n \n \nIn Table 1, the text -based MER experiment using the XLNet model with hyperparameter settings: \nbatch size=4, epoch=3, and learning rate=3e -5 reached 58.88%. With hyperparameter tuning, the XLNet \nmodel increased accuracy by about 10%, increasing accuracy from 39.64 % to 58.8%. In Table 2, experiments \non the CNN -LSTM model using batch size=4, epoch=15, and the number of channels 8, 16, 16, and 32 \nsettings achieved the highest results with an accuracy of 66.67%. The results of hyperparameter tuning on \nCNN-LSTM and XLNe t transformers increase, although the increase in accuracy is not much because the \ndataset is still relatively small. The tuning hyperparameter is also applied to the ANN model. The \nhyperparameter used in the meta -classifier is the number of channels. The number of channels was chosen \nbecause this hyperparameter is quite easy to adjust, and the tuning results are immediately visible. In Table 3, \nthe best result of the hyperparameter tuning ensemble method is 80.56%, with channel numbers 32 and 16. \nThe order of layers used in the neural network model is the dense filter, dropout layer, dense filter, and output \nlayer. The optimizer used is stochastic gradient descent (SGD). \nThe conclusion from the hyperparameter tuning results, the ensemble method significantl y increases \nresults with hyperparameter tuning from the audio model and the text model. The highest result achieved a \nvalidation accuracy of 82.40%, which is classified as very good compared to this paper experiments with \naudio and text classifiers, which did not reach above 80%. This also proves that the multimodal method has \nachieved higher results than the audio and text classifiers. The only thing holding the experiment back is that \nthe dataset is too small, which causes the model not to learn well. Tab le 4 will also show the best result of \neach experiment. \nTable 4 shows the best result after getting the hyperparameter tuning. The proposed audio classifier \ngets 43.12% in test accuracy, lyrics get 58 .88% in test accuracy, and the fusion method gets 80 .56% in test \naccuracy. The fusion method achieves much higher results than audio classifiers or text classifiers. As in the \n[36] study, multi -modality outperforms single since the former has access to a better latent space \n                ISSN: 2302-9285 \nBulletin of Electr Eng & Inf, Vol. 12, No. 1, February 2023: 355-364 \n362 \nrepresentation. Tables 5 -7 show the comparison result for each experiment of this paper with the relevant \nMER experiments research. \n \n \nTable 4. The best result of each experiment \nNo. Method Val accuracy (%) Test accuracy (%) \n1. CNN-LSTM (Audio modal) 44.4 43.12 \n2. XLNet Transformers (Text modal) 58.39 58.88 \n3. Fusion Method 82.40 80.56 \n \n \nTable 5. The comparison result of this paper experiments with the audio MER research \nNo. Research Modal Model Dataset Test accuracy (%) \n1. [22] Audio CNN 774 songs 72.40 \n2. [14] Audio CNN-LSTM 361 Indonesian songs 58.33 \n3 [7] Audio CNN, LSTM, DNN 124 Turkish traditional songs 100 \n4. This paper Audio CNN-LSTM 476 Indonesian songs audio 43.12 \n \n \nTable 6. The comparison result of this paper experiments with the lyrics MER research \nNo. Research Modal Model Dataset Test accuracy (%) \n1. [37] Lyrics BiLSTM 2189 Lyrics 91.00 \n2. [6] Lyrics DNN + XLNet 2595 lyrics +180 songs 94.78 \n3. [3] Lyrics SVM 900 lyrics 74.8 \n4. This paper Lyrics XLNet Transformers 476 Indonesian song lyrics 58.88 \n \n \nTable 7. The comparison result of this paper experiments with the multimodal MER research \nNo. Research Modal Model Dataset Test accuracy (%) \n1. [9] Multimodal DNN + CNN-LSTM 2000 songs + 2000 lyrics 78.20 \n2. [10] Multimodal LSTM + BERT 1200 songs + 1200 lyrics 79.62 \n3. [23] Multimodal Deep convolution network 20000 songs 79.48 \n4. [29] Multimodal CNN-LSTM 1000 chinese songs 78.2 \n5. [31] Multimodal CNN + LSTM 1162 minnan songs 83 \n6. This paper Multimodal Proposed fusion method 476 songs + 476 lyrics 80.56 \n \n \nFrom Tables 5 and 6, it can be concluded that MER text experiments consistently achieve higher \naccuracy than MER audio. This happens because the complexity of Mel-spectrogram data from audio  is \nmuch higher. From Table 5, the highest results for audio were achieved by researc h from [22] got an \naccuracy of 72 .4%. In Table 6, the highest result for the lyric mode was achieved by [6], who obtained an \naccuracy of 94 .78%, and In Table 7, this paper achieved a similar multimodal result compared to the other \nmultimodal research with an accuracy of 80.56%, got slightly lower than [31] with the 83% accuracy. \nFor audio modal, this paper achiev ed an accuracy of 43 .12% and has not been able to outperform \nthe research of [22] with an accuracy of 72.4%. This is due to the lack of an audio dataset and an unbalanced \nmodel. Likewise, the lyric modal of this paper, with an accuracy of 58 .88%, has not been able to outperform \nthe research of [6] with an accuracy of 94 .78%. This is due to the possibility that the XLNet model which \nwas pre-trained was in Malay, considering that the dataset used wa s Indonesian, and the XLNet model was \ntrained in Malay. Meanwhile, for multimodal, this study obtained high results with an accuracy of 80.56% \nwhich can be seen in Table 7. This happens because this study proposes to use a state of art model with the \nhighest results from other research studies; besides that, hyperparameter tuning plays a vital role as a \ndeterminant of the final accuracy of the multimodal system even though the results are not maximized due to \ndataset limitations. \nIn conclusion, the method t hat this research proposes has achieved quite good results even though \nthe results obtained are less than optimal due to the small dataset. However, this study succeeded in \nenhancing and enriching the research of [14], which uses the same dataset with  an accuracy of 58 .33% to \n80.56%. This method also got similar results to current multimodal MER studies. \n \n \n4. CONCLUSION \nThis study introduces multimodal emotion recognition in Indonesian songs that use audio and lyric \nmodes. The model used in the audio -based MER is the CNN-LSTM model, while the lyrics-based MER uses \nBulletin of Electr Eng & Inf  ISSN: 2302-9285  \n \nMultimodal music emotion recognition in Indonesian songs based on CNN-LSTM … (Andrew Steven Sams) \n363 \nthe XLNet model. The stacking ensemble method combines the  output audio and text classifier results. The \nneural network is also used in this research function as a meta classifier that aims to obtain linear prediction \nresults.The multimodal method aims to enrich and improve the accuracy of the audio -based MER which used \nthe CNN-LSTM. After carrying out the testing process, the fusion method achieved an accuracy of 80.56% \nwhich resulted in a significant increase in results compared to the single modal MER study. The results of \nthis study are also competitive wit h the other multimodal MER method. Several areas must be improved in \nfuture research, such as try exploring other methods to improve the multimodal network, improving the text \nmodel with features such as part of speech (POS), or trying to improve the audio  model with CNN -LSTM \nnetworks. Asking the expert help to annotate emotions in music datasets or crowdsourcing methods, the last \nand most important is to add more audio data and lyrics data also, make sure the data is balanced on each \nlabel. \n \n \nACKNOWLEDGEMENTS \nThe author wanted to thank the Research and Technology Transfer Office (RTTO) Department at \nBina Nusantara University for financially supporting this paper. Also, thanks to Amalia Zahra for giving \nadvice and knowledge as the supervisor. \n \n \nREFERENCES \n[1] T. Eerola and H. R. Peltola, “Memorable experiences with sad music -reasons, reactions and mechanisms of three types of \nexperiences,” PLoS ONE, vol. 11, no. 6, 2016, doi: 10.1371/journal.pone.0157444. \n[2] J. S. G -Cañón et al. , “Music Emotion Recognition: Toward new, robust standards in personalized and context -sensitive \napplications,” IEEE Signal Processing Magazine, vol. 38, no. 6, pp. 106–114, Nov. 2021, doi: 10.1109/MSP.2021.3106232. \n[3] R. Panda, R. Malheiro, and R. P. Paiva, “Novel Audio Features for Music Emotion Recognition,” IEEE Transactions on Affective \nComputing, vol. 11, no. 4, pp. 614–626, Oct. 2020, doi: 10.1109/TAFFC.2018.2820691. \n[4] B. Xie, M. Sidulova, and C. H. Park, “Article robust multimodal emotion recognition from conversation with transformer -based \ncrossmodality the title fusion,” Sensors, vol. 21, no. 14, Jul. 2021, doi: 10.3390/s21144913. \n[5] Y. K. A. Kumar and A. K. Saxena, “Stochastic modelling of transition dynamic of mixed mood episodes in bipolar disorder,” \nInternational Journal of Electrical and Computer Engineering (IJECE) , vol. 12, no. 1, Feb. 2022, doi: \n10.11591/ijece.v12i1.pp620-629. \n[6] Y. Agrawal, R. G. R. Shanker, and V. Alluri, “Transfor mer-Based Approach Towards Music Emotion Recognition from Lyrics,” \nin Advances in Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 – April 1, \n2021, Proceedings, Part II, Berlin, Heidelberg, Mar. 2021, pp. 167–175, doi: 10.1007/978-3-030-72240-1_12. \n[7] S. Hizlisoy, S. Yildirim, and Z. Tufekci, “Music emotion recognition using convolutional long short term memory deep neural \nnetworks,” Engineering Science and Technology, an International Journal , vol. 24, no.  3, pp. 760 –767, Jun. 2021, doi: \n10.1016/j.jestch.2020.10.009. \n[8] R. Orjesek, R. Jarina, M. Chmulik, and M. Kuba, “DNN Based Music Emotion Recognition from Raw Audio Signal,” in 2019 \n29th International Conference Radioelektronika (RADIOELEKTRONIKA) , Apr. 2019, pp. 1 –4, doi: \n10.1109/RADIOELEK.2019.8733572. \n[9] C. Chen and Q. Li, “A Multimodal Music Emotion Classification Method Based on Multifeature Combined Network Classifier,” \nMathematical Problems in Engineering, vol. 2020, p. e4606027, Aug. 2020, doi: 10.1155/2020/4606027. \n[10] G. Liu and Z. Tan, “Research on Multi -modal Music Emotion Classification Based on Audio and Lyirc,” in 2020 IEEE 4th \nInformation Technology, Networking, Electronic and Automation Control Conference (ITNEC), Jun. 2020, vol. 1, pp. 2331–2335, \ndoi: 10.1109/ITNEC48623.2020.9084846. \n[11] L. Parisi, S.  Francia, S. Olivastri, and M. S. Tavella, “Exploiting Synchronized Lyrics and Vocal Features for Music Emotion \nDetection,” arXiv, Jan. 15, 2019, doi: 10.48550/arXiv.1901.04831. \n[12] M. A. Priatna and E. C. Djamal, “Precipitation prediction using recurrent  neural networks and long short -term memory,” \nTELKOMNIKA (Telecommunication Computing Electronics and Control) , vol. 18, no. 5, Oct. 2020, doi: \n10.12928/telkomnika.v18i5.14887. \n[13] B. Talafha, A. Abuammar, and M. Al-Ayyoub, “Atar: Attention-based LSTM for Arabizi transliteration,” International Journal of \nElectrical and Computer Engineering (IJECE), vol. 11, no. 3, Jun. 2021, doi: 10.11591/ijece.v11i3.pp2327-2334. \n[14] A. A. Wijaya, I. Yasmina, and A. Zahra, “Indonesian Music Emotion Recognition Based on A udio with Deep Learning \nApproach,” Advances in Science, Technology and Engineering Systems Journal Journal (ASTES) , vol. 6, no. 2, pp. 716 –721, \n2021, doi: 10.25046/aj060283. \n[15] J. S. Murthy, S. G. Matt, S. K. H. Venkatesh, and K. R. Gubbi, “A real -time q uantum-conscious multimodal option mining \nframework using deep learning,” IAES International Journal of Artificial Intelligence (IJ -AI), vol. 11, no. 3, Sep. 2022, doi: \n10.11591/ijai.v11.i3.pp1019-1025. \n[16] S. Bekhet, A. M. Alghamdi, and I. F. Taj -Eddin, “Gender recognition from unconstrained selfie images: a convolutional neural \nnetwork approach,” International Journal of Electrical and Computer Engineering (IJECE) , vol. 12, no. 2, Apr. 2022, doi: \n10.11591/ijece.v12i2.pp2066-2078. \n[17] K. Zhang, H. Zhang,  S. Li, C. Yang, and L. Sun, “The PMEmo Dataset for Music Emotion Recognition,” in Proceedings of the \n2018 ACM on International Conference on Multimedia Retrieval , New York, NY, USA, Jun. 2018, pp. 135 –142, doi: \n10.1145/3206025.3206037. \n[18] Y.-H. Yang, Y.-C. Lin, Y.-F. Su, and H. H. Chen, “A Regression Approach to Music Emotion Recognition,” IEEE Transactions \non Audio, Speech, and Language Processing, vol. 16, no. 2, pp. 448–457, Feb. 2008, doi: 10.1109/TASL.2007.911513. \n[19] T.-Y. Kim and S.-B. Cho, “Predicting residential energy consumption using CNN-LSTM neural networks,” Energy, vol. 182, pp. \n72–81, Sep. 2019, doi: 10.1016/j.energy.2019.05.230. \n[20] W. Lu, J. Li, Y. Li, A. Sun, and J. Wang, “A CNN -LSTM-Based Model to Forecast Stock Prices,” Complexity, vol. 2020, p. \ne6622927, Nov. 2020, doi: 10.1155/2020/6622927. \n                ISSN: 2302-9285 \nBulletin of Electr Eng & Inf, Vol. 12, No. 1, February 2023: 355-364 \n364 \n[21] Y.-H. Yang, Y.-C. Lin, H.-T. Cheng, I.-B. Liao, Y.-C. Ho, and H. H. Chen, “Toward Multi-modal Music Emotion Classification,” \nin Advances in Multimedia Information Processing -PCM 2008, Berli n, Heidelberg, 2008, pp. 70 –79, doi: 10.1007/978-3-540-\n89796-5_8. \n[22] T. Liu, L. Han, L. Ma, and D. Guo, “Audio -based deep music emotion recognition,” AIP Conference Proceedings, vol. 1967, no. \n1, p. 040021, May 2018, doi: 10.1063/1.5039095. \n[23] G. Tong, “Multimodal Music Emotion Recognition Method Based on the Combination of Knowledge Distillation and Transfer \nLearning,” Scientific Programming, vol. 2022, p. e2802573, Feb. 2022, doi: 10.1155/2022/2802573. \n[24] W. Rahman et al., “Integrating Multimodal In formation in Large Pretrained Transformers,” in Proceedings of the 58th Annual \nMeeting of the Association for Computational Linguistics, Online, Jul. 2020, pp. 2359–2369, doi: 10.18653/v1/2020.acl-main.214. \n[25] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. S alakhutdinov, and Q. V. Le, “XLNet: Generalized Autoregressive Pretraining for \nLanguage Understanding.” arXiv, Jan. 02, 2020, doi: 10.48550/arXiv.1906.08237. \n[26] C. V. Nanayakkara and H. A. Caldera, “Music Emotion Recognition With Audio and Lyrics Feature s,” International Journal of \nDigital Information and Wireless Communications (IJDIWC), vol. 6, no. 4, pp. 260–273, 2016. \n[27] M. Sharafi, M. Yazdchi, R. Rasti, and F. Nasimi, “A novel spatio -temporal convolutional neural framework for multimodal \nemotion re cognition,” Biomedical Signal Processing and Control , vol. 78, p. 103970, Sep. 2022, doi: \n10.1016/j.bspc.2022.103970. \n[28] P. Singh, R. Srivastava, K. P. S. Rana, and V. Kumar, “A multimodal hierarchical approach to speech emotion recognition from \naudio and text,” Knowledge-Based Systems, vol. 229, p. 107316, Oct. 2021, doi: 10.1016/j.knosys.2021.107316. \n[29] L. Zhang and Z. T ian, “Research on Music Emotional Expression Based on Reinforcement Learning and Multimodal \nInformation,” Mobile Information Systems, vol. 2022, p. e2616220, Jun. 2022, doi: 10.1155/2022/2616220. \n[30] P. Tzirakis, J. Chen, S. Zafeiriou, and B. Schuller, “E nd-to-end multimodal affect recognition in real -world environments,” \nInformation Fusion, vol. 68, pp. 46–53, Apr. 2021, doi: 10.1016/j.inffus.2020.10.011. \n[31] Z. Xiang, X. Dong, Y. Li, F. Yu, X. Xu, and H. Wu, “Bimodal Emotion Recognition Model for Minnan Songs,” Information, vol. \n11, no. 3, Mar. 2020, doi: 10.3390/info11030145. \n[32] C. Wu, F. Wu, Y. Chen, S. Wu, Z. Yuan, and Y. Huang, “Neural Metaphor Detecting with CNN -LSTM Model,” in Proceedings \nof the Workshop on Figurative Language Processing , New Orleans, Louisiana, Jun. 2018, pp. 110 –114, doi: 10.18653/v1/W18-\n0913. \n[33] K. Song and Y. -S. Kim, “An Enhanced Multimodal Stacking Scheme for Online Pornographic Content Detection,” Applied \nSciences, vol. 10, no. 8, Jan. 2020, doi: 10.3390/app10082943. \n[34] S. Chung, J. Lim, K. J. Noh, G. Kim, and H. Jeong, “Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity \nRecognition Using Deep Learning,” Sensors, vol. 19, no. 7, Jan. 2019, doi: 10.3390/s19071716. \n[35] V. Shinohara, J. Foleiss, and T. Tavares, “Comparing Meta -Classifiers for Automatic Music Genre Classification,” in Anais do \nSimpósio Brasileiro de Computação Musical (SBCM), Sep. 2019, pp. 131–135, doi: 10.5753/sbcm.2019.10434. \n[36] Y. Huang, C. Du, Z. Xu e, X. Chen, H. Zhao, and L. Huang, “What Makes Multi -modal Learning Better than Single (Provably).” \narXiv, Oct. 26, 2021, doi: 10.48550/arXiv.2106.04538. \n[37] J. Abdillah, I. Asror, and Y. F. A. Wibowo, “Emotion Classification of Song Lyrics using Bidirect ional LSTM Method with \nGloVe Word Representation Weighting,” Jurnal RESTI (Rekayasa Sistem dan Teknologi Informasi), vol. 4, no. 4, Aug. 2020. \n \n \nBIOGRAPHIES OF AUTHORS \n \n \nAndrew Steven Sams     is a student currently pursuing a double degree (Bachelor’s \nand Master’s) in Computer Science at Bina Nusantara University . He has been a faculty \nmember since 2017. His research interests include artificial intelligence, signal processing, and \nspeech technology. He can be contacted at email: andrew.sams@binus.ac.id. \n  \n \nAmalia Zahra      is a lecturer at the Master of Information Technology, Bina \nNusantara University, Indonesia. She received her bachelor’s degree in computer science from \nthe Faculty of Computer Science, University of Indonesia (UI) in 2008. She does not have a \nmaster’s degree. Her Ph .D. was obtained from the School of Computer Science and \nInformatics, University College Dublin (UCD), Ireland in 2014. Her research interests cover \nvarious fields in speech technology, suc h as speech recognition, spoken language \nidentification, speaker verification, speech emotion recognition, and so on. Additionally, she \nalso has an interest in natural language processing (NLP), computational linguistics, machine \nlearning, and artificial intelligence. She can be contacted at email: amalia.zahra@binus.edu. \n \n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7061548233032227
    },
    {
      "name": "Speech recognition",
      "score": 0.6534010171890259
    },
    {
      "name": "Lyrics",
      "score": 0.5811426639556885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5728805065155029
    },
    {
      "name": "Spectrogram",
      "score": 0.5079167485237122
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5034741759300232
    },
    {
      "name": "Transformer",
      "score": 0.4893590807914734
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4847812056541443
    },
    {
      "name": "Hyperparameter",
      "score": 0.4428880214691162
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4310901463031769
    },
    {
      "name": "Engineering",
      "score": 0.07270053029060364
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I166073570",
      "name": "Binus University",
      "country": "ID"
    }
  ]
}