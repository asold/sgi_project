{
  "title": "Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions Regarding Bariatric Surgery",
  "url": "https://openalex.org/W4367175507",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Samaan, Jamil S.",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Yeo, Yee Hui",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Rajeev, Nithya",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Hawley, Lauren",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Abel, Stuart",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Ng, Wee Han",
      "affiliations": [
        "University of Bristol"
      ]
    },
    {
      "id": null,
      "name": "Srinivasan, Nitin",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Park, Justin",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Burch, Miguel",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Watson, Rabindra",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Liran, Omer",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Samakar, Kamran",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Samaan, Jamil S.",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Yeo, Yee Hui",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Rajeev, Nithya",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Hawley, Lauren",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Abel, Stuart",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Ng, Wee Han",
      "affiliations": [
        "University of Bristol"
      ]
    },
    {
      "id": null,
      "name": "Srinivasan, Nitin",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Park, Justin",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    },
    {
      "id": null,
      "name": "Burch, Miguel",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Watson, Rabindra",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Liran, Omer",
      "affiliations": [
        "Cedars-Sinai Medical Center"
      ]
    },
    {
      "id": null,
      "name": "Samakar, Kamran",
      "affiliations": [
        "Keck Hospital of USC"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312083290",
    "https://openalex.org/W4311481166",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W2183446018",
    "https://openalex.org/W3081868939",
    "https://openalex.org/W4221129449",
    "https://openalex.org/W4309791509",
    "https://openalex.org/W4212825846",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3048720544",
    "https://openalex.org/W2608420668",
    "https://openalex.org/W3091603611",
    "https://openalex.org/W2155269885",
    "https://openalex.org/W2888969827",
    "https://openalex.org/W2947177211",
    "https://openalex.org/W2945276550",
    "https://openalex.org/W2961746603",
    "https://openalex.org/W2984634401",
    "https://openalex.org/W4206759680",
    "https://openalex.org/W2969785018",
    "https://openalex.org/W3205285085",
    "https://openalex.org/W3041698926",
    "https://openalex.org/W1976550821"
  ],
  "abstract": "Abstract Purpose ChatGPT is a large language model trained on a large dataset covering a broad range of topics, including the medical literature. We aim to examine its accuracy and reproducibility in answering patient questions regarding bariatric surgery. Materials and methods Questions were gathered from nationally regarded professional societies and health institutions as well as Facebook support groups. Board-certified bariatric surgeons graded the accuracy and reproducibility of responses. The grading scale included the following: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and (4) completely incorrect. Reproducibility was determined by asking the model each question twice and examining difference in grading category between the two responses. Results In total, 151 questions related to bariatric surgery were included. The model provided “comprehensive” responses to 131/151 (86.8%) of questions. When examined by category, the model provided “comprehensive” responses to 93.8% of questions related to “efficacy, eligibility and procedure options”; 93.3% related to “preoperative preparation”; 85.3% related to “recovery, risks, and complications”; 88.2% related to “lifestyle changes”; and 66.7% related to “other”. The model provided reproducible answers to 137 (90.7%) of questions. Conclusion The large language model ChatGPT often provided accurate and reproducible responses to common questions related to bariatric surgery. ChatGPT may serve as a helpful adjunct information resource for patients regarding bariatric surgery in addition to standard of care provided by licensed healthcare professionals. We encourage future studies to examine how to leverage this disruptive technology to improve patient outcomes and quality of life. Graphical Abstract",
  "full_text": "Vol:.(1234567890)\nObesity Surgery (2023) 33:1790–1796\nhttps://doi.org/10.1007/s11695-023-06603-5\n1 3\nORIGINAL CONTRIBUTIONS\nAssessing the Accuracy of Responses by the Language Model ChatGPT \nto Questions Regarding Bariatric Surgery\nJamil S. Samaan1  · Yee Hui Yeo1 · Nithya Rajeev2 · Lauren Hawley2 · Stuart Abel2 · Wee Han Ng3 · Nitin Srinivasan2 · \nJustin Park2 · Miguel Burch4 · Rabindra Watson1 · Omer Liran5,6 · Kamran Samakar2\nReceived: 21 February 2023 / Revised: 10 April 2023 / Accepted: 17 April 2023 / Published online: 27 April 2023 \n© The Author(s) 2023\nAbstract\nPurpose ChatGPT is a large language model trained on a large dataset covering a broad range of topics, including the medical \nliterature. We aim to examine its accuracy and reproducibility in answering patient questions regarding bariatric surgery.\nMaterials and methods Questions were gathered from nationally regarded professional societies and health institutions as \nwell as Facebook support groups. Board-certified bariatric surgeons graded the accuracy and reproducibility of responses. The \ngrading scale included the following: (1) comprehensive, (2) correct but inadequate, (3) some correct and some incorrect, and \n(4) completely incorrect. Reproducibility was determined by asking the model each question twice and examining difference \nin grading category between the two responses.\nResults In total, 151 questions related to bariatric surgery were included. The model provided “comprehensive” responses \nto 131/151 (86.8%) of questions. When examined by category, the model provided “comprehensive” responses to 93.8% of \nquestions related to “efficacy, eligibility and procedure options”; 93.3% related to “preoperative preparation”; 85.3% related to \n“recovery, risks, and complications”; 88.2% related to “lifestyle changes”; and 66.7% related to “other”. The model provided \nreproducible answers to 137 (90.7%) of questions.\nConclusion The large language model ChatGPT often provided accurate and reproducible responses to common questions \nrelated to bariatric surgery. ChatGPT may serve as a helpful adjunct information resource for patients regarding bariatric \nsurgery in addition to standard of care provided by licensed healthcare professionals. We encourage future studies to examine \nhow to leverage this disruptive technology to improve patient outcomes and quality of life.\nKeywords Artificial intelligence · ChatGPT · Language learning models · Bariatric surgery · Weight loss · Health literacy\nIntroduction\nThe utilization of artificial intelligence (AI) in medicine \nhas grown significantly in recent years with the advent \nof a broad range of applications and technologies used to \nKey Points  \n• ChatGPT provided accurate responses to bariatric surgery-\nrelated questions.\n• ChatGPT provided reproducible responses to bariatric surgery-\nrelated questions.\n• ChatGPT may serve as an adjunct source of information for \npatients and candidates.\n * Jamil S. Samaan \n jamil.samaan@gmail.com\n1 Karsh Division of Gastroenterology and Hepatology, Cedars-\nSinai Medical Center, 8700 Beverly Blvd, Los Angeles, \nCA 90048, USA\n2 Division of Upper GI and General Surgery, Department \nof Surgery, Health Care Consultation Center, Keck School \nof Medicine of USC, 1510 San Pablo St. #514, Los Angeles, \nCA 90033, USA\n3 Bristol Medical School, University of Bristol, 5 Tyndall Ave, \nBristol BS8 1UD, UK\n4 Department of Surgery, Cedars-Sinai Medical Center, 8700 \nBeverly Blvd, Los Angeles, CA 90048, USA\n5 Department of Psychiatry and Behavioral Sciences, \nCedars-Sinai Medical Center, 8700 Beverly Blvd, \nLos Angeles, CA 90048, USA\n6 Division of Health Services Research, Department \nof Medicine, Cedars-Sinai Medical Center, 8700 Beverly \nBlvd, Los Angeles, CA 90048, USA\n1791Obesity Surgery (2023) 33:1790–1796 \n1 3\nimprove patient care and evidence-based medicine. These \ntechnologies are often used by healthcare providers or \nthe healthcare system for patient care and research. On \nNovember 30, 2022, the chatbot ChatGPT was launched \nby the company OpenAI for public use through a free \nonline platform available to all users with a free account. \nChatGPT is an AI large language model trained on a large \ndataset covering a broad range of topics, including the \nmedical literature. When prompted with inquiries, Chat-\nGPT can provide well-formulated, conversational, and \neasy-to-understand seemingly knowledgeable responses. \nDiscussion regarding the potential of ChatGPT to disrupt \nall fields of academia is ongoing, and its applicability \nis actively under investigation [1 , 2]. One recent study-\nshowed ChatGPT achieved a passing score when prompted \nwith USMLE style questions, while others have shown its \nappropriateness in responding to cardiovascular disease \nprevention-related inquiries and accuracy in responding to \ncommonly asked cirrhosis and hepatocellular carcinoma-\nrelated questions [ 3-5]. There are currently no studies \nexamining the clinical application of ChatGPT in the field \nof bariatric surgery.\nObesity rates are on the rise in the USA, with the preva-\nlence of obesity and severe obesity increasing over the \npast two decades [6 ]. Bariatric surgery has been shown \nto be an effective and safe therapeutic modality for long-\nterm weight loss as well as resolution of obesity-related \ncomorbidities such as diabetes mellitus, hypertension, and \nobstructive sleep apnea [7 ]. Despite its proven efficacy and \nsafety, less than 1% of eligible patients undergo bariatric \nsurgery in the USA [8 ]. The reason for this underutili-\nzation is likely multifactorial, and multiple studies have \nhighlighted the various barriers to care such as provider \nperceptions, insurance coverage, access to resources, and \npatient perceptions [9 ,10]. Some of these barriers are \nrooted in the lack of familiarity with the safety and effi-\ncacy of bariatric surgery, making it difficult for patients \nand their providers to make informed decisions regarding \nreferring for bariatric surgery.\nEven after obtaining a referral, patients typically undergo \na preoperative evaluation process and optimization of preex-\nisting comorbidities, as well as receive education and life-\nstyle counseling. The literature, while limited, reveals that \npatients often seek sources of information outside of their \nhealthcare provider’s office, with various social media ven-\nues being a popular source of information [11]. In light of \nthis, and given ChatGPTs rapidly growing number of users, \nwe anticipate patients may utilize ChatGPT as a source of \ninformation as part of their weight loss journey. Therefore, \nit is crucial to determine the reliability of this new tool. In \nthis study, we aimed to examine the accuracy and reproduc-\nibility of ChatGPT’s responses to commonly asked patient \nquestions regarding bariatric surgery.\nMethods\nQuestion Curation/Data Source\nQuestions were obtained from the Frequently Asked Ques-\ntions pages from the American Society for Metabolic and \nBariatric Surgery (ASMBS) and MedlinePlus Medical \nEncyclopedia, along with two social media support groups \nfor patients who were considering, eligible for, or who had \nrecently undergone a bariatric procedure: Bariatric Surgery \n& Gastric Sleeve Support Group (Facebook) and Gastric \nSleeve and Bariatric Surgery Group, Supporting the New \nYou (Facebook). Questions were curated, screened, and \napproved by three authors (J. S., N. R., K. S.) to evaluate \ntheir inclusion in the study. Only questions regarding bariat-\nric surgery or weight loss surgery were included. Duplicate \nand similar questions from multiple sources were removed \n(Fig.  1). Questions requiring subjective or personalized \nresponses (e.g., How will bariatric surgery change my \nlifespan?) and questions that were vague (e.g., How much \nweight is normally lost after this surgery?) were rephrased to \na generic language format to allow for inclusion in the study \n(Fig. 1). Other questions were grammatically modified to \neliminate ambiguity. A total of 151 questions were included \nand were used to generate responses from ChatGPT. To \nbetter characterize ChatGPTs performance in various top-\nics within bariatric surgery, questions were categorized into \nmultiple groups for statistical analysis purposes: (1) eligibil-\nity, efficacy, and procedure options; (2) preoperative prepa-\nration; (3) recovery, risks, and complications; (4) lifestyle \nchanges; and (5) other.\nChatGPT\nChatGPT is a large language model trained on a large data-\nbase of information from a wide range of sources includ-\ning online websites, books, and articles leading up to 2021 \n[12]. When prompted with inquiries, ChatGPT can provide \nwell-formulated, conversational, and easy-to-understand \nresponses. The creators utilized reinforcement learning from \nhuman feedback (RLHF) to fine-tune the model to follow a \nbroad class of commands and written instructions fine-tuned \nby human preferences as a reward signal [13]. The model \nwas also trained to align with user intentions and minimize \nbias, toxic, or otherwise harmful responses. The source of \ninformation used to train ChatGPT is unknown.\nResponse Generation\nTo generate responses, each question was prompted to Chat-\nGPT (December 15th version). Each individual question was \n1792 Obesity Surgery (2023) 33:1790–1796\n1 3\ninputted twice on separate occasions using the “new chat” \nfunction in order to generate two responses per question. \nThis was done to determine the reproducibility of responses \nto the same question.\nQuestion Grading\nResponses to questions were first independently graded for \naccuracy and reproducibility by two board-certified, fel-\nlowship-trained, active practice academic bariatric surgeon \nreviewers (L. H. and S. A.). Reviewers were instructed to \ngrade the accuracy of responses based on known information \nleading up to 2021. Reproducibility was graded based on the \nsimilarity in accuracy of the two responses per question gen-\nerated by ChatGPT. If the responses were similar, only the \nfirst response from ChatGPT was graded. If the responses \nwere not similar, both responses were individually graded.\nAccuracy of each response was graded with the follow -\ning scale:\n1. Comprehensive: Defined as accurate and comprehen-\nsive, nothing more a board-certified bariatric surgeon \ncan add if asked this question by a patient\n2. Correct but inadequate: All information is correct but \nincomplete; a board-certified bariatric surgeon would \nhave more important information to add if asked this \nquestion by a patient.\n3. Some correct and some incorrect\n4. Completely incorrect\nDisagreement in reproducibility or grading of each \nresponse was resolved by a third board-certified, fellowship-\ntrained, active practice academic bariatric surgeon (K. S.). \nThe final grades were then compiled and used to analyze \nthe overall performance of ChatGPT in answering questions \nrelated to bariatric surgery.\nStatistical Analysis\nThe proportions of responses earning each grade were cal-\nculated. To determine reproducibility, responses were cat-\negorized into two groups: a grade of 1 and 2 comprised the \nfirst group, and a grade of 3 and 4 comprised the second \ngroup. The two responses to each question were considered \nsignificantly different from one another, or not reproduc-\nible, if the assigned grades for each response fell under \ndifferent groups. Interrater reliability between reviewer 1 \nand reviewer 2 was tested using Kappa statistic and showed \nmoderate agreement with a Kappa value of 0.762 with a \n95% confidence interval of 0.684–0.825. All analyses were \nconducted using Microsoft Excel (version 16.69.1).\nResults\nIn total, 151 questions related to bariatric surgery were \ninputted into ChatGPT (Supplementary Table 1). The model \nprovided “comprehensive” responses to 131/151 (86.8%) of \nquestions. The model provided “comprehensive” responses \nto 131/151 (86.8%) of questions. When examined by cat-\negory, the model provided “comprehensive” responses to \n93.8% of questions related to “efficacy, eligibility and \nprocedure options”; 93.3% to “preoperative preparation”; \n85.3% to “recovery, risks, and complications”; and 88.2% \nto “lifestyle changes” (Table 1). The percentage of compre-\nhensive responses was lower for the “others” category, with \nFig. 1  Flow chart of bariatric \nsurgery-related question selec-\ntion\n\n1793Obesity Surgery (2023) 33:1790–1796 \n1 3\nonly two-thirds of responses graded as “comprehensive,” \nwhile 25% of responses were graded as “some correct and \nsome incorrect.” In total, 4 questions across all categories \nwere given a rating of “completely incorrect.” For example, \nwhen prompted regarding the expected scars from surgery, \nthe model described the open approach rather than the com-\nmonly used laparoscopic approach. When prompted regard-\ning the use of straws after surgery, the model explained \n“Using a straw can help to bypass the smaller stomach and \ndeliver fluids directly to the intestines, where they can be \nabsorbed more easily.”\nThe model provided reproducible answers to 137 (90.7%) \nof questions. Reproducibility of responses in the “lifestyle \nchanges” and “others” categories was 100% but lower in \nthe “recovery, risks and complications” (86.7%); “eligibility, \nefficacy and procedure options” (90.6%); and “preoperative \npreparation” (93.3%) categories (Table 2).\nDiscussion\nWe examined the accuracy and reproducibility of the new \nAI model ChatGPT in responding to commonly asked \npatient questions related to bariatric surgery. The responses \nof ChatGPT to patient questions obtained from nationally \nregarded professional societies and health institutions as \nwell as Facebook patient support groups were indepen-\ndently assessed for accuracy and reproducibility by a panel \nof board-certified and fellowship-trained bariatric surgeons. \nOverall, ChatGPT’s responses were accurate with most \nresponses graded as “comprehensive” on our grading scale \nalong with high reproducibility. The model performed well \nin the majority of topics such as “efficacy, eligibility and \nprocedure options”; “preoperative preparation”; “recov -\nery, risks, and complications”; and “lifestyle changes.” Our \nstudy shows the potential significant impact of this technol-\nogy in improving patients’ experience and outcomes in the \nfuture. Furthermore, we highlight important limitations and \nconsiderations.\nPrevious literature has shown patients seek informa-\ntion related to bariatric surgery outside of their healthcare \nproviders, with social media serving as a popular resource \n[11, 14, 15]. Multiple studies have shown limitations in \nthe quality and reliability of online sources of information \nregarding bariatric surgery, including social media [16-18]. \nFurthermore, research has demonstrated that there is a lack \nof easy-to-understand online resources about bariatric sur -\ngery with over 93% of websites surveyed receiving a rating \nof unacceptable readability [19]. Furthermore, navigating \ncurrent search engines can be difficult when searching for \nspecific answers. Search results may be overwhelming, \nnot relevant to the question asked or difficult to ascertain \nthe reliability and accuracy of sources. Given these limita-\ntions, we hypothesize that patients will utilize ChatGPT as \na source of information for their bariatric surgery-related \nquestions. Our reviewers found ChatGPT easy to access \nand understand. Universal access AI models may also serve \nto bridge access and knowledge disparities in communities \nwith limited resources. Our study findings are encouraging, \nand we anticipate significant future interest in this technol-\nogy as it relates not only to bariatric surgery but also other \nfields of medicine.\nChatGPT has the potential to mitigate disparities in health \nliteracy among bariatric surgery candidates and patients. \nLevels of health literacy have been shown to impact bariatric \nsurgery outcomes with lower literacy being associated with \nTable 1  Grading of responses generated by ChatGPT to questions \nrelated to bariatric surgery categorized by question type\nEligibility, efficacy, and procedure options (N = 32)\n  1. Comprehensive 93.8%\n  2. Correct but incomplete 3.1%\n  3. Some correct and some incorrect 0.0%\n  4. Completely incorrect 3.1%\nPreoperative preparation (N = 15)\n  1. Comprehensive 93.3%\n  2. Correct but incomplete 0.0%\n  3. Some correct and some incorrect 6.7%\n  4. Completely incorrect 0.0%\nRecovery, risks, and complications (N = 75)\n  1. Comprehensive 85.3%\n  2. Correct but incomplete 6.7%\n  3. Some correct and some incorrect 4.0%\n  4. Completely incorrect 4.0%\nLifestyle changes (N = 17)\n  1. Comprehensive 88.2%\n  2. Correct but incomplete 5.9%\n  3. Some correct and some incorrect 5.9%\n  4. Completely incorrect 0.0%\nOthers (N = 12)\n  1. Comprehensive 66.7%\n  2. Correct but incomplete 8.3%\n  3. Some correct and some incorrect 25.0%\n  4. Completely incorrect 0.0%\nTable 2  Proportion of questions with reproducible responses gener -\nated by ChatGPT categorized by question type\nReproducibility was defined as no difference in grading categories (1 \nand 2 vs 3 and 4) between the two responses for each question\nEligibility, efficacy, and procedure options (N = 32) 90.6%\nPreoperative preparation (N = 15) 93.3%\nRecovery, risks, and complications (N = 75) 86.7%\nLifestyle changes after surgery (N = 17) 100%\nOthers (N = 12) 100%\n1794 Obesity Surgery (2023) 33:1790–1796\n1 3\nlower short-term as well as long-term postoperative weight \nloss [20- 22]. Other studies have shown the association of \nhealth literacy with adherence to medical appointments 1 \nyear after bariatric surgery [23]. One study demonstrated \nthat low health literacy among bariatric surgery candi-\ndates had a negative association with eventually undergo-\ning bariatric surgery [24]. While there are no studies that \nhave explored the impact of health literacy on bariatric \nsurgery complications, health literacy has been shown to \nbe negatively associated with postoperative complications \nafter colorectal surgery [25, 26]. Our study demonstrates \nthat the currently available version of ChatGPT provided \ncomprehensive responses to questions regarding the safety, \nefficacy, and complications of bariatric surgery for most \nquestions reviewed. Future iterations of this technology \nwill likely offer improvements in accuracy and readability. \nWhile there are no current studies examining the impact \nof the use of ChatGPT on health literacy, we hypothesize \nthat utilization of this model will provide patients with an \neasy-to-understand, accurate, and reproducible resource of \ninformation for patients.\nChatGPT may also serve as a tool for improving the \nperception of the safety and efficacy of bariatric surgery \nboth among prospective candidates and the general public, \nthereby increasing its adoption as a treatment modality for \nsevere obesity. Despite its known efficacy, less than 1% of \neligible patients undergo bariatric surgery. Previous studies \nhave shown both lack of familiarity and discordance between \npatient perceptions and the demonstrated clinical safety and \nefficacy profile of bariatric surgery [10]. These trends were \nalso found among the general public. Furthermore, portrayal \nof bariatric surgery in the media may also contribute to its \nunderutilization with one study in the UK finding 33% of \narticles published in newspapers were negatively slanted \nagainst bariatric surgery [27]. Having easy access to infor -\nmation regarding bariatric surgery may help raise awareness \namong the general public and eligible candidates, thereby \ndecreasing the stigma surrounding surgery and potentially \nincrease its utilization.\nStrengths and Limitations\nTo our knowledge, this is the first study to examine the util-\nity of the model ChatGPT in the field of bariatric surgery. \nPatient questions were obtained from well-regarded sources \nand societies as well as patient questions from Facebook \nsupport groups to provide a comprehensive and realistic \nsample of patient questions. Responses were independently \ngraded by board-certified bariatric surgeons with discrep -\nancies resolved by a blinded third senior reviewer to com-\nprehensively evaluate the accuracy and reproducibility of \nChatGPTs responses.\nThere are several limitations of ChatGPT that should be \nhighlighted and considered when evaluating it as a poten -\ntial source of information for patients. While the model per-\nformed well with accurate and often reproducible responses, \nthere were several responses that contained incorrect infor -\nmation which may be dangerous to patients without the direc-\ntion of a healthcare provider. This limitation supports the \npotential use of ChatGPT as an adjunct source of informa -\ntion but not a replacement of the standard of care provided \nby a team of licensed healthcare professionals. We hope \nthis limitation is minimized with the expected continuous \nimprovement of the model overtime and in turn provide more \naccurate and reproducible responses. Secondly, recommen-\ndations and standards of practice may vary by region, medi-\ncal society, or country of residence, providing a challenge \nin assessing the accuracy of responses for each user around \nthe country or world regarding certain topics. The source of \ninformation in which ChatGPT uses to produce responses is \nunknown which may impact the reliability of its responses for \ncertain topics. Third, ChatGPT may write linguistically con-\nvincing responses which are incorrect or nonsensical. This \nfalse sense of confidence in responses may lead to the adop-\ntion of incorrect and potentially dangerous recommendations \nby unsuspecting users. OpenAI reports that the current ver-\nsion of the model is expected to produce false positives and \nfalse negatives but hope to minimize these limitations with \nuser feedback and ongoing improvement in the model.\nFuture Directions\nChatGPT and future similar technologies are expected to revo-\nlutionize all fields of academia and likely the field of medicine, \nalthough its utility and implementation process remain unclear. \nIf validated by future studies, ChatGPT can be a powerful tool \nto empower patients. Its performance in answering patient ques-\ntions regarding cirrhosis and hepatocellular carcinoma has been \nstudied with promising results, similar to the current study [5]. \nWe encourage future studies to expand on our findings in order \nto study the utility of this revolutionary technology in improv-\ning patient experience and outcomes in bariatric surgery.\nConclusion\nThe large language model ChatGPT provided accurate and \nreproducible responses to common questions related to bari-\natric surgery. ChatGPT may serve as a helpful adjunct source \nof information regarding bariatric surgery for patients and \nsurgical candidates in addition to standard of care provided \nby licensed healthcare professionals. We encourage future \nstudies to examine how to leverage this disruptive technol-\nogy to improve patient outcomes and quality of life.\n1795Obesity Surgery (2023) 33:1790–1796 \n1 3\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s11695- 023- 06603-5.\nFunding Open access funding provided by SCELC, Statewide Califor-\nnia Electronic Library Consortium\nDeclarations \nEthical Approval This article does not contain any studies with human \nparticipants or animals performed by any of the authors.\nInformed Consent Informed consent does not apply.\nConflict of Interest The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. O'Connor S. ChatGPT. Open artificial intelligence platforms in nursing \neducation: tools for academic progress or abuse? Nurse Educ Pract. \n2023;66:103537. https:// doi. org/ 10. 1016/j. nepr. 2022. 103537.\n 2. Graham F. Daily briefing: will ChatGPT kill the essay assignment? \nNature. 2022; https:// doi. org/ 10. 1038/ d41586- 022- 04437-2.\n 3. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, \nChartash D. How does ChatGPT perform on the United States Med-\nical Licensing Examination? The implications of large language \nmodels for medical education and knowledge assessment. JMIR \nMed Educ. 2023;8(9):e45312. https:// doi. org/ 10. 2196/ 45312.\n 4. Sarraju A, Bruemmer D, Van Iterson E, Cho L, Rodriguez F, Laffin \nL. Appropriateness of cardiovascular disease prevention recommen-\ndations obtained from a popular online chat-based artificial intel-\nligence model. JAMA. 2023;329(10):842. https:// doi. org/ 10. 1001/ \njama. 2023. 1044.\n 5. Yeo YH, Samaan JS, Ng WH, Ting PS, Trivedi H, Vipani A, \nAyoub W, Yang JD, Liran O, Spiegel B, Kuo A. Assessing the \nperformance of ChatGPT in answering questions regarding cir -\nrhosis and hepatocellular carcinoma. Clin Mol Hepatol. 2023; \nhttps:// doi. org/ 10. 3350/ cmh. 2023. 0089.\n 6. Obesity is a common, serious, and costly disease. Centers for Dis-\nease Control and Prevention. Accessed January 21, 2023. https:// \nwww. cdc. gov/ obesi ty/ data/ adult. html\n 7. Arterburn DE, Telem DA, Kushner RF, Courcoulas AP. Benefits \nand risks of bariatric surgery in adults: a review. JAMA. 2020 \nSep;324(9):879–87. https:// doi. org/ 10. 1001/ jama. 2020. 12567.\n 8. New study finds most bariatric surgeries performed in north-\neast, and fewest in south where obesity rates are highest, \nand economies are weakest. American Society for Meta-\nbolic and Bariatric Surgery. Published online November 15, \n2018. Accessed January 23, 2023. https:// asmbs. org/ artic les/  \nnew- study- finds- most- baria tric- surge ries- perfo rmed- in- north \neast- and- fewest- in- south- where- obesi ty- rates- are- highe st- and- \necono mies- are- weake st\n 9. Premkumar A, Samaan JS, Samakar K. Factors associated with \nbariatric surgery referral patterns: a systematic review. J Surg Res. \n2022;276:54–75. https:// doi. org/ 10. 1016/j. jss. 2022. 01. 023.\n 10. Rajeev ND, Samaan JS, Premkumar A, Srinivasan N, Yu E, Sama-\nkar K. Patient and the public’s perceptions of bariatric surgery: a \nsystematic review. J Surg Res. 2023;283:385–406. https:// doi. org/ \n10. 1016/j. jss. 2022. 10. 061.\n 11. Scarano Pereira JP, Martinino A, Manicone F, Scarano Pereira \nML, Iglesias Puzas Á, Pouwels S, Martínez JM. Bariatric surgery \non social media: a cross-sectional study. Obes Res Clin Pract. \n2022;16(2):158–62. https:// doi. org/ 10. 1016/j. orcp. 2022. 02. 005.\n 12. openai. ChatGPT: optimizing language models for dialogue. 2023; \nhttps:// openai. com/ blog/ chatg pt/. Accessed 1/1/2023, 2023.\n 13. Ouyang L, Wu J, Jiang X, et al. Training language models to fol-\nlow instructions with human feedback. Adv Neur Inform Proc Syst. \n2022;35:27730. https:// doi. org/ 10. 48550/ ARXIV. 2203. 02155.\n 14. Athanasiadis DI, Roper A, Hilgendorf W, Voss A, Zike T, Embry M, \nBanerjee A, Selzer D, Stefanidis D. Facebook groups provide effec-\ntive social support to patients after bariatric surgery. Surg Endosc. \n2021;35(8):4595–601. https:// doi. org/ 10. 1007/ s00464- 020- 07884-y.\n 15. Koball AM, Jester DJ, Domoff SE, Kallies KJ, Grothe KB, Kothari \nSN. Examination of bariatric surgery Facebook support groups: \na content analysis. Surg Obes Relat Dis. 2017;13(8):1369–75. \nhttps:// doi. org/ 10. 1016/j. soard. 2017. 04. 025.\n 16. Batar N, Kermen S, Sevdin S, Yıldız N, Güçlü D. Assessment of \nthe quality and reliability of information on nutrition after bari-\natric surgery on YouTube. Obes Surg. 2020;30:4905–10. https://  \ndoi. org/ 10. 1007/ s11695- 020- 05015-z.\n 17. Corcelles R, Daigle CR, Talamas HR, Brethauer SA, Schauer PR. \nAssessment of the quality of Internet information on sleeve gas-\ntrectomy. Surg Obes Relat Dis. 2015;11(3):539–44. https:// doi. \norg/ 10. 1016/j. soard. 2014. 08. 014.\n 18. Koball AM, Jester DJ, Pruitt MA, Cripe RV, Henschied JJ, \nDomoff S. Content and accuracy of nutrition-related posts in \nbariatric surgery Facebook support groups. Surg Obes Relat Dis. \n2018;14:1897–902. https:// doi. org/ 10. 1016/j. soard. 2018. 08. 017.\n 19. Meleo-Erwin Z, Basch C, Fera J, Ethan D, Garcia P. Readability of \nonline patient-based information on bariatric surgery. Health Promot \nPerspect. 2019;9(2):156–60. https:// doi. org/ 10. 15171/ hpp. 2019. 22.\n 20. Mahoney ST, Strassle PD, Farrell TM, Duke MC. Does lower \nlevel of education and health literacy affect successful out-\ncomes in bariatric surgery? J Laparoendosc Adv Surg Tech A. \n2019;29(8):1011–5. https:// doi. org/ 10. 1089/ lap. 2018. 0806.\n 21. Erdogdu UE, Cayci HM, Tardu A, Demirci H, Kisakol G, \nGuclu M. Health literacy and weight loss after bariatric sur -\ngery. Obes Surg. 2019;29(12):3948–53. https:// doi. org/ 10. 1007/ \ns11695- 019- 04060-7.\n 22. Miller-Matero LR, Hecht L, Patel S, Martens KM, Hamann A, Carlin \nAM. The influence of health literacy and health numeracy on weight \nloss outcomes following bariatric surgery. Surg Obes Relat Dis. \n2021;17(2):384–9. https:// doi. org/ 10. 1016/j. soard. 2020. 09. 021.\n 23. Hecht LM, Martens KM, Pester BD, Hamann A, Carlin AM, \nMiller-Matero LR. Adherence to medical appointments among \npatients undergoing bariatric surgery: do health literacy, health \nnumeracy, and cognitive functioning play a role? Obes Surg. \n2022;32(4):1391–3. https:// doi. org/ 10. 1007/ s11695- 022- 05905-4.\n 24. Hecht L, Cain S, Clark-Sienkiewicz SM, Martens K, Hamann A, \nCarlin AM, Miller-Matero LR. Health literacy, health numeracy, and \ncognitive functioning among bariatric surgery candidates. Obes Surg. \n2019;29(12):4138–41. https:// doi. org/ 10. 1007/ s11695- 019- 04149-z.\n 25. Theiss LM, Wood T, McLeod MC, Shao C, Santos Marques ID, \nBajpai S, Lopez E, Duong AM, Hollis R, Morris MS, Chu DI. The \nassociation of health literacy and postoperative complications after \n1796 Obesity Surgery (2023) 33:1790–1796\n1 3\ncolorectal surgery: a cohort study. Am J Surg. 2022;223(6):1047–52. \nhttps:// doi. org/ 10. 1016/j. amjsu rg. 2021. 10. 024.\n 26. Baker S, Malone E, Graham L, Dasinger E, Wahl T, Titan A, \nRichman J, Copeland L, Burns E, Whittle J, Hawn M, Morris M. \nPatient-reported health literacy scores are associated with read-\nmissions following surgery. Am J Surg. 2020;220(5):1138–44. \nhttps:// doi. org/ 10. 1016/j. amjsu rg. 2020. 06. 071.\n 27. Williamson JM, Rink JA, Hewin DH. The portrayal of bariatric \nsurgery in the UK print media. Obes Surg. 2012;22(11):1690–4. \nhttps:// doi. org/ 10. 1007/ s11695- 012- 0701-5.\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.8503710031509399
    },
    {
      "name": "Grading (engineering)",
      "score": 0.7444469928741455
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5854972004890442
    },
    {
      "name": "Health care",
      "score": 0.5690225958824158
    },
    {
      "name": "English language",
      "score": 0.47322240471839905
    },
    {
      "name": "Machine learning",
      "score": 0.0
    },
    {
      "name": "Computer science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1282927834",
      "name": "Cedars-Sinai Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800154431",
      "name": "Keck Hospital of USC",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36234482",
      "name": "University of Bristol",
      "country": "GB"
    }
  ]
}