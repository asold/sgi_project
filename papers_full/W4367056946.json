{
  "title": "Hformer: highly efficient vision transformer for low-dose CT denoising",
  "url": "https://openalex.org/W4367056946",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100409615",
      "name": "Shiyu Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Modern Physics"
      ]
    },
    {
      "id": "https://openalex.org/A5072157560",
      "name": "Zhao-Xuan Wang",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A5103202802",
      "name": "Haibo Yang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Modern Physics"
      ]
    },
    {
      "id": "https://openalex.org/A5058229030",
      "name": "Yi-Lun Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Modern Physics"
      ]
    },
    {
      "id": "https://openalex.org/A5100341552",
      "name": "Yang Li",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A5108517562",
      "name": "Quan Pan",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A5101472199",
      "name": "Hongkai Wang",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A5034680858",
      "name": "Chengxin Zhao",
      "affiliations": [
        "Ji Hua Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4317932366",
    "https://openalex.org/W2171697262",
    "https://openalex.org/W4220936665",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3213093647",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W3154435685",
    "https://openalex.org/W3033492948",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W6600050674",
    "https://openalex.org/W4287792756",
    "https://openalex.org/W2973136764",
    "https://openalex.org/W4250955649",
    "https://openalex.org/W1972150100",
    "https://openalex.org/W2035199208",
    "https://openalex.org/W2094366314",
    "https://openalex.org/W2949497190",
    "https://openalex.org/W2002611249",
    "https://openalex.org/W2160547390",
    "https://openalex.org/W2142884793",
    "https://openalex.org/W2041114617",
    "https://openalex.org/W2618025634",
    "https://openalex.org/W2570202822",
    "https://openalex.org/W2525884435",
    "https://openalex.org/W2584483805",
    "https://openalex.org/W4220659948",
    "https://openalex.org/W2938296211",
    "https://openalex.org/W3155767070",
    "https://openalex.org/W3165310467",
    "https://openalex.org/W4308054588",
    "https://openalex.org/W2979411237",
    "https://openalex.org/W4224275128",
    "https://openalex.org/W3203971980",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4287330714",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W4286973560",
    "https://openalex.org/W4221144957",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W6929461855",
    "https://openalex.org/W3171087525",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3180659574",
    "https://openalex.org/W2520164769",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W6602628784",
    "https://openalex.org/W4287122968",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2762996341",
    "https://openalex.org/W3213236098"
  ],
  "abstract": "Abstract In this paper, we propose Hformer, a novel supervised learning model for low-dose computer tomography (LDCT) denoising. Hformer combines the strengths of convolutional neural networks for local feature extraction and transformer models for global feature capture. The performance of Hformer was verified and evaluated based on the AAPM-Mayo Clinic LDCT Grand Challenge Dataset. Compared with the former representative state-of-the-art (SOTA) model designs under different architectures, Hformer achieved optimal metrics without requiring a large number of learning parameters, with metrics of 33.4405 PSNR, 8.6956 RMSE, and 0.9163 SSIM. The experiments demonstrated designed Hformer is a SOTA model for noise suppression, structure preservation, and lesion detection.",
  "full_text": "Vol.:(0123456789)1 3\nNuclear Science and Techniques           (2023) 34:61  \nhttps://doi.org/10.1007/s41365-023-01208-0\nHformer: highly efficient vision transformer for low‑dose CT denoising\nShi‑Yu Zhang1,2,3 · Zhao‑Xuan Wang4  · Hai‑Bo Yang1,2,3  · Yi‑Lun Chen1,2,3  · Yang Li5 · Quan Pan4,5 · \nHong‑Kai Wang6  · Cheng‑Xin Zhao1,2,3\nReceived: 14 December 2022 / Revised: 26 February 2023 / Accepted: 27 February 2023 \n© The Author(s) 2023\nAbstract\nIn this paper, we propose Hformer, a novel supervised learning model for low-dose computer tomography (LDCT) denois-\ning. Hformer combines the strengths of convolutional neural networks for local feature extraction and transformer models \nfor global feature capture. The performance of Hformer was verified and evaluated based on the AAPM-Mayo Clinic LDCT \nGrand Challenge Dataset. Compared with the former representative state-of-the-art (SOTA) model designs under different \narchitectures, Hformer achieved optimal metrics without requiring a large number of learning parameters, with metrics of \n33.4405 PSNR, 8.6956 RMSE, and 0.9163 SSIM. The experiments demonstrated designed Hformer is a SOTA model for \nnoise suppression, structure preservation, and lesion detection.\nKeywords Low-dose CT · Deep learning · Medical image · Image denoising · Convolutional neural networks · Self-\nattention · Residual network · Auto-encoder\n1 Introduction\nComputed tomography (CT) is a diagnostic imaging method \nthat uses precisely aligned X-rays, gamma-rays, ultrasound, \nand ion beams [1 ] to create cross-sectional images of the \nhuman body. It uses a highly sensitive detector and focuses \nX-rays to create 3D images. CT is known for its fast scan \ntime and clear images and is used to examine a variety of \ndiseases. However, it exposes patients to harmful radiation, \nwhich may adversely affect their health if the dose is too \nhigh.\nLow-dose CT (LDCT) has been developed as an alterna-\ntive to reduce the X-ray dose. LDCT uses less radiation than \ntraditional CT (approximately 1/4 of that of the normal-dose \nCT) and causes less radioactive damage to the human body. \nIt is particularly suitable for physical examination screening \nand patients who require multiple examinations. However, \nunlike NDCT images, LDCT images can also be affected by \nnoise and artifacts in clinical use [2]. Therefore, the suppres-\nsion of noise and artifacts in LDCT images is an important \nissue that must be addressed before applying LDCT to clini-\ncal diagnosis.\nIn traditional approaches, researchers use iterative meth-\nods to suppress artifacts and noise by relying on physical \nmodels and priori information. Unfortunately, these algo-\nrithms are difficult to implement in commercial CT scanners \nThis work was supported by the National Natural Science \nFoundation of China (Nos. 11975292, 12222512), the CAS “Light \nof West Chin'' Program, the CAS Pioneer Hundred Talent Program, \nthe Guangdong Major Project of Basic and Applied Basic Research \n(No. 2020B0301030008).\n * Hai-Bo Yang \n yanghaibo@impcas.ac.cn\n Hong-Kai Wang \n h.wang@cicams.ac.cn\n Cheng-Xin Zhao \n chengxin.zhao@impcas.ac.cn\n1 Institute of Modern Physics, Chinese Academy of Sciences, \nLanzhou 730000, China\n2 University of Chinese Academy of Sciences, Beijing 100049, \nChina\n3 Advanced Energy Science and Technology Guangdong \nLaboratory, Huizhou 516003, China\n4 School of Cybersecurity, Northwestern Polytechnical \nUniversity, Xi’an 710072, China\n5 School of Automation, Northwestern Polytechnical \nUniversity, Xi’an 710072, China\n6 Chinese Academy of Medical Sciences and Peking Union \nMedical College, Beijing 100021, China\n S.-Y. Zhang et al.\n1 3   61  Page 2 of 14\nbecause of hardware limitations and high computational \ncosts. With the growing popularity of next-generation arti-\nficial intelligence techniques and deep neural networks \n(DNNs), DNNs have become a mainstream approach to \nLDCT image denoising, which includes both supervised \nand unsupervised learning [3]. Recently, most methods have \nfocused on using convolutional neural networks (CNNs) [4, \n5] to suppress image noise and have achieved promising \nresults. Although CNNs can learn from large-scale training \ndata and obtain superior solutions, they have limitations in \ncapturing global features in images [6–9] because the pool-\ning layer loses a significant amount of valuable informa-\ntion and ignores the correlation between local and global \nfeatures. Additionally, typical CNN models lack generic \ninterpretation modules [10]. These deficiencies negatively \naffect the ability to retrieve richer structural informa-\ntion from denoised images. This also renders the model \nuninterpretable.\nRecently, a transformer model [7 ] has shown excellent \nperformance in computer vision [11– 13] and has been uti-\nlized to enhance the quality of LDCT images. Compared \nwith CNNs, transformer models are better at capturing \nglobal features and interactions between remote features, \nthus acquiring richer features in images. In addition, trans-\nformer models have a higher visual interpretability owing to \ntheir inherent self-attentive block [14, 15]. However, there \nare two primary limitations to transformer models. First, \nthe complexity of the self-attention mechanism computation \nis O(n2d) , and excessive computation can cause problems \nin clinical applications. Second, the transformer is not as \nadept at extracting local features as CNNs. To address these \nlimitations and better combine the advantages of both CNNs \nand transformers, this study proposes the Hformer module, \nwhich combines the advantages of vision transformers to \nachieve a lighter structure and improved results. Specifically, \nHformer comprises the following two aspects:\nA more lightweight convolution encoder. The convolu-\ntion module consists of multiple 3 × 3 depthwise separable \nconvolution (DSC) blocks. Depthwise convolution has a rel-\natively low number of computational parameters. It applies \none convolution kernel to each channel of the input feature \nmap and then combines the outputs of all convolution ker -\nnels to obtain its final output. The number of output chan -\nnels for the convolution operation is equal to the number of \nconvolution kernels, and only one convolution kernel is used \nper channel in depthwise convolution. Therefore, the number \nof output channels for a single channel after the convolution \noperation is also one. In this study, two depth-separable con-\nvolution layers were used to enrich the local representation, \nwhereas standard layer normalization (LN) and the Gaussian \nerror linear unit (GELU) were used to activate nonlinear fea-\nture mapping. Finally, a skip connection was added to allow \ninformation to flow through the network hierarchy. This \nblock is similar to the ConvNeXt block but with a smaller \nkernel size to promote a more lightweight model.\nMore efficient patch-based global interactions encod-\ning module. The self-attention module was suitable for \nlearning global representations. Understanding the intrin -\nsic features of visual tasks is crucial. To take advantage of \nthis, while minimizing the model overhead, we use cross-\ncovariance attention to integrate attention operations on the \nchannel features instead of using attention operations on \nthe global features in the feature map. This approach effec-\ntively reduces the complexity of the self-attention opera-\ntion, thereby reducing the computational time from (HW)2C \nto HWC2 , which is about the linear relationship of image \nresolution. This method not only reduces the computational \neffort from quadratic with respect to the image resolution \nbut also effectively and implicitly encodes local contextual \ninformation.\n2  Related works\n2.1  Traditional\nLDCT image denoising is a research area with important \nclinical applications in medical image denoising. In the \nearly years, researchers mainly used preprocessing meth-\nods such as iterative reconstruction (IR)-based algorithms \nfor denoising LDCT images. This method combines the \nstatistical properties of the data in the sinogram domain, \nprior information in the image domain, and parameters of \nthe imaging system into a unified objective function. Using \ncompressive sensing (CS) [16], some image priors are rep-\nresented as sparse transforms to deal with low-dose, few-\nview, finite-angle, and internal CT problems, such as full \nvariational (TV) and its variants [17], non-local averaging \n(NLM) [18], dictionary learning [19], and low-ranking [20]. \nAlthough IR methods have achieved promising results, they \nhave two limitations. First, the IR techniques are less scal-\nable and migratory. Because this technique needs to be pre-\nconfigured for a specified device, users and other vendors \ndo not have access to detailed information about the specific \nscanner geometries and calibration steps. Second, the com-\nputational overhead associated with the information retrieval \ntechniques is significant. This poses a significant challenge \nin clinical applications.\nAnother option is to post-process the reconstructed LDCT \nimage, which does not depend on the original image and can \nbe applied directly to LDCT images without the need for pre-\nset modules in any CT system. Li et al. [21] used the NLM to \nreconstruct feature similarities within large neighborhoods \nin images. Inspired by sparse representation theory, Aha-\nron et al. applied dictionary learning [22] to denoise LDCT \nimages and significantly improved the denoising quality in \nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 3 of 14    61 \nthe reconstruction of abdominal images [23]. Feruglio et al. \ndemonstrated that block-matching 3D (BM3D) is effective \nfor various X-ray imaging tasks [24]. However, unlike the \nother two methods, this method does not accurately deter -\nmine the noise distribution in the image domain, which hin-\nders the user from achieving the best compromise between \nstructure preservation and noise substitution. In general, the \naccuracy of these traditional methods remains low, owing to \ndata volume limitations [25].\n2.2  Deep learning based methods\nEfficient data-driven deep learning methods have great \npotential in intelligent medicine owing to the limitations of \ndata volume and the consequent low accuracy of traditional \nmethods. It has achieved promising results in various appli-\ncations such as lesion classification, image quality improve-\nment, and organ segmentation. Deep learning can mimic \nhuman information processing by efficiently learning high-\nlevel features from pixel data through a hierarchical network \nframework. Thus, it has been widely used for LDCT image \nreconstruction. In general, deep-learning-based LDCT \nimage denoising methods can be divided into three catego-\nries: convolutional neural network (CNNs)-based methods, \ntransformer-based methods, and their combination.\n2.2.1  CNN in LDCT\nResearchers have used CNN network-based methods to \ndenoise LDCT images. For example, Chen et  al. [26] \napplied lightweight CNNs to an LDCT imaging framework \nand obtained preliminary results. Wurlf et al. [27] mapped \nthe filtered back projection (FBP) workflow to a deep CNN \narchitecture to reduce the reconstruction error to 1/2 of its \noriginal value in the case of limited-angle laminar imaging. \nChen et al. [28] proposed the REDCNN model, which uti-\nlizes convolution, deconvolution, and shortcut connections \nto construct residual coding and decoding convolutional \nneural networks that have been well evaluated for noise \nsuppression, structure preservation, and lesion detection. \nChen et al. [29] proposed the NCS-Unet model, in which \nthe exceptional characteristics of the non-subsampled con-\ntourlet transform (NSCT) and Sobel filter are introduced into \nNCS-Unet. NSCT effectively separates convolved features \ninto high- and low-frequency components, which allows the \nstrengths of both types of information to be merged. Liu \net al. [30] proposed a 3D residual convolutional network to \niteratively estimate the reconstructed images from the LDCT \nresolution. Their method avoids time-consuming itera-\ntive reconstructions. Ma et al. [31] implemented an atten-\ntion-residual dense convolutional neural network (CNN) \napproach, referred to as AttRDN. The AttRDN approach \nemploys an attention mechanism that combines feature \nfusion and global residual learning to remove noise from \ncontaminated LDCT sinograms effectively. The denoising \nprocess was achieved by first extracting noise from the noisy \nsinogram using the attention mechanism and then subtract-\ning the noise obtained from the input sinogram to restore the \ndenoised sinogram. Finally, the CT image was reconstructed \nusing filtered back projection. Xia et al. [32] proposed a \nframework called the parameter-dependent framework \n(PDF), which facilitates the simultaneous training of data \nwith various scanning geometries and dose levels. In the \nproposed framework, the scanning geometry and dose level \nare parameterized and input into two multilayer perceptrons \n(MLPs). These MLPs are utilized to regulate the feature \nmaps of a CT reconstruction network, thereby conditioning \nthe network outputs on different scanning geometries and \ndose levels. Lu et al. [33] presented a pioneering investi-\ngation into the application of a neural architecture search \n(NAS) to LDCT, which culminated in the development of \na memory-efficient multiscale and multilevel NAS solution \nnamed M3NAS. M3NAS synthesizes features from various \nscale cells to detect multiscale structural details in the image \nwhile searching for a hybrid cell and network-level struc-\nture to optimize the performance. M3NAS also substantially \nreduces model parameters and enhances inference speeds. \nHuang et al. [34] proposed a two-stage residual CNN, where \nthe first stage uses a smooth wavelet transform for texture \ndenoising and the second stage combines the mean wavelet \ntransform to enhance image structure. Tan et al. [35] pro-\nposed a new method for reducing noise in LDCT images \nusing a selective feature network and the unsupervised learn-\ning model, CycleGAN. This approach adaptively selects \nfeatures to enhance image quality. Despite the interesting \nresults of CNNs for LDCT, CNN-based models typically \nlack the ability to capture global contextual information \nowing to the characteristics of the limited sensory field of \nCNNs and, thus, are less efficient in modeling the structural \nsimilarity of the entire image [36].\n2.2.2  Transformer in LDCT\nIn recent years, the transformer-based architectures pio -\nneered by Dosovitskiy et  al. [37], which successfully \nexploited transformers for image classification tasks, have \nachieved great success in the field of computer vision. Since \nthen, several transformer-based models have been used \nto solve downstream vision tasks with excellent results, \nincluding image super-resolution [11], denoising [38], and \ncolorization [39]. In LDCT image denoising, Wang et al. \n[40] designed a Uformer with the ability to capture useful \ndependencies for image restoration using non-overlapping \nwindow-based self-attentiveness to reduce computational \neffort while employing deep convolution in the forward \nnetwork to further improve its ability to capture the local \n S.-Y. Zhang et al.\n1 3   61  Page 4 of 14\ncontext. They achieved excellent results in multiple image \nrestoration tasks (e.g., image noise reduction, image rain \nremoval, and image deblurring). Luthra et al. [41] combined \nthe learnable Sobel-Feldman operator for edge enhancement \nand built a transformer architecture-based codec network, \nEformer, for medical image denoising, based on the self-\nattentive mechanism of non-overlapping windows. Wang \net al. [42] used a more powerful token-rearranged replace-\nment convolutional neural networks to include local contex-\ntual information and proposed a convolution-free Token2To-\nken dilated vision transformer (CTformer) for LDCT image \ndenoising.\n2.2.3  Combination of transformer and CNN\nSelf-attention is widely used in CNNs for visual tasks. The \nprimary research direction is to combine VIT and CNNs to \ndesign new backbones. Graham et al. [43] mixed convnet \nand transformer in their LeVit model, and LeVit signifi -\ncantly outperformed the previous convnet and ViT models \nin terms of the speed and accuracy tradeoff. Zhang et al. \n[44] combined the local modeling capability of the residual \nconvolution layer with the non-local modeling capability \nof the Swin transformer block and then inserted them into \nthe UNet architecture as the main building block to achieve \noutstanding results in image noise reduction. CoatNet [45] \ncombines convolution and self-attention to design a novel \ntransformer module that allows the model to focus on more \nlocal and global information simultaneously. Another idea \nis to modify the transformer block using convolution, such \nas replacing the multiheaded attention with a convolutional \nlayer [46], adding additional convolutional layers in parallel \n[47] or serially [ 48] to capture local relations. In addition, \nsome researchers have used local transformer modules in \nconvolution-based network architectures to enhance access \nto global information. For example, Srinivas [49] proposed \na simple but powerful backbone architecture, BoTNet, which \nsimply replaces spatial convolution with global self-attention \nin the last three bottleneck blocks of ResNet and achieves \nstrong performance in image recognition. ConViT [50] inte-\ngrates soft convolutional induction bias through gated posi-\ntional self-attention. The CMT [51] block comprises a deep \nconvolution-based local perceptual unit and a lightweight \ntransformer module.\nWe found that these hybrid network structures combin-\ning convnet and transformer are similar in terms of design \nideas. They use convnet to extract local feature information \nand self-attention to extract local contextual information. \nInspired by these works, we integrated the advantages of \nboth CNN and transformer architectures efficiently, and \nour work helped us achieve SOTA results on LDCT image \ndenoising.\n3  Methods\n3.1  Denoising model\nOur study started from CT images obtained from low-dose \nscan data reconstructed by filtered back projection (FBP). \nThe noise distribution in CT images typically includes a \ncombination of quantum Poisson and electron Gaussian \nnoises. However, the reconstructed images always have a \ncomplex and uneven noise distribution. Furthermore, there \nis no accurate mathematical model that can describe the rela-\ntionship between NDCT and LDCT. This makes obtaining \nhigh-quality denoising results for LDCT images using tra-\nditional methods challenging.\nTherefore, the noise distribution can be more accurately \nmodeled using deep learning methods because deep learning \nis independent of the statistical distribution of image noise. \nLDCT image denoising can be simplified to address the fol-\nlowing problems. Assuming x ∈ RN ×N  represents the LDCT \nimage and y ∈ RN ×N  represents the corresponding NDCT, \nour goal is to identify a function F that maps from x to y:\nwhere F ∶ RN×N → RN×N indicates a process involving the \nelimination of image noise and artifacts.\n3.2  Network architecture\nAs shown in Fig.  1, our network uses a self-encoder struc-\nture for residual learning that includes two convolutional \nlayers, three Hformer blocks, and four scale layers. The \nscale layer has a residual connection between 2 × 2-strided \nconvolution-based down-sampling and 2 × 2-transposed \nconvolution-based up-sampling. Within the encoder, the \ndown-sampling module employs convolution to reduce the \npatch size while simultaneously increasing the number of \nchannels between each level. In contrast, the up-sampling \nmodule within the decoder utilizes transposed convolution \nto increase the patch size while concurrently reducing the \nnumber of channels between each level. This structure is not \nonly suitable for supervised learning of noise distribution \nbut also for image reconstruction and denoising tasks. Next, \nwe present the details of our study.\n3.2.1  Autoencoder\nAn autoencoder (AE) was originally developed for super -\nvised feature learning based on noisy inputs and is also \napplicable to image reconstruction. Both CNNs and trans-\nformers have shown excellent performance in image denois-\ning. However, because CNNs use local perceptual fields for \n(1)y = F (x),\nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 5 of 14    61 \nfeature capture, they cannot directly model global environ-\nments. The transformer compensates for this deficiency. \nTherefore, for LDCT, we propose a residual network com-\nbining three novel technologies, namely AE, CNNs, and \ntransformers, which originated from the work [52]. Instead \nof using fully connected layers for encoding and decoding, \nwe performed feature extraction and image reconstruction \nsymmetrically. Moreover, unlike typical encoding structures, \nit includes residual learning with shortcuts [4 ] to facilitate \nthe operation of a shallow information-focused convolu-\ntional layer and the corresponding deconvolutional layer. In \naddition, this approach solves the gradient disappearance \nproblem, such that deep models can be stably trained [53].\n3.2.2  Patch extraction\nThe training process of deep-learning models requires a \nlarge number of samples. However, this requirement is often \nnot easily satisfied in practice with adequate samples, espe-\ncially for medical imaging. In this study, we used overlap-\nping slices in the CT images. This strategy has been shown \nto be effective in previous studies, where more slices allow \nthe model to detect perceived differences in local areas and \nsignificantly increase the number of samples [54]. In our \nexperiments, we extracted fixed-size patches from LDCT \nimages and the corresponding NDCT images.\n3.2.3  Residual learning\nThe convolution operation gradually extracts information \nfrom the underlying features to the highly abstract features. \nThe deeper the network, the more abstract (semantic) fea-\ntures that can be extracted. For traditional convolutional neu-\nral networks, simply increasing the depth of the network \ncan easily result in gradient disappearance and explosion. \nCommon solutions to this issue include normalized initiali-\nzation and intermediate normalization layers. However, this \nleads to the problem of network degradation, which means \nthat as the number of layers in the network increases, the \naccuracy of the training dataset saturates or even decreases \nas the number of layers increases. This phenomenon is dif-\nferent from and overfitting does not show a decrease in the \naccuracy of the training set.\nIt is common sense that the solution space of the deeper \nnetwork structure contains the solution space of the shal-\nlow network structure, which means that the deeper network \nstructure can obtain better solutions and  perform better than \nthe shallow network. However, this is not the case because \ndeeper networks may have worse training and testing errors \nthan shallow networks. This proves that it is not due to over-\nfitting. This phenomenon is probably caused by the stochas-\ntic gradient descent strategy and the complex structure of the \ndeep network, which does not result in a globally optimal \nsolution but rather a locally optimal solution.\nTherefore, residual learning provides a new way of \nthinking: since deep networks have degeneracy problems \ncompared to shallow networks, is it possible to retain \nthe depth of deep networks and have the advantage of \nshallow networks to avoid degeneracy problems? If the \nlater layers of the deep network are learned as a constant \nmapping h(x)=x  , the model degenerates into a shallow \nnetwork. However, it is often difficult to directly learn \nFig. 1  Overall architecture of Hformer\n S.-Y. Zhang et al.\n1 3   61  Page 6 of 14\nthis constant mapping. Therefore, we require a differ -\nent approach: we redesign the network into a new form: \nH (x)= F(x)+ x→ F(x)= H (x)− x . As long as F(x)=0  , \nthis constitutes a constant mapping H (x)= x , where F(x) \nis the residual.\nResidual learning provides two methods for solving the \ndegradation problem: identity and residual mapping. The \nresidual learning structure is implemented using a forward \nneural network and shortcut linkage, where the shortcut link-\nage is equivalent to simply performing the same mapping \nwithout generating additional parameters or increasing the \ncomputational complexity. The entire network can be trained \nusing end-to-end backpropagation.\nTherefore, residual learning is used to avoid the problem \nof gradient disappearance. This allows the deep model to be \ntrained stably.\n3.2.4  Convolution block\nConsidering that shallow information contains more detailed \ninformation (contour, edge, color, texture, and shape fea-\ntures), using CNNs to extract features by sharing convo-\nlutional kernels ensures a reduced number of network \nparameters and improves model efficiency. CNNs exhibit \ntwo inherent inductive biases: translational invariance and \nlocal correlation. This feature allows CNNs to capture addi-\ntional local information. Inspired by this, we designed a \nshallow feature extraction (reconstruction) module consist-\ning primarily of depth-separable convolutions [55]. The fea-\nture layer is normalized after a depth-separable convolution \nand combined with the normalization of the standard layer \n[56]. Then, two projection convolutions are used to enhance \nthe local representation and channel dimension transforma-\ntion: A Gaussian error linear unit [57] (GELU) is connected \nafter the first projection convolution to activate it for non-\nlinear feature mapping. Finally, a residual join is used to \nsmooth the back-and-forth propagation of the information. \nThis process can be formulated as Eq. (2 ), and its architec-\nture is shown in Fig. 2.\n3.2.5  Hformer block\nThe Hformer block proposed in this study consists of a \ndepth-wise convolution (DWConv)-based perceptual module \nand a transformer module with a lightweight self-attentive \n(LSA) module, as shown in Fig.  3. These two modules are \ndescribed in detail below.\nDWConv based perceptual module. To compensate for \nthe loss in the image domain, we used DWConv with a ker-\nnel size of 7 × 7 in the convolutional perception module to \n(2)\nxi+1 = xi + Linear(GeLU(Linear(LN(DWConv 7×7(xi)))))\nFig. 2  (Color online) Architecture of convolution block\nFig. 3  (Color online) The structure of Hformer block\nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 7 of 14    61 \nprocess the input features and extract features from the local \nperceptual field in the same manner as a conventional convo-\nlution. This approach was inspired by the fact that there are \nmany similarities between local self-attention and DWConv. \nFirst, the latter also has sparse connectivity; that is, the com-\nputation exists only within the kernel size, and there is no \nconnection between individual channels. DWConv also \nexhibited weight-sharing properties. However, convolution \nkernels are shared across all spatial locations and different \nchannels use different convolution kernels, which signifi-\ncantly reduces the number of parameters. In addition, the \nDWConv kernel is a scientific training parameter that is \nfixed once training is completed, whereas the computation of \nattention is a dynamic process. Local self-attention requires \npositional coding to compensate for the lost positional infor-\nmation, whereas DWConv does not.\nLight-weight self-attention. The transformer’s origi-\nnal self-attention has a huge overhead, which is a huge \nburden on computational power. To address this difficulty \nand obtain valid local contextual information, we reduced \nthe dimensionality of the feature map in our Hformer mod-\nule and attempted to compute the attention in the channel \ndimension. Given an input X ∈ RH×W ×C , the original self-\nattentive mechanism first generates the corresponding query \n(Q), key (K), and value (V ) (of the same size as the origi-\nnal input) and then generates a weight matrix of size RN×N \nthrough the dot product of Q and K.\nwhere WQ, WK and W V are the linear operations. Previous \nself-attention calculations were performed along the spatial \ndimension between Q and K, and the results are as follows:\nwhere the scaling factor 1√\ndk\n is based on network depth. How-\never, this process usually requires large computational \ncapacity (video memory) owing to the large size of the input \nfeatures, which makes it difficult to train and deploy the net-\nwork. Therefore, we used the maximum pooling method to \ndownsample the generation of K and Q separately to obtain \ntwo relatively small features, K/uni2032.var and Q/uni2032.var:\n(3)\n⎧\n⎪\n⎨\n⎪⎩\nQ = W QX\nK = W KX\nV = W VX\n(4)Attn(Q,K,V)= softmax\n�\nQK T\n√\ndk\n�\nV,\n(5)K� = Maxpool (K)∈R\nHW\nK2 ×C,\n(6)Q� = Maxpool (Q)∈R\nHW\nK2 ×C.\nTo further reduce the overhead of the model and the algo-\nrithm complexity to a linear relationship with the image \nresolution, we used the attention computed on the chan-\nnel dimension to implicitly encode patch-based global \ninteractions.\nWe transpose K/uni2032.var and apply the dot product to K/uni2032.varT and Q/uni2032.var \nin the channel dimension, and the computed results are sup-\nplemented with Softmax to obtain the attention score matrix \nAttnchannels with dimension C × C , which is applied to V and \nobtain the final attention map. The computational effort of \nthis step is C2(HW) , which is linear in image resolution and \nsubstantially reduces complexity. The attention operation for \nchannel dimensions can be expressed as follows:\n4  Experiment\nDataset. We used the publicly released clinical dataset from \nthe 2016 NIH-AAPM Mayo Clinic LDCT Grand Challenge \n[58] for model training and testing. The dataset consisted of \n2378 low-dose (quarter) images and 2378 normal-dose (full) \nCT images from 10 anonymous patients with 3.0-mm whole-\nlayer slices. We selected patient L506 data for testing, which \ncontained 211 slice images numbered from 000 to 210. We \nused the data from the remaining nine patients for model \ntraining.\nModel training and optimization. Our network is an end-\nto-end mapping M from LDCT images to NDCT images. For \nthe given training data P = {(X 1 ,Y1 ),(X2 ,Y2 ),… ,(Xn,Yn)} \nwhere X i and Y i denote LDCT and NDCT image patches, \nrespectively, n is the total number of training samples. The \nmodel performance can be improved by minimizing the loss \nL(X, /u1D703) between the output CT image and the reference NDCT \nimage, where /u1D703 refers to learnable parameters. This process can \nbe achieved by optimizing the mean square error (MSE) loss \nfunction, as shown in Eq. (8).\nExperiment setup. The experiments were run on CentOS \n7.5 with an Intel Xeon Scalable Gold 6240 CPU@2.6 GHz, \nusing PyTorch 1.11.0, and CUDA 11.2.0. The model was \ntrained using eight NVIDIA Tesla V100 32GB GPU HBM2. \nFor each image, four blocks randomly extracted from all \navailable slices were used for training. The batch size is 16 \nthrough 4000 epochs. The ADAM-W optimizer was used to \nminimize the mean squared error loss, and the learning rate \nwas 1.0 × 10 −5.\n(7)Attnchannels(Q�,K�,V�)=softmax\n�\nK�TQ�\n√\ndk\n�\nV\n(8)L(/u1D703)= 1\nN /uni007C.var/uni007C.varYi− M (Xi)/uni007C.var/uni007C.var\n S.-Y. Zhang et al.\n1 3   61  Page 8 of 14\n4.1  Denoising performance\nThe performance of our net was compared with other SOTA \nmodels, such as RED-CNN [28], SCUNet [44] Uformer [40], \nDU-GAN [59], and CTformer [42]. The selected models \nwere popular LDCT or natural image denoising models pub-\nlished in top journals and conferences. SCUNet and Uformer \nare mainstream deep learning-based image-noise reduction \nalgorithms. Red-CNN is the masterpiece of the convolu-\ntional neural network-based CT noise reduction algorithm, \nand CTformer is the most advanced noise reduction algo-\nrithm based on the LDCT dataset, which has excellent \nresults in image noise reduction tasks. We retrained all the \nmodels based on their officially disclosed codes.\nFor quantitative evaluation, we selected the root mean \nsquare error (RMSE), peak signal-to-noise ratio (PSNR), \nand structural similarity index (SSIM) as the quantitative \nevaluation metrics for image quality. RMSE is a measure of \naccuracy that can be used to compare the predictive perfor -\nmance of different models on the same dataset and can mag-\nnify the error magnitude between the reconstructed image \nand the ground truth image (the larger the error the larger the \nRMSE). This representation is shown in Eq. (9):\nPSNR provides an objective criterion for describing the level \nof image distortion and noise (shown in Eq. 10). The larger \nthe value, the smaller the difference between the recon-\nstructed and reference images.\nSSIM evaluates the similarity of two images in three ways, \nand SSIM is defined as Eq. (11).\nwhere /u1D707im and /u1D7072\nim are the mean and variance of the recon-\nstructed image, respectively; /u1D707gt and /u1D7072\ngt are the mean and \nvariance of the ground truth image, respectively; Σim, gt  is the \ncovariance between the reconstructed and ground truth \n(9)RMSE = 1\nm\nm/uni2211.s1\ni=1\n(im − gt)2.\n(10)\nPSNR = 10 × log10\n/parenleft.s3(2n − 1)2\nMSE\n/parenright.s3\nMSE = 1\nm\nm/uni2211.s1\ni=1\n(im − gt)2\n(11)\nSSIM(im,gt) = L(im,gt)C (im,gt)S(im,gt)\nL(im,gt) =\n2/u1D707im /u1D707gt + c1\n/u1D7072\nim + /u1D7072\ngt + c1\nC (im,gt) =\n2Σim Σgt + c2\nΣ2\nim +Σ 2\ngt + c2\nS(im,gt) =\nΣim, gt + c3\nΣim Σgt + c3\nimages; c1 , c2 , and c3 are constants. The structural similarity \nindex measures the degree of image distortion and the \ndegree of similarity between two images. Unlike MSE and \nPSNR, which measure the absolute error, SSIM is a percep-\ntual model, that is, it is more in line with the intuition of \nhuman eyes. Its value ranges from zero to one. The higher \nthe value of SSIM, the higher the similarity between the \nreconstructed and ground truth images. The number of train-\nable parameters (Param) was used to evaluate model com-\nplexity. Table 1 lists the average metrics of all models for \nL506 patients. Our model has the lowest average RMSE \namong the SOTA methods. This indicates that our model \neffectively suppresses noise and artifacts and maintains a \nhigh degree of spatial smoothing. In terms of information \nreconstruction, our model has the best SSIM compared to its \ncompetitors, preserving the structural details of the recon -\nstructed images. Meanwhile, CTformer had fewer trainable \nparameters than ours. Therefore, we conclude that our net-\nwork is the best noise eliminator compared to its \ncompetitors.\n4.2  Visual evaluation\nTo evaluate the denoising ability of the Hformer proposed \nin this study with the above comparison method, we pro-\nvided slices 034 and 057, two representative results from \na test set consisting of L506 patient data and their corre -\nsponding ROI images. The results are shown in Figs.  4, 5, \n6 and 7 . The corresponding metrics are listed in Tables  2 \nand 3. Figures 4 and 6 show the results of the abdominal CT \nimages. The noise shown in Fig. 4a is primarily distributed \nwithin the abdomen. The outline of the organs and details \nof the tissue structure were significantly by noise. Obvious \nstreaking artifacts can be observed in the spine and liver, \nwhich greatly affect the clinical diagnosis of lesion areas. It \nis easy to see that convolutional network-based RED-CNN \neffectively eliminates most of the noise and artifacts and is \nbetter at retaining the details.\nHowever, RED-CNN is less effective in the structural \nrecovery of images because it has computational charac-\nteristics that can extract high-frequency information more \neffectively, such as image texture details. Moreover, RED-\nCNN is limited by the size of the perceptual field and cannot \neffectively extract more global information. From the results, \nwe can observe that there is over-smoothing of the detailed \ntextures in Uformer and CTformer. This is due to the lack \nof a convolution layer, which results in blurred CT images.\nFor noise reduction and the ability to retain detailed struc-\ntures, the Hformer proposed in this paper also outperforms \nSCUNet. The denoising performance in the liver and lesion \nregions in Fig. 4f is significantly better than that in Fig.  4c. \nCompared with SCUNet based on a parallel structure com-\nbined with convolution and self-attention, Hformer based on \nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 9 of 14    61 \na multiscale convolution module and lightweight self-atten-\ntion exhibits stronger generalization ability and is superior \nin reconstructing LDCT.\nTo further demonstrate the performance of Hformer, \nwe provide a magnified image of the ROI marked with a \nrectangular dashed line in Fig.  4, as shown in Fig.  5. The \nFig. 4  Results of abdominal slice L506-034 from the testing set using different methods. The display window ranges from – 160 to 240 HU\nFig. 5  The corresponding ROI of Fig. 4\n S.-Y. Zhang et al.\n1 3   61  Page 10 of 14\narrow-marked region is a piece of tissue with a uniform \ndensity distribution. However, almost none of the other \nmethods, except Hformer and CTformer correctly recon-\nstructed the internal details of the lesion region. SCUNet, \nUformer, RED-CNN, and CTformer introduced more noise \ninto the image, making it difficult to distinguish the density \ndistribution of this tissue. In our study, DU-GAN and the \nproposed Hformer were effective in recovering the details \nand overall structure, and Hformer performed better than \nDU-GAN in suppressing artifacts.\nAnother result for the test set is shown in Fig.  6, and its \nROI is shown in Fig. 7. Owing to the reduced radiation dose, \nthe structures of many soft tissues are more affected by noise \nduring reconstruction. The internal details of organs are dif-\nficult to distinguish accurately. Although Uformer and SCU-\nNet reconstructed the organ contours well, and the organ \nboundaries were clearly visible, a large amount of noise was \ngenerated inside the organ. As shown in Fig. 7, only Hformer \nTable 1  Quantitative evaluation results of different methods on L506 \nusing the number of learnable parameters (#param.), RMSE, SSIM, \nand PSNR. Our results are the bold-faced numbers\n#param. (M) RMSE SSIM PSNR\nLDCT – 14.2416 0.8759 29.2489\nSCUNet 13 9.4381 0.9066 32.6993\nUformer 12 9.3102 0.9106 33.0623\nRED-CNN 1.85 9.0664 0.9109 33.0695\nCTformer 1.45 9.0233 0.9121 33.0952\nDU-GAN 114.61 8.9464 0.9118 33.1859\nHformer 1.65 8.6956 0.9163 33.4405\nTable 2  Quantitative results of patient L506’s abdominal section 034\nNetwork RMSE SSIM PSNR\nLDCT 12.1360 0.8804 30.3597\nSCUNet 8.4252 0.9126 33.5296\nUformer 8.0657 0.9193 33.9083\nRED-CNN 8.0850 0.9172 33.8876\nCTformer 7.9236 0.9190 34.0627\nDU-GAN 7.9519 0.9181 34.0318\nHformer 7.6457 0.9235 34.3729\nTable 3  Quantitative results of patient L506’s abdominal section 057\nNetwork RMSE SSIM PSNR\nLDCT 16.2190 0.8424 27.8407\nSCUNet 10.3276 0.8821 31.7612\nUformer 10.3909 0.8842 31.7081\nRED-CNN 10.0407 0.8859 32.0059\nCTformer 10.1807 0.8835 31.8857\nDU-GAN 9.9153 0.8866 32.1151\nHformer 9.7170 0.8915 32.2906\nFig. 6  Results of abdominal slice L506-057 from the testing set using different methods. a low dose. The display window ranges from – 160 to \n240 HU\nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 11 of 14    61 \nand CTformer completely reconstructed the internal ves-\nsels of the liver, and the details of Hformer are more clearly \ndepicted. The other networks caused different degrees of \nsmoothing of the textural details of the soft tissues. Although \nCTformer can also obtain a better tissue structure, it is sig-\nnificantly inferior to Hformer in terms of noise suppression \nperformance. In summary, Hformer can effectively use the \nadvantages of convolution and self-attention to effectively \nreconstruct the tissue structure while reducing noise and \npreserving more clinically useful information.\n4.3  Ablation study\nImpact of Hformer blocks. Hformer blocks are used in our \nnetwork to enhance feature integration during the feature \nextraction phase. Compared with VIT, which uses only the \nself-attention mechanism, the Hformer block integrates the \ninherent advantages of convolution and self-attentiveness \nin the feature extraction process. To verify the effectiveness \nof this component, a single ViT model without the Hformer \nblock was designed. We use convolution only in the down-\nsampling stage, with a convolution kernel size of 3 × 3 and \na step size of 2. We subsequently employ a five-layer trans-\nform for feature extraction and denoising purposes, utilizing \nan identical embedding size. The results of the visual com-\nparison are illustrated in Fig.  9a–d. Finally, we can clearly \nsee that Sole-ViT brings additional speckle organization by \nexamining the connected area within the marked region in \nFig. 9e–h. In addition, Fig. 8 and Table 4 show that Hformer \nFig. 7  The corresponding ROI of Fig. 6\nFig. 8  LOSS visualization of Hformer and Sole-ViT on case L506 \nafter different iterations\nTable 4  Quantitative evaluation results of the Sole-ViT, the Hformer, \nand the Hformer with different numbers of blocks\nNET Block #param. (M) RMSE SSIM PSNR\nHformer 1 1.65 8.6956 0.9163 33.4405\nSole-ViT 1 1.99 9.2224 0.9089 32.9161\nHformer 2 1.68 8.7677 0.9154 33.3664\nHformer 4 1.75 8.8271 0.9148 33.3046\n S.-Y. Zhang et al.\n1 3   61  Page 12 of 14\nconverges faster than Sole-Vit with a difference of 0.5244 for \nPSNR, 0.0074 for SSIM, and 0.5268 for RMSE.\nImpact of Hformer numbers. We investigated the \nimpact on the network performance by adjusting the num-\nber of Hformer modules in Fig.  1. The number of modules \nwas set to 1, 2, and 4 blocks. As the number of data blocks \nincreases, the depth of the network increases and the com-\nputational cost also increases slightly. Table  4 shows that \nonly one Hformer module yields better performance than \nthe Hformer with more blocks.\n5  Conclusion\nIn this study, we designed a novel fast LDCT denoising \nmodel. The core of the model is referred to as the Hformer, \nwhich combines the advantages of both CNN and local \nself-attention. We used the well-known dataset AAPM-\nMayo Clinic Low-Dose CT Grand Challenge Dataset to \nevaluate and validate the performance of our proposed \nHformer and compare it with the latest SOTA method. \nThe simulation results show that our model achieves excel-\nlent results in terms of noise suppression and structural \nprotection, with an effective reduction in the number of \ntraining parameters.\nAuthor Contributions All authors contributed to the study conception \nand design. Material preparation, data collection and analysis were \nperformed by Shi-Yu Zhang and Zhao-Xuan Wang, who contributed \nequally. The first draft of the manuscript was written by Shi-Yu Zhang, \nand all authors commented on previous versions of the manuscript. All \nauthors read and approved the final manuscript.\nData Availability Statement The data that support the findings of \nthis study are openly available in Science Data Bank at https://www.\ndoi.org/10.57760/sciencedb.j00186.00063 and http://resolve.pid21.\ncn/31253.11.sciencedb.j00186.00063.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Y. Yang, W. Fang, X. Huang et al., Static superconducting gan-\ntry-based proton CT combined with X-ray CT as prior image \nfor FLASH proton therapy. Nucl. Sci. Tech. 34(1), 11 (2023). \nhttps:// doi. org/ 10. 1007/ s41365- 022- 01163-2\n 2. D. Brenner, E. Hall, Computed tomography-an increasing \nsource of radiation exposure. New Engl. J. Med. 357, 2277–\n2284 (2007). https:// doi. org/ 10. 1056/ NEJMr a0721 49\nFig. 9  The performance of Hformer on case L506 with lesion Pelvic Bone. a LDCT, b Solve-ViT, c Hformer with 2 blocks, d Hformer with 4 \nblocks. e–h are the corresponding magnified ROIs from (a–d)\nHformer: highly efficient vision transformer for low-dose CT denoising  \n1 3 Page 13 of 14    61 \n 3. J. Jing, W. Xia, M. Hou et al., Training low dose CT denoising \nnetwork without high quality reference data. Phy. Med. Bio. 67, \n84002 (2022). https:// doi. org/ 10. 1088/ 1361- 6560/ ac5f70\n 4. K. He, X. Zhang, S. Ren et al., Deep residual learning for \nimage recognition, in Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition (2016), pp. 770–778. \nhttps:// doi. org/ 10. 1109/ CVPR. 2016. 90\n 5. F. Fan, D. Wang, H. Guo et al., On a sparse shortcut topology of \nartificial neural networks. IEEE Trans. Artif. Intell. 3 , 595–608 \n(2021). https:// doi. org/ 10. 1109/ TAI. 2021. 31281 32\n 6. X. Wang, R. Girshick, A. Gupta et al., Non-local neural net-\nworks, in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (2018), pp. 7794–7803\n 7. A. Vaswani, N. Shazeer, N. Parmar et al., Attention is all you \nneed, in 31st Conference on Neural Information Processing Sys-\ntems (NIPS 2017), Long Beach, CA, USA (2017). https:// doi. org/ \n10. 48550/ arXiv. 1706. 03762\n 8. Z. Liu, Y. Lin, Y. Cao et al., Swin transformer: Hierarchical \nvision transformer using shifted windows, in Proceedings of \nthe IEEE/CVF International Conference on Computer Vision  \n(2021), pp. 10012–10022. https:// doi. org/ 10. 48550/ arXiv. 2103. \n14030\n 9. L. Yuan, Y. Chen, T. Wang et al., Tokens-to-token vit: Training \nvision transformers from scratch on imagenet, in Proceedings \nof the IEEE/CVF International Conference on Computer Vision  \n(2021), pp. 558–567. https:// doi. org/ 10. 48550/ arXiv. 2101. 11986\n 10. F. Fan, J. Xiong, M. Li et al., On interpretability of artificial neural \nnetworks: a survey. IEEE Trans. Radiat. Plasma Medical Sci. 5, \n741–760 (2021). https:// doi. org/ 10. 1109/ TRPMS. 2021. 30664 28\n 11. F. Yang, H. Yang, J. Fu, Learning texture transformer network for \nimage super-resolution, in Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (2020), pp. \n5791–5800. https:// doi. org/ 10. 48550/ arXiv. 2006. 04139\n 12. H. Wu, B. Xiao, N. Codella et al., Cvt: Introducing convolutions \nto vision transformers, in Proceedings of the IEEE/CVF Inter -\nnational Conference on Computer Vision (2021), pp. 22–31. \nhttps:// doi. org/ 10. 48550/ arXiv. 2103. 15808\n 13. M. Chen, A. Radford, and R. Child et al., Generative pretraining \nfrom pixels, in International Conference on Machine Learning. \nPMLR (2020), pp. 1691–1703\n 14. S. Abnar, W. Zuidema, Quantifying attention flow in transform-\ners. arXiv:  10485 50/ arXiv. 2005. 00928 (2020). https:// doi. org/ \n10. 48550/ arXiv. 2005. 00928\n 15. G. Montavon, A. Binder, S. Lapuschkin et al., Layer-wise rel-\nevance propagation: an overview, in Explainable AI: Interpreting, \nExplaining and Visualizing Deep Learning (2019), pp. 193–209\n 16. D. Donoho, Compressed sensing. IEEE Trans. Inf. Theory 52, \n1289–1306 (2006). https:// doi. org/ 10. 1109/ TIT. 2006. 871582\n 17. E. Sidky, X. Pan, Image reconstruction in circular cone-beam \ncomputed tomography by constrained, total-variation minimiza-\ntion. Phys. Med. Biol. 53, 4777–4807 (2013). https:// doi. org/ 10. \n1088/ 0031- 9155/ 53/ 17/ 021\n 18. Y. Chen, D. Gao, N. Cong, Bayesian statistical reconstruction for \nlow-dose x-ray computed tomography using an adaptive-weight-\ning nonlocal prior. Comput. Med. Imag. Graphics 33, 495–500 \n(2009). https:// doi. org/ 10. 1016/j. compm edimag. 2008. 12. 007\n 19. Q. Xu, H. Yu, X. Mou, Low-dose X-ray CT reconstruction via \ndictionary learning. IEEE Trans. Med. Imaging 31, 1682–1697 \n(2012). https:// doi. org/ 10. 1109/ TMI. 2012. 21956 69\n 20. J. Cai, X. Jia, H. Gao et al., Cine cone beam ct reconstruction \nusing low-rank matrix factorization: Algorithm and a proof-\nof-principle study. arXiv: 1204. 3595 (2012). https:// doi. org/ 10. \n48550/ arXiv. 1204. 3595\n 21. Z. Li, L. Yu, J. Trzasko et al., Adaptive nonlocal means filter -\ning based on local noise level for ct denoising. Med. Phys. 41, \n011908 (2014). https:// doi. org/ 10. 1118/1. 48516 35\n 22. M. Aharon, M. Elad, A. Bruckstein et al., K-svd: an algorithm \nfor designing overcomplete dictionaries for sparse representa -\ntion. IEEE T. Signal Proc. 54 , 4311–4322 (2006). https://  doi. \norg/ 10. 1109/ TSP. 2006. 881199\n 23. Y. Chen, X. Yin, L. Shi et al., Improving abdomen tumor low-\ndose ct images using a fast dictionary learning based process-\ning. Phys. Med. Biol. 58, 5803 (2013). https:// doi. org/ 10. 1088/ \n0031- 9155/ 58/ 16/ 5803\n 24. P. Feruglio, C. Vinegoni, J. Gros, Block matching 3d random \nnoise filtering for absorption optical projection tomography. \nPhys. Med. Biol. 55, 5401–5415 (2010). https:// doi. org/ 10.  \n1088/ 0031- 9155/ 55/ 18/ 009\n 25. P. Kaur, G. Singh, P. Kaur, A review of denoising medical \nimages using machine learning approaches. Curr. Med. Imaging \nRev. 14, 675–685 (2018). https:// doi. org/ 10. 2174/ 15734 05613 \n66617 04281 54156\n 26. H. Chen, Y. Zhang, W. Zhang et al., Low-dose ct via convolu-\ntional neural network. Biomed. Opt. Express 8, 679–694 (2017). \nhttps:// doi. org/ 10. 1364/ BOE.8. 000679\n 27. T. Würfl, F. Ghesu, V. Christlein et al., Deep learning computed \ntomography. in International conference on medical image com-\nputing and computer-assisted intervention , in Medical Image \nComputing and Computer-Assisted Intervention-MICCAI 2016. \nMICCAI 2016, ed by S. Ourselin, L. Joskowicz, M. Sabuncu \net al. (Springer, 2016), pp. 432–440. https:// doi. org/ 10. 1007/  \n978-3- 319- 46726-9_ 50\n 28. H. Chen, Y. Zhang, M. Kalra et al., Low-dose ct with a residual \nencoder-decoder convolutional neural network. IEEE T. Med. \nImaging 36, 2524–2535 (2017). https:// doi. org/ 10. 1109/ TMI. \n2017. 27152 84\n 29. K. Chen, L. Zhang, J. Liu et al., Robust restoration of low-dose \ncerebral perfusion CT images using NCS-Unet. Nucl. Sci. Tech. \n33, 30 (2022). https:// doi. org/ 10. 1007/ s41365- 022- 01014-0\n 30. J. Liu, Y. Zhang, Q. Zhao et al., Deep iterative reconstruction \nestimation (dire): approximate iterative reconstruction esti-\nmation for low dose ct imaging. Phys. Med. Biol. 64, 135007 \n(2019). https:// doi. org/ 10. 1088/ 1361- 6560/ ab18db\n 31. Y. Ma, Y. Ren, P. Feng et al., Sinogram denoising via attention \nresidual dense convolutional neural network for low-dose com-\nputed tomography. Nucl. Sci. Tech. 32, 41 (2021). https:// doi.  \norg/ 10. 1007/ s41365- 021- 00874-2\n 32. W. Xia, Z. Lu, Y, Huang, et al., CT Reconstruction with PDF: \nparameter-dependent framework for multiple scanning geome-\ntries and dose levels. IEEE Trans. Med. Imaging 40, 3065–3076 \n(2021). https:// doi. org/ 10. 1109/ TMI. 2021. 30858 39\n 33. Z. Lu, W. Xia, Y. Huang et al., M3NAS: multi-scale and multi-\nlevel memory-efficient neural architecture search for low-dose \nCT denoising. IEEE Trans. Med. Imaging 42, 850–863 (2022). \nhttps:// doi. org/ 10. 1109/ TMI. 2022. 32192 86\n 34. L. Huang, H. Jiang, S. Li et al., wo stage residual cnn for texture \ndenoising and structure enhancement on low dose ct image. \nComput. Meth. Prog. Biomed. 184, 105115 (2020). https:// doi.  \norg/ 10. 1016/j. cmpb. 2019. 105115\n 35. C. Tan, Q. Chao, M. Yang et al., A selective kernel-based cycle-\nconsistent generative adversarial network for unpaired low-dose \nCT denoising. Precis. Clin. Med. 5 , pbac011 (2022). https:// doi. \norg/ 10. 1093/ pcmedi/ pbac0 11\n 36. Z. Zhang, L. Yu, X. Liang et al., Transct: dual-path transformer \nfor low dose computed tomography, in International Conference \non Medical Image Computing and Computer-Assisted Intervention \n(Springer, 2021), pp. 55–64. https:// doi. org/ 10. 48550/ arXiv. 2103. \n00634\n 37. A. Dosovitskiy, L. Beyer, A. Kolesnikov et al., An image is worth \n16 ×16 words: Transformers for image recognition at scale. \narXiv:  2010. 11929 (2020). https:// doi. org/ 10. 48550/ arXiv. 2010. \n11929\n S.-Y. Zhang et al.\n1 3   61  Page 14 of 14\n 38. H. Chen, Y. Wang, T. Guo et al., Pre-trained image processing \ntransformer, in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, Vol. 12 (2021), pp. \n299–310. https:// doi. org/ 10. 48550/ arXiv. 2012. 00364\n 39. M. Kumar, D. Weissenborn, N. Kalchbrenner, Colorization trans-\nformer (2021). https:// doi. org/ 10. 48550/ arXiv. 2102. 04432\n 40. Z. Wang, X. Cun, J. Bao et al., Uformer: a general u-shaped trans-\nformer for image restoration, in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition (2022), \npp. 17683–17693\n 41. A. Luthra, H. Sulakhe, T. Mittal et al., Eformer: edge enhance-\nment based transformer for medical image denoising. arXiv:  2109. \n08044 (2021). https:// doi. org/ 10. 48550/ arXiv. 2109. 08044\n 42. D. Wang, F. Fan, Z. Wu et al., Ctformer: convolution-free token-\n2token dilated vision transformer for low-dose ct denoising. arXiv:  \n2202. 13517 (2022). https:// doi. org/ 10. 48550/ arXiv. 2202. 13517\n 43. B. Graham, A. El-Nouby, H. Touvron et al., Levit: a vision trans-\nformer in convnet’s clothing for faster inference. in 2021 IEEE/\nCVF International Conference on Computer Vision (ICCV)  \n(2021), pp. 12239–12249. https:// doi. org/ 10. 48550/ arXiv. 2104. \n01136\n 44. K. Zhang, Y. Li, J. Liang et al., Practical blind denoising via swin-\nconv-unet and data synthesis. arXiv:  2203. 13278 (2022). https:// \ndoi. org/ 10. 48550/ arXiv. 2203. 13278\n 45. Z. Dai, H. Liu, Q.V. Le et al., Coatnet: marrying convolution and \nattention for all data sizes. Adv. Neur. Inform. Proc. Syst. 34, \n3965–3977 (2021). https:// doi. org/ 10. 48550/ arXiv. 2106. 04803\n 46. F. Wu, A. Fan, A. Baevski et al., Pay less attention with light-\nweight and dynamic convolutions. arXiv:  1901. 10430 (2019). \nhttps:// doi. org/ 10. 48550/ arXiv. 1901. 10430\n 47. Z. Wu, Z. Liu, J. Lin et al., Lite transformer with long-short range \nattention. arXiv:  2004. 11886 (2020). https:// doi. org/ 10. 48550/ \narXiv. 2004. 11886\n 48. A. Gulati, J. Qin, C. Chiu et al., Conformer: Convolution-aug-\nmented transformer for speech recognition. arXiv:   2005. 08100 \n(2020). https:// doi. org/ 10. 48550/ arXiv. 2005. 08100\n 49. A. Srinivas, T. Lin, N. Parmar et al., Bottleneck transformers for \nvisual recognition, in Proceedings of the IEEE/CVF Conference \non Computer Vision and Pattern Recognition (2021), pp. 16519–\n16529. https:// doi. org/ 10. 48550/ arXiv. 2101. 11605\n 50. S. d’ Ascoli, H. Touvron, M. L. Leavitt et al., Convit: improving \nvision transformers with soft convolutional inductive biases, in \nInternational Conference on Machine Learning. PMLR (2021), \npp. 2286–2296. https:// doi. org/ 10. 48550/ arXiv. 2107. 06263\n 51. J. Guo, K. Han, H. Wu et al., Cmt: convolutional neural networks \nmeet vision transformers, in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (2022), pp. \n12175–12185. https:// doi. org/ 10. 48550/ arXiv. 2107. 06263\n 52. X. Mao, C. Shen, Y. Yang, Image restoration using very deep \nconvolutional encoder-decoder networks with symmetric skip \nconnections (2016). https:// doi. org/ 10. 48550/ arXiv. 1603. 09056\n 53. K. He, X. Zhang, S. Ren, Identity mappings in deep residual net-\nworks, in European Conference on Computer Vision, vol. 9908 \n(Springer, Cham, 2016), pp. 630–645\n 54. J. Xie, L. Xu, E. Chen, Image denoising and inpainting with deep \nneural networks, in NIPS’12: Proceedings of the 25th Interna-\ntional Conference on Neural Information Processing Systems, vol. \n1 (2012), pp. 341–349\n 55. Q. Han, Z. Fan, Q. Dai et al., On the connection between local \nattention and dynamic depth-wise convolution, in International \nConference on Learning Representations . arXiv:  2106.  04263 \n(2021). https:// doi. org/ 10. 48550/ arXiv. 2106. 04263\n 56. J. Ba, J. Kiros, G. Hinton et al., Layer normalization. arXiv:  1607. \n06450 (2016). https:// doi. org/ 10. 48550/ arXiv. 1607. 06450\n 57. D. Hendrycks, K. Gimpel, Gaussian error linear units (gelus). \narXiv:  1606. 08415 (2016). https:// doi. org/ 10. 48550/ arXiv. 1606. \n08415\n 58. C. McCollough, A. Bartley, R. Carter et al., Low-dose CT for the \ndetection and classification of metastatic liver lesions: results of \nthe 2016 low dose ct grand challenge. Med. Phys. 44, e339–e352 \n(2017). https:// doi. org/ 10. 1002/ mp. 12345\n 59. Z. Huang, J. Zhang, Y. Zhang et al., DU-GAN: Generative adver-\nsarial networks with dual-domain U-Net-based discriminators for \nlow-dose CT denoising. IEEE T. Instrum. Meas. 71, 1–12 (2021). \nhttps:// doi. org/ 10. 1109/ TIM. 2021. 31287 03",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6670085191726685
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6532031297683716
    },
    {
      "name": "Noise reduction",
      "score": 0.6404052972793579
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6128497123718262
    },
    {
      "name": "Transformer",
      "score": 0.5746108293533325
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5637978315353394
    },
    {
      "name": "Deep learning",
      "score": 0.5392635464668274
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.44555428624153137
    },
    {
      "name": "Feature extraction",
      "score": 0.43030327558517456
    },
    {
      "name": "Engineering",
      "score": 0.07796654105186462
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210131271",
      "name": "Institute of Modern Physics",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17145004",
      "name": "Northwestern Polytechnical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200296433",
      "name": "Chinese Academy of Medical Sciences & Peking Union Medical College",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210089285",
      "name": "Ji Hua Laboratory",
      "country": "CN"
    }
  ],
  "cited_by": 23
}