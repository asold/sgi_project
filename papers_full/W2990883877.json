{
  "title": "Improving the role of language model in statistical machine translation (Indonesian-Javanese)",
  "url": "https://openalex.org/W2990883877",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2312148585",
      "name": "Herry Sujaini",
      "affiliations": [
        "Tanjungpura University"
      ]
    },
    {
      "id": "https://openalex.org/A2312148585",
      "name": "Herry Sujaini",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2786253471",
    "https://openalex.org/W2782030774",
    "https://openalex.org/W2893376855",
    "https://openalex.org/W2101451733",
    "https://openalex.org/W2300537905",
    "https://openalex.org/W2884488303",
    "https://openalex.org/W2152633844",
    "https://openalex.org/W2420489898",
    "https://openalex.org/W2048418079",
    "https://openalex.org/W1885811119",
    "https://openalex.org/W2944050646",
    "https://openalex.org/W2156816974",
    "https://openalex.org/W2086792987",
    "https://openalex.org/W2463497863",
    "https://openalex.org/W6650252233",
    "https://openalex.org/W6605210145",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W6680857759",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4247168274",
    "https://openalex.org/W4232792603",
    "https://openalex.org/W2140574400",
    "https://openalex.org/W2330486491",
    "https://openalex.org/W126222424",
    "https://openalex.org/W1999976944"
  ],
  "abstract": "The statistical machine translation (SMT) is widely used by researchers and practitioners in recent years. SMT works with quality that is determined by several important factors, two of which are language and translation model. Research on improving the translation model has been done quite a lot, but the problem of optimizing the language model for use on machine translators has not received much attention. On translator machines, language models usually use trigram models as standard. In this paper, we conducted experiments with four strategies to analyze the role of the language model used in the Indonesian-Javanese translation machine and show improvement compared to the baseline system with the standard language model. The results of this research indicate that the use of 3-gram language models is highly recommended in SMT.",
  "full_text": "International Journal of Electrical and Computer Engineering (IJECE) \nVol. 10, No. 2, April 2020, pp. 2102~2109 \nISSN: 2088-8708, DOI: 10.11591/ijece.v10i2.pp2102-2109      2102 \n  \nJournal homepage: http://ijece.iaescore.com/index.php/IJECE \nImproving the role of language model in statistical machine \ntranslation (Indonesian-Javanese) \n \n \nHerry Sujaini \nDepartement of Informatics, Tanjungpura University, Idonesia \n \n \nArticle Info  ABSTRACT  \nArticle history: \nReceived May 4, 2019 \nRevised Oct 30, 2019 \nAccepted Nov 15, 2019 \n \n The statistical machine translation (SMT) is widely used by researchers and \npractitioners in recent years . SMT works with quality that is determined by \nseveral important factors, two of which are language and translation model. \nResearch on improving the translation model has been done quite a lot, but \nthe problem of optimizing the language model for use on machine translators \nhas not received much attention. On translator machines, language models \nusually use trigram models as standard . In this paper , we conducted \nexperiments with four strategies to analyze the role of the language model \nused in the Indonesian-Javanese translation machine and show improvement \ncompared to the baseline system with the standard language model.  \nThe results of this research indicate that the use of 3 -gram language models \nis highly recommended in SMT. \nKeywords: \nIndonesian-Javanese \nlanguage model  \nStatistical machine translation \n \nCopyright © 2020 Institute of Advanced Engineering and Science.  \nAll rights reserved. \nCorresponding Author: \nHerry Sujaini,  \nDepartment of Informatics, \nTanjungpura University, \nJl. Prof.Dr.H. Hadari Nawawi, Pontianak 78124, Indonesia. \nEmail: hs@unttan.ac.id \n \n \n1. INTRODUCTION  \nStatistical machine translation (SMT) or known as statistical-based machine translation , \nis a paradigm of machine translation where the interpretation is created dependent  on a statistical model \nwhich parameters come from bilingual corpus (parallel corpus) analysis. Corpus is a collection or sample of \nwritten or oral text in the form of data that can be read by using a set of machines and can be noted in  \nthe form of various lingu istic information forms [1]. A quality corpus gre atly influences the outcome of  \na statistical or neural -based translator machine. Many previous researchers have experimented with \nimproving the quality of the corpus [2-6]. \nThe best hypothesis for each input of sentence f is the goal of bilingual corpus analysis by: \n \n (1) \n \n ( ̅| ̅) is a translation model that expresses the probability of the relationship between the source language \nand the target language. Language models that determine the probability of strings in the target language are \ndenoted by    ̅ , normally uses the standard word of trigram model from: \n \n \n(2) \n \nwhich  ̅  = e1 ,..., el. In the trigram model form, each word is predicted based on the previous two-word \nhistory.  \n\nInt J Elec & Comp Eng  ISSN: 2088-8708  \n \nImproving the role of language model in statistical machine translation … (Herry Sujaini) \n2103 \nAlthough machine translator models have continued to develop in recent years, statistical machine \ntranslation (SMT) continues to grow rapidly , with more and more proposed new translation models being \npracticed in various languages [7-9]. Most of the work in SMT concentrates on developing better translation \nmodels. Little exertion has been made to maximize the role of language modeling for machine translation. \nThe purpose of this research was to improve the role of language modeli ng, which in turn will improve  \nthe accuracy of the translation results of an SMT. \nFigure 1 shows t he general statistical languag e machine architecture. The decoder functions as  \na translator machine whose job it is to translate sentences from one language t o another . The results of  \nthe work of the decoder can differ from one another. These results are influenced by the models used, namely \nthe translation model (TM) and the language model (LM) as the main model, and the feature model (FM) \nbesides. TM is gener ated through the training process of a parallel corpus , while the training process of  \na monolingual corpus from the target language  generated LM. FM is usual ly used as an effort to improve  \nthe accuracy of machine translators by adding linguistic features s uch as Part of Speech  (PoS) [10 -12]. \nThe generated PoS can be done with a supervised or unsupervised approach [13]. The main system functions \nas a translator machine to produce the target language from sentence inpu t in the source language called  \nthe decoder. As shown in Figure 1, the parallel corpus is the primary source for building an SMT, while  \nthe monolingual corpus can use sentences that are on the target side of the parallel corpus. The size of  \nthe monolingual corpus can be enlarged by adding other sentences in the same language, even though it does \nnot have a pair in the parallel corpus. \n \n \n \n \nFigure 1. SMT architecture \n \n \nSeveral studies have been conducted to improve the role of linguistic models in various languages \nand methods. Yu et al. [14] explained language models that triggered new topics by calculating contexts and \ntopics and estimate n -gram probabilities under a given topic and adjust  language model scores based on  \nthe distribution of different topics online . The resulting translation pr oved to improve the hypothesis \nconsidered best by the first stage of the system. Zhang et al. [15] have researched by improving the coding of \nautomated veterinary diagnoses through large -scale language modeling. The algorithm proposed by them \naddresses important challenges in veterinary medicine and training in unsupervised learning for clinical \nlanguage development. Mohaghegh [16] reported improved accuracy by enhancing the role of the language \nmodel in the English -Persian translator machine. Monz [17] reported  improved accuracy by enhancing  \nthe role of the language model in Arabic - and Chinese-to-English translator m achines. Banerjee et al. [18] \nreported their research to improve the language model by learning from speech recognition mistakes in a \nlistening reading tutor . Sujaini et al. [19] reported the results of their research to improve the accuracy of \nmachine translators by using the part of speech feature s. The results of this study can increase accuracy by \n6.45% when compared to machine translators without using part of speech.  Jaya and Gupta [ 20] proposed \na better quality SMT that was improved by 2 points in th e Eng lish to Hindi system and 2.93 points in  \nthe Hin -Eng system. These results were obtained as they explored the c orpus augmentation approach for  \nthe English and Hindi Two-Way SMT. \n\n                ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 10, No. 2, April 2020 :  2102 - 2109 \n2104 \nThe l anguage model is designed to obtain the occurrence probability of wor ds (or token).  \nIf W1 = (w1,..., wL) shows the string token of L on fixed vocabulary. The n -gram language model provides \nprobability to   \n  according to: \n \n \n(3) \nwhere the approximation reflects Markov’s assumption that only n -1 token that the newest relevant in \npredicting the next word. \nFor each w substring, for example , f(w) shows the frequ ency of substring occurrence in  \nthe specified target language sequence, usually  very long , which called training data. The maximum -\nlikelihood probability for n-gram is given by its relative frequency. \n \n \n(4) \nIn principle, the predictive accuracy of the language model can be enha nced by increasing the order of  \nn-grams [21]. However, under certain conditions, this can reduce the accuracy of translation when using \ncomplex data, especially if there are many errors in corpus data. This study discusses the best accuracy that \ncan be produced by machine translators by conducting experiments on the use of the order of n -grams \non LM. \n \n \n2. RESEARCH METHOD  \n2.1.  Experimental stages \nThe training data is a parallel corpus of Indonesia – Kromo Javanese language taken from folklore \nmanually translated as many as 5108 sentences. In the process of training, 4500 pairs of sentences and 608 \npairs of sentences were used for the testing process. The experimental stages conducted can be seen in  \nFigure 2. Corpus preparation (preprocessing) was conducted by performing the process of cleaning, \ntokenizing, and lowercas ing to the parallel corpus that has been prepa red. The language model used in  \nthe baseline system was the trigram model of Javanese language  trained by using toolkit SRILM [22], while \nparallel corpus that was ready to use was then trained to obtain word alignments, phrase table, language \nmodel, and model combination weights. The baseline used in this research was trained by using standard \ntools, namely GIZA++ [23], to train the word alignment and Moses for phrase -based coding. Moses is a tool \nthat is an implementation of Statistic Machine Translation. Moses is used to training a statistical model of \ntranslated text from the source language to t he target language. In translati ng the language, Moses requires  \na corpus in two languages, source language and target language. Moses is released under the license pf LGPL \n(Lesser General Public License) and is available as source code an d binary for Windo ws and Linux.  \nIts development is supported by the EuroMatrix project, with the funding by European Commission [24].  \nThe decoder, as a translator machine, was set following the experimental strategy conducted, which was by \nchanging the language model variab les used. For each setting, testing was performed by inputting 608 \nsentences that had been prepared previously. The tests were performed using the BLEU automatic evaluation \nmethod [25]. \n \n \n \n \nFigure 2. Experimental stages \n\nInt J Elec & Comp Eng  ISSN: 2088-8708  \n \nImproving the role of language model in statistical machine translation … (Herry Sujaini) \n2105 \n2.2.  Experiment strategy \nTo see the role of the language model used in Indonesian -Java SMT, in this experiment, four \nstrategies were performed.  \na. The language model was trained from 4500 sentences of the parallel corpus target; in other words, the test \nreference sentence was not in cluded in the training. Furthermore, this first strategy was tested for 3  \nto 7-gram models. \nb. The l anguage model was trained from 4500 sentences of parallel corpus target plus 608 reference \nsentences; in other words, the test reference sentence was included in the training. Furthermore, this first \nstrategy was tested for 3 to 7-gram models. \nc. The l anguage model was trained from 608 reference sentences and 3892 sentences of parallel corpus \ntarget, then added with 100 sentences of the remaining parallel corp us ta rgets for each experiment.  \nSix experiments were added; therefore, the corpus used in each experiment was 4600, 4700, 4800, 4900, \n5000, and 5100 sentences. \nd. The language model was trained from 4500 target sentences, then added 100 reference sentences for each \nexperiment. Six experiments were added; therefore, the corpus used in each experiment was 4600, 4700, \n4800, 4900, 5000, and 5100 sentences. \n \n2.3.  Result and discussion \nThe training data is in the form of the Indonesian-Java parallel corpus as shown in Figure 3. The left \ncolumn is a collection of sentences in the Indonesian language, while the right column is a sentence cluster in \nJavanese, where each line is a translation of the corresponding sentence.  \nExamples of sentences that have been pas sed the process of cleaning, tokeni zing, and lowercasing \nare: \n\"Baru pulang, Kang? Mana Abah?\" Tanya Nyi Iteung \nResult : \n\" lagi mulih , kang? endi abah? \" pitakone nyi iteung \nThe language model was generated from the training process conducted on the target language of \nthe parallel corpus, i .e., Java language. As a baseline, a trigram (3 -gram) training was conducted, then \ntraining was also conducted to produce a comparison machine with 4 -gram, 5-gram, 6 -gram, and 7-gram \nmodels. The example of the 3 -gram language model can be found in Fig ure 4, while the example of  \nthe 7-gram language model can be found in Fig ure 5. For instance, Fig ure 4. says that the probability of  \nthe first word in a sentence being \"dina esuke\" is 10 -0.3746373 = 0.422, the probability g ave the pair word \n\"sawatara dina\" that the next thing that happens is that the sentence ends 10-0.7363669 = 0.183, and so forth. \nFrom the training results, the number of token pairs with their pro babilities for each n -gram are:  \n1-gram=5598, 2-gram=26350, 3-gram=3924, 4-gram=1768, 5-gram=646, 6-gram=181 and 7-gram=53.  \n \n \nLahirnya Itok. Laire Itok. \nSetelah Nyi Iteung hamil, orang serumah semua direpotkan.  Sawise Nyi Iteung mbobot, wong saomah kabeh direpotake.  \nMaklum namanya baru hamil muda, ada -ada saja yang diminta \ndan yang aneh-aneh. \nMaklum jenenge lagi ngandheg enom, ana -ana wae sing dijaluk \nlan sing aneh-aneh. \nHal ini tentu saja membuat bingung orang serumah.  Bab iki mesthi wae gawe bingunge wong saomah.  \nSi Kabayan bingung sekali menghadapi sikap dan permintaan \nmainan \nSi Kabayan bingung banget ngadhepi sikep lan panjaluke \nbojone. \nSedang Abah dan Ambu yang sesudah wanita pengalaman bisa \nmengerti hal itu. \nDene Abah lan Ambu sing wis duwe pengalaman bisa ngerteni \nbab iku. \nBagi Si Kabayan, semua itu membuat dirinya serba repot.  Tumrape Si Kabayan, kabeh mau ndadekake dheweke sarwa \nrepot. \nPermintaannya Nyi Iteung harus cepat dituruti dengan alasan \nmewujudkan bawaan jabang bayi yang ada di dalam perutnya.  \nPanjaluke Nyi Iteung kudu enggal ditindakake kanthi alesan \nmujudake gawan jabang bayi sing ana ing njero wetenge.  \nJika sudah begitu Si Kabayan tidak bisa mengelak.  Manawa wis mangkono Si Kabayan ora bisa suwala.  \nSeperti di hari ini, Nyi Iteung mengatakan keinginannya kepada \nKabayan yang baru pulang dari kebun, setelah membantu Abah \nmenanam ubi. \nKaya ing dina iki, Nyi Iteung ngandhakake pepinginane marang \nKabayan sing lagi mulih saka kebon, sawise mbiyantu Abah \nnandur pohung. \n\"Baru pulang, Kang? Mana Abah?\" Tanya Nyi Iteung. \"Lagi mulih, Kang? Endi Abah?\" Pitakone Nyi Iteung.  \n\"Alhamdulillah, baru saja selesai Nyi.\"  \"Alhamdulillah, lagi wae rampung Nyi.\"  \n\"Abah baru basuh di jamban.\" \"Abah lagi wisuh ing jamban.\" \nJawab Si Kabayan dengan duduk di lincak.  Wangsulane Si Kabayan karo lungguh ing lincak.  \n\"O, syukurlah. Kakang apa masih capek?\"  \"O, syukurlah. Kakang apa isih kesel?\"  \nTanya Nyi Iteung dengan memperhatikan diri Kabayan.  Pitakone Nyi Iteung karo ngawasake awake Kabayan.  \n\"Lumayan, Nyi, orang namanya bekerja di kebun.\" \"Lumayan, Nyi, wong jenenge nyambut gawe ing kebon.\"  \n \nFigure 3. Indonesian-Java parallel corpus  \n                ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 10, No. 2, April 2020 :  2102 - 2109 \n2106 \n \n \nFigure 4. 3-gram language model \n \n \n \n \nFigure 5. 7-gram language model \n \n \nThe first experimental strategy was using 4500 sentences  parallel corpus , and 4500 sentences \nmonolingual corpus of, the results of the experiments produced can be found in Table 1. Machine 1.3 means \nusing strategy 1 with 3-gram; machine 1.4 means using strategy 1 with 4 -gram, and so on. The experiment’s \nresults show that with the addition of n -grams in the monolingual corpus taken from the parallel corpus,  \nit does not show a signif icant increase in accuracy (represented by BLEU value). The highest value on the  \n4-gram to 7-gram model can only increase the accuracy of ((32.46-32.42) /32.42) * 100% = 0.12%. \nThe second experimental strategy was using 4500 sentences parallel corpus , and 5108 sentences \nmonolingual corpus, the results of the experiments produced can be found in Table 2. Machine 2.3 means \nusing strategy 2 with 3-gram; machine 2.4 means using strategy 2 with 4-gram, and so on. The experiment’s \nresults show that with the addi tion of n -grams in the monolingual corpus taken from the parallel corpus, it \ndoes not show an increase in accuracy (represented by BLEU value), even lower than the 3 -gram baseline. \nThe highest score remains on the 3-gram model, which is 40.79. \n \n \nTable 1. Result from strategy 1 \nMachine n-gram BLEU Score (%) \n1.3 3-gram 32.42 \n1.4 4-gram 32.46 \n1.5 5-gram 32.46 \n1.6 6-gram 32.46 \n1.7 7-gram 32.46 \n \nTable 2. Result from strategy 2 \nMachine n-gram BLEU Score (%) \n2.3 3-gram 40.79 \n2.4 4-gram 40.69 \n2.5 5-gram 40.69 \n2.6 6-gram 40.67 \n2.7 7-gram 40.71 \n \n-0.8332769  duwe dhuwit kanggo \n-0.9124582  ing dhuwur panggung \n-0.4353369  pantes dianggo nggeret \n-0.4353369  wis dienteni abah \n-0.8332769  sing digawa , \n-0.1713249  arepa dikaya ngapa \n-0.7363669  gelem dikethak , \n-0.7363669  gelem dikethak sirahe \n-0.4353369  , dina iki \n-0.3746373  <s> dina esuke \n-0.979405   ing dina iki \n-0.5393018  ing dina iku \n-0.4353369  pitung dina pitung \n-0.7363669  sawatara dina </s> \n-0.1713249  sawijining dina , \n-0.4353369  wiwit dina iki \n-0.8332769  sing diparingake dening \n-0.4353369  sing diselipake ing \n-0.4353369  bisa disingkiri maneh \n  \n-1.046955  ta , bah? \" pitakone kabayan </s> \n-1.046955  ora bakal bali maneh , kabayan? \"  \n-1.046955  memedi sing ana njero omah kothong iku  \n-1.046955  <s> \" ah , kowe kuwi pancen \n-1.223046  <s> abah , ambu , lan nyi \n-1.046955  memedi iku ora bakal bali maneh ,  \n-1.046955  wong tuwa pikun sing sedhela maneh bakal  \n-1.046955  <s> nalika kuwi , raden mas banterang  \n-1.700167  <s> putri kenanga lan putri mawar mlengos  \n-1.700167  <s> putri kenanga lan putri mawar padha  \n-1.046955  pikun sing sedhela maneh bakal mlebu kubur  \n-1.046955  rak kanggo sing ana ing njero weteng  \n-1.046955  abah , ambu , lan nyi iteung \n-0.1249387 <s> \" matur nuwun , pak . \n-1.046955  wis kekuras ana ing palagan sak durunge  \n-1.046955  apa , nak? \" pitakone sing dodol \n-1.046955  , bawang , tempe , trasi , \n-1.046955  kanggo sing ana ing njero weteng iki \n  \nInt J Elec & Comp Eng  ISSN: 2088-8708  \n \nImproving the role of language model in statistical machine translation … (Herry Sujaini) \n2107 \nThe third strategy experiment was using a parallel corpus of 4500 sentences and a monolingual \ncorpus of 4500 to 5100 sentences, 4500 baseline sentences consist of 608 reference sentences and  \n3892 sentences of parallel corpus targets. The experiment’s result produced can be found in Table 3 indicates \nthat the addition of monolingual corpus quantities taken from the parallel corpus does not show significant \nincreases at accuracy. Machine 3.45 means using strategy 3 with 4500 sentences; machine 3.46 means using \nstrategy 3 with 4600 sentences, and so on.  The highest value on a 3 .51 can only increase the accuracy of \n((40.81-40.59) / 40.59) * 100% = 0.54%. \nThe fourth strategy experiment was using a parallel corpus of 4500 sentences and a monolingual \ncorpus of 4500 to 5100 sentences, and the whole 4500 baseline sentences were taken from the parallel corpus \ntarget sentence. The results of the experiments produced can be found in Table 4 indicates that the addition of \nmonolingual corpus quantities taken from the reference sentence shows a significant increase at accuracy. \nMachine 4.45 means using strategy 4 with 4500 sentences; machine 4.46 means using strategy 4 with 4600 \nsentences, and so on.  The highest value on a 4 .51 machine with 5100 sentences in monolingual corpus can \nincrease accuracy by ((40.63-32.42) / 32.42) * 100% = 25.32%. \n \n \nTable 3. Result from strategy 3 \nMachine Monolingual corpus BLEU Score (%) \n3.45 4500 40.59 \n3.46 4600 40.46 \n3.47 4700 40.20 \n3.48 4800 40.46 \n3.49 4900 40.64 \n3.50 5000 40.63 \n3.51 5100 40.81 \n \nTable 4. Result from strategy 4 \nMachine Monolingual corpus BLEU Score (%) \n4.45 4500 32.42 \n4.46 4600 35.48 \n4.47 4700 37.19 \n4.48 4800 37.82 \n4.49 4900 38.55 \n4.50 5000 39.52 \n4.51 5100 40.63 \n \n \n \nExperiments conducted on strategies 1 and 2 show that the use of n -gram model from 3 -gram to  \n7-gram does not affect the accuracy of the Indonesian -Java translator machine with a parallel corpus of  \n4500 sentences. This is due to the small number of sentences used in the corpus. The small quantity of corpus \nsentences results in no variation in the probability of each pair of tokens , as seen in the 7 -gram language \nmodel; thus, for SMT using the small corpus, it is best to keep using the 3-gram language model. \nThe monolingual corpus quantity addition experiments used for gradual language model training, \nthe results are demonstrated by strategies 3 and 4. From the experimental results , it is found that the best \nresults are obtained by increasing the quantity of the monolingual corpus outside the parallel corpus in \nstrategy 4, in other words, the monolingual corpus taken from the parallel corpus target language, then added \nwith another sentence beyond the existing sentence in the parallel corpus. The increase of the BLEU score of \neach machine to the baseline can be seen in Fig ure. 6. The e xperiment’s results on strategy 4 show  \na significant increase for each addition of 100 sentences to t he monolingual corpus, as seen in Table 5.  \nFrom the results of this study, it can be co ncluded that the role of the language model is quite important in \nanticipating the sentences to be translated on the SMT, especially when the phrase in the sentence is n ot \ncontained in the translation model. This will certainly be more apparent on SMT with small resources \nbecause the possibility of a sentence to be translated does not exist in the translation model is certainly very \nlarge compared to SMT with large resources. \n \n \n \n \nFigure 6. Increasing of BLEU scores \n \n0\n10\n20\n30\n40\n50\n4 . 4 5 4 . 4 6 4 . 4 7 4 . 4 8 4 . 4 9 4 . 5 0 4 . 5 1 \nB L EU \n                ISSN: 2088-8708 \nInt J Elec & Comp Eng, Vol. 10, No. 2, April 2020 :  2102 - 2109 \n2108 \nTable 5. Increasing accuracy on strategy 4 \nMachine Monolingual corpus Increasing (%) \n4.45 32.42  \n4.46 35.48 9.44 \n4.47 37.19 4.82 \n4.48 37.82 1.69 \n4.49 38.55 1.93 \n4.50 39.52 2.52 \n4.51 40.63 2.81 \n \n \n3. CONCLUSION  \nThe utilization of the n-gram model from 3 -gram to 7 -gram d oes not affect the accuracy of  \nthe Indonesian-Java translator machine. It is recommended that SMT using a small corpus should keep using \na 3-gram language model.  The best result for improving the language model role is to use the the parallel \ncorpus target language as the monolingual corpus, then added as much as possible with other sentences \nbeyond the existing sentence in the parallel corpus. \n \n \nREFERENCES  \n[1] M. Volk, “Parallel Corpora, Terminology Extraction und Machine Translation ,” In: 16. DTTSymposion. \nTerminologie und Text(e), Mannheim, 22 - 24 March 2018, 3-14.  2018.  \n[2] E. Yıldız, A.C. Tantu˘g, and B. Diri. “The effect of parallel corpus quality vs size in English-to-Turkish SMT,” In \nProceedings of the Sixth International Conference on Web services and Semantic Technology (WeST 2014), 2014.  \n[3] A. Imankulova, T. Sato, M. Komachi, “Improving Low -Resource Neural Machine Translation with Filtered \nPseudo-parallel Corpus,” In Proceedings of the 4th Workshop on Asian Translation (WAT2017), Taipei, 2017 \n[4] K.K. Arora and S.S. Agrawa, “Pre-Processing of English -Hindi Corpus for Statistical Machine Translation ,” \nComputación y Sistemas, Vol. 21, No. 4, 2017.  \n[5] H. Tran, Y. Guo, P. Jian, S. Shi, and H. Huang, “Improving Parallel Corpus Quality for Chinese -Vietnamese \nStatistical Machine Translation,” Journal of Beijing Institute of Technology, Vol. 27, No. 1, 2018.  \n[6] M.G. Asparilla, H. Sujaini, and R.D. Nyoto, “ Corpus Quality Improvement to Improve the Quality of Statistical \nTranslator Machines (Case Study of Indonesian Language to Java Krama) ,” Jurnal Linguistik Komputasional,   \nVol. 1, No. 2, 2018. \n[7] J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong, and Q. Liu, “Transl ation Model Adaptation for Statistical \nMachine Translation with  Monolingual Topic Information,”  in Proceedings of the 50th Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers), Jeju Island, 2012. \n[8] G. Neubig, T. Watanabe, \"Optimization for Statistical Machine Translation: A Survey,\" Computational Linguistics, \nVol. 42, No. 1, 2016. \n[9] K.N. Dew, A.M.Turner, Y.K. Choi, A. Bosold, and K. Kirchhoffe, \" Development of Machine Translation \nTechnology for Assisting Health Communication: A Systematic Review ,\" Journal of Biomedical Informatics ,  \nVol. 85, 2018  \n[10] P.J. Antony and K.P. Soman, “Kernel Based Pa rt of Speech Tagger for Kannda,”  in International Conference on \nMachine Learning and Cybernetics, ICMLC 2010, Qingdao, Shandong,  2010.   \n[11] M. Mohaghegh, A. Sarrafxadeh, and T. Moir, \"Improved Language Modeling for English -Persian Statistical \nMachine Translation ,\" in Proceedings of SSST -4, Fourth Workshop on Syntax and Structure in Statistical \nTranslation, COLING 2010, Beijing, 2010.  \n[12] J. Sangeetha, S. Jothilakshmi, and R.N.D. Kumar, \"An Efficient Machine Translation System for English to Indian \nLanguages Using Hybrid Mechanism ,\" International Journal of Engineering and Technology (IJET) , Vol. 6,  \nNo. 4, 2014. \n[13] H. Sujaini, Kuspriyanto, A.A. Arman , and A. Purwarianti, “Extended Word Similarity Based Clustering on \nUnsupervised PoS Induction to Improve English -Indonesian Statistical Machine Translation,” in 16th ORIENTAL \nCOCOSDA/CASLRE-2013, Gurgaon, India, 2013. \n[14] H. Yu, J. Su, Y. Lv, and Q. Liu, “A Topic-Triggered Language Model for Statistical Machine Translation ,” in \nProceedings of the Sixth International Joint Conference on Natural Language Processing, Nagoya, 2013. \n[15] Y. Zhang, A. Nie, A. Zehnder, L. Rodney, and J. Zou, “ VetTag: improving automated veterinary diagnosis coding \nvia large-scale language modeling,” Digital Medicine, 2019.  \n[16] M. Mohaghegh, A. Sarrafzadeh, and T. Moir, “Improved Language Modeling for English -Persian Statistical \nMachine,” in SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, Beijing, 2010. \n[17] C. Monz, “Statistical Machine Translation with Local Language Models,” in Conference on Empirical Methods in \nNatural Language Processing, Edinburgh, 2011.  \n[18] S. Banerjee, J. Mostow, J. Beck, and W. Tam, \"Improving Language Models by Learning from Speech Recognition \nErrors in a Reading Tutor that Listens ,\" in Second International Conference on Applied Artificial Intelligence , \n2003. \nInt J Elec & Comp Eng  ISSN: 2088-8708  \n \nImproving the role of language model in statistical machine translation … (Herry Sujaini) \n2109 \n[19] H. Sujaini, Kuspriyanto, A.A. Arman, and A. Purwarianti, “ A Novel Part -of-Speech Set Devel oping Method for \nStatistical Machine Translation,” TELKOMNIKA (Telecommunication Comput. Electron. Control.), vol. 12, no. 3, \n2014. \n[20] K. Jaya and D. Gupta, “ Exploration of Corpus Augmentation Approach for English-Hindi Bidirectional Statistical \nMachine Translation System,” International Journal of Electrical and Computer Engineering (IJECE) , vol. 6,  \nno. 3, 2016. \n[21] [z] C. Shaoul, C.F. Westbury, and R.H. Baayen,\" The Subjective Frequency of Word n -grams,\" PSIHOLOGIJA, \nVol. 46, No. 4, 2013. \n[22] A. Stolcke, J. Zhen g, W. Wang, and V. Abrash, “SRILM at sixteen: Update and outlook,” in Automatic Speech \nRecognition and Understanding (ASRU), 2011 IEEE Workshop, Waikoloa, 2011.  \n[23] F. J. Och and H. Ney, “A Systematic Comparison of Various Statistical Alignment Models,” Computational \nLinguistics, vol. 1, no. 29, pp. 19-51, 2003.  \n[24] W. Xu and P. Koehn, “ Extending Hiero Decoding in Moses with Cube Growing ,” The Prague Bulletin of \nMathematical Linguistics, 8(1), 2012  \n[25] K. Papineni, S. Roukos, T. Ward and W. -J. Zhu, “BLEU: A Method For Automatic Evaluation of Machine \nTranslation,” in Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL), \nPennsylvania, 2002. \n \n \nBIOGRAPHY OF AUTHOR \n \n \nHerry Sujaini  graduated from a bachelor's degree in the Electrical Engineering Department, \nUniversity of Tanjungpura. He got his master and a doctoral degree from STEI, Bandung Institute of \nTechnology. Since 1997, he has become a lecturer at Informatics De partment, Engineering Faculty, \nUniversity of Tanjungpura. Her research interest is on computational linguistics, mainly on machine \ntranslation and machine learning \n \n \n",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8659520745277405
    },
    {
      "name": "Computer science",
      "score": 0.8464411497116089
    },
    {
      "name": "Trigram",
      "score": 0.8071185350418091
    },
    {
      "name": "Language model",
      "score": 0.6896539330482483
    },
    {
      "name": "Indonesian",
      "score": 0.6839602589607239
    },
    {
      "name": "Natural language processing",
      "score": 0.6330345869064331
    },
    {
      "name": "Artificial intelligence",
      "score": 0.602066159248352
    },
    {
      "name": "Machine translation software usability",
      "score": 0.5887870788574219
    },
    {
      "name": "Evaluation of machine translation",
      "score": 0.5476511716842651
    },
    {
      "name": "Example-based machine translation",
      "score": 0.4367137849330902
    },
    {
      "name": "n-gram",
      "score": 0.42463868856430054
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4229898750782013
    },
    {
      "name": "Linguistics",
      "score": 0.182865172624588
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12176630",
      "name": "Tanjungpura University",
      "country": "ID"
    }
  ],
  "cited_by": 11
}