{
    "title": "Dynamic Attention Seeking to Address the Challenge of Named Entity Recognition of Large Language Models",
    "url": "https://openalex.org/W4399395131",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5102683234",
            "name": "Tomoe Hata",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5099036165",
            "name": "Ruriko Aono",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4200098126",
        "https://openalex.org/W3131324121",
        "https://openalex.org/W3115058362",
        "https://openalex.org/W4392673518",
        "https://openalex.org/W2807800730",
        "https://openalex.org/W3137401156",
        "https://openalex.org/W4386275705",
        "https://openalex.org/W2966394569",
        "https://openalex.org/W4378470138",
        "https://openalex.org/W3213591530",
        "https://openalex.org/W2997394673",
        "https://openalex.org/W4389983939",
        "https://openalex.org/W3082330004",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2512131260",
        "https://openalex.org/W3186783455",
        "https://openalex.org/W3011594683",
        "https://openalex.org/W3098515724",
        "https://openalex.org/W2949966747"
    ],
    "abstract": "Named Entity Recognition (NER) plays a crucial role in natural language processing tasks, enabling systems to accurately identify and classify entities within text. The introduction of a dynamic attention mechanism into the Llama model significantly enhances NER performance by allowing real-time adjustments to attention weights based on contextual relevance. This novel approach addresses the limitations of static attention mechanisms, leading to substantial improvements in accuracy, precision, recall, and F1-score metrics. The modified Llama model outperforms baseline models and previous approaches, demonstrating its superior ability to handle diverse linguistic contexts. Comprehensive evaluations using a diverse dataset reveal the model's robustness and adaptability, with a detailed error analysis providing insights into common misclassifications and their potential causes. Future research directions include exploring hierarchical attention, integrating multi-modal data, leveraging external knowledge bases, and optimizing computational efficiency. The findings demonstrate the potential of dynamic attention mechanisms to revolutionize NER tasks, paving the way for more advanced and accurate natural language processing systems.",
    "full_text": null
}