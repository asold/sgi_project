{
  "title": "Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation",
  "url": "https://openalex.org/W2121870595",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2036086788",
      "name": "Rui Wang",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2112311038",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4200814141",
      "name": "Bao Liang Lu",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1970329226",
      "name": "Masao Utiyama",
      "affiliations": [
        "National Institute of Information and Communications Technology"
      ]
    },
    {
      "id": "https://openalex.org/A100988613",
      "name": "Eiichiro SUMITA",
      "affiliations": [
        "National Institute of Information and Communications Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2293111166",
    "https://openalex.org/W2103078213",
    "https://openalex.org/W83522546",
    "https://openalex.org/W2168621536",
    "https://openalex.org/W2251682575",
    "https://openalex.org/W2251302921",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2116625254",
    "https://openalex.org/W1665921526",
    "https://openalex.org/W2137387514",
    "https://openalex.org/W1839343314",
    "https://openalex.org/W2124008567",
    "https://openalex.org/W2406079600",
    "https://openalex.org/W2251256737",
    "https://openalex.org/W2251098065",
    "https://openalex.org/W2167934275",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W61894391",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W1797288984",
    "https://openalex.org/W2250582358",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W2020073413",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2251091582",
    "https://openalex.org/W2156700117",
    "https://openalex.org/W2143719855",
    "https://openalex.org/W2250489405",
    "https://openalex.org/W2155388323",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W1979625042",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2083545877",
    "https://openalex.org/W2252186615",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2250379827",
    "https://openalex.org/W2250644439",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2285823542",
    "https://openalex.org/W932413789"
  ],
  "abstract": "Since larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT.However, most of the existing LM growing methods need an extra monolingual corpus, where additional LM adaption technology is necessary.In this paper, we propose a novel neural network based bilingual LM growing method, only using the bilingual parallel corpus in SMT.The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus.",
  "full_text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195,\nOctober 25-29, 2014, Doha, Qatar.c⃝2014 Association for Computational Linguistics\nNeural Network Based Bilingual Language Model Growing\nfor Statistical Machine Translation\nRui Wang1,3,∗, Hai Zhao1,3, Bao-Liang Lu1,3, Masao Utiyama2 and Eiichro Sumita2\n1Center for Brain-Like Computing and Machine Intelligence,\nDepartment of Computer Science and Engineering,\nShanghai Jiao Tong University, Shanghai, 200240, China\n2Multilingual Translation Laboratory, MASTAR Project,\nNational Institute of Information and Communications Technology,\n3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan\n3Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China\nwangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn,\n{mutiyama, eiichiro.sumita}@nict.go.jp\nAbstract\nSince larger n-gram Language Model\n(LM) usually performs better in Statistical\nMachine Translation (SMT), how to con-\nstruct efﬁcient large LM is an important\ntopic in SMT. However, most of the ex-\nisting LM growing methods need an extra\nmonolingual corpus, where additional LM\nadaption technology is necessary. In this\npaper, we propose a novel neural network\nbased bilingual LM growing method, only\nusing the bilingual parallel corpus in SMT.\nThe results show that our method can im-\nprove both the perplexity score for LM e-\nvaluation and BLEU score for SMT, and\nsigniﬁcantly outperforms the existing LM\ngrowing methods without extra corpus.\n1 Introduction\n‘Language Model (LM) Growing’ refers to adding\nn-grams outside the corpus together with their\nprobabilities into the original LM. This operation\nis useful as it can make LM perform better through\nletting it become larger and larger, by only using a\nsmall training corpus.\nThere are various methods for adding n-grams\nselected by different criteria from a monolingual\ncorpus (Ristad and Thomas, 1995; Niesler and\nWoodland, 1996; Siu and Ostendorf, 2000; Si-\nivola et al., 2007). However, all of these approach-\nes need additional corpora. Meanwhile the extra\ncorpora from different domains will not result in\nbetter LMs (Clarkson and Robinson, 1997; Iyer et\nal., 1997; Bellegarda, 2004; Koehn and Schroeder,\n∗Part of this work was done as Rui Wang visited in NICT.\n2007). In addition, it is very difﬁcult or even im-\npossible to collect an extra large corpus for some\nspecial domains such as the TED corpus (Cettolo\net al., 2012) or for some rare languages. There-\nfore, to improve the performance of LMs, without\nassistance of extra corpus, is one of important re-\nsearch topics in SMT.\nRecently, Continues Space Language Model\n(CSLM), especially Neural Network based Lan-\nguage Model (NNLM) (Bengio et al., 2003;\nSchwenk, 2007; Mikolov et al., 2010; Le et al.,\n2011), is being actively used in SMT (Schwenk\net al., 2006; Son et al., 2010; Schwenk, 2010;\nSchwenk et al., 2012; Son et al., 2012; Niehues\nand Waibel, 2012). One of the main advantages\nof CSLM is that it can more accurately predic-\nt the probabilities of the n-grams, which are not in\nthe training corpus. However, in practice, CSLM-\ns have not been widely used in the current SMT\nsystems, due to their too high computational cost.\nVaswani and colleagues (2013) propose a\nmethod for reducing the training cost of CSLM\nand apply it to SMT decoder. However, they do\nnot show their improvement for decoding speed,\nand their method is still slower than the n-gram\nLM. There are several other methods for attempt-\ning to implement neural network based LM or\ntranslation model for SMT (Devlin et al., 2014;\nLiu et al., 2014; Auli et al., 2013). However, the\ndecoding speed using n-gram LM is still state-of-\nthe-art one. Some approaches calculate the prob-\nabilities of the n-grams n-grams before decoding,\nand store them in the n-gram format (Wang et al.,\n2013a; Arsoy et al., 2013; Arsoy et al., 2014). The\n‘converted CSLM’ can be directly used in SMT.\nThough more n-grams which are not in the train-\n189\ning corpus can be generated by using some of\nthese ‘ converting’ methods, these methods only\nconsider the monolingual information, and do not\ntake the bilingual information into account.\nWe observe that the translation output of a\nphrase-based SMT system is concatenation of\nphrases from the phrase table, whose probabilities\ncan be calculated by CSLM. Based on this obser-\nvation, a novel neural network based bilingual LM\ngrowing method is proposed using the ‘ connecting\nphrases’. The remainder of this paper is organized\nas follows: In Section 2, we will review the exist-\ning CSLM converting methods. The new neural\nnetwork based bilingual LM growing method will\nbe proposed in Section 3. In Section 4, the exper-\niments will be conducted and the results will be\nanalyzed. We will conclude our work in Section\n5.\n2 Existing CSLM Converting Methods\nTraditional Backoff N-gram LMs (BNLMs) have\nbeen widely used in many NLP tasks (Zhang and\nZhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013;\nZhang et al., 2012; Xu and Zhao, 2012; Wang et\nal., 2013b; Jia and Zhao, 2013; Wang et al., 2014).\nRecently, CSLMs become popular because they\ncan obtain more accurate probability estimation.\n2.1 Continues Space Language Model\nA CSLM implemented in a multi-layer neural net-\nwork contains four layers: the input layer projects\n(ﬁrst layer) all words in the context hi onto the\nprojection layer (second layer); the hidden layer\n(third layer) and the output layer (fourth layer)\nachieve the non-liner probability estimation and\ncalculate the LM probability P (wi|hi) for the giv-\nen context (Schwenk, 2007).\nCSLM is able to calculate the probabilities of\nall words in the vocabulary of the corpus given the\ncontext. However, due to too high computational\ncomplexity, CSLM is mainly used to calculate the\nprobabilities of a subset of the whole vocabulary\n(Schwenk, 2007). This subset is called a short-\nlist, which consists of the most frequent words in\nthe vocabulary. CSLM also calculates the sum of\nthe probabilities of all words not included in the\nshort-list by assigning a neuron with the help of\nBNLM. The probabilities of other words not in the\nshort-list are obtained from an BNLM (Schwenk,\n2007; Schwenk, 2010; Wang et al., 2013a).\nLet wi and hi be the current word and history,\nrespectively. CSLM with a BNLM calculates the\nprobability P (wi|hi) of wi given hi, as follows:\nP (wi|hi) =\n\n\n\nPc(wi|hi)∑\nw∈V0 Pc(w|hi) Ps(hi) if wi ∈ V0\nPb(wi|hi) otherwise\n(1)\nwhere V0 is the short-list, Pc(·) is the probabil-\nity calculated by CSLM, ∑\nw∈V0 Pc(w|hi) is the\nsummary of probabilities of the neuron for all the\nwords in the short-list, Pb(·) is the probability cal-\nculated by the BNLM, and\nPs(hi) =\n∑\nv∈V0\nPb(v|hi). (2)\nWe may regard that CSLM redistributes the\nprobability mass of all words in the short-list,\nwhich is calculated by using the n-gram LM.\n2.2 Existing Converting Methods\nAs baseline systems, our approach proposed in\n(Wang et al., 2013a) only re-writes the probabil-\nities from CSLM into the BNLM, so it can only\nconduct a convert LM with the same size as the o-\nriginal one. The main difference between our pro-\nposed method in this paper and our previous ap-\nproach is that n-grams outside the corpus are gen-\nerated ﬁrstly and the probabilities using CSLM are\ncalculated by using the same method as our previ-\nous approach. That is, the proposed new method\nis the same as our previous one when no grown\nn-grams are generated.\nThe method developed by Arsoy and colleagues\n(Arsoy et al., 2013; Arsoy et al., 2014) adds al-\nl the words in the short-list after the tail word of\nthe i-grams to construct the ( i+1)-grams. For ex-\nample, if the i-gram is “ I want”, then the ( i+1)-\ngrams will be “ I want *”, where “ *” stands for any\nword in the short list. Then the probabilities of\nthe ( i+1)-grams are calculated using ( i+1)-CSLM.\nSo a very large intermediate ( i+1)-grams will have\nto be grown 1, and then be pruned into smaller\nsuitable size using an entropy-based LM pruning\nmethod modiﬁed from (Stolcke, 1998). The ( i+2)-\ngrams are grown using ( i+1)-grams, recursively.\n1In practice, the probabilities of all the target/tail words\nin the short list for the history i-grams can be calculated by\nthe neurons in the output layer at the same time, which will\nsave some time. According to our experiments, the time cost\nfor Arsoy’s growing method is around 4 times more than our\nproposed method, if the LMs which are 10 times larger than\nthe original one are grown with other settings all the same.\n190\n3 Bilingual LM Growing\nThe translation output of a phrase-based SMT sys-\ntem can be regarded as a concatenation of phrases\nin the phrase table (except unknown words). This\nleads to the following procedure:\nStep 1. All the n-grams included in the phrase\ntable should be maintained at ﬁrst.\nStep 2. The connecting phrases are deﬁned in\nthe following way.\nThe wb\na is a target language phrase starting from\nthe a-th word ending with the b-th word, and βwb\naγ\nis a phrase including wb\na as a part of it, where β and\nγ represent any word sequence or none. An i-gram\nphrase wk\n1 wi\nk+1 (1 ≤ k ≤ i □ 1) is a connecting\nphrase2, if :\n(1) wk\n1 is the right (rear) part of one phrase βwk\n1\nin the phrase table, or\n(2) wi\nk+1 is the left (front) part of one phrase\nwi\nk+1γ in the phrase table.\nAfter the probabilities are calculated using C-\nSLM (Eqs.1 and 2), we combine the n-grams in\nthe phrase table from Step 1 and the connecting\nphrases from Step 2.\n3.1 Ranking the Connecting Phrases\nSince the size of connecting phrases is too huge\n(usually more than one Terabyte), it is necessary\nto decide the usefulness of connecting phrases for\nSMT. The more useful connecting phrases can be\nselected, by ranking the appearing probabilities of\nthe connecting phrases in SMT decoding.\nEach line of a phrase table can be simpliﬁed\n(without considering other unrelated scores in the\nphrase table) as\nf |||e |||P (e|f), (3)\nwhere the P (e|f) means the translation probabili-\nty from f(source phrase ) to e(target phrase ),\nwhich can be calculated using bilingual parallel\ntraining data. In decoding, the probability of a tar-\nget phrase e appearing in SMT should be\nPt(e) =\n∑\nf\nPs(f) × P (e|f), (4)\n2We are aware that connecting phrases can be applied to\nnot only two phrases, but also three or more. However the ap-\npearing probabilities (which will be discussed in Eq. 5 of next\nsubsection) of connecting phrases are approximately estimat-\ned. To estimate and compare probabilities of longer phrases\nin different lengths will lead to serious bias, and the experi-\nments also showed using more than two connecting phrases\ndid not perform well (not shown for limited space), so only\ntwo connecting phrases are applied in this paper.\nwhere the Ps(f) means the appearing probability\nof a source phrase, which can be calculated using\nsource language part in the bilingual training data.\nUsing Pt(e)3, we can select the connecting\nphrases e with high appearing probabilities as\nthe n-grams to be added to the original n-\ngrams. These n-grams are called ‘ grown n-\ngrams’. Namely, we build all the connecting\nphrases at ﬁrst, and then we use the appearing\nprobabilities of the connecting phrases to decide\nwhich connecting phrases should be selected. For\nan i-gram connecting phrase wk\n1wi\nk+1, where wk\n1 is\npart of βwk\n1 and wi\nk+1 is part of wi\nk+1γ (the βwk\n1\nand wi\nk+1γ are from the phrase table), the prob-\nability of the connecting phrases can be roughly\nestimated as\nPcon(wk\n1 wi\nk+1) =\ni□1∑\nk=1\n(\n∑\nβ\nPt(βwk\n1 )×\n∑\nγ\nPt(wi\nk+1γ)).\n(5)\nA threshold for Pcon(wk\n1 wi\nk+1) is set, and only\nthe connecting phrases whose appearing probabil-\nities are higher than the threshold will be selected\nas the grown n-grams.\n3.2 Calculating the Probabilities of Grown\nN-grams Using CSLM\nTo our bilingual LM growing method, a 5-gram\nLM and n-gram ( n=2,3,4,5) CSLMs are built by\nusing the target language of the parallel corpus,\nand the phrase table is learned from the parallel\ncorpus.\nThe probabilities of unigram in the original n-\ngram LM will be maintained as they are. The\nn-grams from the bilingual phrase table will be\ngrown by using the ‘ connecting phrases’ method.\nAs the whole connecting phrases are too huge, we\nuse the ranking method to select the more useful\nconnecting phrases. The distribution of different\nn-grams ( n=2,3,4,5) of the grown LMs are set as\nthe same as the original LM.\nThe probabilities of the grown n-grams\n(n=2,3,4,5) are calculated using the 2,3,4,5-\nCSLM, respectively. If the tail (target) words of\nthe grown n-grams are not in the short-list of C-\nSLM, the Pb(·) in Eq. 1 will be applied to calcu-\nlate their probabilities.\n3This Pt(e) hence provides more bilingual information,\nin comparison with using monolingual target LMs only.\n191\nWe combine the n-grams ( n=1,2,3,4,5) togeth-\ner and re-normalize the probabilities and backof-\nf weights of the grown LM. Finally the original\nBNLM and the grown LM are interpolated. The\nentire process is illustrated in Figure 1.\nCorpus \nPhrase Table \nGrown n-grams \nwith Probabilities \nGrown LM \nOutput \nInput \nInterpolate \nGrown n-grams \nCSLM \nBNLM \nConnecting \nPhrases \nFigure 1: NN based bilingual LM growing.\n4 Experiments and Results\n4.1 Experiment Setting up\nThe same setting up of the NTCIR-9 Chinese to\nEnglish translation baseline system (Goto et al.,\n2011) was followed, only with various LMs to\ncompare them. The Moses phrase-based SMT\nsystem was applied (Koehn et al., 2007), togeth-\ner with GIZA++ (Och and Ney, 2003) for align-\nment and MERT (Och, 2003) for tuning on the de-\nvelopment data. Fourteen standard SMT features\nwere used: ﬁve translation model scores, one word\npenalty score, seven distortion scores, and one LM\nscore. The translation performance was measured\nby the case-insensitive BLEU on the tokenized test\ndata.\nWe used the patent data for the Chinese to En-\nglish patent translation subtask from the NTCIR-9\npatent translation task (Goto et al., 2011). The par-\nallel training, development, and test data sets con-\nsist of 1 million (M), 2,000, and 2,000 sentences,\nrespectively.\nUsing SRILM (Stolcke, 2002; Stolcke et al.,\n2011), we trained a 5-gram LM with the interpo-\nlated Kneser-Ney smoothing method using the 1M\nEnglish training sentences containing 42M words\nwithout cutoff. The 2,3,4,5-CSLMs were trained\non the same 1M training sentences using CSLM\ntoolkit (Schwenk, 2007; Schwenk, 2010). The set-\ntings for CSLMs were: input layer of the same\ndimension as vocabulary size (456K), projection\nlayer of dimension 256 for each word, hidden lay-\ner of dimension 384 and output layer (short-list) of\ndimension 8192, which were recommended in the\nCSLM toolkit and (Wang et al., 2013a) 4.\n4Arsoy used around 55 M words as the corpus, including\n4.2 Results\nThe experiment results were divided into four\ngroups: the original BNLMs (BN), the CSLM\nRe-ranking (RE), our previous converting (WA),\nthe Arsoy’s growing, and our growing methods.\nFor our bilingual LM growing method, 5 bilingual\ngrown LMs (BI-1 to 5) were conducted in increas-\ning sizes. For the method of Arsoy, 5 grown LMs\n(AR-1 to 5) with similar size of BI-1 to 5 were also\nconducted, respectively.\nFor the CSLM re-ranking, we used CSLM to\nre-rank the 100-best lists of SMT. Our previous\nconverted LM, Arsoy’s grown LMs and bilingual\ngrown LMs were interpolated with the original\nBNLMs, using default setting of SRILM 5. To re-\nduce the randomness of MERT, we used two meth-\nods for tuning the weights of different SMT fea-\ntures, and two BLEU scores are corresponding to\nthese two methods. The BLEU-s indicated that the\nsame weights of the BNLM (BN) features were\nused for all the SMT systems. The BLEU-i indi-\ncated that the MERT was run independently by\nthree times and the average BLEU scores were\ntaken.\nWe also performed the paired bootstrap re-\nsampling test (Koehn, 2004) 6. Two thousands\nsamples were sampled for each signiﬁcance test.\nThe marks at the right of the BLEU score indicated\nwhether the LMs were signiﬁcantly better/worse\nthan the Arsoy’s grown LMs with the same IDs\nfor SMT (“ ++/□□”: signiﬁcantly better/worse at\nα = 0. 01, “ +/□”: α = 0. 05, no mark: not signif-\nicantly better/worse at α = 0. 05).\nFrom the results shown in Table 1, we can get\nthe following observations:\n(1) Nearly all the bilingual grown LMs outper-\nformed both BNLM and our previous converted\nLM on PPL and BLEU. As the size of grown LM-\ns is increased, the PPL always decreased and the\nBLEU scores trended to increase. These indicated\nthat our proposed method can give better probabil-\nity estimation for LM and better performance for\nSMT.\n(2) In comparison with the grown LMs in Ar-\n84K words as vocabulary, and 20K words as short-list. In this\npaper, we used the same setting as our previous work, which\ncovers 92.89% of the frequency of words in the training cor-\npus, for all the baselines and our method for fair comparison.\n5In our previous work, we used the development data to\ntune the weights of interpolation. In this paper, we used the\ndefault 0.5 as the interpolation weights for fair comparison.\n6We used the code available at http://www.ark.cs.\ncmu.edu/MT\n192\nTable 1: Performance of the Grown LMs\nLMs n-grams PPL BLEU-s BLEU-i ALH\nBN 73.9M 108.8 32.19 32.19 3.03\nRE N/A 97.5 32.34 32.42 N/A\nWA 73.9M 104.4 32.60 32.62 3.03\nAR-1 217.6M 103.3 32.55 32.75 3.14\nAR-2 323.8M 103.1 32.61 32.64 3.18\nAR-3 458.5M 103.0 32.39 32.71 3.20\nAR-4 565.6M 102.8 32.67 32.51 3.21\nAR-5 712.2M 102.5 32.49 32.60 3.22\nBI-1 223.5M 101.9 32.81+ 33.02+ 3.20\nBI-2 343.6M 101.0 32.92+ 33.11++ 3.24\nBI-3 464.5M 100.6 33.08++ 33.25++ 3.26\nBI-4 571.0M 100.3 33.15++ 33.12++ 3.28\nBI-5 705.5M 100.1 33.11++ 33.24++ 3.31\nsoy’s method, our grown LMs obtained better P-\nPL and signiﬁcantly better BLEU with the sim-\nilar size. Furthermore, the improvement of PPL\nand BLEU of the existing methods became satu-\nrated much more quickly than ours did, as the LMs\ngrew.\n(3) The last column was the Average Length of\nthe n-grams Hit (ALH) in SMT decoding for dif-\nferent LMs using the following function\nALH =\n5∑\ni=1\nPi□gram × i, (6)\nwhere the Pi□gram means the ratio of the i-grams\nhit in SMT decoding. There were also positive\ncorrelations between ALH, PPL and BLEUs. The\nALH of bilingual grown LM was longer than that\nof the Arsoy’s grown LM of the similar size. In\nanother word, less back-off was used for our pro-\nposed grown LMs in SMT decoding.\n4.3 Experiments on TED Corpus\nThe TED corpus is in special domain as discussed\nin the introduction, where large extra monolingual\ncorpora are hard to ﬁnd. In this subsection, we\nconducted the SMT experiments on TED corpora\nusing our proposed LM growing method, to eval-\nuate whether our method was adaptable to some\nspecial domains.\nWe mainly followed the baselines of the IWSLT\n2014 evaluation campaign 7, only with a few mod-\niﬁcations such as the LM toolkits and n-gram or-\nder for constructing LMs. The Chinese (CN) to\nEnglish (EN) language pair was chosen, using de-\nv2010 as development data and test2010 as evalu-\nation data. The same LM growing method was ap-\n7https://wit3.fbk.eu/\nplied on TED corpora as on NTCIR corpora. The\nresults were shown in Table 2.\nTable 2: CN-EN TED Experiments\nLMs n-grams PPL BLEU-s\nBN 7.8M 87.1 12.41\nWA 7.8M 85.3 12.73\nBI-1 23.1M 79.2 12.92\nBI-2 49.7M 78.3 13.16\nBI-3 73.4M 77.6 13.24\nTable 2 indicated that our proposed LM grow-\ning method improved both PPL and BLEU in com-\nparison with both BNLM and our previous CSLM\nconverting method, so it was suitable for domain\nadaptation, which is one of focuses of the current\nSMT research.\n5 Conclusion\nIn this paper, we have proposed a neural network\nbased bilingual LM growing method by using the\nbilingual parallel corpus only for SMT. The results\nshow that our proposed method can improve both\nLM and SMT performance, and outperforms the\nexisting LM growing methods signiﬁcantly with-\nout extra corpus. The connecting phrase-based\nmethod can also be applied to LM adaptation.\nAcknowledgments\nWe appreciate the helpful discussion with Dr.\nIsao Goto and Zhongye Jia, and three anony-\nmous reviewers for valuable comments and sug-\ngestions on our paper. Rui Wang, Hai Zhao\nand Bao-Liang Lu were partially supported by\nthe National Natural Science Foundation of Chi-\nna (No. 60903119, No. 61170114, and No.\n61272248), the National Basic Research Program\nof China (No. 2013CB329401), the Science and\nTechnology Commission of Shanghai Municipali-\nty (No. 13511500200), the European Union Sev-\nenth Framework Program (No. 247619), the Cai\nYuanpei Program (CSC fund 201304490199 and\n201304490171), and the art and science interdis-\ncipline funds of Shanghai Jiao Tong University\n(A study on mobilization mechanism and alerting\nthreshold setting for online community, and media\nimage and psychology evaluation: a computation-\nal intelligence approach). The corresponding au-\nthor of this paper, according to the meaning given\nto this role by Shanghai Jiao Tong University, is\nHai Zhao.\n193\nReferences\nEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,\nand Abhinav Sethy. 2013. Converting neural net-\nwork language models into back-off language mod-\nels for efﬁcient decoding in automatic speech recog-\nnition. In Proceedings of ICASSP-2013, Vancouver,\nCanada, May. IEEE.\nEbru Arsoy, Stanley F. Chen, Bhuvana Ramabhadran,\nand Abhinav Sethy. 2014. Converting neural net-\nwork language models into back-off language mod-\nels for efﬁcient decoding in automatic speech recog-\nnition. IEEE/ACM Transactions on Audio, Speech,\nand Language, 22(1):184–192.\nMichael Auli, Michel Galley, Chris Quirk, and Geof-\nfrey Zweig. 2013. Joint language and translation\nmodeling with recurrent neural networks. In Pro-\ncessings of EMNLP-2013, pages 1044–1054, Seat-\ntle, Washington, USA, October. Association for\nComputational Linguistics.\nJerome R Bellegarda. 2004. Statistical language mod-\nel adaptation: review and perspectives. Speech\nCommunication, 42(1):93–108. Adaptation Meth-\nods for Speech Recognition.\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch (JMLR), 3:1137–1155, March.\nMauro Cettolo, Christian Girardi, and Marcello Fed-\nerico. 2012. Wit 3: Web inventory of transcribed\nand translated talks. In Proceedings of EAMT-2012,\npages 261–268, Trento, Italy, May.\nPhilip Clarkson and A.J. Robinson. 1997. Lan-\nguage model adaptation using mixtures and an ex-\nponentially decaying cache. In Proceedings of\nICASSP-1997, volume 2, pages 799–802 vol.2, Mu-\nnich,Germany.\nJacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas\nLamar, Richard Schwartz, and John Makhoul. 2014.\nFast and robust neural network joint models for sta-\ntistical machine translation. In Proceedings of ACL-\n2014, pages 1370–1380, Baltimore, Maryland, June.\nAssociation for Computational Linguistics.\nIsao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and\nBenjamin K. Tsou. 2011. Overview of the paten-\nt machine translation task at the NTCIR-9 work-\nshop. In Proceedings of NTCIR-9 Workshop Meet-\ning, pages 559–578, Tokyo, Japan, December.\nRukmini Iyer, Mari Ostendorf, and Herbert Gish.\n1997. Using out-of-domain data to improve in-\ndomain language models. Signal Processing Letter-\ns, IEEE, 4(8):221–223.\nZhongye Jia and Hai Zhao. 2013. Kyss 1.0: a\nframework for automatic evaluation of chinese input\nmethod engines. In Proceedings of IJCNLP-2013,\npages 1195–1201, Nagoya, Japan, October. Asian\nFederation of Natural Language Processing.\nZhongye Jia and Hai Zhao. 2014. A joint graph mod-\nel for pinyin-to-chinese conversion with typo cor-\nrection. In Proceedings of ACL-2014, pages 1512–\n1523, Baltimore, Maryland, June. Association for\nComputational Linguistics.\nPhilipp Koehn and Josh Schroeder. 2007. Experi-\nments in domain adaptation for statistical machine\ntranslation. In Proceedings of ACL-2007 Workshop\non Statistical Machine Translation, pages 224–227,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertol-\ndi, Brooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexan-\ndra Constantin, and Evan Herbst. 2007. Moses:\nOpen source toolkit for statistical machine transla-\ntion. In Proceedings of ACL-2007, pages 177–180,\nPrague, Czech Republic, June. Association for Com-\nputational Linguistics.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceedings\nof EMNLP-2004, pages 388–395, Barcelona, Spain,\nJuly. Association for Computational Linguistics.\nHai-Son Le, Ilya Oparin, Alexandre Allauzen, J Gau-\nvain, and Franc ¸ois Yvon. 2011. Structured output\nlayer neural network language model. In Proceed-\nings of ICASSP-2011, pages 5524–5527, Prague,\nCzech Republic, May. IEEE.\nShujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014.\nA recursive recurrent neural network for statistical\nmachine translation. In Proceedings of ACL-2014,\npages 1491–1500, Baltimore, Maryland, June. As-\nsociation for Computational Linguistics.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Re-\ncurrent neural network based language model. In\nProceedings of INTERSPEECH-2010, pages 1045–\n1048.\nJan Niehues and Alex Waibel. 2012. Continuous\nspace language models using restricted boltzman-\nn machines. In Proceedings of IWSLT-2012, pages\n311–318, Hong Kong.\nThomas Niesler and Phil Woodland. 1996. A variable-\nlength category-based n-gram language model. In\nProceedings of ICASSP-1996, volume 1, pages 164–\n167 vol. 1.\nFranz Josef Och and Hermann Ney. 2003. A sys-\ntematic comparison of various statistical alignmen-\nt models. Computational Linguistics, 29(1):19–51,\nMarch.\nFranz Josef Och. 2003. Minimum error rate training\nin statistical machine translation. In Proceedings\nof ACL-2003, pages 160–167, Sapporo, Japan, July.\nAssociation for Computational Linguistics.\n194\nEric Sven Ristad and Robert G. Thomas. 1995. New\ntechniques for context modeling. In Proceedings\nof ACL-1995, pages 220–227, Cambridge, Mas-\nsachusetts. Association for Computational Linguis-\ntics.\nHolger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-\nvain. 2006. Continuous space language models for\nstatistical machine translation. In Proceedings of\nCOLING ACL-2006, pages 723–730, Sydney, Aus-\ntralia, July. Association for Computational Linguis-\ntics.\nHolger Schwenk, Anthony Rousseau, and Mohammed\nAttik. 2012. Large, pruned or continuous space\nlanguage models on a gpu for statistical machine\ntranslation. In Proceedings of the NAACL-HLT 2012\nWorkshop: Will We Ever Really Replace the N-gram\nModel? On the Future of Language Modeling for\nHLT, WLM ’12, pages 11–19, Montreal, Canada,\nJune. Association for Computational Linguistics.\nHolger Schwenk. 2007. Continuous space lan-\nguage models. Computer Speech and Language,\n21(3):492–518.\nHolger Schwenk. 2010. Continuous-space language\nmodels for statistical machine translation. The\nPrague Bulletin of Mathematical Linguistics, pages\n137–146.\nVesa Siivola, Teemu Hirsimki, and Sami Virpioja.\n2007. On growing and pruning kneser-ney s-\nmoothed n-gram models. IEEE Transactions on Au-\ndio, Speech, and Language, 15(5):1617–1624.\nManhung Siu and Mari Ostendorf. 2000. Variable n-\ngrams and extensions for conversational speech lan-\nguage modeling. IEEE Transactions on Speech and\nAudio, 8(1):63–75.\nLe Hai Son, Alexandre Allauzen, Guillaume Wis-\nniewski, and Franc ¸ois Yvon. 2010. Training con-\ntinuous space language models: some practical is-\nsues. In Proceedings of EMNLP-2010, pages 778–\n788, Cambridge, Massachusetts, October. Associa-\ntion for Computational Linguistics.\nLe Hai Son, Alexandre Allauzen, and Franc ¸ois Yvon.\n2012. Continuous space translation models with\nneural networks. In Proceedings of NAACL HLT-\n2012, pages 39–48, Montreal, Canada, June. Asso-\nciation for Computational Linguistics.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Vic-\ntor Abrash. 2011. SRILM at sixteen: Update and\noutlook. In Proceedings of INTERSPEECH 2011,\nWaikoloa, HI, USA, December.\nAndreas Stolcke. 1998. Entropy-based pruning of\nbackoff language models. In Proceedings of DARPA\nBroadcast News Transcription and Understanding\nWorkshop, pages 270–274, Lansdowne, V A, USA.\nAndreas Stolcke. 2002. Srilm-an extensible\nlanguage modeling toolkit. In Proceedings of\nINTERSPEECH-2002, pages 257–286, Seattle, US-\nA, November.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum,\nand David Chiang. 2013. Decoding with large-\nscale neural language models improves translation.\nIn Proceedings of EMNLP-2013, pages 1387–1392,\nSeattle, Washington, USA, October. Association for\nComputational Linguistics.\nRui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-\nta, Hai Zhao, and Bao-Liang Lu. 2013a. Convert-\ning continuous-space language models into n-gram\nlanguage models for statistical machine translation.\nIn Proceedings of EMNLP-2013, pages 845–850,\nSeattle, Washington, USA, October. Association for\nComputational Linguistics.\nXiaolin Wang, Hai Zhao, and Bao-Liang Lu. 2013b.\nLabeled alignment for recognizing textual entail-\nment. In Proceedings of IJCNLP-2013, pages 605–\n613, Nagoya, Japan, October. Asian Federation of\nNatural Language Processing.\nXiao-Lin Wang, Yang-Yang Chen, Hai Zhao, and Bao-\nLiang Lu. 2014. Parallelized extreme learning ma-\nchine ensemble based on minmax modular network.\nNeurocomputing, 128(0):31 – 41.\nQiongkai Xu and Hai Zhao. 2012. Using deep lin-\nguistic features for ﬁnding deceptive opinion spam.\nIn Proceedings of COLING-2012, pages 1341–1350,\nMumbai, India, December. The COLING 2012 Or-\nganizing Committee.\nJingyi Zhang and Hai Zhao. 2013. Improving function\nword alignment with frequency and syntactic infor-\nmation. In Proceedings of IJCAI-2013, pages 2211–\n2217. AAAI Press.\nXiaotian Zhang, Hai Zhao, and Cong Hui. 2012.\nA machine learning approach to convert CCGbank\nto Penn treebank. In Proceedings of COLING-\n2012, pages 535–542, Mumbai, India, December.\nThe COLING 2012 Organizing Committee.\nHai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-\nLiang Lu. 2013. An empirical study on word\nsegmentation for chinese machine translation. In\nAlexander Gelbukh, editor, Computational Linguis-\ntics and Intelligent Text Processing, volume 7817 of\nLecture Notes in Computer Science, pages 248–263.\nSpringer Berlin Heidelberg.\n195",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.824086606502533
    },
    {
      "name": "Computer science",
      "score": 0.7928099036216736
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5921885371208191
    },
    {
      "name": "Translation (biology)",
      "score": 0.5865991711616516
    },
    {
      "name": "Natural language processing",
      "score": 0.5657116174697876
    },
    {
      "name": "Language model",
      "score": 0.47078412771224976
    },
    {
      "name": "Example-based machine translation",
      "score": 0.46560293436050415
    },
    {
      "name": "Artificial neural network",
      "score": 0.4579339325428009
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I90023481",
      "name": "National Institute of Information and Communications Technology",
      "country": "JP"
    }
  ]
}