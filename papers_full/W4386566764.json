{
  "title": "Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering",
  "url": "https://openalex.org/W4386566764",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108963801",
      "name": "Wenhu Chen",
      "affiliations": [
        "Google (United States)",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4209920322",
      "name": "Pat Verga",
      "affiliations": [
        "University of Southern California",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2109999465",
      "name": "Michiel de Jong",
      "affiliations": [
        "University of Southern California",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2129265196",
      "name": "John Wieting",
      "affiliations": [
        "University of Southern California",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2115385359",
      "name": "William W. Cohen",
      "affiliations": [
        "Google (United States)",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2606333299",
    "https://openalex.org/W3207095490",
    "https://openalex.org/W3184402450",
    "https://openalex.org/W3125238517",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2945760033",
    "https://openalex.org/W3173027903",
    "https://openalex.org/W2949849869",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3167783161",
    "https://openalex.org/W3169726359",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3167673057",
    "https://openalex.org/W3190126809",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4287122359",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287118989",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4287865052",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W3024786184",
    "https://openalex.org/W3118423943",
    "https://openalex.org/W2937036051",
    "https://openalex.org/W3175627818",
    "https://openalex.org/W2511149293",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3034188538",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4286905627",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3099700870"
  ],
  "abstract": "Existing state-of-the-art methods for open-domain question-answering (ODQA) use an open book approach in which information is first retrieved from a large text corpus or knowledge base (KB) and then reasoned over to produce an answer. A recent alternative is to retrieve from a collection of previously-generated question-answer pairs; this has several practical advantages including being more memory and compute-efficient. Question-answer pairs are also appealing in that they can be viewed as an intermediate between text and KB triples: like KB triples, they often concisely express a single relationship, but like text, have much higher coverage than traditional KBs. In this work, we describe a new QA system that augments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval. The pre-training task substantially simplifies training and greatly improves performance on smaller QA benchmarks. Unlike prior systems of this sort, our QA system can also answer multi-hop questions that do not explicitly appear in the collection of stored question-answer pairs.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1597–1610\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nAugmenting Pre-trained Language Models with\nQA-Memory for Open-Domain Question Answering\nWenhu Chen, Pat Verga, Michiel de Jong†, John Wieting, William W. Cohen\nGoogle Research, University of Southern California†\n{wenhuchen,patverga,jwieting,wcohen}@google.com, msdejong@usc.edu\nAbstract\nExisting state-of-the-art methods for open-\ndomain question-answering (ODQA) use an\nopen book approach in which information is\nﬁrst retrieved from a large text corpus or\nknowledge base (KB) and then reasoned over\nto produce an answer. A recent alternative\nis to retrieve from a collection of previously-\ngenerated question-answer pairs; this has sev-\neral practical advantages including being more\nmemory and compute-efﬁcient. Question-\nanswer pairs are also appealing in that they\ncan be viewed as an intermediate between text\nand KB triples: like KB triples, they often con-\ncisely express a single relationship, but like\ntext, have much higher coverage than tradi-\ntional KBs. In this work, we describe a new\nQA system that augments a text-to-text model\nwith a large memory of question-answer pairs,\nand a new pre-training task for the latent step\nof question retrieval. The pre-training task\nsubstantially simpliﬁes training and greatly\nimproves performance on smaller QA bench-\nmarks. Unlike prior systems of this sort, our\nQA system can also answer multi-hop ques-\ntions that do not explicitly appear in the col-\nlection of stored question-answer pairs.\n1 Introduction\nOpen-domain question answering (ODQA) is a\nwell-studied knowledge-intensive task. State-of-\nthe-art methods require retrieving relevant knowl-\nedge from a large corpus or datastore before rea-\nsoning over this retrieved evidence. Most existing\nmethods retrieve documents (Chen et al., 2017a;\nLee et al., 2019; Karpukhin et al., 2020) or struc-\ntured KB triples (Verga et al., 2021). Recently, a\nfew works have proposed retrieving from a collec-\ntion of question-answer (QA) pairs—an approach\nmade feasible by advances in scalable automatic\nquestion generation. In this setting, a new ques-\ntion is answered by retrieving paraphrases from a\nquestion index, and returning the associated answer\n(Xiao et al., 2021; Lewis et al., 2021). Notably, the\nRePAQ system from (Lewis et al., 2021) won the\n2020 EfﬁcientQA competition (Min et al., 2021),\noutperforming closed-book QA (CBQA) models by\na signiﬁcant margin and matching the prior SoTA\nperformance on NQ (Kwiatkowski et al., 2019).\nA collection of QA pairs is appealing for several\nreasons. As opposed to text passages and much like\na KB triple, QA pairs are often concise, tending\nto express a single relationship. However, unlike\nKB triples, QA collections have good coverage of\nactually asked questions like those in standard open\nQA datasets. RePAQ demonstrated several advan-\ntageous properties such as memory and computa-\ntional efﬁciency, strong selective QA performance\n(i.e. selectively abstaining from answering), and ef-\nfective ensembling with text-retrieval QA systems.\nHowever, question-retrieval QA systems have\nseveral limitations as well. First, there is no large-\nscale supervised data for question-question re-\ntrieval. This contrasts with the step of retriev-\ning text given a question, where supervised data\nis used to build retrievers like DPR (Karpukhin\net al., 2020). To address this, RePAQ uses a latent-\nretrieval training process (similar to REALM (Lee\net al., 2019)), in which the retriever is trained us-\ning the downstream end loss from the QA task.\nThis requires asynchronously updating the index\nas training proceeds, a process that is complex and\ncomputationally expensive. This is also a problem\nfor domains with limited QA data: as we will show,\nRePAQ’s performance is disappointing on smaller\ndatasets like WebQuestions (Berant et al., 2013),\ncontaining only 3K training instances. To address\nthis problem, we introduce a novel pre-training\ntask for question retrieval, which can be applied to\nany text-QA dataset, which great improves perfor-\nmance on smaller datasets.\nA second problem is that RePAQ is limited to\nanswering questions explicitly stored in the index,\nor paraphrases of such questions. This contrasts\n1597\nFigure 1: During pre-training, the encoder ﬁrst encodes textual input and use special token representation to query\nthe QA-memory. The retrieved QA-pairs are integrated to the decoder to generate outputs.\nwith QA systems that retrieve from KBs, which can\ntypically generate complex queries that combine\nthe atomic triples in the KB.To address this, we\npresent an extended model that answers multi-hop\nquestions by iteratively retrieving from a question-\nanswer corpus, the ﬁrst question-retrieval-based\nQA system that addresses this task.\nIn more detail, we propose a new QA-Memory-\nAugmented Transformer (QAMAT) with better\ncompositionality paired with a lower complex-\nity training strategy. QAMAT is based on a\nT5 encoder-decoder (Raffel et al., 2020) paired\nwith an integrated key-value memory (Khandelwal\net al., 2019; Borgeaud et al., 2021) populated with\nquestion-answer pairs (See Figure 1). Given an\ninput, the encoder generates a query representation\nscored against the QA memory and retrieves the\ntop-K relevant QA pairs. The encoder then repro-\ncesses the input along with the retrievals forming a\nQA-injected representation which is passed to the\ndecoder to attend to and generate.\nTo reduce the training (ﬁne-tuning) sample com-\nplexity, we propose to ﬁrst pre-train QAMAT on a\nlarge-scale corpus to teach the model to retrieve and\ninterpret QA pairs. We construct the pre-training\ncorpus by leveraging existing methods for ques-\ntion generation, producing a very large set of po-\ntentially interesting questions from text passages\n(Zhou et al., 2017; Alberti et al., 2019; Lewis et al.,\n2021). For each QA pair and the passage it was\ngenerated from, we mask the answer and train the\nmodel to ﬁll the mask by retrieving and using an\nappropriate QA pair. We show that pre-training\ngreatly boosts the model’s performance and helps\nthe model generalize to different domains. For\nexample, the pre-trained model can achieve a zero-\nshot performance of 40% EM on NQ and TriviaQA\nwithout any ﬁne-tuning.\nThe effectiveness of this pre-training task means\nthat we can avoid the expensive latent training pro-\ncedure used by RePAQ, and instead use an efﬁcient\ntwo-stage training pipeline. In the ﬁrst stage, we\nuse a small local in-batch memory of QA pairs\nto optimize the QA pair encoder. We then freeze\nthe encoder and construct the index for the global\nmemory. In the second stage, we retrieve from this\nﬁxed global memory and continue to optimize the\nremaining parameters—including the parameters\nused to construct queries to the global memory—\nfor better performance.\nLastly, we extend QAMAT to build QAMAT+,\nwhich iteratively retrieves from the memory to gen-\nerate outputs. We demonstrate that QAMAT+ ef-\nfectively chains multiple QA-pairs together to an-\nswer multi-hop questions in HotpotQA (Yang et al.,\n2018) and Musique (Trivedi et al., 2021). Such\ncompositional reasoning capability is nonexistent\nin RePAQ (Lewis et al., 2021).\nIn summary, we develop a new QA augmented\narchitecture which extends the lines of research\nconsidering QA pairs as a representation of knowl-\nedge as well as those on memory-augmented lan-\nguage models. When paired with our proposedpre-\ntraining strategy (section 4), we address many of\nthe shortcomings of previous QA-indexing-based\napproaches leading to lower sample complexity\ntraining and the ability to perform compositional\nreasoning (subsection 3.5).\n2 Related Work\n2.1 Retriever-Reader Models\nRetrieve-and-read models have been widely studied\nto address knowledge-intensive tasks and achieve\nstate-of-the-art performance on most QA tasks.\nThese methods use two models, one to retrieve\nfrom a passage index based on BM25 (Robertson\nand Zaragoza, 2009), and one to perform reading\ncomprehension on the returned passages (Chen\n1598\net al., 2017b). More recently, deep retrieval models\nhave gained more popularity to replace traditional\nstring-similarity retriever.\nDPR (Karpukhin et al., 2020) is a widely\nused supervised approach to achieve better results\nthan BM25 on a large collection of text retrieval\ntasks (Thakur et al., 2021). Contrastive learning\nis used to train the deep retriever model to distin-\nguish between annotated positive and mined nega-\ntive candidates. More recently, ColBERT (Khattab\nand Zaharia, 2020) has been proposed to integrate\nmore ﬁne-grained late fusion between query and\ncontext to improve DPR.\nRetrieval Augmented Generation (RAG) (Lewis\net al., 2020), Fusion-in-Decoder (FiD) (Izacard and\nGrave, 2021) and End-to-end training of Multi-\nDocument Reader and Retriever (EmDR) (Singh\net al., 2021) are proposed to read retrievals to ex-\ntract or generate answers. These models require a\ntrained retriever/reranker to obtain top-K results,\nwhich are fed to the reader to generate the answer.\nAs discussed in section 1, our model provides better\ninterpretability due to atomic knowledge represen-\ntation. In subsection 5.4, we also demonstrate that\nour model’s inference speed is 5x faster.\n2.2 Question Generation\nThe problem of question generation (Zhou et al.,\n2017) has attracted attention from the community\nin recent years. It has been used for data augmen-\ntation (Alberti et al., 2019) to improve current QA\nsystems or to improve retrieval systems (Nogueira\net al., 2019). Pan et al. (2021) also demonstrated\nthat by connecting generated single-hop questions,\nwe can train zero-shot multi-hop question answer-\ning systems. Besides QA, it has also been widely\nused in other domains like evaluating factual con-\nsistency of summarization (Eyal et al., 2019; Wang\net al., 2020) or enhancing contextualized represen-\ntation (Jia et al., 2021). Most related to our work\nis PAQ (Lewis et al., 2021), which aims to gener-\nate and use QA pairs as retrieval units for question\nanswering. The efﬁcacy of this data was further\nveriﬁed when it was used to train DPR, yielding\nbetter domain generalization (O˘guz et al., 2021).\n2.3 Memory-Augmented Language Models\nEnd-to-end memory-augmented language models\naim to train a model to explicitly access external\nmemory. The current work is focused on storing\nentities (Févry et al., 2020), entity mentions (Dhin-\ngra et al., 2019; Sun et al., 2021; de Jong et al.,\n2022) or knowledge triples (Verga et al., 2021).\nMemory attention layers are then used to inﬂuence\nthe computation of transformer layers. These enti-\nties and fact-centric memories are naturally atomic\nand interpretable, and models employing them\nhave shown competitive performance on entity-\nfocused QA datasets like Web-Question-SP (Yih\net al., 2016) and ComplexWebQuestions (Talmor\nand Berant, 2018). However, these models are lim-\nited to integrating entity-centric knowledge and\nclassifying the answer w.r.t a pre-deﬁned entity\nlist. For example, these models cannot handle\nquestions with non-entity answers, e.g. number,\ndate, noun phrases, etc, which are ubiquitous in\nvarious QA datasets like NQ (Kwiatkowski et al.,\n2019), SQuAD (Rajpurkar et al., 2016), or Hot-\npotQA (Yang et al., 2018).\n3 Our Model: QAMAT\n3.1 Problem Deﬁnition\nThe input to our model is a piece of text X =\nx1,··· ,xn, where X is either a question dur-\ning ﬁne-tuning or a paragraph in pre-training.\nPre-training is formulated as a span corruption\ntask (Raffel et al., 2019): given an example in the\npre-training corpus as (X,{Qk,Ak}m\nk=1), where\nA1,··· ,Am correspond to spans in the input\nX. We sample k spans from X as a cloze an-\nswer and replace all tokens within a span with\na [MASK] token, and the model needs to re-\ncover all the answers. During ﬁne-tuning, we add\nan artiﬁcial [MASK] in the question front, and\nlet the model recover this as the answer. The\npre-training/ﬁne-tuning objective function is to\nmaximize the masked language model objectives\np(Y|X) = ∑\nmi∈M p(Y|X,mi)p(mi|X), which\nmarginalizes over the entire memory M. However,\ndue to its intractability in a large-scale memory, we\nadopt an approximation to only sum over the top-K\nmemory entries TopK(M).\nWe deﬁne the encoder function as fθ, which\ntakes an input sequence X as input to generate a\nsequence of vector Fθ(X) ∈Rn×d, where nis the\ninput length and dis the hidden size. The desig-\nnated position of Fθ(X) will be used as the query\nand memory representation, which are denoted\nas fθ(X; [MASK]) ∈Rd (at [MASK] position)\nand memory key/value as fθ(mk\ni; [CLS]) ∈Rd (at\n[CLS] position). For brevity, we leave out [MASK]\nand [CLS] and simply use fθ(·).\nWe also deﬁne a broadcast operator Bn\nk(x) to\n1599\nFigure 2: Architecture: upper ﬁgure shows the retrieval process with shared encoder, the lower ﬁgure shows the\ndecoder process to leverage neural and discrete representation of memory retrieval.\nbroadcast a vector into a matrix by assigning the\nvector xto k-th row while ﬁlling the rest with zero,\ni.e. Bn\nk(x) = [0,...xT,..., 0].\n3.2 Dense Retriever\nThe memory M contains separate key and value\ncomponents, where the key mk\ni contains a ques-\ntion, and the corresponding value mv\ni contains the\nquestion-answer concatenation. To retrieve the top-\nk QA-pairs from the memory, we use our encoder\nfθ to encode X and mi separately and select the\ntop-K entries TopK(M) based on their inner prod-\nuct, i.e. TopKmi∈Mfθ(X) ·fθ(mk\ni).\n3.3 Neural Memory Integration\nAfter the model retrieves the Top-K candidates,\ntheir corresponding memory values mv\ni needs be\nleveraged into the encoder to inﬂuence the decoder\noutputs in a differentiable fashion. We write our\nobjective p(Y|X) as:\n∑\nmi∈TopK(M)\np(Y|X,mi)p(mi|X)\n=\n∑\nmi∈TopK(M)\np(mi|X)gθ(Y|Fθ(X) +Bn\nk[fθ(mv\ni)])\n≈gθ(Y|\n∑\nmi∈TopK(M)\np(mi|X)(Fθ(X) +Bn\nk[fθ(mv\ni)]))\n=gθ(Y|Fθ(X) +Bn\nk[\n∑\nmi∈TopK(M)\np(mi|X)fθ(mv\ni)])\np(mi|X) = efθ(X)·fθ(mk\ni)\n∑\nmi∈TopKM\nefθ(X)·fθ(mk\ni)\nThe probability p(Y|X,mi) is parmeterized by a\ndecoder function gθ, which takes a memory-infused\nencoder representation Fθ(X) + Bn\nk[fθ(mv\ni)] as\ninput. We approximate this marginal probabil-\nity by pulling weighted summation inside the de-\ncoder function gθ to derive an aggregated memory-\ninfused encoder representations Fθ(X) + Bn\nk[···].\nThe retrieval weight p(m|X) is calculated as the\nsoftmax over the retrieval score over top-K items.\nFor simplicity, H(X,Top K(M),p(m|X)) is used\nto denote this encoder representation, thus the ob-\njective can be written as follows:\np(Y|X) =gθ(Y|H(X,TopK(M),p(m|X))) (1)\nAs shown in the upper part of Figure 2, we ﬁrst\nuse weighted-sum over the neural representation of\nretrieved memory entries fθ(mv\ni) and then simply\nadd it to the encoder representation to infuse the re-\ntrieved QA-pair information. These two operations\nare both differentiable, which makes the it possible\nto train retriever latently. In essence, the retriever\nwill increasing weights p(mi|X) on more relevant\nmemory items instead of irrelevant ones.\n3.4 Neural + Discrete Memory Integration\nA disadvantage of adopting weighted-sum∑\nip(mi|X)fθ(mv\ni) ∈ Rd is that all the infor-\nmation from all of the top-K documents are\noverly compressed into a d-dimension vector,\nwhereas the token retrieval representation contains\nmore information. Therefore, we propose to\nadd a ﬁne-grained token-wise representation\nˆH(X,Top K(M)) to help the model access the\nretrieved discrete values mi directly. The represen-\ntation is obtained by encoding the concatenation\nof the input X and retrieved discrete tokens\nˆX = Concat[mk; ··· ; m1; X] ∈R(n+k|m|)×d.\nSuch discrete memory integration greatly en-\nriches the representation for mi and enables cross-\nattention between the query and retrieval, address-\ning the bottleneck problem. However, such dis-\ncrete representation cannot propagate gradients\nback to the retriever. Finally, we propose to com-\nbine the neural memory H(X,·) and discrete mem-\nory ˆH(X,·) integration to combine their merits.\np(Y|X)\n=gθ(Y|H′(X,TopK(M),p(m|X)) +λˆH(X,TopK(M)))\n(2)\nwhere the λis the balancing factor to weight the\ntwo representations. We use H′(...) = [0; H(...)]\nto represent the concatenation of zero-matrix 0 ∈\nRk|m|×d, which has consistent dimension with ˆH.\n1600\nAfter leveraging ˆH, our model demonstrates signif-\nicant improvements on the downstream tasks with\n14% on TriviaQA and 10% on HotpotQA.\nH(X,Top K(M),p(m|X)) is only used to la-\ntently train the retriever, after training, we can drop\nit and only use the concatenated representation\nˆH(X,Top K(M)) as the encoder representation.\nThe decoder gθ will attend to ˆH and perform a\ngreedy search over vocabulary to generate output.\n3.5 Multi-hop Extension\nTo further extend QAMAT’s capability to per-\nform compositional reasoning, we propose a cas-\ncaded architecture (depicted in Figure 3) known\nas QAMAT+, where the model learns to perform\nmultiple rounds of retrieval before feeding the\naugmented inputs to the decoder. Speciﬁcally\nfor two-hop reasoning, we use X as the query\nto retrieve a ﬁrst-round of top-K memory val-\nues TopK(M; 1) with our learned retriever fθ de-\nscribed in subsection 3.2. Next, we augment the\nquery by concatenating the retrieved values as\nX1 = [TopK(M; 1);X]. This new query X1 is\nused to perform a second round of retrieval to ob-\ntain additional top-K memory values,TopK(M; 2).\nBased on TopK(M; 2), we compute the hybrid\nencoder representation H(X1,TopK(M; 2)) and\nˆH(X1,TopK(M; 2))to compute p(Y|X1; θ).\nFigure 3: QAMAT+ architecture: Multi-Hop frame-\nwork for question-answer memory integration.\n4 Training\n4.1 Pre-training Corpus\nOur QA-pairs are constructed by combining 30M\ndeduplicated QA-pairs from PAQ (Lewis et al.,\n2021)(originally 65M, we delete paraphrases to\nkeep a subset) and 30M additional QA-pairs gen-\nerated from our own pipeline. The additional QA-\npairs are populated from non-overlapping passage\nblocks to increase the knowledge coverage over\nWikipedia. Our QA generation pipeline is simi-\nlar to (Lewis et al., 2021) but trained solely on\nSQuAD 2.0 (Rajpurkar et al., 2018) and ﬁltered\nwith a cheap reading comprehension model rather\nthan FiD (Izacard and Grave, 2021), the details are\nFigure 4: Two stage training procedure: in-batch train-\ning with a batch-speciﬁc memory and end-to-end gradi-\nent updates, global training with a ﬁxed global memory\nand partial gradient updates.\ndescribed in the Appendix. The ﬁnal statistics of\nour QA-memory is described in Table 1, where the\ntotal size is comparable to RePAQ.\nMemory Size #Passages Training Data\nDedup-PAQ 30M 10M NaturalQuestions\nAdditional 30M 10M SQuAD 2.0\nCombined 60M 20M -\nTable 1: The breakdown statistics of our QA corpus.\nWe denote the entire memory as M and formu-\nlate the pre-training corpus as {X,{Qk,Ak}m\nk=1},\nwhere X is the passage aligned with multiple QA-\npairs {Qk,Ak}m\nk=1 generated from it.\n4.2 End-to-End Training\nDuring training, the retrieval process is integrated\ninto the model’s training loop. The most widely\nadopted approach to accomplish this is approxi-\nmate nearest neighbor search (ANNS) efﬁciently\nimplemented by several libraries like ScaNN (Guo\net al., 2020), FAISS (Johnson et al., 2019), etc.\nThese libraries require a ﬁxed set of dense vec-\ntors to construct the index and perform a Nearest-\nNeighbor search using approximate algorithms.\nHowever, our memory encoder fθ is continuously\nupdated, which poses great challenges for ANNS\nindex building. REALM (Guu et al., 2020) and\nRePAQ (Lewis et al., 2021) use an asynchronous in-\ndex building sub-process to refresh the index every\nK steps, which is known to be extremely computa-\ntionally expensive, especially with a large memory.\nTo avoid such expensive computation overhead, we\nare inspired by TOME (de Jong et al., 2022) to\nadopt a two-stage training as shown in Figure 4.\nIn-Batch Pre-training In the ﬁrst stage, in-\nstead of using the whole memory, we propose\na batch-speciﬁc memory that concatenates the\npositive, random negative, and hard negative en-\n1601\ntries from each instance in the batch. Assuming\nwe have a batch size of B containing examples\n{Xi,{Qk\ni,Ak\ni}K\nk=1}B\ni=1. For each example there\nexist Kpositive QA-pairs generated from the given\ncontext Xi. Additionally, we mine K hard nega-\ntive QA-pairs {¯Qk\ni, ¯Ak\ni}K\nk=1 for each input Xi to\nincrease retrieval difﬁculty. This hard negative min-\ning is done with BM25 (Robertson and Zaragoza,\n2009) similar to DPR (Karpukhin et al., 2020). We\nconstruct the in-batch memory by aggregating the\nK×Bpositive QA-pairs and K×Bhard negative\nmemory entries, so the in-batch memory ˆM con-\ntains a total of 2K×B QA-pairs (roughly a few\nthousand). Due to the small size of the memory,\nwe can construct the memory index very efﬁciently.\nThus, it enables us to continuously update the mem-\nory encoder parameters fθ to achieve strong QA-\npair retrieval performance.\nGlobal Pre-training and Fine-Tuning In this\nstage, we ﬁrst freeze the memory encoder fθ to\ngenerate memory-key embedding for the entire\nmemory to build its index. We then incorporate the\non-device approximate search algorithm 1 to per-\nform the nearest-neighbor search over the memory\nindex to retrieve the top-K QA-pairs. Formally, we\npropose to maximize the same objective as Equa-\ntion 2 but with stop-gradient applied to p(m|X)\nterm. In this step, the model will only update the\nquery model fθ and the decoder model gθ. During\nﬁne-tuning, we follow the same recipe as the global\npre-training. Instead of feeding masked passages\nas inputs, we use questions with pseudo [MASK]\ntoken in the front as the input.\n4.3 Multihop Extension\nFor our extension model QAMAT+, since the re-\ntrieval augmentation process cannot be learned la-\ntently, i.e. the gradient propagation is blocked in\nthe concatenation step, we add additional supervi-\nsion to maximize the groundtruth retrieval proba-\nbility p(m1|X) for the ﬁrst-round retrieval m1. We\nadd such retrieval supervision objective to the orig-\ninal objective p(Y|X1), where X1 is the retrieval-\naugmented inputs as described in subsection 3.5.\n1https://github.com/google-research/language/\ntree/master/language/mentionmemory\n5 QA Experiments\n5.1 Implementation Details\nOur model is based on the T5-base or large archi-\ntecture implemented in JAX 2 and pre-trained on\n32 TPUs on Google Cloud3. During in-batch train-\ning, our query and index encoder fθ are shared\nand initialized from the T5 encoder (during global\ntraining the index encoder is ﬁxed and the query\nencoder continues to be updated). Our decoder gθ\nis similarly initialized from the T5 decoder. In to-\ntal, we construct ∼60M question-answer pairs as\nthe global memory. The memory key is the ques-\ntion tokenized by T5 sentencepiece model into 32\ntokens, and the memory value is the answer con-\ncatenated with its question tokenized into 40 tokens.\nThe memory is indexed by a pre-computed matrix\nMk ∈R|M|×d computed based on its keys (ques-\ntions). The corresponding top-K memory values\n(question+answer) will be fetched.\nDuring in-batch pre-training, we use a large\nbatch size of 512 and a learning rate of 1e-3,\nwhere each example contains a positive Q-A pair\nand 7 hard negative QA-pairs mined through\nBM25 (Robertson and Zaragoza, 2009). The in-\nbatch memory contains a total of 4096 entries, we\nset Top-k of 4 and update over all the modules. Af-\nter 100K steps of in-batch pre-training, we switch\nto global pre-training with global memory retrieval.\nWe decrease the batch size to 32 and enlarge Top-K\nto 16 for larger memory. We update only the query\nencoder and decoder for another 100K steps. Fi-\nnally, we set K to 32 to ﬁne-tune on downstream\ndatasets with a decreased learning rate of 5e-4.\n5.2 Datasets\nWe evaluate our framework on the three most\nwidely used single-hop open-domain question-\nanswering datasets and two multi-hop open-domain\nquestion-answering datasets\nNQ-Open The NaturalQuestions (Kwiatkowski\net al., 2019) dataset consists of naturally occurring\nGoogle queries and their answers. We follow Lee\net al. (2019) to keep questions that have a \"short\nanswer type\". It consists of 79168 training exam-\nples, 8757 dev examples, and 3610 test examples.\nTriviaQA The TriviaQA dataset is a collection of\ntrivia question-answer pairs that were scraped from\nthe web (Joshi et al., 2017). We use their unﬁltered\n2https://github.com/google-research/t5x\n3https://cloud.google.com/tpu/\n1602\nversion to evaluate our model consisting of 78785\ntraining, 8837 dev, and 113313 test examples.\nWebQuestions The WebQuestion dataset contains\nquestions that were sampled from Google Suggest\nAPI (Berant et al., 2013). The answers are anno-\ntated from FreeBase, the training set contains 3417\nexamples, the dev set contains 361 examples, and\nthe test set contains 2032 examples.\nHotpotQA The HotpotQA dataset contains ques-\ntions generated by human workers by reading two\npassages (Yang et al., 2018). The questions are\ndesigned to require multiple hops and include both\nbridge questions and comparison questions. The\ntraining set contains a total of 90564 examples, the\ndev-set contains 7405 examples for evaluation.\nMusique The Musique dataset contains questions\ncreated by composing multiple questions from ex-\nisting single-hop questions and was constructed to\ncontain less bias and artifacts (Trivedi et al., 2021).\nIn our experiments, we consider only the subset\nof 2-hop questions, resulting in a training set of\n14376 examples and a dev set of 1252 examples\nfor evaluation. While the dataset was originally\ndesigned as a distractor setting (given a question\nand a small number of passages, return the answer),\nwe instead consider an open-domain setting.\n5.3 Baselines\nWe compare our model with baselines from the fol-\nlowing categories. 1) CBQA large language models\n(T5 XXL), which directly outputs an answer with-\nout retrieval. 2) Entity/KG memory-augmented\nmodels that use memory attention to incorporate\nentity-level features into language models (Entities-\nas-Experts (EaE) (Févry et al., 2020), Fact-Injected\nLanguage Model (FilM) (Verga et al., 2021), Men-\ntionMemory (TOME) (de Jong et al., 2022)). 3)\nRetrieve-and-read model, which retrieves passages\nto pass to a reader model which predicts the answer.\n4) QA-retrieval models, which train a retriever to\ncollect QA-pairs from a large datastore, and then\nrerank these QA-pairs (top 50-100) with original\nquery with cross-attention. The highest-ranked an-\nswer is returned as the ﬁnal answer.\n5.4 Single-Hop Results\nOur results are summarized in Table 2 which re-\nports exact-match (EM) score.\nComparison with RePAQ Our main comparison\nis with the previous best QA-retrieval-based ap-\nproach \"RePAQ w/ rerank (XXL ALBERT)\". This\nmodel has a similar number of parameters to QA-\nMAT (Large). Without using an explicit re-ranking\nprocedure, our model performs slightly worse on\nNQ but obtains signiﬁcant gains on TriviaQA and\nWebQuestion. Especially on WebQuestion, which\nonly contains 3K training examples, RePAQ per-\nforms signiﬁcantly worse than the other datasets be-\ncause it requires a high volume of examples to up-\ndate the retriever from scratch. With our proposed\npre-training strategy, QAMAT can initialize from a\nmuch better checkpoint to decrease the sample com-\nplexity, yielding an absolute 6% EM improvement.\nAdditionally, without any ﬁne-tuning, we demon-\nstrate that our model already achieves promising\nresults across these datasets, nearly matching the\nperformance of \"RePAQ w/o rerank\"4.\nComparison with retrieve-and-read models In\ncomparison to this class of model, QAMAT roughly\nmatches the performance of RAG, though it still\nlags behind the SoTA model FiD. However, FiD\nrequires reading 100 passages, i.e. 20K tokens\nwhile our best model works more efﬁciently by\nonly reading top-32 QA-pairs, i.e. 1.2K tokens.\nTo investigate the speed difference between these\napproaches, we compared their inference speeds\nusing the same hardware (32 Google Cloud v3\nTPUs). We found that QAMAT can answer 240\nQs/sec, while FiD only answers 50 Qs/sec, a 5x\ninference time speedup over FiD.\n5.5 Multi-hop Results\nSince the document corpora source of HotpotQA\nand Musique are different from single-hop QA\ndatasets, we adopt question generation model\ntrained on SQuAD 2.0 (Rajpurkar et al., 2018) to\ngenerate questions for these two datasets. To create\nthe document corpora, we gather all of the pro-\nvided positive and negative documents, obtaining\n500K passages for HotpotQA and 85K passages\nfor Musique. We then use the trained generation\nmodels to populate 3M QA pairs for HotpotQA\nand 500K QA pairs for Musique. These QA pairs\nare then used as the memory source for QAMAT+,\nsimulating a (slightly smaller) open-domain setup.\nWhen training QAMAT+ on Musique, we initialize\nfrom HotpotQA’s in-Batch pre-trained checkpoint,\nwhich can bring 5-7% F1 improvement.\n4It’s worth noting that the question generation models are\ntrained using some of these datasets’ training data so this is\nnot truly “zero-shot” performance.\n1603\nModel (Test Set) NQ TQA WQ\nT5-3B (Roberts et al., 2020) 30.4 35.1 33.6\nT5-11B (Roberts et al., 2020) 32.6 42.3 37.2\nEaE (Févry et al., 2020) - 43.2 -\nFILM (Verga et al., 2021) - 29.1 -\nTOME-2 (de Jong et al., 2022) - 53.4 -\nDensePhrases (Lee et al., 2021) 40.9 50.7 -\nREALM (Guu et al., 2020) 40.4 55.8 40.7\nDPR (Karpukhin et al., 2020) 41.5 57.9 42.4\nRAG-Seq (Lewis et al., 2020) 44.5 56.8 45.2\nFiD (Izacard and Grave, 2021) 48.2 65.0 -\nRePAQ (Lewis et al., 2021) 41.2 38.8 29.4 †\nRePAQ+Rerank (Lewis et al., 2021) 47.6 50.7 37.6 †\nQAMAT Zero-Shot (Base) 37.9 34.1 25.9\nQAMAT Zero-Shot (Large) 39.8 40.0 25.1\nQAMAT Fine-tuned (Base) 44.5 53.2 43.0\nQAMAT Fine-tuned (Large) 45.5 54.8 43.6\nTable 2: The main experimental results on single-hop\nquestion answering datasets (NQ=NaturalQuestions,\nTQA=TriviaQA, WQ=WebQuestions), †means Best-\neffort replication using our own implementation.\nModel (Dev Set F1 Score) HPQ MusQ\nT5-3B (Roberts et al., 2020) 27.8 7.5\nT5-11B (Roberts et al., 2020) 30.2 9.0\nMDR+T5-Decoder (Xiong et al., 2020) 62.6 26.8\nRePAQ (Lewis et al., 2021)† 47.8 18.6\nQAMAT 42.0 16.7\nQAMAT+ 57.6 29.8\nTable 3: The main experimental results on MultiHop\nQA datasets with QAMAT and QAMAT+, †means\nBest-effort replication using our own implementation.\nIn Table 3, we show that QAMAT+ achieves\npromising results on both multi-hop datasets, out-\nperforming T5-CBQA and RePAQ by a large mar-\ngin. Additionally, QAMAT+ performs consider-\nably better than the single-hop QAMAT, demon-\nstrating the effectiveness of performing multi-\nround retrieval. Though QAMAT+ still lags behind\nthe document-based model (MDR+T5 Decoder) on\nHotpotQA, it surpasses it on the more challenging\nMusique dataset. These encouraging results sug-\ngest the potential for QAMAT+ to perform compo-\nsitional reasoning over multiple QA-pairs, which\ngreatly increases the coverage of QA datastore to\ncover more composite factual information.\n5.6 Ablation Studies\nNumber of Retrievals To understand the proper-\nties of our model better, we ﬁrst investigate the im-\npact of the number of retrievals, K, on the model’s\nperformance. We gradually increase the Kto col-\nTop-K 1 10 20 30\nNQ-Recall@K 0.41 0.58 0.62 0.64\nTriviaQA-Recll@K 0.46 0.66 0.70 0.72\nNQ-EM@K 0.39 0.42 0.44 0.44\nTriviaQA-EM@K 0.45 0.51 0.53 0.53\nTable 4: The retrieval recall and EM score of different\nretrieval numbers on test sets.\nPre-training Stages NQ TQA WQ\nOnly In-Batch 42.1 48.2 39.7\nOnly Global 26.0 28.9 26.1\nIn-Batch →Global 44.5 53.2 43.0\nTable 5: Downstream EM performance of models when\npre-trained using in-batch, global, or both stages.\nlect the recall and ﬁnal QA performance. The re-\nsults are shown in Table 4. We observe that even\nthough retrieval recall continues to increase beyond\nK >20, the EM score saturates much earlier. Fu-\nture research could improve performance further\nby developing decoders to more accurately exploit\nthese larger retrievals sets.\nImportance of Two-Stage Pre-training We next\nanalyze the importance of the two-stage pre-\ntraining from section 4 by removing either the in-\nbatch or global stage. From our results shown in Ta-\nble 5, we can see that using in-batch pre-training\nalone leads to a degradation in performance when\ncompared to the two-stage approach. This is likely\nbecause the model is never exposed to the full set\nof hard negatives which will be encountered when\nperforming retrieval over the global memory. On\nthe other hand, if we directly pre-train the global-\nmemory model without any in-batch initialization,\nthe retriever performance is nearly random and the\ndecoder consequently learns to ignore the retrieval\nand simply memorize question-answer pairs.\n6 Conclusion\nIn this paper, we propose a more accurate and efﬁ-\ncient architecture to utilize QA-pairs as represen-\ntation units of knowledge. Our proposed model\nQAMAT outperforms RePAQ signiﬁcantly, while\nleveraging our less expensive training procedure.\nFurthermore, we show how a QA-backed model\ncan perform compositional reasoning and address\nmore complex queries. In the future, we hope to fur-\nther close the gap with state-of-the-art document-\nbased retrieve-and-read models and extend this ap-\nproach to a broader set of tasks.\n1604\nLimitations\nOur approach has several limitations: 1) we use\ngenerated question-answer pairs as a knowledge\nbase, which are extracted from web documents.\nIn order to maintain high quality and faithfulness,\nthe question generation pipeline needs to be well\ntrained with a sufﬁcient amount of clean data. Such\nconditions might not hold for other domains outside\nof Wikipedia like biomedical text, thus the general\nQA-as-Knowledge-Base concept could require ad-\nditional innovations to extend to other areas. 2) Our\nlatent retrieval learning requires quasi paired data\nto learn the alignment between the query and mem-\nory. This is hard to satisfy in some domains with\nnoisier data or only a very weak alignment between\na query and the memory. 3) Our model requires\nmined intermediate retrieval signals to train QA-\nMAT+, which currently relies on lexical-overlap-\nbased heuristics. In other cases, this may not be suf-\nﬁcient and instead might require a more principled\ndesign to mine better intermediate supervision.\nEthical Statement\nOur work encourages the model to ground on the\nexisting knowledge populated from large textual\ncollections. We believe it is a reasonable towards\nbuilding more trustworthy and more robust ma-\nchine learning models. Having better attributions\nto knowledge source could help humans better un-\nderstand the model’s rationale for decision making.\nHowever, we do admit that the question genera-\ntion models used to populate the QA knowledge\nbase could potentially exacerbate the biases already\npresent in the original Wikipedia data. We will\nkeep working on this direction to minimize its po-\ntential negative impacts.\nReferences\nChris Alberti, Daniel Andor, Emily Pitler, Jacob De-\nvlin, and Michael Collins. 2019. Synthetic qa cor-\npora generation with roundtrip consistency. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6168–\n6173.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural lan-\nguage processing, pages 1533–1544.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017a. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017b. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nMichiel de Jong, Yury Zemlyanskiy, Nicholas FitzGer-\nald, Fei Sha, and William Cohen. 2022. Mention\nmemory: incorporating textual knowledge into trans-\nformers through entity mention attention. Interna-\ntional Conference on Learning Representations.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachan-\ndran, Graham Neubig, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2019. Differentiable reasoning\nover a virtual knowledge base. In International Con-\nference on Learning Representations.\nMatan Eyal, Tal Baumel, and Michael Elhadad. 2019.\nQuestion answering as an automatic evaluation met-\nric for news article summarization. In Proceedings\nof NAACL-HLT, pages 3938–3948.\nThibault Févry, Livio Baldini Soares, Nicholas Fitzger-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4937–4951.\nRuiqi Guo, Philip Sun, Erik Lindgren, Quan Geng,\nDavid Simcha, Felix Chern, and Sanjiv Kumar. 2020.\nAccelerating large-scale inference with anisotropic\nvector quantization. In International Conference on\nMachine Learning, pages 3887–3896. PMLR.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Proceed-\nings of the 37th International Conference on Ma-\nchine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 3929–3938. PMLR.\nGautier Izacard and Édouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 874–880.\n1605\nRobin Jia, Mike Lewis, and Luke Zettlemoyer. 2021.\nQuestion answering infused pre-training of general-\npurpose contextualized representations. arXiv\npreprint arXiv:2106.08190.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\nﬁcient and effective passage search via contextual-\nized late interaction over bert. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval ,\npages 39–48.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021. Learning dense representations of\nphrases at scale. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6634–6647.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6086–6096.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\ntus Stenetorp, and Sebastian Riedel. 2021. Paq: 65\nmillion probably-asked questions and what you can\ndo with them. Transactions of the Association for\nComputational Linguistics, 9:1098–1115.\nSewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi\nChen, Eunsol Choi, Michael Collins, Kelvin Guu,\nHannaneh Hajishirzi, Kenton Lee, Jennimaria Palo-\nmaki, et al. 2021. Neurips 2020 efﬁcientqa com-\npetition: Systems, analyses and lessons learned.\nIn NeurIPS 2020 Competition and Demonstration\nTrack, pages 86–111. PMLR.\nRodrigo Nogueira, Wei Yang, Jimmy Lin, and\nKyunghyun Cho. 2019. Document expansion by\nquery prediction. arXiv preprint arXiv:1904.08375.\nBarlas O ˘guz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Wen-tau Yih, Sonal\nGupta, et al. 2021. Domain-matched pre-\ntraining tasks for dense retrieval. arXiv preprint\narXiv:2107.13602.\nLiangming Pan, Wenhu Chen, Wenhan Xiong, Min-\nYen Kan, and William Yang Wang. 2021. Unsuper-\nvised multi-hop question answering by question gen-\neration. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5866–5880.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\n1606\nStephen Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Now Publishers Inc.\nDevendra Singh, Siva Reddy, Will Hamilton, Chris\nDyer, and Dani Yogatama. 2021. End-to-end train-\ning of multi-document reader and retriever for open-\ndomain question answering. Advances in Neural In-\nformation Processing Systems, 34.\nHaitian Sun, Pat Verga, Bhuwan Dhingra, Ruslan\nSalakhutdinov, and William W Cohen. 2021. Rea-\nsoning over virtual knowledge bases with open pred-\nicate relations. In International Conference on Ma-\nchine Learning, pages 9966–9977. PMLR.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641–651.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogeneous benchmark for zero-shot evalua-\ntion of information retrieval models. In Thirty-ﬁfth\nConference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track (Round 2).\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2021. Musique: Multi-\nhop questions via single-hop question composition.\narXiv preprint arXiv:2108.00573.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam Weston Cohen. 2021. Adaptable and inter-\npretable neural memory over symbolic knowledge.\nIn Proceedings of NAACL-HLT, pages 3678–3691.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5008–5020.\nJinfeng Xiao, Lidan Wang, Franck Dernoncourt, Trung\nBui, Tong Sun, and Jiawei Han. 2021. Open-domain\nquestion answering with pre-constructed question\nspaces. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 61–67.\nWenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick\nLewis, William Yang Wang, Yashar Mehdad, Scott\nYih, Sebastian Riedel, Douwe Kiela, et al. 2020. An-\nswering complex open-domain questions with multi-\nhop dense retrieval. In International Conference on\nLearning Representations.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380.\nWen-tau Yih, Matthew Richardson, Christopher Meek,\nMing-Wei Chang, and Jina Suh. 2016. The value of\nsemantic parse labeling for knowledge base question\nanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 201–206.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNational CCF Conference on Natural Language\nProcessing and Chinese Computing, pages 662–671.\nSpringer.\n1607\nA Question Answer Pairs as Knowledge\nBase\nWe can see QA-pairs as a virtual knowledge graph,\nwhere the question template deﬁnes the relation,\nthe topic entity in the question deﬁnes the head\nentity node, and the answer denotes the tail entity.\nA typical example is given in Figure 5, such com-\npositionality makes the QA-pair more controllable\nand easy to reason over than documents.\nFigure 5: QA pairs can be seen as virtual knowledge\nbase, where the question can represent complex rela-\ntions connecting subject and answer.\nB Question Generation\nHere, we use existing SQuAD datasets’ <Q, A,\nDocument> triples (Rajpurkar et al., 2016) to train\nanswer extraction, question generation model.\nAnswer Extraction Speciﬁcally, our answer ex-\ntraction model takes a document as the input and\ntrains an encoder-decoder model to generate a po-\ntential answer. We use beam search over the trained\nmodel to ﬁnd the highest-likely answers in the\ngiven document. In our experiment, the answer ex-\ntraction model is trained with the SQuAD dataset,\nwhere the document is given as the input, and the\nanswer spans are the prediction targets.\nQuestion Generation For the question genera-\ntion model, we take the SQuAD dataset and use\ndocument + extracted answer as the input to gen-\nerate questions as the outputs. This step is also ac-\ncomplished by an encoder-decoder model. which is\nmainly purposed for reading comprehension prob-\nlems, where the annotated questions are highly cor-\nrelated with the document containing very few hal-\nlucinations. However, the questions in SQuAD (Ra-\njpurkar et al., 2016) could be contextualized or am-\nbiguous, which could lead to ambiguity problems\nto hurt the retrieval performance. Therefore, we\nadd question ﬁltering to select the most accurate\nQA pairs.\nQuestion Filtering For the question ﬁltering\nmodel, we take the document + generated question\nto generate an answer. We compare the predicted\nanswer vs. the original answer to see if they match\neach other. If not, the QA-pair will be ﬁltered based\non such inconsistency. We use a reading compre-\nhension model trained with SQuAD to predict the\nanswer. The predicted answer based on the docu-\nment will match with the original QA-pair to decide\nits consistency. Such an option runs much faster,\nproviding much higher recall but lower precision\ncompared to the open-domain FiD ﬁltering used\nin (Lewis et al., 2021).\nWe visualize our question generation pipeline\nmentioned above in Figure 6.\nFigure 6: Question generation pipeline: Answers are\nextracted from passages and then questions are gener-\nated conditioned on that contextualized answer. This\nprocedure is used to generate both our model’s QA\nmemory and our pre-training data.\nC Ablation Study\nWe experiment with two variants of memory to see\ntheir performance difference.\nC.1 PAQ memory\nThe ﬁrst version is the standard PAQ corpus (Lewis\net al., 2021) containing 65M QA pairs, where\nthese QA-pairs are generated by models trained on\nNQ (Kwiatkowski et al., 2019) and ﬁltered through\nFiD model (Izacard and Grave, 2021) also trained\non NQ (Kwiatkowski et al., 2019). This memory\nis highly precise due to ODQA-ﬁltering process,\nhowever, it only covers information from 9M out of\nthe 20M passage blocks used in DPR (Karpukhin\net al., 2020).\n1608\nOur memory contains 30M PAQ corpus being\nde-duplicated, i.e. only one question corresponds\nto an answer span. We generate 30M additional\nQA-pairs based on the left-out 10M documents\nfrom PAQ (Lewis et al., 2021) and add these com-\nplementary QA-pairs to form our 60M memory to\nincrease the coverage. However, since our ﬁltering\nprocedure is based on reading comprehension, the\nprecision of QA-pairs is lower than the original\nPAQ memory.\nMemory NQ TriviaQA WebQuestions\nPAQ 65M 44.7 48.0 39.4\nOurs 60M 44.5 53.2 43.0\nTable 6: Impact of different memory over the down-\nstream QA dataset performance.\nAs can be seen, from Table 6, using the\nmost precise but low-coverage PAQ memory from\nPAQ (Lewis et al., 2021) yields the worse results\non TriviaQA and WebQuestions. After adding an\nadditional 30M PAQs to the memory generated by\nour pipeline, we are able to achieve 4-5% improve-\nments on these two datasets while still maintaining\nNQ’s performance.\nC.2 Size of Pre-training Corpus\nNext, we investigate the impact of the size of the\npre-training corpus. As a baseline, we repurpose\nthe aligned query-passage corpus used to train\nDPR (Karpukhin et al., 2020) which we adapt to\nour setting by simply reversing the pairs (120K pas-\nsage -> question retrieval). Additionally, we vary\nthe size of generated pre-training corpus (from 1M\nto 20M instances) to see its impact on the model’s\nﬁnal downstream performance. From Table 7, we\ncan see that the smaller-sized pre-training corpus\ncan drastically reduce the model’s performance,\nwith up to a 5% drop seen on TriviaQA.\nPre-train Examples NQ TQA WQ\n120K 42.5 48.2 39.7\n1M 42.8 48.8 40.2\n5M 43.8 51.5 41.7\n10M 44.3 52.1 42.5\n20M 44.5 53.2 43.0\nTable 7: Impact of pre-training corpus size on ﬁnal\ndownstream EM performance. The upper portion is\npre-trained using the DPR-reverse corpus described in\nsubsection 5.6 and the lower portion uses subsets of our\ngenerated pre-training corpus (subsection 4.1)\nFigure 7: The impact of memory size on downstream\nQA EM performance.\nSize of Memory Finally, we look at how big of\nmemory we need to reach optimal downstream ac-\ncuracy and how the model behaves with a smaller\nmemory. As is shown in Figure 7, having a small\nmemory of less than 5M entries does not improve\nover a model with no memory at all. Due to the\nlack of coverage, the model does not receive a use-\nful signal from the retrieval and is subsequently\nnot incentivized to utilize those retrievals when\nmaking a prediction. However, once the size of\nthe memory increases beyond 15M we observe a\nsteep increase in the ﬁnal performance, indicating\nthat the model is gradually learning to incorporate\nretrieved information retrievals to assist prediction.\nD MultiHop QA Training\nIn order to train the multi-hop QA model, we need\nto have intermediate supervision for the query aug-\nmentation process. Here we use a string-based\nmatch to derive what are the most possible interme-\ndiate questions from a collection of pre-generated\nQA pairs. We depict the mining process as Fig-\nure 8.\n1609\nFigure 8: We ﬁrst ﬁnd the ﬁnal question based on answer string matching with the pre-generated question, and\nthen base on that to trace back the intermediate question.\n1610",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8959896564483643
    },
    {
      "name": "Computer science",
      "score": 0.860713005065918
    },
    {
      "name": "sort",
      "score": 0.6793332695960999
    },
    {
      "name": "Task (project management)",
      "score": 0.6367765069007874
    },
    {
      "name": "Open domain",
      "score": 0.6302512288093567
    },
    {
      "name": "Information retrieval",
      "score": 0.5662668347358704
    },
    {
      "name": "Language model",
      "score": 0.5656774044036865
    },
    {
      "name": "Natural language processing",
      "score": 0.5636751651763916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.494610995054245
    },
    {
      "name": "Knowledge base",
      "score": 0.48020848631858826
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4421081244945526
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 12
}