{
  "title": "MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter",
  "url": "https://openalex.org/W4389523890",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2224355161",
      "name": "Sihang Li",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5006760821",
      "name": "Yanchen Luo",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2118058532",
      "name": "Hao Fei",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2110093419",
      "name": "Yixin Cao",
      "affiliations": [
        "Singapore Management University"
      ]
    },
    {
      "id": "https://openalex.org/A2001534452",
      "name": "Kenji Kawaguchi",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2099081116",
      "name": "Xiang Wang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2730258684",
      "name": "Tat-Seng Chua",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095883070",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4247259022",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3095602948",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W4280534475",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4300978570",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W3005552578",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W1984969847",
    "https://openalex.org/W4323572088",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4376312115",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4286906902",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4212837331",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W3211951295",
    "https://openalex.org/W4295846611",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W4318718899",
    "https://openalex.org/W4389888290",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W568917201",
    "https://openalex.org/W3212456749",
    "https://openalex.org/W3097145107",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception — a critical ability of human professionals in comprehending molecules’ topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (i.e., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder’s representation space and an LM’s text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM’s efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM’s ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines.",
  "full_text": "MolCA: Molecular Graph-Language Modeling\nwith Cross-Modal Projector and Uni-Modal Adapter\nZhiyuan Liu† Sihang Li‡ Yanchen Luo‡ Hao Fei†\nYixin Cao§ Kenji Kawaguchi† Xiang Wang‡∗ Tat-Seng Chua†\n†National University of Singapore, ‡University of Science and Technology of China\n§Singapore Management University\n{acharkq,sihang0520,luoyc0830,caoyixin2011,xiangwang1223}@gmail.com\nhaofei37@nus.edu.sg, {kenji,chuats}@comp.nus.edu.sg\nAbstract\nLanguage Models (LMs) have demonstrated\nimpressive molecule understanding ability on\nvarious 1D text-related tasks. However, they\ninherently lack 2D graph perception — a crit-\nical ability of human professionals in compre-\nhending molecules’ topological structures. To\nbridge this gap, we proposeMolCA: Molecular\nGraph-Language Modeling with Cross-Modal\nProjector and Uni-Modal Adapter. MolCA en-\nables an LM (i.e., Galactica) to understand both\ntext- and graph-based molecular contents via\nthe cross-modal projector. Specifically, the\ncross-modal projector is implemented as a Q-\nFormer to connect a graph encoder’s repre-\nsentation space and an LM’s text space. Fur-\nther, MolCA employs a uni-modal adapter (i.e.,\nLoRA) for the LM’s efficient adaptation to\ndownstream tasks. Unlike previous studies\nthat couple an LM with a graph encoder via\ncross-modal contrastive learning, MolCA re-\ntains the LM’s ability of open-ended text gen-\neration and augments it with 2D graph infor-\nmation. To showcase its effectiveness, we\nextensively benchmark MolCA on tasks of\nmolecule captioning, IUPAC name prediction,\nand molecule-text retrieval, on which MolCA\nsignificantly outperforms the baselines. Our\ncodes and checkpoints can be found at https:\n//github.com/acharkq/MolCA.\n1 Introduction\nLanguage Models (LMs) have demonstrated signif-\nicant achievements across various domains (Devlin\net al., 2019; Zhao et al., 2023). Notably, the wealth\nof biochemical literature in LMs’ pretraining data\nhas enabled LMs to obtain a high-level understand-\ning of biochemical concepts and molecule prop-\nerties. This can be reflected by their promising\nperformances in biochemical and medical question-\nanswering benchmarks (Taylor et al., 2022; Ope-\n∗ Corresponding author. Xiang Wang is also affiliated\nwith Institute of Artificial Intelligence, Institute of Dataspace,\nHefei Comprehensive National Science Center.\nnAI, 2023). Therefore, it becomes increasingly ur-\ngent to incorporate these LMs to augment research\nin chemistry and biology.\nFor this purpose, we aim to utilize LMs for\nmolecule understanding. As shown in Figure 1a,\nmost existing LMs (Touvron et al., 2023; Zhang\net al., 2022; Zeng et al., 2022) represent molecules\nby their 1D Simplified Molecular Input Line Entry\nSystem (SMILES) strings (Weininger, 1988) and\nprocess them in a manner similar to texts. While\nconvenient, treating molecules as strings overlooks\nthe molecules’ 2D graph representations, which are\ncrucial to human professionals in comprehending\nthe molecule structures (Wells, 2012). To com-\nbat that, recent works (Su et al., 2022; Liu et al.,\n2022b) represent molecules as graphs and use a\nGraph Neural Network (GNN; Xu et al., 2019) as\nthe molecular graph encoder. The graph encoder\nis trained jointly with an LM through cross-modal\ncontrastive learning (Radford et al., 2021; Li et al.,\n2022), as illustrated in Figure 1b. However, the ap-\nplication scope of cross-modal contrastive learning\nis limited (Alayrac et al., 2022): it is suitable for\nretrieval tasks, but is insufficient for open-ended\nmolecule-to-text generation tasks, such as molecule\ncaptioning (Edwards et al., 2022) and molecule’s\nIUPAC name prediction (Taylor et al., 2022). This\nis because molecule-to-text generation is a condi-\ntional generation task (Keskar et al., 2019; Raffel\net al., 2020). It requires the LM to understand 2D\ngraphs as the generation conditions, which con-\ntrastive learning cannot achieve. Su et al. (2022)\nattempt to directly input 2D graphs’ representations\ninto LMs, however showing limited improvement.\nTo bridge this gap, we deviseMolCA: Molecular\nGraph-Language Modeling with Cross-Modal Pro-\njector and Uni-Modal Adapter. MolCA enables the\nLM to understand 2D graphs as inputs, therefore\neffectively conditioning the molecule-to-text gener-\nLanguage Model\nInput\nGenerate\nLanguage \nModel\nGraph \nEncoder\nContrast\nGraph \nEncoder\nCross-Modal \nProjector\nCC(=O)OC1=CC=CC=C1C(=O)O \nAcetylsalicylic acid appears as \nodorless white crystals …\nAcetylsalicylic \nacid appears as \nodorless white …\nAcetylsalicylic acid appears as \nodorless white crystals …\nCC(=O)OC1=CC\n=CC=C1C(=O)O \nGenerate\nInput\n(a) 1D language modeling. (b) Cross-modal contrastive learning. (c) MolCA.\nLanguage Model\nUni-Modal \nAdapter\nFigure 1: Comparison of molecular language modeling methods.\nLanguage \nModel\nGraph \nEncoder\nCross-Modal \nProjector\nModel Architecture Downstream Task\nCross-modal retrieval\nGraph \nEncoder\nCross-Modal \nProjector\n -\nGraph \nEncoder\nCross-Modal \nProjector\nGeneration tasks: \nMolecule caption, \nIUPAC name prediction\nStage\nPretrain stage 1 \nPretrain stage 2 \nFine-tune stage Language \nModel\nFigure 2: MolCA’s three-stage training pipeline.\nation process. To enable the LM to understand 2D\ngraphs, we identify that the key challenge is cross-\nmodal alignment (Li et al., 2023; Merullo et al.,\n2023; Alayrac et al., 2022): translating the repre-\nsentations of 2D graphs into 1D soft prompts (Li\nand Liang, 2021) in the text space so that the LM\ncan understand. This translation is facilitated by the\ncross-modal projector, bridging the gap between\nthe graph encoder’s representation space and the\nLM’s input space, as illustrated in Figure 1. Specif-\nically, we implement the cross-modal projector as\na Q-Former (Li et al., 2023) due to its effective-\nness in vision-language tasks. With an effective\ncross-modal projector, we can harness the power\nof existing large LMs (Taylor et al., 2022; Tou-\nvron et al., 2023) for molecule-to-text generation.\nHowever, given a large LM with billion scale pa-\nrameters, its efficiency of downstream fine-tuning\narises as a new problem. Therefore, we integrate\nthe LM with a uni-modal adapter, i.e., LoRA (Hu\net al., 2022), to enable its efficient adaptation.\nAs Figure 2 illustrates, MolCA uses a three-stage\ntraining pipeline to integrate its components. The\ntwo pretrain stages aim to develop the cross-modal\nalignment ability of the cross-modal projector. In\npretrain stage 1, the projector and the encoder are\ntrained to extract the molecule features that are\nthe most relevant to the text. This stage endows\nthe resulting model with powerful molecule-text\nretrieval ability. In pretrain stage 2, the cross-modal\nprojector is connected to a frozen LM and trained\nfor molecule captioning. This task forces the cross-\nmodal projector to produce soft prompts that the\nLM can understand. In the final stage, MolCA is\nfine-tuned for downstream generation tasks.\nOur contributions can be summarized as follows:\n• We propose MolCA, a pioneering method for\nmolecular language modeling. MolCA enables\nan LM to perceive 2D molecular graphs, thereby\nfacilitating molecule-to-text generation tasks.\n• MolCA sets new state-of-the-arts in a variety\nof benchmarks. It surpasses the baselines by\n2.1 and 7.6 BLEU-2 for molecule captioning on\nCheBI-20 (Edwards et al., 2022) and our curated\nPubChem324k dataset, respectively. Moreover,\nin predicting IUPAC names, MolCA shows a\nsignificant advantage of 10.0 BLEU-2 over the\nbaselines. For molecule-text retrieval, MolCA\noutperforms the baselines by 20% retrieval ac-\ncuracy in PubChem324k and achieves the best\nperformances in PCDes (Zeng et al., 2022) and\nMoMu datasets (Su et al., 2022).\n• We conduct ablation studies to show MolCA’s\neffectiveness of incorporating 2D graphs into\nLMs for molecule-related tasks. Additionally,\nour quantitative analysis shows that incorporat-\ning 2D graphs helps improve the LM’s ability to\ncount functional groups inside molecules.\n2 Model Architecture\nHere we introduce three key components of\nMolCA’s architecture: 1) a graph encoder for 2D\nstructure understanding, 2) an LM for text gener-\nation, and 3) a cross-modal projector to connect\nthe graph encoder and the LM. We describe the\nuni-modal adapter in Section 3.3.\nGraph Encoder. Given the rich structural pat-\nterns in molecules, we leverage a GNN-based en-\ncoder to encode molecular graphs. Specifically,\nwe employ a five-layer GINE (Hu et al., 2020)\nthat is pretrained on 2 million molecules from the\nZINC15 (Sterling and Irwin, 2015) dataset by con-\ntrastive learning (You et al., 2020). Given a molec-\nular graph g, the graph encoder f can generate\nGraph \nEncoder\nSelf-attention\nCross-attention\nFeed-forward\nSelf-attention\nFeed-forward\nMolecule-text \ncontrasting\nSelf-attention\nCross-attention\nFeed-forward Feed-forward\nCross-attention\nFeed-forward Feed-forward\nMolecule-text \nmatching\nMolecule \ncaptioning\nNx\n[CLS] Acetylsalicylic \nacid appears as \nodorless white …\nNx Nx\nCross-Modal Projector: \nQ-Former\nQuery \ntokens\nInput \n1D text\nSelf-attention Causal self-att\nInput 2D \nmolecule\nFigure 3: MolCA’s pretrain stage 1. The graph encoder and the cross-modal projector (i.e., Q-Former) are jointly\noptimized using three cross-modal tasks. Modules of the same color share weights.\nstructure-aware features for every node of g:\nf(g) =Z ∈ R|g|×d, (1)\nwhere |g| denotes the number of nodes in g.\nLanguage Model. To achieve effective text gen-\neration performance, we employ Galactica (Tay-\nlor et al., 2022) as the base LM. Galactica is pre-\ntrained on a large collection of scientific litera-\nture, which encompasses fields like chemistry, bi-\nology, and medicine. Its promising performance\nin text-based science question-answering bench-\nmarks (Hendrycks et al., 2021; Jin et al., 2019) un-\nderscores its understanding of high-level biochem-\nical concepts. Notably, Galactica can process 1D\nSMILES of molecules, which can potentially ben-\nefit our downstream tasks. Galactica is a decoder-\nonly transformer LM based on the OPT (Zhang\net al., 2022) architecture.\nCross-Modal Projector. We implement the\ncross-modal projector as a Querying-Transformer\n(Q-Former) (Li et al., 2023) to map the graph en-\ncoder’s outputs to the LM’s input text space. As\nshown in Figure 3, Q-former has different pro-\ncedures for processing 2D molecular graphs and\n1D texts. Given text inputs, Q-Former inserts\n[CLS] tokens at the beginning and processes the\ntexts by N layers of self-attention modules and\nfeed-forward networks. The self-attention modules\nadopt causal masks (Raffel et al., 2020) when the\npretraining task is text generation. On the other\nhand, given a molecular graph g, Q-Former works\nas a molecule feature extractor. Specifically, it\nmaintains a set of learnable query tokens {qk}Nq\nk=1\nas inputs. These query tokens can interact with\nthe graph encoder’s output Z through the cross-\nattention modules (Vaswani et al., 2017) and extract\nmolecule features. The cross-attention modules are\nadded every two layers. Additionally, the query\ntokens can interact with the text inputs through the\nsame self-attention modules. Note that, the query\ntokens and text inputs are processed by different\nfeed-forward networks, in order to maintain capac-\nities for processing molecules and texts.\nWe initialize Q-Former from Sci-BERT (Belt-\nagy et al., 2019), an encoder-only transformer\npretrained on scientific publications. Q-Former’s\ncross-attention modules are randomly initialized.\n3 Training Pipeline\nThis section delves into the details of MolCA’s\nthree-stage training pipeline ( cf. Figure 2). The\ntwo pretrain stages leverage a dataset of molecule-\ntext pairs D = {(g1, y1), (g2, y2), ...} to train the\ncross-modal projector and the graph encoder. The\ngoal of pretraining is to translate 2D molecular\ngraphs into soft prompts that a frozen LM can un-\nderstand. The fine-tune stage focuses on efficient\nadaptation to downstream generation tasks.\n3.1 Pretrain Stage 1: Learning to Extract\nText Relevant Molecule Representations\nIn this stage, we aim to optimize the cross-modal\nprojector (i.e., Q-Former) to extract the molecule\nfeatures most relevant to the text input. This stage\nserves as a “warmup” training for the cross-modal\nprojector before connecting to the LM. Inspired\nby BLIP2 (Li et al., 2023), we simultaneously ap-\nply three cross-modal pretraining tasks that are\ntailored for Q-Former’s architecture: molecule-text\ncontrasting, molecule-text matching, and molecule\ncaptioning. These pretraining tasks endow the Q-\nFormer with a strong molecule-text retrieval ability.\nTherefore, we save the resulting model from this\nstage for downstream retrieval tasks. We now elab-\norate on the three pretraining tasks.\nMolecule-Text Contrasting (MTC). We apply\ncross-modal contrastive learning (Radford et al.,\n2021) to train the Q-Former to extract text-revelant\nmolecule features. In this task, query tokens and\ntext inputs are fed into the Q-Former separately\n(left of Figure 3) to obtain Q-Former’s molecule\nrepresentations and text representations.\nFormally, let {(g1, y1), ...,(gB, yB)} be a batch\nof molecule-text pairs. We denote gi’s Q-Former\nrepresentations as {mik}Nq\nk=1 (each element for one\nquery token), and denote yi’s Q-Former representa-\ntion as ti (representation of the [CLS] token). For\narbitrary i, j∈ [1, B], we measure the similarity\nbetween ti and {mjk}Nq\nk=1 by computing the max-\nimum similarity between ti and every element in\n{mjk}Nq\nk=1. The MTC loss ℓMTC can be written as:\nℓg2t =\nBX\ni=1\nlog exp(maxk cos(mik, ti)/τ)PB\nj=1 exp(maxk cos(mik, tj)/τ)\n,\nℓt2g =\nBX\ni=1\nlog exp(maxk cos(ti, mik)/τ)PB\nj=1 exp(maxk cos(ti, mjk)/τ)\n,\nℓMTC = − 1\nB ℓg2t − 1\nB ℓt2g, (2)\nwhere cos(·, ·)/τ is the temperature-scaled cosine\nsimilarity. Temperature τ is empirically set to 0.1.\nMolecule-Text Matching (MTM).MTM is a bi-\nnary classification task, aiming to predict whether\na molecule-text pair is matched (positive) or un-\nmatched (negative). As Figure 3 illustrates, MTM\nallows the queries and the texts to interact through\nthe same self-attention module. In this way, the\nqueries can extract multi-modal information from\nboth molecules and texts. For MTM prediction, we\nattach a linear classifier after the mean pooling of\nall queries’ Q-Former representations. Let ρ(g, y)\ndenotes MTM’s predicted probability that (g, y) is\nmatched. MTM loss ℓMTM can be written as:\nℓMTM = 1\nB Ej,k∼U(1,B)\n\u0002 BX\ni=1\n−log ρ(gi, yi)+\nlog ρ(gi, yj) + logρ(gk, yi)\n\u0003\n, (3)\nwhere U(1, B) is a uniform distribution; yj and gk\nare random negative samples in batch.\nSimilar to MTC, MTM also computes the simi-\nlarity between molecule-text pairs. The difference\nis that MTM can capture more fine-grained simi-\nlarity between a molecule and a text through the\nself-attention and cross-attention modules, com-\npared to the simple cosine similarity used by MTC.\nTherefore, in retrieval experiments, we use MTC\nto first retrieve the top k samples and use MTM for\nre-ranking, thereby improving the performance.\nMolecule Captioning (MCap). MCap aims to\ngenerate the molecule’s text description based on\nthe molecule representations. For this task, we\nadopt a special masking strategy in self-attention\nmodules to ensure that the queries learn to ex-\ntract molecule features that correspond to the\ntext descriptions. Specifically, we employ the bi-\ndirectional self-attention masks for queries, allow-\ning them to see each other but not the text tokens.\nFurther, we apply causal masks for texts on the\nsame self-attention module to perform autoregres-\nsive decoding of text descriptions. Each text token\ncan see the queries and the preceding text, but not\nthe subsequent text tokens. Since the text tokens\ncannot directly interact with the graph encoder, they\nmust obtain molecule information from the queries,\nforcing the queries to extract molecule information\nthrough the cross-attention modules. Let p1(y|g)\nbe the probability of Q-Former generating text y\nfor a graph g. We use the following loss function:\nℓMCap = − 1\nB\nBX\ni=1\nlog p1(yi|gi) (4)\n3.2 Pretrain Stage 2: Aligning 2D Molecular\nGraphs to Texts via Language Modeling\nIn this stage, we aim to align the cross-modal pro-\njector’s outputs to the text space of a frozen LM.\nAs Figure 4 illustrates, we feed the cross-modal\nprojector’s representations of 2D molecular graphs\nto the frozen LM as inputs, and train the model to\ngenerate molecules’ text descriptions. This process\nencourages the cross-modal projector to provide\nrepresentations that the LM can understand, so as\nto prompt the text generation. Additionally, we\nalso use a molecule’s 1D SMILES to guide the\ngeneration (cf. Figure 4). This is because most\nLMs (Taylor et al., 2022; Touvron et al., 2023;\nZhang et al., 2022) use SMILES during pretraining.\nTherefore, these LMs have established some cor-\nrelations between SMILES and their text contexts.\nThus, including SMILES can potentially prompt\nthe corresponding biochemical knowledge. On the\nother hand, incorporating 2D graphs can help cap-\nture structural patterns that are hard to learn from\n1D SMILES. We will show later in experiments\nthat combining 2D graphs and 1D SMILES can\nboost performance.\nGraph \nEncoder\nCross-Modal \nProjector\nLanguage Model\nAcetylsalicylic acid appears as \nodorless white crystals …\nCC(=O)OC1=CC\n=CC=C1C(=O)O \nGenerate\nInput\nFigure 4: MolCA’s pretrain stage 2\nby molecule captioning.\nGraph \nEncoder\nCross-Modal \nProjector\nLanguage Model\n2-acetyloxybenzoic acid.\nCC(=O)OC1=CC\n=CC=C1C(=O)O \nGenerate\nInput\nThe molecule’s \nIUPAC name is\nPretrained \nweights\nUni-Modal Adapter: \nLoRA\n<latexit sha1_base64=\"+hFOCxuHkfmzQGLq/EkihlUwmIs=\">AAACEXicjVDLSsNAFJ3UV62vqEs3g0XoqjTF17LoxmUV+4Cmhslk0g6dTMLMjVBCfsGNv+LGhSJu3bnzb5w+FioKHhg495x7uXeOnwiuoVb7sAoLi0vLK8XV0tr6xuaWvb3T1nGqKGvRWMSq6xPNBJesBRwE6yaKkcgXrOOPzid+55YpzWN5DeOE9SMykDzklICRPLviRgSGfph1cpdLPKv87Cq/yQLPcYFHTOPAq+eeXXaqtSnw36SM5mh69rsbxDSNmAQqiNY9p5ZAPyMKOBUsL7mpZgmhIzJgPUMlMZv62fRHOT4wSoDDWJknAU/VrxMZibQeR77pnFysf3oT8Tevl0J42s+4TFJgks4WhanAEONJPDjgilEQY0MIVdzciumQKELBhFj6XwjtetU5rh5dHpYbZ/M4imgP7aMKctAJaqAL1EQtRNEdekBP6Nm6tx6tF+t11lqw5jO76Bust0/aiJ2p</latexit>\nW 2 R d 1 ⇥ d 2\n<latexit sha1_base64=\"fb8b+Nru003hdZ7BXNbY7Hz47Y8=\">AAACD3icjVC7TsMwFL0pr1JeAUYWiwrEVDWI11iVhbEg+pCaUDmu01p1nMh2kKoof8DCr7AwgBArKxt/g9N2AAQSR7J07jn36l4fP+ZM6Wr1wyrMzS8sLhWXSyura+sb9uZWS0WJJLRJIh7Jjo8V5UzQpmaa004sKQ59Ttv+6Dz327dUKhaJaz2OqRfigWABI1gbqWfvuyHWQz9I65nLBJpWfnqV3aT9nuNqFlKFZNazy06lOgH6m5RhhkbPfnf7EUlCKjThWKmuU421l2KpGeE0K7mJojEmIzygXUMFNnu8dPKfDO0ZpY+CSJonNJqoXydSHCo1Dn3Tmd+rfnq5+JvXTXRw5qVMxImmgkwXBQlHOkJ5OKjPJCWajw3BRDJzKyJDLDHRJsLS/0JoHVack8rx5VG5Vp/FUYQd2IUDcOAUanABDWgCgTt4gCd4tu6tR+vFep22FqzZzDZ8g/X2CYmUnP0=</latexit>\nB 2 R d 1 ⇥ r\n<latexit sha1_base64=\"bLdpZu6WqaMamlmeWegyA7G0y9I=\">AAACD3icjVC7TsMwFHV4lvIqMLJYVCCmKq14jQUWxoLoQ2pC5LhOa9VxIvsGqYryByz8CgsDCLGysvE3OG0HQCBxJEvnnnOv7vXxY8E12PaHNTM7N7+wWFgqLq+srq2XNjZbOkoUZU0aiUh1fKKZ4JI1gYNgnVgxEvqCtf3hee63b5nSPJLXMIqZG5K+5AGnBIzklfackMDAD9LTzOESTyo/vcpuUuUAD5nGPa+WeaVytWKPgf8mZTRFwyu9O72IJiGTQAXRulu1Y3BTooBTwbKik2gWEzokfdY1VBKzyU3H/8nwrlF6OIiUeRLwWP06kZJQ61Hom878Xv3Ty8XfvG4CwYmbchknwCSdLAoSgSHCeTi4xxWjIEaGEKq4uRXTAVGEgomw+L8QWrVK9ahyeHlQrp9N4yigbbSD9lEVHaM6ukAN1EQU3aEH9ISerXvr0XqxXietM9Z0Zgt9g/X2CYuInP0=</latexit>\nA 2 R r ⇥ d 2\nFigure 5: MolCA’s fine-tune stage for molecule-to-text generation.\nThe example shows the prediction of a molecule’s IUPAC name.\nFormally, consider a molecule-text pair (g, y)\nand g’s SMILES repsentation s, The cross-modal\nprojector representations of g are denoted as\n{mk}Nq\nk=1. We define p2(·) as the text distribution\nparameterized by the frozen LM. We optimize the\ncross-modal projector and the graph encoder by\nminimizing the following loss function:\n− log p2(y|{mk}Nq\nk=1, s)\n= −\nLX\nl=1\nlog p2(yl|y1, ..., yl−1, {mk}Nq\nk=1, s). (5)\n3.3 Fine-tune Stage: Uni-Modal Adapter for\nEfficient Downstream Adaptation\nIn this stage, we fine-tune MolCA for downstream\ngeneration tasks. As Figure 5 illustrates, we ap-\npend a text prompt of the task description after the\nmolecule representations. Then, we apply language\nmodeling loss to fine-tune MolCA for generation\ntasks, such as molecule’s IUPAC name prediction.\nUni-Modal Adapter. In MolCA, the LM is ac-\ncounted for a large portion of computation over-\nhead: it can have ∼1B parameters, while the cross-\nmodal projector and graph encoder only have a\ntotal of ∼0.1B parameters. Therefore, we employ a\nuni-modal adapter for the LM’s efficient adaptation\nto downstream tasks. Specifically, we employ the\nLoRA (Hu et al., 2022) adapter due to its simple\nimplementation and promising performances (Liu\net al., 2022a). As shown in Figure 5, for selected\nweight matrices ( e.g., W ∈ Rd1×d2 ) in the LM,\nLoRA adds pairs of rank decomposition matrices\n(e.g., BA, B ∈ Rd1×r, A ∈ Rr×d2 ) in parallel to\nthem. The original h = Wx layer is changed to:\nh = Wx + BAx, (6)\nwhere W is kept frozen and the newly added BA\nis trained during adaptation. Given a small r ≪\nSubset Size Avg mol len Min text len Avg text len\nPretrain 298083 35 1 16\nTrain 12000 32 20 60\nValid 1000 32 20 61\nTest 2000 31 20 60\nTable 1: Statistics of the PubChem324k dataset. We\ncount the text length by splitting the text at spaces.\nmin(d1, d2), LoRA can effectively adapt the LM\nto downstream tasks while requiring little memory\noverhead for storing gradients.\n4 Experiments\n4.1 Experimental Setting\nHere we briefly present the experimental settings.\nMore details can be found in Appendix B.\nPubChem324k Dataset. We collect PubChem-\n324k – a dataset containing 324k molecule-text\npairs from the PubChem website1. Table 1 presents\nthe dataset statistics. Notice that, the dataset in-\ncludes many uninformative texts, such as “The\nmolecule is a peptide”. Therefore, we sample a\nhigh-quality subset of 15k pairs with text longer\nthan 19 words for downstream tasks. This high-\nquality subset is further randomly divided into\nthe train/valid/test sets. The remaining dataset,\nwhich is more noisy, is used for pretraining. Ad-\nditionally, we filter our pretrain subset to exclude\nmolecules from the valid/test sets of other down-\nstream datasets, including CheBI-20 (Edwards\net al., 2022), PCDes (Zeng et al., 2022), and\nMoMu (Su et al., 2022) datasets. The dataset after\nfiltering includes totally 313k molecule-text pairs.\nBaselines. For generation tasks, we compare\nMolCA with the following baselines: T5 (Raffel\net al., 2020), MolT5 (Edwards et al., 2022), and\nMoMu (Su et al., 2022). For molecule-text retrieval,\n1https://pubchem.ncbi.nlm.nih.gov\nModel #Trainable params BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\n1D SMILES\nMolT5-Small 80M, full ft 14.8 8.5 26.5 13.5 23.6 18.5\nMolT5-Base 250M, full ft 30.1 20.9 40.3 25.1 33.8 35.6\nMolT5-Large 780M, full ft 30.2 22.2 41.5 25.9 34.8 36.6\n1D SMILES + 2D Graph\nMoMu-Small 82M, full ft 19.1 12.0 29.7 16.3 26.7 21.8\nMoMu-Base 252M, full ft 30.2 21.5 40.5 25.1 34.4 34.2\nMoMu-Large 782M, full ft 31.1 22.8 41.8 25.7 36.7 36.2\nMolCA, MolT5-Large 877M, full ft 32.9 26.3 49.8 35.7 44.2 42.4\nMolCA, Galac125M 222M, full ft 31.9 24.3 47.3 33.9 43.2 41.6\nMolCA, Galac1.3B 100M, LoRA ft* 38.7 30.3 50.2 35.9 44.5 45.6\n(a) PubChem324k dataset. Baseline performances are reproduced using their source codes (Edwards et al., 2022; Su et al., 2022).\nModel #Trainable params BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\n1D SMILES\nT5-Small 80M, full ft 50.1 41.5 60.2 44.6 54.5 53.2\nT5-Base 250M, full ft 51.1 42.3 60.7 45.1 55.0 53.9\nT5-Large 780M, full ft 55.8 46.7 63.0 47.8 56.9 58.6\nMolT5-Small 80M, full ft 51.9 43.6 62.0 46.9 56.3 55.1\nMolT5-Base 250M, full ft 54.0 45.7 63.4 48.5 57.8 56.9\nMolT5-Large 780M, full ft 59.4 50.8 65.4 51.0 59.4 61.4\n1D SMILES + 2D Graph\nMoMu-Small 82M, full ft 53.2 44.5 - - 56.4 55.7\nMoMu-Base 252M, full ft 54.9 46.2 - - 57.5 57.6\nMoMu-Large 782M, full ft 59.9 51.5 - - 59.3 59.7\nMolCA, Galac125M 222M, full ft 61.2 52.6 67.4 52.1 60.6 63.6\nMolCA, Galac1.3B 110M, LoRA ft* 62.0 53.1 68.1 53.7 61.8 65.1\n(b) CheBI-20 dataset. Baseline performances are borrowed from their original papers (Edwards et al., 2022; Su et al., 2022).\nTable 2: Performances (%) of molecule captioning on the PubChem324k and CheBI-20 datasets. Bold indicates the\nbest performance and underline indicates the second best performance. Full ft denotes full parameter fine-tuning.\n*The LoRA configurations for PubChem324k and CheBI-20 datasets are different. Details are in Appendix B.\nwe also include these methods: MoleculeSTM (Liu\net al., 2022b), KV-PLM (Zeng et al., 2022), and\nSci-BERT (Beltagy et al., 2019).\n4.2 Molecule Captioning\nWe evaluate MolCA for molecule captioning on the\ndatasets of PubChem324k and CheBI-20 (Edwards\net al., 2022). Specifically, we implement MolCA\nwith the base LMs of Galactica1.3B, Galactica125M,\nand MolT5-Large. We employ full parameter fine-\ntuning for Galactica125M and MolT5-Large due to\ntheir smaller scales. We fine-tune MolCA and base-\nlines on the dataset’s training set and report the test\nset performance selected by the valid set. Follow-\ning (Edwards et al., 2022), we adopt BLEU (Pap-\nineni et al., 2002), ROUGE (Lin, 2004), and ME-\nTEOR (Banerjee and Lavie, 2005) as the evaluation\nmetrics. As shown in Table 2, we observe that:\n1. MolCA consistently outperforms the base-\nlines by a large margin. Specifcally, MolCA,\nGalac1.3B achieves the highest performance on all\nmetrics. It outperforms the baselines by 7.6 BLEU-\n2 on PubChem324k and 2.1 BLEU-2 on CheBI-20.\n2. MolCA, Galac125M outperforms baselines\nof larger sizes across all metrics, showing that\nMolCA’s advantage is not limited to model scale.\n4.3 IUPAC Name Prediction\nThe International Union of Pure and Applied Chem-\nistry (IUPAC) has established a standardized nam-\ning system for chemical compounds, known as IU-\nPAC names (Favre and Powell, 2013). Notably,\nthis naming system relies on identifying specific\nmolecule structures, including hydrocarbon chains\nand double/triple bonds. Therefore, correctly pre-\ndicting IUPAC names indicates a model’s profi-\nciency to understand molecule structures. We\nfine-tune MolCA and baselines using the Pub-\nChem324k’s training set to generate a molecule’s\nIUPAC name. As shown in Table 3, MolCA consis-\ntently outperforms the baselines by a large margin\nof 10.0 BLEU-2, highlighting MolCA’s advantage\nin comprehending molecule structures.\n4.4 Molecule-Text Retrieval\nWe evaluate MolCA for molecule-text retrieval on\nthe datasets of PubChem324k, PCDes (Zeng et al.,\nModel #Trainable params BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\n1D SMILES\nMolT5-Small 80M, full ft 48.6 35.2 40.0 16.1 34.3 42.5\nMolT5-Base 250M, full ft 52.7 41.5 50.7 26.0 44.3 53.2\nMolT5-Large 780M, full ft 59.4 49.7 55.9 33.3 49.1 58.5\n1D SMILES + 2D Graph\nMolCA, Galac125M 222M, full ft 73.9 66.3 69.0 47.8 63.2 71.8\nMolCA, Galac1.3B 100M, LoRA ft 75.0 66.6 69.6 48.2 63.4 72.1\nTable 3: Performances (%) of predicting molecule’s IUPAC names on the PubChem324k dataset. Baseline\nperformances are obtained by running their source codes (Edwards et al., 2022).\nM2T T2M\nModel Acc R@20 Acc R@20\n1D SMILES\nSci-BERT 39.7 85.8 37.5 85.2\nKV-PLM 38.8 86.0 37.7 85.5\n2D Graph\nMoMu-S* 11.5 41.2 12.6 43.6\nMoMu-K* 11.3 41.0 12.4 39.9\nMoMu-S 40.9 86.2 40.8 86.1\nMoMu-K 41.8 87.5 41.6 87.8\nMoleculeSTM 45.8 88.4 44.3 90.3\nMolCA w/o MTM 58.3 92.3 56.0 90.6\nMolCA 66.6 94.6 66.0 93.5\n(a) Performances (%) in the PubChem324k dataset.\nPCDes dataset MoMu dataset\nModel M2T T2M M2T T2M\n1D SMILES\nSci-BERT† 60.7 60.8 0.3 0.3\nKV-PLM† 75.9 64.3 0.5 0.3\n2D Graph\nMoMu-S† 79.1 75.5 43.3 43.4\nMoMu-K† 80.2 79.0 43.7 43.5\nMoleculeSTM 80.4 77.0 70.5 66.9\nMolCA w/o MTM 80.6 76.5 68.5 64.8\nMolCA 85.6 82.3 76.8 73.3\n(b) Recall@20 (%) in the PCDes and MoMu datasets.\nTable 4: Molecule-text retrieval performances. We re-\nport performances of using molecule to retrieve text\n(M2T) and using text to retrieve molecule (T2M). * de-\nnotes performance evaluated on the baseline’s released\ncheckpoint. † denotes result borrowed from (Su et al.,\n2022). Other models are trained on PubChem324k’s\npretrain subset. The complete results are in Appendix C\n2022) and MoMu (Su et al., 2022). Specifically, we\nevaluate MolCA’s checkpoint from pretrain stage\n1 without further fine-tuning. For all experiments,\nMolCA first retrieves the top 128 candidates us-\ning MTC, then employs the MTM module for re-\nranking. We select Accuracy (Acc) and Recall@20\n(R@20) as the evaluation metrics, and report the\nperformances of retrieval in the entire test set. As\nshown in Table 4, we observe that:\n1. MolCA demonstrates superior performance\nover baselines. Specifically, in PubChem324k,\nMolCA improves the accuracy by more than 20%\nover the baselines. In PCDes and MoMu, MolCA\nalso consistently outperforms the baselines, demon-\nstrating its effectiveness for molecule-text retrieval.\n2. Incorporating MTM significantly improves\nMolCA’s performance. This can be attributed to\nMTM’s ability to model long-range interactions\nbetween molecule features and texts, achieved by\nthe cross-attention and self-attention modules.\n3. MolCA’s good performances can be par-\ntially attributed to our larger pretrain dataset – Pub-\nChem324k. As shown in Table 4a, we compare\nthe performances of MoMu’s original checkpoint\n(pretrained on 15k molecule-text pairs) with our\nreproduced MoMu using PubChem324k. The latter\nimproves the retrieval accuracy by over 25%.\n4.5 Ablation Study on Representation Types\nHere we ablate the two representations types of\nmolecules: 1D SMILES and 2D graphs. We com-\npare MolCA with its two variants: 1) 1D SMILES:\nan LM that uses only 1D SMILES for pretraining\nand fine-tuning. For a fair comparison, we pretrain\nthis variant on PubChem324k’s pretrain subset for\nmolecule captioning before its downstream adapta-\ntion; 2) 2D Graph: this variant follows the original\nMolCA’s training pipeline, except not using 1D\nSMILES in pretrain stage 2 and fine-tune stage.\nEnd Task Ablation. Table 5 presents the re-\nsults for molecule-to-text generation and molecule\nproperty prediction (Hu et al., 2020) tasks. We can\nobserve that combing 2D graphs and 1D SMILES\nleads to improved performance in all the compared\ntasks. This demonstrates MolCA’s effectiveness in\nincorporating molecules’ 2D graph representations.\nCounting Functional Groups (FGs). We ablate\nMolCA’s capability of counting 85 types of FGs\ninside molecules. An FG is a molecule’s subgraph\nthat exhibits consistent chemical behaviors across\ndifferent molecules (Rong et al., 2020). Correctly\nRepresentation type BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\nMolecule Captioning, PubChem324k\n1D SMILES 34.6 26.9 46.3 32.3 41.5 41.1\n2D Graph 34.5 26.2 46.4 31.6 41.2 40.9\n1D SMILES + 2D Graph 38.7 30.3 50.2 35.9 44.5 45.6\nMolecule Captioning, CheBI-20\n1D SMILES 55.3 45.8 64.3 48.8 58.0 60.3\n1D SMILES + 2D Graph 62.0 53.1 68.1 53.7 61.8 65.1\nIUPAC Name Prediction, PubChem324k\n1D SMILES 71.0 60.6 68.4 45.8 61.5 71.5\n1D SMILES + 2D Graph 75.0 66.6 69.6 48.2 63.4 72.1\n(a) Ablating the representation type on tasks of molecule captioning and IUPAC name prediction.\nRepresentation type Bace BBBP ClinTox ToxCast Sider Tox21 Mean\n1D SMILES 79.3±0.8 70.8±0.6 89.0±1.7 56.2±0.7 61.1±1.2 76.0±0.5 72.1\n1D SMILES + 2D Graph 79.8±0.5 70.0±0.5 89.5±0.7 64.5±0.8 63.0±1.7 77.2±0.5 74.0\n(b) ROC-AUC (%) scores on six molecule property prediction datasets from MoleculeNet (Wu et al., 2018). We use scaffold\nsplit following (Hu et al., 2020). We report the performance’s mean values and standard deviations across three random seeds.\nTable 5: Ablating molecule’s representation types. All compared models fine-tune the base LM of Galactica1.3B.\n0 50 100\nFine-tune epochs\n0.0\n0.5\n1.0MSE loss\n(a) Training set loss.\n0 50 100\nFine-tune epochs\n0.3\n0.4\n0.5RMSE\n1D SMILES\n1D SMILES + 2D Graph (b) Valid set performance.\nFigure 6: Ablating MolCA for counting FGs inside\nmolecules from PubChem324k. We plot average values\nacross three random seeds. Blue shades indicate the\nrange of ± one standard deviation. Evaluation metric\nis Root Mean Square Error (RMSE). Lower value indi-\ncates better performance.\ncounting FGs can help understand a molecule’s\nproperties. As shown in Figure 6, incorporating\n2D graphs significantly improves MolCA’s perfor-\nmance in counting FGs, thereby enhancing its abil-\nity in understanding molecule structures.\n5 Related Works\nHere we briefly review the molecule-related lit-\nerature. We discuss MolCA’s relations to vision-\nlanguage pretraining methods in Appendix A.\nMolecule Understanding via 1D Language\nModeling. Due to the extensive biochemical liter-\nature in their training corpus, some open-domain\nLMs (Zhang et al., 2022; Touvron et al., 2023;\nChowdhery et al., 2022) have obtained a high-level\nunderstanding of molecular and chemical concepts.\nThis is demonstrated through their promising per-\nformances in text-related biochemical and medical\nquestion-answering benchmarks (Hendrycks et al.,\n2021; Jin et al., 2019). Among these LMs, Galac-\ntica (Taylor et al., 2022) shows competitive perfor-\nmances for using a corpus that is primarily com-\nposed of scientific literature. Focusing on the chem-\nistry domain, KV-PLM (Zeng et al., 2022) models\nmolecules by applying masked language modeling\nloss on 1D SMILES. Vaucher et al. (2021) pro-\npose to predict the chemistry experiment actions by\nreading chemical reaction equations. MolT5 (Ed-\nwards et al., 2022) presents several T5-based (Raf-\nfel et al., 2020) LMs for SMILES-to-text and text-\nto-SMILES translations. Further, Christofidellis\net al. (2023) propose to fine-tune T5 for chem-\nical reaction prediction and retrosynthesis tasks.\nMolCA is different from these methods that exclu-\nsively utilize 1D SMILES to represent molecules.\nInstead, MolCA aims to enable LMs to perceive\nmolecules’ 2D graph representations.\nMolecule-Text Contrastive Learning. Driven\nby the demand of a molecule-text retrieval system,\nText2Mol (Edwards et al., 2021) employs cross-\nmodal contrastive learning to train a molecular\ngraph encoder of GCNs (Kipf and Welling, 2017)\nand a text encoder of Sci-BERT (Beltagy et al.,\n2019). Subsequent works (Su et al., 2022; Liu\net al., 2022b; Seidl et al., 2023) have proposed en-\nhancements, including the addition of inter-modal\ncontrastive learning loss (Su et al., 2022) and apply-\ning the model for text-based molecule editing (Liu\net al., 2022b). However, cross-modal contrastive\nlearning is unsuitable for open-ended conditional\ngeneration task (Alayrac et al., 2022), because of its\nfocus on learning a similarity function. To resolve\nthe problem, we propose MolCA to enable the\nLM’s understanding of 2D molecular graphs, facili-\ntating MolCA’s capability of open-ended molecule-\nto-text generation.\n6 Conclusion and Future Works\nIn this work, we propose MolCA, a novel molec-\nular language modeling method. MolCA aims to\nenable LMs to perceive 2D graphs for molecule-to-\ntext generation. For this purpose, MolCA features\na cross-modal projector to map representations of\n2D graphs into the text space of LMs. It also em-\nploys a uni-modal adapter for efficient downstream\nadaptation. MolCA achieves state-of-the-art perfor-\nmances on molecule captioning and molecule-text\nretrieval benchmarks. Looking forward, we are\ninterested in exploring LMs for 3D molecular mod-\neling and drug discovery tasks.\nLimitations\nThis work focuses on utilizing LMs’ generation\nability for molecule-text tasks. Other interesting\nabilities of LMs, like in-context learning and chain-\nof-thought reasoning, are beyond the scope of this\nresearch. We leave that to future exploration.\nWhile MolCA offers improvements over base-\nlines, we observe that the current performance\nin molecule captioning is not yet sufficient for\npractical application. This can be attributed to\nthe scale of pretraining data. To our knowledge,\nour PubChem324k dataset is the largest dataset of\nmolecule-text pairs. However, compared to the\n∼10M scale dataset (Changpinyo et al., 2021) for\nvision-language pretraining, our dataset, consists\nof 324k data points, is comparatively smaller and\nlimits the model’s performance. Remedy solutions\nmay include mining weakly supervised data from\nbiochemical literature.\nBroader Impacts\nOur work has established new state-of-the-art per-\nformances in molecule captioning and molecule-\ntext retrieval. It has broader impacts in two as-\npects: 1) for chemistry professionals, our method\nof molecule captioning and molecule-text retrieval\ncould be useful tools, potentially speeding up their\nresearch process; 2) for individuals without spe-\ncialized chemistry knowledge, our method could\nprovide a more affordable way to access the basic\nchemical information of molecules.\nOur model shares the risks of most LMs. It can\ngenerate inaccurate information and can potentially\nbe abused to produce biased content. Further, con-\nsidering the limited scale of our training data, we\nstrongly advise strictly testing our model before\napplying it in real applications.\nAcknowledgement\nThis research is supported by the National Natu-\nral Science Foundation of China (92270114) and\nthe University Synergy Innovation Program of An-\nhui Province (GXXT-2022-040). This material is\nbased upon work supported by the Google Cloud\nResearch Credit program with the award (6NW8-\nCF7K-3AG4-1WH1). This research is supported\nby NExT Research Center.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob L. Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand\nSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karén Si-\nmonyan. 2022. Flamingo: a visual language model\nfor few-shot learning. In NeurIPS.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments. In IEEvalua-\ntion@ACL, pages 65–72. Association for Computa-\ntional Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nEMNLP/IJCNLP (1), pages 3613–3618. Association\nfor Computational Linguistics.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In CVPR, pages 3558–3568. Com-\nputer Vision Foundation / IEEE.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nDimitrios Christofidellis, Giorgio Giannone, Jannis\nBorn, Ole Winther, Teodoro Laino, and Matteo Man-\nica. 2023. Unifying molecular and textual represen-\ntations via multi-task language modelling. In ICML.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning. In NeurIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186. Associ-\nation for Computational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nCarl Edwards, Tuan Manh Lai, Kevin Ros, Garrett\nHonke, Kyunghyun Cho, and Heng Ji. 2022. Trans-\nlation between molecules and natural language. In\nEMNLP, pages 375–413. Association for Computa-\ntional Linguistics.\nCarl Edwards, ChengXiang Zhai, and Heng Ji. 2021.\nText2mol: Cross-modal molecule retrieval with natu-\nral language queries. In EMNLP (1), pages 595–607.\nAssociation for Computational Linguistics.\nHenri A Favre and Warren H Powell. 2013. Nomencla-\nture of organic chemistry: IUPAC recommendations\nand preferred names 2013. Royal Society of Chem-\nistry.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, et al. 2023. Mme: A compre-\nhensive evaluation benchmark for multimodal large\nlanguage models. arXiv preprint arXiv:2306.13394.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In ICLR. OpenReview.net.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In ICLR. OpenReview.net.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik,\nPercy Liang, Vijay Pande, and Jure Leskovec. 2020.\nStrategies for pre-training graph neural networks. In\nICLR.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In\nEMNLP/IJCNLP (1), pages 2567–2577. Association\nfor Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nSunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindu-\nlyte, Jia He, Siqian He, Qingliang Li, Benjamin A.\nShoemaker, Paul A. Thiessen, Bo Yu, Leonid Za-\nslavsky, Jian Zhang, and Evan Bolton. 2021. Pub-\nchem in 2021: new data content and improved\nweb interfaces. Nucleic Acids Res., 49(Database-\nIssue):D1388–D1395.\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In ICLR.\nGreg Landrum. 2013. Rdkit documentation. Release,\n1(1-79):4.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. CoRR, abs/2301.12597.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nACL/IJCNLP (1), pages 4582–4597. Association for\nComputational Linguistics.\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui,\nWanli Ouyang, Jing Shao, Fengwei Yu, and Jun-\njie Yan. 2022. Supervision exists everywhere: A\ndata efficient contrastive language-image pre-training\nparadigm. In ICLR. OpenReview.net.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raf-\nfel. 2022a. Few-shot parameter-efficient fine-tuning\nis better and cheaper than in-context learning. In\nNeurIPS.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nShengchao Liu, Weili Nie, Chengpeng Wang, Jiarui\nLu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei\nXiao, and Anima Anandkumar. 2022b. Multi-modal\nmolecule structure-text model for text-based retrieval\nand editing. CoRR, abs/2212.10789.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR (Poster). Open-\nReview.net.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut,\nYounes Belkada, and Sayak Paul. 2022. Peft: State-\nof-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/huggingface/peft.\nJack Merullo, Louis Castricato, Carsten Eickhoff, and\nEllie Pavlick. 2023. Linearly mapping from image\nto text space. In ICLR.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL, pages 311–318.\nACL.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In ICML, volume 139 of Proceedings\nof Machine Learning Research, pages 8748–8763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie,\nYing Wei, Wenbing Huang, and Junzhou Huang.\n2020. Self-supervised graph transformer on large-\nscale molecular data. In NeurIPS.\nPhilipp Seidl, Andreu Vall, Sepp Hochreiter, and\nGünter Klambauer. 2023. Enhancing activity pre-\ndiction models in drug discovery with the ability\nto understand human language. arXiv preprint\narXiv:2303.03363.\nTeague Sterling and John J. Irwin. 2015. ZINC 15 -\nligand discovery for everyone. J. Chem. Inf. Model.,\n55(11):2324–2337.\nBing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiang-\nmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-\nRong Wen. 2022. A molecular multimodal founda-\ntion model associating molecule graphs with natural\nlanguage. CoRR, abs/2209.05481.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, An-\ndrew Poulton, Viktor Kerkez, and Robert Stojnic.\n2022. Galactica: A large language model for science.\nCoRR, abs/2211.09085.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In NeurIPS, pages 200–212.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nAlain C Vaucher, Philippe Schwaller, Joppe Geluykens,\nVishnu H Nair, Anna Iuliano, and Teodoro Laino.\n2021. Inferring experimental procedures from text-\nbased representations of chemical reactions. Nature\ncommunications, 12(1):2573.\nDavid Weininger. 1988. Smiles, a chemical language\nand information system. 1. introduction to methodol-\nogy and encoding rules. J. Chem. Inf. Comput. Sci.,\n28(1):31–36.\nAlexander Frank Wells. 2012. Structural inorganic\nchemistry. Oxford university press.\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg,\nJoseph Gomes, Caleb Geniesse, Aneesh S Pappu,\nKarl Leswing, and Vijay Pande. 2018. Moleculenet:\na benchmark for molecular machine learning. Chem-\nical science, 9(2):513–530.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie\nJegelka. 2019. How powerful are graph neural net-\nworks? In ICLR.\nLewei Yao, Runhui Huang, Lu Hou, Guansong Lu,\nMinzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo\nLi, Xin Jiang, and Chunjing Xu. 2022. FILIP: fine-\ngrained interactive language-image pre-training. In\nICLR. OpenReview.net.\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen,\nZhangyang Wang, and Yang Shen. 2020. Graph con-\ntrastive learning with augmentations. In NeurIPS.\nZheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\n2022. A deep-learning system bridging molecule\nstructure and biomedical text with comprehension\ncomparable to human professionals. Nature commu-\nnications, 13(1):862.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nA Complete Related Works\nWe present the complete literature review. In addi-\ntion to the molecule-related literature, as addressed\nin the main body of the paper, we also discuss\nMolCA’s relation to vision-language pretraining.\nMolecule Understanding via 1D Language\nModeling. Due to the extensive biochemical liter-\nature in their training corpus, some open-domain\nLMs (Zhang et al., 2022; Touvron et al., 2023;\nChowdhery et al., 2022) have obtained a high-level\nunderstanding of molecular and chemical concepts.\nThis is demonstrated through their promising per-\nformances in text-related biochemical and medical\nquestion-answering benchmarks (Hendrycks et al.,\n2021; Jin et al., 2019). Among these LMs, Galac-\ntica (Taylor et al., 2022) shows competitive perfor-\nmances for using a corpus that is primarily com-\nposed of scientific literature. Focusing on the chem-\nistry domain, KV-PLM (Zeng et al., 2022) models\nmolecules by applying masked language modeling\nloss on 1D SMILES. Vaucher et al. (2021) pro-\npose to predict the chemistry experiment actions by\nreading chemical reaction equations. MolT5 (Ed-\nwards et al., 2022) presents several T5-based (Raf-\nfel et al., 2020) LMs for SMILES-to-text and text-\nto-SMILES translations. Further, Christofidellis\net al. (2023) propose to fine-tune T5 for chem-\nical reaction prediction and retrosynthesis tasks.\nMolCA is different from these methods that exclu-\nsively utilize 1D SMILES to represent molecules.\nInstead, MolCA aims to enable LMs to perceive\nmolecules’ 2D graph representations.\nMolecule-Text Contrastive Learning. Driven\nby the demand of a molecule-text retrieval system,\nText2Mol (Edwards et al., 2021) employs cross-\nmodal contrastive learning to train a molecular\ngraph encoder of GCNs (Kipf and Welling, 2017)\nand a text encoder of Sci-BERT (Beltagy et al.,\n2019). Subsequent works (Su et al., 2022; Liu\net al., 2022b; Seidl et al., 2023) have proposed im-\nprovements, including the addition of inter-modal\ncontrastive learning loss (Su et al., 2022) and apply-\ning the model for text-based molecule editing (Liu\net al., 2022b). However, cross-modal contrastive\nlearning is unsuitable for open-ended conditional\ngeneration task (Alayrac et al., 2022), because of its\nfocus on learning a similarity function. To resolve\nthe problem, we propose MolCA to enable the\nLM’s understanding of 2D molecular graphs, facili-\ntating MolCA’s capability of open-ended molecule-\nto-text generation.\nVision-Language Pretraining (VLP). Both\nVLP and Molecular Language Modeling aim to\nbridge the gap between text and another modality.\nNotably, VLP methods of CLIP (Radford et al.,\n2021) and others (Li et al., 2022; Yao et al., 2022)\nuse contrastive learning to connect a visual encoder\nand a text encoder. These methods can be applied\nfor tasks like image-text retrieval and zero-shot\nimage classification. Recently, a series of VLP\nworks (Tsimpoukelli et al., 2021; Merullo et al.,\n2023; Li et al., 2023; Alayrac et al., 2022) show\nthat visual features can be aligned to the text space\nof LMs. This cross-modal alignment allows LMs to\nutilize their language generation and few-shot learn-\ning abilities for multi-modal tasks. MolCA draws\ninspiration from these findings. To the best of our\nknowledge, we are the first to align 2D molecu-\nlar graphs to the text space of LMs. Furthermore,\nwe incorporate a uni-modal adapter to improve the\nadaptation efficiency on downstream tasks.\nB Experimental Settings\nPretrain Settings. MolCA’s pretrain stage 1 has\n50 epochs and pretrain stage 2 has 10 epochs. Q-\nFormer has 8 query tokens ( Nq = 8). Our opti-\nmizer’s configuration follows (Li et al., 2023). We\nuse the AdamW optimizer (Loshchilov and Hut-\nter, 2019) with a weight-decay of 0.05. The learn-\ning rate is scheduled by a combination of linear\nwarmup and cosine decay. The peak learning rate\nis 1e-4 and the warmup has 1000 steps.\nMolecule Captioning. MolCA is fine-tuned\nfor 100 epochs using the same configuration of\noptimizer and learning rate scheduler. LoRA is\nimplemented using the OpenDelta library (Ding\net al., 2022) and the PEFT library (Mangrulkar\net al., 2022). For the PubChem324k dataset, we set\nLoRA’s rankr to 8 and apply LoRA to Galactica’s\nmodules of [q_proj, v_proj] . This configura-\ntion yields a LoRA adapter with 2M parameters,\nwhich constitutes 0.12% of the parameters in the\nGalactica1.3B. For the CheBI-20 dataset, we set\nLoRA’s rankr to 16 and apply LoRA to Galactica’s\nmodules of [q_proj, v_proj, out_proj, fc1,\nfc2]. This configuration yields a LoRA adapter\nwith 12M parameters, which constitutes 0.94% of\nthe parameters in the Galactica1.3B.\nIUPAC Name Prediction. We collect IUPAC\nnames for molecules in the train/valid/test sets of\nPubChem324k using the PubChemPy library2. The\n2https://github.com/mcs07/PubChemPy\nSubset Size Usage Avg mol len Avg text len Min text len Max text len\nPretrain 298083 Pretrain stage 1 & 2 35 16 1 1305\nTrain 12000 Downstream fine-tune 32 60 20 937\nValid 1000 Downstream validation 32 61 20 1197\nTest 2000 Downstream test 31 60 20 879\nTable 6: Statistics of the PubChem324k dataset.\nRetrieval in batch Retrieval in test set\nM2T (%) T2M (%) M2T (%) T2M (%)\nModel Acc R@20 Acc R@20 Acc R@20 Acc R@20\n1D SMILES\nSci-BERT 83.2 97.6 82.4 97.2 39.7 85.8 37.5 85.2\nKV-PLM 83.2 97.8 82.7 97.5 38.8 86.0 37.7 85.5\n2D Graph\nMoMu-S* 42.3 90.1 43.7 90.1 11.5 41.2 12.6 43.6\nMoMu-K* 43.3 90.4 45.8 89.0 11.3 41.0 12.4 39.9\nMoMu-S 83.5 98.5 83.0 98.7 40.9 86.2 40.8 86.1\nMoMu-K 83.8 98.7 83.5 98.6 41.8 87.5 41.6 87.8\nMoleculeSTM 85.9 98.2 85.6 98.4 45.8 88.4 44.3 90.3\nMolCA w/o MTM 85.5 98.3 83.8 98.2 58.3 92.3 56.0 90.6\nMolCA 89.9 99.7 88.8 99.3 66.6 94.6 66.0 93.5\n(a) Molcule-text retrieval performances in the PubChem324k dataset.\nRetrieval in batch Retrieval in test set\nM2T (%) T2M (%) M2T (%) T2M (%)\nModel Acc R@20 Acc R@20 Acc R@20 Acc R@20\n1D SMILES\nSci-BERT† 62.6 61.8 60.7 60.8\nKV-PLM† 77.9 65.0 75.9 64.3\n2D Graph\nMoMu-S† 80.6 77.0 79.1 75.5\nMoMu-K† 81.1 80.2 80.2 79.0\nMoleculeSTM 81.4 98.5 78.9 97.5 39.5 80.4 35.8 77.0\nMolCA w/o MTM 80.9 98.1 77.9 97.5 37.7 80.6 35.3 76.5\nMolCA 86.4 99.8 84.8 98.5 48.1 85.6 46.0 82.3\n(b) Molecule-text retrieval performances in the PCDes dataset.\nRetrieval in batch Retrieval in test set\nM2T (%) T2M (%) M2T (%) T2M (%)\nModel Acc R@20 Acc R@20 Acc R@20 Acc R@20\n1D SMILES\nSci-BERT† 1.4 1.6 0.3 0.3\nKV-PLM† 1.5 1.3 0.5 0.3\n2D Graph\nMoMu-S† 45.7 40.0 43.3 43.4\nMoMu-K† 46.2 38.5 43.7 43.5\nMoleculeSTM* 67.6 96.2 64.1 96.3 24.0 70.5 23.7 66.9\nMolCA w/o MTM 65.0 95.9 63.3 95.9 22.5 68.5 21.1 64.8\nMolCA 73.4 98.5 72.8 97.5 30.6 76.8 29.8 73.3\n(c) Molecule-text retrieval performances in the MoMu dataset.\nTable 7: Complete molecule-text retrieval performances on the datasets of PubChem324k, PCDes and MoMu. *\ndenotes performance evaluated on the baseline’s released checkpoint. † denotes result borrowed from (Su et al.,\n2022). Other models are trained on PubChem324k’s pretrain subset.\nexperiment uses the same hyperparameters as the\nmolecule captioning experiment. We append a text\nprompt “The molecule’s IUPAC name is” after the\nmolecule representations as the task description (cf.\nModel Pretrain Stage 1 Pretrain Stage 2 BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\nMolCA, Galac1.3B ✗ ✗ 35.8 27.6 47.4 33.0 42.1 42.2\nMolCA, Galac1.3B ✓ ✗ 36.7 28.3 48.6 34.1 43.3 43.5\nMolCA, Galac1.3B ✓ ✓ 38.7 30.3 50.2 35.9 44.5 45.6\nTable 8: Ablating MolCA’s two pretrain stages by the task of molecule captioning in the PubChem324k dataset.\nCross-Modal Projector Representation Type BLEU-2 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR\n- 1D SMILES 33.7 26.0 45.4 31.6 40.7 40.3\nLinear 1D SMILES + 2D Graph 35.2 28.1 48.2 33.0 42.1 43.5\nQ-Former 1D SMILES + 2D Graph 39.8 31.7 51.7 37.3 46.2 46.8\nTable 9: Comparing different cross-modal projectors for molecule captioning on the PubChem324k dataset. All the\ncompared methods apply LoRA fine-tuning on Galactica1.3B.\nThe molecule is a long-chain fatty acid that is \nbehenic acid substituted at position 2 by a hydroxy \ngroup. It is a 2-hydroxy fatty acid. It is functionally \nrelated to a docosanoic acid. It is a conjugate acid of \na 2-hydroxybehenate.\nGround truth\nThe molecule is a 2-hydroxy fatty acid that is \nhexacosanoic acid substituted at position 2 by a \nhydroxy group. It is a long-chain fatty acid. It is \nfunctionally related to an hexacosanoic acid. It is \na conjugate acid of a 2-hydroxyhexacosanoate.\nThe molecule is an amino disaccharide consisting of \nalpha-(...) joined in sequence by a (1->4) glycosidic \nbond. It is a disaccharide derivative, an \noligosaccharide sulfate, a member of sulfamic acids, \na monocarboxylic acid (...)\nThe molecule is a disaccharide that consists of \n2-O-(...) residues joined in sequence by a (1-\n>4) glycosidic bond. It is a disaccharide, an \namino disaccharide, and a member of sulfamic \nacids.\nMolCAMolecule\n(a) Samples of molecule captioning.\n(2S)-2,4-diamino-4-oxo \nbutanoic acid\n(2S)-2,4-diamino-4-\noxobutanoic acid\nGround truth MolCAMolecule\n2-(3,4-dichlorophe \nnoxy)acetic acid\n2-(2,4-dichloropheno \nxy)acetic acid\n1,3-thiazolidine-4-\ncarboxylic acid\nthiomorpholine-3-\ncarboxylic acid\n4-chloro-2,5-dioxo-cycl \nohexa-2,5-dien-1-olate\n2-chloro-5,6-diox \nopyridin-3-olate (b) Samples of IUPAC name prediction.\nFigure 7: Examples of MolCA’s molecule-to-text generation results. We highlight text snippets in blue that correctly\ndescribe the molecule structures in the predicted texts. To save space, some parts of texts are replaced by (...).\nFigure 5).\nMolecule-Text Retrieval. We use MolCA’s\ncheckpoint from pretrain stage 1 for retrieval with-\nout fine-tuning on any other datasets. This is simi-\nlar to the setting of zero-shot retrieval in (Su et al.,\n2022; Liu et al., 2022b).\nMolecule Property Prediction. Following (Hu\net al., 2020), we fine-tune the models for 100\nepochs and report the test performance selected\nby the valid set. For molecule classification, we\nattach a linear classifier after the mean pooling\nof the LM’s hidden states of the last layer. We\nuse the AdamW optimizer with a constant learn-\ning rate of 1e-4 and weight decay of 0.05. This\nexperiment uses the same LoRA configuration as\nthe molecule captioning experiment in the Pub-\nChem324k dataset.\nCounting Functional Groups (FGs). We use\nthe molecules in PubChem324k’s train set for fine-\ntuning and use the molecules in the valid set for\nevaluation. Following (Rong et al., 2020), we use\nRDkit (Landrum, 2013) to obtain the ground truth\ncounts of FGs in every molecule. For each FG type,\nwe employ a separate linear classifier to regress\nits numbers. Our model is trained using the Mean\nSquare Error (MSE) loss function. Other settings,\nincluding optimizer and LoRA, are the same as the\nMolecule Property Prediction experiment.\nGalactica. Following the instructions in (Tay-\nlor et al., 2022), we wrap SMILES sequences\nwith special tokens of [START_I_SMILES] and\n[END_I_SMILES] before feeding them into Galac-\ntica.\nPubChem324k Dataset. Our dataset collection\nprocess follows the procedures described in (Liu\net al., 2022b). The resulting dataset is larger due\nto the frequent updates made to the PubChem\ndatabase (Kim et al., 2021). For each molecule\nin this website, we use the “description” field in\nits webpage as the corresponding text description.\nTo avoid information leakage, we replace any com-\nmon name or IUPAC name of the molecule at the\nbeginning of texts with a text template (i.e., “The\nmolecule”). Detailed statistics of PubChem324k\nare presented in Table 6.\nC More Experimental Results\nMolecule-Text Retrieval . Here we present\nMolCA’s complete molecule-text retrieval perfor-\nmance on the PubChem324k, PCDes, and MoMu\ndatasets. Following (Su et al., 2022), we report the\nperformance of retrieval in a batch of 64 random\nSample 1 SMILES: C([C@@H]1[C@H]([C@@H]([C@H](C(O1)O)NS(=O)(=O)O)O)O[C@H]2\n[C@@H]([C@H](C(=C(O2)C(=O)O)O)O)O)OS(=O)(=O)O\nGround truth The molecule is an amino disaccharide consisting of alpha-(...) joined in sequence by a (1->4)\nglycosidic bond. It is a disaccharide derivative, an oligosaccharide sulfate, a member of sulfamic\nacids, a monocarboxylic acid (...)\n1D SMILES The molecule is a disaccharide sulfate consisting of 2-acetamido-(...) joined in sequence by a\n(1->4) glycosidic bond. It is functionally related to a N-acetyl-D-glucosamine and a N-acetyl-D-\ngalactosamine.\n1D SMILES + 2D Graph The molecule is a disaccharide that consists of 2-O-(...) residues joined in sequence by a (1->4)\nglycosidic bond. It is a disaccharide, an amino disaccharide, and a member of sulfamic acids.\nSample 2 SMILES: CCCCCCCCCCCCCCCCCCCCC(C(=O)O)O\nGround truth The molecule is a long-chain fatty acid that is behenic acid substituted at position 2 by a hydroxy\ngroup. It is a 2-hydroxy fatty acid. It is functionally related to a docosanoic acid. It is a conjugate\nacid of a 2-hydroxybehenate.\n1D SMILES The molecule is a 2-hydroxy fatty acid that is the 2-hydroxy derivative of tetracosanoic acid. It is\nfunctionally related to a tetracosanoic acid. It is a conjugate acid of a 2-hydroxytetracosanoate.\n1D SMILES + 2D Graph The molecule is a 2-hydroxy fatty acid that is hexacosanoic acid substituted at position 2 by a\nhydroxy group. It is a long-chain fatty acid. It is functionally related to an hexacosanoic acid. It\nis a conjugate acid of a 2-hydroxyhexacosanoate.\nTable 10: Molecule captioning samples of MolCA (i.e., 1D SMILES + 2D Graph) and its variant of using only 1D\nSMILES. We highlight text snippets in blue that correctly describe the molecule structures in the predicted texts. To\nsave space, some parts of texts are replaced by (...).\nsamples and the performance of retrieval in the en-\ntire test set. As shown in Table 7, our conclusions\nalign with those from Section 4.4: 1) MolCA con-\nsistently outperforms the baselines for molecule-\ntext retrieval; 2) applying the MTM module for\nre-ranking is crucial for MolCA’s molecule-text\nretrieval performances.\nAblating the Pretrain Stages. We conduct ab-\nlation studies on MolCA’s two pretrain stages. As\nshown in Table 8, both the two pretrain stages have\nsignificant contributions to MolCA’s molecule cap-\ntioning performances.\nAblating the Cross-Modal Projector. We com-\npare the performances of our selected cross-modal\nprojector Q-Former and a linear cross-modal pro-\njector. For the linear cross-modal projector, we\nfeed the node representations from the graph en-\ncoder to the base LM after the linear projector layer.\nWe tune the weights of the graph encoder, linear\nprojector, and the base LM’s LoRA adapter. The\nexperimental setting and hyperparameters are the\nsame as those of MolCA. Table 9 shows the results.\nWe can observe that: 1) Linear cross-modal projec-\ntor underperforms Q-Former. We conjecture that\na linear layer is suboptimal to bridge the modal-\nity gap between 2D molecules and 1D texts. This\naligns with findings in the MME benchmark (Fu\net al., 2023), where Q-Former-based methods (e.g.,\nBLIP-2, InstructBLIP (Dai et al., 2023), MiniGPT-\n4 (Zhu et al., 2023)) outperform linear cross-modal\nprojector based method ( e.g., LLaV A (Liu et al.,\n2023)). 2) Linear cross-modal projector slightly\noutperforms the SMILES-only baseline. We at-\ntribute this improvement to the usage of 2D molec-\nular graphs, but the gains are limited because the\nlinear projector is less effective.\nMolCA’s Generation Results. Figure 7 shows\nMolCA’s molecule-to-text generation results. The\ntwo samples of molecule captioning is also pre-\nsented in Table 10. Specifically, we compare\nMolCA (i.e., 1D SMILES + 2D Graph) and its\nvariant that is pretrained and fine-tuned using only\n1D SMILES. We can observe that using both 1D\nSMILES and 2D graph leads to more accurate de-\nscriptions of molecule structures.\nComputational Cost. We present the real-world\ntraining time of MolCA’s three training stages in\nTable 11. All experiments are conducted on two\nNVIDIA A100 40 GB GPUs. Notably, we observe\nthat the fine-tuning stage is affordable in terms of\ncomputational resources.\nStage Base LM Dataset Epochs Time\nPretrain stage 1 - PubChem324k pretrain subset 50 18.0h\nPretrain stage 2 Galac 1.3B, freeze PubChem324k pretrain subset 10 9.0h\nPretrain stage 2 Galac 125M, freeze PubChem324k pretrain subset 10 3.0h\nFine-tune stage Galac 1.3B, LoRA ft PubChem324k train subset 100 6.0h\nFine-tune stage Galac 125M, full ft PubChem324k train subset 100 1.5h\nTable 11: Compuational cost for MolCA’s three stages.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7616106867790222
    },
    {
      "name": "Modal",
      "score": 0.632280170917511
    },
    {
      "name": "Adapter (computing)",
      "score": 0.6251896619796753
    },
    {
      "name": "Graph",
      "score": 0.5506184697151184
    },
    {
      "name": "Theoretical computer science",
      "score": 0.48418763279914856
    },
    {
      "name": "Encoder",
      "score": 0.4653881788253784
    },
    {
      "name": "Natural language processing",
      "score": 0.4467077851295471
    },
    {
      "name": "Pairwise comparison",
      "score": 0.41894716024398804
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3783727288246155
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ]
}