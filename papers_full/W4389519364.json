{
    "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
    "url": "https://openalex.org/W4389519364",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2753978064",
            "name": "Shangbin Feng",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2914372190",
            "name": "Vidhisha Balachandran",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2315100200",
            "name": "Yu-Yang Bai",
            "affiliations": [
                "Xi'an Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2234266251",
            "name": "Yulia Tsvetkov",
            "affiliations": [
                "University of Washington"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3169283369",
        "https://openalex.org/W4281629850",
        "https://openalex.org/W4385571140",
        "https://openalex.org/W3213990450",
        "https://openalex.org/W3101551503",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W4285177011",
        "https://openalex.org/W4287887686",
        "https://openalex.org/W4385572316",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W2996371683",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W3196696529",
        "https://openalex.org/W3174306305",
        "https://openalex.org/W2577581976",
        "https://openalex.org/W4385573374",
        "https://openalex.org/W4361230777",
        "https://openalex.org/W4385570231",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2995448904",
        "https://openalex.org/W3099396524",
        "https://openalex.org/W3170432046",
        "https://openalex.org/W3153444823",
        "https://openalex.org/W3172205429",
        "https://openalex.org/W4389520103",
        "https://openalex.org/W3176640961",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2263338482",
        "https://openalex.org/W4205902686",
        "https://openalex.org/W3171434230",
        "https://openalex.org/W4281657280",
        "https://openalex.org/W4224440661",
        "https://openalex.org/W2995089337",
        "https://openalex.org/W2890419825",
        "https://openalex.org/W3099977667",
        "https://openalex.org/W4287889327",
        "https://openalex.org/W3138643305",
        "https://openalex.org/W3198859314",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W3151929433",
        "https://openalex.org/W2945760033",
        "https://openalex.org/W3186545525",
        "https://openalex.org/W3098101986",
        "https://openalex.org/W3212619741",
        "https://openalex.org/W4298182985",
        "https://openalex.org/W4385573391",
        "https://openalex.org/W3028709955",
        "https://openalex.org/W4287887766",
        "https://openalex.org/W3034383590",
        "https://openalex.org/W4285151662",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4205550663",
        "https://openalex.org/W4287854421",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287855005",
        "https://openalex.org/W4287780085",
        "https://openalex.org/W2575845411",
        "https://openalex.org/W4307003748",
        "https://openalex.org/W3021738738",
        "https://openalex.org/W3040558716",
        "https://openalex.org/W3115703502",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W3034535994",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4223651117",
        "https://openalex.org/W4288027758",
        "https://openalex.org/W4287854917",
        "https://openalex.org/W4385573912",
        "https://openalex.org/W3176477796",
        "https://openalex.org/W3106255016",
        "https://openalex.org/W3207773588",
        "https://openalex.org/W3213441987",
        "https://openalex.org/W4385571374",
        "https://openalex.org/W4385570171",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4226281578",
        "https://openalex.org/W3103420681",
        "https://openalex.org/W3020786614",
        "https://openalex.org/W4385569799",
        "https://openalex.org/W4205621476",
        "https://openalex.org/W3197332064",
        "https://openalex.org/W2952484617",
        "https://openalex.org/W3034188538",
        "https://openalex.org/W3106234277",
        "https://openalex.org/W4287854890",
        "https://openalex.org/W2950620609",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W3177242492",
        "https://openalex.org/W3099878876",
        "https://openalex.org/W3155901769",
        "https://openalex.org/W4287887652",
        "https://openalex.org/W3174660442",
        "https://openalex.org/W4385570458",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4286903249",
        "https://openalex.org/W2724394450",
        "https://openalex.org/W4205477024",
        "https://openalex.org/W4385571830",
        "https://openalex.org/W2768957049",
        "https://openalex.org/W4385570038",
        "https://openalex.org/W4288029087",
        "https://openalex.org/W4287027142",
        "https://openalex.org/W4226118367",
        "https://openalex.org/W4385570935",
        "https://openalex.org/W4285120654"
    ],
    "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB—a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on entity-specific facts, facts extracted from auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and easily generalizable across domains.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 933–952\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nFACT KB: Generalizable Factuality Evaluation using Language Models\nEnhanced with Factual Knowledge\nShangbin Feng1 Vidhisha Balachandran2 Yuyang Bai3 Yulia Tsvetkov1\n1University of Washington 2Carnegie Mellon University 3Xi’an Jiaotong University\n{shangbin, yuliats}@cs.washington.edu vbalacha@cs.cmu.edu 1206944633@stu.xjtu.edu.cn\nAbstract\nEvaluating the factual consistency of automati-\ncally generated summaries is essential for the\nprogress and adoption of reliable summariza-\ntion systems. Despite recent advances, exist-\ning factuality evaluation models are not ro-\nbust, being especially prone to entity and re-\nlation errors in new domains. We propose FAC-\nTKB—a simple new approach to factuality\nevaluation that is generalizable across domains,\nin particular with respect to entities and re-\nlations. FACT KB is based on language mod-\nels pretrained using facts extracted from exter-\nnal knowledge bases. We introduce three types\nof complementary factuality pretraining objec-\ntives based on entity-specific facts, facts ex-\ntracted from auxiliary knowledge about entities,\nand facts constructed compositionally through\nknowledge base walks. The resulting factual-\nity evaluation model achieves state-of-the-art\nperformance on two in-domain news summa-\nrization benchmarks as well as on three out-\nof-domain scientific literature datasets. Further\nanalysis of FACT KB shows improved ability to\ndetect erroneous entities and relations in sum-\nmaries and is robust and easily generalizable\nacross domains. Code and data are available at\nhttps://github.com/BunsenFeng/FactKB.\n1 Introduction\nGenerating factually accurate document summaries\nin addition to fluent and informative ones is critical\nto the adoption of summarization models (Kry ´s-\nci´nski et al., 2020; Goyal and Durrett, 2020). How-\never, evaluating the factual consistency of sum-\nmaries is still challenging, especially in special-\nized domains like scientific or legal (Cachola et al.,\n2020; Goldsack et al., 2022; Polsley et al., 2016;\nKanapala et al., 2019). The key reason is that the\nmajority of existing approaches employ neural clas-\nsifiers trained on synthetic data constructed from a\nrelatively small set of documents (Kry´sci´nski et al.,\n2020; Goyal and Durrett, 2020). These factuality\nQAGS: Factual\nQAGS: Factual\n DAE: Factual\nDAE: Factual\n FactCC: Factual\nFactCC: Factual\nQAGS: Factual\n DAE: Factual\n FactCC: Factual\nArticle:\nThe stone got past the elephant's fence and a ditch \nseparating the animal and visitors… Scientific Director of \nthe Amboseli Trust for Elephants, says that targeted \nthrowing of stones and branches by elephants is very \nunusual. \"It can happen when elephants are frustrated or \nbored... The moments after the girl was struck at Rabat \nZoo on Tuesday were filmed by a bystander and uploaded \nonto YouTube ...\nSummary:\nAn elephant has been hit by a stone at a zoo in western \nFrance after it was hit by a tree.\nGold: Not FactualGold: Not Factual\nArticle:\nThe stone got past the elephant's fence and a ditch \nseparating the animal and visitors… Scientific Director of \nthe Amboseli Trust for Elephants, says that targeted \nthrowing of stones and branches by elephants is very \nunusual. \"It can happen when elephants are frustrated or \nbored... The moments after the girl was struck at Rabat \nZoo on Tuesday were filmed by a bystander and uploaded \nonto YouTube ...\nSummary:\nAn elephant has been hit by a stone at a zoo in western \nFrance after it was hit by a tree.\nGold: Not Factual\nFigure 1: Existing factuality models struggle to identify\nsemantic frame errors encompassing entities and rela-\ntions. In the example, they fail to identify an error in the\ngenerated summary about who was hit by the stone.\nclassifiers are thus not robust to ever-growing infor-\nmation, in which the distribution of entities, events,\nand their relations changes greatly across time and\ndomains (Elsahar and Gallé, 2019; Laparra et al.,\n2020). Pagnoni et al. (2021) highlighted this limi-\ntation, finding that over 50% of factuality errors in\nthe XSUM (Narayan et al., 2018) summarization\ndataset stem from semantic frame errors, namely\nentities, events, and relations between them, as il-\nlustrated in Figure 1.\nTo address these issues, we develop a new fac-\ntuality evaluation model with improved factual\nknowledge representation, specifically focusing on\nentities and relations. Entity-oriented pretraining\nobjectives have been shown to improve QA and\nreasoning tasks (Yasunaga et al., 2022; Liu et al.,\n2022b); we thus hypothesize that similar objectives\ncan aid factuality evaluation in better detecting se-\nmantic frame errors in generated summaries.\nWe propose FACT KB, a novel factuality eval-\nuation model built upon language models (LMs)\naugmented with factual knowledge (§2). The LMs\n933\n...\n...Language Model\n...\n...Language Model Fact-Enhanced LMFact-Enhanced LM\n...\n...\nFact-Enhanced LM\n...\n...\nFactKBFactKB\n...\n...\nFactKB\n...\n...\nwas born in is in\na country of\nis\ndoctoral advisor\nwas born in\nis in\na member of ...\n......\n<MASK><MASK>\nWeil der Stadt\nWeil der Stadt\nEuropean \nUnion\nEuropean \nUnion\nEurope\nEurope\nAstronomer\nAstronomer\nMichael \nMaestlin\nMichael \nMaestlin\n Göppingen\nGöppingen\nJohannes \nKepler\nJohannes \nKepler\nKnowledge \nBase\nKnowledge Walk\nis\nJohannes \nKepler\nJohannes Kepler is <MASK>. \nJohannes Kepler is a key figure \nin the 17th-century Scientific \nRevolution, best known for his \nlaws of planetary motion ...\n<MASK><MASK>\nWikipedia\nWikipedia\nKnowledge \nBase\nKnowledge \nBase\nEvidence Extraction\nJohannes Kepler is a key figure \nin the 17th-century Scientific \nRevolution, best known for his \nlaws of planetary motion ...\nEntity Wiki\nKepler doctoral advisor Michael Maestlin. \nKepler is <MASK>. Kepler born in Weil \nder Stadt. Kepler writes Somnium.Knowledge \nBase\nFactuality Evaluation Model\nFactuality Pretraining\nFactCollect\nFine-tuning\nwrites born in\nis\n<MASK><MASK>\nSomnium\nSomnium\n Weil der Stadt\nWeil der Stadt\nMichael \nMaestlin\nMichael \nMaestlin\nJohannes \nKepler\nJohannes \nKepler\ndoctoral advisor\nwrites born in\nis\n<MASK>\nSomnium\n Weil der Stadt\nMichael \nMaestlin\nJohannes \nKepler\ndoctoral advisor\nAstronomer\n<MASK>=\n<MASK>=\nAstronomer\n<MASK>=\nAstronomer\n<MASK>=\nAstronomer\n<MASK>=\nAstronomer\n<MASK>=\nAstronomer\n<MASK>=\nGermany\nGermany\n<MASK>=\nGermany\n<MASK>=\nGermany\n<MASK>=\nKepler was born in Weil der Stadt is in \n<MASK> is a country in Europe...\nFigure 2: Overview of FACT KB. FACT KB pretrains LMs using three entity-centric pretraining strategies to improve\nfact representations. The objectives are designed to fill masked entities/relations in KB facts using i)Entity Wiki -\ndirect facts about entities ii) Evidence Extraction - auxiliary knowledge about entities and iii) Knowledge Walk -\ncompositional knowledge from the KB. The pretrained LMs are then fine-tuned for robust factuality evaluation.\nare pretrained with knowledge-focused objectives\nusing text synthesized from external knowledge\nbases (KBs) which store high-quality facts about\nentities and relations. We propose three types of\ncomplementary pretraining strategies: (1) entity\nwiki, with a focus on improving entity understand-\ning; (2) evidence extraction, with a focus on in-\ncorporating supporting evidence from surrounding\ncontext; and (3) knowledge walks, with a focus\non augmenting compositional reasoning about en-\ntities. For factuality evaluation, we first pretrain a\nlanguage model using these three entity-centric pre-\ntraining strategies, and then fine-tune the enhanced\nLM on a factual error detection dataset.\nWe evaluate FACT KB’s correlation with human\nfactuality judgments across three settings (§3). In\nin-domain (news) summarization, FACT KB sig-\nnificantly outperforms baselines by 2–7 balanced\naccuracy (BACC) points on the FactCollect dataset\n(Ribeiro et al., 2022) and 10–12 correlation points\non the FRANK benchmark (Pagnoni et al., 2021),\nparticularly showing marked improvements in se-\nmantic frame errors. In out-of-domain experiments,\nFACT KB consistently outperforms existing ap-\nproaches by 3–5 BACC points on three datasets in\nbiomedical and scientific domains (Saakyan et al.,\n2021; Sarrouti et al., 2021; Wadden et al., 2020),\ndemonstrating stronger generalizability to unseen\ndocuments in new domains. Further analysis shows\nthat FACT KB is compatible with different LMs and\nKBs while presenting a lightweight and easy-to-use\napproach to factuality evaluation. Code, data, and\ntrained factuality evaluation models are publicly\navailable.\n2 F ACT KB Methodology\nFACT KB aims to improve the robustness and gen-\neralizability of factuality evaluation by a simple\nfactuality pretraining, which improves entity and\nrelation representations in LMs. We first propose\nthree pretraining strategies (§2.1). We then describe\nthe training process to (1) pretrain an LM using\nthe proposed strategies and (2) fine-tune the fact-\nenhanced LM on a factuality error detection dataset,\nresulting in FACT KB (§2.2). Figure 2 presents an\noverview of our approach.\n2.1 Factuality Pretraining\nKnowledge bases are rich reservoirs of facts about\nentities and relations (Vrande ˇci´c and Krötzsch,\n2014; Pellissier Tanon et al., 2020), and we ex-\nplore the possibility of leveraging external KBs as\n“fact teachers” to enhance an LM’s representation\nof entities and relations.\nLet KB = ( E,R,A,ϵ,φ ), where E =\n{e1,...,e N}represents the entities in the KB,\nR= {r1,...,r M}denotes the relations in the KB,\nA denotes the adjacency matrix where aij = kin-\ndicates relation rk connecting entities ei and ej\n(ei,rk,ej) ∈KB, ϵ(·) :E→ str and φ(·) :R→\nstr map the entities and relations to their textual\nnames. We propose three novel types of factuality\npretraining strategies that leverage the KB.\nStrategy 1: Entity Wiki Entities in KBs often\nhave multiple edges connecting them to other enti-\nties via relations, each representing a distinct but\nrelated fact about the entity. Inspired by the task of\nknowledge base completion (Bordes et al., 2013;\nVashishth et al., 2019) to predict missing connec-\n934\nFactuality Pretraining Corpus Size Bound # Tokens Example\nENTITYWIKI ∝|E| 5.4M Johannes Kepler is born in Italy. Johannes\nKepler is an [MASK]. [SEP] Johannes Ke-\npler is the author of Astronomia nova.. . .\nEVIDENCEEXTRACTION ∝||A||0 12.2M Hillary Clinton party affiliation [MASK]\nHillary Diane Rodham Clinton is an Amer-\nican politician,. . .Member of the Demo-\ncratic Party, she was the nominee. . .\nKNOWLEDGEWALK ∝|E|(||A||0\n|E|)k 2.7M University of Edinburgh located in Scot-\nland located in [MASK] is a continent. . .\nTable 1: Summary of the three factuality pretraining strategies.\ntions in KBs based on available KB facts, we pro-\npose the entity wiki factuality pretraining, where an\nLM is pretrained with the task of predicting masked\nentities or relations in KB facts. Specifically, for\neach entity ei ∈E, we retrieve its one-hop neigh-\nborhood in the KB as Eei = {ej | ∃rk s.t.aij =\nk}. We then synthesize a sentence using entity ei\nand its connected one-hop facts:\ndi = concatej∈Eei\n[\nϵ(ei)φ(rk|aij = k)ϵ(ej)[SEP]\n]\nwhere concat denotes string concatenation and\n[SEP] denotes the special token. Repeating this\ngeneration process for all e ∈ E, we produce a\ncorpus of entity facts as {di}|E|\ni=1 with the max size\nbeing the number of entities |E|. We use this entity\nwiki corpus to pretrain an LM for better factual rea-\nsoning by randomly masking entities and relations\nin it and training the LM to predict the mask given\nthe surrounding facts about an entity. We randomly\nmask the corpora with probability pand pretrain\nLMs with the masked language modeling objective.\nWe expect this objective to train LMs to infer facts\nfrom surrounding knowledge and penalize unsup-\nported hallucinations about entities and relations.\nStrategy 2: Evidence Extraction The goal of\nthis pretraining strategy is to enhance the model’s\nability to evaluate facts based on relevant evi-\ndence. We begin by randomly selecting a triple\n(ei,rk,ej) ∈KB and use the first paragraph of the\nWikipedia description of ei as the auxiliary knowl-\nedge. We synthesize a sentence using the two as:\ndi = ϵ(ei) φ(rk) [MASK] Wikipedia(ei)\nwhere we mask out ϵ(ej) and [MASK] denotes\nthe special token, Wikipedia(·) : E→ str maps\nentities to the first paragraph of their Wikipedia de-\nscription. Repeating this process N times with ran-\ndomly selected triples, we obtain a corpus of triples\npaired with auxiliary knowledge {di}N\ni=1. The cor-\npus size is bounded by all KB triples represented as\nthe L0 norm of the adjacency matrix ||A||0. We use\nthis corpus for the evidence extraction factuality\npretraining and train the LM to predict the mask by\nusing relevant evidence in the auxiliary paragraph.\nThrough this, we aim to augmentFACT KB’s ability\nto implicitly select evidence from the document to\nsupport its factuality evaluation.\nStrategy 3: Knowledge Walk Natural language\ndocuments often include compositional statements\nabout entities and relations (Feldman and El-\nYaniv, 2019; Wang and Pan, 2022), but pretrained\nLMs struggle with such compositional reasoning\n(Press et al., 2022). To improve FACT KB’s abil-\nity to understand multi-hop claims, we propose\nthe knowledge walk factuality pretraining strat-\negy. Specifically, we randomly select a starting en-\ntity e(0) and randomly select an entity e(1) from\nits direct neighborhood Ee(0) , resulting in a one-\nhop triple {e(0),r(0,1),e(1)}where r(0,1) denotes\nthe relation between e(0) and e(1). Now, from\ne(1), we randomly select an entity from it’s di-\nrect neighborhood to take the next step. We re-\npeat this process for Ktimes, and obtain a K-\nhop random walk of triples beginning at e(0):\n{e(0), r(0,1), e(1), ··· , r(K−1, K), e(K)}. We then\nproduce a sentence based on the K-hop walk:\ndi = ϵ(e(0)) concatK−1\ni=0\n[\nφ(r(i,i+1)) ϵ(e(i+1))\n]\nRepeating this K-hop walk N times with differ-\nent randomly selected starting entities, we obtain\n{di}N\ni=1 as the corpus for the knowledge walk fac-\ntuality pretraining, whose size is bounded by the\nnumber of all possible K-hop walks as |E|(||A||0\n|E| )k.\nIn this corpus, we randomly mask entities or rela-\ntions in each group of facts with probability pand\ntrain an LM to predict the masked element using\nthe compositional facts around it using the masked\nlanguage model objective. Through this pretraining,\nwe expect FACT KB to improve in compositional\n935\nModel All Data CNN/DM XSUM\nBACC F1 BACC F1 BACC F1\nQAGS 79.8 79 .7 64 .2 76 .2 59 .3 85 .2\nQUALS 78.3 78 .5 60 .8 76 .2 57 .5 82 .2\nROBERTA 76.1 76 .5 62 .5 76 .2 62 .1 78 .3\nFALSESUM 78.9 78 .2 53 .7 34 .6 61 .1 64 .3\nFALSESUM+ 84.2 83 .7 64 .2 77 .1 67 .4 82 .1\nSUMMAC 86.6 86 .2 75 .4 83 .5 71 .9 90 .4\nFACTCC 76.0 76 .3 69 .0 77 .8 55 .9 73 .9\nFACTCC+ 83.9(±0.4) 84.2(±0.4) 68.0(±1.0) 83.7(±0.5) 58.3(±2.2) 84.0(±1.0)\nFACTGRAPH 86.3(±1.3) 86.7(±1.1) 73.0(±2.3) 86.8(±0.8) 68.6(±2.3) 86.6(±2.0)\nFACTGRAPH-ADAPTERS 87.6(±0.7) 87.8(±0.7) 76.0(±2.8) 87.5(±0.4) 69.9(±2.3) 88.4(±1.2)\nFACTKB-WIKI 89.3(±0.4)∗ 89.5(±0.5)∗ 77.3(±0.3)∗ 88.2(±0.6)∗ 77.3(±1.3)∗ 91.8(±1.2)∗\nFACTKB-EVIDENCE 89.4(±0.2)∗ 89.5(±0.3)∗ 77.7(±1.4)∗ 87.9(±0.7) 76.8(±1.9)∗ 90.8(±0.8)∗\nFACTKB-WALK 89.1(±0.4)∗ 89.3(±0.5)∗ 78.3(±1.2)∗ 87.7(±0.4) 76.4(±0.3)∗ 90.4(±1.4)∗\nTable 2: Performance of FACT KB on the FactCollect dataset. We report average performance and standard deviation\nacross 5 random seeds. Best performance is shown in bold, while * indicates statistical significance. FACT KB\nsignificantly outperforms existing factuality evaluation approaches on in-domain evaluation.\nfact understanding about entities and relations ap-\npearing in the summary and the input document.\nWe briefly summarize the three factuality pre-\ntraining strategies and provide examples in Table 1.\n2.2 F ACT KB Training\nWe initialize FACT KB with encoder-based LMs\nand pretrain FACT KB separately with each of\nthe three factuality pretraining corpora using the\nmasked language modeling objective to study the\neffectiveness of each strategy. This results in fact-\nenhanced LMs with the ability to better represent\nfacts, entities, and relations. Finally, we fine-tune\nFACT KB on human-annotated factual error detec-\ntion datasets with the sequence classification set-\nting, taking SUMMARY [SEP] DOCUMENT as input\nand produce FACTUAL or NON -FACTUAL labels.\nThe [CLS] token is adopted for classification. As a\nresult, we obtain FACT KB, our entailment-based\nfactuality evaluation model that classifies machine-\ngenerated summaries as factual or non-factual.\n3 Data and Experiment Settings\n3.1 Training\nData We use YAGO (Pellissier Tanon et al.,\n2020), an encyclopedic knowledge base based on\nWikidata (Vrandeˇci´c and Krötzsch, 2014), to con-\nstruct the three types of factuality pretraining cor-\npora, while we discuss FACT KB’s compatibility\nwith different KBs in Section 5.2. For finetuning,\nwe use the FactCollect dataset (Ribeiro et al., 2022),\na dataset for factual error detection that gathers\nhuman annotations from different sources (Wang\net al., 2020a; Kry´sci´nski et al., 2020; Maynez et al.,\n2020; Pagnoni et al., 2021) and consolidates them\ninto a single dataset. It mainly focuses on the\nnews media domain, covering summaries and ar-\nticles from CNN, Daily Mail, and BBC. FactCol-\nlect follows a binary classification setting where\neach (SUMMARY , ARTICLE ) pair has a FACTUAL\nor NON -FACTUAL label. We present more details\nabout the FactCollect dataset in Appendix C.\nSettings We use a ROBERTA-BASE (Liu et al.,\n2019) checkpoint and continue pretraining sepa-\nrately on each of the three factuality pretraining\ncorpora. We discuss FACT KB’s compatibility with\ndifferent LM initializations in Section §5.2. We\nassign corpus size parameter N = 1e5, masking\nprobability p= 0.15, and knowledge walk length\nK= 5 in the experiments, while we discuss the\neffect of corpus size and knowledge walk length\nin Appendix 5.4. We use a learning rate of 2e−5\nfor pretraining, 1e−4 for fine-tuning, a batch size\nof 32, and the RAdam optimizer. Pretraining is\nconducted for 5 epochs and fine-tuning has 50 max-\nimum epochs with early stopping. More hyperpa-\nrameter settings are presented in Appendix 3.1.\nHyperparameters We propose to further pre-\ntrain LM checkpoints with three types of factuality\npretraining and fine-tune on factuality evaluation\ndatasets. We present hyperparameters for the pre-\ntraining and fine-tuning stage in Table 4. We mostly\nfollow the hyperparameters in Gururangan et al.\n(2020) for the pretraining stage. The default hy-\nperparameters on Huggingface Transformers are\nadopted if not included in Table 4.\n936\nModel All Data CNN/DM XSUM\nρ p-val r p-val ρ p-val r p-val ρ p-val r p-val\nQAGS .22 .00 .23 .00 .34 .00 .27 .00 .07 .05 .06 .09\nQUALS .22 .00 .19 .00 .31 .00 .27 .00 .14 .00 .07 .03\nDAE .17 .00 .20 .00 .27 .00 .22 .00 .03 .38 .33 .00\nROBERTA .35 .00 .41 .00 .43 .00 .31 .00 .23 .00 .15 .00\nFALSESUM .05 .00 .04 .11 .07 .05 .07 .03 .04 .28 .04 .35\nFALSESUM+ .22 .00 .26 .00 .27 .00 .33 .00 .24 .00 .27 .00\nSUMMAC .33 .00 .35 .00 .42 .00 .36 .00 .24 .00 .25 .00\nFACTCC .20 .00 .29 .00 .36 .00 .30 .00 .06 .07 .19 .00\nFACTCC+ .32 .00 .38 .00 .40 .00 .28 .00 .24 .00 .16 .00\nFACTGRAPH .35 .00 .42 .00 .45 .00 .34 .00 .30 .00 .49 .00\nFACTKB-WIKI .46 .00 .52 .00 .57 .00 .49 .00 .29 .00 .39 .00\nFACTKB-EVIDENCE .43 .00 .49 .00 .53 .00 .45 .00 .31 .00 .37 .00\nFACTKB-WALK .47 .00 .52 .00 .57 .00 .45 .00 .35 .00 .36 .00\nTable 3: Correlation of FACT KB with human judgments of factuality on the FRANK benchmark. Best performance\nis shown in bold. FACT KB has the highest correlation with human judgments across five of the six settings.\nPretraining Stage Fine-Tuning Stage\nHyperparameter Value Hyperparameter Value\nLEARNING RATE 2e-5 LEARNING RATE 1e-4\nWEIGHT DECAY 1e-5 WEIGHT DECAY 1e-5\nMAX EPOCHS 5 MAX EPOCHS 50\nBATCH SIZE 32 BATCH SIZE 32\nOPTIMIZER ADAM OPTIMIZER RADAM\nADAM EPSILON 1e-6\nADAM BETA 0.9, 0.98\nWARMUP RATIO 0.06\nEVIDENCE:N 1e5\nWALK:N 1e5\nWALK:K 5\nTable 4: Hyperparameter settings of FACT KB.\n3.2 Evaluation\nTo study the robustness of FACT KB, we perform\nboth in-domain and out-of-domain evaluation.\nIn-Domain Evaluation Since most research and\nresources on summarization and factuality are in\nthe news media domain, we leverage the FactCol-\nlect dataset (Ribeiro et al., 2022) and the FRANK\nbenchmark (Pagnoni et al., 2021) for in-domain\nfactuality evaluation. We evaluate FACT KB on\nthe held-out test set of the FactCollect dataset.\nFRANK (Pagnoni et al., 2021) is a factuality eval-\nuation benchmark with human judgments on the\nfactual consistency of model-generated summaries\ncollected across 9 summarization models along\nwith human annotations on the category of factual\nerrors. Following the FRANK benchmark guide-\nlines, we use two correlation measures (Pearson\n(Benesty et al., 2009) and Spearman (Myers and\nSirois, 2004)). We present more details about the\nFRANK benchmark in Appendix C. Following pre-\nvious work (Ribeiro et al., 2022), we trainFACT KB\non the FactCollect dataset without the FRANK sub-\nset for the FRANK evaluation.\nGeneralizable Factuality Evaluation Summa-\nrization systems are used in diverse domains in the\nreal world, including but not limited to news media\n(Liu et al., 2022c; Eyal et al., 2019; Li et al., 2016),\nsocial media (Syed et al., 2019; Kano et al., 2018;\nHe et al., 2020), and scientific literature (Cachola\net al., 2020; Lev et al., 2019). Consequently, factu-\nality metrics should also provide reliable factuality\nscores in the face of shifting domains. To study this,\nwe perform an out-of-domain evaluation using un-\nseen documents and summaries from the scientific\ndomain. To establish a test bed for generalizable\nfactuality evaluation, we make use of three datasets\nin the scientific literature domain:\n• CovidFact (Saakyan et al., 2021) collects claims\nfrom the r/COVID19 subreddit and verifies them\nagainst relevant scientific literature and Google\nsearch results, resulting in a binary classification\nsetting that is similar to the FactCollect dataset.\n• HealthVer (Sarrouti et al., 2021) consists of\nclaims sourced from TREC-COVID (V oorhees\net al., 2021) and verified against the CORD-19\n(Wang et al., 2020b) corpus. While HealthVer\noriginally follows a three-way classification set-\nting (SUPPORT , REFUTE , NOT ENOUGH INFOR -\nMATION ), we remove the examples in the \"NOT\nENOUGH INFORMATION \" category to evaluate\nmodels as they are trained on the binary classifi-\ncation setting (factual, non-factual).\n937\nModel CovidFact HealthVer SciFact\nBACC F1 BACC F1 BACC F1\nRANDOM 52.7 41 .3 46 .8 53 .0 49 .0 57 .5\nFACTCC 52.3 49 .2 51 .8 51 .9 42 .7 45 .9\nFACTCC+ 51.1 50 .5 49 .5 51 .6 48 .6 55 .2\nFACTGRAPH 57.6 53 .5 55 .1 24 .3 61 .0 42 .2\nFACTGRAPH-EDGE 50.6 48 .4 50 .6 53 .5 56 .7 68 .2\nFALSESUM 50.6 41 .6 56 .8 51 .2 45 .7 65 .3\nFALSESUM+ 50.1 41 .2 57 .3 51 .6 51 .9 65 .4\nSUMMAC 57.6 53 .4 52 .5 41 .2 59 .8 46 .9\nROBERTA 59.0 (±3.2) 46.4 (±4.3) 55.0 (±2.2) 50.0 (±3.9) 58.1 (±4.0) 71.3 (±3.5)\nFACTKB-WIKI 64.8(±0.3)∗ 54.4(±0.7)∗ 60.1(±0.4)∗ 71.6(±2.9)∗ 62.9 (±0.4)∗ 72.3 (±1.1)∗\nFACTKB-EVIDENCE 63.9 (±0.6)∗ 53.3 (±1.7) 59.0 (±1.0)∗ 70.8 (±0.9)∗ 61.4 (±0.5)∗ 74.1(±1.6)∗\nFACTKB-WALK 63.7 (±1.0)∗ 53.1 (±1.6) 58.5 (±0.5)∗ 68.7 (±1.7)∗ 63.1(±1.1)∗ 67.6 (±4.1)\nTable 5: Performance of FACT KB on out-of-domain scientific datasets. We report average performance and standard\ndeviation across 5 random seeds. Best performance is shown in bold, while * indicates statistical significance.\nFACT KB exhibits better generalization to new domains across all three datasets.\n• SciFact (Wadden et al., 2020) includes claims\nsourced from citation sentences in biomedical\nliterature and verified against the cited paper’s\nabstract. While SciFact uses three-way classifi-\ncation that includes \" NOT ENOUGH INFORMA -\nTION \", we similarly remove them in this work.\nWe leverage the well-organized version of the three\ndatasets in Wadden et al. (2022). 1 We train and\nvalidate FACT KB with the FactCollect dataset from\nthe news domain and evaluate on the test set of\nthese datasets for zero-shot transfer learning.\nBaselines We compare FACT KB with differ-\nent types of existing factuality evaluation mod-\nels: QAGS (Wang et al., 2020a), QUALS (Nan\net al., 2021), DAE (Goyal and Durrett, 2020),\nFalseSum (Utama et al., 2022), SummaC (Laban\net al., 2022), FactCC (Kry´sci´nski et al., 2020), and\nFactGraph (Ribeiro et al., 2022). Since training\ndata is a key factor in factuality evaluation mod-\nels and they are often used off-the-shelf, we in-\nclude factuality evaluation measures trained on\nboth synthetic data (QAGS, QUALS, DAE, Sum-\nmaC, FalseSum, FactCC) and human-annotated\ndata (RoBERTa, FalseSum+, FactCC+, FactGraph,\nFactGraph-edge). We follow the same train/dev/test\ndataset split and experiment settings so that the\nresults are directly comparable. We present more\ndetails about the baselines in Appendix D.\n4 Results\nIn-Domain Results We evaluate FACT KB and\nbaselines on the FactCollect dataset using the en-\n1Dataset statistics are presented in Table 8.\ntire held-out test data, the CNN/DM subset and\nthe XSUM (BBC) subset, and report balanced ac-\ncuracy scores and micro F1 scores. We run each\nmethod five times with different random seeds and\nreport the average performance as well as the stan-\ndard deviation. Table 2 demonstrates that FACT KB\nsignificantly (*) outperforms all baseline factuality\nevaluation methods by 3.8 BACC points on aver-\nage across the three dataset settings. This demon-\nstrates that the introduction of KBs and factuality\npretraining is beneficial for factuality evaluation.\nAmong the three factuality pretraining strategies,\nall of them outperform baseline models, suggesting\nthat FACT KB’s general methodology is compatible\nwith different types of KB utilization.\nHuman Correlation We evaluate FACT KB and\nbaselines on the FRANK benchmark to study how\nwell FACT KB correlates with human judgments.\nWe use the official script 2 to report the Pearson (ρ)\nand Spearman (r) correlation and p-values. Results\nin Table 3 show that classification-based metrics\n(FactCC, FactGraph, and FACT KB) generally out-\nperform QA-based metrics (QAGS and QUALS ).\nFACT KB significantly advances the state-of-the-art\non the FRANK benchmark, resulting in the im-\nprovement of 5-15 correlation points across mul-\ntiple settings. Our results show that FACT KB is\nhighly correlated with human judgments, making\nit a practical approach for evaluating the factual\nconsistency of generated news summaries.\nOut-of-Domain Results We evaluate FACT KB\nand existing factuality evaluation models on out-of-\n2https://github.com/artidoro/frank\n938\n/uni00000051/uni00000052/uni00000003/uni0000002e/uni00000025\n/uni00000035/uni00000052/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044\n/uni00000028/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000055/uni00000044\n/uni00000025/uni00000024/uni00000035/uni00000037\n/uni00000027/uni00000048/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044\n/uni00000024/uni0000002f/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000047/uni0000004c/uni00000056/uni00000057/uni0000004c/uni0000004f/uni00000035/uni00000052/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044\n/uni0000001b/uni00000018/uni00000011/uni00000016\n/uni0000001b/uni00000018/uni00000011/uni0000001b\n/uni0000001b/uni00000017/uni00000011/uni00000013\n/uni0000001b/uni0000001a/uni00000011/uni00000018\n/uni0000001b/uni00000018/uni00000011/uni00000018\n/uni0000001b/uni00000018/uni00000011/uni00000013\n/uni00000059/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044\n/uni0000003c/uni00000024/uni0000002a/uni00000032/uni0000003a/uni0000002c/uni0000002e/uni0000002c/uni00000026/uni00000033/uni00000031/uni00000048/uni00000057/uni00000024/uni00000057/uni00000052/uni00000050/uni0000004c/uni00000046/uni0000002e/uni0000002a/uni00000024/uni00000033/uni00000038/uni00000030/uni0000002f/uni00000036\n/uni0000001b/uni0000001c/uni00000011/uni00000017/uni0000001b/uni0000001a/uni00000011/uni00000019/uni0000001b/uni00000019/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001b/uni0000001c/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni0000001a\n/uni0000001b/uni0000001a/uni00000011/uni00000013/uni0000001b/uni00000019/uni00000011/uni0000001b/uni0000001b/uni00000017/uni00000011/uni00000017/uni0000001b/uni00000019/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni0000001b/uni0000001b/uni0000001a/uni00000011/uni00000016\n/uni0000001b/uni00000017/uni00000011/uni00000018/uni0000001b/uni00000018/uni00000011/uni00000017/uni0000001b/uni00000016/uni00000011/uni0000001b/uni0000001b/uni00000018/uni00000011/uni00000014/uni0000001b/uni00000017/uni00000011/uni0000001c/uni0000001b/uni00000018/uni00000011/uni0000001c\n/uni0000001b/uni0000001a/uni00000011/uni0000001c/uni0000001b/uni0000001a/uni00000011/uni0000001c/uni0000001b/uni00000019/uni00000011/uni00000019/uni0000001b/uni00000018/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000013/uni00000011/uni00000014\n/uni0000001b/uni0000001b/uni00000011/uni00000019/uni0000001b/uni0000001a/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni00000018/uni00000011/uni0000001b/uni0000001b/uni0000001b/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000017\n/uni0000001b/uni0000001a/uni00000011/uni00000015/uni0000001b/uni00000019/uni00000011/uni0000001a/uni0000001b/uni00000018/uni00000011/uni00000019/uni0000001b/uni00000018/uni00000011/uni00000016/uni0000001b/uni0000001a/uni00000011/uni00000017/uni0000001b/uni00000019/uni00000011/uni0000001b\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni0000005a/uni0000004c/uni0000004e/uni0000004c\n/uni0000003c/uni00000024/uni0000002a/uni00000032/uni0000003a/uni0000002c/uni0000002e/uni0000002c/uni00000026/uni00000033/uni00000031/uni00000048/uni00000057/uni00000024/uni00000057/uni00000052/uni00000050/uni0000004c/uni00000046/uni0000002e/uni0000002a/uni00000024/uni00000033/uni00000038/uni00000030/uni0000002f/uni00000036\n/uni0000001b/uni0000001c/uni00000011/uni00000017/uni0000001b/uni0000001b/uni00000011/uni00000016/uni0000001b/uni0000001a/uni00000011/uni00000019/uni0000001b/uni0000001b/uni00000011/uni00000013/uni0000001b/uni0000001b/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni00000019\n/uni0000001b/uni0000001a/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni00000019/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni00000019/uni00000011/uni0000001c\n/uni0000001b/uni00000018/uni00000011/uni0000001c/uni0000001b/uni00000017/uni00000011/uni00000014/uni0000001b/uni00000017/uni00000011/uni0000001c/uni0000001b/uni00000018/uni00000011/uni00000019/uni0000001b/uni00000016/uni00000011/uni00000016/uni0000001b/uni00000017/uni00000011/uni0000001c\n/uni0000001b/uni0000001c/uni00000011/uni00000013/uni0000001b/uni0000001c/uni00000011/uni00000013/uni0000001b/uni0000001c/uni00000011/uni00000013/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001b/uni0000001b/uni00000011/uni00000017/uni0000001b/uni0000001b/uni00000011/uni00000013\n/uni0000001b/uni0000001a/uni00000011/uni00000018/uni0000001b/uni0000001c/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni0000001c/uni0000001b/uni0000001b/uni00000011/uni00000019/uni0000001b/uni0000001a/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni0000001a\n/uni0000001b/uni00000019/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni00000019/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni0000001a/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni0000001c\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni00000048/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni0000003c/uni00000024/uni0000002a/uni00000032/uni0000003a/uni0000002c/uni0000002e/uni0000002c/uni00000026/uni00000033/uni00000031/uni00000048/uni00000057/uni00000024/uni00000057/uni00000052/uni00000050/uni0000004c/uni00000046/uni0000002e/uni0000002a/uni00000024/uni00000033/uni00000038/uni00000030/uni0000002f/uni00000036\n/uni0000001b/uni0000001c/uni00000011/uni00000014/uni0000001b/uni0000001a/uni00000011/uni0000001b/uni0000001b/uni0000001a/uni00000011/uni00000017/uni0000001b/uni0000001a/uni00000011/uni00000017/uni0000001b/uni00000019/uni00000011/uni00000019/uni0000001b/uni0000001b/uni00000011/uni00000017\n/uni0000001b/uni00000018/uni00000011/uni00000018/uni0000001b/uni0000001a/uni00000011/uni00000013/uni0000001b/uni00000019/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni00000015\n/uni0000001b/uni00000016/uni00000011/uni0000001c/uni0000001b/uni00000017/uni00000011/uni00000016/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni00000018/uni00000011/uni00000014/uni0000001b/uni00000017/uni00000011/uni0000001c/uni0000001b/uni00000017/uni00000011/uni0000001c\n/uni0000001b/uni0000001a/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni0000001a/uni0000001b/uni0000001b/uni00000011/uni00000015/uni0000001b/uni0000001a/uni00000011/uni00000013/uni0000001b/uni0000001c/uni00000011/uni00000014/uni0000001b/uni0000001c/uni00000011/uni00000015\n/uni0000001b/uni0000001a/uni00000011/uni0000001a/uni0000001b/uni0000001a/uni00000011/uni00000013/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni0000001a/uni00000011/uni00000019/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni0000001a/uni00000011/uni00000014\n/uni0000001b/uni00000019/uni00000011/uni00000015/uni0000001b/uni00000019/uni00000011/uni00000014/uni0000001b/uni00000019/uni00000011/uni00000017/uni0000001b/uni00000019/uni00000011/uni00000016/uni0000001b/uni00000019/uni00000011/uni00000019/uni0000001b/uni00000019/uni00000011/uni0000001a\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni0000005a/uni00000044/uni0000004f/uni0000004e\n/uni00000017\n/uni00000016\n/uni00000015\n/uni00000014\n/uni00000013\n/uni00000014\n/uni00000015\n/uni00000016\n/uni00000017\nFigure 3: Compatibility of FACT KB across various LMs and KBs. We report BACC scores of different setups on\nthe FactCollect dataset. FACT KB is a general method compatible with various LM and KB settings.\n/uni00000056/uni00000048/uni00000050/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000046/uni00000003/uni00000049/uni00000055/uni00000044/uni00000050/uni00000048/uni00000047/uni0000004c/uni00000056/uni00000046/uni00000052/uni00000058/uni00000055/uni00000056/uni00000048/uni00000046/uni00000052/uni00000051/uni00000057/uni00000048/uni00000051/uni00000057/uni00000003/uni00000059/uni00000048/uni00000055/uni0000004c/uni00000049/uni0000004c/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017/uni00000033/uni00000048/uni00000044/uni00000055/uni00000056/uni00000052/uni00000051/uni00000003/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000025/uni00000028/uni00000035/uni00000037/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002a/uni00000055/uni00000044/uni00000053/uni0000004b\n/uni00000034/uni00000024/uni0000002a/uni00000036\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni0000005a/uni00000044/uni0000004f/uni0000004e\n/uni00000027/uni00000024/uni00000028\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni0000005a/uni0000004c/uni0000004e/uni0000004c\n/uni00000029/uni00000044/uni00000046/uni00000057/uni00000026/uni00000026\n/uni00000029/uni00000044/uni00000046/uni00000057/uni0000002e/uni00000025/uni00000010/uni00000048/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\nFigure 4: Correlation of FACT KB and baselines with hu-\nman judgments across error categories. FACT KB shows\nsignificant improvement in capturing semantic frame\nerrors and has slightly better or on-par performance on\ndiscourse and content verifiability errors.\ndomain scientific literature datasets in a zero-shot\nmanner. Results are presented in Table 5, which\ndemonstrate that while existing factuality evalua-\ntion models previously achieve good performance\nin the in-domain setting, they exhibit severe perfor-\nmance drops on the three out-of-domain datasets,\nperforming only slightly better than random factu-\nality scores (RANDOM ). This suggests that existing\napproaches are not generalizable to other domains,\nlimiting their applicability. On the contrary, FAC-\nTKB significantly (*) outperforms existing factual-\nity metrics by 4.1 BACC points on average across\nthe three out-of-domain datasets. Our results sug-\ngest that the factuality pretraining strategies enable\nFACT KB to better represent facts (entities and re-\nlations) in a new domain, making the factuality\nevaluation model more robust to shifting domains.\n5 Analysis and Discussion\n5.1 Where did F ACT KB Improve?\nTo better understand FACT KB’s improvement over\nexisting approaches, we leverage the factual error\ntypology in the FRANK benchmark (Pagnoni et al.,\n2021) and examine FACT KB’s performance on the\nthree error categories: semantic frame, discourse,\nand content verifiability errors. Using the official\nscript in the FRANK benchmark, we remove each\ncategory of errors and report changes in correlation\nscores. Higher variation indicates a greater influ-\nence on a model’s ability to handle a certain type\nof error. Figure 4 demonstrates that FACT KB is\nsignificantly better at identifying semantic frame\nerrors, which focus on entities and relations. This\nindicates that our KB-based factuality pretraining\nstrategies successfully result in a better understand-\ning of the facts regarding entities and relations.\nFACT KB also has good performance in other cat-\negories, resulting in a factuality evaluation model\nthat captures diverse types of errors and advances\nthe state-of-the-art across the board. We conduct\nfurther qualitative analysis in Appendix B.\n5.2 KB and LM Compatibility\nFACT KB uses pretrained LMs for initialization,\nleverages external KBs for factuality pretraining,\nand trains on factuality evaluation datasets to result\nin a factuality metric. Our general methodology to\nleverage knowledge bases as fact teachers for gen-\neralizable factuality evaluation could work with dif-\nferent LM and KB combinations. To study whether\nout approach works across different settings, we ap-\nply the FACT KB methodology to six different LMs\n(RoBERTa, Electra, BART, DeBERTa, ALBERT,\nand distilRoBERTa) and pretrain the LMs on fac-\ntuality pretraining corpora constructed based on\nsix different KBs (YAGO, Wikidata, ConceptNet,\nAtomic, KGAP, and UMLS). For each combination,\nwe initialize a particular LM and pretrain it using\nthe proposed three pretraining strategies based on a\nparticular KB. For each setting, we evaluate the re-\nsulting model using the FactCollect dataset and re-\nport the BACC scores. We present the performance\n939\nMetric Pearson Spearman Usage Steps\nQAGS .22 .23 1) extract answer candidates 2) generate the questions 3) answer the\ngenerated questions 4) compare the answers to obtain the QAGS\nfactuality score\nQUALS .22 .19 1) generating question and answer pairs from summaries 2) filter the\ngenerated question and answer for high-quality pairs 3) evaluate the\ngenerated question and answer pairs using the source document as\ninput, compute QUALS scores for each summary\nDAE .17 .20 1) preprocess summaries and documents with dependency parsing 2)\nrun the pretrained model to get DAE scores\nFACTCC .20 .29 1) run the pretrained model to get FactCC scores\nFACTGRAPH .35 .42 1) build abstract meaning representation graphs 2) run the pretrained\nmodel to get FactGraph scores\nFACTKB .47 .52 1) run the pretrained model to get FACTKB scores\nTable 6: Usage steps of factuality metrics and their performance on the FRANK benchmark. FACT KB (WALK)\npresents a state-of-the-art factuality metric with minimum hassle when evaluating new summaries and articles.\nof different settings in Figure 3, which illustrates\nthat regardless of which LM and KB, FACT KB\ngenerally results in improved factual error detec-\ntion capabilities compared to the vanilla LM check-\npoints without factuality pretraining. In addition,\ncertain LMs (RoBERTa and DeBERTa) and KBs\n(YAGO, KGAP, and UMLS) are better than oth-\ners, suggesting that the choice of the base LM and\nexternal KB warrants further research. Our results\ndemonstrate that FACT KB is a general pretraining\napproach that can be applied to various LM-KB\ncombinations to improve fact representations and\ndevelop better factuality evaluation models.\n5.3 Simplicity Study\nWhile existing factuality evaluation approaches re-\nquire additional processing (such as computing the\ndependency structure (Goyal and Durrett, 2020)\nand AMR graphs (Ribeiro et al., 2022) or running\nmultiple iterations of question generation (Fabbri\net al., 2022)) in the face of new data, FACT KB re-\nquires no preprocessing and only uses a fine-tuned\nRoBERTa for sequence classification. We summa-\nrize the steps involved in using existing approaches\nand their performance on the FRANK benchmark\nin Table 6, which demonstrates that FACT KB not\nonly has state-of-the-art performance but is also a\nlightweight and simple factuality evaluation model.\n5.4 Parameter Analysis\nCorpus size. For evidence extraction and knowl-\nedge walk, the pretraining corpus size N is con-\ntrollable and governs the amount of information\ntowards augmenting FACT KB’s ability towards fac-\ntual errors regarding entities and relations. While\nwe adopted N = 1e5 in the main experiments,\nwe further explore the effect of factuality pretrain-\ning corpus size in Figure 5. It is illustrated that\nN = 1e4 or N = 1e5 are generally desirable set-\ntings, while factuality pretraining with too large\nNs might be counterproductive. This could in part\nbe attributed to catastrophic forgetting (Ramasesh\net al., 2021), which warrants further research.\nPretraining epoch. FACT KB further pretrains\nLM checkpoints on the three factuality pretrain-\ning corpora, while the training epoch governs the\nintensity of such exercises. We adopted 5 epochs\nof continued pretraining in the main experiments,\nwhile we further explore the effect of pretraining\nepochs in Figure 5. it is demonstrated that 1 to 10\nepochs are generally desirable while exercising too\nmuch might be counterproductive.\nKnowledge walk length. An important aspect\nof the knowledge walk factuality pretraining is the\ngenerated walk length K, which governs the de-\ngree of compositionality in the pretraining corpus.\nWhile we adopted K= 5in the main experiments,\nwe further explore the effect of Kin Figure 5. It\nis illustrated that K= 5performs best by provid-\ning a moderate amount of compositionality in the\nfactuality pretraining corpora.\n6 Related Work\nFactuality Evaluation Recent advances in text\nsummarization have presented models and systems\nthat are capable of generating increasingly fluent,\ncontrollable, and informative summaries of docu-\nments (Liu and Lapata, 2019; Balachandran et al.,\n2021; Meng et al., 2022; Tang et al., 2022; Gold-\nsack et al., 2022; Peng et al., 2021; Aharoni et al.,\n940\n2023; Liu et al., 2022d; Rothe et al., 2021; Narayan\net al., 2021; Bhattacharjee et al., 2023; Chen et al.,\n2023b; He et al., 2023; Liu et al., 2023b; Chen\net al., 2023a). However, they suffer from hallucina-\ntion and might not be factually faithful towards the\nsource document (Cao et al., 2018; Pagnoni et al.,\n2021; Balachandran et al., 2022; Tang et al., 2023;\nLiu et al., 2023a; Luo et al., 2023), leading to in-\ncreased research in factuality evaluation. QA-based\napproaches (Wang et al., 2020a; Nan et al., 2021;\nScialom et al., 2021; Fabbri et al., 2022) attempt\nto generate and answer questions based on sum-\nmaries and documents and judge the factuality by\ncomparing answers. Later approaches are generally\nentailment-based (Kry´sci´nski et al., 2020; Goyal\nand Durrett, 2020, 2021; Laban et al., 2022; Ribeiro\net al., 2022), proposing to classify (summary, docu-\nment) pairs into FACTUAL or NON -FACTUAL labels.\nAmong them, FactCC (Kry´sci´nski et al., 2020) is\none of the first entailment-based metrics and is\ntrained on synthetic data; DAE (Goyal and Durrett,\n2020, 2021) proposes to leverage the dependency\nstructure of summaries and documents; FactGraph\n(Ribeiro et al., 2022) builds abstract meaning repre-\nsentation graphs and adopts graph neural networks\nfor joint representation learning along the textual\ncontent. In addition, hypothesis re-ranking (Gar-\nneau and Lamontagne, 2021), counterfactual esti-\nmation (Xie et al., 2021), NLI models (Utama et al.,\n2022), phrase-level localization (Takatsuka et al.,\n2022), and weighting facts in the source document\n(Xu et al., 2020) were also explored in factuality\nevaluation. Moving beyond a binary concept of fac-\ntuality, FRANK (Pagnoni et al., 2021) promotes a\nfine-grained understanding of factuality and pro-\nposes a typology of factuality errors. Inspired by\nits analysis that semantic frame errors, errors re-\ngarding entities and relations, are a major source of\nfactuality errors yet under-explored by existing fac-\ntuality metrics, we propose FACT KB to leverage\nexternal KBs for factuality pretraining and help en-\nforce better factuality towards entities and relations\ndiscussed in summaries and documents.\nKnowledge Bases in NLP Knowledge base is a\nstandard format for structured knowledge represen-\ntation. One application of KBs in NLP is to inject\nknowledge and augment LMs, where different ap-\nproaches focused aspects such as pretraining (Chen\net al., 2020; Agarwal et al., 2021; Rosset et al.,\n2020; Li et al., 2022), document graphs (Hu et al.,\n2021; Zhang et al., 2022a), KB structure (Yasunaga\n1k 10k 100k 1m 10m\ncorpus size\n0.80\n0.85\n0.90\n0.95BACC\nwalk\nevidence\n1k 10k 100k 1m 10m\ncorpus size\n0.80\n0.85\n0.90\n0.95F1\nwalk\nevidence\n1 5 10 50 100\npre-training epoch\n0.80\n0.85\n0.90\n0.95BACC\nwalk\nevidence\nwiki\n1 5 10 50 100\npre-training epoch\n0.80\n0.85\n0.90\n0.95F1\nwalk\nevidence\nwiki\n1 2 5 10 50\nwalk length\n0.80\n0.85\n0.90\n0.95BACC\nwalk\n1 2 5 10 50\nwalk length\n0.80\n0.85\n0.90\n0.95F1\nwalk\nFigure 5: Parameter analysis of pretraining corpus size,\nepoch, and knowledge walk length.\net al., 2021; Zhang et al., 2022b), and long docu-\nments (Feng et al., 2023). KB-enhanced approaches\nalso advanced numerous NLP tasks, ranging from\nquestion answering (Mitra et al., 2022; Bosselut\net al., 2021; Oguz et al., 2022; Feng et al., 2022;\nHeo et al., 2022; Ma et al., 2022), text generation\n(Rony et al., 2022; Dognin et al., 2021; Yu et al.,\n2021), and commonsense reasoning (Kim et al.,\n2022; Jung et al., 2022; Amayuelas et al., 2021;\nLiu et al., 2022a). In this work, we tap into KBs’\nnature as high-quality reservoirs of factual informa-\ntion and construct factuality pretraining objectives\nto augment factuality evaluation.\n7 Conclusion\nWe propose FACT KB, a simple and novel approach\nto factuality evaluation using language models pre-\ntrained on facts from external KBs to improve en-\ntity and relation representations. Specifically, we\nleverage KBs to construct three factuality pretrain-\ning objectives: entity wiki, evidence extraction, and\nknowledge walk. FACT KB pretrains an LM using\nthe three objectives and fine-tunes the resulting\nmodel on factuality evaluation datasets. Extensive\nexperiments demonstrate that FACT KB advances\nthe state-of-the-art in both in-domain and out-of-\ndomain factuality evaluation, better correlates with\nhuman factuality annotations, and better detects se-\nmantic frame errors. FACT KB presents an easy-to-\nuse and generalizable factuality metric, facilitating\nresearch on factually-consistent summarization.\n941\nLimitations\nLM and KB Selection While Section 5.2 offers\nempirical evidence that FACT KB is compatible\nwith 6 language models and 6 external knowledge\nbases, it remains unclear upfront which LM and\nKB combination would be most desirable. While\nempirical performance could be a good guide, there\nare several unaddressed possibilities: For language\nmodels, it is possible to leverage an ensemble of\nFACT KBs seeded with different LM checkpoints\nand architectures. This might result in better fac-\ntuality evaluation, but would also dramatically in-\ncrease the computation costs when evaluating on\nnew data. For knowledge bases, it is possible to\nleverage domain expertise and select an external\nknowledge base that would be most helpful for the\ndomain adaptation of factuality evaluation. It is\nalso possible to leverage a combination of existing\nknowledge bases for FACT KB’s factuality pretrain-\ning, while the specific combination and how to\napply different factuality pretraining to different\nKBs are hard to determine. All in all, FACT KB\npresents a general KB-enhanced factuality metric\nwith numerous possibilities, while we leave some\nof these considerations to future work.\nFACT KB training is not end-to-end. FACT KB\nhas a two-step training process: pretraining with\nKB-based factuality pretraining and fine-tuning on\nfactuality evaluation datasets. This creates several\nlimitations, among which is the difficulty of hy-\nperparameter tuning. Appendix 3.1 presents the\nstudy of hyperparameters in the factuality pretrain-\ning stage, which demonstrates that FACT KB works\nbest with a moderate but not excessive amount of\nfactuality pretraining. This reliance on certain hy-\nperparameter configurations is further complicated\nby more hyperparameter choices in the fine-tuning\nstage. While the current hyperparameter setting\nin Appendix 3.1 achieves state-of-the-art empiri-\ncal performance, we acknowledge the difficulty in\nFACT KB hyperparameter tuning.\nOut-of-Domain Factuality Evaluation. An im-\nportant focus of this work is out-of-domain factual-\nity evaluation: Summarization systems face input\ndocuments from varying domains, which requires\nfactuality metrics to also generalize to different\ndocument domains. Existing metrics struggle with\nsemantic frame errors and such struggle is exacer-\nbated by the domain shift of entities and relations,\nwhile FACT KB offers a stronger and more gener-\nalizable factuality metric. However, in this work,\nwe mainly focused on the additional domain of\nscientific literature, while other potential domains\nremain underexplored such as social media (Syed\net al., 2019; Kano et al., 2018; He et al., 2020). We\nleave it to future work the exploration of FACT KB\nand existing factuality metrics on more document\ndomains that are present in summarization systems.\nTradeoff between Performance and Granularity\nExisting approaches (Kry´sci´nski et al., 2020; Takat-\nsuka et al., 2022) struggle with semantic frame er-\nrors and involve heavy preprocessing, while they\nprovide fine-grained analysis and specific localiza-\ntion of summarization errors. FACT KB achieves\nsignificantly better factuality evaluation results and\nis easier to use while lacking the ability of error\nlocalization. We argue that this tradeoff should be\nconsidered with the use case in mind: for LLM\nevaluation, it is better to have an accurate metric\nfor benchmarking efforts and an efficient metric for\nhandling large-scale LM generation. As a result,\nFACT KB provides a valuable tool for factuality\nevaluation and LLM research.\nEthics Statement\nLM and KB Bias FACT KB is initialized with\npretrained language model checkpoints and lever-\nages knowledge-base-based factuality pretraining.\nConsequently, FACT KB might pick up the biases\nof the adopted language models (Liang et al., 2021;\nNadeem et al., 2021; Shaikh et al., 2023; Tan and\nCelis, 2019) and knowledge bases (Fisher et al.,\n2020; Mehrabi et al., 2021). As a result, FACT KB\nmight leverage these biases in judging the factu-\nality of summaries, further reinforcing the bias in\ntext summarization systems. We leave it to future\nwork on understanding and mitigating the bias of\nfactuality metrics.\nMisuse Potential FACT KB leverages high-\nquality and factual knowledge bases to generate fac-\ntuality pretraining corpora and augment LM’s abil-\nity to stay factual with respect to entities and rela-\ntions discussed in the summary and document. On\nthe contrary, if non-factual and misleading knowl-\nedge is leveraged for the three factuality pretraining\nstrategies, it might jeopardize the factuality ofFAC-\nTKB and make it insensitive to misinformation and\nfalsehoods in summaries and documents. As a re-\nsult, we encourage the responsible use of FACT KB\nand the factuality pretraining methodology.\n942\nAcknowledgements\nWe thank the reviewers, the area chair, members of\nTsvetshop, and the UW NLP Group for their feed-\nback. This research was supported by This research\nis supported in part by the Office of the Director\nof National Intelligence (ODNI), Intelligence Ad-\nvanced Research Projects Activity (IARPA), via the\nHIATUS Program contract #2022-22072200004.\nThis material is also funded by the DARPA Grant\nunder Contract No. HR001120C0124. We also\ngratefully acknowledge support from NSF CA-\nREER Grant No. IIS2142739 and the Alfred\nP. Sloan Foundation Fellowship. The views and\nconclusions contained herein are those of the au-\nthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of ODNI, IARPA, or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein.\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554–3565.\nRoee Aharoni, Shashi Narayan, Joshua Maynez,\nJonathan Herzig, Elizabeth Clark, and Mirella La-\npata. 2023. Multilingual summarization with factual\nconsistency evaluation. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n3562–3591.\nAlfonso Amayuelas, Shuai Zhang, Xi Susie Rao, and\nCe Zhang. 2021. Neural methods for logical reason-\ning over knowledge graphs. In International Confer-\nence on Learning Representations.\nVidhisha Balachandran, Hannaneh Hajishirzi, William\nCohen, and Yulia Tsvetkov. 2022. Correcting diverse\nfactual errors in abstractive summarization via post-\nediting and language model infilling. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing.\nVidhisha Balachandran, Artidoro Pagnoni, Jay Yoon\nLee, Dheeraj Rajagopal, Jaime Carbonell, and Yu-\nlia Tsvetkov. 2021. StructSum: Summarization via\nstructured representations. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 2575–2585.\nJacob Benesty, Jingdong Chen, Yiteng Huang, and Is-\nrael Cohen. 2009. Pearson correlation coefficient.\nIn Noise reduction in speech processing, pages 1–4.\nSpringer.\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ah-\nmad, Yuan-Fang Li, Yong-Bin Kang, and Rifat\nShahriyar. 2023. CrossSum: Beyond English-centric\ncross-lingual summarization for 1,500+ language\npairs. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 2541–2564.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. \" O’Reilly Media,\nInc.\".\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in neural information pro-\ncessing systems, 26.\nAntoine Bosselut, Ronan Le Bras, and Yejin Choi. 2021.\nDynamic neuro-symbolic knowledge graph construc-\ntion for zero-shot commonsense question answering.\nIn Proceedings of the 35th AAAI Conference on Arti-\nficial Intelligence (AAAI).\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel S\nWeld. 2020. Tldr: Extreme summarization of sci-\nentific documents. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4766–4777.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\nFaithful to the original: Fact aware neural abstractive\nsummarization. In thirty-second AAAI conference on\nartificial intelligence.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang\nWang. 2020. Kgpt: Knowledge-grounded pre-\ntraining for data-to-text generation. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 8635–\n8648.\nXiuying Chen, Guodong Long, Chongyang Tao,\nMingzhe Li, Xin Gao, Chengqi Zhang, and Xian-\ngliang Zhang. 2023a. Improving the robustness of\nsummarization systems with dual augmentation. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 6846–6857.\nYulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chen-\nguang Zhu, Michael Zeng, and Yue Zhang. 2023b.\nUniSumm and SummZoo: Unified model and diverse\nbenchmark for few-shot summarization. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12833–12855.\n943\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nPierre Dognin, Inkit Padhi, Igor Melnyk, and Payel Das.\n2021. ReGen: Reinforcement learning for text and\nknowledge base generation using pretrained language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1084–1099.\nHady Elsahar and Matthias Gallé. 2019. To annotate or\nnot? predicting performance drop under domain shift.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2163–2173.\nMatan Eyal, Tal Baumel, and Michael Elhadad. 2019.\nQuestion answering as an automatic evaluation met-\nric for news article summarization. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3938–3948.\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. QAFactEval: Improved QA-\nbased factual consistency evaluation for summariza-\ntion. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 2587–2601.\nWilliam Falcon and The PyTorch Lightning team. 2019.\nPyTorch Lightning.\nYair Feldman and Ran El-Yaniv. 2019. Multi-hop para-\ngraph retrieval for open-domain question answering.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 2296–\n2309.\nShangbin Feng, Zilong Chen, Wenqian Zhang, Qingyao\nLi, Qinghua Zheng, Xiaojun Chang, and Minnan Luo.\n2021. Kgap: Knowledge graph augmented political\nperspective detection in news media. arXiv preprint\narXiv:2108.03861.\nShangbin Feng, Zhaoxuan Tan, Wenqian Zhang, Zhenyu\nLei, and Yulia Tsvetkov. 2023. KALM: Knowledge-\naware integration of local, document, and global con-\ntexts for long document understanding. In Proceed-\nings of ACL 2023, pages 2116–2138.\nYue Feng, Zhen Han, Mingming Sun, and Ping Li.\n2022. Multi-hop open-domain question answering\nover structured and unstructured knowledge. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 151–156, Seattle, United States.\nJoseph Fisher, Arpit Mittal, Dave Palfrey, and Chris-\ntos Christodoulopoulos. 2020. Debiasing knowledge\ngraph embeddings. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7332–7345.\nNicolas Garneau and Luc Lamontagne. 2021. Trainable\nranking models to evaluate the semantic accuracy\nof data-to-text neural generator. In Proceedings of\nthe 2nd Workshop on Evaluation and Comparison of\nNLP Systems, pages 51–61.\nTomas Goldsack, Zhihao Zhang, Chenghua Lin, and\nCarolina Scarton. 2022. Making science simple: Cor-\npora for the lay summarisation of scientific literature.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing.\nTanya Goyal and Greg Durrett. 2020. Evaluating factu-\nality in generation with dependency-level entailment.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 3592–3603.\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360.\nCharles R Harris, K Jarrod Millman, Stéfan J Van\nDer Walt, Ralf Gommers, Pauli Virtanen, David Cour-\nnapeau, Eric Wieser, Julian Taylor, Sebastian Berg,\nNathaniel J Smith, et al. 2020. Array programming\nwith numpy. Nature, 585(7825):357–362.\nPengcheng He, Baolin Peng, Song Wang, Yang Liu,\nRuochen Xu, Hany Hassan, Yu Shi, Chenguang Zhu,\nWayne Xiong, Michael Zeng, Jianfeng Gao, and Xue-\ndong Huang. 2023. Z-code++: A pre-trained lan-\nguage model optimized for abstractive summariza-\ntion. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 5095–5112.\nRuifang He, Liangliang Zhao, and Huanyu Liu. 2020.\nTWEETSUM: Event oriented social summarization\ndataset. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n5731–5736.\nYu-Jung Heo, Eun-Sol Kim, Woo Suk Choi, and\nByoung-Tak Zhang. 2022. Hypergraph trans-\nformer: Weakly-supervised multi-hop reasoning for\nknowledge-based visual question answering. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 373–390.\n944\nLinmei Hu, Tianchi Yang, Luhao Zhang, Wanjun Zhong,\nDuyu Tang, Chuan Shi, Nan Duan, and Ming Zhou.\n2021. Compare to the knowledge: Graph neural fake\nnews detection with external knowledge. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 754–763.\nYong-Ho Jung, Jun-Hyung Park, Joon-Young Choi,\nMingyu Lee, Junho Kim, Kang-Min Kim, and\nSangKeun Lee. 2022. Learning from missing rela-\ntions: Contrastive learning with commonsense knowl-\nedge graphs for commonsense inference. In Findings\nof the Association for Computational Linguistics:\nACL 2022, pages 1514–1523.\nAmbedkar Kanapala, Sukomal Pal, and Rajendra Pa-\nmula. 2019. Text summarization from legal doc-\numents: a survey. Artificial Intelligence Review ,\n51(3):371–402.\nRyuji Kano, Yasuhide Miura, Motoki Taniguchi, Yan-\nYing Chen, Francine Chen, and Tomoko Ohkuma.\n2018. Harnessing popularity in social media for\nextractive summarization of online conversations.\nIn Proceedings of the 2018 Conference on Empir-\nical Methods in Natural Language Processing, pages\n1139–1145.\nYu Jin Kim, Beong-woo Kwak, Youngwook Kim,\nReinald Kim Amplayo, Seung-won Hwang, and Jiny-\noung Yeo. 2022. Modularized transfer learning with\nmultiple knowledge graphs for zero-shot common-\nsense reasoning. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2244–2257.\nWojciech Kry´sci´nski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-Visiting NLI-\nbased Models for Inconsistency Detection in Summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nTimothée Lacroix, Guillaume Obozinski, and Nicolas\nUsunier. 2019. Tensor decompositions for temporal\nknowledge base completion. In International Con-\nference on Learning Representations.\nEgoitz Laparra, Steven Bethard, and Timothy A Miller.\n2020. Rethinking domain adaptation for machine\nlearning over clinical language. JAMIA open ,\n3(2):146–150.\nGuy Lev, Michal Shmueli-Scheuer, Jonathan Herzig,\nAchiya Jerbi, and David Konopnicki. 2019. Talk-\nSumm: A dataset and scalable annotation method for\nscientific paper summarization based on conference\ntalks. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2125–2131.\nChen Li, Zhongyu Wei, Yang Liu, Yang Jin, and Fei\nHuang. 2016. Using relevant public posts to en-\nhance news article summarization. In Proceedings of\nCOLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers, pages\n557–566.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Chengjie Sun,\nBingquan Liu, Zhenzhou Ji, Xin Jiang, and Qun Liu.\n2022. Pre-training language models with determin-\nistic factual knowledge. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understand-\ning and mitigating social biases in language models.\nIn International Conference on Machine Learning,\npages 6565–6576. PMLR.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022a. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3154–3169.\nXiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong\nQiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, and Jie\nTang. 2022b. Mask and reason: Pre-training knowl-\nedge graph transformers for complex logical queries.\nIn Proceedings of the 28th ACM SIGKDD Confer-\nence on Knowledge Discovery and Data Mining ,\npages 1120–1130.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740.\nYang Liu, Chenguang Zhu, and Michael Zeng. 2022c.\nEnd-to-end segmentation-based news summarization.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022, pages 544–554.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nYixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Hal-\nfaker, Dragomir Radev, and Ahmed Hassan Awadal-\nlah. 2023a. On improving summarization factual\nconsistency from natural language feedback. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 15144–15161.\n945\nYixin Liu, Alex Fabbri, Pengfei Liu, Yilun Zhao, Liny-\nong Nan, Ruilin Han, Simeng Han, Shafiq Joty,\nChien-Sheng Wu, Caiming Xiong, and Dragomir\nRadev. 2023b. Revisiting the gold standard: Ground-\ning summarization evaluation with robust human\nevaluation. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4140–4170.\nYongtai Liu, Joshua Maynez, Gonçalo Simões, and\nShashi Narayan. 2022d. Data augmentation for low-\nresource dialogue summarization. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, pages 703–710.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\nKaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,\nand Jianfeng Gao. 2022. Open domain question\nanswering with a unified knowledge interface. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1605–1620, Dublin, Ireland.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919.\nNinareh Mehrabi, Pei Zhou, Fred Morstatter, Jay Pu-\njara, Xiang Ren, and Aram Galstyan. 2021. Lawyers\nare dishonest? quantifying representational harms in\ncommonsense knowledge resources. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 5016–5033.\nKevin Meng, David Bau, Alex J Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in gpt. In Advances in Neural Information\nProcessing Systems.\nSayantan Mitra, Roshni Ramnani, and Shubhashis Sen-\ngupta. 2022. Constraint-based multi-hop question\nanswering with knowledge graph. In Proceedings of\nthe 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies: Industry Track, pages\n280–288.\nLeann Myers and Maria J Sirois. 2004. Spearman cor-\nrelation coefficients, differences between. Encyclo-\npedia of statistical sciences, 12.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoset: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371.\nFeng Nan, Cicero dos Santos, Henghui Zhu, Patrick\nNg, Kathleen Mckeown, Ramesh Nallapati, Dejiao\nZhang, Zhiguo Wang, Andrew O Arnold, and Bing\nXiang. 2021. Improving factual consistency of ab-\nstractive summarization via question answering. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 6881–\n6894.\nShashi Narayan, Shay Cohen, and Maria Lapata. 2018.\nDon’t give me the details, just the summary! topic-\naware convolutional neural networks for extreme\nsummarization. In 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n1797–1807. Association for Computational Linguis-\ntics.\nShashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo\nSimões, Vitaly Nikolaev, and Ryan McDonald. 2021.\nPlanning with learned entity prompts for abstractive\nsummarization. Transactions of the Association for\nComputational Linguistics, 9:1475–1492.\nBarlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Scott Yih. 2022.\nUniK-QA: Unified representations of structured and\nunstructured knowledge for open-domain question\nanswering. In Findings of the Association for Compu-\ntational Linguistics: NAACL 2022, pages 1535–1546,\nSeattle, United States.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for fac-\ntuality metrics. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812–4829.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825–2830.\nThomas Pellissier Tanon, Gerhard Weikum, and Fabian\nSuchanek. 2020. Yago 4: A reason-able knowledge\nbase. In European Semantic Web Conference, pages\n583–596. Springer.\nXutan Peng, Yi Zheng, Chenghua Lin, and Advaith\nSiddharthan. 2021. Summarising historical text in\nmodern languages. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\n946\nfor Computational Linguistics: Main Volume, pages\n3123–3142.\nSeth Polsley, Pooja Jhunjhunwala, and Ruihong Huang.\n2016. Casesummarizer: a system for automated sum-\nmarization of legal texts. In Proceedings of COLING\n2016, the 26th international conference on Compu-\ntational Linguistics: System Demonstrations, pages\n258–262.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nVinay Venkatesh Ramasesh, Aitor Lewkowycz, and\nEthan Dyer. 2021. Effect of scale on catastrophic\nforgetting in neural networks. In International Con-\nference on Learning Representations.\nLeonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych,\nMarkus Dreyer, and Mohit Bansal. 2022. FactGraph:\nEvaluating factuality in summarization with semantic\ngraph representations. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3238–3253.\nMd Rashad Al Hasan Rony, Ricardo Usbeck, and Jens\nLehmann. 2022. DialoKG: Knowledge-structure\naware task-oriented dialogue generation. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2557–2571, Seattle, United\nStates.\nCorby Rosset, Chenyan Xiong, Minh Hieu Phan,\nXia Song, Paul Bennett, and Saurabh Tiwary.\n2020. Knowledge-aware language model pretraining.\nArXiv, abs/2007.00655.\nSascha Rothe, Joshua Maynez, and Shashi Narayan.\n2021. A thorough evaluation of task-specific pre-\ntraining for summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 140–145.\nArkadiy Saakyan, Tuhin Chakrabarty, and Smaranda\nMuresan. 2021. Covid-fact: Fact extraction and veri-\nfication of real-world claims on covid-19 pandemic.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n2116–2129.\nMourad Sarrouti, Asma Ben Abacha, Yassine M’rabet,\nand Dina Demner-Fushman. 2021. Evidence-based\nfact-checking of health-related claims. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3499–3512.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6594–6604.\nOmar Shaikh, Hongxin Zhang, William Held, Michael\nBernstein, and Diyi Yang. 2023. On second thought,\nlet’s not think step by step! bias and toxicity in zero-\nshot reasoning. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4454–4470.\nSiamak Shakeri, Cicero dos Santos, Henghui Zhu,\nPatrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nal-\nlapati, and Bing Xiang. 2020. End-to-end synthetic\ndata generation for domain adaptation of question\nanswering systems. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5445–5460.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-first AAAI conference on\nartificial intelligence.\nShahbaz Syed, Michael Völske, Nedim Lipka, Benno\nStein, Hinrich Schütze, and Martin Potthast. 2019.\nTowards summarization for social media - results of\nthe TL;DR challenge. In Proceedings of the 12th\nInternational Conference on Natural Language Gen-\neration, pages 523–528.\nMasato Takatsuka, Tetsunori Kobayashi, and Yoshihiko\nHayashi. 2022. Phrase-level localization of inconsis-\ntency errors in summarization by weak supervision.\nIn Proceedings of the 29th International Conference\non Computational Linguistics, pages 6151–6164.\nYi Chern Tan and L Elisa Celis. 2019. Assessing so-\ncial and intersectional biases in contextualized word\nrepresentations. Advances in Neural Information\nProcessing Systems, 32.\nLiyan Tang, Tanya Goyal, Alex Fabbri, Philippe La-\nban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscin-\nski, Justin Rousseau, and Greg Durrett. 2023. Un-\nderstanding factual errors in summarization: Errors,\nsummarizers, datasets, error detectors. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11626–11644.\nXiangru Tang, Arjun Nair, Borui Wang, Bingyao Wang,\nJai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz,\nYashar Mehdad, and Dragomir Radev. 2022. CON-\nFIT: Toward faithful dialogue summarization with\nlinguistically-informed contrastive fine-tuning. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5657–5668.\nPrasetya Utama, Joshua Bambrick, Nafise Moosavi,\nand Iryna Gurevych. 2022. Falsesum: Generating\ndocument-level NLI examples for recognizing fac-\ntual inconsistency in summarization. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2763–2776.\n947\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha Talukdar. 2019. Composition-based multi-\nrelational graph convolutional networks. In Interna-\ntional Conference on Learning Representations.\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTrec-covid: constructing a pandemic information re-\ntrieval test collection. In ACM SIGIR Forum, vol-\nume 54, pages 1–12. ACM New York, NY , USA.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or fiction: Verifying\nscientific claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534–7550.\nDavid Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan,\nIz Beltagy, and Hannaneh Hajishirzi. 2022. Mul-\ntiVerS: Improving scientific claim verification with\nweak supervision and full-document context. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 61–76.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020a.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Rus-\nsell Reas, Jiangjiang Yang, Doug Burdick, Darrin\nEide, Kathryn Funk, Yannis Katsis, Rodney Michael\nKinney, et al. 2020b. Cord-19: The covid-19 open\nresearch dataset. In Proceedings of the 1st Workshop\non NLP for COVID-19 at ACL 2020.\nWenya Wang and Sinno Pan. 2022. Deep inductive\nlogic reasoning for multi-hop reading comprehension.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4999–5009.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4602–4625.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020\nconference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nYuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, and\nBolin Ding. 2021. Factual consistency evaluation\nfor text summarization via counterfactual estimation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 100–110.\nXinnuo Xu, Ondˇrej Dušek, Jingyi Li, Verena Rieser, and\nIoannis Konstas. 2020. Fact-based content weighting\nfor evaluating abstractive summarisation. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 5071–5081.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren,\nXikun Zhang, Christopher D Manning, Percy S\nLiang, and Jure Leskovec. 2022. Deep bidirectional\nlanguage-knowledge graph pretraining. Advances in\nNeural Information Processing Systems, 35:37309–\n37323.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\nsoning with language models and knowledge graphs\nfor question answering. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 535–546.\nWenhao Yu, Meng Jiang, Zhiting Hu, Qingyun Wang,\nHeng Ji, and Nazneen Rajani. 2021. Knowledge-\nenriched natural language generation. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: Tutorial Abstracts,\npages 11–16.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nWenqian Zhang, Shangbin Feng, Zilong Chen, Zhenyu\nLei, Jundong Li, and Minnan Luo. 2022a. KCD:\nKnowledge walks and textual cues enhanced political\nperspective detection in news media. In Proceedings\nof NAACL 2022, pages 4129–4140.\nX Zhang, A Bosselut, M Yasunaga, H Ren, P Liang,\nC Manning, and J Leskovec. 2022b. Greaselm:\nGraph reasoning enhanced language models for ques-\ntion answering. In International Conference on Rep-\nresentation Learning (ICLR).\n948\nA Merging the three strategies\nWe also tried combining the three factuality pre-\ntraining strategies to obtain FACT KB- COMBINED .\nWe evaluate it on the FactCollect dataset and\npresent results in Table 7. It is demonstrated that\nFACT KB- COMBINED is not significantly better\nthan using a single factuality pretraining strategy,\nwhile we will make all versions of FACT KB pub-\nlicly available.\nB Qualitative Analysis\nWe present examples of (summary, article) pairs\nand their factuality scores in Table 9 and 10, where\nFACT KB is significantly closer to human judgment\nthan existing factuality metrics. It is demonstrated\nthat while existing factuality metrics are insensitive\nto major errors in entities and relations,FACT KB is\ncapable of identifying inconsistencies and enforc-\ning strict factuality standards.\nC Dataset Details\nWe present more details about the adopted datasets\nin Table 8. There might be minor differences in\ncertain numbers with the original dataset as a re-\nsult of data preprocessing. FRANK (Pagnoni et al.,\n2021) does not explicitly have binary labels such as\n{FACTUAL , NOT FACTUAL }. It also does not have a\ntraining set due to its nature as an evaluation bench-\nmark. HealthVer (Sarrouti et al., 2021) and SciFact\n(Wadden et al., 2020) originally hadNOT ENOUGH\nINFORMATION labels, while we removed such ex-\namples in the out-of-domain factuality evaluation\nto ensure their compatibility with FactCollect.\nD Baseline Details\nWe present more details about baseline factuality\nmetrics in the following:\n• BERTScore (Zhang et al., 2019) is a general\nmetric for text generation evaluation based on\npretrained BERT (Devlin et al., 2019).\n• QAGS (Wang et al., 2020a) is a QA-based factu-\nality metric, asking questions about summaries\nand articles while examining whether the answers\nare consistent.\n• QUALS (Nan et al., 2021) is a QA-based factual-\nity metric that uses QAGen (Shakeri et al., 2020)\nto generate both questions and answers from the\nsummary.\n• DAE (Goyal and Durrett, 2020) leverages the\ndependency structure of the summary and article\nto design a factuality metric.\n• SummaC (Laban et al., 2022) proposes to revisit\nand repurpose NLI models for detecting factual\ninconsistencies in text summarization.\n• FalseSum (Utama et al., 2022) augments NLI\ntraining data with controllable text generation for\nbetter factuality evaluation.\n• FactCC (Kry´sci´nski et al., 2020) is an\nentailment-based factuality metric trained on syn-\nthetic data evaluating factuality with binary clas-\nsification. FactCC+ is a variant of FactCC pro-\nviding explanations. FactCC+ is an enhanced\nversion trained with human-annotated data.\n• FactGraph (Ribeiro et al., 2022) is an\nentailment-based factuality metric based on\njointly analyzing the textual content and AMR\ngraphs of the summary and article. FactGraph-\nadapters is an enhanced version with pretrained\nadapters for both the text and graph modules.\nE LM and KB Details\nIn Section 5.2, we explored whether FACT KB\nis compatible with different language models\nand knowledge bases. For LMs, we used the\nROBERTA -BASE , GOOGLE /ELECTRA -BASE -\nDISCRIMINATOR , FACEBOOK /BART-BASE ,\nALBERT -BASE -V2, MICROSOFT /DEBERTA -V3-\nBASE , DISTILROBERTA -BASE LM checkpoints\non Huggingface Transformers. For the six KBs,\nwe used their organized versions: YAGO15k at\nLacroix et al. (2019), Wikidata5M at Wang et al.\n(2021), Atomic at West et al. (2022), ConceptNet\nat Zhang et al. (2022b), KGAP at Feng et al.\n(2021), and UMLS at Zhang et al. (2022b).\nF Statistical Significance Test Details\nWe use the student t-test for statistical significance\nanalysis throughout the paper. Specifically, the\nt-test calculator for 2 independent means 3 was\nadopted for the calculations. We use (*) to denote\nstatistical significance in Tables 2 and 5.\n3https://www.socscistatistics.com/tests/\nstudentttest/default2.aspx\n949\nModel All Data CNN/DM XSUM\nBACC F1 BACC F1 BACC F1\nFACTKB-WIKI 89.3 (±0.4) 89.5 (±0.5) 77.3 (±0.3) 88.2 (±0.6) 77.3 (±1.3) 91.8 (±1.2)\nFACTKB-EVIDENCE 89.4 (±0.2) 89.5 (±0.3) 77.7 (±1.4) 87.9 (±0.7) 76.8 (±1.9) 90.8 (±0.8)\nFACTKB-WALK 89.1 (±0.4) 89.3 (±0.5) 78.3 (±1.2) 87.7 (±0.4) 76.4 (±0.3) 90.4 (±1.4)\nFACTKB-COMBINED 89.0 89 .7 76 .0 88 .1 74 .2 89 .1\nTable 7: Performance of various FACT KB settings on the FactCollect dataset.\nDataset # Datapoint # Class Class Distribution Train/Dev/Test Split Proposed In\nFACTCOLLECT 9,567 2 4994 / 4573 8667 / 300 / 600 Ribeiro et al. (2022)\nFRANK 2,246 / / 0 / 671 / 1575 Pagnoni et al. (2021)\nCOVIDFACT 1,257 2 401 / 856 846 / 94 / 317 Saakyan et al. (2021)\nHEALTHVER 4,447 3 →2 2,758 / 1,689 3,340 / 508 / 599 Sarrouti et al. (2021)\nSCIFACT 773 3 →2 508 / 265 508 / 56 / 209 Wadden et al. (2020)\nTable 8: Statistics of the datasets and benchmarks adopted in this work.\nG Computational Resources\nWe used a GPU cluster with 16 NVIDIA A40\nGPUs, 1988G memory, and 104 CPU cores for the\nexperiments. Factuality pretraining with the default\nhyperparameters takes around 1.5 hours, while fine-\ntuning language models on the FactCollect dataset\ntakes around 30 minutes.\nH Scientific Artifacts\nFACT KB would not be possible without many\nopen-source scientific artifacts, including pytorch\n(Paszke et al., 2019), pytorch lightning (Falcon and\nThe PyTorch Lightning team, 2019), transform-\ners (Wolf et al., 2020), sklearn (Pedregosa et al.,\n2011), numpy (Harris et al., 2020), nltk (Bird et al.,\n2009), and the six adopted knowledge bases (Pel-\nlissier Tanon et al., 2020; Vrandeˇci´c and Krötzsch,\n2014; West et al., 2022; Speer et al., 2017; Feng\net al., 2021; Zhang et al., 2022b). We commit to\nmaking our code and data publicly available upon\nacceptance to facilitate reproduction and further\nresearch.\n950\nQAGS DAE FactCC FactKBGold Summary Article\n0.3000 0.9990 1.0000 0.0035 0 plans to build\na new gener-\nation of royal\nnavy frigates\non the isle\nof wight have\nbeen submit-\nted to the gov-\nernment.\nThe decommissioned Type 22 frigates HMS Cumberland, HMS Campbeltown,\nHMS Chatham and HMS Cornwall are currently moored in Portsmouth Har-\nbour.Bidders had until 23 January to register an interest in the former Devonport-\nbased ships.The BBC understands no proposals to preserve the ships have been\nsubmitted.Those who have registered an interest are finalising their bids with view-\nings set to take place in late February and March.A final decision is not expected\nuntil the spring.The government’s Disposal Services Authority, which is handling\nthe sale, wants to award at least one of the frigates to a UK ship recycler to deter-\nmine the capacity of the UK’s industry in the field.Penny Mordaunt, Conservative\nMP for Portsmouth North, said it was important UK recyclers had the chance to\nprove themselves in the field but she was also keen to see at least one of them\nsaved from the scrapyard.She added: \"For anyone that has served on a ship it’s your\nhome, you’ve literally been through the wars with it... and you want them to have a\nnoble second life.\"My preference is to go for the reef and diving attraction.\"We’ve\ngot to get best value for the budget but a reef would also generate income for part\nof the country through tourism.\"The Ministry of Defence has previously said it\nwill \"consider all options\" for the frigates to ensure \"best financial return for the\ntaxpayer\".A spokeswoman would not comment on the number or nature of the bids\nreceived due to \"commercial sensitivity\".Originally designed as a specialist anti-\nsubmarine ship, the Type 22 frigate evolved into a powerful surface combatant with\nsubstantial anti-surface, anti-submarine and anti-aircraft weapons systems.They\nwere also known for having excellent command and control, and communication\nfacilities, making them ideal flagships on deployments, with a complement of\nabout 280 crew.Last year, the aircraft carrier HMS Ark Royal was sold as scrap for\n£3m.\n0.5333 0.9296 1.0000 0.0043 0 an elephant\nhas been hit\nby a stone\nat a zoo\nin western\nfrance after it\nwas hit by a\ntree.\nThe stone got past the elephant’s fence and a ditch separating the animal and\nvisitors, the zoo said in a statement.The girl was taken to hospital and died within\na few hours, the zoo added.The zoo statement said the enclosure met international\nstandards and said \"this kind of accident is rare, unpredictable and unusual\".Africa\nLive: More on this and other storiesThe statement went on (in French) to point\nout two other recent incidents in the US:Phyllis Lee, Scientific Director of the\nAmboseli Trust for Elephants, says that targeted throwing of stones and branches by\nelephants is very unusual.\"It can happen when elephants are frustrated or bored. In\nmy opinion, it’s unlikely the elephant was directly targeting the girl - but exhibiting\nfrustration. You can’t predict what animals in captivity will do.\"The moments\nafter the girl was struck at Rabat Zoo on Tuesday were filmed by a bystander and\nuploaded onto YouTube.The video shows the elephant waving its trunk behind a\nfence and swerves round to show a stone on the ground.Metres away people are\ngathered around the girl, holding her head and stroking her leg.\n0.6000 0.9994 1.0000 0.0037 0 a woman has\nbeen arrested\nafter a fire\nbroke out in\na restaurant\nin greater\nmanchester\ncity centre,\npolice have\nsaid.\nThe victim was queuing for food at the branch in St George’s Street, Canterbury\nat about 02:15 GMT on Friday when the assault occurred.Investigating officers\nsaid three men entered the restaurant and began being noisy and bumping into\npeople.It is believed one of the group then set light to the woman’s hair.Officers\nhave released CCTV images of three men they are keen to speak to regarding\nthe attack.Det Sgt Barry Carr said: \"Fortunately the fire was put out quickly and\nthe victim was not seriously hurt, but things could clearly have turned out much\nworse.\"This was a nasty and extremely dangerous thing to do, and I urge anyone\nwho recognises the men in the CCTV images to contact me as soon as possible.\"\n0.8000 0.9974 1.0000 0.0044 0 tata steel has\nconfirmed\nit is in talks\nwith the\ncompany\nabout selling\nits long\nproducts\ndivision.\nThe firm said it had signed a Letter of Intent to enter into exclusive negotiations\nwith Liberty House Group.More than 1,700 people are employed in the division,\nwhich has factories in Rotherham and Stocksbridge.Steel union Community said\nit welcomed news of negotiations following \"months of unnecessary stress and\nconcern\".More on this and other South Yorkshire storiesThe union’s general secre-\ntary Roy Rickhuss said: \"This is a positive step for the UK steel industry; however\nthere remain huge challenges which government must address.\"The union said it\nwould be seeking urgent talks with Liberty House Group and would be asking what\ntheir plans were for investment, protecting jobs and providing decent pensions for\nmembers in retirement.Tata Steel’s UK boss Bimlendra Jha said the announcement\nwas \"an important step forward\".\"We now look forward to working with Liberty\non the due diligence and other work streams so that the sale can be successfully\nconcluded,\" he said.The Speciality Steels unit makes high-end components for\nthe automotive, aerospace and oil industries.In April, Tata sold its long-products\ndivision, based in Scunthorpe, to Greybull Capital, a UK-based investment firm.\n0.3000 0.9990 1.0000 0.0058 0 the site of a\nnew burial\nsite in oxford\nhas been\napproved\nby the city\ncouncil.\nOxford City Council said the money had mostly been used for \"ground investi-\ngations of possible sites\" but nowhere suitable had been found.Two cemeteries\nstill have space, in Wolvercote and Botley, but they are expected to be full by\n2018 and 2021.The council said it had not given up and was \"still exploring op-\ntions\".Linda Smith, board member for leisure, parks and sport, said the council\nhas been \"searching for a suitable new burial site for many years\".She added: \"But\nultimately, as with new housing sites, we have run out of suitable land within\nOxford.\"So far all the council-owned sites that we have identified have, following\nground investigations and surveys, had to be discounted.\"Either due to the size of\nthe site, the ground conditions, a high water table or a covenant restricting the use\nof the site.\"After the two remaining cemeteries are full the council said only the\nreopening of family plots, the use of a few reserved plots, and the interment of\nashes would be possible.The last increase in burial space in Oxford was in 1932.\nTable 9: Qualitative analysis of FACT KB and existing factuality metrics, part 1.\n951\nQAGS DAE FactCC FactKBGold Summary Article\n0.6000 0.9886 1.0000 0.0037 0 plans to\ndemolish and\ndemolish\nparts of a\nseaside resort\nand build\nmore than\n1, 000 old\nbuildings\nhave been\napproved.\nThree Victorian hotels will go to make way for a six-storey, four star hotel and\ntwo assisted-living apartment blocks, at East Cliff in Bournemouth.English Her-\nitage strongly objected to the scale of the development in what is a designated\nconservation area.But, councillors voted seven to three in favour saying it would\nhelp tourism.Chair of the planning board and Conservative ward councillor David\nKelsey, said the buildings earmarked for demolition were nice but no longer \"nec-\nessarily functional\".\"They’ve come to the end of their working lives, we need to\npreserve the tourism aspect while improving living for older people in the town,\" he\nsaid.\"The loss of buildings and trees are always regrettable but we can’t stand still,\nwe need to move forward.\"The site on Grove Road and East Overcliff Drive will\nget a 90-room hotel along with a nine-storey and seven-storey building, comprising\n122 assisted-living apartments.Applicants The East Cliff Project LLP will demolish\nBay View Court, The Cottonwood and the Ocean View hotels.The council received\n246 letters supporting the plans.Forty-nine residents and the Ancient Monuments\nSociety wrote to object to the demolition, stating that despite being altered, they\nstill \"give a sense of the historic character of the area\".English Heritage said the\nscale of the development would cause \"severe harm\" to the conservation area.\n0.6000 0.9928 1.0000 0.0059 0 russia´s new\npresident has\ncalled for a\nnew law to\nallow russian\ncitizens to be\nbarred from\nleaving the\ncountry.\nPro-Kremlin party A Just Russia put forward both bills, and linked them directly to\nthe situation in Ukraine.Separatist and pro-Russian feelings are strong in Ukraine’s\nCrimea region, which is now the focus of the crisis.Russian MPs say a referendum\nor a plea from a territory’s leaders would be enough to trigger the new provi-\nsions.There are already many Russian citizens in Crimea.In Sevastopol, base of\nthe Russian Black Sea Fleet, a majority hold Russian passports.Under Russia’s\nexisting law, a neighbouring state would have to sign a treaty with Russia to al-\nlow part of its territory to become a new \"subject\" of the Russian Federation.But\nMikhail Yemelyanov, deputy leader of A Just Russia, said the law had been drafted\nfor peaceful times, and did not go far enough for situations where a state was\nfalling apart.\"In conditions where a neighbouring state is disintegrating I don’t\nthink the Russian Federation should be restricted in its ability to accept a territory\nwhose people have expressed a clear will and desire to be in Russia,\" he said.Since\nRussia’s war with Georgia in 2008, the breakaway Georgian territories of Abk-\nhazia and South Ossetia have come under Moscow’s control.Russia poured troops\ninto both regions to help pro-Russian separatists who did not recognise Georgia’s\nauthority.The other bill to be considered by the Duma - Russia’s lower house -\nwould speed up the procedures for issuing Russian passports.Passport applicants\nwould not have to pay a state tax, and previous residence in Russia would no longer\nbe required.In addition, they would not have to have sufficient funds to support\nthemselves and would not have to give up their Ukrainian citizenship.The bill’s\npreamble says it is aimed \"at supporting the fraternal people of Ukraine, especially\nthe Russian-speaking ones, who are defenceless in the face of the ’brown threat’,\"\na reference to World War Two fascists who wore brown uniforms.The bill would\nallow Ukrainians to apply for Russian passports at Russian diplomatic missions\nbefore 1 August, and they could become citizens after two months, instead of\nwaiting a year, as is currently the norm.The plan to have a new fast-track procedure\nfor issuing Russian passports was announced in Sevastopol on Thursday by A Just\nRussia leader Sergei Mironov.Several Russian MPs have also gone to Crimea, in-\ncluding Russian celebrities - former Olympic ice skating champion Irina Rodnina,\nformer cosmonaut Valentina Tereshkova and heavyweight boxer Nikolai Valuev.\n0.7000 0.9984 1.0000 0.0047 0 a 19-year-old\nman has been\narrested in\nconnection\nwith the fatal\nshooting of\nan 18-year-\nold student in\nthe southern\nindian state\nof\nThe shooting occurred at a hostel attached to the private Pragati Residential School\nin Bangalore city.Police say the alleged gunman, identified as Mahesh, was working\nas an office assistant in the school.Incidents of gun crime at schools and colleges\nin India are very rare. It is not clear what prompted the shooting.Police said on\nThursday that Mahesh had been remanded until 12 April.Mahesh is alleged to\nhave barged into the room of 18-year-old Gautami and shot her in the head with a\npistol on on Tuesday evening.He then shot another student, Sirisha, who suffered\nsevere injuries but is believed to be out of danger, say police.He was arrested on\nWednesday after a manhunt.India has strict control laws, although a large number\nof feuds are settled with firearms.In 2007, a 14-year-old schoolboy was shot dead\nby two fellow students at a school campus near the capital, Delhi.\n0.5000 0.9995 1.0000 0.0036 0 police have\nappealed for\nhelp to trace\ntwo men who\nthreatened a\nwoman with\na knife at a\nquarry in fife.\nThe men entered the Post Office in Quarrywood Avenue, in the Barmulloch area,\nat 07:55 on Friday.They threatened a member of staff with a knife and demanded\nmoney before escaping with the cash.The 27-year-old worker was said by police\nto have been badly shaken but otherwise unharmed by the ordeal.Both suspects\nare white, and one of them was about 35-40 years old with short brown hair and\nwearing a black jumper.Det Sgt Raymond Hunter said officers had been carrying\nout door-to-door inquiries and were in the process of collecting CCTV images\nfrom the surrounding area.He added: \"There are a number of other shops in this\narea and people may have seen the two men prior to or after the incident.\"I am\ntherefore appealing to anyone who was in the area or any local residents to contact\nus - any information you have could assist our enquiry.\"\n0.5000 0.9999 1.0000 0.0063 0 the number\nof people\nusing plastic\ncarrier bags\nin england\nhas reached a\nrecord high.\nThe Department for Environment, Food and Rural Affairs found the number had\ngone up by 200 million since 2013.There has been a big problem with plastic\ncarrier bags in the last few years, many of them can’t be recycled and are often\nthrown away after they have been used.The bags end up in rubbish dumps and even\nrivers causing big problems for the environment.From October people in England\nwill have to pay 5p for their plastic bags in a bid to encourage them to reuse\nthe ones that they already have.Supermarkets in Wales, Scotland and Northern\nIreland, where people are charged for carrier bags, have all seen a decrease in bags\nused.Campaigners are hoping the charge in England will lessen the amount of bags\nbeing thrown away, helping the environment.\nTable 10: Qualitative analysis of FACT KB and existing factuality metrics, part 2.\n952"
}