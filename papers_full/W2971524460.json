{
    "title": "Adaptively Sparse Transformers",
    "url": "https://openalex.org/W2971524460",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4290222256",
            "name": "Correia, Gonçalo M.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223170844",
            "name": "Niculae, Vlad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207161097",
            "name": "Martins, André F. T.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963807318",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2859444450",
        "https://openalex.org/W1983874169",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2963062480",
        "https://openalex.org/W2017697298",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2805493160",
        "https://openalex.org/W2963123301",
        "https://openalex.org/W2963828549",
        "https://openalex.org/W2963502387",
        "https://openalex.org/W1551360398",
        "https://openalex.org/W2505728881",
        "https://openalex.org/W2950858167",
        "https://openalex.org/W3005389111",
        "https://openalex.org/W2270190199",
        "https://openalex.org/W2962822108",
        "https://openalex.org/W2888539709",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W2962943802",
        "https://openalex.org/W2962839844",
        "https://openalex.org/W2963970238",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W2921569601"
    ],
    "abstract": "Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with $α$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the $α$ parameter -- which controls the shape and sparsity of $α$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.",
    "full_text": "Adaptively Sparse Transformers\nGonc ¸alo M. Correiaä\ngoncalo.correia@lx.it.pt\nVlad Niculaeä\nvlad@vene.ro\näInstituto de Telecomunicac ¸˜oes, Lisbon, Portugal\nãUnbabel, Lisbon, Portugal\nAndr´e F.T. Martinsä ã\nandre.martins@unbabel.com\nAbstract\nAttention mechanisms have become ubiqui-\ntous in NLP. Recent architectures, notably\nthe Transformer, learn powerful context-aware\nword representations through layered, multi-\nheaded attention. The multiple heads learn\ndiverse types of word relationships. How-\never, with standard softmax attention, all at-\ntention heads are dense, assigning a non-zero\nweight to all context words. In this work, we\nintroduce the adaptively sparse Transformer,\nwherein attention heads have ﬂexible, context-\ndependent sparsity patterns. This sparsity is\naccomplished by replacing softmax with α-\nentmax: a differentiable generalization of soft-\nmax that allows low-scoring words to receive\nprecisely zero weight. Moreover, we derive a\nmethod to automatically learn the αparameter\n– which controls the shape and sparsity of α-\nentmax – allowing attention heads to choose\nbetween focused or spread-out behavior. Our\nadaptively sparse Transformer improves inter-\npretability and head diversity when compared\nto softmax Transformers on machine transla-\ntion datasets. Findings of the quantitative and\nqualitative analysis of our approach include\nthat heads in different layers learn different\nsparsity preferences and tend to be more di-\nverse in their attention distributions than soft-\nmax Transformers. Furthermore, at no cost in\naccuracy, sparsity in attention heads helps to\nuncover different head specializations.\n1 Introduction\nThe Transformer architecture (Vaswani et al., 2017)\nfor deep neural networks has quickly risen to promi-\nnence in NLP through its efﬁciency and perfor-\nmance, leading to improvements in the state of the\nart of Neural Machine Translation (NMT; Junczys-\nDowmunt et al., 2018; Ott et al., 2018), as well as\ninspiring other powerful general-purpose models\nlike BERT (Devlin et al., 2019) and GPT-2 (Rad-\nford et al., 2019). At the heart of the Transformer\nThequickbrownfox jumpsover Thequickbrownfox jumpsover Thequickbrownfox jumpsover\nhead 1\nhead 4\nhead 3\nhead 2\nSparse Transformer Adaptive Span \nTransformer\nAdaptively Sparse \nTransformer (Ours)\nFigure 1: Attention distributions of different self-\nattention heads for the time step of the token “over”,\nshown to compare our model to other related work.\nWhile the sparse Transformer (Child et al., 2019) and\nthe adaptive span Transformer (Sukhbaatar et al., 2019)\nonly attend to words within a contiguous span of the\npast tokens, our model is not only able to obtain differ-\nent and not necessarily contiguous sparsity patterns for\neach attention head, but is also able to tune its support\nover which tokens to attend adaptively.\nlie multi-head attention mechanisms: each word\nis represented by multiple different weighted aver-\nages of its relevant context. As suggested by recent\nworks on interpreting attention head roles, sepa-\nrate attention heads may learn to look for various\nrelationships between tokens (Tang et al., 2018; Ra-\nganato and Tiedemann, 2018; Mareˇcek and Rosa,\n2018; Tenney et al., 2019; V oita et al., 2019).\nThe attention distribution of each head is pre-\ndicted typically using the softmax normalizing\ntransform. As a result, all context words have\nnon-zero attention weight. Recent work on sin-\ngle attention architectures suggest that using sparse\nnormalizing transforms in attention mechanisms\nsuch as sparsemax – which can yield exactly zero\nprobabilities for irrelevant words – may improve\nperformance and interpretability (Malaviya et al.,\n2018; Deng et al., 2018; Peters et al., 2019). Qual-\nitative analysis of attention heads (Vaswani et al.,\n2017, Figure 5) suggests that, depending on what\nphenomena they capture, heads tend to favor ﬂatter\nor more peaked distributions.\nRecent works have proposed sparse Transform-\narXiv:1909.00015v2  [cs.CL]  6 Sep 2019\ners (Child et al., 2019) and adaptive span Trans-\nformers (Sukhbaatar et al., 2019). However, the\n“sparsity” of those models only limits the attention\nto a contiguous span of past tokens, while in this\nwork we propose a highly adaptive Transformer\nmodel that is capable of attending to a sparse set of\nwords that are not necessarily contiguous. Figure 1\nshows the relationship of these methods with ours.\nOur contributions are the following:\n• We introduce sparse attention into the Trans-\nformer architecture, showing that it eases inter-\npretability and leads to slight accuracy gains.\n• We propose an adaptive version of sparse at-\ntention, where the shape of each attention\nhead is learnable and can vary continuously\nand dynamically between the dense limit case\nof softmax and the sparse, piecewise-linear\nsparsemax case.1\n• We make an extensive analysis of the added\ninterpretability of these models, identifying\nboth crisper examples of attention head behav-\nior observed in previous work, as well as novel\nbehaviors unraveled thanks to the sparsity and\nadaptivity of our proposed model.\n2 Background\n2.1 The Transformer\nIn NMT, the Transformer (Vaswani et al., 2017)\nis a sequence-to-sequence (seq2seq) model which\nmaps an input sequence to an output sequence\nthrough hierarchical multi-head attention mech-\nanisms, yielding a dynamic, context-dependent\nstrategy for propagating information within and\nacross sentences. It contrasts with previous seq2seq\nmodels, which usually rely either on costly gated\nrecurrent operations (often LSTMs: Bahdanau\net al., 2015; Luong et al., 2015) or static convo-\nlutions (Gehring et al., 2017).\nGiven nquery contexts and msequence items\nunder consideration, attention mechanisms com-\npute, for each query, a weighted representation of\nthe items. The particular attention mechanism used\nin Vaswani et al. (2017) is calledscaled dot-product\nattention, and it is computed in the following way:\nAtt(Q,K,V) = π\n(QK⊤\n√\nd\n)\nV, (1)\n1Code and pip package available at https://github.\ncom/deep-spin/entmax.\nwhere Q∈Rn×d contains representations of the\nqueries, K,V ∈Rm×d are the keys and values\nof the items attended over, and d is the dimen-\nsionality of these representations. The πmapping\nnormalizes row-wise using softmax, π(Z)ij =\nsoftmax(zi)j, where\nsoftmax(z) = exp(zj)∑\nj′exp(zj′). (2)\nIn words, the keys are used to compute a relevance\nscore between each item and query. Then, normal-\nized attention weights are computed using softmax,\nand these are used to weight thevalues of each item\nat each query context.\nHowever, for complex tasks, different parts of a\nsequence may be relevant in different ways, moti-\nvating multi-head attention in Transformers. This\nis simply the application of Equation 1 in paral-\nlel H times, each with a different, learned linear\ntransformation that allows specialization:\nHeadi(Q,K,V)= Att(QWQ\ni,KWK\ni ,VW V\ni ) (3)\nIn the Transformer, there are three separate multi-\nhead attention mechanisms for distinct purposes:\n• Encoder self-attention: builds rich, layered\nrepresentations of each input word, by attend-\ning on the entire input sentence.\n• Context attention: selects a representative\nweighted average of the encodings of the input\nwords, at each time step of the decoder.\n• Decoder self-attention: attends over the par-\ntial output sentence fragment produced so far.\nTogether, these mechanisms enable the contextual-\nized ﬂow of information between the input sentence\nand the sequential decoder.\n2.2 Sparse Attention\nThe softmax mapping (Equation 2) is elementwise\nproportional to exp, therefore it can never assign a\nweight of exactly zero. Thus, unnecessary items\nare still taken into consideration to some extent.\nSince its output sums to one, this invariably means\nless weight is assigned to the relevant items, po-\ntentially harming performance and interpretabil-\nity (Jain and Wallace, 2019). This has motivated a\nline of research on learning networks with sparse\nmappings (Martins and Astudillo, 2016; Niculae\nand Blondel, 2017; Louizos et al., 2018; Shao et al.,\n2019). We focus on a recently-introduced ﬂexible\nfamily of transformations,α-entmax (Blondel et al.,\n2019; Peters et al., 2019), deﬁned as:\nα-entmax(z) := argmax\np∈△d\np⊤z+ HT\nα(p), (4)\nwhere △d := {p∈Rd : ∑\nipi = 1}is the prob-\nability simplex, and, for α ≥1, HT\nα is the Tsallis\ncontinuous family of entropies (Tsallis, 1988):\nHT\nα(p):=\n{ 1\nα(α−1)\n∑\nj\n(\npj −pα\nj\n)\n, α̸= 1,\n−∑\nj pj log pj, α = 1.\n(5)\nThis family contains the well-known Shannon and\nGini entropies, corresponding to the cases α = 1\nand α= 2, respectively.\nEquation 4 involves a convex optimization sub-\nproblem. Using the deﬁnition of HT\nα, the optimality\nconditions may be used to derive the following\nform for the solution (Appendix B.2):\nα-entmax(z) = [(α−1)z−τ1]\n1/α−1\n+ , (6)\nwhere [·]+ is the positive part (ReLU) function,\n1 denotes the vector of all ones, and τ – which\nacts like a threshold – is the Lagrange multiplier\ncorresponding to the ∑\nipi = 1 constraint.\nProperties of α-entmax. The appeal of α-\nentmax for attention rests on the following prop-\nerties. For α = 1 (i.e., when HT\nα becomes the\nShannon entropy), it exactly recovers the softmax\nmapping (We provide a short derivation in Ap-\npendix B.3.). For all α> 1 it permits sparse solu-\ntions, in stark contrast to softmax. In particular, for\nα= 2, it recovers the sparsemax mapping (Martins\nand Astudillo, 2016), which is piecewise linear. In-\nbetween, as αincreases, the mapping continuously\ngets sparser as its curvature changes.\nTo compute the value of α-entmax, one must\nﬁnd the threshold τ such that the r.h.s. in Equa-\ntion 6 sums to one. Blondel et al. (2019) propose\na general bisection algorithm. Peters et al. (2019)\nintroduce a faster, exact algorithm forα= 1.5, and\nenable using α-entmax with ﬁxed αwithin a neu-\nral network by showing that theα-entmax Jacobian\nw.r.t. zfor p⋆ = α-entmax(z) is\n∂α-entmax(z)\n∂z = diag(s) − 1∑\nj sj\nss⊤,\nwhere si =\n{\n(p⋆\ni)2−α, p ⋆\ni >0,\n0, p ⋆\ni = 0.\n(7)\nOur work furthers the study of α-entmax by\nproviding a derivation of the Jacobian w.r.t. the\nhyper-parameter α(Section 3), thereby allowing\nthe shape and sparsity of the mapping to be learned\nautomatically. This is particularly appealing in the\ncontext of multi-head attention mechanisms, where\nwe shall show in Section 5.1 that different heads\ntend to learn different sparsity behaviors.\n3 Adaptively Sparse Transformers\nwith α-entmax\nWe now propose a novel Transformer architecture\nwherein we simply replace softmax with α-entmax\nin the attention heads. Concretely, we replace the\nrow normalization πin Equation 1 by\nπ(Z)ij = α-entmax(zi)j (8)\nThis change leads to sparse attention weights, as\nlong as α> 1; in particular, α= 1.5 is a sensible\nstarting point (Peters et al., 2019).\nDifferent α per head. Unlike LSTM-based\nseq2seq models, where αcan be more easily tuned\nby grid search, in a Transformer, there are many\nattention heads in multiple layers. Crucial to the\npower of such models, the different heads capture\ndifferent linguistic phenomena, some of them iso-\nlating important words, others spreading out atten-\ntion across phrases (Vaswani et al., 2017, Figure 5).\nThis motivates using different, adaptive αvalues\nfor each attention head, such that some heads may\nlearn to be sparser, and others may become closer\nto softmax. We propose doing so by treating the α\nvalues as neural network parameters, optimized via\nstochastic gradients along with the other weights.\nDerivatives w.r.t. α. In order to optimize αau-\ntomatically via gradient methods, we must com-\npute the Jacobian of the entmax output w.r.t. α.\nSince entmax is deﬁned through an optimization\nproblem, this is non-trivial and cannot be simply\nhandled through automatic differentiation; it falls\nwithin the domain of argmin differentiation, an ac-\ntive research topic in optimization (Gould et al.,\n2016; Amos and Kolter, 2017).\nOne of our key contributions is the derivation\nof a closed-form expression for this Jacobian. The\nnext proposition provides such an expression, en-\nabling entmax layers with adaptive α. To the best\nof our knowledge, ours is the ﬁrst neural network\nmodule that can automatically, continuously vary\nin shape away from softmax and toward sparse\nmappings like sparsemax.\nProposition 1. Let p⋆ := α-entmax(z) be the so-\nlution of Equation 4. Denote the distribution ˜pi :=\n(p⋆\ni)2−α\n/∑\nj(p⋆\nj)2−α and let hi := −p⋆\ni log p⋆\ni. The ith\ncomponent of the Jacobian g:= ∂α-entmax(z)\n∂α is\ngi =\n\n\n\np⋆\ni−˜pi\n(α−1)2 +\nhi−˜pi\n∑\njhj\nα−1 , α> 1,\nhilog p⋆\ni−p⋆\ni\n∑\njhjlog p⋆\nj\n2 , α = 1.\n(9)\nThe proof uses implicit function differentiation and\nis given in Appendix C.\nProposition 1 provides the remaining missing\npiece needed for training adaptively sparse Trans-\nformers. In the following section, we evaluate this\nstrategy on neural machine translation, and analyze\nthe behavior of the learned attention heads.\n4 Experiments\nWe apply our adaptively sparse Transformers on\nfour machine translation tasks. For comparison,\na natural baseline is the standard Transformer ar-\nchitecture using the softmax transform in its multi-\nhead attention mechanisms. We consider two other\nmodel variants in our experiments that make use of\ndifferent normalizing transformations:\n• 1.5-entmax: a Transformer with sparse ent-\nmax attention with ﬁxed α= 1.5 for all heads.\nThis is a novel model, since 1.5-entmax had\nonly been proposed for RNN-based NMT\nmodels (Peters et al., 2019), but never in\nTransformers, where attention modules are\nnot just one single component of the seq2seq\nmodel but rather an integral part of all of the\nmodel components.\n• α-entmax: an adaptive Transformer with\nsparse entmax attention with a different,\nlearned αt\ni,j for each head.\nThe adaptive model has an additional scalar pa-\nrameter per attention head per layer for each of the\nthree attention mechanisms (encoder self-attention,\ncontext attention, and decoder self-attention), i.e.,\n{\nat\ni,j ∈R : i∈{1,...,L },j ∈{1,...,H },\nt∈{enc,ctx,dec}\n}\n,\n(10)\nand we set αt\ni,j = 1 +sigmoid(at\ni,j) ∈]1,2[. All or\nsome of the αvalues can be tied if desired, but we\nkeep them independent for analysis purposes.\nDatasets. Our models were trained on 4 machine\ntranslation datasets of different training sizes:\n• IWSLT 2017 German →English (DE\u0001EN, Cet-\ntolo et al., 2017): 200K sentence pairs.\n• KFTT Japanese →English ( JA\u0001EN, Neubig,\n2011): 300K sentence pairs.\n• WMT 2016 Romanian →English (RO\u0001EN, Bo-\njar et al., 2016): 600K sentence pairs.\n• WMT 2014 English →German (EN\u0001DE, Bojar\net al., 2014): 4.5M sentence pairs.\nAll of these datasets were preprocessed with\nbyte-pair encoding (BPE; Sennrich et al., 2016),\nusing joint segmentations of 32k merge operations.\nTraining. We follow the dimensions of the\nTransformer-Base model of Vaswani et al. (2017):\nThe number of layers is L = 6 and number of\nheads is H = 8 in the encoder self-attention, the\ncontext attention, and the decoder self-attention.\nWe use a mini-batch size of 8192 tokens and warm\nup the learning rate linearly until 20k steps, after\nwhich it decays according to an inverse square root\nschedule. All models were trained until conver-\ngence of validation accuracy, and evaluation was\ndone at each 10k steps for RO\u0001EN and EN\u0001DE\nand at each 5k steps for DE\u0001EN and JA\u0001EN. The\nend-to-end computational overhead of our methods,\nwhen compared to standard softmax, is relatively\nsmall; in training tokens per second, the models\nusing α-entmax and 1.5-entmax are, respectively,\n75% and 90% the speed of the softmax model.\nResults. We report test set tokenized BLEU (Pa-\npineni et al., 2002) results in Table 1. We can see\nthat replacing softmax by entmax does not hurt\nperformance in any of the datasets; indeed, sparse\nattention Transformers tend to have slightly higher\nBLEU, but their sparsity leads to a better poten-\ntial for analysis. In the next section, we make use\nof this potential by exploring the learned internal\nmechanics of the self-attention heads.\n5 Analysis\nWe conduct an analysis for the higher-resource\ndataset WMT 2014 English →German of the at-\ntention in the sparse adaptive Transformer model\n(α-entmax) at multiple levels: we analyze high-\nlevel statistics as well as individual head behavior.\nMoreover, we make a qualitative analysis of the\ninterpretability capabilities of our models.\nactivation DE\u0001EN JA \u0001EN RO \u0001EN EN \u0001DE\nsoftmax 29.79 21.57 32.70 26.02\n1.5-entmax 29.83 22.13 33.10 25.89\nα-entmax 29.90 21.74 32.89 26.93\nTable 1: Machine translation tokenized BLEU test results on IWSLT 2017 DE\u0001EN, KFTT JA\u0001EN, WMT 2016\nRO\u0001EN and WMT 2014 EN\u0001DE, respectively.\n5.1 High-Level Statistics\nWhat kind of αvalues are learned? Figure 2\nshows the learning trajectories of the αparameters\nof a selected subset of heads. We generally observe\na tendency for the randomly-initialized αparame-\nters to decrease initially, suggesting that softmax-\nlike behavior may be preferable while the model\nis still very uncertain. After around one thousand\nsteps, some heads change direction and become\nsparser, perhaps as they become more conﬁdent\nand specialized. This shows that the initialization\nof αdoes not predetermine its sparsity level or the\nrole the head will have throughout. In particular,\nhead 8 in the encoder self-attention layer 2 ﬁrst\ndrops to around α= 1.3 before becoming one of\nthe sparsest heads, with α≈2.\nThe overall distribution of αvalues at conver-\ngence can be seen in Figure 3. We can observe\nthat the encoder self-attention blocks learn to con-\ncentrate the αvalues in two modes: a very sparse\none around α→2, and a dense one between soft-\nmax and 1.5-entmax. However, the decoder self\nand context attention only learn to distribute these\nparameters in a single mode. We show next that\nthis is reﬂected in the average density of attention\nweight vectors as well.\nAttention weight density when translating.\nFor any α >1, it would still be possible for the\nweight matrices in Equation 3 to learn re-scalings\nso as to make attention sparser or denser. To visu-\nalize the impact of adaptive αvalues, we compare\nthe empirical attention weight density (the aver-\nage number of tokens receiving non-zero attention)\nwithin each module, against sparse Transformers\nwith ﬁxed α= 1.5.\nFigure 4 shows that, with ﬁxed α= 1.5, heads\ntend to be sparse and similarly-distributed in all\nthree attention modules. With learned α, there are\ntwo notable changes: (i) a prominent mode corre-\nsponding to fully dense probabilities, showing that\nour models learn to combine sparse and dense atten-\ntion, and (ii) a distinction between the encoder self-\n0 2000 4000 6000 8000 10000 12000\ntraining steps\n1.0\n1.2\n1.4\n1.6\n1.8\ndecoder, layer 1, head 8\nencoder, layer 1, head 3\nencoder, layer 1, head 4\nencoder, layer 2, head 8\nencoder, layer 6, head 2\nFigure 2: Trajectories of α values for a subset of\nthe heads during training. Initialized at random, most\nheads become denser in the beginning, before converg-\ning. This suggests that dense attention may be more\nbeneﬁcial while the network is still uncertain, being re-\nplaced by sparse attention afterwards.\nattention – whose background distribution tends\ntoward extreme sparsity – and the other two mod-\nules, who exhibit more uniform background distri-\nbutions. This suggests that perhaps entirely sparse\nTransformers are suboptimal.\nThe fact that the decoder seems to prefer denser\nattention distributions might be attributed to it be-\ning auto-regressive, only having access to past to-\nkens and not the full sentence. We speculate that\nit might lose too much information if it assigned\nweights of zero to too many tokens in the self-\nattention, since there are fewer tokens to attend to\nin the ﬁrst place.\nTeasing this down into separate layers, Figure 5\nshows the average (sorted) density of each head for\neach layer. We observe that α-entmax is able to\nlearn different sparsity patterns at each layer, lead-\ning to more variance in individual head behavior, to\nclearly-identiﬁed dense and sparse heads, and over-\nall to different tendencies compared to the ﬁxed\ncase of α= 1.5.\nHead diversity. To measure the overall disagree-\nment between attention heads, as a measure of head\n0\n10\n20\nEncoder\nSelf-Attention\n0\n10\n20\nContext\nAttention\n1.0 1.2 1.4 1.6 1.8 2.0\n0\n10\n20\nDecoder\nSelf-Attention\nFigure 3: Distribution of learned αvalues per attention\nblock. While the encoder self-attention has a bimodal\ndistribution of values of α, the decoder self-attention\nand context attention have a single mode.\ndiversity, we use the following generalization of\nthe Jensen-Shannon divergence:\nJS = HS\n\n1\nH\nH∑\nj=1\npj\n\n− 1\nH\nH∑\nj=1\nHS(pj) (11)\nwhere pj is the vector of attention weights as-\nsigned by head jto each word in the sequence, and\nHS is the Shannon entropy, base-adjusted based on\nthe dimension of psuch that JS ≤1. We average\nthis measure over the entire validation set. The\nhigher this metric is, the more the heads are taking\ndifferent roles in the model.\nFigure 6 shows that both sparse Transformer\nvariants show more diversity than the traditional\nsoftmax one. Interestingly, diversity seems to peak\nin the middle layers of the encoder self-attention\nand context attention, while this is not the case for\nthe decoder self-attention.\nThe statistics shown in this section can be found\nfor the other language pairs in Appendix A.\n5.2 Identifying Head Specializations\nPrevious work pointed out some speciﬁc roles\nplayed by different heads in the softmax Trans-\nformer model (V oita et al., 2018; Tang et al., 2018;\nV oita et al., 2019). Identifying the specialization of\na head can be done by observing the type of tokens\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0 0.5 1.0\n-entmax\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nContext\nAttention\n0.0 0.5 1.0\n0.0 0.5 1.0\ndensity\n0\n10k\n30k\n50k\nDecoder\nSelf-Attention\n0.0 0.5 1.0\ndensity\nFigure 4: Distribution of attention densities (average\nnumber of tokens receiving non-zero attention weight)\nfor all attention heads and all validation sentences.\nWhen compared to 1.5-entmax, α-entmax distributes\nthe sparsity in a more uniform manner, with a clear\nmode at fully dense attentions, corresponding to the\nheads with low α. In the softmax case, this distribution\nwould lead to a single bar with density 1.\nor sequences that the head often assigns most of its\nattention weight; this is facilitated by sparsity.\nPositional heads. One particular type of head, as\nnoted by V oita et al. (2019), is the positional head.\nThese heads tend to focus their attention on either\nthe previous or next token in the sequence, thus\nobtaining representations of the neighborhood of\nthe current time step. In Figure 7, we show atten-\ntion plots for such heads, found for each of the\nstudied models. The sparsity of our models allows\nthese heads to be more conﬁdent in their represen-\ntations, by assigning the whole probability distribu-\ntion to a single token in the sequence. Concretely,\nwe may measure a positional head’sconﬁdence as\nthe average attention weight assigned to the pre-\nvious token. The softmax model has three heads\nfor position −1, with median conﬁdence 93.5%.\nThe 1.5-entmax model also has three heads for\nthis position, with median conﬁdence 94.4%. The\nadaptive model has four heads, with median con-\nﬁdences 95.9%, the lowest-conﬁdence head being\ndense with α= 1.18, while the highest-conﬁdence\nhead being sparse (α= 1.91).\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1 2 3 4 5 6\nLayers\nFigure 5: Head density per layer for ﬁxed and learned\nα. Each line corresponds to an attention head; lower\nvalues mean that that attention head is sparser. Learned\nαhas higher variance.\nFor position +1, the models each dedicate one\nhead, with conﬁdence around 95%, slightly higher\nfor entmax. The adaptive model sets α= 1.96 for\nthis head.\nBPE-merging head. Due to the sparsity of our\nmodels, we are able to identify other head special-\nizations, easily identifying which heads should be\nfurther analysed. In Figure 8 we show one such\nhead where the αvalue is particularly high (in the\nencoder, layer 1, head 4 depicted in Figure 2). We\nfound that this head most often looks at the cur-\nrent time step with high conﬁdence, making it a\npositional head with offset 0. However, this head\noften spreads weight sparsely over 2-3 neighbor-\ning tokens, when the tokens are part of the same\nBPE cluster2 or hyphenated words. As this head\nis in the ﬁrst layer, it provides a useful service to\nthe higher layers by combining information evenly\nwithin some BPE clusters.\nFor each BPE cluster or cluster of hyphenated\nwords, we computed a score between 0 and 1 that\ncorresponds to the maximum attention mass as-\nsigned by any token to the rest of the tokens inside\nthe cluster in order to quantify the BPE-merging\n2BPE-segmented words are denoted by ∼ in the ﬁgures.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.20\n0.25\n0.30\n0.35\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.25\n0.30\n0.35\nDecoder\nSelf-Attention\nFigure 6: Jensen-Shannon Divergence between heads\nat each layer. Measures the disagreement between\nheads: the higher the value, the more the heads are dis-\nagreeing with each other in terms of where to attend.\nModels using sparse entmax have more diverse atten-\ntion than the softmax baseline.\ncapabilities of these heads.3 There are not any at-\ntention heads in the softmax model that are able\nto obtain a score over 80%, while for 1.5-entmax\nand α-entmax there are two heads in each (83.3%\nand 85.6% for 1.5-entmax and 88.5% and 89.8%\nfor α-entmax).\nInterrogation head. On the other hand, in Fig-\nure 9 we show a head for which our adaptively\nsparse model chose an α close to 1, making it\ncloser to softmax (also shown in encoder, layer 1,\nhead 3 depicted in Figure 2). We observe that this\nhead assigns a high probability to question marks\nat the end of the sentence in time steps where the\ncurrent token is interrogative, thus making it an\ninterrogation-detecting head. We also observe this\ntype of heads in the other models, which we also\ndepict in Figure 9. The average attention weight\nplaced on the question mark when the current to-\nken is an interrogative word is 98.5% for softmax,\n97.0% for 1.5-entmax, and 99.5% for α-entmax.\nFurthermore, we can examine sentences where\nsome tendentially sparse heads become less so, thus\nidentifying sources of ambiguity where the head\n3If the cluster has size 1, the score is the weight the token\nassigns to itself.\nweweren't farawaylastseason.\nsoftmax\nwe\nweren\n't\nfar\naway\nlast\nseason\n.\nweweren't farawaylastseason.\n1.5-entmax\nweweren't farawaylastseason.\n-entmax\nFigure 7: Self-attention from the most conﬁdently\nprevious-position head in each model. The learned pa-\nrameter in the α-entmax model is α = 1 .91. Quanti-\ntatively more conﬁdent, visual inspection conﬁrms that\nthe adaptive head behaves more consistently.\nrulesforblo~wingupbal~lo~ons, forbananasanda cir~cus\nrules\nfor\nblo~\nwing\nup\nbal~\nlo~\nons\n,\nfor\nbananas\nand\na\ncir~\ncus\none- two- three- four.\none\n-\ntwo\n-\nthree\n-\nfour\n.\nareyounotconfir~mingthiswithwhatyouhavestated?\nare\nyou\nnot\nconfir~\nming\nthis\nwith\nwhat\nyou\nhave\nstated\n?\nthiscouldcomebackto ha~untthem.\nthis\ncould\ncome\nback\nto\nha~\nunt\nthem\n.\nFigure 8: BPE-merging head (α = 1 .91) discovered\nin the α-entmax model. Found in the ﬁrst encoder\nlayer, this head learns to discover some subword units\nand combine their information, leaving most words in-\ntact. It places 99.09% of its probability mass within the\nsame BPE cluster as the current token: more than any\nhead in any other model.\nis less conﬁdent in its prediction. An example is\nshown in Figure 10 where sparsity in the same head\ndiffers for sentences of similar length.\n6 Related Work\nSparse attention. Prior work has developed\nsparse attention mechanisms, including appli-\ncations to NMT (Martins and Astudillo, 2016;\nMalaviya et al., 2018; Niculae and Blondel, 2017;\nShao et al., 2019; Maruf et al., 2019). Peters et al.\n(2019) introduced the entmax function this work\nbuilds upon. In their work, there is a single atten-\ntion mechanism which is controlled by a ﬁxed α.\nIn contrast, this is the ﬁrst work to allow such atten-\nhowever, whatis Ar~man~i Polo?\nsoftmax\nhowever\n,\nwhat\nis\nAr~\nman~\ni\nPolo\n?\nhowever, whatis Ar~man~i Polo?\n1.5-entmax\nhowever, whatis Ar~man~i Polo?\n-entmax\nyouwonderwhatmorepeopleexpect.\nsoftmax\nyou\nwonder\nwhat\nmore\npeople\nexpect\n.\nyouwonderwhatmorepeopleexpect.\n1.5-entmax\nyouwonderwhatmorepeopleexpect.\n-entmax\nFigure 9: Interrogation-detecting heads in the three\nmodels. The top sentence is interrogative while the\nbottom one is declarative but includes the interrogative\nword “what”. In the top example, these interrogation\nheads assign a high probability to the question mark in\nthe time step of the interrogative word (with ≥97.0%\nprobability), while in the bottom example since there\nis no question mark, the same head does not assign a\nhigh probability to the last token in the sentence dur-\ning the interrogative word time step. Surprisingly, this\nhead prefers a low α = 1.05, as can be seen from the\ndense weights. This allows the head to identify the\nnoun phrase “Armani Polo” better.\ntion mappings to dynamically adapt their curvature\nand sparsity, by automatically adjusting the contin-\nuous αparameter. We also provide the ﬁrst results\nusing sparse attention in a Transformer model.\nFixed sparsity patterns. Recent research im-\nproves the scalability of Transformer-like networks\nthrough static, ﬁxed sparsity patterns (Child et al.,\n2019; Wu et al., 2019). Our adaptively-sparse\nTransformer can dynamically select a sparsity pat-\ntern that ﬁnds relevant words regardless of their po-\nsition (e.g., Figure 9). Moreover, the two strategies\ncould be combined. In a concurrent line of research,\nSukhbaatar et al. (2019) propose an adaptive atten-\ntion span for Transformer language models. While\ntheir work has each head learn a different contigu-\nous span of context tokens to attend to, our work\nﬁnds different sparsity patterns in the same span.\nInterestingly, some of their ﬁndings mirror ours –\nwe found that attention heads in the last layers tend\nto be denser on average when compared to the ones\nin the ﬁrst layers, while their work has found that\nlower layers tend to have a shorter attention span\ncompared to higher layers.\nhere, thislayeris thin.\nhere\n,\nthis\nlayer\nis\nthin\n.\nwhichsymptomsindicatea sex~uallytransmitteddisease?\nwhich\nsymptoms\nindicate\na\nsex~\nually\ntransmitted\ndisease\n?\nFigure 10: Example of two sentences of similar length\nwhere the same head (α= 1.33) exhibits different spar-\nsity. The longer phrase in the example on the right\n“a sexually transmitted disease” is handled with higher\nconﬁdence, leading to more sparsity.\nTransformer interpretability. The original\nTransformer paper (Vaswani et al., 2017) shows\nattention visualizations, from which some specula-\ntion can be made of the roles the several attention\nheads have. Mare ˇcek and Rosa (2018) study the\nsyntactic abilities of the Transformer self-attention,\nwhile Raganato and Tiedemann (2018) extract\ndependency relations from the attention weights.\nTenney et al. (2019) ﬁnd that the self-attentions in\nBERT (Devlin et al., 2019) follow a sequence of\nprocesses that resembles a classical NLP pipeline.\nRegarding redundancy of heads, V oita et al. (2019)\ndevelop a method that is able to prune heads of\nthe multi-head attention module and make an\nempirical study of the role that each head has\nin self-attention (positional, syntactic and rare\nwords). Li et al. (2018) also aim to reduce head\nredundancy by adding a regularization term to\nthe loss that maximizes head disagreement and\nobtain improved results. While not considering\nTransformer attentions, Jain and Wallace (2019)\nshow that traditional attention mechanisms do not\nnecessarily improve interpretability since softmax\nattention is vulnerable to an adversarial attack\nleading to wildly different model predictions\nfor the same attention weights. Sparse attention\nmay mitigate these issues; however, our work\nfocuses mostly on a more mechanical aspect of\ninterpretation by analyzing head behavior, rather\nthan on explanations for predictions.\n7 Conclusion and Future Work\nWe contribute a novel strategy for adaptively sparse\nattention, and, in particular, for adaptively sparse\nTransformers. We present the ﬁrst empirical analy-\nsis of Transformers with sparse attention mappings\n(i.e., entmax), showing potential in both translation\naccuracy as well as in model interpretability.\nIn particular, we analyzed how the attention\nheads in the proposed adaptively sparse Trans-\nformer can specialize more and with higher con-\nﬁdence. Our adaptivity strategy relies only on\ngradient-based optimization, side-stepping costly\nper-head hyper-parameter searches. Further speed-\nups are possible by leveraging more parallelism in\nthe bisection algorithm for computing α-entmax.\nFinally, some of the automatically-learned be-\nhaviors of our adaptively sparse Transformers – for\ninstance, the near-deterministic positional heads or\nthe subword joining head – may provide new ideas\nfor designing static variations of the Transformer.\nAcknowledgments\nThis work was supported by the European Re-\nsearch Council (ERC StG DeepSPIN 758969),\nand by the Funda c ¸˜ao para a Ci ˆencia e Tecnolo-\ngia through contracts UID/EEA/50008/2019 and\nCMUPERI/TIC/0046/2014 (GoLocal). We are\ngrateful to Ben Peters for the α-entmax code and\nErick Fonseca, Marcos Treviso, Pedro Martins, and\nTsvetomila Mihaylova for insightful group discus-\nsion. We thank Mathieu Blondel for the idea to\nlearn α. We would also like to thank the anony-\nmous reviewers for their helpful feedback.\nReferences\nBrandon Amos and J. Zico Kolter. 2017. OptNet:\nDifferentiable optimization as a layer in neural net-\nworks. In Proc. ICML.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. ICLR.\nMathieu Blondel, Andr ´e FT Martins, and Vlad Nicu-\nlae. 2019. Learning classiﬁers with Fenchel-Young\nlosses: Generalized entropies, margins, and algo-\nrithms. In Proc. AISTATS.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, et al. 2014. Findings of the 2014\nworkshop on statistical machine translation. In Proc.\nWorkshop on Statistical Machine Translation.\nOndrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, et al. 2016. Findings of the\n2016 conference on machine translation. In Proc.\nWMT.\nM Cettolo, M Federico, L Bentivogli, J Niehues,\nS St ¨uker, K Sudoh, K Yoshino, and C Federmann.\n2017. Overview of the IWSLT 2017 evaluation cam-\npaign. In Proc. IWSLT.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse Transformers. preprint arXiv:1904.10509.\nFrank H Clarke. 1990. Optimization and Nonsmooth\nAnalysis. SIAM.\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and\nAlexander Rush. 2018. Latent alignment and varia-\ntional attention. In Proc. NeurIPS.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. NAACL-HLT.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proc. ICML.\nStephen Gould, Basura Fernando, Anoop Cherian, Pe-\nter Anderson, Rodrigo Santa Cruz, and Edison Guo.\n2016. On differentiating parameterized argmin and\nargmax problems with application to bi-level opti-\nmization. preprint arXiv:1607.05447.\nMichael Held, Philip Wolfe, and Harlan P Crowder.\n1974. Validation of subgradient optimization. Math-\nematical Programming, 6(1):62–88.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. In Proc. NAACL-HLT.\nMarcin Junczys-Dowmunt, Kenneth Heaﬁeld, Hieu\nHoang, Roman Grundkiewicz, and Anthony Aue.\n2018. Marian: Cost-effective high-quality neural\nmachine translation in C++. In Proc. WNMT.\nJian Li, Zhaopeng Tu, Baosong Yang, Michael R Lyu,\nand Tong Zhang. 2018. Multi-Head Attention with\nDisagreement Regularization. In Proc. EMNLP.\nChristos Louizos, Max Welling, and Diederik P\nKingma. 2018. Learning sparse neural networks\nthrough L0 regularization. Proc. ICLR.\nMinh-Thang Luong, Hieu Pham, and Christopher D\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. In Proc. EMNLP.\nChaitanya Malaviya, Pedro Ferreira, and Andr ´e FT\nMartins. 2018. Sparse and constrained attention for\nneural machine translation. In Proc. ACL.\nDavid Mare ˇcek and Rudolf Rosa. 2018. Extract-\ning syntactic trees from Transformer encoder self-\nattentions. In Proc. BlackboxNLP.\nAndr´e FT Martins and Ram ´on Fernandez Astudillo.\n2016. From softmax to sparsemax: A sparse model\nof attention and multi-label classiﬁcation. In Proc.\nof ICML.\nSameen Maruf, Andr ´e FT Martins, and Gholam-\nreza Haffari. 2019. Selective attention for\ncontext-aware neural machine translation. preprint\narXiv:1903.08788.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nVlad Niculae and Mathieu Blondel. 2017. A regular-\nized framework for sparse and structured neural at-\ntention. In Proc. NeurIPS.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proc. WMT.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proc. ACL.\nBen Peters, Vlad Niculae, and Andr´e FT Martins. 2019.\nSparse sequence-to-sequence models. In Proc. ACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\npreprint.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in Transformer-\nbased machine translation. In Proc. BlackboxNLP.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. ACL.\nWenqi Shao, Tianjian Meng, Jingyu Li, Ruimao Zhang,\nYudian Li, Xiaogang Wang, and Ping Luo. 2019.\nSSN: Learning sparse switchable normalization via\nSparsestMax. In Proc. CVPR.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive At-\ntention Span in Transformers. In Proc. ACL.\nGongbo Tang, Mathias M¨uller, Annette Rios, and Rico\nSennrich. 2018. Why self-attention? A targeted\nevaluation of neural machine translation architec-\ntures. In Proc. EMNLP.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProc. ACL.\nConstantino Tsallis. 1988. Possible generalization of\nBoltzmann-Gibbs statistics. Journal of Statistical\nPhysics, 52:479–487.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NeurIPS.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine transla-\ntion learns anaphora resolution. In Proc. ACL.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proc. ACL.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less atten-\ntion with lightweight and dynamic convolutions. In\nProc. ICLR.\nSupplementary Material\nA High-Level Statistics Analysis of Other Language Pairs\n0\n10\n20\n30\nEncoder\nSelf-Attention\n0\n10\n20\n30\nContext\nAttention\n1.0 1.2 1.4 1.6 1.8 2.0\n0\n10\n20\n30\nDecoder\nSelf-Attention\n(a) WMT 2016 RO\u0001EN.\n0\n10\n20\n30\nEncoder\nSelf-Attention\n0\n10\n20\n30\nContext\nAttention\n1.0 1.2 1.4 1.6 1.8 2.0\n0\n10\n20\n30\nDecoder\nSelf-Attention (b) KFTT JA\u0001EN.\n0\n10\n20\nEncoder\nSelf-Attention\n0\n10\n20\nContext\nAttention\n1.0 1.2 1.4 1.6 1.8 2.0\n0\n10\n20\nDecoder\nSelf-Attention\n(c) WMT 2014 EN\u0001DE.\n0\n10\n20\n30\nEncoder\nSelf-Attention\n0\n10\n20\n30\nContext\nAttention\n1.0 1.2 1.4 1.6 1.8 2.0\n0\n10\n20\n30\nDecoder\nSelf-Attention (d) IWSLT 2017 DE\u0001EN.\nFigure 11: Histograms of αvalues.\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0 0.5 1.0\n-entmax\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nContext\nAttention\n0.0 0.5 1.0\n0.0 0.5 1.0\ndensity\n0\n10k\n30k\n50k\nDecoder\nSelf-Attention\n0.0 0.5 1.0\ndensity\n(a) WMT 2016 RO\u0001EN.\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0 0.5 1.0\n-entmax\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nContext\nAttention\n0.0 0.5 1.0\n0.0 0.5 1.0\ndensity\n0\n10k\n30k\n50k\nDecoder\nSelf-Attention\n0.0 0.5 1.0\ndensity (b) KFTT JA\u0001EN.\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0 0.5 1.0\n-entmax\n0.0 0.5 1.0\n0\n10k\n30k\n50k\nContext\nAttention\n0.0 0.5 1.0\n0.0 0.5 1.0\ndensity\n0\n10k\n30k\n50k\nDecoder\nSelf-Attention\n0.0 0.5 1.0\ndensity\n(c) WMT 2014 EN\u0001DE.\n0.0 0.5 1.0\n0\n50k\n100k\n150k\nEncoder\nSelf-Attention\n1.5-entmax\n0.0 0.5 1.0\n-entmax\n0.0 0.5 1.0\n0\n50k\n100k\n150k\nContext\nAttention\n0.0 0.5 1.0\n0.0 0.5 1.0\ndensity\n0\n50k\n100k\n150k\nDecoder\nSelf-Attention\n0.0 0.5 1.0\ndensity (d) IWSLT 2017 DE\u0001EN.\nFigure 12: Histograms of head densities.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.3\n0.4\n0.5\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.3\n0.4\nDecoder\nSelf-Attention\n(a) WMT 2016 RO\u0001EN.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.2\n0.3\n0.4\n0.5\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.25\n0.30\n0.35\n0.40\nDecoder\nSelf-Attention\n (b) KFTT JA\u0001EN.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.20\n0.25\n0.30\n0.35\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.25\n0.30\n0.35\nDecoder\nSelf-Attention\n(c) WMT 2014 EN\u0001DE.\n0.4\n0.5\nEncoder\nSelf-Attention\nsoftmax\n1.5-entmax\n-entmax\n0.3\n0.4\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.25\n0.30\n0.35\nDecoder\nSelf-Attention\n (d) IWSLT 2017 DE\u0001EN.\nFigure 13: Jensen-Shannon divergence over layers.\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1 2 3 4 5 6\nLayers\n(a) WMT 2016 RO\u0001EN.\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1 2 3 4 5 6\nLayers (b) KFTT JA\u0001EN.\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1 2 3 4 5 6\nLayers\n(c) WMT 2014 EN\u0001DE.\n0.0\n0.5\n1.0\nEncoder\nSelf-Attention\nfixed = 1.5\n learned \n0.0\n0.5\n1.0\nContext\nAttention\n1 2 3 4 5 6\nLayers\n0.0\n0.5\n1.0\nDecoder\nSelf-Attention\n1 2 3 4 5 6\nLayers (d) IWSLT 2017 DE\u0001EN.\nFigure 14: Head densities over layers.\nB Background\nB.1 Regularized Fenchel-Young prediction functions\nDeﬁnition 1 (Blondel et al. 2019). Let Ω: △d →R ∪{∞}be a strictly convex regularization function.\nWe deﬁne the prediction function πΩ as\nπΩ(z) = argmax\np∈△d\n(\np⊤z−Ω(p)\n)\n(12)\nB.2 Characterizing the α-entmax mapping\nLemma 1 (Peters et al. 2019). For any z, there exists a unique τ⋆ such that\nα-entmax(z) = [(α−1)z−τ⋆1]\n1/α−1\n+ . (13)\nProof: From the deﬁnition of α-entmax,\nα-entmax(z) := argmax\np∈△d\np⊤z+ HT\nα(p), (14)\nwe may easily identify it with a regularized prediction function (Def. 1):\nα-entmax(z) ≡π−HTα(z).\nWe ﬁrst note that for all p∈△d,\n−(α−1)HT\nα(p) = 1\nα\nd∑\ni=1\npα\ni + const. (15)\nFrom the constant invariance and scaling properties of πΩ (Blondel et al., 2019, Proposition 1, items 4–5),\nπ−HTα(z) = πΩ((α−1)z), with Ω(p) =\nd∑\nj=1\ng(pj), g (t) = tα\nα.\nUsing (Blondel et al., 2019, Proposition 5), noting that g′(t) = tα−1 and (g′)−1(u) = u\n1/α−1, yields\nπΩ(z) = [z−τ⋆1]\n1/α−1\n+ , and therefore α-entmax(z) = [(α−1)z−τ⋆1]\n1/α−1\n+ . (16)\nSince HT\nα is strictly convex on the simplex, α-entmax has a unique solution p⋆. Equation 16 implicitly\ndeﬁnes a one-to-one mapping between p⋆ and τ⋆ as long as p⋆ ∈△, therefore τ⋆ is also unique.\nB.3 Connections to softmax and sparsemax\nThe Euclidean projection onto the simplex, sometimes referred to, in the context of neural attention, as\nsparsemax (Martins and Astudillo, 2016), is deﬁned as\nsparsemax(z) := argmin\np∈△\n∥p−z∥2\n2. (17)\nThe solution can be characterized through the unique threshold τ such that ∑\nisparsemax(z)i = 1 and\n(Held et al., 1974)\nsparsemax(z) = [z−τ1]+ . (18)\nThus, each coordinate of the sparsemax solution is a piecewise-linear function. Visibly, this expression\nis recovered when setting α = 2 in the α-entmax expression (Equation 21); for other values of α, the\nexponent induces curvature.\nOn the other hand, the well-known softmax is usually deﬁned through the expression\nsoftmax(z)i := exp(zi)∑\nj exp(zj), (19)\nwhich can be shown to be the unique solution of the optimization problem\nsoftmax(z)i = argmax\np∈△\np⊤z+ HS(p), (20)\nwhere HS(p) := −∑\nipilog pi is the Shannon entropy. Indeed, setting the gradient to 0 yields the\ncondition log pi = zj−νi−τ−1, where τ and ν >0 are Lagrange multipliers for the simplex constraints∑\nipi = 1 and pi ≥0, respectively. Since the l.h.s. is only ﬁnite for pi >0, we must have νi = 0 for all i,\nby complementary slackness. Thus, the solution must have the form pi = exp(zi)/Z, yielding Equation 19.\nC Jacobian of α-entmax w.r.t. the shape parameter α: Proof of Proposition 1\nRecall that the entmax transformation is deﬁned as:\nα-entmax(z) := argmax\np∈△d\np⊤z+ HT\nα(p), (21)\nwhere α≥1 and HT\nα is the Tsallis entropy,\nHT\nα(p):=\n{ 1\nα(α−1)\n∑\nj\n(\npj −pα\nj\n)\n, α̸= 1,\nHS(p), α = 1,\n(22)\nand HS(p) := −∑\nj pj log pj is the Shannon entropy.\nIn this section, we derive the Jacobian of entmax with respect to the scalar parameter α.\nC.1 General case of α> 1\nFrom the KKT conditions associated with the optimization problem in Eq. 21, we have that the solution\np⋆ has the following form, coordinate-wise:\np⋆\ni = [(α−1)(zi −τ⋆)]1/(α−1)\n+ , (23)\nwhere τ⋆ is a scalar Lagrange multiplier that ensures that p⋆ normalizes to 1, i.e., it is deﬁned implicitly\nby the condition: ∑\ni\n[(α−1)(zi −τ⋆)]1/(α−1)\n+ = 1. (24)\nFor general values of α, Eq. 24 lacks a closed form solution. This makes the computation of the Jacobian\n∂α-entmax(z)\n∂α (25)\nnon-trivial. Fortunately, we can use the technique of implicit differentiation to obtain this Jacobian.\nThe Jacobian exists almost everywhere, and the expressions we derive expressions yield a generalized\nJacobian (Clarke, 1990) at any non-differentiable points that may occur for certain (α, z) pairs. We begin\nby noting that ∂p⋆\ni\n∂α = 0 if p⋆\ni = 0, because increasing αkeeps sparse coordinates sparse.4 Therefore we\nneed to worry only about coordinates that are in the support of p⋆. We will assume hereafter that the ith\ncoordinate of p⋆ is non-zero. We have:\n∂p⋆\ni\n∂α = ∂\n∂α[(α−1)(zi −τ⋆)]\n1\nα−1\n= ∂\n∂α exp\n[ 1\nα−1 log[(α−1)(zi −τ⋆)]\n]\n= p⋆\ni\n∂\n∂α\n[ 1\nα−1 log[(α−1)(zi −τ⋆)]\n]\n= p⋆\ni\n(α−1)2\n[ ∂\n∂α[(α−1)(zi −τ⋆)]\nzi −τ⋆ −log[(α−1)(zi −τ⋆)]\n]\n= p⋆\ni\n(α−1)2\n[\nzi −τ⋆ −(α−1)∂τ⋆\n∂α\nzi −τ⋆ −log[(α−1)(zi −τ⋆)]\n]\n= p⋆\ni\n(α−1)2\n[\n1 − α−1\nzi −τ⋆\n∂τ⋆\n∂α −log[(α−1)(zi −τ⋆)]\n]\n. (26)\nWe can see that this Jacobian depends on ∂τ⋆\n∂α , which we now compute using implicit differentiation.\nLet S= {i: p⋆\ni >0}). By differentiating both sides of Eq. 24, re-using some of the steps in Eq. 26, and\nrecalling Eq. 23, we get\n0 =\n∑\ni∈S\n∂\n∂α[(α−1)(zi −τ⋆)]1/(α−1)\n=\n∑\ni∈S\np⋆\ni\n(α−1)2\n[\n1 − α−1\nzi −τ⋆\n∂τ⋆\n∂α −log[(α−1)(zi −τ⋆)]\n]\n= 1\n(α−1)2 −∂τ⋆\n∂α\n∑\ni∈S\np⋆\ni\n(α−1)(zi −τ⋆) −\n∑\ni∈S\np⋆\ni\n(α−1)2 log[(α−1)(zi −τ⋆)]\n= 1\n(α−1)2 −∂τ⋆\n∂α\n∑\ni\n(p⋆\ni)2−α −\n∑\ni\np⋆\ni\nα−1 log p⋆\ni\n= 1\n(α−1)2 −∂τ⋆\n∂α\n∑\ni\n(p⋆\ni)2−α + HS(p∗)\nα−1 , (27)\nfrom which we obtain:\n∂τ⋆\n∂α =\n1\n(α−1)2 + HS(p⋆)\nα−1∑\ni(p⋆\ni)2−α . (28)\nFinally, plugging Eq. 28 into Eq. 26, we get:\n∂p⋆\ni\n∂α = p⋆\ni\n(α−1)2\n[\n1 − 1\n(p⋆\ni)α−1\n∂τ⋆\n∂α −(α−1) logp⋆\ni\n]\n= p⋆\ni\n(α−1)2\n\n1 − 1\n(p⋆\ni)α−1\n1\n(α−1)2 + HS(p⋆)\nα−1∑\ni(p⋆\ni)2−α −(α−1) logp⋆\ni\n\n\n= p⋆\ni −˜pi(α)\n(α−1)2 −p⋆\ni log p⋆\ni + ˜pi(α)HS(p⋆)\nα−1 , (29)\n4This follows from the margin property of HT\nα (Blondel et al., 2019).\nwhere we denote by\n˜pi(α) = (p⋆\ni)2−α\n∑\nj(p⋆\nj)2−α. (30)\nThe distribution ˜p(α) can be interpreted as a “skewed” distribution obtained fromp⋆, which appears in\nthe Jacobian of α-entmax(z) w.r.t. zas well (Peters et al., 2019).\nC.2 Solving the indetermination for α= 1\nWe can write Eq. 29 as\n∂p⋆\ni\n∂α = p⋆\ni −˜pi(α) −(α−1)(p⋆\ni log p⋆\ni + ˜pi(α)HS(p⋆))\n(α−1)2 . (31)\nWhen α→1+, we have ˜p(α) →p⋆, which leads to a 0\n0 indetermination.\nTo solve this indetermination, we will need to apply L’H ˆopital’s rule twice. Let us ﬁrst compute the\nderivative of ˜pi(α) with respect to α. We have\n∂\n∂α(p⋆\ni)2−α = −(p⋆\ni)2−αlog p⋆\ni, (32)\ntherefore\n∂\n∂α˜pi(α) = ∂\n∂α\n(p⋆\ni)2−α\n∑\nj(p⋆\nj)2−α\n=\n−(p⋆\ni)2−αlog p⋆\ni\n∑\nj(p⋆\nj)2−α + (p⋆\ni)2−α∑\nj(p⋆\nj)2−αlog p⋆\nj\n(∑\nj(p⋆\nj)2−α\n)2\n= −˜pi(α) logp⋆\ni + ˜pi(α)\n∑\nj\n˜pj(α) logp⋆\nj. (33)\nDifferentiating the numerator and denominator in Eq. 31, we get:\n∂p⋆\ni\n∂α = lim\nα→1+\n(1 + (α−1)HS(p⋆))˜pi(α)(log p⋆\ni −∑\nj ˜pj(α) logp⋆\nj) −p⋆\ni log p⋆\ni −˜pi(α)HS(p⋆)\n2(α−1)\n= A+ B, (34)\nwith\nA = lim\nα→1+\nHS(p⋆)˜pi(α)(log p⋆\ni −∑\nj ˜pj(α) logp⋆\nj)HS(p⋆)\n2\n= HS(p⋆)p⋆\ni log p⋆\ni + p⋆\ni(HS(p⋆))2\n2 , (35)\nand\nB = lim\nα→1+\n˜pi(α)(log p⋆\ni −∑\nj ˜pj(α) logp⋆\nj) −p⋆\ni log p⋆\ni −˜pi(α)HS(p⋆)\n2(α−1) . (36)\nWhen α→1+, Bbecomes again a 0\n0 indetermination, which we can solve by applying again L’Hˆopital’s\nrule. Differentiating the numerator and denominator in Eq. 36:\nB = 1\n2 lim\nα→1+\n\n\n˜pi(α) logp⋆\ni\n\n∑\nj\n˜pj(α) logp⋆\nj −log p⋆\ni\n\n\n−˜pi(α)\n\n∑\nj\n˜pj(α) logp⋆\nj −log p⋆\ni\n\n\n\n∑\nj\n˜pj(α) logp⋆\nj + HS(p⋆)\n\n\n−˜pi(α)\n∑\nj\n˜pj(α) logp⋆\nj\n(∑\nk\n˜pk(α) logp⋆\nk −log p⋆\nj\n)\n\n\n=\n−p⋆\ni log p⋆\ni(HS(p⋆) + logp⋆\ni) + p⋆\ni\n∑\nj p⋆\nj log p⋆\nj(HS(p⋆) + logp⋆\nj)\n2\n=\n−HS(p⋆)p⋆\ni log p⋆\ni −p⋆\ni(HS(p⋆))2 −p⋆\ni log2 p⋆\ni + p⋆\ni\n∑\nj p⋆\nj log2 p⋆\nj\n2 . (37)\nFinally, summing Eq. 35 and Eq. 37, we get\n∂p⋆\ni\n∂α\n⏐⏐⏐⏐\nα=1\n=\n−p⋆\ni log2 p⋆\ni + p⋆\ni\n∑\nj p⋆\nj log2 p⋆\nj\n2 . (38)\nC.3 Summary\nTo sum up, we have the following expression for the Jacobian ofα-entmax with respect to α:\n∂p⋆\ni\n∂α =\n\n\n\np⋆\ni−˜pi(α)\n(α−1)2 −p⋆\ni log p⋆\ni+˜pi(α)HS(p⋆)\nα−1 , for α> 1\n−p⋆\ni log2 p⋆\ni+p⋆\ni\n∑\njp⋆\nj log2 p⋆\nj\n2 , for α= 1.\n(39)"
}