{
  "title": "Molecule generation using transformers and policy gradient reinforcement learning",
  "url": "https://openalex.org/W4378909195",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3045102892",
      "name": "Eyal Mazuz",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A2804232609",
      "name": "Guy Shtar",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A1966632966",
      "name": "Bracha Shapira",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A1979308116",
      "name": "Lior Rokach",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A3045102892",
      "name": "Eyal Mazuz",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A2804232609",
      "name": "Guy Shtar",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A1966632966",
      "name": "Bracha Shapira",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    },
    {
      "id": "https://openalex.org/A1979308116",
      "name": "Lior Rokach",
      "affiliations": [
        "Ben-Gurion University of the Negev"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4232607903",
    "https://openalex.org/W2909240409",
    "https://openalex.org/W1977242738",
    "https://openalex.org/W4248107770",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2773987374",
    "https://openalex.org/W4321349946",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2176516200",
    "https://openalex.org/W2153693853",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2558999090",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W3007309629",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W3104956673",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W2135732933",
    "https://openalex.org/W3100751385"
  ],
  "abstract": "Abstract Generating novel valid molecules is often a difficult task, because the vast chemical space relies on the intuition of experienced chemists. In recent years, deep learning models have helped accelerate this process. These advanced models can also help identify suitable molecules for disease treatment. In this paper, we propose Taiga, a transformer-based architecture for the generation of molecules with desired properties. Using a two-stage approach, we first treat the problem as a language modeling task of predicting the next token, using SMILES strings. Then, we use reinforcement learning to optimize molecular properties such as QED. This approach allows our model to learn the underlying rules of chemistry and more easily optimize for molecules with desired properties. Our evaluation of Taiga, which was performed with multiple datasets and tasks, shows that Taiga is comparable to, or even outperforms, state-of-the-art baselines for molecule optimization, with improvements in the QED ranging from 2 to over 20 percent. The improvement was demonstrated both on datasets containing lead molecules and random molecules. We also show that with its two stages, Taiga is capable of generating molecules with higher biological property scores than the same model without reinforcement learning.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports\nMolecule generation using \ntransformers and policy gradient \nreinforcement learning\nEyal Mazuz *, Guy Shtar , Bracha Shapira  & Lior Rokach \nGenerating novel valid molecules is often a difficult task, because the vast chemical space relies on the \nintuition of experienced chemists. In recent years, deep learning models have helped accelerate this \nprocess. These advanced models can also help identify suitable molecules for disease treatment. In \nthis paper, we propose Taiga, a transformer-based architecture for the generation of molecules with \ndesired properties. Using a two-stage approach, we first treat the problem as a language modeling \ntask of predicting the next token, using SMILES strings. Then, we use reinforcement learning to \noptimize molecular properties such as QED. This approach allows our model to learn the underlying \nrules of chemistry and more easily optimize for molecules with desired properties. Our evaluation of \nTaiga, which was performed with multiple datasets and tasks, shows that Taiga is comparable to, \nor even outperforms, state-of-the-art baselines for molecule optimization, with improvements in \nthe QED ranging from 2 to over 20 percent. The improvement was demonstrated both on datasets \ncontaining lead molecules and random molecules. We also show that with its two stages, Taiga is \ncapable of generating molecules with higher biological property scores than the same model without \nreinforcement learning.\nA major challenge in drug discovery is designing drugs with the desired properties. The chemical space of \npotential drug-like molecules is between 1023 to 1060 , of which about 108 of molecules are  synthesized1,2. Addi-\ntionally, the average cost of developing a new drug is one to two billion US dollars, and the average development \ntime is 13  years3. Traditionally, chemists and pharmacologists use their intuition and expertise to identify new \n molecules4. While Lipinski’s “rule of five”5 may reduce the number of possible drug-like molecules, the search \nspace remains large. In order to narrow the space further, high-throughput screening (HTS) is used; however, \nthe task remains daunting. Additionally, computational methods can be used to narrow the drug search space \nand shorten the time needed to develop new drugs.\nIn recent years, there have been many attempts to use deep learning, particularly generative models, for drug \n design6,7. However, the task of generating optimized and valid molecules using computational methods remains \nchallenging due to the large search space and the small number of labeled samples.\nThere have been several attempts to use SMILES (simplified molecular-input line-entry system) strings as a \nrepresentation for  molecules8. For example, Gomez et al. tried using generative models based on SMILES strings \nfor the molecule generation task. However, the proposed methods only managed to generate a low percentage of \nmolecules that were considered valid due to the complicated grammatical rules of  SMILES6,9.\nRecently, the use of reinforcement learning (RL) has gained attention due to its ability to solve a wide range \nof problems such as playing the game of  Go10 and operating machines. RL systems excel in these tasks thanks \nto their ability to make sequential decisions and maximize defined long-term rewards; this allows for the direct \noptimization of desirable new drug properties that are not derived from the model itself when using generative \nmodels such as recurrent neural networks (RNNs).\nLater, RL optimization was incorporated into SMILES generation methods to generate molecules with desired \nproperties, such as high IC50 values for JAK2, using a recurrent neural network (RNN) 11 such optimization is \ntechnically challenging, since it tends to cause the model to converge toward a set of primarily invalid molecules, \nsince RNNs cannot handle long sequences.\nTo improve the rate of valid molecules generated, other studies constrained the input of generative models \nwhen producing molecules by forcing the model to adhere to certain rules when generating molecules. Some \nstudies proposed the use of variational autoencoders (V AEs) to generate valid molecules by learning the distribu-\ntion of a latent space and sampling from it, instead of sequentially generating the molecule token by  token12,13. \nHowever, the validity rate of these methods was relatively low; these results could be explained by the lower \nOPEN\nBen-Gurion University of the Negev, Beersheba, Israel. *email: mazuze@post.bgu.ac.il\n2\nVol:.(1234567890)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nvalidity rate obtained in those studies for unseen molecules compared to known ones. To address this issue, the \nauthors proposing the junction tree variational autoencoder (JTV AE)7 represented molecules as junction trees \nin order to encode the sub-spaces of the molecular graph. This allowed the decoder to generate valid molecules \nby utilizing only valid components while considering how they interact.\nTwo approaches exist to deal with the issue of long sequences presented by RNN models. The first is the use \nof factional-based architectures which are a wavelet-based architecture that compresses the associated operator’s \nkernel using fine-grained wavelets and thus handles long sequence  data14,15. The other approach, which is now \nde-facto the gold standard in neural language processing (NLP), is the use of transformer  models16,17 which \nachieve state-of-the-art results on many NLP tasks.\nIn this paper, we propose a new RL-based method to generate molecules with desired properties, which \novercomes the problem of generating valid molecules with desired properties. We use a transformer-based archi-\ntecture, utilizing SMILES string representations in a two-stage approach. To the best of our knowledge, this is the \nfirst application that utilizes both transformer models and reinforcement learning together for molecule graph \ngeneration. In the first stage, the model learns to embed these discrete string representations in a vector space. \nThen, in the second stage, the model optimizes the vector space in order to generate molecules with the desired \nproperties, such as QED (quantitative estimate of drug-likeness) or pIC50. The use of an attention mechanism \nallows our method to gain an understanding of the underlying chemical rules that make a valid molecule by \nperforming a simple language modeling task, using just a small amount of data. Then, the understanding gained \nregarding those rules, along with policy gradient RL, is used to generate molecules with the desired properties.\nWe evaluate our model on multiple datasets with various properties on the tasks of molecule generation and \noptimization for the desired properties and compare it to several state-of-the-art  approaches1–3,7,18,19 that use \ndifferent representations and techniques for molecule generation. We demonstrate our model’s ability to gener-\nate a high percentage of valid molecules and rival methods that use other techniques to ensure the generation \nof valid molecules. Additionally, unlike previous research that only focuses on the top molecules generated, \nwe show our model’s ability to generate large number of molecules with a high mean QED, which defines how \ndrug-like a molecule is, while maintaining a low synthetic accessibility score, a theoretic score of how hard it is \nto synthesize the molecule.\nIn the task of optimizing a biological property (i.e., IC50), we show that Taiga is capable of improving existing \nmolecules and generating molecules with the desired biological properties. Our main contributions are as follows:\n• Introducing Taiga, an RL-based method that utilizes the transformer architecture, which is capable of gen -\nerating novel and diverse molecules.\n• Demonstrating that the use of an attention mechanism combined with policy gradient RL can overcome the \nexisting challenges of generating valid molecules represented as SMILES strings.\n• Performing extensive experiments using several datasets with a range of properties and multiple metrics to \nevaluate the performance of our method’s components (Fig. 1).\nResults\nIn this section, to evaluate Taiga’s performance, we validate it on 2 common tasks: molecule generation and \nmolecule optimization, for the molecule generation task we compare Taiga to the performance of several SOTA \nbaselines using multiple datasets and metrics.\nMolecule generation. 1. Data: We used three datasets in our experiments: the  Moses20,  Zinc21, and \n GDB1322 datasets.\n• The Moses dataset consists of 1.6M training set molecules extracted from the Zinc clean lead collection.\n• The Zinc dataset consists of 250K molecules extracted from the Zinc database.\n• The GDB13 rand dataset consists of 1M random molecules extracted from the larger GDB13 database.\nThese datasets differ from one another in terms of the number and type of molecules included. Experimenting \non these different datasets allows us to demonstrate the generalization ability of the methods evaluated.\n2. Baselines: We compared our method to various approaches: (1)  GCPN1, a method that uses proximal \npolicy optimization (PPO) on molecular graphs, with the addition of adversarial loss; (2)  JTV AE7, a method \nwhich uses the junction tree algorithm to generate molecular graphs with autoencoders; (3)  MolGPT2, a method \nthat generates SMILES using transformer architecture only; (4)  MolGAN18, a method that generates molecular \ngraphs using GANs and DDPG; (5)  MolDQN3, a method that works on molecular graphs using Q-learning; (6) \n GraphDF19 a discrete normalizing flow method; and (7) an LSTM version of our method which also uses policy \ngradient RL, similar  to11. We ran all models with their released code, optimizing for the same target property \n(i.e., QED), each with its respective reward function, as described in the original papers, on the same hardware \ncontaining one TITAN RTX GPU, 60GB RAM, and an eight-core CPU.\n3. Model Configuration: Taiga consists of four decoder layers, each with eight attention heads, a feedforward \nsize of 1024, and an initial embedding size of 512, as those parameters are frequently used in transformer models \nand were not optimized during this research. The model was trained for three epochs on the language modeling \ntask and 100 steps, with each step averaging 500 episodes (molecules) in the RL stage. We use the REINFORCE \nalgorithm with a discount factor of 0.99 and a maximum sequence size of 150 characters, we used character-level \ntokenization for all our experiments. The LSTM-PG uses the same hyperparameters as Taiga for all experiments.\nWe designed the following reward function for our model:\n3\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\n4. Metrics:  We utilized the following widely used metrics to evaluate the methods:\n• Validity: the percentage of valid molecules a model is able to generate. If RDKit was able to process the \nSMILES string and generate a Molecule object, then the validity of that molecule is set to 1. Otherwise, \nvalidity is set to 0.\n• Novelty: the percentage of molecules generated that do not appear in the training data.\n• Diversity: the percentage of unique molecules the model can generate .\n• Quantitative Estimation of Drug-Likeness (QED), the geometric mean of eight common molecule properties \n(molecular weight, AlogP , hydrogen bond donors, hydrogen donor acceptors, molecular polar surface area, \nrotatable bonds, aromatic rings, structural alerts), that can estimate how well the molecule behaves in the \nhuman body, This value can be range from 0 to 1, and the higher the QED value, the similar a molecule is to \nexisting drugs and will have higher chance surviving the drug-discovery  process23.\n• Synthetic Accessibility Score (SAS), an estimation of the how easy it is to synthesize a molecule by calculating \nthe fragment score penalized by the complexity of the molecule; it ranges from 0 to 10, where a molecule \nwith an SAS value above 6 is considered challenging to  synthesize24.\nWe calculated the QED and SAS metrics after removing all of the invalid molecules from the set of generated \nmolecules. Novelty, Validity and Diversity are all crucial metrics for assessing performance of generative models \nin the drug discovery. These metrics have different use meanings, since a model can have 100% diversity with \n0% novelty by just generating all molecules from the dataset it was training on. and vice versa, a model can \nhave 100% novelty but 0% diversity by generating only 1 unique molecule but generating over and over again. \nFor all methods, we generated the molecules after the optimization stage. For each method, we generated 25K \nmolecules to calculate the metrics.\n5. Results:  Table 2 , Figure 2 and Supplementary Figure S1 presents the results for molecule generation and \noptimization across all models and datasets , and the aggregate results of the validity, QED, and diversity for all \nmodels. Supplementary Table S1 and Supplementary Figure S2 present the results of the top-3 molecules gener-\nated with each dataset in terms of their respective QED scores. As can be seen in Table  2 and Supplementary \nFigure S1, on the GDB13 dataset, which has a lower mean QED than the other datasets (see Table 1), Taiga is the \nonly method that was able to obtain QED scores higher than the dataset’s mean QED score. However, Taiga wasn’t \n(1)R(sT ) =\n{\n10 ∗ QED (sT ), ifsT is valid molecule\n0, otherwise\nStage 1S tage 2\n宆宒宑宀宆孴宆宆宆孫宒孬宆宆孴\n孳孱孳孺 宆\n孳孱孳孶 学\n孳孱孼 孫\n孳 实宅宒宖宠\n孳 实守宒宖宠\n孳\na: Molecule generation process\n宄宪宨宱宷宖宷室宷宨 宄宦宷宬宲宱 宑宨宻宷季家宷室宷宨宕 宨宺室宵宧\n宄宪宨宱宷宖宷室宷宨 宄宦宷宬宲宱 宑宨宻宷季家宷室宷宨宕 宨宺室宵宧\n孳孱孳孺 宆\n孳孱孳孶 学\n孳 孫\n孳 实宅宒宖宠\n孳孱孼 实守宒宖宠\nb: Last generation step and reward calculation\nc n\n0.674\n宓宵宲害宨\n宵宷宼季宓宵宨宧宬宦宷\n宲宵\n孫宨孱宪\n孱季宄宱宷宬季宦室宱宦宨宵孬\n宐宲宯宨\n宦宸宯室宵季宓\n宵宲害宨宵宷\n宼\n孫宨孱宪孱季宔\n守宇孬\n宒宋\n宒\n宑\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孴\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学宱\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孵\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孶\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学宱孰孴\n宐室家宮宨宧季宄宷宷宨宱宷宬宲宱\n安宨宨宧季安宲宵宺室宵宧\n宕宨宏官\n宏宬宱宨室宵\n宏宬宱宨室宵\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孴\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学宱\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孵\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学孶\n宄宷宷宨宱宷宬宲宱\n宋宨室宧季学宱孰孴\n宐室家宮宨宧季宄宷宷宨宱宷宬宲宱\n安宨宨宧季安宲宵宺室宵宧\n实宅宒宖宠 宆宒 宑宀 宆孴宆宆宆 孴\n宆 宒宑 宀 宆孴宆 宆宆 孴 实守宒宖宠\n宇宨宦宲宧宨宵\n宅宯宲宦宮季学孴\n宇宨宦宲宧宨宵\n宅宯宲宦宮季学宱\n宗室宬宪室\n宱\n室\n宱\n室\nFigure 1.  Overview of the training process. Stage 1: We train the transformer model on the language-modeling \ntask of predicting the next token , e.g. in the figure when the last input token is ‘1’ Taiga tries to predict the next \ntoken in the sequence which is ‘C’ , since it’s an auto-regressive model, it can only attend to previous tokens, \ni.e. those in the green squares indicated by the blue dotted line, and not the subsequent tokens in the gray \nsquares. The next token prediction is done in parallel because of the attention mechanism. Stage 2: (a) The agent \nreceives the SMILES string and predicts the next token by sampling from the output distribution, which is then \nappended to the SMILES string to create the next state. A reward of zero is given to the molecule at any step \nthat is not the final step. (b) The agent completes the molecule generation process by predicting a [EOS] token \n(which is not appended to the SMILES string), and a reward greater than zero is provided either by computing \nthe property directly from the molecule by using libraries such as RDKit or by feeding the generated molecule to \nan external property predictor (e.g. Chemprop) which generates the reward for the molecule.\n4\nVol:.(1234567890)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nable to improve the SAS scores of the generated molecules relatively to the dataset’s mean SAS score, and achieved \nlower SAS scores than LSTM-PG which achieved the best SAS scores for this dataset. This is probably Due to the \nfact that the GDB13 1m rand dataset is a random subset of the entire GDB13 dataset and is not preprocessed to \ncontain lead-like molecules, thus making it more challenging for optimization and reaching good performance. \nOn this dataset, Taiga also achieved the best scores in terms of novelty and diversity while maintaining a high \nvalidity score, this means that the molecules generated are not only valid but high novelty score means we have \nhigher chance to come across a lead molecule (since high novelty mean it doesn’t overfit and generates molecules \nfrom the training set). As seen in Supplementary Table S1 and Supplementary Figure S2 our method excels and \nis the only one that generated molecules with a QED value above 0.9. When looking at the SAS scores of the best \nmolecules in terms of their QED in the GDB dataset, we find mixed results; this most likely occurred, because \nnone of the methods directly tries to optimize the SAS score, and therefore when a model generates molecules, \nit generates compounds that are more complex and thus have a higher QED score but are harder to synthesize.\nOf the three examined datasets, Taiga achieved the best optimization results with the Moses dataset, while \nstill maintaining a high value for validity, diversity, and novelty metrics averaging around 97%. Compared to \ngraph-based methods like JTV AE and GCPN, which represent molecules as complex graphs to generate a high \nrate of valid molecules, our method achieved comparable results on the diversity and novelty metrics and was \nnot far behind on the validity metric meaning that SMILES-based method can generate high percentage of valid \nmolecules. On the majority of metrics, our method performed the same or better than SMILES-based methods \nsuch as MolGPT and LSTM-PG, but MolGPT was able to achieve higher novelty score for the Zinc and Moses \ndatasets. In Supplementary Table S1, we can see that Taiga generated molecules with a higher QED value than \nthat obtained by graph-based methods and other SMILES-based methods, while LSTM-PG did manage to cre-\nated a high amount of valid molecules and achieved the best SAS scores on the Moses dataset (due to the fact it \nTable 1.  Overall training dataset statistics.\nDataset Number of molecules QED SAS Max atoms\nGDB13 rand 1,000,000 0.513 4.96 13\nMoses 1,584,663 0.810 2.45 30\nZinc 249,456 0.731 3.05 38\nTable 2.  Performance comparison on the property generation task; the mean score and standard deviation of \nthe molecules is presented. Significant values are in bold. ∗The authors support the GDB dataset in their code, \nhowever we were unable to run the code; our attempt to contact the authors was unsuccessful. ∗∗ It is infeasible \nto create training data from the Moses dataset, even when using more than 128GB of RAM. ∗∗∗ MolDQN is \nuses only RL and does not need a training set; therefore, it does not have a novelty score and is listed separately.\nDataset Model QED(↑) SAS(↓) Validity(↑) Diversity(↑) Novelty(↑)\nGDB13\nGCPN∗ – – – – –\nJTV AE 0.49±0.10 4.88±0.64 100% 89% 100%\nMolGPT 0.50±0.12 4.93±0.85 90% 100% 99%\nMolGAN 0.50±0.11 4.98±1.08 89% 8% 99%\nGraphDF 0.38±0.13 4.93±0.83 100% 99% 99%\nLSTM 0.51±0.12 3.88±0.83 99% 96% 96%\nTaiga 0.64±0.11 4.87±0.78 95% 100% 100%\nMoses\nGCPN 0.64±0.15 4.38±0.85 99% 99% 99%\nJTV AE 0.70±0.12 4.24±0.70 100% 100% 100%\nMolGPT 0.75±0.11 2.88±0.58 62% 98% 99%\nMolGAN∗∗ – – – – –\nGraphDF 0.42±0.13 4.76±0.80 100% 99% 99%\nLSTM 0.80±0.07 2.06±0.46 95% 90% 82%\nTaiga 0.83±0.07 2.25±0.49 97% 99% 95%\nZinc\nGCPN 0.65±0.15 4.53±0.86 99% 99% 99%\nJTV AE 0.64±0.14 4.69±0.76 100% 100% 100%\nMolGPT 0.67±0.16 3.98±0.99 57% 99% 99%\nMolGAN 0.33±0.00 5.23±0.00 100% 0% 100%\nGraphDF 0.42±0.13 4.80±0.91 99% 99% 100%\nLSTM 0.73±0.14 2.47±0.88 56% 96% 96%\nTaiga 0.75±0.11 2.89±0.92 88% 98% 97%\n– MolDQN∗∗∗ 0.78±0.10 6.31±0.83 100% 65% –\n5\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\ngenerated molecules which lower QED score which are less complex), it wasn’t able to improve the metric it was \noptimized for which means that it can’t generate both optimized and valid molecules which further emphasizes \nthe limitation of LSTM-based method in comparison to Transformers.\nOn the Zinc dataset (see Table 2 and Supplementary Figure S1), most methods generated molecules with an \naverage QED value similar to the dataset mean and a high SAS score, but Taiga generated molecules with a QED \nvalue higher than the mean. Although graph-based methods such as GCPN and JTV AE were able to achieve a \nhigher value on the validity metric than SMILES-based methods with this dataset, however, they achieved lower \nmean QED scores. This makes them less favorable for generating potential molecules even if they obtained a \nperfect validity. In addition, we can see that Taiga’s validity scores are higher than those of MolGPT and LSTM-\nPG which strengthen the idea of using transformers and RL together in a 2-stage training scheme. Although \nMolGAN achieved numerical results on this dataset, the SMILES strings it generated was only “ .*.*.*.*.*.*.*” \nwhich was accepted as valid by RDKit but is not practical in any sense.\nIn Supplementary Table S1S1 and Supplementary Figure S2, we can see that Taiga obtained the best QED \nscores on the top molecules, while other methods such as LSTM-PG, GCPN, and MolGAN failed to obtain \ngood QED scores for top molecules. In addition, in terms of the SAS score, Taiga demonstrated superiority over \nalmost all other methods, obtaining better SAS scores for most molecules aside from LSTM-PG, which was able \nto generate molecules with better SAS scores. This is due to the fact that LSTM-PG can’t handle long sequences \nand generate shorter SMILES which result in smaller molecules with less atom and bonds which have a lower \ncomplex structure and thus easier to synthesize, since the SAS score penalize for complexity.\nCompared to MolDQN, which achieved better QED scores than Taiga on two of the three datasets without \nusing a dataset during the training process, MolDQN achieved the lowest diversity scores out of all methods, \nwhich means that MolDQN is unable to generate a diverse set of molecules and generates the same molecule \nrepeatedly. Similarly, it achieved the lowest SAS score out of all methods, thus generating molecules that are \ndifficult to synthesize. This is due to the fact that MolDQN is a Q-learning algorithm, which at test time uses a \ngreedy approach, and chooses actions based on the highest Q-values when generating molecules.\nAggregating the results, as seen in Fig.  2, shows Taiga’s superior performance across all datasets. Taiga is \nlocated in the upper-right corner of the figure (high validity, high QED) and has high diversity (indicated by a \nlarger circle). Most of the examined methods were unable to generate a large amount of valid molecules with \nhigh QED values; some methods (e.g., JTV AE) were able to achieve good validity and diversity scores but at the \ncost of degraded performance on the target properties that the model tried to optimize. On the other hand, Taiga \nachieved better performance on the target task of optimizing for the desired property at the cost of a slightly \nlower validity rate.\nProperty optimization. In this subsection, we evaluate Taiga’s ability to optimize biological properties \nwith therapeutic function, which are harder to predict than QED; such a task requires additional supervised \nlearners to predict molecular properties.\n1. Data: We used 2 datasets, the first is IC50 data extracted from the ChEMBL  database25 and extracted all of \nthe molecules that have exact pIC50 values, i.e., we removed molecules for which only a range is available. pIC50 \nis the negative log of the IC50 value by using the following formula: 9 − log10(IC 50) . We focused specifically on \nthe BACE (Beta-secretase 1) protein. After filtering out 10,164 molecules, we ended up with 9,331 samples with \nexact pIC50 values. The second is a dataset of molecules that used in cancer treatments, we collected around 400 \nmolecules from various sources that had indication for some anti-cancer activity (FDA approval, clinical trials, \nDrugBank, etc. ), and were assessed as such by a pharmacologist and around 1000 molecules that are not known \nfor treating cancer were randomly sampled from the list of FDA approved drugs after filtering after filtering out \ndrugs that were already examined in cancer-related clinical trials (as reported in ClinicalTrials.gov) or drugs \nthat are chemically similar to those drugs.\n2. Model Configuration: We used the same configuration for Taiga as the one for the molecule generation task. \nFor the property prediction model we utilize  Chemprop26, a message passing graph neural network (MPNN) \nsince its ability to predict potential  molecules 27. We train the model with the default parameters the library \n(https:// github. com/ chemp rop/ chemp rop/.)  provides, that is for 30 epochs with a batch size of 50, learning rate \nof 1e−4 and ensemble size of 2, using supervised learning on the curated dataset.\nInspired by Popova et al.11, we used the following reward function for the IC50 task:\nand for the anti-cancer prediction we use the following reward function:\nwhere Chemprop(sT ) is the raw output probability of the MPNN which ranges between 0 and 1 model raw output \nprobabilities.\n3. Results: The results presented in Table  3 demonstrate Taiga’s ability to maximize pIC50 values with the \ndifferent datasets. We can see that when using all of the datasets as baselines, Taiga can be optimized for biologi-\ncal properties. On average, Taiga increased the pIC50 value by 20% when converting to IC50 values; this is the \nequivalent of reducing the concentration by a factor of 3-5 for the same therapeutic effect. We can see that the \nvalidity constraint of equation 2 helps maintain the same validity scores as the baseline. This prevents overfitting \n(2)R (sT ) =\n{\nexp\n(pIC50\n3\n)\n, ifsT is valid molecule\n0, otherwise\n(3)R(sT ) =\n{\nChemprop(sT ), ifsT is valid molecule\n0, otherwise\n6\nVol:.(1234567890)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nby generating random strings that can exploit the property predictor. Looking at other metrics such as the QED \nor SAS, we can see that Taiga was able to generate molecules with improved pIC50 values and at the same main-\ntained similar SAS and QED values to those of the baseline; this means that it did not only do the molecules \nhave a better potential for treatment, they are also easy to synthesize and have drug-like properties. With two of \nthe three datasets, Taiga was able to keep generating a set of novel and diverse molecules after the RL stage, and \non the Zinc and GDB dataset, the novelty and diversity scores decreased but by just a small margin. This means \nthat Taiga was able to generate a set of molecules with higher pIC50 values while ensuring that the molecules \nare different from each other and were not seen during the training process.\nWhen generating molecules to have anti-cancer activity, we also see that Taiga can maximize and generate \nmolecules with high potential for cancer treatments without compromising other metrics, When calculating \nmolecular similarity to existing anti-cancer therapeutics, the top molecules generated are chemically similar, \nwhich means that Taiga did manage to learn some understanding on what makes a drug anti-cancer. We also see \nTaiga’s manages to generate a high amount of novel and diverse molecules while maintaining a high validity rate. \nLooking at the other metrics, such as QED or SAS, we can see that Taiga can generate anti-cancer molecules while \nmaintaining around the same SAS score but having lower QED scores. When looking at anti-cancer molecules, \nsome of them violate Lipinski’s rule of 5, so it makes sense to have lower QED (which as part of its average uses \nproperties from the rule of 5) scores as a trade-off for anti-cancer activity.\nAblation study. We conducted an ablation study to evaluate the contribution of the RL stage on Taiga’s \nperformance. As can be seen in the results presented in Table  4, with the Moses dataset, before the RL stage \nTaiga underperformed in terms of validity and novelty when generating molecules; in addition, the QED value \nobtained was similar to the mean QED of the dataset (see Table  1). However, after the RL stage, the model was \nable to find a policy that enables better maximization of the QED. This is demonstrated by the increase in the \nmean QED of the molecules generated and the increase in the validity and novelty scores.\nWe can see a similar phenomenon with the GDB dataset when comparing the results before and after the RL \nstage. Although the model was able to generate more valid molecules before the RL stage, the difference in the \nTable 3.  Performance on the property optimization task. Baseline refers to the results obtained after the \nlanguage modeling stage and before RL. Maximized refers to the results obtained after the language modeling \nstage and RL stage calculated by the raw output of the property predictor. Significant values are in bold.\nDataset Type Metric Value(↑) QED(↑) SAS(↓) Validity(↑) Diversity(↑) Novelty(↑)\nGDB13\nBaseline pIC50 5.180 0.521 5.07 97.49% 99.99% 99.91%\nMaximized pIC50 10.462 0.563 5.03 97.84% 86.58% 86.58%\nBaseline Anti-cancer 0.158 0.521 5.07 97.49% 100.00% 99.89%\nMaximized Anti-cancer 0.844 0.552 4.65 95.54% 99.99% 99.99%\nMoses\nBaseline pIC50 4.949 0.805 2.42 93.72% 99.87% 89.55%\nMaximized pIC50 5.612 0.795 2.41 95.52% 99.78% 95.59%\nBaseline Anti-cancer 0.304 0.805 2.42 93.77% 99.87% 89.55%\nMaximized Anti-cancer 0.774 0.749 2.12 96.43% 98.12% 93.20%\nZinc\nBaseline pIC50 4.936 0.727 3.09 74.94% 99.98% 99.93%\nMaximized pIC50 6.333 0.671 2.99 92.33% 93.32% 93.22%\nBaseline Anti-cancer 0.215 0.726 3.09 74.94% 99.99% 99.99%\nMaximized Anti-cancer 0.824 0.782 1.98 93.35% 87.84% 87.96%\nTable 4.  Comparison of the results for the main metrics for Taiga, with and without the RL stage.\nGDB13\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑)\nw/o RL 97.78% 99.99% 99.90% 0.510\nw/ RL 95.82% 100.00% 100.00% 0.646\n Stage\nMoses\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑)\nw/o RL 92.82% 99.40% 94.17% 0.806\nw/ RL 97.21% 98.90% 95.56% 0.831\nZinc\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑)\nw/o RL 69.13% 99.97% 99.94% 0.733\nw/ RL 85.46% 99.70% 99.60% 0.757\n7\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nmean QED value obtained before and after the RL stage emphasizes the fact that the model was able to learn how \nto generate highly optimized molecules with just a slight trade-off in terms of the validity.\nA similar improvement was seen with the Zinc dataset; before the RL stage, Taiga struggled to generate valid \nmolecules and obtained a mean QED similar to that of the dataset itself. After the RL stage, Taiga generated more \nthan 15% more valid molecules without significantly compromising the performance in terms of the diversity \nand novelty metrics; its mean QED also improved.\nAnother setting we tested is using RL directly without incorporating the language modeling task first. The \nresults are not included in the table since the model wasn’t able to converge at all. After the first 20 steps it reached \na point where the it failed to generate valid molecules (or any molecular formula at all).\nWe also tested the effects of using different tokenizers on Taiga’s ability to generate optimized molecules. We \nswitched the character-level tokenizer for a BPE  tokenizer28. In the end, the vocabulary consisted of 500 tokens \n(excluding special tokens such as [BOS], [EOS], etc.).\nTable 5 summarize the results of our experiments. As can be seen, using the BPE tokenizer resulted in lower \nQED and SAS scores for both the GDB and Moses datasets. On the Zinc dataset, we can see that both QED \nand SAS scores have been improved by the BPE tokenizer. Moreover, it improved the model’s ability to generate \nvalid molecules.\nI believe this is due to the fact that the zinc dataset contains the greatest number and diversity of molecules. \nIt can be determined by examining the number of unique single characters in each dataset (Zinc: 34, Moses: 26, \nGDB: 21) that are used as the initial vocabulary when training the BPE tokenizer. If the initial vocabulary and \ndataset are not diverse enough, the vocabulary will end up with similar tokens, but longer (e.g. CCCC and CCC \nCCC ). As a result, the model learns very short sequences, however on the Zinc dataset, because of its large initial \nvocabulary and diverse set of molecules, we will still be able to learn longer sequences with a large vocabulary \nthat is sufficiently diverse.\nThis improves the validity score since the sequences are shorter than the character-level and each token con-\ntains a group of atoms (and bonds), so the model has lower chances of generating invalid molecules compared \nto using character-level tokens, where each token can cause the molecule to become invalid.\nTo further assess Taiga’s ability to generate novel molecules and not overfit to the training data, for each dataset \nwe calculated the novelty scores of the molecules generated during the molecule generation stage based on the \nother datasets. As seen in Table  6, Taiga generated novel and unseen molecules that do not exist in the other \nFigure 2.  Models’ QED performance as a function of the molecule validity rate. The size of the dot represents \nthe diversity value (higher is better); models that are closer to the top-right corner are considered better.\n8\nVol:.(1234567890)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\ndatasets. This reinforces the idea that existing difficulties with SMILES strings can be overcome by combining \ntransformers with RL.\nDiscussion\nIn this paper, we propose a solution for the de-novo drug design problem of generating novel molecules with \ndesired properties. We introduce Taiga, a transformer-based architecture for the generation of molecules with \ndesired properties. Taiga uses a two-stage approach by first learning a language modeling task and then optimiz-\ning for the desired properties using RL.\nOur results demonstrate that the use of an attention mechanism enabled our method to overcome the problem \nof generating invalid SMILES strings. When compared to an RNN using the same RL technique, Taiga achieved \nsimilar or better results on all metrics across all of the examined datasets.\nWhile all of the examined methods try to achieve the highest QED scores by directly optimizing for it on \nthe generation task, Taiga outperformed state-of-the-art methods when generating molecules with the highest \nQED scores and obtained similar or better results in terms of validity, novelty, and diversity when generating \narbitrary molecules.\nWhen optimizing for biological properties like pIC50, Taiga reduced the concentration required by a factor \nof 3–5 for the same therapeutic effect (evaluated by an external property predictor), while maintaining similar \nscores on all other metrics, when using all datasets as baselines for training. When optimizing for anti-cancer \nactivity, Taiga manages to achieve better anti-cancer activity while maintaining similar scores on all other metrics. \nAdditionally, when examined by expert pharmacologists, several of the top molecules generated we evaluated \nas easily synthesizable and have a high chance of having anti-cancer properties, this emphasise the advantage of \nthe RL stage, which allows us to optimize properties that are not derived from the model itself.\nOur proposed method for molecule generation can enhance the drug development process by generating \ncandidate molecules with improved therapeutic properties that are better than those of existing drugs on the \nmarket. The drug development process takes an average of 13 years, of which half are spent searching for lead \nmolecules, and our proposed method can help reduce the time devoted to this task.\nFuture work can explore reward function that try to optimize several properties or doing constrained opti -\nmization from existing molecules.\nMethods\nProblem formulation. We define the molecule generation and optimization tasks similarly to the for -\nmulations used by the authors presenting  GraphDF19. Given a set of molecules {mi}M\ni=1 and a score function \nR(m) → R , the molecule generation task is defined as learning a generation model pθ (·) , such that pθ (m) is \nthe probability of generating molecule m. The optimization task is defined as maximizing EM ∼pθ [R(m)] with \nTable 5.  Comparison of the results for the main metrics for Taiga, with different tokenizer schemes. \nSignificant values are in bold.\nTokenizer GDB13\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑) SAS(↓)\nBPE 94.48% 100.00% 100.00% 0.625 5.38\nChar 95.82% 100.00% 100.00% 0.646 4.87\nMoses\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑) SAS(↓)\nBPE 97.38% 99.86% 86.78% 0.816 2.44\nChar 97.21% 98.90% 95.56%  0.831 2.25\nZinc\nValidity(↑) Divesity(↑) Novelty(↑) QED(↑) SAS(↓)\nBPE 96.13% 99.40% 99.21% 0.794 2.58\nChar 85.46% 99.70% 95.60% 0.757 2.89\nTable 6.  Cross-dataset novelty scores of the molecules generated. Rows indicate the source of the pre-training \ndataset used to train Taiga and generate molecules with. Columns indicate the actual dataset we compare \nnovelty scores with.\nTest dataset\nMoses Zinc GDB13\nTrain dataset\nMoses 95.56 98.74 98.90\nZinc 98.82 95.60 99.70\nGDB13 100.00 100.00 100.00\n9\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nrespect to R (in the context of molecules, R can be the IC50 of the molecule or or any property one might want \nto maximize).\nOverview. Figure 1 illustrates our proposed method. Taiga is based on a two-stage process in which we first \ntrain a transformer-based architecture on a general language modeling task by having the model predict the \nnext token in the sequence of SMILES strings. Then we apply policy gradient RL (specifically, the REINFORCE \nalgorithm) to achieve the desired molecular properties by learning a policy that maximizes the desired property \nas the reward in the RL stage. The main advantage of our proposed method is its utilization of a pretrained model \ncapable of learning both the underlying rules of chemistry and the grammar of SMILES strings, which acts as \nan initial policy by training on the next-character prediction task. This improves the model when applying the \nREINFORCE algorithm.\nLanguage model. Similar to  MolGPT2, we use a GPT-like decoder-based transformer model as an auto-\nregressive model for language modeling. The model consists of several decoder-only blocks stacked one after \nanother. Each block uses the self-attention mechanism. This attention mechanism takes a set of keys, queries, \nand values (q, k, v) as inputs, applies a dot product operation between the queries and the keys, and then com-\nputes the attention weights for the values by using the softmax function on the result of the dot product. The \nattention mechanism is formulated as follows:\nIn order to learn different representations, we use MultiHeadAttention, which allows us to attend information \nfor different positions at the same time:\nwhere W Q\ni , W K\ni , W V\ni  are the projection matrices of head i. To train a model that can carry out text generation \ntasks, we mask future tokens to prevent tokens from attending consecutive tokens when computing the self-\nattention mechanism.\nWe then define a transformer decoder block as follows:\nwhere x l−1 is the input from the previous block, MLP is a multi-layer feed-forward network and MHA is the \nMultiHeadAttention defined previously. We can then stack as many layers of decoder blocks as we want to cre-\nate out model.\nReinforcement learning. In this subsection, we formulate the RL problem for molecule graph generation. \nWe define the SMILES generation as a Markov decision process M = (S, A, P, R, γ) .\n• Observation Space: A single state is represented as a vector F, we assume there is a finite set of character types \nthat can be used to represent a SMILES string bounded by n, in which F ∈ R l where fi belongs to {0, ..., n} . \nS ={ si} is the state space of all possible intermediate SMILES strings with length t ≤ T  ; T denotes the ter-\nminal state after the model generates a [EOS] token or reaches a maximal length; and s0 , which is the initial \nstate, is an empty string.\n• Action Space: A ={ ai} is the set of actions the agent can take. In our case, all of the possible actions are the \ncharacters in the vocabulary you can append to the SMILES representation of the molecule, so we assume \nthat a i belongs to {0, ..., n}.\n• Transition Dynamics: P is the transition dynamics that specify the probability of reaching a certain state \ngiven the current state and the chosen action, p(st+1 |st,at) , since the state and action space consist of only \ncharacters; the transition dynamics are simply p(st+1 |st,at) = 1 , since appending a character is deterministic.\n• Reward Function: R is the reward function for a given molecule. We define the reward as zero for all inter-\nmediate states, R (st) = 0 . R (sT ) = f(sT ) is a function applied to the molecule generated, and γ is the discount \nfactor.\nPolicy gradients. We can now define the task of finding the set of parameters for our transformer-based \nnetwork which maximizes the expected reward of our objective function J (θ ):\nwhere dπ is the state distribution and Vπ is the value function. Since it is unreasonable to compute the sum of \nall terminal states, which are all of the states that end with the [EOS] token, due to the large number of terminal \nstates, we sample them. Based on the rule of large numbers, we can approximate this sum. Then we determine \nthe gradient of the expected value using policy π θ (a|s).\n(4)Attention(Q, K , V ) = softmax(QK T /\n√\nd)V ,\n(5)MultiHead(Q ,K ,V ) =Concat(head1 ,..., headn)W o,\n(6)headi =Attention(QW Q\ni , KW K\ni , VW V\ni ),\n(7)\nzl = xl−1 + MHA (LayerNorm(xl−1))\nxl = zl+ MLP (LayerNorm(zl))\n(8)max\nθ\nJ(θ) =\n∑\ns∈S\ndπ (s)V π (s) =\n∑\n|s|=T\ndπ (s)V π (s),\n10\nVol:.(1234567890)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nwhere G t is the return of the trajectory and is defined as:\nData availability\nThe ZINC, Moses and GDB13 datasets used in this study are available online. The code is available at https://  \ngithub. com/ eyalm azuz/ MolGen.\nReceived: 8 September 2022; Accepted: 22 May 2023\nReferences\n 1. Y ou, J., Liu, B., Ying, R., Pande, V . & Leskovec, J. Graph convolutional policy network for goal-directed molecular graph generation. \nIn Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, 6412–6422 (Curran \nAssociates Inc., Red Hook, NY , USA, 2018).\n 2. Bagal, V ., Aggarwal, R., Vinod, P . & Priyakumar, U. D. Molgpt: Molecular generation using a transformer-decoder model. J. Chem. \nInf. Model. (2021).\n 3. Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley, P . Optimization of molecules via deep reinforcement learning. Sci. Rep. 9, 1–10 \n(2019).\n 4. Topliss, J. G. Utilization of operational schemes for analog synthesis in drug design. J. Med. Chem. 15, 1006–1011 (1972).\n 5. Lipinski, C. A., Lombardo, F ., Dominy, B. W . & Feeney, P . J. Experimental and computational approaches to estimate solubility and \npermeability in drug discovery and development settings. Adv. Drug Deliv. Rev. 23, 3–25 (1997).\n 6. Gómez-Bombarelli, R. et al.  Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. \nSci. 4, 268–276 (2018).\n 7. Jin, W ., Barzilay, R. & Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. In International Confer-\nence on Machine Learning, 2323–2332 (PMLR, 2018).\n 8. Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. \nInf. Comput. Sci. 28, 31–36 (1988).\n 9. Gupta, A. et al. Generative recurrent networks for de novo drug design. Mol. Inf. 37, 1700111 (2018).\n 10. Silver, D. et al. Mastering the game of go with deep neural networks and tree search. Nature 529, 484–489 (2016).\n 11. Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning for de novo drug design. Sci. Adv. 4, eaap7885 (2018).\n 12. Kusner, M. J., Paige, B. & Hernández-Lobato, J. M. Grammar variational autoencoder. In International Conference on Machine \nLearning, 1945–1954 (PMLR, 2017).\n 13. Dai, H., Tian, Y ., Dai, B., Skiena, S. & Song, L. Syntax-directed variational autoencoder for structured data. In International Confer-\nence on Learning Representations (2018).\n 14. Gupta, G., Xiao, X. & Bogdan, P . Multiwavelet-based operator learning for differential equations. Adv. Neural. Inf. Process. Syst. \n34, 24048–24062 (2021).\n 15. Yin, C. et al. Fractional dynamics foster deep learning of copd stage prediction. Adv. Sci. 2203485 (2023).\n 16. Vaswani, A. et al. Attention is all you need. In Advances in neural information processing systems, 5998–6008 (2017).\n 17. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. \narXiv preprint arXiv: 1810. 04805 (2018).\n 18. De Cao, N. & Kipf, T. MolGAN: An implicit generative model for small molecular graphs. ICML 2018 workshop on Theoretical \nFoundations and Applications of Deep Generative Models (2018).\n 19. Luo, Y ., Y an, K. & Ji, S. Graphdf: A discrete flow model for molecular graph generation. In Meila, M. & Zhang, T. (eds.) Proceed-\nings of the 38th International Conference on Machine Learning, vol. 139 of Proceedings of Machine Learning Research, 7192–7203 \n(PMLR, 2021).\n 20. Polykovskiy, D. et al. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Front. Pharmacol.  \n(2020).\n 21. Irwin, J. J. & Shoichet, B. K. Zinc—A free database of commercially available compounds for virtual screening. J. Chem. Inf. Model. \n45, 177–182 (2005).\n 22. Blum, L. C. & Reymond, J.-L. 970 million druglike small molecules for virtual screening in the chemical universe database gdb-13. \nJ. Am. Chem. Soc. 131, 8732–8733 (2009).\n 23. Bickerton, G. R., Paolini, G. V ., Besnard, J., Muresan, S. & Hopkins, A. L. Quantifying the chemical beauty of drugs. Nat. Chem. \n4, 90–98 (2012).\n 24. Ertl, P . & Schuffenhauer, A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and \nfragment contributions. J. Cheminformat. 1, 1–11 (2009).\n 25. Gaulton, A. et al. The ChEMBL database in 2017. Nucleic Acids Res. 45, D945–D954. https:// doi. org/ 10. 1093/ nar/ gkw10 74 (2016).\n 26. Y ang, K. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59, 3370–3388 (2019).\n 27. Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell 180, 688–702 (2020).\n 28. Sennrich, R., Haddow, B. & Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv: 1508.  \n07909 (2015).\nAuthor contributions\nE.M. suggested the idea and performed the computations, G.S, B.S, L.R. supervised this work. All authors con -\ntributed to the final manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 023- 35648-w.\n(9)∇θ J(θ) = Eπ [Q π (s,a)∇θ ln π θ (a|s)]= Eπ [G t∇θ ln π θ (a|s)]\n(10)Gt = Rt+1 + γ Rt+2 + γ 2 Rt+2 +···+ γ T −1 RT .\n11\nVol.:(0123456789)Scientific Reports |         (2023) 13:8799  | https://doi.org/10.1038/s41598-023-35648-w\nwww.nature.com/scientificreports/\nCorrespondence and requests for materials should be addressed to E.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7671875953674316
    },
    {
      "name": "Reinforcement",
      "score": 0.5851453542709351
    },
    {
      "name": "Transformer",
      "score": 0.5722274780273438
    },
    {
      "name": "Computer science",
      "score": 0.5553994178771973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44136473536491394
    },
    {
      "name": "Psychology",
      "score": 0.1746828854084015
    },
    {
      "name": "Electrical engineering",
      "score": 0.13317272067070007
    },
    {
      "name": "Engineering",
      "score": 0.12968498468399048
    },
    {
      "name": "Social psychology",
      "score": 0.060582756996154785
    },
    {
      "name": "Voltage",
      "score": 0.05168315768241882
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I124227911",
      "name": "Ben-Gurion University of the Negev",
      "country": "IL"
    }
  ],
  "cited_by": 51
}