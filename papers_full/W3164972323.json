{
  "title": "True Few-Shot Learning with Language Models",
  "url": "https://openalex.org/W3164972323",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223480186",
      "name": "Perez, Ethan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226463760",
      "name": "Kiela, Douwe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3152515526",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2796293253",
    "https://openalex.org/W2112081648",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W2804017338",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W2123222757",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2054658115",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W3152497014",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W2794523151",
    "https://openalex.org/W2033419168",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3093116395",
    "https://openalex.org/W2158001550",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W2132119275",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2907283777",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W1585280831",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3157374291",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W2951775809",
    "https://openalex.org/W3106031848",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W1866987985",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2050297026",
    "https://openalex.org/W2168939893",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3085177480",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W3035542229",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W2970697704",
    "https://openalex.org/W1981251392",
    "https://openalex.org/W2166107799",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W2950182411",
    "https://openalex.org/W3168685366",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W60686164",
    "https://openalex.org/W3135934234",
    "https://openalex.org/W2945290257",
    "https://openalex.org/W3002104146",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3100436891",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2753160622",
    "https://openalex.org/W2962824887",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W1506069954",
    "https://openalex.org/W2995253937",
    "https://openalex.org/W2810075754",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2142635246",
    "https://openalex.org/W1845972764",
    "https://openalex.org/W1480376833"
  ],
  "abstract": "Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (\"prompts\"). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.",
  "full_text": "True Few-Shot Learning with Language Models\nEthan Perez1, Douwe Kiela2, Kyunghyun Cho13\n1New York University,2Facebook AI Research,\n3CIFAR Fellow in Learning in Machines & Brains\nperez@nyu.edu\nAbstract\nPretrained language models (LMs) perform well on many tasks even when learning\nfrom a few examples, but prior work uses many held-out examples to tune various\naspects of learning, such as hyperparameters, training objectives, and natural\nlanguage templates (“prompts”). Here, we evaluate the few-shot ability of LMs\nwhen such held-out examples are unavailable, a setting we call true few-shot\nlearning. We test two model selection criteria, cross-validation and minimum\ndescription length, for choosing LM prompts and hyperparameters in the true\nfew-shot setting. On average, both marginally outperform random selection and\ngreatly underperform selection based on held-out examples. Moreover, selection\ncriteria often prefer models that perform signiﬁcantly worse than randomly-selected\nones. We ﬁnd similar results even when taking into account our uncertainty in a\nmodel’s true performance during selection, as well as when varying the amount\nof computation and number of examples used for selection. Overall, our ﬁndings\nsuggest that prior work signiﬁcantly overestimated the true few-shot ability of LMs\ngiven the difﬁculty of few-shot model selection.\n1 Introduction\nMajor progress in language model (LM) pretraining has led to the idea that LMs can learn a new task\nusing a small number of examples only, i.e., few-shot learning [1–3]. Few-shot learning overcomes\nmany challenges with data-rich supervised learning: collecting labeled data is expensive, often\nrequires experts, and scales poorly with the number of tasks. However, the few-shot performance of\nLMs is very sensitive to the textual task description [“prompt”;3–6], order of training examples [6–8],\ndecoding strategy [9, 10], and other hyperparameters [3, 5, 9, 11, 12], as well as the learning algorithm\nitself [3, 12]. Thus, effective model selection is crucial for obtaining good few-shot performance.\nThere are issues with how recent work approaches model selection in few-shot learning, however.\nPrior work uses large train or held-out sets with many examples to choose prompts [2, 12, 13] and\nhyperparameters [12]. Other work claims to use no validation set for hyperparameter selection [3,\n11, 14] but does not describe how they design other aspects of their learning algorithm (e.g., training\nobjectives). It is unlikely that no validation examples were used, given the sophisticated nature of the\nproposed algorithms. In this work, we examine if prior few-shot learning methods still perform well\nwhen using only the provided examples for model selection, a setting we term true few-shot learning.\nWe ﬁnd that true few-shot model selection yields prompts that marginally outperform random selection\nand greatly underperform selection based on held-out examples. Our result shows that prior work\nmay have greatly overestimated the few-shot ability of LMs. In other words, one reason that prompts\nare so effective [“worth many examples”;15] is that they are often tuned using many examples. We\nevaluate two standard model selection criteria – cross-validation (CV) and minimum description\nlength (MDL) – ﬁnding that both obtain only limited improvements over random selection and\nperform much worse than selection using held-out examples. For prompt selection, our observation\nholds for 9 LMs ranging over 3 orders of magnitude in size [1, 2, 16] on 3 classiﬁcation tasks and 41\nPreprint. Code available at https://github.com/ethanjperez/true_few_shot\narXiv:2105.11447v1  [cs.CL]  24 May 2021\ntasks in the LAMA benchmark [17]. For choosing hyperparameters, true few-shot selection causes\nperformance to drop by 2-10% across 8 tasks for ADAPET [12], a state-of-the-art few-shot method.\nFurthermore, true few-shot model selection has high variance in performance; selected models often\ndo much worse than randomly-chosen ones. We ﬁnd similar results when varying the number of\nexamples used, amount of computation, and conservativeness of our selection criterion. Altogether,\nour results suggest that model selection is a fundamental roadblock to true few-shot learning.\n2 Can We Do Model Selection in Few-Shot Learning?\nPrior work uses the phrase “few-shot learning” in multiple senses, raising questions about what it\nmeans to do few-shot learning. We categorize few-shot learning into three distinct settings, each of\nwhich assumes access to different data. Here, we formally disambiguate between these settings to\nhelp future work avoid inadvertently comparing few-shot methods that operate in different settings.\nConsider the supervised learning scenario where we have a dataset of inputs x1:N and labels y1:N,\nsampled from a distribution over datasets D. We aim to determine the learning algorithm A∗ ∈\nA1,..., AA with the smallest generalization loss Lat predicting y given xon unseen validation\nexamples Dval ∼D after learning on training examples Dtrain ∼D. We say that an algorithm\nA(Dtrain,R) maps a training dataset Dtrain and various random factors Rthat inﬂuence training to\na function that predicts ygiven x. Aspeciﬁes, e.g., the model architecture, hyperparameters, and\nprompt. Rincludes random factors that impact the results of a learning algorithm, such as parameter\ninitialization and the order of training examples for online learning algorithms like stochastic gradient\ndescent. We say that Aobtains a generalization loss L(A(Dtrain,R),Dval) on a given validation set\nDval. We aim to ﬁnd the A∗that minimizes the expected loss across training and validation sets:\nEL(A,R) =EDtrain,Dval\n[\nL\n(\nA(Dtrain,R); Dval\n)]\nIn data-rich supervised learning, EL(A,R) is usually evaluated with a single train-validation split\n(Dtrain,Dval). Since large Dtrain and Dval are not always available, the traditional few-shot setting\nevaluates EL(A,R) with many small (Dtrain,Dval) drawn from many, distinct distributionsD[see,\ne.g., work in meta-learning 18–21]. Each distribution Dis sampled from D∗, a distribution over\ndistributions (e.g., of similar tasks), so we call this setting multi-distribution few-shot learning.\nRecent work does not assume access to data from other distributions, performing few-shot learning\nusing only a few examples from a single distribution to update a pretrained LM [2, 12]. These papers\nuse a large validation set Dval to tune the learning algorithm A, a setting we term tuned few-shot\nlearning. For example, Brown et al. [2] try prompts with different phrasings and numbers of training\nexamples to improve the validation accuracy of GPT-3. Tam et al. [12] choose the early stopping\niteration, prompt, and other model-speciﬁc hyperparameters based on validation performance. Tuned\nfew-shot learning relies on many labeled examples, so we argue that tuned few-shot learning does not\nqualify as few-shot learning. If many validation examples are available, they could be incorporated\ninto the training set and trained on using data-rich supervised learning. Tuned few-shot learning\nalgorithms should be compared against data-rich supervised learning algorithms that use the same\namount of total data |Dtrain|+ |Dval|.\nIn this work, we evaluate the success of tuned few-shot learning methods when no large Dval is\navailable, a setting we term true few-shot learning. Formally, we aim to choose a learning algorithm\nAwith low expected loss EL(A,R) using only a small training set Dtrain drawn from a single\ndistribution. Here, we must choose Aby approximating EL(A,R), e.g., using cross-validation.\nSeveral papers claim to circumvent the need to estimate EL(A,R) by choosing hyperparameters\nbased on an educated guess [3, 9, 14]. However, the proposed learning algorithms themselves are\nquite sophisticated, and it is unclear how they were designed if not by using validation performance.\nOther work chooses the learning algorithm and hyperparameters using one or multiple other datasets\nbefore evaluating on the target dataset [5, 11]. Such approaches fall under multi-distribution few-shot\nlearning and cannot be directly compared to methods that attempt to perform true few-shot learning,\neven though prior work has made such comparisons [14].\nIn what follows, we describe two model selection criteria – cross-validation and minimum description\nlength – which we use to evaluate tuned few-shot methods in the true few-shot setting.\n2\n2.1 Cross-validation\nCross-Validation (CV) [22–24] is one of the most widely used methods for estimating generalization\nloss [25]. CV has also been used in prior work on multi-distribution few-shot learning [26, 27]. CV\nrandomly partitions Dtrain into Kequally-sized folds F(Dtrain)1,...,F (Dtrain)K and evaluates the\naverage loss on a validation fold F(Dtrain)k when training on the remaining data F(Dtrain)¬k:\nCV(A,R,F ) =Ek∼Unif(1,K)\n[\nL\n(\nA(F(Dtrain)¬k,R); F(Dtrain)k\n)]\nIn this way, CV forms K train-validation splits out of the pool of labeled examples. CV with one\nexample per fold (K = N folds) is commonly referred to as leave-one-out CV (LOOCV).\n2.2 Minimum description length\nWe may also form train-validation splits in a different manner than CV , drawing inspiration from work\non the Minimum Description Length (MDL) principle [28]. MDL can be estimated by evaluating the\naverage loss on a fold F(D)k when training on the previous folds F(D)1:k−1:\nMDL(A,R,F ) =Ek∼Unif(1,K)\n[\nL\n(\nA(F(Dtrain)1:k−1,R); F(Dtrain)k\n)]\nThis procedure is referred to as “online coding” [29, 30], as it evaluates the generalization loss of the\nalgorithm as it learns “online” from more and more data.1 There are other ways to evaluate MDL [see\n31, for an overview]. We use online coding as it has been shown to be an effective way to estimate\nMDL, especially for deep learning methods [32].\nMDL measures generalization because it evaluates how much a learning algorithm compresses the\nlabels y1:N given the inputs x1:N, and because better compression implies better generalization [33].\nRecent work has used MDL to determine which learning algorithms are most effective at explaining\nthe given data [Rissanen Data Analysis; 10, 34].\n2.3 Variance matters\nWe evaluate the generalization loss of the algorithm chosen by CV (likewise for MDL):\nL(ACV(Dtrain,R),Dval), where ACV = arg min\nA\nER,F[CV(A,R,F )].\nThe above loss should be low in expectation, across different datasets Dtrain ∼D, Dval ∼D,\nand random factors R,F: EDtrain,Dval,R,F[L(ACV(Dtrain,R),Dval)]. The loss should also be low\nin variance: VDtrain,Dval,R,F[L(ACV(Dtrain,R),Dval)]. Low variance implies that CV/MDL reliably\nchoose an algorithm that generalizes toDval when trained with a givenDtrain and random factors R,F.\nReliability is important for many practical or commercial applications where worst-case performance\nis important, such as image recognition [35, 36], dialogue systems [37, 38], and robotics [39, 40].\nWe also experiment with explicitly taking into account an algorithm’s variance during model selection,\nchoosing ACV to minimize a conservative estimate of CV ,CVα(A), chosen such that the probability\nPrR,F[CV(A,R,F ) <CVα(A)] is high:\nCVα(A) =ER,F[CV(A,R,F )] +α\n√\nVR,F[CV(A,R,F )]\nwhere α is a hyperparameter set based on the desired probability. In particular, if CV(A,R,F )\nfollows a normal distributionNwhen sampling R,F, then CV(A,R,F ) ≤CVα(A) with probability∫α\n−∞N(µ = 0,σ = 1) for a given R,F. CVα(A) resembles the Watanabe Akaike Information\nCriterion [41], which estimates the generalization of a model trained with Ausing the expected loss\nfrom a model trained with Aplus the variance in training loss across models trained with A.\n1Online coding formally computes a sum over L(.) rather than the expectation, which differs by a constant\nfactor. The two are equivalent for our purposes (ranking A).\n3\n2.4 Other model selection criteria\nPrior work has developed other model selection criteria such as the Akaike Information Criterion [AIC;\n42], Watanabe-Akaike Information Criterion [WAIC; 41], and Mallows’Cp [43]. These methods\noften rely on assumptions or quantities that are not available in the context of deep learning (AIC,\nMallows’Cp) or are approximations of LOOCV (WAIC). Since state-of-the-art few-shot learning\nmethods tend to be based on deep learning, we focus on CV and MDL as our model selection criteria.\nIn Appendix §A, we also test several other criteria that are applicable to deep learning methods.\nSelection criteria can be optimized automatically, e.g. with bayesian optimization [ 44–46],\nevolutionary methods [45, 47, 48], reinforcement learning [49], or gradient descent [50–53]. Such\nmethods aim to match the performance of exhaustive search, the optimal approach (used in our work).\n3 True Few-Shot Prompt Selection\nRecent work on LMs performs few-shot learning by providing training examples as input in the form\nof a natural language “prompt” [2, 3, 9]. For example, for a question-answering task, Brown et al. [2]\nprepend input examples with “READING COMPREHENSION ANSWER KEY” before providing\nthem to GPT-3 (see Appendix Table 2 for more examples). They then have the LM complete the\nremaining words in the prompt, conditioning on earlier words (including various input examples),\nfollowing the LM’s pretraining objective (next word prediction). No parameter updates are involved.\nIt is not obviousa priori which prompts will generalize well for a given LM, and there is high variance\nin how well different prompts generalize [3, 6], even between prompts with minor differences [e.g.,\none comma; 5]. Thus, it is important to choose prompts using a limited number of labeled examples\nto achieve true few-shot learning.\n3.1 Experimental setup\nIn what follows, we test on LAMA [ 17], a benchmark for retrieving facts with LMs, for which\nprior work has developed many strategies for designing prompts [4, 54–56]. LAMA evaluates the\naccuracy of LMs at choosing the correct target object for various ( subject, relation, object)\ntriples present in knowledge bases, such as (Dante, born-in, Florence). We use the “TREx” split,\nwhich consists of 41 relations (up to 1k examples each). Petroni et al. [ 17] design a prompt for\neach relation, which an LM completes to predict an answer (e.g., “The birthplace of Dante was _”).\nSome relations have multiple valid target entities, so LAMA evaluates how often one of the true\nanswers matches the top-predicted token (out of 20k candidates). We only use examples from the\nLAMA-UnHelpfulNames subset [LAMA-UHN; 57] which ﬁlters out easy-to-guess examples (e.g.,\n“The Apple Watch was created by _” with the answerApple). We test the 5-shot accuracy of 9 popular\nLMs of various sizes: GPT-3 [175B, 13B, 6.7B, 2.7B parameter models; 2], GPT-2 [1.5B, 782M,\n345M, 117M models; 2], and DistilGPT-2 [16], a distilled, 82M parameter version of GPT-2 117M.2\nPrompts To form our set of candidate prompts A1,..., AA, we rely on LAMA as well as the\nLanguage model Prompt And Query Archive [LPAQA;4]. For each relation, we use the manually-\nwritten prompt from LAMA, as well as LPAQA prompts formed by (1) paraphrasing the manual\nprompt using back-translation (2) mining from Wikipedia, and (3) paraphrasing the top mined prompt.\nFor each relation, we use up to 16 prompts with a mean of 12 prompts (see Appendix §D.1 for more\ndetails on the prompts we use).\nComputing CV and MDL As the loss function L, we use the negative log-likelihood (NLL) of the\nlabel given the input over all evaluation examples ∑\n(x,y) −log p(y|x). We use NLL following prior\nwork in MDL [32, 60, 10], to retain MDL’s property as a measure of label compression. For CV , NLL\navoids ties between different prompts that would arise from using accuracy in the context of such\nlimited data (e.g., 5 examples). For all prompt experiments, we use K = N folds (where N is the\nnumber of training examples) for both MDL and CV (here, LOOCV). Here, N-fold CV requires N\n2We use OpenAI’s API for GPT-3 (https://beta.openai.com/) and HuggingFace Transformers [58] via\nPyTorch [59] for GPT-2 and DistilGPT-2. OpenAI does not disclose the sizes of their API-provided models, so\nwe follow prior work [6, 7] and assume that the four API models are the four largest ones from Brown et al. [2].\nWe plan to update our paper should OpenAI release model details.\n4\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n0\n10\n20\n30\n40\n50\n60T est Acc. of Chosen Prompt (%)\nWorst\nMean\nMDL\nCV\nBest\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n20\n0\n20\n40\n60\n80\n100\nRelative T est Acc. of\nChosen Prompt (%)\nMDL\nCV\nFigure 1: Left: LAMA-UHN accuracy of CV/MDL-chosen prompts vs. accuracy of the worst,\naverage (randomly-selected), and best prompt (prior work). Right: The average accuracy gain from\nusing CV/MDL-chosen prompts instead of randomly-chosen ones, relative to the gain from the best\nprompt. We plot mean/std. err. across 5 runs with different training sets. Across all model sizes,\nCV/MDL-chosen prompts obtain only small improvements over randomly-chosen ones and perform\nfar worse than the best prompts.\nforward passes to evaluate the loss on each of the N examples when conditioning on the N−1 other\nexamples. N-fold MDL can be computed using a single LM forward pass to compute the loss on\neach example conditioned on the previous examples. This feature makes MDL more computationally\nefﬁcient than CV , and enables us to compute more estimates of MDL given a ﬁxed compute budget.\nMarginalizing out example order The order of training examples impacts the generalization of\nLMs [7], so we treat order as a random factorRthat we marginalize over to evaluate the generalization\nof a prompt A. We compute the exact ER,F[CV(A,R,F )] and ER,F[MDL(A,R,F )] by averaging\nover all N! training example orders. We use N = 5examples to limit N!. We estimate the average\ntest accuracy on N! = 120examples in LAMA, excluding the training examples, by evaluating on\none test example per permutation of training examples. We compute CV , MDL, and test accuracy\nwith N! = 120forward passes in total by appending a test example to each permutation of training\nexamples, and we compute all selection criteria using the same set of N! = 120forward passes to\nmaximize comparability across different methods. We show the test accuracy from CV/MDL-chosen\nprompts, averaged over all relations. For comparison, we show the test accuracy of always choosing\n(1) the best prompt, chosen using held-out accuracy as in prior work, (2) the worst prompt, as a lower\nbound, and (3) random prompts (we show the mean accuracy over all prompts).\n3.2 How well does prompt selection do in true few-shot learning?\nFig. 1 (left) shows the results; prompt selection obtains marginal improvements over random selection\nacross model sizes ranging over 3 orders of magnitude. Prompts chosen by CV and MDL alike\nunderperform the best prompt (chosen using held-out performance) by 5-7% absolute on average. In\nfact, prompts chosen based on held-out performance often outperform larger models whose prompts\nare chosen in a true few-shot manner. CV and MDL do tend to choose better-than-average prompts,\nbut only close the gap between the average and best prompts by 20-40%, as shown in Fig. 1 (right).\nFig. 2 (left) shows that CV/MDL struggle to choose the prompt with the highest test accuracy. Poor\ntop-prompt selection is especially prevalent for larger models like GPT-3 175B that have spurred\ninterest in prompt design (only 21% accuracy for CV vs. 9% for random chance). Altogether, our\nresults show that effective prompt selection is difﬁcult in the true few-shot setting, and that prior work\noverestimated the ability of LMs by using held-out examples for prompt selection.\n3.3 How reliably does prompt selection improve over the average prompt?\nIf the expected improvement from prompt selection is small, can we at least obtain an improvement\nwith high probability for any given task and training set? Fig. 1 (left) shows that the worst prompts\nperform far worse than average, so it would be useful if prompt selection helped to avoid the worst\n5\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n10\n15\n20\n25\n30\n35Acc. at Choosing T op Prompt (%)\nMDL\nCV\n40\n 20\n 0 20 40 60\nThreshold for Acc. Gain over Mean Prompt\n0\n20\n40\n60\n80\n100\n% of Time Acc. Gain\nBelow Threshold\nParams (B)\n0.08\n0.1\n0.3\n0.8\n1.5\n2.7\n6.7\n13\n175\n40\n 20\n 0 20 40\nThreshold for Acc. Gain over Random Prompt\nCV = 3\nCV = 2\nCV = 1\nCV\nFigure 2: Left: CV/MDL have low accuracy at choosing the best prompt (mean/std. err. across 5\nruns with different training sets). Middle: The chance of various accuracy gains on LAMA over the\naverage prompt, when using prompts chosen by CV , and (Right) conservative estimates of CV that\nalso minimize variance in CV; CV often chooses worse-than-average prompts, an issue that is not\nmitigated with conservative prompt selection.\nprompts. We examine the probability with which prompt selection obtains various accuracy gains\nover the average (randomly-chosen) prompt and show results in Fig. 2 (middle) for CV (and similar\nresults in Appendix §B for MDL).\nCV/MDL-chosen prompts show high variance in test accuracy relative to the average prompt. For\nmost model sizes (.1B-6.7B), the chance of improving over the average, randomly-chosen prompt is\nonly ∼56% for CV and ∼55% for MDL. The performance of prompt selection forms a long-tailed\ndistribution; there is a ∼27% chance that prompt selection causes an accuracy drop of ∼13% for all\nmodel sizes and CV/MDL alike. Furthermore, the tails grow heavier as model size increases. For\nthe largest model (GPT-3 175B), CV/MDL-chosen prompts sometimes do far worse than average,\ne.g., 40% worse, 5% of the time. Our results suggest a troubling trend: as models grow bigger and\ngeneralize better, our ability to reliably choose good prompts degrades. One possible explanation\nis that larger models have the capacity to draw more complex decision boundaries, requiring more\nexamples to estimate the true expected loss on unseen examples; we may need to scale validation\nsets along with model size. Overall, the limited average-case gains from prompt selection cannot\nbe expected with any reasonable conﬁdence in the true few-shot setting, a problem that will only\nbecome worse with larger models.\n3.4 Can we increase the likelihood of improved performance from prompt selection?\nAs we have shown, CV and MDL do not reliably choose better-than-average prompts. Here, we\nexplore the extent to which we can reduce the variance in generalization by explicitly preferring\nprompts with low variance (§2.3). For the largest model (GPT-3 175B), we choose prompts based on\na conservative estimate of generalization loss, CVα (§2.3). We show the test accuracy for the prompt\nchosen with various levels of conﬁdence α∈{1,2,3}and with CV (α= 0).\nAs shown in Fig. 2 (right), all αlead to a similar distribution of performance gain as CV . For example,\nCV outperforms the average prompt 50% of the time vs. 51% for α = 2. These results suggest\nthat it is non-trivial to choose prompts that reliably perform better than random selection, even\nwhen explicitly minimizing variance in generalization, further highlighting the difﬁculty of reliably\nselecting good prompts in the true few-shot setting.\n3.5 Does prompt selection improve with more labeled examples?\nThe poor performance of prompt selection methods may be due to using such a small number of\nlabeled examples. As the number of labeled examples increases, we expect prompt selection methods\nto improve. Thus, true few-shot prompt selection may be possible with a few dozen examples (though\nit is not always possible to use more examples, due to limits on input length for LMs like GPT). We\ntherefore examine the test accuracy of CV/MDL-chosen prompts as we use an increasing number of\nlabeled examples N ∈{5,10,15,20,30,40}. For N ≥10, it is not feasible to marginalize over all\npossible training example permutations, so we randomly sample 120 permutations (to match N = 5)\nsuch that each example occurs the same number of times in each position (i.e., to use each example\nas the held-out CV fold the same number of times). We run the experiment for ≤6.7B parameter\nmodels, since it is prohibitively costly to run with larger models via the OpenAI API.\n6\n5 10 15 20 30 40\nNumber of Training Examples\n20\n30\n40\n50\n60\nRelative T est Accuracy of\nChosen Prompt (%)\nLOOCV\n5 10 15 20 30 40\nNumber of Training Examples\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\nAccuracy at Choosing\nBest Prompt (%)\nLOOCV\nDGPT2 82M\nGPT2 117M\nGPT2 345M\nGPT2 762M\nGPT2 1.5B\nGPT3 2.7B\nGPT3 6.7B\nFigure 3: Increasing the number of examples up to 40 does not clearly improve CV in terms of (Left)\naccuracy gain over the average prompt (scaled to 0), relative to the best one (scaled to 100) or (Right)\naccuracy at choosing the best prompt. Mean/std. err. on LAMA over 5 runs (varying train sets).\n1 5 10 15 20 30 40 60 120\n# Forward Passes\n0\n20\n40\n60\n80\n100\nRelative T est Accuracy of\nChosen Prompt (%)\n5 Shot\nSelection\nMethod\nMDL\nLOOCV\nBest\nMean\n1 15 30 60 120\n# Forward Passes\n15 Shot\n1 30 60 120\n# Forward Passes\n30 Shot\nFigure 4: For N ∈{5,10,30}-shot learning, increasing the compute used to estimate CV/MDL does\nnot notably improve the accuracy of chosen prompts beyond a certain point (1 forward pass for MDL,\nN forward passes for CV). Mean/std. err. across 5 runs for GPT-3 6.7B.\nAs shown in Fig. 3, there is no consistent trend in the performance of prompt selection, both in terms\nof task performance (left) and in terms of accuracy at choosing the highest accuracy prompt (right).\nEven in higher-data regimes (40 examples), CV/MDL struggle to choose effective prompts and do not\nconsistently, across model sizes, perform better than choosing examples based on 5 examples. Our\nﬁndings are surprising, because the true-few shot setting is where prompt design has been thought\nmost promising, due to the scarcity of training data [15]. However, the true few-shot setting is also\none in which prompt selection is hardest, greatly undermining the potential value of prompts.\n3.6 Does prompt selection improve with more computation?\nIn the preceding sections, we computed ER,F[CV(A,R,F )] using a ﬁxed number of samples for\nR. Can we improve prompt selection by using more samples, at the cost of increased computation?\nTo answer this question, we vary the number of samples of R(and thus LM forward passes) used\nto compute the above expectation and choose prompts as described in §2.3. To estimate CV with a\nsingle forward pass, we sample a single fold k(here, a single example) and evaluate accuracy on fold\nkwhen conditioning the LM on all others folds. Fig. 4 shows the results for N ∈{5,15,30}training\nexamples using the largest model from §3.5 (GPT-3 6.7B).\nComputation is not the bottleneck in prompt selection, as test accuracy roughly plateaus after one\nforward pass for MDL and N forward passes for CV . This observation holds acrossN, as well as all\nmodels with <6.7B parameters (omitted for space). Our results suggest that true few-shot prompt\nselection is fundamentally limited by the number of examples available.\n3.7 To what extent are chosen prompts speciﬁc to the model?\nWe investigate the extent to which CV/MDL-chosen prompts differ from the best, test-chosen prompts\nin other ways, aside from accuracy. To this end, we examine how well a model does when using a\nprompt chosen for another model, which we refer to as “prompt transfer.” Prompt transfer indicates\nhow tailored the chosen prompt is to a given model. For each model, we examine the average gain of\n7\n0.080.10.30.81.52.76.7 13175\nPrompt Selection Model\n0.08\n0.1\n0.3\n0.8\n1.5\n2.7\n6.7\n13\n175\nPrediction Model\n21 24 20 20 19 22 21 14 17\n18 26 20 21 24 20 24 21 18\n16 18 19 26 28 19 25 22 17\n21 17 23 31 31 25 26 28 27\n26 20 30 23 30 19 22 23 22\n17 20 23 17 26 28 24 30 27\n16 21 20 20 28 22 23 31 33\n18 13 17 15 20 17 21 27 27\n12 6 6 2 15 4 1 10 19\nMDL\n0.080.10.30.81.52.76.7 13175\nPrompt Selection Model\n33 22 18 11 16 5 5 -0 -8\n25 33 18 25 29 19 14 2 0\n17 17 27 25 18 18 20 12 6\n21 24 20 41 28 18 24 9 15\n16 20 28 33 31 17 21 9 10\n12 20 19 23 16 35 19 19 9\n16 18 15 25 17 23 23 22 14\n13 15 9 12 13 21 21 29 20\n7 6 3 10 3 17 8 11 24\nCV\n0.080.10.30.81.52.76.7 13175\nPrompt Selection Model\n100 36 18 17 17 10 7 -0 -4\n29 100 29 33 25 16 19 15 2\n14 23 100 40 32 27 18 15 6\n11 33 52 100 42 31 28 21 13\n3 23 40 37 100 32 32 17 20\n6 20 37 41 33 100 33 15 24\n4 17 28 27 35 38 100 41 25\n2 12 27 21 30 35 43 100 39\n-2 9 12 8 23 25 29 39 100\nTest Accuracy\n0\n10\n20\n30\n40\n50\nRelative T est Accuracy\nof Chosen Prompt(%)\nFigure 5: A model’s accuracy with the prompt chosen for another model using MDL, CV , or test\naccuracy. We show LAMA accuracy relative to the average prompt (scaled to 0) and best prompt\n(scaled to 100) for a model size. CV/MDL show different patterns in prompt transfer than test acc.\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n50\n55\n60\n65\n70T est Acc. of Chosen Prompt (%)\nRTE\nSelection\nMethod\nWorst\nMean\nMDL\nCV\nBest\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n49\n50\n51\n52\n53\n54\nWiC\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n40\n45\n50\n55\n60\n65\n70\n75\n80\nCB\nFigure 6: Accuracy of CV/MDL-chosen prompts vs. accuracy of the worst, average (randomly-\nselected), and best prompt (prior work), on three classiﬁcation tasks (mean/std. err. over 5 runs).\nCV/MDL-chosen prompts generally perform several points worse than the best prompt and do not\nconsistently improve over the average prompt across tasks and model sizes.\nthe chosen prompt over the average prompt, relative to the maximum possible gain, i.e., scaling the\ntest accuracy for each model so that the average prompt scores 0% and the top prompt scores 100%.\nAs shown in Fig. 5, prompts chosen based on test accuracy generalize reasonably well across models\nof similar sizes, a pattern that degrades as we examine CV and especially MDL. For CV , prompts\nchosen using one model size do transfer better to similar model sizes, but CV-chosen prompts do\nnot transfer as effectively as test-chosen ones. For MDL, the chosen prompts are not particularly\ntailored to the given model, performing similarly across many model sizes. Overall, even the pattern\nof prompt transfer differs between test accuracy and CV/MDL.\n3.8 Is prompt selection challenging on other tasks?\nWe now examine the extent to which our results on LAMA tasks hold on other kinds of NLP tasks. We\nexamine three classiﬁcation tasks for which prior work has designed various prompts: Recognizing\nTextual Entailment (RTE), CommitmentBank (CB), and Word-in-Context (WiC). RTE and CB involve\ndetecting whether one sentence entails or contradicts another, and WiC involves determining if a\npolysemous word is used with the same sense in two sentences (e.g., “Room and board” and “He\nnailed boards across the windows.”); See Appendix§D.2 for further task details. We evaluate the\naccuracy of GPT models when using prompts chosen by CV , MDL, and test accuracy, as we did\nfor LAMA. For each task, we evaluate held-out accuracy using the full validation set when using 5\ntraining examples randomly sampled from the task train set, while ensuring that we include at least\none example per class. We evaluate the mean/std. error over 5 train sets. As our set of prompts,\nwe use the manually-written prompts from [2] and [9] – 3 prompts for RTE/CB and 4 prompts for\nWiC. Schick and Schütze [9] designed prompts for bidirectional LMs, so when necessary, we modify\ntheir prompts to be suitable for left-to-right LMs (see Appendix §D.2 for prompts). Fig. 6 shows the\naccuracy of the chosen prompts on each task.\n8\n5\n 0 5 10\nThreshold for Acc. Gain over Mean Prompt\n0\n20\n40\n60\n80\n100% of Time Acc. Gain Below Threshold\nRTE\nParams (B)\n0.08\n0.1\n0.3\n0.8\n1.5\n2.7\n6.7\n13\n175\n3\n 2\n 1\n 0 1 2 3\nThreshold for Acc. Gain over Mean Prompt\nWiC\n40\n 30\n 20\n 10\n 0 10 20 30\nThreshold for Acc. Gain over Mean Prompt\nCB\nFigure 7: The chance of various accuracy gains over the average prompt from CV on RTE, WiC, and\nCB. CV often chooses prompts that are below average (RTE, WiC) or far below average (CB).\nWe observe as similar trend as before, that across tasks and model sizes, the CV/MDL-chosen prompt\nalmost always obtains lower average accuracy than choosing based on test accuracy. The trend holds\neven when choosing between fewer prompts (here, 3-4). CV/MDL-chosen prompts vary greatly in\ntest accuracy across tasks and model sizes, often choosing worse-than-average prompts (e.g., on CB).\nWe examine the variance in chosen prompt accuracy in more detail, by showing the chance that\nselection obtains various accuracy gains over the average prompt. Here, we choose prompts with CV\nusing N forward passes (one evaluation per fold), as it represents a good tradeoff between compute\nand accuracy that is likely to be used in practice. As shown in Fig. 7, accuracy gains are again highly\ndispersed, often negative, and not consistently achieved. For CB, there is a 20% change of a 15%\naccuracy drop for GPT-3 175B. Model sizes vary greatly in how often the CV-chosen prompt leads to\nimprovement, e.g., from 38-82% for WiC and 1-83% for CB. Overall, our earlier ﬁndings carry over\nto other kinds of tasks, indicating that prompt selection is challenging in general.\n4 True Few-Shot Hyperparameter Selection\nHaving shown that true few-shot prompt selection is challenging, we now study the effectiveness of\nmodel selection methods in the context of hyperparameter selection more generally. As our model,\nwe examine ADAPET [12], as it is open-source3 and currently the top-performing few-shot model\naccording to SuperGLUE [61], a standard benchmark in NLP. ADAPET ﬁnetunes the pretrained\nALBERTxxlarge-v2 LM [62] to (1) classify each label as correct or incorrect given the input and (2)\nto predict randomly masked out input tokens given the label and unmasked input tokens, similar to\nMasked LM [63]. ADAPET was developed in the context of tuned few-shot learning, as ADAPET’s\nhyperparameters were chosen based on generalization to validation examples. We investigate how\nADAPET does in the true few-shot setting.\nWe evaluate the impact of using validation examples to choose two hyperparameters: the early\nstopping checkpoint and fraction of words masked for the masked LM objective. ADAPET\nperforms T = 1000 gradient updates on batches of 16 examples and chooses the checkpoint at\nT ∈{250,500,750,1000}with the highest validation accuracy. ADAPET also chooses the best\nmasking fraction M ∈{0.075,0.10,0.105,0.15}. Following ADAPET, we evaluate on SuperGLUE,\na suite of 8 NLP tasks. SuperGLUE consists of four question-answering tasks (BoolQ, COPA,\nMultiRC, ReCoRD), a coreference resolution task (WSC), as well as WiC, RTE, and CB discussed\nin §3.8 (see Appendix §D.2 for task details). We use CV/MDL to choose T and M (out of 16\ntotal combinations) and then train a model on the full dataset with the chosen T and M. We use\nFewGLUE [9], the 32-example subset of SuperGLUE used in prior work on few-shot learning. We\nalso use 3 other 32-example subsets that we randomly sample from SuperGLUE, to estimate variance\nin performance across training sets. ADAPET uses a prompt during ﬁne-tuning, choosing the prompt\nbased on validation examples. To avoid using validation-tuned prompts, we use the ﬁrst prompt for\nevery task as the authors do for ablation studies. Since training ADAPET is expensive, we evaluate\nCV/MDL with K = 8folds.4 We show results in Table 1.\n3https://github.com/rrmenon10/ADAPET\n4See Appendix §D.4 for details on how we evaluate MDL on different SuperGLUE tasks.\n9\nBoolQ CB COPA RTE WiC WSC MultiRC ReCoRD Avg\nAcc Acc/F1 Acc Acc Acc Acc EM/F1 EM/F1\nWorst 75.04.8 79.52.3/67.37.8 76.82.2 63.24.0 49.01.3 77.21.8 38.57.4/80.02.9 76.21.8/86.51.2 69.41.5\nMean 79.01.5 85.92.3/74.511.0 81.12.9 70.82.5 51.51.8 82.52.7 44.26.6/82.32.7 78.31.3/87.80.8 73.91.2\nMDL 76.55.8 85.75.6/74.813.4 82.02.9 70.48.5 52.23.0 82.03.1 39.78.1/80.63.2 78.90.7/88.20.4 73.42.8\nCV 78.92.4 83.95.3/69.210.3 80.53.3 68.77.0 51.11.6 83.12.6 41.97.2/81.43.1 78.71.6/88.11.0 73.02.1\nBest 80.91.0 89.83.1/79.813.4 84.84.5 76.71.8 54.12.3 86.61.8 46.86.9/83.42.9 80.41.1/89.20.7 77.20.9\nADAPET[12] 80.3 89.3 / 86.8 89.0 76.5 54.4 81.7 39.2 / 80.1 85.4 / 92.1 77.3\niPET[9] 80.6 92.9 / 92.4 95.0 74.0 52.2 80.1 33.0 / 74.0 86.0 / 86.5 76.8\nPET[9] 79.4 85.1 / 59.4 95.0 69.8 52.4 80.1 37.9 / 77.3 86.0 / 86.5 74.1\nGPT-3[2] 77.5 82.1 / 57.2 92.0 72.9 55.3 75.0 32.5 / 74.8 89.0 / 90.1 73.2\nTable 1: ADAPET results on SuperGLUE validation when choosing early stopping checkpoint and\nmasked LM rate using CV/MDL vs. the worst/mean/best hyperparameters chosen with validation\n(meanstd. dev. over four 32-shot train sets). On all tasks, CV/MDL-chosen hyperparameters perform\nsimilar to or worse than average, and several points below the best hyperparameters.\nResults Across all SuperGLUE tasks, CV/MDL hyperparameter selection performs similar to\nor worse than average (randomly-chosen) hyperparameters and several points worse than the best\nhyperparameters. In the true few-shot setting, the average SuperGLUE performance of ADAPET\ndrops below that of earlier methods (PET and iPET), highlighting how the use of validation examples\ncan give the false appearance of progress in few-shot learning. On MultiRC, CV/MDL choose\nhyperparameters that give similar performance to the worst hyperparameters, another indication that\nmodel selection methods do not consistently prevent worst-case behavior in the true few-shot setting.\nPreliminary analysis in Appendix §B suggests that choosing better-than-average hyperparameters\nrequires several thousand examples. Overall, our results indicate that it is not just prompt selection\nbut model selection in general that is challenging in very low-data regimes.\n5 Conclusion and Future Work\nOur work shows that it is challenging to make even the most basic decisions about few-shot learning\nalgorithms using only a few labeled examples. Instead, it may be more promising to make additional\nassumptions. The meta-learning setting assumes access to data from many other tasks in order to\nperform learning and model selection [ 20, 21, 64, 65]. Transfer learning and multitask learning\nassume access to data that is directly related to the task with limited data [66–69]. Data augmentation\ntechniques assume there is a viable way to create more data from limited data [ 70–73]. Other\napproaches assume unlabeled data and develop unsupervised model selection techniques [74–76].\nWhen labeled data is cheap, the simplest approach is to assume more examples for validation—\nin which case we might be better off training on the additional examples. Unless we make such\nassumptions explicit, we cannot make meaningful comparisons between few-shot learning algorithms.\nWe ﬁnd the above avenues to be more promising future directions than true few-shot learning given\nthe challenge of model selection.\nInspired by prior work [77, 78], we offer recommendations for future work in true few-shot learning:\n• Report all hyperparameters (prompts) considered and the hyperparameter selection criteria.\n• Include validation examples in the number of examples used by a few-shot learning\nalgorithm. Validation examples include all examples used to decide on any aspect of learning:\nhyperparameters, prompts, training objectives, decoding strategies, model architecture, etc.\n• Once you have decided on the learning algorithm, submit your model for test evaluation\ndirectly, without ﬁrst evaluating on validation. Report the total number of test evaluations\nconducted (ideally, just one). Use the validation set only after test evaluation for any\nablations you report, to avoid making decisions about your algorithm with the validation set.\n• Do not rely on hyperparameters from prior work that were tuned using validation examples\nfor the same benchmark (e.g., SuperGLUE), to avoid indirectly beneﬁting from validation\nexamples. Instead, re-tune such hyperparameters using only the given few-shot examples.\nThe above protocols are strict but mimic how a true few-shot learning algorithm would be used in a\nreal, low-data setting. To ensure researchers comply with such strict protocols, future benchmarks\nmay need to keep large test sets private while releasing only a few labeled examples.\n10\nGiven our negative results on true few-shot learning, a major question remains: is it possible to\nselect models in a true zero-shot setting? Prior work uses LMs for zero-shot learning by choosing\nan arbitrary prompt [17, 79] which requires no data but is suboptimal [4]. Other efforts try multiple\nprompts and choose between them via trial and error alongside manual evaluation [1], effectively\nleveraging human supervision. CLIP [ 13] achieves high zero-shot accuracy on ImageNet after\nextensively tuning prompts and label names using ImageNet’s training set (1.28M examples), as we\nnoticed from the open-source code.5 The authors report a 5% accuracy gain from tuning prompts\nalone, but the training examples used for tuning are not available in true zero-shot learning. Without\nany labeled data, the problem of model selection is even more challenging than in the true few-shot\ncase. Overall, our work provides guidance for future work in few-shot learning by clarifying the\nassumptions made by the true few-shot setting and empirically demonstrates that model selection is a\nmajor roadblock to true few-shot learning.\n6 Limitations and Broader Impact\nWe facilitate fair comparisons between few-shot methods in future work by disambiguating between\nthree few-shot settings: multi-distribution, tuned, and true few-shot learning. We highlight that\none setting, tuned few-shot learning, gives up the practical advantage of few-shot learning by using\nmany labeled examples. Furthermore, we show that several tuned few-shot learning algorithms work\nsigniﬁcantly worse in the true few-shot setting, without tuning on many examples. Our study is\nnot exhaustive, however, and it is possible that effective true few-shot model selection is possible\nusing other criteria (§2.4) or even heuristics not explored here. In this event, our work will have\ndiscouraged work on a few-shot learning setting with applications to low-data settings, e.g., that\ninvolve low-resource languages or expert annotation. Overall, however, we believe our work will\nredirect future work to few-shot settings with more practical applications.\nWe show that it is hard to detect when a small input change hurts an LM’s generalization, even\nwhen the change appears reasonable to human readers. We argue that practitioners will beneﬁt from\nknowing such limitations, but they may also be discouraged from deploying LMs in many useful\ncontexts, such as question-answering, hate speech detection, automatic translation, and commercial\ndialogue systems. Our ﬁndings may also encourage adversaries to target LM-based applications and\nhighlight which models are most susceptible to attack (e.g., larger models). By shedding light on the\nshortcomings of (few-shot) LMs, we hope to spur future work to address these shortcomings.\nAcknowledgments\nWe are grateful to OpenAI for providing access and credits to GPT-3 via the API Academic Access\nProgram, and we thank Miles Brundage, David Schnurr, Felipe Such, Ryan Lowe, and Ilya Sutskever\nfor help with the API. We thank GPT-3 authors Benjamin Mann and Gretchen Krueger for helpful\nfeedback on our paper. We thank Rakesh Menon for assistance with the ADAPET codebase,\nShenglong Wang for cluster support, Zhengbao Jiang for LPAQA prompts, and Tal Linzen, Patrick\nLewis, Eric Wallace, Adam Fisch, Stephen Roller, Aravind Rajeswaran, Gretchen Krueger, Amanda\nNgo, Udit Arora, Sébastian Jean, Jason Phang, and the NYU NLP group for feedback on our draft. KC\nis partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning:\nfrom pattern recognition to AI) and Samsung Research (Improving Deep Learning using Latent\nStructure). KC also thanks Naver, eBay, NVIDIA, and NSF Award 1922658 for support. EP is\ngrateful to NSF and Open Philanthropy for fellowship support.\nReferences\n[1] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners, 2019.\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n5https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_\nImageNet.ipynb\n11\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[3] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few-shot text classiﬁcation\nand natural language inference. Computing Research Repository, arXiv:2001.07676, 2020.\n[4] Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? TACL, 8:423–438, 2020.\n[5] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n[6] Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use:\nImproving few-shot performance of language models, 2021.\n[7] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to ﬁnd them: Overcoming few-shot prompt order sensitivity, 2021.\n[8] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, L. Carin, and W. Chen. What makes\ngood in-context examples for gpt-3? arXiv, abs/2101.06804, 2021.\n[9] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are\nalso few-shot learners. Computing Research Repository, arXiv:2009.07118, 2020.\n[10] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. Rissanen data analysis: Examining dataset\ncharacteristics via description length. In ICML, 2021.\n[11] Timo Schick and H. Schutze. Few-shot text generation with pattern-exploiting training. arXiv,\nabs/2012.11926, 2020.\n[12] Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving\nand simplifying pattern exploiting training. arxiv preprint arXiv:2103.11955, 2021.\n[13] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision, 2021.\n[14] Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot\nlearner, 2021.\n[15] Teven Le Scao and Alexander M. Rush. How many data points is a prompt worth?, 2021.\n[16] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv, abs/1910.01108, 2019.\n[17] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander Miller. Language models as knowledge bases? In EMNLP, pages 2463–2473,\nHong Kong, China, November 2019. ACL.\n[18] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra.\nMatching networks for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and\nR. Garnett, editors, NeuRIPS, volume 29. Curran Associates, Inc., 2016.\n[19] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In\nI. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\neditors, NeuRIPS, volume 30. Curran Associates, Inc., 2017.\n[20] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.\n[21] Ke Li and Jitendra Malik. Learning to optimize. arXiv, abs/1606.01885, 2017.\n[22] David M. Allen. The relationship between variable selection and data agumentation and a\nmethod for prediction. Technometrics, 16(1):125–127, 1974.\n12\n[23] M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the\nRoyal Statistical Society. Series A (Methodological), 36:111–133, 1974.\n[24] Seymour Geisser. The predictive sample reuse method with applications. Journal of the\nAmerican Statistical Association, 70(350):320–328, 1975.\n[25] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning.\nSpringer Series in Statistics. Springer New York Inc., New York, NY , USA, 2001.\n[26] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast\nadaptation of deep networks. In ICML, volume 70 of ICML’17, pages 1126–1135. JMLR.org,\n2017.\n[27] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with\nimplicit gradients. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and\nR. Garnett, editors, NeurIPS, volume 32. Curran Associates, Inc., 2019.\n[28] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465 – 471, 1978.\n[29] J. Rissanen. Universal coding, information, prediction, and estimation. IEEE Transactions on\nInformation Theory, 30(4):629–636, 1984.\n[30] A. P. Dawid. Present position and potential developments: Some personal views: Statistical\ntheory: The prequential approach. Journal of the Royal Statistical Society. Series A (General),\n147(2):278–292, 1984.\n[31] Peter Grünwald. A tutorial introduction to the minimum description length principle. CoRR,\nmath.ST/0406077, 06 2004.\n[32] Léonard Blier and Yann Ollivier. The description length of deep learning models. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors,NeuRIPS,\nvolume 31, pages 2216–2226. Curran Associates, Inc., 2018.\n[33] Alselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Occam’s\nrazor. Inf. Process. Lett., 24(6):377–380, April 1987.\n[34] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela.\nMasked language modeling and the distributional hypothesis: Order word matters pre-training\nfor little. CoRR, abs/2104.06644, 2021.\n[35] P. J. Phillips, Hyeonjoon Moon, S. A. Rizvi, and P. J. Rauss. The feret evaluation methodology\nfor face-recognition algorithms. TPAMI, 22(10):1090–1104, 2000.\n[36] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in\ncommercial gender classiﬁcation. In Sorelle A. Friedler and Christo Wilson, editors, Fairness,\nAccountability and Transparency, volume 81 of PMLR, pages 77–91, New York, NY , USA,\n23–24 Feb 2018. PMLR.\n[37] Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve\nFried, Ryan Lowe, and Joelle Pineau. Ethical challenges in data-driven dialogue systems. In\nAAAI/ACM Conference on AI, Ethics, and Society, AIES ’18, page 123–129, New York, NY ,\nUSA, 2018. Association for Computing Machinery.\n[38] Chandra Khatri, Behnam Hedayatnia, Anu Venkatesh, Jeff Nunn, Yi Pan, Qing Liu, Han Song,\nAnna Gottardi, Sanjeev Kwatra, Sanju Pancholi, Ming Cheng, Qinglang Chen, Lauren Stubel,\nKarthik Gopalakrishnan, Kate Bland, Raefer Gabriel, Arindam Mandal, Dilek Hakkani-Tür,\nGene Hwang, Nate Michel, Eric King, and Rohit Prasad. Advancing the state of the art in open\ndomain dialog systems through the alexa prize. CoRR, abs/1812.10757, 2018.\n[39] Javier García, Fern, and o Fernández. A comprehensive survey on safe reinforcement learning.\nJMLR, 16(42):1437–1480, 2015.\n[40] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul F. Christiano, John Schulman, and Dan Mané.\nConcrete problems in AI safety. CoRR, abs/1606.06565, 2016.\n13\n[41] Sumio Watanabe. Asymptotic equivalence of bayes cross validation and widely applicable\ninformation criterion in singular learning theory. JMLR, 11(116):3571–3594, 2010.\n[42] H. Akaike. A new look at the statistical model identiﬁcation. TACON, 19(6):716–723, 1974.\n[43] C. L. Mallows. Some comments on cp. Technometrics, 15(4):661–675, 1973.\n[44] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization\nfor general algorithm conﬁguration. In Carlos A. Coello Coello, editor, Learning and Intelligent\nOptimization, pages 507–523, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.\n[45] James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. Algorithms for hyper-\nparameter optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q.\nWeinberger, editors,NeuRIPS, volume 24. Curran Associates, Inc., 2011.\n[46] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine\nlearning algorithms. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors,\nNeuRIPS, volume 25. Curran Associates, Inc., 2012.\n[47] Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon,\nBala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat. Chapter\n15 - evolving deep neural networks. In Robert Kozma, Cesare Alippi, Yoonsuck Choe, and\nFrancesco Carlo Morabito, editors, Artiﬁcial Intelligence in the Age of Neural Networks and\nBrain Computing, pages 293–312. Academic Press, 2019.\n[48] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V . Le. Regularized evolution for\nimage classiﬁer architecture search. AAAI, 33(01):4780–4789, Jul. 2019.\n[49] Barret Zoph and Quoc V . Le. Neural architecture search with reinforcement learning. InICLR.\nOpenReview.net, 2017.\n[50] J. Larsen, L.K. Hansen, C. Svarer, and M. Ohlsson. Design and regularization of neural\nnetworks: the optimal use of a validation set. In Neural Networks for Signal Processing VI.\nIEEE Signal Processing Society Workshop, pages 62–71, 1996.\n[51] Yoshua Bengio. Gradient-Based Optimization of Hyperparameters. Neural Computation,\n12(8):1889–1900, 08 2000.\n[52] O. Chapelle, V . Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for\nsupport vector machines. Machine Learning, 46:131–159, 2004.\n[53] Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search.\nIn ICLR, 2019.\n[54] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric Wallace, and Sameer Singh.\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated\nPrompts. In EMNLP, pages 4222–4235, Online, November 2020. ACL.\n[55] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\nunderstands, too, 2021.\n[56] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [MASK]: learning vs.\nlearning to recall. CoRR, abs/2104.05240, 2021.\n[57] Nina Poerner, Ulli Waltinger, and Hinrich Schütze. E-BERT: Efﬁcient-yet-effective entity\nembeddings for BERT. In Findings of EMNLP, pages 803–818, Online, November 2020. ACL.\n[58] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-\nthe-art natural language processing. In EMNLP: System Demonstrations, pages 38–45, Online,\nOctober 2020. ACL.\n14\n[59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-\nBuc, E. Fox, and R. Garnett, editors, NeuRIPS, pages 8024–8035. Curran Associates, Inc.,\n2019.\n[60] Elena V oita and Ivan Titov. Information-theoretic probing with minimum description length. In\nEMNLP, pages 183–196, Online, November 2020. ACL.\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, NeuRIPS, volume 32. Curran Associates, Inc., 2019.\n[62] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. In ICLR,\n2020.\n[63] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In NAACL, pages 4171–4186,\nMinneapolis, Minnesota, June 2019. ACL.\n[64] Eleni Triantaﬁllou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross\nGoroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle.\nMeta-dataset: A dataset of datasets for learning to learn from few examples. In ICLR, 2020.\n[65] Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossﬁt: A few-shot learning challenge for\ncross-task generalization in NLP. CoRR, abs/2104.08835, 2021.\n[66] Rich Caruana. Learning many related tasks at the same time with backpropagation. In\nG. Tesauro, D. Touretzky, and T. Leen, editors,NeuRIPS, volume 7. MIT Press, 1995.\n[67] Rich Caruana. Multitask learning. Machine Learning, 28(1):41–75, July 1997.\n[68] Jason Phang, Thibault Févry, and Samuel R. Bowman. Sentence encoders on stilts:\nSupplementary training on intermediate labeled-data tasks. CoRR, abs/1811.01088, 2018.\n[69] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks\nfor natural language understanding. In ACL, pages 4487–4496, Florence, Italy, July 2019. ACL.\n[70] Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas\nLukasiewicz. A surprisingly robust trick for the Winograd schema challenge. In ACL, pages\n4837–4842, Florence, Italy, July 2019. ACL.\n[71] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data\naugmentation for consistency training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin, editors, NeuRIPS, volume 33, pages 6256–6268. Curran Associates, Inc., 2020.\n[72] Jiaao Chen, Zichao Yang, and Diyi Yang. MixText: Linguistically-informed interpolation of\nhidden space for semi-supervised text classiﬁcation. In ACL, pages 2147–2157, Online, July\n2020. ACL.\n[73] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-\nPing Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmentation\nfor commonsense reasoning. In Findings of EMNLP, pages 1008–1025, Online, November\n2020. ACL.\n[74] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural\nmachine translation. In ICLR, 2018.\n[75] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato.\nUnsupervised machine translation using monolingual corpora only. In ICLR, 2018.\n15\n[76] Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised\nquestion decomposition for question answering. In EMNLP, pages 8864–8880, Online,\nNovember 2020. ACL.\n[77] Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow. Realistic\nevaluation of deep semi-supervised learning algorithms. CoRR, abs/1804.09170, 2018.\n[78] Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your\nwork: Improved reporting of experimental results. In EMNLP, pages 2185–2194, Hong Kong,\nChina, November 2019. ACL.\n[79] Allyson Ettinger. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics\nfor language models. TACL, 8:34–48, 2020.\n[80] Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-\none-out cross-validation and waic. Statistics and Computing, 27(5):1413–1432, September\n2017.\n[81] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A.\nSmith. Fine-tuning pretrained language models: Weight initializations, data orders, and early\nstopping. CoRR, abs/2002.06305, 2020.\n[82] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In\nNAACL, pages 2924–2936, Minneapolis, Minnesota, June 2019. ACL.\n[83] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:\nInvestigating projection in naturally occurring discourse. Sinn und Bedeutung, 23(2):107–124,\nJuly 2019.\n[84] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment\nchallenge. In Joaquin Quiñonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alché\nBuc, editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object\nClassiﬁcation, and Recognising Tectual Entailment, pages 177–190, Berlin, Heidelberg, 2006.\nSpringer Berlin Heidelberg.\n[85] Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini,\nand Idan Szpektor. The second PASCAL recognising textual entailment challenge. In Second\nPASCAL Challenges Workshop on Recognising Textual Entailment, 2006.\n[86] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal\nrecognizing textual entailment challenge. In ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, RTE ’07, page 1–9, USA, 2007. ACL.\n[87] Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini.\nThe ﬁfth pascal recognizing textual entailment challenge. In TAC, 2009.\n[88] Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: The word-in-context dataset for\nevaluating context-sensitive meaning representations. In NAACL. ACL, 2019.\n[89] Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In\nKR, KR’12, page 552–561. AAAI Press, 2012.\n[90] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In\nNAACL. ACL, 2018.\n[91] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nReCoRD: Bridging the gap between human and machine commonsense reading comprehension.\narXiv preprint 1810.12885, 2018.\n16\nA True Few-Shot Prompt Selection with Other Generalization Criteria\nHere, we evaluate the performance of prompts chosen using other generalization criteria, to examine\nthe extent to which poor prompt selection is speciﬁc to CV and MDL. We evaluate on LAMA and\nfollow the same experimental setup used to evaluate CV/MDL, as described in §3.1. As before,\nwe examine the average test accuracy of the prompt chosen by a particular criterion, as well as the\npercentage of the time that a given criterion chose the prompt with the highest test accuracy. We now\ndescribe the other criteria we test.\nA.1 Bayesian Cross-Validation\nBayesian CV is a variant of CV that evaluates a learning algorithm Abased on its expected loss on a\nheld-out fold after marginalizing over the model according the posterior distribution [for an overview,\nsee 80]. In our setup, each model corresponds to a unique set of random factors Rtrained by A.\nGiven some inputs X = x1:N and labels Y = y1:N, we assume a uniform prior p(R) over Rand\nassume that Rand Xare independent (p(R|X) =p(R)). We then derive the posterior probability as:\np(R|X,Y ) =p(Y|R,X)p(R|X)\np(Y|X) = p(Y|R,X)\np(Y|X) = p(Y|R,X)∑\nR′ p(Y|R′,X)\nwhere for any R′:\np(Y|R′,X) =\nN∏\ni=1\np(yi|y1:i−1,X,R ′) =\nN∏\ni=1\np(yi|y1:i−1,x1:i,R′).\nThe second equality holds because pis a left-to-right LM that predicts yi only based on the input xi\nand earlier examples (x1:i−1,y1:i−1). We marginalize out the model over the posterior distribution:\nCVBayes(A,R,F ) =Ek∼Unif(1,K)\n[\nL\n(\nER∼p(R|F(Dtrain)¬k)[A(F(Dtrain)¬k,R)]; F(Dtrain)k\n)]\nWe then choose the algorithm (prompt) that minimizes ER,F[CVBayes(A,R,F )], where R is the\norder of training examples.\nA.2 Interpolating between CV and MDL\nOur experiments in the main paper suggest that CV/MDL behave differently in terms of prompt\nselection. In this section, we describe a way to interpolate between CV and MDL, in order to devise a\nnew criterion that may inherit advantageous properties from both CV and MDL. Similar to MDL, we\nmeasure the expected loss on a held-out fold F(Dtrain)k when training on the previous F(Dtrain)1:k−1\nfolds, doing so across all k= 1,...,K folds. However, we now weight the loss on F(Dtrain)k by a\nfactor that depends on the number of training examples, p(k; β) ∝exp(−β|F(Dtrain)1:k−1|), where\nβis an inverse temperature hyperparameter. MDL is equivalent to using a uniform weight over all\ntrain sizes (β = 0), and CV is equivalent to using a non-zero weight for only the largest train size\n(β = ∞). Formally, we deﬁne the interpolated criteria, MDLβ(A,R,F ), as follows:\nMDLβ(A,R,F ) =Ek∼p(k;β)\n[\nL\n(\nA(F(Dtrain)1:k−1,R); F(Dtrain)k\n)]\n.\nWe set the hyperparameter β to the default value of β = 1to avoid having to choose β based on\na limited number of examples available in true few-shot learning. We choose the algorithm that\nminimizes ER,F[MDLβ(A,R,F )].\nA.3 Joint Log-Probability\nUp to this point, we have used generalization criteria that use the NLL of the label given the\ninput, −log p(y|x), as the loss function L. However, other loss functions may correlate better with\ngeneralization. In particular, we hypothesize that a good prompt leads the LM to give the entire input\n(x,y) high probability, i.e., a low, joint log-probability −log p(x,y). We thus use −log p(x,y) as\nthe loss function to measure CV and MDL, which we refer to as CVx,y and MDLx,y, respectively.\nSince −log p(x,y) = [−log p(y|x)] + [−log p(x)], joint log-probability is equivalent to the label\n17\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n0\n10\n20\n30\n40\n50\n60T est Acc. of Chosen Prompt (%)\nWorst\nMean\nMDLx, y\nCVx, y\nMDL\nMDL = 1\nCV\nCVBayes\nBest\n0.08 0.1 0.3 0.8 1.5 2.7 6.7 13 175\nModel Parameters (B)\n10\n15\n20\n25\n30\n35Acc. at Choosing T op Prompt (%)\nMDLx, y\nCVx, y\nMDL\nMDL = 1\nCV\nCVBayes\nFigure 8: Top: LAMA-UHN accuracy of prompts chosen using different generalization criteria\nvs. accuracy of the worst, average (randomly-selected), and best prompt (prior work). Bottom:\nThe average accuracy gain from using criteria-chosen prompts instead of randomly-chosen ones,\nrelative to the gain from the best prompt. We plot mean/std. err. across 5 runs with different\ntraining sets. Across all model sizes, criteria-chosen prompts obtain only small improvements over\nrandomly-chosen ones and perform far worse than the best prompts.\n40\n 20\n 0 20 40 60\nThreshold for Acc. Gain over Mean Prompt\n0\n20\n40\n60\n80\n100\n% of Time Acc. Gain\nBelow Threshold\nParams (B)\n0.08\n0.1\n0.3\n0.8\n1.5\n2.7\n6.7\n13\n175\n5 10 15 20 30 40\nNumber of Training Examples\n15\n20\n25\n30\n35\n40\n45\n50\nRelative T est Accuracy of\nChosen Prompt (%)\nMDL\n5 10 15 20 30 40\nNumber of Training Examples\n15\n20\n25\n30\nAccuracy at Choosing\nBest Prompt (%)\nMDL\nDGPT2 82M\nGPT2 117M\nGPT2 345M\nGPT2 762M\nGPT2 1.5B\nGPT3 2.7B\nGPT3 6.7B\nFigure 9: Left: Chance of various accuracy gains for MDL-chosen prompts over average (randomly-\nchosen) prompts on LAMA-UHN. As with CV , there is a wide variance in accuracy gains, especially\nfor larger models, and a signiﬁcant chance of choosing a worse-than-average prompt. Middle:\nIncreasing the number of examples up to 40 does not clearly improve MDL in terms of acc. gain over\nthe average prompt (scaled to 0), relative to the best one (scaled to 100) or (Right) acc. at choosing\nthe best prompt (mean/std. err. on LAMA over 5 runs with different train sets).\nNLL −log p(y|x) used before, with an additional term −log p(x) that measures the input NLL.\nWe measure −log p(x,y) by evaluating the total NLL of all tokens in the prompt-formatted (x,y)\npair (including prompt tokens). We choose the algorithm that minimizes ER,F[CVx,y(A,R,F )] or\nER,F[MDLx,y(A,R,F )].\nA.4 Results\nAs shown in Fig. 8 (top), all criteria choose prompts with a similar average accuracy, close to the\naverage accuracy of randomly-chosen prompts. Likewise, all criteria are similarly inaccurate at\nchoosing the highest accuracy prompt, as shown in Fig 8 (bottom). These results show that true\nfew-shot prompt selection is challenging not only for CV and MDL but also many other criteria.\n18\n32 67 139 289 601 1252 2607 5428\nTrain Size\n50\n55\n60\n65\n70Accuracy (%)\nWiC\nMean\nCV\n32 72 162 366 824 1857 4184 9427\nTrain Size\n76\n78\n80\n82\n84\n86\n88Accuracy (%)\nBoolQ\nMean\nCV\n358 523 765 1117 1633 2387 3489 5100\nTrain Size\n40\n45\n50\n55EM (%)\nMultiRC\nMean\nCV\nFigure 10: ADAPET accuracy using CV-chosen hyperparameters as the number of examples increases.\nThe shaded region shows the range of accuracies obtained using the same training set but different\nhyperparameter settings (16 in total).\nB Additional Results with MDL\nIn the main paper, we showed several results for CV alone for brevity, so in this section, we show the\ncorresponding plots for MDL as well. The overall trends are the same for both CV and MDL.\nIn §3.3, we found that the gains from choosing prompts using CV are high variance, a variance that\nincreases with model size (Fig. 2). Here, we show the same results but for MDL in Fig. 9 (left).\nSimilar to CV , MDL-chosen prompts have high variance in test accuracy relative to the average\nprompt, especially for larger models. This ﬁnding suggests that the high variance is due not to CV in\nparticular, but to the inherent difﬁculty of true few-shot model selection.\nIn §3.5, we examined if increasing the number of examples improves prompt selection for CV . Fig. 9\n(middle/right) shows the results for MDL, which are similar to those for CV . When increasing the\nexamples used, we do not observe a consistent increase in the gain achieved by MDL over random\nselection, relative to the best prompt (Fig. 9 middle). Similarly, we do not observe a consistent\nincrease in the accuracy of MDL at choosing the best prompt (Fig. 9 right). For some model sizes,\nthere may potentially be some improvement with more examples, but the standard error is high, and\nthe overall accuracies achieved by MDL are still lower than those from CV shown earlier in Fig. 3.\nOverall, model selection is challenging for both CV and MDL, even as we approach the maximum\nnumber of examples that can ﬁt in the context of GPT models.\nC How many examples do you need for effective model selection?\nHere, we conduct a preliminary analysis on the minimum number of examples is necessary to choose\na better-than-average model. We examine this question in the context of ADAPET, which can handle\nan arbitrary number of examples (GPT-based models can only handle a number of examples that ﬁt\nwithin the LM input—2048 tokens or ∼1500 words). We use the same setup and hyperparameter\nrange as in §4 but vary the number of training examples.\nFig. 10 shows accuracy on WiC and BoolQ of CV-chosen hyperparameters, compared to the worst,\naverage, and best hyperparameters. For WiC and MultiRC, CV requires >2-3k examples to choose\nbetter-than-average hyperparameters. For BoolQ, CV performs similar to the average hyperparameters\neven when using up to 9k examples. This result may be due to the fact that we retrain the model\nusing the CV-chosen hyperparameters, but ﬁnetuning pretrained LMs often has high variance in\nperformance [68, 81]. Thus, when more data is available, CV may be outperformed by using a single\ntrain-validation split and choosing the model that does well on the validation split, without retraining\non the combined train+validation set. We leave further exploration of model selection in higher data\nregimes as an important direction for future work.\nD Task and Experimental Details\nD.1 LAMA\nPrompts Used For the full list LPAQA prompts, please see https://github.com/jzbjyb/\nLPAQA/tree/master/prompt. There are up to 90 LPAQA prompts per relation, so we use a subset\n19\nTask Prompt Label Names\nRTE His family has steadfastly denied the charges.\nquestion: The charges were denied by his family. True or False?\nanswer: True\nTrue, False\nThe charges were denied by his family?\nHis family has steadfastly denied the charges.\nTherefore, the answer is yes.\nyes, no\n“The charges were denied by his family”?\n“His family has steadfastly denied the charges.”, so the answer is yes.\nyes, no\nCB He’d gone. Philip had to get them back. His Dad would kill him if he found that\nhe’d taken them.\nquestion: Philip had taken them. true, false, or neither?\nanswer: true\ntrue, false,\nneither\nPhilip had taken them?\nHe’d gone. Philip had to get them back. His Dad would kill him if he found that\nhe’d taken them.\nTherefore, the answer is yes.\nyes, no,\nmaybe\n“Philip had taken them”?\n“He’d gone. Philip had to get them back. His Dad would kill him if he found that\nhe’d taken them.”\nTherefore, the answer is yes.\nyes, no,\nmaybe\nWiC Room and board.\nHe nailed boards across the windows.\nquestion: Is the word ‘board’ used in the same way in the two sentences above?\nanswer: no\nno, yes\n“Room and board.” / “He nailed boards across the windows.”. Similar sense of\n“board”? No.\nNo, Yes\nRoom and board. He nailed boards across the windows. Does “board” have the\nsame meaning in both sentences? No.\nNo, Yes\nboard.\n- “Room and board.” (Sense 1a)\n- “He nailed boards across the windows.” (Sense 2a)\n2a, 1b\nTable 2: The different prompts we use for RTE, CB, and WiC. Weunderline the token to predict. For\neach dataset, the ﬁrst prompt is the one from GPT-3 [2] and the others are from [9], modiﬁed to be\ncompatible with left-to-right LMs when necessary.\nof prompts to evaluate the impact of a small amount of validation-based prompt tuning. We ﬁlter out\nprompts that do not end with the target answer blanked out (“Geoffrey Hinton was _ profession.”),\nwhich cannot be easily used with left-to-right LMs like GPT. For mined prompts (group 2), we choose\nthe 5 prompts that occur most frequently in Wikipedia, similar to [4]. We include all prompts if fewer\nthan 5 are available. For paraphrased prompts (groups 1 and 3), we choose up to 5 prompts with the\nhighest round-trip back-translation probability, similar to [4]. Finally, we de-duplicate prompts, as\nsome prompts occur in multiple groups.\nD.2 SuperGLUE\nDatasets Here, we go into more detail about various tasks in SuperGLUE [61]. BoolQ [Boolean\nQuestions; 82] involves answering a yes/no question about a paragraph. COPA [Choice of Plausible\nAlternatives; 83] involves determining the cause (or effect) of a given premise from two possible\nchoices. RTE (Recognizing Textual Entailment) is a 2-sentence classiﬁcation task to determine if\na given premise entails a given hypothesis (2-way classiﬁcation between entailed and not entailed\n20\nclasses) [84–87]. Similarly, CB [CommitmentBank; 83] is an entailment detection task but with 3\nclasses (entailed, contradicted, and neither). WiC [Word-in-Context, 88] involves determining if a\npolysemous word is used with the same sense in two sentences. WSC [Winograd Schema Challenge,\n89] is a coreference resolution task to determine the correct referrent of a pronoun in a sentence\nfrom among the provided choices. MultiRC [Multi-Sentence Reading Comprehension, 90] is a\nquestion-answering task where each example consists of a context paragraph, a question about that\nparagraph, and a list of possible answers, multiple of which can be correct. ReCoRD [Reading\nComprehension with Commonsense Reasoning Dataset, 91] is a multiple-choice question-answering\ntask, where each example consists of a news article and a cloze-style question about the article in\nwhich one entity is masked out. A system must predict the masked out entity from a list of possible\nentities in the provided passage.\nPrompts Used In Table 2, we show the prompts we used for RTE, CB, and WiC in §3.8. Following\n[3], we also vary the textual label names used to get the logits for a given output class. I.e., for RTE,\nwe use the logit for the word “True” as the probability for the “entailed” class and “False” for the\n“not entailed” class. We compute class probabilities using a softmax over the above class logits.\nD.3 Dataset and model licenses\nLAMA is licensed under CC 4.0. 6 The licenses for SuperGLUE datasets allow for their use and\nredistribution in a research context (see each individual dataset papers for license details). These\ndatasets do not contain private, personally identiﬁable information but may contain offensive content.\nGPT-2/DistilGPT-2 models are licensed under a modiﬁed MIT license.7 GPT-3 models are licensed by\nOpenAI API to customers via a non-exclusive, non-sublicensable, non-transferable, non-assignable,\nrevocable license.8\nD.4 Computing MDL with ADAPET\nFor MDL as formulated in §2.2, it is not possible to evaluate on the ﬁrst fold of training data, since the\nlearning algorithm (here, ﬁnetuning) requires some initial training data. MDL requires evaluating the\nloss of the learning algorithm Aon the ﬁrst fold of data without any training data. Since ﬁnetuning is\nnot possible without training data, we say that, in this case, Areturns a uniform distribution over all\nlabels, following prior work [e.g., 32].9 We use 16 examples (one mini-batch) in the ﬁrst fold and 2\nexamples per fold for a remaining 8 folds, to match the number of models we train for CV . As before,\nwe use NLL as the loss L, which is straightforward for most tasks. For WSC and ReCoRD, ADAPET\nreturns class probabilities ∈{0,1}which we smooth as {ϵ,1 −ϵ}with ϵ= 10−6 to avoid ∞loss\nvalues for CV/MDL. For MultiRC, ADAPET makes several binary predictions per example, so we\nsum the NLLs for these predictions to compute per-example loss.\nD.5 Computational Cost\nWe use the OpenAI API to evaluate GPT-3 models, costing a total of $2826.73 for all experiments.\nFor GPT-2 experiments, we use a single AMD MI50 GPU (32GB GPU memory) to perform model\ninference, which requires at most 8 hours (usually less) for all GPT-2/DistilGPT-2 models to evaluate\nER,F[CV(A,R,F )], ER,F[MDL(A,R,F )], and expected test accuracy for LAMA and SuperGLUE\n(any number of training examples). For ADAPET experiments, we use a single AMD MI50 GPU for\nup to 12 hours to run training and inference for a single model and hyperparamater setting.\n6https://github.com/facebookresearch/LAMA/blob/master/LICENSE\n7https://github.com/openai/gpt-2/blob/master/LICENSE\n8https://beta.openai.com/policies/terms-of-use\n9This technique can be viewed as evaluating the labels’ MDL or compression rate where the ﬁrst fold is\ncompressed using a uniform distribution rather than a learning algorithm.\n21",
  "topic": "Selection (genetic algorithm)",
  "concepts": [
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.835946798324585
    },
    {
      "name": "Hyperparameter",
      "score": 0.8113619089126587
    },
    {
      "name": "Computer science",
      "score": 0.746553897857666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6338534355163574
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6323391199111938
    },
    {
      "name": "Machine learning",
      "score": 0.6006981134414673
    },
    {
      "name": "Model selection",
      "score": 0.5304607152938843
    },
    {
      "name": "Language model",
      "score": 0.4910147190093994
    },
    {
      "name": "Computation",
      "score": 0.4827217757701874
    },
    {
      "name": "Natural language processing",
      "score": 0.3810238838195801
    },
    {
      "name": "Algorithm",
      "score": 0.10688495635986328
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}