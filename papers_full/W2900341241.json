{
  "title": "UBC-NLP at IEST 2018: Learning Implicit Emotion With an Ensemble of Language Models",
  "url": "https://openalex.org/W2900341241",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2805263177",
      "name": "Hassan Alhuzali",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2109009100",
      "name": "Mohamed Elaraby",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A3119026467",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1971222444",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2046677541",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1569507287",
    "https://openalex.org/W2466778245",
    "https://openalex.org/W2741447225",
    "https://openalex.org/W2963662881",
    "https://openalex.org/W3106003309",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2891209320",
    "https://openalex.org/W1513398909",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2805351602",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W4302343710"
  ],
  "abstract": "We describe UBC-NLP contribution to IEST-2018, focused at learning implicit emotion in Twitter data. Among the 30 participating teams, our system ranked the 4th (with 69.3% F-score). Post competition, we were able to score slightly higher than the 3rd ranking system (reaching 70.7%). Our system is trained on top of a pre-trained language model (LM), fine-tuned on the data provided by the task organizers. Our best results are acquired by an average of an ensemble of language models. We also offer an analysis of system performance and the impact of training data size on the task. For example, we show that training our best model for only one epoch with < 40% of the data enables better performance than the baseline reported by Klinger et al. (2018) for the task.",
  "full_text": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 342–347\nBrussels, Belgium, October 31, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n342\nUBC-NLP at IEST 2018: Learning Implicit Emotion With an Ensemble of\nLanguage Models\nHassan Alhuzali Mohamed Elaraby Muhammad Abdul-Mageed\nNatural Language Processing Lab\nThe University of British Columbia\n{halhuzali,mohamed.elaraby}@alumni.ubc.ca,muhammad.mageeed@ubc.ca\nAbstract\nWe describe UBC-NLP contribution to IEST-\n2018, focused at learning implicit emotion\nin Twitter data. Among the 30 participating\nteams, our system ranked the 4th (with 69.3%\nF-score). Post competition, we were able to\nscore slightly higher than the 3rd ranking sys-\ntem (reaching 70.7%). Our system is trained\non top of a pre-trained language model (LM),\nﬁne-tuned on the data provided by the task or-\nganizers. Our best results are acquired by an\naverage of an ensemble of language models.\nWe also offer an analysis of system perfor-\nmance and the impact of training data size on\nthe task. For example, we show that training\nour best model for only one epoch with<40%\nof the data enables better performance than the\nbaseline reported by Klinger et al. (2018) for\nthe task.\n1 Introduction\nEmotion is essential in human experience and\ncommunication, lending special signiﬁcance to\nnatural language processing systems aimed at\nlearning it. Emotion detection systems can be ap-\nplied in a host of domains, including health and\nwell-being, user proﬁling, education, and mar-\nketing. There is a small, yet growing, body of\nNLP literature on emotion. Early works focused\non creating and manually labeling datasets. The\nSemEval 2007 Affective Text task Strapparava\nand Mihalcea (2007) and Aman and Szpakowicz\n(2007) are two examples that target the news and\nblog domains respectively. In these works, data\nwere labeled for the 6 basic emotions of Ekman\n(Ekman, 1972). More recent works exploit dis-\ntant supervision (Mintz et al., 2009) to automat-\nically acquire emotion data for training systems.\nMore speciﬁcally, a number of works use hashtags\nlike #happy and #sad, especially occurring ﬁ-\nnally in Twitter data, as a proxy of emotion (Wang\net al., 2012; Mohammad and Kiritchenko, 2015;\nV olkova and Bachrach, 2016). Abdul-Mageed and\nUngar (2017) report state-of-the-art results using a\nlarge dataset acquired with hashtags. Other works\nexploit emojis to capture emotion carrying data\n(Felbo et al., 2017). Alhuzali et al. (2018) intro-\nduce a third effective approach that leverages ﬁrst-\nperson seed phrases like “I’m happy that”to col-\nlect emotion data.\nKlinger et al. (2018) propose yet a fourth\nmethod for collecting emotion data that depends\non the existence of the expression ”emotion-word\n+ one of the following words (when, that or be-\ncause)” in a tweet, regardless of the position of the\nemotion word. In the “Implicit Emotion” shared\ntask 1, participants were provided data represent-\ning the 6 emotions in the set (anger, disgust, fear,\njoy, sad, surprise). The trigger word was removed\nfrom each tweet. To illustrate, the task is to predict\nthe emotion in a tweet like “Boys who like Star-\nbucks make me [#TRIGGERWORD#] because we\ncan go on cute coffee dates” (with the triggered\nword labeled asjoy). In this paper, we describe our\nsystem submitted as part of the competition. Over-\nall, our submission ranked the 4th out of the 30\nparticipating teams. With further experiments, we\nwere able to acquire better results, which would\nrank our model at top 3 (70.7% F-score).\nThe rest of the paper is organized as follows:\nSection 2 describes the data. Section 3 offers a\ndescription of the methods employed in our work.\nSection 3 is where we present our results, and we\nperform an analysis of these results in Section 5.\nWe list negative experiments in Section 6 and con-\nclude in Section 7.\n1http://implicitemotions.wassa2018.com\n343\n2 Data\nWe use the Twitter dataset released by the organiz-\ners of the “Implicit Emotion” task, as described in\nthe previous section. The data are partitioned into\n153,383 tweets for training, 9591 tweets for val-\nidation, and 28,757 data points for testing. The\ntraining and validation sets were provided early\nfor system development, while the test set was\nreleased one week before the deadline of system\nsubmission. The full details of the dataset can be\nfound in Klinger et al. (2018). We now describe\nour methods in the nesxt section.\n3 Methods\n3.1 Pre-processing\nWe adopt a simple pre-processing scheme, similar\nto most of the pre-trained models we employ. This\ninvolves lowercasing all text and ﬁltering out urls\nand user mentions. We also split clusters of emojis\ninto individual emojis, following Duppada et al.\n(2018). For our vocabulary V , we retain the top\n100k words and then remove all words occurring\n<2 times, which leaves |V|= 23,656.\n3.2 Models\nWe develop a host of models based on deep neural\nnetworks, using some of these models as our base-\nline models. As an additional baseline, we com-\npare to Klinger et al. (2018) who propose a model\nbased on Logistic Regression with a bag of word\nunigrams (BOW). All our deep learning models\nare based on variations of recurrent neural net-\nworks (RNNs), which have proved useful for sev-\neral NLP tasks. RNNs are able to capture sequen-\ntial dependencies especially in time series data, of\nwhich language can be seen as an example. One\nweakness of RNNs, however, lies in gradient ei-\nther vanishing or exploding during training. Long-\nshort term memory (LSTM) networks were devel-\noped to target this problem, and hence we employ\nthese in our work. We also use a bidirectional ver-\nsion (BiLSTM) where the vector of representation\nis built as a concatenation of two vectors, one that\nruns from left-to-right and another running from\nright-to-left. Ultimately, we generate a ﬁxed-size\nrepresentation for a given tweet using the last hid-\nden state for the Fwd and Bwd LSTM. Our sys-\ntems can be categorized as follows: (1) Systems\ntuning simple pre-trained embeddings, (2) Sys-\ntems tuning embeddings from language models,\nand (3) Systems directly tuning language models.\nWe treat #1 and #2 as baseline systems, while our\nbest models are based on #3.\n3.2.1 Systems With Simple Embeddings\nCharacter and/or Word embeddings (Mikolov\net al., 2013; Pennington et al., 2014; Bojanowski\net al., 2016) have boosted performance on a host of\nNLP tasks. Most state of the art systems now ﬁne-\ntune these embeddings as a simple transfer learn-\ning technique targeting the ﬁrst layer of a network\n(McCann et al., 2017). We make use of one such\npre-trained embeddings (fastText) to identify the\nutility of tuning its learned weights on the task.\nFastText: The ﬁrst embedding model is fast-\nText 2(Bojanowski et al., 2016), which builds rep-\nresentations based on characters, rather than only\nwords, thus alleviating issues of complex mor-\nphology characetrestic of many languages like\nArabic, Hebrew, and Swedish, but also enhancing\nrepresentations for languages of simpler morphol-\nogy like English. Additionally, fastText partially\nsolves issues with out-of-vocabulary words since\nit exploits character sequences. FastText is trained\non the Common Crawl dataset, consisting of 600B\ntokens.\nFor this and the next set of experiments (i.e., ex-\nperiments in 3.2.2), we train both an LSTM and\nBiLSTM. Since we treat these as baseline sys-\ntems, especially with our goal to report our ex-\nperiments in available space for the competition,\nwe try a small set of hyper-parameters, identifying\nbest settings on our validation set. We train each\nnetwork for 4 epochs each. For optimization, we\nuse Adam(Kingma and Ba, 2014). The model’s\nweights W are initialized from a normal distri-\nbution W ∼ N with a small standard deviation\nof σ = 0.05. We apply two sources of regular-\nization: dropout: we apply a dropout rate of 0.5\non the input embeddings to prevent co-adaptation\nof hidden units’ activation, and L2 −norm: we\nalso apply an L2-norm regularization with a small\nvalue (0.0001) on the hidden units layer to prevent\nthe network from over-ﬁtting on training set. Each\nof the networks has a single hidden layer. Net-\nwork architectures and hyper-parameters are listed\nin Table 1.\n3.2.2 Embedding From LMs\nPeters et al. (2018) build embeddings directly from\n2https://fasttext.cc/docs/en/\nenglish-vectors.html\n344\nHyper-Parameter Value\nEmbed-dim-fastText 300\nEmbed-dim-ELMo 1024\nlayers 1\nunits 300\nbatch size 32\nepochs 4\ndropout 0.5\nTable 1: Network architecture and hyper-parameters\nfor experiments with simple pre-trained embeddings\nwith fastText 3.2.1 and ELMo 3.2.2 across our LSTM\nand BiLSTM networks.\nlanguage models, which they refer to as ELMo.\nELMo is shown to capture both complex char-\nacteristics of words (as syntax and semantics) as\nwell as the usage of these words across various\nlinguistic contexts, thanks to its language model-\ning component. ELMo is trained on a dataset of\nWikipedia and is publicly available 3, which we\nuse as our input layer. More speciﬁcally, we ex-\ntract the weighted sum of the 3 layers (word em-\nbedding, Bi-lstm-outputs1, and Bi-lstm-outputs2)\nand follow the same network architectures and\nhyper-parameters employed with fastText as we\nexplain before.\n3.2.3 Fine-Tuning LMs: ULMFiT\nAnother recent improvement in training NLP sys-\ntems is related to the way these systems are ﬁne-\ntuned, especially vis-a-vis how different layers in\nthe network operate during training time. Howard\nand Ruder (2018) present ULMFiT4, an exam-\nple such systems that is pre-trained on a language\nmodel exploiting Wikitext-103. Ultimately, ULM-\nFiT employs a number of techniques for train-\ning. These include “gradual unfreezing”, which\naims at ﬁne-tuning each layer of the network in-\ndependently and then ﬁne-tuning all layers to-\ngether. Gradual unfreezing proves useful for re-\nducing the risk of overﬁtting as also found in Felbo\net al. (2017). ULMFiT also uses “discriminative\nﬁne-tuning”, which tunes each layer with differ-\nent learning rates, the idea being different lay-\ners capture different types of information (Howard\nand Ruder, 2018; Peters et al., 2018). Howard\nand Ruder (2018) also use different learning rates,\nwhich they refer to as “slanted triangular learning\n3https://github.com/allenai/bilm-tf\n4http://nlp.fast.ai/category/\nclassification.html.\nrates”, at different times of the training process.\nWith ULMFiT, we experiment with different vari-\nations of LMs5: forward (Fwd), backward (Bwd),\nand an average of these (BiLM (Fwd+Bwd)). We\nfollow Howard and Ruder (2018) in training each\nof the Fwd and Bwd models independently on the\ntraining data provided by the task organizers, and\nthen combining their predictions using an ensem-\nble averaging. This is the setting we refer to as\nBiLM. As we show in Section 3, this is a beneﬁ-\ncial measure (similar to what Howard and Ruder\n(2018) also found). For our hyper-parameters for\nthis iteration of experiments, we identify them on\nour validation set. We list the network architec-\ntures and hyper-parameters for this set of experi-\nments in Table 2.\nHyper-Parameter Value\ndim-size 400\nvocab 23,656\nbatches 64\nlayers 3\nunits 1,150\nepochs 19\nTable 2: Hyper-parameters for our submitted system\nexploiting ﬁne-tuned language models from Howard\nand Ruder (2018).\n4 Results\nTable 3 shows results of all our models inF-score.\nAs the Table shows, all our models achieve siz-\nable gains over the logistic regression model in-\ntroduced by (Klinger et al., 2018) as a baseline for\nthe competition ( F-score = 60%). Even though\nour models trained based on fastText and ELMo\neach has a single hidden layer, which is not that\ndeep, these at least 1.5% higher than the logistic\nregression model. We also observe that ELMo em-\nbeddings, which are acquired from language mod-\nels rather than optimized from sequences of to-\nkens, achieves higher performance than FastText\nembeddings. This is not surprising, and aligns\nwith the results reported by Peters et al. (2018).\nFor results with ULMFiT, as Table 3 shows, it\nacquires gains over all the other models. As men-\ntioned earlier, we experiment with different vari-\nations of LMs (Fwd, Bwd, and BiLM). Results in\nour submitted system are based on the Fwd model,\nand are at 69.4%. After system submission, we\n5Fwd and Bwd LMs are offered by the authors of the\nULMFiT model (Howard and Ruder, 2018).\n345\nalso experimented with the Bwd and BiLM mod-\nels and were able to acquire even higher gains,\nputting our best performance at 70.7% (which\nmoves us to the top 3 position).\nSystem Dev Test\nBaseline (Klinger et al., 2018)\nBOW Log-Reg 0.601 0 .601\nEmbeddings\nFastText(Bojanowski et al., 2016)\nLSTM 0.629 0 .629\nBi-LSTM 0.628 0 .626\nEmbed. from LM (ELMo)(Peters et al., 2018)\nLSTM 0.635 0 .635\nBi-LSTM 0.615 0 .614\nFine-Tuned LM(Howard and Ruder, 2018)\nFwd LM (submitted system) 0.694 0 .693\nBwd LM 0.686 0 .693\nBiLM 0.707 0.707\nTable 3: Results: BiLM refers to an ensemble of both\nthe Fwd and Bwd LMs.\n5 Analysis\n5.1 Error Analysis\nFigure 1: Confusion matrix of errors in F-score across\nthe different emotion classes.\nUsing predictions from our best model (as de-\nscribed in Table 2), we investigate the extent with\nwhich each emotion is mislabeled and the cate-\ngories with which it is confused. Figure 1 shows\nthe confusion matrix of this analysis. As the Fig-\nure shows, anger is predicted with least F-score\n(% = 63), followed by sadness (% = 66). Figure\n1 also shows that anger is most confused for sur-\nprise and sadness is most confused for anger. Ad-\nditionally, disgust is the third most confused cat-\negory ( % = 66 ), and is mislabeled as surprise\n9% of the time. These results suggest overlap in\nthe ways each of the emotions is expressed in the\ntraining data.\nTo further investigate these observations, we\nmeasure the shared vocabulary between the differ-\nent classes. Figure 2 shows percentages of lex-\nical overlap in the data, and does conﬁrm that\nsome categories share unigram tokens to varying\ndegrees. Lexical overlap between classes seem to\nalign with the error matrix in Figure 1. For exam-\nple, anger overlaps most with surprise (% = 9)\nand sadness overlaps most with anger (% = 10).\nThese ﬁndings are not surprising, since our mod-\nels are based on lexical input and do not involve\nother types of information (e.g., POS tags). Table\n4 offers examples of overlap in the form of lexi-\ncal sequences between test data and training data\nacross a number of classes.\nFigure 2: Heat Map for percentages of shared vocabu-\nlary between emotion classes.\n5.2 Size of Training Data\nWe also investigate the impact of training data size\non model accuracy. For this purpose, we train\nmodels with different data sizes with the best pa-\nrameter settings shown in Table 2 6. Figure 3\nshows the impact of different percentages of train-\ning examples on model performance. We test\n6Due to the high computational cost of training these\nmodels, we only train each model with one epoch for this\nanalysis.\n346\nTest Example True Predicted Train Example True\nI’m [#TRIGGERWORD#]\nthat\nlike none of\nmy friends at school\nhave seen national\nlamoon’s day\ndisgust sad\n[#TRIGGERWORD#]\nthat like\nnone of\nmy videos\nfrom last night ..\nsad\nhey luke! I’m so\n[#TRIGGERWORD#]\nbecause you\ndon’t follow me,\nnot lies,\nbut please follow me\nanger sad\ni’m so\n[#TRIGGERWORD#]\nbecause\nyou don’t\nfollow me\nsad\nTable 4: Examples overlapping lexical sequences in test and training data.\nmodel performance for this analysis on our vali-\ndation data.\nInterestingly, as Figure 3 shows, the model ex-\nceeds the baseline model reported by the task or-\nganizers (Klinger et al., 2018) when trained on\nonly 10% of the training data. Additionally, the\nmodel outperforms the fastText and ELMo mod-\nels by only seeing 40% of the training data. Once\nthe model has access to 80% of the training data,\nits gains start to increase relatively slowly. In ad-\ndition to the positive, yet unsurprising, impact that\ntraining data size has on performance, the results\nalso reﬂect the utility of employing the pre-trained\nlanguage model.\nFigure 3: Impact of training data size on model per-\nformance, tested on our validation data. Results in F-\nscore.\nIn order to further inspect this observation re-\ngarding the impact of language modeling, we use\nthe same architecture reported in Table 2 to train\na classiﬁer that does not have access to the pre-\ntrained LM. We ﬁnd the classiﬁer to achieve only\n63.8 F-score. Again, this demonstrates the advan-\ntage of using the pra-trained LM.\n6 Negative Experiments\nWe performed a number of negative experiments\nthat we report brieﬂy here. Our intuition is\nthat training our models with Twitter-speciﬁc data\nshould help classiﬁcation. For this reason, we\ntrained ULMFiT with 4.5 million tweets with the\nsame settings reported in Table 2. We did not ﬁnd\nthis set of experiments to yield gains over the re-\nsults reported in Table 3, however. For example,\nan Fwd LM trained on Twitter domain data yields\n67.9% F-score, which is 1.4% less than the F-\nscore obtained by the Wikipedia-trained Fwd LM\nin 3. The loss might be due to the smaller size of\nthe Twitter data we train on, as compared to the\nWikipedia data the ULMFiT is originally trained\non (i.e., >103 million tokens).\n7 Conclusion\nIn this paper, we described our system submitted\nto IEST-2018 task, focused on learning implicit\nemotion from Twitter data. We explored the util-\nity of tuning different word- and character-level\npre-trained representations and language model-\ning methods to minimize training loss. We found\nthat the method introduced by Howard and Ruder\n(2018) yields best performance on the task. We\nnote that our baselines employing sub-word em-\nbeddings (fastText) and embeddings from lan-\nguage models (ELMo) can be improved by us-\ning deeper neural architectures with larger model\ncapacity, which we cast for future work. We\nhave also shown that the classiﬁer confuses certain\nemotion classes with one another, possible due to\noverlap of lexical sequences between training and\ntest data. This reﬂects the difﬁculty of the task.\n347\n8 Acknowledgement\nWe acknowledge the support of the Natural\nSciences and Engineering Research Council of\nCanada (NSERC). The research was enabled in\npart by support provided by WestGrid ( https:\n//www.westgrid.ca/) and Compute Canada\n(www.computecanada.ca).\nReferences\nMuhammad Abdul-Mageed and Lyle Ungar. 2017.\nEmonet: Fine-grained emotion detection with gated\nrecurrent neural networks. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 718–728.\nHassan Alhuzali, Muhammad Abdul-Mageed, and\nLyle Ungar. 2018. Enabling deep learning of emo-\ntion with ﬁrst-person seed expressions. In Pro-\nceedings of the Second Workshop on Computational\nModeling of Peoples Opinions, Personality, and\nEmotions in Social Media, pages 25–35.\nSaima Aman and Stan Szpakowicz. 2007. Identifying\nexpressions of emotion in text. In Text, Speech and\nDialogue, pages 196–205. Springer.\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016. Enriching word vec-\ntors with subword information. arXiv preprint\narXiv:1607.04606.\nVenkatesh Duppada, Royal Jain, and Sushant Hiray.\n2018. Seernet at semeval-2018 task 1: Domain\nadaptation for affect in tweets. arXiv preprint\narXiv:1804.06137.\nP. Ekman. 1972. Universal and cultural differences in\nfacial expression of emotion. Nebraska Symposium\non Motivation, pages 207–283.\nBjarke Felbo, Alan Mislove, Anders Søgaard, Iyad\nRahwan, and Sune Lehmann. 2017. Using millions\nof emoji occurrences to learn any-domain represen-\ntations for detecting sentiment, emotion and sar-\ncasm. In Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nJeremy Howard and Sebastian Ruder. 2018. Fine-\ntuned language models for text classiﬁcation. arXiv\npreprint arXiv:1801.06146.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nRoman Klinger, Orph ´ee de Clercq, Saif M. Moham-\nmad, and Alexandra Balahur. 2018. Iest: Wassa-\n2018 implicit emotions shared task. In Proceedings\nof the 9th Workshop on Computational Approaches\nto Subjectivity, Sentiment and Social Media Anal-\nysis, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6297–6308.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP: Vol-\nume 2-Volume 2, pages 1003–1011. Association for\nComputational Linguistics.\nSaif M Mohammad and Svetlana Kiritchenko. 2015.\nUsing hashtags to capture ﬁne emotion cate-\ngories from tweets. Computational Intelligence,\n31(2):301–326.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, volume 14, pages 1532–\n1543.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nCarlo Strapparava and Rada Mihalcea. 2007. Semeval-\n2007 task 14: Affective text. In Proceedings of\nthe 4th International Workshop on Semantic Evalu-\nations, pages 70–74. Association for Computational\nLinguistics.\nSvitlana V olkova and Yoram Bachrach. 2016. Inferring\nperceived demographics from user emotional tone\nand user-environment emotional contrast. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL.\nWenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,\nand Amit P Sheth. 2012. Harnessing twitter”\nbig data” for automatic emotion identiﬁcation. In\nPrivacy, Security, Risk and Trust (PASSAT), 2012\nInternational Conference on and 2012 Interna-\ntional Confernece on Social Computing (Social-\nCom), pages 587–592. IEEE.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7734472751617432
    },
    {
      "name": "Task (project management)",
      "score": 0.7250714302062988
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6857150197029114
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6575551629066467
    },
    {
      "name": "Natural language processing",
      "score": 0.6444264054298401
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6417559385299683
    },
    {
      "name": "Language model",
      "score": 0.6389857530593872
    },
    {
      "name": "Training set",
      "score": 0.4450167119503021
    },
    {
      "name": "Ensemble forecasting",
      "score": 0.4232291877269745
    },
    {
      "name": "Machine learning",
      "score": 0.3514167070388794
    },
    {
      "name": "Speech recognition",
      "score": 0.3205597400665283
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ],
  "cited_by": 6
}