{
  "title": "Reasoning with Language Model is Planning with World Model",
  "url": "https://openalex.org/W4389520747",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2336628527",
      "name": "Shibo Hao",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2081609342",
      "name": "Yi Gu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2928892178",
      "name": "Haodi Ma",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2190789014",
      "name": "Joshua Hong",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2096525380",
      "name": "Zhen Wang",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2145611632",
      "name": "Daisy Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156033562",
      "name": "Zhiting Hu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1625390266",
    "https://openalex.org/W4361192878",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4385570599",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W1537435730",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W2995298643",
    "https://openalex.org/W4318903783",
    "https://openalex.org/W1714211023",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2799002257",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W2912694244",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W2061146398",
    "https://openalex.org/W2795843265",
    "https://openalex.org/W4287779179",
    "https://openalex.org/W1607969422",
    "https://openalex.org/W1588241099",
    "https://openalex.org/W3118210634",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W3122690883",
    "https://openalex.org/W2032152873",
    "https://openalex.org/W568157109",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2059569317",
    "https://openalex.org/W4389665359",
    "https://openalex.org/W4224220194",
    "https://openalex.org/W4283721947",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W4377371656",
    "https://openalex.org/W2890208753",
    "https://openalex.org/W4307786843",
    "https://openalex.org/W3161700695",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385572270",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2084092936",
    "https://openalex.org/W2772709170",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W4320165837",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4377130745",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2000214310",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4281557260"
  ],
  "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, particularly with Chain-of-Thought-style prompts. However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning. This is due to LLMs‚Äô absence of an internal world model for predicting world states (e.g., environment status, variable values) and simulating long-term action outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monte Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, properly balancing exploration v.s. exploitation to achieve a high-reward reasoning path efficiently. We apply RAP to a variety of challenging reasoning problems, such as plan generation, math reasoning, and logical inference. Empirical results demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency, e.g., RAP on LLaMA-33B surpasses CoT on GPT-4 with 33% relative improvement in plan generation.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8154‚Äì8173\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nReasoning with Language Model is Planning with World Model\nShibo Hao‚àó‚ô£ Yi Gu‚àó ‚àó‚ô£Haodi Ma‚ô¢Joshua Jiahua Hong‚ô£\nZhen Wang‚ô£‚ô† Daisy Zhe Wang‚ô¢Zhiting Hu‚ô£\n‚ô£UC San Diego, ‚ô¢University of Florida\n‚ô†Mohamed bin Zayed University of Artificial Intelligence\n{s5hao, yig025, jjhong, zhw085, zhh019}@ucsd.edu\n{ma.haodi, daisyw}@ufl.edu\nAbstract\nLarge language models (LLMs) have shown\nremarkable reasoning capabilities, particularly\nwith chain-of-thought (CoT) prompting. How-\never, LLMs sometimes still struggle with prob-\nlems that are easy for humans, such as gener-\nating action plans to achieve given goals in an\nenvironment, or performing complex math or\nlogical reasoning. The deficiency stems from\nthe key fact that LLMs lack an internal world\nmodel to predict the world state (e.g., envi-\nronment status, intermediate variable values)\nand simulate long-term outcomes of actions.\nThis prevents LLMs from performing deliber-\nate planning akin to human brains, which in-\nvolves exploring alternative reasoning paths,\nanticipating future states and rewards, and it-\neratively refining existing reasoning steps. To\novercome the limitations, we propose a new\nLLM reasoning framework, Reasoning vi a\nPlanning (RAP). RAP repurposes the LLM\nas both a world model and a reasoning agent,\nand incorporates a principled planning algo-\nrithm based on Monte Carlo Tree Search for\nstrategic exploration in the vast reasoning space.\nDuring reasoning, the LLM (as agent) incre-\nmentally builds a reasoning tree under the guid-\nance of the LLM (as world model) and rewards,\nand efficiently obtains a high-reward reason-\ning path with a proper balance between explo-\nration vs. exploitation. We apply RAP to vari-\nous challenging reasoning problems including\nplan generation, math reasoning, and logical\ninference, and demonstrate its superiority over\nstrong baselines. RAP with LLaMA-33B even\nsurpasses CoT with GPT-4, achieving 33% rela-\ntive improvement in a plan generation setting.1\n1 Introduction\nLarge language models (LLMs) have exhibited\nemergent reasoning abilities in a wide range of\ntasks (Brown et al., 2020; Chowdhery et al., 2022;\n‚àóequal contribution\n1The code is available at https://github.com/Ber666/\nllm-reasoners\nOpenAI, 2023). Recent approaches further boost\ntheir ability by prompting LLMs to generate inter-\nmediate reasoning steps, e.g., Chain-of-Thought,\nCoT (Wei et al., 2022) or answer a series of sub-\nquestions, e.g., least-to-most prompting (Zhou\net al., 2022). However, LLMs still face difficul-\nties with tasks that humans find easy. For example,\nin creating action plans to move blocks to a tar-\nget state, GPT-3 (Brown et al., 2020) achieves a\nsuccess rate of only 1%, compared to 78% for hu-\nmans (Valmeekam et al., 2022); these models also\nstruggle with complex tasks that require multiple\nsteps of math, logical, or commonsense reason-\ning (Huang and Chang, 2022; Mialon et al., 2023).\nHumans possess an internal world model , a\nmental representation of the environment (Johnson-\nLaird, 1983, 2010; Gentner and Stevens, 2014),\nwhich enables humans to simulate actions and their\neffects on the world‚Äôs state for deliberate planning\nfor complex tasks of motor control, imagery, infer-\nence, and decision making (Tolman, 1948; Briscoe,\n2011; Schulkin, 2012; LeCun, 2022). For example,\nto make an action plan towards a goal, planning\nwith the world model involves exploring various\nalternative courses of actions, assessing the likely\noutcomes by rolling out possible future scenarios,\nand iteratively refining the plan based on the assess-\nment (Huys et al., 2012; Gasparski and Orel, 2014;\nHo et al., 2021). This is in stark contrast to the\ncurrent LLM reasoning, which instinctively gener-\nates a reasoning trace in an autoregressive manner.\nIn particular, we identify several key limitations\nof the current reasoning with LLMs, including (1)\nthe lack of an internal world model to simulate the\nstate of the world (e.g., the configuration of blocks,\nthe values of intermediate variables), which is the\nfoundation of human planning; (2) the absence of\na reward mechanism to assess and guide the rea-\nsoning towards the desired state; and due to both\nlimitations, (3) the incapability of balancing explo-\nration vs. exploitation to efficiently explore vast\n8154\nWorld \nModel\nLanguage Model\nReasoning via Planning (RAP)\nChain-of-Thought\nLanguage Model\nFigure 1: An overview of Reasoning via Planning (RAP). Compared with previous LLM reasoning methods like\nChain-of-Thought (Wei et al., 2022), we explicitly model the world state from a world model (repurposed from the\nlanguage model), and leverage advanced planning algorithms to solve the reasoning problems.\nreasoning space.\nTo address these limitations, this paper proposes\na new framework,Reasoning via Planning (RAP),\nthat enables LLMs to reason in a manner close\nto humans‚Äô conscious planning. RAP augments\nthe LLM with a world model, and reasons with\nprincipled planning (specifically Monte Carlo Tree\nSearch, MCTS) to produce high-reward reasoning\ntraces after efficient exploration (Figure 1). No-\ntably, we acquire the world model by repurpos-\ning the LLM itself with appropriate prompts. Dur-\ning the reasoning, the LLM strategically builds a\nreasoning tree by iteratively considering the most\npromising reasoning steps (actions) and using the\nworld model (the same, repurposed LLM) to look\nahead for future outcomes. The estimated future re-\nwards are then backpropagated to update the LLM‚Äôs\nbeliefs about the current reasoning steps, guiding\nit to refine the reasoning by exploring better al-\nternatives. Our MCTS-based planning effectively\nmaintains a proper balance between exploration (of\nunvisited reasoning traces) and exploitation (of the\nbest reasoning steps identified so far).\nWe show RAP is a general framework applica-\nble to a diverse range of challenging problems and\nachieves substantial improvements over recent pop-\nular LLM reasoning methods. For plan generation,\nparticularly in 2/4/6-step problems of Blocksworld\n(Valmeekam et al., 2023), RAP achieves an aver-\nage success rate of 64% while CoT fails almost\ncompletely. Moreover, LLaMA-33B with RAP\nsurpasses GPT-4 with CoT by 33% relative im-\nprovement. In the domains of mathematical reason-\ning, such as GSM8K (Cobbe et al., 2021) and logi-\ncal inference exemplified by PrOntoQA (Saparov\nand He, 2022), RAP also consistently improves\nover strong baselines, including CoT, least-to-most\nprompting, and their self-consistency variants.\n2 Related Work\nReasoning with LLMs. LLM reasoning typically\ninvolves decomposing complex questions into se-\nquential intermediate steps (a.k.a. chains) before\nproducing the final answer, exemplified by Chain-\nof-Thought (CoT) prompting and its variants (Wei\net al., 2022; Kojima et al., 2022). The basic CoT\ngenerates chains all at once and can induce ad-\nditional errors as the step count increases. Self-\nConsistency (Wang et al., 2022) samples multi-\nple chains to choose the best answer via major-\nity voting. Least-to-most prompting (Zhou et al.,\n2022) reduces the question into simpler subques-\ntions and answers them sequentially. Similar to our\nreward formulation, recent works have explored\nself-evaluation approaches to provide feedback for\nintermediate steps (Welleck et al., 2022; Shinn\net al., 2023; Paul et al., 2023). Aligned with our\nstate formulation, Li et al. (2022) incorporate la-\ntent ‚Äúsituations‚Äù into LLMs, referring to the state\nof entities from the context. More relevantly, re-\ncent works have started to explore more complex\nstructures guided by some search algorithms. For\ninstance, CoRe (Zhu et al., 2022) fine-tunes rea-\nsoning step generator and verifier for math word\nproblems with MCTS for decoding. Concurrently\nto our work, Yao et al. (2023) apply heuristic-based\n8155\nPickup orange\n(r = 0.6) (r = 0.4)\nPickup blue\nStack it on blueStack it on orange\n‚Ä¶\n‚Ä¶\n{ }\nInitial State: The orange block is on the table, the \nblue block is on the table, and the red block‚Ä¶\nGoal: The orange block is on the blue block, and \nthe yellow block is on the orange block.\nJulie is reading‚Ä¶ She wants to read half of the remaining \npages tomorrow. How many pages should she read?\n(r = 0.7)\nQ1: How many pages \nhas she read?\n(r = 0.5)\nQ1: How many pages did \nJulie read today?\nQ1: How ‚Ä¶ read?\nA1: 30\nQ1: How ‚Ä¶Today? \nA1: 24\nQ2: How many pages has Julie \nread till now?\n(r = 0.8)\n(r = 0.3)\nQ2: How many pages \nshould she read \ntomorrow?\nQ1: ‚Ä¶\nQ2: How ‚Ä¶now?\nA2: 36\nQ1:  How‚Ä¶ today?\nA1: 24\n‚Ä¶\nQT: How ... tomorrow?\nAT: 42\n(r = 0.3) (r = 0.9)\n‚Ä¶\n(Answer: 42)\n(1) Carnivores are carnivorous \n(2) Animals are not unicellular\n(3) Carnivores are mammals ‚Ä¶ \nFact: Fae is a feline  \nHypothesis: Fae is unicellular?\n(1) Carnivores are \ncarnivorous\n(4) Every cat is a feline\n(r = 0.8) (r = 0.1)\n(3) Carnivores are \nmammals\n(r = 0.8)(r = 0.8)\n‚Ä¶ Fae is a mammal\nFae is a not unicellular\n(The hypothesis is false)\nFae is a feline\nFae is a cat\n(5) Each feline is a \ncarnivores\nFae is a carnivore\n(Goal achieved)\nFigure 2: RAP for plan generation in Blocksworld (left), math reasoning in GSM8K (middle), and logical reasoning\nin PrOntoQA (right).\nsearch, like depth-/breadth-first search, for better\nreasoning paths. However, none of the above meth-\nods formally introduce the world model and instan-\ntiates the reward and state into a unified framework.\nCompared with these search-guided methods, RAP\nis a more principled framework to combine world\nmodel and reward with advanced planning.\nPlanning with LLMs. Planning, a central ability\nin intelligent agents, involves generating a series\nof actions to achieve a specific goal (McCarthy,\n1963; Bylander, 1994). Classical planning methods\nhave been widely adopted in robots and embod-\nied environments (Camacho and Alba, 2013; Jiang\net al., 2019). Recently, prompting LLMs to do\nplanning directly has gained attention and shown\npotential (Huang et al., 2022; Singh et al., 2022;\nDing et al., 2023). Moreover, based on LLMs‚Äô pow-\nerful programming ability (Lyu et al., 2023; Jojic\net al., 2023; Liu et al., 2023), recent works first\ntranslate natural language instructions into the exe-\ncutable programming languages, such as Planning\nDomain Description Language (PDDL), and runs\nclassical planning algorithms, such as LLM+P (Liu\net al., 2023). However, code-based planning is\nconstrained by its narrow domains and the environ-\nment, while RAP can handle open-domain prob-\nlems, such as math and logical reasoning. More\nrelated works on world models and planning are\ndiscussed in the Appendix D.\n3 Reasoning via Planning (RAP)\nIn this section, we present the Reasoning via Plan-\nning (RAP) framework that enables LLMs to strate-\ngically plan a coherent reasoning trace for solving\na wide range of reasoning tasks. We first build\nthe world model by repurposing the LLM with\nprompting (Section 3.1). The world model serves\nas the foundation for deliberate planning, by al-\nlowing the LLM to plan ahead and seek out the\nexpected outcomes in the future. We then intro-\nduce the rewards for assessing each state during\nreasoning in Section 3.2. Guided by the world\nmodel and rewards, the planning with Monte Carlo\nTree Search (MCTS) efficiently explores the vast\nreasoning space and finds optimal reasoning traces\n(Section 3.3). Finally, when multiple promising\nreasoning traces are acquired during planning, we\nfurther introduce an aggregation method in Sec-\ntion 3.4 that yields an ensembled result and further\nboosts the reasoning performance.\n3.1 Language Model as World Model\nIn general, a world model predicts the next state of\nthe reasoning after applying anaction to the current\nstate (Ha and Schmidhuber, 2018b; Matsuo et al.,\n2022). RAP enables us to instantiate the general\nconcepts of state and action in different ways de-\npending on the specific reasoning problems at hand\n(Figure 2). For example, in Blocksworld, it is natu-\nral to define a state as the configuration of blocks\n(described in natural language), and an action to be\na behavior of moving a block (e.g., ‚Äúpickup the\norange block‚Äù). In a math reasoning problem, we\nuse the state to represent the values of intermedi-\nate variables, and set an action to be a subquestion\nthat drives the reasoning to derive new values. In\nlogical reasoning, a state is a fact we are focusing\non, and an action is to choose a rule for the next\ndeduction.\nWith the definition of state and action, the rea-\nsoning process can thus be described as a Markov\ndecision process (MDP): given the current state\n8156\nst,t=0,1,...,T, e.g., the initial state s0, the LLM (as\na reasoning agent) generates an action space by\nsampling from its generative distribution at ‚àº\np(a|st, c), where c is a proper prompt (e.g., in-\ncontext demonstrations). Once an action is cho-\nsen, the world model then predicts the next state\nst+1 of the reasoning. Specifically, we repurpose\nthe same LLM to obtain a state transition distribu-\ntion p(st+1|st, at, c‚Ä≤), where c‚Ä≤is another prompt\nto guide the LLM to generate a state. For instance,\nin Blocksworld, the LLM (as the world model) gen-\nerates text st+1 to describe the new configuration\nof blocks, given previous state st and the action at.\nContinuing the process results in a reasoning\ntrace, which consists of a sequence of interleaved\nstates and actions (s0, a0, s1, . . . , aT‚àí1, sT). This\ndiffers from the previous reasoning methods, such\nas Chain-of-Thought (Wei et al., 2022), where\nthe intermediate reasoning steps consist of only\na sequence of actions, e.g., ( a0 = ‚Äúpickup red\nblock‚Äù, a1 = ‚Äústack on yellow block‚Äù,. . . )\n(see comparisons in Figure 1). Augmenting the\nreasoning with the (predicted) world states helps\nthe LLM with a more grounded and coherent infer-\nence. Note that the full reasoning trace is simulated\nby the LLM itself (as a reasoning agent with an\ninternal world model) without interacting with the\nexternal real environment. This resembles humans\ncontemplating a possible plan in their minds. The\ncapability of simulating future states, by introduc-\ning the world model, allows us to incorporate prin-\ncipled planning algorithms to efficiently explore the\nvast reasoning space as described in Section 3.3.\n3.2 Reward Design\nDuring reasoning, we want to assess the feasibil-\nity and desirability of each reasoning step, and\nguide the reasoning based on the assessment (Sec-\ntion 3.3). The assessment of each reasoning step\n(i.e., applying an action at to the state st) is per-\nformed by a reward function rt = r(st, at) ‚ààR.\nSimilar to the state and action, the reward function\ncan be specified in different ways to accommodate\nany knowledge or preferences about the reason-\ning problem of interest. Here we introduce several\ncommon rewards applicable to different tasks and\nshown to be effective in our experiments.\nLikelihood of the action. When an action is gen-\nerated by the LLM conditioning on the in-context\ndemonstration and the current state, the probability\nof the specific action reflects the LLM‚Äôs preference.\nWe thus can incorporate the log probability of the\naction as a reward. This reward reflects the ‚Äúin-\nstinct‚Äù of LLMs as an agent, and can be also used\nas a prior for which action to explore.\nConfidence of the state. State prediction is non-\ntrivial in some problems, e.g., in math reasoning\n(Figure 2, middle), given an action (i.e., a subques-\ntion), the world model predicts the next state by\nanswering the subquestion. We incorporate the con-\nfidence of the state (i.e., answers in this case) as\na reward. Specifically, we draw multiple sample\nanswers from the world model, and use the propor-\ntion of the most frequent answer as the confidence.\nHigher confidence indicates that the state predic-\ntion is more consistent with the world knowledge\nof LLMs (Hao et al., 2023b), which typically leads\nto a more reliable reasoning step.\nSelf-evaluation by the LLM. It‚Äôs sometimes eas-\nier to recognize the errors in reasoning than avoid\ngenerating them in advance. Thus, it‚Äôs beneficial\nto allow the LLM to criticize itself with the ques-\ntion ‚ÄúIs this reasoning step correct?‚Äù, and\nuse the next-word probability of the token ‚Äú Yes‚Äù\nas a reward. The reward evaluates LLM‚Äôs own\nestimation of the correctness of reasoning. Note\nthat the specific problems for self-evaluation can\nbe different depending on the tasks.\nTask-specific heuristics. RAP also allows us to\nflexibly plug in other task-specific heuristics into\nthe reward function. For example, in plan gener-\nation for Blocksworld, we compare the predicted\ncurrent state of blocks with the goal to calculate a\nreward (Section 4.1). The reward encourages the\nplan of movements to actively pace towards the\ntarget.\n3.3 Planning with Monte Carlo Tree Search\nOnce equipped with the world model (Section 3.1)\nand rewards (Section 3.2), LLMs can reason with\nany planning algorithms. We adopt Monte Carlo\nTree Search (MCTS) (Kocsis and Szepesv√°ri, 2006;\nCoulom, 2007), a powerful planning algorithm\nthat strategically explores the space of reasoning\ntrees and strikes a proper balance between explo-\nration and exploitation to find high-reward reason-\ning traces efficiently.\nSpecifically, MCTS builds a reasoning tree it-\neratively, where each node represents a state, and\neach edge represents an action and the transition\nfrom the current state to the next state after apply-\ning the action (Figure 1). To guide the LLM agent\n8157\nPickuporangePickupblue\nStackiton blueStackiton orange‚Ä¶\nPickuporangePickupblue\nStackiton blueStackiton orange‚Ä¶PickuporangePickupred\nPickuporangePickupblue\nStackiton blueStackiton orange‚Ä¶PickuporangePickupred\nQ\n‚Ä¶\nQ\nQ\nQ\nQ\nQ\nreward\n(a) Selection(b) Expansion(c) Simulation(d) Back-propagation\nQ\nùë†!ùëé!ùë†\"ùëé\"ùë†#ùëé#ùë†$\nùë†% ‚Ä¶\nFigure 3: An illustration of the four phases in an iteration in MCTS planning (Section 3.3).\nto expand and explore the most promising nodes\nof the tree, the algorithm maintains a state-action\nvalue function Q : S√óA‚Ü¶‚ÜíR, where Q(s, a) es-\ntimates the expected future reward of taking action\na in state s. Figure 3 illustrates four operations\nin each iteration to expand the tree and update Q\nvalues. The process continues until a specified com-\nputational budget (e.g., the number of iterations) is\nreached, and the resulting traces are acquired from\nthe tree. More details and the pseudo-code of the\nplanning algorithm are given in Appendix A and\nAlgorithm 1.\nSelection. The first phase selects a portion of the\nexisting tree that is most promising for further ex-\npansion in the next phase. Starting from the root\nnode (i.e., initial state s0), at each level of the tree,\nthe algorithm selects a child node as the next node.\nThe phase finishes when a leaf node of the cur-\nrent tree is reached. Figure 3(a) highlights the\nselected path in red. To balance between explo-\nration (of less-visited nodes) and exploitation (of\nhigh-value nodes), we use the well-known Upper\nConfidence bounds applied to Trees (UCT) (Kocsis\nand Szepesv√°ri, 2006) to select each child node.\nSpecifically, at node s, we select the action in the\ntree by considering both the Q value (for exploita-\ntion) and uncertainty (for exploration):\na‚àó= arg max\na‚ààA(s)\n[\nQ(s, a) +w\n‚àö\nln N(s)\nN(c(s, a))\n]\n, (1)\nwhere N(s) is the number of times node s has\nbeen visited in previous iterations, andc(s, a) is the\nchild node of applying a in state s. The less a child\nnode was visited before (i.e., the more uncertain\nabout this child node), the higher the second term\nin the equation. The weight w controls the balance\nbetween exploration and exploitation.\nExpansion. This phase expands the tree by adding\nnew child nodes to the leaf node selected above.\nGiven the state of the leaf node, we use the LLM (as\nagent) to sample d possible actions (e.g., subques-\ntions in math reasoning), and then use the LLM (as\nworld model) to predict the respective next state,\nresulting in d child nodes. Note that if the leaf\nnode selected above is a terminal node (the end of\na reasoning chain) already, we will skip expansion\nand jump to back-propagation.\nSimulation. To estimate the expected future re-\nwards (Q values), this phase simulates the future sit-\nuations of the current node using the world model.\nStarting from the current node as above, at each\nnode s, we create an action following a roll-out\npolicy and use the world model to predict the next\nstate. The roll-out process continues until a termi-\nnal state is reached. There could be many ways to\ndefine the roll-out policy (e.g., by adding different\nrandomness). In our experiments, for simplicity\nand reduced noises, we follow a similar process as\nin the expansion above, i.e., generating d candidate\nactions and picking one of the largest local reward\na‚Ä≤= maxa‚Ä≤r(s, a). In practice, for efficiency, we\ndiscard the computationally costly components in\nr (e.g., the reward from the confidence of state re-\nquires sampling the answer multiple times), and\nuse the resulting lightweight reward function for\nselecting actions during simulation.\nBack-propagation. Once we reach a terminal state\nin the above phases, we obtain a reasoning path\nfrom the root node to the terminal node. We now\nback-propagate the rewards on the path to update\n8158\nthe Q value of each state-action pair along the path.\nSpecifically, we update Q(s, a) by aggregating the\nrewards in all future steps of node s.\nOnce a predetermined number of MCTS itera-\ntions is reached, we terminate the algorithm and\nselect the final reasoning trace from the constructed\ntree for evaluation. There are various ways for the\nselection. One is to start from the root node and it-\neratively choose the action with the highestQ value\nuntil reaching a terminal. Also, one can directly\nselect the path from the iterations that yielded the\nhighest reward, or opt to choose the leaf node (and\nthe respective root-to-leaf path) that has been vis-\nited the most. In practice, we observed that the\nsecond strategy often yields the best results.\n3.4 RAP-Aggregation\nFor problems, such as math reasoning (Section 4.2)\nwhere only the final answer is required, RAP could\nproduce multiple traces and answers from differ-\nent MCTS iterations, which will be aggregated to\nproduce the final answer. We refer to such a mech-\nanism as RAP-Aggregation. Note that problems\nlike plan generation or logical inference require\na complete reasoning trace as output; thus, RAP-\nAggregation will not be applied.\n4 Experiments\nIn this section, we demonstrate the flexibility and\neffectiveness of our RAP framework by applying\nit to a wide range of problems, including plan gen-\neration in an embodied environment (4.1), mathe-\nmatical reasoning for solving math word problems\n(4.2), and logical reasoning for verifying hypothe-\nses (4.3). The subsequent sections demonstrate\nhow the world model formulation in RAP enables\na versatile design of the state and action, catering\nto various reasoning contexts.\nWe primarily compare RAP with chain-of-\nthought (CoT) (Wei et al., 2022), and its variants\nlike least-to-most prompting (Zhou et al., 2022) as\nbaselines. We also consider ensembling multiple\nreasoning paths if applicable (also known as self-\nconsistency (Wang et al., 2022)). Moreover, we\ncompare RAP with GPT-4 (OpenAI, 2023) when\ncomputation resources allow. By default, we use\nthe LLaMA-33B model (Touvron et al., 2023a) as\nthe base LLM for both our methods and baselines,\nwith a sampling temperature of 0.8. All prompts\nare listed in Appendix C.\n4.1 Plan Generation\nThe plan generation task aims to produce a se-\nquence of actions to achieve a given goal, possibly\nwith additional constraints. The ability to generate\nplans is important for intelligent embodied agents,\ne.g. household robots (Puig et al., 2018).\nTask setup. To explore the viability of the RAP\nframework for plan generation tasks, we adapt\nand evaluate RAP on the Blocksworld bench-\nmark (Valmeekam et al., 2022), where an agent\nis asked to rearrange the blocks into stacks in a par-\nticular order. We define a state as the current orien-\ntation of the blocks and an action as an instruction\nthat moves blocks. Specifically, an action is com-\nposed of one of the 4 verbs (i.e.,STACK, UNSTACK ,\nPUT, and PICKUP ) and manipulated objects. For\nthe action space, we generate the currently valid\nactions given the domain restrictions on actions\nand the current orientation of the blocks. To tran-\nsit between states, we take the current action and\nquery the LLM to predict the state changes to the\nrelevant blocks. We then update the current state\nby adding the new block conditions and removing\nthe conditions that are no longer true. Once a state\nhas met all conditions in the goal or the depth limit\nof the tree is reached, we terminate the associated\nnode.\nTo assess the quality of actions within this do-\nmain, we use two separate rewards. First, we\nprompt the LLM with some example test cases\nalong with their solutions, and then calculate the\nlog probability of the action given the current state\n(‚ÄúLikelihood of action‚Äù reward in Section 3.2), de-\nnoted as r1. This reward reflects the intuition of\nthe LLM as the reasoning agent. It‚Äôs typically in-\ndicative when there are few steps left to the goal,\nwhile not as reliable for a distant goal. Additionally,\nwe compare the new state after performing an ac-\ntion with the goal and provide a reward, r2, scaling\nwith the number of conditions met (‚ÄúTask-specific\nheuristics‚Äù reward). Specifically, when all the con-\nditions are met, we assign a super large reward to\nmake sure this plan will be selected as the solution.\nResults. We use test cases from the Blocksworld\ndataset (Valmeekam et al., 2023) and group them\nby minimum number of actions required, resulting\nin 30 cases solvable within 2 steps, 57 cases within\n4 steps, and 114 cases within 6 steps. There are\nat most 5 blocks in each test case. As the baseline\nmethod, we prompt the LLM with 4 test cases with\ncorresponding solutions, and ask it to generate a\n8159\nCoT\nunstack the orange \nblock from on top of the \nyellow block\nstack the orange block \non top of the red block\npick up the yellow block\nstack the yellow block \non top of the blue block\n(Goal: The red block is on blue block; the orange block is on the red block)\nRAP\nI have that, the red block is clear, the blue block is clear, the orange block is clear, \nthe hand is empty, the orange block is on top of the yellow block, the red block is \non the table, the blue block is on the table and the yellow block is on the table.\nPick up the red block.\nI have that, the blue block is clear, the orange block is clear, the red block is in the \nhand, the hand is holding the red block, the orange block is on top of the yellow \nblock, the blue block is on the table, and the yellow block is on the table.\nStack the red block on top of the blue block.\nI have that, the orange block is clear, the red block is clear, the hand is empty, the \norange block is on top of the yellow block, the red block is on top of the blue block, \nthe blue block is on the table, and the yellow block is on the table.\nUnstack the orange block from on top of the yellow block.\nI have that, the orange block is in the hand, the red block is clear, the yellow block \nis clear, the hand is holding the orange block, the red block is on top of the blue \nblock, the blue block is on the table, and the yellow block is on the table.\nStack the orange block on top of the red block.\nI have that, the orange block is clear, the yellow block is clear, the hand is empty, \nthe orange block is on top of the red block, the red block is on top of the blue \nblock, the blue block is on the table, and the yellow block is on the table.\nFigure 4: Comparing reasoning traces in Blocksworld from CoT (left) and RAP (right).\nMethod 2-step 4-step 6-step\nCoT 0.17 0.02 0.00\nCoT - pass@10 0.23 0.07 0.00\nCoT (GPT-4) 0.50 0.63 0.40\nRAP(10) 1.00 0.86 0.26\nRAP(20) 1.00 0.88 0.42\nTable 1: Results on Blocksworld. RAP(10) and RAP(20)\nrefer to our method where the iteration number is set to\n10 and 20, respectively. ‚Äúpass@10‚Äù means 10 plans are\nsampled for each test case, and the test case is regarded\nas solved if at least one plan is correct. All other settings\nincluding RAP, only evaluate a single plan.\nplan for a new question. This setting is the same\nas one described in Valmeekam et al. (2022), and\nwe denote it as Chain-of-Thought (CoT) as the\nsolution is generated step by step. For RAP, the\nsame prompt is shown to help LLMs calculate r1.\nAs shown in Table 1, CoT with LLaMA-33B\ncan only generate successful plans for a few 2-step\ncases, and completely fails on harder problems.\nRAP substantially improves over CoT by nearly\nsolving all problems within 4 steps, and a part of\n6-step problems, achieving an average success rate\nof 64%. It‚Äôs worth noting that the searching space\nof 6-step problems can be as large as 56, while our\nalgorithm can find a successful plan 42% of the\ntime within 20 iterations. Even more, our frame-\nwork allows LLaMA-33B to outperform GPT-4 by\n33% relative gain, which is known to have much\nstronger reasoning ability (Bubeck et al., 2023).\nCase study. We compare the reasoning paths from\nCoT and RAP in Figure 4. We summarize the\nreasons accounting for the improvement: (1) By\nmaintaining the world state during reasoning, RAP\ncan recognize valid actions for the current state,\navoiding generating illegal plans. (2) RAP is capa-\nble of backtracking and trying out other solutions\nwhen the first intuition from the LLM doesn‚Äôt work.\nSpecifically, CoT attempts to achieve the second\ngoal, i.e. ‚Äúorange on red‚Äù, and achieve that with\nthe first two steps. However, accomplishing the\nsecond goal first would prevent the first goal from\nbeing satisfied. On the contrary, even though RAP\nmakes the same mistakes in the first iterations, our\nframework drives the agent to explore other possi-\nble paths (as described in Section 3.3) and finally\ngenerate a successful plan. (3) When calculating\nrt, we can only feed the current state to the LLM\nand hide the history. E.g., in the case of Figure 4,\nto calculate the reward for a2, the LLM is provided\nwith a ‚Äúnew‚Äù test case, in which s2 is the initial\nstate. This significantly lowers the difficulties of\nthe last few steps, and saves more iterations for\nharder decisions of the first few steps.\n4.2 Math Reasoning\nTask setup. Math reasoning tasks, such as GSM8k\n(Cobbe et al., 2021), often include a description and\na final question. To arrive at the answer to the final\nquestion, it is necessary to undertake multi-step\nmathematical calculations based on the problem‚Äôs\ncontext. It is thus natural to decompose the final\nquestion into a sequence of smaller sub-questions\n(Figure 2, right). We define a state as the values\nof intermediate variables, and an action as to pro-\npose an incremental sub-question about a unknown\nintermediate variable. The world model then re-\nsponds to the sub-question using the intermediate\nvariables and the problem description, adding the\nnew intermediate variable value into the next state.\nWe combine the self-evaluation of helpfulness by\nLLM rt,1 and the confidence of state rt,2 using\nweighted geometric mean rt = rŒ±\nt,1 ‚àór1‚àíŒ±\nt,2 as the\nreward function. This reward encourages more rel-\nevant and useful sub-questions. To account for the\n8160\nMethod Accuracy (%)\nChain-of-Thought 29.4\n+ SC(10) 46.8\nLeast-to-Most 25.5\n+ SC(10) 42.5\nRAP(1) 40.0\nRAP(10) 48.6\n+ aggr 51.6\nTable 2: Results on GSM8k. The superscripts indicate\nthe number of samples or iterations.\n1 2 3 4 5 6 7 8 9 10\nNumber of samples (iterations)\n25\n30\n35\n40\n45\n50Accuracy\nMethod\nLeast-to-most\nChain-of-thoughts\nRAP\nRAP (aggr)\nFigure 5: Results on GSM-8K, with different numbers\nof sampled paths or iterations.\nimpact of the reasoning path‚Äôs length on the reward,\nwe compute the Q value by using the maximum\nof average rewards in future steps.\nQ‚àó(st, at) = max\nst,at,rt,...,sl,al,rl,sl+1\navg(rt, . . . , rl). (2)\nAs a related work, Least-to-Most prompting\n(Zhou et al., 2022) shares a similar idea to us in\nsub-question decomposition, but they generate sub-\nquestions all at once. On the contrary, RAP con-\nsiders each action at based on the current state st,\nwhich enables more informed decisions.\nResults. We evaluate our framework on GSM8k, a\ndataset of grade school math word problems. We\nalso evaluate the base model with CoT prompting\n(Wei et al., 2022), Least-to-Most prompting (Zhou\net al., 2022), and their self-consistency (Wang et al.,\n2022) variants, as the baselines. We use the same\n4-shot examples demonstrations for all methods.\nAs shown in Table 2, our RAP framework an-\nswers 48.8% of the problems correctly, outperform-\ning both the Chain-of-Thought and the Least-to-\nMost prompting with Self-Consistency. Notably,\nthis result is achieved when RAP only selects only\none reasoning trace based on the reward. The in-\ntroduction of RAP-Aggregate further improves the\naccuracy by ‚àº3%. We also calculate the accuracy\nwith different numbers of iterations in MCTS and\nself-consistency samples in baselines, as illustrated\nMethod Pred Acc Proof Acc\nCoT 87.8 64.8\nCoT + SC 89.8 -\nRAP (Ours) 94.2 78.8\nTable 3: Results on ProntoQA.\nin Figure 5. We find that across all numbers of itera-\ntions/samples, RAP-Aggregation outperforms base-\nlines consistently, which indicates that when only a\nfew iterations/samples are allowed, our framework\nis significantly better at finding reliable reasoning\npaths with the guide of reward.\n4.3 Logical Reasoning\nTask setup. A logical reasoning task (e.g. PrOn-\ntoQA (Saparov and He, 2022)) typically provides\na set of facts and logical rules, and a model is re-\nquired to verify if a hypothesis fact is true or false\nby applying the logical rules to the given facts, as\nillustrated in Figure 2. These tasks not only re-\nquire the correct final answer (true/false), but also\na detailed proof demonstrating the result. To apply\nour framework, we define the state as a fact we\nare focusing on, analogous to the human‚Äôs working\nmemory (Baddeley, 1992) for inference. An action\nis defined as selecting a rule from the fact set. The\nworld model performs a one-hop reasoning step to\nget a new fact as the next state. The reward is cal-\nculated with Self-evaluation (Section 3.2. Specif-\nically, we prompt the LLM with a few examples\nwith their labels to help it better understand the\nquality of reasoning steps. We use the average re-\nward of future steps to update the Q function, the\nsame as Equation (2) for GSM8k.\nResults. We assess the performance of our RAP\nframework on PrOntoQA (Saparov and He, 2022)\nand adopt their settings of ‚Äútrue‚Äù ontology (using\nreal-world knowledge), ‚Äúrandom‚Äù ordering of rules.\nWe mix the examples requiring 3, 4, and 5 rea-\nsoning hops in a correct proof to prevent LLM\nfrom memorizing when to finish the reasoning. We\nsample 500 examples from the generation script\nreleased by Saparov and He (2022). We compare\nboth the prediction accuracy of the final answer and\nthe accuracy of the entire proof. We do 20 iterations\nfor MCTS and 20 samples for self-consistency.\nAs the results presented in Table 3, our frame-\nwork achieves a correct answer rate of 94.2%\nand a proof accuracy of 78.8%, surpassing the\nCoT baseline by 14% proof accuracy and the self-\nconsistency CoT baseline by 4.4% prediction ac-\ncuracy. Such substantial improvements clearly\n8161\nSetting Method 2-step 4-step 6-step 8-step 10-step 12-step All\nEasy CoT 0.49 0.18 0.06 0.01 0.01 0.00 0.08\nRAP(10) 1.00 0.99 0.75 0.61 0.32 0.32 0.65\nHard CoT 0.22 0.14 0.02 0.02 0.00 0.00 0.05\nRAP(10) 0.67 0.76 0.74 0.48 0.17 0.09 0.51\nTable 4: Results on the full Blocksworld with Llama-2 70B.\ndemonstrate the effectiveness of RAP in solving\nlogical reasoning problems in PrOntoQA. Also, as\nthe case illustrated in Figure 2, RAP can effectively\nrecognize when a reasoning chain comes to a dead\nend, and propagate the signal back to earlier reason-\ning steps, with the planning algorithm allowing it to\nexplore alternatives to the previous steps. The self-\nevaluation reward further helps RAP to recognize\npotential incorrect reasoning steps, encouraging the\nagent to avoid them in future iterations.\n5 Analysis\n5.1 Complex problems\nTo further study whether RAP can help stronger\nLLMs to solve more complex problems, we\nconduct experiments on the full Blocksworld\n(Valmeekam et al., 2023) dataset using a more ca-\npable LLM, Llama-2 70B (Touvron et al., 2023b).\nThe full Blocksworld (Valmeekam et al., 2023)\ncomprises 602 test cases. We group them based\non the minimum number of actions required for\neach test case. Our experiments are conducted in\ntwo distinct settings: Easy and Hard. In Easy set-\nting, we assume prior knowledge of the minimum\nnumber of actions for each case. Leveraging this\ninformation, we use demonstration cases that share\nthe same minimum number of actions as the test\ncase. For each group of cases, we randomly select\n10 cases to create a pool of demonstration cases,\nleaving the remaining cases as the test set. Dur-\ning inference, we randomly sample 4-shot demon-\nstration cases from this pool and utilize them to\nformulate prompts. In the Hard setting, we ran-\ndomly select 10 cases from the full dataset to form\na demonstration pool and subsequently exclude\nthese cases from the test set. During inference,\nwe randomly sample 4-shot demonstration cases\nfrom this global pool, irrespective of the minimum\nnumber of actions required for the test case.\nWe employ chain-of-thought prompting (Wei\net al., 2022) as a baseline, and evaluate our RAP(10)\n(with 10 iterations) with an improved prompting\ntechnique (Appendix E). Our experimental results\nare summarized in Table 4. In both the Easy and\nHard settings, RAP demonstrates superior perfor-\nmance over CoT by a substantial margin. Notably,\nwhen the test case necessitates a larger number of\nsteps (six or more) to solve, CoT exhibits a severe\ndrop in success rates, whereas RAP maintains a\nrelatively high success rate. Comparing these re-\nsults with Section 4.1, we additionally conclude\nthat RAP is a general framework able to enhance\nthe reasoning abilities of LLMs, regardless of their\nintrinsic capabilities.\n5.2 Reward Choice\nIn our main experiments, we choose the combina-\ntion of rewards in our current experiments based on\nheuristics and our exploratory experiments. To un-\nderstand the effects of the reward choice for LLM\nreasoning, we supplement comprehensive experi-\nments on rewards for plan generation (Table 5) and\nmath reasoning (Table 6).\nGenerally, the combination of multiple rewards\ncontributes to the performance. However, the ef-\nfects of a reward depends on the nature of tasks.\nFor example, the action likelihood reward is es-\nsential for plan generation, but not very helpful to\nmathmatical reasoning. More discussions are in\nAppendix F.\n6 Conclusion\nIn this paper, we present Reasoning via Planning\n(RAP), a novel LLM reasoning framework that\nequips LLMs with an ability to reason akin to\nhuman-like strategic planning. Our framework,\nwhich repurposes the LLM to act as both a world\nmodel and a reasoning agent, enables the LLM\nto simulate states of the world and anticipate ac-\ntion outcomes, and achieve an effective balance\nbetween exploration and exploitation via Monte-\nCarlo Tree Search. Extensive experiments on a\nvariety of challenging reasoning problems demon-\nstrate RAP‚Äôs superiority over several contemporary\nCoT-based reasoning approaches, and even the ad-\nvanced GPT-4 in certain settings.\n8162\nLimitations\nIn this work, we mainly focus on utilizing frozen\nLLMs, whose abilities might be bounded by the\npre-training. In the future, it is worth exploring how\nto fine-tune LLMs to better reason and serve as a\nworld model (Xiang et al., 2023), as well as how to\ncombine external tools (Hao et al., 2023a; Schick\net al., 2023) with RAP to solve more complex real-\nworld problems.\nEthics Statement\nIn this paper, we primarily focus on the applica-\ntions on plan generation, mathematical reasoning,\nand logical reasoning, posing no significant ethical\nconcerns. We recognize that future research on bor-\nder applications of reasoning with LLMs may pose\na risk of misuse, and we recommend careful con-\nsideration of all aspects of safety before relevant\ntechniques are applied to the real world.\nReferences\nAlan Baddeley. 1992. Working memory. Science,\n255(5044):556‚Äì559.\nRobert Eamon Briscoe. 2011. Mental imagery and the\nvarieties of amodal perception. Pacific Philosophical\nQuarterly, 92(2):153‚Äì173.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nS√©bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nTom Bylander. 1994. The computational complexity of\npropositional strips planning. Artificial Intelligence,\n69(1-2):165‚Äì204.\nEduardo F Camacho and Carlos Bordons Alba. 2013.\nModel predictive control. Springer science & busi-\nness media.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nR√©mi Coulom. 2007. Efficient selectivity and backup\noperators in monte-carlo tree search. In Comput-\ners and Games: 5th International Conference, CG\n2006, Turin, Italy, May 29-31, 2006. Revised Papers\n5, pages 72‚Äì83. Springer.\nYan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi\nZhang. 2023. Task and motion planning with large\nlanguage models for object rearrangement. arXiv\npreprint arXiv:2303.06247.\nWojciech W Gasparski and Tufan Orel. 2014. Desig-\nnology: Studies on Planning for Action , volume 1.\nTransaction Publishers.\nDedre Gentner and Albert L Stevens. 2014. Mental\nmodels. Psychology Press.\nDavid Ha and J√ºrgen Schmidhuber. 2018a. Recurrent\nworld models facilitate policy evolution. Advances\nin neural information processing systems, 31.\nDavid Ha and J√ºrgen Schmidhuber. 2018b. World mod-\nels. arXiv preprint arXiv:1803.10122.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mo-\nhammad Norouzi. 2019. Dream to control: Learn-\ning behaviors by latent imagination. arXiv preprint\narXiv:1912.01603.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi,\nand Jimmy Ba. 2020. Mastering atari with discrete\nworld models. arXiv preprint arXiv:2010.02193.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.\n2023a. Toolkengpt: Augmenting frozen language\nmodels with massive tools via tool embeddings. Ad-\nvances in neural information processing systems, 36.\nShibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Xiyan\nShao, Hengzhe Zhang, Eric Xing, and Zhiting Hu.\n2023b. Bertnet: Harvesting knowledge graphs with\narbitrary relations from pretrained language models.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 5000‚Äì5015.\nMark K Ho, David Abel, Carlos G Correa, Michael L\nLittman, Jonathan D Cohen, and Thomas L Griffiths.\n2021. Control of mental representations in human\nplanning. arXiv e-prints, pages arXiv‚Äì2105.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\narXiv preprint arXiv:2212.10403.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tomp-\nson, Igor Mordatch, Yevgen Chebotar, et al. 2022.\nInner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint\narXiv:2207.05608.\n8163\nQuentin JM Huys, Neir Eshel, Elizabeth O‚ÄôNions, Luke\nSheridan, Peter Dayan, and Jonathan P Roiser. 2012.\nBonsai trees in your head: how the pavlovian system\nsculpts goal-directed choices by pruning decision\ntrees. PLoS computational biology, 8(3):e1002410.\nYu-qian Jiang, Shi-qi Zhang, Piyush Khandelwal, and\nPeter Stone. 2019. Task planning in robotics: an\nempirical comparison of pddl-and asp-based systems.\nFrontiers of Information Technology & Electronic\nEngineering, 20:363‚Äì373.\nPhilip N Johnson-Laird. 2010. Mental models and\nhuman reasoning. Proceedings of the National\nAcademy of Sciences, 107(43):18243‚Äì18250.\nPhilip Nicholas Johnson-Laird. 1983. Mental models:\nTowards a cognitive science of language, inference,\nand consciousness. 6. Harvard University Press.\nAna Jojic, Zhen Wang, and Nebojsa Jojic. 2023. Gpt is\nbecoming a turing machine: Here are some ways to\nprogram it. arXiv preprint arXiv:2303.14310.\nLevente Kocsis and Csaba Szepesv√°ri. 2006. Bandit\nbased monte-carlo planning. In Machine Learning:\nECML 2006: 17th European Conference on Machine\nLearning Berlin, Germany, September 18-22, 2006\nProceedings 17, pages 282‚Äì293. Springer.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nYann LeCun. 2022. A path towards autonomous ma-\nchine intelligence version 0.9. 2, 2022-06-27. Open\nReview, 62.\nBelinda Z Li, Maxwell Nye, and Jacob Andreas. 2022.\nLanguage modeling with latent situations. arXiv\npreprint arXiv:2212.10012.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\n2023. Llm+ p: Empowering large language mod-\nels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. arXiv preprint arXiv:2301.13379.\nYutaka Matsuo, Yann LeCun, Maneesh Sahani, Doina\nPrecup, David Silver, Masashi Sugiyama, Eiji\nUchibe, and Jun Morimoto. 2022. Deep learning,\nreinforcement learning, and world models. Neural\nNetworks.\nJohn McCarthy. 1963. Situations, actions, and causal\nlaws. Technical report, STANFORD UNIV CA\nDEPT OF COMPUTER SCIENCE.\nGr√©goire Mialon, Roberto Dess√¨, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi√®re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nOpenAI. 2023. Gpt-4 technical report.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and\nBoi Faltings. 2023. Refiner: Reasoning feedback\non intermediate representations. arXiv preprint\narXiv:2304.01904.\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba.\n2018. Virtualhome: Simulating household activities\nvia programs.\nAbulhair Saparov and He He. 2022. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. arXiv preprint arXiv:2210.01240.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hu-\nbert, Karen Simonyan, Laurent Sifre, Simon Schmitt,\nArthur Guez, Edward Lockhart, Demis Hassabis,\nThore Graepel, et al. 2020. Mastering atari, go, chess\nand shogi by planning with a learned model. Nature,\n588(7839):604‚Äì609.\nJay Schulkin. 2012. Action, perception and the brain:\nAdaptation and cephalic expression. Springer.\nRamanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter\nAbbeel, Danijar Hafner, and Deepak Pathak. 2020.\nPlanning to explore via self-supervised world models.\nIn International Conference on Machine Learning,\npages 8583‚Äì8592. PMLR.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023.\nReflexion: an autonomous agent with dynamic mem-\nory and self-reflection. ArXiv, abs/2303.11366.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioan-\nnis Antonoglou, Matthew Lai, Arthur Guez, Marc\nLanctot, Laurent Sifre, Dharshan Kumaran, Thore\nGraepel, et al. 2017. Mastering chess and shogi by\nself-play with a general reinforcement learning algo-\nrithm. arXiv preprint arXiv:1712.01815.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter\nFox, Jesse Thomason, and Animesh Garg. 2022.\nProgprompt: Generating situated robot task plans\nusing large language models. arXiv preprint\narXiv:2209.11302.\nEdward C Tolman. 1948. Cognitive maps in rats and\nmen. Psychological review, 55(4):189.\n8164\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can‚Äôt plan (a benchmark for llms on plan-\nning and reasoning about change). arXiv preprint\narXiv:2206.10498.\nKarthik Valmeekam, Sarath Sreedharan, Matthew Mar-\nquez, Alberto Olmo, and Subbarao Kambhampati.\n2023. On the planning abilities of large language\nmodels (a critical investigation with a proposed\nbenchmark). arXiv preprint arXiv:2302.06706.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to\nself-correct. arXiv preprint arXiv:2211.00053.\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner,\nPieter Abbeel, and Ken Goldberg. 2023. Day-\ndreamer: World models for physical robot learning.\nIn Conference on Robot Learning, pages 2226‚Äì2240.\nPMLR.\nJiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui\nWang, Zichao Yang, and Zhiting Hu. 2023. Language\nmodels meet world models: Embodied experiences\nenhance language models. Advances in neural infor-\nmation processing systems, 36.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nDenny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,\nRuyi Gan, Jiaxing Zhang, and Yujiu Yang. 2022.\nSolving math word problem via cooperative rea-\nsoning induced language models. arXiv preprint\narXiv:2210.16257.\n8165\nA MCTS Planning\nWe adapt MCTS to search for the optimal reason-\ning path (Algorithm 1). Compared with traditional\napplications of MCTS, we are faced with a large\nreasoning space, and the heavy computational cost\nof LLMs. Thus, we made several modifications\nto the classic MCTS in our implementation: (1)\nFor open domain problems, e.g., math problems,\nit‚Äôs impossible to enumerate all actions (subques-\ntions), so we reduce the action space by sampling\na fixed number of potential actions from LLMs,\nconditioned on a prompt of the current state and in-\ncontext demonstration. (2) In the selection phase,\nif there are actions that haven‚Äôt been visited before,\nwe estimate the Q value with lightweight local re-\nwards, e.g., self-evaluation reward, and then select\nthe action with UCT. This provides prior knowl-\nedge for the exploration, which is crucial given the\nlimited iteration budgets.\nB Experiment Settings\nB.1 Language Model Decoding\nWe use random sampling with a temperature of 0.8.\nThe generation is cut off at the maximum length of\n2048 or a newline token.\nB.2 Computing Resources\nAll of our experiments run on 4 √óNVIDIA A5000\nGPUs with 24GB memory.\nC Prompt\nC.1 Plan Generation\nWe show the prompt to calculate the action likeli-\nhood for RAP below. The same prompt is also ap-\nplied in CoT baseline. <init_state>and <goals>\nwould be instantiated by the problem to solve.\nI am playing with a set of blocks where\nI need to arrange the blocks into\nstacks. Here are the actions I can\ndo\nPick up a block\nUnstack a block from on top of another\nblock\nPut down a block\nStack a block on top of another block\nI have the following restrictions on my\nactions:\nI can only pick up or unstack one block\nat a time.\nI can only pick up or unstack a block\nif my hand is empty.\nI can only pick up a block if the block\nis on the table and the block is\nclear. A block is clear if the\nblock has no other blocks on top of\nit and if the block is not picked\nup.\nI can only unstack a block from on top\nof another block if the block I am\nunstacking was really on top of the\nother block.\nI can only unstack a block from on top\nof another block if the block I am\nunstacking is clear.\nOnce I pick up or unstack a block, I am\nholding the block.\nI can only put down a block that I am\nholding.\nI can only stack a block on top of\nanother block if I am holding the\nblock being stacked.\nI can only stack a block on top of\nanother block if the block onto\nwhich I am stacking the block is\nclear.\nOnce I put down or stack a block, my\nhand becomes empty.\n[STATEMENT]\nAs initial conditions I have that, the\nred block is clear, the yellow\nblock is clear, the hand is empty,\nthe red block is on top of the blue\nblock, the yellow block is on top\nof the orange block, the blue block\nis on the table and the orange\nblock is on the table.\nMy goal is to have that the orange\nblock is on top of the red block.\nMy plan is as follows:\n[PLAN]\nunstack the yellow block from on top of\nthe orange block\nput down the yellow block\npick up the orange block\n8166\nAlgorithm 1 RAP-MCTS\nRequire: Initial state s0, state transition probability function pŒ∏, reward function rŒ∏, action generator pœï, number\nof generated actions d, depth limit L, number of roll-outs N, and exploration weight w\n1: Initialize memory of actions A : S‚Ü¶‚ÜíA, children c : S√óA‚Ü¶‚ÜíS and rewards r : S√óA‚Ü¶‚ÜíR\n2: Initialize the state-action value function Q : S√óA‚Ü¶‚ÜíR and visit counter N : S‚Ü¶‚ÜíN\n3: for n ‚Üê0, . . . , N‚àí1 do\n4: t ‚Üê0\n5: while N(st) > 0 do ‚ñ∑ Selection\n6: N(st) ‚ÜêN(st) + 1\n7: at ‚Üêarg maxa‚ààA(st)\n[\nQ(st, a) +w\n‚àö\nln N(st)\nN(c(st,a))\n]\n8: rt = r(st, at), st+1 ‚Üêc(st, at)\n9: t ‚Üêt + 1\n10: end while\n11: while st is not a terminal state ‚àßt ‚â§L do\n12: for i ‚Üê1, . . . , ddo ‚ñ∑ Expansion\n13: Sample a(i)\nt ‚àºpœï(a |st), s(i)\nt+1 ‚àºpŒ∏(st, a(i)\nt ), and r(i)\nt ‚àºrŒ∏(st, a(i)\nt )\n14: Update A(st) ‚Üê{a(i)\nt }d\ni=1, c(st, a(i)\nt ) ‚Üês(i)\nt+1, and r(st, at) ‚Üêr(i)\nt\n15: end for\n16: at+1 ‚Üêarg maxa‚ààA(st) r(st, at) ‚ñ∑ Simulation\n17: rt ‚Üêr(st, at), st+1 ‚Üêc(st, at)\n18: t ‚Üêt + 1\n19: end while\n20: for t‚Ä≤‚Üêt, . . . ,0 do ‚ñ∑ Back propagation\n21: Update Q(st‚Ä≤ , at‚Ä≤ ) with {rt‚Ä≤ , rt‚Ä≤+1, . . . , rt}\n22: end for\n23: end for\nstack the orange block on top of the\nred block\n[PLAN END]\n[STATEMENT]\nAs initial conditions I have that, the\norange block is clear, the yellow\nblock is clear, the hand is empty,\nthe blue block is on top of the red\nblock, the orange block is on top\nof the blue block, the red block is\non the table and the yellow block\nis on the table.\nMy goal is to have that the blue block\nis on top of the red block and the\nyellow block is on top of the\norange block.\nMy plan is as follows:\n[PLAN]\npick up the yellow block\nstack the yellow block on top of the\norange block\n[PLAN END]\n[STATEMENT]\nAs initial conditions I have that, the\nred block is clear, the blue block\nis clear, the orange block is\nclear, the hand is empty, the blue\nblock is on top of the yellow\nblock, the red block is on the\ntable, the orange block is on the\ntable and the yellow block is on\nthe table.\nMy goal is to have that the blue block\nis on top of the orange block and\nthe yellow block is on top of the\nred block.\nMy plan is as follows:\n[PLAN]\nunstack the blue block from on top of\nthe yellow block\nstack the blue block on top of the\norange block\npick up the yellow block\nstack the yellow block on top of the\nred block\n[PLAN END]\n[STATEMENT]\nAs initial conditions I have that, the\nred block is clear, the blue block\nis clear, the yellow block is\nclear, the hand is empty, the\n8167\nyellow block is on top of the\norange block, the red block is on\nthe table, the blue block is on the\ntable and the orange block is on\nthe table.\nMy goal is to have that the orange\nblock is on top of the blue block\nand the yellow block is on top of\nthe red block.\nMy plan is as follows:\n[PLAN]\nunstack the yellow block from on top of\nthe orange block\nstack the yellow block on top of the\nred block\npick up the orange block\nstack the orange block on top of the\nblue block\n[PLAN END]\n[STATEMENT]\nAs initial conditions I have that,\n<initial_state>\nMy goal is to have that <goals>.\nMy plan is as follows:\n[PLAN]\nFor the next state prediction with the world\nmodel, we apply the prompts conditioned on the\nlast action. Here we show the prompt to update\nthe state after a ‚Äúpick up‚Äù action as an example.\nAgain, <state> and <action> would be instanti-\nated with the current state and action.\nI am playing with a set of blocks where\nI need to arrange the blocks into\nstacks. Here are the actions I can\ndo\nPick up a block\nUnstack a block from on top of another\nblock\nPut down a block\nStack a block on top of another block\nI have the following restrictions on my\nactions:\nI can only pick up or unstack one block\nat a time.\nI can only pick up or unstack a block\nif my hand is empty.\nI can only pick up a block if the block\nis on the table and the block is\nclear. A block is clear if the\nblock has no other blocks on top of\nit and if the block is not picked\nup.\nI can only unstack a block from on top\nof another block if the block I am\nunstacking was really on top of the\nother block.\nI can only unstack a block from on top\nof another block if the block I am\nunstacking is clear. Once I pick up\nor unstack a block, I am holding\nthe block.\nI can only put down a block that I am\nholding.\nI can only stack a block on top of\nanother block if I am holding the\nblock being stacked.\nI can only stack a block on top of\nanother block if the block onto\nwhich I am stacking the block is\nclear. Once I put down or stack a\nblock, my hand becomes empty.\nAfter being given an initial state and\nan action, give the new state after\nperforming the action.\n[SCENARIO 1]\n[STATE 0] I have that, the white block\nis clear, the cyan block is clear,\nthe brown block is clear, the hand\nis empty, the white block is on top\nof the purple block, the purple\nblock is on the table, the cyan\nblock is on the table and the brown\nblock is on the table.\n[ACTION] Pick up the brown block.\n[CHANGE] The hand was empty and is now\nholding the brown block, the brown\nblock was on the table and is now\nin the hand, and the brown block is\nno longer clear.\n[STATE 1] I have that, the white block\nis clear, the cyan block is clear,\nthe brown block is in the hand, the\nhand is holding the brown block,\n8168\nthe white block is on top of the\npurple block, the purple block is\non the table and the cyan block is\non the table.\n[SCENARIO 2]\n[STATE 0] I have that, the purple block\nis clear, the cyan block is clear,\nthe white block is clear, the hand\nis empty, the white block is on top\nof the brown block, the purple\nblock is on the table, the cyan\nblock is on the table and the brown\nblock is on the table.\n[ACTION] Pick up the cyan block.\n[CHANGE] The hand was empty and is now\nholding the cyan block, the cyan\nblock was on the table and is now\nin the hand, and the cyan block is\nno longer clear.\n[STATE 1] I have that, the cyan block\nis in the hand, the white block is\nclear, the purple block is clear,\nthe hand is holding the cyan block,\nthe white block is on top of the\nbrown block, the purple block is on\nthe table and the brown block is on\nthe table.\n[SCENARIO 3]\n[STATE 0] <state>\n[ACTION] <action>\n[CHANGE]\nC.2 Math Reasoning\nWe show the prompt of RAP for math reasoning\nas below. The prompt is used for both action pro-\nposal and next state prediction. After instantiate\n<question>, we append a prefix Question 5.1to\nthe prompt, so that we can sample the first action\nwith the LLM. The future actions are sampled sim-\nilarly, except that all previous sub-questions and\nsub-answers need to be appended to the prompt,\nfollowing the formats of in-context demonstration.\nThe next state prediction, i.e., answering the sub-\nquestion, works in the same way.\nGiven a question, please decompose it\ninto sub-questions. For each\nsub-question, please answer it in a\ncomplete sentence, ending with \"The\nanswer is\". When the original\nquestion is answerable, please\nstart the subquestion with \"Now we\ncan answer the question: \".\nQuestion 1: Four years ago, Kody was\nonly half as old as Mohamed. If\nMohamed is currently twice as 30\nyears old, how old is Kody?\nQuestion 1.1: How old is Mohamed?\nAnswer 1.1: He is currently 30 * 2 = 60\nyears old. The answer is 60.\nQuestion 1.2: How old was Mohamed four\nyears ago?\nAnswer 1.2: Four years ago, he must\nhave been 60 - 4 = 56 years old.\nThe answer is 56.\nQuestion 1.3: How old was Kody four\nyears ago?\nAnswer 1.3: Kody was half as old as\nMohamed four years ago. Thus, Kody\nwas 56 / 2 = 28 years old. The\nanswer is 28.\nQuestion 1.4: Now we can answer the\nquestion: How old is Kody?\nAnswer 1.4: She is currently 28 + 4 =\n32 years old. The answer is 32.\nQuestion 2: On a moonless night, three\nfireflies danced in the evening\nbreeze. They were joined by four\nless than a dozen more fireflies\nbefore two of the fireflies flew\naway. How many fireflies remained?\nQuestion 2.1: How many fireflies joined?\nAnswer 2.1: The fireflies were joined\nby four less than a dozen more\nfireflies, which are 12 - 4 = 8\nfireflies. The answer is 8.\nQuestion 2.2: Now we can answer the\nquestion: How many fireflies\nremained?\nAnswer 2.2: Three fireflies were\ndancing originally. They were\njoined by 8 fireflies before two of\nthem flew away. So there were 3 + 8\n- 2 = 9 remaining. The answer is 9.\nQuestion 3: Ali has four $10 bills and\nsix $20 bills that he saved after\nworking for Mr. James on his farm.\nAli gives her sister half of the\n8169\ntotal money he has and uses 3/5 of\nthe remaining amount of money to\nbuy dinner. Calculate the amount of\nmoney he has after buying the\ndinner.\nQuestion 3.1: How much money does Ali\nhave in total?\nAnswer 3.1: Ali has four $10 bills and\nsix $20 bills. So he has 4 * 10 + 6\n* 20 = 160 dollars. The answer is\n160.\nQuestion 3.2: How much money does Ali\ngive to his sister?\nAnswer 3.2: Ali gives half of the total\nmoney he has to his sister. So he\ngives 160 / 2 = 80 dollars to his\nsister. The answer is 80.\nQuestion 3.3: How much money does Ali\nhave after giving his sister the\nmoney?\nAnswer 3.3: After giving his sister the\nmoney, Ali has 160 - 80 = 80\ndollars left. The answer is 80.\nQuestion 3.4: How much money does Ali\nuse to buy dinner?\nAnswer 3.4: Ali uses 3/5 of the\nremaining amount of money to buy\ndinner. So he uses 80 * 3/5 = 48\ndollars to buy dinner. The answer\nis 48.\nQuestion 3.5: Now we can answer the\nquestion: How much money does Ali\nhave after buying the dinner?\nAnswer 3.5: After buying the dinner,\nAli has 80 - 48 = 32 dollars left.\nThe answer is 32.\nQuestion 4: A car is driving through a\ntunnel with many turns. After a\nwhile, the car must travel through\na ring that requires a total of 4\nright-hand turns. After the 1st\nturn, it travels 5 meters. After\nthe 2nd turn, it travels 8 meters.\nAfter the 3rd turn, it travels a\nlittle further and at the 4th turn,\nit immediately exits the tunnel. If\nthe car has driven a total of 23\nmeters around the ring, how far did\nit have to travel after the 3rd\nturn?\nQuestion 4.1: How far did the car\ntravel except for the 3rd turn?\nAnswer 4.1: It travels 5 meters after\nthe 1st, 8 meters after the 2nd,\nand 0 meters after the 4th turn.\nIt‚Äôs a total of 5 + 8 + 0 = 13\nmeters. The answer is 13.\nQuestion 4.2: Now we can answer the\nquestion: How far did the car have\nto travel after the 3rd turn?\nAnswer 4.2: The car has driven a total\nof 23 meters around the ring. It\ntravels 13 meters except for the\n3rd turn. So it has to travel 23 -\n13 = 10 meters after the 3rd turn.\nThe answer is 10.\nQuestion 5: <question>\nC.3 Logical Reasoning\nWe show the prompt for action proposal, action\nlikelihood calculation, and next state prediction.\n<fact> and <query> would be instantiated with\nthe problem.\nGiven a list of facts, and a current\nclaim, output one possible fact as\nthe next step. Be sure to copy the\nexact sentences in the facts. Do\nnot change any wording. Do not\ncreate your own words.\nFacts 1: Each lepidopteran is an\ninsect. Each arthropod is a\nprotostome. Every animal is\nmulticellular. Protostomes are\ninvertebrates. Each whale is bony.\nEach painted lady is a butterfly.\nInvertebrates are animals.\nButterflies are lepidopterans. Each\ninsect is six-legged. Every insect\nis an arthropod. Arthropods are not\nbony.\nQuery 1: True or false: Sally is not\nbony.\nClaim 1.1: Sally is an insect.\nNext 1.1: Each insect is six-legged.\nClaim 1.2: Sally is a butterfly.\nNext 1.2: Butterflies are lepidopterans.\nClaim 1.3: Sally is a lepidopteran.\nNext 1.3: Each lepidopteran is an\ninsect.\n8170\nClaim 1.4: Sally is not bony.\nNext 1.4: Finish.\nClaim 1.5: Sally is an arthropod.\nNext 1.5: Arthropods are not bony.\nClaim 1.6: Sally is a painted lady.\nNext 1.6: Each painted lady is a\nbutterfly.\nFacts 2: Prime numbers are natural\nnumbers. Every Mersenne prime is\nnot composite. Imaginary numbers\nare not real. Every real number is\na number. Natural numbers are\nintegers. Every real number is\nreal. Every Mersenne prime is a\nprime number. Natural numbers are\npositive. Prime numbers are not\ncomposite. Integers are real\nnumbers.\nQuery 2: True or false: 127 is not real.\nClaim 2.1: 127 is real.\nNext 2.1: Finish.\nClaim 2.1: 127 is a natural number.\nNext 2.1: Natural numbers are integers.\nClaim 2.2: 127 is a prime number.\nNext 2.2: Prime numbers are natural\nnumbers.\nClaim 2.3: 127 is a real number.\nNext 2.3: Every real number is real.\nClaim 2.4: 127 is a Mersenne prime.\nNext 2.4: Every Mersenne prime is a\nprime number.\nClaim 2.5: 127 is an integer.\nNext 2.5: Integers are real numbers.\nFacts 3: Lepidopterans are insects.\nEvery animal is multicellular. Each\ninsect is an arthropod. Each\ninvertebrate is an animal. Insects\nare six-legged. Arthropods are\nsmall. Arthropods are\ninvertebrates. Each butterfly is a\nlepidopteran. Whales are not small.\nQuery 3: True or false: Polly is not\nsmall.\nClaim 3.1: Polly is an arthropod.\nNext 3.1: Arthropods are small.\nClaim 3.2: Polly is an insect.\nNext 3.2: Each insect is an arthropod.\nClaim 3.3: Polly is small.\nNext 3.3: Finish.\nClaim 3.4: Polly is a lepidopteran.\nNext 3.4: Lepidopterans are insects.\nFacts 4: Every cat is a feline. Mammals\nare vertebrates. Bilaterians are\nanimals. Vertebrates are chordates.\nCarnivores are mammals. Mammals are\nnot cold-blooded. Each chordate is\na bilaterian. Every feline is a\ncarnivore. Snakes are cold-blooded.\nAnimals are not unicellular. Every\ncarnivore is not herbivorous.\nQuery 4: True or false: Fae is not\ncold-blooded.\nClaim 4.1: Fae is a feline.\nNext 4.1: Every feline is a carnivore.\nClaim 4.2: Fae is not cold-blooded.\nNext 4.2: Finish.\nClaim 4.2: Fae is a mammal.\nNext 4.2: Mammals are not cold-blooded.\nClaim 4.3: Fae is a cat.\nNext 4.3: Every cat is a feline.\nClaim 4.4: Fae is a carnivore.\nNext 4.4: Carnivores are mammals.\nFacts 5: Prime numbers are prime. Real\nnumbers are numbers. Every integer\nis a real number. Real numbers are\nnot imaginary. Mersenne primes are\nprime numbers. Complex numbers are\nimaginary. Each prime number is a\nnatural number. Natural numbers are\npositive. Each Mersenne prime is\nprime. Each natural number is an\ninteger.\nQuery 5: True or false: 7 is imaginary.\nClaim 5.1: 7 is not imaginary.\nNext 5.1: Finish.\nClaim 5.1: 7 is a natural number.\nNext 5.1: Each natural number is an\ninteger.\nClaim 5.2: 7 is a prime number.\nNext 5.2: Each prime number is a\nnatural number.\nClaim 5.3: 7 is a real number.\nNext 5.3: Real numbers are not\nimaginary.\nClaim 5.4: 7 is an integer.\nNext 5.4: Every integer is a real\nnumber.\n8171\nFacts 6: Spiders are not six-legged.\nInsects are six-legged. Insects are\narthropods. Every animal is not\nunicellular. Invertebrates are\nanimals. Lepidopterans are insects.\nEvery arthropod is segmented.\nArthropods are invertebrates. Every\nbutterfly is a lepidopteran. Stella\nis a butterfly.\nQuery 6: True or false: Stella is\nsix-legged.\nClaim 6.1: Stella is an insect.\nNext 6.1: Insects are six-legged.\nClaim 6.2: Stella is a lepidopteran.\nNext 6.2: Lepidopterans are insects.\nClaim 6.3: Stella is a butterfly.\nNext 6.3: Every butterfly is a\nlepidopteran.\nClaim 6.4: Stella is six-legged.\nNext 6.4: Finish.\nFacts 7: <fact>\nQuery 7: <query>\nD Related work: world model and\nplanning\nRecent years have witnessed successful applica-\ntions of planning algorithms (Sekar et al., 2020),\nsuch as AlphaZero (Silver et al., 2017), and\nMuZero (Schrittwieser et al., 2020). These algo-\nrithms are typically based on tree-structured search\nand are designed to effectively maintain the balance\nof exploration and exploitation. Knowledge of tran-\nsition dynamics is the prerequisite for planning,\nand recent research on model-based reinforcement\nlearning propose to learn a world model (or dynam-\nics model) to plan or assist policy learning. To im-\nprove sample efficiency, previous research attempts\nto learn a world model from offline trajectories, and\ndirectly learn a policy within the world model (Ha\nand Schmidhuber, 2018a,b). With latent imagina-\ntion in a world model, RL agents can be trained to\nsolve long-horizon tasks (Hafner et al., 2019, 2020).\nBesides, the world model is also shown to be help-\nful to physical robot learning (Wu et al., 2023). In\nthis paper, we use LLMs as world models and apply\na planning algorithm to search for a reasoning path.\nThis is similar in spirit to model predictive control\n(Camacho and Alba, 2013). Compared with pre-\nvious works, our framework uses general LLMs\nas the world model and can be adapted to a wide\nTable 5: Ablation study on Blocksworld. R1 is action\nlikelihood reward, R2 is task-specific reward, and R3 is\nself-evaluation reward.\nR1 R2 R3 Success\n‚úì ‚úì ‚úó 0.88\n‚úì ‚úì ‚úì 0.91\n‚úì ‚úó ‚úó 0.46\n‚úó ‚úì ‚úó 0.21\n‚úó ‚úó ‚úì 0.14\n‚úó ‚úó ‚úó 0.02\nTable 6: Ablation study on GSM8k (first 300 examples).\nR1 is state transition confidence reward, R2 is action\nlikelihood reward, and R3 is self-evaluation reward.\nR1 R2 R3 RAP(1) RAP(10) +aggr\n‚úì ‚úó ‚úì 0.410 0.450 0.503\n‚úì ‚úó ‚úó 0.350 0.447 0.490\n‚úì ‚úì ‚úó 0.373 0.423 0.443\nrange of open-domain reasoning tasks. Xiang et al.\n(2023) propose to train LLMs wih a external world\nmodel to gain embodied experience, while RAP\nfocuses on the inference stage and is compatible\nwith any training methods.\nE Adaptive Prompting\nThrough our preliminary experiments, we observed\nthat the performance of LLMs is impacted by the\ndiscrepancy in difficulty between demonstration\ncases and the test cases. In the case of RAP, when\na new state is predicted, we reformulate the remain-\ning task as a new test case, initialized with the pre-\ndicted new state. This new test case would require\na smaller minimum number of actions, leading to\na disparity in the distribution of the demonstration\ncases and the new cases. To mitigate this issue, we\npre-compute the intermediate states of the demon-\nstration cases beforehand. During inference, we\ntruncate the trace from the beginning for each new\nstate in an iteration, which reduces the minimum\naction number of the demonstration cases as the\nsearch tree deepens. This technique significantly\nenhances the performance of RAP, especially for\nmore intricate problems, which are more suscepti-\nble to distribution mismatches.\n8172\nF Reward Choice\nResults. We conduct comprehensive experiments\non rewards for plan generation (Table 5) and math\nreasoning (Table 6). Note that, in both tables, the\nfirst row indicates the setting we use in the main\nexperiments. As shown in Table 5, the combina-\ntion of action likelihood and task-specific reward\n(row 1) can significantly outperform the single re-\nward baselines (row 3, 4, 5). Interestingly, adding\nthe self-evaluation reward can further improve the\nperformance slightly (row 2). Furthermore, as\nthe results on the first 300 samples of GSM8k\nshown in Table 6, we can see adding either ac-\ntion likelihood (row 3) or self-evaluation (row 1)\non top of confidence reward (row 2) can boost the\nRAP performance of only using confidence reward\n(row 1) with one iteration, but action likelihood\nreward downgrades the accuracy with more itera-\ntions. The self-evaluation reward leads to the best\nperformance overall. This indicates the importance\nof self-evaluation reward in guiding reasoning as\nan effective and computationally efficient prior to\nexploration.\nSelf-evaluation and action likelihood. The re-\nwards of self-evaluation and action likelihood are\nof particular interest, as they can be applied to a\nwide range of reasoning tasks. Generally, the best\nusage and combination with other rewards require\nempirical design and understanding of the task na-\nture, and their effectiveness can vary significantly\nacross different tasks. Here, we provide some intu-\nitions behind the reward choices:\n(a) For the problems in which one reasoning step\nis short and structured, the action likelihood can\nbe very indicative. Otherwise, it may be disturbed\nby unimportant tokens and become unreliable. For\ninstance, a single step within the Blocksworld do-\nmain typically adheres to specific patterns (e.g.,\nPICK /PUT /STACK a block. . . ), rendering the ac-\ntion likelihood indicative. However, in the math\ndomain, a reasoning step is expressed in natural\nlanguage sentences, allowing for greater freedom\nand potentially introducing noise.\n(b) For the problems where it‚Äôs easier to recog-\nnize some errors afterward than avoid them during\ngeneration, self-evaluation emerges as a helpful\nmechanism for enhancing reasoning accuracy. In\nmathematical reasoning, LLMs may struggle to\ngenerate a correct reasoning step in the first place,\nbut the detection of calculation or logic errors is\nmore feasible. In Blocksworlds, however, assessing\nthe quality of a candidate action is not straightfor-\nward and still requires multi-step reasoning. This\ncharacteristic diminishes the accuracy of the self-\nevaluation reward, making it less helpful especially\ngiven that likelihood already provides a good intu-\nition for search.\n8173",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6415758728981018
    },
    {
      "name": "Deductive reasoning",
      "score": 0.636182963848114
    },
    {
      "name": "Reasoning system",
      "score": 0.5698752999305725
    },
    {
      "name": "Inference",
      "score": 0.5309678316116333
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.4952479898929596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4446820616722107
    },
    {
      "name": "Logical reasoning",
      "score": 0.44141387939453125
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.43773502111434937
    },
    {
      "name": "Action (physics)",
      "score": 0.4327477812767029
    },
    {
      "name": "Abductive reasoning",
      "score": 0.42806994915008545
    },
    {
      "name": "Automated reasoning",
      "score": 0.4278213083744049
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.4162450432777405
    },
    {
      "name": "Management science",
      "score": 0.3537401258945465
    },
    {
      "name": "Engineering",
      "score": 0.08526194095611572
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}