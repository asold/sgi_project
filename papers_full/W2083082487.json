{
    "title": "Pipelined language model construction for Polish speech recognition",
    "url": "https://openalex.org/W2083082487",
    "year": 2013,
    "authors": [
        {
            "id": "https://openalex.org/A2139917659",
            "name": "Jerzy Sas",
            "affiliations": [
                "Wrocław University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5057877941",
            "name": "Andrzej Żołnierek",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6607467106",
        "https://openalex.org/W1972928807",
        "https://openalex.org/W2260775077",
        "https://openalex.org/W2087291293",
        "https://openalex.org/W2072223048",
        "https://openalex.org/W1551395122",
        "https://openalex.org/W2063918473",
        "https://openalex.org/W2185726469",
        "https://openalex.org/W1571394205",
        "https://openalex.org/W1575229083",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2186701992",
        "https://openalex.org/W1505790639",
        "https://openalex.org/W2190094906",
        "https://openalex.org/W2076222873",
        "https://openalex.org/W4368375361",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W1510525348",
        "https://openalex.org/W10704533",
        "https://openalex.org/W22168010",
        "https://openalex.org/W2120011233",
        "https://openalex.org/W1901329589",
        "https://openalex.org/W2096077649",
        "https://openalex.org/W1029334962",
        "https://openalex.org/W2097657973",
        "https://openalex.org/W2084175759",
        "https://openalex.org/W2334311265",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2112874453",
        "https://openalex.org/W1994856949",
        "https://openalex.org/W1846632694",
        "https://openalex.org/W100305897",
        "https://openalex.org/W1579838312",
        "https://openalex.org/W1502984613",
        "https://openalex.org/W1594264430",
        "https://openalex.org/W2500236599"
    ],
    "abstract": "Abstract The aim of works described in this article is to elaborate and experimentally evaluate a consistent method of Language Model (LM) construction for the sake of Polish speech recognition. In the proposed method we tried to take into account the features and specific problems experienced in practical applications of speech recognition in the Polish language, reach inflection, a loose word order and the tendency for short word deletion. The LM is created in five stages. Each successive stage takes the model prepared at the previous stage and modifies or extends it so as to improve its properties. At the first stage, typical methods of LM smoothing are used to create the initial model. Four most frequently used methods of LM construction are here. At the second stage the model is extended in order to take into account words indirectly co-occurring in the corpus. At the next stage, LM modifications are aimed at reduction of short word deletion errors, which occur frequently in Polish speech recognition. The fourth stage extends the model by insertion of words that were not observed in the corpus. Finally the model is modified so as to assure highly accurate recognition of very important utterances. The performance of the methods applied is tested in four language domains.",
    "full_text": "Int. J. Appl. Math. Comput. Sci., 2013, V ol. 23, No. 3, 649–668\nDOI: 10.2478/amcs-2013-0049\nPIPELINED LANGUAGE MODEL CONSTRUCTION FOR POLISH SPEECH\nRECOGNITION\nJERZY SAS ∗,A NDRZEJ ˙ZOŁNIEREK ∗∗\n∗ Institute of Informatics\nWrocław University of Technology, Wybrze˙ze Wyspia´nskiego 27, 50-370 Wrocław, Poland\ne-mail: jerzy.sas@pwr.wroc.pl\n∗∗Department of Systems and Computer Networks\nWrocław University of Technology, Wybrze˙ze Wyspia´nskiego 27, 50-370 Wrocław, Poland\ne-mail: andrzej.zolnierek@pwr.wroc.pl\nThe aim of works described in this article is to elaborate and experimentally evaluate a consistent method of Language\nModel (LM) construction for the sake of Polish speech recognition. In the proposed method we tried to take into account\nthe features and speciﬁc problems experienced in practical applications of speech recognition in the Polish language, reach\ninﬂection, a loose word order and the tendency for short word deletion. The LM is created in ﬁve stages. Each successive\nstage takes the model prepared at the previous stage and modiﬁes or extends it so as to improve its properties. At the ﬁrst\nstage, typical methods of LM smoothing are used to c reate the initial model. Four most frequently used methods of LM\nconstruction are here. At the second stage the model is extended in order to take into account words indirectly co-occurring\nin the corpus. At the next stage, LM modiﬁcations are aimed at reduction of short word deletion errors, which occur\nfrequently in Polish speech recognition. The fourth stage ext ends the model by insertion of words that were not observed\nin the corpus. Finally the model is modiﬁed so as to assure highl y accurate recognition of very i mportant utterances. The\nperformance of the methods applied is tested in four language domains.\nKeywords: automatic speech recognition, hidden Markov model, adaptive language model.\n1. Introduction\nAutomatic Speech Recognition(ASR) over the past twenty\nyears has been the challenge for researchers all over\nthe world. Although various ASR paradigms have been\nproposed and investigated, the most popular approach to\nASR is the one where the Hidden Markov Model (HMM)\nof speech is created using the Language Model (LM)\nand the Acoustic Model (AM). The LM represents the\nstochastic properties of the language. In the course of\nASR recognition, it is used to estimate the probabilities\nof word sequences that can constitute fragments of\nutterances being recognized. The LM is typically created\nusing a representative set of texts from the domain of\nASR application. The set of texts used to construct LM\nis usually called the corpus. Different LM construction\nmethods have been investigated in order to achieve\nthe accepted level of user satisfaction with a spoken\nman–machine dialog, which can be measured empirically\nas the Word Error Rate (WER). A WER less than 5–10%\nmust be reached in order for ASR-based software to\nbe widely accepted (Devine et al. , 2007). A typical\napproach lies in applying a stochastic n-gram LM, but\nbecause of the insufﬁcient amount of the available data\ndifferent discounting techniques composed with back-\noff methods are proposed to obtain a smoothed LM,\nwhich assigns non-zero probabilities also to n-grams not\noccurring in the text corpus. In this typical approach\n(Goodman, 2001; Jurafsky and Matrin, 2009; Chen and\nGoodman, 1999; Gale and Sampson, 1995; Katz, 1987)\nthe probability mass obtained by discounting n-gram\nprobabilities is distributed in different ways among all\nnon-observed n-grams taking into account (n − 1)-gram\nprobabilities.\nSuch static strategies strongly depend on the size\nof the vocabulary used, and in order to improve the\nquality of the ASR system, different adaptive (dynamic)\ntechnique are used. A class-based LM was applied by\nBrown et al. (1992), Ward and Issar (1996) or Niesler\n\n650 J. Sas and A. ˙Zołnierek\net al. (1998). The method described by Chen and\nChan (2003) or Sarukkai and Ballard (1996) introduces a\ntrigger pair model to investigate a long distance dependent\nrelationship. The method presented by Mikolov et al.\n(2011) is based on a combination (in the form of linear\ninterpolation) of advanced language modeling techniques\nsuch as the class-based model, the cache model, the\nmaximum entropy model, structured LM and others. The\nresults of Iyer and Ostendorf (1999) suggest modelling\nlong distance dependence using topic mixtures model.\nWhile the accuracy of advanced commercial and\nexperimental ASR systems for English reaches 97–98%,\nthe accuracy achieved typically for other languages is\nsigniﬁcantly lower. The main reason of difﬁculties in\nachieving a low WER is rich inﬂection of the language\nand loose word order permitted by the language syntax.\nSlavonic languages are particularly difﬁcult for ASR due\nto these reasons (Zi´ ołko et al. , 2010; Mauces et al. ,\n2003). Because of rich inﬂecti on, the language dictionary\ncontains much more word forms than in the case of\ninﬂectionally simple languages. Experiments described\nby Whittaker and Woodland (2003) show that in order to\nobtain a similar level of corpus coverage, the dictionary\nfor Russian has to contain almost 7 times more words\nthan is needed for English. Firstly, a big number of words\nin the dictionary leads to computational problems in the\nrecognition process. Additionally, phonetic differences of\nword form pronunciations are often insigniﬁcant, which\nleads to problems in distinguishing word forms based\non acoustic evidence. Therefore, for languages having\nspeciﬁc properties differe nt methods taking into account\ntheir individual features are considered.\nFor the Lithuanian language, Vaiciunas et al.\n(2004) proposed word clustering ﬁrst and then linear\ninterpolation of a classical model with a class-based\nmodel. Another source of difﬁculties in ASR for\nlanguages like Czech or Slovak or Polish is their loose\nword order. In these languages the word order is not\nso strictly imposed by the language syntax as, e.g., in\nEnglish or German. If a sequence of words constitutes\na syntactically and semantica lly correct utterance, then\nit is very likely that the permutation of these words\nalso constitutes a syntactically correct phrase. As a\nresult, the language model perplexity of Polish is much\nhigher than that of English (Zi´ołkoet al., 2010; Jurafsky\nand Matrin, 2009). A typical LM based on counting\nn-grams appearing in the language corpus and estimating\nthe probability of the next n-th word conditioned on the\npreceding sequence of n − 1 words is less effective in\nsupporting ASR. This is because the actual conditional\nprobability p(w\ni|wi−n+1,...,w i−1) is more uniformly\ndistributed among words wi. For the Czech and Slovak\nlanguages, Brychcin and Konopik (2011) proposed to use\nmorphological knowledge in a class-based n-gram LM\nwith linear interpolation.\nIn loose word order languages, instead of merely\nrelying only on the sequences of words actually observed\non adjacent positions in the training corpus, it seems\nreasonable to increase the n-gram probabilities of all\nword pairs that co-occur in the utterances in the language\ncorpus. In this way, a longer context of words can be\ntaken into account in the language model. Incorporation\nof a distant context into the LM has been considered in a\nnumber of publications. One of the possibilities is to take\nhigher order n-gram models.\nExperiments described by Goodman (2001) show\nthat increasing the n-gram order up to 6 improves the\nperplexity. This however, requires a very huge language\ncorpus in order for higher order n-gram probabilities\nto be estimated reliably. Another approach utilizes\nthe observation that words once observed in the text\nare likely to be repeated again. This leads to the\nabove-mentioned concept of a dynamic language model\nwhere the probabilities of words just observed in the text\nare temporarily boosted (Jelinek et al., 2001).\nASR accuracy can be also improved by applying\na multistage approach, where the earlier stage provides\na set of alternative word sequences and the subsequent\nstages re-evaluate the candidate sequence scoring by\napplying longer distance word co-occurrence properties.\nThis concept was applied in the Julius ASR system\n(Lee et al. , 2001) and proved to be effective also for\nPolish. The method presented by Piasecki and Broda\n(2007) exploits the concept o f semantic similarities of\nwords. It was originally proposed for handwriting\nrecognition but can be easily adapted to ASR needs.\nThe likelihood is boosted for sequences containing word\ncombinations that are seman tically similar each to other.\nThe semantic similarity can be deﬁned in various ways,\nbut one of possibilities is to base it on word co-occurrence\nfrequency in the language corpus. Another idea described\nby Kolorenc et al. (2006) explores the inﬂuence of\nmulti-words (compound words) in the continuous speech\nrecognition system of the Czech language. Multi-words\nare made of short words (at most three characters long)\nand frequently the followi ng or the preceding word and\nare added to the vocabulary. Quite a different approach\nto ASR in Polish is presented in the work of Zi´ ołko\net al. (2010), where instead of the HMM the method\nusing the Levenshtein distance is proposed. Another\npaper concerning ASR for the Polish language (Zi´ ołko\net al. , 2011) considers speciﬁc acoustic features of the\nlanguage.\nThe adaptive approach presented in our previous\npaper (Sas and ˙Zołnierek, 2011) lies in modiﬁcation of\ntypical n-gram LM. The modiﬁcations are arranged so as\nto boost the pr obabilities of n-grams consisting of words\nthat co-occur in utterances but are not direct neighbors.\nThe modiﬁcation can be applied to any backoff LM that is\nbased on discounting. In a typical approach (Goodman,\nPipelined language model construction for Polish speech recognition 651\n2001) the probability mass obtained by discounting\nestimated n-gram probabilities is distributed among all\nnot observed n-grams proportionally to (n − 1)-gram\nprobabilities. In our approach proposed previously\n(Sas and ˙Zołnierek, 2011), more probability is allocated\nto co-occurring n-grams at the expense of lowering\nthe probability allocated to n-grams whose components\ndid not occur in the same utterance in the language\ncorpus. The factor by which the co-occurring n-gram\nprobabilities a re boosted is set so as to minimize\nthe cross-perplexity computed using the subset of the\nlanguage corpus excluded from the set used for LM\nconstruction. Application of an LM prepared in this way\nin ASR reduced the overall WER of speech recognition\nby about 4%.\nAlthough many concepts aimed at constructing\neffective LMs for ASR have been proposed and\ninvestigated for other languages, still relatively little\nwork has been carried out towards the veriﬁcation of\nthese concepts in the case of Polish ASR. The method’s\nproperties conﬁrmed in the environment of one language\nmay not be conﬁrmed for other languages. Therefore,\nthe ultimate aim of the works described in this paper\nis to verify the effectiveness of methods related to LM\nconstruction for the sake of ASR in Polish and to combine\nselected methods into a consistent procedure of LM\nconstruction. In the proposed procedure we try to take into\naccount speciﬁc features and problems related to Polish\nASR. The problems considered here are as follows:\n• lack of large corpora published as full texts ( in\nextenso)—as a result, language models must be\nbuilt from limited corpora (this particularly concerns\nnarrow domain ASR applications, e.g., in medical\ninformation systems);\n• loose word order in sentences;\n• frequent appearance of ASR errors typical for Polish\nphonetics like the tendency for short word deletion;\n• practical issues occurring in ASR applications: the\nnecessity to boost recognition accuracy of very\nimportant utterances (that may not be represented in\nthe corpus used to build the LM) and insertion of\nout-of-corpus words into the language model.\nThe method of LM construction proposed and\nexperimentally evaluated in this article creates the\nLM by applying a sequence of stages. Each stage\nmodiﬁes or extends the LM provided to its input by\napplying the operation aimed at the speciﬁc LM feature.\nFor this reason we called the proposed method the\npipelined LM construction . At each stage, various\nalternative methods are experimentally evaluated. The\none that gives the LM which exhibits the highest ASR\naccuracy in tests is recommended.\nThe ﬁrst stage consists in building an initial\nsmoothed stochastic LM. At this stage we compare\nvarious smoothing methods, in order to test whether\nthere are signiﬁcant differences in ASR accuracy between\nLMs created using various smoothing methods. The\nbest smoothing method is used to build the initial LM.\nIt is then passed to the next stage, where the model is\nimproved by taking into account indirectly co-occurring\nwords. The proposed met hod boosts the p robability of\nbigrams corresponding to indirectly co-occurring words.\nThe next stage modiﬁes the LM so as to avoid short\nword deletion errors as much as possible. The fourth\nstage introduces out-of-corpus words to the LM. These\nare words that do not appear in the corpus utterances\nbut are necessary in a particular ASR application. At\nthe last stage the LM is modiﬁed so as to increase the\naccuracy of very important phrase recognition. These\nphrases are utterances (or fragments of utterances) which\nare of crucial importance for the speaker. They may be\nunder-represented in the corpus, thus their recognition\naccuracy may be not sufﬁcient.\nIn pipelined language model construction we try to\nachieve WER reduction by applying speciﬁc methods of\nmodel extension or modiﬁcation. At all stages except for\nthe ﬁrst one, our own methods were used. At the second\nstage of the pipeline we used our own method of distant\nco-occurring bigram boosting, outlined earlier (Sas and\n˙Zołnierek, 2011). Here its performance was evaluated\nin various domain-speciﬁc areas of ASR application\nand compared to that of the LM created using typical\nsmoothing methods. At the third stage, the concept of\nmulti-words was utilized. It is not quite new and was used\nby other authors. In the approach presented by Chen and\nChan (2003) multi-words are used as speciﬁc collocation\ncontexts for other words that are frequently associated\nwith them. Kolorenc et al. (2006) apply the multi-word\nconcept for the same purpose as we consider here. They,\nhowever, create multi-words by only analyzing short word\noccurrence in speciﬁc bigrams in the corpus.\nWe used here a slightly different, novel approach.\nThe novelty consists in taking into account acoustic\nsimilarities as the criterion of multi-word application is\nthe particular context. The idea of combing the LM with\nthe ﬂat out-of-corpus word list used here at the fourth\nstage of the pipeline can be found in the work of Brown\net al. (1992). In our work we applied the class-based\napproach to Polish language modeling, using Part-Of-\nSpeech (POS) tagging. n-gram probabilities for classes\nare estimated in a typical way from the corpus. The\nmethod used at this stage is not quite novel because it\njust combines techniques presented by other authors. Our\naim here was to ﬁnd out if this known approach can\nsigniﬁcantly improve the accuracy of out-of-corpus word\nrecognition. At the last stage our own method is applied,\nwhich modiﬁes the model so as to achieve high accuracy\nof very important utterance recognition. The element of\nnovelty at this stage consists in using the HMM as the tool\n652 J. Sas and A. ˙Zołnierek\nfor artiﬁcial speech samples generation.\nThe organization of the paper is as follows. In\nSection 2 the approach to ASR based on the LM is\nshortly described. The next section presents selected\nLM smoothing techniques which were used as candidates\nto create the baseline LM at the ﬁrst stage of the\npipeline. In Section 4, the method of co-occurringn-gram\nprobabilities boosting is presented in detail. Experimental\nevaluation and comparison of LMs created using various\nsmoothing techniques in Polish speech recognition are\ndescribed in Section 5. The idea of application of\nmulti-words to short word deletion error avoidance is\npresented in Section 6. Section 7 is devoted to the\nmethod of combing the LM with the ﬂat word list. In\nSection 8, the method of boosting the pr obabilities of\nvery important phrases is described. For all new methods\npresented in Sections 6–8, the results of their empirical\ninvestigations are included in the corresponding sections.\nThe last section presents conclusions and further research\ndirections.\n2. Automatic speech recognition with\nacoustic and language models\nA typical approach to the problem of automatic speech\nrecognition consists in building acoustic models and\nlanguage models combined into a compound hidden\nMarkov model. HMMs proved to be an efﬁcient\ntechnique in modeling sequential processes related to\nvarious man-machine interactions as speech, handwriting\nor gesture recognition (Kasprzak et al., 2012). Although\nother approaches were tested in the ASR domain, the\nHMM still remains the primary speech modeling and\nrecognition technique.\nThe HMM speech model can be considered a\nthree-level system. On the lowest level, simple Markov\nmodels for individual phonemes speciﬁc to the language\nare created and trained. A uniform HMM topology for\neach phoneme is assumed. It consists of three observation\nemitting states. The state transition probabilities as well\nas the parameters of observations emission probability\ndensity functions for all phoneme HMMs are estimated\nusing the Baum–Welch procedure. On the middle\nlevel, the models of words are created by concatenating\nmodels of subsequent phonemes appearing in the phonetic\ntranscription of the word. Because the phoneme HMMs\ncan be multiplied applied in various words, training of the\nHMM for the language consisting of a set of words does\nnot require all words from the language to be presented\nduring training. Then for each admissible word from\nthe dictionary D =( w\n1,w2,...,w N ) we deal with the\nword HMM which is built by concatenating HMMs for\nsubsequent phonemes.\nFinally, on the highest level, the compound HMM\nof the whole utterance is built by connecting word\nHMMs in one language HMM. The probabilities of\ntransition from the terminal state of a word HMM to the\ninitial state of another word HMM are taken from the\ndomain-speciﬁc n-gram language model. Details of this\nprocedure are presented in the next section. In automatic\nspeech recognition we start with acquisition of the speech\nacoustic signal from the sound device and segment\nit into fragments being individual utterances separated\nby silence. The isolated utterances are recognized\nindependently. Every utterance being recognized is\nconverted into a sequence of vectors of observations\n(o\n1,o2,...,o t). Then, ﬁnally, the recognition with a\ncompound HMM consists in ﬁnding such a word sequence\nW∗ which maximizes its conditional probability given the\nsequence of observations:\nW∗\n=a r g m a x\nwi1,...,wik ∈D+\nP(wi1,...,w ik |o1,o2,...,o t)),\n(1)\nwhere D+ denotes the set of all nonempty sequences of\nwords from the dictionary D.\n3. Backoff LM smoothing\nWe will be considering here the languages being sets\nof sequences of words coming from the ﬁnite dictionary\nD. A stochastic n-gram language model is the set of\ndata that makes it possible to estimate the probability of\nappearance of then-th word wi provided that the sequence\nof preceding n − 1 words wi−n+1,wi−n+2,...,w i−1 is\nknown. In other words, the LM provides the method to\ncompute the estimation of\np(wi|wi−n+1,wi−n+2,...,w i−1). (2)\nA sequence of n consecutive words is called the n-gram.\nThe conditional probabilities can be given explicitly in the\nLM or they can be deﬁned procedurally. The language\nmodel is usually constructed from the language corpus\nwhich is a sufﬁciently large set of sample phrases in the\nlanguage being modeled. The most obvious way to ﬁnd\nout the probability estimatesis to count the occurrences of\nn-grams in the model and to apply Maximum Likelihood\n(ML) estimation:\np\nML (wi|wi−n+1,wi−n+2,...,w i−1)\n= c(wi−n+1,wi−n+2,...,w i−1,wi)\nc(wi−n+1,wi−n+2,...,w i−1) , (3)\nwhere c(w1,w2,...,w n) is the number of n-gram\noccurrences w1,w2,...,w n in the corpus. Due to a\nlimited size of the corpus, nonzero ML estimates can be\nobtained only for a very limited set of n-grams. For this\nreason, in practice, low n-gram orders are used—in most\ncases n does not exceed 3.\nPipelined language model construction for Polish speech recognition 653\nIn our experiments we use a bigram LM, which\ncorresponds to setting n =2 . Limiting the n-gram\norder still does not solve the data sparseness problem\ncompletely in the case of languages consisting of\nthousands of words. It is still very likely that many\nn-grams that may appear in typical language use are\nmissing in the corpus or the number of their occurrences\nis not big enough to allow reliable ML estimation of\nrelated probabilities. To prevent underestimation of\nprobabilities of missing n-grams, the concept of a back-\noff is applied. Backing off consists in using lower order\nn-gram probabilities when the number of occurrences of\nan n-gram in the corpus is not sufﬁcient. In such a case\nthe probability (2) is approximated using the lower order\nn-gram probability p(w\ni|wi−n+2,...,w i−1).\nIn the bigram LM the probabilities p(wi|wi−1)\nare approximated by prior wi word probabilities p(wi)\nwhich in most cases can be reliably estimated with ML\nestimators:\np(w\ni)= c(wi)\n∑\nw∈D c(w), (4)\nwhere c(w) is the number of unigram w occurrences in\nthe corpus. While missing n-gram probabilities estimated\nwith the ML estimator are underestimated (nulled), the\nprobabilities of n-grams occurring in the corpus only\na few times are usually over-estimated. Therefore the\nconcept of the backoff is complemented with that of dis-\ncounting. Probability discounting consists in subtracting\nsome probability mass from the probabilities (2) estimated\nwith the ML based on the formula (3). As result,\nfor bigrams\n1 occurring in the corpus, the probability\npML (wi|wi−1) estimated using an ordinary ML estimator\nis replaced by the discounted probability pd(wi|wi−1) ≤\npML (wi|wi−1).\nIn discounted backoff L Ms, the probability mass\ndiscounted from the ML es timates of probabilities\npML (wi|wi−1) for bigrams actually occurring in the\ncorpus is distributed among words that never occurred\nas successors of w\ni−1. The discounted probability mass\nβ(w) for any word w can be computed as\nβ(w)=1 −\n∑\nwk :c(w,wk)>0\npd(wk|w), (5)\nwhere c(wi,wi−1) is the number of bigram (wi,wi−1)\noccurrences in the corpus. The conditional probabilities\nof words wi that never appeared as successors of w\nare proportional to their pr ior probab ilities computed\naccording to (4). The probabilities p(wi|w),h o w e v e r ,\nhave to sum up to 1.0 for every word w over all words\nfrom the dictionary D. Therefore the probabilities for\nbigrams (w,w i) not observed in the corpus are ﬁnally\n1In the further part of the paper we will restrict our discussion to\nbigram language models.\ncomputed as\np(wi|w)= α(w)p(wi), (6)\nwhere\nα(w)= β(w)\n∑\nwk :c(w,wk)=0 p(wk)\n= β(w)\n1 − ∑\nwk :c(w,wk)>0 p(wk).\n(7)\nVarious schemes of discounting were proposed and\ntested in various LM applications (Goodman, 2001).\nIn our test, for the comparison purpose, the following\nsmoothing methods were chosen:\n• Good–Turing (GT) estimate,\n• Absolute Discounting (AD),\n• Kneser–Ney (KN) smoothing,\n• Modiﬁed Kneser–Ney (MKN) smoothing.\nThe details of the methods can be found in the work\nof Goodman (2001), and their effectiveness in Polish ASR\nwill be investigated in Section 5. Let us brieﬂy recall the\nideas of above mentioned methods, using the notation of\nGoodman (2001).\nThe Good–Turing estimate states that for any\nbigram that occursr times we should pretend that it occurs\nr\n∗ times:\nr∗ =( r +1 )nr+1\nnr\n, (8)\nwhere nr is the number of bigrams that occur exactly r\ntimes in the corpus. Consequently, we can calculate the\nprobability for a bi-gram δ with r counts,\npGT (δ)= r∗\nN , (9)\nwhere N = ∑ ∞\nr=0 nrr∗ .\nIn absolute discounting , the bigram probability\nestimate pABS (wi|wi−1) is computed by subtracting a\nﬁxed discount 0 ≤ d ≤ 1 from each nonzero count\nof bigram occurrences, and by mixing the discounted\nML bigram estimate with the unigram estimate of the\nsuccessor word wi, i.e.,\npABS(wi|wi−1)= max[c(wi−1,wi) − d,0]\n∑\nw∈D c(wi−1,w ) +\n+( 1− λwi−1)pABS (wi).\n(10)\nTo make this distribution sum up to 1, we should take\n1 − λwi−1 = d\n∑\nw∈D c(wi−1,w )N1+(wi−1•), (11)\nwhere the number of unique words that follow the\npredecessor wi−1 is deﬁned as\n654 J. Sas and A. ˙Zołnierek\nN1+(wi−1•)= |{ wi : c(wi−1,wi) > 0}| . (12)\nThe notation N1+ is meant to evoke the number of words\nthat have one or more counts, and • is meant to evoke a\nfree variable that is summed over.\nKneser–Ney smoothing is an extension of absolute\ndiscounting where the lower-order distribution that one\ncombines with a higher-order distribution is built in\nanother manner. For a bigram model, we select a\nsmoothed distribution p\nKN that satisﬁes the following\nconstraint on unigram marginals for all wi:\n∑\nwi−1∈D\npKN (wi−1,wi)= c(wi)\n∑\nw∈D c(w). (13)\nThe Kneser–Ney model can be presented in the same\nrecursive way as (10), i.e.,\npKN (wi|wi−1)= max[c(wi−1,wi) − d,0]\n∑\nw∈D c(wi−1,w )\n+ κ(wi−1)pKN (wi).\n(14)\nwhere\nκ(wi−1)= d\n∑\nw∈D c(wi−1,w )N1+(wi−1•). (15)\nThe unigram probabilities can be calculated as follows:\npKN (wi)= N1+(•wi)\nN1+(••) , (16)\nwhere\nN1+(•wi)= |{ wi−1 : c(wi−1,wi) > 0}| (17)\nand\nN1+(••)=\n∑\nwi ∈D\n(N1+(•wi)). (18)\nIn modiﬁed Kneser–Ney smoothing , the method\nproposed by Goodman (2001), instead of using a single\ndiscount d for all nonzero counts we use three different\nparameters d1, d2 and d3+ that are applied to bigrams with\none, two, and three or more counts, respectively. Now the\nformula (14) turns into\npMKN (wi|wi−1)= c(wi−1,wi) − d(c(wi−1,wi))\n∑\nw∈D c(wi−1,w )\n+ γ(wi−1)pMKN (wi)\n(19)\nwhere\nd(c)=\n⎧\n⎪⎪\n⎨\n⎪⎪\n⎩\n0 if c =0 ,\nd\n1 if c =1 ,\nd2 if c =2 ,\nd3+ if c ≥ 3.\n(20)\nTo make the distribution sum to 1, we take\nγ(wi−1)\n= d1N1(wi−1•)+ d2N2(wi−1•)+ d3+N3+(wi−1•)\n∑\nw∈D c(wi−1,w ) ,\n(21)\nwhere N2(wi−1•) and N3+(wi−1•) are deﬁned\nanalogously to N1(wi−1•) (12).\n4. Probability boosting for indirectly\nco-occurring n-grams\nThe idea presented by Sas and ˙Zołnierek (2011) lies in\nincreasing the probability p(wk|w) for words w and wk\nthat occur in the corpus close each to other but do not\nnecessarily appear in adjacent positions. The idea behind\nthis concept is motivated by the fact that, in loose word\norder languages like Polish, if two words co-occur in the\nsame utterance then it is likely that they will occur in\nother utterances in adjacent positions. Thus, appearance\nof the co-occurrence of words in the corpus in distant\npositions can be an indication to increase the probability\nof the corresponding bigram. We assume here that the\nlanguage corpus consists of clearly separated utterances.\nThe probability of a bigram will be boosted if its two\ncomponents co-occur in the same utterance.\nLet us consider a single word w ∈D .L e t N(w)\ndenote the set of words that appear at least once in the\ncorpus directly after the word w, i.e., ∀w\ni ∈N (w):\nc(w,w i) > 0.B y F(w) we will denote the set of words\nthat co-occur in at least one utterance with the word w\nbut do not belong to N(w). X(w) denotes all remaining\nwords from the dictionary (X(w)= D\\N (w) \\F (w)).\nIn the ordinary LM the probabilities for bigrams\n(w,w i) for wi ∈F (w)∪X (w) are calculated according to\n(6), where α(w) is uniformly calculated using the formula\n(7). In order to boost the probability of bigrams consisting\nof words from F(w), their probabilities will be increased\nby the factor λ> 1.0, common for the whole model, i.e.,\n∀wi ∈F (w): p(wi|w)= λα(w)p(wi). (22)\nWe assume that the total discounted probability mass\nβ(w) deﬁned in Eqn. (5) remains unchanged. To achieve\nthis, the probabilities assigned to bigrams consisting of\nwords from X(w) must be lowered appropriately. Now\nthe probabilities (6) are multiplied by the factor ¯λ(w) <\n1.0, which must be individually calculated for each w,s o\nas the total probability mass β(w) is preserved:\nλα(w)\n∑\nwk ∈F(w)\np(wk)+ ¯λ(w)α(w)\n∑\nwk ∈X(w)\np(wk)\n=1 −\n∑\nwk ∈N(w)\npd(wk|w)= β(w),\n(23)\nPipelined language model construction for Polish speech recognition 655\nwhere pd(wk|w) is the discounted probability obtained\nwith any discounting method. Hence, ¯λ(w) can be\ncalculated as\n¯λ(w)\n=\n1 − ∑\nwk ∈N(w)\npd(wk|w) − λα(w) ∑\nwk ∈F(w)\np(wk)\nα(w) ∑\nwk ∈X(w)\np(wk) .\n(24)\nFinally, the probability p∗(wi|w) that the modiﬁed\nlanguage model assigns to a bigram (w,w i) can be\ndeﬁned as\np∗(wi|w)=\n⎧\n⎨\n⎩\npd(wi|w) if wi ∈N (w),\nλα(w)p(wi) if wi ∈F (w),\n¯λ(w)α(w)p(wi) if wi ∈X (w).\n(25)\nThe value of λ, common for the whole model, can\nbe computed so as to maximize the probability that the\nmodel assigns to the separated fragment of the language\ncorpus. In order to do it, the corpus is divided into two\nparts: the training part T and evaluation part E. The latter\nis a set of word sequences s\n1,s 2,...,s n. Assume that\neach sequence starts with the speciﬁc start tag “<s>”a n d\nends with the end tag “</s>”:\nsj =( < s >,w(j)\n1 , w(j)\n2 ,..., w(j)\nl(sj),</ s >). (26)\nThe probability that the LM assigns to the utterance\nsj can be computed as\nP(sj; LM(T ,λ)) =\nl(sj )+1∏\nk=1\np∗(w(j)\nk |w(j)\nk−1; LM(T ,λ)),\n(27)\nwhere w(j)\n0 = < s >, w(j)\nl(sj )+1 = </ s > and l(sj)\ndenotes the length of the utterance sj. The probability\nthat the language model LM(T ,λ) assigns to the whole\nevaluation set E is\nP(E; LM(T ,λ)) =\n∏\ns∈E\nP(s; LM(T ,λ)). (28)\nThe value of λ is determined in an iterative procedure so\nas to maximize the probability (28).\n5. Experimental evaluation of LMs in\nPolish speech recognition\nIn order to evaluate the performance of LMs created using\nthe methods described in the previous sections a series\nof experiments was carried out. The LM performance is\nassessed by (a) the perplexity computed on the sentence\nset representative for a domain and (b) the word error\nrate of a speech recognizer which uses the LM being\nevaluated. The perplexity is the measure of LM quality.\nIt is based on the average probability that the tested LM\nassigns to sentences in the test set computed “per word”\n(Goodman, 2001). The lower the LM perplexity, the\nbetter the language stochasticproperties approximation by\nthe LM. However, from the practical point of view the\nultimate LM assessment should be rather determined by\nevaluating the LM contribution to the accuracy increase\nof the ASR process.\nFour domains of the Polish language which differ in\ncomplexity were used in the experiment\n2:\n• CT: texts from the domain of medical diagnostic\nimage reporting, mainly related to CT and MRI\nmodalities; dictionary size—23 thousands of words,\ncorpus size—22 MB;\n• TL: a collection of not copyrighted texts of the\nPolish literature or foreign language books translated\nto Polish; dictionary size—81 thousands of words,\ncorpus size—8 MB;\n• GM: general medicine texts consisting of elements of\nmedical documentation, medical examination reports\nand medical articles co llected from Wikipedia;\ndictionary size—119 thousands of words, corpus\nsize—94 MB;\n• PL: general purpose Polish language texts consisting\nof the Polish literature (8%), medical documentation\nsamples (11%), samples of newspaper articles\n(1%), reports from the Polish Parliament sessions\n(35%), Senate of Republic of Poland proceedings\n(35%), European Parliament Proceedings from\nPolish-English parallel corpus (Koehn, 2005) (10%);\ndictionary size—576 thousands of words, corpus\nsize—370 MB.\nThe experiment consists of two stages. At the ﬁrst\nstage, the performance of typical language models built\nwith the smoothing techniques described in Section 3\nare compared by their perplexities and by the WER\nof a speech recognizer based on the model being\ncompared. Then the best smoothing method, taking into\naccount the WER, is selected for further experiments.\nAt the second stage, the chosen method is combined\nwith the backoff technique proposed here. The speech\nrecognition accuracy obtained using the modiﬁed model is\ncompared with the corresponding accuracy achieved with\nthe conventional LM.\nThe acoustic model for ASR was created in a\nspeaker-dependent manner as a triphone model using\nspeech samples recorded by a single male speaker. The\n2The corpora used to build the LMs employed in the ex-\nperiments described in this article as well as the recorded\nutterances used in the acoustic models are available at\nhttp://sun10.ci.pwr.wroc.pl/˜sas/ASR.\n656 J. Sas and A. ˙Zołnierek\ntotal duration of training utterances is about 5 hours. For\nASR accuracy testing the individual set of utterances was\nused for each domain LM. The duration of the utterances\nin the test set was in the range of 50–70 minutes. The HTK\ntoolkit (Young and Everman, 2009) was applied to build\nthe acoustic model. The open source recognition engine\nJulius (Lee et al., 2001) was used as the speech recognizer.\nThe results of the ﬁrst stage of the experiment\nconcerning perplexity co mparison and ASR accuracy\nevaluation are shown in Tables 1 and 2, respectively.\nJust for the sake of comparison, results obtained with\nthe unigram (U) LM are also shown. No smoothing is\napplied in the case of the unigram LM. The symbols\nAD, GT, KN, MKN denote methods of smoothing as\ndescribed in Section 3, while CT, TL, GM, PL denote\nthe language corpora applied. Additionally, Table 2\ncontains the column “Total”, where the ASR accuracy of\nthe combination of utterances from the speciﬁc domains\nCT, TL, GM, PL is presented. For the sake of\nstatistical signiﬁcance evaluation of the obtained results,\nthe conﬁdence interval radius ϵ was determined for each\nsmoothing method and the utterance combination. The\nconﬁdence level 1 − α =0 .9 was used.\nIt can be observed that there are practically no\nsigniﬁcant differences between bigram models created\nwith various discounting methods. Despite the loose\nword order of the Polish language, bigram models have\nmuch lower perplexity than unigram models (U). The\nbigram/unigram perplexity ratios for Polish are similar to\nTable 1. Perplexity of models based on various smoothing\nmethods (the ﬁrst row contains results obtained with\nthe unsmoothed unigram model).\nMethod\n CT\n TL\n GM\n PL\nU\n 751.5\n 2688.3\n 1596.3\n 4233.6\nAD\n 35.9\n 748.4\n 69.2\n 672.1\nGT\n 35.9\n 733.2\n 68.2\n 665.4\nKN\n 34.5\n 713.0\n 65.7\n 633.3\nMKN\n 34.9\n 720.4\n 66.6\n 633.5\nTable 2. ASR accuracy obtained with models based on various\nsmoothing methods (ϵ: conﬁdence interval radius).\nMethod\n CT\n TL\n GM\n PL\n Total\nU\n 0.937\n 0.893\n 0.925\n 0.904\n 0.914\nϵ=0.0025\nAD\n 0.953\n 0.906\n 0.942\n 0.916\n 0.928\nϵ=0.0023\nGT\n 0.952\n 0.908\n 0.949\n 0.920\n 0.932\nϵ=0.0022\nKN\n 0.960\n 0.914\n 0.951\n 0.924\n 0.936\nϵ=0.0022\nMKN\n 0.957\n 0.917\n 0.949\n 0.920\n 0.934\nϵ=0.0022\nthese reported for English by Jurafsky and Matrin (2009).\nASR in Polish is, however, much less sensitive to LM\nperplexity than English. Goodman (2001) claims that\nthe increase of cross-entropy (which is a logarithm of\nperplexity) by 0.2 results in the absolute increase of the\nWER by 1%. The results pr esented in Tables 1 and\n2 show much weaker dependence of the WER on the\nmodel cross-entropy. Let us consider cross-entropies and\nWERs of the CT and PL models obtained using the KN\nmethod. According to Goodman (2001), cross-entropy\nH is deﬁned as H(T)=l o g\n2(PP (T)),w h e r ePP\nis the perplexity and T is the test set. The difference\nof the cross-entropy between CT and PL models is\nlog2(633.3) − log2(34.4) = 4.2.\nThe increase in cross-entropy by 4.2 results in\nthe increase of the WER by only about 3.5%. Also\nthe superiority of the modiﬁed Kneser–Ney smoothing\nmethod reported by Goodman (2001) over all other\nmethods is not conﬁrmed in the case of the Polish\nlanguage. The performance of the modiﬁed Kneser–Ney\nmodel is even slightly worse than that of the original\nKneser–Ney model. Although the performances of all\nbigram models are similar, the original Kneser–Ney\nmodel achieves the best results both in perplexity and\nWER comparisons. To verify the statistical signiﬁcance\nof the obtained result, the conﬁdence intervals of ASR\naccuracy estimation were determined at the conﬁdence\nlevel 1 − α =0 .9. The analysis of conﬁdence intervals\npresented in the rightmost column in Table 2 shows\nthat only the unmodiﬁed Kneser–Ney method marginally\noutperforms other compared techniques. It was used for\nfurther experiments.\nAt the second stage, Kneser–Ney smoothing was\ncombined with our bigram boosting technique described\nin Section 4. The accuracy of the speech recognizer was\nthen evaluated for the modiﬁed model. ASR accuracies\nin various language domains are presented in Table 3.\nCT, TL, GM and PL denote language corpora applied.\nThe rightmost column contains combined results obtained\nusing the mixture of utterances coming from individual\ndomains. The results of the Kneser–Ney method (the\nsame as in Table 2) used here as a baseline for comparison\nare presented in the ﬁrst row. The accuracies obtained\nusing the LM created with the bigram boosting method\nfrom Section 4 are presented in the second row. The last\nrow contains the achieved relative WER reduction rate.\nThe application of indirectly co-occurring bigrams\nboosting resulted in small but observable improvement of\nthe ASR accuracy in the case of all language domains\nexcept for the simplest CT model. The average relative\nWER reduction is about 5.7%. To show the statistical\nsigniﬁcance of the obtained results, the conﬁdence\ninterval radiuses of the accuracy estimates were calculated\nfor the combined test utterances set. The radii for\naccuracy estimaties for baseline Kneser–Ney and the\nPipelined language model construction for Polish speech recognition 657\nbigram boosting method are shown in the rightmost\ncolumn of Table 3. The conﬁdence intervals do not\noverlap, hence the superiority of the model created using\nbigram boosting seems to be statistically signiﬁcant.\n6. Application of multi-words\nSome words appear most frequently in speciﬁc\ncollocations. Pairs of strongly collocated words can\nbe replaced in the LM by their combinations called\nhere multi-words. Application of multi-words extends\nthe context represented in the n-gram LM and hence\nmakes it possible to model a language more precisely\nwithout extension of its order. On the other hand,\nhowever, too intensive application of multi-words reduces\nLM generalization abilities because it assigns more\nprobability to n-grams actually appearing in the corpus\nand lowers the backoff probability mass. In the extreme\ncase, if all sentences appearing in the text corpus are\nconverted to multi-words, only utterances represented in\nthe corpus can be recognized. The problem is therefore\nwhich word pairs appearing in the corpus should be\nreplaced by the corresponding multi-words.\nApplication of multi-words can be particularly\nbeneﬁcial in the case of two-word collocations where\nat least one word is very short (in particular, where it\nconsists just of a single phoneme). Our experiments\nwith ASR applied to Polish show that recognizers exhibit\nstrong tendency to discard Short Words (SW) from the\nrecognized phrase. It is most often observed if the phone\nending the preceding word or beginning the next word is\nacoustically similar to the phone being the pronunciation\nof the short word. We will call such the situation the\ndeleting context and resulting recognition error will be\ncalled the Short Word Deletion (SWD) error, for short.\nExperiments with ASR in Polish (Sas, 2010) show that\nASR decoders tend to falsely skip a short word in deleting\ncontexts, which leads to approximately 3% of the WER.\nCombining a short word with the adjacent one could\nimprove ASR accuracy, in particular in the case when\nword insertion penalties are used.\nThe concept of multi-words is not new and\nTable 3. ASR accuracy and WER reduction obtained with the\nmodel based on indirectly co-occurring bigram proba-\nbility boosting (ϵ: conﬁdence interval radius).\nCT\n TL\n GM\n PL\n Total\nKneser–\nNey(KN)\n0.960\n 0.914\n 0.951\n 0.924\n 0.936\nϵ=0.0022\nBigrams\nboosting\n0.959\n 0.920\n 0.955\n 0.929\n 0.940\nϵ=0.0021\nRelative\nWER\nreduction\n-2.5%\n 6.9%\n 8.1%\n 6.5%\n 5.7%\nits application to improve LM prediction abilities is\ndescribed in the literature. In the approach presented\nby Chen and Chan (2003) multi-words are used as a\nspeciﬁc collocation contexts for other words that are\nfrequently associated with them. Kolorenc et al. (2006)\napply the multi-word concept for the same purpose as we\nconsider here. They, however, create multi-words by only\nanalyzing short word occurrence in speciﬁc bigrams in the\ncorpus. The novelty of the approach presented here lies in\ntaking into account acoustic similarities as the criterion of\nmulti-word selection.\nWe compared two approaches aimed at SWD\navoidance. The ﬁrst one utilizes the multi-word concept.\nIt consists in text corpus modiﬁcation by concatenating\nwords constituting deletion contexts into multi-words.\nThen the typical language model building procedure is\napplied to the modiﬁed corpus. If a multi-word is\nrecognized by the decoder, it is split into its component\nwords at the post processing stage executed after the basic\nrecognition of an utterance.\nThe second approach consists in boosting bigram\nprobabilities f or bigrams corresponding to deletion\ncontexts in a way similar to the one described in Section 4.\nBoth methods lead to LM modiﬁcation oriented to SWD\nerror rate decrease.\nThe ﬁrst problem that needs to be solved is how to\nselect pairs of words that constitute the deletion context.\nOur experiences with ASR applied to the Polish language\nproved that deleting contexts comprise mainly pairs of\nwords where one of them is a single-phoneme word. In\nPolish language there exist the following single phoneme\nwords: ‘i’(and), ‘a’(and/but depending on the context),\n‘o’(about), ‘u’(at), ‘w’(in/inside), ‘z’(with/out of). The\nstrongest tendency to delete the short word appears if the\nneighboring phoneme in the adjacent word is the same as\nthe phoneme being the short word. A weaker tendency of\nSWD errors appears in the situations where ‘w’or ‘z’are\npronounced as voiceless consonants and the neighboring\nphoneme in adjacent word is also voiceless.\nExperiments described by Sas (2010) show that most\nof SWD errors are related to ‘w’ and ‘z’ preposition\ndeletion in deleting contexts. In Polish single-word\nprepositions and conjunctions causing a majority SWD\nerrors are most strongly collocated with successive\nwords. Therefore we will restrict our discussion to\ndeleting contexts where the short word is the ﬁrst\nelement of a bigram. In the text corpus related to\nmedical diagnostic image reporting used in experiments,\nthe relative frequency of ‘w’/‘z’preposition occurrence\nin deleting contexts is about 3%. With the average\nSWD probability in deleting contexts close to 0.5, this\nintroduces the overall deletion error rate 1.5%. With\nthe overall error rate achievable for speaker dependent\nASR close to 8%, SWD errors constitute about 20%\nof word errors in speech recognition. Reduction of\n658 J. Sas and A. ˙Zołnierek\nthe SWD error rate would then observably contribute to\nspeech recognition accuracy improvement. In the method\nproposed here we focus on typical deleting contexts that\nare represented by bigrams appearing in the text corpus\nused to create the language model; however, the concepts\napplied can be extended also to other deleting contexts for\nbigrams not appearing in the corpus.\n6.1. Application of multi-words to reduction of SWD\nerrors. This method of reducing SWD errors consists\nin a selective replacement of bigrams corresponding to\ndeleting contexts by multi-words being a concatenation\nof original words constituting the deletion context. In\norder to preserve the generalization ability of the resultant\nLM, not all but only randomly selected deleting contexts\nare replaced by multi-words. The probability of the\nreplacement of a particular occurrence of the deleting\ncontext intuitively should depend on the SWD probability\nin this context. Alternatively, it can be set up so as to\nobtain the best model according to the cross-perplexity\ncriterion. The replacement probabilities are deﬁned for\ndistinguished groups of deleting contexts. The groups are\ndistinguished based on acoustic similarities of adjacent\nphonemes:\n• A: SW is ‘w’ and the right neighboring word\npronounced in this context begins with the same\nphoneme;\n• B: SW is ‘w’ and the right neighboring word\npronounced in this context begins with a voiceless\nphoneme, e.g. ‘w sytuacji’(‘in the situation... ’), ‘w\nprzypadku’(‘in the case ... ’);\n• C: SW is ‘z’ and the right neighboring word\npronounced in this context begins with the same\nphoneme;\n• D: SW is ‘z’ and the right neighboring word\npronounced in this context begins with a voiceless\nphoneme, e.g., ‘z samym’(‘with only... ’), ‘z powodu’\n(‘because of ... ’);\n• E: SW is ‘a’, ‘i’, ‘o’or ‘u’and the right neighboring\nword pronounced in this context begins with the\nsame phoneme.\nLet p\nA, pB, pC, pD, pE denote the probabilities of\nthe replacement of deleting contexts by the corresponding\nmulti-words in the text corpus. Two methods were\nconsidered to determine these probabilities:\n• the probabilities p\nX are just estimates of SWD errors\nmade by the speech recognizer in corresponding\ngroups of deleting contexts X—the higher the SWD\nprobability in a group, the more frequently the\ndeleting context is replaced by the multi-word in the\ncorpus;\n• the probabilities are determined so as to maximize\nthe cross perplexity of the obtained model tested on\nthe evaluation text setΩ disjoint form the corpus used\nto create the language model.\nThe ﬁrst method utilizes the actual tendency of\nthe speech recognizer to SWD errors, but unfortunately\nmakes the resultant LM dependent on acoustic properties\nof the speech and hence introduces speaker dependency.\nAn LM optimized for one speaker may be inappropriate\nfor another.\nThe second method is independent of the speaker.\nLet LM(p\nA,...,p E) denote the model created with\nthe modiﬁed corpus where the probabilities of deleting\ncontext replacement in groups A ,...,E are pA,...,p E.\nLet wi,1,wi,2,...,w i,l(i) denote the sequence of words\nconstituting the i-th utterance in the evaluation set.\nLet mi,1,m i,2,...,m i,lM (i) denote the sequence\ncorresponding to the i-th utterance, where word\npairs constituting deleting contexts were replaced by\ncorresponding multi-words ( mi,j is here either the\noriginal word occurring in the utterance or a multi-word\nobtained by deleting context substitution). The numbers\nl(i) and lM (i) denote lengths of the original and modiﬁed\ni-th utterance.\nThe average probability per word that the LM\nassigns to the i-th utterance wi,1,wi,2,...,w i,l(i) can be\ncalculated as follows:\n˜p(wi,1,wi,2,...,w i,l(i); LM(pA,...,p E))\n=m a x{(\nl(i)∏\nj=1\np(wi,j|wi,j−1))1/l(i),\n(\nlM (i)∏\nj=1\np(mi,j|mi,j−1))1/lM (i)}.\n(29)\nBecause both variants (cons isting of original words or\ncontaining corres ponding multi-words) of the utterance\nare equally accepted, the greater of the two probabilities\np(wi,1,wi,2,...,w i,l(i)) and p(mi,1,m i,2,...,m i,lM (i))\nis selected. The probability assigned to the whole\nevaluation set consisting of nΩ utterances is calculated as\na product of probabilities deﬁned in Eqn. (29):\np(Ω;LM(pA,...,p E))\n=\nnΩ∏\ni=1\n˜p(wi,1,wi,2,...,w i,l(i); LM(pA,...,p E)). (30)\nThe probabilities pA,...,p E should be set so as to\nmaximize the probability (30).\n6.2. Reduction of SWD errors by direct modiﬁca-\ntions of probabilities in the LM. An alternative method\nconsists in direct boosting of bigram probabilities in\nPipelined language model construction for Polish speech recognition 659\nthe LM. Now we assume that we start with the LM\ncreated using the methods described in Section 4. Our\naim is to modify the LM so as to reduce the total\nWER by limiting the rate of SWD errors. Increasing\nprobabilities p(w\ni−1|wi) for word pairs (wi−1,wi)\nconstituting deleting contexts obviously leads to SWD\nerror reduction, but it may also increase the rate of errors\nof false insertions of short words in contexts where they\nactually do not occur. Therefore the method of conditional\nprobability modiﬁcation in the LM should keep balance\nbetween resultant tendencies to reduce SWD and false\nshort word insertion errors.\nThe idea of the approach proposed here is similar to\nthe one presented by Sas (2010). The method described in\nthis article assumes a kind of post processing, therefore it\nmay not be applicable to standard ASR tools, which do not\nmake it possible to change its processing pipeline. Here\nwe used the modiﬁcation concerning only the LM.\nAs has already been pointed out, in Polish most of\nsingle-phoneme words are strongly collocated rather with\nthe successive word. Our aim is therefore to boost the\nprobability p(w\ni−1|wi) for words constituting deleting\ncontexts, where wi−1 is a short word. Making a decision\non the boost rate individually for deleting contexts is\ninfeasible due to a lack of speciﬁc information about the\nrecognizer tendency to make SWD errors in individual\ncontexts. It seems rather reasonable to apply the same\nboost rate for deleting contexts that are acoustically\nsimilar, which results in setting the boost rate for groups\nA ,...,E speciﬁed in the previous section. One way\nto achieve the probability boost is to apply the power\nfunction to the original probability:\np\n′(wi−1|wi)= pμX (wi−1|wi),X ∈{ A ,...,E },\n(31)\nwhere 0 <μ X < 1.T h e μX factors are established for\ndeleting contexts belonging to groups A ,...,E .\nThe typical LM bigram is a “forward” model, i.e., it\ncomputes the unigram (prior) probabilities of words and\nconditional probabilities of a successive word conditioned\non its predecessor. The proposed methods explicitly\nmodiﬁes backward probabilities p(wi−1|wi). Then the\nmodiﬁed probabilities need to be converted to forward\nprobabilities p\n′(wi|wi−1) contained in the typical forward\nmodel. The conversion can be obtained by simple\napplication of the Bayes rule:\np′(wi|wi−1)= p′(wi−1|wi) p(wi)\np(wi−1), (32)\nwhere the prior probabilities p(wi),p (wi−1) can be taken\nfrom the input LM.\nBoosting the selected (for the words considered\nbelonging to the deleting contexts, i.e., to the groups\nA ,...,E ) probabilities according to the formula (31)\nin effect causes the boostin g of the proba bilities (32)\nincluded in the LM. Only the probabilities of bigrams\nconstituting deleting contexts which appear in the corpus\nare modiﬁed. Consequently, for each word w from the\nset (‘w’, ‘z’, ‘a’, ‘i’, ‘o’, ‘u’) the discounted probability\nmass β(w) needs to be updated. Now the formula (5) is\nm o d i ﬁ e ds oa st ou s ep\n′\nd(wk|w) instead of pd(wk|w).F o r\nexample, for the word w =‘w’the modiﬁed formula is\nβ(w)= 1 −\n∑\nwk :(w,wk)∈(A∪B)\np′\nd(wk|w)\n−\n∑\nwk :(w,wk )̸∈(A∪B)\npd(wk|w),\n(33)\nwhere A and B are sets of words constituting deleting\ncontexts of ‘w’ deﬁned in Section 6.1. Similarly, we\ncan calculate the discounted probability mass β(w) for\nother considered words appearing in deleting contexts.\nThese probability masses, which for every analysed\nword are less than previously, now are used for\ncalculating backed-off probabilities for bigrams not\nexplicitly represented in the initial model, i.e., using the\nformulas (6), (7), (22) and (24). Finally, we obtain the\nmodiﬁed consistent LM as presented in the formula (25).\nThe complete procedure of LM modiﬁcation consists of\nthree steps:\n1. estimate the boosting factors μ\nx for deleting context\ngroups A ,...,E ;\n2. recalculate forward cond itional probabilities for all\npairs of words constituting deleting contexts that are\nexplicitly represented in the LM;\n3. update related model parameters β(w) to preserve\nmodel consistency.\n6.3. Experimental evaluation. The performance of\ndescribed methods was compared experimentally. We also\ncompared it with the results obtained for an alternative\nidea described by Sas (2010), where SWD errors were\ncorrected at the postprocessing stage carried out after the\ntypical HMM-based recognition process was completed.\nIn order to evaluate the proposed methods, the SWD error\nrate was estimated using the test set. The test utterances\nare selected so as to contain at least one deleting context or\nthe word that appears in the corpus as the right neighbor in\nfrequently occurring deleting contexts. The environment\nfor the experiment as well as domain speciﬁc text corpora\nwere the same as described in Section 5.\nFor each domain, the test set was extracted from the\ncorpus and left aside. For the multi-word based method\nthe corpus was appropriately modiﬁed and then the LM\nwas created using Kneser–Ney smoothing combined with\nindirectly co-occurring bigram boosting. For the method\nconsisting in direct modiﬁcations of bigram probabilities,\nthe same technique of initial LM creation was used. Three\n660 J. Sas and A. ˙Zołnierek\nmethods were compared: (a) the one described by Sas\n(2010), which carries out SWD error correction as the post\nprocessing step, (b) the one based on multi-words, where\nthese probabilities were determined using the perplexity\nminimization, and (c) the one that applies direct bigram\nprobability boosting for deleting contexts. For each of the\ndomains presented in Section 5, the SWD error count was\nevaluated for the original LM model and for that obtained\nby applying approaches being compared (further denoted\ncorrespondingly by LM\na, LMb and LMc). Because\ndeleting context bigram probability boosting may lead to\nerrors consisting in insertion of unuttered short words,\nfalse insertion errors were also counted. The performance\nof the method using the model LM\nX in relation to the\nbaseline (unmodiﬁed) model LM can be assessed by the\ngain factor η(LMX) computed as\nη(LMX)=1 − nd(LMX)+ nf (LMX)\nnd(LM)+ nf (LM) , (34)\nwhere nd(LM) and nf (LM) are counts of SWD errors\nand false insertion errors occurring in recognition based\non the model LM.\nThe higher (closer to 1.0) the gain factor value, the\nmore effective the model LM\nX in relation to the baseline\nmodel LM. The assessment results for the methods\nare presented in Table 4. The results in the columns\nCT, TL, GM, PL were obtained using utterances coming\nfrom corresponding domains speciﬁed in Section 5. The\nrightmost column (Total) contains results computed using\nthe combination of utterances coming from all domains\n(CT, TL, GM, PL).\nFor the sake of statistical signiﬁcance evaluation of\nthe obtained results, SWD error estimates e(LM\nX) and\ntheir conﬁdence interval radiiϵ(LMX) were computed for\nthe compared methods at the conﬁdence level1−α =0 .9.\nThe meaning of symbols used in Table 4 is as follows:nw\nis the total number of words in the test set, nDC is the\nthe number of deleting contexts in the test set, e(LM),\nϵ(LM) is the estimated SWD error rate in the baseline\nreference model LM and its conﬁdence interval radius,\ne(LMX), ϵ(LMX) are the estimated SWD error and its\nconﬁdence interval radius obtained using the language\nmodel LMX, η(LMX) is the gain factor of the model\nLMX with respect to the reference model.\nThe method consisting in direct modiﬁcation of\ndeleting context bigram p robabilities i n the language\nmodel results in the highest gain factor. It reduces\nalmost 44% of SWD-related errors. The method\nbased on multi-words application and the substitution\nprobability computed with merely resultant model\nperplexity minimization exhibits the worst performance.\nHowever, with almost 35% of SWD-error reduction,\nit seems usable in practice as well. The conﬁdence\nintervals of the estimated SWD errors (e(LM\nX) −\nϵ(LMX),e (LMX)+ ϵ(LMX)) for the methods being\ncompared do not overlap with the conﬁdence interval of\nthe estimated error of the reference model (e(LM) −\nϵ(LM),e (LM)+ ϵ(LM)). Hence the performance of all\ncompared methods related to SWD errors is signiﬁcantly\nbetter than that of the baseline model.\nThe results presented here were obtained by merely\nmodifying the LM used by the speech recognizer.\nThey can be compared to another alternative method\nalso aimed at SWD-error reduction described by Sas\n(2010). His method applies an additional postprocessing\nstage, therefore it is more troublesome in application,\nin particular, as far as using standard ASR tools is\nconsidered.\nThe average SDW reduction obtained with the\nalternative method is 0.45, which is a slightly better result\nthan those achieved by methods described in this article.\nThe difference, however, is not practically signiﬁcant.\nThe superiority of the approach presented here lies in its\neasier implementation in the case of applying standard\nARS tools. Additionally, in the case of LM\na, it can be\ncreated in a purely speaker-independent manner without\nconsidering any properties of the acoustic model, which\nis not possible when applying the method presented by\nSas (2010).\n7. Combining the LM with a ﬂat word list\nIn many applications, except for a corpus that can be used\nto build stochastic n-gram LM, we have also the ﬂat list\nTable 4. Comparison of SWD error rates obtained with the orig-\ninal and modiﬁed language models.\nCT\n TL\n GM\n PL\n Total\nnw\n 2134\n 2370\n 2049\n 2417\n 8970\nnDC\n 307\n 238\n 213\n 265\n 1023\nnd (LM)\n 171\n 137\n 130\n 155\n 593\nnf (LM)\n 4\n 3\n 4\n 6\n 17\ne(LM)\n 0.082\n 0.059\n 0.065\n 0.067\n 0.068\nϵe (LM)\n 0.010\n 0.008\n 0.009\n 0.008\n 0.004\nnd (LMa)\n 89\n 78\n 69\n 81\n 317\nnf (LMa)\n 11\n 9\n 9\n 13\n 42\ne(LMa)\n 0.047\n 0.037\n 0.038\n 0.039\n 0.040\nϵe (LMa)\n 0.007\n 0.006\n 0.007\n 0.006\n 0.003\nη(LMa)\n 0.43\n 0.38\n 0.42\n 0.42\n 0.41\nnd (LMb )\n 102\n 88\n 81\n 93\n 364\nnf (LMb )\n 7\n 7\n 9\n 10\n 33\ne(LMb)\n 0.051\n 0.040\n 0.044\n 0.043\n 0.044\nϵe (LMb )\n 0.008\n 0.006\n 0.007\n 0.007\n 0.003\nη(LMb )\n 0.38\n 0.32\n 0.33\n 0.36\n 0.35\nnd (LMc)\n 83\n 69\n 65\n 73\n 290\nnf (LMc)\n 13\n 11\n 12\n 17\n 53\ne(LMc)\n 0.045\n 0.034\n 0.038\n 0.037\n 0.038\nϵe (LMc)\n 0.007\n 0.006\n 0.007\n 0.006\n 0.003\nη(LMc)\n 0.45\n 0.43\n 0.42\n 0.44\n 0.44\n\nPipelined language model construction for Polish speech recognition 661\nof Out-Of-Corpus (OOC) words that do not appear in\nthe corpus, but belong to the general language glossary.\nOur aim is to include OOC words into the n-gram LM\ncreated in a typical way from the text corpus. The problem\nis how to combine the OOC word list with the LM, in\nparticular how to estimate unigram probabilities for OOC\nwords and n-gram probabilities for n-grams containing\nOOC words. Approximate unigram probabilities of OOC\nwords can be acquired or they are not known at all. A\ntypical case of an OOC word is the set of names (person\nnames, surnames, city or street names, etc.) speciﬁed\nexplicitly. In these cases, the approximate probabilities of\nOOC word occurrence can be derived from sources other\nthan the corpus of texts, e.g., from databases containing\nrecords related to named entities. In other cases there is\nno explicit knowledge about OOC word occurrences.\nWe propose here to approximate their frequencies by\napplying a class-based approach to language modeling,\nwhich utilizes part-of-speech tagging. The concept of\nclass-based modeling was primarily proposed by Brown\net al. (1992) and then followed by other researches\nwith various grouping criteria. It is not only unigram\nprobabilities for OOC words that can be approximated\nin this way, but also their n-gram probabilities. One\nreasonable approach is to divide OOC and corpus words\ninto classes according t o their POS tags. Then n-gram\nprobabilities for classes can be estimated in a typical way\nfrom the corpus and they can be applied to calculate\nword n-gram probabilities both for words appearing in the\ncorpus and for OOC words.\nExperiments presented by Niesler et al. (1998) show\nthat a class-based approach utilizing POS tagging is not as\nefﬁcient as application of categories based on stochastic\nproperties of n-grams occurring in the corpus. In the case\nof the problem being considered here, we cannot apply the\nlatter approach to OOC words because they are not present\nin the corpus. Therefore we based the solution merely on\nPOS grouping.\nLet us consider the set of POS classes{C\n1,...,C K}\ncorresponding to various part s of speech and to speciﬁc\ninﬂectional forms (e.g., case, plural/singular forms for\nnouns). Let\n c(w) denote the set of POS classes for the\nword w. Due to the POS tagging ambiguity, this set\nmay consist of more than one class. The set of POS\nclasses can be determined from the corpus and OOC\nwords automatically. For the precise context-dependent\ntagging of Polish words, we can use tools described by\nPiasecki (2007) as well as Piasecki and Radziszewski\n(2008). As another option, the simpler tool Morfeusz\ndescribed by Woli´ nski (2006) can be applied to ﬁnd\ngrammatical categories of isolated words. The class-based\nmodel LM\nCB is then created using the standard method\ndescribed by Niesler et al. (1998). The standard word\nn-gram model LMW is also created using the same\ncorpus. The model LMW is then extended by OOC\nword inclusion in two steps. In the ﬁrst one, the total\nprobability of all OOC words occurrences is estimated\nand it is discounted from the probability mass assigned to\nunigrams actually occurring in the corpus. In the second\nstep, the discounted probability mass is redistributed\namong OOC words.\nIn order to assign non-zer o unigram pr obabilities\nto OOC words, the fraction of prior probabilities of\nwords occurring in the corpus is discounted and the saved\nprobability mass is distributed among OOC words. It is\nreasonable to approximate the discounted probability as\nthe probability that a word in the utterance is an Out-\nOf-Vocabulary (OOV) word\n3. This probability can be\nestimated by a simple experiment where the text corpus\nutilized in the LM creation procedure is used again.\nObviously, the stochastic properties of the corpus will not\nbe changed signiﬁcantly if we extract a single sentence\nfrom it. For words in the extracted sentence, checks are\nmade if they appear in the remaining part of the corpus.\nBy applying this experiment to all individual\nsentences in a leave-one-out manner, we can count the\nnumber of missing word occurrences n\nf . The overall\nprobability pf of OOV word occurrence can be then\napproximated as\npf = nf\nnT + nf\n, (35)\nwhere nf is the count of words that occur in the corpus\nonly once and nT is total number of word occurrences\nin the corpus. Let us assume that OOC words cover an\narbitrary assumed fraction δ of all OOV words. Therefore,\nthe probability mass that will be assigned to OOC words\nwill be δp\nf , and the same mass of probability must be\ndiscounted from unigram probabilities of the words in the\ncorpus. The updated proba bilities of corpus words p′(w)\ncan be calculated as p′(w)= p(w)(1 − δpf ),w h e r ep(w)\nis the prior word probability in the primary LMW .\nThe discounted probability mass δpf is distributed\namong POS classes resulting in class residual\nprobabilities pr(ci). The residual probabilities are\nproportional to the corres ponding class p robabilities\ncomputed in the LMC model. Only these classes\nhaving their members in OOC word set are considered.\nProbabilities p\nr(ci) are ﬁnally redistributed among OOC\nwords belonging to them. OOC words are assigned\nto POS classes using their POS tagging. The words,\nhowever, often cannot be assigned to a unique POS class\nunambiguously. If a word is assigned to various POS\nclasses in the corpus, then it should be given higher\nprobability than the word assigned only to a single class.\nIt leads to the following formula for the ﬁnal OOC word\n3OOV is not quite the same as OOC. By OOC we denote the words\nfrom a ﬁnite, explicitly given list, while OOV is the set of all words\nbelonging to the language that do not occur in the corpus: OOC ⊆\nOOV .\n662 J. Sas and A. ˙Zołnierek\nunigram probability pOOC (w):\npOOC (w)= δpf\n∑\nc∈\nc(w) p(c)/nc\n∑\nc∈Ξ(OOC) p(c), (36)\nwhere c is the symbol of the POS class, pc is the POS\nclass probability determined using LMCB ,\n c(w) denotes\nthe set of POS classes to which the word w was assigned\nby the tagger,nc is the number of OOC words tagged with\nthe class c (i.e., nc =c a r d{w : w ∈ OOC ∧ c ∈\n c(w)})\nand Ξ(OOC) is the set of POS classes that appear at least\nonce as a tag of an OOC word. Because we have no\ninformation about OOC word frequencies other than that\nresulting from POS classiﬁcation, we assume that each\noccurrence of a word in the POS class is equally probable.\nIn the resultant LM, bigrams (w\ni,wi+1),w h e r e\nwi+1 ∈ OOC, do not appear explicitly because such word\npairs were by deﬁnition not encountered in the corpus.\nTherefore, similarly as in the case of other word pairs not\noccurring in the corpus, the bigram probability for word\npairs containing an OOC word is calculated by backing\noff to unigram probability.\n7.1. Empirical evaluation. The aim of the experiment\ndescribed here is to compare the proposed method of OOC\nword list inclusion into the LM with a simpler approach,\nwhere all unigram probabilities of all new words are set\nequal each to other. In this rival method, the mass of\ndiscounted probability is determined also by estimating\nthe probability of OOV word occurrence, but next it is\ndistributed uniformly among OOC words.\nThe class set was created based on the POS\nassignment to words and their inﬂectional features speciﬁc\nfor individual parts of speech. For nouns and adjectives,\n42 classes were created based on the case, number and\ngender. 18 classes were created for verbs based on the\ntense, person and number. 35 classes were created for\nnumerals. For the remaining parts of speech, a single class\nwas used for each individual part. The Morfeusz program\nwas used for assigning POS tags to words. Morfeusz is\nbased on a database consisting of veriﬁed words tagging.\nTherefore, it is not able to assign tags to new words not\nregistered in its database. In our experiment, for words\nnot recognized by Morfeusz, the classes were determined\nbased on other correctly tagged words having the same\nlongest sufﬁx. If σ(w) is the set of words sharing the\nlongest sufﬁx with the wordw, then the set of classes\nc(w)\ncontaining w is determined as\nc(w)=\n⋃\nv∈σ(w)\nc(v) (37)\nThe data for the experiment were prepared in the\nfollowing way. First, the fraction of the available corpus\nwas excluded from the text set used later to create the\nbaseline LM. This fraction was selected so as to contain\nall occurrences of the least frequently occurring words.\nThe OOC word list was created from all words in the\nexcluded set that do not occur in the remaining part of\nthe corpus. The test set was created be selecting such\nutterances from the excluded set which contained at least\none OOC word. The utterances selected in this way were\nthen recorded as speech samples and used to test the\nrecognizer performance. The baseline LM was prepared\nusing the remaining part of the corpus. Finally, this\nmodel was extended with OOC words using two methods\nbeing compared (uniform OOC word probabilities and\nOOC probabilities c omputed using POS grouping). The\nexperiment was carried out for 4 domains: CT, TL, GM\nand PL, presented in Section 5. For each domain, OOC\nwords recognition accuracy was computed as\nAcc(LM)= n\nOOC − ne(LM)\nnOOC\n, (38)\nwhere nOOC is the count of OOC word occurrences in\ntest utterances and ne(LM) is the count of OOC words\nrecognized incorrectly using the LM.\nResults for the LM created by uniform probability\ndistribution among OOC words ( LMU) and for the LM\ncreated using the class-based approach ( LMCB )a r e\nshown in Table 5. The ﬁrst row contains counts of words\nn in the test sets. The counts of OOC words occurrences\nnOOC are given in the second row. The meaning of\nthe symbols Acc(LM) and ne(LM) is as in the formula\n(38). For comparison, the 3 bottom rows contain results\nobtained using the test set consisting entirely of words\noccurring in the text corpus and the recognizer based on\nthe unigram language model ( LM\nug). The conﬁdence\nintervals of the estimated accuracies were determined at\nthe conﬁdence level 1 − α =0 .9 and are denoted by\nϵAcc(LM).\nIt can be observed that application of the class-based\napproach in computing unigram probabilities of OOC\nwords does not lead to a signiﬁcant improvement of\nTable 5. Speech recognition accura cy of utterances containing\nOOC words.\nCT\n TL\n GM\n PL\n Total\nn\n 9540\n 11076\n 10767\n 12699\n 44082\nnOCC\n 1678\n 2270\n 1939\n 2151\n 8037\nne(LMU )\n 120\n 236\n 173\n 257\n 768\nAcc(LMU )\n 0.928\n 0.896\n 0.911\n 0.880\n 0.902\nϵAcc(LMU )\n 0.010\n 0.011\n 0.011\n 0.011\n 0.005\nne (LMCB )\n 113\n 228\n 177\n 238\n 756\nAcc(LMCB )\n 0.933\n 0.899\n 0.908\n 0.889\n 0.906\nϵAcc(LMCB )\n 0.010\n 0.010\n 0.011\n 0.011\n 0.005\nne (LMug )\n 105\n 247\n 141\n 208\n 701\nAcc(LMug )\n 0.937\n 0.891\n 0.927\n 0.903\n 0.913\nϵAcc(LMug )\n 0.010\n 0.011\n 0.010\n 0.011\n 0.005\n\nPipelined language model construction for Polish speech recognition 663\nthe OOC word recognition accuracy in comparison with\napplication of the simpler method, where OOC words are\ngiven uniformly distributed probabilities. The conﬁdence\nintervals determined for th e estimated accuracies of the\ncompared models overlap strongly. This indicates that\nthe models obtained using the compared methods are not\nsigniﬁcantly different. The achieved OOC recognition\naccuracy is very close to the overall recognition accuracy\nobtained with a completely ﬂat unigram language model.\nThis observation is consistent with the conclusion drawn\nin Section 5 that in Polish (and probably in other\nloose word order languages) strongly smoothed language\nmodels exhibit good properties in ASR.\n8. Boosting the probability of very\nimportant phrases\nIn some cases of ASR applications, there are especially\nsigniﬁcant or frequently used Very Important Utterances\n(VIU) that should be recogni zed with very high accuracy\n(e.g., commands interleaved with text being dictated to\nthe ASR system). Recognition errors in such utterances\nare particularly annoying and users insist on improving\nrecognition accuracy of these utterances, even at the\nexpense of slight reduction of the accuracy of other\nutterances. The obvious way to achieve this goal is to\n(a) add artiﬁcially many instances of important utterances\nto the text corpus used to build the LM or (b) increase\nsigniﬁcantly probabilities of n-grams constituting the\nutterance. Radical modiﬁcation of the LM aimed at\nselected utterances may, however, lead to a signiﬁcant\ndegradation of recognition of other utterances consisting\nof words phonetically similar to those appearing in\nimportant utterances. The problem is therefore how\nto modify the LM so as to assure the probability of\ncorrect VIU recognition at least at the speciﬁed level α\nof accuracy, while minimizing the degradation of other\nutterances’ recognition quality.\nLet us deﬁne the problem more formally as follows.\nLet U = {u\n1,...,u m} denote the set of VIUs where each\nVIU is a sequence of words ui =( wi1,wi2,...,w ili\n).\nIn particular, the sequence may contain just a single\nword. We assume that the LM used in speech recognition\ncontains all words appearing in the set U.O u r a i m i s t o\nkeep VIU-related recognition error rate at the speciﬁed\nlevel α. The VIU-related error consists in erroneous\nrecognition of VIU utterances and in recognizing other\nutterance as one of VIUs. The ﬁrst type error probability\nfor the single utterance u ∈ U is\np\neI (u)= P(Ψ(O(v);LM)) ̸= u|v = u), (39)\nwhile the second type error probability is\npeII (u)= P(Ψ(O(v);LM)) = u|v ̸= u)\n=\n∑\nv∈W+ p(v)P(Ψ(O(v);LM)) = u)\n∑\nv∈W+ p(v) . (40)\nΨ(O(v);LM) denotes here the recognizer applied to the\nobservation sequence O(v) extracted from the acoustic\nsignal of the utterance v. The LM is the language\nmodel used by the recognizer. W+ denotes the set\nof all word sequences consisting of words appearing in\nthe LM, excluding words being VIUs. In the approach\npresented here, the suppression of the VIU-related error\nrate is achieved by modifying the LM so that, for each\nu ∈ U, p\neI (u) <α and peII (u) <α . The error\nconsisting in recognition of one VIU as another VIU is\nneglected. This is motivated by the fact that in a majority\nof applications VIUs can be selected intentionally so as\nto minimize mutual acoustic similarity among them. The\nmethod of VIU set selection that minimizes VIUs mutual\nsimilarity is presented by Sas (2009).\nThe method of second type error probability\ncomputation deﬁned in Eqn. (40) is formally well-founded\nbut troublesome in practice due to the summation over\nall possible word sequences W\n+. In practice it can be\nsimpliﬁed by summing only over the relatively small set\nq(u) of utterances that are lik ely to appear in real speech\nor are likely to be misrecognized as u. Thus, instead of\nconsidering the overall second type error probability as\ndeﬁned in (40), only the error probability conditioned on\nthe set q(u) can be considered:\nˆp\neII (u)=\n∑\nv∈q(u) p(v)P(Ψ(O(v);LM)) = u)\n∑\nv∈q(u) p(v) . (41)\nThe VIU can be either a single word or a sequence\nof words. In the latter case, the VIU can be replaced\nby a corresponding multi-word, which will be added to\nthe vocabulary of words in the model. Hence, hereafter\nwe will assume that VIUs u\ni are single words. In\nmost applications it seems to be reasonable to assume\nthat VIUs are uttered as isolated utterances. Thus, a\nmethod of LM modiﬁcation can be used that is similar\nto that one proposed in the previous section, where\nsome discounted unigram probability was redistributed\namong multi-words. Now, however, according to the\nassumed isolation of VIUs, more appropriate approach is\nto redistribute bigram probabilities p(w|⟨s⟩) for bigrams\n(⟨s⟩,w ) containing words w that appear in the initial\nLM. The symbol ⟨s⟩ denotes here the pseudo-word\nrepresenting the beginning of the utterance. The\ndiscounted probability will be assigned to bigrams(⟨s⟩,u )\nfor u ∈ U.\nThe unigram probabilities for words occurring in the\ncorpus do not need to be modiﬁed. If VIUs are new words\nor multi-words, then for formal LM consistency they must\n664 J. Sas and A. ˙Zołnierek\nbe added to unigrams. However, their probabilities are\nset to zero. This is because we assume here that VIUs\nwill be always recognized as isolated utterances, so they\ncan occur only as successors of the pseudo-word ⟨s⟩.I n\nthe case of a bigram LM, the unigram probabilities are\nnecessary only in order to compute the probability if the\nbigram is not explicitly contained in the LM, i.e., if the\nsuccessor word wi+1 does not belong to the set N(wi)\nspeciﬁed in Section 4. In such a case, the backoff to\nunigrams is applied according to the formula (25). The\nexplicit bigrams (⟨s⟩,u ) for all u ∈ U will be added to\nthe LM. For all other preceding words, the appearance of\nVIU as a successor is forbidden, which will be achieved\nby setting VIU unigram probabilities to zero.\nWe will need to discount the probabilities p(w|⟨s⟩)\nfor bigrams represented explicitly in the initial LM and\nto redistribute the discounted probability mass among\nbigrams containing VIUs as successors. In order to\npreserve the model properties related to other utterances,\nour aim is to modify the model as little as possible so as\nto achieve the assumed VIU recognition accuracies. Thus\nthe problems to be solved are the following:\n(a) how much of the probability mass should be\ndiscounted from the probabilities p(w|⟨s⟩);\n(b) how the discounted probability mass should be\nredistributed among the probabilities p(u|⟨s⟩),u ∈\nU.\nAlternatively, the problem is: What should the minimal\nprobabilities p(u|⟨s⟩) be which yield VIU-related error\nwithin the assumed interval (0,α).\nA theoretical solution to the optimal selection of\nVIU bigram probabilities is difﬁcult due to the complexity\nof the computations involved in the Viterbi procedure\n(Young and Everman, 2009; Lee et al. , 2001) typically\napplied in HMM-based speech recognizers. A purely\nempirical approach is not feasible in most cases either due\nto the necessity to collect a big number of VIU acoustic\nsamples. Therefore we propose here a procedure that\nutilizes the existing acoustic model as a kind of artiﬁcial\nspeech sample synthesizer.\nLet r(u) denote the set of speech samples created\nby uttering the VIU u,a n dl e tR denote the sum of sets\nR = ⋃\nu∈U r(u). Let us denote by g(u) the set of speech\nsamples obtained by uttering other utterances out of U\nthat are likely to be incorrectly recognized as u.T h e s e t s\nr(u) and g(u) constitute the veriﬁcation set used in order\nto modify the LM appropriately. The VIU-related errors\npeI (u) and ˆpeII (u) can be estimated empirically as\npeI (u)= neI (u)\ncard(r(u)),\n peII (u)= neII (u)\ncard(g(u)),\n(42)\nwhere neI (u) is the count of samples from the set\nr(u) incorrectly recognized, and neII (u) is the count\nof samples from the set g(u) incorrectly recognized as\nu. VIU-related error approximation can be used in the\niterative procedure of LM modiﬁcation. In the i-th\niteration it ﬁnds the minimal bigram probability p(ui|⟨s⟩)\nthat leads to the error estimate on the test set within the\nassumed interval.\nProbabilities established once for already processed\nVIUs are not changed in the later steps of the procedure,\nso the necessary discounted probability mass is always\nobtained from other bigrams representing words actually\nappearing in the corpus and not being VIUs. Application\nof the probability estimates (42) instead of the true\nprobabilities (39) and (41) is acceptable when they are\nprecise. Of course, this requires many speech samples\nin sets r(u) and q(u). In the method proposed here the\nreal utterances were replaced by observation sequences\nproduced by the HMM model used as a random automaton\nthat produces observation sequences.\nThe utterances represented in the sample set g(u)\nshould be drawn from the corpus, so that they are likely\nto appear in the real utterances. To make the second type\nerror estimation feasible, the amount of testing utterances\nmust be kept within the reasonable limit. For this reason,\nthe set g(u) should consist of utterances that are likely to\nbe misrecognized when O(u) is presented to the speech\nrecognizer. Most of speech recognizers provide the option\nto deliver not only the most likely words sequence but\nrather a list of N-best candidates. In the method presented\nhere, the word sequences used to create the set g(u) ⊂\nW\n+ are obtained by gathering N-best word sequences not\nbeing the actually spoken VIU obtained when recognizing\nspeech samples from the set r(u). In this way, the set of\nword sequences q(u) is obtained. Then for each sequence\nfrom q(u) the corresponding random HMM automaton\nis conﬁgured. By running this automaton randomly, an\nunlimited number of artiﬁcial speech samples constituting\nthe set g(u) can be created. By Q we will denote the set\nof veriﬁcation utterances created in this way.\nThe misrecognition of an utterance is the result of\nits acoustic similarity to other sequences of words. The\nlikelihood of VIU misrecognition is used to order them\nin the procedure of LM modiﬁcation. Misrecognition\nlikelihood can be evaluated by acoustic similarity to\nthe most similar word sequence that can be constructed\nfrom the words in the LM vocabulary. The most likely\nmisrecognition results can be taken from the set q(u)\nobtained as described above. For each u ∈ U its\nsimilarity to all elements of q(u) can be evaluated, and\nﬁnally the misrecognition likelihood can be computed\nbased on the similarity to the most similar misrecognized\nsequence of words. For the sake of this method,\nacoustic similarity of two utterances is calculated as the\nedit distance (Levenshtein distance) between sequences\nof phones obtained as phonetic transcriptions of the\nutterances. The original edit distance, being the number of\nPipelined language model construction for Polish speech recognition 665\ninsertion, deletion and substitution operations, is modiﬁed\nso as the substitutions of various types of phones are\nassigned various weights corresponding to their acoustic\nsimilarities.\nThe complete procedure applied here to modify the\ninput LM can be deﬁned as Algorithm 1. It iteratively\nincreases the bigram probabilityp(u|⟨s⟩) for the utterance\nu ∈ U being currently processed until the ﬁrst type error\nprobability falls within the required interval or the second\ntype error probability goes out of it. The permissible\nbigram probability interval is (pmin,p max),w h e r epmin =\n1/k, pmax =1 /2m), k is the number of words in the\nvocabulary and m is the number of VUIs in the set U.\nAlgorithm 1. LM modiﬁcation for correct VIU\nrecognition.\ncreate the set of observation sequences R and Q\nusing the random automaton based on LM/AM;\norder the set U by VIU importance;\nfor all u ∈ U in decreasing order do\nfor p = pmin; p ≤ pmax;s t e pδ do\nLM’ = LM;\nset p(u|⟨s⟩)= p in LM’;\ndiscount the probability p from\nbigrams p(w|⟨s⟩),w /∈ U;\ncalculate estimates\n peI (u),\n peII (u) using LM′;\nif\n peI (u) <α or\n peII (u) >α then\nbreak;\nend if\nend for\nLM = LM′;\nend for\nreturn LM;\n8.1. Empirical evaluation. The proposed method is\nbased on the assumption that the accuracy of recognition\nof the artiﬁcially created utterances is close to the\naccuracy achieved for the human speaker. The aim of the\nexperiment is to verify this assumption. We also want to\ntest how the boosting of VIU bigram probability impacts\nthe recognition of other acoustically similar utterances. In\nthe experiment, the speaker-independent, gender speciﬁc\nacoustic model was used. We tested the set of VIUs\nbeing commands that control the editing of the text being\ndictated. The set of commands in Polish and their\ntranslations into English are listed in Table 6. The test set\nwas created by uttering commands by the set of speakers.\nEach command was uttered 20 times by each of the 6\nspeakers (3 female, 3 male).\nThen the initial LM was modiﬁed using the tuning\nprocedure described in the previous section to achieve\nthe VIU recognition error rate at the level of α ∈\n(0.01, 0.07). The actual accuracy of VIU recognition was\nthen evaluated using the set of test utterances. The results\nare shown in Fig. 1.\nIt can be observed that the actual error rate is greater\nthan the expected one based on testing with artiﬁcial\nutterances. This can be explained by the fact that the\nacoustic model does not simulate the speech perfectly, so\nthe error rate for real speech is higher than that evaluated\nusing the model which is also used in the recognition\nprocedure. The dependence of the predicted and actual\nerror rate is, however, almost linear, apart from the\ninterval (0.1, 0.2), where the assumed error rate cannot\nbe achieved due to an increasing second type error. The\nexperiment shows that in order to obtain the assumed\nﬁrst type error close to α\n′the model should be tuned for\nα ≈ 0.7α′.\nDecreasing ﬁrst type error peI(u) by increasing\nthe probability p(u|⟨s⟩) leads to the increase of the\nsecond type error consisting in erroneous recognition of\nother utterances as u. The proposed algorithm prevents\nexcessive increase of the second type error by limiting the\nbigram probability p(u|⟨s⟩) if p\neII (u) exceeds assumed\nTable 6. Set of VIUs used in the experiment.\nCommand utterance\n Translation into English\nWyczy´s´c\n Clear\nCofnij\n Back\nZako´ncz\n Terminate\nZapisz\n Save\nNowa linia\n Insert the new line\nZapisz do pliku\n Save to a ﬁle\nZaznacz słowo\n Select word\nNa koniec\n Go to the end\nNa pocza¸tek\n Go to the beginning\nNaste¸pne słowo\n Go to the next word\nPoprzednie słowo\n Go to the previous word\nDu˙ze litery\n Upper case letters\nMałe litery\n Lower case letters\nFig. 1. First type actual error\n peI (u) vs. the expected error level\nα.\n666 J. Sas and A. ˙Zołnierek\nlimit α. In Fig. 1 this can be observed for α< 0.02,\nwhere the actual ﬁrst type error is not further reduced\nbecause the second type e rror exceeds its limit. When\nLM is being updated in the iterative algorithm, it uses\nthe estimates of ﬁrst and second type errors based on the\nartiﬁcial utterances. The legitimacy of artiﬁcial utterance\napplication when estimating the second type error was\nveriﬁed experimentally. In the experiment, second\ntype error estimation based on artiﬁcial utterances was\ncompared with its estimate based on actual human-spoken\nutterances from q(u) sets. Estimates obtained with\nmodiﬁed LMs created for various values of α ∈\n(0.01, 0.07) were compared. Results are shown in Fig. 2.\nThe results obtained in experiments related to the\nVIU are obviously speciﬁc for the VIU set and the LM\nbeing used. For other VIUs and/or LMs, the relation\nof the actual accuracy to that obtained with artiﬁcial test\nsets as well as the relation of the ﬁrst type to the second\ntype errors can be different. We believe, however, that\ngeneral tendencies demonstrated in this particular case\nare also valid in different conditions. Thus achieving\nthe sufﬁciently low VIU related error using the proposed\nmethod is possible without de teriorating ASR accuracy\nfor other utterances.\n9. Conclusions and future work\nIn this paper, we focused on the problems of constructing\nlanguage models for automatic speech recognition in\nPolish. The main aim of the described works was\nto take into account the speciﬁc features of the\nPolish language when building the language models,\nsuch as rich inﬂection, loose word order, or frequent\nappearance of short words that exhibit the tendency to\nbe falsely removed in the recognized sentence. We\nalso took into account practical issues often encountered\nwhen constructing the language model for the speciﬁc\napplication: the need to extend the model with words\nFig. 2. Comparison of second type error estimates obtained with\nartiﬁcial and human-spoken utterances.\nthat did not appear in the text corpus and the need\nto recognize the small set of selected utterances with\nvery high accuracy. We proposed the pipelined model\nconstruction method, where the initial model is created\nat the ﬁrst stage and then extended or improved at\nsubsequent stages.\nAt the ﬁrst stage we tested various smoothing\nmethods used when building the initial language model:\nabsolute discounting, Good–Turing, Kneser–Ney and\nmodiﬁed Kneser–Ney smoothing. The tests were carried\nout for four subdomains of Polish speech that differ\nin complexity (available amount of texts in the corpus,\nsize of the dictionary). The tests showed that there are\nno signiﬁcant differences in speech recognition accuracy\nobtained when applying language models constructed\nusing the compared techniques. The Kneser-Ney\nmethod showed marginally better performance than other\nmethods, hence it was used as the baseline at further stages\nof model building.\nAt the next stage, the method that takes into account\nthe loose word order in Polish was applied to modify the\nmodel. The method boosts th e probabilitie s of bigrams\nwhich were observed at distant positions in sentences\nbelonging to the corpus. As a result, we obtained small\nbut observable improvement in recognition accuracy in\ndomains where model perplexity is high and speech\nrecognition is difﬁcult. In the case of simple models,\nwhere perplexity is low, model modiﬁcation resulted in\nworsening recognition accuracy. Hence, the practical\nconclusion is that the big ram probability boosting of\ndistant co-occurring words should be applied only to\nmodels of high perplexity.\nThe model modiﬁcation introduced at the third stage\nis aimed at avoidance of short word deletion. Our\nexperiences with practical application of ASR to Polish\nspeech showed that it is one of the most common errors.\nThe methods proposed here achieve the goal by modifying\nthe language model only. They were compared with\nanother method (proposed earlier by the same author)\nwhich carries out short word deletion correction as the\npostprocessing stage. The performance of all compared\nmethods is similar. Short word deletion error rate relative\nreduction is at the level of 40%. The methods proposed\nhere are, however, easier in application, because the effect\ncan be obtained by merely modifying the language model.\nAs a result, the method can be used in any ASR system,\nwithout the need to modify the system logic.\nThe aim of the next stage is to extend the model\nwith additional words not appearing in the corpus.\nTwo methods of model extension with the ﬂat list of\nadditional words were compared. The ﬁrst, simpler\nmethod assigns equal unigram probabilities to all new\nwords. The alternative method assigns probabilities to\nnew unigrams using part-of-speech tagging. Experimental\nevaluation showed that there are no statistically signiﬁcant\nPipelined language model construction for Polish speech recognition 667\ndifferences between the compared methods. Hence\nthe practical recommendation is to use the method that\nassigns uniform probabilities to new words, due to its\nsimplicity.\nAt the last stage a method was proposed that\nmodiﬁes the model so as to assure that the selected\nutterances are recognized with an arbitrarily high level\nof accuracy. The experiments carried out with the set\nof important utterances being the spoken text editor\ncommands proved that the goal can be achieved without\nsigniﬁcant degradation of other utterances recognition\naccuracy.\nThe overall conclusion following from the\nexperiments described here is that the Polish language is\nhard for speech recognition due to its speciﬁc features.\nApplication of methods focused speciﬁcally at individual\nsources of difﬁculties may improve recognition accuracy.\nThe procedure of language model construction presented\nhere is an example of such an approach. Although the\nexperiments were carried out for the Polish language,\nwe believe that similar results can be obtained for other\nlanguages of similar properties, e.g., for Slavonic ones.\nThe concept of the language construction pipeline\ncan be extended with more stages aimed at other language\nfeatures or speciﬁc type of errors in speech recognition.\nA promising direction seems to be at application of\nclass-based models combined with ordinary word-based\nones. A class-based approach relying on POS tagging can\nbe used to exclude speciﬁc sequences of words that are\nalmost impossible to appear in spoken language. This can\nbe useful in eliminating the error consisting in successive\nappearance of short conjunctions or prepositions. This\ntype of error often occurs in utterance fragments being\nshort pauses not detected at the acoustic level.\nExperiments in pattern recognition prove that\napplication of classiﬁer combinations can increase\nrecognition accuracy (Wo´ zniak and Krawczyk, 2012).\nThe classiﬁer combination concept can be also applied\nto ASR. One of the possibilities is to combine\nclassiﬁers based on LMs created using various corpora or\nconstruction techniques. Model combination can be also\nused at the level of LM construction for a single classiﬁer.\nA direction that seems to be insufﬁciently explored in the\ncase of Polish ASR, is a model construction by combining\nsimpler models. It can be applied to the construction of a\ndomain speciﬁc language model by combining a model\ncreated from a relatively small corpus of domain-speciﬁc\ntexts with a domain-independent model built using a\nbigger corpus.\nReferences\nBrown, P., deSouza, P.V ., Mercer, R.L., Pietra, V .J.D. and\nLai, J.C. (1992). Class-based n-gram models of natural\nlanguage, Computational Linguistics 18(1): 467–479.\nBrychcin, T. and Konopik, M. (2011). Morphological based\nlanguage models for inﬂectional languages,Proceedings of\nthe 6th IEEE International Conference on Intelligent Data\nAcquisition and Advanced Computing Systems, Praque,\nCzech Republic, pp. 560–563.\nChen, S. and Goodman, S. (1999). An empirical study of\nsmoothing techniques for language modeling, Computer\nSpeech and Language 1(13): 359–394.\nChen, Y . and Chan, K. (2003). Extended multi-word trigger\npair language model using data mining technique,Systems,\nMan and Cybernetics 1(1): 262–267.\nDevine, E., Gaehde, S. and Curtis, A. (2007). Comparative\nevaluation of three continuousspeech recognition software\npackages in the generation of medical reports, Journal of\nAmerican Medical Informatics Association 1(7): 462–468.\nGale, A. and Sampson, G. (1995). Good–Turing frequency\nestimation without tears, Journal of Quantitative Linguis-\ntics 2(1): 217–239.\nGoodman, J. (2001). A bit of progress in language modeling\nextended version, Technical Report MSR-TR-2001-72 ,\nMachine Learning and Applied Statistics Group, Microsoft\nResearch, Redmond, WA.\nIyer, R. and Ostendorf, M. (1999). Modeling long distance\ndependence in language: Topic mixtures versus dynamic\ncache models, IEEE Transactions on Speech and Audio\nProcessing 7(1): 30–39.\nJelinek, F., Merialdo, B., Roukos, S. and Strauss, M. (2001). A\ndynamic language model for speech recognition, Proceed-\nings of the Workshop on Speech and Natural Language,\nHLT’91, Paciﬁc Grove, CA, USA,pp. 293–295.\nJurafsky, D. and Matrin, J. (2009). Speech and Language Pro-\ncessing. An Introduction to Natural Language Process-\ning, Computational Linguistics and Speech Recognition ,\nPearson Prentice Hall, Englewood Cliffs, NJ.\nKasprzak, W., Wilkowski, A. and Czapnik, K. (2012). Hand\ngesture recognition based on free-form contours and\nprobabilistic inference, International Journal of Applied\nMathematics and Computer Science 22(2): 437–448, DOI:\n10.2478/v10006-012-0033-6.\nKatz, S. (1987). Estimation of p robabilities fro m sparse data\nfor the language model component of a speech recognizer,\nIEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing 35(3): 400–401.\nKoehn, P. (2005). Europarl: A parallel corpus for statistical\nmachine translation, MIT Summit 2005, Phuket, Thailand,\npp. 79–86.\nKolorenc, J., Nouza, J. and Cerva, P. (2006). Multi-words in the\nCzech TV and radio news transcription system, Proceed-\nings of SPECOM 2006, St. Petersburg, Russia, pp. 70–74.\nLee, A., Kawahara, T. and Shikano, K. (2001). Julius—an open\nsource real-time large vocabulary recognition engine, Pro-\nceedings of the European Conference on Speech Commu-\nnication and Technology (EUROSPEECH), Aalborg, Den-\nmark, pp. 1691–1694.\n668 J. Sas and A. ˙Zołnierek\nMauces, M., Rotownik, T. and Zemljak, M. (2003). Modelling\nhighly inﬂected Slovenian language, International Journal\nof Speech Technology1(6): 254–257.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L. and Cernocky,\nJ. (2011). Empirical evaluation and combination of\nadvanced language modeling techniques, INTERSPEECH,\nISCA, Florence, Italy, pp. 605–608.\nNiesler, T., Whittaker, E.W.D. and Woodland, P. (1998).\nComparison of part-of-speech and automatically\nderived category-based language models for speech\nrecognition, Proceedings of ICASSP 98, Seattle, WA, USA,\npp. 177–180.\nPiasecki, M. (2007). Polish tagger TaKIPI: Rule based\nconstruction and optimisation, Task Quarterly\n11(1): 151–167.\nPiasecki, M. and Broda, B. (2007). Correction of medical\nhandwriting OCR based on semantic similarity, in H. Yin,\nP. Tino, E. Corchado, W. Byrne and X. Yao (Eds.),Intelli-\ngent Data Engineering and Automated Learning—IDEAL\n2007, Lecture Notes in Computer Science, V ol. 4881,\nSpringer-Verlag, Heidelberg, pp. 437–446.\nPiasecki, M. and Radziszewski, A. (2008). Morphological\nprediction for Polish by a statistical at e r g oindex, Systems\nScience 34(4): 7–17.\nSarukkai, R. and Ballard, D . (1996). Word s et probability\nboosting for improved spontan eous dialogue recognition.\nThe ab and tab algorithms, Technical Report TR-601 ,\nUniversity of Rochester, New York, NY .\nSas, J. (2009). Optimal spoken dialog control in hands-free\nmedical information systems, Journal of Medical Infor-\nmatics and Technologies 13: 113–120.\nSas, J. (2010). Application of local bidirectional language\nmodel to error correction in Polish medical speech\nrecognition, Journal of Medical Informatics and Technolo-\ngies 15(1): 127–134.\nSas, J. and ˙Zołnierek, A. (2011). Distant co-occurrence language\nmodel for ASR in loose word order languages, Proceed-\nings of the International Conference on Computer Recog-\nnition Systems Cores 2011, Wrocław, Poland, pp. 767–778.\nVaiciunas, A., Kaminskas, V . and Raskinis, G. (2004).\nStatistical language models of Lithuanian based on word\nclustering and morphol ogical decomposition, Informatica\n15(4): 565–580.\nWard, W. and Issar, S. (1996). A class based language model\nfor speech recognition, Acoustics, Speech, and Signal Pro-\ncessing, ICASSP 96, Atlanta, GA, USA, pp. 416–418.\nWhittaker, E. and Woodland, P. (2003). Language modelling for\nRussian and English using words and classes, Computer\nSpeech and Language 17(1): 87–104.\nWoli ´nski, M. (2006). Morfeusz—a practical tool for the\nmorphological analysis of Polish, Inteligent Processing\nand Web Mining: IIPWM 06, Ustro´n, Poland, pp. 503–512.\nWo ´zniak, M. and Krawczyk, B. (2012). Combined classiﬁer\nbased on feature space partitioning,\nInternational Jour-\nnal of Applied Mathematics and Computer Science\n22(4): 855–866, DOI: 10.2478/v10006-012-0063-0.\nYoung, S. and Everman, G. (2009). The HTK Book (for HTK\nVersion 3.4), Cambridge University, Cambridge.\nZi´ołko, B., Skurzok, D. and Zi´ołko, M. (2010). Word n-grams\nfor Polish, Proceedings of the 10th IASTED Interna-\ntional Conference on Artiﬁcial Intelligence and Applica-\ntions (AIA 2010), Innsbruck, Austria, pp. 197–201.\nZi´ołko, J., Gałka, J., Jadczyk, T., Skurzok, D. and Masior, M.\n(2011). Automatic speech rec ognition system dedicated\nfor Polish, Proceedings of the INTERSPEECH 2011 Con-\nference, Florence, Italy, pp. 3315–3316.\nZi´ołko, J., Gałka, J. and Skurzok, D. (2010). Sp eech modelling\nusing phoneme segmentation and modiﬁed weighted\nLevenshtein distance, Proceedings of the ICALP2010 Col-\nloquium, Bordeaux, France, pp. 743–746.\nJerzy Sas received his Ph.D. degree from the\nInstitute of Biocybernetics and Biomedical En-\ngineering of the Polish Academy of Sciences in\n1993. Currently he is an assistant professor at\nthe Institute of Informatics, Wrocław University\nof Technology. His research focuses on speech\nrecognition, medical informatics and photorealis-\ntic computer graphics. Doctor Sas has published\nover 90 papers. He has been involved in many re-\nsearch and developments projects related to med-\nical informatics. He was also the leader of the development team of a\ncommercially available speech recognition system for Polish aimed at\nmedical and mobile applications.\nAndrzej ˙Zołnierek works at the Department\nof Systems and Computer Networks, Faculty of\nElectronics, Wrocław University of Technology.\nHe received his Ph.D. degree from the Institute\nof Biocybernetics and Biomedical Engineering of\nthe Polish Academy of Sciences in 1986. His\nmain interests are in pattern recognition and ar-\ntiﬁcial intelligence and their applications, as well\nas in image and speech analysis in medical diag-\nnosis support. He has published over 40 papers.\nHe has been involved in projects related to application of pattern recog-\nnition to medical diagnosis.\nReceived: 10 March 2012\nRevised: 18 March 2013"
}