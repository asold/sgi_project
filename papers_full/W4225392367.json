{
  "title": "TLMOTE: A Topic-based Language Modelling Approach for Text Oversampling",
  "url": "https://openalex.org/W4225392367",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5071233426",
      "name": "Arjun Choudhry",
      "affiliations": [
        "Delhi Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5077452078",
      "name": "Seba Susan",
      "affiliations": [
        "Delhi Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5070964231",
      "name": "Anmol Bansal",
      "affiliations": [
        "Delhi Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5036343226",
      "name": "Anubhav Sharma",
      "affiliations": [
        "Delhi Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2913694443",
    "https://openalex.org/W6639619044",
    "https://openalex.org/W2148143831",
    "https://openalex.org/W2910152998",
    "https://openalex.org/W2132791018",
    "https://openalex.org/W2104933073",
    "https://openalex.org/W6776535907",
    "https://openalex.org/W3217696222",
    "https://openalex.org/W3015826555",
    "https://openalex.org/W2471991522",
    "https://openalex.org/W2954283529",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2974739457",
    "https://openalex.org/W2946140722",
    "https://openalex.org/W6695662000",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2284289336",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W4250857377"
  ],
  "abstract": "Training machine learning and deep learning models on unbalanced datasets can lead to a bias portrayed by the models towards the majority classes. To tackle the problem of bias towards majority classes, researchers have presented various techniques to oversample the minority class data points. Most of the available state-of-the-art oversampling techniques generate artificial data points which cannot be comprehensibly understood by the reader, despite the synthetic data points generated being similar to the original minority class data points. In this work, we present Topic-based Language Modelling Approach for Text Oversampling (TLMOTE), a novel text oversampling technique for supervised learning from unbalanced datasets. TLMOTE improves upon previous approaches by generating data points which can be intelligibly understood by the reader, can relate to the main topics of the minority class, and introduces more variations to the synthetic data generated. We evaluate the efficacy of our approach on various tasks like Suggestion Mining SemEval 2019 Task 9 Subtasks A and B, SMS Spam Detection, and Sentiment Analysis. Experimental results verify that oversampling unbalanced datasets using TLMOTE yields a higher macro F1 score than with other oversampling techniques.",
  "full_text": "TLMOTE: A Topic-based Language Modelling Approach for Text Oversampling\nArjun Choudhry, Seba Susan, Anmol Bansal, Anubhav Sharma\nDelhi Technological University, New Delhi, India\n{arjunchoudhry 2k18it031, sebasusan, anmolbansal 2k18it025, anubhavsharma 2k18it029}@dtu.ac.in\nAbstract\nTraining machine learning and deep learning models on\nunbalanced datasets can lead to a bias portrayed by the\nmodels towards the majority classes. To tackle the prob-\nlem of bias towards majority classes, researchers have\npresented various techniques to oversample the minor-\nity class data points. Most of the available state-of-\nthe-art oversampling techniques generate artificial data\npoints which cannot be comprehensibly understood by\nthe reader, despite the synthetic data points generated\nbeing similar to the original minority class data points.\nIn this work, we present Topic-based Language Mod-\nelling Approach for Text Oversampling (TLMOTE), a\nnovel text oversampling technique for supervised learn-\ning from unbalanced datasets. TLMOTE improves upon\nprevious approaches by generating data points which\ncan be intelligibly understood by the reader, can re-\nlate to the main topics of the minority class, and in-\ntroduces more variations to the synthetic data gener-\nated. We evaluate the efficacy of our approach on var-\nious tasks like Suggestion Mining SemEval 2019 Task\n9 Subtasks A and B, SMS Spam Detection, and Sen-\ntiment Analysis. Experimental results verify that over-\nsampling unbalanced datasets using TLMOTE yields a\nhigher macro F1 score than with other oversampling\ntechniques.\n1 Introduction\nClass imbalance is one of the most prominent issues sur-\nrounding supervised learning models for years, despite the\nsignificant improvements in classifier architectures and pre-\nprocessing approaches. Classifiers trained on unbalanced\ndatasets tend to be biased towards classes with more data\npoints, or the majority classes, reducing the efficacy of the\nclassifier in predicting the correct label for a given input.\nTo this end, significant work has been done to ameliorate\nthe problem of class imbalance in supervised learning tasks.\nChawla et al. (2002) presented Synthetic Minority Over-\nsampling Technique (SMOTE), a minority class oversam-\npling technique which works in the Euclidean space and has\nbeen widely applied to a wide variety of tasks.\nOver the years, many variations of SMOTE have been in-\ntroduced to improve the viability of data points generated\nCopyright © 2021by the authors. All rights reserved.\nfor the minority classes. He et al. (2008) introduced Adap-\ntive Synthetic Sampling Approach for Imbalanced Learning\n(ADASYN), a synthetic sampling approach which uses a\nweighted distribution for the minority classes based on the\ndifficulty level in their learning. Unlike SMOTE, in which\neach point is given a uniform weight, in ADASYN, a den-\nsity distribution decides the number of synthetic samples\nthat need to be generated for a specific point. Han, Wang,\nand Mao (2005) proposedBorderline-SMOTE, a variation of\nSMOTE in which minority class data points whose neigh-\nbours are all majority class data points are considered as\nnoise points, and are ignored during oversampling.\nSome recent works have also used undersampling of the\nmajority class, or hybrid of oversampling and undersam-\npling approaches for tackling the problem of class imbal-\nance. Susan and Kumar (2018) proposed a hybrid model of\nminority oversampling and Particle Swarm Optimization-\nbased majority undersampling. These SMOTE-based ap-\nproaches are applicable across multiple modalities. There\nhave been some approaches proposed to tackle the problem\nof class imbalance for specific modalities as well. Saini and\nSusan (2019) proposed a minority oversampling approach\nfor the classification of breast cancer histopathological im-\nages, by the generation of synthetic image samples using\naffine transformations. Moreo, Esuli, and Sebastiani (2016)\nproposed Distributed Random Oversampling(DRO), a text\noversampling approach based on assigning a probabilistic\ngenerative function to each sample in the minority class of\nthe training set. Anand et al. (2018) tackled class imbalance\nin Phishing URL Detection datasets using Text Generative\nAdversarial Networks, and generated synthetic URLs simi-\nlar to those in the original dataset. Leekha, Goswami, and\nJain (2020) proposed Language Model-based Oversampling\nTechnique (LMOTE), a minority class oversampling tech-\nnique exclusively for textual data. They found that LMOTE\nperformed comparably to SMOTE, while allowing for a\nmore intuitive understanding of the generated samples.\nIn this work, we introduce Topic-based Language Mod-\nelling approach for Text Oversampling(TLMOTE), an over-\nsampling technique using Latent Dirichlet Allocation-based\nTopic Modelling and Language Modelling for the generation\nof synthetic data points for the minority classes. We evalu-\nate TLMOTE against SMOTE and LMOTE on Suggestion\nMining SemEval 2019 Task 9 Subtasks A and B, SMS Spam\nDetection, and Sentiment Analysis for a variety of machine\nlearning and deep learning classifiers. The organization of\nthis paper is as follows. TLMOTE is presented in Section 2,\nthe experimental results are discussed in Section 3, and the\nconclusions are summarized in Section 4.\n2 Methodology\n2.1 Datasets and Preprocessing\nWe use the SemEval 2019 Task 9 Subtasks A and B datasets\nfor Suggestion Mining (Negi, Daudert, and Buitelaar, 2019),\nSMS Spam Classification dataset (Almeida, Hidalgo, and\nYamakami, 2011) made available in UCI machine learn-\ning repository, and the COVID-19 Vaccine Tweets1 dataset\nfor sentiment analysis. The Suggestion Mining Subtask A\ndataset consists of training and testing data from suggestion\nforums for Windows platform developers, while the Subtask\nB dataset consists of the same training data as Task A, and\ntesting data from the hotel reviews domain. The Covid Vac-\ncine Tweets dataset consists of tweets on various COVID-19\nvaccines, annotated with their correct sentiments. The distri-\nbutions of these datasets are shown in Table 1.\nClass Size\nSugg 2085\nNon-Sugg 6415\nClass Size\nSpam 747\nHam 4827\nClass Size\nNeg 420\nNeut 3680\nPos 1900\nTable 1: Class-wise distribution for Suggestion Mining\ntraining dataset, SMS Spam Classification dataset, and\nCOVID-19 Vaccine Tweets dataset for Sentiment Analysis.\nThe training set for both Suggestion Mining Subtasks A\nand B are same. During pre-processing, we convert the text\nto lower case, and de-contract verbs forms (eg. “I’ll” to\n“I will”). We further remove all punctuation marks before\ntraining and testing the classifiers on the unbalanced and\noversampled data.\n2.2 Observed Shortcomings of LMOTE\nUpon generating synthetic data points using LMOTE for the\nsuggestions class in the suggestion mining train set, we ob-\nserve that some of the instances generated are highly du-\nplicative. This is especially true for synthetic data points\ngenerated using 5-grams with relatively lower frequency.\nDuring the extraction of the most common 5-grams in the\nsuggestions class, the frequency of these 5-grams sharply\ndrops to 1, and we observe 5-grams that occur consecu-\ntively within a data point in the initial data. Oversampling\nusing these 5-grams leads to the generation of multiple near-\nidentical data points, which can lead to model over-fitting.\nSome examples of successive n-grams generated by the\nn-gram generator which led to duplicate data points sam-\npled by the language model, along with the generated syn-\nthetic samples, are shown in Table 2. We observed many\ncases of such instances across all datasets oversampled using\nLMOTE.\n1https://www.kaggle.com/datasciencetool/covid19-vaccine-\ntweets-with-sentiment-annotation\n5-gram Synthetic datapoint generated by LMOTE\ni would expect\nthe store\ni would expect the store to be capable of searching\nthe partial words also or may be more accessible and\nthe rating can be the only send from the camera\nusing more either h264 or mpeg4\nwould expect\nthe store to\nwould expect the store to be capable of searching the\npartial words also or may be more accessible and the\nrating can be the only send from the camera using\nmore either h264 or mpeg4\nexpect the store\nto be\nexpect the store to be capable of searching the partial\nwords also or may be more accessible and the rating\ncan be the only send from the camera using more\neither h264 or mpeg4\nthe store to\nbe capable\nthe store to be capable of searching the partial words\nalso or may be more accessible and the rating can be\nthe only send from the camera using more either\nh264 or mpeg4\nTable 2: Examples of duplicity in the synthetic data points\ngenerated for the suggestion mining dataset using LMOTE,\ndue to consecutively occurring 5-gram seed values.\n2.3 TLMOTE Proposed Procedure\nTo reduce the duplicity of generated data points for the mi-\nnority class, we generate 5-grams which consist of words\nthat relate most to the prominent topics of the minority\nclasses. These words, and thereby the 5-grams are extracted\nusing topic modelling with Latent Dirichlet Allocation (Blei,\nNg, and Jordan, 2003).\nWe further use three language models, which take the last\n4 tokens, 5 tokens and 6 tokens, respectively, as the input\nto predict the next word. We randomly select one word out\nof the three predictions made by the language models as the\nnext word in the sequence, and append it to the end of the\nsequence. This process continues till a random number of\nwords between 20 and 30 are appended after the 5-gram.\nUsing three language models randomly, instead of one, in-\ntroduces more variation into the data points generated, even\nwhen similar 5-grams are taken as the seed.\nFor each minority class of a dataset, TLMOTE follows the\nfollowing procedure to generate synthetic samples:\nFind top α topics in the minority classWe employ topic\nmodelling using Latent Dirichlet Allocation for extracting\nthe (α = 100) most important topics in the minority class\nsamples, using the Gensim package.\nExtract the top 10 keywords Kt for each topic t We\nfurther extract the 10 most prominent keywords Kt for each\ntopic t extracted, and create a list L consisting of the set of\nall top 10 keywords for each topic.\nL = K1 ∪ K2 ∪ ...K100\nExtract the topη 5-grams from the minority class sam-\nples We provide L to the n-gram extractor, and extract η 5-\ngrams, which contain one or more of the keywords in L. η\nequals the deficit of samples in the minority class as com-\npared to the majority class. This ensures that sequences of\n5-grams that occur consecutively do not get extracted in sig-\nnificant numbers, as the chain gets broken when a certain\n5-gram does not contain a keyword from the list L.\nTrain three language models on the minority class\nsamples We train three BiLSTM-based language models on\nthe minority class samples, which predict the probability dis-\ntribution of the next word wx over the entire vocabulary V\nof the minority class samples, based on the previous 4 words\n(wx−4, wx−3, wx−2, wx−1), 5 words ( wx−5, wx−4, wx−3,\nwx−2, wx−1) and 6 words ( wx−6, wx−5, wx−4, wx−3,\nwx−2, wx−1), respectively. The language models are trained\nusing GloVe (Pennington, Socher, and Manning, 2014) 100\ndimension embeddings. The models predict the probability\ndistributions P(wi | wx−4 wx−3 wx−2 wx−1) ∀i ∈ (V ),\nP(wi | wx−5 wx−4 wx−3 wx−2 wx−1) ∀i ∈ (V ), and\nP(wi | wx−6 wx−5 wx−4 wx−3 wx−2 wx−1) ∀i ∈ (V ),\nsuch that:\nwx4 = arg max\nwi\nP(wi | wx−4 wx−3 wx−2 wx−1)\nwx5 = arg max\nwi\nP(wi | wx−5 wx−4 wx−3 wx−2 wx−1)\nwx6 = arg max\nwi\nP(wi | wx−6 wx−5 wx−4 wx−3 wx−2 wx−1)\nwx = random(wx4, wx5, wx6)\nGenerate synthetic samples using the extracted 5-\ngrams and the three language models We generate syn-\nthetic data points using the three language models and a 5-\ngram from the set η, by appending the next word wx for a\nrandom number of times between 20 and 30. This process is\nrepeated for all 5-grams in η.\nThe proposed algorithm for TLMOTE is as follows:\nAlgorithm 1 Topic-based Language Modelling Approach\nfor Text Oversampling (TLMOTE)\nInput:\nMs = {ri ∈ M |ni = {‘minority class sample′}}\nSamples from a minority class M;\nt- Number of topics to extract;\nk- Number of most prominent keywords per topic;\nη- Number of n-grams to use in TLMOTE;\nn- type of n-grams e.g. 5 for 5-grams, etc.;\nN- Number of synthetic samples required.\nOutput: S : N synthetic samples for the minority class\n1: keywords ← TopicModel(Ms, t, k)\n2: topic n grams ← NGrams(Ms, keywords, n =\n5, N)\n3: 4gram LM ← TrainLanguageModel(Ms, n= 4)\n4: 5gram LM ← TrainLanguageModel(Ms, n= 5)\n5: 6gram LM ← TrainLanguageModel(Ms, n= 6)\n6: Initialize S ← Ms\n7: while |S| < N do\n8: seed ← SeedGenerate(topic n grams)\n9: sample ← TLMOTEGenerate(seed, 4gram LM,\n5gram LM, 6gram LM)\n10: S ← S ∪sample\n11: end while\n12: return S\nAlgorithm 1 illustrates our proposed TLMOTE oversam-\npling approach. Some of the procedures used in the algo-\nrithm are described below:\nTopicModel(Ms, t, k): It extracts the top t topics from\nthe minority class samples, and generates the topk keywords\nby weightage for each topic. It returns the combined list of\nthe most prominent keywords for all t topics.\nNGrams(Ms, keywords, n = 5, η = N): It returns\nthe top N 5-grams from the minority class samples, which\ncontain one or more keywords extracted by TopicModel.\nTrainLanguageModel(Ms, n ): It trains an n-gram\nBiLSTM-based language model on Ms.\nSeedGenerate(topic based n grams): It iterates over\nthe list ofn grams provided by NGrams and returns the next\none as seed for each synthetic sample to be generated.\nTLMOTEGenerate(seed, 4gram LM, 5gram LM,\n6gram LM): It takes the 4-gram, 5-gram and 6-gram lan-\nguage models, and an n-gram seed provided by SeedGen-\nerate as input, and generates a synthetic sample for a ran-\ndom length between 25 and 35 tokens as output. Algorithm\n2 summarizes the randomised approach used using three lan-\nguage models for the generation of synthetic samples.\nWe have made the code for TLMOTE available online2.\nAlgorithm 2 TLMOTEGenerate procedure from TLMOTE\nalgorithm\nInput:\nseed - n-gram to be used for generating synthetic sample;\n4gram LM- 4-gram Language Model;\n5gram LM- 5-gram Language Model;\n6gram LM- 6-gram Language Model;\nt- Number of tokens used as input to the language models;\nmin- Minimum length of tokens to be appended to seed in\nsynthetic sample;\nmax- Maximum length of tokens to be appended to seed in\nsynthetic sample;\nOutput: Sent : Synthetic sample generated\n1: length ← random(min, max)\n2: Initialize Sent ← seed\n3: while |Sent| < length do\n4: Initialize words ← [ ]\n5: words ← words ∪\n4gram LM(Last t tokens(Sent, t= 4))\n6: words ← words ∪\n5gram LM(Last t tokens(Sent, t= 5))\n7: if |Sent| >= 6then\n8: words ← words ∪\n6gram LM(Last t tokens(Sent, t= 6))\n9: end if\n10: next token ← RandomWord(words)\n11: Sent ← Concatenate(Sent, next token)\n12: end while\n13: return Sent\nSome of the procedures used in Algorithm 2 are described\nbelow:\n2https://github.com/Arjun7m/TLMOTE\nrandom(min, max): It generates a number between min\nand max.\nLast t tokens(Sent, t): It returns the last t tokens in the\nlist Sent.\nRandomWord(words): It returns one of the tokens in the\nlist words randomly.\nConcatenate(Sent, next token): It appends the gener-\nated next token to the end of Sent.\n2.4 Classification Models\nWe evaluate the efficacy of TLMOTE by training and eval-\nuating a variety of machine learning and deep learning\nclassifiers on the unbalanced datasets, as well as datasets\noversampled using SMOTE, LMOTE, TLMOTE and one\nof its variations. Among machine learning classifiers, we\ntrain and evaluate Random Forest (RF), Support Vector Ma-\nchine (SVM) and Logistic Regression (LR) classifiers, while\namong deep learning architectures, we train and evaluate\nCNN-LSTM (Zhou et al., 2015) neural network, Attention-\nbased Convolutional Bi-LSTM neural network (ACBiL-\nSTM) (Kamyab, Liu, and Adjeisah, 2021) and Attention-\nbased Recurrent Convolutional neural network (ARC) (Guo\net al., 2019).\nWe optimise all machine learning models using Grid-\nSearch, while all deep learning models are optimised using\nEarly Stopping to prevent over-fitting.\n3 Results and Discussion\nThe experiments were performed using Python 3.7 version\nsoftware on an NVIDIA Tesla P100 GPU. We evaluated\nthe performance of a variety of deep learning and machine\nlearning classifiers trained on the unbalanced datasets, as\nwell as datasets oversampled using SMOTE, LMOTE and\nTLMOTE (referred to as TLMOTE (R) in the tables). To\n(A)\n (B)\n(C)\n (D)\nFigure 1: 3-dimensional PCA plots for the Suggestion Min-\ning tasks oversampled using: (A) LMOTE (B) TLMOTE (C)\nLMOTE Reverse View (D) TLMOTE Reverse View.\nassess the impact of using three language models in a ran-\ndomised order, we also oversample the datasets using a vari-\nation of TLMOTE (referred to as TLMOTE (M) in the ta-\nbles), which selects the token with the highest probability\namong the three outputs of the language models, instead of\nchoosing randomly.\nModel Class UnbalancedSMOTELMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1 F1\nSVMNon-suggestion0.91 0.88 0.90 0.91 0.92Suggestion 0.33 0.38 0.36 0.38 0.38Macro avg 0.62 0.63 0.63 0.64 0.65\nRF Non-suggestion0.95 0.91 0.94 0.94 0.94Suggestion 0.32 0.46 0.48 0.51 0.50Macro avg 0.64 0.69 0.71 0.72 0.72\nLR Non-suggestion0.88 0.87 0.88 0.89 0.90Suggestion 0.29 0.31 0.33 0.36 0.38Macro avg 0.58 0.59 0.61 0.63 0.64\nTable 3: Performance evaluation of various Machine Learn-\ning models trained on the SemEval 2019 Task 9 Subtask A.\nModel Class UnbalancedLMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1\nCNN-LSTMNon-suggestion0.92 0.94 0.93 0.95Suggestion 0.49 0.51 0.55 0.58Macro avg 0.70 0.72 0.74 0.76\nACBiLSTMNon-suggestion0.93 0.96 0.94 0.95Suggestion 0.56 0.58 0.61 0.62Macro avg 0.75 0.77 0.78 0.79\nARC Non-suggestion0.95 0.94 0.95 0.95Suggestion 0.54 0.55 0.56 0.58Macro avg 0.74 0.75 0.75 0.77\nTable 4: Performance evaluation of various Deep Learning\nmodels trained on the SemEval 2019 Task 9 Subtask A.\nModel Class UnbalancedSMOTELMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1 F1\nSVMNon-suggestion0.73 0.73 0.72 0.72 0.71Suggestion 0.06 0.21 0.08 0.12 0.17Macro avg 0.39 0.47 0.40 0.42 0.44\nRF Non-suggestion0.74 0.74 0.74 0.74 0.73Suggestion 0.04 0.10 0.10 0.10 0.12Macro avg 0.39 0.42 0.42 0.42 0.43\nLR Non-suggestion0.71 0.71 0.71 0.72 0.71Suggestion 0.22 0.25 0.23 0.23 0.25Macro avg 0.46 0.48 0.47 0.48 0.48\nTable 5: Performance evaluation of various Machine Learn-\ning models trained on the SemEval 2019 Task 9 Subtask B.\nWe use the provided train-test split for the Suggestion\nMining tasks, and randomly split the other datasets in the\nratio 80:20 for training and testing sets of the other datasets.\nFor the sake of a fair comparison, we use the same training\nsplits for all oversampling techniques. Performance evalu-\nations for all combinations of classifiers and oversampling\ntechniques using F1 score for each class, as well as Macro\nF1 score for the total testing set, are illustrated in Tables 3\nand 4 for Suggestion Mining Subtask A, Tables 5 and 7 for\nSuggestion Mining Subtask B, Tables 8 and 9 for the SMS\nSpam Detection task, and Tables 10 and 11 for the Senti-\nment Analysis task. Figure 1 represents Principal Compo-\nnent Analysis (PCA) (Jolliffe, 2002) graphs for the oversam-\npled training sets generated for the suggestion mining task\nusing TLMOTE (R) and LMOTE, plotted using text embed-\ndings generated using a Tf-IDf vectorizer fit on the original\ndata points. Some prominent findings we observed are:\nClassifiers trained on datasets oversampled using TL-\nMOTE outperformed those trained on datasets oversam-\n5-gram Synthetic datapoint generated by TLMOTE\nplease make support for setting please make support for setting set automationid on corewindows and messagedialog could be\nsupported in windows 10 too popular apps are displayed to every phone\nmake support for setting set make support for setting set automationid on corewindows and messagedialog should be\nadded to uwp for wp images should be tailored to be in a specific way\nsupport for setting set automationidsupport for setting set automationid on corewindows and messagedialog should be supported in store\napps like we have download media like for windows 78 for windows 8 plz see this\nTable 6: Examples of synthetic data points generated using TLMOTE for the suggestion mining training set. Data points show\nvariations, even for consecutively occurring 5-gram seed values.\nModel Class UnbalancedLMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1\nCNN-LSTMNon-suggestion0.74 0.75 0.75 0.74Suggestion 0.25 0.27 0.33 0.34Macro avg 0.49 0.51 0.54 0.54\nACBiLSTMNon-suggestion0.75 0.75 0.75 0.75Suggestion 0.29 0.24 0.32 0.32Macro avg 0.52 0.50 0.54 0.53\nARC Non-suggestion0.75 0.74 0.75 0.75Suggestion 0.26 0.20 0.26 0.28Macro avg 0.50 0.47 0.50 0.52\nTable 7: Performance evaluation of various Deep Learning\nmodels trained on the SemEval 2019 Task 9 Subtask B.\nModel Class UnbalancedSMOTELMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1 F1\nSVM Ham 0.98 0.99 0.99 0.99 0.99Spam 0.88 0.96 0.94 0.95 0.97Macro avg0.93 0.98 0.96 0.97 0.98\nRF Ham 0.99 0.99 0.99 0.99 0.99Spam 0.90 0.92 0.94 0.96 0.96Macro avg0.94 0.95 0.96 0.98 0.98\nLR Ham 0.98 0.99 0.99 0.99 0.99Spam 0.88 0.95 0.95 0.96 0.97Macro avg0.93 0.97 0.97 0.98 0.98\nTable 8: Performance evaluation of various Machine Learn-\ning models trained on the SMS Spam Filtering dataset.\npled using SMOTE and LMOTE We observed that clas-\nsifiers trained using TLMOTE outperformed those trained\nusing SMOTE and LMOTE in terms of the macro F1 score\nacross nearly all datasets and model architectures. In most\ncases, we observed that classifiers trained on datasets over-\nsampled using TLMOTE portrayed a 1-5 percent improve-\nment in macro F1 score over their counterparts trained\non datasets oversampled using SMOTE and LMOTE, in-\ndicating significantly reduced bias shown by the classifiers\ntrained on datasets oversampled using TLMOTE towards the\nmajority classes. We saw slightly greater improvements with\nTLMOTE, as compared to LMOTE, in case of deep learn-\ning models (Tables 4, 7, 9 and 11) when compared with ma-\nchine learning models (Tables 3, 5, 8 and 10) on the same\ndatasets. This can be attributed to over-fitting in case of deep\nlearning models when trained on datasets oversampled using\nLMOTE. Due to increased variations in the instances gener-\nated by TLMOTE, deep learning models trained on datasets\noversampled using TLMOTE avoid the issue of overfitting.\nOversampling with three language models in a ran-\ndomized order leads to marginal improvements in the\nperformance of the classifiers We observed that classifiers\ntrained using TLMOTE with three language models, with\n4, 5 and 6 tokens as input length, respectively, and used in\na randomized order (referred to as TLMOTE (R) in the ta-\nModel Class UnbalancedLMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1\nCNN-LSTMHam 0.99 1.00 1.00 1.00Spam 0.95 0.97 0.98 0.98Macro avg0.97 0.98 0.99 0.99\nACBiLSTMHam 0.99 1.00 0.99 1.00Spam 0.96 0.98 0.97 0.98Macro avg0.98 0.99 0.98 0.99\nARC Ham 0.99 0.99 0.99 1.00Spam 0.95 0.97 0.97 0.97Macro avg0.97 0.98 0.98 0.98\nTable 9: Performance evaluation of various Deep Learning\nmodels trained on the SMS Spam Filtering dataset.\nModel Class UnbalancedSMOTELMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1 F1\nSVM Negative 0.00 0.33 0.31 0.32 0.32Neutral 0.82 0.73 0.76 0.77 0.75Positive 0.57 0.62 0.61 0.60 0.63Macro avg0.46 0.56 0.56 0.57 0.57\nRF Negative 0.00 0.18 0.17 0.28 0.28Neutral 0.79 0.79 0.78 0.76 0.79Positive 0.51 0.61 0.57 0.59 0.60Macro avg0.44 0.53 0.51 0.55 0.56\nLR Negative 0.08 0.34 0.36 0.33 0.36Neutral 0.81 0.76 0.79 0.77 0.78Positive 0.60 0.64 0.62 0.63 0.60Macro avg0.49 0.58 0.59 0.57 0.58\nTable 10: Performance evaluation of various Machine\nLearning models trained on the Sentiment Analysis dataset.\nModel Class UnbalancedLMOTETLMOTE(M)TLMOTE(R)F1 F1 F1 F1\nCNN-LSTMNegative 0.19 0.26 0.31 0.28Neutral 0.78 0.75 0.77 0.77Positive 0.59 0.60 0.59 0.60Macro avg0.52 0.54 0.56 0.55\nACBiLSTMNegative 0.11 0.20 0.23 0.25Neutral 0.77 0.77 0.78 0.78Positive 0.55 0.56 0.54 0.56Macro avg0.48 0.51 0.52 0.53\nARC Negative 0.21 0.30 0.23 0.28Neutral 0.75 0.71 0.79 0.77Positive 0.59 0.59 0.59 0.63Macro avg0.52 0.53 0.54 0.56\nTable 11: Performance evaluation of various Deep Learning\nmodels trained on the Sentiment Analysis dataset.\nbles), outperform their counterparts trained on datasets over-\nsampled using TLMOTE (M), where the next word is taken\nfrom that language model which gives the highest probabil-\nity for a word in the probability distribution.\nUpon dimensionality reduction using Principal Com-\nponent Analysis, synthetic samples generated using TL-\nMOTE more closely and uniformly resemble the original\nsamples that those generated using LMOTEWe observed\nthat synthetic samples from TLMOTE more closely follow\nthe boundary of the original samples in the PCA plots in Fig-\nure 1, as compared to those generated using LMOTE, which\nare skewed further away from the original data points. Fur-\nther, the distribution is more uniform in case of TLMOTE\nsamples.\nSynthetic datapoints generated using TLMOTE (R)\nshow significantly lower duplicity than those generated\nusing LMOTE for consecutively occurring 5-gram seeds\nWe observed that synthetic samples generated using TL-\nMOTE (R) in Table 6 show substantial variations even\nfor consecutively occurring 5-grams extracted by the topic-\nbased n-gram generator. This reduces the possibility of the\nclassifiers to overfit on the training set, and thus help im-\nprove performance on the test set.\n4 Conclusion\nIn this work, we presented Topic-based Language Modelling\napproach for Text Oversampling (TLMOTE), a variation of\nLMOTE with three language models with 4, 5 and 6 tokens\nas input, respectively, used in a randomized order, and 5-\ngram seeds extracted based on the words relating to the most\nimportant topics in the minority class data points. Experi-\nmental evaluations show that classifiers trained on datasets\noversampled with TLMOTE outperform those trained on\ndatasets oversampled using other approaches like SMOTE\nand LMOTE in terms of minority class F1 score and macro-\naveraged F1 score.\nReferences\nAlmeida, T. A.; Hidalgo, J. M. G.; and Yamakami, A.\n2011. Contributions to the study of sms spam filter-\ning: New collection and results. In Proceedings of the\n11th ACM Symposium on Document Engineering, Do-\ncEng ’11, 259–262. New York, NY , USA: Association\nfor Computing Machinery.\nAnand, A.; Gorde, K.; Antony Moniz, J. R.; Park, N.;\nChakraborty, T.; and Chu, B.-T. 2018. Phishing url de-\ntection with oversampling based on text generative adver-\nsarial networks. In 2018 IEEE International Conference\non Big Data (Big Data), 1168–1177.\nBlei, D. M.; Ng, A. Y .; and Jordan, M. I. 2003. Latent dirich-\nlet allocation. J. Mach. Learn. Res.3(null):993–1022.\nChawla, N.; Bowyer, K.; Hall, L.; and Kegelmeyer, W. 2002.\nSmote: Synthetic minority over-sampling technique. J.\nArtif. Intell. Res. (JAIR)16:321–357.\nGuo, X.; Zhang, H.; Yang, H.; Xu, L.; and Ye, Z. 2019.\nA single attention-based combination of cnn and rnn for\nrelation classification. IEEE Access7:12467–12475.\nHan, H.; Wang, W.-Y .; and Mao, B.-H. 2005. Borderline-\nsmote: A new over-sampling method in imbalanced data\nsets learning. In Huang, D.-S.; Zhang, X.-P.; and Huang,\nG.-B., eds., Advances in Intelligent Computing, 878–887.\nBerlin, Heidelberg: Springer Berlin Heidelberg.\nHe, H.; Bai, Y .; Garcia, E. A.; and Li, S. 2008. Adasyn:\nAdaptive synthetic sampling approach for imbalanced\nlearning. In 2008 IEEE International Joint Conference\non Neural Networks (IEEE World Congress on Computa-\ntional Intelligence), 1322–1328.\nJolliffe, I. 2002. Principal component analysis. New York:\nSpringer Verlag.\nKamyab, A.; Liu, G.; and Adjeisah, M. 2021. Attention-\nbased cnn and bi-lstm model based on tf-idf and glove\nword embedding for sentiment analysis. Applied Sci-\nences.\nLeekha, M.; Goswami, M.; and Jain, M. 2020. A multi-\ntask approach to open domain suggestion mining using\nlanguage model for text over-sampling. In Lecture Notes\nin Computer Science. Springer International Publishing.\n223–229.\nMoreo, A.; Esuli, A.; and Sebastiani, F. 2016. Distribu-\ntional random oversampling for imbalanced text classifi-\ncation. In Proceedings of the 39th International ACM SI-\nGIR Conference on Research and Development in Infor-\nmation Retrieval, SIGIR ’16, 805–808. New York, NY ,\nUSA: Association for Computing Machinery.\nNegi, S.; Daudert, T.; and Buitelaar, P. 2019. SemEval-2019\ntask 9: Suggestion mining from online reviews and fo-\nrums. In Proceedings of the 13th International Workshop\non Semantic Evaluation, 877–887. Minneapolis, Min-\nnesota, USA: Association for Computational Linguistics.\nPennington, J.; Socher, R.; and Manning, C. 2014. GloVe:\nGlobal vectors for word representation. In Proceedings\nof the 2014 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), 1532–1543. Doha,\nQatar: Association for Computational Linguistics.\nSaini, M., and Susan, S. 2019. Data augmentation of mi-\nnority class with transfer learning for classification of im-\nbalanced breast cancer dataset using inception-v3. In Pat-\ntern Recognition and Image Analysis: 9th Iberian Confer-\nence, IbPRIA 2019, Madrid, Spain, July 1–4, 2019, Pro-\nceedings, Part I, 409–420. Berlin, Heidelberg: Springer-\nVerlag.\nSusan, S., and Kumar, A. 2018. Hybrid of intelligent minor-\nity oversampling and pso-based intelligent majority un-\ndersampling for learning from imbalanced datasets. In\nAbraham, A.; Cherukuri, A. K.; Melin, P.; and Gandhi, N.,\neds., Intelligent Systems Design and Applications - 18th\nInternational Conference on Intelligent Systems Design\nand Applications, ISDA 2018, Vellore, India, December\n6-8, 2018, Volume 2, volume 941 of Advances in Intelli-\ngent Systems and Computing, 760–769. Springer.\nZhou, C.; Sun, C.; Liu, Z.; and Lau, F. C. M. 2015. A c-lstm\nneural network for text classification.",
  "topic": "Oversampling",
  "concepts": [
    {
      "name": "Oversampling",
      "score": 0.9863080978393555
    },
    {
      "name": "Computer science",
      "score": 0.7545403838157654
    },
    {
      "name": "Class (philosophy)",
      "score": 0.7501418590545654
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7187846899032593
    },
    {
      "name": "Machine learning",
      "score": 0.6937121152877808
    },
    {
      "name": "SemEval",
      "score": 0.5642935633659363
    },
    {
      "name": "Task (project management)",
      "score": 0.535454511642456
    },
    {
      "name": "Natural language processing",
      "score": 0.5285478830337524
    },
    {
      "name": "Macro",
      "score": 0.42523109912872314
    },
    {
      "name": "Language model",
      "score": 0.4187600612640381
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}