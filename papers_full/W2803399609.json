{
  "title": "A Comparison of Modeling Units in Sequence-to-Sequence Speech Recognition with the Transformer on Mandarin Chinese",
  "url": "https://openalex.org/W2803399609",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2357468579",
      "name": "Zhou, Shiyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743815191",
      "name": "Dong, Linhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2067510015",
      "name": "Xu Shuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1937328933",
      "name": "Xu Bo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2750499125",
    "https://openalex.org/W2514969556",
    "https://openalex.org/W2627092829",
    "https://openalex.org/W2800434466",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963920996",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2775304348",
    "https://openalex.org/W2749514303",
    "https://openalex.org/W2963882470",
    "https://openalex.org/W2513938599",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2147768505",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2775766866",
    "https://openalex.org/W1525636403"
  ],
  "abstract": "The choice of modeling units is critical to automatic speech recognition (ASR) tasks. Conventional ASR systems typically choose context-dependent states (CD-states) or context-dependent phonemes (CD-phonemes) as their modeling units. However, it has been challenged by sequence-to-sequence attention-based models, which integrate an acoustic, pronunciation and language model into a single neural network. On English ASR tasks, previous attempts have already shown that the modeling unit of graphemes can outperform that of phonemes by sequence-to-sequence attention-based model. In this paper, we are concerned with modeling units on Mandarin Chinese ASR tasks using sequence-to-sequence attention-based models with the Transformer. Five modeling units are explored including context-independent phonemes (CI-phonemes), syllables, words, sub-words and characters. Experiments on HKUST datasets demonstrate that the lexicon free modeling units can outperform lexicon related modeling units in terms of character error rate (CER). Among five modeling units, character based model performs best and establishes a new state-of-the-art CER of $26.64\\%$ on HKUST datasets without a hand-designed lexicon and an extra language model integration, which corresponds to a $4.8\\%$ relative improvement over the existing best CER of $28.0\\%$ by the joint CTC-attention based encoder-decoder network.",
  "full_text": "A Comparison of Modeling Units in Sequence-to-Sequence Speech Recognition\nwith the Transformer on Mandarin Chinese\nShiyu Zhou1,2, Linhao Dong1,2, Shuang Xu1, Bo Xu1\n1Institute of Automation, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n{zhoushiyu2013, donglinhao2015, shuang.xu, xubo}@ia.ac.cn\nAbstract\nThe choice of modeling units is critical to automatic speech\nrecognition (ASR) tasks. Conventional ASR systems typi-\ncally choose context-dependent states (CD-states) or context-\ndependent phonemes (CD-phonemes) as their modeling units.\nHowever, it has been challenged by sequence-to-sequence\nattention-based models, which integrate an acoustic, pronunci-\nation and language model into a single neural network. On En-\nglish ASR tasks, previous attempts have already shown that the\nmodeling unit of graphemes can outperform that of phonemes\nby sequence-to-sequence attention-based model.\nIn this paper, we are concerned with modeling units\non Mandarin Chinese ASR tasks using sequence-to-sequence\nattention-based models with the Transformer. Five model-\ning units are explored including context-independent phonemes\n(CI-phonemes), syllables, words, sub-words and characters.\nExperiments on HKUST datasets demonstrate that the lexicon\nfree modeling units can outperform lexicon related modeling\nunits in terms of character error rate (CER). Among ﬁve model-\ning units, character based model performs best and establishes a\nnew state-of-the-art CER of 26.64% on HKUST datasets with-\nout a hand-designed lexicon and an extra language model in-\ntegration, which corresponds to a 4.8% relative improvement\nover the existing best CER of28.0% by the joint CTC-attention\nbased encoder-decoder network.\nIndex Terms: ASR, multi-head attention, modeling units,\nsequence-to-sequence, Transformer\n1. Introduction\nConventional ASR systems consist of three independent com-\nponents: an acoustic model (AM), a pronunciation model (PM)\nand a language model (LM), all of which are trained indepen-\ndently. CD-states and CD-phonemes are dominant as their mod-\neling units in such systems [1, 2, 3]. However, it recently has\nbeen challenged by sequence-to-sequence attention-based mod-\nels. These models are commonly comprised of an encoder,\nwhich consists of multiple recurrent neural network (RNN) lay-\ners that model the acoustics, and a decoder, which consists of\none or more RNN layers that predict the output sub-word se-\nquence. An attention layer acts as the interface between the\nencoder and the decoder: it selects frames in the encoder repre-\nsentation that the decoder should attend to in order to predict the\nnext sub-word unit [4]. In [5], Tara et al. experimentally ver-\niﬁed that the grapheme-based sequence-to-sequence attention-\nbased model can outperform the corresponding phoneme-based\nmodel on English ASR tasks. This work is very interesting\nand amazing since a hand-designed lexicon might be removed\nfrom ASR systems. As we known, it is very laborious and\ntime-consuming to generate a pronunciation lexicon. Without\na hand-designed lexicon, the design of ASR systems would\nbe simpliﬁed greatly. Furthermore, the latest work shows that\nattention-based encoder-decoder architecture achieves a new\nstate-of-the-art WER on a12500 hour English voice search task\nusing the word piece models (WPM), which are sub-word units\nranging from graphemes all the way up to entire words [6].\nSince the outstanding performance of grapheme-based\nmodeling units on English ASR tasks, we conjecture that maybe\nthere is no need for a hand-designed lexicon on Mandarin Chi-\nnese ASR tasks as well by sequence-to-sequence attention-\nbased models. In Mandarin Chinese, if a hand-designed lexicon\nis removed, the modeling units can be words, sub-words and\ncharacters. Character-based sequence-to-sequence attention-\nbased models have been investigated on Mandarin Chinese ASR\ntasks in [7, 8], but the performance comparison with differ-\nent modeling units are not explored before. Building on our\nwork [9], which shows that syllable based model with the Trans-\nformer can perform better than CI-phoneme based counterpart,\nwe investigate ﬁve modeling units on Mandarin Chinese ASR\ntasks, including CI-phonemes, syllables (pinyins with tones),\nwords, sub-words and characters. The Transformer is chosen\nto be the basic architecture of sequence-to-sequence attention-\nbased model in this paper [9, 10]. Experiments on HKUST\ndatasets conﬁrm our hypothesis that the lexicon free modeling\nunits, i.e. words, sub-words and characters, can outperform\nlexicon related modeling units, i.e. CI-phonemes and sylla-\nbles. Among ﬁve modeling units, character based model with\nthe Transformer achieves the best result and establishes a new\nstate-of-the-art CER of 26.64% on HKUST datasets without a\nhand-designed lexicon and an extra language model integration,\nwhich is a 4.8% relative reduction in CER compared to the\nexisting best CER of 28.0% by the joint CTC-attention based\nencoder-decoder network with a separate RNN-LM integration\n[11].\nThe rest of the paper is organized as follows. After an\noverview of the related work in Section 2, Section 3 describes\nthe proposed method in detail. we then show experimental re-\nsults in Section 4 and conclude this work in Section 5.\n2. Related work\nSequence-to-sequence attention-based models have achieved\npromising results on English ASR tasks and various model-\ning units have been studied recently, such as CI-phonemes,\nCD-phonemes, graphemes and WPM [4, 5, 6, 12]. In [5],\nTara et al. ﬁrst explored sequence-to-sequence attention-based\nmodel trained with phonemes for ASR tasks and compared\nthe modeling units of graphemes and phonemes. They ex-\nperimentally veriﬁed that the grapheme-based sequence-to-\nsequence attention-based model can outperform the corre-\nsponding phoneme-based model on English ASR tasks. Fur-\narXiv:1805.06239v2  [eess.AS]  18 May 2018\nthermore, the modeling units of WPM have been explored in\n[6], which are sub-word units ranging from graphemes all the\nway up to entire words. It achieved a new state-of-the-art WER\non a 12500 hour English voice search task.\nAlthough sequence-to-sequence attention-based models\nperform very well on English ASR tasks, related works are quite\nfew on Mandarin Chinese ASR tasks. Chan et al. ﬁrst proposed\nCharacter-Pinyin sequence-to-sequence attention-based model\non Mandarin Chinese ASR tasks. The Pinyin information was\nused during training for improving the performance of the char-\nacter model. Instead of using joint Character-Pinyin model, [8]\ndirectly used Chinese characters as network output by mapping\nthe one-hot character representation to an embedding vector via\na neural network layer. What’s more, [13] compared the mod-\neling units of characters and syllables by sequence-to-sequence\nattention-based models.\nBesides the modeling unit of character, the modeling units\nof words and sub-words are investigated on Mandarin Chinese\nASR tasks in this paper. Sub-word units encoded by byte pair\nencoding (BPE) have been explored on neural machine transla-\ntion (NMT) tasks to address out-of-vocabulary (OOV) problem\non open-vocabulary translation [14], which iteratively replace\nthe most frequent pair of characters with a single, unused sym-\nbol. We extend it to Mandarin Chinese ASR tasks. BPE is ca-\npable of encoding an open vocabulary with a compact symbol\nvocabulary of variable-length sub-word units, which requires no\nshortlist.\n3. System overview\n3.1. ASR Transformer model architecture\nThe Transformer model architecture is the same as sequence-to-\nsequence attention-based models except relying entirely on self-\nattention and position-wise, fully connected layers for both the\nencoder and decoder [15]. The encoder maps an input sequence\nof symbol representations x = (x1,...,x n) to a sequence of con-\ntinuous representations z = (z1,...,z n). Given z, the decoder\nthen generates an output sequence y = (y1,...,y m) of symbols\none element at a time.\nThe ASR Transformer architecture used in this work is the\nsame as our work [9] which is shown in Figure 1. It stacks\nmulti-head attention (MHA) [15] and position-wise, fully con-\nnected layers for both the encode and decoder. The encoder is\ncomposed of a stack of N identical layers. Each layer has two\nsub-layers. The ﬁrst is a MHA, and the second is a position-\nwise fully connected feed-forward network. Residual connec-\ntions are employed around each of the two sub-layers, followed\nby a layer normalization. The decoder is similar to the encoder\nexcept inserting a third sub-layer to perform a MHA over the\noutput of the encoder stack. To prevent leftward information\nﬂow and preserve the auto-regressive property in the decoder,\nthe self-attention sub-layers in the decoder mask out all values\ncorresponding to illegal connections. In addition, positional en-\ncodings [15] are added to the input at the bottoms of these en-\ncoder and decoder stacks, which inject some information about\nthe relative or absolute position of the tokens in the sequence.\nThe difference between the NMT Transformer [15] and the\nASR Transformer is the input of the encoder. we add a linear\ntransformation with a layer normalization to convert the log-\nMel ﬁlterbank feature to the model dimension dmodel for di-\nmension matching, which is marked out by a dotted line in Fig-\nure 1.\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nFeed\nForward\nAdd & Norm\nAdd & Norm\nMasked\nMulti-Head\nAttention\nK V Q\nPositional\nEncoding\nOutput\nEmbedding\nMulti-Head\nAttention\nAdd & Norm\nAdd & Norm\nOutputs\n(shifted right)\nFeed\nForward\nAdd & Norm\nK V Q\nLinear\nOutput\nProbabilities\nSoftmax\nN×\nN×\nFbank\nDim & Norm\nFigure 1: The architecture of the ASR Transformer.\n3.2. Modeling units\nFive modeling units are compared on Mandarin Chinese ASR\ntasks, including CI-phonemes, syllables, words, sub-words and\ncharacters. Table 1 summarizes the different number of output\nunits investigated by this paper. We show an example of various\nmodeling units in Table 2.\nTable 1: Different modeling units explored in this paper.\nModeling units Number of outputs\nCI-phonemes 122\nSyllables 1388\nCharacters 3900\nSub-words 11039\nWords 28444\n3.2.1. CI-phoneme and syllable units\nCI-phoneme and syllable units are compared in our work [9],\nwhich 118 CI-phonemes without silence (phonemes with tones)\nare employed in the CI-phoneme based experiments and 1384\nsyllables (pinyins with tones) in the syllable based experi-\nments. Extra tokens (i.e. an unknown token ( <UNK>), a\npadding token ( <PAD>), and sentence start and end tokens\n(<S>/<\\S>)) are appended to the outputs, making the to-\ntal number of outputs 122 and 1388 respectively in the CI-\nphoneme based model and syllable based model. Standard tied-\nstate cross-word triphone GMM-HMMs are ﬁrst trained with\nmaximum likelihood estimation to generate CI-phoneme align-\nments on training set. Then syllable alignments are generated\nthrough these CI-phoneme alignments according to the lexicon,\nwhich can handle multiple pronunciations of the same word in\nMandarin Chinese.\nThe outputs are CI-phoneme sequences or syllable se-\nquences during decoding stage. In order to convert CI-phoneme\nsequences or syllable sequences into word sequences, a greedy\ncascading decoder with the Transformer [9] is proposed. First,\nthe best CI-phoneme or syllable sequence sis calculated by the\nASR Transformer from observationXwith a beam size β. And\nthen, the best word sequence W is chosen by the NMT Trans-\nformer from the best CI-phoneme or syllable sequence swith a\nbeam size γ. Through cascading these two Transformer models,\nwe assume that Pr(W|X) can be approximated.\nHere the beam size β = 13and γ = 6are employed in this\nwork.\n3.2.2. Sub-word units\nSub-word units, using in this paper, are generated by BPE 1\n[14], which iteratively merges the most frequent pair of char-\nacters or character sequences with a single, unused symbol.\nFirstly, the symbol vocabulary with the character vocabulary\nis initialized, and each word is represented as a sequence of\ncharacters plus a special end-of-word symbol ‘@@’, which al-\nlows to restore the original tokenization. Then, all symbol pairs\nare counted iteratively and each occurrence of the most frequent\npair (‘A’, ‘B’) are replaced with a new symbol ‘AB’. Each merge\noperation produces a new symbol which represents a character\nn-gram. Frequent character n-grams (or whole words) are even-\ntually merged into a single symbol. Then the ﬁnal symbol vo-\ncabulary size is equal to the size of the initial vocabulary, plus\nthe number of merge operations , which is the hyperparameter\nof this algorithm [14].\nBPE is capable of encoding an open vocabulary with a\ncompact symbol vocabulary of variable-length sub-word units,\nwhich requires no shortlist. After encoded by BPE, the sub-\nword units are ranging from characters all the way up to entire\nwords. Thus there are no OOV words with BPE and high fre-\nquent sub-words can be preserved.\nIn our experiments, we choose the number of merge oper-\nations 5000, which generates the number of sub-words units\n11035 from the training transcripts. After appended with 4 ex-\ntra tokens, the total number of outputs is 11039.\n3.2.3. Word and character units\nFor word units, we collect all words from the training tran-\nscripts. Appended with 4 extra tokens, the total number of out-\nputs is 28444.\nFor character units, all Mandarin Chinese characters to-\ngether with English words in training transcripts are collected,\nwhich are appended with 4 extra tokens to generate the total\nnumber of outputs 3900 2.\n1https://github.com/rsennrich/subword-nmt\n2we manually delete two tokens · and +, which are not Mandarin\nChinese characters.\n4. Experiment\n4.1. Data\nThe HKUST corpus (LDC2005S15, LDC2005T32), a corpus\nof Mandarin Chinese conversational telephone speech, is col-\nlected and transcribed by Hong Kong University of Science and\nTechnology (HKUST) [16], which contains 150-hour speech,\nand 873 calls in the training set and 24 calls in the test set. All\nexperiments are conducted using 80-dimensional log-Mel ﬁlter-\nbank features, computed with a 25ms window and shifted every\n10ms. The features are normalized via mean subtraction and\nvariance normalization on the speaker basis. Similar to [17, 18],\nat the current framet, these features are stacked with 3 frames to\nthe left and downsampled to a 30ms frame rate. As in [11], we\ngenerate more training data by linearly scaling the audio lengths\nby factors of 0.9 and 1.1 (speed perturb.), which can improve\nthe performance in our experiments.\nTable 2: An example of various modeling units in this paper.\nModeling units Example\nCI-phonemes Y IY1 JH UH3 NG3 X IY4 N4 N IY4 AE4 N4\nSyllables YI1 ZHONG3 XIN4 NIAN4\nCharacters 一 种 信 念\nSub-words 一种 信@@ 念\nWords 一种 信念\n4.2. Training\nWe perform our experiments on the base model and big model\n(i.e. D512-H8 and D1024-H16 respectively) of the Transformer\nfrom [15]. The basic architecture of these two models is the\nsame but different parameters setting. Table 3 lists the experi-\nmental parameters between these two models. The Adam algo-\nrithm [19] with gradient clipping and warmup is used for opti-\nmization. During training, label smoothing of value ϵls = 0.1\nis employed [20]. After trained, the last 20 checkpoints are av-\neraged to make the performance more stable [15].\nTable 3: Experimental parameters conﬁguration.\nmodel N dmodel h dk dv warmup\nD512-H8 6 512 8 64 64 4000 steps\nD1024-H16 6 1024 16 64 64 12000 steps\nIn the CI-phoneme and syllable based model, we cascade\nan ASR Transformer and a NMT Transformer to generate word\nsequences from observation X. However, we do not employ a\nNMT Transformer anymore in the word, sub-word and charac-\nter based model, since the beam search results from the ASR\nTransformer are already the Chinese character level. The total\nparameters of different modeling units list in Table 4.\n4.3. Results\nAccording to the description from Section 3.2, we can see that\nthe modeling units of words, sub-words and characters are lex-\nicon free, which do not need a hand-designed lexicon. On the\ncontrary, the modeling units of CI-phonemes and syllables need\na hand-designed lexicon.\nOur results are summarized in Table 5. It is clear to see\nthat the lexicon free modeling units, i.e. words, sub-words and\nTable 4: Total parameters of different modeling units.\nmodel D512-H8\n(ASR)\nD1024-H16\n(ASR)\nD512-H8\n(NMT)\nCI-phonemes 57M 227M 71M\nSyllables 58M 228M 72M\nWords 71M 256M −\nSub-words 63M 238M −\nCharacters 59M 231M −\ncharacters, can outperform corresponding lexicon related mod-\neling units, i.e. CI-phonemes and syllables on HKUST datasets.\nIt conﬁrms our hypothesis that we can remove the need for\na hand-designed lexicon on Mandarin Chinese ASR tasks by\nsequence-to-sequence attention-based models. What’s more,\nwe note here that the sub-word based model performs better\nthan the word based counterpart. It represents that the modeling\nunit of sub-words is superior to that of words, since sub-word\nunits encoded by BPE have fewer number of outputs and with-\nout OOV problems. However, the sub-word based model per-\nforms worse than the character based model. The possible rea-\nson is that the modeling unit of sub-words is bigger than that of\ncharacters which is difﬁcult to train. We will conduct our exper-\niments on larger datasets and compare the performance between\nthe modeling units of sub-words and characters in future work.\nFinally, among ﬁve modeling units, character based model with\nthe Transformer achieves the best result. It demonstrates that\nthe modeling unit of character is suitable for Mandarin Chi-\nnese ASR tasks by sequence-to-sequence attention-based mod-\nels, which can simplify the design of ASR systems greatly.\nTable 5: Comparison of different modeling units with the Trans-\nformer on HKUST datasets in CER (%).\nModeling units Model CER\nCI-phonemes [9]\nD512-H8 32.94\nD1024-H16 30.65\nD1024-H16 (speed perturb) 30.72\nSyllables [9]\nD512-H8 31.80\nD1024-H16 29.87\nD1024-H16 (speed perturb) 28.77\nWords\nD512-H8 31.98\nD1024-H16 28.74\nD1024-H16 (speed perturb) 27.42\nSub-words\nD512-H8 30.22\nD1024-H16 28.28\nD1024-H16 (speed perturb) 27.26\nCharacters\nD512-H8 29.00\nD1024-H16 27.70\nD1024-H16 (speed perturb) 26.64\n4.4. Comparison with previous works\nIn Table 6, we compare our experimental results to other model\narchitectures from the literature on HKUST datasets. First, we\ncan ﬁnd that our best results of different modeling units are\ncomparable or superior to the best result by the deep multidi-\nmensional residual learning with 9 LSTM layers [21], which is\na hybrid LSTM-HMM system with the modeling unit of CD-\nstates. We can observe that the best CER 26.64% of char-\nacter based model with the Transformer on HKUST datasets\nachieves a 13.4% relative reduction compared to the best CER\nof 30.79% by the deep multidimensional residual learning with\n9 LSTM layers. It shows the superiority of the sequence-to-\nsequence attention-based model compared to the hybrid LSTM-\nHMM system.\nMoreover, we can note that our best results with the model-\ning units of words, sub-words and characters are superior to the\nexisting best CER of 28.0% by the joint CTC-attention based\nencoder-decoder network with a separate RNN-LM integration\n[11], which is the state-of-the-art on HKUST datasets to the best\nof our knowledge. Character based model with the Transformer\nestablishes a new state-of-the-art CER of 26.64% on HKUST\ndatasets without a hand-designed lexicon and an extra language\nmodel integration, which is a 7.8% relative reduction in CER\ncompared to the CER of28.9% of the joint CTC-attention based\nencoder-decoder network when no external language model is\nused, and a 4.8% relative reduction in CER compared to the\nexisting best CER of 28.0% by the joint CTC-attention based\nencoder-decoder network with separate RNN-LM [11].\nTable 6: CER (%) on HKUST datasets compared to previous\nworks.\nmodel CER\nLSTMP-9×800P512-F444 [21] 30.79\nCTC-attention+joint dec. (speed perturb., one-pass)\n+VGG net\n+RNN-LM (separate) [11]\n28.9\n28.0\nCI-phonemes-D1024-H16 [9] 30.65\nSyllables-D1024-H16 (speed perturb) [9] 28.77\nWords-D1024-H16 (speed perturb) 27.42\nSub-words-D1024-H16 (speed perturb) 27.26\nCharacters-D1024-H16 (speed perturb) 26.64\n5. Conclusions\nIn this paper we compared ﬁve modeling units on Mandarin\nChinese ASR tasks by sequence-to-sequence attention-based\nmodel with the Transformer, including CI-phonemes, syllables,\nwords, sub-words and characters. We experimentally veriﬁed\nthat the lexicon free modeling units, i.e. words, sub-words and\ncharacters, can outperform lexicon related modeling units, i.e.\nCI-phonemes and syllables on HKUST datasets. It represents\nthat maybe we can remove the need for a hand-designed lexi-\ncon on Mandarin Chinese ASR tasks by sequence-to-sequence\nattention-based models. Among ﬁve modeling units, charac-\nter based model achieves the best result and establishes a new\nstate-of-the-art CER of 26.64% on HKUST datasets without a\nhand-designed lexicon and an extra language model integration,\nwhich corresponds to a 4.8% relative improvement over the\nexisting best CER of 28.0% by the joint CTC-attention based\nencoder-decoder network. Moreover, we ﬁnd that sub-word\nbased model with the Transformer, encoded by BPE, achieves\na promising result, although it is slightly worse than character\nbased counterpart.\n6. Acknowledgements\nThe authors would like to thank Chunqi Wang and Feng Wang\nfor insightful discussions.\n7. References\n[1] G. E. Dahl, D. Yu, L. Deng, and A. Acero, “Context-dependent\npre-trained deep neural networks for large-vocabulary speech\nrecognition,” IEEE Transactions on audio, speech, and language\nprocessing, vol. 20, no. 1, pp. 30–42, 2012.\n[2] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory re-\ncurrent neural network architectures for large scale acoustic mod-\neling,” in Fifteenth annual conference of the international speech\ncommunication association, 2014.\n[3] A. Senior, H. Sak, and I. Shafran, “Context dependent phone mod-\nels for lstm rnn acoustic modelling,” inAcoustics, Speech and Sig-\nnal Processing (ICASSP), 2015 IEEE International Conference\non. IEEE, 2015, pp. 4585–4589.\n[4] R. Prabhavalkar, T. N. Sainath, B. Li, K. Rao, and N. Jaitly, “An\nanalysis of attention in sequence-to-sequence models,,” in Proc.\nof Interspeech, 2017.\n[5] T. N. Sainath, R. Prabhavalkar, S. Kumar, S. Lee, A. Kannan,\nD. Rybach, V . Schogol, P. Nguyen, B. Li, Y . Wuet al., “No need\nfor a lexicon? evaluating the value of the pronunciation lexica in\nend-to-end models,”arXiv preprint arXiv:1712.01864, 2017.\n[6] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Goninaet al., “State-\nof-the-art speech recognition with sequence-to-sequence models,”\narXiv preprint arXiv:1712.01769, 2017.\n[7] W. Chan and I. Lane, “On online attention-based speech recog-\nnition and joint mandarin character-pinyin training.” in INTER-\nSPEECH, 2016, pp. 3404–3408.\n[8] C. Shan, J. Zhang, Y . Wang, and L. Xie, “Attention-based end-to-\nend speech recognition on voice search.”\n[9] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-Based Sequence-\nto-Sequence Speech Recognition with the Transformer in Man-\ndarin Chinese,”ArXiv e-prints, Apr. 2018.\n[10] B. X. Linhao Dong, Shuang Xu, “Speech-transformer: A no-\nrecurrence sequence-to-sequence model for speech recognition,”\nin Acoustics, Speech and Signal Processing (ICASSP), 2018 IEEE\nInternational Conference on. IEEE, 2018, pp. 5884–5888.\n[11] T. Hori, S. Watanabe, Y . Zhang, and W. Chan, “Advances in joint\nctc-attention based end-to-end speech recognition with a deep cnn\nencoder and rnn-lm,”arXiv preprint arXiv:1706.02737, 2017.\n[12] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, “A comparison of sequence-to-sequence models for\nspeech recognition,” inProc. Interspeech, 2017, pp. 939–943.\n[13] W. Zou, D. Jiang, S. Zhao, and X. Li, “A comparable study\nof modeling units for end-to-end mandarin speech recognition,”\narXiv preprint arXiv:1805.03832, 2018.\n[14] R. Sennrich, B. Haddow, and A. Birch, “Neural machine\ntranslation of rare words with subword units,” arXiv preprint\narXiv:1508.07909, 2015.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n6000–6010.\n[16] Y . Liu, P. Fung, Y . Yang, C. Cieri, S. Huang, and D. Graff,\n“Hkust/mts: A very large scale mandarin telephone speech cor-\npus,” in Chinese Spoken Language Processing. Springer, 2006,\npp. 724–735.\n[17] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate\nrecurrent neural network acoustic models for speech recognition,”\narXiv preprint arXiv:1507.06947, 2015.\n[18] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, “An analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,” arXiv preprint\narXiv:1712.01996, 2017.\n[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,”arXiv preprint arXiv:1412.6980, 2014.\n[20] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-\nthinking the inception architecture for computer vision,” in Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016, pp. 2818–2826.\n[21] Y . Zhao, S. Xu, and B. Xu, “Multidimensional residual learning\nbased on recurrent neural networks for acoustic modeling,” Inter-\nspeech 2016, pp. 3419–3423, 2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.766057014465332
    },
    {
      "name": "Pronunciation",
      "score": 0.7383553981781006
    },
    {
      "name": "Language model",
      "score": 0.7268198132514954
    },
    {
      "name": "Mandarin Chinese",
      "score": 0.7037135362625122
    },
    {
      "name": "Speech recognition",
      "score": 0.7008256316184998
    },
    {
      "name": "Transformer",
      "score": 0.6895354986190796
    },
    {
      "name": "Lexicon",
      "score": 0.6749913692474365
    },
    {
      "name": "Natural language processing",
      "score": 0.5636725425720215
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5597655177116394
    },
    {
      "name": "Word error rate",
      "score": 0.5170674920082092
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5147852897644043
    },
    {
      "name": "Hidden Markov model",
      "score": 0.5018751621246338
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4741223156452179
    },
    {
      "name": "Artificial neural network",
      "score": 0.45276960730552673
    },
    {
      "name": "Encoder",
      "score": 0.42279738187789917
    },
    {
      "name": "Context model",
      "score": 0.4118933379650116
    },
    {
      "name": "Linguistics",
      "score": 0.1517808437347412
    },
    {
      "name": "Engineering",
      "score": 0.07936078310012817
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}