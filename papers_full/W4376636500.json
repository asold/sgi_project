{
  "title": "Clinical Knowledge and Reasoning Abilities of AI Large Language Models in Anesthesiology: A Comparative Study on the ABA Exam",
  "url": "https://openalex.org/W4376636500",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3117986091",
      "name": "Mirana C. Angel",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A4377385750",
      "name": "Joseph B. Rinehart",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A4377385751",
      "name": "Maxime P. Canneson",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2060797211",
      "name": "Pierre Baldi",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A3117986091",
      "name": "Mirana C. Angel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4377385750",
      "name": "Joseph B. Rinehart",
      "affiliations": [
        "University of California, Irvine",
        "Irvine University"
      ]
    },
    {
      "id": "https://openalex.org/A4377385751",
      "name": "Maxime P. Canneson",
      "affiliations": [
        "Los Angeles Medical Center",
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2060797211",
      "name": "Pierre Baldi",
      "affiliations": [
        "Irvine University",
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4255889725",
    "https://openalex.org/W2809596283",
    "https://openalex.org/W2802159733",
    "https://openalex.org/W3049028910",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4323660312",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4365211596",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3158419028",
    "https://openalex.org/W4221163727",
    "https://openalex.org/W4327810286",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W1992232496",
    "https://openalex.org/W2123003455",
    "https://openalex.org/W2939142069",
    "https://openalex.org/W2486676057"
  ],
  "abstract": "Abstract Over the past decade, Artificial Intelligence (AI) has expanded significantly with increased adoption across various industries, including medicine. Recently, AI’s large language models such as GPT-3, Bard, and GPT-4 have demonstrated remarkable language capabilities. While previous studies have explored their potential in general medical knowledge tasks, here we assess their clinical knowledge and reasoning abilities in a specialized medical context. We study and compare their performances on both the written and oral portions of the comprehensive and challenging American Board of Anesthesiology (ABA) exam, which evaluates candidates’ knowledge and competence in anesthesia practice. In addition, we invited two board examiners to evaluate AI’s answers without disclosing to them the origin of those responses. Our results reveal that only GPT-4 successfully passed the written exam, achieving an accuracy of 78% on the basic section and 80% on the advanced section. In comparison, the less recent or smaller GPT-3 and Bard models scored 58% and 47% on the basic exam, and 50% and 46% on the advanced exam, respectively. Consequently, only GPT-4 was evaluated in the oral exam, with examiners concluding that it had a high likelihood of passing the actual ABA exam. Additionally, we observe that these models exhibit varying degrees of proficiency across distinct topics, which could serve as an indicator of the relative quality of information contained in the corresponding training datasets. This may also act as a predictor for determining which anesthesiology subspecialty is most likely to witness the earliest integration with AI.",
  "full_text": "1 \n \nClinical Knowledge and Reasoning Abilities of AI Large Language Models in \nAnesthesiology: A Comparative Study on the ABA Exam  \n \nMirana C. Angel MSc1,2, Joseph B. Rinehart MD3, Maxime P. Canneson MD PhD4, Pierre Baldi \nPhD1,2,* \n1. Department of Computer Science, University of California Irvine, Irvine, CA 92697, USA \n2. Institute for Genomics and Bioinformatics, University of California Irvine, Irvine CA 92697, USA \n3. Department of Anesthesiology & Perioperative Care, University of California Irvine, Irvine CA 92697, USA \n4. Department of Anesthesiology & Perioperative Medicine, University of California, Los Angeles, Los \nAngeles, CA 90095 \n \nThe authors declare no conflicts of interest. \nFunding: NIH R01EB029751 (Cannesson, Baldi, and Rinehart). \nAddress correspondence to:  \nJoseph Rinehart MD, UCI Health Anesthesia, Orange, CA 92868, United States, e-mail: jrinehar@hs.uci.edu.  \nor: \n Pierre Baldi PhD., Department of Computer Science, University of California, Irvine, CA 92697, United States, \ne-mail: pfbaldi@ics.uci.edu. \nAbstract word count: 249 \nIntroduction word count: 773 \nDiscussion word count: 1056 \nApril 20, 2023 \n \n \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \n \nAbstract \nOver the past decade, Artificial Intelligence (AI) has expanded significantly with increased \nadoption across various industries, including medicine. Recently, AI’s large language models \nsuch as GPT-3, Bard, and GPT-4 have demonstrated remarkable language capabilities. While \nprevious studies have explored their potential in general medical knowledge tasks, here we \nassess their clinical knowledge and reasoning abilities in a specialized medical context. We study \nand compare their performances on both the written and oral portions of the comprehensive and \nchallenging American Board of Anesthesiology (ABA) exam, which evaluates candidates' \nknowledge and competence in anesthesia practice. In addition, we invited two board examiners \nto evaluate AI’s answers without disclosing to them the origin of those responses. Our results \nreveal that only GPT-4 successfully passed the written exam, achieving an accuracy of 78% on \nthe basic section and 80% on the advanced section. In comparison, the less recent or smaller \nGPT-3 and Bard models scored 58% and 47% on the basic exam, and 50% and 46% on the \nadvanced exam, respectively. Consequently, only GPT-4 was evaluated in the oral exam, with \nexaminers concluding that it had a high likelihood of passing the actual ABA exam. \nAdditionally, we observe that these models exhibit varying degrees of proficiency across distinct \ntopics, which could serve as an indicator of the relative quality of information contained in the \ncorresponding training datasets. This may also act as a predictor for determining which \nanesthesiology subspecialty is most likely to witness the earliest integration with AI. \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n3 \n \nIntroduction \nIn recent years, Artificial Intelligence (AI) primarily in the form of machine learning, in \nparticular deep learning, has experienced a significant expansion driven primarily by progress in \ncomputational power and big data availability. 1 In the medical field, AI's potential to increase \naccuracy and expedite diagnoses has led to its application in numerous areas, including \nradiology, pathology, and genomics. For example, AI has been employed to examine medical \nimages and videos for early indicators of diseases such as cancer, predict disease likelihood, and \ntailor treatment plans based on a patient's genetic profile.\n2,3,4  In addition, AI-based large \nlanguage models (LLMs), trained on very large corpora of text, are now able to fluidly generate \nhigh-quality text (and software) on almost any subject, opening new opportunities for \ntransforming healthcare. But how knowledgeable and capable of reasoning are LLMs in \nmedicine? One previous study has explored their potential in general medical knowledge tasks.\n5 \nHere we assess their clinical knowledge and reasoning abilities in a specialized medical context.  \n \nMost LLMs utilize neural network architectures known as transformers, which incorporate \nattention mechanisms for processing input sequences. This approach has demonstrated \nsignificant efficacy in tasks such as language translation and text generation.\n6,7 Here we consider \nsome of the currently most notable LLMs, including the Generative Pre-trained Transformer-3 \n(GPT-3), Bard, and the Generative Pre-trained Transformer-4 (GPT-4). 8,9,10 \n \nThese models utilize the transformer's decoder-only architecture 9,11, and their primary \ndifferences lie in their size and the nature of the data they were trained on. The GPT-3 model \npossesses 175 billion parameters and has already been shown to excel in several tasks. The GPT-\nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n4 \n \n4 model overcomes many of the limitations of GPT-3 by enlarging the model size to one trillion \nparameters. Both versions of GPT were pre-trained on a vast text corpus, followed by fine-tuning \nto perform specific tasks. 8,9 In addition, the models were enhanced using the Reinforcement \nLearning from Human Feedback (RLHF) method, which utilizes human-generated feedback to \nenable model alignment to human preferences, guiding the model toward generating more \naccurate and appropriate responses. 10 On the other hand, Google's Bard employs the Language \nModel for Dialogue Applications (LaMDA) with 137 billion parameters and is primarily pre-\ntrained on public dialog data and web text, which leads to improved conversational \nunderstanding and accurate dialog-style responses.11  \n \nThese systems, which are more or less capable of passing the Turing test and far exceed humans \nin their speed, fluidity, and capability to speak many languages, raise the anthropomorphic, ill-\ndefined, question of how much LLMs can actually \"understand\". Many studies suggest that \nLLMs are prone to hallucinations and errors, sometimes struggling with reasoning, causality, and \ncommon-sense understanding.\n12,13 However, it is important to acknowledge that complex \nsystems operating in large domains are bound to make errors o ccasionally, similar to visual \nillusions in the human visual cortex. Moreover, LLMs have been rapidly improving, with fewer \nand fewer errors being observed, and it has been shown that careful prompting can aid LLMs in \nfocusing, self-correcting, and reasoning, improving their capabilities even further. \n14     \n \nTo assess the capability of LLMs to understand medical data, researchers recently evaluated \nGPT-3's capacity to answer questions from the United States Medical Licensing Examination \n(USMLE). The study's results demonstrated that the model performed exceptionally well on the \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n5 \n \nexam, exhibiting a high degree of accuracy and fluency in medical reasoning.15 Thus these initial \nresults suggest that large language models have the potential for transforming both medical \neducation and practice. \n \nWhile the USMLE is a widely recognized exam for general medical knowledge, it may not fully \nencompass the complex scenarios medical specialists face in their practice. Hence, in this work, \nwe aim to further assess the clinical knowledge and reasoning abilities of large language models, \nnamely GPT-3, BARD, and GPT-4, by examining their performance on the American Board of \nAnesthesiology (ABA) exam. This exam comprises three parts: the Basic Exam, the Advanced \nExam, and the Applied Exam. The Basic Exam evaluates fundamental knowledge in \nanesthesiology and is typically taken during the second post-graduation year (PGY-2) of \nresidency training. The Advanced Exam is more comprehensive, covering topics such as patient \ncare, pharmacology, and medical knowledge, and is taken in the first year following graduation \nfrom a residency program. The Applied Exam, taken only after passing the first two exams, \nfinally assesses a candidate's clinical competence in anesthesia practice, comprising both a \nStructured Oral Examination (SOE) and a hands-on Objective Structured Clinical Examination \n(OSCE). The tri-part ABA exam is considered one of the most challenging exams in the medical \nfield, necessitating extensive knowledge and training in anesthesiology. Passing the exams is a \nsignificant accomplishment and is required for certification by the American Board of \nAnesthesiology.\n16,17 By analyzing the models' performance on this exam, we can better \ncomprehend their potential utility in highly specialized and versatile medical scenarios, offering \ninsights into the strengths and limitations of large language models in the medical domain. \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n6 \n \n \nMaterial and Methods \nThe ABA examination is a tripartite evaluation that encompasses the Basic Exam, Advanced \nExam, and Applied Exam. For the basic exam, we used the full set of sample questions provided \non the ABA website, which contains 60 multiple-choice questions with answer keys provided.\n18 \n \nThe Advanced Exam was formulated using the book, \"Anesthesia Review: 1000 Questions and \nAnswers to Blast the BASICS and Ace the ADVANCED.\" This book's advanced topics section \ncomprises 14 detailed chapters in the following domains: Advanced Monitors, Pain, Pediatric \nAnesthesia, Obstetric Anesthesia, Cardiac and Vascular Anesthesia, Thoracic Anesthesia, ENT \nAnesthesia, Anesthesia for Special Indications, Orthopedic Anesthesia, Trauma, Anesthesia for \nAmbulatory Surgery, Geriatrics, Critical Care, and Ethics.\n19 We chose five questions at random \nfrom each chapter, resulting in a final 70-item multiple-choice questionnaire. The multiple-\nchoice questions and possible answers from both the basic and advanced sample exams were \nentered into GPT-3 and GPT-4 using the ChatGPT plus user interface\n20 and entered into Bard \nusing the user interface provided by Google. 21 The questions were entered individually and the \nanswer from the AI was recorded.  Some of the multiple-choice questions contained images that \ncould not serve as input for these language models. In these cases, we described the images using \nwords. The selected multiple-choice responses from the models were finally compared with the \nanswer keys for scoring.  \n \nFor the Applied exam, it was not feasible to perform the OSCE portion of the advanced exam \nwhich involves things like hands-on ultrasound stations and interpretation of live monitors, so we \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n7 \n \nwere only able to explore the SOE component (we assume at the present time no AI is prepared \nto pass the interactive OSCE). Further, we only tested GPT-4 in the SOE because GPT-3 and \nBard did not pass the basic and advanced multiple-choice exams.  \n \nThe SOE exam is described by the American Society of Anesthesiologists on their website as \nbeing intended “to assess your judgment, adaptability, and organization.”  As such, the \nexaminers are trained to probe examinees for both depth and breadth of knowledge, assess \nadaptability in light of new information, and do so in a very time-intensive condition.  This exam \nformat and the interactive cadence under time pressure are nearly impossible to replicate \nfaithfully with a typed terminal interaction, and each one of the examiners who reviewed the \nblinded results individually commented that their ability to “assess” a candidate was extremely \nlimited without the ability to interact directly themselves under the true testing conditions.  \nFurther, as an AI model, GPT-4 has no true “Judgment” or “Adaptability” (or even true \napplication of information) in the form we think of to be tested.  Nevertheless, the content of the \nresponses can certainly be judged for organization and the appearance or presentation of \njudgment and adaptability, within the noted limitations.  The other challenge in interpreting the \nresults is that for the SOE each examinee is scored by multiple examiners, and each individual \nscore is normalized for the examiner’s specific scoring style as well as the scores for that specific \nSOE question at that day and time and more, all of which make it nearly impossible for any \nexaminer to definitively and solely judge “passing” versus “failing”.  Thus, the most objective \ncomparison we could hope to achieve was to solicit examiner opinions on whether the observed \nperformance was high or low in their opinion, and whether they believed in their experience this \nwas likely passing, failing, or indeterminate performance. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n8 \n \n \nFor the SOE stem we used the sample exam found on the ABA website.  We chose the first long-\nstem format which includes sub-topics in intraoperative (4 questions) and post-operative care (6 \nquestions), followed by three unrelated patient short-stem topics. 22 ChatGPT-4, even under the \npaid subscription model, currently has a limit of ‘questions’ that can be asked in a day. The \ninteractive nature of the SOE caused that limit to be hit quickly in just a fraction of the total \nexam, so we had to be strategic in our interaction with the AI model. We chose, therefore, to \n‘administer’ the exam in two distinct approaches.   In the first approach, we provided the AI with \nthe complete patient stem, and then presented the entirety of a question block at once and \nallowed the AI to respond to the entire block at once.  While dissimilar to the actual exam \nformat, this allowed us to reduce the number of “questions” being counted by the AI’s daily limit \nand assess the complete responses of the AI to the exam topics for the component when \npresented in their entirety. \n \nIn the second format, we used an interactive approach to the questions more consistent with the \nactual exam format, spreading the interactions out over several days when the ceiling was hit \neach day. The exam was managed by an ABA examiner and included additional attempts for \nprobing depth, breadth, adaptability, and detail as might be encountered during the examination. \nIn particular, we focused on the application of knowledge and decision-making (judgment) in \nresponses in this portion (which we assumed might be more challenging) as opposed to medical \nknowledge and organization (which we assumed would be easier for the AI).   \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n9 \n \nSince the question stem we used was found on the ABA website it was possible that written \n\"ideal’ responses might exist somewhere on the web that had been incorporated into the AI \ndatabase, so to pursue the interactive format in greater depth and novelty, a different “grab-bag” \ntopic stem was presented from a very old exam (one of the authors’ favorites for really \nchallenging critical thinking and application) and pursued with a bit more novelty and open \nexploration, asking the AI to make medical judgment calls in a very brief but potentially \ncomplex and unstated case history.   \n \nFor the purposes of this exam, any qualifications the AI made about being an AI were ignored \n(and deleted when shared with the blinded reviewers), and when possible firm answers were \nsolicited.  The AI model was given instructions at the beginning of the exam to “pretend it was \nbeing examined in anesthesiology”, that it was the highest-level expert available, and to keep its \nresponses as terse as possible as though it was being examined in a timed test while retaining the \nhighest priority information in the responses. \n \nFor the SOE exam scoring, two authors – one a current ABA board examiner (JR) and one the \nchair of his department and an experienced clinician (MC) – evaluated the AI responses in \ncomparison to their years of experience with actual examinees and trainees and rated the \nresponses on their approximated probability that a human respondent would pass the exam if \nproviding the given performance.  As a further validation, the second exam format was shared \nwith two other current or former ABA SOE examiners who were blinded to the origin of the \nresponses and asked to review a portion of “a transcript from a mock oral exam to be used for \nresident training purposes” and provide their thoughts on whether they thought this mock \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n10 \n \nexaminee’s performance was a good example or not, and whether they would ultimately pass if \nthis was their true exam. \n \n \nResults \nIn the assessment with basic exam questions, GPT-3 and Bard achieved scores of 58.33% and \n46.67% respectively, which suggests that they would likely be unable to pass a comprehensive \n200-question examination. In contrast, the more advanced language model, GPT-4, obtained a \nscore of 78.33%, demonstrating a greater probability of successfully passing the actual \nexamination (Table 1). \n \nModel Raw Score Percent Score \nGPT-3 35/60 58.33% \nBard 28/60 46.67% \nGPT-4 47/60 78.33% \n \nTable 1: Test Results of Different Models on the Sample Basic Exam \n \nIn the advanced assessment sample, GPT-3 and Bard obtained scores of 50.00% and 45.71% \nrespectively, suggesting that their chances of passing a comprehensive 200-question examination \nare low. However, the more sophisticated language model, GPT-4, achieved an 80.00% score, \nsurpassing its performance in the basic exam and demonstrating a higher probability of \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n11 \n \nsuccessfully completing the actual advanced examination (Table 2). Moreover, the performance \nof the models varied across individual subtopics as illustrated in Table 2 and Figure 1. \n \n \n \nTopics GPT-3 Bard GPT-4 \nAdvanced Monitors 3/5 3/5 4/5 \nPain 5/5 5/5 5/5 \nPediatric Anesthesia 3/5 1/5 3/5 \nObstetric Anesthesia 1/5 1/5 3/5 \nCardiac and Vascular \nAnesthesia 2/5 4/5 4/5 \nThoracic Anesthesia 5/5 5/5 5/5 \nENT Anesthesia 2/5 2/5 4/5 \nAnesthesia for Special \nIndications 3/5 4/5 4/5 \nOrthopedic Anesthesia 0/5 0/5 2/5 \nTrauma 1/5 0/5 3/5 \nAnesthesia for \nAmbulatory Surgery 2/5 0/5 4/5 \nGeriatrics 3/5 2/5 5/5 \nCritical Care 2/5 2/5 5/5 \nEthics and \nProfessionalism 3/5 3/5 5/5 \nTotal Raw Score 35/70 32/70 56/70 \nTotal Percent Score 50.00% 45.71% 80.00% \n \nTable 2: Test Result of the Sample Advanced Exam of Different Models by Topic \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n12 \n \nFigure 1: Number of Correct Responses per Five Questions in each Topic in the Sample\nAdvanced Produced by Different Models \n \nThe SOE interactions are provided as Appendix 1 (the static exam with complete questions asked\nen bloc) and Appendix 2 (the interactive format).  On the SOE portion of the applied exam,\nwhile not consistent with an actual exam for the “complete question and answer s” format\ninitially tried, both evaluators agreed the contents of the responses had some notable gaps but did\nnot raise any critical “failing” concerns. The organization of the responses was, unsurprisingly,\nexcellent. The medical knowledge content was also high as might be expected.  The\nincorporation of information about the current patient was also quite good, with the AI\nconsistently incorporating specific patient details into the responses it provided and the\n“decision-making” descriptions.  During the response to hemorrhage, for example, the AI\n \nle \ned \nm, \nat \nid \nly, \nhe \nAI \nhe \nAI \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n13 \n \nspecifically stated it would “administer the two units that are available while asking for \nadditional units”. In general, the incorporation of information from the patient stem was above \nwhat might be expected during a real exam, and the blood gas analysis was excellent.  The AI \nresponse even appears to demonstrate judgment via risk assessment when, in response to \ndifferences in extubation criteria for this thoracotomy patient compared to a cholecystectomy \npatient, it states: “In an ASA-1 cholecystectomy patient, these specific concerns might not be as \nsignificant, as they typically have no significant medical history and are undergoing a less \ninvasive surgery.” \n \nIn the ‘interactive’ format, a few more deficiencies were found with the AI responses, and \nrepetition of some phrases and elements was seen, though this latter may have been modifiable \nwith additional instructions to the AI to limit repetition. Both unblinded evaluators considered \nthe responses sub-ideal, but potentially passing if it had come from a true exam.  Asking for \nclarifications or asking for additional depth, changing the scenario, and even asking the AI to \nprioritize importance when it gave detailed lists resulted in generally reasonable responses.   \n \nThe deficiencies most likely encountered were improper prioritization and inappropriate choices.  \nFor example, the AI did not focus on the current neurological and vascular status of the carotid \nendarterectomy patient to understand the urgency of a new procedure, focusing instead on \nanesthetic and medication history (Appendix 2 lines 377-407). Another example is the AI \ninitially maintaining that a left-sided double-lumen tube would be preferred even for a left \npneumonectomy procedure (Appendix 2 lines 97-101). There was additional confusion around \noptimizing oxygenation in one-lung ventilation in the responses (Appendix 2 lines 174-283). \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n14 \n \nWhen these possibly concerning responses were explored, however, as an ABA examiner would \ndo during the actual exam, the AI in each case ultimately ended up appropriately revising its \ninitial flawed decisions.  After questioning a left sided-tube in the presence of a hilar resection, \nthe AI “realized” a left-sided tube was problematic and then stated it would revise its previous \nresponse (Appendix 2 lines 101-117).  It did the same for the confused portions of oxygenation \nin one-lung ventilation (Appendix 2 lines 232-281), and the neurological history of the CEA \npatient (Appendix 2 lines 406-420).  During the testing, we began to wonder if the AI was \nsimply changing priorities in response to any subtle suggestion to do so, so we deliberately \nfollowed the same questioning format for what we considered a moderate but lower priority \nhistory for the CEA patient: renal function.  Interestingly, in this circumstance the AI correctly \ndescribed the importance of this system but chose not to revise its previous response when asked \n(appropriately in our opinion), suggesting there is some decision function taking place and it \nisn’t just responding to the user input as cues (Appendix 2 lines 438-466).   \n \nThe unblinded evaluators both noted that the evaluation of the SOE exam is complex.  Real \ncandidates make simple and even large mistakes frequently during sessions due to the stress, \nimportance, and speed of the exam format and the examiners usually choose to probe such \nmistakes a bit to give the examinee time to reconsider their responses. As such, while a mistake \nfollowed by a correction on exploration is obviously less preferred than an initially correct \nresponse, such mistakes are not usually ‘failing criteria’ when corrected, and even if uncorrected \nmay not be sufficient alone if performance otherwise is good.  In the present case, the evaluators \nfelt that the mistakes being made were of moderate concern, but with appropriate corrections \nwhen probed. A human candidate responding in this manner would not receive a definite failure, \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n15 \n \nbut neither would they receive a definite pass.  Beyond that, it is impossible to be certain, but \nboth unblinded reviewers guessed that the performance, as observed, was probably more likely \nto pass than not.  Certainly, the exceptional organization and clarity of the responses (which were \nsuperior to most true examinees) would have benefitted the exam.  \n \nThe blinded reviewers also both immediately qualified that interpretation of performance via a \nwritten transcript, without the ability to interact and the experience of the time pressure during \nthe exam, was limited. They all also commented that, despite the attempts to instruct the AI to \nlimit responses, the “examinee” responses were too wordy and often repetitive.  Both blinded \nreviewers, in light of the limitations, expressed that while the performance was ‘moderate’ there \nwas ‘some’ to ‘reasonable’ probability that the examinee would ultimately pass the exam. \n \nDiscussion \nThe passing threshold for the ABA written exam varies annually, however, it is reasonable to \nposit that a score exceeding 75% typically denotes a passing grade.\n23 Consequently, only GPT-4 \nmanaged to pass both the basic and advanced exams. This finding further corroborates the \nhypothesis that enhancing model size can improve task performance.\n8,9 As future language \nmodels become increasingly large and intricate, it is plausible to anticipate accuracy levels \nnearing 100% when evaluating them with analogous exam problems. \n \nWhile GPT-3 and Bard were unable to pass the multiple-choice exams, examining their \nperformance across various topics remains intriguing. For the advanced exam sample, we \nrandomly selected practice questions from 14 distinct topics, with the models displaying \ndifferential performance in each. Remarkably, all models obtained perfect scores in thoracic and \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n16 \n \npain management categories, while they achieved the lowest scores in obstetrics, orthopedics, \ntrauma, and ambulatory surgeries (Figure 1). Given that the models' training data predominantly \noriginates from the internet, this suggests a correlation between the availability of accurate \ninformation on specific topics and the models' performance. Furthermore, these findings may \nserve as an indicator for prioritizing AI integration within certain anesthesia subspecialties. \n \nFurthermore, we observed that the language models exhibit a substantially higher accuracy when \npresented with multiple-choice questions involving purely textual content. However, their \naccuracy declines significantly when faced with questions containing numerical calculations or \ninvolving numbers. This can be attributed to the fact that these language models are \npredominantly trained on text-based data and lack specific training in numerical calculations and \nthe language constructs around conveying these calculations.\n7,8,9,11 As a result, the models \nstruggle to fully comprehend questions involving numerical calculations, leading to imprecise \nresponses. Additionally, numerical calculations demand greater precision and exactness, which \nmay not always be within the model's capabilities, resulting in further inaccuracies. To enhance \nthe accuracy of large language models in solving problems with numerical calculations, it is \nessential to incorporate a specialized training dataset comprising numerical data during the \nmodel's training phase. \n \nThe applied SOE exam results are perhaps more impressive than the multiple-choice exam \nperformance.  While only GPT-4 was tested at this level, the evaluators felt that the responses \nand performance were ‘moderate’ when compared to an anesthesiology residency-trained \nphysician who had already passed the written exams.  When unblinded and understood to come \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n17 \n \nfrom an AI model that didn’t go to medical school or residency or have any experience actually \nsitting in an operating room and managing an anesthetic, however, the responses to the questions \nwere incredibly concise, informative, and inclusive of the patient background history provided. \nThe adaptation of responses to new information was convincing, as was the revision of previous \nstatements considering subsequent questions.  \n \nAs noted above, the only real initial “flaw” we detected during examining was the prioritization \nof information when asked to narrow down a long list (which in all fairness the AI did revise \nlater when asked to reconsider), and apparent gaps in the information incorporated in an initial \nresponse.  Based on the known limitations of these AI models at this stage, it seems plausible \nthat limits are more likely to be found with the more synthetic (and less “regurgitative”) exams, \nbut even this is not certain as overall the AI’s “synthetic” functions in replicating the application \nand presentation of judgment were certainly superior to at least some human examinees we have \nencountered.   Moreover, as these models continue to expand in training data, size, and scope, \nthere is no reason to expect that performance will do anything but improve. \n \nInterpreting the results from the standpoint of the AI itself, the context  of the responses was \nalways accurate - the AI always appeared to attempt to answer the intent of the question, \ndemonstrating the impressive natural language processing of the system.  Anecdotally, \n‘mistakes’ seemed to come from ‘blind spots’ in the information incorporated into the responses. \nWhen the blind spot was probed and the AI focused on it, the subsequent responses improved. If \nwe were going to engage in a series of such exams in written format with human and AI \nrespondents, our best guess to differentiate the two would be to ask broad questions that require \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n18 \n \nlarge amounts of unstated assumptions to be incorporated in order to provide a correct response. \nWe would then attempt to identify the humans via the inclusion, and the machines via the \ninclusion gaps, of all relevant, but especially ‘obvious’ circumstantial unstated data.  For \nexample, that a left pneumonectomy requires hilar resection, and a left-sided tube is not feasible, \nis something that should be obvious to trained anesthesiologists, but the AI did not seem to \nincorporate it until it was asked to focus on exactly that point. \n \n As it stands, the performance of the AI in this process was impressive enough that it leads one to \nquestion whether we shouldn’t already, as highly trained clinicians, be using these AI systems to \nhelp us avoid some of the more common cognitive errors that may occur during critical events.  \nWe have a code leader at all hospital codes, and typically a dedicated scribe to record events.  \nPerhaps an additional resource to consult the AI for missed items on the differential and missed \npriority interventions to prompt the humans in the room is a well-invested effort. \n \nThe ability of GPT-4 to pass the ABA examination carries both positive and negative \nimplications for medical education. On one hand, the model's success highlights the potential of \nlarge language models to act as influential tools in medical education, enhancing the quality of \ninstruction and preparing students for examinations. This progress could pave the way for the \ncreation of more efficient and effective pedagogical approaches, offering students tailored \nlearning experiences that accommodate their unique learning styles. \n \nOn the other hand, there are potential negative consequences that educators must consider. The \noverreliance on large language models could contribute to a decline in critical thinking and \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n19 \n \nproblem-solving abilities, as students might depend excessively on the model's outputs rather \nthan cultivating their own analytical skills. To address this, educators could assign tasks \ninvolving greater reasoning and numerical analysis, and place increased emphasis on in-person \ndiscussions over written assignments. In summary, the incorporation of large language models \ninto medical education holds the potential to transform the way students learn; however, \neducators must remain cognizant of their limitations and strive to maintain a balance with \ntraditional teaching methods. \n \nConclusion \nThis study evaluated the clinical knowledge and reasoning capabilities of large language models, \nspecifically GPT-4, in anesthesiology by assessing their performance on the American Board of \nAnesthesiology exam. Our findings highlight the relationship between model size and task \naccuracy and suggest potential areas for AI integration within anesthesiology subspecialties \nbased on varied performance across topics. Future research should address the limitations and \nethical implications of deploying AI in clinical settings, as well as explore ways to augment \nhuman decision-making processes with AI-driven insights to maximize the benefits of these \ntechnologies in medicine. \n \n \n \n \n \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n20 \n \n \n \nReferences \n1. Baldi P. Deep Learning in Science. Cambridge University Press . 2021. \ndoi:10.1017/9781108955652 \n2. Urban G, Tripathi P, Alkayali T, et al. Deep Learning Localizes and Identifies Polyps in \nReal Time With 96% Accuracy in Screening Colonoscopy. Gastroenterology. 2018 \nOct;155(4):1069-1078.e8. doi: 10.1053/j.gastro.2018.06.037. Epub 2018 Jun 18. PMID: \n29928897; PMCID: PMC6174102.  \n3. Chang P, Grinband J, Weinberg BD, et al. Deep-Learning Convolutional Neural \nNetworks Accurately Classify Genetic Mutations in Gliomas. AJNR Am J Neuroradiol. \n2018 Jul;39(7):1201-1207. doi: 10.3174/ajnr.A5667. Epub 2018 May 10. PMID: \n29748206; PMCID: PMC6880932.  \n4. Tai MC-T. The impact of Artificial Intelligence on Human Society and Bioethics. Tzu \nChi Medical Journal. 2020;32(4):339. doi:10.4103/tcmj.tcmj_71_20  \n5. Gilson A, Safranek C, Huang T, et al. How Does ChatGPT Perform on the United States \nMedical Licensing Examination? The Implications of Large Language Models for \nMedical Education and Knowledge Assessment, JMIR Med Educ . 2023;9:e45312, DOI: \n10.2196/45312 \n6. Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need. arXiv. 2017; \nhttps://doi.org/10.48550/arXiv.1706.03762 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n21 \n \n7. Baldi P, Vershynin R. The Quarks of Attention: Structure and Capacity of Neural \nAttention Building Blocks. Artificial Intelligence. 2023; \nhttps://doi.org/10.1016/j.artint.2023.103901 \n8. Brown T, Mann B, Ryder N, et al., Language Models are Few-Shot Learners. arXiv. \n2020; https://doi.org/10.48550/arXiv.2005.14165 \n9. OpenAI. GPT-4 Technical Report. arXiv. 2023; \nhttps://doi.org/10.48550/arXiv.2303.08774 \n10. Yuan Z, Yuan H, Tan C, Wang W, Huang S, Huang F. RRHF: Rank Responses to Align \nLanguage Models with Human Feedback without tears. arXiv. 2023; \nhttps://doi.org/10.48550/arXiv.2304.05302 \n11. Thoppilan R, De Freitas D, Hall J, et al. LaMDA: Language Models for Dialog \nApplications. arXiv. 2022; https://doi.org/10.48550/arXiv.2201.08239  \n12. Huang Y, Feng X, Feng X, Qin B. The Factual Inconsistency Problem in Abstractive \nText Summarization: A Survey. arXiv. 2023; https://doi.org/10.48550/arXiv.2104.14839 \n13. Ji Z, Lee N, Frieske R, et al. Survey of Hallucination in Natural Language Generation. \nACM Computing Surveys. 2023; 248: 1-38. https://doi.org/10.1145/3571730 \n14. Manakul P, Liusie A, Gales M. SelfCheckGPT: Zero-Resource Black-Box Hallucination \nDetection for Generative Large Language Models. arXiv. 2023; \nhttps://doi.org/10.48550/arXiv.2303.08896 \n15. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: \nPotential for AI-assisted medical education using large language models. PLOS Digital \nHealth. 2023; 2(2): e0000198. https://doi.org/10.1371/journal.pdig.0000198 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint \n22 \n \n16. Bacon D, Lema M, To define a specialty: A brief history of the American Board of \nAnesthesiology's first written examination. J Clin Anesth.  1992; 489-497, \nhttps://doi.org/10.1016/0952-8180(92)90226-Q \n17. McClintock JC, Gravlee GP. Predicting Success on the Certification Examinations of the \nAmerican Board of Anesthesiology. Anesthesiology. 2010; 112:212–219. \nhttps://doi.org/10.1097/ALN.0b013e3181c62e2f \n18. American Board of Anesthesiology, BASIC Examination Questions. 2022; \nhttps://www.theaba.org/certification-exam-type/basic-exam/ \n19. Chiao F, MSc; Dinner M. Anesthesia Review: 1000 Questions and Answers to Blast the \nBASICS and Ace the ADVANCED, 1st ed. Anesthesia & Analgesia . 2019;128(6):p \ne118, DOI: 10.1213/ANE.0000000000004151 \n20. American Board of Anesthesiology, Sample Standardized Oral Exam Questions. 2022; \nhttps://www.theaba.org/certification-exam-type/applied-exam/ \n21. Open AI. ChatGPT Plus. 2023; https://openai.com/blog/chatgpt-plus \n22. Google. Bard. 2023; https://bard.google.com/ \n23. Cheney MA, Dimeola MA, Nagy CJ. Predicting Success: Does Performance on the \nAnesthesia Knowledge Test - 6 (AKT-6) correlate with the American Board of \nAnesthesiology (ABA) Licensing Exam first-time pass rate? J Educ Perioper Med . \n2014;16(1):E067. PMID: 27175397; PMCID: PMC4719557 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted May 16, 2023. ; https://doi.org/10.1101/2023.05.10.23289805doi: medRxiv preprint ",
  "topic": "Subspecialty",
  "concepts": [
    {
      "name": "Subspecialty",
      "score": 0.680050253868103
    },
    {
      "name": "Anesthesiology",
      "score": 0.642186164855957
    },
    {
      "name": "Competence (human resources)",
      "score": 0.6380097270011902
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5292232632637024
    },
    {
      "name": "Medical education",
      "score": 0.44784820079803467
    },
    {
      "name": "Medical knowledge",
      "score": 0.4427727460861206
    },
    {
      "name": "Computer science",
      "score": 0.4418317675590515
    },
    {
      "name": "Psychology",
      "score": 0.4024168848991394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3805869519710541
    },
    {
      "name": "Natural language processing",
      "score": 0.3457793593406677
    },
    {
      "name": "Medicine",
      "score": 0.2725200057029724
    },
    {
      "name": "Pathology",
      "score": 0.2020655870437622
    },
    {
      "name": "Social psychology",
      "score": 0.09531351923942566
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ],
  "cited_by": 21
}