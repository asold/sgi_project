{
    "title": "An investigative design of optimum stochastic language model for bangla autocomplete",
    "url": "https://openalex.org/W2906706645",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A5036792941",
            "name": "Md.Iftakher Alam Eyamin",
            "affiliations": [
                "Daffodil International University"
            ]
        },
        {
            "id": "https://openalex.org/A5004962306",
            "name": "Md. Tarek Habib",
            "affiliations": [
                "Daffodil International University"
            ]
        },
        {
            "id": "https://openalex.org/A5076972917",
            "name": "Muhammad Ifte Khairul Islam",
            "affiliations": [
                "Daffodil International University"
            ]
        },
        {
            "id": "https://openalex.org/A5008419984",
            "name": "Md. Sadekur Rahman",
            "affiliations": [
                "Daffodil International University"
            ]
        },
        {
            "id": "https://openalex.org/A5032935765",
            "name": "Md. Abbas Ali Khan",
            "affiliations": [
                "Daffodil International University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4293869204",
        "https://openalex.org/W2794139335",
        "https://openalex.org/W1599268073",
        "https://openalex.org/W2127357421",
        "https://openalex.org/W2563525636",
        "https://openalex.org/W6676214546",
        "https://openalex.org/W1508812494",
        "https://openalex.org/W2171139677",
        "https://openalex.org/W2646417138",
        "https://openalex.org/W2162039701",
        "https://openalex.org/W2124059530",
        "https://openalex.org/W6630725953",
        "https://openalex.org/W6678539225",
        "https://openalex.org/W2735497574",
        "https://openalex.org/W2129329204",
        "https://openalex.org/W2482589566",
        "https://openalex.org/W1515760596",
        "https://openalex.org/W2108484244",
        "https://openalex.org/W4301652706",
        "https://openalex.org/W4251507406",
        "https://openalex.org/W2124814993",
        "https://openalex.org/W182831726",
        "https://openalex.org/W2591002653"
    ],
    "abstract": "&lt;p class=\"Abstract\"&gt;Word completion and word prediction are two important phenomena in typing that have extreme effect on aiding disable people and students while using keyboard or other similar devices. Such autocomplete technique also helps students significantly during learning process through constructing proper keywords during web searching. A lot of works are conducted for English language, but for Bangla, it is still very inadequate as well as the metrics used for performance computation is not rigorous yet. Bangla is one of the mostly spoken languages (3.05% of world population) and ranked as seventh among all the languages in the world. In this paper, word prediction on Bangla sentence by using stochastic, i.e. &lt;em&gt;N&lt;/em&gt;-gram based language models are proposed for autocomplete a sentence by predicting a set of words rather than a single word, which was done in previous work. A novel approach is proposed in order to find the optimum language model based on performance metric. In addition, for finding out better performance, a large Bangla corpus of different word types is used.&lt;/p&gt;",
    "full_text": "Indonesian Journal of Electrical Engineering and Computer Science \nVol. 13, No. 2, February 2019, pp. 671~676 \nISSN: 2502-4752, DOI: 10.11591/ijeecs.v13.i2.pp671-676 ÔÅ≤     671 \n  \nJournal homepage: http://iaescore.com/journals/index.php/ijeecs \nAn investigative design of optimum stochastic language model \nfor bangla autocomplete \n \n \nMd. Iftakher Alam Eyamin, Md. Tarek Habib, Muhammad Ifte Khairul Islam,  \nMd. Sadekur Rahman, Md. Abbas Ali Khan \nDaffodil International University, 4/2, Sobhanbag, Mirpur Rd, Bangladesh \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Aug 6, 2018 \nRevised Nov 22, 2018 \nAccepted Dec 3, 2018 \n \n Word completion and word prediction are two important phenomena in typing \nthat have extreme effect on aiding disable people and students while using \nkeyboard or other similar devices. Such autocomplete technique also helps \nstudents significantly during learning process through constructing proper \nkeywords during web searching. A lot of works are conducted for English \nlanguage, but for Bangla, it is still very inadequate as well as the metrics used \nfor performance computation is not rigorous yet. Bangla is one of the mostly \nspoken languages (3.05% of world population) and ranked as seventh among \nall the languages in the world. In this paper, word prediction on Bangla \nsentence by using stochastic, i.e. N-gram based language models are proposed \nfor autocomplete a sentence by predicting a set of words rat her than a single \nword, which was done in previous work. A novel approach is proposed in order \nto find the optimum language model based on performance metric. In addition, \nfor finding out better performance, a large Bangla corpus of different word \ntypes is used. \nKeywords: \nWord prediction \nNatural language processing \nLanguage model \nN-gram \nMachine learning \nEager learning \nPerformance metric Copyright ¬© 2019 Institute of Advanced Engineering and Science.  \nAll rights reserved. \nCorresponding Author: \nMd. Iftakher Alam Eyamin, \nDaffodil International University , \n4/2, Sobhanbag, Mirpur Rd, Dhaka 1207, Bangladesh. \nEmail: iftakher.eyamin@gmail.com \n \n \n1. INTRODUCTION \nInnovation in writing and typing of a language is so important. Especially for disable persons and \nearly learners of the language. A person having disability can live a comfortable life if he or she has the \nopportunity of typing a note, an email or anything else comfortably with the aid of autocomplete. In addition, \nfor the early learners in any field ( i.e. students, novice researchers) the autocomplete technique might be \nbeneficial during the learning process by providing most suitable suggestions while searching for new topics \nwith keywords. Though Bangla is one of the most widely spoken languages (3.05% of world population) and \nconsidered seventh language of all languages in the world [1], no work was found on automated Autocomplete. \nIn recent couple of years, very few efforts have been made for word prediction, specially focused on Bangla \nlanguage. In the research work on Bangla word prediction [2], stochastic, i.e. N-gram based language models \nare proposed for completing a sentence by predicting a single word. The next improvement took place in the \nwork of M. T. Habib et. al. [3], where word prediction on sentence by using stochastic, i.e. N-gram based \nlanguage models. They have used a novel metric to assess the performances of their proposed model. Although \nthey achieved good accuracy, it is a matter of fact that opportunities still remain for improvement. Artificial \nIntelligence used for word prediction in Spanish is  also observed in [4], in which using the chart bottom -up \ntechnique, syntactic and semantic analysis is done for word prediction. H. Al-Mubaid [5] presented an effective \nmethod of word prediction in English using machine learning. In [6] Nagalavi and Hanum anthappa have \napplied N-gram based word prediction model in order to establish the link between different blocks of a piece \nof writing in e-newspaper in English retaining with the sentence reading order. Some related work use N-gram \n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 13, No. 2, February 2019 :  671 ‚Äì 676 \n672 \nlanguage model for Autocomplete in Urdu language [7] and in Hindi language [8] for detecting disambiguation \nin Hindi word. Some research works in Bangla language, e.g. Bangla grammar checker [9] using N-gram \nlanguage model, checking the correctness of Bangla word [10], verification of Bangla sentence structure [11], \nand validity determination of Bangla sentences [12] are also conducted. There are some different word \nprediction tools such as AutoComplete by Microsoft, AutoFill by Google Chrome, TypingAid, LetMeType \netc. In [13] so ftware with improved training and recall algorithms are suggested to solve the sentence \ncompletion problem using the cogent confabulation model, which can remember sentences with 100% \naccuracy in the training files. An N-gram model is constructed in [14], which was used to compute 30 \nalternative words for a given low frequency word in a sentence, and human judges then picked the best impostor \nwords, based on a set of provided guidelines Index-based retrieval algorithm and a cluster-based approach are \nproposed at [15] for sentence-completion. Bickel et al. [16] learned a linearly cast N-gram model for sentence \ncompletion. Bhatia et al. [17] extracted frequently occurring phrases and N-grams from text collections and \ndeployed them for generating and ranking au to-completion candidates for partial queries in the absence of \nsearch logs. A new approach is proposed in [18], for learning to personalize auto-completion rankings based. \nWord prediction means guessing the next word in a sentence. Auto complete or Autocomplete works \nso that the user types the first letter or letters of a word and the program provides one or more higher probable \nwords. If the word he intends to type is included in the list, he can select it, for example by using the number \nof keys. If the word that the user wants is not predicted, the user must type the next letter of the predicted word. \nAt this time, the word choice(s) is altered so that the words provided begin with the same letters as those that \nhave been selected or the word that the user wants appears it is selected.  \nAutocomplete technique complete word by analyzing previous word flow and first letter of the word \nfor auto completing a word and sentence with more accuracy and reduces misspelling. N-gram language model \nis important technique for word prediction. \nThe problem addressed in this paper is about stochastically predicting a suitable word to complete an \nincomplete sentence, which consists of some words and a single character. Let w1w2 w3 ‚Ä¶ wm-1 wm be a sentence \ni.e. sequence of words, where wm = c1c2c3‚Ä¶‚Ä¶cn and w1w2 w3 ‚Ä¶ wm-1c1  has already been typed. The problem is \nto build a language model which takes w1w2 w3 ‚Ä¶ wm-1c1 as input and predicts an n-tuple of word fragments \n(vm1,vm2, vm3, ‚Ä¶vmn) as output in order to match the remaining untyped word fragment c2 c3c4‚Ä¶‚Ä¶cn, as shown \nin Figure 1.  \n \n \n \nFigure 1. Language Model \n \n \nWe use large data corpus for training in N-gram language model for completing correct Bangla word \nto complete a Bangla sentence with more accuracy. In this paper,  we propose an investigating design of \noptimum stochastic language model for Bangla autocomplete using supervised machine learning technique \nbased on different N-gram language modeling. Probability is based on counting things or word in most cases. \nIn our previous works [2, 3], we used different types of language models for word prediction. Both these two \nworks are different from the work presented in this paper because word prediction o bviously differs from \nautocomplete, i.e. word completion. In these earlier works word [2] or word set [3] is predicted based on one \nor more preceding words, but in this work, word fragment set is being predicted based on one or mor e words \nand a single character. \nThe rest of the paper is organized as follows. In Section 2, comes the description of our approach to \nsolve the problem. Section 3 describes how we apply our entire methodology and what results are achieved. In \nSection 4, we investigate results obtained in order to develop an understanding about the merits of our proposed \napproach. Finally, we summarize our work along with limitations, and discuss the scope for future work in \nSection 5. \n \n \n2. PROPOSED METHOD \nWe begin with five language models, namely uni gram, bigram, trigram, backoff and linear \ninterpolation. All these language models are based on N-gram approximation. Bayesian classifiers have been \n\nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \nAn investigative design of optimum stochastic language model for bangla‚Ä¶ (Md. Iftakher Alam Eyamin) \n673 \nused in [19 -21]. As opposed to Bayesian, classifier  assumes no correlation between words in the same text, \nwhere N-gram language model assume relationships between the words, and evaluate the probability of a word \nbeing before or after another word . The ordinary equation for the N-gram approximation to the conditional \nprobability of the next word in a sequence is: \n \nùëÉ(ùë§ùëõ|ùë§1\nùëõ‚àí1) ‚âà ùëÉ(ùë§ùëõ|ùë§ùëõ‚àíùëÅ+1\nùëõ‚àí1 ) (1) \n \nEquation 1 shows that probability of a word wn given all the previous words can be presumptive by \nthe probability given only the previous N words. If N = 1, 2, 3 in (1), the model becomes unigram, bigram and \ntrigram language model, respectively, and so on. \nWe train our full corpus along with our five language models. We calculate each word N-gram \nportability. We predict the word, which have the most frequency based on N-gram probability. Then we check \nthe first character of the predicted word with our given character. If the first character of the predicted word \nand given word matched we select the word as predicted or result word. \nThe performance of each language model is measured by taking both the matching of predicted word \nwith intended word as well as the order of matching into account. Therefore, accuracy and failure rate are used \nin order to address this issue. If is mention that the equation 2 & 3 used from our previous paper [3]. If wm \nmatches with vmi, (i.e. wm equals vmi, where 1 ‚â§ i ‚â§ n + 1), then the accuracy is \n \nAccuracy = \n%1001 ÔÇ¥ÔÄ≠ÔÄ´\nn\nin  (2) \n \nFailure occurs when i equals n + 1. (n + 1)-th match means no match has taken place, i.e. accuracy \nequals 0. If in an experiment a language model fails to predict f times, i.e. f failures occur, out of p predictions, \nthen the failure rate is \n \nFailure rate = \n%100ÔÇ¥p\nf  (3) \n \nAnother aspect of the problem is empirical. Given a number of language models, we need to come up \nwith the one, which outperforms all other models in terms of accuracy for possibly small value of n. \n \n \n3. RESEARCH METHOD  \nWe train a language model based on a corpus se tting n, the prediction length, with 1. Then accuracy \nof the trained model is tested. The value of n is increased by 1 and the language model is trained and tested. \nThe process continues until insignificant change in accuracy occurs and the value exceeds t he average word \nlength of corpus, |w |. Here is to mention that as the value of n increases, so is for accuracy too. Although larger \nvalue of n involves better accuracy, it increases the value of i, the number of position in n-tuple at which \nprediction matches. Thus, it also involves larger number of key strokes required. This is why the average word \nlength of corpus is used in looping condition. In this way, n*, the considerable optimum value of n is \nautomatically calculated for every language model stated earlier, which is given as pseudocode in Algorithm \n1 of [3]. The best model is chosen by the technique, which was describe in our previous paper [3]. \nA set of training modules of word prediction were developed to compute unigram, bigram, trigram, \nbackoff as well as linear interpolation based on N-gram. The implementation is different in respect to the \nprevious work [4] as the prediction is built with a set of words and a character instead of a sin gle one during \nfinding out the best language model among these language models. These models are used to determine \ndifferent probabilities by counting frequencies of words in a very large corpus, which has been constructed \nfrom the popular Bangla newspaper  the ‚ÄúDaily Prothom Alo‚Äù. The corpus contains more than 12 million \n(12,203,790) words and about 1 million (937,349) sentences, where total number of unique words is 294,371 \nand average word length (|w |) is 7. \nDuring this work, we divided the entire corpus  into two parts, namely training part and testing part. \nThe holdout method [22] is used to split the corpus at the proportion of two -thirds for training and one -third \nfor testing. Therefore, this work starts with a training corpus of size more than seven ( 7) hundred thousand \nsentences. In order to avoid model over -fitting problem (i.e. to have lower training error but higher \ngeneralization error), a validation set is used. In accordance with this approach, the original training data is \ndivided into two smaller subsets. One of the subsets is used for training, while the other one (i.e. the validation \nset) is used for calculating the generalization error. Two -thirds of the training set is fixed for model building \nwhile the remaining one -third is used for error  estimation. The holdout method is repeated for five times in \norder to find the best model. After finding out the best model, the accuracy of the model is computed using the \n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 13, No. 2, February 2019 :  671 ‚Äì 676 \n674 \ntest set, through which the considerable optimum prediction length (n*) is determined automatically based on \nAlgorithm 1of [3].The entire approach is shown in Figure 2. \n \n \n \n \nFigure 2. Proposed approach for specific model \n \n \n4. RESULT AND DISCURTION  \nThe optimum prediction length (n*) along with the accuracy of each model is shown on Table 1. \n \n \nTable 1. Optimum prediction length (n*) of all language models \nLanguage Model Prediction Length Optimum value of n (n* ) n=1 n=2 n=3 n=4 n=5 n=6 n=7 \nUnigram 3.4% 12.65% 21.32% 25.37% 29.48% 33.75% 38.62% 7 \nBigram 59.90% 65.35% 69.14% 72.52% 74.95% 77.06% 79.12% 7 \nTrigram 75.74% 79.21% 81.02% 81.93% 82.48% 83.03% 83.38% 7 \nBackoff 75.74% 80% 82.39% 83.41% 84.73% 85.42% 85.96% 7 \nLinear Interpolation 73.76% 79.70% 83.33% 86.39% 88.42% 90.10% 91.58% 7 \n \n \nIn addition, a detailed investigation is conducted (shown on Table 2) to evaluate the performance of \nthe classifier for all models by varying the length of test sentences, i.e. unigram, bigram, trigram, backoff and \nlinear interpolation. After finding out the different accuracy rate of top three models with the test set consists \nof sentences with different lengths, the average accuracy of the model‚Äôs (i.e. trigram, backoff and linear \ninterpolation) is computed(see Figure 3) which might lead us in finding out the best language model. During \nfinding out the accuracy of each model, it is noticed that, sometimes models show almost same accuracy during \nthe process of predicting the suitable word. Therefore, keeping track of the failure rate is considered as a \nsignificant task, as some models might show same accuracy but with different failure rate. In Table 3, the \nfailure rate of all the models is presented. After finding out the different failure rate of top three models with \nthe test set consists of sentences with different lengths, the average failure rate (see Figure 4) of the top model‚Äôs \n(i.e. trigram, backoff and linear interpolation) is computed which might lead us in finding out the best language \nmodel; as during the process of finding out the accuracy some models have shown almost similar accurac y \nwhich makes the selection process difficult. \n \n \nTable 2. All models‚Äô accuracy across the \navailability of words \nAvailable \nWords in Test \nSentences \nAccuracy of Language Model \nTrigram Backoff Linear \nInterpolation \n1 15% 20% 37.78% \n2 34.22% 44% 61.07% \n3 59.99% 74.16% 63.57% \n4 60.68% 76.21% 64.56% \n5 61.30% 79.2% 73.21% \n6 61.52% 79.99% 73.40% \n7 67.5% 82.63% 74.28% \n8 68.26% 83.43% 74.34% \n9 68.69% 85.40% 75% \n10 70.12% 85.75% 75% \n11 70.71% 87.40% 76.73% \n12 71.59% 90% 82.63% \n13 83.57% 91.11% 83.92% \n14 83.57% 91.99% 92.14% \n15 87.85% 93.6% 93.57% \n \nTable 3. All models‚Äô failure rate with the \navailability of the words in test sentence \nAvailable \nWords in Test \nSentences \nFailure Rate of Language Model \nTrigram Backoff Linear \nInterpolation \n1 64.44% 54.28% 60.00% \n2 39.28% 25% 35.71% \n3 38.63% 21.62% 30.43% \n4 36.95% 20% 28.57% \n5 36.95% 19.04% 25% \n6 32.14% 16.34% 25% \n7 30.43% 15.78% 25% \n8 30.43% 13.51% 23.91% \n9 28.76% 12.12% 22.72% \n10 28.76% 12.04% 21.73% \n11 28.57% 10.71% 19.56% \n12 27.27% 9.52% 14.54% \n13 14.28% 7.40% 14.28% \n14 14.28% 5.71% 7.14% \n15 10.71% 4.21% 3.57% \n \n\nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752 ÔÅ≤ \n \nAn investigative design of optimum stochastic language model for bangla‚Ä¶ (Md. Iftakher Alam Eyamin) \n675 \nDuring experiment, as shown on Table 1, it is noticeable, the top three models have shown good \naccuracy among all the models, though linear interpolation model shows slightly better performance in terms \nof predicting next possible word with optimum prediction length seven, i.e. n* = 7. Therefore, to find out the \nbest model, in the second phase, a further deep investigation is conducted, as shown on Table 2, to find out, \nhow the top three models behave against the test sets with different sizes (average) of sentences. From the \nexperiment in second phase, all the top models behave similar like before consequently, a third phase is \nrequired, in which the failure rate of the top models is computed (see Table 3). Though, in some cases the \ntrigram, backoff and linear interpolation method show almost same accuracy, but the failure rate of the other \ntwo models (trigram and backoff) is higher compared to the linear interpolation. Moreover, from the average \naccuracy and average failure rate of all models ( Figure 3 and Figure 4 respectively) it is obvious to come up \nwith the final decision that linear interpolation model accomplishes most accuracy among all other models \nduring the word prediction process. The accuracy rate along with the increment of the prediction length of the \nlinear interpolation model is shown on Figure 5. \n \n \n  \n  \nFigure 3. Average accuracy of language models Figure 4. Average failure rate of language models \n \n \n \n \nFigure 5. Accuracy and Failure along with the prediction length of Linear Interpolation model \n\n      ÔÅ≤          ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 13, No. 2, February 2019 :  671 ‚Äì 676 \n676 \nAlthough the linear interpolation model has shown better performance than other top models (91.98% \nwith n* = 7) and the experiment result is promising. \n \n \n5. CONCLUSION \nThe focus of this research was modeling, training and apply techniques that can assist in automatic \nBangla word completion. For the purpose of this research, a large and rich Bangla corpus is applied and \nsupervised machine learning technique based on popul ar N-gram language model is used. Among five -\nlanguage model to determine the best language model is the main contribution of the research. Though during \nthe several phases of experiments, in terms of both accuracy and failure rate, the linear interpolation  \noutperforms the other models. For the future work, a further testing with the present models is planned with \nlarger corpus. An adaptive software for Bangla automated word completion based on this work will be \ndeveloped. \n \n \nREFERENCES \n[1] List of languages by number of native speakers, Available at:  \nhttps://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers. (Last Accessed: March 10, 2018). \n[2] M. M. Haque, M. T Habib and M. M. Rahman. ‚Äú Automated Word Prediction in Bangla Language Using Sto chastic \nLanguage Models ‚Äù. Academy & Industry Research Collaboration Center (AIRCC) International Journal in \nFoundations of Computer Science & Technology. November 2015, vol. 5, no. 6, pp. 67‚Äì75.  \n[3]. M. T. Habib, A. Al -Mamun, M. S. Rahman, S. M. T. Siddiq uee and F. Ahmed. \"An Exploratory Approach to Find a \nNovel Metric Based Optimum Language Model for Automatic Bangla Word Prediction\".  International Journal of \nIntelligent Systems and Applications (IJISA), February 2018, vol. 10, no. 2, pp. 47-54. \n[4] N. Garay-Vitoria and J. Gonzalez-Abascal, (2005). \"Application of Artificial Intelligence Methods in a Word-Prediction \nAid\". Laboratory of Human-Computer Interaction for Special Needs. \n[5] H. Al-Mubaid, \"A Learning-Classification Based Approach for Word Predi ction\". The International Arab Journal of \nInformation Technology, 2007,Vol. 4, No. 3. \n[6] D. Nagalaviand and M. Hanumanthappa. ‚ÄúN-gram Word prediction language models to identify the sequence of article \nblocks in English e-newspapers‚Äù. In Proceedings of International Conference on Computation System and Information \nTechnology for Sustainable Solutions (CSITSS), 2016. \n[7] Q. Abbas, (2014). \"A Stochastic Prediction Interface for Urdu\". Intelligent Systems and Applications, Vol.7, No.1, pp \n94-100.  \n[8] U. P. Singh, V. Goyal and A. Rani. \"Disambiguating Hindi Words Using N-Gram Smoothing Models\". International \nJournal of Engineering Sciences.2014, Vol.10, Issue June, pp 26-29. \n[9] J. Alam, N. Uzzaman and M. khan. \" N-gram based Statistical Grammar Checker for Ban gla and English\". In \nProceedings of International Conference on Computer and Information Technology. 2006. \n[10] N. H. Khan, G. C. Saha, B. Sarker and M. H. Rahman. \"Checking the Correctness of Bangla Words using N-Gram\". \nInternational Journal of Computer Application, 2014, Vol. 89, No. 11. \n[11] N. H. Khan, M. F. Khan, M. M. Islam, M. H. Rahman and B. Sarker. \"Verification of Bangla Sentence Structure using \nN-Gram\". Global Journal of Computer Science and Technology. 2007, vol. 14, issue 1. \n[12] M. R. Rahman, M. T. Habib, M. S. Rahman, S. B. Shuvo and M. S. Uddin. ‚ÄúAn Investigative Design Based Statistical \nApproach for Determining Bangla Sentence Validity ‚Äù. International Journal  of Computer Science and Network \nSecurity. November 2016, vol. 16, no. 11, pp. 30‚Äì37. \n[13] Q. Qiu et al. \"Confabulation based sentence completion for machine reading\". 2011 IEEE Symposium on \nComputational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB). Paris. 2011, pp. 1-8. \n[14] G. Zweig, C. J. C. Burges. Tech report: ‚ÄúThe Microsoft Research Sentence Completion Challenge‚Äù. 2011. \n[15] K. Grabski and T. Scheffer. Sentence completion. In Pro c. SIGIR, pages 433‚Äì439, Sheffield, United Kingdom, 2004. \n[16] S. Bickel, P. Haider, and T. Scheffer. Learning to com-plete sentences. In Proceedings. ECML, volume 3720 of Lecture \nNotes in Computer Science, pages 497{504. Springer, 2005}. \n[17] S. Bhatia, D. Majumdar, and P. Mitra. Query suggestions in the absence of query logs. In Proceedings. SIGIR. Beijing, \nChina, 2011, pp. 795-804. \n[18]. Daniel Jurafsky and James H. Martin. Speech and Language processing, USA: Prentice-Hall, Inc. 2000. \n[19] K. C. Rani, Y. Prasanth.  \"A Decision System for Predicting Diabetes using Neural Networks \". IAES International \nJournal of Artificial Intelligence (IJ-AI). June 2017, Vol. 6, No. 2, pp 56-65. \n[20] S. Shah, K. Kumar, Ra. K. Saravanaguru, ‚ÄúSentimental Analysis of Twitter Data Using Classifier Algorithms‚Äù. \nInternational Journal of Electrical and Computer Engineering (IJECE). February 2016, Vol. 6, No. 1, pp. 357-366. \n[21] Ashwin V, ‚ÄúTwitter Tweet Classifier‚Äù, ‚Äú IAES International Journal of Artificial Intelligence (IJ -AI)‚Äù. March 2016, \nVol. 5, No. 1, pp. 41-44. \n[22] P.-N. Tan, M. Steinbach, and V. Kumar, ‚ÄúIntroduction to Data Mining,‚Äù Addison-Wesley, 2006. \n "
}