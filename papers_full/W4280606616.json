{
  "title": "LAS-Transformer: An Enhanced Transformer Based on the Local Attention Mechanism for Speech Recognition",
  "url": "https://openalex.org/W4280606616",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099981782",
      "name": "Pengbin Fu",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2530776132",
      "name": "Daxing Liu",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095932062",
      "name": "Huirong Yang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099981782",
      "name": "Pengbin Fu",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2530776132",
      "name": "Daxing Liu",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095932062",
      "name": "Huirong Yang",
      "affiliations": [
        "Beijing University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2158373110",
    "https://openalex.org/W2193413348",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2963414781",
    "https://openalex.org/W3197507772",
    "https://openalex.org/W6623517193",
    "https://openalex.org/W2962826786",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W2889163603",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W2972389417",
    "https://openalex.org/W2972818416",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2915716523",
    "https://openalex.org/W2966661",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W3015537910",
    "https://openalex.org/W2981857663",
    "https://openalex.org/W3007328579"
  ],
  "abstract": "Recently, Transformer-based models have shown promising results in automatic speech recognition (ASR), outperforming models based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). However, directly applying a Transformer to the ASR task does not exploit the correlation among speech frames effectively, leaving the model trapped in a sub-optimal solution. To this end, we propose a local attention Transformer model for speech recognition that combines the high correlation among speech frames. Specifically, we use relative positional embedding, rather than absolute positional embedding, to improve the generalization of the Transformer for speech sequences of different lengths. Secondly, we add local attention based on parametric positional relations to the self-attentive module and explicitly incorporate prior knowledge into the self-attentive module to make the training process insensitive to hyperparameters, thus improving the performance. Experiments carried out on the LibriSpeech dataset show that our proposed approach achieves a word error rate of 2.3/5.5% by language model fusion without any external data and reduces the word error rate by 17.8/9.8% compared to the baseline. The results are also close to, or better than, other state-of-the-art end-to-end models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7442909479141235
    },
    {
      "name": "Transformer",
      "score": 0.7391623258590698
    },
    {
      "name": "Speech recognition",
      "score": 0.625203013420105
    },
    {
      "name": "Word error rate",
      "score": 0.5583006739616394
    },
    {
      "name": "Embedding",
      "score": 0.5515807867050171
    },
    {
      "name": "Language model",
      "score": 0.5352514386177063
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5237933397293091
    },
    {
      "name": "Hyperparameter",
      "score": 0.4998466968536377
    },
    {
      "name": "Parametric statistics",
      "score": 0.48667556047439575
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46253326535224915
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4280718266963959
    },
    {
      "name": "Correlation",
      "score": 0.414937287569046
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3962112069129944
    },
    {
      "name": "Artificial neural network",
      "score": 0.35388797521591187
    },
    {
      "name": "Mathematics",
      "score": 0.10956454277038574
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    }
  ]
}