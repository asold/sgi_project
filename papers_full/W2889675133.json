{
  "title": "Put It Back: Entity Typing with Language Model Enhancement",
  "url": "https://openalex.org/W2889675133",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2106810651",
      "name": "Ji Xin",
      "affiliations": [
        "Tsinghua University",
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2101703816",
      "name": "Hao Zhu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1699691160",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2236405165",
    "https://openalex.org/W2042792636",
    "https://openalex.org/W2962713724",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2515462165",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2788024509",
    "https://openalex.org/W2251239360",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963085936",
    "https://openalex.org/W2528039253",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2739759126",
    "https://openalex.org/W2104411075",
    "https://openalex.org/W2564425030",
    "https://openalex.org/W2149713870",
    "https://openalex.org/W2789018230",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2072692647",
    "https://openalex.org/W2251091211",
    "https://openalex.org/W2132096166",
    "https://openalex.org/W2251264718",
    "https://openalex.org/W2963672540",
    "https://openalex.org/W2283140239",
    "https://openalex.org/W638568100"
  ],
  "abstract": "Entity typing aims to classify semantic types of an entity mention in a specific context. Most existing models obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose entity typing with language model enhancement. It utilizes a language model to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels. Experiments on benchmark datasets demonstrate that our method is capable of enhancing the entity typing model with information from the language model, and significantly outperforms the state-of-the-art baseline. Code and data for this paper can be found from https://github.com/thunlp/LME.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 993–998\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n993\nPut It Back: Entity Typing with Language Model Enhancement\nJi Xin1,2, Hao Zhu1, Xu Han1, Zhiyuan Liu1∗, Maosong Sun1\n1State Key Laboratory on Intelligent Technology and System\nBeijing National Research Center for Information Science and Technology\nDepartment of Computer Science and Technology, Tsinghua University, China\n2David R. Cheriton School of Computer Science, University of Waterloo, Canada\nji.xin@uwaterloo.ca\n{zhuhao15,hanxu17}@mails.tsinghua.edu.cn\n{liuzy,sms}@tsinghua.edu.cn\nAbstract\nEntity typing aims to classify semantic types\nof an entity mention in a speciﬁc context.\nMost existing models obtain training data us-\ning distant supervision, and inevitably suf-\nfer from the problem of noisy labels. To\naddress this issue, we propose entity typing\nwith language model enhancement. It uti-\nlizes a language model to measure the com-\npatibility between context sentences and la-\nbels, and thereby automatically focuses more\non context-dependent labels. Experiments\non benchmark datasets demonstrate that our\nmethod is capable of enhancing the entity typ-\ning model with information from the language\nmodel, and signiﬁcantly outperforms the state-\nof-the-art baseline. Code and data for this pa-\nper can be found from https://github.\ncom/thunlp/LME.\n1 Introduction\nEntity typing classiﬁes semantic types of an en-\ntity mention in a context sentence, and can be\nbeneﬁcial for a large number of natural language\nprocessing tasks (Neelakantan and Chang, 2015),\nsuch as entity linking (Chabchoub et al., 2016),\nrelation extraction (Miwa and Sasaki, 2014), and\nquestion answering (Yahya et al., 2013). Fine-\ngrained entity typing (FET) (Ling and Weld, 2012;\nYosef et al., 2012; Yao et al., 2013; Gillick\net al., 2014; Del Corro et al., 2015; Yogatama\net al., 2015; Yaghoobzadeh and Sch ¨utze, 2015;\nRen et al., 2016a; Yuan and Downey, 2018) is\nbased on a large set of ﬁne-grained types and is\ntherefore more challenging. So far, neural mod-\nels (Dong et al., 2015; Shimaoka et al., 2017; Xin\net al., 2018) have achieved state-of-the-art results\non FET.\nAll current FET models rely on distant super-\nvision (DS) (Mintz et al., 2009) to obtain training\n∗ Corresponding author: Zhiyuan Liu.\nRaw Schwarzenegger was elected to be the governor.\nSchwarzenegger acted in the ﬁlm Terminator.\nGood (A) politician was elected to be the governor.\n(An) actor acted in the ﬁlm Terminator.\nBad (An) actor was elected to be the governor.\n(A) politician acted in the ﬁlm Terminator.\nTable 1: Examples of entity mention—type name re-\nplacement.\ndata, due to the lack of large-scale human-labeled\ndata. Such reliance on DS has been a signiﬁ-\ncant problem for entity typing. In DS, an entity\nmention in the context sentence is ﬁrst linked to\na named entity in the knowledge base (KB). The\nentity has type labels 1 stored in the KB, and all\nlabels will be assigned to this entity mention. In\nother words, these are noisy global labels with-\nout considering the speciﬁc context of the entity\nmention. On the other hand, entity typing aims\nto predict context-dependent types of the entity\nmention, and test datasets are all human-labeled.\nThe difference between DS and human annotation\nleads to a huge gap in performances between train-\ning/development and test dataset.2\nTo address this problem, we propose En-\ntity Typing with Language Model Enhancement\n(LME). It is able to measure the compatibility be-\ntween the context sentence and each distantly su-\npervised label, in an unsupervised manner using\nmeaning of the label.\nIn previous works, the hierarchical structure of\nlabels has been considered (Ren et al., 2016b;\nKarn et al., 2017; Xu and Barbosa, 2018). How-\never, to the best of our knowledge, precious\n1 Since entities are classiﬁed into labels of types, type and\nlabel have the same meaning in this paper.\n2 In the W IKI dataset, strict accuracies and macro-F1\nscores are respectively 72.3%/89.2% on the development set\nand 59.7%/79.0% on the test set, using the model NFGEC\nfrom (Shimaoka et al., 2017).\n994\ninformation inside names of labels has never\nbeen used. For example, whether the label is\n/person/actor or /foo/bar makes no dif-\nference. We argue that, the meaning of entity men-\ntion words can also be expressed by the name of its\ncontext-dependent type, to some extent. Based\non this argument, replacements with context-\ndependent types make more sense than those with\nglobal-but-context-irrelevant ones. We provide an\nexample in Table 1. The entity Schwarzenegger\nhas types /actor and /politician, and we\ncan see that replacements with context-dependent\ntypes produce better sentences.\nThe natural way to evaluate the soundness of\nsentences is language modeling (Bengio et al.,\n2003; Mikolov et al., 2010). Our method em-\nploys a language model to evaluate the soundness\nof each synthetic sentence generated by replacing\nthe entity mention with its type’s name. It is able\nto focus more on context-dependent types.\nWe conduct experiments to compare our model\nwith the state-of-the-art baseline on two widely\nused datasets. The results demonstrate that LME\nis capable of improving entity typing systems by\nconsidering the meaning of labels, and alleviating\nthe problem of noise in distantly-supervised entity\ntyping.\n2 Model\nOur model (Figure 1) consists of two parts: an en-\ntity typing (ET) module, and a language model en-\nhancement (LME) module.\nET predicts a probability distribution vector y\nfor an entity mention, where each entry yi repre-\nsents the predicted probability for each type label.\nIn the training phase, LME optimizes a lan-\nguage model whose input includes y, and also\nback-propagates gradients through y to parame-\nters inside ET. In the testing phase, LME is not\ninvolved and y is directly used for inference: if yi\nis greater than a threshold 0.5, the ith type is con-\nsidered true; if all entries are below the threshold,\nthe type with the greatest entry is considered true.\n2.1 Entity Typing Module\nEntity typing is deﬁned on an ontology T (the set\nof all labels). Given an entity mention e and its\ncontext sentence s = {l1,l2,...,e,r 1,r2,...}(li\nand ri are left and right context words), the typing\nmodel predicts a vector y indicating the probabil-\nSchwarzenegger acted in the film Terminator\nFeatureContextMention\nLanguage Model\nET\nModule\n/person\n/actor\n/politician\n…\n…\nLME \nModule\n(__) acted    in       the      film  Terminator\n/person\n/actor\n/politician\n…\n…\ny\nJlm\nFigure 1: Our model: an entity typing (ET) module and\na language model enhancement (LME) module.\nity distribution over all labels in the ontology:\ny = σ(Wy [vM ; vC; vF ]), (1)\nwhere σ is the sigmoid function, Wy is a param-\neter matrix, and [; ; ]denotes concatenation. Three\nvectors: Mention, Context and Feature, are built\nfrom eand sas follows:\nEntity mention vector There may be multi-\nple words e1,e2,... in the entity mention, and vM\nis the average of word embeddings of these words.\nContext vector Two bi-directional LSTMs\n(Hochreiter and Schmidhuber, 1997; Schuster and\nPaliwal, 1997) are used for left and right con-\ntext words. The outputs of BiLSTMs further go\nthrough a self-attention layer. vC is the concate-\nnation of the attention-layer outputs.\nHand-crafted feature vector A sparse fea-\nture vectorf is built from the entity mentione. The\nfeatures are adopted from those used by Gillick\net al. (2014) and Yogatama et al. (2015). vF is a\ndense projection of f:\nvF = Wf f, (2)\nwhere Wf is the projection matrix.\nAfter y is calculated, DS provides a label vector\ny∗∈{0,1}|T|, where |T| is the number of labels.\nThe loss function for typing is the cross-entropy\n995\nbetween y and y∗:\nJtype = H(y∗,y)\n= −∑\ni y∗\ni log(yi) + (1−y∗\ni ) log(1−yi),\n(3)\n2.2 Language Model Enhancement Module\nThe core part of the LME module is an LSTM lan-\nguage model (Sundermeyer et al., 2012). The lan-\nguage model takes a sentence {w1,w2,...,w n}as\ninput, and assigns a probability to this sentence.\nConcretely, at step i, the LSTM reads the word\nsub-sequence {w1,...,w i}, and predicts the prob-\nability of wi+1 succeeding the sub-sequence. A\nwell trained language model predicts high proba-\nbility for a reasonable sentence.\nBefore applying the LME module to enhance\nthe ET module, the language model is pre-trained\nwith sentences from the training set. The loss\nfunction for sin the pre-train phase is:\nJpre = LM({l1,l2,..., e,r1,r2,...}), (4)\nwhere bold face letters are word embeddings\nfor corresponding words. LM(·) is the language\nmodel loss function: accumulative step-wise log-\nprobability of each word of the input sequence.\nA well-trained language model calculates smaller\nloss for a more reasonable sentence.\nAfter pre-training the language model, the LME\nmodule is combined with the ET module. Con-\ncretely, we assign an embedding vector Li for\neach label, and take the sum of label embeddings\nweighted by y. The sum h replaces e in the input\nsequence of the language model:\nh = ∑T\ni=1 yiLi, (5)\nJlm = LM({l1,l2,..., h,r1,r2,...}), (6)\nwhere L is the matrix of all label embeddings, and\nJlm is loss function of the language model used\nin the training phase. In order to ensure that label\nembeddings are in the same semantic space with\nword embeddings, L is initialized with word em-\nbeddings of the labels’ names.\nIn the training phase, parameters of the ET mod-\nule are updated w.r.t.\nJtrain = Jtype + λJlm, (7)\nwhere λis the weight to balance the loss.\nThe ET module has a much smaller parame-\nter space than the language model. In order to\nmake full use of the gradients, we only update pa-\nrameters of the ET module and ﬁx the language\nmodel in the training phase. Now that the lan-\nguage model is ﬁxed, when Jlm is being mini-\nmized, it adjusts the probability distribution in y.\nIf a label i is compatible with the context sen-\ntence, its corresponding entry yi is expected to\nhave a high value. Gradients are back-propagated\nthrough y and update parameters of the ET mod-\nule. In this way, y can learn to be more context-\ndependent.\n3 Experiments\n3.1 Dataset\nWe employ two well-established and widely-used\ndataset for evaluating our model: W IKI (Ling\nand Weld, 2012) and ONTO NOTES (Gillick et al.,\n2014).\nTraining parts of both datasets are labeled with\nDS, and testing parts are annotated by human.\nTherefore they are suitable for evaluating how\nour model can narrow the gap between DS and\nground-truth context-dependent labels. Statistics\nof the two datasets are provided in Table 2.\nDataset Train Development Test\nWIKI 2,000,000 10,000 563\nONTO NOTES 251,039 2,202 8,963\nTable 2: Number of instances in each part of datasets.\n3.2 Experiment Settings\nThe baseline for comparison is the hybrid model\nNFGEC proposed by Shimaoka et al. (2017). It\nis described as the ET module of our model. Our\nown model is referred to as NFGEC+LME.\nWe implement our model based on the source\ncode of NFGEC. 3 For a fair comparison, the ET\nmodule is unchanged, including all hyperparame-\nters and methods of parameter random initializa-\ntion. Word embeddings are initialized with pre-\ntrained embeddings provided by Pennington et al.\n(2014).\nThere are a few additional hyperparameters in\nour model. The most important one is λ, the\nweight between two parts of the loss function.\nOther ones include the learning rate rfor pretrain-\ning the language model and the hidden size h of\nLSTM used in the language model. We perform\n3https://github.com/shimaokasonse/\nNFGEC\n996\na grid-search based on performances on the de-\nvelopment set, and set r = 0.005 and h = 500.\nDetails of λwill be discussed in Section 3.4.\n3.3 Overall Results\nWe compare vanilla NFGEC and NFGEC+LME\nin Table 3. The results of NFGEC come from\nthe paper by Shimaoka et al. (2017). For run-\nning NFGEC+LME, λis set to 0.005 in WIKI and\n0.001 in ONTO NOTES .\nEvaluation metrics include strict accuracy,\nmacro-F1 score and micro-F1 score (Ling and\nWeld, 2012).\nDataset W IKI ONTONOTES\nMetric Strict Macro Micro Strict Macro Micro\nNFGEC 59.68 78.97 75.36 50.89 70.80 64.93\n+LME 62.88 80.61 76.95 52.90 72.41 65.17\nTable 3: Performance of entity typing, evaluated by\nstrict accuracy, macro-F1 and micro-F1 score. (%)\nFrom the results we see that:\n(1) In both datasets, LME consistently helps\nNFGEC to better classify entity mentions into\ntheir context-dependent types. We can see im-\nprovements in all metrics. This is because LME is\ncapable of evaluating the appropriateness of each\nlabel and distinguishing context-dependent ones\nfrom global-but-context-irrelevant ones. There-\nfore LME helps the system to focus on more rea-\nsonable types.\n(2) Among all metrics, the improvement on\nstrict accuracy is the most signiﬁcant. Strict ac-\ncuracy is the proportion of entity mentions whose\npredicted types are completely identical with hu-\nman annotation. It is therefore the most impor-\ntant metric for evaluating how robust the system is\nagainst noisy labels. The ability of LME alleviat-\ning noises from DS contributes to improving strict\naccuracy most.\n3.4 Analysis of λ\nWe choose the optimalλvalues for results in Table\n3 according to their performances on the develop-\nment set. After they are chosen, we compare the\nresults on the test set under different values ofλin\nFigure 2.\nConclusions from the previous subsection can\nbe seen again: when λ is set to a proper value,\nour model can consistently outperform the base-\nline over all metrics; strict accuracy is the metric\nwith the most signiﬁcant improvements.\n0.0 0.001 0.005 0.01\nλ\n-2\n-1\n0\n1\n2\n3\nImprovement from NFGEC(%)\nWiki\n0.0 0.001 0.005 0.01\nλ\n-2\n-1\n0\n1\n2\nOntoNotes\nstrict\nmacro\nmicro\nFigure 2: The performance under different λ values.\nλ = 0 is the vanilla NFGEC. Note that values of\nthe vertical axis are improvements compared to vanilla\nNFGEC.\nAlso, we notice that the performances deteri-\norate when λ grows too large, and may even be\nworse than the baseline. The reason is that LME\nis a kind of regularization: its role is only in the\ntraining phase, exchanging the performance on\ntraining set with that on test set. So λ, as a reg-\nularization coefﬁcient, must be carefully chosen.\n3.5 Qualitative Analysis\nIn order to have an intuitive feeling of the model,\nwe provide an example of LME’s effect.\nIn the following sentence (from the test set of\nWIKI ), both models try to predict the type ofLake\nPlacid which, in this very context, is a town in\nNew York. We show all labels with at least one\nscore over the threshold 0.5, or is annotated true\nby human in Table 4.\nScaringe dismissed Brian Barrett ofLake\nPlacid as one of his defense attorneys.\nType NFGEC +LME Human\n/person 0.622 0.150 False\n/location 0.323 0.627 True\n/location/city 0.024 0.208 True\nTable 4: An example, showing the scores by two mod-\nels as well as human annotation.\nNFGEC predicts a high score for /person\nand a low score for /location, probably be-\ncause both words of the entity mention are ﬁrst-\nletter capitalized and thus look like a person’s\nname. LME, however, may consider the sen-\ntence structure person of locationto be more rea-\nsonable than person of person, and makes the\n997\ncorrect judgment between these two labels. As\nfor /location/city, LME also shows higher\nconﬁdence than NFGEC, but it is still regretfully\nbelow the threshold. This also demonstrates a\nweakness of LME: limited by the performance of\nthe ET module. Addressing this limitation can be\nconsidered as a future direction for improvement.\n4 Conclusion\nIn this paper, we propose a novel architecture\nLME to improve entity typing systems. It utilizes\na language model and a set of label embeddings\nto judge the compatibility between labels and con-\ntext sentences, and reduces noises introduced by\nDS. Experiments demonstrate that LME is capable\nof helping NFGEC, a state-of-the-art entity typing\nmodel, to alleviate the problem of noisy labels,\nand reaching a new state-of-the-art performance.\nSince the LME module does not depend on the ET\nmodule, we are conﬁdent that LME can be adapted\nto other entity typing systems as well.\nFuture Work Utilizing meaning of labels to al-\nleviate the problem of noises from DS is an intere-\nsting direction. We make the ﬁrst attempt in this\npaper, and we believe the direction is worth further\nexploring. For example, (1) how to train a lan-\nguage model that is sensitive with incorrect labels;\n(2) how to combine meaning of labels with the hi-\nerarchical structure of types; (3) how to ﬁnd the\noptimal λeasily for a new dataset. LME may also\nbe extended to other tasks that also suffer from\nnoises and incompleteness of DS, such as relation\nextraction (Takamatsu et al., 2012; Ritter et al.,\n2013; Lin et al., 2016). However, since a relation\ndoes not have a speciﬁc location in the sentence, it\nneeds more effort than a simple replacement.\nAcknowledgment\nThis work is supported by the 973 Program (No.\n2014CB340501), the National Natural Science\nFoundation of China (NSFC No. 61572273,\n61661146007) and Tsinghua University Initiative\nScientiﬁc Research Program (20151080406). This\nwork is also funded by China Association for Sci-\nence and Technology (2016QNRC001). We also\nthank anonymous reviewers for their insightful\nsuggestions.\nReferences\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. JMLR.\nMohamed Chabchoub, Michel Gagnon, and Amal\nZouaq. 2016. Collective disambiguation and seman-\ntic annotation for entity linking and typing. In Pro-\nceedings of SWEC.\nLuciano Del Corro, Abdalghani Abujabal, Rainer\nGemulla, and Gerhard Weikum. 2015. Finet:\nContext-aware ﬁne-grained named entity typing. In\nProceedings of EMNLP.\nLi Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu.\n2015. A hybrid neural model for type classiﬁcation\nof entity mentions. In Proceedings of IJCAI.\nDan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse\nKirchner, and David Huynh. 2014. Context-\ndependent ﬁne-grained entity type tagging. arXiv\npreprint arXiv:1412.1820.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\nSanjeev Karn, Ulli Waltinger, and Hinrich Sch ¨utze.\n2017. End-to-end trainable attentive decoder for hi-\nerarchical entity classiﬁcation. In Proceedings of\nEACL.\nYankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan,\nand Maosong Sun. 2016. Neural relation extraction\nwith selective attention over instances. In Proceed-\nings of ACL.\nXiao Ling and Daniel S Weld. 2012. Fine-grained en-\ntity recognition. In Proceedings of AAAI.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of INTERSPEECH.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In Proceedings of ACL.\nMakoto Miwa and Yutaka Sasaki. 2014. Modeling\njoint entity and relation extraction with table repre-\nsentation. In Proceedings of EMNLP.\nArvind Neelakantan and Ming-Wei Chang. 2015. In-\nferring missing entity type instances for knowledge\nbase completion: New dataset and methods. Pro-\nceedings of NAACL.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of EMNLP.\nXiang Ren, Wenqi He, Meng Qu, Lifu Huang, Heng\nJi, and Jiawei Han. 2016a. Afet: Automatic ﬁne-\ngrained entity typing by hierarchical partial-label\nembedding. In Proceedings of EMNLP.\n998\nXiang Ren, Wenqi He, Meng Qu, Clare R V oss, Heng\nJi, and Jiawei Han. 2016b. Label noise reduction in\nentity typing by heterogeneous partial-label embed-\nding. In Proceedings of KDD.\nAlan Ritter, Mausam, Luke Zettlemoyer, and Oren Et-\nzioni. 2013. Modeling missing data in distant super-\nvision for information extraction. TACL.\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE Transactions\non Signal Processing.\nSonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and\nSebastian Riedel. 2017. Neural architectures for\nﬁne-grained entity type classiﬁcation. In Proceed-\nings of EACL.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn Proceedings of INTERSPEECH.\nShingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.\n2012. Reducing wrong labels in distant supervision\nfor relation extraction. In Proceedings of ACL.\nJi Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun.\n2018. Improving neural ﬁne-grained entity typing\nwith knowledge attention. In Proceedings of AAAI.\nPeng Xu and Denilson Barbosa. 2018. Neural ﬁne-\ngrained entity type classiﬁcation with hierarchy-\naware loss. In Proceedings of NAACL.\nYadollah Yaghoobzadeh and Hinrich Sch ¨utze. 2015.\nCorpus-level ﬁne-grained entity typing using con-\ntextual information. In Proceedings of EMNLP.\nMohamed Yahya, Klaus Berberich, Shady Elbassuoni,\nand Gerhard Weikum. 2013. Robust question an-\nswering over the web of linked data. In Proceedings\nof CIKM.\nLimin Yao, Sebastian Riedel, and Andrew McCallum.\n2013. Universal schema for entity type prediction.\nIn Proceedings of Workshop on AKBC.\nDani Yogatama, Daniel Gillick, and Nevena Lazic.\n2015. Embedding methods for ﬁne grained entity\ntype classiﬁcation. In Proceedings of ACL.\nMohamed Amir Yosef, Sandro Bauer, Johannes Hof-\nfart, Marc Spaniol, and Gerhard Weikum. 2012.\nHYENA: Hierarchical type classiﬁcation for entity\nnames. In Proceedings of COLING.\nZheng Yuan and Doug Downey. 2018. Otyper: A neu-\nral architecture for open named entity typing. In\nProceedings of AAAI.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8952000141143799
    },
    {
      "name": "Natural language processing",
      "score": 0.6937741041183472
    },
    {
      "name": "Language model",
      "score": 0.665917158126831
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5883545279502869
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5364068150520325
    },
    {
      "name": "Entity linking",
      "score": 0.4938517212867737
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46337810158729553
    },
    {
      "name": "Code (set theory)",
      "score": 0.46167895197868347
    },
    {
      "name": "Source code",
      "score": 0.43290793895721436
    },
    {
      "name": "Typing",
      "score": 0.4322342276573181
    },
    {
      "name": "Baseline (sea)",
      "score": 0.41475972533226013
    },
    {
      "name": "Information retrieval",
      "score": 0.41467660665512085
    },
    {
      "name": "Programming language",
      "score": 0.2566509246826172
    },
    {
      "name": "Speech recognition",
      "score": 0.15277516841888428
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Knowledge base",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}