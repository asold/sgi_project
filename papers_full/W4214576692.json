{
    "title": "Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer",
    "url": "https://openalex.org/W4214576692",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2773273286",
            "name": "Seungyeon Rhyu",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2139630975",
            "name": "HyeonSeok Choi",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2101246290",
            "name": "Sarah Kim",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2099145312",
            "name": "Kyogu Lee",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2773273286",
            "name": "Seungyeon Rhyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2139630975",
            "name": "HyeonSeok Choi",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2101246290",
            "name": "Sarah Kim",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2099145312",
            "name": "Kyogu Lee",
            "affiliations": [
                "Seoul National University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2414720345",
        "https://openalex.org/W2112221503",
        "https://openalex.org/W2142996485",
        "https://openalex.org/W2115325536",
        "https://openalex.org/W6606553729",
        "https://openalex.org/W6894144410",
        "https://openalex.org/W6714284124",
        "https://openalex.org/W6746292182",
        "https://openalex.org/W6746781809",
        "https://openalex.org/W3124507227",
        "https://openalex.org/W3161106948",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755406732",
        "https://openalex.org/W6751252338",
        "https://openalex.org/W3092879656",
        "https://openalex.org/W3137883189",
        "https://openalex.org/W6640963894",
        "https://openalex.org/W1843386516",
        "https://openalex.org/W6749351710",
        "https://openalex.org/W6774882322",
        "https://openalex.org/W2963713328",
        "https://openalex.org/W2998560404",
        "https://openalex.org/W3112493053",
        "https://openalex.org/W6771747371",
        "https://openalex.org/W3015625764",
        "https://openalex.org/W6763362620",
        "https://openalex.org/W6770099786",
        "https://openalex.org/W6780713994",
        "https://openalex.org/W6782288222",
        "https://openalex.org/W6612795296",
        "https://openalex.org/W1983033388",
        "https://openalex.org/W3027186469",
        "https://openalex.org/W2963408210",
        "https://openalex.org/W6765759998",
        "https://openalex.org/W6781678156",
        "https://openalex.org/W6782602824",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W2026162824",
        "https://openalex.org/W6687045409",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6754559877",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6755312952",
        "https://openalex.org/W6676368125",
        "https://openalex.org/W1975929202",
        "https://openalex.org/W6682743674",
        "https://openalex.org/W2096551245",
        "https://openalex.org/W4247577772",
        "https://openalex.org/W6749053379",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3013310839",
        "https://openalex.org/W1556449458",
        "https://openalex.org/W1959608418"
    ],
    "abstract": "Recent deep learning approaches for melody harmonization have achieved remarkable performance by overcoming the uneven chord distributions of music data. However, most of these approaches have not attempted to capture an original melodic structure and generate structured chord sequences with appropriate rhythms. Hence, we use a Transformer-based architecture that directly maps lower-level melody notes into a semantic higher-level chord sequence. In particular, we encode the binary piano roll of a melody into a note-based representation. Furthermore, we address the flexible generation of various chords with Transformer expanded with a VAE framework. We propose three Transformer-based melody harmonization models: 1) the standard Transformer-based model for the neural translation of a melody to chords (STHarm); 2) the variational Transformer-based model for learning the global representation of complete music (VTHarm); and 3) the regularized variational Transformer-based model for the controllable generation of chords (rVTHarm). Experimental results demonstrate that the proposed models generate more structured, diverse chord sequences than LSTM-based models.",
    "full_text": "Received February 5, 2022, accepted February 19, 2022, date of publication February 28, 2022, date of current version March 17, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3155467\nTranslating Melody to Chord: Structured\nand Flexible Harmonization of\nMelody With Transformer\nSEUNGYEON RHYU\n1, HYEONSEOK CHOI\n 1, SARAH KIM\n 1,\nAND KYOGU LEE\n 1,2, (Member, IEEE)\n1Department of Intelligence and Information, Seoul National University, Seoul 08826, Republic of Korea\n2Artiﬁcial Intelligence Institute, Seoul National University, Seoul 08826, Republic of Korea\nCorresponding author: Kyogu Lee (kglee@snu.ac.kr)\nThis work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was\ngranted by Institutional Review Board (IRB) of Seoul National University.\nABSTRACT Recent deep learning approaches for melody harmonization have achieved remarkable perfor-\nmance by overcoming the uneven chord distributions of music data. However, most of these approaches\nhave not attempted to capture an original melodic structure and generate structured chord sequences\nwith appropriate rhythms. Hence, we use a Transformer-based architecture that directly maps lower-level\nmelody notes into a semantic higher-level chord sequence. In particular, we encode the binary piano roll\nof a melody into a note-based representation. Furthermore, we address the ﬂexible generation of various\nchords with Transformer expanded with a V AE framework. We propose three Transformer-based melody\nharmonization models: 1) the standard Transformer-based model for the neural translation of a melody\nto chords (STHarm); 2) the variational Transformer-based model for learning the global representation of\ncomplete music (VTHarm); and 3) the regularized variational Transformer-based model for the controllable\ngeneration of chords (rVTHarm). Experimental results demonstrate that the proposed models generate more\nstructured, diverse chord sequences than LSTM-based models.\nINDEX TERMS Music information retrieval, computer generated music, neural networks, self-supervised\nlearning.\nI. INTRODUCTION\nAutomatic melody harmonization, which ﬁnds a coherent\nchord sequence that ﬁts the given notes in a melody, is an\nessential topic in music generation. This task, which imi-\ntates the harmonizing process, is important for understanding\nhuman composition [1]. It is also practical for commercial\nuse since it can reduce barriers to creating music without\nexpertise [2], [3].\nA melody harmonization task requires capturing the\nlong-term dependencies in music since a constrained sets\nof chord progressions can consistently interact with a given\nmelody [4]. This has motivated the use of linguistic tech-\nniques such as context-free grammar [5], genetic algo-\nrithms [6], or hidden Markov models (HMMs) [3], [7], [8].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Juntao Fei\n.\nRecently, deep learning approaches with bidirectional long\nshort-term memory (BLSTM) showed robust performance by\neffective nonlinear sequential modeling of bar- or half-bar-\nbased melody and chords [9]–[11]. Moreover, these studies\nsuccessfully overcame the uneven chord distributions that are\nin common musical data.\nNevertheless, these LSTM-based studies had limitations\nin generating concrete chord structures. First, the models\nwere unable to encode an original melodic structure despite\ntheir sequential architectures [4]. The notes in a melody\nwere aggregated within a chord duration into a pitch-class\nhistogram before being fed to the model. Second, the models\ndid not explicitly consider capturing the patterns of chord pro-\ngressions. Chord labels correspond to the constant time grids\n(e.g., a bar or half-bar). Sequential modeling of grid-based\nchord labels is likely to result in ambiguous patterns or hier-\narchies of the generated outputs [8].\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 28261\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nHence, we attempt to utilize a recent language model,\nTransformer, for structured melody harmonization. Trans-\nformer directly encodes inter- and intra-structures between\ntwo sequential data in dynamic length [12]. Thus, with Trans-\nformer, we can approach melody harmonization as the trans-\nlation between two different languages, melody notes and\nchord labels, which share a semantic musical context.\nHowever, conventional Transformer-based studies encoded\nmusic as a series of musical events [15]. Using event-based\nrepresentations differs from how humans perceive a rendered\nor score-written melody for harmonization [16]. Instead,\na grid-based melody representation can be more intuitive\nfor modeling melodic patterns synchronized with chord\nlabels [4], [17], [18]. In our work, we convert a melody into a\nmore intuitive note-based representation, where each frame\nrepresents one note. To this end, we use a novel time-to-note\ncompression method to map a binary piano roll representation\ninto a note-based embedding.\nIn addition, we expand the conventional chord predic-\ntion task to a ﬂexible harmonization task using a variational\nautoencoder (V AE) [19]. A melody can introduce diverse\ninterpretations from multiple perspectives toward its musical\nstructure or the arrangers’ personalities [10], [20]. Therefore,\nit is more intuitive to sample chords from the proper distribu-\ntion of real-world music. Current music generation systems\nhave also leveraged V AE-based methods to produce creative\noutputs from the latent space [21]. However, most previous\nstudies of melody harmonization have aimed at the static\ngeneration of chords with ﬁxed model parameters. Thus,\nwe utilize the V AE setting, which explicitly approximates the\ngeneral chord distribution, for stochastic harmonization.\nWe concretely use the variational Transformer inspired\nby Lin et al. [22]. They used a Transformer-based model\nextended by a conditional V AE framework to gener-\nate a response from a conditional context. We leverage\nthis seq2seq architecture to achieve a variational neural\nmachine translation (VNMT) from a given melody to the\nchords [23]–[25]. To the best of our knowledge, we are\nthe ﬁrst to apply the VNMT approach to music generation.\nIn particular, our approach is different from previous music\ngeneration studies using the variational Transformer, which\nmostly served as an autoencoder [26], [27].\nFurthermore, we attempt to regularize the variational\nTransformer for controlling the chord outputs through a dis-\nentangled representation. Generating arbitrary sets of chords\nmay not satisfy users who would like to create music based on\ntheir own tastes. In terms of building interactive music gen-\neration systems as well as learning a good representation for\nsequential data, controllable generation with the V AE frame-\nwork has mainly been approached by recent studies. These\nstudies have aimed to learn disentangled representations for\nhigh-level musical features, such as pitch, rhythm, harmony,\ncontext, or arousal, through supervised learning [28]–[31].\nInspired by these studies, we use domain-speciﬁc induc-\ntive bias to achieve a disentangled representation for the\nwell-summarized context of the target melody and chords.\nIn particular, we exploit an auxiliary regularization method\nproposed by Pati and Lerch [32] to force the target represen-\ntation to be related to the musical attribute. We set the number\nof unique chords in a chord progression as a controllable\nattribute of the generated chords.\nIn this paper, we propose three Transformer-based mod-\nels for structured and ﬂexible chord generation from a\ngiven melody. These models are based on three types of\nTransformer architecture: 1) the Standard Transformer for\nstructured Harmonization (STHarm), 2) the Variational\nTransformer for ﬂexible Harmonization (VTHarm), and\n3) the regularized Variational Transformer for controllable\nHarmonization (rVTHarm). Our contribution also lies in\nthe substantial evaluations of each model’s performance\nusing multiple datasets. One dataset is a benchmark dataset\nof popular music that is used for the direct comparison\nwith previous approaches. The other dataset contains music\nfrom the contemporary genre, such as jazz, which pos-\nsesses relatively higher musical tension than popular music.\nThese datasets also differ by whether a key signature is\nnormalized. Therefore, we assess the harmonization models\nin various dataset settings. The experimental results sup-\nport that STHarm, VTHarm, and rVTHarm can capture\nstructured contexts within and between melody and chord\nsequences, increase chord diversity, and explicitly control\nchord outputs, respectively, compared to LSTM-based mod-\nels. The source code for the proposed methods is available at\nhttps://github.com/rsy1026/harmonizers_transformer.\nII. RELATED WORKS\nA. MELODY HARMONIZATION\nRule-based studies aim to simulate structural chord progres-\nsions carefully using linguistic techniques and heavy domain\nknowledge [4], [5], [33], [34]. Generic algorithms (GAs)\nwere early probabilistic solutions that were combined with\nrule-based constraints [6], [20]. Machine learning approaches\nsuch as hidden Markov models (HMMs) demonstrate the use\nof probabilistic modeling to assess temporal dependency in\nmusic [3]. However, due to the inability of a standard HMM\nto capture elaborate harmonic functions, the HMM-based\nmodel was improved with domain knowledge [7] or\ntree-structured Markov models based on probabilistic\ncontext-free grammar [8], [35].\nLim et al. [9] utilized a stacked bidirectional long short-\nterm memory (BLSTM) model to predict a chord for each bar\nof a given melody that was aggregated into a pitch-class his-\ntogram. This LSTM-based approach successfully improved\nmodel robustness for the skewed distribution of commonly\nused chords. Recently, Yeh et al. [10] revisited a sufﬁcient\nnumber of conventional methods and consequently proposed\nMTHarmonizer, a deep multitask model that predicts chords\nwith correct phrasings by directly supervising harmonic func-\ntions. Canonical metrics for assessing the coherence and\ndiversity of the created chord sequence were also proposed.\nSun et al. [11] used the orderless neural autoregressive\n28262 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nTABLE 1. Summary of the differences between the previous and proposed approaches for melody harmonization. The top attributes are related\nto the model architecture and experimental settings. The bottom attributes are related to the objectives of the studies. Bolded text indicates\ndistinct differences between the proposed methods and the other methods.\ndistribution estimation (NADE) and the blocked Gibbs sam-\npling method to approximate the complex joint probability\namong chords given a melody. They provided the model with\na masked chord sequence so that the model could predict\nmasked entries and leveraged class weights to efﬁciently\nbalance the uneven distribution of chords.\nThese LSTM-based models shared the same data repre-\nsentation and model architecture. In particular, the models\nby Yeh et al. and Sun et al., which improved the musical\ngrammar or the diversity of chord types, were extensions\nof the model by Lim et al.. However, we assume that the\nLSTM-based approach is limited to modeling a serialized\nchord sequence without capturing the realistic pattern of\nchords. Our proposed models, which investigate the intrap-\natterns and interrelationship of the melody and chords, reveal\nthe main difference in the model architecture. Table 1 sum-\nmarizes additional details on how the proposed models differ\nfrom the LSTM-based approaches in terms of experimental\nsettings and objectives.\nB. TRANSFORMER-BASED MUSIC GENERATION\nMusic Transformer, introduced by Huang et al. [15], was\none of the successful models for various objectives of\nlong-term symbolic music generation. These researchers\napplied an event-based representation to polyphonic music\nperformance data [36]. LakhNES used the extended Trans-\nformer architecture, Transformer-XL, to generate plausible\nmulti-instrumental game sound chips [37]. Pop Music Trans-\nformer also used Transformer-XL and a novel data repre-\nsentation called ‘‘revamped MIDI-derived events (REMI)’’\nwas proposed to consider the metrical structure for gener-\nating polyphonic pop music [17]. Jazz Transformer adapted\nREMI to jazz music to create long-term coherent jazz lead\nsheets [38]. More recently, chord conditioned melody trans-\nformer (CMT) leveraged Transformer decoders to generate a\ngrid-based melody given a chord progression [18]. This work\nattempted to create a melody with proper rhythms that were\nwell aligned with the given chords. This work was similar to\nthe current interest of our study.\nFurthermore, Choi et al. [26] proposed a Transformer-based\nautoencoder that achieved global representation for the\nmusical contexts of polyphonic piano performance data.\nJiang et al. [27] introduced a hierarchical Transformer V AE\nto learn context-sensitive melody representation with self-\nattention blocks, enabling the model to control the melodic\nand rhythmic contexts.\nC. MUSIC GENERATION FROM DISCRIMINATIVE\nLEARNING\nDiscriminative learning frameworks have been adapted to\nmusic generation studies. For example, Pati and Lerch [32]\nproposed a novel loss function for regularizing a latent vari-\nable to correlate with one musical attribute. EC 2-V AE and\nExtRes decoupled representations of a melody’s pitch and\nrhythmic attributes by intermediate supervision [28], [29].\nFaderNet controlled polyphonic music by both low-level and\nhigh-level musical attributes using direct supervision and\nthe regularization scheme from Pati and Lerch [30], [32].\nWang et al. [39] decoupled chord and texture attributes\nfor interpretable generation of polyphonic music. PianoTree\nV AE aimed to achieve a representation of a tree-structured\nmusical syntax [31].\nIII. PROPOSED METHOD\nWe propose three models based on Transformer target-\ning structured and ﬂexible melody harmonization. The ﬁrst\nmodel uses the standard Transformer model to translate a\nmelody to a chord sequence. The second model uses the\nvariational Transformer to learn a global latent representation\nof the complete music [22]. The last model regularizes the\nrepresentation of the variational Transformer to control har-\nmonic attributes. We name these models STHarm, VTHarm,\nand rVTHarm, respectively. In each model, the Transformer\nencoder receives a given melody, and the decoder generates a\nchord sequence according to the attention weights computed\nbetween the melody and chords. The overall structures of the\nproposed models are illustrated in Fig. 1.\nA. STANDARD TRANSFORMER MODEL (STHarm)\nSTHarm generally follows the original Transformer model,\nexcept that the input and output representations are not event-\nbased [12]. Instead, we use a binary melody piano roll and\nserialized chord labels instead of musical event tokens. Each\nframe of the melody piano roll represents the same temporal\nlength.\nLet x1:T ∈{0, 1}T ×|P|be a one-hot vector sequence of a\ngiven melody, where T is the length of the melody, |P|is the\nnumber of pitches, and t is a time index by the length of a\nVOLUME 10, 2022 28263\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nFIGURE 1. The overall architectures of the proposed methods: (a) STHarm and (b) VTHarm and rVTHarm. VTHarm and rVTHarm share the same\narchitecture. The colored area and dotted lines represent the modified parts, from the vanilla Transformer.\nsixteenth note. The encoder receives the input x1:T to capture\nthe notewise melodic context as (1):\ne(S)\nT =Embedding(x1:T )\ne(S)\nN =TimeToNote(e(S)\nT +wT ,M)\nEnc(x1:T ) =Self-AttBlocks(e(S)\nN +wN ) (1)\nwhere eT , eN , S, and N denote the time-level embed-\nding vectors, note-level embedding vectors, STHarm, and\nthe number of melody notes, respectively, Embedding and\nSelf-AttBlocks denote the embedding layer and L multi-\nhead self-attention blocks that are identical to the vanilla\nTransformer, respectively [12], w∗denotes a sinusoidal posi-\ntional embedding scaled by a trainable weight [40], and\nTimeToNote is a novel method that we propose to convert the\ntimewise embedding to the notewise embedding to capture the\nnote patterns in a melody.\nIn the Time2Note procedure, we add the scaled positional\nembedding wT to e(S)\nT . Then, we transfer it to the notewise\nembedding e(S)\nN with average pooling by an alignment matrix\nM ∈ {0,1}T ×N as (2), where M indicates the alignment\npath between a piano roll and a series of notes. This process\nenables each frame of the notewise embedding to preserve the\ninformation of the original note duration:\nTimeToNote(e,M) =Linear\n(\nMT ·e\n∑T\nt=1 Mt,1:N\n)\n(2)\nwhere Linear denotes a fully connected layer. The com-\npressed embedding e(S)\n1:N is added to another scaled positional\nembedding wN and passes through the L multihead self-\nattention blocks.\nThe decoder receives the right-shifted target chords and\ncomputes attention with the encoder output Enc(x 1:T ) to pre-\ndict the chords as (3):\ne(S)\nO =Embedding(y0:O−1)\n´e(S)\nO =AttBlocks(e(S)\nO +wO,Enc(x1:T ))\np(˜y1:O) =Softmax(Linear(´e(S)\n1:O)) (3)\nwhere y0:O−1 ∈{0, 1}O×|C|is a sequence of one-hot vectors\nfor the right-shifted target chords, O is the length of the chord\nsequence, |C|is the number of chord classes, and AttBlocks\ndenotes L loops of the Transformer attention blocks. The ﬁnal\nprobabilities are estimated by a ﬁnal linear layer with softmax\nactivation.\nB. VARIATIONAL TRANSFORMER MODEL (VTHarm)\nThe proposed architecture of VTHarm is inspired by [22].\nVTHarm has an additional probabilistic encoder for a latent\nvariable z, where z represents the global attribute of the\naggregated melody and chords. We denote this encoder as\nthe context encoder. We add a global key signature label as\na conditional input token to the model. The key signature is\nessential for an arbitrary melody to obtain a certain harmonic\ncontext [41]. The key signature token can aid the model in\nspecifying the latent space and sampling the outputs from the\n28264 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nconstrained chord distributions. In contrast, STHarm does not\nuse this token since it ﬁnds the mean distribution for chords\nthat best ﬁt a given melody.\nThe encoder used in VTHarm is identical to the encoder\nused in STHarm, except that the conditional token c is con-\ncatenated at the beginning of the note-based melody embed-\nding e(V)\nN as in (4):\ne(V)\nN+1 =Concats(c,e(V)\nN )\nEnc(c,x1:T ) =Self-AttBlocks(e(V)\nN+1 +wN+1) (4)\nwhere Concat s denotes the concatenation over the sequence\ndimension. The self-attention block can connect c and the\nremaining parts of the embedding and convey any constraints\nto the whole embedding.\nThe context encoder infers the latent representation z from\nthe encoder output, chord input y, and conditional token c\nas (5):\ne(V)\nO+1 =Concats(c,Embedding(y1:O))\n´e(V)\nO+1 =Self-AttBlock(e(V)\nO+1 +wO+1)\nr =Concatd (Pool(Enc(c,x1:T )),Pool(´e(V)\nO+1))\n[µ,σ] =Linear(r) z ∼N(µ,σ) (5)\nwhere V denotes VTHarm, Concat d denotes the concatena-\ntion over the feature dimension, Pool denotes the average\npooling over time, and self-AttBlock denotes only one loop of\nthe self-attention block. The context encoder maps the chord\ninput y1:O into the embedding e(V)\nO . Then, c is concatenated\nat the beginning of e(V)\nO over the sequence dimension before\nthe multihead self-attention blocks. The self-attention output\ncontains the harmonic context according to the key informa-\ntion. It is mean-aggregated over time so that it represents the\nglobal information of the chords [26]. The encoder output\nE(c,x1:T ) is also mean aggregated over time to represent the\nglobal attribute of a melody. These two aggregated vectors\nare concatenated over the feature dimension and pass through\nthe bottleneck, resulting in two parameters, µ, and σ. The\nlatent code z is inferred from µand σ through the reparam-\neterization trick, and its prior is assumed to be the normal\ndistribution [19].\nThe decoder reconstructs the target chords from the\nright-shifted chord input and encoder output, conditioned by\nc and the latent variable z as (6):\ne(V)\no =Concats(z +c,Embedding(y1:O−1))\n´e(V)\nO =AttBlocks(e(V)\nO +wO,Enc(x1:T ))\np(˜y1:O) =Softmax(Linear(´e(V)\nO )) (6)\nThe right-shifted chord input is ﬁrst encoded with the same\nlookup table from the context encoder. The latent variable\nz and the key signature token c are added to the beginning,\nwhich corresponds to the ‘‘start-of-sequence’’ part of the\nchord embedding. The following attention network transfers\nthe aggregated information from z and c to all frames of the\nembedding. The rest of the Transformer decoder reconstructs\nthe target chords.\nC. REGULARIZED VARIATIONAL TRANSFORMER\nMODEL (rVTHarm)\nTraining VTHarm alone cannot guarantee a disentangled\nrepresentation of the desired aspect. Therefore, rVTHarm\naims to achieve a disentangled representation to control\nthe generated chord outputs. We use the auxiliary loss by\nPati et al. [32] to directly supervise the latent representation\nz. In this study, we choose the number of unique chords in the\nprogression, or chord coverage, as a naive attribute for the\nchord complexity [10].\nThe regularization function from Pati et al. assumes that\nthe target dimension of the latent representation can be\ndisentangled by its monotonic relationship with a speciﬁc\nattribute [32]. For example, the target attribute value should\nincrease when the constrained latent dimension is modu-\nlated toward a positive direction. To this end, the differ-\nence between the attribute values of an arbitrary pair of two\nsamples is forced to be the same sign as that between the\ncorresponding latent representations. Let ai and aj be the\ntarget attribute values of the ith and jth batches, respectively,\nwhere i,j ∈[1,B] and B is the batch size. Similarly, let zr\ni\nand zr\nj be the rth dimension values of the latent variables of\nthe ith and jth batches, respectively. A distance matrix Dr is\ncomputed between all pairs of zr\ni and zr\nj in the mini-batch.\nThe corresponding Da is computed in the same way between\nall pairs of ai and aj. We minimize the difference between Dr\nand Da as (7):\nLReg =MSE(tanh(Dr ),sign(Da)) (7)\nwhere MSE is the mean squared error. In this paper, we reg-\nularize the ﬁrst dimension of z, so r =1.\nD. TRAINING OBJECTIVES\nThe main objective for STHarm is maximizing the log\nlikelihood of the estimated chord sequence y given the\nmelody x:\nLST =E[−log pθ(y|x)] (8)\nwhere θare the model parameters of STHarm.\nIn VTHarm, the main goal is to approximate the marginal\ndistribution of y through the objective of negative evidence\nlower bound (ELBO) by minimizing the losses for the recon-\nstruction and Kullback-Leibler divergence (KLD) [19]. The\nchord probability pθ(y) and posterior distribution qφ(z) are\nconditioned by the melody input x and key signature token c,\nwhereas the prior pθ(z) is the normal distribution following\nthe conditional V AE framework [42]:\nLVT =Eqφ(z|x,y,c)[−log pθ(y|x,z,c)]\n+λKL KL(qφ(z|x,y,c)∥pθ(z)) (9)\nwhere qφ is the posterior distribution of z parameterized\nby φ, and λKL is a hyperparameter for balancing the\nKLD loss term [21], [43].\nThis training objective is expanded in rVTHarm by\nthe explicit regularization of the latent space. Therefore,\nVOLUME 10, 2022 28265\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nrVTHarm shares the overall objective with VTHarm except\nfor the added regularization term as (10):\nLrVT =LVT +λRegLReg (10)\nwhere λReg is a hyperparameter for balancing the auxiliary\nloss term.\nTo generate chords, VTHarm and rVTHarm autoregres-\nsively sample the chord output y1:O from the melody\ninput x1:T , latent variable z, and conditional token c as (11):\npθ(y|x,z,c) =\n∏\nO\npθ(yo|x1:t ,y0:o−1,z,c) (11)\nwhere z is sampled from the normal prior N(0,1).\nIV. EXPERIMENTAL SETTINGS\nWe conduct objective and subjective evaluations for the three\nproposed methods. In this section, we explain the settings for\nthe corresponding experiments. We ﬁrst introduce the two\ndatasets used for the experiments. Next, we summarize the\nbaseline models, model settings, and metrics for the evalua-\ntions.\nA. DATASETS\nWe set |P|= 13 for the 12 pitch classes and rest. We convert\nall chords into one of the 72 chords, which are triad chords\nin major, minor, diminished, and seventh chords in major,\nminor, dominant, so that |C| =72. Each note and chord\nare quantized by lengths of sixteenth note and a half-measure\nfor all datasets, respectively. The length of each batch is a\nmaximum of 8 measures. We only use songs with a time\nsignature of 4/4 and all songs are set to 120 BPM. The train-\ning, validation, and test sets for each dataset are divided into\napproximately an 8:1:1 ratio. We construct batches by slicing\neach song into excerpts of 8-measures where 2-measures\noverlap. For each test, we extract 8-measure excerpts without\nan overlap. We use two public datasets that differ in some\nexperimental settings as well as musical characteristics: the\nChord Melody Dataset (CMD) and the Hooktheory Lead\nSheet Dataset (HLSD).\n1) THE CHORD MELODY DATASET (CMD)\nCMD [14] is composed of 473 songs in contemporary gen-\nres such as jazz and pop. The songs in this database are\nonly in the major key, and most of them are transposed to\nall 12 keys. We choose this dataset to examine the model\nperformance from the complex chords in various keys with\nnontrivial tensions. The lead sheets are in the music extensible\nmarkup language (MusicXML) format, where the melody\nand chord labels are manually annotated and are parsed with\nthe existing MusicXML parser [44], [45]. We use 389 songs\nfor the training set and the rest for the validation and test\nsets (48 songs each). As a result, we use 36,528, 1,756,\nand 165 samples for the training, validation, and test sets,\nrespectively.\n2) THE HOOKTHEORY LEAD SHEET DATASET (HLSD)\nHLSD [13] is an online database of melody and chord annota-\ntions that cover various genres, such as the pop, new age, and\noriginal soundtracks. This dataset has been constructed on a\ncrowdsourcing platform called TheoryTab, 1 in which users\nhave transcribed a large number of high quality melodies and\nchords. This dataset contains the raw annotations of melodies\nand chords in XML format, JSON data of the symbolic fea-\ntures of melodies and chords, and piano-roll ﬁgures depicting\nthe melody and chords. We use the JSON data for 9,218 songs\ndivided into 13,335 parts. We also normalize all songs into\nC major or C minor, as in previous studies [10], [11]. Fol-\nlowing Sun et al. [11], we use 500 parts for the test set and\nthe other 500 parts for the validation set. As a result, we use\n32,619, 1,346, and 809 samples for the training, validation,\nand test sets, respectively.\nB. COMPARATIVE METHODS\nWe use two baseline models and one ground truth for our\nstudy. BLSTM by Lim et al. [9] is composed of two stacked\nlayers of bidirectional LSTM. This model has been a base\nfor most of the recent deep learning approaches [10], [11].\nWe use BLSTM to compare the stacked RNN structure with\nTransformer. ONADE by Sun et al. [11] uses the order-\nless NADE and Gibbs sampling. This model represents a\nBLSTM-based model with randomness and improved chord\ndiversity. For the ground truth, we use the original pro-\ngressions from the datasets. We denote the ground truth\nas Human.\nC. TRAINING\nThe embedding sizes of the melody and chord are 128 and\n256, respectively. We use a hidden size of 256, attention head\nsize of 4, number of attention blocks L of 4, and size of the\nlatent variable z of 16. A dropout layer is used after every\nscaled positional encoding at a rate of 0.2. We use an Adam\noptimizer [46] with an initial learning rate of 1e-4, which\nis reduced to 95% after every epoch. We train the proposed\nmodels for 100 epochs with a batch size of 128. To select the\nvalue of λKL , we refer to several studies on V AE-based music\ngeneration in which a scaling weight smaller than 1 encour-\nages better reconstruction [21], [47]. Then, we empirically set\nλKL and λReg to be 0.1 and 1, respectively, which results in the\nbest performance.\nThe models are implemented and evaluated in Python 3\nand the PyTorch deep learning framework of version 1.5.0.\nFor training each model, we use one NVIDIA GeForce\nGTX 1080 Ti. We mostly refer to the previous implemen-\ntations [40], [48] when implementing the vanilla Trans-\nformer. For implementing and training BLSTM and ONADE,\nwe use the original settings [9], [11]. The gradients are\nall clipped to 1 for the learning stability during training of\nall models. VTHarm, rVTHarm, and ONADE are assessed\nwith 10 test samples per melody due to their randomness.\n1https://www.hooktheory.com/theorytab\n28266 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nOther models are evaluated with the samples in maximum\nprobabilities. We use the truncation trick with a threshold\nof 3 for VTHarm and rVTHarm in qualitative and subjective\ntests [49].\nD. METRICS\nWe introduce three categories of metrics for evaluating the\nproposed models: chord coherence and diversity, harmonic\nsimilarity, and subjective evaluation.\n1) CHORD COHERENCE AND DIVERSITY\nWe use six canonical metrics proposed by Yeh et al. that\nhave been leveraged by recent studies [10], [11]. In brief,\nchord histogram entropy (CHE)and chord coverage (CC)\nmeasure chord diversity. Chord tonal distance (CTD)mea-\nsures the coherence of the chord transition. Chord tone\nto non-chord tone ratio (CTR),pitch consonance score\n(PCS), and melody-chord tonal distance (MTD)measure\nthe coherence between the melody and chords:\n• Chord histogram entropy (CHE).This metric com-\nputes the entropy from the histogram of |C|bins that\ncounts the occurrences of the chord classes within the\nchord sequence:\nCHE =−\n|C|∑\ni=1\npi log pi (12)\nwhere pi denotes the probability of the ith bin of the\nhistogram.\n• Chord coverage (CC).This metric is the number of\nunique chord labels that occur in the chord sequence.\n• Chord tonal distance (CTD). This metric is the\nEuclidean distance between the 6-D tonal feature vec-\ntors that represent the two adjacent chords. These vec-\ntors are calculated using the pitch class proﬁle (PCP)\nfeatures [50], [51]. We compute the average of the\nCTD values for all pairs of adjacent chords in each\nprogression. Each CTD is calculated as (13):\nCTDn(d) = 1\n∥cn∥1\n11∑\nl=0\n8(d,l)cn(l)\n0 ≤d ≤5 0 ≤l ≤11 (13)\nwhere n is the chord index, d is one of the dimension\nindices of the 6-D tonal space, cn is the PCP vector of\nthe nth chord, where the number of entries for the chord\ntones is 1 (c n ∈{0, 1}), l denotes one of the 12 entries\nof the PCP vectors, where each entry corresponds to\neach pitch class, and φ(d,l) denotes the dth basis of the\n6-D tonal space for the lth entry of the PCP vector.\nEach basis is deﬁned as (14):\nφl =\n\n\n\n8(0,l)\n8(1,l)\n8(2,l)\n8(3,l)\n8(4,l)\n8(5,l)\n\n\n\n=\n\n\n\n\n\nr1 sin l 7π\n6\nr1 cos l 7π\n6\nr2 sin l 3π\n2\nr2 cos l 3π\n2\nr3 sin l 2π\n3\nr3 cos l 2π\n3\n\n\n\n\n\n0 ≤l ≤11\n(14)\nwhere φl is the complete transition matrix of the 6-D\nfeature vector for the lth entry of the PCP vector, r1, r2\nand r3 are the radii of the three circles that represent\nthe 6-D tonal space. They are set to 1, 1, and 0.5,\nrespectively, as in Harte et al. [51].\n• Chord tone to non-chord tone ratio (CTR).Originally\nnamed CTnCTR, this metric is the ratio of the number of\nchord tones compared to the number of nonchord tones\nand proper nonchord tones, which have a maximum of\n2-semitone intervals to the right-after note:\nCTR =nc +np\nnc +nn\n(15)\nwhere nc, nn, and np denote the number of chord tones,\nnonchord tones, and proper nonchord tones, respec-\ntively, that are computed from the melody notes and\ncorresponding chord labels.\n• Pitch consonance score (PCS).This metric is a con-\nsonance score based on pitch intervals between the\nmelody note and corresponding chord notes. The pitches\nof the melody notes are assumed to always be higher\nthan those of the chord notes. According to the pitch\ninterval, PCS is one of {−1, 0, 1}: 1 for perfect 1st\nand 5th, major/minor 3rd and 6th; 0 for perfect 4th;\nand −1 for other intervals. The PCS values within each\nsixteenth-note window are aggregated into the average.\nWe compute the total average of the aggregated PCS for\nall windows over time.\n• Melody-chord tonal distance (MTD). Originally\nnamed MCTD, this metric is the tonal distance between\neach melody note and its corresponding chord label. It is\ncalculated in the same way as CTD. Each MTD value is\nweighted by the duration of the corresponding melody\nnote. We average the MTD values for all of the melody\nnotes and their chord labels.\n2) HARMONIC SIMILARITY\nWe measure the similarity between the generated and\nhuman-composed chords with three metrics and assume that\nthe chord progressions in the human-composed music inherit\nhierarchical and metrical structures [16], [52]. Hence, we set\nVOLUME 10, 2022 28267\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nTABLE 2. Evaluation results for chord coherence and diversity. CHE and CC measure the chord diversity, whereas the remaining four metrics measure the\nchord coherence: CTD measures the coherence of the chord progression itself. CTD, CTR, PCS, and MTD measure how harmonic the chord progression is\nwith the given melody.\nthe human-composed music as the ground truths of the\nstructured harmonization. Concretely, a system that gener-\nates chord progressions similar to human-composed music\nis assumed to achieve more structured harmonization [35].\nBrieﬂy, the Levenshtein edit distance (LD)is the global\nmatching score between two chord sequences. The tonal\npitch step distance (TPSD) and directed interval class\ndistance (DICD) measure the distance between two chord\nprogressions:\n• Levenshtein edit distance (LD).LD is the Levenshtein\nedit distance between the generated chord labels and\nthe ground-truth labels [35]. It measures the extent to\nwhich the generated chords are substituted for human-\ncomposed chords.\n• Tonal pitch step distance (TPSD).TPSD computes the\ngeometrical dissimilarity between the generated chords\nand the ground-truth chords in terms of the tonal pitch\nspace (TPS) chord distance rule [53]. The TPS between\nchord x and chord y is computed as (16):\nTPS(x,y) =j +k (16)\nwhere j is the least number of steps in one direction\nfrom the chordal root of x to that of y according to the\ncircle-of-ﬁfths rule. In the circle-of-ﬁfths rule, all pitch\nclasses are arranged in intervals of either perfect ﬁfth\nor fourth [54]. The variable k is the number of unique\npitch class indices in the four levels (root, ﬁfths, triadic,\ndiatonic) within the basic space of y compared to x [53].\nThat is, if the pitch class index is shared by y and x,\nit is not counted. We compute the TPS values between\nall pairs of adjacent chords within each progression,\nresulting in a step function. TPSD is calculated as the\narea between the two step functions derived from the two\nchord progressions.\n• Directed interval class distance (DICD).DICD com-\nputes the city block distance between the directed inter-\nval class (DIC) representation vectors for the chord\ntransitions [55]. DIC is the histogram vector of the\ndirectional pitch interval classes, ranging from −5 to 6,\ncomputed between all pairs of chord notes from the\ntwo adjacent chords. We calculate each pitch interval\nfrom each note of the ﬁrst chord to all notes of the\nsecond chord. DICD indicates both the tonal distance\nand direction between the two successive chords.\n3) SUBJECTIVE EVALUATION\nWe expand the conventional criteria [10], [11] for deeper\nanalysis of human judgment. Harmonicity measures how\ncoherent the chords are with a given melody. Unexpected-\nness measures how much the chords deviate from expecta-\ntion. Complexity measures how complex chord progression\nis perceived to be. Preference measures personal favor for\nchord progression [9].\nV. EVALUATION\nIn this section, we introduce the experimental results of\nthe objective and subjective evaluations into several cate-\ngories as follows. First, we compare the results of the pro-\nposed models in chord coherence and diversitywith the\nbaseline models. Next, we measure harmonic similarity to\nhuman-composed musicfor all models to examine whether\nthe proposed models can result in structured harmonization.\nThen, we check with the controllability of rVTHarmfor\nthe intended factor compared with VTHarm. In addition,\nwe introduce the results for the subjective evaluation and\ndiscuss the corresponding results. Moreover, we illustrate\nsome qualitative resultsfor all models to verify the strength\nof the proposed model. Last, we show an ablation study\nto investigate the inﬂuence of the information of the key\nsignature added to the variational models.\nA. CHORD COHERENCE AND DIVERSITY\nWe evaluate the overall coherence and diversity of the gen-\nerated chords. Table 2 shows the results for all models.\nVTHarm and rVTHarm show higher CHE and CC than the\nbaseline models in both datasets. This result indicates that\nthese models have higher chord diversity than the baseline\nmodels. STHarm, on the other hand, reveals the lowest CTD\nand the lowest CHE and CC for all datasets except for CHE on\nHLSD. This implies that STHarm can generate smoother and\nsimpler chord transitions than other models [11]. BLSTM and\nONADE show better PCS and MTD but lower chord diversity\nthan the proposed models.\nMeanwhile, Human shows worse scores for chord coher-\nence than STHarm for the following reasons. 1) The\n28268 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nTABLE 3. Evaluation results for the chord similarity metrics. Lower scores\ncorrespond to higher human composition similarity.\nhuman-composed samples from CMD and HLSD include\n72 different chord types with various amounts of musical\ntensions. 2) STHarm may generate common chords more fre-\nquently from the average chord distribution than the human-\ncomposed music, as shown in the lower diversity scores.\nConcretely, the most frequent chords in real-world music are\ndiatonic chords such as the C, G, and F major chords in\nthe C major key [9]. Since these chords have relatively less\nmusical tension with respect to a melody, they are close to the\nmelody under a music-theoretical space. Thus, these chords\nmay obtain better coherence scores than other chords with\nmore musical tension.\nMoreover, Human shows lower diversity scores than the\nvariational models. We assume that this is because these mod-\nels can produce some infrequent chords far from the mean\ndistribution of real-world music. The nature of stochastic\ngeneration models draws samples from the normal distribu-\ntion [49]. Some of the generated chords may violate the given\nkey signature but increase the information outside the certain\nharmonic context. Hence, they may contribute to higher chord\ndiversity than human-composed music.\nConsequently, the overall results reﬂect a trade-off\nbetween chord coherence and diversity [6], [10]. Addition-\nally, Human cannot serve as the upper bound for the six met-\nrics in both datasets. Therefore, these metrics cannot function\nas complete criteria for determining the good harmonization\nbut only show the model tendencies in the music-theoretical\nperspective [10], [11]. Hence, we are inspired to use addi-\ntional criteria to evaluate the generated outputs with respect\nto human-composed chords.\nB. HARMONIC SIMILARITY TO HUMAN\nWe investigate the harmonic similarity between the\nhuman-composed and generated chords. We use the samples\nfrom Human as the ground truth. This explicit comparison\nwith Human can provide insight into whether the generated\nchords from each model are as well-structured as human-\ncomposed music [8].\nFIGURE 2. Visualization of (a) tSNE results and (b) two dimension values\nfrom z. The top (purple) and bottom (indigo) rows represent the CMD and\nHLSD, respectively. The hue of each plot represents the chord coverage\nvalue.\nTABLE 4. Pearson’s correlation coefficients betweenαand CC of the\ngenerated outputs from VTHarm and rVTHarm. CMD and HLSD are the\nChord Melody Dataset and Hooktheory Lead Sheet Dataset, respectively.\nThe harmonic similarity results are shown in Table 3.\nBLSTM shows the lowest LD compared to the proposed mod-\nels, whereas ONADE shows the highest LD in all datasets.\nThis indicates that BLSTM is better than the proposed models\nat providing the right chords to the melody. However, the\nbetter matching of individual chords does not correspond\nto the higher similarity of the chord sequence in terms of\nmusical structure [53].\nFor TPSD and DICD, STHarm shows the lowest scores in\nall datasets. This implies that STHarm can generate chord\npatterns that is more similar to Human than other models.\nVTHarm and rVTHarm show higher LD scores than BLSTM\nbut better similarity scores than ONADE. This indicates that\nthe VT models tend to have higher substitution probabilities\nbetween chords than BLSTM [53]. This is possible because\nthe VT models are trained to induce some infrequent chords\nthat are far from the mean distribution of real-world chords.\nNonetheless, the VT models are better than ONADE at creat-\ning more human-like chord patterns, even with a larger variety\nof chord types.\nC. CONTROLLING CHORD COMPLEXITY\nWe verify the monotonic relationship between the chord\nattribute and z from rVTHarm. We use VTHarm and\nrVTHarm to infer z from the test melodies and chords. Then,\nthe dimension of z is reduced by two with t-stochastic neigh-\nbor embedding (tSNE) [30]. When visualizing, we use the\nchord coverage value as the third dimension (hue). The tSNE\nresults and two dimensions, the ﬁrst and third, of the original\nz are illustrated in Fig. 2. This ﬁgure shows that the tSNE\nresults of rVTHarm are grouped by the attribute compared\nVOLUME 10, 2022 28269\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nTABLE 5. Subjective evaluation results for the six methods according to whether the participants have known the given melody.\nto VTHarm. The ﬁrst dimension of z from rVTHarm is also\nshown to be monotonically related to the attribute [32].\nIn addition, we examine the attention maps of rVTHarm\nwith different values of α. We randomly sample z, where α\nis set to be one of {−3,0,3}, and generate the chords from z\nand the test melodies. We sum the attention matrices along the\nhead dimension to see the aggregated weights. Fig. 3 shows\nthat the attention weights become balanced and diagonal\nwhen αincreases from −3 to 3. This implies that the decoder\nof rVTHarm tends to focus on more melody notes when α\nincreases.\nFurthermore, we compute Pearson’s correlation coefﬁ-\ncients between α and the CC scores of the corresponding\nchord outputs. Table 4 shows that rVTHarm reveals higher\ncorrelation coefﬁcients than VTHarm for all datasets. This\nconﬁrms that rVTHarm derives a meaningful representation\nfor the intended chord attribute compared to VTHarm.\nD. SUBJECTIVE EVALUATION\nWe conduct a listening test for subjective evaluation.\nWe extract the samples in 8-measure length from the arbi-\ntrary parts of each melody. For rVTHarm, we sample z\nby setting a to randomly be {−3,0,3}. The listening test\ncomprises ten trials, where each trial contains six samples\nof all comparative methods for one melody. A participant 2\ngrades four metrics, Harmonicity (H), Unexpectedness (U),\nComplexity (C), and Preference (P), on a ﬁve-point Likert\nscale for each method [10], [11]. We denote these metrics as\n‘‘H’’, ‘‘U’’, ‘‘C’’, and ‘‘P’’ for simplicity. We collect answers\non whether a participant is familiar with a given melody as\nin Lim et al. [9]. A total of 36 participants were involved in\nthe listening test: 3 participants had degrees in music. Thirty-\ntwo participants indicated that they had musical backgrounds,\nand 25 participants mentioned that they usually listened to\npopular music.\nTable 5 shows that the results mainly support the quantita-\ntive evaluation results. In contrast, STHarm shows the highest\nH score regardless of melody awareness. This suggests that\nSTHarm outputs plausible chords to listen to than the baseline\nmodels. For U and C, VTHarm shows the highest scores, and\n2Every experimental protocol was approved by the Institutional Review\nBoard (IRB) of Seoul National University. Written consent forms were\ncollected from the participants, and the study was conducted according to\nthe ethical standards outlined in the 1962 Helsinki Declaration.\nFIGURE 3. The generated results from rVTHarm in the piano-rolls (top)\nand the corresponding attention matrices (bottom). (a), (b), and\n(c) represent the results from different values ofa ∈{−3, 0,3}.\nTABLE 6. Pearson’s correlation coefficients of U score with P and C\nscores for Human (H), BLSTM (B), ONADE (O), STHarm (S), VTHarm (V),\nand rVTHarm (R) according to the melody awareness.\nthe variational models show lower harmonicity and prefer-\nence scores than STHarm. We assume that the variational\nmodels tend to generate more chords far from the mean\ndistribution of the learned music data than STHarm. Such\nunique chords can reveal more inharmonicity than the fre-\nquent chords, and it may have provided the participants with\nunpleasant feelings. In addition, most participants listened\nto popular music, where common chords with less musical\ntension are used. Therefore, it may have led the participants\nproviding poorer scores on preference as well as harmonicity.\nNevertheless, VTHarm shows a better P score than ONADE\nwith lower U and C scores. This means that VTHarm is\nmore persuasive than the baseline model with lower chord\ncomplexity.\nWe also analyze the subjective results according to melody\nawareness. The results for the two-way analysis of vari-\nance (ANOV A) show that melody awareness and method type\nsigniﬁcantly affect all metric scores (p < 0.05). All models\nachieve a higher P score than without awareness with melody\nawareness. In particular, VTHarm and rVTHarm show higher\n28270 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nFIGURE 4. The generated samples of the five models and the human-composed chords given the melody from the song ‘‘Stella by\nStarlight. ’’ The orange box emphasizes the results from the three proposed models in which the harmonic rhythms follow the binary\nmetrical structure. In contrast, the baseline models show the syncopated rhythms for some chords.\nFIGURE 5. The generated samples of the five models and the human-composed chords given the melody from the song ‘‘Shiny Stockings’’ .\nThe orange box focuses on the results from the three proposed models in which the chord roots progress along the circle-of-fifths rule. The\nred arrows indicate the chromatic progressions where the chord notes descend or ascend by intervals of a major or minor second. These\nprogressions are related to the given melody, where a certain pattern also develops chromatically.\nP scores than ONADE, whereas they have similar or higher\nU and C scores to ONADE. This implies that the participants\nperceive the samples from the VT models to be more plau-\nsible than the baseline models when they know the melody,\neven though the VT models have comparable complexity and\nunexpectedness to the baseline models.\nVOLUME 10, 2022 28271\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\nTABLE 7. Evaluation results of the chord similarity metrics according to\nadding the condition tokenc. VT and rVT denote VTHarm and rVTHarm,\nrespectively.\nWhen the melody is unaware, BLSTM and rVTHarm\nobtain signiﬁcantly lower Preference scores than when the\nmelody is aware (p <0.001). We further compute Pearson’s\ncorrelation coefﬁcient of U with C or P scores, as shown\nin Table 6. As a result, rVTHarm reveals the most nega-\ntive correlation of U with both C and P scores when the\nmelody is aware. This indicates that 1) controlled chords\nare more unexpected and unpleasant with a familiar melody,\nand 2) some factors other than complexity seem to cause an\nincreased unexpectedness in rVTHarm. However, the mean\npreference score of rVTHarm signiﬁcantly increases with\nmelody awareness. This implies that the familiarity of the\nmelody may strongly compensate for the high unexpect-\nedness of rVTHarm. This tendency needs further investi-\ngation to improve the robustness of controllable melody\nharmonization.\nE. QUALITATIVE RESULTS\nFigs. 4 and 5 show some of the actual samples from the\nlistening test for all ﬁve models as well as the human-\ncomposed music. These samples reveal the strengths of the\nproposed models. First, Fig. 4 mainly shows that the proposed\nmodels tend to reproduce the binary metrical structure of the\nchords compared to the baseline models. The binary metric\nstructure is close to real-world music, most of which has\nbeen composed of four beats and strongly inﬂuenced by\nmetrical boundaries [52]. In contrast, the chords generated\nfrom the baseline models show some syncopated rhythms,\nwhich can weaken the metrical boundaries. Fig. 5 illustrates\nanother advantage of the proposed models, which is that\nthe majority of the chord roots tend to shift in intervals\neither of perfect fourth or ﬁfth according to the circle-of-\nﬁfths rule. This aspect reﬂects conventional Western music\ntheory, which serves as domain knowledge for modeling\nreal-world music [51], [54]. Moreover, the proposed models\nare shown to generate some natural chromatic progressions\naccording to the given melody. On the other hand, the baseline\nmodels show some short transitions on the circle-of-ﬁfths\nat arbitrary spots, in contrast to the melody with regular\nphrasings.\nF. ABLATION STUDY\nWe conduct an ablation study to verify the beneﬁt of adding\nthe conditional token c to VTHarm and rVTHarm. We assume\nthat c provides key signature information that can efﬁciently\nconstrain the latent space to a concrete harmonic context,\nimproving the chord structuredness and reconstruction per-\nformance of the model. We compute the chord similarity\nmetrics between the ground truth and generated chords from\nthe VT models according to the presence of c. The results\nare demonstrated in Table 7. This table shows that the\nVT models without c mostly obtain worse scores for all\nsimilarity metrics than the models with c. This indicates that\nadding key signature information to the VT models in most\ncases not only enhances the one-by-one accuracy but also\nimproves the structure of the generated chords to be more\nhuman-like.\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, we have proposed melody harmonization mod-\nels using the standard Transformer (STHarm), variational\nTransformer (VTHarm), and regularized variational Trans-\nformer (rVTHarm). We show that STHarm can create struc-\ntured chords that are more human-like than LSTM-based\nmodels. VTHarm and rVTHarm can also generate more\nplausible chords than the baseline models with compara-\nble chord diversity, especially when the melody is familiar.\nFurthermore, rVTHarm can control chord outputs with the\ndisentangled representation for the intended attribute. Our\nstudy is limited to the shallow investigation of the connec-\ntion between controllable attributes and melody awareness.\nTherefore, we plan to deeply explore the effect of melody\nawareness for more persuasive melody harmonization.\nREFERENCES\n[1] D. Makris, I. Karydis, and S. Sioutas, ‘‘Automatic melodic harmonization:\nAn overview, challenges and future directions,’’ in Trends in Music Infor-\nmation Seeking, Behavior, and Retrieval for Creativity. Hershey, PA, USA:\nIGI Global, 2016.\n[2] S. A. Raczyński, S. Fukayama, and E. Vincent, ‘‘Melody harmonization\nwith interpolated probabilistic models,’’ J. New Music Res., vol. 42, no. 3,\npp. 223–235, Sep. 2013.\n[3] I. Simon, D. Morris, and S. Basu, ‘‘MySong: Automatic accompaniment\ngeneration for vocal melodies,’’ in Proc. 27th Annu. CHI Conf. Hum.\nFactors Comput. Syst. (CHI), 2008, pp. 725–734.\n[4] J.-F. Paiement, D. Eck, and S. Bengio, ‘‘Probabilistic melodic harmo-\nnization,’’ in Proc. 19th Conf. Can. Soc. Comput. Studies Intell., 2006,\npp. 218–229.\n[5] M. J. Steedman, ‘‘The blues and the abstract truth: Music and mental\nmodels,’’ in Mental Models in Cognitive Science . NJ, USA: Lawrence\nErlbaum Associates, 1996, pp. 305–318.\n[6] A. R. R. Freitas and F. G. Guimaraes, ‘‘Melody harmonization in evolu-\ntionary music using multiobjective genetic algorithms,’’ in Proc. 8th Sound\nMusic Comput. Conf. (SMC), Padova, Italy, 2011, pp. 1–8.\n[7] M. Kaliakatsos-Papakostas and E. Cambouropoulos, ‘‘Probabilistic har-\nmonization with ﬁxed intermediate chord constraints,’’ in Proc. 40th\nICMC, Athens, Greece, 2014, pp. 1–8.\n[8] H. Tsushima, E. Nakamura, K. Itoyama, and K. Yoshii, ‘‘Function- and\nrhythm-aware melody harmonization based on tree-structured parsing and\nsplit-merge sampling of chord sequences,’’ in Proc. 18th ISMIR, Suzhou,\nChina, 2017, pp. 1–7.\n28272 VOLUME 10, 2022\nS. Rhyuet al.: Translating Melody to Chord: Structured and Flexible Harmonization of Melody With Transformer\n[9] H. Lim, S. Rhyu, and K. Lee, ‘‘Chord generation from symbolic melody\nusing BLSTM networks,’’ in Proc. 18th ISMIR, 2017, pp. 1–7.\n[10] Y.-C. Yeh, W.-Y. Hsiao, S. Fukayama, T. Kitahara, B. Genchel, H.-M. Liu,\nH.-W. Dong, Y. Chen, T. Leong, and Y.-H. Yang, ‘‘Automatic melody\nharmonization with triad chords: A comparative study,’’ J. New Music Res.,\nvol. 50, no. 1, pp. 37–51, Jan. 2021.\n[11] C.-E. Sun, Y.-W. Chen, H.-S. Lee, Y.-H. Chen, and H.-M. Wang, ‘‘Melody\nharmonization using orderless NADE, chord balancing, and blocked Gibbs\nsampling,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), Jun. 2021, pp. 4145–4149.\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. 31st\nConf. NeurIPS, 2017, pp. 1–11.\n[13] C. Anderson, D. Carlton, R. Miyakawa, and D. Schwachhofer.\n(2021). Hooktheory. Accessed: Sep. 5, 2021. [Online]. Available:\nhttps://www.hooktheory.com\n[14] S. Hiehn. (2019). Chord Melody Dataset. Accessed: Sep. 5, 2021. [Online].\nAvailable: https://github.com/shiehn/chord-melody-dataset\n[15] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck,\n‘‘Music transformer,’’ 2018, arXiv:1809.04281.\n[16] H. C. Longuet-Higgins and M. J. Steedman, ‘‘On interpreting bach,’’ Mach.\nIntell., vol. 6, pp. 221–241, Jan. 1971.\n[17] Y.-S. Huang and Y.-H. Yang, ‘‘Pop music transformer: Beat-based model-\ning and generation of expressive pop piano compositions,’’ in Proc. 28th\nACM Int. Conf. Multimedia, Oct. 2020, pp. 1180–1188.\n[18] K. Choi, J. Park, W. Heo, S. Jeon, and J. Park, ‘‘Chord conditioned\nmelody generation with transformer based decoders,’’ IEEE Access, vol. 9,\npp. 42071–42080, 2021, doi: 10.1109/ACCESS.2021.3065831.\n[19] D. P. Kingma and M. Welling, ‘‘Auto-encoding variational Bayes,’’ 2013,\narXiv:1312.6114.\n[20] M. Tokumaru, K. Yamashita, N. Muranaka, and S. Imanishi, ‘‘Membership\nfunctions in automatic harmonization system,’’ in Proc. 28th IEEE Int.\nSymp. Multiple- Valued Log., May 1998, pp. 350–355.\n[21] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, ‘‘A hierarchical\nlatent vector model for learning long-term structure in music,’’ in Proc.\n35th ICML, Stockholm, Sweden, 2018, pp. 4364–4373.\n[22] Z. Lin, G. I. Winata, P. Xu, Z. Liu, and P. Fung, ‘‘Variational transformers\nfor diverse response generation,’’ 2020, arXiv:2003.12738.\n[23] B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang, ‘‘Variational neural\nmachine translation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess., 2016, pp. 124–141.\n[24] X. Sheng, L. Xu, J. Guo, J. Liu, R. Zhao, and Y. Xu, ‘‘IntroVNMT: An\nintrospective model for variational neural machine translation,’’ in Proc.\n34th AAAI, New York, NY, USA, 2020, pp. 8830–8837.\n[25] X. Liu, J. Zhao, S. Sun, H. Liu, and H. Yang, ‘‘Variational multimodal\nmachine translation with underlying semantic alignment,’’ Inf. Fusion,\nvol. 69, pp. 73–80, May 2021.\n[26] K. Choi, C. Hawthorne, I. Simon, M. Dinculescu, and J. Engel, ‘‘Encoding\nmusical style with transformer autoencoders,’’ in Proc. 37th ICML, 2020,\npp. 1899–1908.\n[27] J. Jiang, G. G. Xia, D. B. Carlton, C. N. Anderson, and R. H. Miyakawa,\n‘‘Transformer V AE: A hierarchical model for structure-aware and inter-\npretable music representation learning,’’ in Proc. IEEE Int. Conf. Acoust.,\nSpeech Signal Process. (ICASSP), May 2020, pp. 516–520.\n[28] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and G. Xia, ‘‘Deep music\nanalogy via latent representation disentanglement,’’ in Proc. 20th ISMIR,\nDelft, The Netherlands, 2019, pp. 596–603.\n[29] T. Akama, ‘‘Controlling symbolic music generation based on concept\nlearning from domain knowledge,’’ inProc. 20th ISMIR, Delft, The Nether-\nlands, 2019, pp. 816–823.\n[30] H. H. Tan and D. Herremans, ‘‘Music fadernets: Controllable music gen-\neration based on high-level features via low-level feature modelling,’’ in\nProc. 21st ISMIR, Montreal, QC, Canada, 2020, pp. 1–8.\n[31] Z. Wang, Y. Zhang, Y. Zhang, J. Jiang, R. Yang, J. Zhao, and G. Xia, ‘‘Pian-\notree V AE: Structured representation learning for polyphonic music,’’ in\nProc. 21st ISMIR, Montreal, QC, Canada, 2020, pp. 1–8.\n[32] A. Pati and A. Lerch, ‘‘Latent space regularization for explicit control of\nmusical attributes,’’ in Proc. 36th ICML, 2019, pp. 1–3.\n[33] P. R. Illescas, D. Rizo, and J. M. Quereda, ‘‘Harmonic, melodic, and func-\ntional automatic analysis,’’ in Proc. 33rd ICMC, Copenhagen, Denmark,\n2007, pp. 1–7.\n[34] H. V. Koops, J. P. Magalhães, and W. B. de Haas, ‘‘A functional approach to\nautomatic melody harmonisation,’’ in Proc. 1st ACM SIGPLAN Workshop\nFunct. Art, Music, Modeling Design (FARM), 2013, pp. 47–58.\n[35] H. Tsushima, E. Nakamura, and K. Yoshii, ‘‘Bayesian melody harmoniza-\ntion based on a tree-structured generative model of chord sequences and\nmelodies,’’IEEE/ACM Trans. Audio, Speech, Language Process., vol. 28,\npp. 1644–1655, 2020.\n[36] S. Oore, I. Simon, S. Dieleman, D. Eck, and K. Simonyan, ‘‘This time\nwith feeling: Learning expressive musical performance,’’ Neural Comput.\nAppl., vol. 32, no. 4, pp. 955–967, Feb. 2020.\n[37] C. Donahue, H. H. Mao, Y. E. Li, G. W. Cottrell, and J. McAuley,\n‘‘LakhNES: Improving multi-instrumental music generation with cross-\ndomain pre-training,’’ in Proc. 20th ISMIR, Delft, The Netherlands, 2019,\npp. 1–8.\n[38] S.-L. Wu and Y.-H. Yang, ‘‘The Jazz transformer on the front line:\nExploring the shortcomings of AI-composed music through quantitative\nmeasures,’’ in Proc. 21st ISMIR, Montreal, QC, Canada, 2020, pp. 1–8.\n[39] Z. Wang, D. Wang, Y. Zhang, and G. Xia, ‘‘Learning interpretable rep-\nresentation for controllable polyphonic music generation,’’ in Proc. 21st\nISMIR, Montreal, QC, Canada, 2020, pp. 1–8.\n[40] N. Li, S. Liu, Y. Liu, S. Zhao, M. Liu, and M. Zhou, ‘‘Neural speech synthe-\nsis with transformer network,’’ in Proc. 33rd AAAI, 2019, pp. 6706–6713.\n[41] C. Raphael and J. Stoddard, ‘‘Functional harmonic analysis using\nprobabilistic models,’’ Comput. Music J., vol. 28, no. 3, pp. 45–52,\nSep. 2004.\n[42] K. Sohn, X. Yan, and H. Lee, ‘‘Learning structured output representation\nusing deep conditional generative models,’’ in Proc. 28th NeurIPS, Mon-\ntreal, QC, Canada, 2015, pp. 1–9.\n[43] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick,\nS. Mohamed, and A. Lerchner, ‘‘β -V AE: Learning basic visual concepts\nwith a constrained variational framework,’’ in Proc. 5th ICLR, Toulon,\nFrance, 2017, pp. 1–22.\n[44] A. Roberts and C. F. Hawthorne. (2021). Magenta Musicxml Parser.\nAccessed: Sep. 5, 2021. [Online]. Available: https://github.com/\nmagenta/note-seq/blob/main/note_seq/musicxml_parser.%py\n[45] D. Jeong, T. Kwon, and J. Nam, ‘‘VirtuosoNet: A hierarchical attention\nRNN for generating expressive piano performance from music score,’’ in\nProc. 32nd NeurIPS, Montreal, QC, Canada 2018, pp. 1–5.\n[46] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[47] G. Brunner, A. Konrad, Y. Wang, and R. Wattenhofer, ‘‘MIDI-V AE: Mod-\neling dynamics and instrumentation of music with applications to style\ntransfer,’’ in Proc. 21st ISMIR, Paris, France, 2018, pp. 1–8.\n[48] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\n17th Assoc. Comput. Linguist. (NAACL), 2018, pp. 1–6.\n[49] A. Brock, J. Donahue, and K. Simonyan, ‘‘Large scale GAN training\nfor high ﬁdelity natural image synthesis,’’ in Proc. 7th ICLR, 2019,\npp. 1–39.\n[50] T. Fujishima, ‘‘Realtime chord recognition of musical sound: A sys-\ntem using common lisp music,’’ in Proc. ICMC, Beijing, China, 1999,\npp. 464–467.\n[51] C. Harte, M. Sandler, and M. Gasser, ‘‘Detecting harmonic change in musi-\ncal audio,’’ in Proc. 1st ACM Workshop Audio Music Comput. Multimedia\n(AMCMM), 2006, pp. 21–26.\n[52] J.-F. Paiement, D. Eck, and S. Bengio, ‘‘A probabilistic model for chord\nprogressions,’’ in Proc. 6th ISMIR, London, U.K., 2005, pp. 1–14.\n[53] W. B. de Haas, F. Wiering, and R. C. Veltkamp, ‘‘A geometrical distance\nmeasure for determining the similarity of musical harmony,’’ Int. J. Multi-\nmedia Inf. Retr., vol. 2, no. 3, pp. 189–202, Sep. 2013.\n[54] F. Lerdahl, ‘‘Tonal pitch space,’’ Music Perception , vol. 5, no. 3,\npp. 315–349, Apr. 1988.\n[55] E. Cambouropoulos, ‘‘A directional interval class representation of chord\ntransitions,’’ in Proc. 12th Int. Conf. Music Percept. Cogn. (ICMPC),\nThessaloniki, Greece, 2012, pp. 1–5.\nVOLUME 10, 2022 28273"
}