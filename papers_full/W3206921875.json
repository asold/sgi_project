{
    "title": "Deep Scaffold Hopping with Multi-modal Transformer Neural Networks",
    "url": "https://openalex.org/W3206921875",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2910834915",
            "name": "Shuangjia Zheng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2226258273",
            "name": "Zengrong Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2511477483",
            "name": "Ai Haitao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103126149",
            "name": "Hongming Chen",
            "affiliations": [
                "Guangzhou Regenerative Medicine and Health Guangdong Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2132719217",
            "name": "Daiguo Deng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130433203",
            "name": "Yuedong Yang",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2992613109",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W4230627164",
        "https://openalex.org/W2067300585",
        "https://openalex.org/W2621742623",
        "https://openalex.org/W2156125289",
        "https://openalex.org/W3098269892",
        "https://openalex.org/W2916581152",
        "https://openalex.org/W2578240541",
        "https://openalex.org/W2992072991",
        "https://openalex.org/W2031441006",
        "https://openalex.org/W2095196563",
        "https://openalex.org/W1993452491",
        "https://openalex.org/W2003242987",
        "https://openalex.org/W4243021945",
        "https://openalex.org/W2903193068",
        "https://openalex.org/W3210504661",
        "https://openalex.org/W2529159351",
        "https://openalex.org/W2134967712",
        "https://openalex.org/W2966702937",
        "https://openalex.org/W2063496698",
        "https://openalex.org/W2074026532",
        "https://openalex.org/W4242346854",
        "https://openalex.org/W2562257444",
        "https://openalex.org/W2061937956",
        "https://openalex.org/W2005518172",
        "https://openalex.org/W2951433247",
        "https://openalex.org/W2035162463",
        "https://openalex.org/W3011847211",
        "https://openalex.org/W4297951436",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2800465968",
        "https://openalex.org/W3012836410",
        "https://openalex.org/W2994678679",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1970859533",
        "https://openalex.org/W2923747700",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2046911317",
        "https://openalex.org/W2790808809",
        "https://openalex.org/W2902415322",
        "https://openalex.org/W2914635984",
        "https://openalex.org/W2899216155",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W4235871131",
        "https://openalex.org/W4312039930",
        "https://openalex.org/W2912212024",
        "https://openalex.org/W2268755124",
        "https://openalex.org/W2100233978",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2069244634",
        "https://openalex.org/W1988037271",
        "https://openalex.org/W3034516664",
        "https://openalex.org/W2075186867",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3109493217"
    ],
    "abstract": "Scaffold hopping, aiming to identify molecules with novel scaffolds but share a similar target biological activity toward known hit molecules, has always been a topic of interest in rational drug design. Computer-aided scaffold hopping would be a valuable tool but at present it suffers from limited search space and incomplete expert-defined rules and thus provides results of unsatisfactory quality. To addree the issue, we describe a fully data-driven model that learns to perform target-centric scaffold hopping tasks. Our deep multi-modal model, DeepHop, accepts a hit molecule and an interest target protein sequence as inputs and design bioisosteric molecular structures to the target compound. The model was trained on 50K experimental scaffold hopping pairs curated from the public bioactivity database, which spans 40 kinases commonly investigated by medicinal chemists. Extensive experiments demonstrated that DeepHop could design more than 70% molecules with improved bioactivity, high 3D similarity, while low 2D scaffold similarity to the template molecules. Our method achieves 2.2 times larger efficiency than state-of-the-art deep learning methods and 4.7 times than rule-based methods. Case studies have also shown the advantages and usefulness of DeepHop in practical scaffold hopping scenario.",
    "full_text": "Deep Scaffold Hopping with Multi-modal Transformer Neural \nNetworks \nShuangjia Zheng1#, Zengrong Lei2#, Haitao Ai2, Hongming Chen3, Daiguo Deng2*, Yuedong Yang1* \n1School of Data and Computer Science, Sun Yat-sen University, China, 132 East Circle at University \nCity, Guangzhou 510006, China \n2Fermion Technology Co., Ltd, 1088 Newport East Road, Guangzhou 510335, China \n3Centre of Chemistry and Chemical Biology, Bioland Laboratory (Guangzhou Regenerative \nMedicine and Health Guangdong Laboratory), Guangzhou 510530, China  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAbstract \nScaffold hopping, aiming to identify molecules with novel scaffolds but share a similar target \nbiological activity toward known hit molecules, has always been a topic of interest in rational drug \ndesign. Computer-aided scaffold hopping would be a valuable tool but at present it suffers from limited \nsearch space and incomplete expert -defined rules and thus provides results of unsatisfactory quality. \nTo addree the issue, we describe a fully data-driven model that learns to perform target-centric scaffold \nhopping tasks. Our deep multi -modal model, DeepHop, accepts a hit molecule and an interest target \nprotein sequence as inputs and design bioisosteric molecular structures to the target compound . The \nmodel was trained on 50K exper imental scaffold hopping pairs curated from the public bioactivity \ndatabase, which spans 40 kinases commonly investigated by  medicinal chemists. Extensive \nexperiments demonstrated that DeepHop could design more than 70% molecules with improved \nbioactivity, high 3D similarity, while low 2D scaffold similarity to the template molecules. Our method \nachieves 2.2 times larger efficiency than state-of-the-art deep learning methods and 4.7 times than rule-\nbased methods. Case studies have also shown the advantages and usefulness of DeepHop in practical \nscaffold hopping scenario.  \n  \nIntroduction \nOver the past decades,  the rapid developments of both high-throughput screening (HTS) 1 and \nfragment-based screening technologies2 have largely facilitated the hit identification process for drug \ndiscovery. These screening strategies, together with the combinatorial compound library, discover \nextensive collections of diverse chemical series as starting points (hits) for further work towards \nidentifying clinical candidates.3 However, in general these identified hits have weak potency and do \nnot necessarily possess ideal ADMET profile. Usually lead identification (LI), i.e. transform a hit to a \nmore potent lead compound), and lead optimization (LO) process is needed to further  optimize their \nstructures to improve the potency and ADMET properties before the drug discovery project can be \nmoved into clinic.  \nOne common stra tegy in LI is scaffold hopping. The goal of scaffold  hopping is to discover \nstructurally distinct compounds starting from a known reference compound for a given target protein \nby changing the backbone of the compound, while the three -dimensional structure or the \npharmacophore of the original compound is largely unchanged to preserve the  biological activity4-5. \nThis terminology was coined by Schineider and co-workers5 and has been widely used, as such design \ncan result in novel hit series and improved molecular properties including activity, toxicity,  synthetic \naccessibility, and selectivity. However, when a â€œhopâ€ is applied to a hit molecule, there is no guarantee \nthat the corresponding transformation will work in an expected way. This can be due to an incomplete \nunderstanding of the protein -ligand interaction mechanism 6, unfavorable ADMET properties of \nhopped structure, or activity cliff.7 Predicting which hops can be successful is quite challenging even \nfor the best human medicinal chemists. \nTo this end, a variety of computational methods have been proposed to help chemists to find better \nscaffold hops.4, 8-9 Many of them utilized matching algorithms based on two-dimensional (2D) or three-\ndimensional (3D) molecular representations10-14. Others developed fragment replacement techniques, \nincluding fingerprints and combining experimental information 15-19. However, a lmost all  methods \npublished to date relied exclusively on a predefined database to select a molecule or a fragment, with \nthe differences between approaches arising solely from the searching algorithms of the database, the \nways to define similarity of compound pairs or the contents of the scanned database. As a result, these \nmethods are inherently restricted to a set of pre-defined rules or examples, limiting the exploration of \nvast chemical space.  \nIn parallel, upon the call for the more exhaustive and inte lligent exploration of chemical space, the \nfield of de novo  molecule design has been advanced by recent breakthroughs in deep generative \nmodels20-21. Various generative architectures, including RNNs 22-24, autoencoders25-27, and generative \nadversarial networks (GANs) 28 have been proven to be effective methods for generating desirable \nmolecules, which are either represented by simplified  molecular input line entry specifi cation \n(SMILES)29 or molecular graph. However, all the methods aim for the same goal, which is to design \nstructurally â€œdiverseâ€ compounds from scratch , has the capability to  search in the whole drug -like \nspace and doesnâ€™t rely on any predefined database or rules, no matter for scaffold hopping or derivative \ngeneration.  \nMore recently, two different lines of research are carried out for molecule design under scaffold \nconstraint. One research line is named as scaffold -based molecule design worked by Lim et al. 30 and \nLi et al.31, where the graph generative models were applied to extend a given scaffold by sequentially \nadding atoms and  bonds. In this context, the generated derivatives are guaranteed to maintain the \nscaffold with certainty, and their properties can thus be controlled by conditioning  the generation \nprocess on desired properties. However, due to the nature of these tasks, the shapes of the suggested \nmolecules often differ significantly from the starting points in the 3D level, and many of the proposed \ntransformations are R-group modifications.32 The other research line is referred to as fragment linking. \nThe original idea of Imrie and co -workers is to join fragments together with a generated linker while \nkeeping the relative conformations of the fragments 33. Yang et al. further extended it as a sentence \ncompletion problem with  transformer neural networks 34. Althoug h these approaches claim its \ncapability in scaffold hopping since they can generate molecules with high 3D similarity to the original \nmolecule, the 2D similarity is often higher than expected due to the nature of fragment replacement, \nresulting in unfavora ble intellectual property issues. Moreover, all these models were trained in a \nligand-based paradigm using a large number of bioactive compounds in a broad sense from public \ndatabase, regardless of the correspondent relationship between bioactivity and spe cific target protein. \nThis imposes a limitation on applying these methods into the target-centric drug development process. \nIn this work, we introduce a novel target-based scaffold hopping framework, DeepHop, to optimize \nhit/lead compounds  based on a multi-modal deep  generative model . Our method takes as input a \nreference molecule X and a specified protein target sequence Z, and designs a scaffold hop Y \nincorporating 2D and 3D structural information, protein target information, as well as bioactivity \ninformation. The model has been trained with over 50K constructed scaffold hopping pairs across 40 \nkinases. Extensive experiments show that our model is capable of generating bioisosteric molecular \nstructures for seed molecules with novel backbones but improved activity. More importantly, our \nmodel could be easily extended to new protein targets outside the  training set, which is essential for \ntarget-centric drug development.  \n \n \nMethods \nTask Definition. An exemplary scaffold hop is shown in Figure 1. In this work , we broadly define a \nscaffold hopping process as such: given an input reference molecule X and a specified protein target \nZ, the model predicts the â€œhoppedâ€ molecule Y with improved pharmaceutical activity and dissimilar \n2D structure but similar 3D structure.   \n \nFigure 1. A typical scaffold hop extracted from tankyrase -2 inhibitors. 4 The two compounds have \ndifferent scaffolds (2 D Tanimoto scaffold similarity = 0.2), but similar 3D shape s ( 3D shape and \npharmacophore similarity =0.6).  \nData preparation. There have only been a limited number of successfully reported examples for \nscaffold hopping. As such, for training and large -scale evaluation, we  constructed sets of scaffold -\nhopping pairs using a custom-made similarity scoring function from a subset of ChEMBL20.35  \nSpecifically, we first processed the ChEMBL20 dataset by filtering kinase -related target proteins \n\nwith at least 300 up to 5000 unique bioactivity instances. The scaffold hopping application in the kinase \nfamily has always been a topic of interest because the kinase patent literature is  notoriously \ncomplicated.36 We further filtered out the SMILES strings containing disconnected ions or fragments. \nThe molecules were then normalized using  RDKit, which involved the removal of salt and isotopes, \nas well  as charge neutralization. After the preprocessing, the final data set contained 103,511 \nbioactivity data points across 152 kinases (Table S1). Note that we used pChEMBL values as the \nstandard activity unit, which were defined as: -Log(molar IC50, XC50, EC50, AC50, Ki, Kd or \nPotency). \nMulti-task QSAR model.  Before constru cting the scaffold hopping pairs, one important factor \nrequired to assess the performance of scaffold hopping is whether the generated molecules have similar \nbioactivity on the desired targets. To enable a rapid and accurate profiling of generated molecule s, \nvirtual profiling models were trained on all the data points in the above -curated kinase datasets. We \nevaluated several state-of-the-art messages passing neural networks (MPNNs)37-38 and multi-task deep \nneural networks (MTDNN) 39 with molecular graphs or molecular fingerprints as the molecular \nrepresentations. MTDNN was found to obviously outperform other models with an average R2 of 0.62 \nand RMSE of 0.61 (pCHEMBL value) on internal test sets. Thus, the MTDNN model was used as the \nvirtual profiling model in the following studies. All the modeling details and results are shown in \nSupplementary files.  \nFor the quality of the virtual bioactive assessments, we kept only targets that had a 5 -fold cross-\nvalidation R2 higher than 0.70, resulting in 40 targets in the end. \nConstruction of scaffold hopping pairs.  The scaffold hopping definition emphasized o n two key \ncomponents: (i) different core structures and (ii) similar topology and pharmacophore that ensure \nbiological activities of the new compounds relative to the parent compounds. To mimic the scaffold \nhopping scenario, we constructed our data set following the idea of matched molecular pairs (MMPs) \ncutting algorithm proposed by Hussain et al 40. More specifically, we sampled target-based hopping \npairs ((X; Y)|Z) with significant bioactivity improvement (pCHEMBL Value â‰¥ 1) for new compound \nY over original compound X in the context of protein Z and a strict molecular similarity condition of \n(2D scaffold similarity (X; Y) â‰¤ 0.6)âˆ©(3D similarity (X; Y) â‰¥ 0.6).  \nThe 2D scaffold similarity was measured by the Tanimoto score over  Morgan fingerprints41 of the \ncompound scaffolds and 3D molecular similarity by the shape and color similarity score (SC score) \nused by Imrie et al. 33. To provide molecular 3D structural information, we generated 3D conformers \nfor the curated data set using RDKit and took the lowest-energy conformation among 100 samples for \neach molecule. The SC score was computed by the pharmacophoric feature similarity42 and the shape \nsimilarity43 between a pair of molecular conformers. The SC score is a float value in the range of [0, \n1], and a higher value represents a higher similarity between the generated molecule and its parent \ncompound. Scores above 0.6 indicate a fair structural match, and those above 0.8 indicate an excellent \none. \nTo avoid redundancy of training pairs, we only allowed up to 10 hops for each source molecule. For \neach target, we first randomly took 10% molecules as the test set. The rest 90% molec ules were \nconstructed into scaffold hopping pairs and randomly divided into two sets of a ratio of 9:1 for training \nand validating. These processing steps resulted in a training set of 57,537 pairs over 40 kinases.  \nIndependent test set. To explore the generalization ability to proteins that have never been observed \nduring the training process, we further used CD hit44 to retrieve six typical target proteins (the R 2 of \nQSAR models of these six proteins are higher than 0.65) from the curated database as the independent \ntest set based on the sequence identity. Among them, three proteins (CHEMBL2208, CHEMBL2147, \nCHEMBL2523) are non -homologous with sequence identity less than 25% to any sequence in the \ntraining set, while others (CHEMBL4225, CHEMBL2292, CHEMBL2041) are homologous to the \ntraining set with the highest sequence identities of 59%, 63%, 76%, respectively. The compounds in  \nthese six proteins would never be observed in the previous training, validating, and testing processes.  \nThe details of these six proteins have been shown in Table S2. \nModel Architecture. A novel multi -modal graph transformer model was proposed for generat ing \nscaffold hops with inputs of a source molecule and a protein sequence based on the transformer \narchitecture45. Transformer, as a classical encoder-decoder architecture, has recently shown the state \nof the art performances in many  sequence-to-sequence translation tasks, including machine \ntranslation46, retrosynthesis 47, a nd fragment assembly 34. In previo us chemical applications like \nretrosynthesis and fragment assembly, chemical structures were often simply converted into SMILES \nstrings that ignored spatial information naturally embedded in chemical 3D conformers. In addition, \nnone of them considered the protein target information during the transformation of the molecule pairs. \nObviously, both of these two features play crucial roles in the scaffold hopping task that need to be \nconsidered. \n \n \nFigure 2.  The basic a rchitecture of the multi -modal transformer  model DeepHop. The model \ncomprises three main components: (1) a 3D graph neural network for molecular conformer embedding, \n(2) a pre -trained encoder for the target protein embedding, and (3) a transformer for mapping the \nscaffold hopping pairs.  \nTo this e nd, DeepHop (as shown in Figure 2)  introduces a graph neural network (GNN) -based \nmolecular conformer encoder and a protein encoder to ensure the conformational information as well \nas the protein information to satisfy the explicit criterion. Basically, DeepHop comprises three main \ncomponents: (1) a 3D graph neural network (GNN) for molecular conformer embedding, (2) a pre -\ntrained encoder for target protein embedding, and (3) a transformer for mapping the scaffold hopping \npairs.  \n\n3D graph conformer encoder. We adopted a simple 3D spatial GNN as the conformer encoder \nfollowing the strategy of Danel et al. 48, which can learn both the molecular graph representation and \nspatial distances between atoms in the 3D space. The GNN follows the paradigm of message passing \nneural networks. The input of the conformer encoder is a 3D molecular graph ğº = (ğ‘‰, ğ¸), where ğ‘‰ =\n{ ğ‘£1, â€¦ , ğ‘£ğ‘›} denotes a set of nodes (atoms) and ğ¸ = [ğ‘’ğ‘–ğ‘—]ğ‘–,ğ‘—=1\nğ‘›  represents edges  (bonds) between \natoms i and j. Each atom ğ‘£ğ‘– is represented by a d-dimensional initial feature vector hi containing the \n2D chemical features computed by RDkit (See more details in Table S3). The atom is additionally \nattached with its 3D coordinates ğ‘ğ‘–  âˆˆ â„ğ‘¡ obtained by the molecular conformer. The 3D GNN then \nupdates the atom embedding with message passing operations: \nâ„ğ‘–\n(ğ‘™+1)(ğ‘ˆ, ğ‘) = âˆ‘ ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘ˆğ‘‡(ğ‘ğ‘— âˆ’ ğ‘ğ‘–)+ ğ‘) â¨€ â„ğ‘—\n(ğ‘™)\nğ‘—âˆˆğ‘ğ‘–\n) \nwhere ğ‘ˆ âˆˆ â„ğ‘¡Ã—ğ‘‘, ğ‘ âˆˆ â„ğ‘‘ are trainable network parameters, d is the dimension of the feature vector \nâ„ğ‘—\n(ğ‘™), l denotes the updating at the l-th iteration and â¨€ denotes element-wise multiplication.  \nHerein, the overall atom embeddings can be described as ğ»(ğ‘™) = {â„1\n(ğ‘™), â€¦ â„ğ‘›\n(ğ‘™)}. In the last iteration \nof the node embedding updating, we introduced a Gated Recurrent Unit (GRU) network49 to increase \nthe power of the network and obtained the final atom embedding, as shown as  \nğ»Ì‚(ğ‘£) = ğºğ‘…ğ‘ˆ(ğ»(ğ‘™)(ğ‘£)) \nProtein encoder. Compared to the drug molecules, protein  molecules are much bigger, typically \ncontaining more than 1,000 heavy atoms. To avoid a bulky model that contains too many parameters, \nwe adopted Tasks Assessing Protein  Embeddings (TAPE)50, a recently proposed semi -supervised \nprotein sequence re presentation learning method, to generate the protein pre -trained embeddings. \nTAPE was trained by a large transformer neural network in an unsupervised paradigm with millions \nof protein sequences. After training, it can generate an information-enriched feature vector for an input \nprotein sequence. Formally, a protein can be described as a linear sequence  that consists of a list of \namino acid residues ğ‘ƒ = (ğ‘Ÿ1, â€¦ ğ‘Ÿğ‘™). After processing through the TAPE, a fixed vector ğ»ğ‘ can be \nobtained, where ğ»ğ‘ is a k-dimensional pre-trained feature vector for protein sequence ğ‘ƒ. \nTransformer architecture. The fundamental architecture of DeepHop is a typical Transformer neural \nnetwork containing multiple encoder -decoder modules. Each encoder layer consists of a multi -head \nself-attention sub-layer and a position -wise feed forward network (FFN) sub -layer. Multi -head \nattention has several scaled dot -product attention functions working in parallel, which allows the \nmodel to focus on messages from different subspaces at different positions. An attention mechanism \ncomputes the dot products of the query ( Q) with all keys (K), introduces a scaling factor dk (equal to \nthe size of weight matrices) to avoid excessive dot products, and then applies a softmax function to \nobtain the weights on the values (V). Formally \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n) ğ‘‰        (1) \nThe FFN sub -layer ado pts the ReLU activation. 51 Then, layer normalization 52-53 and a residual \nconnection54 were introduced to link the above two sub-layers. Each decoder layer has three sub-layers, \nincluding an FFN sub-layer and two attention sub-layers. The decoder self-attention sub-layer utilizes \na mask function to hinder attending to unseen future tokens. The encoder-decoder attention layer helps \nthe decoder to focus on essential parts in the source sequence, and to capture the relationship between \nthe encoder and decoder. \nFor a given source molecule, we concatenate the learned 3D graph representations ğ»Ì‚ğ‘   with \nSMILES sequence embedding Ms = (s1, ..., sm) and convert them through a simple linear transformation. \nThe combined multi -modal molecular representations are then sent to the Transformer encoder to \nconvert into a latent representation ğ¿ âˆˆ â„ğ‘šÃ—ğ‘“, where m is the sequence length of molecular SMILES \nand f is the hidden state dimension. Afterwards, we concatenate ğ¿ with target protein embedding \nğ»ğ‘ âˆˆ â„ğ‘˜, resulting in a comprehensive representation ğ¿Ì‚ âˆˆ â„ğ‘šÃ—(ğ‘“+ğ‘˜). Given ğ¿Ì‚, the decoder iteratively \ngenerates an output SMILES sequence Y = (y1, ..., yo) until the ending token \"âŸ¨/sâŸ©\" is generated. \nDuring training, the model minimizes the cross-entropy loss between the target sequence Mt = (t1, ..., \ntk) and the output sequence Y.  \nâ„’(ğ‘Œ, ğ‘€) = âˆ’ âˆ‘ ğ‘¦ğ‘– ğ‘™ğ‘œğ‘” ğ‘¡ğ‘–\nğ‘˜\nğ‘–=1\n         \nBaseline Models. We compare our approaches with the following baselines: \n1. MMPA. We utilized the implementation by Hussain et al. 55 to perform MMPA as a baseline \nalgorithm. Molecular transformation rules wer e extracted from the kinase dataset for \ncorresponding tasks. During the test, we translated a source molecule multiple times using different \nmatching transformation rules and selected the translation with the highest average bioactivity as \nscored by the virtual profiling model. \n2. Seq2seq. Our second baseline is a seq2seq model that utilizes SMILES strings to encode molecules. \nThe model has been successfully applied to other molecule transformation tasks56. \n3. G2G. The third baseline i s a Graph -to-Graph model 57 that extends the junction variational \nautoencoder (V AE) via attention mechanism and generative adversari al networks (GAN). The \nmodel is capable of translating the current molecule to a similar molecule with pre-defined desired \nproperty (e.g., logP). \nNotably, these models were not designed for multi -task transformation. For a fair comparison, we \nchoose a single target protein CHEMBL2835, which contained the largest number of scaffold pairs in \nour training set, as a representative to evaluate the effectiveness of the baselines and our model. \nEvaluation Metrics. Our model aims to generate various hops for an inp ut molecule for a specific \ntarget. We quantitatively analyze the hopping success rate, bioactivity improvement, validity, \nuniqueness, diversity, and novelty of different methods. \nïƒ¼ Success rate is a metric that considers both similarity and bioactivity improvement. Since the task \nis to generate a molecule that (i) is similar to the input  molecule and (ii) has bioactivity \nimprovement simultaneously. We design criteria to judge whether it satisfies  these two \nrequirements: the generated molecule Y should (a) meet the structural condition, i.e., (2D scaffold \nsimilarity (X; Y) â‰¤ 0.6) âˆ©(3D similarity (X; Y) â‰¥ 0.6) ; (b) has a positive bioactivity gain, i.e., \npBioactivity(Y) - pBioactivity(X) â‰¥ 0, where the multi-task QSAR models are used to compute the \nactivity of generated molecules. A constraint suc cess rate is also accounted by confining the \npBioactivity(Y) - pBioactivity(X) â‰¥ 1. \nïƒ¼ Bioactivity improvement  is the average improvement of biological activity between source \nmolecule and the generated molecule computed as pBioactivity(Y) - pBioactivity(X) \nïƒ¼ Validity is the percentage of generated chemically validly generated molecules;  \nïƒ¼ Uniqueness refers to the number of unique structures generated;  \nïƒ¼ Novelty refers to the percentage of  novel molecules among the chemically validly generated \nmolecules (not present in the training set); \nïƒ¼ Diversity is the average pairwise Tanimoto distance between a set of molecules, where Tanimoto \ndistance is defined as dist(X; Y ) = 1 âˆ’ sim(X; Y ).  \nModel Training and Optimization of Hyperparameters . The DeepHop model was implemented \nbased on OpenNMT58 and all scripts were written in Python59 (version 3.7). The models were trained \non four GPU (Nvidia 2080Ti) and saved checkpoint per epoch. The best hyperparameters were decided \nbased on the loss of the validation set. We adopted the beam sea rch procedure60 to generate multiple \ncandidates with different beam widths. All generated candidates were canonicalized using RDkit and \ncompared to the source molecules. \n \nResults and Discussion \nIn this section, we mainly discussed our DeepHop performance from four parts. First, we evaluated \nour model with different training paradigms on the kinase internal datasets. Then, we compared o ur \nmethods with other state-of-the-art deep learning models as well as conventional methods on a single \ntarget protein. Subsequently, we tested our model in unseen protein sets and performed few -shot \ntransfer learning on proteins with low performance. Lastly, our DeepHop method was applied to \nseveral case study examples to demonstrate the capability of the model for practical scaffold hopping.  \n \nModel Performance on the multi -kinase dataset. We first assessed the performance of models on \nthe internal test set with different training paradigms (single-task, multi-task and DeepHop). Note that \nboth the single -task and multi -task models did not integrate the protein information. The top 10 \ncandidate sequences for each reference compound were generated (note that  the generated sequences \nare not guaranteed to be valid or unique SMILES strings). Table 1 lists the average results on a total \nof 40 targets. Our multi-modal DeepHops achieved a success rate of 65.2Â± 17.5 and constraint success \nrate of 43.7Â± 21.0, significantly outperforming those by Multi -task model (success rate = 58.9Â±20.9, \nconstraint success rate = 34.6Â±19.7) and Single -Task models (success rate = 27.5Â±15.9, constraint \nsuccess rate = 15.5Â±14.7). The poor performance in most of the metrics by the single -task model \nshould be due to the relatively small number of data points for each single kinase task, leading to a \nfragile model that is difficult to learn the transformation between scaffold pairs.  When integrating all \nthe pairs from different kinase sets fo r multi -task learning, the model is capable of capturing key \nstructural information in molecular translation, achieving a top-10 success rate of 58.9%. However, as \nthere is no protein target information given to the model, its bioactivity improvement is mu ch lower \nthan multi -modal DeepHop, with an average bioactivity improvement of 0.64 versus 0.97. When \nsimultaneously considering the structural condition and bioactivity condition, DeepHop vastly \noutperforms the multi-task model with an increase of 9.6% on the Constraint Success Rate, indicating \nthat the protein information can help the model learn specific bioactivity information for each target.  \n \nTable 1. Performance comparison of different training settings on curated internal test set. \nMetrics \nModels \nSingle-task Multi-task DeepHop \nSuccess Rate (%) 27.5Â±15.9 58.9Â±20.9 65.2Â±17.5 \nConstraint Success (%) 15.5Â±14.7 34.6Â±19.7 43.7Â±21.0 \nImprovement 0.53Â±0.31 0.64Â±0.28 0.97Â±0.24 \nValidity (%) 12.9Â±6.3 92.7Â±3.8 95.7Â±3.8 \nUniqueness (%) 8.7Â±5.5 88.2Â±8.8 76.4Â±9.3 \nNovelty (%) 99.0Â±5.1 99.6Â±0.3 99.4Â±0.5 \n \nComparison of other Methods. Since all the previous methods were performed on a single target or \nproperty, it is impractical to evaluate them in the same setting. For a relatively fair comparison, we \nchoose the Tyrosine-protein kinase JAK1 (CHEMBL2835), which has the largest number of scaffold \nhopping pairs in our training set, as a representative. Table 2 summarizes the validity, success rate, and \nimprovement of  the molecules generated by different models for co mparison. Despite  the strict \nrestriction imposed in the scaffolds, DeepHop achieved an order-of-magnitude improvement over the \ncurrent state-of-the-art baseline G2G model with 2.6 times of increases in both the success rate and \nthe constraint success rate. Though our validity is slightly lower than G2G (95.8% vs 99.5), the 66.2% \nuniqueness (4.8 times higher than the one by G2G) indicates the quality and diversity of our generated \nmolecules. The low success rates and uniqueness by G2G are probably because th e model cannot \nprocess the transformation between two 2D dissimilar graphs, as it only focused on the 2D topological \ninformation while ignored the 3D spatial information. In addition, the G2G model suffered from a \nlimited chemical space because of the smal l size of substructure vocabulary derived from the small \ntraining set, resulting in poor uniqueness value. On the other hand, though MMPA had higher \nuniqueness than DeepHop (98.6 vs. 66.2%), it achieved a poor success rate and constraint success rate \nthat are 4.7 and 5.1 times lower than our DeepHop model. In addition, the lower bioactivity \nimprovement indicates that the simple rule -based method cannot handle scaffold hopping well. \nCompared to these two baselines, seq2seq performed relatively well with a su ccess rate of 32.6%, \nindicating that the SMILES sequence transformation model is better suited for scaffold hopping tasks. \nHowever, the low uniqueness by the seq2seq model suggests that the generated hops are also stuck \ninto a limited chemical space. In summary, DeepHop has a balanced performance in all metrics.  \n \n \nTable 2. Performance comparison of models on the Tyrosine-protein kinase JAK1 (CHEMBL2835).  \nMetrics \nModels \nValidity \n(%) \nSuccess rate \n(%) \nConstraint \nsuccess rate \n(%) \nImprovement Uniqueness \n(%) \nMMPA 100 15.7 6.6 0.29 98.6 \nSeq2seq 34.6 32.6 13.4 0.28 16.9 \nG2G 99.5 27.7 13.1 1.11 13.9 \nDeepHop  95.8 73.2 33.8 0.41 66.2 \n \nProperties of the Generated Molecules. We further dug into the property of generated molecules by \ncomparing Drug-likeness score (QED score), the calculated water-octanol partition coefficient (logP), \nmolecule weight (MW), bioactivity improvement, and similarity with source ones. \n   For each source compound in the test set, the properties of its top 10 candidates generated by \ndifferent models were calculated. As shown in Fig 3a-c, the hopping molecules generated by DeepHop \nare similar to the source molecules in the distribution of QED, logP, and MW. In contrast, there is a \nlarge deviation in the distribution among  source molecules  and molecules generated by G2G and \nMMPA, suggesting that both methods cannot maintain the properties of source molecules when \nperforming the molecular transformation. Though Seq2seq also generated a similar distribution to the \nsource molecules, it has much smaller diversity and scale due to its much lower validity and uniqueness. \n \nFigure 3. Distribution of chemical properties (a) QED, (b) logP, (c) Weight, (d) diversity, (e) 2D \nscaffold and (f) 3D structural similarity for source molecules and generated m olecules from different \nmodels. \nWhen comparing the diversity of different models, MMPA achieved the highest diversity score (Figure \n3d), and DeepHop had slightly lower diversity. The seq2seq and G2G models kept much lower \ndiversity, indicating that both mo dels are prone to overfit the training set and get stuck in small \nchemical space. Besides, the compounds generated by G2G and MMPA are less similar than the ones \ngenerated by DeepHop, especially the 3D similarity. These results again proved that DeepHop overall \noutperformed state -of-the-art methods even in a single protein task and can generate high -quality \nscaffold hops for seed molecules efficiently.  \nFigure 4 shows two example of the top-predicted molecules generated by DeepHop. The modified \ngroups lead to big changes in 2D while small changes in 3D.  More cases are shown in the \nSupplementary File. \n\n \nFigure 4. Example of top-4 successful hops ((2D scaffold similarity â‰¤ 0.6)âˆ©(3D similarity (X; Y) â‰¥ \n0.6)âˆ©Activity Improvement â‰¥ 1 ) with two test molecules generated by DeepHop. The changes in \nthe generated molecules compared with starting molecule are highlighted in red. \n \nModel Performance on External Targets. We have shown that DeepHop achieves good performance \nin the internal test set. However, in real -world cases, scaffold hopping is often required for target \n\nproteins that have only a few known active compounds, and thus it is unable to construct sufficient \nscaffold hopping pairs for training. To mimic this scenario, we further examined whether DeepHop \ncan be generalized to external targets that have never been observed in the training set. Following the \nsame sampling strategy as above, we generated ten molecules for each parent molecule on six unseen \ntargets.  \nTable 3.  The independent tests of three heterogen eous proteins without homologs and three \nhomogeneous proteins with homologs to the proteins in the training set. \nMetrics \nChEMBL Target \nHomogeneous Heterogeneous \nCHEMBL \n4225 \nCHEMBL \n2041 \nCHEMBL \n2292 \nCHEMBL \n2208 \nCHEMBL \n4523 \nCHEMBL \n2147 \nSuccess Rate 0.765  0.630  0.705  0.024  0.055  0.129 \nConstraint \nSuccess Rate 0.471 0.519 0.341 0.024 0.009 0.036 \nImprovement 0.515 1.259 0.824 -0.378 -1.210 -1.263 \n \nAs shown in Table 3, the homogeneous targets performed very well in the external test set, even if all \nthe molecular structures and protein sequences in these tasks have never been observed by the model. \nThe results are expected as the deep learning models are often capable of generalizing to similar tasks \nwhile insufficient to perform tasks that are out -of-scope. It also suggests that when there are only a \nfew known actives for a specific target protein that has over 60% sequence identity similarity to the \ntraining target proteins, DeepHop can be alternatively applied to generate scaffold hops directly \nwithout the need of re-training from scratch. We also noticed that the model achieved low success rates \non three heterogeneous protein targets that are non -homologous to our training proteins (sequence \nID<25%). \nFor the heterogeneous target proteins, we wonder how many scaffold pairs are required to achieve \na decent hopping. To this end, we equipped the model with the scheme of transfer learning and tested \nhow well it can design inhibitors  for the unfamiliar proteins. Specifically, the trained DeepHop were \nfine-tuned with 5%, 20%, 50%, 80% of scaffold hopping pairs from each unseen target protein, \nrespectively. \nAs shown in Figure 5, by iteration with 100 epochs, only 5% (around 40~200) scaffold pairs can \nhelp unseen proteins to achieve fair success rates. At this  point, the uniqueness of the generated \nmolecules is poor because of the overfitting of limited data points. Thereafter, with the increase of \nscaffold hopping pairs, the model can gradually achieve a decent level of success rates and uniqueness. \nNote that the improvements are stable after fine-tuning 5% pairs, suggesting that the bioactivity feature \nis easy to capture compared to structural ones. \n \nFigure 5. Transfer learning with different ratios of scaffold hopping pairs on the heterogeneous unseen \nprotein targets.  \n \nScaffold Hopping Case Study. Next, we chose PIM-1 kinase (CHEMBL2147), a well-studied target \nfor antitumor drugs, as a representative to mimic a real-world scaffold hopping process. To search for \nnovel inhibitors of the PIM-1 kinase, Saluste and co-workers once reported a typical fragment hopping \nby replacing imidazopyridazine scaffold with triazolopyridine, which maintained the primary activity \nand significantly improved off-target selectivity as well as ADME property16.  \nWe started with one lead inhibitor (seed 1, IC50 = 0.024nM) and two hit inhibitors (seed 2, IC50 = \n\n155nM; seed 3, IC50 = 130nM), and aimed to generate potential scaffold hopping candidates with the \nimproved pharmaceutical property. We used the trained model to generate 500 candidates for three \nseed compounds, respectively. All the generated candidates were then carried out with the docking \nprocess using AutoDock Vina61.  \n \nTable 4. Scaffold hopping case study on PIM-1 kinase with three seed compounds.  \nMetrics \nScaffold Hopping \nSeed 1 Seed 2 Seed 3 \nUnique structures 387 264 332 \nStructurally successful hops 51 66 40 \nPredicted activity < Lead 11 138 167 \nDocking score < Lead 102 184 202 \n \n  As shown in Table 4, DeepHops can generate a large number of novel hops for each molecule by \nsimply increasing the beam search width. The uniqueness values for seeds 1-3 are 77.4%, 52.8% and \n66.4%, respectively. Among them, there are 51, 66, and 40 structurally successful hops generated for \nseed 1,2 and 3, meeting the requirements of (2D scaffold similarity â‰¤ 0.6)  âˆ© (3D similarity (X; Y) â‰¥ \n0.6). In terms of bioactivity, we found that 26.4%, 69.7% and 60.8% of generated hops have a better \ndocking scores than the seed compounds, demonstrating the effectiveness of our model. It is worth to \nnote that even though the seed 1 has extremely high activity (IC50 = 0.024nM), there are 11 molecules \nto have better-predicted activities and 102 molecules that have better docking scores, suggesting that \nDeepHop could be a powerful tool in developing Me-too or Me-better molecules. \nSeveral examples are shown in Figure 6. All scaffol ds hops meet the condition of structure while  \nobtaining a similar or improved activities compared to the starting seeds. \n \nFigure 6. Overlay of the seed inhibitors (sliver) and top-predicted hops (colors). The 2D structures are \nshown below, and the structural similarity (2D Scaffold Tanimoto Similarity and 3D Shape and Color \nSimilarity) and docking scores are attached in the upper left. Protein structure is retrieved from 5KZI62. \n \n\nConclusions \nIn the current study, we have proposed a novel multi -modal deep generative model, DeepHop, for \nscaffold hopping, which is a critical task in rational drug design. The model can generate large sets of \npotential hops with novel back bones and improved bioactivities. This can be used in not only early \ndrug discovery phases like hit -to-lead or lead optimization, but also patents b usting for Me-Too and \nMe-Better molecules. Furthermore, we demonstrated that the model could generalize to n ew target \nproteins if fine -tuned with a small set of active compounds. This enables the generation of scaffold \nhops in low source scenarios. Through several case examples, we have shown that our method can be \napplied to practical scaffold hopping tasks, where most of the generated molecules have better docking \nscores than the original seeds while maintaining 3D similar but 2D dissimilar structure.  \nIn the following work, we will seek to classify the types of scaffold hopping by deeply analyzing \nthe molecula r transformation paradigm. The hopping mode, like heterocycle replacement, ring -\nopening and ring closure, and topology transformation, should become a controlled condition. \nAdditionally, a broad -spectrum target proteins dataset will be constructed and test ed in order to \nenhance its scope of use. Furthermore, an interesting extension to the  DeepHop models would be to \nuse multi-objective reinforcement learning to allow our generated hops to match the comprehensive \nexpectation (e.g., ADMET, synthesizability) of medicinal chemists. \nThe use of DeepHop in chemistry can only be validated experimentally, but we believe that the \npipeline and model architecture  described in our work constitutes an important early step toward \nsolving the structure based rational drug design.  \n \n \nCorresponding Authors  \n*Email: yangyd25@mail.sysu.edu.cn (Y .Y). \n*Email: deco@fulmz.com (D.D). \n \nAuthorsâ€™ contributions \n#S.Z. and Z.L. contributed equally to this work. \n \nAuthorsâ€™ contributions \nS.Z., Z.L., and H.A. contributed concept and implementation. S.Z., H. C. and Y .Y . wrote the manuscript. \nAll authors contributed to the interpretation of results. All authors reviewed and approved the final \nmanuscript.  \n \nNotes \nZ.L, H.A and D.D declare a financial interest in that they are employees of and shareholders in Fermion \nTechnology Co., Ltd. \n \n  \nReference \n1. Macarron, R.; Banks, M. N.; Bojanic, D.; Burns, D. J.; Cirovic, D. A.; Garyantes, T.; Green, D. V.; Hertzberg, R. \nP.; Janzen, W. P.; Paslay, J. W.; Schopfer, U.; Sittampalam, G. S., Impact of high -throughput screening in \nbiomedical research. Nat Rev Drug Discov 2011, 10 (3), 188-95. \n2. Ecker, D. J.; Crooke, S. T., Combinatorial drug discovery: whi ch methods will produce the greatest value? \nBiotechnology (N Y) 1995, 13 (4), 351-60. \n3. Fattori, D.; Squarcia, A.; Bartoli, S., Fragment -based approach to drug lead discovery: overview and \nadvances in various techniques. Drugs R D 2008, 9 (4), 217-27. \n4. Hu, Y.; Stumpfe, D.; Bajorath, J., Recent Advances in Scaffold Hopping. J Med Chem 2017, 60 (4), 1238-\n1246. \n5. Schneider, G.; Neidhart, W.; Giller, T.; Schmid, G., \"Scaffold -Hopping\" by Topological Pharmacophore \nSearch: A Contribution to Virtual Screening. Angew Chem Int Ed Engl 1999, 38 (19), 2894-2896. \n6. Zheng, S.; Li, Y.; Chen, S.; Xu, J.; Yang, Y., Predicting drug â€“protein interaction using quasi -visual question \nanswering system. Nature Machine Intelligence 2020, 2 (2), 134-140. \n7. Rush, T. S., 3rd; Grant, J. A.; Mosyak, L.; Nicholls, A., A shape -based 3-D scaffold hopping method and its \napplication to a bacterial protein-protein interaction. J Med Chem 2005, 48 (5), 1489-95. \n8. Hu, Y.; Stumpfe, D.; Bajorath, J., Computational Exploration of Molecular Sc affolds in Medicinal Chemistry. \nJ Med Chem 2016, 59 (9), 4062-76. \n9. Sun, H.; Tawa, G.; Wallqvist, A., Classification of scaffold -hopping approaches. Drug Discov Today 2012, \n17 (7-8), 310-24. \n10. Nakano, H.; Miyao, T.; Funatsu, K., Exploring Topological Ph armacophore Graphs for Scaffold Hopping. J \nChem Inf Model 2020, 60 (4), 2073-2081. \n11. Laufkotter, O.; Sturm, N.; Bajorath, J.; Chen, H.; Engkvist, O., Combining structural and bioactivity -based \nfingerprints improves prediction performance and scaffold hopping capability. J Cheminform 2019, 11 (1), \n54. \n12. Renner, S.; Schneider, G., Scaffold-hopping potential of ligand-based similarity concepts. ChemMedChem \n2006, 1 (2), 181-5. \n13. Grisoni, F.; Merk, D.; Byrne, R.; Schneider, G., Scaffold-Hopping from Synthetic Drugs by Holistic Molecular \nRepresentation. Sci Rep 2018, 8 (1), 16469. \n14. Reutlinger, M.; Koch, C. P.; Reker, D.; Todoroff, N.; Schneider, P.; Rodrigues, T.; Schneider, G., Chemically \nAdvanced Template Search (CATS) for Scaffold -Hopping and Prospecti ve Target Prediction for 'Orphan' \nMolecules. Mol Inform 2013, 32 (2), 133-138. \n15. Floresta, G.; Amata, E.; Dichiara, M.; Marrazzo, A.; Salerno, L.; Romeo, G.; Prezzavento, O.; Pittala, V.; \nRescifina, A., Identification of Potentially Potent Heme Oxygenase 1 Inhibitors through 3D-QSAR Coupled \nto Scaffold-Hopping Analysis. ChemMedChem 2018, 13 (13), 1336-1342. \n16. Saluste, G.; Albarran, M. I.; Alvarez, R. M.; Rabal, O.; Ortega, M. A.; Blanco, C.; Kurz, G.; Salgado, A.; Pevarello, \nP.; Bischoff, J. R.; Pastor, J.; Oyarzabal, J., Fragment-hopping-based discovery of a novel chemical series of \nproto-oncogene PIM-1 kinase inhibitors. PLoS One 2012, 7 (10), e45964. \n17. Stahura, F. L.; Xue, L.; Godden, J. W.; Bajorath, J., Molecular scaffold -based design and comparis on of \ncombinatorial libraries focused on the ATP -binding site of protein kinases. J Mol Graph Model 1999, 17 \n(1), 1-9, 51-2. \n18. Vainio, M. J.; Kogej, T.; Raubacher, F.; Sadowski, J., Scaffold hopping by fragment replacement. J Chem Inf \nModel 2013, 53 (7), 1825-35. \n19. Rabal, O.; Amr, F. I.; Oyarzabal, J., Novel Scaffold FingerPrint (SFP): applications in scaffold hopping and \nscaffold-based selection of diverse compounds. J Chem Inf Model 2015, 55 (1), 1-18. \n20. Chen, H.; Engkvist, O.; Wang, Y.; Olivecrona,  M.; Blaschke, T., The rise of deep learning in drug discovery. \nDrug Discov Today 2018, 23 (6), 1241-1250. \n21. Xu, Y.; Lin, K.; Wang, S.; Wang, L.; Cai, C.; Song, C.; Lai, L.; Pei, J., Deep learning for molecular generation. \nFuture Med Chem 2019, 11 (6), 567-597. \n22. Mikolov, T.; KarafiÃ¡t, M.; Burget, L.; ÄŒernockÃ½, J.; Khudanpur, S. In Recurrent neural network based language \nmodel, INTERSPEECH, 2010. \n23. Segler, M. H. S.; Kogej, T.; Tyrchan, C.; Waller, M. P., Generating Focused Molecule Libraries for Drug \nDiscovery with Recurrent Neural Networks. ACS Cent Sci 2018, 4 (1), 120-131. \n24. Zheng, S.; Yan, X.; Gu, Q.; Yang, Y.; Du, Y.; Lu, Y.; Xu, J., QBMG: quasi -biogenic molecule generator with \ndeep recurrent neural network. J Cheminform 2019, 11 (1), 5. \n25. Gomez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; Hernandez-Lobato, J. M.; Sanchez-Lengeling, B.; Sheberla, \nD.; Aguilera-Iparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru-Guzik, A., Automatic Chemical Design Using \na Data-Driven Continuous Representation of Molecules. ACS Cent Sci 2018, 4 (2), 268-276. \n26. Skalic, M.; Jimenez, J.; Sabbadin, D.; De Fabritiis, G., Shape -Based Generative Modeling for de Novo Drug \nDesign. J Chem Inf Model 2019, 59 (3), 1205-1214. \n27. Stahl, N.; Falkman, G.; Karlsson, A.; Mathiason, G.; Bostrom, J., Deep Reinforcement Learning for \nMultiparameter Optimization in de novo Drug Design. J Chem Inf Model 2019, 59 (7), 3166-3176. \n28. De Cao, N.; Kipf, T., MolGAN: An implicit generative model fo r small molecular graphs. arXiv preprint \narXiv:1805.11973 2018. \n29. Weininger, D., SMILES, a chemical language and information system. 1. Introduction to methodology and \nencoding rules. Journal of chemical information and computer sciences 1988, 28 (1), 31-36. \n30. Lim, J.; Hwang, S.-Y.; Moon, S.; Kim, S.; Kim, W. Y., Scaffold-based molecular design with a graph generative \nmodel. Chemical Science 2020, 11 (4), 1153-1164. \n31. Li, Y.; Hu, J.; Wang, Y.; Zhou, J.; Zhang, L.; Liu, Z., DeepScaffold: A Comprehensiv e Tool for Scaffold-Based \nDe Novo Drug Discovery Using Deep Learning. J Chem Inf Model 2020, 60 (1), 77-91. \n32. ArÃºs-Pous, J.; Patronov, A.; Bjerrum, E. J.; Tyrchan, C.; Reymond, J.-L.; Chen, H.; Engkvist, O., SMILES-based \ndeep generative scaffold decorator for de-novo drug design. Journal of Cheminformatics 2020, 12 (1), 1-\n18. \n33. Imrie, F.; Bradley, A. R.; van der Schaar, M.; Deane, C. M., Deep Generative Models for 3D Linker Design. J \nChem Inf Model 2020, 60 (4), 1983-1995. \n34. Yang, Y.; Zheng, S.; Su, S .; Zhao, C.; Xu, J.; Chen, H., SyntaLinker: automatic fragment linking with deep \nconditional transformer neural networks. Chemical Science 2020, 11 (31), 8312-8322. \n35. Gaulton, A.; Bellis, L. J.; Bento, A. P.; Chambers, J.; Davies, M.; Hersey, A.; Light, Y.; McGlinchey, S.; \nMichalovich, D.; Al -Lazikani, B.; Overington, J. P., ChEMBL: a large -scale bioactivity database for drug \ndiscovery. Nucleic Acids Res 2012, 40 (Database issue), D1100-7. \n36. Southall, N. T.; Ajay, Kinase patent space visualization using chemical replacements. J Med Chem 2006, 49 \n(6), 2103-9. \n37. Song, Y.; Zheng, S.; Niu, Z.; Fu, Z. -H.; Lu, Y.; Yang, Y. In Communicative Representation Learning on \nAttributed Molecular Graphs, IJCAI: 2020. \n38. Yang, K.; Swanson, K.; Jin, W.; Coley, C.; Eiden, P.; Gao, H.; Guzman-Perez, A.; Hopper, T.; Kelley, B.; Mathea, \nM., Analyzing learned molecular representations for property prediction. Journal of chemical information \nand modeling 2019, 59 (8), 3370-3388. \n39. Li, X.; Li, Z.; Wu, X.; Xiong, Z.; Yang, T. ; Fu, Z.; Liu, X.; Tan, X.; Zhong, F.; Wan, X.; Wang, D.; Ding, X.; Yang, \nR.; Hou, H.; Li, C.; Liu, H.; Chen, K.; Jiang, H.; Zheng, M., Deep Learning Enhancing Kinome -Wide \nPolypharmacology Profiling: Model Construction and Experiment Validation. J Med Chem 2020, 63 (16), \n8723-8737. \n40. Hussain, J.; Rea, C., Computationally efficient algorithm to identify matched molecular pairs (MMPs) in large \ndata sets. Journal of chemical information and modeling 2010, 50 (3), 339-348. \n41. Rogers, D.; Hahn, M., Extended -connectivity fingerprints. Journal of chemical information and modeling \n2010, 50 (5), 742-754. \n42. Landrum, G. A.; Penzotti, J. E.; Putta, S., Feature -map vectors: a new class of informative descriptors for \ncomputational drug discovery. Journal of computer-aided molecular design 2006, 20 (12), 751-762. \n43. Putta, S.; Landrum, G. A.; Penzotti, J. E., Conformation mining: an algorithm for finding biologically relevant \nconformations. Journal of medicinal chemistry 2005, 48 (9), 3313-3318. \n44. Li, W.; Godzik, A., Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide \nsequences. Bioinformatics 2006, 22 (13), 1658-1659. \n45. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Å.; Polosukhin, I. In \nAttention is all you need, Advances in neural information processing systems, 2017; pp 5998-6008. \n46. Wang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; Chao, L. S., Learning deep transformer models for \nmachine translation. arXiv preprint arXiv:1906.01787 2019. \n47. Zheng, S.; Rao, J.; Zhang, Z.; Xu, J.; Yang, Y., Predicting Retrosynthetic Reactions Using Self -Corrected \nTransformer Neural Networks. J Chem Inf Model 2020, 60 (1), 47-55. \n48. Danel, T.; Spurek, P.; Tabor, J.; Åšmieja, M.; Struski, Å.; SÅ‚owik, A.; Maziarka, Å., Spatial Graph Convolutional \nNetworks. arXiv preprint arXiv:1909.05310 2019. \n49. Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y., Empirical evaluation of gated recurrent neural networks on \nsequence modeling. arXiv preprint arXiv:1412.3555 2014. \n50. Rao, R.; Bhattacharya, N.; Thomas, N.; Duan, Y.; Chen, P.; Canny, J.; Abbeel, P.; Song, Y. In Evaluating protein \ntransfer learning with TAPE, Advances in Neural Information Processing Systems, 2019; pp 9689-9701. \n51. Nair, V.; Hinton, G. E., In ICML, 2010. \n52. Ba, J.; Kiros, J. R.; Hinton, G. E., Layer Normalization. ArXiv 2016, abs/1607.06450. \n53. Barrault, L.; Bojar, O. e.; Costa -jussÃ , M. R.; Federmann, C.; Fishel, M.; Graham, Y.; Haddow, B.; Huck, M.; \nKoehn, P.; Malmasi, S.; Monz, C.; MÃ¼ller, M.; Pal, S.; Post, M.; Zampieri, M. In Findings of the 2019 Conference \non Machine Translation (WMT19), Proceedings of the Fourth Conference on Machine Translation (Volume \n2: Shared Task Papers, Day 1), Florence, Italy, aug; Association for Computational Linguistics: Florence, Italy, \n2019; pp 1-61. \n54. He, K.; Zhang, X.; Ren, S.; Sun, J., Deep Residual Learning for Image Recognition. CoRR 2015, \nabs/1512.03385. \n55. Hussain, J.; Rea, C., Computationally efficient algorithm to identify matched molecular pairs (MMPs) in large \ndata sets. J Chem Inf Model 2010, 50 (3), 339-48. \n56. Liu, B.; Ramsundar, B.; Kawthekar, P.; Shi, J.; Gomes, J.; Luu Nguyen, Q.; Ho, S.; Sloane, J.; Wender, P.; Pande, \nV., Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models. ACS Cent Sci 2017, 3 \n(10), 1103-1113. \n57. Jin, W.; Yang, K.; Barzilay, R.; Jaakkola, T., Learning multimodal graph -to-graph translation for molecular \noptimization. arXiv preprint arXiv:1812.01070 2018. \n58. Klein, G.; Kim, Y.; Deng, Y.; Senellart, J.; Rush, A. M., OpenNMT: Open -Source Toolkit for Neural Machine \nTranslation. CoRR 2017, abs/1701.02810. \n59. Python Core Team. Python: A dynamic, open source programming language. Python Software Foundation. \nURL https://www.python.org/. \n60. Ow, P. S.; Morton, T. E., Filtered beam search in schedulingâ€ . International Journal of Production Research \n1988, 26 (1), 35-62. \n61. Trott, O.; Olson, A. J., AutoDock Vina: improving the speed and accur acy of docking with a new scoring \nfunction, efficient optimization, and multithreading. Journal of computational chemistry 2010, 31 (2), 455-\n461. \n62. Wurz, R. P.; Sastri, C.; D'Amico, D. C.; Herberich, B.; Jackson, C. L. M.; Pettus, L. H.; Tasker, A. S.; W u, B.; \nGuerrero, N.; Lipford, J. R.; Winston, J. T.; Yang, Y.; Wang, P.; Nguyen, Y.; Andrews, K. L.; Huang, X.; Lee, M. \nR.; Mohr, C.; Zhang, J. D.; Reid, D. L.; Xu, Y.; Zhou, Y.; Wang, H. L., Discovery of imidazopyridazines as potent \nPim-1/2 kinase inhibitors. Bioorg Med Chem Lett 2016, 26 (22), 5580-5590. \n "
}