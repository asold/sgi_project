{
  "title": "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization",
  "url": "https://openalex.org/W3168784392",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5101705761",
      "name": "Yichen Jiang",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5030468199",
      "name": "Aslı Çelikyılmaz",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5088217655",
      "name": "Paul Smolensky",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5002176908",
      "name": "Paul Soulos",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5001484153",
      "name": "Sudha Rao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5033846851",
      "name": "Hamid Palangi",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5036987560",
      "name": "Roland Fernandez",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5045938229",
      "name": "Caitlin Smith",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5001987532",
      "name": "Mohit Bansal",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5114910293",
      "name": "Jianfeng Gao",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2962884827",
    "https://openalex.org/W4239231974",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W4300971949",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2981037730",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2808224048",
    "https://openalex.org/W2963013168",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2963548995",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963385935",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963899396",
    "https://openalex.org/W2013494846",
    "https://openalex.org/W3099766584",
    "https://openalex.org/W2952215948",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2963090675",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2808293684",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2898526707",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W3102685975",
    "https://openalex.org/W2891585150",
    "https://openalex.org/W3035576622",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2952467640",
    "https://openalex.org/W2896739098",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2962911098"
  ],
  "abstract": "Yichen Jiang, Asli Celikyilmaz, Paul Smolensky, Paul Soulos, Sudha Rao, Hamid Palangi, Roland Fernandez, Caitlin Smith, Mohit Bansal, Jianfeng Gao. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 4780–4793\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n4780\nEnriching Transformers with Structured Tensor-Product Representations\nfor Abstractive Summarization\nYichen Jiang∗ 1, Asli Celikyilmaz2, Paul Smolensky2,3, Paul Soulos∗ 3, Sudha Rao2,\nHamid Palangi2, Roland Fernandez2, Caitlin Smith∗ 3, Mohit Bansal1, Jianfeng Gao2\n1UNC Chapel Hill 2Microsoft Research, Redmond 3Johns Hopkins University\n{yichenj, mbansal}@cs.unc.edu\n{aslicel,psmo,sudha.rao,hpalangi,rfernand,jfgao}@microsoft.com\n{psoulos1, csmit372}@jhu.edu\nAbstract\nAbstractive summarization, the task of gener-\nating a concise summary of input documents,\nrequires: (1) reasoning over the source doc-\nument to determine the salient pieces of in-\nformation scattered across the long document,\nand (2) composing a cohesive text by recon-\nstructing these salient facts into a shorter sum-\nmary that faithfully reﬂects the complex re-\nlations connecting these facts. In this paper,\nwe adapt TP-T RANSFORMER (Schlag et al.,\n2019), an architecture that enriches the orig-\ninal Transformer (Vaswani et al., 2017) with\nthe explicitly compositional Tensor Product\nRepresentation (TPR), for the task of abstrac-\ntive summarization. The key feature of our\nmodel is a structural bias that we introduce by\nencoding two separate representations for each\ntoken to represent the syntactic structure (with\nrole vectors) and semantic content (with ﬁller\nvectors) separately. The model then binds the\nrole and ﬁller vectors into the TPR as the layer\noutput. We argue that the structured interme-\ndiate representations enable the model to take\nbetter control of the contents (salient facts) and\nstructures (the syntax that connects the facts)\nwhen generating the summary. Empirically,\nwe show that our TP-T RANSFORMER outper-\nforms the Transformer and the original TP-\nTRANSFORMER signiﬁcantly on several ab-\nstractive summarization datasets based on both\nautomatic and human evaluations. On sev-\neral syntactic and semantic probing tasks, we\ndemonstrate the emergent structural informa-\ntion in the role vectors and improved syntactic\ninterpretability in the TPR layer outputs.1\n1 Introduction\nAbstractive summarization is the task of generating\na shorter version of a source text without necessar-\nily reusing the sentences from the original source,\n∗Work partially done while at Microsoft Research.\n1Code and models are available at\nhttps://github.com/jiangycTarheel/\nTPT-Summ\n12\nOriginal Text (Truncated): Authorities said the incident took place on\nSao Joao beach in Caparica, south-west of Lisbon. The National\nMaritime Authority said a middle-aged man and a young girl died after\nthey were unable to avoid the plane. [....] Other reports said the victims\nhad been sunbathing when the plane made its emergency landing. […]\nVideo footage from the scene carried by local broadcasters showed a\nsmall recreational plane parked on the sand, apparently intact and\nsurrounded by beachgoers and emergency workers. […]\nReference Summary: A man and a child have been killed after a light\naircraft made an emergency landing on a beach in Portugal.\nFigure 1: An example document and its one line summary\nfrom XSum dataset. Document content that is composed into\nan abstractive summary is color-coded.\nwhile preserving the meaning of its salient contents.\nIt is a complex task that requires: semantic under-\nstanding of the source text and reasoning over its\nlexical units, making inferences about their relation\nto extract salient facts which are scattered across\nthe long document, as well as generating a con-\ncise and coherent sequence of new sentences that\ncovers the salient facts. While humans are remark-\nably good at this type of reasoning and abstraction,\ndeveloping models that are capable of extraction,\ncomprehension, abstraction, and reformulation of\nsalient contents has been an open research question.\nOne prominent aspect of abstractive summariza-\ntion is that models struggle with combining multi-\nple salient aspects in the source text into a coherent\nand grammatical set of sentences that preserve the\noriginal information in the source document. As\nshown in Fig. 1, these pieces of salient information\n(“death\", “emergency landing\", “beach\") are often\nconnected by complex syntactic, causal, and tempo-\nral relations and are loosely grouped under the main\ntopic of the source document. The transformer\nmodels (Vaswani et al., 2017) encode syntactic and\nsemantic information of the input text into a single\nrepresentation space with the self-attention, and\ndecode the salient aspects into a short summary\nwith the cross-attention. However, despite the large\nnumber of training examples, current state-of-the-\nart transformer based approaches still struggle with\nsystematic generalization of the composition of\nmultiple salient pieces of information.\n4781\nIn this paper, we investigate new types of com-\nputational primitives for transformers based on\nTensor Product Representations (TPRs) (Smolen-\nsky, 1990) which are explicitly-compositional vec-\ntor embeddings of symbolic structures. A Ten-\nsor Product Representation encodes a constituent\nin a symbolic structure as a composite of a role,\nwhich encodes the structural information (e.g.,\nthe dependency relation with another word), and\na ﬁller, which encodes the content of the con-\nstituent (e.g., the meaning of a word). Analo-\ngously, the TP-T RANSFORMER constructs a pair\nof representations for every token at every layer:\na ﬁller vector returned by attention and a novel\nrole vector. As visualized in Fig. 2, the model\nthen binds the role and ﬁller vectors to produce\nthe output of every token as a TPR. We adapt the\nTP-T RANSFORMER (Schlag et al., 2019), which\nwas proposed for solving mathematics problems,\nfor the task of abstractive summarization. Unlike\nthe original TP-T RANSFORMER , which directly\nprojects the input representation into a continuous\nrole vector space, our model generates the role vec-\ntors by attending to a learned dictionary of role\nembeddings (Palangi et al., 2018). We observe that\nmost learned role attention distributions are approx-\nimately one-hot, thus restricting the role vectors to\na highly discrete space. This structural inductive\nbias encourages the TP-T RANSFORMER to encode\nthe syntactic information in the discrete roles while\nisolating the semantics in the continuous ﬁllers.\nTo test the ability of our TP-T RANSFORMER\nwith discrete roles against the standard Transformer\nand the TP-T RANSFORMER with continuous roles,\nwe build several models from scratch on a num-\nber of summarization datasets spanning differ-\nent degrees of abstractiveness, output summary\nlengths, and domains. Our TP-T RANSFORMER\nsigniﬁcantly outperforms the standard Transformer\nand the TP-T RANSFORMER with continuous roles\non the XSum (Narayan et al., 2018), Wiki-\nhow (Koupaee and Wang, 2018), and Arxiv (Co-\nhan et al., 2018) datasets and achieves competitive\nperformance on the CNN/Daily Mail (Hermann\net al., 2015; Nallapati et al., 2016) dataset, mea-\nsured by automatic metrics including ROUGE (Lin,\n2004) and METEOR (Denkowski and Lavie, 2014).\nOur human evaluations on XSum and Wikihow\ndatasets also correlate with the automatic metrics,\ndemonstrating that summaries generated by ourTP-\nTRANSFORMER are indeed better than the Trans-\nformer’s generations.\nFurthermore, to investigate the structural repre-\nsentation that naturally emerges during training and\nthe advantage of having compositional TPR hidden\nstates, we design a suite of decoder probing tasks to\nexplore the information encoded in the role, ﬁller,\nand TPR space. We adopt the encoder probing\ntask design presented in Tenney et al. (2019b) and\ncreate four decoder probing tasks: Part-of-speech\ntagging (POS), Dependency Labeling (DEP), Se-\nmantic Role Labeling (SRL), and Named Entity\nLabeling (NEL). Our ﬁndings collectively show\nthat the decoder’s role vectors encode a wealth of\nsyntactic structures, aiding the decoder in deducing\nthe syntactic features (e.g., being a proper noun,\nbeing the object of the root predicate) of the next\ntoken to be generated. The decoder’s ﬁller vectors\non the other hand encode more semantic informa-\ntion (e.g., being a person’s name). Furthermore,\nwe observe that having the compositional TPR re-\nsults in a more interpretable ﬁnal representation\nthan the original Transformer has at every layer,\nregarding the syntactic features of the next word\nto be generated. Our results support our hypoth-\nesis that by disentangling semantics and syntax,\nsuch structured intermediate representations enable\nthe model to better control both the content to be\nconveyed and the syntactic structure needed to ex-\npress it, ultimately improving the factuality and\ngrammaticality of the generated summaries.\nOur overall contributions are as follows: (1)\nwe present a novel adaptation of the original\nTransformer architecture that incorporates a dic-\ntionary of role embeddings at every layer and gen-\nerates Tensor Product Representation by binding\nthe role vectors with attention outputs (ﬁller vec-\ntors); (2) show that our TP-T RANSFORMER out-\nperforms the Transformer as well as the original\nTP-T RANSFORMER (Schlag et al., 2019) on sev-\neral abstractive summarization datasets; and (3)\ndemonstrate the emergent structures in representa-\ntions by revealing the disentangled syntactic and\nsemantic information encoded in the role and ﬁller\nspaces.\n2 The TP-T RANSFORMER\nWe build our TP-T RANSFORMER based on the\nTransformer architecture used in Raffel et al.\n(2020). A TP-T RANSFORMER encoder applied\nto a sequence of tokens i = 1, ..., Ican be seen as\na 2-dimensional lattice of cells (i, l) where i is the\n4782\nLinearVKQScaled Dot-Product AttentionLinearLinear\nLinear\nRoleEmbeddingsRole-Attention\nMulti-Head Attention Linear\nTPRFillers (F) Roles (R)Concat\nConcat\nFigure 2: The Filler and Role Binding operation of the TP-\nTRANSFORMER Model architecture.\nposition of the input token and l = 1, ..., Lare the\nlayer indices. All cells in the encoder have the same\narchitecture and the cells at the same layer share\nthe same weights. We introduce the basic compo-\nnents of a TP-T RANSFORMER cell in Sec. 2.2 and\nits encoder and decoder cells in Sec. 2.3.\n2.1 Tensor-Product Representation Basics\nTensor-Product Representations (TPR; (Smolen-\nsky, 1990)) are explicitly-compositional vector em-\nbeddings of symbolic structures, where each con-\nstituent of the structure is represented as the prod-\nuct of a role vector, which encodes its structural\ninformation, and a ﬁller vector, which contains the\ncontent. The TPR of a whole structure is the sum of\nthe representation of its constituents. To represent\nany 3-digit number using TPRs, we need three role\nvectors: {r(p1): Ones place, r(p2): Tens place,\nr(p3): Hundreds place}and ten ﬁller vectors f for\nten digits. For example, the TPR of the number\n985 is r(p1)⊗f(5)+ r(p2)⊗f(8)+ r(p3)⊗f(9),\nwhere ⊗is the tensor product. When representing\na number, the role vectors operate similarly as the\npositional embeddings in a Transformer (Vaswani\net al., 2017). However, when representing natural\nlanguages, the role vectors need to encode a variety\nof structural information (e.g., predicate-argument,\ntense, etc) and thus it is infeasible to hand-design\nan entire suite of role vectors as we did for numbers.\nTo overcome this challenge, for every token, we dy-\nnamically compute its role vector from a dictionary\nof a ﬁnite number of role embeddings learned with\nthe entire model and treat the self-attention outputs\nas the ﬁllers. We introduce the full computation\nprocedure in Sec. 2.2.2.\n2.2 The TP-T RANSFORMER Cell\nSimilar to the basic Transformer cell, at every\nlayer, a TP-T RANSFORMER Encoder cell starts\nwith a layer normalization and the multi-head self-\nattention followed by a residual layer. Then, the\ncell treats the output vectors as ﬁllers and binds\nthem to role vectors to construct a Tensor Product\nRepresentation, which is then passed through the\nfeed-forward network to yield the ﬁnal states.\n2.2.1 Multi-Head Attention\nThe TP-T RANSFORMER cell adopts multi-head\nattention (Vaswani et al., 2017) to enable informa-\ntion passing between tokens. At any layer, denote\nthe input vectors as X∈Rkx×dm and the attention\ntarget vectors as Y ∈Rky×dm , where kx, ky are the\nlength of the sequences and dm is the dimension of\nthe input vectors. In the case of self attention, we\nhave Y =X; while for the encoder-decoder cross at-\ntention, Y is the encoder’s output vectors. We ﬁrst\napply layer normalization (Ba et al., 2016) to get\nˆX and then linearly project it to the query, key, and\nvalue vectors for each attention head h = 1, ..., H.\nQh = ˆXWh\nq + bh\nq\nKh = Y Wh\nk + bh\nk\nV h = Y Wh\nv + bh\nv\n(1)\nwhere Wq, Wk, Wv ∈ Rdm×dk . The attention\noutput matrix ¯V for each head h is computed as:\n¯V = softmax(QKT\n√dk\n)V (2)\nwhere dk is the dimension of the key vectors K.\nThe multi-head attention output O is the concate-\nnation of the attention outputs from all heads fol-\nlowed by another linear projection Wo ∈Rdm×dm .\nWe end the Multi-head Attention with a residual\nconnection with the layer input vectors ˆX:\nMHAttn(X, Y) = ˆX + [ ¯V1, ...,¯VH]Wo (3)\nwhere ¯Vh is the attention output for the h-th head.\n2.2.2 Computing TPRs\nRole Embeddings. Following Palangi et al.\n(2018), but departing from Schlag et al. (2019),\nevery layer of our TP-T RANSFORMER is equipped\nwith a dictionary r ∈RNr×dr of Nr distinct role\nembeddings with a dimension of dr. Each role\nembedding rn, n=1,. . .,Nr, is randomly initialized\n4783\nin the entire network. The role embeddings are\nnormalized before computing role vectors:\nˆrn = rn\n∥rn∥2\nfor n = 1, ..., Nr (4)\nAt each layer, the model computes a weighted\ncombination of these role embeddings ˆr to form a\nunique role vector for every token.\nMulti-Head TPR Binding. Our ﬁller vectors\ncorrespond to the multi-head attention output F =\nMHAttn(X) (Eqn. 3). The ﬁller F of each token\nhas a corresponding role vector R. We ﬁrst com-\npute the Rh ∈Rdr at every head h = 1, ..., Has\na weighted average of the normalized role embed-\ndings ˆr. We then concatenate the Rh ∈Rkx×dr\nof H heads to get the multi-head role vectors\nR ∈Rkx×(dr·H) for all kx tokens. We deﬁne this\nprocess formally as:\nRh = softmax(FWh\nr )ˆr\nR = [R1, ..., RH]\n(5)\nwhere Wr ∈Rdm×Nr is the linear projection that\ncomputes the attention scores over the role embed-\ndings for every token.2\nWe use a Hadamard product3 to approximate the\nfull Tensor product in binding the role vectors R\nwith ﬁller vectors F, as it was shown in Schlag et al.\n(2019) that using the Hadamard products allows\nlearning an optimial lower-rank approximation of\nthe full TPRs. The binding operation is followed by\nan addition with the unbound ﬁllers (F) to return\nthe residual TPR vectors.\nTPR(F) = R ⊙F + F (6)\n2.2.3 Residual Feed-forward Layer\nThe feed-forward layer of a cell consists of a linear\nprojection followed by a ReLU activation and a\nsecond linear projection. The feed-forward output\nis then added to the input vectors:\nFF(X) = X +ReLU(XWg +bg)Wf +bf (7)\nHere, Wg∈Rdm×df , bg∈Rdf , Wf ∈Rdf ×dm ,\nbf ∈Rdm , and x is the function argument.\n2We set dr · H = dm so that the multi-head role vectors\nR have the same dimension as F.\n3The Hadamard (or elementwise) product is the diagonal\nof the full tensor product.\n2.3 TP-T RANSFORMER Encoder & Decoder\nGiven the components of our basic TP-\nTRANSFORMER cell in the previous section,\nwe now describe how we construct the TP-\nTRANSFORMER encoder and decoder.\nFirst, the self-attention and the encoder-decoder\ncross-attention for every token can be computed as:\nSelf(X) = TPR(MHAttn(X, X))\nCross(Y, H) = TPR(MHAttn(Y, H)) (8)\nwhere H is the output of the encoder’s ﬁnal layer.\nY represent the previous layer’s output vectors of\neither the partially (so-far) decoded sequence at test\ntime or the masked reference summary at training\ntime. The encoder and decoder’s operations at\nevery layer can be summarized as:\nEncode(X) = FF(Self(X))\nDecode(H, Y) = FF(Cross(Self(Y ), H)) (9)\nAfter L layers of encoding and decoding, the ﬁnal\ndistribution of the i-th output token is given by:\nˆzi = softmax(ET yi,L) (10)\nwhere YL = Decode(H, YL−1) are the decoder’s\noutput states at the last layer and E is the tied in-\nput/output word embeddings.\n3 Summarization Experiments\n3.1 Abstractive Summarization Datasets\nWe train our models on four English abstractive\nsummarization datasets varying the level of ab-\nstractiveness (explained below) and the length of\nsummaries, as well as input domain.\nXSum (Narayan et al., 2018) consists of 227k\nBBC articles from 2010 to 2017 concerning various\nsubjects along with professionally written single-\nsentence summaries. Its summaries cover a wide\nvariety of syntactic structures (relative clause, etc)\nand relations (causal, temporal, etc).\nWikihow (Koupaee and Wang, 2018) is a dataset\nconsisting of instructions from the WikiHow.com\nwebsite. Each of 200k examples has multiple\ninstruction-step paragraphs, each paired with a\nsummarizing sentence. The task is to generate the\nconcatenated summaries of all paragraphs.\n4784\nDatasets Summary\nXSum Luxury fashion designer Burberry has returned to proﬁt after opening new stores and spending more on online marketing.\nWikihow Build a trustworthy bond with your piggy . Research different training methods . Choose the training method that works best for you\nand your guinea pig. Gather the materials that you will need for training.\nArxiv\n(Abbreviated)\nWe study the phase behavior of a nematic liquid crystal conﬁned between a ﬂat substrate with strong anchoring and a\npatterned substrate whose structure and local anchoring strength we vary. [. . . ] In addition the effective energy method allows one\nto determine the energy barriers between two states in a bistable nematic device .\nCNN/DM Mentally ill inmates in Miami are housed on the \"forgotten ﬂoor\" . Judge Steven Leifman says most are there as a result of\n\"avoidable felonies\". While CNN tours facility, patient shouts: \"I am the son of the president\".\nTable 1: Example summaries from XSum, Arxiv, Wikihow, and CNN/Daily Mail datasets. Text segments directly extracted\nfrom the source document are underlined.\nDatasets Split # beam Transformer TPT-c (Schlag et al., 2019) TPT-d (Ours)\nXSum\nDev 1 33.34/12.07/26.47/22.28 30.73/10.38/24.39/21.14 34.61/13.13/27.59/23.43\n4 34.48/13.08/27.29/24.59 31.83/11.28/25.11/22.39 35.70/14.11/28.38/25.80\nTest 1 33.22/11.90/26.32/23.02 30.74/10.23/24.32/21.11 34.62/12.98/27.49/24.38\n4 34.46/12.97/27.21/24.42 32.01/11.26/25.19/22.45 35.84/14.06/28.40/25.79\nWikihow\nDev 1 33.11/11.90/25.46/19.00 28.44/7.65/20.07/16.38 34.12/12.36/26.02/20.16\n4 35.85/13.32/26.83/21.57 29.98/8.34/20.70/17.95 36.54/13.69/27.21/22.53\nTest 1 33.40/12.18/25.66/19.31 28.63/7.82/20.23/16.49 34.19/12.47/25.99/20.23\n4 35.91/13.49/27.01/21.57 30.13/8.50/20.78/18.11 36.70/13.75/27.36/22.53\nArxiv\nDev 1 35.08/10.13/31.86/19.91 32.27/7.50/29.34/17.72 35.91/10.32/32.55/20.82\n4 37.95/11.48/34.03/23.31 34.45/8.40/30.91/20.17 38.35/11.56/34.32/23.74\nTest 1 35.00/9.98/31.79/19.72 32.46/7.53/29.47/17.75 35.82/10.12/32.46/20.65\n4 38.01/11.33/34.02/23.19 34.68/8.50/31.15/20.17 38.36/11.43/34.29/23.61\nCNN/DM\nDev 1 40.56/18.18/37.73/31.91 39.66/17.45/36.99/31.15 40.61/18.17/31.77/31.35\n4 41.97/19.23/38.84/34.55 41.49/18.83/38.45/34.14 41.81/19.11/38.73/34.49\nTest 1 39.83/17.63/37.02/31.75 39.10/16.96/36.41/31.15 39.63/17.35/36.80/31.57\n4 41.22/18.70/38.09/34.50 40.68/18.19/37.70/33.99 41.01/18.38/37.91/34.34\nTable 2: Automatic evaluation results on the dev/test set of XSum, Arxiv, Wikihow, and CNN/Daily Mail dataset. The results\nin every cell represent F1 variant of ROUGE-1/ROUGE-2/ROUGE-L/METEOR scores. The best ROUGE scores with a\nstatistically signiﬁcant advantage, and the best METEOR scores with at least 0.3 advantage are bolded.\nArxiv (Cohan et al., 2018) is a long document\nsummarization dataset of scientiﬁc publications\nfrom arXiv.org (113k). The task is to generate the\nabstract from the paper body.\nCNN/Daily Mail (Hermann et al., 2015; Nallap-\nati et al., 2016) dataset contains 93k articles from\nCNN and 220k articles from the Daily Mail. Ev-\nery article is accompanied by a few human-written\nbullet points about its content. We use the non-\nanonymized version used in See et al. (2017).\nDataset Abstractiveness. We show a summary\nfrom each of these four datasets in Table 1. Accord-\ning to the comparison made by Zhang et al. (2020)\nusing the coverage and density measures (Grusky\net al., 2018), the XSum and Wikihow datasets are\nmore abstractive than the others since their sum-\nmaries rarely contain large chunks of words over-\nlapping with the source documents. CNN/Daily\nMail is the least abstractive of the four. Further-\nmore, in most cases, a sentence in a CNN/Daily\nMail summary only refers to a single sentence from\nthe source document as suggested in Lebanoff et al.\n(2019), while a sentence in an XSum or Wikihow\nsummary usually aggregates information from mul-\ntiple source sentences.\n3.2 Experimental Setup\nThe Transformer and the twoTP-T RANSFORMERS\nall have 6 layers, 8 heads per layer, dimension per\nhead dk=64, model dimension dm=512, and feed-\nforward dimension df =2048 for the encoder and de-\ncoder. Our TP-T RANSFORMER with discrete roles\nhas Nr=50 role embeddings of dimension dr=64\nat every layer. For each dataset above, we train the\nall three models from scratch using an Adafactor\nOptimizer (Shazeer and Stern, 2018) with square\nroot learning rate decay and dropout rate of 0.1.\nWe evaluate the models using automatic metrics\nincluding ROUGE F1 score and METEOR.\n3.3 Results\nWe report automatic metric scores from our eval-\nuated models in Table 2. We refer to the TP-\nTRANSFORMER , with freely-generated continu-\nous role vectors (no role dictionary) (Schlag et al.,\n4785\nDatasets Models Grammar Coherency Faithfulness Saliency Repetition Overall\nXSum\nTransformer wins 39 48 43 50 38 48\nTP-T RANSFORMER wins 47 48 46 47 42 52\nTie / No agreement 34 24 31 23 40 20\nWikihow\nTransformer wins 45 45 43 54 48 43\nTP-T RANSFORMER wins 48 45 46 47 48 59\nTie / No agreement 27 30 31 19 24 18\nTable 3: Human Evaluation results on 120 random samples from the XSum (Narayan et al., 2018) and Wikihow (Koupaee and\nWang, 2018) test sets. The best numbers with an advantage of at least 5 points are underlined.\n2019) as TPT-c, and our own TP-T RANSFORMER\nwith a discrete set of role embeddings as TPT-d.\nOn the XSum, Arxiv, and Wikihow datasets, our\nTP-T RANSFORMER (TPT-d) outperforms the orig-\ninal Transformer on all metrics. On the CNN/Daily\nMail dataset, both models obtain similar perfor-\nmance across all metrics. On every dataset, the\nTPT-c model which excels on the mathematics\ndataset, is the worst among the three models be-\ning compared. This suggests that continuous role\nvectors are not suited to the summarization tasks.\nAs we explain in Sec. 3.1, CNN/Daily Mail is\nthe least abstractive one among the four datasets. In\ncontrast, summaries from the XSum and Wikihow\ndatasets contain very few n-grams (n>2) that can\nbe copied from the source documents and thus push\nthe model’s ability to compose a coherent sum-\nmary restating the salient aspects from the source.\nFurthermore, as illustrated in Table 1, the XSum\nsummary contains a long sentence that combines\nmultiple pieces of information scattered through\nthe long source document. These facts are usually\nconnected by syntactic, temporal4, or causal5 rela-\ntions and thus the model must be able to connect\nand reason across these salient facts and then con-\nvert them into a coherent sentence that faithfully\nreﬂects the original facts and their relations. We\nargue that the compositional TPR can better en-\nable these abilities required for XSum, where we\nindeed ﬁnd that our TP-T RANSFORMER achieves\nthe largest advantage over the Transformer among\nits improvements on all datasets.\n3.4 Human Evaluation\nWe conduct human evaluation to compare the sum-\nmaries generated by the Transformer and our TP-\nTRANSFORMER . We randomly sample 120 ex-\namples from the test sets of XSum and Wikihow\ndatasets with the beam-searched model summaries.\n4“returned to proﬁt after opening new stores\"\n5“Opening new stores and spending more on online mar-\nketing\" caused \"more proﬁt\".\nWe refer to appendix for the complete setup. As\nshown in Table 3, on the XSum dataset, summaries\ngenerated by the TP-T RANSFORMER are signif-\nicantly better in grammar. This corroborates our\nclaim that having the TPR can improve the model’s\nability to follow the correct syntax in compos-\ning the summary. On the Wikihow dataset, the\nTransformer receives more votes in regarding the\nsaliency. However, our TP-T RANSFORMER main-\ntains an advantage in grammar and achieves signif-\nicantly better overall preferences.\nUnfaithful XSum Examples It is well-known\nthat the XSum dataset contains a portion of un-\nfaithful reference summaries that mention facts not\nincluded in the source article (Durmus et al., 2020;\nMaynez et al., 2020). Therefore, we are interested\nto ﬁnd out whether our TP-T RANSFORMER is bet-\nter than the baseline only at expressing the faithful\ncontent or it can also generate some external, “un-\nfaithful\" facts that the baseline can’t cover. To\nanswer this question, we randomly sample 100\nexamples from the XSum dev set and manually\nexamine the source document, reference summary,\nand the two generated summaries. Among these\n100 examples, we identify 71 examples whose ref-\nerence summary includes “unfaithful\" facts that are\nnot mentioned in the source. In 21 out of 71 exam-\nples, the Transformer baseline manages to generate\nsome “unfaithful\" facts that match those in the ref-\nerence while our TP-T RANSFORMER achieves this\nin 17 examples. Such “unfaithful\" facts that were\nrecovered by the models include the full name of a\nperson when only the last name is mentioned in the\nsource, the political party or the job title of a per-\nson, each of which can be attributed to at least one\nexample seen by models during the training. There-\nfore, we believe that both models learn to draw\nexternal information from its memory of the seen\nexamples, while our TP-T RANSFORMER doesn’t\ndo better than the baseline Transformer at referring\nto external facts to obtain higher ROUGE scores.\n4786\n4 Probing Experiments\nProbing is a method to test whether some particular\ninformation is present in the model’s encodings.\nTo achieve this, an auxiliary classiﬁer is trained\nto predict speciﬁed linguistic features from the\nmodel’s internal representations. We probe dif-\nferent components (roles, ﬁller, TPRs) in our TP-\nTRANSFORMER s as well as the attention+residual\noutputs (equivalent to the ﬁller) of the Transformer\nto assess the naturally emergent structures encoded\nin the role vectors and the effectiveness of the TPR\nin the decoding process. By conducting the probing\nexperiments, we aim to (1) provide some insights\nand evidence of the different information encoded\nby the role and ﬁller vectors; and (2) explain the\nROUGE advantage of our TP-T RANSFORMER by\nshowing that its output representation can better\nencode the linguistic structural information con-\ncerning multiple probing tasks.\n4.1 Decoder Probing Tasks\nWhen studying an encoder, previous works probe\nits i-th intermediate representation at a certain layer\nfor information about the i-th input token For a de-\ncoder, however, we probe itsi-th representation for\nclues about the i-th token it generates given the\ni −1 previously generated tokens as the input. In-\ntuitively, we are probing for the decoder’s internal\ndecision about the syntactic roles and semantic con-\ntent of this token before it was ultimately selected.\nBased on encoder probing tasks used by Tenney\net al. (2019b), we select and adapt four tasks to\nprobe our decoders.\nPart-of-speech tagging (POS) is the syntactic\ntask of assigning tags such as noun (singular/mass\nnoun: NN, proper noun: NNP, etc), verb (past\ntense: VBD, past participle: VBN, etc), adjective\n(comparative: JJR, etc), etc. to each token i. We\nlet s1 = [i, i+ 1) be a single token, and seek to\npredict its POS tag.\nDependency labeling (DEP) seeks to predict\nthe functional relationships of one token relative\nto another: e.g. is it a modiﬁer-head relationship,\na subject-verb relationship, etc. We take s1 =\n[i, i+ 1) to be a single token and s2 = [j, j+ 1)\nto be its syntactic head, and seek to predict the\ndependency relation between tokens i and j.\nSemantic role labeling (SRL) is the task of im-\nposing predicate-argument structure onto a sen-\ntence. We let s1 = [ i1, j1) represent a known\nTasks Layer Transformer TPT-d (Ours)\nPOS\n1 -/58.4/58.4 36.1/57.1/58.2\n2 -/65.4/65.4 43.6/63.5/64.4\n3 -/68.6/68.3 50.4/67.4/68.5\n4 -/70.7/70.7 50.4/70.8/72.1\n5 -/72.5/72.5 53.4/73.3/73.9\n6 -/73.3/73.3 56.0/73.9/74.5\nDEP\n1 -/78.1/78.1 53.1/78.8/78.9\n2 -/85.0/85.0 59.9/84.8/84.7\n3 -/87.1/87.1 66.7/87.4/87.3\n4 -/87.4/87.4 62.9/88.3/88.2\n5 -/85.0/85.0 64.8/88.3/87.6\n6 -/86.1/86.1 60.8/86.8/86.6\nSRL\n1 -/78.2/78.2 73.1/78.5/78.4\n2 -/79.0/79.0 73.8/79.8/79.3\n3 -/79.6/79.6 73.8/79.9/80.0\n4 -/78.7/78.7 73.1/80.1/80.2\n5 -/77.7/77.7 72.9/79.9/79.8\n6 -/78.1/78.1 71.8/79.2/78.2\nNEL\n1 -/59.7/59.7 33.3/61.4/60.8\n2 -/67.6/67.6 37.6/68.1/68.2\n3 -/69.6/69.6 41.5/70.9/71.0\n4 -/71.8/71.8 43.6/74.3/73.2\n5 -/72.3/72.3 44.7/76.3/75.7\n6 -/73.3/73.3 42.2/76.1/73.8\nTable 4: Results (F1 scores) of probing different interme-\ndiate representations in decoders trained on XSum dataset.\nThe results in every cell are presented in the order of roles,\nﬁllers, and ﬁnal representations. The best numbers with an\nadvantage of at least 0.5 F1 scores are bolded.\npredicate (e.g., “push\") and s2 = [ i2, j2) repre-\nsent a known argument (“Peter\") of that predicate,\nand seek to predict the role that the argument s2\nﬁlls–e.g. ARG0 (agent, the pusher) vs. ARG1\n(patient, the pushee).\nNamed entity labeling (NEL) is the task of pre-\ndicting the category of an entity. The categories\ninclude PERSON, LOCATION, ORGANIZATION,\netc. We let s1 = [ i, j) represent a known entity\nspan and seek to predict its type.\n4.2 Experimental Setup\nAs there is no existing dataset for probing decoders,\nwe create our own training and evaluation data by\nrunning off-the-shelf models on the summarization\ndatasets. Speciﬁcally, to probe a decoder trained on\nthe XSum dataset on the POS task, we run an POS\ntagger on the reference summaries from the XSum\ntraining set and the model-generated summaries\nfor the XSum dev set to create the ground-truth la-\nbels for the training set and model-speciﬁc dev set.\nWe restore the model trained on a summarization\ndataset and freeze its parameters. Following Ten-\nney et al. (2019b), we train a span convolution\nlayer followed by a 2-layer MLP on top of the tar-\nget representation that project it onto the output\n4787\nlabel space.\n4.3 Results\nTable 4 presents the results of probing the de-\ncoder of a TP-T RANSFORMER trained on the\nXSum (Narayan et al., 2018) dataset. Note that the\nTransformer doesn’t have role vectors. It directly\noutputs the vector after the multi-head attention\nand the residual layer. Therefore, its ﬁllers and\nﬁnal representations are equivalent.\nThe decoder role vectors can encode grammat-\nical information while the ﬁller vectors repre-\nsent the semantics. We ﬁrst focus on the results\nof POS tagging probing task. Overall, we see a\ntrend of increasing scores as the representations\nget closer to the ﬁnal step of computing the distri-\nbution over the vocabulary. This implies that, as\nthe computation progresses through the layers, the\ngenerated representations are gradually deciding\nthe POS tag of the next word to generate. Next,\nwe observe that the role vectors (the 1st number in\nthe TPT-d column) of TP-T RANSFORMER encode\na considerable amount of information about the\nPOS tag of the next word generated. Additionally,\nbecause the job of deducing the POS tag of the\nnext word is partially shared by the role vectors,\nthe ﬁller vectors’ performance degrades compared\nto the Transformer. This pattern demonstrates that\nthe TP-T RANSFORMER ’s decoder is representing\nthe next word to be generated as a composite of\nstructural information encoded in the role vectors\nand semantic contents encoded in the ﬁller vectors.\nComparing the ﬁllers (the 2nd number in TPT-d\ncolumn) with the TPR (the 3rd number in the TPT-\nd column) of TP-T RANSFORMER , we see that the\nTPRs, which bind the roles and ﬁllers, outperform\nthe roles and ﬁllers alone at every layer. This in-\ndicates that the TPR effectively aggregates the lin-\nguistic knowledge encoded in the roles and ﬁllers\ninto a shared space, where the POS tag of the next\nword can be decoded more easily than in the role\nspace or ﬁller space alone. Last, the ﬁnal represen-\ntations of TP-T RANSFORMER achieve higher F1\nscores than their counterparts in the Transformer in\nthe last three layers. This demonstrates the beneﬁts\nof having the TPR in interpreting the POS tag of\nthe word to be generated.\nWhen we consider the Dependency labeling\n(DEP) and Semantic role labeling (SRL) tasks,\nwe observe that our TP-T RANSFORMER ’s ﬁnal\nrepresentations consistently beat the Transformer\nacross all layers, with only one exception in the\nDEP task at the layer 2. We also observe that the\nTP-T RANSFORMER ’s advantage becomes larger\nin the last three layers except for the ﬁnal layer in\nSRL task. However, unlike in the POS task, the\nTPR only achieve similar F1 scores to the ﬁllers.\nFinally, in the Named entity labeling (NEL)\ntask which is considered to require more semantic\ninformation rather than syntax, the role vectors’\nperformance is poorer than their performance in\nthe three syntactic tasks. For example, the TP-\nTRANSFORMER ’s ﬁnal representations at layer 6\nobtain similar F1 scores in the POS and NEL tasks\n(74.5 VS 73.8), but its role vectors only achieve\na 42.2 F1 score in the NEL tasks compared to the\n56.0 in the POS. However, even though the role\nvectors encode little information about the named\nentity type of the next token to be generated, the\nTPR still strongly outperforms the Transformer’s\nﬁller-only representation at every layer. We argue\nthat although the syntactic information encoded in\nthe role vectors is not enough to predict the correct\nnamed entity, it is still a beneﬁcial complement\nto the knowledge encoded in the distributed ﬁller\nvectors in certain situations. For example, whether\nthe subject “Chanel\" refers to aPERSON or an OR-\nGANIZATION could depend on its syntactic role\nand its relation to other words in the sentence (e.g.,\nwhether it is the subject or object of “wears”) .\nCompositional representations improves inter-\npretability of the representations. Overall, by\nprobing the different intermediate representations\nof the TP-T RANSFORMER and the Transformer,\nwe show that having the compositional TPR results\nin more interpretable ﬁnal representations at every\nlayer regarding the syntactic features of the next\nword to be generated. Considering automatic eval-\nuations generated summaries in Sec. 3.3, we argue\nthat this compositionality in learned representation\nand its syntactic interpretability enable the decoder\nto take better control of the syntactic structure of\nthe generation when assembling multiple distant\nfacts, and thus lead to summaries of better quality.\n4.4 Discrete Role Vectors\nDuring the training of our TP-T RANSFORMER\nmodels on the summarization datasets, we observe\nthat most learned role attention distributions are\napproximately one-hot, as more than 90% of the\nrole attention distributions (as computed in Eqn. 5)\nhave a maximum score larger than 0.98. Because\n4788\neach role vector is the concatenation of H vectors,\neach selected from Nr role embeddings, the com-\npletely one-hot role attentions will yield (Nr)H\npossible role vectors. Therefore, the learned, ap-\nproximately one-hot role vectors span (Nr)H dis-\ncrete subspaces, each of which only covers the\nclose proximity of a concatenation of H role em-\nbeddings. This ﬁnding indicates that as we repre-\nsent the role vectors as multi-head attention over a\nlearnable dictionary of role embeddings, the struc-\ntural inductive bias: (1) pushes the role vector\nspace to be even more discrete, and (2) induces\nthe syntactic structures encoded in these discrete\nrole vectors. We also believe there is a connection\nbetween the above two effects, as the structural,\nsyntactic information favors a lower-dimensional\nor even discrete space while the distributed, seman-\ntic information favors a higher-dimensional space.\n5 Related Work\nExplicit TPR Structures in Neural Networks\nWhile earlier TPR work based on (Smolensky,\n1990) focused on computability rather than learn-\nability questions, recently TPRs have been incor-\nporated into several recurrent deep learning mod-\nels in order to solve various NLP tasks including\nPart-of-Speech tagging, constituency parsing, im-\nage captioning (Huang et al., 2018, 2019), question\nanswering (Palangi et al., 2018; Schlag and Schmid-\nhuber, 2018), and natural-to-formal language gener-\nation (program synthesis) (Chen et al., 2020). Most\nrecently, TPRs have been introduced into Trans-\nformer architectures, starting with Schlag et al.\n(2019) which introduced the TP-T RANSFORMER\nto improve the performance and interpretability of\nmathematical problem solving models. This model\ngenerated continuous role vectors by directly pro-\njecting from layer inputs, whereas our model in-\ndexes from a dictionary of role embeddings to form\nthe role vectors which are shown to reside in a\nhighly discrete space.\nStructured Representations for Abstractive\nSummarization Compared to the extractive\nmethods, abstractive summarization models usu-\nally fail to show extractive properties, and have ten-\ndency to copy text from the source (See et al., 2017;\nPaulus et al., 2018; Pasunuru and Bansal, 2018; Ce-\nlikyilmaz et al., 2018). More recent approaches\nthat use standard transformers deal with this issue\nby introducing hierarchical structures to encode lo-\ncal and global information separately focusing on\nonly the semantic content (Liu and Lapata, 2018,\n2019). To preserve salient source relations and\ngenerate abstractive summaries of the source docu-\nment, previous work infused models with semantic\nparsers: while Song et al. (2018) introduces a new\nstructure-infused copy mechanism that combines\nthe source syntactic structure with the copy mech-\nanism, Liao et al. (2018) uses abstract meaning\nrepresentations (AMR). While these approaches re-\nquire that the document sentence semantic parsers\nare provided beforehand, our models can implicitly\nlearn to approximate the syntactic structure and\nsemantic content in their representations.\n6 Conclusion\nIn this work, we enrich the Transformer model\nwith the structured Tensor Product Representation\nfor abstractive summarization tasks. We repre-\nsent every token as a pair of role and ﬁller vec-\ntors. We show that our TP-T RANSFORMER with\ndiscrete roles outperforms Transformer and TP-\nTRANSFORMER with continuous roles on several\nabstractive summarization datasets, in both met-\nrics scores and human evaluation. We further\ndemonstrate the syntactic structures encoded in\nthe role vectors and show the improved syntactic\ninterpretability in our model’s hidden states.\n7 Ethics Statement\nIn this work we propose a new encoder-decoder\nmodeling architecture and build several models\nto benchmark our new architecture with baseline\narchitectures on several open source summarization\ndatasets.\nIntended use. Our architecture is designed to\nbuild models of abstractive summarization. Po-\ntentially our architecture could be used to train\nmodels for summarizing any type of company in-\nternal datasets (e.g., internal documents, reports,\nmeetings, legal forms, etc.) to further improve\nthe productivity and efﬁciency of the users in their\ndaily activities without needing to read long docu-\nments.\nFailure mode. Even though our models yield fac-\ntually consistent summaries, as judged by human\nevaluation, they can still generate factually incon-\nsistent summaries or sometimes hallucinate infor-\nmation that the source document does not include.\nThis might be due to the bias or noise in the train-\ning data. Model builders wanting to use our archi-\n4789\ntecture to build models on their company internal\ndatasets should build models with consideration of\nintellectual properties and privacy rights.\nMisuse Potential. We note the models to be built\nwith our architecture should be used with careful\nconsideration. The generated summaries produced\nby our models are not controlled and use gener-\native approaches, therefore, they could generate\nunreliable text. Researchers working on abstractive\nsummarization should focus on generating factu-\nally correct, ethical and reliable text. If our models\nare trained on news datasets, a careful considera-\ntion should be made on factuality of the generated\ntext and measures have been taken to prevent model\nhallucinations.\nAcknowledgments\nWe thank the reviewers for their helpful com-\nments. This work was partially supported by NSF-\nCAREER Award 1846185 and a Microsoft Investi-\ngator Fellowship.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents\nfor abstractive summarization. In 16th Annual\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, New Orleans, USA.\nKezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul\nSmolensky, Kenneth D Forbus, and Jianfeng Gao.\n2020. Mapping natural-language problems to\nformal-language solutions using structured neural\nrepresentations. In Proceedings of the ICML.\nKyunghyun Cho, Bart van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings\nof the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n1724–1734, Doha, Qatar. Association for Computa-\ntional Linguistics.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Na-\nzli Goharian. 2018. A discourse-aware attention\nmodel for abstractive summarization of long doc-\numents. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume2 (Short Papers), pages 615–\n621, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language speciﬁc translation evaluation\nfor any target language. In Proceedings of the ninth\nworkshop on statistical machine translation, pages\n376–380.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A\nquestion answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n5055–5070, Online. Association for Computational\nLinguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS), pages 1–\n6, Melbourne, Australia. Association for Computa-\ntional Linguistics.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 708–719,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nQiuyuan Huang, Li Deng, Dapeng Wu, Chang Liu,\nand Xiaodong He. 2019. Attentive tensor product\nlearning. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 33(01):1344–1351.\n4790\nQiuyuan Huang, Paul Smolensky, Xiaodong He,\nLi Deng, and Dapeng Wu. 2018. Tensor prod-\nuct generation networks for deep NLP model-\ning. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, V olume1 (Long Papers), pages 1263–\n1273, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nMahnaz Koupaee and William Yang Wang. 2018. Wik-\nihow: A large scale text summarization dataset.\narXiv preprint arXiv:1810.09305.\nLogan Lebanoff, Kaiqiang Song, Franck Dernoncourt,\nDoo Soon Kim, Seokhwan Kim, Walter Chang, and\nFei Liu. 2019. Scoring sentence singletons and\npairs for abstractive summarization. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 2175–2189, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nKexin Liao, Logan Lebanoff, and Fei Liu. 2018.\nAbstract Meaning Representation for multi-\ndocument summarization. In Proceedings of the\n27th International Conference on Computational\nLinguistics, pages 1178–1190, Santa Fe, New\nMexico, USA. Association for Computational\nLinguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out: Proceedings of the ACL-04 workshop,\nvolume 8. Barcelona, Spain.\nYongjie Lin, Yi Chern Tan, and Robert Frank.\n2019. Open sesame: Getting inside BERT’s\nlinguistic knowledge. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 241–\n253, Florence, Italy. Association for Computational\nLinguistics.\nYang Liu and Mirella Lapata. 2018. Learning struc-\ntured text representations. In Transactions of the\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Hierarchical\ntransformers for multi-document summarization. In\nTransactions of the Association for Computational\nLinguistics.\nChristopher D. Manning, Mihai Surdeanu, John\nBauer, Jenny Finkel, Steven J. Bethard, and David\nMcClosky. 2014. The Stanford CoreNLP natu-\nral language processing toolkit. In Association\nfor Computational Linguistics (ACL) System\nDemonstrations, pages 55–60.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nR. Thomas McCoy, Tal Linzen, Ewan Dunbar, and\nPaul Smolensky. 2019. RNNs implicitly implement\ntensor-product representations. In International\nConference on Learning Representations.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summariza-\ntion using sequence-to-sequence rnns and beyond.\nIn Computational Natural Language Learning.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the sum-\nmary! topic-aware convolutional neural networks\nfor extreme summarization. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1797–1807, Brussels,\nBelgium. Association for Computational Linguis-\ntics.\nH. Palangi, P. Smolensky, X. He, and L. Deng.\n2018. Question-answering with grammatically-\ninterpretable representations. In AAAI.\nRamakanth Pasunuru and Mohit Bansal. 2018. Mul-\ntireward reinforced summarization with saliency\nand entailment. In 16th Annual Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, New Orleans, USA.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive\nsummarization. In 6th International Conference on\nLearning Representations, Vancouver,BC, Canada.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nImanol Schlag and Jürgen Schmidhuber. 2018. Learn-\ning to reason with third order tensor products.\nIn Advances in Neural Information Processing\nSystems, pages 10002–10013.\nImanol Schlag, Paul Smolensky, Roland Fernandez,\nNebojsa Jojic, Jürgen Schmidhuber, and Jianfeng\nGao. 2019. Enhancing the transformer with explicit\nrelational encoding for math problem solving. arXiv\npreprint arXiv:1910.06611.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get to the point: Summarization\nwith pointer-generator networks. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (V olume 1: Long\nPapers), pages 1073–1083, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\n4791\nPaul Smolensky. 1990. Tensor product variable bind-\ning and the representation of symbolic structures in\nconnectionist systems. Artiﬁcial intelligence, 46(1-\n2):159–216.\nKaiqiang Song, Lin Zhao, and Fei Liu. 2018. Structure-\ninfused copy mechanisms for abstractive summa-\nrization. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1717–1729, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nPaul Soulos, Tom McCoy, Tal Linzen, and Paul\nSmolensky. 2019. Discovering the compositional\nstructure of vector representations with role learning\nnetworks. arXiv preprint arXiv:1910.09113.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n4593–4601, Florence, Italy. Association for Compu-\ntational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in con-\ntextualized word representations. In International\nConference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in neural information\nprocessing systems, pages 5998–6008.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter J Liu. 2020. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nIn Proceedings of the 37 th International Conference\non Machine Learning.\nAppendix\nA TP-T RANSFORMER Architecture\nWe provide Fig. 3 to visualize the full encoder-\ndecoder architecture of our TP-T RANSFORMER .\nB Summarization Experimental Setup\nThe Transformer and the twoTP-T RANSFORMERS\nall have 6 layers, 8 heads per layer, dimension per\nhead dk=64, model dimension dm=512, and feed-\nforward dimension df =2048 for the encoder and de-\ncoder. Our TP-T RANSFORMER with discrete roles\nhas Nr=50 role embeddings of dimension dr = 64\nat every layer. We search the optimal Nr from\n{20, 50, 100, 200, 1000}and select the one with\nthe best validation set performance. For each of the\nFigure 3: TP-T RANSFORMER model architecture.\ndataset above, we train the all three models from\nscratch using an Adafactor Optimizer (Shazeer and\nStern, 2018) with square root learning rate decay\nand dropout rate of 0.1. The total number of pa-\nrameter of the Transformer, TP-T RANSFORMER\nwith continuous roles, and ourTP-T RANSFORMER\nwith discrete roles are 60506880, 65234688, and\n64258080 respectively. Every model is trained on\n4 NVidia V100 GPUs (32GB) with a batch size of\n32 per GPU.\nB.1 Human Evaluation\nWe conduct human evaluation to compare the sum-\nmaries generated by the original Transformer and\nour TP-T RANSFORMER . We randomly sample 120\nexamples from the test sets of XSum and Wikihow\ndatasets with the corresponding beam-searched\nmodel summaries. For every example, we show\nthe source document, the reference summary, and\ntwo model summaries shufﬂed in order to three\nhuman evaluators, and ask them to decide which\nsummary is better in six different aspects: grammar,\ncoherency, factuality, saliency, redundancy, and an\n4792\noverall preference. We then take the majority vote\nof every examples from its three human annotators.\nC Probing Experimental Setup\nAs there is no existing dataset for probing decoders,\nwe create our own training and evaluation data by\nrunning off-the-shelf models on the summarization\ndatasets. Speciﬁcally, to probe a decoder trained on\nthe XSum dataset on the POS task, we run an POS\ntagger on the reference summaries from the XSum\ntraining set and the model-generated summaries\nfor the XSum dev set to create the ground-truth\nlabels for the training set and model-speciﬁc dev\nset. We use Stanford CoreNLP (Manning et al.,\n2014) to get the labels for POS, dependency and\nnamed entity probing tasks. We use a BERT-base\nmodel (Devlin et al., 2019) from AllenNLP (Gard-\nner et al., 2018) to get the ground-truth labels for\nSRL. We restore the model trained on a summariza-\ntion dataset and freeze its parameters during the\nprobing. We simply add a linear layer on top of the\ntarget representation to project it onto the output\nlabel space.\nD Related Works\nImplicit TPR Encodings in Neural Networks\nMcCoy et al. (2019) showed that, in GRU-\nbased (Cho et al., 2014) encoder-decoder networks\nperforming fully-compositional string manipula-\ntions, trained on extensive data that fully exempli-\nﬁes the range of possible compositions, the medial\nencoding between encoder and decoder could be\nextremely well approximated by TPRs. Soulos\net al. (2019) presented the ROLE model that learns\nits own role scheme to optimize the ﬁt of a TPR\napproximation to a given set of internal represen-\ntations in a pre-trained target neural network, re-\nmoving the need for human-generated hypotheses\nabout the role schemes the network might be imple-\nmenting. While this work successfully interprets\nthe Tensor Product Representation in fully compo-\nsitional tasks, abstractive summarization, as well\nas most other NLP tasks, are only partially com-\npositional and the symbolic rules in language are\nmuch more complex. Although these two works\nshowed that Tensor Product Representation can\nnaturally emerge in a unstructured representations,\nwe argue that standard models only learn TPRs\nwithout any special bias to do so when the compo-\nsitional structure of the task is simple and blatant\nand when the training set makes that painfully clear\nby providing a good sample of the compositional\npossibilities. That is possible for the simple string\ntasks addressed in the two previous works, but not\nin the abstractive summarization as well as other\nreal-world NLP tasks, where we show that hav-\ning explicit TPR helps in modeling the structure\ninformation.\nSequence Models Encode Implicit Structure.\nSeveral recent works have shown that the pretrained\nTransformer-based BERT (Devlin et al., 2019)\nembeddings implicitly encode structural linguis-\ntic relations with various interpretation methods.\nThe ﬁrst, and also the most popular method (Ten-\nney et al., 2019a) is to train an auxiliary classi-\nﬁer to probe the model’s hidden representations\nfor speciﬁc linguistic information. The second\nmethod (Lin et al., 2019) abstracts the Transformer\nmodel into a graph based on the attention weights,\nand explores syntactic structures based on the\ngraph’s structure. The third method (Hewitt and\nManning, 2019) sees the hidden representations of\nBERT as in a metric space and directly connect the\ndistance between representations to the distance\nbetween elements in a symbolic structure (e.g.,\na dependency-parse tree) to extract the implicit\nstructures without extra training. The interpreta-\ntion method deployed here falls under the probing\nfamily, but future work will also pursue other inter-\npretation methods.\nE Examples of Generated Summary\nWe provide examples generated by the Transformer\nbaseline and our TP-T RANSFORMER in Table 5\nand Table 6.\n4793\nDatasets Summary\nSource\nNottinghamshire Police said it would expand its categories to include misogynistic incidents.It means abuse or harassment which\nmight not be a crime can be reported to and investigated by the police, and support for the victim put in place.Nottingham Women’s\nCentre said it hopes it will help give more victims the courage to report incidents.Chief Constable Sue Fish claimed it will make the\ncounty a safer place for women. </br>\"What women face, often on a daily basis, is absolutely unacceptable and can be extremely\ndistressing,\" she said. </br>\"Nottinghamshire Police is committed to taking misogynistic hate crime seriously and encourages anyone\nwho is affected by it to contact us without hesitation. </br>\"Work on the idea ﬁrst started with the Nottinghamshire Safer for Women\nConference last year, co-hosted by the police with the Nottingham Women’s Centre.BBC TV reporter Sarah Teale was harassed in the\nstreet while reporting on the conference.The force deﬁnes misogyny hate crime as: \"Incidents against women that are motivated by\nan attitude of a man towards a woman and includes behaviour targeted towards a woman by men simply because they are a woman.\n</br>\"The classiﬁcation now means people can report incidents which might not be considered to be a crime and the police will\ninvestigate.Nottingham Women’s Centre has been helping train call centre, force control staff and ofﬁcers on the beat to recognise\nmisogynistic hate crime and ways to tackle it.These ofﬁcers will also examine if and how a victim can be supported or if anything can\nbe done to help prevent them being targeted again.Domestic abuse will not be recorded as a misogyny hate crime because it has its own\nprocedure, the force said.Melanie Jeffs, centre manager at Nottingham Women’s Centre, said: \"We’re pleased to see Nottinghamshire\nPolice recognise the breadth of violence and intimidation that women experience on a daily basis in our communities. </br>\"She added:\n\"Recording this as a hate crime will give us a detailed picture of how often, when and where it is happening. </br>It has been very\ndifﬁcult to build that picture before but we will now get detailed data to analyse. </br>\"Showing that the police take it seriously will\nalso give people the conﬁdence to come forward and report offences. </br>\"A crime that the victim or any other person perceives to\nbe motivated by hostility or prejudice towards any aspect of a person’s identity.Police forces in England, Wales and Northern Ireland\nannually monitor ﬁve strands of hate crime:Forces can include their own deﬁnition of a hate crime with several recently adding sub\ncultures.\nReference Harassment of women is to be recorded as a hate crime in a bid to tackle sexist abuse.\nTransformer Women who commit misogyny and harassed a woman are to be asked to take part in an anti-Semitic conference.\nTP-\nTRANSFORMER\nA police force has launched a national drive to combat misogyny and hate crimes in Nottinghamshire.\nTable 5: An example from the XSum dev set and the summaries generated by the Transformer baseline andTP-T RANSFORMER .\nDatasets Summary\nSource\nSixty patrol boats will protect the UK’s two new aircraft carriers which are due to arrive at Portsmouth Naval Base in 2017.The ﬁrst\ncarrier, HMS Queen Elizabeth, is expected to be operational in 2020. </br>\"We are going to see a bigger Royal Navy and the ﬂagship...\nwill be here in Portsmouth,\" Michael Fallon said.The 60 Paciﬁc 24 rigid-hulled inﬂatable boats will be built by BAE systems to \"guard\nthe carriers in the harbour and our new frigates and destroyers\", Mr Fallon said.He said they will also enhance security by providing a\nrapid response in rescue, anti-piracy and counter-narcotics missions in the area.Mr Fallon said: \"Through the defence review, defence\nspending is going to go up every April for the rest of this parliament.He said as part of the larger investment, the government will also\nbe able to provide the new aircraft carriers with sufﬁcient ﬁghter jets. </br>\"We have said we will maintain a minimum ﬂeet of 19\ndestroyers and frigates, but as the older frigates are retired we also hope to add a lighter frigate between the offshore patrol vessel and\nType 26 and to build more of those as well. </br>\"Mr Fallon’s visit to Portsmouth Naval Base comes as work has begun to rebuild the\njetty for the arrival of HMS Queen Elizabeth in 2017.Floating cranes are also dredging Portsmouth harbour to prepare deeper channels\nfor the aircraft carriers to sail from the base, which are the largest ships ever built for the Royal Navy. </br>\"This is a huge ﬁnancial\ninvestment in making sure the channel is wide enough, in enlarging the jetty here so they can take the carriers and in making sure the\ncarriers are properly guarded,\" Mr Fallon said.Taller than Nelson’s Column and longer than Portsmouth’s Spinnaker Tower laid on its\nside, the new carriers will displace 65,000 tonnes of water.To make room for the carriers three million cubic metres of clay, sand and\ngravel will be removed from a two-mile stretch of Portsmouth Harbour covering an area the size of 200 football pitches.\nReference Increased spending will result in a \"bigger\" Royal Navy, the defence secretary has said, as he announced a new £13.5m shipbuilding\ncontract.\nTransformer The Royal Navy’s new aircraft carriers will be patrolling the Portsmouth harbour this year, the defence secretary has said.\nTP-\nTRANSFORMER\nPlans for a new Royal Navy aircraft carriers to be built in Portsmouth have been unveiled.\nTable 6: An example from the XSum dev set and the summaries generated by the Transformer baseline andTP-T RANSFORMER .",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8289251327514648
    },
    {
      "name": "Computational linguistics",
      "score": 0.6134414672851562
    },
    {
      "name": "Computer science",
      "score": 0.5566191673278809
    },
    {
      "name": "Natural language processing",
      "score": 0.5388875007629395
    },
    {
      "name": "Tensor product",
      "score": 0.5020253658294678
    },
    {
      "name": "Linguistics",
      "score": 0.48617690801620483
    },
    {
      "name": "Corpus linguistics",
      "score": 0.4766158163547516
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4189499616622925
    },
    {
      "name": "Cognitive science",
      "score": 0.3348500430583954
    },
    {
      "name": "Mathematics",
      "score": 0.23916083574295044
    },
    {
      "name": "Philosophy",
      "score": 0.2244679033756256
    },
    {
      "name": "Psychology",
      "score": 0.1996096670627594
    },
    {
      "name": "Pure mathematics",
      "score": 0.07507836818695068
    }
  ]
}