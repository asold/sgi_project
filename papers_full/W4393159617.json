{
  "title": "FM-OV3D: Foundation Model-Based Cross-Modal Knowledge Blending for Open-Vocabulary 3D Detection",
  "url": "https://openalex.org/W4393159617",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100331488",
      "name": "Dongmei Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100429515",
      "name": "Chang Li",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5068482110",
      "name": "Renrui Zhang",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5018112941",
      "name": "Shenghao Xie",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A5100422542",
      "name": "Wei Xue",
      "affiliations": [
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101585261",
      "name": "Xiaodong Xie",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5013030532",
      "name": "Shanghang Zhang",
      "affiliations": [
        "Peking University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4284961860",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W4309956986",
    "https://openalex.org/W6849084415",
    "https://openalex.org/W2948672349",
    "https://openalex.org/W4304701323",
    "https://openalex.org/W6600194071",
    "https://openalex.org/W6792832003",
    "https://openalex.org/W4362598614",
    "https://openalex.org/W6801377519",
    "https://openalex.org/W6761849836",
    "https://openalex.org/W2997998901",
    "https://openalex.org/W3044379940",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6640343702",
    "https://openalex.org/W3110226019",
    "https://openalex.org/W6810884384",
    "https://openalex.org/W4323323799",
    "https://openalex.org/W4311559667",
    "https://openalex.org/W3034485414",
    "https://openalex.org/W4221145783",
    "https://openalex.org/W4309805219",
    "https://openalex.org/W4386555501",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4324128075",
    "https://openalex.org/W4372283850",
    "https://openalex.org/W4362679702",
    "https://openalex.org/W3173859428",
    "https://openalex.org/W4386436509",
    "https://openalex.org/W4390872508",
    "https://openalex.org/W4366457071",
    "https://openalex.org/W4312424618",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4284669903",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3096754345",
    "https://openalex.org/W4382458283",
    "https://openalex.org/W4312818263",
    "https://openalex.org/W4287246434",
    "https://openalex.org/W2988715931",
    "https://openalex.org/W4312367717",
    "https://openalex.org/W4386075561",
    "https://openalex.org/W4386075985",
    "https://openalex.org/W4365460648",
    "https://openalex.org/W4361193728",
    "https://openalex.org/W4386075705",
    "https://openalex.org/W4366735603",
    "https://openalex.org/W4214624153",
    "https://openalex.org/W1923184257",
    "https://openalex.org/W4386071598",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W4367628410"
  ],
  "abstract": "The superior performances of pre-trained foundation models in various visual tasks underscore their potential to enhance the 2D models' open-vocabulary ability. Existing methods explore analogous applications in the 3D space. However, most of them only center around knowledge extraction from singular foundation models, which limits the open-vocabulary ability of 3D models. We hypothesize that leveraging complementary pre-trained knowledge from various foundation models can improve knowledge transfer from 2D pre-trained visual language models to the 3D space. In this work, we propose FM-OV3D, a method of Foundation Model-based Cross-modal Knowledge Blending for Open-Vocabulary 3D Detection, which improves the open-vocabulary localization and recognition abilities of 3D model by blending knowledge from multiple pre-trained foundation models, achieving true open-vocabulary without facing constraints from original 3D datasets. Specifically, to learn the open-vocabulary 3D localization ability, we adopt the open-vocabulary localization knowledge of the Grounded-Segment-Anything model. For open-vocabulary 3D recognition ability, We leverage the knowledge of generative foundation models, including GPT-3 and Stable Diffusion models, and cross-modal discriminative models like CLIP. The experimental results on two popular benchmarks for open-vocabulary 3D object detection show that our model efficiently learns knowledge from multiple foundation models to enhance the open-vocabulary ability of the 3D model and successfully achieves state-of-the-art performance in open-vocabulary 3D object detection tasks. Code is released at https://github.com/dmzhang0425/FM-OV3D.git.",
  "full_text": "FM-OV3D: Foundation Model-Based Cross-Modal Knowledge Blending for\nOpen-Vocabulary 3D Detection\nDongmei Zhang1*, Chang Li1*, Renrui Zhang2*, Shenghao Xie3\nWei Xue4, Xiaodong Xie1, Shanghang Zhang1‚Ä†\n1National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University\n2The Chinese University of Hong Kong\n3Wuhan University\n4Hong Kong University of Science and Technology\ndmzhang@stu.pku.edu.cn, {1900012977, donxie, shanghang}@pku.edu.cn,\nrenruizhang@link.cuhk.edu.hk, xieshenghao@whu.edu.cn, xuewei.x@gmail.com\nAbstract\nThe superior performances of pre-trained foundation mod-\nels in various visual tasks underscore their potential to\nenhance the 2D models‚Äô open-vocabulary ability. Existing\nmethods explore analogous applications in the 3D space.\nHowever, most of them only center around knowledge ex-\ntraction from singular foundation models, which limits the\nopen-vocabulary ability of 3D models. We hypothesize that\nleveraging complementary pre-trained knowledge from var-\nious foundation models can improve knowledge transfer\nfrom 2D pre-trained visual language models to the 3D\nspace. In this work, we propose FM-OV3D, a method of\nFoundation Model-Based Cross-Modal Knowledge Blend-\ning for Open-Vocabulary 3D Detection, which improves\nthe open-vocabulary localization and recognition abilities\nof 3D model by blending knowledge from multiple pre-\ntrained foundation models, achieving true open-vocabulary\nwithout facing constraints from original 3D datasets. Specif-\nically, to learn the open-vocabulary 3D localization ability,\nwe adopt the open-vocabulary localization knowledge of the\nGrounded-Segment-Anything model. For open-vocabulary\n3D recognition ability, We leverage the knowledge of gen-\nerative foundation models, including GPT-3 and Stable Dif-\nfusion models, and cross-modal discriminative models like\nCLIP. The experimental results on two popular benchmarks\nfor open-vocabulary 3D object detection show that our model\nefficiently learns knowledge from multiple foundation mod-\nels to enhance the open-vocabulary ability of the 3D model\nand successfully achieves state-of-the-art performance in\nopen-vocabulary 3D object detection tasks. Code is released\nat https://github.com/dmzhang0425/FM-OV3D.git.\nIntroduction\nOpen-vocabulary ability refers to the ability of models to\ngenerate or understand concepts that have not been explicitly\nincluded in training datasets. Pre-trained foundation mod-\nels‚Äô high performances in various 2D open-vocabulary vi-\nsual tasks demonstrate their strong open-vocabulary abili-\n*These authors contributed equally.\n‚Ä†Corresponding authors.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nties (Liang et al. 2023; Liu et al. 2023a). However, utiliz-\ning vision-text pairs in training to enable such ability in 3D\nmodels is challenging, since it is difficult to collect a sizable\ndataset of 3D point clouds paired with texts.\nThe knowledge embedded in pre-trained foundation mod-\nels can potentially enhance 3D models. Despite the differ-\nences in modalities between 3D point-cloud and 2D images,\nthey both share visual information about objects. There have\nbeen efforts to investigate how to transfer knowledge from\n2D to 3D models, leading to various distinct methods (Zhang\net al. 2022; Zhu et al. 2023; Zhang et al. 2023d).\nHowever, many of them primarily extract knowledge from\nindividual models. Considering the disparities in training\nobjectives, model architectures, and training data among\nvarious models, pre-trained models‚Äô knowledge, abilities, or\nperception of the world may exhibit diversity. This diver-\nsity can potentially enhance the open-vocabulary ability of\n3D models complementarily. For example, different from\nthe contrastive vision-language knowledge in CLIP (Rad-\nford et al. 2021), SAM (Kirillov et al. 2023) is designed\nto segment all objects in an image, providing information\nabout their positions and sizes. Moreover, when vocabulary\nis involved in training open-vocabulary models to corre-\nlate visual and textual features, existing methods only use\npredefined class lists or captions, failing to provide rich\ninformation about the classes themselves, thereby limiting\nrecognition performance. As a text-generative model, GPT-\n3 (Brown et al. 2020) has a rich understanding of various\nclasses and can serve as a source of textual knowledge.\nTherefore, we hypothesize that harnessing complementary\npre-trained knowledge from different models can facilitate\nknowledge transfer from 2D pre-trained models to 3D space.\nIn this work, we propose FM-OV3D, a method of\nFoundation Model-Based Cross-Modal Knowledge Blend-\ning for Open-Vocabulary 3D Detection, which improves\nthe open-vocabulary localization and recognition ability of\n3D models by incorporating knowledge from diverse pre-\ntrained foundation models, without requiring any manual\nannotations. Specifically, to train the open-vocabulary lo-\ncalization ability of 3D models, we utilize the object local-\nization knowledge within the Grounded-Segment-Anything\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16723\nmodel to generate 2D bounding boxes. To enhance the\nopen-vocabulary recognition ability of 3D models, we as-\nsociate the semantics among three different modalities of\nthe same class: point cloud features from the 3D detec-\ntor, CLIP extracted textual features of GPT-3 generated lan-\nguage prompts, and CLIP extracted visual features of Sta-\nble Diffusion-generated 2D visual prompts and point clouds‚Äô\npaired images. We perform open-vocabulary object detec-\ntion in testing by comparing point cloud features and text\nfeatures in a common feature space. Moreover, our method\ncan be applied to any manually selected open-vocabulary\ntraining set since our GPT-3 language prompts and Stable\nDiffusion visual prompts can be generated regarding any se-\nlected class. The major contributions of our work include:\n‚Ä¢ We propose that leveraging complementary pre-trained\nknowledge from various foundation models can facilitate\nknowledge transfer from 2D pre-trained visual language\nmodels to the 3D space.\n‚Ä¢ We propose FM-OV3D, a method of foundation\nmodel-based cross-modal knowledge blending for open-\nvocabulary 3D detection, which incorporates knowl-\nedge of various foundation models to enhance the open-\nvocabulary localization and recognition ability of 3D\nmodels without requiring any manual annotation, which\ncan be easily transferred to any 3D dataset.\n‚Ä¢ Experiments conducted on two public and commonly\nused open-vocabulary 3D point-cloud object detection\ndatasets achieve state-of-the-art performances, demon-\nstrating that our method is effective.\nRelated Work\nPre-trained Foundation Models\nPre-trained foundation models are trained on massive\namounts of data on a pre-defined proxy task. Models learn\nstatistical structures and grasp the intrinsic links within\ntraining data, acquiring extensive knowledge. GPT-3 (Brown\net al. 2020) and other large language models (LLMs) (Zhang\net al. 2023a; Gao et al. 2023; Han et al. 2023) are trained on\na vast collection of internet text in self-supervised learning.\nThey can generate human-like language responses and have\nbeen applied to various natural language processing down-\nstream tasks (Wang et al. 2023; Ni and Li 2023). SAM (Kir-\nillov et al. 2023) and PerSAM (Zhang et al. 2023c) suc-\ncessfully incorporate visual content-relevant knowledge and\nhave demonstrated high performances on various tasks (Liu\net al. 2023b; Hu and Li 2023). However, these models‚Äô scope\nof knowledge is limited by insufficient training data across\nvarious modalities, training methods, and proxy task types\nused in training. As a result, existing pre-trained large mod-\nels‚Äô applicability to downstream tasks is limited.\nRecent research explores combining these pre-trained\nfoundation models in various modalities. For example,\nCaFo (Zhang et al. 2023b) cascades a variety of pre-trained\nfoundation models to achieve better image classification per-\nformance. Grounding DINO (Liu et al. 2023a), which blends\nthe knowledge of DINO with textual prompts, has state-of-\nthe-art results in zero-shot settings. Grounded-SAM (Kir-\nillov et al. 2023; Liu et al. 2023a), which combines Ground-\ning DINO (Liu et al. 2023a) with SAM (Kirillov et al.\n2023), enhances detection and segmentation abilities simul-\ntaneously. However, constrained by fused models‚Äô limited\nmodalities, there are still multi-modal problems, for in-\nstance, object detection and segmentation in 3D scenarios,\nyet to be explored.\nOpen-Vocabulary 2D/3D Object Detection\nOpen-vocabulary detection requires models to localize\nand recognize novel classes with training on only base\nclasses (De Rijk et al. 2022; Bangalath et al. 2022; Rahman,\nKhan, and Barnes 2020; Rahman, Khan, and Porikli 2020;\nZareian et al. 2021). Typically, knowledge of novel classes is\nindirectly implicated by cues from other modalities, for ex-\nample, textual cues. To enhance open-vocabulary detection\ncapabilities, some studies explore rich image-text pairs‚Äô se-\nmantics‚Äô extraction (Zareian et al. 2021). Some works (Rah-\nman, Khan, and Barnes 2020; Rahman, Khan, and Porikli\n2020; Zareian et al. 2021) replace visual detectors‚Äô classifi-\ncation layer with a visual-textual embedding to achieve ro-\nbust performance in open-vocabulary settings.\nIn 3D point cloud detection tasks, directly applying\nvisual-language pre-trained models faces the challenge of\nacquiring large-scale point cloud-text pairs and the gap be-\ntween image and point cloud modalities. Existing works\nseek solutions in utilizing foundation models‚Äô knowledge on\n3D tasks. PointCLIP series (Zhang et al. 2022; Zhu et al.\n2023; Guo* et al. 2022) use CLIP to process multi-view\nimages projected from 3D modality, and Point-Bind&Point-\nLLM (Guo et al. 2023) leverage LLMs and multi-modality\nsemantics to achieve zero-shot 3D analysis. However, CLIP\nand LLMs are not trained for localization, and the localiza-\ntion in 2D space and 3D space differs significantly, which\nlimits the localization capability of this method. Luet al. (Lu\net al. 2022) expands the 3D detector‚Äôs vocabulary with Im-\nageNet1K (Russakovsky et al. 2015). Concurrently, Lu et\nal. (Lu et al. 2023) proposes a divide-and-conquer strat-\negy to connect textual information with point clouds. They\nmight limit the 3D detector‚Äôs generalization ability originat-\ning from the dataset applied. In this paper, we leverage mul-\ntiple pre-trained models‚Äô knowledge from textual and image\nmodalities, requiring no human annotations, enhancing our\nmodel‚Äôs detection performance in open-vocabulary settings.\nMethodology\nAs shown in Figure 1, during training, we take raw point\nclouds, corresponding 2D images, and training vocabularies\nas input. During testing, the requirement for 2D images is\neliminated, and the model relies only on testing vocabulary\nand the raw 3D point clouds.\nOur 3D detector is required to predict the 3D bounding\nboxes transformed from the 2D results of Grounded-SAM\nto improve the open-vocabulary localization ability. Regard-\ning 3D open-vocabulary recognition ability, we blend the\nknowledge of GPT-3, Stable Diffusion, and CLIP. We con-\nduct our recognition loss utilizing 3D features extracted by\n3D detector, 2D features, and textual features extracted by\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16724\nOpen Vocabulary \nTraining Classes\nbed\ndog\nflag‚Ä¶\nùë¥\nDetect sofa, \nmicrowave, \nand ‚Ä¶3D\nDetector\nGrounding \nDINO SAM\nGrounded-SAM\nGPT-3\nStable \nDiffusion\nCLIP\n3D bounding boxes:\nPoint cloud features:\nText features:\nImage features:\nOpen Vocabulary Prompting\n‚Ä¶ùë¥\n‚Ä¶ùë¥\n3D\nDetector CLIP\nOpen Vocabulary \nTesting Classes\nPredictions\n‚Ä¶ùë©\nsofamicrowave ‚Ä¶\n2D paired image\n3D point cloud\n3D point \ncloud‚Ä¶\n‚Ä¶\n‚Ä¶\nsofa microwave\nPoint Cloud‚Äôs \nPaired Images\nTraining Testing\nGrounded-SAM boxes:\nùë≥ùíçùíêùíÑ\nùë≥ùíìùíÜùíÑùíêùíà\nFigure 1: The pipeline of FM-OV3D. Given raw point clouds, corresponding 2D images, and training vocabulary, we train\nour model leveraging extensive knowledge from pre-trained foundation models, without requiring any annotations. M and B\nrepresent the size of the training vocabulary and the number of point cloud samples in a training batch, respectively. Lloc aims\nat improving our 3D detector‚Äôs localization ability, while Lrecog is designed to improve its recognition ability. Point cloud\nvisualization and its paired 2D image are selected from the SUN RGB-D (Song, Lichtenberg, and Xiao 2015) dataset, while\ntraining vocabulary is from the LVIS (Gupta, Dollar, and Girshick 2019) dataset. ‚Äò*‚Äô means the model is frozen.\nCLIP, leveraging CLIP‚Äôs rich cross-modal knowledge. De-\ntails are explained in the following sections.\nOpen-Vocabulary 3D Localization\nFor the 3D detector‚Äôs open-vocabulary localization abil-\nity, we employ the Grounded-Segment-Anything model, de-\nnoted as Grounded-SAM, to generate 3D bounding boxes\nfor each point-cloud data and require our 3D detector to pre-\ndict them.\nAfter being prompted with a training vocabulary,\nGrounded-SAM generates 2D detection boxes on 2D images\ncorresponding to a set of images for a given point cloud. The\ntraining vocabulary consists of the LVIS (Gupta, Dollar, and\nGirshick 2019) dataset that encompasses classes represented\nin text. From this dataset, a subset ofM classes {a1, . . . , aM\n} is selected for prompting. The method is agnostic about the\nclasses of the 2D detection boxes. The process of generating\n2D bounding boxes for image I is given by:\nBox2DI = GroundedSAM (a1, . . . , aM , I) (1)\nwhere Box2DI ‚àà R4 represents the 2D boxes generated by\nGrounded-SAM. We then project these 2D bounding boxes\nvia projection K to 3D space and perform clustering to\ntighten the 3D bounding boxes.\nBox3DI = Cluster(Box2DI ‚ó¶ K‚àí1) (2)\nwhere K is the projection matrix, which is provided in the\ndatasets, and Box3DI ‚àà R7 represents the transformed 3D\nbounding boxes. Clustering is a density-based clustering\napproach performed on points inside the projected bounding\nbox to eliminate irrelevant outliers.\nWe supervise our 3D detector‚Äôs predicted 3D bounding\nboxes Box3Dpc ‚àà R7 by above matched 3D bounding\nboxes Box3DI. We compute bounding box regression loss\nfollowing 3DETR (Misra, Girdhar, and Joulin 2021) demon-\nstrated by Equation 3.\nLloc =\nPX\nI=1\nL3D\nbox(Box3DI, Box3Dpc) (3)\nwhere L3D\nbox represents regression loss between Box3Dpc\nand the targetBox3DI. P represents the number of matched\nbounding boxes in training. By minimizing the value ofLloc,\nwe enhance our 3D detector‚Äôs open-vocabulary localization\nability without requiring any annotation.\nOpen-Vocabulary 3D Recognition\nWe improve our 3D detector‚Äôs open-vocabulary recogni-\ntion ability by blending knowledge of single-modal gener-\native foundation models, including GPT-3 and Stable Dif-\nfusion models, and cross-modal discriminative models like\nCLIP. Specifically, we utilize GPT-3 to generate rich textual\nprompts and the Stable Diffusion model to generate rich 2D\nvisual prompts, then use CLIP to extract their features. Then\nwe blend the knowledge originating from foundation mod-\nels onto the 3D domain by aligning object class semantics\nacross point cloud, images, and text modalities.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16725\nText Prompt Generation GPT-3, with 175 billion param-\neters, is trained on a substantial amount of internet text in a\nself-supervised manner. We use it to generate rich, detailed,\nhuman-like language descriptions on training vocabulary.\nFor every training class, we prompt GPT-3 to generate de-\ntailed descriptions. We adopt existing templates from (Pratt\net al. 2023) and prompt GPT-3 with ten rounds each, includ-\ning ‚ÄúDescribe what {class} look like‚Äù, ‚ÄúHow can you iden-\ntify {class} ?‚Äù, ‚ÄúWhat does {class} look like?‚Äù, ‚ÄúDescribe\nan image from the internet of {class} ‚Äù and ‚ÄúA caption of\nan image of {class}:‚Äù. We denote generated text prompts\nof M training classes as {T1, . . . , TM }, and the overall text\nprompts as T. The prompts generated by GPT-3 contain ex-\ntensive interpretations of semantic concepts, thus providing\nhigh-quality, diverse knowledge in textual modality.\n2D Visual Prompt Generation We generate rich 2D im-\nages to provide 3D model with abundant visual representa-\ntions of open-vocabulary classes, broadening the vocabulary\nin the 3D dataset used in training. Stable Diffusion has an\nextensive textual-visual understanding and can generate syn-\nthesized 2D images according to textual prompts. Therefore,\nutilizing GPT-generated detailed descriptions T1, . . . , TM\nfor training vocabulary, we generate corresponding 2D im-\nages SI.\nSIi = SD(Ti), SI = SD(T) (4)\nwhere i ranges from 1 to M, SI denotes all the 2D vi-\nsual prompts generated forM classes in training vocabulary.\nGiven any training vocabulary, we can expand the training\ndata in 2D vision and language modalities without requir-\ning any human annotations, tackling the problem of limited\nrepresented classes in annotated 3D datasets.\nKnowledge Blending The knowledge of GPT-3, Stable\nDiffusion, and CLIP are blended by aligning object class se-\nmantics across three modalities: point cloud features from\nour 3D detector, CLIP-extracted 2D image features of\nStable-Diffusion generated 2D visual prompts and CLIP-\nextracted text features of textual prompts.\nAfter blending, our 3D detector is trained to grasp the\nintrinsic links between visual objects in 3D modality and\nsemantic concepts in 1D text modality. We first project\nour 3D detector‚Äôs predicted bounding boxes Box3Dpc onto\npaired 2D image via dataset-provided projection matrix K,\nthen get image crops Ipc. The point cloud ROI features\nwithin Box3Dpc are also extracted, denoted as Fpc. Ex-\nploiting CLIP‚Äôs visual-textual knowledge, we use CLIP to\nextract GPT-generated prompts T‚Äôs textual feature Ft, Sta-\nble Diffusion-generated 2D image SI‚Äôs features F2DSI ,\nand image crops Ipc‚Äôs features F2Dpc. The combination of\nF2DSI and F2Dpc is represented as F2D. The recognition\nloss among point cloud features, text features, and image\nfeatures is given by:\nLrecog = Lcl(Fpc, F2D) +Lcl(Fpc, Ft) (5)\nSpecifically, given a batch of features with size B of 3D\npoint cloud samples, Lcl following (Oord, Li, and Vinyals\n2018) can be computed as follows:\nLcl(F1, F2) =‚àí 1\nB\nBX\nb=1\nlog(f(b, positive)\nf(b) ) (6)\nF1 can be Fpc and F2 can be F2D or Ft in Equation 5.\nf(b, positive) and f(b) for every sample fb in the batch can\nbe computed as follows:\nf(b, positive) =\nnX\nj=1\nexp(f\n‚Ä≤\nbfj/œÑ),\nf(b) =\nBX\nk=1\nexp(f\n‚Ä≤\nbfk/œÑ)\n(7)\nwhere œÑ is the temperature parameter, n is the number of\npositive samples.\nThe total loss function of our 3D detector can be com-\nputed as Equation 8:\nL = Lloc + Lrecog (8)\nExperiments\nIn this section, we evaluate our FM-OV3D on widely used\n3D detection datasets and analyze the incorporated founda-\ntion model‚Äôs effects on open-vocabulary 3D detection mod-\nels. We also discuss the influence of some key parameters.\nDatasets and Evaluation Metrics\nDatasets We conduct experiments on public, widely used\ndatasets SUN RGB-D(Song, Lichtenberg, and Xiao 2015)\nand ScanNet (Dai et al. 2017) in 3D object detection tasks.\nThe provided point-cloud data and corresponding images,\ntogether with the matrix K, are used in our method.\nEvaluation Metrics We use mean Average Precision\n(AP), and Average Recall (AR) at IoU thresholds of 0.25\nand 0.5, denoted as mAP25, mAP50, AR25, and AR50, as\nour primary metrics.\nImplementation Details\nWe adopt LVIS (Gupta, Dollar, and Girshick 2019) as our\ntraining vocabulary. 600 random classes sampled from our\ntraining vocabulary are used to prompt Grounded-SAM to\ngenerate 2D bounding boxes. We adopt five templates as\nthe commands for generating GPT-3 textual prompts and\nlater compute the mean textual feature of each class, follow-\ning (Pratt et al. 2023). We apply the stable-diffusion-v1-4\nmodel commanded by GPT-3 generated prompts. CLIP ver-\nsion ViT-B/32 is used for extracting features. We conduct\nour ablation studies on the SUN RGB-D dataset.\nWe train our model in 400 epochs. The base learning rate\nis set to 7e-4. We load 8 3D scenes onto each GPU in ev-\nery batch. We adopt 3DETR (Misra, Girdhar, and Joulin\n2021) as the 3D detector. Experiments are conducted on two\nNVIDIA GeForce RTX 3090 GPUs and A100 SXM4 80GB\nGPUs. In evaluation, we take our 3D detector‚Äôs predicted 3D\nboxes as the localization result and the CLIP-predicted label\nof its corresponding 2D image crop as its label output.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16726\nMethod toilet bed chair bathtub sofa dresser scanner fridge lamp desk mean\nGroupFree3D 0.23 0.04 1.25 0.03 0.21 0.21 0.14 0.10 0.03 3.02 0.53\nV oteNet 0.12 0.05 1.12 0.03 0.09 0.15 0.06 0.11 0.04 2.10 0.39\nH3DNet 0.24 0.10 1.28 0.05 0.22 0.22 0.13 0.14 0.03 6.09 0.85\n3DETR 1.57 0.23 0.77 0.24 0.04 0.61 0.32 0.36 0.01 8.92 1.31\nOS-PointCLIP 7.90 2.84 3.28 0.14 1.18 0.39 0.14 0.98 0.31 5.46 2.26\nOS-Image2Point 2.14 0.09 3.25 0.01 0.15 0.55 0.04 0.27 0.02 5.48 1.20\nDetic-ModelNet 3.56 1.25 2.98 0.02 1.02 0.42 0.03 0.63 0.12 5.13 1.52\nDetic-ImageNet 0.01 0.02 0.19 0.00 0.00 1.19 0.23 0.19 0.00 7.23 0.91\nOV-3DETIC 43.97 6.17 0.89 45.75 2.26 8.22 0.02 8.32 0.07 14.60 13.03\nFM-OV3D* 32.40 18.81 27.82 15.14 35.40 7.53 1.95 9.67 13.57 7.47 16.98\nFM-OV3D 55.00 38.80 19.20 41.91 23.82 3.52 0.36 5.95 17.40 8.77 21.47\nTable 1: Detection results (AP25) on SUN RGB-D dataset. We report the accuracy of different classes and their mean score. *\ndenotes our annotation-free version.\nMethod toilet bed chair sofa dresser table cabinet bookshelf pillow sink mean\nGroupFree3D 0.63 0.52 1.25 0.52 0.20 0.59 0.52 0.25 0.01 0.15 0.49\nV oteNet 0.04 0.02 0.12 0.00 0.02 0.11 0.07 0.05 0.00 0.00 0.04\nH3DNet 0.55 0.29 1.70 0.31 0.18 0.76 0.49 0.40 0.01 0.10 0.48\n3DETR 2.60 0.81 0.90 1.27 0.36 1.37 0.99 2.25 0.00 0.59 1.11\nOS-PointCLIP 6.55 2.29 6.31 3.88 0.66 7.17 0.68 2.05 0.55 0.79 3.09\nOS-Image2Point 0.24 0.77 0.96 1.39 0.24 2.82 0.95 0.91 0.00 0.08 0.84\nDetic-ModelNet 4.25 0.98 4.56 1.20 0.21 3.21 0.56 1.25 0.00 0.65 1.69\nDetic-ImageNet 0.04 0.01 0.16 0.01 0.52 1.79 0.54 0.28 0.04 0.70 0.41\nOV-3DETIC 48.99 2.63 7.27 18.64 2.77 14.34 2.35 4.54 3.93 21.08 12.65\nFM-OV3D* 2.17 41.11 27.91 33.25 0.67 12.60 2.28 8.47 9.08 5.83 14.34\nFM-OV3D 62.32 41.97 22.24 31.80 1.89 10.73 1.38 0.11 12.26 30.62 21.53\nTable 2: Detection results (AP 25) on ScanNet dataset. We report the accuracy of different classes and their mean score. *\ndenotes our annotation-free version.\nPerformance on Open-Vocabulary 3D Detection\nSince no prior studies have addressed open-vocabulary 3D\npoint cloud detection problems by avoiding the need for\nhuman annotations, we compare our model‚Äôs performance\nwith a 3D detection model (Lu et al. 2022) that utilizes hu-\nman annotations and is exposed to open-set knowledge from\nother modalities. We select our open-testing classes follow-\ning (Lu et al. 2022) and adopt their models discussed (Liu\net al. 2021; Qi et al. 2019; Zhang et al. 2020; Misra, Gird-\nhar, and Joulin 2021; Zhang et al. 2022; Xu et al. 2021;\nZhou et al. 2022; Lu et al. 2022) in an open set setting for\ncomparison. We denote our model trained in an annotation-\nfree setting as FM-OV3D*, and FM-OV3D represents the\nmodel trained only utilizing knowledge blending, utilizing\nDetic (Zhou et al. 2022) for 2D bounding box predictions.\nResults of our experiments on SUN RGB-D and ScanNet\nare shown in Table 1 and Table 2.\nOur annotation-free model surpasses existing open-set 3D\npoint cloud detector benchmarks, reaching 16.98% on SUN\nRGB-D and 14.34% on ScanNet in the mAP25, demon-\nstrating our model‚Äôs outstanding performance on detecting\n3D objects outside the training vocabulary, indicating its\nstrong open-vocabulary ability. Furthermore, compared to\nOV-3DETIC (Lu et al. 2022), which achieves strong perfor-\nmance by leveraging the knowledge in 2D image datasets,\nour model blends the knowledge from both textual and\n2D visual modalities. Utilizing pre-trained models‚Äô gener-\native knowledge allows our 3D detector to grasp the intrin-\nsic links among three modalities, without exploiting knowl-\nedge from other datasets. Therefore, our method has no con-\nstraints originating from our leveraged cross-modal knowl-\nedge. Also, our method does not require human annotation\nin training. Our outstanding experiment results on open-\nvocabulary testing classes indicate that by incorporating\ngeneral representations learned by various foundation mod-\nels, we can bridge the gap between the limited classes in\nannotated 3D datasets and real-world applications, improv-\ning the 3D detector‚Äôs open-vocabulary ability. Our method\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16727\nModels | Anno. mAP25 AR25 mAP50 AR50\nGPT | ‚úì 18.09 53.87 1.88 11.58\nSD | ‚úì 16.34 47.69\n1.10 8.60\nGPT + SD | ‚úì 18.19 49.90 1.93 10.05\nGS | - 16.47\n55.59 1.84 11.47\nGS + GPT | - 15.78 54.23 1.99 12.99\nGS + GPT + SD | - 16.98 57.22 1.86 12.16\nTable 3: Ablation study (%) of pre-trained foundation mod-\nels. ‚ÄòGPT‚Äô represents GPT-3, ‚ÄòGS‚Äô represents Grounded-\nSAM and ‚ÄòSD‚Äô is short for Stable Diffusion. ‚ÄòAnno.‚Äô indi-\ncates whether 2D and 3D annotations are used.\nTP mAP25 mAP50 AR25 AR50\n12 11.66 1.10 45.37 7.67\n25 11.70 1.38 48.41 8.26\n51 12.50 1.16 46.73 8.65\nTable 4: Ablation study (%) of the number of selected text\nprompts of each class. TP represents the number of GPT-3\ngenerated text prompts selected in an individual class.\ncan be applied to any selected open-vocabulary training set\nwithout utilizing data other than raw 3D point cloud data.\nOur model FM-OV3D that only leverages our knowledge\nblending stage demonstrates significantly enhanced perfor-\nmance, outperforming previous methods by 8.44% on SUN\nRGB-D and 8.88% on ScanNet in terms of mAP25. Despite\nthe notable improvement in overall mAP25 performance,\nwe observe certain classes where FM-OV3D exhibits less\nsatisfactory results. For example, the performance on the\n‚Äòscanner‚Äô of FM-OV3D falls short compared to FM-OV3D*.\nThis suggests the potential for further enhancement of FM-\nOV3D by enriching its visual information related to open-\nvocabulary concepts.\nAblation Study\nThe effect of pre-trained foundation models.We inves-\ntigate the effects of each pre-trained foundation model in our\n3D detector‚Äôs training, as shown in Table 3. ‚ÄòGS‚Äô denotes\nusing the 2D bounding boxes generated by the Grounded-\nSAM model, ‚ÄòGPT-3‚Äô represents that we utilize GPT-3 to\ngenerate text prompts according to the open-vocabulary\ntraining classes, and ‚ÄòSD‚Äô denotes the 2D visual prompts\ngeneration utilizing the Stable Diffusion model. We apply\nbounding boxes generated by Detic (Zhou et al. 2022) as\nour 2D detection baseline, which are marked as annotation-\nneeded in Table 3. The experimental results of our method\nare demonstrated in the last row.\nCompared to the annotation-need model which is trained\nusing only 2D visual prompts generated by Stable Diffusion\nor text prompts by GPT-3, the same model which is trained\nVP mAP25 mAP50 AR25 AR50\n0 14.09 2.00 50.21 12.56\n3 12.84 2.24 51.18 13.11\n5 15.49 2.13 53.48 11.92\n7 12.93 2.20 47.98 10.20\n10 13.76 1.73 52.14 12.05\nTable 5: Ablation study (%) of number of classes of 2D vi-\nsual prompts. V Prepresents the number of selected Stable-\nDiffusion generated 2D visual prompts.\nusing both 2D visual prompts and text prompts shows bet-\nter performance in mAP25 and mAP50. Similarly, when\ntraining the annotation-free model with both text and 2D\nvisual prompts, its performance surpasses that of using ei-\nther prompt alone or having no prompts at all. Specifically,\nthe model incorporating both prompts achieves the best per-\nformance on the more stringent mAP25 and AR25 metrics.\nMeanwhile, the model exclusively utilizing text prompts at-\ntains the best mAP and AR when the threshold is set to\n50. These results demonstrate the effectiveness of diverse\nsemantic information from text-generative and cross-modal\ngenerative models even in the presence of annotations. It\nis reasonable since the method that only enlarges textual\ninformation lacks bridging between visual representations\nand textual knowledge. Although combining rich 2D visual\nprompts can enrich the original 3D dataset‚Äôs visual data, the\nmodel lacks direct visual-textual cross-modal understand-\ning, and GPT-3 includes rich semantics information on target\nconcepts and is in a more direct form of the model‚Äôs object\nrecognition predictions.\nWe investigate the effect of bounding boxes generated\nby open-vocabulary classes-prompted Grounded-SAM. In\ncomparison to using Detic (Zhou et al. 2022) for generating\n2D bounding boxes during training our detector‚Äôs localiza-\ntion abilities, our method demonstrates outstanding perfor-\nmance in AR25 and AR50 without relying on any manual\nannotations, indicating its strong ability to localize objects\nin 3D modalities accurately.\nOverall, compared with models that ditch one component\nin our design, the results of our method demonstrate that\nwith the open-vocabulary 2D grounding abilities, language\ngenerative knowledge, and 2D visual generative knowledge\ncombined, the 3D detector can fully understand the con-\ncepts‚Äô representation in three modalities, achieving superior\nresults in open vocabulary 3D detection tasks.\nNumber of selected text prompts of each class.We gen-\nerate multiple text prompts using GPT-3 in our method. We\nstudy the influence of the number of generated text prompts\nfor an individual class on the performance of our model. The\nresults are shown in Table 4. Considering training difficulty\nand time constraints, we evaluate the performance of 12, 25,\nand all text prompts. Experimental results indicate that uti-\nlizing all text prompts yields the highest mAP25, demon-\nstrating the effectiveness of rich textual features in enhanc-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16728\nOriginal Picture:\nDetic:\nthe Grounding-Segment-Anything model:\nFigure 2: Bounding boxes generated by the Grounded-\nSegment-Anything model and Detic Original pictures are se-\nlected from the SUN RGB-D dataset.\ning the recognition capability of the detector. However, even\nwhen using only half of the text prompts (i.e., 25), the model\nstill achieves the best mAP50 and AR25. Although reduc-\ning the number of text prompts has a certain impact on the\nmodel‚Äôs recognition performance, it remains a competitive\noption as it achieves comparable performance with less gen-\neration and training burden. Further reduction in the number\nof prompts significantly affects the model‚Äôs performance,\nunderscoring the importance of incorporating text prompts.\nNumber of classes of used 2D visual prompts. The\nperformances of applying different numbers of classes of\n2D visual prompts are shown in Table 5. We report the\nbest mAP25 and AR25 when applying 5 classes of visual\nprompts. Compared to using a smaller number of classes\nor ditching 2D visual prompts, the performance achieved\nwith 5 classes demonstrates the effectiveness of 2D visual\nprompts in enhancing the model‚Äôs recognition capability.\nThe model‚Äôs performance declines as the number of classes\nincreases. This can be attributed to the introduction of ex-\ncessive negative samples, which introduce noise and con-\nflicts, hindering the model‚Äôs ability to learn accurate and ro-\nbust feature representations and increasing the difficulty of\ntraining. Therefore, incorporating 2D visual prompts is cru-\ncial for enhancing model performance, but the number of\nselected classes applied should be taken into consideration\nto achieve optimal results.\nQualitative Analysis\nWe compare the bounding boxes generated by Grounded-\nSAM to the 2D boxes generated by Detic (Zhou et al.\n2022). We assign the class prediction of boxes generated by\nGrounded-SAM to a random value since we do not further\nuse these labels for predictions. In Figure 2, objects localized\nby the Grounded-SAM are based on our prompted open-\nText Prompts 2D Visual Prompts\n1. A suitcase is generally a rectangular shaped bag \nwith a handle and zipper.\n1. A teddy bear lying on a bed with a pillow.\n1. The vase is blue and white with a floral design.\n1. A wardrobe is a piece of furniture with shelves \nand a hanging area for storing clothes.\n1.                  2.  \n2. A suitcase is a piece of luggage that is used for \ncarrying clothes and other items on a trip.\n2. The teddy bear can be identified by its soft fur, \nlarge ears, and plump body.\n2. A vase is a container used to hold flowers or \nother decorative items.\n2. Typical wardrobe might contain a clothing rail, \nshelves, and drawers.\n1.                  2.  \n1.                  2.  \n1.                  2.  \nFigure 3: Visualization of GPT-generated prompts and their\npaired 2D visual prompts generated by Stable Diffusion on\nopen vocabulary training dataset sampled from LVIS.\nvocabulary classes, not facing constraints originating from\nthe classes detected by pre-trained 2D detectors. More ob-\njects are detected when Grounded-SAM is applied. Since we\nuse replaceable, open-vocabulary training classes to prompt\nthe model, we achieve strong open-vocabulary ability.\nWe visualize GPT-3-generated text prompts and the\npaired 2D visual prompts generated by stable diffusion ac-\ncording to the GPT-3 text prompts. In Figure 3, we show\nthat our visual prompts are diverse and variant in their di-\nrect representation of the commanded classes due to the rich\nGPT-3 generated descriptions, enriching the visual informa-\ntion and alleviating original 3D datasets‚Äô data insufficiency\nproblem. Our visual prompts also successfully grasp the se-\nmantic concepts of the commanded classes, utilizing Stable\nDiffusion‚Äôs vision-language knowledge.\nConclusion\nWe demonstrate that leveraging complementary pre-trained\nknowledge from various foundation models can improve the\nknowledge transfer from 2D pre-trained foundation models\nto the 3D space, therefore enhancing the open-vocabulary\nability of 3D models. We propose FM-OV3D, a foundation\nmodel-based cross-modal knowledge blending for open-\nvocabulary 3D object detection method that correlates multi-\nmodal knowledge from different foundation models onto\nthe 3D modality without requiring any human annotation.\nWe train our 3D detector in aspects of localization and\nrecognition. For open-vocabulary localization, we integrate\nthe Grounded-Segment-Anything model‚Äôs 2D knowledge by\ntransforming its 2D bounding box predictions to supervise\nour model‚Äôs localization results. For open-vocabulary recog-\nnition, we blend the knowledge from pre-trained text and im-\nage generative models and cross-modal discriminative mod-\nels with knowledge in 3D modality, bridging the gap be-\ntween abundant real-world classes and the insufficiency of\nclasses in 3D datasets. We conduct experiments on SUN\nRGB-D and ScanNet datasets, and our experimental results\ndemonstrate that our method is effective.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16729\nAcknowledgments\nShanghang Zhang is supported by the National\nKey Research and Development Project of China\n(No.2022ZD0117801). This work was supported by\nEmerging Engineering Interdisciplinary Youth Program at\nPeking University under Grant 7100604372.\nReferences\nBangalath, H.; Maaz, M.; Khattak, M. U.; Khan, S. H.; and\nShahbaz Khan, F. 2022. Bridging the gap between object\nand image-level representations for open-vocabulary detec-\ntion. Advances in Neural Information Processing Systems,\n35: 33781‚Äì33794.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877‚Äì\n1901.\nDai, A.; Chang, A. X.; Savva, M.; Halber, M.; Funkhouser,\nT.; and Nie√üner, M. 2017. Scannet: Richly-annotated 3d\nreconstructions of indoor scenes. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 5828‚Äì5839.\nDe Rijk, P.; Schneider, L.; Cordts, M.; and Gavrila, D.\n2022. Structural Knowledge Distillation for Object Detec-\ntion. Advances in Neural Information Processing Systems,\n35: 3858‚Äì3870.\nGao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;\nZhang, W.; Lu, P.; He, C.; Yue, X.; et al. 2023. Llama-\nadapter v2: Parameter-efficient visual instruction model.\narXiv preprint arXiv:2304.15010.\nGuo*, Z.; Zhang*, R.; Qiu*, L.; Ma, X.; Miao, X.; He, X.;\nand Cui, B. 2022. CALIP: Zero-Shot Enhancement of CLIP\nwith Parameter-free Attention. AAAI 2023 Oral.\nGuo, Z.; Zhang, R.; Zhu, X.; Tang, Y .; Ma, X.; Han, J.;\nChen, K.; Gao, P.; Li, X.; Li, H.; et al. 2023. Point-bind &\npoint-llm: Aligning point cloud with multi-modality for 3d\nunderstanding, generation, and instruction following. arXiv\npreprint arXiv:2309.00615.\nGupta, A.; Dollar, P.; and Girshick, R. 2019. Lvis: A dataset\nfor large vocabulary instance segmentation. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, 5356‚Äì5364.\nHan, J.; Zhang, R.; Shao, W.; Gao, P.; Xu, P.; Xiao, H.;\nZhang, K.; Liu, C.; Wen, S.; Guo, Z.; et al. 2023. Imagebind-\nllm: Multi-modality instruction tuning. arXiv preprint\narXiv:2309.03905.\nHu, C.; and Li, X. 2023. When sam meets medical im-\nages: An investigation of segment anything model (sam)\non multi-phase liver tumor segmentation. arXiv preprint\narXiv:2304.08506.\nKirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.;\nGustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo,\nW.-Y .; et al. 2023. Segment anything. arXiv preprint\narXiv:2304.02643.\nLiang, F.; Wu, B.; Dai, X.; Li, K.; Zhao, Y .; Zhang, H.;\nZhang, P.; Vajda, P.; and Marculescu, D. 2023. Open-\nvocabulary semantic segmentation with mask-adapted clip.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 7061‚Äì7070.\nLiu, S.; Zeng, Z.; Ren, T.; Li, F.; Zhang, H.; Yang, J.; Li,\nC.; Yang, J.; Su, H.; Zhu, J.; et al. 2023a. Grounding dino:\nMarrying dino with grounded pre-training for open-set ob-\nject detection. arXiv preprint arXiv:2303.05499.\nLiu, Y .; Zhang, J.; She, Z.; Kheradmand, A.; and Armand,\nM. 2023b. Samm (segment any medical model): A 3d slicer\nintegration to sam. arXiv preprint arXiv:2304.05622.\nLiu, Z.; Zhang, Z.; Cao, Y .; Hu, H.; and Tong, X. 2021.\nGroup-free 3d object detection via transformers. InProceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, 2949‚Äì2958.\nLu, Y .; Xu, C.; Wei, X.; Xie, X.; Tomizuka, M.; Keutzer,\nK.; and Zhang, S. 2022. Open-vocabulary 3d detection\nvia image-level class and debiased cross-modal contrastive\nlearning. arXiv preprint arXiv:2207.01987.\nLu, Y .; Xu, C.; Wei, X.; Xie, X.; Tomizuka, M.; Keutzer,\nK.; and Zhang, S. 2023. Open-vocabulary point-cloud ob-\nject detection without 3d annotation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1190‚Äì1199.\nMisra, I.; Girdhar, R.; and Joulin, A. 2021. An end-to-end\ntransformer model for 3d object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2906‚Äì2917.\nNi, X.; and Li, P. 2023. Unified Text Structuralization\nwith Instruction-tuned Language Models. arXiv preprint\narXiv:2303.14956.\nOord, A. v. d.; Li, Y .; and Vinyals, O. 2018. Representation\nlearning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nPratt, S.; Covert, I.; Liu, R.; and Farhadi, A. 2023. What\ndoes a platypus look like? Generating customized prompts\nfor zero-shot image classification. arXiv:2209.03320.\nQi, C. R.; Litany, O.; He, K.; and Guibas, L. J. 2019. Deep\nhough voting for 3d object detection in point clouds. In\nproceedings of the IEEE/CVF International Conference on\nComputer Vision, 9277‚Äì9286.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748‚Äì8763. PMLR.\nRahman, S.; Khan, S.; and Barnes, N. 2020. Improved\nvisual-semantic alignment for zero-shot object detection. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, 11932‚Äì11939.\nRahman, S.; Khan, S. H.; and Porikli, F. 2020. Zero-shot\nobject detection: Joint recognition and localization of novel\nconcepts. International Journal of Computer Vision, 128:\n2979‚Äì2999.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16730\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\nBerg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale Vi-\nsual Recognition Challenge. International Journal of Com-\nputer Vision (IJCV), 115(3): 211‚Äì252.\nSong, S.; Lichtenberg, S. P.; and Xiao, J. 2015. Sun rgb-d:\nA rgb-d scene understanding benchmark suite. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 567‚Äì576.\nWang, S.; Sun, X.; Li, X.; Ouyang, R.; Wu, F.; Zhang,\nT.; Li, J.; and Wang, G. 2023. GPT-NER: Named Entity\nRecognition via Large Language Models. arXiv preprint\narXiv:2304.10428.\nXu, C.; Yang, S.; Galanti, T.; Wu, B.; Yue, X.; Zhai, B.;\nZhan, W.; Vajda, P.; Keutzer, K.; and Tomizuka, M. 2021.\nImage2Point: 3D Point-Cloud Understanding with 2D Im-\nage Pretrained Models. arXiv preprint arXiv:2106.04180.\nZareian, A.; Rosa, K. D.; Hu, D. H.; and Chang, S.-F. 2021.\nOpen-vocabulary object detection using captions. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 14393‚Äì14402.\nZhang, R.; Guo, Z.; Zhang, W.; Li, K.; Miao, X.; Cui, B.;\nQiao, Y .; Gao, P.; and Li, H. 2022. Pointclip: Point cloud\nunderstanding by clip. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n8552‚Äì8562.\nZhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,\nH.; Gao, P.; and Qiao, Y . 2023a. LLaMA-Adapter: Efficient\nFine-tuning of Language Models with Zero-init Attention.\narXiv preprint arXiv:2303.16199.\nZhang, R.; Hu, X.; Li, B.; Huang, S.; Deng, H.; Li, H.; Qiao,\nY .; and Gao, P. 2023b. Prompt, Generate, then Cache: Cas-\ncade of Foundation Models makes Strong Few-shot Learn-\ners. CVPR 2023.\nZhang, R.; Jiang, Z.; Guo, Z.; Yan, S.; Pan, J.; Dong, H.;\nGao, P.; and Li, H. 2023c. Personalize segment anything\nmodel with one shot. arXiv preprint arXiv:2305.03048.\nZhang, R.; Wang, L.; Qiao, Y .; Gao, P.; and Li, H. 2023d.\nLearning 3D Representations from 2D Pre-trained Models\nvia Image-to-Point Masked Autoencoders. CVPR 2023.\nZhang, Z.; Sun, B.; Yang, H.; and Huang, Q. 2020. H3dnet:\n3d object detection using hybrid geometric primitives. In\nComputer Vision‚ÄìECCV 2020: 16th European Conference,\nGlasgow, UK, August 23‚Äì28, 2020, Proceedings, Part XII\n16, 311‚Äì329. Springer.\nZhou, X.; Girdhar, R.; Joulin, A.; Kr ¬®ahenb¬®uhl, P.; and\nMisra, I. 2022. Detecting twenty-thousand classes using\nimage-level supervision. In Computer Vision‚ÄìECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23‚Äì27,\n2022, Proceedings, Part IX, 350‚Äì368. Springer.\nZhu, X.; Zhang, R.; He, B.; Guo, Z.; Zeng, Z.; Qin, Z.;\nZhang, S.; and Gao, P. 2023. Pointclip v2: Prompting clip\nand gpt for powerful 3d open-world learning. ICCV 2023.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n16731",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7112749814987183
    },
    {
      "name": "Modal",
      "score": 0.6617566347122192
    },
    {
      "name": "Vocabulary",
      "score": 0.6154036521911621
    },
    {
      "name": "Computer science",
      "score": 0.5054893493652344
    },
    {
      "name": "Engineering",
      "score": 0.34282028675079346
    },
    {
      "name": "Artificial intelligence",
      "score": 0.329983115196228
    },
    {
      "name": "Architectural engineering",
      "score": 0.3274712562561035
    },
    {
      "name": "Linguistics",
      "score": 0.19885364174842834
    },
    {
      "name": "Materials science",
      "score": 0.15867799520492554
    },
    {
      "name": "History",
      "score": 0.1064310073852539
    },
    {
      "name": "Philosophy",
      "score": 0.08166468143463135
    },
    {
      "name": "Composite material",
      "score": 0.07987946271896362
    },
    {
      "name": "Archaeology",
      "score": 0.056389957666397095
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    }
  ]
}