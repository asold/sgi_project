{
    "title": "On Designing a SwinIris Transformer Based Iris Recognition System",
    "url": "https://openalex.org/W4392061596",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2976238396",
            "name": "Runqing Gao",
            "affiliations": [
                "University of Georgia"
            ]
        },
        {
            "id": "https://openalex.org/A214859206",
            "name": "Thirimachos Bourlai",
            "affiliations": [
                "University of Georgia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2302077480",
        "https://openalex.org/W2112132530",
        "https://openalex.org/W4296830378",
        "https://openalex.org/W3121062617",
        "https://openalex.org/W4312612446",
        "https://openalex.org/W4285985208",
        "https://openalex.org/W3132246842",
        "https://openalex.org/W4226185647",
        "https://openalex.org/W2908357147",
        "https://openalex.org/W4310090413",
        "https://openalex.org/W4220962795",
        "https://openalex.org/W2942211779",
        "https://openalex.org/W3193618043",
        "https://openalex.org/W3035758039",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W3011098590",
        "https://openalex.org/W3197772899",
        "https://openalex.org/W4206268882",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4296794745",
        "https://openalex.org/W4223970271",
        "https://openalex.org/W4283080861",
        "https://openalex.org/W6810133556",
        "https://openalex.org/W4322743576",
        "https://openalex.org/W6847249830",
        "https://openalex.org/W1562596824",
        "https://openalex.org/W1251066901",
        "https://openalex.org/W4250819038",
        "https://openalex.org/W3125655613",
        "https://openalex.org/W2941794491",
        "https://openalex.org/W2801061858",
        "https://openalex.org/W3161981408",
        "https://openalex.org/W3157688273",
        "https://openalex.org/W4362544391",
        "https://openalex.org/W2102796633",
        "https://openalex.org/W3034630913",
        "https://openalex.org/W4310018648",
        "https://openalex.org/W3090136216",
        "https://openalex.org/W3119601694",
        "https://openalex.org/W3146846819",
        "https://openalex.org/W2594561511",
        "https://openalex.org/W4211133159",
        "https://openalex.org/W4381195709",
        "https://openalex.org/W4224141993",
        "https://openalex.org/W4214575921",
        "https://openalex.org/W3081154641",
        "https://openalex.org/W4385196142",
        "https://openalex.org/W4221148657",
        "https://openalex.org/W4307473288"
    ],
    "abstract": "In this paper, we discuss our proposed unified approach to iris recognition called the SwinIris. This approach works efficiently when combing a tuned version of the original Swin Transformer with a set of iris recognition processes. While the original Swin Transformer has recently been used on different biometric modalities due to its competitive advantage over other architectures, it has not been used for iris recognition. Thus, in our work, our proposed deep learning architecture is using a pre-trained Swin Transformer model that is fine-tuned with a set of linear layers. This model is a set of algorithmic aspects that achieve competitive iris-matching accuracy when using any of the selected iris databases selected to test our approach. Specifically, our proposed SwinIris Transformer-based iris recognition system is composed of four modules, namely, the eye detection, iris detection, iris segmentation, and iris classification modules. The proposed system commences with a detection process that aims to identify eyes within the camera-captured original iris images. Subsequently, the iris detection process identifies the iris patterns within the detected iris images to ensure that they are present and can be segmented. The third module involves iris segmentation, which extracts iris features utilized by the fourth and final module that matches iris images using the SwinIris-based Transformer model. The performance of the proposed system is evaluated on small- and large-scale iris datasets, including the CASIA-Iris- Thousand, CASIA-Iris-Lamp, CASIA-IntervalV4, and CASIA- IntervalV3. Our proposed model has a competitive iris classification accuracy when compared to various academic state-of-the-art methodologies, resulting in a classification performance ranging from 95.14&#x0025; to 99.56&#x0025;.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.1 120000\nOn Designing a SwinIris Transformer based Iris\nRecognition System\nRUNQING GAO1, (Student Member, IEEE), THIRIMACHOS BOURLAI 2 (Senior Fellow Member,\nIEEE)\n1MILAB, School of Electrical and Computer Engineering, The University of Georgia, Athens, GA 30602\nCorresponding author: Thirimachos Bourlai (e-mail: thirimachos.bourlai@uga.edu).\nABSTRACT In this paper, we discuss our proposed unified approach to iris recognition called the SwinIris.\nThis approach works efficiently when combing a tuned version of the original Swin Transformer with a\nset of iris recognition processes. While the original Swin Transformer has recently been used on different\nbiometric modalities due to its competitive advantage over other architectures, it has not been used for\niris recognition. Thus, in our work, our proposed deep learning architecture is using a pre-trained Swin\nTransformer model that is fine-tuned with a set of linear layers. This model is a set of algorithmic aspects\nthat achieve competitive iris-matching accuracy when using any of the selected iris databases selected to test\nour approach. Specifically, our proposed SwinIris Transformer-based iris recognition system is composed\nof four modules, namely, the eye detection, iris detection, iris segmentation, and iris classification modules.\nThe proposed system commences with a detection process that aims to identify eyes within the camera-\ncaptured original iris images. Subsequently, the iris detection process identifies the iris patterns within the\ndetected iris images to ensure that they are present and can be segmented. The third module involves iris\nsegmentation, which extracts iris features utilized by the fourth and final module that matches iris images\nusing the SwinIris-based Transformer model.\nThe performance of the proposed system is evaluated on small- and large-scale iris datasets, including\nthe CASIA-Iris- Thousand, CASIA-Iris-Lamp, CASIA-IntervalV4, and CASIA- IntervalV3. Our proposed\nmodel has a competitive iris classification accuracy when compared to various academic state-of-the-art\nmethodologies, resulting in a classification performance ranging from 95.14% to 99.56%.\nINDEX TERMS Biometrics, Swin Transformer, deep learning, human identification, iris recognition.\nI. INTRODUCTION\nI\nN biometric authentication studies, iris recognition stands\nas a pioneering technology. It is leveraging the intricate\nand unique patterns within the human iris for secure and\nreliable identification. Iris patterns are considered unique\ndue to their intricate nature formed during the early stages\nof human development; they also remain relatively stable\nthroughout a person’s lifetime.[44]. Iris features demonstrate\na random a arrangement, and they include Fuchs’ crypts,\nWolfflin nodules, pigment spots, contraction furrows and\nconjunctival melanosis [1]. Such features generate a unique\npattern for each subject, contributing to the iris’s uniqueness\nfor reliable biometric identification. There are many advan-\ntages of using the iris module for human authentication -,i.e.,\n(a) iris is a unique and stable biometric identifier. Its patterns\nare distinct, ensuring reliable identification over time;(b) iris\noffer a non-intrusive and hygienic authentication. Unlike\nother modalities such as fingerprints, the iris requires only\npassive image capture; (c) iris recognition systems result in\nhigh recognition accuracy compared to other modules; (d) iris\nrecognition is user-friendly and doesn’t necessitate active user\ncooperation. Thus, it is highly accepted among individuals;\n(e) Additionally, irises can be combined with other modalities\n(multimodal systems) that enhance overall biometric system\naccuracy.\nFor the processing of four datasets, we used in this study,\nthis paper draws on the Hough transform algorithm [2] for\niris detection and the morphological techniques to extract iris\ninformation for iris segmentation. Finally, a deep learning\nmodel is built to perform iris recognition on the cut iris\nimages. The SwinIris Transformer-based model proposed in\nthis paper is more efficient and accurate and as we will show,\nit can efficiently solve the multi-classification problem.\nWhile there are various iris recognition algorithms pro-\nposed in the literature [3],the majority, of both commercial\nand research-based ones, involve extracting iris characteris-\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\ntics as a crucial step in the authentication process. Conse-\nquently, the performance of these algorithms heavily relies\non the efficiency and accuracy of an iris classification(multi-\nclasses) algorithm used. The original Swin Transformer is\nan advanced backbone of computer vision tasks including\nclassification, and segmentation problems. Unlike convolu-\ntion neural network(CNN) downsampling of feature maps is\nachieved by using the convolution operation, i.e., the Swin\ntransformer down sample feature maps in a pure transformer\nnetwork without convolution.\nII. MOTIVATION AND CONTRIBUTION\nThe contribution of the proposed approach is fully realized in\nthe following three ways.\nA. UNIQUE BIOMETRIC TRAIT-HUAMAN IRIS\nThe iris is the colored, ring-shaped part of the eye, and it\nis known for its distinctive patterns that vary from person\nto person. Iris is a combination of crypts, wolflin nodules,\nconcentrated furrows, and pigment spots which are shown in\nFigure 1. The iris contains a large number of distinguishable\nfeatures, such as radial lines, concentric circles, and irregular\nshapes. Automated iris recognition systems that apply deep\nlearning approaches use these features to create a unique\nbiometric template for each individual. On the other hand,\nUnlike some other biometric traits, such as facial features that\ncan change with age or facial expressions, the iris remains\nstable over time. This stability ensures the reliability and\nlongevity of iris-based recognition systems.\nFIGURE 1: An Iris sample consists of various features [4]\nB. EFFICIENT BACKBONE-SWINIRIS ARCHITECTURE\nGiven our objective to conduct experimentation on numer-\nous diverse datasets, the pivotal criterion in our architectural\nselection was its lightweight nature and multi-compatibility.\nSwin Transformer architecture possesses the capability to\nhandle a diverse array of image sizes without necessitating\nsignificant architectural alterations, rendering them apt for\naccommodating both high-resolution and low-resolution im-\nages. On the other hand, Swin Transformers leverage the\nshifted window mechanism, enabling them to effectively de-\ncrease the memory footprint in comparison to conventional\nVision Transformers (ViTs). This attribute renders Swin\nTransformers more amenable to training on restricted com-\nputational resources and facilitates the management of larger\nbatch sizes. More importantly, Swin Transformers demon-\nstrate competitive performance in vision tasks while utilizing\na reduced number of parameters compared to other large-\nscale vision models. This characteristic effectively diminishes\nthe model’s memory requirements, rendering it more acces-\nsible and practical for real-world applications.\nDue to the numerous advantages of the Swin Transformer\narchitecture mentioned above, we have adopted its core\nframework and upgraded it to the SwinIris architecture in-\ntroduced in this paper. This architecture can better handle eye\nimages while retaining the advantages of the original Swin\ntransformer, significantly improving accuracy in classifica-\ntion tasks.\nC. MODEL VALIDATION\nWe evaluated the performance of the proposed method by\ncomputing iris classification accuracy on the CASIA-Iris-\nThousand, CASIA-Iris-IntervalV3, CASIA-Iris-IntervalV4,\nCASIA-Iris-Lamp datasets, and compared the performance\nof the methods mentioned in this paper with research iris\nrecognition deep learning neural networks. The experimental\nresults demonstrated that our proposed architecture can effec-\ntively achieve iris recognition whether on large-scale datasets\nor small-scale datasets yielding a classification accuracy that\nranges from 95.49% to 99.17%. The results of the proposed\nmethod are higher than other proposed structures in the litera-\nture. Furthermore, this paper presents a comparative analysis\nbetween the original Swin Transformer framework and our\nproposed SwinIris framework on four distinct datasets. The\ncomparison will encompass evaluations of running speed and\naccuracy as the primary metrics. Thus, By evaluating several\naspects, we can conclude that our proposed system maintains\na high level of accuracy for different datasets without the\nneed to change the architecture according to the different\ndatasets, and it performs better than using only the original\nSwin Transformer architecture.\nTable 1 compares the conventional workflow components\nto our approach to emphasize our contributions.\nTable 1: Datasets and distribution of proposed method\nAuthors Iris segmentation\nManually Classifier Feature\nSelection\nValidation\nDatabases\nYang [5] No DualSANet Yes 3\nSingh [6] No VGG16+CNN Yes 1\nAlwawi [7] No CNN Yes 1\nGarg [8] No BPNN Yes 1\nSun [9] No OCFON+CNN Yes 2\nProposed\nArch No Proposed\nSwinIris Yes 4\nIII. RELATED WORK\nAs shown above, we demonstrated the comparison of con-\nventional workflow components [7] [5] [8] [6] [9] to our\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\napproach. Related to the aforementioned problem, the most\nrelevant works in the literature are discussed. Table 2 sum-\nmarizes current studies which are shown below.\nBalashanmugam et al. [10] proposed a model to feed pre-\nprocessed accurate iris boundary into Alexnet deep learning\nneural network-based system for classification. Li et al. [11]\npropose a hybrid approach that combines learning-based and\nedge-based algorithms for iris segmentation. Experimental\nresults showed that the proposed iris segmentation method\nachieved 95.49% accuracy on the challenging CASIA-Iris-\nThousand database. In the study [12], Ali also included the\nsetup of a convolution neural network (CNN), a convolution\nneural network that does not require feature extraction, as\nwell as a comparison of the two types of classifiers made. Iris\nimages are obtained from the Chinese Academy of Sciences\nInstitute of Automation dataset (CASIA Iris-V1), a common\ndatabase used for the iris recognition system. The proposed\nalgorithm is claimed to be straightforward, simple, efficient,\nand fast. The experimental results show that the proposed\nalgorithm achieves a high classification accuracy of approx-\nimately 95.5% and 95% for CNN and NN based on SDD\nfeatures respectively. The proposed algorithms outperformed\nother studies in the literature and claim to require less time\nfor determining the location of the iris region.\nLat et al. [13] aimed to solve the problem of determining\nthe classification threshold for large datasets of iris images\nand extracting more discriminatory features from as many\niris images as possible. The paper proposes a novel iris\nidentification framework that integrates the lightweight Mo-\nbileNet architecture with customized ArcFace and Triplet loss\nfunctions. By combining two loss functions, it is possible to\nimprove the compactness within a class and the discrepan-\ncies between classes. Alwawi et al. [7] developed the deep\nlearning technology-based new 2D convolutional neural net-\nwork (CNN) model introduced for extracting the features and\nclassifying the iris patterns. Zhao et al. [14] proposed a deep\nlearning method based on the capsule network architecture in\niris recognition. They provide a modified routing algorithm\nbased on the dynamic routing between two capsule layers to\nmake this technique adapt to iris recognition. Migration learn-\ning makes the deep learning method available even when the\nnumber of samples is limited. Therefore, three state-of-the-\nart pretrained models, VGG16, InceptionV3, and ResNet50,\nare introduced. They divide the three networks into a series of\nsubnetwork structures according to the number of their major\nconstituent blocks. They are used as the convolutional part\nto extract primary features, instead of a single convolutional\nlayer in the capsule network. The Hough transform algorithm\nand algorithm uses morphological techniques proposed by\nMoslhi [2] to help the processing of iris detection and iris\nsegmentation and has great performance.\nA. HOUGH TRANSFORMER ALGORITHM\nThe works related to the Hough Transformer algorithm have\nbeen presented in different aspects.\nJayanthi et al. [16] applied the Hough Transformer model\nto localize the region of interest, i.e., iris in an effective way.\nThis algorithm provides high-quality images for the subse-\nquent iris segmentation process as well as for iris recognition\nby using the MRCNN [17]with Inception v2 [18] model.\nZhao et al. [19] integrate the classical Hough transform\ntechnique into deeply learned representations, culminating in\nthe proposal of a one-shot end-to-end learning framework\nfor line detection. Increasing the flight time of Unmanned\nAerial Vehicles (UA Vs) is a crucial and challenging task for\nUA V design engineers. This is particularly important for tasks\nsuch as monitoring, mapping, or signal conversion. Aleksandr\nLapušinskij et al. [20]propose an algorithm that is based on\nthe application of Hough transform and Canny edge detection\nmethods, which have not been previously used for such tasks.\nThe achieved average accuracy of 87% on an imbalanced\ndataset demonstrates the practical applicability of the pro-\nposed method in detecting thermal updrafts associated with\ncumulus clouds.The process of detecting marking segments\nin an image involves sliding window processing, where, at\neach window position, Fast Hough Transform (FHT) [21] is\nused to calculate and identify straight lines. Moreover, the\nidentified segments are then grouped based on their relative\npositions.\nB. SWIN TRANSFORMER\nOne of the primary challenges in biometrics is the ability\nto accurately recognize individuals in real-world scenarios.\nTraditional biometric methods, such as fingerprint and face\nrecognition, can be unreliable due to changes in lighting\nconditions, pose, and occlusion. Swin Transformers offer a\npromising solution to these challenges by providing a more\nrobust and adaptable approach to human identification.\nLiu et al. [22] proposed a new vision Transformer, called\nSwin Transformer, that capably serves as a general-purpose\nbackbone for computer vision. This hierarchical architecture\nhas the flexibility to model at various scales and has linear\ncomputational complexity with respect to image size. These\nqualities of the Swin Transformer make it compatible with\na broad range of vision tasks, including image classifica-\ntion and dense prediction tasks such as object detection\nand semantic segmentation. Lu et al. [23] applied the Swin\nTransformer architecture to recognize and segment individ-\nual pigs. The Swin Transformer-based approach achieved\nrecognition accuracy of 93.0% and a segmentation accuracy\nof 86.9% for individual pigs in surveillance video images.\nEven when faced with challenging scenarios such as overlap-\nping, occlusion, and deformation, the technique demonstrated\noutstanding recognition performance for substitute pigs. A\nmethod, named “Swin-MLP”, based on Swin Transformer\nand multi-layer perceptron (MLP) to identify the strawberry\nappearance quality is proposed by Zheng et al. [24] in order\nto overcome the problem of excessive training time.The\nproposed method achieves an accuracy of 98.45%, which\nis 2.61% higher than the original Swin-T model. Moreover,\nthe Swin-MLP model has a significantly faster training time\nof only 16.79 s compared to other models. Jiang et al. [25]\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nTable 2: Comparison of algorithms for iris recognition system [15]\nAuthor Algorithm Advantages Disadvantages (if any) Hypothesis tests and the\nmodel results (if any)\nBalashanmugam et al. [10] modified Alex network It effectively identifies\niris boundaries and\nfeatures, improving over\ncurrent models.Utilizes\npreprocessed iris images,\nleading to more accurate\nfeature extraction.\nThe complexity of deep\nlearning models can require\nsignificant computational\nresources.The method’s\neffectiveness across diverse\niris databases or in real-\nworld scenarios is not\nexplicitly discussed.\nhe proposed system classi-\nfies iris with 99.1% accu-\nracy. Sensitivity, specificity,\nand F1-score are 99.68%,\n98.36%, and 0.995 respec-\ntively.\nLi et al. [11] a combination method of\nlearning-based and edge-\nbased algorithms\nRobust detection and seg-\nmentation of the iris in di-\nverse and challenging im-\nages.Fast performance suit-\nable for real-time applica-\ntions.\nRelies heavily on deep\nlearning, which may require\nsubstantial computational\nresources.\nHigh accuracy (95.49%)\non the challenging\nCASIA-Iris-Thousand\ndatabase.It demonstrated a\nsignificant improvement in\naccuracy and performance\ncompared to previous\nmethods, specifically for\niris segmentation tasks\nin challenging image\nconditions.\nAli et al. [12] semi-discrete matrix\ndecomposition (SDD) and\nconvolution neural network\n(CNN)\nEfficient and fast,\nsuitable for real-time\napplications.Does not\nrequire a normalization\nstage, reducing\ncomputational costs.\nThe complexity and\nresource requirements\nof deep learning models\nare not discussed.Limited\nevaluation on a single\ndataset (CASIA Iris-V1).\nThe method outperformed\nprevious works in terms of\nclassification accuracy and\nrequired less time for iris\nlocalization. High classifi-\ncation accuracy (95.5% for\nCNN and 95% for NN)\nLat et al. [13] Boosting Iris Recognition\nby Margin-Based Loss\nFunctions\nEfficient discrimination\nbetween classes, enhancing\nfeature representation.Skips\nthe normalization step,\nsimplifying preprocessing.\nThe model’s performance in\ndiverse or more challenging\nreal-world scenarios is not\nthoroughly explored.\nHigh accuracy, with EER\n(Equal Error Rate) as low as\n0.45% for IITD and 1.87%\nfor CASIA-Iris-Thousand\ndatasets.\nAlwawi et al. [7] a deep learning-based Con-\nvolutional Neural Network\n(CNN) model\nRapid processing with\na testing time of\n12 seconds.Effective\nfeature extraction and\nclassification.\nLimited testing on\nmore diverse iris\ndatabases.Potential resource\nintensiveness due to deep\nlearning.\nHigh overall training accu-\nracy of 95.33%,The model\nachieved 100% testing accu-\nracy\nZhao et al. [14] Capsule Network architec-\nture.\nHigh accuracy in iris\nrecognition with varying\nnetwork structures.Capsule\nNetwork architecture\nincreases robustness against\nvarying light conditions and\npupil sizes.\nThe complexity of Capsule\nNetworks might require sig-\nnificant computational re-\nsources.Limited exploration\nof the model’s performance\nin diverse real-world condi-\ntions.\nAchieved high accuracy\nacross different datasets\n(JluV3.1, JluV4, and\nCASIA-V4 Lamp).\nintroduce a novel 3D medical image segmentation method\ncalled SwinBTS. This approach combines a transformer, a\nconvolutional neural network, and an encoder-decoder struc-\nture to formulate 3D brain tumor semantic segmentation\nas a sequence-to-sequence prediction task. In their study,\nKim et al. [26] propose a Swin transformer-based method\nfor facial expression recognition using an in-the-wild audio-\nvisual dataset from the Aff-Wild Expression dataset. In this\nstudy, Cosma et al. [27] investigate the application of five dis-\ntinct vision transformer architectures for self-supervised gait\nrecognition. Specifically, they fine-tune and pre-train the ViT,\nCaiT, CrossFormer, Token2Token, and TwinsSVT models on\ntwo large-scale gait datasets, GREW and DenseGait. Grosz et\nal. [28] introduced the first use of a Vision Transformer (ViT)\nto learn a discriminative fixed-length fingerprint embedding\nand by fusing embeddings learned by CNNs and ViTs the\nauthors claim they can reach near parity with a commercial\nstate-of-the-art (SOTA) matcher.\nOverall, Existing studies often depend on manually de-\nsigned methods grounded in domain knowledge, potentially\nunderutilizing deep learning’s full potential. This reliance\ncontributes to inaccuracies such as false detection or seg-\nmentation of images, impacting iris recognition precision.\nFurthermore, these studies typically focus on employing\nsophisticated architectures without tailoring them specifically\nfor iris recognition enhancement. Moreover, their application\nis often limited to one or two publicly available datasets,\nquestioning the methods’ broad applicability and effective-\nness in diverse scenarios. Ultimately, these approaches do\nnot significantly advance the performance in iris recognition\ntasks.\nThe current research aims to address the existing limita-\ntions in iris recognition systems by proposing more advanced\ndeep learning techniques for feature extraction and matching.\nBy moving away from traditional handcrafted methods and\nemploying more sophisticated deep neural networks, the\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nFIGURE 2: The workflow of the proposed approach. On the left-hand side, It can be seen that the process of the whole iris\nrecognition system. Starting with data acquisition from various CASIA datasets, images undergo processing steps including eye\ndetection, iris detection, and segmentation. These processed images are then classified using the SwInIris Architecture. Model\nperformance is validated through cross-validation, comparison with other datasets, and benchmarking against state-of-the-art\nmethods.The images on the right side of the diagram showcase sample iris images from each of the datasets mentioned in the\nData Acquisition section and the subsequent processed images from the Image Processing steps.\nresearch seeks to improve the accuracy and robustness of iris\nrecognition systems, making them more effective in various\nsettings and environments. The use of Swin Transformers\nfor human identification related research and beyond shows\npromise for improving accuracy and robustness in real-world\nobject recognition scenarios. The current literature shows\nthat more research is needed to explore their performance\non larger and more diverse datasets, as well as their ability\nto generalize to new individuals and modalities. This paper\naims to assist in this direction, focusing on using, to the best\nof our knowledge, the Swin transformer to iris images for the\nfirst time.\nIV. METHODOLOGY\nOur iris recognition system contains four main steps:\n1)Dataset Acquisition 2)Image Processing 3)Classification\nModel 4) Model Validation. The workflow is shown in Figure\n2.\nA. DATASET ACQUISITION\nThe CASIA datasets, developed and maintained by the Chi-\nnese Academy of Sciences Institute of Automation (CASIA),\nare a collection of widely used benchmark datasets in the field\nof computer vision and pattern recognition. These datasets\nare specifically designed to facilitate research and evaluation\nin various areas, such as face recognition, iris recognition,\nhandwritten character recognition, and more.\n• The CASIA Iris Thousand dataset contains iris\nimages from 1000 subjects, which were collected\nusing IKEMB-100 camera produced by IrisKing(\nhttp://www.irisking.com). IKEMB-100 is a dual-eye iris\ncamera with friendly visual feedback. The main sources\nof intra-class variations in CASIA-Iris-Thousand are\neyeglasses and specular reflections. Since CASIA-Iris-\nThousand is the first publicly available iris dataset with\none thousand subjects, it is well-suited for studying the\nuniqueness of iris features and developing novel iris\nclassification.\n• Iris images of CASIA-Iris-Interval version 3 and ver-\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nsion 4were captured with our self-developed close-up\niris camera. The most compelling feature of our iris\ncamera is that we have designed a circular NIR LED\narray, with suitable luminous flux for iris imaging. Our\niris camera can capture apparent iris images because of\nthis novel design.\n• The CASIA-Iris-Lamp dataset was collected using a\nhand-held OKI iris sensor. A lamp was turned on /off\nclose to the subject to introduce more intra-class varia-\ntions when we collected CASIA-Iris-Lamp. This dataset\nis good for studying problems of non-linear iris normal-\nization and robust iris feature representation.\nTable 3: Datasets and distribution of proposed method\nDataset Name Class Train Testing Total\nCASIA-Iris-Thousand 1000 16000 4000 20000\nCASIA-Iris-IntervalV3 249 2111 528 2639\nCASIA-Iris-IntervalV4 249 2111 528 2639\nCASIA-Iris-Lamp 411 12969 3243 16212\nTable 2 compares datasets that are used in this paper. The\ninitially segmented features of the iris were divided into train-\ning and test images. There are around 20000 and 2639 images\nfor both the left and right eye in the CASIA-Iris-Thousand\nand CASIA-Interval respectively. The CASIA-Lamp dataset\ncontains 16212 iris images, split into 12969 and 3243 for\ntraining and testing, respectively.\nB. IMAGE PROCESSING\n1) Eye Detection\nThe model employs Haar cascade classifiers for eye detection\ndue to their speed, minimal computational time requirement,\nand high accuracy levels [29]. Images captured from the\ncamera are subjected to Haar cascade classifiers to identify\neyes within them. This phase guarantees the presence of eyes\nin the images, allowing them to proceed to the subsequent step\nsolely if eyes are successfully detected.\n2) Iris Detection\nThe iris can be defined as the region located between the pupil\nand the remaining part of the eye. As discussed above, the\nHough Transformer model has been applied in many fields,\nwe defined a Hough Transformer model mathematically be-\nlow in order to detect distance d.\nd = x × cos(α) +y × sin(α)\nwhere d represents the Perpendicular distance from the\norigin to the tangent line to the edge,x and y represent the\nhorizontal and vertical coordinates of each pixel, α is the\nangle between d and x axis.\n3) Iris segmentation\nIris segmentation plays a significant role in the iris recog-\nnition process since the features extracted from this step\nwill be used in the process of iris classification. Therefore,\nthe accuracy of the classification will highly depend on\nsegmented iris images.\nwe draw on the segmentation model proposed called morpho-\nlogical operations [2]. Morphology in image processing gives\nimages shape and analysis, and it has various uses in fields\nlike cellular biology [30]. Morphological operations involve\ninteractions between objects and structural components. The\nmethod starts out by defining the \"selected reference\" for\neach image, which is the total number of pixels in the image\nwhen it successfully completes the morphological process\nwith threshold = 0. In working reference, this threshold starts\nto rise, and it is contrasted with the golden reference. The\nthreshold will continue until the working reference starts to\ndeviate from the golden reference by a specific amount, at\nwhich point the procedure will stop. The working reference\nwill be selected for the subsequent stage. The contour border\nalgorithm [31]. Figure 3 displays the results in their final\nform.\nFIGURE 3: (a) a segmented iris image from the CASIA-Iris-\nThousand dataset, (b) an example of a segmented iris image\nfrom the CASIA-IntervalV3 dataset,(c) and (d) two samples\nof segmented iris images from the CASIA-IntervalV4 and\nCASIA-Lamp,respectively\nC. IRIS CLASSIFICATION\n1) Swin Iris Architecture\nIn the preceding motivation section, we have already men-\ntioned the diverse advantages associated with the Swin Trans-\nformer architecture. Our aspiration is to leverage these merits\nto enhance the efficacy of iris recognition. The proposed\narch presents a model that has been devised with the Swin\nTransformer serving as its backbone, specifically tailored for\nthe purpose of iris classification. There is the original arch\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\n(a) Original Arch\n (b) SwinIris Arch\nFIGURE 4: Compare two Architectures. where LE is linear encoding, the basic block is composed of Swin Transformer\nBlock(ST Block) and Patch Merging(PM). The additional layers consist of linear layers, linear normalization(LN), and ReLU.\nplot(left) and the proposed arch plot (right) shown in Figure\n4.\nIn this study, we compare two architectures: the Original\nArch and the proposed SwinIris Arch to study the role of the\nSwinIris arch in improving recognition accuracy. The original\narch demonstrates the backbone of Swin Transformer archi-\ntecture, which mainly combines Swin Transformer blockers\nand patch mergings. Patch merging in the Swin Transformer\narchitecture involves aggregating information from smaller\npatches of an input image to create larger patches, enabling\nglobal context understanding. The Swin Transformer blocker\nis a fundamental building block in Swin Transformers, com-\nbining shifted windows and token mixing to efficiently cap-\nture both local and global features in an image. However, we\nfound that simply applying the Swin Transformer architecture\nto address the problem did not yield satisfactory recognition\ncapabilities. Therefore, to achieve improved iris recognition\nresults, we designed a model with the Swin Transformer\narchitecture as the backbone, supplemented by the addition\nof LinearNorm layers, linear layers, and ReLU layers, then\nfine-tuning the whole model as the final one.\nThe original Swin Transformer, while robust for general\ncomputer vision tasks, is not inherently optimized for the\nhigh-resolution pattern recognition required in iris recogni-\ntion. Iris images present unique challenges, including com-\nplex textures and minute spatial hierarchies that are critical\nfor accurate identification. By integrating additional linear\nlayers into the SwinIris architecture, we provide the model\nwith the capacity to capture and interpret these iris patterns\nmore effectively. These layers act as specialized refinements,\nenhancing the model’s sensitivity and specificity to the dis-\ntinctive features of iris textures. This adaptation allows the\nSwinIris to better understand and leverage the spatial re-\nlationships and subtle variations within the iris, which are\ncrucial for accurate and reliable recognition. The enhanced\narchitecture, therefore, not only retains the inherent strengths\nof the Swin Transformer but also becomes more attuned to the\nspecific requirements of iris recognition, leading to superior\nperformance compared to the original framework.\nD. MODEL VALIDATION\nGeneralizability is a fundamental characteristic of biometric\nrecognition systems. It allows the evaluation and testing\nof the same recognition system, including its parameters,\non different datasets. Therefore, in this study, to validate\nthe proposed method, various databases were utilized. Each\ndataset was randomly divided into training and testing sets,\nwhich were non-overlapping. Additionally, each experiment\non every database was repeated to ensure randomness in\ntraining and testing. The evaluation indicators applied in this\nstudy are shown below.\nThe metrics we employed, including accuracy, precision,\nand robustness, were deliberately chosen to provide a com-\nprehensive evaluation of the SwinIris model. Accuracy is\ncrucial for determining the model’s correct recognition rate,\nwhich directly impacts its practicality in real-world scenarios.\nPrecision offers insights into the model’s ability to minimize\nfalse positives, which is essential for applications where\nsecurity and reliability are paramount. Robustness, on the\nother hand, evaluates the model’s stability and performance\nconsistency across varied conditions and datasets. Together,\nthese metrics offer a holistic view of the model’s performance,\nensuring its effectiveness and reliability. By understanding\nand optimizing these metrics, we aim to enhance the SwinIris\nmodel’s applicability, ensuring it meets the rigorous demands\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nof real-world iris recognition applications.\n1) Accuracy\nIn the context of iris recognition technology, accuracy is a\ncritical performance metric quantifying the system’s ability\nto precisely match an individual’s unique iris pattern with the\npre-existing template or enrollment data stored in a database.\nThis metric serves as a cornerstone for evaluating the effec-\ntiveness and reliability of iris recognition systems. Mathemat-\nically, accuracy is defined as the ratio of correctly identified\nor matched instances to the total number of instances eval-\nuated. This ratio encapsulates both the true positive and true\nnegative identifications, offering a comprehensive measure of\nthe system’s proficiency in distinguishing and verifying indi-\nvidual iris patterns against a database of registered templates.\nAccurate iris recognition is pivotal in ensuring secure and\nefficient identification processes, particularly in applications\nwhere security and individual identification are paramount.\nHere we define accuracy mathematically.\nAccuracy = TP+TN\nTP+TN+FP+FN\nwhere TP , TN,FP , and FN are true positive, true negative,\nfalse positive, and false negative, respectively.\n2) Sensitivity\nIn academic terminology, sensitivity, also known as the true\npositive rate, is a statistical measure used to evaluate the per-\nformance of a classification model, particularly in the context\nof binary classification tasks. It is defined as the proportion\nof actual positive instances that are correctly identified by the\nmodel. Mathematically, sensitivity is expressed as the ratio\nof the number of true positive outcomes (instances where the\nmodel correctly predicts the positive class) to the total number\nof instances that are actually positive in the dataset. This\nratio is a crucial indicator of the model’s ability to detect and\ncorrectly classify positive instances, reflecting its effective-\nness in identifying cases of interest in a dataset characterized\nby binary outcomes. The formula for sensitivity is typically\nrepresented as:\nSensitivity = TP\nTP+FN\nwhere TP , TN,FP , and FN are true positive, true negative,\nfalse positive, and false negative, respectively.\n3) Specificity\nIn the scholarly discourse of statistical analysis and model\nevaluation, specificity is a paramount metric, particularly in\nthe realm of classification tasks. It is articulated as the pro-\nportion of actual negative cases that are accurately identified\nas negative by the classification model. Specificity, thus,\nserves as a critical measure of a model’s ability to correctly\nrecognize instances that do not belong to the positive class, or\nin more specific terms, the instances that should be rejected or\nclassified as negative. Mathematically, specificity is defined\nas the ratio of true negative outcomes (instances where the\nmodel correctly predicts the negative class) to the total num-\nber of actual negative instances in the dataset. This ratio is\npivotal in assessing the model’s precision in excluding non-\nrelevant or negative cases, thereby reflecting its effectiveness\nin avoiding false alarms or incorrect positive identifications.\nThe formula for specificity is conventionally represented as:\nSpecificity = TN\nTN+FP\nwhere TP , TN,FP , and FN are true positive, true negative,\nfalse positive, and false negative, respectively.\nE. EXPERIMENT SETUP\nWe trained and tested our model on a computer server pro-\nvided by Multispectral Imagery Labtory(MILAB) at Univer-\nsity of Georgia.The hardware and software environments are\ngiven in Table 4.\nTable 4: Software and Hardware environment\nGPU OS Python Pytorch Keras Cuda\nRTX 3080 Ubuntu\n20.04.6 LTS\nPython\n3.8.16\ntorch\n2.0.0\nKeras\n2.0.8 11.3\nV. RESULTS AND DISCUSSION\nA. CASIA DATABASES\nInitially, the basic Swin Transformer cannot produce good\nperformances on datasets. For example, the Swin Transformer\nmodel without pretraining only produced 65% accuracy on\ntesting in the CASIA-Iris-Thousand dataset [32]. Table 5\ncompares two architectures on different datasets. we compare\nthem in terms of training speed, and accuracy. The databases\nutilized can be listed as follows :\n1) Chinese Academy of Sciences Institute of Automation Iris\nthousand(CASIA-Iris-Thousand)\nThis dataset contains 1000 classes, we randomly selected\n16000 iris images for training, training on the original ar-\nchitecture(Original Arch) was completed in a total of 455\nminutes with a validation accuracy of 65% for successful iris\nclassification. However, The accuracy obtained by using our\nproposed SwinIris Arch architecture is much higher than the\noriginal architecture even though it consumes more time.\n2) Chinese Academy of Sciences Institute of Automation Iris\nInterval Version 3/ Version\n4(CASIA-Iris-IntervalV3/CASIA-Iris-IntervalV4)\nCASIA-Iris-IntervalV3 includes 249 classes as same as\nCASIA-Iris-IntervalV4. They took a similar amount of time\nto train on the original arch, 58 minutes, and 63 minutes,\nrespectively. Similarly, the accuracy is well improved when\ncompared to the original SwinIris architecture.\n3) Chinese Academy of Sciences Institute of Automation Iris\nLamp(CASIA-Iris-Lamp)\nThis dataset contains 411 classes, 12969 iris images as the\ntraining set. On the original arch, it took longer time on the\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nTable 5: Comparison of two architectures on different datasets\nDataset Name Original Arch Proposed SwinIris Arch\nClass Train Set Speed(s) Highest Accuracy Speed (s) Highest Accuracy%\nCASIA-Iris-Thousand 1000 16000 455 65.02 714 99.56\nCASIA-Iris-IntervalV3 249 2111 58 50.69 96 95.14\nCASIA-Iris-IntervalV4 249 2111 63 54.6 90 95.83\nCASIA-Iris-Lamp 411 12969 369 68.52 579 99.17\nTable 6: Comparison of FRR and Accuracy with each dataset\nFRR%@0.01%FAR Highest Acc %\nDataset Name Original Removed Non-Iris Original Removed Non-Iris\nCASIA-Iris-Thousand 4.17 2.43 99.56 99.78\nCASIA-Iris-IntervalV3 10.67 10.41 95.14 97.22\nCASIA-Iris-IntervalV4 20.15 29.72 95.83 97.74\nCASIA-Iris-Lamp 6.53 6.14 99.17 99.63\nSwinIris arch than on the original arch. However, the accuracy\nof the proposed approach is close to 100%. For a better\ncomparison, we plotted Figure 5 to visualize the differences.\nB. EXPERIMENTAL RESULTS\nFigure 5 also visualizes the iris recognition rates and speed\nfor each database for both architectures. It obviously can be\nseen that the performance of the proposed SwinIris arch is\nbetter than the original arch in terms of accuracy. However,\nthe speed of the SwinIris arch is slower than the original arch.\nit makes sense that the SwinIris arch is more complex than\nthe original arch. As we can see, We can see that, on the\nCASIA-Iris-IntervalV3 and on the CASIA-Iris-IntervalV4,\nthe SwinIris architecture is much slower than on the remain-\ning two datasets.\nThus, we used the weights which were already pre-trained\non ImageNet1K with SwinV2-T for the baseline, and we\nfound that the epoch number has not the most significant\neffect on the final model when we set it at 300. The optimizer\nwe used was Adam and the cross-entropy loss was applied as\nthe loss function. Then we fined-tune the whole final model to\nensure we can obtain a higher accuracy than only fined-tune\ntwo linear classification layers. For each dataset, we utilized\ncross-validation to select from both left and right iris images\nfor training/validation and testing in the ratio of 8:2. The\ntesting accuracy of each dataset and FRR by using SwinIris\narch are shown in Table 6.\nThe FRR at FAR column in table 6 represents two key\nperformance metrics for biometric identification systems: the\nFalse Rejection Rate (FRR) at a specific False Acceptance\nRate (FAR) threshold. The FRR is the rate at which genuine\nidentification attempts are incorrectly denied by the system,\na critical measure of user convenience and system efficacy.\nThe FAR, set here at an exceptionally low 0.01%, indicates\nthe system’s tolerance for falsely accepting an impostor, re-\nflecting its security level. Table 6 presents before and after the\nremoval of non-iris parts from images. Notably, the removal\nof non-iris parts consistently lowers the FRR and improves\naccuracy across most datasets, with the most significant de-\ncrease in FRR observed in the CASIA-Iris-Thousand dataset.\nIn fact, this metric balances the trade-off between usability\nand security—too high an FRR would inconvenience users,\nwhile a high FAR could compromise security. The column\ncompares these rates before and after removing non-iris el-\nements from the images, showing a decrease in FRR upon\nremoval. This indicates that the systems become less likely\nto reject genuine attempts when the images are preprocessed\nto eliminate irrelevant features, thereby enhancing both user\nexperience and security assurance by focusing on the most\ndistinctive and informative parts of the iris.\nFor origin datasets, We achieved 99.56% and 99.17% clas-\nsification accuracy on CASIA-Iris-Thousand and CASIA-\nIris-Lamp datasets reached 95.14% and 95.83% on CASIA\nversion 3,4, respectively.\nFor the removal of non-iris datasets, we approached\n99.78% and 99.63% classification on CASIA-Iris-Thousand\nand CASIA-Iris-Lamp datasets, also reached 97.22% and\n97.74% on CASIA version 3,4, respectively.It can be seen that\nthe architecture proposed has great performance not only on\nsmall-scale datasets but also on large-scale databases.\nIn addition, The accuracy and loss curves for the large-\nbase and small-base datasets are shown in Figure 6. Here\nwe display accuracy and loss curves on CASIA-IntervalV3,\nCASIA-IntervalV4(small-scale dataset) and CASIA-Iris-\nLamp,CASIA-Iris-Thousand(large-scale dataset). On the first\nrow of figure 6, It shows that at about 200 epochs the ac-\ncuracy growth rate is not increasing significantly and grad-\nually tends to overfit on the training set of CASIA-Iris-\nIntervalV3, the accuracy of the testing set increases slowly\nafter 225 epochs. On the CASIA-Iris-Lamp dataset, It can\nbe seen overfitting when both the training set and test ma-\nchine curves are around 150 epochs. On the second row\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nFIGURE 5: The speed and highest accuracy of the two proposed architectures (Original Arch and Proposed SwinIris Arch ) on\neach database.\nof figure 6, it can be seen that at about 175 epochs, the\naccuracy growth rate is not increasing significantly and tend\nto be more smoothly on the CASIA-IntervalV4 dataset. On\nthe CASIA-Iris-Thousand dataset(right side on the second\nrow), the accuracy of the training set increases slowly af-\nter 800 epochs, the accuracy of testing set grouth slowly\nat around 700 epochs.The performance of the proposed\nmethod is analyzed by using sensitivity and specificity\nare shown in Table 7. For CASIA-Iris-Thousand,CASIA-\nIntervalV3,CASIAIntervalV4,CASIA-Iris-Lamp, the sensi-\ntivity varies from 90.44% to 99.11%. The specificity varies\nfrom 98.4% to 99.98%.\nTable 7: Comparison of the sensitivity and specificity with\neach dataset\nDataset Name Testing\nSensitivity% Specificity %\nCASIA-Iris-Thousand 99.01 98.4\nCASIA-Iris-IntervalV3 90.44 99.97\nCASIA-Iris-IntervalV4 92.78 99.98\nCASIA-Iris-Lamp 99.11 99.92\nThe image shows a set of four box plots in figure 7, two\nin the top row and two in the bottom row, each representing\nthe training and testing accuracy over epochs for different\ndatasets. The box plots are color-coded, with training accu-\nracy in orange and testing accuracy in purple.\nIn the top left, the box plot for CASIA-IntervalV3 displays\na high median training accuracy at approximately 0.97, with a\ncompact interquartile range (IQR), indicating low variability\nin the training accuracy. The testing accuracy box plot has a\nlower median near 0.85 and a wider IQR, suggesting more\nvariability in testing accuracy compared to training.\nMoving to the top right, the box plot for CASIA-Iris-Lamp\nshows an even higher median training accuracy, almost per-\nfect at 1.0, and an extremely narrow IQR, which demonstrates\nvery consistent training results. The testing accuracy has a\nmedian around 0.95 with a slightly larger IQR than training,\nyet it still indicates strong and less variable performance\nduring testing.\nOn the bottom left, the CASIA-IntervalV4 box plot for\ntraining accuracy has a median near 0.95 with a slightly larger\nIQR than the top left plot, suggesting a bit more variability in\ntraining accuracy. The testing accuracy has a median around\n0.8 and an even larger IQR, indicative of greater variability in\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nFIGURE 6: First row displays the proposed model accuracy and model loss with 300 epochs for CASIA-IntervalV3 dataset\nand CASIA-Iris-Lamp dataset, the left side of the second row displays the proposed model accuracy and model loss with 300\nepochs for CASIA-IntervalV4 dataset, the right side of this row shows the proposed model accuracy and model loss with 1000\nepochs for CASIA-Iris-Thousand dataset.\nFIGURE 7: Here are box plots for four datasets that trained and tested by the proposed model. The first row from left to right\nshows the box plots for CASIA-IntervalV3 and CASIA-Iris-Lamp, and the second row from left to right shows the training and\ntesting process box plots at CASIA-IntervalV4 and CASIA-Iris-Thousand, respectively.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nTable 8: Comparison of the proposed SwinIris model with previous approaches\nDatabase Authors Date Subjects Method Speed Accuracy%\nCASIA-Iris-Thousand\nLiu et al. [33] 2019 1000 F-CNN , F-capsule —— 83.1\nAlaslani et al. [34] 2018 60 AlexNet 6 0.06 seconds 98\nWang et al. [35] 2021 100 Base-Net 6 seconds 99.66\nHafner et al. [36] 2021 1000 DenseNet-201 —— 98.5\nJayanthi et al. [16] 2021 1000 MRCNN+Inception v2 —— 99.14\nPrasanth et al. [37] 2023 1000 CNN —— 99.23\nEbrahimpour [38] 2023 1000 MobileNetV2 —— 99.22\nSingh [6] 2022 1000 VGG16+CNN —— 96\nYang et al. [5] 2021 1000 DualSANet —— 99.64\nProposed 2023 1000 SwinIris 714 seconds 99.56\n(99.78)\nCASIA-Iris-IntervalV3\nDaugman [39] —— 249 —— —— 90.59\nJan et al. [40] 2020 121 —— —— 99\nMathias et al. [41] 2022 249 B-spline —— 94.60\nGarg [8] 2021 50 BPNN+GA —— 96.40\nJan et al. [42] 2021 249 NIR+CHT 430 seconds 99.3\nProposed 2023 249 SwinIris 96 seconds 95.14\n(97.22)\nCASIA-Iris-IntervalV4\nGupta et al. [43] 2020 373(L+R) HsIrisNetCNN —— 99.72\nGupta et al. [44] 2021 185 FrDIrisNet(CNN) —— 99.72\nHe et al. [45] 2017 88 Data-driven Gabor filter optimization —— 99.98\nSoni et al. [46] 2021 42 NASNet —— 100\nArora et al. [47] 2023 249 Ensemble Learning —— 87.24\nProposed 2023 249 SwinIris 90 seconds 95.83\n(97.74)\nCASIA-Iris-Lamp\nSun et al. [48] 2022 100 MCFFN —— 98.62\nLei et al. [49] 2022 200 AttentionMTL —— 92.81\nPetrov et al. [50] 2020 —— Daugman integro-differential operator —— 79\nSun [9] 2021 275 OCFON —— 98.50\nProposed 2023 411 SwinIris 579 seconds 99.17\n(99.63)\ntesting performance.\nLastly, the bottom right box plot for CASIA-Iris-Thousand\nshows a training accuracy median near 0.99 with a very\nsmall IQR, indicating consistent training performance. The\ntesting accuracy has a median approximately at 0.88 and a\nnoticeably larger IQR compared to its training counterpart,\nwhich suggests more variation in the testing phase.\nOverall, the plots suggest that each model tends to perform\nbetter on training data, as evidenced by the generally higher\nmedians and tighter IQRs for training accuracy across all\ndatasets. Testing accuracies are lower and more variable\nacross the datasets.\nThe test accuracies obtained will be utilized to compare\nour iris recognition system with previous approaches.For the\nproposed method, we put two accuracies for each dataset.\nThe values in parentheses are the dataset with non-iris re-\nmoved. Table 8 illustrates a comparative analysis of various\niris recognition techniques [2] on all datasets employed in\nthis study. On the CASIA-Iris-Thousand, We can observe\nthat the accuracy of these two methods is slightly better\nthan the proposed architecture. However, Wang’s proposed\narchitecture achieved an accuracy of 99.66% on only 100\nsamples, whereas the framework presented in this paper\nachieved this accuracy on 1000 samples. That is, it is ten\ntimes the sample size, with only a minor decrease in accu-\nracy to 99.56%. Yang’s [5] architecture does apply to the\nentire dataset, but our architecture has a higher accuracy on\nthe CASIA-iris-Thousand dataset of Removed non-iris. On\nthe CASIA-Iris-IntervalV3 dataset, Jan’s [40] [42] methods\ngot good performance on 121 subjects and 249 subjects,\nrespectively. However, our model is much faster than previous\nmodels while ensuring high accuracy. On the CASIA-Iris-\nIntervalV4 dataset, Gupta [43] supposed the left and right iris\nare considered as different classes, thus, there are 373 subjects\nfor the experiments. Despite our accuracy being slightly lower\nthan the previous architecture, we have a larger number of\nexperimental subjects included. This serves as evidence of\nthe stability of our architecture. Arora et al. [47]’s ensemble\nlearning model in 2023 got lower accuracy than the proposed\nSwinIris model on 249 subjects. Furthermore, our model\ndemonstrates superior performance on the CASIA-Iris-Lamp\ndataset. Among these methods, the architecture we proposed\nhas achieved the highest of accuracy and has been tested on\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\na larger dataset(411 subjects). The longer processing time\nfor SwinIris compared to other methods could be due to its\nmore complex architecture and deeper layers, which, while\nresulting in slightly higher accuracy, also lead to increased\ncomputational demands and consequently longer processing\ntimes. Moreover, in the context of the CASIA-Iris-Thousand\ndataset, while the methodologies of Alaslani et al. and Wang\net al. demonstrate faster performance, it is crucial to note\nthat their experiments were conducted on a significantly\nsmaller subset of the dataset. In contrast, the SwinIris model\nis applied to the entire dataset, maintaining high accuracy\nand exemplifying model stability, albeit at the expense of\nincreased processing time.\nThe outcomes demonstrate that the proposed model is\nrobust, given that the accuracy rates range from 95.14% to\n99.56% across all datasets, thereby indicating its competence\nin various settings and environments.\nC. LIMITATION\nIn the spirit of comprehensive analysis and transparency, it\nis imperative to acknowledge the limitations encountered in\nour study, particularly in the context of the state of iris image\nprocessing. We found some processed images which were\nfalsely detected or segmented. Here are examples of four\nfalse-detected images shown below(Figure 8 and 9).\nFIGURE 8: Examples of false-detected iris images\nIn the process of detecting eyes, the eye images of some\nsamples do not catch the eyeballs but randomly generate\nbounding boxes or circles. In addition, it can be seen that\nthe pupil is not centrally located within the false-segmented\nimage, and there is a partial loss of information from the\niris, which could impede the subsequent feature extraction\nprocess. These could ultimately affect the accuracy of iris\nrecognition systems.\nFIGURE 9: Examples of false-segmented iris images\nVI. CONCLUSION\nWe have displayed our proposed iris recognition system based\non Swin Transformer architecture designed and developed to\nautomatically classify multi-classes with a great performance\nof accuracy from 95% to 99%. There are some advantages\nof the proposed method. a) Advanced Architectural Design:\nthe proposed SwinIris Transformer-based biometrics match-\ning System is based on the Swin transformer that was suc-\ncessfully redesigned, adjusted and tuned for iris recognition,\ni.e. a completely different process than the one it was origi-\nnally designed for. Our proposed approach is demonstrating\ncompetitive accuracy ranging from 95.14% to 99.56% across\nvarious datasets (including CASIA-Iris-Thousand, CASIA-\nIris-Lamp, and others). This performance is a testament to\nthe advanced architectural design and the effective utilization\nof deep learning in biometric recognition.b) Comprehensive\nValidation:The model underwent extensive validation across\nmultiple datasets under varying conditions, demonstrating\nrobustness and generalizability. While not perfect, the high\naccuracy rates across diverse settings signify the model’s\npracticality and reliability in real-world applications. c) Speed\nimprovement: fine-tuning the pre-trained Swin Transformer\nmodel achieves higher accuracy, improves the convergence\nspeed, and reduces the annotation effort in comparison to\ntraining with randomly initialized weights from scratch.\nExperimental results show that the accuracy level of\nthe two architectures are apparently different. The SwinIris\nmodel outperforms the original Swin Transformer model in\nterms of the metrics that we used as well as the training\nand testing time. Moreover, the proposed approach shows an\nenhancement in the accuracy compared to the state-of-art in\nterms of either speed, using larger subjects set in different\nenvironments, or studying various iris recognition systems.\nNevertheless, in future work, it is advisable to update and\nenhance the iris detection and iris segmentation techniques,\nas the techniques employed in this paper do not perform\noptimally on large-scale datasets. Nevertheless, it should be\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\nnoted that the iris detection and segmentation processes in this\npaper are automated, saving a considerable amount of time.\nFurthermore, the proportion of iris images with suboptimal\nsegmentation is minimal. Additionally, in future endeavors,\nthere is a need for more neural network models and architec-\ntures, such as the Interimage network.\nAlso, we will conduct a comprehensive evaluation of\nSwinIRIS, focusing on its robustness to various image degra-\ndation factors. This will include assessing the impact of dif-\nferent image resolutions, the presence of additive noise, and\nvariations in training set size on the overall performance of\nthe SwinIRIS system. By doing so, we aim to understand\nand improve the model’s resilience and reliability under di-\nverse and challenging conditions. Monitoring the subsequent\ndevelopment of the model is an important step in the study\n[51], We intend to rigorously monitor and evaluate our model,\nparticularly under diverse and challenging conditions when\nnew data is introduced. This proactive approach aims to en-\nsure robustness and accuracy. Notably, the SwinIris architec-\nture presented demonstrates significant potential, exhibiting\nadaptability across various datasets and conditions. Future\nwork will include a comprehensive evaluation of SwinIRIS,\nfocusing on its resilience to a spectrum of image degradation\nfactors, thereby enhancing the model’s reliability under di-\nverse scenarios. We acknowledge that no system is without\nits imperfections, the proposed SwinIris Transformer-based\nIris Recognition System demonstrates substantial promise\nand effectiveness. We are committed to ongoing research and\ndevelopment to further enhance its performance.\nREFERENCES\n[1] M. Edwards, D. Cha, S. Krithika, M. Johnson, and E. J. Parra, ‘‘Analysis\nof iris surface features in populations of diverse ancestry,’’ Royal Society\nopen science, vol. 3, no. 1, p. 150424, 2016.\n[2] O. M. Moslhi, ‘‘New full iris recognition system and iris segmentation\ntechnique using image processing and deep convolutional neural network,’’\n2020.\n[3] S. Sheela and P. Vijaya, ‘‘Iris recognition methods-survey,’’ International\nJournal of Computer Applications , vol. 3, no. 5, pp. 19–25, 2010.\n[4] T. Balashanmugam, K. Sengottaiyan, M. S. Kulandairaj, and H. Dang, ‘‘An\neffective model for the iris regional characteristics and classification using\ndeep learning alex network,’’ IET Image Processing , vol. 17, no. 1, pp.\n227–238, 2023.\n[5] K. Yang, Z. Xu, and J. Fei, ‘‘Dualsanet: Dual spatial attention network for\niris recognition,’’ in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision , 2021, pp. 889–897.\n[6] A. Singh, A. Pandey, M. Rakhra, D. Singh, G. Singh, and O. Dahiya,\n‘‘An iris recognition system using cnn & vgg16 technique,’’ in 2022\n10th International Conference on Reliability, Infocom Technologies and\nOptimization (Trends and Future Directions)(ICRITO) . IEEE, 2022, pp.\n1–6.\n[7] B. K. O. C. Alwawi and A. F. Y . Althabhawee, ‘‘Towards more accurate and\nefficient human iris recognition model using deep learning technology,’’\nTELKOMNIKA (Telecommunication Computing Electronics and Control) ,\nvol. 20, no. 4, pp. 817–824, 2022.\n[8] M. Garg, A. Arora, and S. Gupta, ‘‘An efficient human identification\nthrough iris recognition system,’’ Journal of Signal Processing Systems ,\nvol. 93, pp. 701–708, 2021.\n[9] J. Sun, S. Zhao, S. Miao, X. Wang, and Y . Yu, ‘‘Open-set iris recognition\nbased on deep learning,’’ IET Image Processing , vol. 16, no. 9, pp. 2361–\n2372, 2022.\n[10] D. Balashanmugam, K. Sengottaiyan, M. Kulandairaj, and D. Hien, ‘‘An\neffective model for the iris regional characteristics and classification using\ndeep learning alex network,’’ IET Image Processing , vol. 17, pp. n/a–n/a,\n09 2022.\n[11] Y . hui Li, P.-J. Huang, and Y . Juan, ‘‘An efficient and robust iris seg-\nmentation algorithm using deep learning,’’ Mob. Inf. Syst. , vol. 2019, pp.\n4 568 929:1–4 568 929:14, 2019.\n[12] E. Ali, H. jaber, and N. Kadhim, ‘‘New algorithm for localization of iris\nrecognition using deep learning neural networks,’’ Indonesian Journal of\nElectrical Engineering and Computer Science , vol. 29, p. 110, 01 2022.\n[13] R. Alinia Lat, S. Danishvar, H. Heravi, and M. Danishvar, ‘‘Boosting iris\nrecognition by margin-based loss functions,’’ Algorithms, vol. 15, no. 4, p.\n118, 2022.\n[14] T. Zhao, Y . Liu, G. Huo, and X. Zhu, ‘‘A deep learning iris recognition\nmethod based on capsule network architecture,’’ IEEE Access , vol. 7, pp.\n49 691–49 701, 2019.\n[15] J. R. Malgheet, N. B. Manshor, L. S. Affendey, and A. B. Abdul Halin, ‘‘Iris\nrecognition development techniques: a comprehensive review,’’ Complex-\nity, vol. 2021, pp. 1–32, 2021.\n[16] J. Jayanthi, E. L. Lydia, N. Krishnaraj, T. Jayasankar, R. L. Babu, and\nR. A. Suji, ‘‘An effective deep learning features based integrated framework\nfor iris detection and recognition,’’ Journal of ambient intelligence and\nhumanized computing, vol. 12, pp. 3271–3281, 2021.\n[17] K. He, G. Gkioxari, P. Dollár, and R. Girshick, ‘‘Mask r-cnn,’’ 2018.\n[18] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, ‘‘Rethinking\nthe inception architecture for computer vision,’’ 2015.\n[19] K. Zhao, Q. Han, C.-B. Zhang, J. Xu, and M.-M. Cheng, ‘‘Deep hough\ntransform for semantic line detection,’’ IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , vol. 44, no. 9, pp. 4793–4806, 2021.\n[20] A. Lapušinskij, I. Suzdalev, N. Goranin, J. Janulevičius, S. Ramanauskait ˙e,\nand G. Stank ¯unavičius, ‘‘The application of hough transform and canny\nedge detector methods for the visual detection of cumuliform clouds,’’\nSensors, vol. 21, no. 17, p. 5821, 2021.\n[21] C. Romanengo, S. Biasotti, and B. Falcidieno, ‘‘Hough transform for\ndetecting space curves in digital 3d models,’’ Journal of Mathematical\nImaging and Vision , vol. 64, no. 3, pp. 284–297, 2022.\n[22] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, ‘‘Swin\ntransformer: Hierarchical vision transformer using shifted windows,’’ in\nProceedings of the IEEE/CVF international conference on computer vi-\nsion, 2021, pp. 10 012–10 022.\n[23] J. Lu, W. Wang, K. Zhao, and H. Wang, ‘‘Recognition and segmentation\nof individual pigs based on swin transformer,’’ Animal Genetics , vol. 53,\nno. 6, pp. 794–802, 2022.\n[24] H. Zheng, G. Wang, and X. Li, ‘‘Swin-mlp: a strawberry appearance quality\nidentification method by swin transformer and multi-layer perceptron,’’\nJournal of Food Measurement and Characterization , vol. 16, pp. 1–12, 08\n2022.\n[25] Y . Jiang, Y . Zhang, X. Lin, J. Dong, T. Cheng, and J. Liang, ‘‘Swinbts:\nA method for 3d multimodal brain tumor segmentation using swin\ntransformer,’’ Brain Sciences , vol. 12, no. 6, 2022. [Online]. Available:\nhttps://www.mdpi.com/2076-3425/12/6/797\n[26] J.-H. Kim, N. Kim, and C. S. Won, ‘‘Facial expression recognition with\nswin transformer,’’ arXiv preprint arXiv:2203.13472 , 2022.\n[27] A. Cosma, A. Catruna, and E. Radoi, ‘‘Exploring self-supervised vision\ntransformers for gait recognition in the wild,’’ Sensors, vol. 23, no. 5,\n2023. [Online]. Available: https://www.mdpi.com/1424-8220/23/5/2680\n[28] S. A. Grosz, J. J. Engelsma, R. Ranjan, N. Ramakrishnan, M. Aggarwal,\nG. G. Medioni, and A. K. Jain, ‘‘Minutiae-guided fingerprint embeddings\nvia vision transformers,’’ arXiv preprint arXiv:2210.13994 , 2022.\n[29] A. Kasinski and A. Schmidt, ‘‘The architecture of the face and eyes\ndetection system based on cascade classifiers,’’ in Computer Recognition\nSystems 2. Springer, 2007, pp. 124–131.\n[30] S. Umer, B. C. Dhara, and B. Chanda, ‘‘Iris recognition using multiscale\nmorphologic features,’’ Pattern Recognition Letters , vol. 65, pp. 67–74,\n2015.\n[31] S. Suzuki et al., ‘‘Topological structural analysis of digitized binary images\nby border following,’’ Computer vision, graphics, and image processing ,\nvol. 30, no. 1, pp. 32–46, 1985.\n[32] L. Omelina, J. Goga, J. Pavlovicova, M. Oravec, and B. Jansen, ‘‘A survey\nof iris datasets,’’ Image and Vision Computing , vol. 108, p. 104109, 2021.\n[33] M. Liu, Z. Zhou, P. Shang, and D. Xu, ‘‘Fuzzified image enhancement for\ndeep learning in iris recognition,’’ IEEE Transactions on Fuzzy Systems ,\nvol. 28, no. 1, pp. 92–99, 2019.\n[34] M. G. Alaslani, ‘‘Convolutional neural network based feature extraction for\niris recognition,’’ International Journal of Computer Science & Informa-\ntion Technology (IJCSIT) Vol , vol. 10, 2018.\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRunqing Gaoet al.: On Designing a SwinIris Transformer based Iris Recognition System\n[35] Q. Wang, X. Meng, T. Sun, and X. Zhang, ‘‘A light iris segmentation\nnetwork,’’The Visual Computer , pp. 1–11, 2021.\n[36] A. Hafner, P. Peer, Ž. Emeršič, and M. Vitek, ‘‘Deep iris feature extraction,’’\nin 2021 International Conference on Artificial Intelligence in Information\nand Communication (ICAIIC) . IEEE, 2021, pp. 258–262.\n[37] N. Prasanth, S. Nethi, N. Srinivasu, C. U. S. Kiran, T. Ketineni, and\nG. Pradeepini, ‘‘Fusion of iris and periocular biometrics authentication\nusing cnn,’’ in 2023 7th International Conference on Computing Method-\nologies and Communication (ICCMC) . IEEE, 2023, pp. 378–382.\n[38] N. EBRAHIMPOUR, ‘‘Iris recognition using mobilenet for biometric\nauthentication,’’ in 9th INTERNATIONAL ZEUGMA CONFERENCE ON\nSCIENTIFIC RESEARCH. Gaziantep, Turkey , 2023, pp. 583–588.\n[39] J. G. Daugman, ‘‘High confidence visual recognition of persons by a test\nof statistical independence,’’ IEEE transactions on pattern analysis and\nmachine intelligence, vol. 15, no. 11, pp. 1148–1161, 1993.\n[40] F. Jan and N. Min-Allah, ‘‘An effective iris segmentation scheme for noisy\nimages,’’ Biocybernetics and Biomedical Engineering , vol. 40, no. 3, pp.\n1064–1080, 2020.\n[41] G. P. Mathias, J. Gagan, B. V . Mallya, and J. H. Kumar, ‘‘A unified\napproach for automated segmentation of pupil and iris in on-axis im-\nages,’’ Computer Methods and Programs in Biomedicine Update , vol. 2,\np. 100084, 2022.\n[42] F. Jan, N. Min-Allah, S. Agha, I. Usman, and I. Khan, ‘‘A robust iris\nlocalization scheme for the iris recognition,’’ Multimedia Tools and Ap-\nplications, vol. 80, pp. 4579–4605, 2021.\n[43] R. Gupta and P. Sehgal, ‘‘Hsirisnet: Histogram based iris recognition to\nallay replay and template attack using deep learning perspective,’’ Pattern\nRecognition and Image Analysis , vol. 30, pp. 786–794, 2020.\n[44] ——, ‘‘Iris recognition using selective feature set in frequency domain\nusing deep learning perspective: Frdirisnet,’’ in Cybernetics, Cognition\nand Machine Learning Applications: Proceedings of ICCCMLA 2020 .\nSpringer, 2021, pp. 249–259.\n[45] F. He, Y . Han, H. Wang, J. Ji, Y . Liu, and Z. Ma, ‘‘Deep learning architecture\nfor iris recognition based on optimal gabor filters and deep belief network,’’\nJournal of Electronic Imaging , vol. 26, no. 2, pp. 023 005–023 005, 2017.\n[46] A. Soni, T. Patidar, R. Kumar, K. Bharath, S. Balaji, and R. Rajendran,\n‘‘Iris recognition using hough transform and neural architecture search\nnetwork,’’ in 2021 Innovations in Power and Advanced Computing Tech-\nnologies (i-PACT). IEEE, 2021, pp. 1–5.\n[47] A. Arora, A. Gupta, B. Jindal, and G. Gupta, ‘‘Smart iris classification\nusing weighted average ensemble learning,’’ in 2023 International Con-\nference on Disruptive Technologies (ICDT) . IEEE, 2023, pp. 124–130.\n[48] J. Sun, S. Zhao, Y . Yu, X. Wang, and L. Zhou, ‘‘Iris recognition based\non local circular gabor filters and multi-scale convolution feature fusion\nnetwork,’’Multimedia Tools and Applications , vol. 81, no. 23, pp. 33 051–\n33 065, 2022.\n[49] S. Lei, B. Dong, A. Shan, Y . Li, W. Zhang, and F. Xiao, ‘‘Attention meta-\ntransfer learning approach for few-shot iris recognition,’’ Computers and\nElectrical Engineering, vol. 99, p. 107848, 2022.\n[50] I. Petrov and N. Minakova, ‘‘Optimization method for non-cooperative iris\nrecognition task using daugman integro-differential operator,’’ in Journal\nof Physics: Conference Series , vol. 1615, no. 1. IOP Publishing, 2020, p.\n012007.\n[51] Y . Akkem, S. K. Biswas, and A. Varanasi, ‘‘Smart farming monitoring us-\ning ml and mlops,’’ in International Conference On Innovative Computing\nAnd Communication. Springer, 2023, pp. 665–675.\nRUNQING GAO complemented her B.S. in In-\nformation Management and Information Systems\nin China and M.S. at the University of Georgia\n(UGA). She is currently pursuing her Ph.D. in En-\ngineering with the School of Electrical and Com-\nputer Engineering at the University of Georgia\nunder the supervision of Dr. Thirimachos Bourlai\nin the Multispectral Imagery Lab (MILAB).\nTHIRIMACHOS BOURLAI (Senior Member,\nIEEE) is an Associate Professor in the School\nof Electrical and Computer Engineering and an\nAdjunct Faculty at the Institute for Cybersecurity\nand Privacy, both at the University of Georgia.\nHe also serves as an Adjunct Faculty at WVU in\nthe Lane Department of Computer Science and\nEngineering and in the School of Medicine. He\nis the founder and director of the Multispectral\nImagery Lab, a Series Editor of the Advanced\nSciences and Technologies for Security Applications, an Associate Editor of\nthe Elsevier Pattern Recognition Letters Journal and of the IET Electronics\nLetters Journal. He is also a Senior Member of IEEE, a member of the Board\nof Directors at the Document Security Alliance, the VP on Education of\nthe IEEE Biometrics Council (2019-2021), and a member of the Academic\nResearch and Innovation Expert Group of the Biometrics Institute. He has\npublished 4 books \"Face Recognition Across the Imaging Spectrum, 2016\",\n\"Surveillance in Action, 2018\", and \"Securing Social Identity in Mobile Plat-\nforms, 2020\", “Disease Control through Social Network Surveillance, 2022”,\nhas 3 patents, over 120 journals, conference papers, and book chapters.\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3369035\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}