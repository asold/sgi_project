{
    "title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization",
    "url": "https://openalex.org/W3176770275",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2808171854",
            "name": "Xiachong Feng",
            "affiliations": [
                "Peng Cheng Laboratory",
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2224392042",
            "name": "Xiaocheng Feng",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2133020571",
            "name": "Libo Qin",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098930276",
            "name": "Bing Qin",
            "affiliations": [
                "Peng Cheng Laboratory",
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098738246",
            "name": "Ting Liu",
            "affiliations": [
                "Harbin Institute of Technology",
                "Peng Cheng Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2125336414",
        "https://openalex.org/W2949807892",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4287636206",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W3117373959",
        "https://openalex.org/W3100560913",
        "https://openalex.org/W3103417625",
        "https://openalex.org/W3116705004",
        "https://openalex.org/W2014571624",
        "https://openalex.org/W3102999298",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2996403597",
        "https://openalex.org/W3008323921",
        "https://openalex.org/W3104257895",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3094217371",
        "https://openalex.org/W3010293452",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2989743967",
        "https://openalex.org/W1525595230",
        "https://openalex.org/W2080922525",
        "https://openalex.org/W2889984458",
        "https://openalex.org/W3085629518",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2011950632",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3093871960",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2971167298",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2139414852",
        "https://openalex.org/W2962704246",
        "https://openalex.org/W2574535369",
        "https://openalex.org/W2949530332",
        "https://openalex.org/W2970419734",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W1626945812",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W3116498179",
        "https://openalex.org/W3188385856",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W3120948048",
        "https://openalex.org/W3118364895",
        "https://openalex.org/W1831406492",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4302285968"
    ],
    "abstract": "Xiachong Feng, Xiaocheng Feng, Libo Qin, Bing Qin, Ting Liu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1479â€“1491\nAugust 1â€“6, 2021. Â©2021 Association for Computational Linguistics\n1479\nLanguage Model as an Annotator: Exploring DialoGPT\nfor Dialogue Summarization\nXiachong Feng1, Xiaocheng Feng1,2âˆ—, Libo Qin1, Bing Qin1,2, Ting Liu1,2\n1Harbin Institute of Technology, China\n2Peng Cheng Laboratory, China\n{xiachongfeng,xcfeng,lbqin,bqin,tliu}@ir.hit.edu.cn\nAbstract\nCurrent dialogue summarization systems usu-\nally encode the text with a number of gen-\neral semantic features (e.g., keywords and top-\nics) to gain more powerful dialogue modeling\ncapabilities. However, these features are ob-\ntained via open-domain toolkits that are dialog-\nagnostic or heavily relied on human annota-\ntions. In this paper, we show how DialoGPT\n(Zhang et al., 2020b), a pre-trained model for\nconversational response generation, can be de-\nveloped as an unsupervised dialogue annotator,\nwhich takes advantage of dialogue background\nknowledge encoded in DialoGPT. We apply\nDialoGPT to label three types of features on\ntwo dialogue summarization datasets, SAM-\nSum and AMI, and employ pre-trained and\nnon pre-trained models as our summarizers.\nExperimental results show that our proposed\nmethod can obtain remarkable improvements\non both datasets and achieves new state-of-the-\nart performance on the SAMSum dataset1.\n1 Introduction\nDialogue summarization aims to generate a suc-\ncinct summary while retaining essential informa-\ntion of the dialogue (Gurevych and Strube, 2004;\nChen and Yang, 2020). Theoretically, Peyrard\n(2019) point out that a good summary is intuitively\nrelated to three aspects, including Informativeness,\nRedundancy and Relevance.\nTo this end, previous works have taken the above\nthree aspects into account by incorporating auxil-\niary annotations into the dialogue. To improve\ninformativeness, some works annotated linguisti-\ncally speciï¬c words (e.g., nouns and verbs), do-\nmain terminologies and topic words in the dialogue\n(Riedhammer et al., 2008; Koay et al., 2020; Zhao\net al., 2020). To reduce redundancy, some works\nâˆ—Corresponding author.\n1Our codes are available at: https://github.com/\nxcfcode/PLM_annotator\nused sentence similarity-based methods to anno-\ntate redundant utterances. (Zechner, 2002; Murray\net al., 2005). To improve relevance, some works\nannotated topics for the dialogue (Li et al., 2019;\nLiu et al., 2019; Chen and Yang, 2020). How-\never, these annotations are usually obtained via\nopen-domain toolkits, which are not suitable for\ndialogues, or require manual annotations, which\nare labor-consuming.\nTo alleviate the above problem, we explore the\npre-trained language model as an unsupervised an-\nnotator to automatically provide annotations for the\ndialogue. Recently, some works have investigated\nthe use of pre-trained language models in an unsu-\npervised manner. For example, Sainz and Rigau\n(2021) exploited pre-trained models for assigning\ndomain labels to WordNet synsets. The successful\nrecipe is that a model is obtained extensive knowl-\nedge via pre-training on a huge volume of data.\nWhen it comes to the dialogue domain, DialoGPT\n(Zhang et al., 2020b) is a SOTA conversational\nresponse generation model, which is pre-trained\non the massive dialogue data. Therefore, we draw\nsupport from DialoGPT and present our DialoGPT\nannotator, which can perform three dialogue anno-\ntation tasks, including keywords extraction, redun-\ndancy detection and topic segmentation, to measure\ninformativeness, redundancy and relevance of the\ninput dialogue, respectively.\nKeywords Extraction aims to automatically\nidentify important words in the dialogue (shown\nin Figure 1(a)). Our DialoGPT annotator extracts\nunpredictable words as keywords. We assume that\nkeywords contain high information, which are dif-\nï¬cult to be predicted considering both background\nknowledge encoded in the DialoGPT and contex-\ntual information of dialogue context. Redundancy\nDetection aims to detect redundant utterances that\nhave no core contribution to the overall meaning of\nthe dialogue (shown in Figure 1(b)). OurDialoGPT\n1480\nRemember we are seeing the wedding planner after workSure, where are we meeting her? At NonnaRitaâ€™s I want to order seafood tagliatelle Hahawhy notWe remmberspaghetti pomodoro disaster from our last meetingOmg it was over her white blouseI'll make timefor itGreat!\nBlair:Chuck:Blair:Chuck: Blair:Chuck:Blair:Chuck:Blair: [Topic3][Topic2]\n[Topic1]\nDialogueRemember we are seeing the wedding planner after workSure, where are we meeting her? At NonnaRitaâ€™s I want to order seafood tagliatelle Hahawhy notWe remmberspaghetti pomodoro disaster from our last meetingOmg it was over her white blouseI'll make timefor itGreat!\nBlair:Chuck:Blair:Chuck: Blair:Chuck:Blair:Chuck:Blair:\nDialogueRemember we are seeing the wedding planner after workSure, where are we meeting her? At NonnaRitaâ€™s I want to order seafood tagliatelleHahawhy notWe remmberspaghetti pomodoro disasterfrom our last meetingOmg it was over her white blouseI'll make timefor itGreat!\nBlair and Chuck are going to meet the wedding plannerafter work at NonnaRitaâ€™s. The tagliatelleserved at NonnaRitaâ€™s are very good. \nBlair:Chuck:Blair:Chuck: Blair:Chuck:Blair:Chuck:Blair:\nDialogue\nSummary[Topic1] [Topic2]\n(c) Topic Segmentation (b) Redundancy Detection (a) Keywords Extraction \nFigure 1: Example dialogue from SAMSum (Gliwa et al., 2019) with the human annotated summary. (a) Keywords\nextraction aims to extract words that are most important to the dialogue. (b) Redundancy detection aims to detect\nnonsigniï¬cant utterances in the dialogue. (c) Topic segmentation aims to divide the whole dialogue into several\nï¬ne-grained topics. All three auxiliary information can do good to ï¬nal summary generation.\nannotator detects utterances that are useless for di-\nalogue context representation as redundant. We\nassume that if adding a new utterance does not\nchange the dialogue context representation, then\nthis utterance has no effect on predicting the re-\nsponse, so it is redundant. Topic Segmentation\naims to divide a dialogue into topically coherent\nsegments (shown in Figure 1(c)). Our DialoGPT\nannotator inserts a topic segmentation point before\none utterance if it is unpredictable. We assume that\nif an utterance is difï¬cult to be inferred from the\ndialogue context based on DialoGPT, this utterance\nmay belong to a new topic.\nWe use our DialoGPT annotator to annotate the\nSAMSum (Gliwa et al., 2019) and AMI (Carletta\net al., 2005) datasets. Each annotation is converted\ninto a speciï¬c identiï¬er and we insert them into the\ndialogue text. Then, we employ pre-traind BART\n(Lewis et al., 2020) and non pre-trained PGN (See\net al., 2017) as our summarizers. Extensive experi-\nmental results show that our method can obtain con-\nsistent and remarkable improvements over strong\nbaselines on both datasets and achieves new state-\nof-the-art performance on the SAMSum dataset.\n2 Preliminaries\nIn this section, we will describe the task deï¬nition\nas well as the background of DialoGPT.\n2.1 Task Deï¬nition\nGiven an input dialogue D, a dialogue summa-\nrizer aims to produce a condensed summary S,\nwhere Dconsists of |D|utterances [u1,u2,...u|D|]\nand Sconsists of |S|words [s1,s2,...s|S|]. Each\nutterance ui is compose of a sequence of words\n[ui,1,ui,2,...ui,|ui|,EOSi], where i âˆˆ [1 : |D|]\nand EOSi indicates the end of the utterance. Be-\nsides, each utterance ui associates with a speaker\npi. Thus, this task can be formalized as producing\nthe summary Sgiven the dialogue sequence: D=\n[p1,u1,1,..., EOS1,...,p |D|,u|D|,1,..., EOS|D|]\n2.2 DialoGPT\nDialoGPT (Zhang et al., 2020b) is a neural con-\nversational response generation model, which in-\nherits from GPT-2 (Radford et al., 2019) and is\ntrained on 147M conversation-like exchanges ex-\ntracted from Reddit comment chains. There are\n3 different sizes of the model with total parame-\nters of 117M, 345M and 762M respectively. It\nachieves state-of-the-art results over various dia-\nlogue generation benchmarks. Given the dialogue\ncontext uiâˆ’1 = [uiâˆ’1,1,...,u iâˆ’1,|uiâˆ’1|,EOSiâˆ’1],\nDialoGPT aims to produce the response ui =\n[ui,1,...,u i,|ui|,EOSi], which can be formalized as\nthe conditional probability of P(ui|uiâˆ’1). It ï¬rst\ntakes the context word sequence of no more than\n1024 tokens and outputs the representation of the se-\nquence hi = (hiâˆ’1,1,..., hiâˆ’1,|uiâˆ’1|,hiâˆ’1,EOSiâˆ’1 ),\nwhere hiâˆ’1,EOSiâˆ’1 can be viewed as the repre-\nsentation of dialogue context uiâˆ’1. Then, Di-\naloGPT starts decoding the response by attend-\ning to the context token representations and par-\ntially decoded response tokens until reaching EOS.\nThe loss function is the negative log-likelihood\nof the response word sequence LDialoGPT =\nâˆ’âˆ‘|ui|\nt=1 log p(ui,t|ui,1 ...u i,tâˆ’1,uiâˆ’1). Itâ€™s worth\nnoting that DialoGPT tokenizes texts with the same\nbyte-pair encoding as GPT-2, thus either context or\nresponse tokens are tokenized into subwords.\n1481\nContext:yooguys.EOS1Response: hey wassup. EOS2Context:hey wassup. EOS2Response: Remmberthemeeting EOS3Context:Remmberthemeeting EOS3Response: Ialmostforgetit. EOS4Context: Ialmostforgetit. EOS4Response: fine EOS5Context: fine EOS5Response: Where? EOS6Context: Where? EOS6Response: atBarbara's place. EOS7\nTom: yooguys. EOS1John: hey wassup. EOS2Tom: RemmberthemeetingEOS3John: Ialmostforgetit. EOS4Tom:fine EOS5John: Where? EOS6Tom: atBarbara's place. EOS7\n(a) Context-response Pairs\n(b) Dialogue Sequence\nDialoGPTyooguys.EOS1hey wassup. EOS2Remmberthemeeting EOS3Ialmostforgetit. EOS4fine EOS5Where? EOS6atBarbara's place. EOS7\nOriginal Dialogue\nyooguys.EOS1...at Barbara's place. EOS7\n...\nDialoGPThey\nRemmber\nwassup.\nthemeeting\nEOS2\nEOS3\nRemmberthemeeting\nGolden:loss31\n(d) Dialogue ContextRepresentation\n(c) Word-level and Utterance-level Loss\nPrediction:\nğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ\n(g) Redundancy Detection \n0.7110.9980.9910.6420.5730.993\n(e) Keywords Extraction \n(f) Topic Segmentation \nloss3\nSegmentationPoint \nloss32loss33loss34 ... Extracted Keywords\nTom: yooguys. EOS1John: [RD] hey wassup. EOS2[TS]Tom: Remmberthemeeting EOS3John: [RD] Ialmostforgetit. EOS4Tom:[RD] fineEOS5[TS]John: Where? EOS6Tom: atBarbara's place. EOS7#KEY# Tom John meetingBarbara's\nLabelled Dialogue\n(â… ) Dialogue Preprocessing(â…¢) Annotation(â…¡) DialoGPTForward Passing\nAvg\nloss2loss3loss4loss5loss6loss7ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ•\nSegmentationPoint \nğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ•ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸğ’‰ğ‘¬ğ‘¶ğ‘ºğŸğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ‘ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ’ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ“ğ’‰ğ‘¬ğ‘¶ğ‘ºğŸ”\nloss31loss32loss33loss34...loss71loss72loss73loss34\nDialoGPTAnnotator\nFigure 2: Illustration of our DialoGPT annotator. (I) Given one dialogue, we preprocess it into two formats:\ncontext-response pairs and the dialogue sequence. (II) We input them into the DialoGPT, after the forward pass,\nwe can get the word-level and utterance-level predicted losses and representations for dialogue context. (III) We\nperform three annotation tasks: keywords extraction, redundancy detection and topic segmentation. Finally, we\ncan get a labelled dialogue. #KEY#, [RD] and [TS] are speciï¬c tags, which are inserted into the dialogue.\n3 Method\nIn this section, we will ï¬rst introduce our DialoGPT\nannotator. The workï¬‚ow consists of three steps\n(1) dialogue preprocessing; (2) DialoGPT forward\npassing; (3) annotation. The overall framework\nis shown in Figure 2. Then, we will describe our\ndialogue summarizer, including BART and PGN.\n3.1 Dialogue Preprocessing\nDialogue preprocessing aims to trans-\nform the original dialogue D =\n[p1,u1,1,..., EOS1,...,p |D|,u|D|,1,..., EOS|D|]\ninto the format that DialoGPT can process.\nSpeciï¬cally, we transform it into two formats.\nThe ï¬rst one is context-response pairs (shown in\nFigure 2(a)). Given a dialogue D, two adjacent\nutterances (uiâˆ’1,ui) are combined into a context-\nresponse pair, where iâˆˆ[2 :|D|] . The second one\nis dialogue sequence (shown in Figure 2(b)). All\nthe utterances in the dialogue Dare serialized into\na sequence [u1,1,..., EOS1,...,u |D|,1,..., EOS|D|],\nwith EOS separates each utterance.\nNote that either for context-response pairs or the\ndialogue sequence, we do not take speaker infor-\nmation p into consideration. The reason is that\nDialoGPT is trained on a huge volume of conver-\nsational data without speaker information. Even\nso, Zhang et al. (2020b) proved that DialoGPT can\nsimulate real-world dialogues in various scenes and\nhas already learned diverse response generation\npatterns between the same speakers or different\nspeakers according to the given context.\n3.2 DialoGPT Forward Passing\nDialoGPT forward passing has two purposes. (1)\nFor each context-response pair, we aim to get the\nword-level and utterance-level predicted losses for\nthe response (shown in Figure 2(c)). (2) For the di-\nalogue sequence, we aim to get the representations\nfor each EOS (shown in Figure 2(d)).\nFor the ï¬rst purpose, given one context-response\npair (uiâˆ’1,ui), we input the context words uiâˆ’1 =\n[uiâˆ’1,1,uiâˆ’1,2,...,u iâˆ’1,|uiâˆ’1|,EOSiâˆ’1] into the Di-\naloGPT and start to decode the response. At\neach decode step t, we calculate the negative log-\nlikelihood between the predicted distribution and\nthe golden target from the given response.\nlossi,t = âˆ’log p(ui,t|ui,<t,uiâˆ’1)\nlossi = 1\n|ui|+ 1\n|ui|+1âˆ‘\nt=1\nlossi,t\n(1)\nwhere lossi,t and lossi are the predicted losses for\neach word and each utterance respectively2.\nFor the second purpose, after the single forward\npass of DialoGPT over the dialogue sequence, we\ncan get representations H for each token on the\ntop of the DialoGPT. Afterward, we extract all\nrepresentations for each EOS.\nhEOS1 ,hEOS2 ,..., hEOS|D| = H (EOS) (2)\nwhere each hEOSi can be viewed as the representa-\ntion for the dialogue context [u1,...,u i].\n2Note that DialoGPT uses BPE to tokenize texts, thus,\nlosses are calculated at the sub-word level. We recover the\nword-level predicted loss by averaging the losses of multiple\nsub-words. Besides, since the ï¬rst utterance u1 can only be\nserved as the context, so we do not compute loss for u1.\n1482\nFigure 3: Illustration of redundancy detection process.\nThe initial redundant utterances set is âˆ…. hEOSi is the\nrepresentation for dialogue context covering the ï¬rst i\nutterances. We detect redundant utterances based on\nthe cosine similarity between representations of dia-\nlogue context. For example, the similarity score be-\ntween hEOS4 and hEOS5 exceeds the pre-deï¬ned thresh-\nold (tRD is 0.99), which means adding utteranceu5 into\nthe dialogue context brings little information, thus the\nutterance u5 is detected as redundant.\n3.3 Annotation\n3.3.1 Keywords Extraction: DialoGPT KE\nMotivation Considering both background knowl-\nedge encoded in the DialoGPT and contextual in-\nformation of the dialogue context, if one word in\nthe golden response is difï¬cult to be inferred from\nDialoGPT, we assume that it contains high infor-\nmation and can be viewed as a keyword.\nGiven a dialogue D, we have loss lossi,j for\neach word ui,j, where i âˆˆ[2 : |D|]. We extract\nrKE percent of words with the highest loss as key-\nwords, where rKE is a hyper-parameter 3. More-\nover, the names of all speakers P mentioned in\nthe dialogue are also added into the keywords set.\nFinally, we append a speciï¬c tag #KEY# and the\nkeywords to the end of the original dialogue D.\nThe new dialogue with keywords annotation is\nDKE = [p1,u1,1,...,î´™ î´˜î´— î´š\nD\n#KEY#,P,Key1,Key2,...î´™ î´˜î´— î´š\nkeywords\n].4\n3We use a heuristic rule to predetermine the possible value\nof rKE by calculating the average of length of summaries\n(remove stopwords) divided by the length of dialogues in the\ntrain set. We search the best rKE based on the calculated score.\n4In experiments, we ï¬nd that the predicted loss for the ï¬rst\nword of each utterance is extremely high, probably due to the\nï¬rst word in the response is the most uncertain and hard to be\npredicted. Thus, we ignore the ï¬rst word of each utterance.\n3.3.2 Redundancy Detection: DialoGPT RD\nMotivation DialoGPT inherits a decoder archi-\ntecture, where one token attends to all previous\ntokens to aggregate information. Thus, given\nthe representation hEOSi for each EOSi, it can be\nviewed as the representation for the dialogue con-\ntext [u1,u2,...,u i]. Adding a new utterance ui+1,\nif the new context representation hEOSi+1 is simi-\nlar to the previous hEOSi , we assume that the new\nutterance ui+1 brings little information and has\nsmall effects on predicting the response, thus ui+1\nbecomes a redundant utterance.\nWe start with the last two dialogue context repre-\nsentations hEOS|D|âˆ’1 and hEOS|D|, and calculate the\ncosine similarity between them. If the similarity\nscore exceeds the threshold tRD, the utterance u|D|\nis detected as redundant. tRD is a hyper-parameter.\nIf the similarity score doesnâ€™t exceed the threshold\ntRD, we move forward one step to calculate the\nsimilarity between hEOS|D|âˆ’2 and hEOS|D|âˆ’1 , and\nrepeat the process until reaching hEOS1 . An exam-\nple is shown in Figure 3.\nWe insert a speciï¬c tag [RD] before each\nredundant utterance. For example, if utter-\nance u1 is redundant, the new dialogue with\nredundant utterances annotation is DRD =\n[p1,[RD],u1,1,..., EOS1,...,p |D|,..., EOS|D|].\n3.3.3 Topic Segmentation: DialoGPT TS\nMotivation DialoGPT is skilled in generating the\ncontext-consistent response. Therefore, if the re-\nsponse is difï¬cult to be predicted given the context\nbased on DialoGPT, we assume the response may\nbelong to another topic and there is a topic segmen-\ntation between the context and response.\nGiven a dialogue D, we have loss lossi for each\nutterance ui, where i âˆˆ[2 : |D|]. We select rTS\npercent of utterances with the highest loss as topic\nsegmentation points. rTS is a hyper-parameter 5.\nBefore each selected utterance, we insert a speciï¬c\ntag [TS]. For example, if there is a segmenta-\ntion point between utterance u1 and utterance u2,\nthe new dialogue with topic annotation is DTS =\n[p1,u1,1,..., EOS1,[TS],p2,u2,1,..., EOS2,...].\n5We use a heuristic rule to predetermine the possible value\nof rTS by calculating the average of the number of summary\nsentences divided by the number of dialogue utterances in the\ntrain set. This is based on the observation that each sentence\nin golden summary tends to correspond to one topic of the\ndialogue. We search the bestrTS based on the calculated score.\n1483\n3.4 Summarizer\nWe employ two kinds of summarizer, one isBART\n(Lewis et al., 2020), which is a Transformer-based\nmodel and pre-trained on a huge volume of data.\nThe other one is PGN (See et al., 2017), which is\na LSTM-based model. Both models inherit a typi-\ncal sequence-to-sequence framework, which ï¬rst\nencodes the source dialogue Dto distributed repre-\nsentations and then generates the target summary\nSwith the decoder.\nBART BART adopts the Transformer (Vaswani\net al., 2017) as the backbone architecture. It ï¬rst\nmap the source dialogue into distributed represen-\ntations, based on which a decoder generates the\ntarget sequence:\nXN = ENCODER (X0)\nN\n:=\nn=1\nFFN\n(\nATT(Xnâˆ’1)\n)\nY M = DECODER (Y 0,XN)\nM\n:=\nm=1\nFFN\n(\nATT\n(\nATT(Y mâˆ’1),XN))\n(3)\nwhere\nN\n:=\nn=1\ndenotes N identical encoding layers,\nM\n:=\nm=1\ndenotes M identical decoding layers, X0 de-\nnotes the sum of the word embeddings Xemb and\nposition embeddings Xpos of D, Y 0 denotes that\nof the shifted right S, FFN(Â·) denotes a position-\nwise feed-forward network, and ATT(Â·) denotes\na multi-head attention. Residual connection (He\net al., 2016) and layer normalization (Ba et al.,\n2016) are used in each sub-layer, which are sup-\npressed in Equation 3 for clarity. Finally, the output\nrepresentation Y M of the decoder is projected into\nthe vocabulary space and the decoder outputs the\nhighest probability token.\nPGN PGN is a hybrid model of the typical\nSeq2Seq Attention model (Nallapati et al., 2016)\nand Pointer-Network (Vinyals et al., 2015). The\ninput dialogue is fed into the LSTM encoder token\nby token, producing the encoder hidden states. The\ndecoder receives word embedding of the previous\nword and generates a distribution to decide the tar-\nget token, retaining decoder hidden states. PGN not\nonly allows to generate from the ï¬xed vocabulary,\nbut also allows to copy from the input tokens.\nTraining Objective Model parameters Î¸ are\ntrained to maximize the conditional likelihood of\nTrain Valid Test\nSAMSum\n# 14732 818 819\nAvg.Turns 11.13 10.72 11.24\nAvg.Tokens 120.26 117.46 122.71\nAvg.Sum 22.81 22.80 22.47\nAMI\n# 97 20 20\nAvg.Turns 310.23 345.70 324.40\nAvg.Tokens 4859.52 5056.25 5257.80\nAvg.Sum 323.74 321.25 328.20\nTable 1: Statistics for SAMSum and AMI datasets.\nâ€œ#â€ means the number of dialogue-summary pairs,\nâ€œAvg.Turnsâ€, â€œAvg.Tokensâ€ and â€œAvg.Sumâ€ mean the\naverage number of turns of dialogues, tokens of dia-\nlogues and tokens of summaries respectively.\nthe outputs in a parallel training corpus (D,S):\narg max\nÎ¸\nâˆ‘\n(D,S)âˆˆ(D,S)\nlog p(S|D; Î¸). (4)\n4 Experiments\n4.1 Datasets\nWe experiment on 2 datasets (statistics in Table 1):\nSAMSum (Gliwa et al., 2019) is a human-\ngenerated dialogue summary dataset, which con-\ntains dialogues in various scenes of the real-life.\nAMI (Carletta et al., 2005) is a meeting summary\ndataset. Each meeting contains four participants\nand is about a remote control design project.\n4.2 Implementation Details\nDialoGPT We initialize DialoGPT withDialoGPT-\nlarge6. For SAMSum, we set keywords extraction\nratio rKE to 15, similarity threshold tRD to 0.99 and\ntopic segmentation ratio rTS to 15. For AMI, rKE\nis 4, tRD is 0.95 and rTS is 5 7.\nBART We initialize BART withbart.large8 . For\nï¬ne-tuning on SAMSum, the learning rate is set to\n3e-05, the dropout rate is 0.1, the warmup is set to\n400. At the test process, beam size is 5, minimum\ndecoded length is 5 and maximum length is 100.\nPGN The word embedding size is set to 300 and\ninitialized with the pre-trained GloVe vector. The\ndimension of encoder and pointer decoder is set\nto 200. The dropout is set to 0.5. The learning\nrate is 0.001. At the test process, beam size is 10,\nminimum decoded length is 280 and maximum\nlength is 4509.\n6https://huggingface.co/transformers\n7We show more hyper-parameter search results for SAM-\nSum and AMI datasets in the supplementary ï¬le.\n8https://github.com/pytorch/fairseq\n9https://github.com/OpenNMT/OpenNMT-py\n1484\nModel R-1 R-2 R-L\nExtractive\nLONGEST-3 32.46 10.27 29.92\nTextRank 29.27 8.02 28.78\nAbstractive\nTransformer 36.62 11.18 33.06\nD-HGN 42.03 18.07 39.56\nTGDGA 43.11 19.15 40.49\nDialoGPT 39.77 16.58 38.42\nMV-BART 53.42 27.98 49.97â€ â€ \nOurs\nBART 52.98 27.67 49.06\nBART(DKE) 53.43â€ â€  28.03â€ â€  49.93\nBART(DRD) 53.39 28.01 49.49\nBART(DTS) 53.34 27.85 49.64\nBART(DALL ) 53.70â€  28.79â€  50.81â€ \nTable 2: Test set results on the SAMSum dataset,\nwhere â€œRâ€ is short for â€œROUGEâ€. BART means ï¬ne-\ntuning BART on the original SAMSum. BART( DKE),\nBART(DRD) and BART( DTS) represent ï¬ne-tuning\nBART on the SAMSum with keywords, redundancy\nand topic annotation respectively. DALL means the\nSAMSum with all three annotations. â€ and â€ â€ indicate\nthe ï¬rst-ranked and second-ranked results respectively.\n4.3 Baselines and Metrics\nFor SAMSum, LONGEST-3 views the ï¬rst three\nutterances as the summary. TextRank (Mihal-\ncea and Tarau, 2004) is a traditional graph-based\nmethod. Transformer (Vaswani et al., 2017) is a\nseq2seq method based on full self-attention oper-\nations. D-HGN (Feng et al., 2020a) incorporates\ncommonsense knowledge to help understand di-\nalogues. TGDGA (Zhao et al., 2020) uses topic\nwords and models graph structures for dialogues.\nDialoGPT (Zhang et al., 2020b) means that ï¬ne-\ntuning DialoGPT on the SAMSum. MV-BART\n(Chen and Yang, 2020) is a BART-based method\nthat incorporates topic and stage information.\nFor AMI, SummaRunner (Nallapati et al.,\n2017) is an extractive method based on hierar-\nchical RNN network. UNS (Shang et al., 2018)\nis a fully unsupervised and graph-based method.\nTopicSeg (Li et al., 2019) incorporates topics to\nmodel the meeting. HMNet (Zhu et al., 2020) is a\ntransformer-based method that incorporates POS\nand entity information and is pre-trained on news\nsummarization dataset.\nWe adopt ROUGE (Lin, 2004) and BERTScore\n(Zhang et al., 2020a) for evaluating our models.\nModel R-1 R-2 R-L\nExtractive\nTextRank 35.19 6.13 15.70\nSummaRunner 30.98 5.54 13.91\nAbstractive\nUNS 37.86 7.84 13.72\nTopicSeg 51.53â€ â€  12.23 25.47â€ \nHMNet 52.36â€  18.63â€  24.00\nOurs\nPGN 48.34 16.02 23.49\nPGN(DKE) 50.22 17.74 24.11\nPGN(DRD) 50.62 16.86 24.27\nPGN(DTS) 48.59 16.07 24.05\nPGN(DALL ) 50.91 17.75â€ â€  24.59â€ â€ \nTable 3: Test set results on the AMI dataset.\nPGN(DKE), PGN(DRD) and PGN(DTS) represent train-\ning PGN on the AMI with keywords, redundancy and\ntopic annotation respectively.\nSAMSum AMI\nModel BS Model BS\nBART 86.91 PGN 80.51\nMV-BART 88.46 HMNet 82.24\nBART(DALL ) 90.04 PGN(DALL ) 82.76\nTable 4: Test set results on the SAMSum and AMI.\nâ€œBSâ€ is short for BERTScore.\n4.4 Automatic Evaluation\nThe results on SAMSum and AMI are shown in Ta-\nble 2 and 3 respectively. We can see that using our\nannotated datasets DKE, DRD and DTS, both BART\nand PGN can obtain improvements. Furthermore,\nour BART(DALL ) achieves SOTA performance.\nFor SAMSum, itâ€™s worth noting that BART(DKE)\nperforms better compared with BART( DRD) and\nBART(DTS). We attribute this to the fact that key-\nwords can retain essential information for shorter\ndialogues. For AMI, PGN( DRD) contributes the\nmost, which shows the importance of detecting re-\ndundancy in verbose meeting transcripts. Although\nHMNet and TopicSeg achieve better scores, HM-\nNet needs news summarization dataset to pre-train\nthe model and TopicSeg designs complex attention\nmechanism to incorporate topic information.\nIn terms of new embedding-based metric\nBERTScore (shown in Table 4), our method\nBART(DALL ) and PGN(DALL ) can consistently out-\nperform the baseline models10.\n10Evaluation details are shown in the supplementary ï¬le.\n1485\nModel Info. Conc. Cov.\nSAMSum\nGolden 4.37 4.26 4.27\nBART 3.66 3.65 3.66\nMV-BART 3.85 3.76 3.88\nBART(DKE) 3.88 3.77 3.79\nBART(DRD) 3.74 3.98 â€  3.89\nBART(DTS) 3.95â€ â€  3.76 4.01â€ â€ \nBART(DALL ) 4.05â€  3.78â€ â€  4.08â€ \nAMI\nGolden 4.70 3.85 4.35\nPGN 2.92 3.08 2.70\nHMNet 3.52â€  2.40 3.40â€ \nPGN(DKE) 3.20 3.08 3.00\nPGN(DRD) 3.15 3.25â€  3.00\nPGN(DTS) 3.05 3.10â€ â€  3.17â€ â€ \nPGN(DALL ) 3.33â€ â€  3.25â€  3.10\nTable 5: Human evaluation results. â€œInfo.â€ is short for\ninformativeness, â€œConc.â€ for conciseness, â€œCov.â€ for\ncoverage. For SAMSum, the inter-annotator agreement\n(Fleissâ€™ kappa) scores for each metric are 0.46, 0.37 and\n0.43 respectively. For AMI, Fleissâ€™ kappa scores are\n0.48, 0.40 and 0.41 respectively.\n4.5 Human Evaluation\nWe conduct a human evaluation of the dialogue\nsummary to assess its informativeness, conciseness\nand coverage. Informativeness measures how well\nthe summary includes key information. Concise-\nness measures how well the summary discards the\nredundant information. Coverage measures how\nwell the summary covers each part of the dialogue.\nWe randomly sample 100 dialogues (SAMSum)\nand 10 meetings (AMI) with corresponding gener-\nated summaries to conduct the evaluation. In order\nto reduce variance caused by humans, we have 4\nhuman evaluators and they were asked to rate each\nsummary on a scale of 1 to 5 (higher is better) for\neach metric. The results are shown in Table 5.\nWe can see that our method can achieve higher\nscores in all three metrics. Especially, combined\nwith DRD, our model can get the best score in con-\nciseness. Besides, combined with DTS, our model\ncan perform better in coverage. However, HMNet\ngets the best score in informativeness and coverage.\nWe argue this is because HMNet forces a minimum\nsummary length of 400. Due to this, it scores the\nworst in conciseness. For the AMI, we also ï¬nd\nthere is still a gap between the scores of generated\nsummaries and the scores of golden summaries,\nindicating that the AMI is more difï¬cult.\nMethod R-1 R-2 R-L\nRule-Based Methods\nEntities 53.36 27.71 49.69\nNouns and Verbs 52.75 27.48 48.82\nTraditional Methods\nTextRank 53.29 27.66 49.33\nTopic words 53.28 27.76 49.59\nPre-trained Language Model-Based Methods\nKeyBERT\nw/ BERT emb 52.39 27.14 48.52\nw/ DialoGPT emb 53.14 27.25 49.42\nOurs\nDialoGPTKE 53.43 28.03 49.93\nTable 6: Test set results of ï¬ne-tuning BART on the\nSAMSum that is annotated with keywords using vari-\nous methods. Entities, nouns and verbs are obtained\nby Qi et al. (2020). Topic words are obtained by a\npre-trained LDA model (Narayan et al., 2018). Key-\nBERT (Grootendorst, 2020) leverages pre-trained lan-\nguage model embeddings to create keywords.\nMethod Precision Recall F1\nTextRank 47.74% 17.44% 23.22%\nEntities 60.42% 17.80% 25.38%\nDialoGPTKE 33.20% 29.49% 30.31%\nTable 7: Quantitative evaluation for keywords on SAM-\nSum test set by viewing reference summary words as\ngolden keywords.\n4.6 Analysis\nEffect of DialoGPT KE. To verify the effective-\nness of our DialoGPTKE method, we ï¬ne-tune\nBART on SAMSum, which is annotated by var-\nious keywords extraction methods. The results are\nshown in Table 6. We can see that our method\nachieves higher scores. The results also show that\nentities play an important role in the summary gen-\neration. Besides, combined with DialoGPT embed-\ndings, KeyBERT can get better results.\nTo give a quantitative evaluation, we view ref-\nerence summary words as golden keywords and\ncalculate the precision, recall and F1 scores for ex-\ntracted keywords. The results are shown in Table\n7. Directly using entities as keywords can get the\nbest precision score. However, both TextRank and\nEntities perform poorly in recall. Our method gets\nthe best score in terms of F1 and its advantage is\nmainly reï¬‚ected in recall score, which shows our\nmethod can extract more diverse keywords.\n1486\nModel R-1 R-2 R-L\nSAMSum\nRule-based 53.00 27.71 49.68\nDialoGPTRD 53.39 28.01 49.49\nAMI\nRule-based 50.19 16.45 23.95\nDialoGPTRD 50.62 16.86 24.27\nTable 8: Test set results on the SAMSum and AMI\ndatasets that are annotated with redundant utterances.\nâ€œRule-basedâ€ indicates annotating utterances that con-\ntain no noun, verb and adjective as redundant.\nEffect of DialoGPT RD. To verify the effective-\nness of our DialoGPTRD method, we compare it\nwith a Rule-based method (Dinarelli et al., 2009),\nwhich annotates utterances without noun, verb and\nadjective as redundant. The results are shown in Ta-\nble 8. We can see that our method performs better.\nEspecially, our method shows more advantages for\nlong and verbose meeting transcripts in the AMI.\nEffect of DialoGPT TS. To verify the effective-\nness of our DialoGPTTS method, we compare it\nwith the C99 algorithm (Choi, 2000), which is\na sentence similarity-based segmentation method.\nChen and Yang (2020) enhance it with BERT (De-\nvlin et al., 2019) embeddings. We further combine\nthe algorithm with DialoGPT embeddings. The\nresults are shown in Table 9. We can see that\nour method can get comparable results with the\nstrong baseline C99(w/ DialoGPT emb). For AMI,\ncombined with golden topic annotation, PGN can\nachieve the best result, which shows modeling top-\nics is an essential task for dialogue summarization.\n4.7 Case Study\nFigure 4 shows summaries generated by different\nmodels for an example dialogue in the SAMSum\ndataset. We can see that BART (Lewis et al., 2020)\ntends to generate long and redundant summaries.\nBy incorporating topic and stage information, MV-\nBART (Chen and Yang, 2020) can generate sum-\nmaries that cover main topics of the dialogue. How-\never, it still suffers from redundancy problem. Our\nBART(DALL ) can get higher ROUGE scores while\ngenerating better summaries. The generated sum-\nmary can include extracted keywords and corre-\nspond to each topic of the dialogue. We also ï¬nd\nthat even some redundant utterances have already\nbeen detected, our model still generate the sum-\nmary contains some redundant information. We\nModel R-1 R-2 R-L\nSAMSum\nC99\nw/ BERT emb 52.80 27.78 49.50\nw/ DialoGPT emb 53.33 28.04 49.39\nDialoGPTTS 53.34 27.85 49.64\nAMI\nGolden 50.28 19.73 24.45\nC99\nw/ BERT emb 48.53 15.84 23.63\nw/ DialoGPT emb 49.22 16.79 23.88\nDialoGPTTS 48.59 16.07 24.05\nTable 9: Test set results on SAMSum and AMI that are\nannotated with topic segmentation in various methods.\nC99 (Choi, 2000) segments dialogues based on inter-\nsentence similarities. Beside, the AMI has golden topic\nsegmentation annotations.\nattribute this to the fact that the small dataset leads\nto insufï¬cient training of the model.\n5 Related Work\nDialogue Summarization Current works mainly\nincorporate auxiliary information to help better\nmodeling dialogues. Some works used various\ntypes of keywords to identify the core part of the\ndialogue, including entities (Zhu et al., 2020), do-\nmain terminologies (Koay et al., 2020) and topic\nwords (Zhao et al., 2020). Some works aimed to\nreduce redundancy, Zechner (2002); Murray et al.\n(2005) used sentence-level similarity-based meth-\nods. Some works incorporate topics as a coarse-\ngrained dialogue structure (Li et al., 2019; Liu et al.,\n2019; Chen and Yang, 2020). Other works also\nexplored dialogue act (Goo and Chen, 2018), dia-\nlogue discourse (Feng et al., 2020b) and common-\nsense knowledge (Feng et al., 2020a). In this paper,\nwe combine three types of auxiliary information\nto help better modeling dialogues, including key-\nwords, redundant utterances and topics.\nPre-trained Language Models Pre-trained mod-\nels such as BERT (Devlin et al., 2019) and GPT-3\n(Brown et al., 2020) have advanced various NLP\ntasks. On one hand, some works utilized the\nknowledge contained in pre-trained models by ï¬ne-\ntuning on supervised data of downstream tasks\n(Qin et al., 2019; Liu and Lapata, 2019; Qin et al.,\n2020). On the other hand, some works examined\nthe knowledge in an unsupervised manner (Jiang\net al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku-\n1487\nRob : Hey there , what's up ? Bob : Not much , watching the game . You ? Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appearedyet Rob : I've heard that Jim is planning to celebrate his birthday Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the next weekend a good timebut he is going for a skiing trip with his family Rob : Sohe said that he might organize something this weekend Rob : [RD] Nothing super fancy most likely a meetup with a few friends at some bar Rob : Would you like to come ? Bob : Sure , that would be nice Bob : But he has not invited me , so I don't want to be rude Rob : [RD] Most likely because it is not a real party . When I see himI'll let him know Bob : [RD] That would be cool I actually haven'tseen him in person for a while now Rob : [RD] Yeah , facebook does that to people Bob : ok , take care and see you on weekend ! Rob : yeah , see you then ! #KEY# Rob Bob watchingHaving people boring fuck writing true run some cook have appeared Jimcelebrate right normally weekend skiing said organizesuper fancy most invited When facebook does take weekendBARTRob is watching the game . Bob is having a few people over . Jim's birthday is next Wednesday . He is going for a skiing trip with his family . He might organize a meetup with a few friends at some bar this weekend . Rob will let Bob know if he can come . Bob hasn't seen Jim in person for a while .MV-BARTBob and Rob are watching the game. Jim is going for a skiing trip with his family next weekend. He might organize a meetup with a few friends at some bar this weekend. Bob will let him know if he wants to come.Bob hasn't seen Jim in person for a while .\nBART(DALL)Roband Bob are watchingthe game . Jimis going for a skiing trip with his family next weekend. He might organizea meetup with a few friends at some bar this weekend. Robwill let him know if he can come .GoldenRob and Bob are watchingthe game . Bob will run some errands on the weekend. Jim's birthday is next wednesday . He might organize a meetup this weekend . Bob will see rob on the weekend .\n[Topic1]\n[Topic2]\n[Topic3]\n[Topic4]\n[Topic1] [Topic2][Topic3]\nR-1 : 50.00R-2 : 29.79R-L : 48.46\nR-1 : 52.27R-2 : 23.26R-L : 47.62\nR-1 : 54.55R-2 : 29.33R-L : 53.10\nFigure 4: Example dialogue in the SAMSum dataset and summaries generated by different models. Keyowrds,\nredundant utterances and topics are annotated by our DialoGPT Annotator. â€œRâ€ is short for ROUGE. Our model\nBART(DALL ) can get higher ROUGE scores while generating the better summary.\nmar et al. (2020) explored pre-trained models for\nconditional data augmentation. Wang et al. (2020)\nused the knowledge in pre-trained models to con-\nstruct knowledge graphs. In this paper, we belong\nto the second paradigm and propose our DialoGPT\nannotator that can perform three annotation tasks\nin an unsupervised manner.\n6 Conclusion\nWe investigate to use DialoGPT as unsupervised an-\nnotators for dialogue summarization, including key-\nwords extraction, redundancy detection and topic\nsegmentation. We conduct our DialoGPT annotator\non two datasets, SAMSum and AMI. Experimental\nresults show that our method consistently obtains\nimprovements upon pre-traind summarizer (BART)\nand non pre-trained summarizer (PGN) on both\ndatasets. Besides, combining all three annotations,\nour summarizer can achieve new state-of-the-art\nperformance on the SAMSum dataset.\nAcknowledgments\nThis work is supported by the National Key R&D\nProgram of China via grant 2018YFB1005103\nand National Natural Science Foundation of China\n(NSFC) via grant 61906053 and 61976073. We\nthank all the anonymous reviewers for their insight-\nful comments. We also thank Lifu Huang and Xin-\nwei Geng for helpful discussion.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. In arXiv.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\n1488\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nJean Carletta, Simone Ashby, Sebastien Bourban, Mike\nFlynn, Mael Guillemot, Thomas Hain, Jaroslav\nKadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa\nKronenthal, et al. 2005. The ami meeting cor-\npus: A pre-announcement. In International work-\nshop on machine learning for multimodal interac-\ntion. Springer.\nJiaao Chen and Diyi Yang. 2020. Multi-view sequence-\nto-sequence models with conversational structure\nfor abstractive dialogue summarization. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4106â€“4118, Online. Association for Computational\nLinguistics.\nFreddy Y . Y . Choi. 2000. Advances in domain inde-\npendent linear text segmentation. In 1st Meeting of\nthe North American Chapter of the Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171â€“4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMarco Dinarelli, Silvia Quarteroni, Sara Tonelli,\nAlessandro Moschitti, and Giuseppe Riccardi. 2009.\nAnnotating spoken dialogs: from speech segments\nto dialog acts and frame semantics. In Proceedings\nof SRSL 2009, the 2nd Workshop on Semantic Rep-\nresentation of Spoken Language, pages 34â€“41.\nXiachong Feng, X. Feng, B. Qin, and T. Liu. 2020a.\nIncorporating commonsense knowledge into ab-\nstractive dialogue summarization via heterogeneous\ngraph networks. ArXiv, abs/2010.10044.\nXiachong Feng, Xiaocheng Feng, Bing Qin, and Xin-\nwei Geng. 2020b. Dialogue discourse-aware graph\nmodel and data augmentation for meeting summa-\nrization.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and\nAleksander Wawer. 2019. SAMSum corpus: A\nhuman-annotated dialogue dataset for abstractive\nsummarization. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization , pages 70â€“79,\nHong Kong, China. Association for Computational\nLinguistics.\nChih-Wen Goo and Yun-Nung Chen. 2018. Abstrac-\ntive dialogue summarization with sentence-gated\nmodeling optimized by dialogue acts. 2018 IEEE\nSpoken Language Technology Workshop (SLT) ,\npages 735â€“742.\nMaarten Grootendorst. 2020. Keybert: Minimal key-\nword extraction with bert.\nIryna Gurevych and Michael Strube. 2004. Semantic\nsimilarity applied to spoken dialogue summarization.\nIn COLING 2004: Proceedings of the 20th Inter-\nnational Conference on Computational Linguistics ,\npages 764â€“770, Geneva, Switzerland. COLING.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016 , pages 770â€“778.\nIEEE Computer Society.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423â€“438.\nJia Jin Koay, Alexander Roustai, Xiaojin Dai, Dillon\nBurns, Alec Kerrigan, and Fei Liu. 2020. How\ndomain terminology affects meeting summarization\nperformance. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 5689â€“5695, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18â€“26, Suzhou, China. Association for Com-\nputational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871â€“7880, Online. Association\nfor Computational Linguistics.\nManling Li, Lingyu Zhang, Heng Ji, and Richard J.\nRadke. 2019. Keep meeting summaries on topic:\nAbstractive multi-modal meeting summarization. In\nProceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2190â€“2196, Florence, Italy. Association for Compu-\ntational Linguistics.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of\nPre-Trained Language Models. In Proceedings of\n1489\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6862â€“\n6868, Online. Association for Computational Lin-\nguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74â€“81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730â€“3740, Hong Kong,\nChina. Association for Computational Linguistics.\nZhengyuan Liu, A. Ng, Sheldon Lee Shao Guang, AiTi\nAw, and Nancy F. Chen. 2019. Topic-aware pointer-\ngenerator networks for summarizing spoken conver-\nsations. 2019 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU) , pages 814â€“\n821.\nRada Mihalcea and Paul Tarau. 2004. TextRank:\nBringing order into text. In Proceedings of the 2004\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 404â€“411, Barcelona, Spain.\nAssociation for Computational Linguistics.\nGabriel Murray, S. Renals, and J. Carletta. 2005. Ex-\ntractive summarization of meeting recordings. In\nINTERSPEECH.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.\nSummarunner: A recurrent neural network based se-\nquence model for extractive summarization of docu-\nments. In Proceedings of the AAAI Conference on\nArtiï¬cial Intelligence, volume 31.\nRamesh Nallapati, Bowen Zhou, C. D. Santos, C Â¸ aglar\nGÂ¨ulc Â¸ehre, and Bing Xiang. 2016. Abstractive text\nsummarization using sequence-to-sequence rnns and\nbeyond. In CoNLL.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Donâ€™t give me the details, just the summary!\nTopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium.\nMaxime Peyrard. 2019. A simple theoretical model of\nimportance for summarization. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1059â€“1073, Florence,\nItaly. Association for Computational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101â€“\n108, Online. Association for Computational Linguis-\ntics.\nLibo Qin, Wanxiang Che, Yangming Li, Haoyang Wen,\nand T. Liu. 2019. A stack-propagation framework\nwith token-level intent detection for spoken lan-\nguage understanding. In EMNLP/IJCNLP.\nLibo Qin, Zhouyang Li, Wanxiang Che, Minheng Ni,\nand Ting Liu. 2020. Co-gat: A co-interactive graph\nattention network for joint dialog act recognition and\nsentiment classiï¬cation. ArXiv, abs/2012.13260.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nK. Riedhammer, B. Favre, and Dilek Z. Hakkani-T Â¨ur.\n2008. A keyphrase based approach to interactive\nmeeting summarization. 2008 IEEE Spoken Lan-\nguage Technology Workshop, pages 153â€“156.\nOscar Sainz and German Rigau. 2021.\nAsk2transformers: Zero-shot domain labelling\nwith pre-trained language models.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073â€“\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nGuokan Shang, Wensi Ding, Zekun Zhang, An-\ntoine Tixier, Polykarpos Meladianos, Michalis Vazir-\ngiannis, and Jean-Pierre Lorr Â´e. 2018. Unsuper-\nvised abstractive meeting summarization with multi-\nsentence compression and budgeted submodular\nmaximization. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 664â€“\n674, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998â€“6008.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural\nInformation Processing Systems 28: Annual Con-\nference on Neural Information Processing Systems\n2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 2692â€“2700.\nC. Wang, Xiao Liu, and D. Song. 2020. Lan-\nguage models are open knowledge graphs. ArXiv,\nabs/2010.11967.\nLiqiang Xiao, Lu Wang, Hao He, and Yaohui Jin.\n2020. Modeling content importance for summariza-\ntion with pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\n1490\nin Natural Language Processing (EMNLP) , pages\n3606â€“3611, Online. Association for Computational\nLinguistics.\nKlaus Zechner. 2002. Automatic summarization of\nopen-domain multiparty dialogues in diverse genres.\nComputational Linguistics, 28(4):447â€“485.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020a. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020b. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 270â€“\n278, Online. Association for Computational Linguis-\ntics.\nLulu Zhao, Weiran Xu, and Jun Guo. 2020. Improving\nabstractive dialogue summarization with graph struc-\ntures and topic words. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 437â€“449, Barcelona, Spain (Online). In-\nternational Committee on Computational Linguis-\ntics.\nChenguang Zhu, Ruochen Xu, Michael Zeng, and Xue-\ndong Huang. 2020. A hierarchical network for ab-\nstractive meeting summarization with cross-domain\npretraining. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 194â€“\n203, Online. Association for Computational Linguis-\ntics.\nA Evaluation Details\nFor ROUGE (Lin, 2004), we employ Py-rouge 11\npackage to evaluate our models following Gliwa\net al. (2019). For BERTScore (Zhang et al.,\n2020a), we use the ofï¬cial implementation 12\nto evaluate our models. The detailed com-\nmand line for BERTScore is bert-score -r\ngolden.txt -c gen.txt --lang en.\nB Ablation Studies for Annotations\nTo further verify the effectiveness of our method,\nwe conduct ablation studies for each annotation.\nThe results are shown in Table 10 and Table 11.\nWe can ï¬nd that: (1) For both datasets, train-\ning summarizers based on datasets with two of\nthree annotations can obtain improvements. (2)\nFor both datasets, training summarizers based on\n11https://pypi.org/project/py-rouge/\n12https://github.com/Tiiiger/bert score\nModel R-1 R-2 R-L\nOurs\nBART 52.98 27.67 49.06\nBART(DKE) 53.43 28.03 49.93\nBART(DRD) 53.39 28.01 49.49\nBART(DTS) 53.34 27.85 49.64\nBART(DKE+RD) 53.56 28.65 50.55\nBART(DKE+TS) 53.51 28.13 50.00\nBART(DRD+TS) 53.64 28.33 50.13\nBART(DALL ) 53.70 28.79 50.81\nTable 10: Test set results on the SAMSum dataset.\nBART means ï¬ne-tuning BART on the original SAM-\nSum. BART( DKE), BART(DRD) and BART(DTS) rep-\nresent ï¬ne-tuning BART on the SAMSum with key-\nwords, redundancy and topic annotation respectively.\nBART(DKE+RD) represent ï¬ne-tuning BART on the\nSAMSum with keywords and redundancy annotations.\nDALL means the SAMSum with all three annotations.\nModel R-1 R-2 R-L\nOurs\nPGN 48.34 16.02 23.49\nPGN(DKE) 50.22 17.74 24.11\nPGN(DRD) 50.62 16.86 24.27\nPGN(DTS) 48.59 16.07 24.05\nPGN(DKE+RD) 50.74 17.11 24.52\nPGN(DKE+TS) 50.69 16.83 24.33\nPGN(DRD+TS) 50.70 16.96 24.38\nPGN(DALL ) 50.91 17.75 24.59\nTable 11: Test set results on the AMI dataset.\nPGN(DKE), PGN(DRD) and PGN(DTS) represent train-\ning PGN on the AMI with keywords, redundancy and\ntopic annotation respectively. PGN(DKE+RD) represent\ntraining PGN on the AMI with both keywords and re-\ndundancy annotations.\ndatasets with two of three annotations can sur-\npass corresponding summarizers that are trained\nbased on datasets with one type of annotation (e.g.,\nBART(DKE+RD) is better than BART( DKE) and\nBART(DRD)). (3) Compared with summarizers\nthat are trained on DRD+TS and DKE+RD, summa-\nrizers that are trained on DKE+TS get relatively\nsmall improvements on both datasets. Neverthe-\nless, it indicates thatDialoGPTKE and DialoGPTTS\nstill have non-overlapping parts. (4) Combining all\nthree annotations, both summarizers can achieve\nthe best results in all ROUGE scores.\n1491\nC Hyper-parameter Search Results\nTables 12 to 17 show the hyper-parameter search\nresults. Finally, for SAMSum (Gliwa et al., 2019),\nwe set keywords extraction ratio rKE to 15, simi-\nlarity threshold tRD to 0.99 and topic segmentation\nratio rTS to 15. for AMI (Carletta et al., 2005), rKE\nis 4, tRD is 0.95 and rTS is 5.\nModel rKE R-1 R-2 R-L\nBART(DKE) 10 52.17 26.64 48.34\nBART(DKE) 15 53.43 28.03 49.93\nBART(DKE) 20 53.20 28.01 49.46\nBART(DKE) 25 52.78 27.35 48.67\nTable 12: Test set results on the SAMSum dataset.\nBART(DKE) means ï¬ne-tuning BART on SAMSum\nwith keywords annotation. rKE means different key-\nwords extraction ratios.\nModel rKE R-1 R-2 R-L\nPGN(DKE) 3 49.76 16.03 23.64\nPGN(DKE) 4 50.22 17.74 24.11\nPGN(DKE) 5 49.63 16.71 23.88\nPGN(DKE) 6 49.70 16.92 24.42\nTable 13: Test set results on the AMI dataset.\nPGN(DKE) means training PGN on AMI with key-\nwords annotation. rKE means different keywords ex-\ntraction ratios.\nModel tRD R-1 R-2 R-L\nBART(DRD) 0.95 52.29 26.71 48.53\nBART(DRD) 0.96 53.20 27.98 49.68\nBART(DRD) 0.97 52.17 27.10 48.34\nBART(DRD) 0.98 53.29 27.89 49.71\nBART(DRD) 0.99 53.39 28.01 49.49\nTable 14: Test set results on the SAMSum dataset.\nBART(DRD) means ï¬ne-tuning BART on SAMSum\nwith redundant utterances annotation. tRD means dif-\nferent similarity thresholds.\nModel tRD R-1 R-2 R-L\nPGN(DRD) 0.95 50.62 16.86 24.27\nPGN(DRD) 0.96 49.68 16.54 24.70\nPGN(DRD) 0.97 50.18 16.12 24.56\nPGN(DRD) 0.98 48.63 15.17 23.50\nPGN(DRD) 0.99 47.15 13.94 22.53\nTable 15: Test set results on the AMI dataset.\nPGN(DRD) means training PGN on AMI with redun-\ndant utterances annotation. tRD means different similar-\nity thresholds.\nModel rTS R-1 R-2 R-L\nBART(DTS) 10 53.21 27.38 49.32\nBART(DTS) 15 53.34 27.85 49.64\nBART(DTS) 20 52.82 27.34 49.05\nBART(DTS) 25 53.04 27.49 49.70\nTable 16: Test set results on the SAMSum dataset.\nBART(DTS) means ï¬ne-tuning BART on SAMSum\nwith topic annotation. rTS means different topic seg-\nmentation ratios.\nModel rTS R-1 R-2 R-L\nPGN(DTS) 4 49.39 16.02 23.89\nPGN(DTS) 5 48.59 16.07 24.05\nPGN(DTS) 6 49.89 16.04 23.01\nPGN(DTS) 7 49.37 16.07 23.46\nTable 17: Test set results on the AMI dataset.\nPGN(DTS) means training PGN on AMI with topic an-\nnotation. rTS means different topic segmentation ratios."
}