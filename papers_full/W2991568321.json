{
    "title": "A transformer-based approach to irony and sarcasm detection",
    "url": "https://openalex.org/W2991568321",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4282668253",
            "name": "Potamias, Rolandos Alexandros",
            "affiliations": [
                "Imperial College London"
            ]
        },
        {
            "id": "https://openalex.org/A2750083155",
            "name": "Siolas Georgios",
            "affiliations": [
                "National Technical University of Athens"
            ]
        },
        {
            "id": "https://openalex.org/A4302692500",
            "name": "Stafylopatis, Andreas-Georgios",
            "affiliations": [
                "National Technical University of Athens"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2964126051",
        "https://openalex.org/W2766533978",
        "https://openalex.org/W2251740185",
        "https://openalex.org/W2112344871",
        "https://openalex.org/W2963723676",
        "https://openalex.org/W2053605134",
        "https://openalex.org/W2157961599",
        "https://openalex.org/W2137527984",
        "https://openalex.org/W2891177506",
        "https://openalex.org/W2290577549",
        "https://openalex.org/W4293206760",
        "https://openalex.org/W2125864357",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2754876402",
        "https://openalex.org/W2954824927",
        "https://openalex.org/W2906869079",
        "https://openalex.org/W2463843470",
        "https://openalex.org/W2586600557",
        "https://openalex.org/W2101217916",
        "https://openalex.org/W2512532697",
        "https://openalex.org/W2250480277",
        "https://openalex.org/W2250680874",
        "https://openalex.org/W3000293211",
        "https://openalex.org/W2078470255",
        "https://openalex.org/W2463060288",
        "https://openalex.org/W2782679411",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2604716683",
        "https://openalex.org/W2964101952",
        "https://openalex.org/W2250473257",
        "https://openalex.org/W2781487490",
        "https://openalex.org/W2803454059",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2052017327",
        "https://openalex.org/W2968386811",
        "https://openalex.org/W2915002815",
        "https://openalex.org/W2265846598",
        "https://openalex.org/W2159397589",
        "https://openalex.org/W2537906591",
        "https://openalex.org/W4233906183",
        "https://openalex.org/W6600424091",
        "https://openalex.org/W2251622960",
        "https://openalex.org/W2556934051",
        "https://openalex.org/W2162786494",
        "https://openalex.org/W2251938308",
        "https://openalex.org/W2963563735",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2953470141",
        "https://openalex.org/W2954944481",
        "https://openalex.org/W2946805728",
        "https://openalex.org/W2024011160",
        "https://openalex.org/W2566441476",
        "https://openalex.org/W2038634595",
        "https://openalex.org/W2041400887",
        "https://openalex.org/W2250243742",
        "https://openalex.org/W2803762112",
        "https://openalex.org/W2397798297",
        "https://openalex.org/W2962681323",
        "https://openalex.org/W2179531049",
        "https://openalex.org/W2787414878",
        "https://openalex.org/W2251971374",
        "https://openalex.org/W2158153487",
        "https://openalex.org/W2806834924",
        "https://openalex.org/W6601211009",
        "https://openalex.org/W2101449828",
        "https://openalex.org/W6601141708",
        "https://openalex.org/W6601323341",
        "https://openalex.org/W2945662754",
        "https://openalex.org/W2576683119",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2979795568",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2032928173",
        "https://openalex.org/W2154359981",
        "https://openalex.org/W2753593955",
        "https://openalex.org/W3152231500",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W2962202409",
        "https://openalex.org/W2803424915",
        "https://openalex.org/W2930786691",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1912242145",
        "https://openalex.org/W2165044314",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W2740711318",
        "https://openalex.org/W2557283755",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2893676516",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W109881707",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2886186188",
        "https://openalex.org/W2794557536",
        "https://openalex.org/W2738554243",
        "https://openalex.org/W2607623312",
        "https://openalex.org/W2250489604",
        "https://openalex.org/W2329667378",
        "https://openalex.org/W2962753370",
        "https://openalex.org/W2592746732",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2807333695",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2970377225",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2250710744",
        "https://openalex.org/W2563351168",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2578900279"
    ],
    "abstract": null,
    "full_text": "S.I. : EMERGING APPLICATIONS OF DEEP LEARNING AND SPIKING ANN\nA transformer-based approach to irony and sarcasm detection\nRolandos Alexandros Potamias1 • Georgios Siolas2 • Andreas - Georgios Stafylopatis2\nReceived: 12 November 2019 / Accepted: 4 June 2020 / Published online: 22 June 2020\n/C211 The Author(s) 2020\nAbstract\nFigurative language (FL) seems ubiquitous in all social media discussion forums and chats, posing extra challenges to\nsentiment analysis endeavors. Identiﬁcation of FL schemas in short texts remains largely an unresolved issue in the broader\nﬁeld of natural language processing, mainly due to their contradictory and metaphorical meaning content. The main FL\nexpression forms are sarcasm, irony and metaphor. In the present paper, we employ advanced deep learning methodologies\nto tackle the problem of identifying the aforementioned FL forms. Signiﬁcantly extending our previous work (Potamias\net al., in: International conference on engineering applications of neural networks, Springer, Berlin, pp 164–175, 2019), we\npropose a neural network methodology that builds on a recently proposed pre-trained transformer-based network archi-\ntecture which is further enhanced with the employment and devise of a recurrent convolutional neural network. With this\nsetup, data preprocessing is kept in minimum. The performance of the devised hybrid neural architecture is tested on four\nbenchmark datasets, and contrasted with other relevant state-of-the-art methodologies and systems. Results demonstrate\nthat the proposed methodology achieves state-of-the-art performance under all benchmark datasets, outperforming, even by\na large margin, all other methodologies and published studies.\nKeywords Sentiment analysis /C1 Natural language processing /C1 Figurative language /C1 Sarcasm /C1 Irony /C1 Deep learning /C1\nTransformer networks\n1 Introduction\nIn the networked-world era, the production of (structured\nor unstructured) data is increasing with most of our\nknowledge being created and communicated via web-based\nsocial channels [ 96]. Such data explosion raises the need\nfor efﬁcient and reliable solutions for the management,\nanalysis and interpretation of huge data sizes. Analyzing\nand extracting knowledge from massive data collections is\nnot only a big issue per se, but also challenges the data\nanalytics state-of-the-art [103], with statistical and machine\nlearning methodologies paving the way, and deep learning\n(DL) taking over and presenting highly accurate solutions\n[29]. Relevant applications in the ﬁeld of social media\ncover a wide spectrum, from the categorization of major\ndisasters [43] and the identiﬁcation of suggestions [ 74]t o\ninducing users’ appeal to political parties [ 2].\nThe raising of computational social science [ 56] and\nmainly its social media dimension [ 67] challenge con-\ntemporary computational linguistics and text-analytics\nendeavors. The challenge concerns the advancement of text\nanalytics methodologies toward the transformation of\nunstructured excerpts into some kind of structured data via\nthe identiﬁcation of special passage characteristics, such as\nits emotional content (e.g., anger, joy, sadness) [49]. In this\ncontext, sentiment analysis (SA) comes into play, targeting\nthe devise and development of efﬁcient algorithmic pro-\ncesses for the automatic extraction of a writer’s sentiment\nor emotion as conveyed in text excerpts. Relevant efforts\nfocus on tracking the sentiment polarity of single utter-\nances, which in most cases is loaded with a lot of subjec-\ntivity and a degree of vagueness [ 58]. Contemporary\nRolandos Alexandros Potamias: Work performed while at\nNational Technical University of Athens.\n& Rolandos Alexandros Potamias\nr.potamias@imperial.ac.uk\nGeorgios Siolas\ngsiolas@islab.ntua.gr\nAndreas - Georgios Stafylopatis\nandreas@cs.ntua.gr\n1 Department of Computing, Imperial College London,\nLondon, UK\n2 School of Electrical and Computer Engineering, National\nTechnical University of Athens, Athens, Greece\n123\nNeural Computing and Applications (2020) 32:17309–17320\nhttps://doi.org/10.1007/s00521-020-05102-3(0123456789().,-volV)(0123456789().,- volV)\nresearch in the ﬁeld utilizes data from social media\nresources (e.g., Facebook, Twitter) as well as other short\ntext references in blogs, forums, etc. [ 75]. However, users\nof social media tend to violate common grammar and\nvocabulary rules and even use various ﬁgurative language\nforms to communicate their message. In such situations,\nthe sentiment inclination underlying the literal content of\nthe conveyed concept may signiﬁcantly differ from its\nﬁgurative context, making SA tasks even more puzzling.\nEvidently, single turn text lacks in detecting sentiment\npolarity on sarcastic and ironic expressions, as already\n‘‘signiﬁed in the relevant SemEval-2014 Sentiment Anal-\nysis task 9’’ [83]. Moreover, lacking of facial expressions\nand voice tone require context-aware approaches to tackle\nsuch a challenging task and overcome its ambiguities [ 31].\nAs sentiment is the emotion behind customer engagement,\nSA ﬁnds its realization in automated customer-aware ser-\nvices, elaborating over user’s emotional intensities [ 13].\nMost of the related studies utilize single turn texts from\ntopic-speciﬁc sources, such as Twitter, Amazon and\nIMDB. Handcrafted and sentiment-oriented features,\nindicative of emotion polarity, are utilized to represent\nrespective excerpt cases. The formed data are then fed\ntraditional machine learning classiﬁers (e.g., SVM, random\nforest, multilayer perceptrons) or DL techniques and\nrespective complex neural architectures, in order to induce\nanalytical models that are able to capture the underlying\nsentiment content and polarity of passages [ 33, 42, 84].\nThe linguistic phenomenon of ﬁgurative language (FL)\nrefers to the contradiction between the literal and the non-\nliteral meaning of an utterance [ 17]. Literal written lan-\nguage assigns ‘exact’ (or ‘real’) meaning to the used words\n(or phrases) without any reference to putative speech ﬁg-\nures. In contrast, FL schemas exploit non-literal mentions\nthat deviate from the exact concept presented by the used\nwords and phrases. FL is rich of various linguistic phe-\nnomena like ‘metonymy’ reference to an entity stands for\nanother of the same domain, a more general case of ‘syn-\nonymy’; and ‘metaphors’ systematic interchange between\nentities from different abstract domains [ 18]. Besides the\nphilosophical considerations, theories and debates about\nthe exact nature of FL, ﬁndings from the neuroscience\nresearch domain present clear evidence on the presence of\ndifferentiating FL processing patterns in the human brain\n[6, 13, 46, 60, 95], even for woman–man attraction situa-\ntions! [ 23], a fact that makes FL processing even more\nchallenging and difﬁcult to tackle. Indeed, this is the case\nof pragmatic FL phenomena like irony and sarcasm that\nmain intention of in most of the cases, are characterized by\nan oppositeness to the literal language context. It is crucial\nto distinguish between the literal meaning of an expression\nconsidered as a whole from its constituents’ words and\nphrases. As literal meaning is assumed to be invariant in all\ncontext at least in its classical conceptualization [ 47], it is\nexactly this separation of an expression from its context\nthat permits and opens the road to computational approa-\nches in detecting and characterizing FL utterance.\nWe may identify three common FL expression forms,\nnamely irony, sarcasm and metaphor. In this paper, ﬁgu-\nrative expressions, and especially ironic or sarcastic ones,\nare considered as a way of indirect denial. From this point\nof view, the interpretation and ultimately identiﬁcation of\nthe indirect meaning involved in a passage does not entail\nthe cancellation of the indirectly rejected message and its\nreplacement with the intentionally implied message (as\nadvocated in [ 12, 30]). On the contrary, ironic/sarcastic\nexpressions presuppose the processing of both the indi-\nrectly rejected and the implied message so that the differ-\nence between them can be identiﬁed. This view differs\nfrom the assumption that irony and sarcasm involve only\none interpretation [ 32, 85]. Holding that irony activates\nboth grammatical/explicit and ironic/involved notions\nprovides that irony will be more difﬁcult to grasp than a\nnon-ironic use of the same expression.\nDespite that all forms of FL are well-studied linguistic\nphenomena [32\n], computational approaches fail to identify\nthe polarity of them within a text. The inﬂuence of FL in\nsentiment classiﬁcation emerged both on SemEval-2014\nsentiment analysis task [ 18, 83]. Results show that natural\nlanguage processing (NLP) systems effective in most other\ntasks see their performance drop when dealing with ﬁgu-\nrative forms of language. Thus, methods capable of\ndetecting, separating and classifying forms of FL would be\nvaluable building blocks for a system that could ultimately\nprovide a full-spectrum sentiment analysis of natural\nlanguage.\nIn the literature, we encounter some major drawbacks of\nprevious studies and we aim to resolve with our proposed\nmethod:\n• Many studies tackle ﬁgurative language by utilizing a\nwide range of engineered features (e.g., lexical and\nsentiment-based features) [ 21, 28, 76, 78, 79, 87] mak-\ning classiﬁcation frameworks not feasible.\n• Several approaches search words on large dictionaries\nwhich demand large computational times and can be\nconsidered as impractical [ 76, 87].\n• Many studies exhaustively preprocess the input texts,\nincluding stemming, tagging, emoji processing, etc.,\nthat tend to be time-consuming especially in large\ndatasets [52, 91].\n• Many approaches attempt to create datasets using social\nmedia API’s to automatically collect data rather than\nexploiting their system on benchmark datasets, with\nproven quality. To this end, it is impossible to be\ncompared and evaluated [ 52, 57, 91].\n17310 Neural Computing and Applications (2020) 32:17309–17320\n123\nTo tackle the aforementioned problems, we propose an\nend-to-end methodology containing none handcrafted\nengineered features or lexicon dictionaries, a preprocessing\nstep that includes only de-capitalization and we evaluate\nour system on several benchmark dataset. To the best of\nour knowledge, this is the ﬁrst time that an unsupervised\npre-trained transformer method is used to capture ﬁgurative\nlanguage in many of its forms.\nThe rest of the paper is structured as follows: In Sect. 2,\nwe present the related work on the ﬁeld of FL detection; in\nSect. 3, we shortly describe the background of recent\nadvances in natural language processing that achieve high\nperformance in a wide range of tasks and will be used to\ncompare performance; in Sect. 4 we present our proposed\nmethod; the results of our experiments are presented in\nSect. 5; and ﬁnally, our conclusion is in Sect. 6.\n2 Literature review\nAlthough the NLP community have researched all aspects\nof FL independently, none of the proposed systems were\nevaluated on more than one type. Related work on FL\ndetection and classiﬁcation tasks could be categorized into\ntwo main categories, according to the studied task:\n(a) irony and sarcasm detection and (b) sentiment analysis\nof FL excerpts. Even if sarcasm and irony are not identical\nphenomena, we will present those types together, as they\nappear in the literature.\n2.1 Irony and sarcasm detection\nRecently, the detection of ironic and sarcastic meanings\nfrom respective literal ones have raised scientiﬁc interest\ndue to the intrinsic difﬁculties to differentiate between\nthem. Apart from English language, irony and sarcasm\ndetection have been widely explored on other languages as\nwell, such as Italian [ 86], Japanese [ 36], Spanish [ 68] and\nGreek [10]. In the review analysis that follows, we group\nrelated approaches according to the their adopted key\nconcepts to handle FL.\n2.1.1 Approaches based on unexpectedness\nand contradictory factors\nReyes et al. [80, 81] were the ﬁrst that attempted to capture\nirony and sarcasm in social media. They introduced the\nconcepts of unexpectedness and contradiction that seems to\nbe frequent in FL expressions. The unexpectedness factor\nwas also adopted as a key concept in other studies as well.\nIn particular, Barbieri and Saggion [ 4] compared tweets\nwith sarcastic content with other topics such as, #politics,\n#education, #humor. The measure of unexpectedness was\ncalculated using the American National Corpus Frequency\nData source as well as the morphology of tweets, using\nrandom forests (RF) and decision trees (DT) classiﬁers. In\nthe same direction, Buschmeir et al. [ 7] considered unex-\npectedness as an emotional imbalance between words in\nthe text. Ghosh et al. [ 26] identiﬁed sarcasm using support\nvector machines (SVM) using as features the identiﬁed\ncontradictions within each tweet.\n2.1.2 Content and context-based approaches\nInspired by the contradictory and unexpectedness concepts,\nfollow-up approaches utilized features that expose infor-\nmation about the content of each passage including: N-\ngram patterns, acronyms and adverbs [ 8]; semi-supervised\nattributes like word frequencies [ 16]; statistical and\nsemantic features [ 79]; and Linguistic Inquiry and Word\nCount (LIWC) dictionary along with syntactic and psycho-\nlinguistic features [77]. LIWC corpus [70] was also utilized\nin [ 28], comparing sarcastic tweets with positive and\nnegative ones using an SVM classiﬁer. Similarly, using\nseveral lexical resources [ 87], and syntactic and sentiment\nrelated features [ 57], the respective researchers explored\ndifferences between sarcastic and ironic expressions.\nAffective and structural features are also employed to\npredict irony with conventional machine learning classi-\nﬁers (DT, SVM, naı¨ ve Bayes/NB) in [ 20]. In a follow-up\nstudy [21], a knowledge-based k-NN classiﬁer was fed with\na feature set that captures a wide range of linguistic phe-\nnomena (e.g., structural, emotional). Signiﬁcant results\nwere achieved in [ 91], were a combination of lexical,\nsemantic and syntactic features passed through an SVM\nclassiﬁer that outperformed LSTM deep neural network\napproaches. Apart from local content, several approaches\nclaimed that global context may be essential to capture FL\nphenomena. In particular, in [ 93] it is claimed that cap-\nturing previous and following comments on Reddit\nincreases classiﬁcation performance. Users’ behavioral\ninformation seems to be also beneﬁcial as it captures useful\ncontextual information in Twitter post [ 78]. A novel\nunsupervised probabilistic modeling approach to detect\nirony was also introduced in [ 66].\n2.1.3 Deep learning approaches\nAlthough several DL methodologies, such as recurrent\nneural networks (RNNs), are able to capture hidden\ndependencies between terms within text passages and can\nbe considered as content-based, we grouped all DL studies\nfor readability purposes. Word embeddings, i.e., learned\nmappings of words to real-valued vectors [ 62], play a key\nrole in the success of RNNs and other DL neural archi-\ntectures that utilize pre-trained word embeddings to tackle\nNeural Computing and Applications (2020) 32:17309–17320 17311\n123\nFL. In fact, the combination of word embeddings with\nconvolutional neural networks (CNN), so-called CNN-\nLSTM units, was introduced by Kumar et al. [ 53] and\nGhosh and Veale [ 25] achieving state-of-the-art perfor-\nmance. Attentive RNNs exhibit also good performance\nwhen matched with pre-trained Word2Vec embeddings\n[39], and contextual information [102]. Following the same\napproach, an LSTM-based intra-attention was introduced\nin [ 89] that achieved increased performance. A different\napproach, founded on the claim that number present sig-\nniﬁcant indicators, was introduced by Dubey et al. [ 19].\nUsing an attentive CNN on a dataset with sarcastic tweets\nthat contain numbers, showed notable results. An ensemble\nof a shallow classiﬁer with lexical, pragmatic and semantic\nfeatures, utilizing a bidirectional LSTM model is presented\nin [ 51]. In a subsequent study [ 52], the researchers engi-\nneered a soft attention LSTM model coupled with a CNN.\nContextual DL approaches are also employed, utilizing\npre-trained along with user embeddings structured from\nprevious posts [ 1] or, personality embeddings passed\nthrough CNNs [34]. ELMo embeddings [73] are utilized in\n[40]. In our previous approach, we implemented an\nensemble deep learning classiﬁer (DESC) [ 76], capturing\ncontent and semantic information. In particular, we\nemployed an extensive feature set of a total 44 features\nleveraging syntactic, demonstrative, sentiment and read-\nability information from each text along with Tf-idf fea-\ntures. In addition, an attentive bidirectional LSTM model\ntrained with GloVe pre-trained word embeddings was uti-\nlized to structure an ensemble classiﬁer processing differ-\nent text representations. DESC model performed state-of-\nthe-art results on several FL tasks.\n2.2 Sentiment analysis on figurative language\nThe Semantic Evaluation Workshop-2015 [ 24] proposed a\njoint task to evaluate the impact of FL in sentiment analysis\non ironic, sarcastic and metaphorical tweets, with a number\nof submissions achieving highly performance results. The\nClaC team [69] exploited four lexicons to extract attributes\nas well as syntactic features to identify sentiment polarity.\nThe UPF team [ 3] introduced a regression classiﬁcation\nmethodology on tweet features extracted with the use of the\nwidely utilized SentiWordNet and DepecheMood lexicons.\nThe LLT-PolyU team [99] used semi-supervised regression\nand decision trees on extracted unigram and bi-gram fea-\ntures, coupled with features that capture potential contra-\ndictions at short distances. An SVM-based classiﬁer on\nextracted n-gram and Tf-idf features was used by the Elirf\nteam [ 27] coupled with speciﬁc lexicons such as Afﬁn,\nPatter and Jeffrey 10. Finally, the LT3 team [ 90] used an\nensemble regression and SVM semi-supervised classiﬁer\nwith lexical features extracted with the use of WordNet and\nDBpedia11.\n3 The background: recent advances\nin natural language processing\nDue to the limitations of annotated datasets and the high\ncost of data collection, unsupervised learning approaches\ntend to be an easier way toward training networks.\nRecently, transfer learning approaches, i.e., the transfer of\nalready acquired knowledge to new conditions, are gaining\nattention in several domain adaptation problems [ 22]. In\nfact, pre-trained embeddings representations, such as\nGloVe, ElMo and USE, coupled with transfer learning\narchitectures were introduced and managed to achieve\nstate-of-the-art results on various NLP tasks [ 37]. In the\ncurrent section, we summarize those methods in order to\nintroduce our proposed transfer learning system in Sect. 5.\nModel speciﬁcations used for the state-of-the-art models\ncan be found in ‘‘Appendix’’.\n3.1 Contextual embeddings\nPre-trained word embeddings proved to increase classiﬁ-\ncation performances in many NLP tasks. In particular,\nglobal vectors (GloVe) [ 71] and Word2Vec [ 63] became\npopular in various tasks due to their ability to capture\nrepresentative semantic representations of words, trained\non large amount of data. However, in various studies (e.g.,\n[61, 72, 73]), it is argued that the actual meaning of words\nalong with their semantics representations varies according\nto their context. Following this assumption, researchers in\n[73] present an approach that is based on the creation of\npre-trained word embeddings through building a bidirec-\ntional language model, i.e., predicting next word within a\nsequence. The ELMo model was exhaustingly trained on\n30 million sentences corpus [ 11], with a two-layered\nbidirectional LSTM architecture, aiming to predict both\nnext and previous words, introducing the concept of con-\ntextual embeddings. The ﬁnal embeddings vector is pro-\nduced by a task-speciﬁc weighted sum of the two\ndirectional hidden layers of LSTM models. Another con-\ntextual approach for creating embedding vector represen-\ntations is proposed in [ 9], where complete sentences,\ninstead of words, are mapped to a latent vector space. The\napproach provides two variations of universal sentence\nencoder (USE) with some trade-offs in computation and\naccuracy. The ﬁrst approach consists of a computationally\nintensive transformer that resembles a transformer network\n[92], proved to achieve higher performance ﬁgures. In\ncontrast, the second approach provides a lightweight model\nthat averages input embedding weights for words and bi-\n17312 Neural Computing and Applications (2020) 32:17309–17320\n123\ngrams by utilizing of a deep average network (DAN) [ 41].\nThe output of the DAN is passed through a feed-forward\nneural network in order to produce the sentence embed-\ndings. Both approaches take as input lowercased PTB\ntokenized\n1 strings and output a 512-dimensional sentence\nembedding vectors.\n3.2 Transformer methods\nSequence-to-sequence (seq2seq) methods using encoder-\ndecoder schemes are a popular choice for several tasks\nsuch as machine translation, text summarization and\nquestion answering [ 88]. However, encoder’s contextual\nrepresentations are uncertain when dealing with long-range\ndependencies. To address these drawbacks, Vaswani et al.\n[92] introduced a novel network architecture, called\ntransformer, relying entirely on self-attention units to map\ninput sequences to output sequences without the use of\nRNNs. The transformer’s decoder unit architecture con-\ntains a masked multi-head attention layer, followed by a\nmulti-head attention unit and a feed-forward network,\nwhereas the decoder unit is almost identical without the\nmasked attention unit. Multi-head self-attention layers are\ncalculated in parallel facing the computational costs of\nregular attention layers used by previous seq2seq network\narchitectures. In [ 17] the authors presented a model that is\nfounded on ﬁndings from various previous studies (e.g.,\n[14, 38, 73, 77, 92]), which achieved state-of-the-art results\non eleven NLP tasks, called BERT—bidirectional encoder\nrepresentations from transformers. The BERT training\nprocess is split into two phases: the unsupervised pre-\ntraining phase and the ﬁne-tuning phase using labeled data\nfor down-streaming tasks. In contrast with previous pro-\nposed models (e.g., [73, 77]), BERT uses masked language\nmodels (MLMs) to enable pre-trained deep bidirectional\nrepresentations. In the pre-training phase, the model is\ntrained with a large amount of unlabeled data from Wiki-\npedia, BookCorpus [104] and WordPiece [98] embeddings.\nIn this training part, the model was tested on two tasks; on\nthe ﬁrst task, the model randomly masks 15% of the input\ntokens aiming to capture conceptual representations of\nword sequences by predicting masked words inside the\ncorpus, whereas in the second task, the model is given two\nsentences and tries to predict whether the second sentence\nis the next sentence of the ﬁrst. In the second phase, BERT\nis extended with a task-related classiﬁer model that is\ntrained on a supervised manner. During this supervised\nphase, the pre-trained BERT model receives minimal\nchanges, with the classiﬁer’s parameters trained in order to\nminimize the loss function. Two models presented in [ 17],\na ‘‘Base Bert’’ model with 12 encoder layers (i.e.,\ntransformer blocks), feed-forward networks with 768 hid-\nden units and 12 attention heads, and a ‘‘Large Bert’’ model\nwith 24 encoder layers 1024 feed-the pre-trained Bert\nmodel, an architecture almost identical with the afore-\nmentioned transformer network. A [CLS] token is supplied\nin the input as the ﬁrst token, the ﬁnal hidden state of which\nis aggregated for classiﬁcation tasks. Despite the achieved\nbreakthroughs, the BERT model suffers from several\ndrawbacks. Firstly, BERT, as all language models using\ntransformers, assumes (and pre-supposes) independence\nbetween the masked words from the input sequence, and\nneglects all the positional and dependency information\nbetween words. In other words, for the prediction of a\nmasked token both word and position embeddings are\nmasked out, even if positional information is a key-aspect\nof NLP [ 15]. In addition, the [MASK] token, which is\nsubstituted with masked words, is mostly absent in ﬁne-\ntuning phase for down-streaming tasks, leading to a pre-\ntraining ﬁne-turning discrepancy. To address the cons of\nBERT, a permutation language model was introduced, so-\ncalled XLnet, trained to predict masked tokens in a non-\nsequential random order, factorizing likelihood in an\nautoregressive manner without the independence assump-\ntion and without relying on any input corruption [ 100]. In\nparticular, a query stream is used that extends embedding\nrepresentations to incorporate positional information about\nthe masked words. The original representation set (content\nstream), including both token and positional embeddings,\nis then used as input to the query stream following a\nscheme called ‘‘Two-Stream SelfAttention’’. To overcome\nthe problem of slow convergence, the authors propose the\nprediction of the last token in the permutation phase,\ninstead of predicting the entire sequence. Finally, XLnet\nuses also a special token for the classiﬁcation and separa-\ntion of the input sequence, [CLS] and [SEP], respectively;\nhowever, it also learns an embedding that denotes whether\nthe two words are from the same segment. This is similar to\nrelative positional encodings introduced in TrasformerXL\n[15], and extents the ability of XLnet to cope with tasks\nthat encompass arbitrary input segments. Recently, a\nreplication study [ 59], suggested several modiﬁcations in\nthe training procedure of BERT which outperforms the\noriginal XLNet architecture on several NLP tasks. The\noptimized model, called robustly optimized BERT\napproach (RoBERTa), used 10 times more data (160 GB\ncompared with the 16 GB originally exploited), and is\ntrained with far more epochs than the BERT model (500 K\nvs. 100 K), using also 8 times larger batch sizes, and a\nbyte-level BPE vocabulary instead of the character-level\nvocabulary that was previously utilized. Another signiﬁ-\ncant modiﬁcation was the dynamic masking technique\ninstead of the single static mask used in BERT. In addition,\nRoBERTa model removes the next sentence prediction\n1 https://nlp.stanford.edu/software/tokenizer.html.\nNeural Computing and Applications (2020) 32:17309–17320 17313\n123\nobjective used in BERT, following advises by several other\nstudies that question the NSP loss term [ 44, 55, 101].\n4 Proposed method: recurrent CNN\nRoBERTA (RCNN-RoBERTa)\nThe intuition behind our proposed RCNN-RoBERTa\napproach is founded on the following observation: As pre-\ntrained networks are beneﬁcial for several down-streaming\ntasks, their outputs could be further enhanced if processed\nproperly by other networks. Toward this end, we devised\nan end-to-end model that utilizes pre-trained RoBERTa\n[59] weights combined with a RCNN in order to capture\ncontextual information. The RoBERTa network architec-\nture is utilized in order to efﬁciently map words onto a rich\nembedding space. To improve RoBERTa’s performance\nand identify FL within a sentence, it is essential to capture\nthe dependencies within RoBERTa’s pre-trained word-\nembeddings. This task can be tackled with an RNN layer\nsuited to capture temporal reliant information, in contrast,\nto fully-connected and 1D convolution layers that are not\nable to delineate with such dependencies. In addition,\naiming to enhance the proposed network architecture, the\nRNN layer is followed with a fully connected layer that\nsimulates 1D convolution with a large kernel (see below),\nwhich is capable to capture spatiotemporal dependencies in\nRoBERTa’s projected latent space. Actually, the proposed\nleaning model is based on a hybrid DL neural architecture\nthat utilizes pre-trained transformer models and feed the\nhidden representations of the transformer into a recurrent\nconvolutional neural network (RCNN), similar to [ 54]. In\nparticular, we employed the RoBERTa base model with 12\nhidden states and 12 attention heads, and used its output\nhidden states as an embedding layer to a RCNN. As already\nstated, contradictions and long-time dependencies within a\nsentence may be used as strong identiﬁers of FL expres-\nsions. RNNs are often used to capture temporal relation-\nships between words. However they are strongly biased,\ni.e., later words are tending to be more dominant that\nprevious ones. This problem can be alleviated with CNNs,\nwhich, as unbiased models, can determine semantic rela-\ntionships between words with max-pooling [ 54, 65]. Nev-\nertheless, contextual information in CNNs is depended\ntotally on kernel sizes. Thus, we appropriately modiﬁed the\nRCNN model presented in [ 54] in order to capture unbi-\nased recurrent informative relationships within text. In\nparticular, we implemented a bidirectional LSTM\n(BiLSTM) layer, which is fed with RoBERTa’s ﬁnal hid-\nden layer weights. The output of LSTM is concatenated\nwith the embedded weights, and passed through a feed-\nforward network, acting as a 1D convolution layer with\nlarge kernel, and a max-pooling layer. Finally, softmax\nfunction is used for the output layer. Table 1 shows the\nparameters used in training, and Fig. 1 illustrates the pro-\nposed deep network architecture.\n5 Experimental results\nTo assess the performance of the proposed method, we\nperformed an exhaustive comparison with several\nadvanced state-of-the-art methodologies along with pub-\nlished results. Nowadays trends in NLP community tend to\nexplicitly utilize deep learning methodologies as the most\nconvenient way to approach various semantic analysis\ntasks. In the past decade, RNNs such as LSTM and GRUs\nwere the most popular choice, whereas the last years the\nimpact of attention-based models such as transformers\nseems to outperform all previous methods, even by a large\nmargin [ 17, 92]. On the contrary, classical machine\nlearning algorithms such as SVM, k-nearest neighbors\n(kNN) and tree-based models (decision trees, random for-\nest) have been considered inappropriate for real-world\napplications, due to their demand on hand-crafted feature\nextraction and exhaustive preprocessing strategies. In order\nto have a reasonable kNN or SVM algorithm, there should\nbe a lot of effort to embed sentences on word level to a\nhigher space that a classiﬁer may recognize patterns. In\nsupport of the arguments made, in our previous study [ 76],\nclassical machine learning algorithms supported with rich\nand informative features failed to compete deep learning\nmethodologies and proved non-feasible to FL detection. To\nthis end, in this study we acquired several state-of-the-art\nmodels to compare our proposed method. The used\nmethodologies were appropriately implemented using the\navailable codes and guidelines, and include: ELMo [ 73],\nTable 1 Selected hyperparameters used in our proposed method\nRCNN-RoBERTa\nHyperparameter Value\nRoBERTa layers 12\nRoBERTa attention heads 12\nLSTM units 64\nLSTM dropout 0.1\nBatch size 10\nAdam epsilon 1e -6\nEpochs 5\nLearning rate 2e -5\nWeight decay 1e -5\nThe hyperparameters were settled following a grid search based on a\nﬁvefold cross-validation process; the ﬁnally selected parameters are\nthe ones that exhibit the best performance\n17314 Neural Computing and Applications (2020) 32:17309–17320\n123\nUSE [ 9], NBSVM [ 94], FastText [ 45], XLnet base cased\nmodel (XLnet) [ 100], BERT [ 17] in two setups: BERT\nbase cased (BERT-Cased) and BERT base uncased\n(BERT-Uncased) models, and RoBERTa base model [ 59].\nThe settings and the hyper-parameters used for training the\naforementioned models can be found in ‘‘Appendix’’. The\npublished results were acquired from the respective origi-\nnal publication (the reference publication is indicated in the\nrespective tables). For the comparison we utilized bench-\nmark datasets that include ironic, sarcastic and metaphoric\nexpressions. Namely, we used the dataset provided in\n‘‘Semantic Evaluation Workshop Task 3’’ (SemEval-2018)\nthat contains ironic tweets [ 35]; Riloff’s high-quality sar-\ncastic unbalanced dataset [ 82]; a large dataset containing\npolitical comments from Reddit [48]; and a SA dataset that\ncontains tweets with various FL forms from ‘‘SemEval-\n2015 Task 11’’ [ 24]. All datasets are used in a binary\nclassiﬁcation manner (i.e., irony/sarcasm vs. literal), except\nfrom thec‘‘SemEval-2015 Task 11’’ dataset where the task\nis to predict a sentiment integer score (from - 5 to 5) for\neach tweet (refer to [ 76] for more details). For a fair\ncomparison, we split the datasets on train/test stets as\nproposed by the authors providing the datasets or by fol-\nlowing the settings of the respective published studies. The\nevaluation was made across standard ﬁve metrics, namely\naccuracy (Acc), precision (Pre), recall (Rec), F1-score (F1)\nand area under the receiver operating characteristics curve\n(AUC). For the SA task the cosine similarity metric (Cos)\nand mean squared error (MSE) metrics are used, as pro-\nposed in the original study [ 24].\nThe results are summarized in Tables 2, 3, 4 and 5; each\ntable refers to the respective comparison study. All\ntables present the performance results of our proposed\nmethod (‘‘Proposed’’) and contrast them to eight state-of-\nthe-art baseline methodologies along with published results\nusing the same dataset. Speciﬁcally, Table 2 presents the\nresults obtained using the ironic dataset used in SemEval-\n2018 Task 3.A, compared with recently published studies\nand two high performing teams from the respective\nSemEval shared task [ 5, 97]. Tables 3 and 4 summarize\nresults obtained using Sarcastic datasets (Reddit SARC\npolitics [ 48] and Riloff Twitter [ 82]). Finally, Table 5\ncompares the results from baseline models, from top two\nFig. 1 The proposed RCNN-RoBERTa methodology, consisting of a\nRoBERTa pre-trained transformer followed by a bidirectional LSTM\nlayer (BiLSTM). Pooling is applied to the representation vector of\nconcatenated RoBERTa and LSTM outputs and passed through a\nfully connected softmax-activated layer. We refer the reader to\n[59, 92] for RoBERTa transformer-based architecture\nTable 2 Comparison of RCNN-RoBERTa with state-of-the-art neural\nnetwork classiﬁers and published results on SemEval-2018 dataset\nIrony/SemVal-2018-Task 3.A [ 35]\nSystem Acc Pre Rec F1 AUC\nELMo 0.66 0.66 0.67 0.66 0.72\nUSE 0.69 0.67 0.67 0.67 0.74\nNBSVM 0.69 0.70 0.69 0.69 0.73\nFastText 0.69 0.71 0.69 0.69 0.73\nXLnet 0.71 0.71 0.72 0.70 0.80\nBERT-Cased 0.70 0.69 0.70 0.69 0.77\nBERT-Uncased 0.69 0.68 0.69 0.68 0.77\nRoBERTa 0.79 0.78 0.79 0.78 0.89\nWu et al. [ 97] 0.74 0.63 0.80 0.71 –\nIlic´ et al. [ 40] 0.71 0.70 0.70 0.70 –\nTHU_NGN [97] 0.73 0.63 0.80 0.71 –\nNTUA-SLP [5] 0.73 0.65 0.69 0.67 –\nZhang et al. [ 102] – – – 0.71 –\nDESC [76] 0.74 0.73 0.73 0.73 0.78\nProposed 0.82 0.81 0.80 0.80 0.89\nBold ﬁgures indicate superior performance\nTable 3 Comparison of RCNN-RoBERTa with state-of-the-art neural\nnetwork classiﬁers and published results on Reddit Politics dataset\nReddit SARC2.0 politics [ 48]\nSystem Acc Pre Rec F1 AUC\nELMo 0.70 0.70 0.70 0.70 0.77\nUSE 0.75 0.75 0.75 0.75 0.82\nNBSVM 0.65 0.65 0.65 0.65 0.68\nFastText 0.63 0.65 0.61 0.63 0.64\nXLnet 0.76 0.77 0.74 0.76 0.83\nBERT-Cased 0.76 0.76 0.75 0.76 0.84\nBERT-Uncased 0.77 0.77 0.77 0.77 0.84\nRoBERTa 0.77 0.77 0.77 0.77 0.85\nCASCADE [34] 0.74 – – 0.75 –\nIlic´ et al. [ 40] 0.79 – – – –\nKhodak et al. [ 48] 0.77 – – – –\nProposed 0.79 0.78 0.78 0.78 0.85\nBold ﬁgures indicate superior performance\nNeural Computing and Applications (2020) 32:17309–17320 17315\n123\nranked task participants [ 3, 69], from our previous study\nwith the DESC methodology [ 76] with the proposed\nRCNN-RoBERTa framework on a Sentiment Analysis task\nwith ﬁgurative language, using the SemEval 2015 Task 11\ndataset.\nAs it can be easily observed, the proposed RCNN-\nRoBERTa approach outperforms all approaches as well as\nall methods with published results, for the respective binary\nclassiﬁcation tasks (Tables 2, 3, 4). In particular, the\nRCNN architecture seems to reinforce RoBERTa model by\n2–5% F1 score, increasing also the classiﬁcation\nconﬁdence, in terms of AUC performance. Note also that\nRoBERTa-RCNN show better behavior, compared to\nRoBERTa, on imbalanced datasets (Riloff [ 82], SemEval-\n2015 [ 24]). Also, one-way ANOVA Tukey test [ 64]\nrevealed that RoBERTa-RCNN model outperforms by a\nstatistical signiﬁcant margin the maximum values of all\nmetrics of previously published approaches, i.e., p ¼\n0:015; p\\0:05 for ironic tweets and p ¼ 0:003; p\\0:01\nfor Riloff sarcastic tweets. Furthermore, the proposed\nmethod increased the state-of-the-art performance even by\na large margin in terms of accuracy, F1 and AUC score.\nOur previous approach, DESC (introduced in [ 76]), per-\nforms slightly better in terms of cosine similarity for the\nsentiment scoring task (Table 5, 0.820 vs. 0.810), with the\nRCNN-RoBERTa approach to perform better and manag-\ning to signiﬁcantly improve the MSE measure by almost\n33.5% (2.480 vs. 1.450).\n6 Conclusion\nIn this study, we propose the ﬁrst transformer based\nmethodology, leveraging the pre-trained RoBERTa model\ncombined with a recurrent convolutional neural network, to\ntackle ﬁgurative language in social media. Our network is\ncompared with all, to the best of our knowledge, published\napproaches under four different benchmark dataset. In\naddition, we aim to minimize preprocessing and engineered\nfeature extraction steps which are, as we claim, unnecessary\nwhen using overly trained deep learning methods such as\ntransformers. In fact, handcrafted features along with pre-\nprocessing techniques such as stemming and tagging on\nhuge datasets containing thousands of samples are almost\nprohibited in terms of their computation cost. Our proposed\nmodel, RCNN-RoBERTa, achieves state-of-the-art perfor-\nmance under six metrics over four benchmark dataset,\ndenoting that transfer learning non-literal forms of language.\nMoreover, RCNN-RoBERTa model outperforms all other\nstate-of-the-art approaches tested including BERT, XLnet,\nELMo and USE under all metric, some by a large factor.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nTable 4 Comparison of RCNN-RoBERTa with state-of-the-art neural\nnetwork classiﬁers and published results on Sarcastic Rillof’s dataset\nRiloff sarcastic dataset [ 82]\nSystem Acc Pre Rec F1 AUC\nELMo 0.85 0.85 0.86 0.85 0.89\nUSE 0.87 0.81 0.76 0.78 0.89\nNBSVM 0.75 0.59 0.57 0.58 0.60\nFastText 0.83 0.83 0.61 0.64 0.85\nXLnet 0.86 0.88 0.86 0.86 0.92\nBERT-Cased 0.86 0.87 0.85 0.86 0.91\nBERT-Uncased 0.87 0.88 0.87 0.87 0.91\nRoBERTa 0.89 0.85 0.84 0.85 0.91\nFarı´as et al. [ 20] – – – 0.75 –\nIlic´ et al. [ 40] 0.86 0.78 0.77 0.75 –\nTay et al. [ 89] 0.82 0.74 0.73 0.73 –\nDESC [76] 0.87 0.86 0.87 0.87 0.86\nGhosh and Veale [ 25] – 0.88 0.88 0.88 –\nProposed 0.91 0.90 0.90 0.90 0.94\nBold ﬁgures indicate superior performance\nTable 5 Comparison of RCNN-\nRoBERTa with state-of-the-art\nneural network classiﬁers and\npublished results on Task11—\nSemEval-2015 dataset\n(sentiment analysis of ﬁgurative\nlanguage expression)\nSemEval-2015 Task 11 [ 24]\nSystem COS MSE\nELMo 0.710 3.610\nUSE 0.71 3.17\nNBSVM 0.69 3.23\nFastText 0.72 2.99\nXLnet 0.76 1.84\nBERT-Cased 0.72 1.97\nBERT-Uncased 0.79 1.54\nRoBERTa 0.78 1.55\nUPF [3] 0.71 2.46\nClaC [69] 0.76 2.12\nDESC [76] 0.82 2.48\nProposed 0.81 1.45\nBold ﬁgures indicate superior\nperformance\n17316 Neural Computing and Applications (2020) 32:17309–17320\n123\nAppendix\nIn our experiments we compared our model with several\nseven different classiﬁers under different settings. For the\nELMo system, we used the mean-pooling of all contextual-\nized word representations, i.e., character-based embedding\nrepresentations and the output of the two layer LSTM\nresulting with a 1024-dimensional vector, and passed it\nthrough two deep dense ReLu activated layers with 256 and\n64 units. Similarly, USE embeddings are trained with a\ntransformer encoder and output 512-dimensional vector for\neach sample, which is also passed through two deep dense\nReLu activated layers with 256 and 64 units. Both ELMo and\nUSE embeddings retrieved from TensorFlow Hub.\n2 NBSVM\nsystem was modiﬁed according to [ 94] and trained with a\n10/C0 3 leaning rate for 5 epochs with Adam optimizer [ 50].\nFastText system was implemented by utilizing pre-trained\nembeddings [45] passed through a global max-pooling and a\n64 unit fully connected layer. System was trained with Adam\noptimizer with learning rate 0.1 for 3 epochs. XLnet model\nimplemented using the base-cased model with 12 layers, 768\nhidden units and 12 attention heads. Model trained with\nlearning rate 4/C2 10\n/C0 5 using 10/C0 5 weight decay for 3 epochs.\nWe exploited both cased and uncased BERT-base models\ncontaining 12 layers, 768 hidden units and 12 attention heads.\nWe trained models for 3 epochs with learning rate 2 /C2 10/C0 5\nusing 10/C0 5 weight decay. We trained RoBERTa model fol-\nlowing the setting of BERT model. RoBERTa, XLnet and\nBERT models implemented usi ng pytorch-transformers\nlibrary\n3 and were topped with two dense fully connected\nlayers used as the output classiﬁer.\nReferences\n1. Amir S, Wallace BC, Lyu H, Silva PCMJ (2016) Modelling\ncontext with user embeddings for sarcasm detection in social\nmedia. arXiv preprint arXiv:1607.00976\n2. Antonakaki D, Spiliotopoulos D, Samaras CV, Pratikakis P,\nIoannidis S, Fragopoulou P (2017) Social media analysis during\npolitical turbulence. PLoS ONE 12(10):1–23\n3. Barbieri F, Ronzano F, Saggion H (2015) UPF-taln: SemEval\n2015 tasks 10 and 11. Sentiment analysis of literal and ﬁgurative\nlanguage in Twitter. In: Proceedings of the 9th international\nworkshop on semantic evaluation (SemEval 2015). Association\nfor Computational Linguistics, Denver, pp 704–708\n4. Barbieri F, Saggion H (2014) Modelling irony in Twitter. In:\nEACL\n5. Baziotis C, Nikolaos A, Papalampidi P, Kolovou A, Para-\nskevopoulos G, Ellinas N, Potamianos A (2018) NTUA-SLP at\nSemEval-2018 task 3: tracking ironic tweets using ensembles of\nword and character level attentive RNNs. In: Proceedings of the\n12th international workshop on semantic evaluation. Associa-\ntion for Computational Linguistics, New Orleans, pp 613–621\n6. Benedek M, Beaty R, Jauk E, Koschutnig K, Fink A, Silvia PJ,\nDunst B, Neubauer AC (2014) Creating metaphors: the neural\nbasis of ﬁgurative language production. NeuroImage 90:99–106\n7. Buschmeier K, Cimiano P, Klinger R (2014) An impact analysis\nof features in a classiﬁcation approach to irony detection in\nproduct reviews. In: Proceedings of the 5th workshop on com-\nputational approaches to subjectivity, sentiment and social\nmedia analysis. Association for Computational Linguistics,\nBaltimore, pp 42–49\n8. Carvalho P (2009) Clues for detecting irony in user-generated\ncontents: Oh...!! it’s ‘‘so easy. In: International CIKM workshop\non topic-sentiment analysis for mass opinion measurement,\nHong Kong\n9. Cer D, Yang Y, Kong SY, Hua N, Limtiaco N, John RS, Con-\nstant N, Guajardo-Cespedes M, Yuan S, Tar C et al (2018)\nUniversal sentence encoder. arXiv preprint arXiv:1803.11175\n10. Charalampakis B, Spathis D, Kouslis E, Kermanidis K (2016) A\ncomparison between semi-supervised and supervised text min-\ning techniques on detecting irony in greek political tweets. Eng\nAppl Artif Intell 51:50–57\n11. Chelba C, Mikolov T, Schuster M, Ge Q, Brants T, Koehn P,\nRobinson T (2013) One billion word benchmark for measuring\nprogress in statistical language modeling. arXiv preprint arXiv:\n1312.3005\n12. Clark HH, Gerrig RJ (1984) On the pretense theory of irony.\nJ Exp Psychol Gen 113:121–126\n13. Cuccio V, Ambrosecchia M, Ferri F, Carapezza M, Piparo FL,\nFogassi L, Gallese V (2014) How the context matters. Literal\nand ﬁgurative meaning in the embodied language paradigm.\nPLoS ONE 9(12):e115381\n14. Dai AM, Le QV (2015) Semi-supervised sequence learning. In:\nAdvances in Neural Information Processing Systems,\npp 3079–3087\n15. Dai Z, Yang Z, Yang Y, Cohen WW, Carbonell J, Le QV,\nSalakhutdinov R (2019) Transformer-xl: attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint arXiv:\n1901.02860\n16. Davidov D, Tsur O, Rappoport A (2010) Semi-supervised\nrecognition of sarcastic sentences in Twitter and Amazon. In:\nProceedings of the fourteenth conference on computational\nnatural language learning, CoNLL ’10. Association for Com-\nputational Linguistics, Stroudsburg, pp 107–116\n17. Devlin J, Chang MW, Lee K, Toutanova K (2019) BERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding. In: Proceedings of the 2019 conference of the North\nAmerican Chapter of the Association for Computational Lin-\nguistics: human language technologies, volume 1 (long and\nshort papers). Association for Computational Linguistics, Min-\nneapolis, pp 4171–4186\n18. Dridi A, Recupero DR (2019) Leveraging semantics for senti-\nment polarity detection in social media. Int J Mach Learn\nCybern 10(8):2045–2055\n19. Dubey A, Kumar L, Somani A, Joshi A, Bhattacharyya P (2019)\n‘‘When numbers matter!’’: detecting sarcasm in numerical por-\ntions of text. In: Proceedings of the tenth workshop on com-\nputational approaches to subjectivity, sentiment and social\nmedia analysis, pp 72–80\n20. Farı´as DIH, Montes-y-Go´mez M, Escalante HJ, Rosso P, Patti V\n(2018) A knowledge-based weighted KNN for detecting irony in\nTwitter. In: Mexican international conference on artiﬁcial\nintelligence. Springer, Berlin, pp 194–206\n21. Farı´as DIH, Patti V, Rosso P (2016) Irony detection in Twitter:\nthe role of affective content. ACM Trans Internet Technol\n(TOIT) 16(3):19\n2 https://tfhub.dev/s?module-type=text-embedding.\n3 https://huggingface.co/transformers/.\nNeural Computing and Applications (2020) 32:17309–17320 17317\n123\n22. Ganin Y, Ustinova E, Ajakan H, Germain P, Larochelle H,\nLaviolette F, Marchand M, Lempitsky V (2016) Domain-ad-\nversarial training of neural networks. J Mach Learn Res\n17(1):2096–2030\n23. Gao Z, Gao S, Xu L, Zheng X, Ma X, Luo L, Kendrick KM\n(2017) Women prefer men who use metaphorical language when\npaying compliments in a romantic context. Sci Rep 7:40871\n24. Ghosh A, Li G, Veale T, Rosso P, Shutova E, Barnden J, Reyes\nA (2015) SemEval-2015 task 11: sentiment analysis of ﬁgurative\nlanguage in Twitter. In: Proceedings of the 9th international\nworkshop on semantic evaluation (SemEval 2015). Association\nfor Computational Linguistics, Denver, pp 470–478\n25. Ghosh A, Veale T (2016) Fracking sarcasm using neural net-\nwork. In: Proceedings of the 7th workshop on computational\napproaches to subjectivity, sentiment and social media analysis,\npp 161–169\n26. Ghosh D, Guo W, Muresan S (2015) Sarcastic or not: word\nembeddings to predict the literal or sarcastic meaning of words.\nIn: EMNLP\n27. Gime´nez M, Pla F, Hurtado LF (2015) ELiRF: a SVM approach\nfor SA tasks in Twitter at SemEval-2015. In: Proceedings of the\n9th international workshop on semantic evaluation (SemEval\n2015). Association for Computational Linguistics, Denver,\npp 574–581\n28. Gonza´ilez-Iba´n˜ez RI, Muresan S, Wacholder N (2011) Identi-\nfying sarcasm in Twitter: a closer look. In: ACL\n29. Goodfellow I, Bengio Y, Courville A (2016) Deep learning.\nMIT Press, Cambridge\n30. Grice HP (2008) Further notes on logic and conversation. In:\nAdler JE, Rips LJ (eds) Reasoning: studies of human inference\nand its foundations. Cambridge University Press, Cambridge,\npp 765–773\n31. Gupta U, Chatterjee A, Srikanth R, Agrawal P (2017) A senti-\nment-and-semantics-based approach for emotion detection in\ntextual conversations\n32. Gibbs RW (1986) On the psycholinguistics of sarcasm. J Exp\nPsychol Gen 115:3–15\n33. Hangya V, Farkas R (2017) A comparative empirical study on\nsocial media sentiment analysis over various genres and lan-\nguages. Artif Intell Rev 47(4):485–505\n34. Hazarika D, Poria S, Gorantla S, Cambria E, Zimmermann R,\nMihalcea R (2018) Cascade: contextual sarcasm detection in\nonline discussion forums. arXiv preprint arXiv:1805.06413\n35. Hee CV, Lefever E, Hoste V (2018) SemEval-2018 task 3: irony\ndetection in English tweets. In: SemEval@NAACL-HLT\n36. Hiai S, Shimada K (2018) Sarcasm detection using features\nbased on indicator and roles. In: International conference on soft\ncomputing and data mining. Springer, Berlin, pp 418–428\n37. Howard J, Ruder S (2018) Universal language model ﬁne-tuning\nfor text classiﬁcation. In: Proceedings of the 56th annual\nmeeting of the Association for Computational Linguistics (vol-\nume 1: long papers). Association for Computational Linguistics,\nMelbourne, pp 328–339\n38. Howard J, Ruder S (2018) Universal language model ﬁne-tuning\nfor text classiﬁcation. arXiv preprint arXiv:1801.06146\n39. Huang YH, Huang HH, Chen HH (2017) Irony detection with\nattentive recurrent neural networks. In: ECIR\n40. Ilic´ S, Marrese-Taylor E, Balazs JA, Matsuo Y (2018) Deep\ncontextualized word representations for detecting sarcasm and\nirony. arXiv preprint arXiv:1809.09795\n41. Iyyer M, Manjunatha V, Boyd-Graber J, Daume ´ III H (2015)\nDeep unordered composition rivals syntactic methods for text\nclassiﬁcation. In: Proceedings of the 53rd annual meeting of the\nAssociation for Computational Linguistics and the 7th interna-\ntional joint conference on natural language processing (volume\n1: long papers). Association for Computational Linguistics,\nBeijing, pp 1681–1691\n42. Jianqiang Z, Xiaolin G, Xuejun Z (2018) Deep convolution\nneural networks for Twitter sentiment analysis. IEEE Access\n6:23253–23260\n43. Joseph JK, Dev KA, Pradeepkumar AP, Mohan M (2018)\nChapter 16—Big data analytics and social media in disaster\nmanagement. In: Samui P, Kim D, Ghosh CBTIDS (eds) Inte-\ngrating disaster science and management. Elsevier, Amsterdam,\npp 287–294\n44. Joshi M, Chen D, Liu Y, Weld DS, Zettlemoyer L, Levy O\n(2019) Spanbert: improving pre-training by representing and\npredicting spans. arXiv preprint arXiv:1907.10529\n45. Joulin A, Grave E, Bojanowski P, Douze M, Je ´gou H, Mikolov\nT (2016) Fasttext. zip: compressing text classiﬁcation models.\narXiv preprint arXiv:1612.03651\n46. Kasparian K (2013) Hemispheric differences in ﬁgurative lan-\nguage processing: contributions of neuroimaging methods and\nchallenges in reconciling current empirical ﬁndings. J Neuroling\n26:1–21\n47. Katz JJ (1977) Propositional structure and illocutionary force: a\nstudy of the contribution of sentence meaning to speech acts/\nJerrold J. Katz. The Language and thought series. Crowell, New\nYork\n48. Khodak M, Saunshi N, Vodrahalli K (2017) A large self-anno-\ntated corpus for sarcasm. arXiv e-prints\n49. Kim E, Klinger R (2018) A survey on sentiment and emotion\nanalysis for computational literary studies\n50. Kingma DP, Ba J (2014) Adam: a method for stochastic opti-\nmization. arXiv e-prints\n51. Kumar A, Garg G (2019) Empirical study of shallow and deep\nlearning models for sarcasm detection using context in bench-\nmark datasets. J Ambient Intell Humaniz Comput 1–16\n52. Kumar A, Sangwan SR, Arora A, Nayyar A, Abdel-Basset M\net al (2019) Sarcasm detection using soft attention-based bidi-\nrectional long short-term memory model with convolution net-\nwork. IEEE Access 7:23319–23328\n53. Kumar L, Somani A, Bhattacharyya P (2017) ‘ ‘Having 2 hours\nto write a paper is fun!’’: detecting sarcasm in numerical por-\ntions of text. arXiv e-prints\n54. Lai S, Xu L, Liu K, Zhao J (2015) Recurrent convolutional\nneural networks for text classiﬁcation. In: Twenty-ninth AAAI\nconference on artiﬁcial intelligence\n55. Lample G, Conneau A (2019) Cross-lingual language model\npretraining. arXiv preprint arXiv:1901.07291\n56. Lazer D, Pentland A, Adamic L, Aral S, Barabasi AL, Brewer\nD, Christakis N, Contractor N, Fowler J, Gutmann M, Jebara T,\nKing G, Macy M, Roy D, Van Alstyne M (2009) Life in the\nnetwork: the coming age of computational social science. Sci-\nence (New York, N. Y.) 323(5915):721–723\n57. Ling J, Klinger R (2016) An empirical, quantitative analysis of\nthe differences between sarcasm and irony. In: European\nsemantic web conference. Springer, Berlin, pp 203–216\n58. Liu B (2015) Sentiment analysis—mining opinions, sentiments,\nand emotions. Cambridge University Press, Cambridge\n59. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis\nM, Zettlemoyer L, Stoyanov V (2019) Roberta: a robustly\noptimized bert pretraining approach. arXiv preprint arXiv:1907.\n11692\n60. Loenneker-Rodman B, Narayanan S (2010) Computational\napproaches to ﬁgurative language. Cambridge Encyclopedia of\nPsycholinguistics. Cambridge University Press, Cambridge\n61. McCann B, Bradbury J, Xiong C, Socher R (2017) Learned in\ntranslation: Contextualized word vectors. In: Advances in\nNeural Information Processing Systems, pp 6294–6305\n17318 Neural Computing and Applications (2020) 32:17309–17320\n123\n62. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient esti-\nmation of word representations in vector space. arXiv e-prints\n63. Mikolov T, Sutskever I, Chen K, Corrado G, Dean J (2013)\nDistributed representations of words and phrases and their\ncompositionality. arXiv e-prints\n64. Montgomery DC (2017) Design and analysis of experiments, 9th\nedn. Wiley, New York\n65. Nguyen TH, Grishman R (2015) Relation extraction: perspec-\ntive from convolutional neural networks. In: Proceedings of the\n1st workshop on vector space modeling for natural language\nprocessing. Association for Computational Linguistics, Denver,\npp 39–48\n66. Nozza D, Fersini E, Messina E (2016) Unsupervised irony\ndetection: a probabilistic model with word embeddings. In:\nKDIR, pp 68–76\n67. Oboler A, Welsh K, Cruz L (2012) The danger of big data:\nsocial media as computational social science. First Monday\n17(7)\n68. Ortega-Bueno R, Rangel F, Herna ´ndez Farıas D, Rosso P,\nMontes-y-Go´mez M, Medina Pagola JE (2019) Overview of the\ntask on irony detection in Spanish variants. In: Proceedings of\nthe Iberian languages evaluation forum (IberLEF 2019), co-lo-\ncated with 34th conference of the Spanish Society for natural\nlanguage processing (SEPLN 2019). CEUR-WS.org\n69. O¨ zdemir C, Bergler S (2015) CLaC-SentiPipe: SemEval2015\nsubtasks 10 B, E, and task 11. In: Proceedings of the 9th\ninternational workshop on semantic evaluation (SemEval 2015).\nAssociation for Computational Linguistics, Denver, pp 479–485\n70. Pennebaker J, Francis M (1999) Linguistic inquiry and word\ncount. Lawrence Erlbaum Associates, Incorporated, Mahwah\n71. Pennington J, Socher R, Manning CD (2014) Glove: global\nvectors for word representation. EMNLP 14:1532–1543\n72. Peters ME, Ammar W, Bhagavatula C, Power R (2017) Semi-\nsupervised sequence tagging with bidirectional language mod-\nels. arXiv preprint arXiv:1705.00108\n73. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K,\nZettlemoyer L (2018) Deep contextualized word representa-\ntions. arXiv preprint arXiv:1802.05365\n74. Potamias RA, Neofytou A, Siolas G (2019) NTUA-ISLab at\nSemEval-2019 task 9: mining suggestions in the wild. In: Pro-\nceedings of the 13th international workshop on semantic eval-\nuation. Association for Computational Linguistics, Minneapolis,\npp 1224–1230\n75. Potamias RA, Siolas G (2019) NTUA-ISLab at SemEval-2019\ntask 3: determining emotions in contextual conversations with\ndeep learning. In: Proceedings of the 13th international work-\nshop on semantic evaluation. Association for Computational\nLinguistics, Minneapolis, pp 277–281\n76. Potamias RA, Siolas G, Stafylopatis A (2019) A robust deep\nensemble classiﬁer for ﬁgurative language detection. In: Inter-\nnational conference on engineering applications of neural net-\nworks. Springer, Berlin, pp 164–175\n77. Radford A, Narasimhan K, Salimans T, Sutskever I (2018)\nImproving language understanding by generative pre-training\n78. Rajadesingan A, Zafarani R, Liu H (2015) Sarcasm detection on\nTwitter: a behavioral modeling approach. In: WSDM\n79. Ravi K, Ravi V (2017) A novel automatic satire and irony\ndetection using ensembled feature selection and data mining.\nKnowl Based Syst 120:15–33\n80. Reyes A, Rosso P, Buscaldi D (2012) From humor recognition\nto irony detection: the ﬁgurative language of social media. Data\nKnowl Eng 74:1–12\n81. Reyes A, Rosso P, Veale T (2013) A multidimensional approach\nfor detecting irony in Twitter. Lang Resour Eval 47(1):239–268\n82. Riloff E, Qadir A, Surve P, De Silva L, Gilbert N, Huang R\n(2013) Sarcasm as contrast between a positive sentiment and\nnegative situation. In: EMNLP 2013—2013 conference on\nempirical methods in natural language processing, proceedings\nof the conference. Association for Computational Linguistics\n(ACL), pp 704–714\n83. Rosenthal S, Ritter A, Nakov P, Stoyanov V (2014) SemEval-\n2014 task 9: sentiment analysis in Twitter. In: Proceedings of the\n8th international workshop on semantic evaluation (SemEval\n2014). Association for Computational Linguistics, Dublin,\npp 73–80\n84. Singh NK, Tomar DS, Sangaiah AK (2020) Sentiment analysis:\na review and comparative analysis over social media. J Ambient\nIntell Human Comput 11:97–117\n85. Sperber D, Wilson D (1981) Irony and the use-mention dis-\ntinction. In: Cole P (ed) Radical pragmatics. Academic Press,\nNew York, pp 295–318\n86. Stranisci M, Bosco C, Farias H, Irazu D, Patti V (2016)\nAnnotating sentiment and irony in the online italian political\ndebate on #labuonascuola. In: Tenth international conference on\nlanguage resources and evaluation LREC 2016. ELRA,\npp 2892–2899\n87. Sulis E, Farı´as DIH, Rosso P, Patti V, Ruffo G (2016) Figurative\nmessages and affect in Twitter: differences between #irony,\n#sarcasm and #not. Knowl Based Syst 108:132–143\n88. Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence\nlearning with neural networks. In: Advances in neural infor-\nmation processing systems, pp 3104–3112\n89. Tay Y, Luu AT, Hui SC, Su J (2018) Reasoning with sarcasm by\nreading in-between. In: Proceedings of the 56th annual meeting\nof the Association for Computational Linguistics (volume 1:\nlong papers). Association for Computational Linguistics, Mel-\nbourne, pp 1010–1020\n90. Van Hee C, Lefever E, Hoste V (2015) LT3: sentiment analysis\nof ﬁgurative tweets—piece of cake #notreally. In: Proceedings\nof the 9th international workshop on semantic evaluation\n(SemEval 2015). Association for Computational Linguistics,\nDenver, pp 684–688\n91. Van Hee C, Lefever E, Hoste V (2018) Exploring the ﬁne-\ngrained analysis and automatic detection of irony on Twitter.\nLang Resour Eval 52(3):707–731\n92. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez\nAN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. In:\nAdvances in neural information processing systems,\npp 5998–6008\n93. Wallace BC, Choe DK, Charniak E (2015) Sparse, contextually\ninformed models for irony detection: exploiting user commu-\nnities, entities and sentiment. In: ACL-IJCNLP 2015—53rd\nannual meeting of the Association for Computational Linguistics\n(ACL), proceedings of the conference, vol 1\n94. Wang S, Manning CD (2012) Baselines and bigrams: simple,\ngood sentiment and topic classiﬁcation. In: Proceedings of the\n50th annual meeting of the association for computational lin-\nguistics: short papers, vol 2. Association for Computational\nLinguistics, pp 90–94\n95. Weiland H, Bambini V, Schumacher PB (2014) The role of\nliteral meaning in ﬁgurative language comprehension: evidence\nfrom masked priming ERP. Front Hum Neurosci 8:583\n96. Winbey JP (2019) The social fact. The MIT Press, Cambridge\n97. Wu C, Wu F, Wu S, Liu J, Yuan Z, Huang Y (2018) THU\\_ngn\nat SemEval-2018 task 3: tweet irony detection with densely\nconnected LSTM and multi-task learning. In: SemE-\nval@NAACL-HLT\n98. Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W,\nKrikun M, Cao Y, Gao Q, Macherey K et al (2016) Google’s\nneural machine translation system: bridging the gap between\nhuman and machine translation. arXiv preprint arXiv:1609.\n08144\nNeural Computing and Applications (2020) 32:17309–17320 17319\n123\n99. Xu H, Santus E, Laszlo A, Huang CR (2015) LLT-PolyU:\nidentifying sentiment intensity in ironic tweets. In: Proceedings\nof the 9th international workshop on semantic evaluation\n(SemEval 2015). Association for Computational Linguistics,\nDenver, pp 673–678\n100. Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV\n(2019) Xlnet: generalized autoregressive pretraining for lan-\nguage understanding. arXiv preprint arXiv:1906.08237\n101. You Y, Li J, Hseu J, Song X, Demmel J, Hsieh CJ (2019)\nReducing bert pre-training time from 3 days to 76 min. arXiv\npreprint arXiv:1904.00962\n102. Zhang S, Zhang X, Chan J, Rosso P (2019) Irony detection via\nsentiment-based transfer learning. Inf Process Manag\n56(5):1633–1644\n103. Zhou L, Pan S, Wang J, Vasilakos AV (2017) Machine learning\non big data: opportunities and challenges. Neurocomputing\n237:350–361\n104. Zhu Y, Kiros R, Zemel R, Salakhutdinov R, Urtasun R, Torralba\nA, Fidler S (2015) Aligning books and movies: towards story-\nlike visual explanations by watching movies and reading books.\nIn: Proceedings of the IEEE international conference on com-\nputer vision, pp 19–27\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n17320 Neural Computing and Applications (2020) 32:17309–17320\n123"
}