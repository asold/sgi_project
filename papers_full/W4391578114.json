{
    "title": "HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer",
    "url": "https://openalex.org/W4391578114",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5103227553",
            "name": "Shuzhe Wang",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A5015862760",
            "name": "Zakaria Laskar",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A5081819688",
            "name": "Iaroslav Melekhov",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A5100785312",
            "name": "Xiaotian Li",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A5100458737",
            "name": "Yi Zhao",
            "affiliations": [
                "Aalto University"
            ]
        },
        {
            "id": "https://openalex.org/A5046083819",
            "name": "Giorgos Tolias",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A5057931031",
            "name": "Juho Kannala",
            "affiliations": [
                "Aalto University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2951019013",
        "https://openalex.org/W2892865870",
        "https://openalex.org/W2612112834",
        "https://openalex.org/W1677409904",
        "https://openalex.org/W3204873281",
        "https://openalex.org/W2556455135",
        "https://openalex.org/W2472269674",
        "https://openalex.org/W2963856988",
        "https://openalex.org/W3010479006",
        "https://openalex.org/W2987672160",
        "https://openalex.org/W3007117364",
        "https://openalex.org/W2795645133",
        "https://openalex.org/W2974696956",
        "https://openalex.org/W2803564000",
        "https://openalex.org/W1491719799",
        "https://openalex.org/W2982681214",
        "https://openalex.org/W2898255154",
        "https://openalex.org/W2593174349",
        "https://openalex.org/W4312974334",
        "https://openalex.org/W4225463530",
        "https://openalex.org/W3043075211",
        "https://openalex.org/W2983230029",
        "https://openalex.org/W2979458572",
        "https://openalex.org/W2085261163",
        "https://openalex.org/W3170946828",
        "https://openalex.org/W2081605477",
        "https://openalex.org/W1929856797",
        "https://openalex.org/W3166581875",
        "https://openalex.org/W3203518786",
        "https://openalex.org/W3034573343",
        "https://openalex.org/W2963024893",
        "https://openalex.org/W2605111497",
        "https://openalex.org/W2618011341",
        "https://openalex.org/W2200124539",
        "https://openalex.org/W2964175348",
        "https://openalex.org/W3017008958",
        "https://openalex.org/W2963765142",
        "https://openalex.org/W2964141169",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W2963059198",
        "https://openalex.org/W2963053725",
        "https://openalex.org/W6782813449",
        "https://openalex.org/W2604913662",
        "https://openalex.org/W4226142246",
        "https://openalex.org/W2602709638",
        "https://openalex.org/W2963523575",
        "https://openalex.org/W2963730218",
        "https://openalex.org/W2618039218",
        "https://openalex.org/W6802692370",
        "https://openalex.org/W2760103357",
        "https://openalex.org/W2963125676",
        "https://openalex.org/W2798302276",
        "https://openalex.org/W2991115635",
        "https://openalex.org/W2605947573",
        "https://openalex.org/W2792747672",
        "https://openalex.org/W2117228865",
        "https://openalex.org/W2892805284",
        "https://openalex.org/W2962705366",
        "https://openalex.org/W3034275286",
        "https://openalex.org/W3176602998",
        "https://openalex.org/W2129000642",
        "https://openalex.org/W153084048",
        "https://openalex.org/W2522940611",
        "https://openalex.org/W2740418457",
        "https://openalex.org/W2922243907",
        "https://openalex.org/W2519683295",
        "https://openalex.org/W3138197200",
        "https://openalex.org/W4312848448",
        "https://openalex.org/W1989476314",
        "https://openalex.org/W1869500417",
        "https://openalex.org/W3166285241",
        "https://openalex.org/W2963210849",
        "https://openalex.org/W2737260104",
        "https://openalex.org/W2962799344",
        "https://openalex.org/W2963272646",
        "https://openalex.org/W1893935112",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2584731199",
        "https://openalex.org/W3107540572",
        "https://openalex.org/W4214771064",
        "https://openalex.org/W2981873476",
        "https://openalex.org/W2948604360",
        "https://openalex.org/W2987570663",
        "https://openalex.org/W3035397262",
        "https://openalex.org/W1955055330",
        "https://openalex.org/W3175295430",
        "https://openalex.org/W3099342433",
        "https://openalex.org/W4297792979"
    ],
    "abstract": null,
    "full_text": "International Journal of Computer Vision (2024) 132:2530–2550\nhttps://doi.org/10.1007/s11263-023-01982-9\nHSCNet++: Hierarchical Scene Coordinate Classiﬁcation and\nRegression for Visual Localization with Transformer\nShuzhe Wang 1 · Zakaria Laskar 2 · Iaroslav Melekhov 1 · Xiaotian Li 1 · Yi Zhao 1 · Giorgos Tolias 2 · Juho Kannala 1\nReceived: 10 February 2023 / Accepted: 25 December 2023 / Published online: 6 February 2024\n© The Author(s) 2024\nAbstract\nVisual localization is critical to many applications in computer vision and robotics. To address single-image RGB localization,\nstate-of-the-art feature-based methods match local descriptors between a query image and a pre-built 3D model. Recently,\ndeep neural networks have been exploited to regress the mapping between raw pixels and 3D coordinates in the scene, and\nthus the matching is implicitly performed by the forward pass through the network. However, in a large and ambiguous\nenvironment, learning such a regression task directly can be difﬁcult for a single network. In this work, we present a new\nhierarchical scene coordinate network to predict pixel scene coordinates in a coarse-to-ﬁne manner from a single RGB image.\nThe proposed method, which is an extension of HSCNet, allows us to train compact models which scale robustly to large\nenvironments. It sets a new state-of-the-art for single-image localization on the 7-Scenes, 12-Scenes, Cambridge Landmarks\ndatasets, and the combined indoor scenes.\nKeywords Scene coordinate regression · Hierarchical classiﬁcation · Visual localization · Transformers\n1 Introduction\nEstimating the six degrees-of-freedom (6-DoF) camera pose\nfrom a given RGB image is a key component in many com-\nputer vision systems such as augmented reality, autonomous\nCommunicated by Xiaowei Zhou.\nB Shuzhe Wang\nShuzhe.Wang@aalto.ﬁ\nZakaria Laskar\nlaskazak@fel.cvut.cz\nIaroslav Melekhov\nIaroslav.Melekhov@aalto.ﬁ\nXiaotian Li\nXiaotian.Li@aalto.ﬁ\nYi Zhao\nYi.Zhao@aalto.ﬁ\nGiorgos Tolias\ntoliageo@fel.cvut.cz\nJuho Kannala\nJuho.Kannala@aalto.ﬁ\n1 Aalto University, Espoo, Finland\n2 Visual Recognition Group, Faculty of Electrical Engineering,\nCzech Technical University in Prague, Prague 6, Czechia\ndriving, and robotics. Classical methods (Sattler et al., 2011,\n2012, 2016a; Taira et al., 2018; Sarlin et al., 2019) establish\n2D-2D(-3D) correspondences between query and database\nlocal descriptors, followed by PnP-based camera pose esti-\nmation. Although powerful, these methods are memory and\ncomputationally inefﬁcient requiring to keep an immense\namount of local image descriptors and to perform hierarchi-\ncal descriptor matching in a RANSAC loop to infer camera\npose.\nOn the other hand, end-to-end pose regression methods\nthat directly regress the camera pose parameters are much\nfaster and scalable (Kendall et al., 2015; Balntas et al.,\n2018; Chen et al., 2021; Shavit & Keller, 2022). However,\nsuch methods are signiﬁcantly less accurate than the ones\nbased on local descriptors. A better trade-off between accu-\nracy and computational efﬁciency is offered by structured\nlocalization approaches (Brachmann et al., 2017; Brach-\nmann & Rother, 2018, 2021; Shotton et al., 2013; Li et al.,\n2020; Wang et al., 2021). Structured methods are trained\nto learn an implicit representation of the 3D environment\nby directly regressing 3D scene coordinates correspond-\ning to a 2D pixel location in a given input image. This\ndirectly provides 2D-3D correspondences and avoids storing\nand explicitly matching database local descriptors with the\nquery. For small-scale scenes, the scene-coordinate regres-\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2531\nsion methods work on par (Brachmann et al., 2021)o r\noutperform (Brachmann & Rother, 2018, 2021) local image\ndescriptors-based approaches. Nevertheless, the storage and\ncomputational beneﬁts of structured-based methods are supe-\nrior to their classical counterparts.\nExisting scene-coordinate regression approaches (Brach-\nmann et al., 2017; Brachmann & Rother, 2018, 2021)a r e\ndesigned to predict scene coordinates from a small local\nimage patch that provides robustness to viewpoint changes.\nHowever, such methods are limited in applicability to larger\nscenes where ambiguity from visually similar local image\npatches cannot be resolved with a limited receptive ﬁeld.\nUsing larger receptive ﬁeld sizes, up to the full image, to\nregress the coordinates can mitigate the issues from ambi-\nguities by encoding more context. This, however, has been\nshown to be prone to overﬁtting the larger input patterns in\nthe case of limited training data, even if data augmentation\nalleviates this problem to some extent (Li et al., 2018; Brach-\nmann & Rother, 2021).\nIncreasing context by enlarging the receptive ﬁeld while\nmaintaining the distinctiveness of local descriptors or not\noverﬁtting is a challenging problem. We address this using a\nspecial network architecture, called HSCNet (Li et al., 2020),\nwhich hierarchically encodes scene context using a series\nof classiﬁcation layers before making the ﬁnal coordinate\nprediction. The overall pipeline is illustrated in Fig. 1. Specif-\nically, the network predicts scene coordinates progressively\nin a coarse-to-ﬁne manner, where predictions correspond\nto a region in the scene at the coarse level and coordinate\nresiduals at the ﬁnest level. The predictions at each level are\nconditioned on both descriptors and predictions from the pre-\nceding level which is the key component in large scenes as\nwe experimentally demonstrate in this work. This condition-\ning leverages FiLM (Perez et al., 2018) layers that allow to\ngradually increase the receptive ﬁeld. The HSCNet approach\nutilizes CNNs to encode the descriptors and predictions. In\nthis work, we extend this idea and propose the transformer-\nbased (V aswani et al.,2017) conditioning mechanism, named\nHSCNet++, which is more efﬁcient in capturing global con-\ntext into local representations through attention and does\nnot require heavy conventional layers to enlarge the recep-\ntive ﬁeld. The architecture manages to improve coordinate\nprediction at all levels, both coarse and ﬁne. We integrate\ndynamic position information in the form of predicted coarse\npositional encoding, without the need to learn or construct\nexplicitly position embeddings and show promising results\non several camera relocalization benchmarks.\nWe further extend HSCNet++ by removing the depen-\ndency on dense ground truth scene coordinates. Dense\ncoordinates limit the applicability of HSCNet to outdoor\nscenes. Similar to Brachmann and Rother ( 2018), HSCNet\naddressed the issue of sparse data on Cambridge dataset\n(Kendall et al., 2015) by using MVS-based densiﬁcation\n(Schönberger et al., 2016). However, these methods either\nintroduce additional noise and are costly to obtain. Directly\ntraining HSCNet with sparse supervision leads to a signiﬁ-\ncant performance drop. In HSCNet++, we propose a simple\nyet effective pseudo-labelling method, where ground-truth\nlabels at each pixel location are propagated to a ﬁxed spa-\ntial neighbourhood. This is based on the assumption that\nnearby pixels share similar statistics. To provide robustness to\npseudo-label noise, symmetric objective functions based on\ncross-entropy and re-projection loss are proposed. While the\nsymmetric cross-entropy cost function provides robustness\nto the classiﬁcation layers of HSCNet, the re-projection loss\nrectiﬁes the noise in pseudo-labelled 3D scene coordinates.\nFig. 1 HSCNet architecture. The ground-truth scene 3D coordinates are\nhierarchically quantized into regions and sub-regions. Direct branches\nof the network sequentially predict discrete regions and sub-regions, and\ncontinuous 3D coordinates, with the processing of each branch being\nconditioned on the result of the previous one. Given an input image,\nHSCNet predicts 3D coordinates for 2D image pixels, which then form\nthe input to PnP-RANSAC for 6DoF pose estimation\n123\n2532 International Journal of Computer Vision (2024) 132:2530–2550\nThis work is a summary and extension of HSCNet. We val-\nidate our approach on three datasets used in previous works:\n7-Scenes (Shotton et al., 2013), 12-Scenes (V alentin et al.,\n2016), and Cambridge Landmarks (Kendall et al., 2015).\nOur approach demonstrates consistently better performance\nand achieves state-of-the-art results for single-image cam-\nera relocalization. In addition, by compiling the 7-Scenes\nand 12-Scenes datasets into single large scenes we show that\nour approach scales more robustly to larger environments. In\nsummary, our contributions are as follows:\n1. Compared to HSCNet, we utilize an improved trans-\nformer based conditioning mechanism that efﬁciently\nand effectively encodes global spatial information to\nscene coordinate prediction pipeline, resulting in a sig-\nniﬁcant performance improvement from 84.8% to 88.7%\non indoor localization while requiring only 57% of the\nmemory footprint;\n2. We extend HSCNet to optionally leverage the sparse\nground truth only in the training procedure by introducing\npseudo ground truth labels and angle-based re-projection\nerrors. When using sparse supervision for training, the\nproposed approach achieves better performance on the\nCambridge outdoor camera relocalization dataset com-\npared to the MVS-based densiﬁed training data;\n3. We show that the classical pixel-based positional encod-\ning in our conditioning mechanism suffers from a sig-\nniﬁcant performance drop, especially in scenes exhibit-\ning substantial repetitive patterns. Our spatial positional\nencoding inspired by the FiLM layer eliminates this\nproblem and achieves SoTA performance on several\nimage-based localization benchmarks.\n2 Related Work\nExisting methods for visual localization are reviewed depend-\ning on the category they belong to.\nClassical visual localization methods assume that a scene\nis represented by a 3D model, which is a result of process-\ning a set of database images. Each 3D point of the model\nis associated with one or several database local descriptors.\nGiven a query image, a sparse set of keypoints and their local\ndescriptors are obtained using traditional (Calonder et al.,\n2010;L o w e ,2004; Rublee et al., 2011; Bay et al., 2006)o r\nlearned CNN-based (DeTone et al., 2018; Revaud et al., 2019;\nDusmanu et al., 2019; Melekhov et al., 2021, 2020; Luo et\nal., 2019; Wang et al., 2020; Tian et al., 2017; Balntas et\nal., 2016; Zagoruyko & Komodakis, 2015; Han et al., 2015;\nMelekhov et al., 2017; Simo-Serra et al., 2015; Mishchuk\net al., 2017) approaches. The query local descriptors are\nthen matched with local descriptors extracted from database\nimages to establish tentative 2D-3D matches. These tenta-\ntive matches are then geometrically veriﬁed using RANSAC\n(Fischler & Bolles, 1981) and the camera pose is estimated\nvia PnP . Although these methods produce a very accurate\npose estimate, the computational cost of sparse keypoint\nmatching becomes a limitation, especially for large-scale\nenvironments. The large computational cost is addressed by\nimage retrieval-based methods (Arandjelovi´ ce ta l . , 2016;\nRadenovi´c et al., 2016) restricting matching query descrip-\ntors to local descriptors extracted from top-ranked database\nimages only. Moreover, despite the recent advancements of\nlearned keypoint detectors and descriptors (Wang et al., 2020;\nDusmanu et al., 2019; Melekhov et al., 2020, 2021; Sun et al.,\n2021; Zhou et al., 2021; Revaud et al., 2019; Tyszkiewicz et\nal., 2020), extracting discriminative local descriptors which\nare robust to different viewpoint and illumination changes is\nstill an open problem.\nAbsolute camera pose regression (APR) methods aim to alle-\nviate the limitations of structure-based methods by using a\nneural network that directly regresses the camera pose of a\nquery image (Kendall et al., 2015; Brahmbhatt et al., 2018;\nKendall & Cipolla, 2016, 2017; Melekhov et al., 2017; Walch\net al., 2017\n; Chen et al., 2021, 2022) that is given as input to\nthe network. The network is trained on database images with\nground-truth poses by optimizing a weighted combination\nof orientation and translation L2 losses (Kendall et al., 2015;\nMelekhov et al., 2017), leveraging uncertainty (Kendall et\nal., 2018), utilizing temporal consistency of the sequential\nimages (Walch et al., 2017; Radwan et al., 2018; V alada et\nal., 2018; Xue et al., 2019) or using GNNs (Xue et al., 2020)\nand Transformers (Shavit et al., 2021). The APR methods are\nscalable, fast, and memory efﬁcient since they do not require\nstoring a 3D model. However, their accuracy is an order of\nmagnitude lower compared to the one obtained by structure-\nbased localization approaches and comparable with image\nretrieval methods (Sattler et al., 2019). Moreover, the APR\napproaches require a different network to be trained and eval-\nuated per scene when the scenes are registered to different\ncoordinate frames.\nRelative camera pose regression (RPR) methods, in contrast\nto APR, train a network to predict relative pose between the\nquery image and each of the top-ranked database images\n(Ding et al., 2019; Laskar et al., 2017; Balntas et al., 2018),\nobtained by image retrieval (Arandjelovi´ ce ta l . , 2016; Rade-\nnovi´c et al., 2016). The camera location is then obtained\nvia triangulation from two relative translation estimations\nveriﬁed by RANSAC. This leads to better generalization per-\nformance without using scene-speciﬁc training. However, the\nRPR methods suffer from low localization accuracy similarly\nto APR.\nScene coordinate regression (SCR) methods learn the ﬁrst\nstage of the pipeline in the structure-based approaches.\nNamely, either a random forest (Brachmann et al., 2016;\nCavallari et al., 2020, 2017; Guzmán-Rivera et al., 2014;\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2533\nM a s s i c e t ie ta l . ,2017; Meng et al., 2017, 2018; Shotton et al.,\n2013; V alentin et al., 2015) or a neural network (Brachmann\net al., 2017; Brachmann & Rother, 2018, 2019a, c, 2021;\nBudvytis et al., 2019; Bui et al., 2018; Cavallari et al., 2019;\nLi et al., 2018; Massiceti et al., 2017) is trained to directly\npredict 3D scene coordinates for the pixels and thus the 2D-\n3D correspondences are established. These methods do not\nexplicitly rely on feature detection, description, and match-\ning, and are able to provide correspondences densely. They\nare more accurate than traditional feature-based methods at\nsmall and medium scales, but usually do not scale well to\nlarger scenes (Brachmann & Rother, 2018, 2019a). In order\nto generalize well to novel viewpoints, these methods typi-\ncally rely on only local image patches to produce the scene\ncoordinate predictions. However, this may introduce ambi-\nguities due to similar local appearances, especially when the\nscale of the scene is large. To resolve local appearance ambi-\nguities, we introduce element-wise conditioning layers to\nmodulate the intermediate feature maps of the network using\ncoarse discrete location information. We show this leads to\nbetter localization performance, and we can robustly scale to\nlarger environments.\nJoint classiﬁcation-regression frameworks have been proven\neffective in solving various vision tasks. For example, Rogez\net al. ( 2017, 2019) proposed a classiﬁcation-regression\napproach for human pose estimation from single images.\nIn Brachmann et al. ( 2016), a joint classiﬁcation-regression\nforest is trained to predict scene identiﬁers and scene coordi-\nnates. In Weinzaepfel et al. ( 2019), a CNN is used to detect\nand segment a predeﬁned set of planar Objects-of-Interest\n(OOIs), and then, to regress dense matches to their reference\nimages. In Budvytis et al. ( 2019), scene coordinate regression\nis formulated as two separate tasks of object instance recog-\nnition and local coordinate regression. In Brachmann and\nRother ( 2019a), multiple scene coordinate regression net-\nworks are trained as a mixture of experts along with a gating\nnetwork which assesses the relevance of each expert for a\ngiven input, and the ﬁnal pose estimate is obtained using a\nnovel RANSAC framework, i.e. Expert Sample Consensus\n(ESAC). In contrast to existing approaches, in our work, we\nuse spatially dense discrete location labels deﬁned for all pix-\nels, and propose FiLM-like (Perez et al., 2018) conditioning\nlayers to propagate information in the hierarchy. We show\nthat our novel framework allows us to achieve high localiza-\ntion accuracy with one single compact model.\nTransformer has already shown a positive impact on the\nproblem of visual localization. Shavit et al. ( 2021)s h o w\nthat multi-headed transformer architectures can be used to\nimprove end-to-end absolute camera pose localization in\nmultiple scenes with a single trained model. Similarly, Super-\nGlue, LoFTR and COTR (Sarlin et al., 2020; Sun et al.,\n2021; Jiang et al., 2021) demonstrate the usefulness of trans-\nformer architectures in learning local descriptor models.\nInspired by the above success, the paper proposes methods to\nextend transformer architecture to the structured localization\nmethod.\n3 Problem Formulation and Notation\nThe goal of camera pose estimation is to predict the 6-DoF\npose p(x) ∈ R6 for an RGB image x. We adopt a standard\ntwo-step approach. As a ﬁrst step, 3D coordinates are pre-\ndicted for each pixel, or some of the pixels, of an image.\nThose are the coordinates from a known 3D scene. Such\npredictions result in a set of 2D-3D correspondences. As a\nsecond and ﬁnal step, these correspondences are fed into the\nPnP algorithm that estimates the camera pose. In this work,\nwe focus on the 3D coordinate prediction task.\nWe rely on a function f :[ 0,1]\nW ×H×3 → Rw×h×3,\nw = W /8 and h = H/81 that provides such coordinate\npredictions given an input image x of resolution equal to\nW × H pixels; the predicted coordinates for image x are\ngiven by ˆy(x) = f (x).\nThe known 3D environment is represented by a set of\ntraining images, with known ground-truth labels per pixel in\nthe form of 3D coordinates. The training set comprises pairs\nof the form (x, y(x)) for image x and ground-truth 3D coor-\ndinates y(x). In case ground-truth is available only sparsely,\ni.e. on small part of the image pixels, a corresponding binary\nmask m(x) ∈{ 0,1}\nw×h denotes which are the valid pixels.\nThe value of ground-truth or prediction at a particular pixel is\ndenoted by subscript i, e.g. y (x)i for the ground-truth coor-\ndinate of pixel i.\n4 HSCNet++: Hierarchical Scene Coordinate\nPrediction with Transformers\n4.1 Overview\nA baseline conventional approach for this task is to use a\nfully convolutional network (FCN) that maps input images\nto 3D coordinate predictions and is trained with a regres-\nsion loss. The proposed architecture extends this scheme\nby constructing a hierarchy of labels, from coarse-level to\nﬁne-level, and by adding extra layers to predict those labels.\nHierarchical discrete labels are deﬁned by partitioning the\nground-truth 3D points of the scene with hierarchical k-\nmeans. The number of levels in the hierarchy is ﬁxed to 2\nin this work. In this way, in addition to the ground-truth 3D\n1 The spatial resolution of the prediction is smaller, by a factor of 8,\nthan that of the input image. The coordinate predictions are provided\nfor a down-sampled version of the image, which is aligned with the use\nof deep CNNs that inherently perform such down-sampling.\n123\n2534 International Journal of Computer Vision (2024) 132:2530–2550\nscene coordinates, each pixel in a training image is also asso-\nciated with two discrete labels, namely region and sub-region\nlabels, obtained at different levels of the clustering hierar-\nchy. Region and sub-region labels are denoted by one-hot\nencodings y\nr (x) ∈{ 0,1}w×h×k1 and ys (x) ∈{ 0,1}w×h×k2 ,\nrespectively. The ﬁne-level information is given by the resid-\nual between the ground-truth 3D point and the corresponding\nsub-region center, which we denote by y\n3D(x) ∈ Rw×h×3.\nGround-truth 3D pixel coordinates y(x) are replaced by\nyr (x), ys (x), and y3D(x). Sub-region centers and residuals,\nwhen combined by addition, compose the pixel 3D coordi-\nnates, i.e. y (x) = c(yr (x) × k2 + ys (x)) + y3D(x), where c\nis a function providing the sub-region center.\nThe proposed architecture includes two classiﬁcation\nbranches for regions and sub-regions, which provide the label\npredictions in the form of the k-dimensional probability dis-\ntributions, and a regression branch for the residual prediction.\nRegions, sub-region and residual predictions are denoted by\nˆy\nr (x), ˆys (x), and ˆy3D(x), respectively. A key ingredient is\nto propagate coarse region information to inform the predic-\ntions at ﬁner levels, which is achieved by conditioning layers\nbefore the classiﬁcation/regression layers.\n4.2 Preliminaries\nWe describe FiLM layers and transformer blocks, which we\nuse in the proposed architecture.\nThe FiLM Perez et al. ( 2018) conditioning layer represents a\nblock whose processing is conditioned on an auxiliary input.\nConditioning relies on parameter generators γ,β to generate\na set of scaling and shifting parameters γ(l) and β(l),t h e\nauxiliary input l ∈ R\nw×h×d is the (sub-)region label encod-\ning. The conditioning is processed by\nφ(F,l) = γ(l) ⊙ F + β(l), (1)\nwhere ⊙ is the Hadamard product, F ∈ Rw×h×d is the main\ninput. Therefore, the parameters of the FiLM layer are con-\nditioned on the auxiliary. The FiLM-based processing is a\nway to jointly encode the main and the auxiliary input. In the\nfollowing, it is used to encode the predicted (sub-)regions\ninformation together with the image features.\nTransformer We view a 3D activation tensor of size w×h×d\nas a set of w × h vectors/tokens and provide them as input\nto transformer blocks. The vanilla transformer has the com-\nputational complexity that is quadratic in the cardinality\nn = w × h of input set, which is computationally unafford-\nable in our case. Inspired by prior work (Sun et al., 2021),\nwe apply the linear transformer (Katharopoulos et al., 2020)\nthat reduces the complexity from O(N\n2) to O(N ) by using\nthe associativity property of matrix products and replacing\nthe exponential similarity kernel with a linear dot-product\nkernel.\nConsequently, the transformer modules that are part of our\narchitecture do not have a signiﬁcant impact on run time.\n4.3 HSCNet++ Architecture\nThis section presents the model architecture for HSCNet++\nand discusses the difference compared to the original HSC-\nNet architecture.\nOverview of the model architecture The overall architecture\nof HSCNet++ is summarized in Fig. 2. We ﬁrst present the\nmodel as it operates during inference and then clarify the\ndifferences between training and inference. An FCN back-\nbone is used for dense feature encoding and is denoted by\nF (x) ∈ R\nw×h×d . This is a mapping of the input image to a\ndense feature tensor which represents the appearance of the\ninput image.\nPrediction of region labels is performed ﬁrst. A module\ngr : Rw×h×d → Rw×h×d is used that consists of convo-\nlutional layers and a transformer block. Its input is feature\nmap F (x) and the output is given by x\nr = gr (F (x)).\nFeature map processing is performed within the local con-\ntext of the receptive ﬁeld with convolutions and within a\nglobal context with the transformer. The region predictor\nh\nr : Rw×h×d → Rw×h×k1 comprises a 1 × 1 convolutional\nlayer and is used to obtain the region prediction denoted by\nˆyr (x) = hr (xr ).\nThen, sub-region prediction is performed. A module gs :\nRw×h×d × Rw×h×k2 → Rw×h×d is used, which consists\nof convolutional layers and transformer blocks, but also\nFiLM layers, therefore the two inputs. The main input is\nthe feature map F (x), while the auxiliary input is the region\nprediction ˆy\nr (x) from the earlier stage. In practice, ˆyr (x)\nis passed through a series of convolutional layers before\ninputted to the FiLM layer as shown in Fig. 3 (c, middle\nblock). Conditioning on region predictions is a way to jointly\nencode appearance and geometry which comes in the form\nof region prediction. Therefore, conditioning on region pre-\ndictions is used to improve sub-region predictions. Then,\nx\ns = gs (F (x), ˆyr (x)) is fed into the sub-region predictor\nhs : Rw×h×d → Rw×h×k2 comprising a 1 × 1 convolution\nlayer, whose output is denoted by ˆys (x) = hs (xs ) and con-\nstitutes the sub-region prediction.\nNow, residual prediction is performed. Similar to the ear-\nlier stage, feature map F (x) is processed by conditioning\non the concatenation of region and sub-region predictions,\ni.e. ˆyr (x) and ˆys (x). This is denoted by module g3D :\nRw×h×d ×Rw×h×(k1+k2) → Rw×h×d and consists of convo-\nlutional and FiLM layers and transformer blocks. Similarly,\nas before, concatenated region and sub-region predictions are\npassed through a series of convolutional layers before being\ninputted to the FiLM layer as an auxiliary input ( c.f.Fig. 3\n(c, right block)). Then, x\n3D = g3D(F (x), ˆys (x)) is fed into\nthe residual predictor to obtain ˆy3D(x) = h3D(x3D), where\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2535\nFig. 2 An overview of the proposed HSCNet++. The ﬁgure shows the\nnetwork architecture of the proposed HSCNet++. The depicted losses\ncorrespond to the case of learning with dense ground-truth. Note that\nthe switch is applied during inference when the predicted labels are\nencoded instead of the ground-truth labels\nh3D : Rw×h×d → Rw×h×3 consists of a 1 × 1 convolution.\nThe detailed architecture for HSCNet++ and the different\nmodules is shown in Fig. 3.\nSynergy between FiLM and transformers Modules gs and\ng3D include the use of FiLM layers followed by transformer\nblocks. Transformers typically rely on the use of 2D posi-\ntional encodings (V aswani et al., 2017) in order to take the\nposition of activations into account. Discarding those posi-\ntions is not an appropriate choice for our task. Nevertheless,\nour architecture design dispenses with the need for those\nclassical positional encodings. This is due to the fact that\nFiLM layers jointly encode appearance with 3D coordinate\npredictions, instead of the 2D positions within the image. To\nthe best of our knowledge, such form of geometry encoding\nfor transformers has not appeared in the computer vision or\nmachine literature before. We experimentally show that this\nis an effective design choice.\nCompared to HSCNet, the proposed HSCNet++ incorpo-\nrates the use of transformers. The design choice of placing\nthem right after FiLM layers supports their synergy due to\nthe mentioned case of encoding positions.\n4.4 Training\nWhen training with dense supervision, the following losses\nare adopted. Classiﬁcation loss ℓc is applied to the output of\nthe two classiﬁcation branches,\nℓc = ℓce(ˆyr (x), yr (x)) + ℓce(ˆys (x), ys (x)) (2)\nWhere ℓce is the cross-entropy loss. Additionally, regression\nloss ℓr , in particular mean squared error, is applied on ˆy3D(x)\nand y3D(x). The total loss L is a weighted sum of the two\nclassiﬁcation losses and the regression loss.\nL = λ1ℓc + λ2ℓr (3)\nWhere λ1 and λ2 are the weights for each term. We observe\nthat the regression prediction is more sensitive to localization\nperformance. Thus, a larger weight is assigned to the ℓr .\n4.5 Inference\nDuring inference, the predicted 3D coordinates ˆy(x) and\ntheir corresponding 2D pixels are fed into the PnP-RANSAC\nloop to estimate the 6-DoF camera pose. These predicted 3D\ncoordinates are obtained by simply summing the center of\npredicted sub-regions c(y\nr (x) × k2 + ys (x)) and predicted\nresiduals ˆy3D(x).\nWe differentiate on how conditioning is conducted dur-\ning training and inference as shown in Fig. 2.A tt r a i n i n g\ntime, conditioning is performed using the ground truth (sub-\n)region labels, i.e. y\nr (x) and ys (x) are the second inputs of\nthe conditioning blocks. At test time, conditioning is imple-\nmented using predicted (sub-)region labels. Speciﬁcally, the\none-hot encodings of the argmax operation of ˆy\nr (x)and ˆys (x)\nare the second inputs of the conditioning blocks.\n123\n2536 International Journal of Computer Vision (2024) 132:2530–2550\nFig. 3 HSCNet++ detailed architecture. The ﬁgure shows the detailed\nnetwork architecture of the main pipeline and the FiLM conditioning\nnetwork. For experiments on the combined scenes we added two more\nlayers in the ﬁrst conditioning generator, g\ns that are marked in (dotted)\nred. We also roughly doubled the channel counts that are highlighted in\nred, cyan and violet for i7-Scenes, i12-Scenes and i19-Scenes, respec-\ntively (Color ﬁgure online)\n4.6 Training with Sparse Supervision\nWhen only sparse ground truth of 3D coordinates, indicated\nby mask m(x) for image x, is available, the straightforward\napproach is to apply the loss only on pixels where the mask\nvalue is 1, which we refer to as valid pixel . Instead, we pro-\npose to perform propagation of the available labels to nearby\npixels and use two additional losses that are appropriately\nhandling the scarcity of the labels. We refer to the HSCNet++\nmodel trained with such sparse supervision as HSCNet++(S).\nLabel propagation (LP) We rely on a smoothness assump-\ntion: labels do not change much in a small pixel neighbor-\nhood. Consequently, we propagate the labels in a local neigh-\nborhood around each pixel. The neighborhood is deﬁned by\na square area of size z × z. All neighbors of a valid pixel are\nmarked as valid too and ground-truth maps, namely y\nr (x),\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2537\nys (x), and y3D(x), are updated by replicating the label of the\noriginal pixel to the neighboring pixels. Then, the classiﬁca-\ntion and regression losses are applied to the newly obtained\nvalid pixels after propagation. This is seen as some form of\npseudo-labeling that increases the density of the available\nlabels.\nSymmetric cross-entropy loss (SCE) Pseudo-labels are expected\nto include noise. This noise will typically be larger if propa-\ngation reaches background pixels starting from a foreground-\nobject valid pixel. We quantitatively analyze the percentage\nof noisy labels with the increasing of neighbor radius in\nSect. 5.4. Thus, we face a challenging task which is learning\ncorrect classiﬁcation with noisy labels. The traditional cross-\nentropy loss is not reliable in such a scenario as it exhibits\noverﬁtting to noisy labels on some \"easy\" classes and suffers\nfrom under learning on some “hard” classes (Wang et al.,\n2019).\nFollowing (Wang et al., 2019), we increase the robustness\nof the classiﬁcation with minimal cost by introducing the\nsymmetric cross-entropy loss. The additional reverse cross\nentropy loss in SCE is a noise-tolerant term that exhibits the\nproperty of overestimating and underestimating the target\nvalue resulting in the same loss. This property makes it more\nadaptive to noisy labels and allows the model to cope bet-\nter with label noise. The SCE loss is deﬁned as a weighted\nsummation of the following terms:\nl\nsce = λcelce + λsce lrce (4)\nWhere lrce is the reverse cross-entropy loss. For a valid pixel\ni ∈ I ,t h e lrce is:\nℓrce (x,i) =ˆyr (x)i log yr (x)i , (5)\ncompared to the conventional one deﬁned as follows:\nℓ\nce(x,i) = yr (x)i log ˆyr (x)i , (6)\nRe-projection error loss (Rep) Besides the SCE Loss to\npredict the correct labels from noise, we also adopt the re-\nprojection loss as a semi-supervised term to further enhance\nboth the labels and distance residual prediction. The loss term\nis especially efﬁcient in scenes with a large amount of texture-\nless or repeating patterns. However, the vanilla re-projection\nloss requires careful initialization to avoid the impact of\nunstable gradients from degenerate 3D predictions ( e.g. too\nfar or behind the camera). Training with vanilla re-projection\nloss requires extra geometric constraints and a long conver-\ngence time (Brachmann & Rother, 2021). Inspired by Li et al.\n(2018), we employ the angle-based re-projection loss which\naims to minimize the angle θ between two rays that share the\ncamera center. This strategy forces predictions to lie in front\nof the camera, ensuring smoother gradients during training.\nConsequently, it eliminates the need for a time-consuming\ninitialization step and mitigates the burden of related geo-\nmetric constraints.\nGiven ground-truth camera pose P, the loss for pixel i of\nimage x, whose 2D coordinates in the image are denoted by\np\ni , is given by\nℓrep (x,i) =| |γi P−1 ˆyi (x) − fC −1 pi ||, (7)\nwhere γi =| | fC −1 pi ||/||P−1 ˆy(x)i ||, f is the focal length,\nand C is the intrinsic matrix. The angle-based re-projection\nloss is computed in the camera coordinate system between\ntwo points on a 3D sphere centered at the camera center and\ntouching the image plane at the ground-truth pixel location,\ni.e. radius of the sphere is || fC\n−1 pi ||. The two points on\nthe sphere correspond to the locations where the vector from\nthe camera center to the predicted 3D point and ground-truth\npixel location (both in camera coordinate system) intersect\nthe 3D sphere represented by ﬁrst and second terms in Eq. 7\nrespectively.\nNote that the re-projection loss is not added to the total\nloss in the beginning epochs for a fast training convergence.\nSimilar to our dense setting, the total loss for sparse supervi-\nsion is the weighted summation of regression loss, symmetric\nclassiﬁcation loss, and re-projection loss, ℓ\nsparse = ℓsce +\nλ2ℓr + λ3ℓrep .\n5 Experiments\nIn this section, we discuss the experimental setup and\nemployed datasets, present our results, and compare our\napproach to state-of-the-art localization methods.\n5.1 Experimental Setup\nDatasets We use three standard benchmarks for the eval-\nuation; namely, 7-Scenes (Shotton et al., 2013), 12-Scenes\n(V alentin et al., 2016), and Cambridge Landmarks (Kendall\net al., 2015), The 7-Scenes dataset covers a volume of ∼ 6m3\nfor each individual scene. The 3D models and ground truth\nposes are included in the dataset. 12-Scenes is another indoor\nRGB-D dataset that contains 4 large scenes with a total of 12\nrooms, the volume ranges 14–79 m\n3 for each room. The union\nof these two datasets forms the 19-Scenes dataset. Cambridge\nLandmarks dataset is a standard benchmark for evaluating\nscene coordinate methods in outdoor scenes. It is a small-\nscale outdoor dataset consisting of 6 individual scenes, and\nthe ground truth pose is provided by structure-from-motion.\nFollowing prior work (Brachmann & Rother, 2019a), we\nconduct experiments per scene, i.e. the individual scenes set-\nting, but also by training a single model on all scenes of a\ncorresponding dataset, i.e. the combined scenes setting. The\n123\n2538 International Journal of Computer Vision (2024) 132:2530–2550\nTable 1 Indoor localization: individual scene setting (7-Scenes)\nMethod 7-Scenes Accuracy\nChess Fire Heads Ofﬁce Pumpkin Red Kitchen Stairs\nt,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦\nMapNet (Brahmbhatt et al., 2018) 8.0 3.30 27.0 11.70 18.0 13.30 17.0 5.20 22.0 4.00 23.0 4.90 30.0 12.10 –\nGeometric PoseNet (Kendall & Cipolla, 2017) 13.0 4.50 27.0 11.30 17.0 13.00 19.0 5.60 26.0 4.80 23.0 5.40 35.0 12.40 –\nAttTxf (Shavit et al., 2021) 11.0 4.66 24.0 9.60 14.0 12.19 17.0 5.66 18.0 4.44 17.0 5.94 26.0 8.45 –\nLSTM-Pose (Walch et al., 2017) 24.0 5.80 34.0 11.90 21.0 13.70 30.0 8.10 33.0 7.00 37.0 8.80 40.0 13.70 –\nAnchorNet(Saha et al., 2018) 6.0 3.90 16.0 11.10 9.0 11.20 11.0 5.40 14.0 3.60 13.0 5.30 21.0 11.90 –\nLENS (Moreau et al., 2021) 3.0 1.30 10.0 3.70 7.0 5.80 7.0 1.90 8.0 2.20 9.0 2.20 14.0 3.60 –\nAS (Sattler et al., 2016a) 3.0 0.87 2.0 1.01 1.0 0.82 4.0 1.15 7.0 1.69 5.0 1.72 4.0 1.01 68.7\nHLoc (Sarlin et al., 2019)2 . 0 0.85 2.0 0.94 1.0 0.75 3.0 0.92 5.0 1.30 4.0 1.40 5.0 1.47 73.1\nPixLoc (Sarlin et al., 2021)2 . 0 0.80 2.0 0.73 1 .0 0.82 3.0 0.82 4.0 1.21 3.0 1.20 5.0 1.30 75.7\nVS-Net (Huang et al., 2021) 1.50 .50 1 .9 0.80 1.2 0.70 2.1 0.60 3.7 1.00 3.6 1.10 2.80 .80 –\nSFT-CR (Guan et al., 2021) 2.1 0.70 2.0 0.78 1.1 0.81 2.4 0.66 3.4 0.98 3.4 1.06 3.5 0.97 –\nDSAC++ (Brachmann & Rother, 2018)2 . 0 0.50 2.0 0.90 1.0 0.80 3.0 0.70 4.0 1.10 4.0 1.10 9.0 2.60 76.1\nDSAC⋆ (3D) (Brachmann & Rother, 2021)2 . 0 1.10 2.0 1.24 1.0 1.82 3.0 1.15 4.0 1.34 4.0 1.68 3.0 1.16 85.2\nReg-only (Li et al., 2020)2 . 0 0.70 2.0 0.90 1.0 0.80 3.0 0.90 4.0 1.10 5.0 1.40 4.0 1.00 74.7\nHSCNet (Li et al., 2020)2 . 0 0.70 2.0 0.90 1.0 0.90 3.0 0.80 4.0 1.00 4.0 1.20 3.0 0.80 84.8\nHSCNet++ 2.0 0.63 2.0 0.79 1.0 0.80 2.0 0.65 3.00 .85 3 .0 1.09 3.0 0.83 88.7\nFor each scene of 7-Scenes dataset we report the median translation ( t, cm) and orientation ( r, ◦) error. The best and second best results are in bold and underlined . Note that except VS-Net (Huang\net al., 2021) and SFT-CR (Guan et al., 2021), the rest results are reported in centimeter precision for translation error\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2539\nTable 2 Indoor localization: individual scene setting (12-Scenes)\nScenes Methods\nReg-only\nLi et al. ( 2020)\nDSAC*(3D) Brachmann\nand Rother ( 2021)\nHSCNet\nLi et al. ( 2020)\nHSCNet++\nt,c m r, ◦ Acc t,c m r, ◦ Acc t,c m r, ◦ Acc t,c m r, ◦ Acc\nKitchen-1 0.8 0.4 100 ––– 0 . 8 0.4 100 0 .70 .4 100\nLiving-1 1.1 0.4 100 ––– 1 . 1 0.4 100 1 .00 .4 100\nBed 1.3 0.6 100 ––– 0.90 .4 100 1.0 0.4 100\nKitchen-2 0.8 0.4 100 ––– 0.70 .3 100 0.8 0.4 100\nLiving-2 1.4 0.6 100 ––– 1.00 .4 100 1 .00 .4 100\nLuke 2.0 0.9 93.8 – – – 1.20 .5 96.3 1.3 0.6 98.1\nGate362 1.1 0.5 100 ––– 1.00 .4 100 1 .0 0.5 100\nGate381 1.6 0.7 98.8 – – – 1.2 0.6 99.11 .10 .5 98.6\nLounge 1.5 0.5 99.4 – – – 1.4 0.5 100 1 .30 .4 100\nManolis 1.4 0.7 97.2 – – – 1.10 .5 100 1.2 0.5 100\nFloor. 5a 1.6 0.7 97.0 – – – 1.20 .59 8 .8 1.3 0.5 96\nFloor. 5b 1.9 0.6 93.3 – – – 1.5 0.5 97.3 1.40 .49 9 .5\nAccuracy 96.4 99.1 99.1 99.4\nSimilar to the 7-Scenes localization benchmark, we provide the median translation ( t, cm), orientation ( r, ◦) error, and accuracy with the error\nthreshold of 5 cm and 5 ◦. The best accuracy results are in bold\ncombined settings of the given indoor localization bench-\nmarks are denoted by i7-Scenes, i12-Scenes, and i19-Scenes,\nrespectively.\nCompeting methods In this work, we compare the proposed\napproach with the following methods: (1) pose regression\nmethods that directly regress absolute or relative camera pose\nparameters: MapNet (Brahmbhatt et al., 2018), Geometric\nPoseNet (Kendall & Cipolla, 2017), AttTxf (Shavit et al.,\n2021), LSTM-Pose (Walch et al., 2017), AnchorNet (Saha et\nal., 2018) and LENS (Moreau et al., 2021); (2) local feature\nbased pipelines based on SIFT such as Active Search (AS)\n(Sattler et al., 2016a) and HLoc (Sarlin et al., 2019) based\non CNN descriptors; (3) DSAC\n⋆ (3D) (Brachmann & Rother,\n2021): the latest scene coordinate regression approach with\n3D model; (4) VS-Net (Huang et al., 2021): scene-speciﬁc\nsegmentation and voting; (5) PixLoc (Sarlin et al., 2021):\nscene-agnostic network; (6) SFT-CR (Guan et al., 2021):\nscene coordinate regression with global context-guidance.\nIn addition, we also compare with (7) ESAC (Brachmann &\nRother, 2019a) on the combined scenes. We also consider a\nbaseline called Reg-only without the hierarchical classiﬁca-\ntion layers.\nEvaluation metrics We report the median translation and ori-\nentation error ( cm,\n◦) as well as the accuracy of test images\nunder the threshold of (5 cm,5◦) on indoor scenes. On Out-\ndoor Cambridge Landmarks (Kendall et al., 2015), we report\nonly the median pose error as in previous methods (Brach-\nmann & Rother, 2021; Brachmann et al., 2017;L ie ta l . ,\n2020).\nTraining details We generate the region labels by hierarchical\nK-means. For 7-Scenes, 12-Scenes, and Cambridge land-\nmarks, we adopt 2-level ground truth labels with a branching\nfactor of 25 for all the levels. Furthermore, for combined\nscenes, i7-Scenes, i12-Scenes, and i19-Scenes, the ﬁrst level\nbranching factor is set to 7 ×25, 12 ×25, and 19 ×25, respec-\ntively. For the individual scene setting, training is performed\nfor 300K iterations with Adam optimizer. For the combined\nscenes the number of iterations is set to 900K. Throughout all\nexperiments, we use a batch size of 1 with the initial learning\nrate of 10\n−4.\nThe classiﬁcation loss weights λ1 is set to 1 for all datasets,\nwhile regression loss weight λ2 is 10 for single scenes and 10 5\nfor combined scenes. In the sparse supervision setting, λce\nand λrce are set to 0.1 and 1, respectively, while λ2 follows\nthe dense setting, and λ3 is increased from 0 to 0.1 after\nﬁrst 10 epochs. We initialize the network by training with lr\nusing pseudo-label coordinates and later also add lrep after\n10 epochs. When training with sparse supervision, we select\nthe neighborhood size z = 11 to propagate labels, and use\nthe cluster centers obtained from dense scene coordinates for\na direct comparison.\nData augmentation is also effective in increasing the ﬁnal\naccuracy. Thus, similar to HSCNet (Li et al., 2020), we\nrandomly augment training images using translation, rota-\ntion, scaling and shearing by uniform sampling from [ −20%,\n20%], [ −30\n◦,3 0 ◦], [0.7, 1.5], [ −10◦,1 0 ◦] respectively. In\naddition, images are augmented with additive brightness uni-\nformly sampled from [ −20, 20].\n123\n2540 International Journal of Computer Vision (2024) 132:2530–2550\nTable 3 Indoor localization:\ncombined scene setting Method Localization Accuracy (%)\ni7-Scenes i12-Scenes i19-Scenes\nReg-only (Li et al., 2020) 37.9 5.0 5.7\nESAC (Brachmann & Rother, 2019a) 70.3 97.1 88.1\nHSCNet (Li et al., 2020) 83.3 99.3 92.5\nHSCNet++ 88.3 99.5 93.6\nThe table presents average localization accuracy under 5 cm/5◦ of baseline models and proposed methods on\ni7-Scenes, i12-Scenes, and i19-Scenes datasets\nPose estimation We follow the same PnP-RANSAC pipeline\nand parameters setting as in Brachmann and Rother ( 2018).\nThe inlier threshold and the softness factor are set to τ = 10\nand β = 0.5, respectively. We randomly select 4 correspon-\ndences to formulate a minimal set for a PnP algorithm to\ngenerate a camera pose hypothesis, and a set of 256 initial\nhypotheses are sampled. Similar to Brachmann and Rother\n(2018, 2021), a pose reﬁnement process is performed until\nconvergence for a maximum of 100 iterations.\nArchitecture details The detailed architecture of HSCNet++\nis shown in Fig. 3; we also visualize the block details of the\nFiLM conditioning network and the transformer modules.\nBy removing the transformer layers, we derive the archi-\ntecture of HSCNet. Additionally, the number of channels in\nthe last branch, g\n3D of HSCNet is 4096, while it is 2048\nfor HSCNet++ that reduces memory cost ( c.f.Sect. 5.6). For\nexperiments on the combined scenes we added two more\nlayers in the ﬁrst conditioning generator, g\ns that are marked\nin (dotted) red. We also roughly doubled the channel counts\nthat are highlighted in red, cyan and violet for i7-Scenes, i12-\nScenes and i19-Scenes, respectively. For individual scenes,\nwe add 2 multi-head attention layers (MHA) to both clas-\nsiﬁcation and regression conditioning blocks, while in the\ncombined setting, the number of MHA is set to 5.\n5.2 Results for HSCNet and HSCNet++\nIndividual scenes setting. We present results on 7-Scenes\nand 12-Scenes in Table 1 and Table 2, accordingly. All mod-\nels are trained and evaluated individually on each scene of\nthe corresponding dataset. Results show that HSCNet is still\ncompetitive with respect to methods published later. With the\naddition of transformers, HSCNet++ further boosts the aver-\nage performance by 4% on 7-Scenes and obtains the best\naccuracy on 7-Scenes among the competitors.\nCombined scenes setting To test the scalability of scene-\ncoordinate regression methods, we go beyond small-scale\nenvironments such as individual scenes in 7-Scenes and\n12-Scenes and use the combined scenes, i.e. i7-Scenes, i12-\nScenes, and i19-Scenes by combining the former datasets.\nResults on the combined scenes setting presented in Table\n3 including comparison with the regression-only baseline\nand ESAC. Results show that our method scales well with\nincrease in number of scenes compared to Reg-only baseline.\nIt is to be noted that ESAC requires training and storing mul-\ntiple networks specializing in local parts of the environment,\nwhereas our approach requires only a single model. Results\nshow that our approach outperforms ESAC on i7-Scenes\nand i12-Scenes, while performing comparably on i19-Scenes\n(87.9% vs.88.1%). ESAC and our approach could be com-\nbined for very large-scale scenes, but we do not explore this\noption in this work. HSCNet++ advances the state-of-the-art\non all datasets, demonstrating the utility of transformers for\nthis task.\nCambridge Landmarks Table 4 reports the results of three\ntypes of visual localization methods on Cambridge land-\nmarks. AS (Sattler et al., 2016a) and HLoc (Sarlin et al.,\n2019) estimate the camera poses with sparse SfM ground\ntruth. DSAC++, DSAC* and our approaches train a scene-\ncoordinate regression model with MVS-densiﬁed depth\nmaps, VS-Net leverages the hybrid of the two. Both HSCNet\nand HSCNet++ perform better than other scene coordinate\nmethods DSAC++ and DSAC*. The performance is com-\nparable to more recent approaches. However, we observe\nthat the models trained with MVS-densiﬁed pseudo ground\ntruth show a slightly worse performance compared to the\napproaches that use the sparse SfM 3D map. HSCNet++\nshows even worse performance by adding the transformer\nmodules. Such results motivated us to extend the HSCNet++\nto train with sparse supervision and our hypothesis is that\nthe MVS densiﬁcation introduces more noise to the dense\nsupervision. The performance of HSCNet++(S) trained with\nsparse supervision on Cambridge landmarks in Sect. 5.5 ver-\niﬁed our hypothesis.\n5.3 Ablations: HSCNet\nData augmentation Using geometric and color data augmen-\ntation provides robustness to lighting and viewpoint changes\n(DeTone et al., 2018; Melekhov et al., 2021). We investi-\ngate the impact of data augmentation and summarize the\nobtained results in Table 5a. Applying data augmentations\nleads to better localization accuracy. Note that without data\naugmentation, the proposed approach still provides compara-\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2541\nTable 4 Outdoor localization: individual scene setting (Cambridge)\nMethod Cambridge\nKings College Great Court Old Hospital Shop Facade St Mary Church\nt,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦ t,c m r, ◦\nAS (Sattler et al., 2016a) 2 40 . 1 3 1 30 . 2 2 2 00 . 3 6 4 0.21 8 0.25\nHLoc (Sarlin et al., 2019)1 6 0.11 12 0.20 15 0 .30 4 0 .20 7 0 .21\nPixLoc (Sarlin et al., 2021) 14 0.24 30 0.14 16 0.32 5 0.23 10 0.34\nVS-Net (Huang et al., 2021)1 6 0 . 2 0 2 2 0.10 16 0.30 6 0.30 8 0.30\nDSAC++ (Brachmann & Rother, 2018) 13 0.40 40 0.20 20 0.30 6 0.30 13 0.40\nDSAC⋆ (3D) (Brachmann & Rother, 2021) 15 0.30 49 0.30 21 0.40 5 0.30 13 0.40\nH S C N e t( L ie ta l . ,2020) 1 80 . 3 0 2 80 . 2 0 1 9 0.30 6 0.30 9 0.30\nHSCNet++ 19 0.34 39 0.23 20 0.31 6 0.24 9 0.27\nFor each scene of the dataset we report the median translation ( t, cm) and orientation ( r, ◦) error. The best results are in bold\nble results to state of the art methods ( c.f.ESAC (Brachmann\n& Rother, 2019a) in Table 3 vs.row 3 of Table 5a).\nConditioning mechanism The two key components of HSC-\nNet are the coarse-to-ﬁne joint classiﬁcation-regression mod-\nule and its combination with the conditioning mechanism.\nTheir impact is evaluated and results are shown in Table 5a.\nWe train a variant of our network without the conditioning\nmechanism, i.e. we remove all the conditioning generators\nand layers. The network still estimates scene coordinates\nin a coarse-to-ﬁne manner by using the predicted location\nlabels, but there is no coarse location information that is\nfed to inﬂuence the network activations at the ﬁner levels.\nResults indicate the importance of the conditioning mecha-\nnism for accurate scene coordinate prediction. Compared to\nsingle scene setting in Tables 1 and 3, the performance of\nregression only baseline drops signiﬁcantly in the combined\nscene setting as shown in Table 5a.\nHierarchy and partition granularity The robustness of HSC-\nNet to the label hierarchy hyperparameter by varying depth\nand width are reported in Table 5. The results show that\nthe performance of our approach is robust w.r .t.the choice\nof these hyperparameters, with a signiﬁcant drop in perfor-\nmance observed only for the smallest 2-level label hierarchy.\nIncreasing the number of classiﬁcation layers from 2 is not\nalways beneﬁcial and only brings marginal improvement\nin 7-Scenes, while increasing the computational costs. We\nobserve the best trade-off for the partition of 25 × 25 for\nboth 7-Scenes and 175 × 25 for i7-Scenes (175 = 7 × 25\ndue to 7 scenes combined).\n5.4 Ablations: HSCNet++\nImpact of internal transformer encoder layers. In this abla-\ntion, we remove transformers encoders tr and ts , while only\nt3D remains. This variant is denoted by HSCNet++ † and\nTable 6a shows a small to noticeable drop in all cases.\nTo factor out the impact of multi-headed attention (MHA)\nlayers, we report results in Table 6a, which shows that\nincreasing the number of MHA layers in HSCNet++ † does\nnot lead to performance improvement. It is worth mention-\ning that HSCNet++\n† with 8 MHA layers has 2 million more\nparameters than HSCNet++. Our intuition is that this hap-\npens due to the improvement of predictions at coarse levels\nof the network. To test the above hypothesis, we compute the\naccuracy of the sub-region predictions. For each valid pixel\nin a query image, this metric evaluates whether the valid pixel\nis correctly classiﬁed. Results in Table 6c show that adding\ntransformers at classiﬁcation branches helps to improve the\nlabel classiﬁcation accuracy. However, the sub-region predic-\ntion accuracy does not always correlate with the localization\nperformance. This can be attributed to RANSAC-based ﬁlter-\ning of ﬁnal 3D scene coordinates for camera pose estimation.\nThat is, incorrect 3D scene predictions due to erroneous sub-\nregion predictions can be detected as outliers by RANSAC.\nImpact of positional encoding. We compare the proposed way\nof providing region (position) information to the transformer\nblocks with the classical positional encoding used in trans-\nformers. As label encoding is an inherent part of HSCNet, for\na direct comparison with positional encoding, we addition-\nally add the positional encoding right before the transformer\nblock and perform experiments on i7-Scenes. Results pre-\nsented in Table 6b show that with the additional position\nencoding the results noticeably drop.\n5.5 Results for HSCNet++(S)\nWe now present results for HSCNet++(S) with sparse\nsupervision and study the pseudo-labeling and loss functions\nin detail. For indoor scenes, we synthetically sparsify dense\ncoordinates using sparse SIFT-based SfM reconstruction.\nThat is, we select the subset of dense 3D coordinates whose\n2D re-projections (pixel locations) are also registered in the\n123\n2542 International Journal of Computer Vision (2024) 132:2530–2550\nTable 5 Ablation for HSCNet\n(a) Data augmentation and conditioning mechanism\nMethod Localization Accuracy (%)\ni7-Scenes i12-Scenes i19-Scenes\nH S C N e t( L ie ta l . ,2020) 83.3 99.3 92.5\nw/o conditioning 70.3 97.1 88.1\nw/o augmentation 71.5 98.7 87.9\nReg-only 37.9 5.0 5.7\n(b) Label hierarchy: 7-Scenes dataset\nLabel hierarchy Accuracy, %\n9×9 82.9\n49×49 85.0\n10×100×100 85.9\n10×100×100×100 85.5\n625 85.3\n25×25 84.8\n(c) Label hierarchy: i7-Scenes dataset\nLabel hierarchy Accuracy, %\n63×9 80.6\n343×49 83.7\n70×100×100 83.0\n70×100×100×100 82.1\n7×25×25 83.0\n175×25 83.3\nAverage pose accuracy obtained with different hierarchy settings. The models with 4-level label hierarchy are classiﬁcation-only, i.e. the ﬁnal\nregression layer is omitted\nTable 6 Ablations for HSCNet++. We analyze the inﬂuence of different design choices of the proposed approach on i7-Scenes\nBold values highlight the architecture that gives the best results\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2543\nFig. 4 Scene coordiantes visualization on i7-Scenes. We visualize\nthe scene coordinate predictions for three test images with HSCNet,\nHSCNet++, and HSCNet++(S) on i7-Scenes. The XYZ coordinates are\nmapped to the heatmap, and the ground truth scene coordinates are com-\nputed from the depth maps. For each image, the left column is the correct\npredicted label and the right column is the predicted scene coordinates\nFig. 5 Median Error for HSCNet++(S). We show the frames with median pose estimation error in each scene and visualize the accuracy by\noverlaying the query image (right) with a rendered image (left, grayscale) using the estimated pose and the ground truth 3D model\n123\n2544 International Journal of Computer Vision (2024) 132:2530–2550\nTable 7 HSCNet++(S) results\nMethod Localization\nAccuracy (%) ↑ Error (cm/ ◦) ↓\n7-Scenes i7-Scenes Cambridge\nHSCNet 84.8 83.3 16.0 / 0.28\nHSCNet++ 88.7 88.3 18.6 / 0.28\nHSCNet++(S) 85.2 78.5 12.4 / 0.24\nThe table presents average localization accuracy (%) under 5 cm/5◦ and\naverage median pose error (cm/ ◦) of HSCNet++(S) and dense counter-\nparts on 7-Scenes, i7-Scenes and Cambridge\nFig. 6 Impact of neighborhood size z. The percentage of accurate labels\nand valid pixels change with the increasing of neighborhood window\nsize z\nSfM reconstruction. For the outdoor Cambridge dataset, we\ndirectly obtain the keypoints of training images from the pro-\nvided SfM models.\nThe localization performance on 7-Scenes, i7-Scenes,\nand Cambridge datasets is provided in Fig. 5 and Table 7.\nResults show that even with sparse coordinate supervision,\nHSCNet++(S) achieves competitive results on 7-Scenes with\nrespect to the dense counterpart, even outperforming HSC-\nNet. On the more challenging combined scene setup of\nTable 9 Impact of z on pose estimation\nMethods Scenes\nRed Kitchen GreatCourt\nt,c m r, ◦ Accuracy, % t,c m r, ◦\nz = 0 6 1.37 65.5 32 0.28\nz = 7 4 1.14 70.3 18 0.11\nz = 11 4 1.15 72.9 18 0.11\nz = 15 4 1.12 71.7 21 0.14\nz = 19 3 1.12 73.0 35 0.20\nWe report the pose estimation results (median errors and accuracy) on\nRed Kitchen and Great Court with different neighborhood window size\ni7-Scenes, HSCNet++(S) lacks by 10% indicating a further\nrequirement for future research in this direction. However,\non the outdoor dataset Cambridge Landmarks, where only\nsparse coordinate data is available in most cases, HSC-\nNet++(S) outperforms HSCNet and HSCNet++, which are\ntrained on MVS-densiﬁed (Brachmann & Rother, 2018;\nSchönberger et al., 2016; Li et al., 2020) data, by a large mar-\ngin. It demonstrates the effectiveness of our label propagation\nand supports our hypothesis that noisy dense ground truth\nfrom MVS harms the training process. The largest improve-\nment is observed on Kings College, Great Court and Old\nHospital with median pose errors ( cm/◦)o f1 5/0.24, 18/0.11\nand 15 /0.30 respectively ( c.f.Table 4). On average median\npose error, HSCNet++ (S) outperforms PixLoc (15/0.25),\nVSNet (13.6/0.24) and DSAC* (20.6/0.34).\nComponent ablations We formulate ablations on 7-Scenes to\nexamine the components in the proposed HSCNet++(S). We\nﬁrst train the model without the proposed label propagation,\ni.e. only with sparse keypoint pixels only as the baseline.\nThen, for the HSCNet++(S), we present three variants by\nremoving each component - transformers, symmetric cross-\nentropy and re-projection loss in HSCNet++(S) as shown\nTable 8 Ablations for HSCNet++(S)\nMethod Chess Fire Heads Ofﬁce Pumpkin Kitchen Stairs Average\nError HSCNet++(S) 2 / 0.70 2 / 0.72 1 / 0.8 2 / 0.69 4 / 1.00 4 / 1.15 3 / 1.02 –\nw/o LP 3 / 0.86 3 / 0.91 3 / 1.47 5 / 1.15 6 / 1.37 5 / 1.39 7 / 1.91 –\nw/o Txf 2 / 0.70 2 / 0.94 1 / 0.76 3 / 0.78 4 / 1.12 4 / 1.2 3 / 1.01 –\nw/o SCE 2 / 0.75 2 / 0.77 1 / 0.85 3 / 0.71 4 / 1.04 4 / 1.14 4 / 1.03 –\nw/o Rep 2 / 0.70 2 / 0.80 1 / 0.93 3 / 0.81 4 / 1.09 4 / 1.35 5 / 1.32 –\nAccuracy HSCNet++(S) 98.1 97.0 98.8 88.2 65.1 72.9 76.6 85.2\nw/o LP 86.0 81.1 85.4 56.0 39.4 49.6 36.2 62.0\nw/o Txf 97.3 98.8 99.6 85.6 59.4 64.4 80.5 83.7\nw/o SCE 97.6 96.2 96.5 84.2 64.1 70.1 73.2 83.1\nw/o Rep 97.5 98.2 96.8 80.2 62.8 64.8 55.0 79.3\nThe results of HSCNet++(S) and various variants are presented, the table shows the median translation and rotation errors (Error) and localization\naccuracy (Accuracy) under 5 cm/5 ◦\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2545\nTable 10 Comparison of the\nmodel capacity and runtime Dataset 7-Scenes i7-Scenes\nHSCNet HSCNet++ HSCNet HSCNet++\nModel Size, Mb 147.9 84.5 163 113.5\nTraining time, ms/iter ∼125 ∼89 ∼135 ∼133\nInference time, ms/query ∼85–130 ∼85-130 ∼85-130 ∼85-130\nWe compare the statistics of the model of HSCNet and HSCNet++, we provide the results on the same software\nand hardware setting\nin Table 8. The baseline achieves only 62.0% on average\naccuracy which is signiﬁcantly worse than our result (85.2%).\nV ariants without the use of transformer layers ( w/o Txf),\nSCE and Rep models show worse performance compared\nto HSCNet++(S) on average. Results demonstrate that the\nsynergy of individual components leads the superior results.\nImpact of LP neighborhood size. In this section, we analyze\nthe impact of the LP neighborhood window size, z. We vary\nthe neighborhood size z range from 0 → 19 on RedKitchen\nas ablation, and the results are reported in Fig. 6 and Table 9.\nFigure 6 shows that increasing the size of z, also increases\npseudo-label noise shown by a decrease in the percentage of\naccurate labels. For e.g. when z = 11 the fraction of noisy\nlabels is 15%. Results in Table 9 shows that there is a trade-off\nbetween increasing z, and camera localization accuracy. This\neffect is more pronounced in the outdoor scene, Great Court\nfrom the Cambridge dataset, where increasing z from 0 → 11\nreduces median pose error ( t/r)f r o m3 2 /0.28 → 18/0.11.\nBut increasing z further from 11 → 19 increases median\npose error from 18 /0.11 → 35/0.2. Limiting the spatial\nproximity of pseudo-labels to initial sparse labels seems a\nsuitable choice.\n5.6 Model Capacity and Efficiency\nModel capacity As mentioned in Sect. 4.3, we prune some\nheavy convolution layers compared to HSCNet. To demon-\nstrate the efﬁciency of this setting, Table 10 reports the model\nsize of HSCNet and HSCNet++ on 7-Scenes and i7-Scenes.\nOur method has a memory footprint reduction of 43% com-\npared to HSCNet on the individual scene training and 30%\nreduction on the combined scenes.\nRuntime For a fair comparison of the running time, we run all\nthe experiments on NVIDIA GeForce RTX 2080 Ti GPU and\nAMD Ryzen Threadripper 2950x CPU. It takes ∼7.4 h for\n300k iterations on individual scene training for HSCNet++\nand ∼10.4 h on HSCNet with the same setting. We show\nthe approximate training time for one iteration in Table 10.\nIt is clear that HSCNet++ has a smaller memory footprint\nand faster training time while offering higher accuracy. We\nalso notice that the training time grows with the number of\nmulti-head attention layers increases.\nWe have not observed a clear difference between the two\nmethods in the inference running time. The running time\nvaries from around 85 ms to 130 ms to localize one image.\nThis is mainly dependent on the accuracy of predicted 2D-3D\ncorrespondences fed into the RANSAC-PnP loop.\n6 Conclusion\nWe have proposed a novel hierarchical coarse-to-ﬁne approach\nfor scene coordinate prediction. The network beneﬁts from\nFiLM-like conditioning of coarse region predictions for bet-\nter scene coordinate prediction. Experimentally we demon-\nstrate that both hierarchical and prediction conditioning are\nrequired for improvement. The method is extended to handle\nsparse labels using the proposed pseudo-labeling approach.\nAdaptation of symmetric cross-entropy and re-projection\nlosses provides robustness to pseudo-label noise. We also\nshow that the synergy of each component proposed in this\nwork is needed for the best performance.\nResults show that the proposed hierarchical scene coor-\ndinate network is more accurate than previous regression\nonly approaches for single-image RGB localization. The pro-\nposed method is also more scalable as shown by results on\nthree indoor datasets. In addition, the proposed method is\nextended to handle sparse labels using less costly methods\nthan existing methods and obtaining better results on outdoor\nscenes.\nAcknowledgements This work was supported by the Academy of\nFinland (grant No. 327911, 353138), Junior Star GACR (Grant\nNo. GM 21-28830 M) and Programme Johannes Amos Comenius\nCZ.02.01.01/00/22_010/0003405. We acknowledge the computational\nresources provided by the Aalto Science-IT project, CSC-IT Cen-\nter for Science, Finland, and OP VVV funded project CZ.02.1.01/\n0.0/0.0/16_019/0000765 “Research Center for Informatics”. We thank\nDr. Jakob V erbeek for contributing to the HSCNet publication.\nFunding Open Access funding provided by Aalto University.\nData Availibility The datasets generated during and/or analysed during\nthe current study are available in the RGB-D Dataset 7-Scenes, RGB\nCamera Relocalization 12-Scenes, and PoseNet project repositories.\nDeclarations\nConﬂict of interest The authors have no competing interests to declare\nthat are relevant to the content of this article.\n123\n2546 International Journal of Computer Vision (2024) 132:2530–2550\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nAppendix\nWe detail our experiments on Aachen dataset (Sattler et al.,\n2018). Following the earlier HSCNet work (Li et al., 2020)\nwe modify the architecture for this experiment, and therefore\npresent it separately in this appendix.\nHSCNet++ Architecture for Aachen\nFor large-scale datasets such as Aachen Day-Night, the scene\ncoordinate network is underperforming due to the challenge\nof extracting reliable features in the end-to-end training pro-\ncedure. Thus, instead of training a feature extractor from\nscratch as in the HSCNet dense setting, we leverage the pre-\ntrained SuperPoint network (DeTone et al., 2018) to extract\nmore reliable image features as input. We modify our network\nto consider the SuperPoint features as input. Therefore, the\ndense set of local features is replaced by a sparse set of fea-\ntures. As a consequence, in the follow-up processing we are\nusing convolutional layers with 1 × 1 convolutions. FiLM\nconditioning layers together with transformer modules are\nintegrated in a similar way.\nDue to the large scale of the scene, a retrieval process is\nused during inference to provide contextual evidence. Pre-\ndictions are conditioned on the retrieved image id. During\ntraining, the image id of each training image is used as addi-\ntional input, in the same spirit as the region labels. It is seen as\nthe coarsest piece of localization information within the large\nscale scene; next coarsest is the discretized region labels. Dur-\ning inference, the image id of the retrieved image is provided\nas additional input. The retrieval method used is NetVLAD\n(Arandjelovi´ce ta l . , 2016) and the search is performed with\nthe test image as query and the training images as database.\nWe use multiple top retrieved images, perform the process\nfor each of them, and maintain the predicted camera pose\nassociated with the largest number of inliers. The detailed\narchitecture of HSCNet++ for Aachen is shown in Fig. 7 and\ndenoted by HSCNet++(A). This variant only relies on clas-\nsiﬁcation branches and no regression branch is used, which\nmeans that the ﬁnal predictions are quantized 3D coordinates.\nThere are four classiﬁcation branches in total. K-means with\na branching factor of 100 is used, which results in 685k valid\nFig. 7 An overview of the proposed HSCNet++(A) on Aachen. The ﬁgure shows the network architecture of the modiﬁed HSCNet++ for large-scale\nAachen dataset. Here, y0, y1, y2, y3 are coarse-to-ﬁne label predictions\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2547\nTable 11 Accuracy on the Aachen dataset\nMethod Aachen day Aachen night\n0.25m, 2° / 0.5m, 5° / 5 m, 10° 0.5m, 2° / 1 m, 5° / 5 m, 10°\nAS (Sattler et al., 2016b) 57.3 / 83.7 / 96.6 28.6 / 37.8 / 51.0\nHLoc (Sarlin et al., 2020) 89.6 / 95.4 / 98.88 6 .7 / 93.9 / 100.0\nPixLoc (Sarlin et al., 2021) 64.3 / 69.3 / 77.4 51.0 / 55.1 / 67.3\nESAC (50 experts) (Brachmann & Rother, 2019b) 42.6 / 59.6 / 75.5 6.1 / 10.2 / 18.4\nHSCNet++(A) 72.7 / 81.6 / 91.4 43.9 / 57.1 / 76.5\nHSCNet(A) top-10 71.1 / 81.9 / 91.7 40.8 / 56.1 / 76.5\nHSCNet(A) top-1 64.0 / 76.1 / 85.4 28.6 / 38.8 / 59.2\nHSCNet(A) top-1 (regression) 47.8 / 61.8 / 79.9 11.2 / 17.3 / 39.8\nHSCNet(A) w/o retrieval 50.6 / 56.3 / 70.1 12.2 / 12.2 / 22.4\nWe report localization performance as a percentage (%) of correctly localized query images for 3 different thresholds. The best results are highligh ted\nin bold\nclusters at the ﬁnest level. Removing transformer modules\nfrom this architecture results in the HSCNet(A) architecture.\nThe network is trained for 900K iterations with a batch\nsize of 1 and a learning rate of 10 −4. We use Adam (Kingma\n&B a , 2014) optimizer and halve the learning rate every 50K\niterations for the last 200K iterations. During training, only\nthose Superpoint keypoints are kept that are triangulated in\nthe sparse 3D model. At test time, top 2K Superpoint key-\npoints are kept per image based on keypoint scores after\nnon-maximum suppression (NMS).\nResults are presented in Table 11. Using more neighbors\nprovides a good performance boost, while not conditioning\non the image ids, therefore not using retrieval at all during\ninference, results in a large drop in performance. Chang-\ning the large branch into regression instead of classiﬁcation\ncompromises performance as well. The transformer mod-\nules noticeably boost the performance in this experiment\nas well. We compare with ESAC (Brachmann & Rother,\n2019a), PixLoc (Sarlin et al., 2021) and local feature-based\nmethods AS (Sattler et al., 2016b) and HLoc (Sarlin et\nal., 2020). The results indicate HSCNet++ surpasses end-\nto-end methods, ESAC and PixLoc across most thresholds.\nThe proposed approach, alongside other end-to-end methods,\nfalls short compared to local feature-based methods such as\nHLoc (Sarlin et al., 2020). The performance gap becomes\nmore evident in night-time settings showing the limited\nrobustness of end-to-end methods to illumination variations.\nHowever, HLoc’s reliance on maintaining 3D maps can be\nquite challenging for large-scale environments, especially\non mobile devices constrained by storage and communi-\ncation bandwidth limitations. Therefore, the consideration\nof the memory-accuracy trade-off is imperative. While our\nmodel only requires 0.24GB, local feature methods like HLoc\ndemand 7.8GB for their local descriptor database. Neverthe-\nless, the accuracy of the proposed method is susceptible to\na notable decline when faced with a substantial increase in\nscene scale for a ﬁxed model size. This limitation could be\naddressed by deploying different models for distinct parts of\na large scene by maintaining the memory-accuracy trade-off.\nReferences\nArandjelovi´c, R., Gronat, P ., Torii, A., Pajdla, T. & Sivic, J. (2016).\nNetVLAD: CNN architecture for weakly supervised place recog-\nnition. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition (CVPR) (pp. 5297–5307).\nBalntas, V ., Li, S. & Prisacariu, V . (2018). RelocNet: Continuous metric\nlearning relocalisation using neural nets. In Proceedings of the\nEuropean conference on computer vision (ECCV) (pp. 751–767).\nSpringer International Publishing.\nBalntas, V ., Riba, E., Ponsa, D. & Mikolajczyk, K. (2016). Learning\nlocal feature descriptors with triplets and shallow convolutional\nneural networks. In Proceedings of the British machine vision con-\nference (BMVC)\nBay, H., Tuytelaars, T. & V an Gool, L. (2006). SURF: Speeded up robust\nfeatures. In Proceedings of the European conference on computer\nvision (ECCV) (pp. 404–417). Springer International Publishing.\nBrachmann, E., Humenberger, M., Rother, C. & Sattler, T. (2021). On\nthe limits of pseudo ground truth in visual camera re-localisation.\nIn Proceedings of the IEEE/CVF international conference on com-\nputer vision (ICCV) (pp. 6218–6228).\nBrachmann, E., Krull, A., Nowozin, S., Shotton, J., Michel, F.,\nGumhold, S. & Rother, C. (2017). DSAC - Differentiable\nRANSAC for camera localization. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (CVPR) (pp. 6684–6692).\nBrachmann, E., Michel, F., Krull, A., Yang, M.Y ., Gumhold, S. &\nRother, C. (2016). Uncertainty-driven 6D pose estimation of\nobjects and scenes from a single RGB image. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR) (pp. 3364–3372).\nBrachmann, E. & Rother, C. (2018). Learning less is more - 6D cam-\nera localization via 3D surface regression. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) (pp. 4654–4662)\nBrachmann, E. & Rother, C. (2019). Expert sample consensus applied\nto camera re-localization. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision (ICCV) (pp. 7524–7533)\n123\n2548 International Journal of Computer Vision (2024) 132:2530–2550\nBrachmann, E. & Rother, C. (2019). Expert sample consensus applied\nto camera re-localization. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision (ICCV) (pp. 7525–7534).\nBrachmann, E. & Rother, C. (2019). Neural-guided RANSAC: Learn-\ning where to sample model hypotheses. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV)\n(pp. 4322–4331).\nBrachmann, E., & Rother, C. (2021). Visual camera re-localization from\nRGB and RGB-D images using DSAC. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 44 (9), 5847–5865.\nBrahmbhatt, S., Gu, J., Kim, K., Hays, J. & Kautz, J. (2018). Geometry-\naware learning of maps for camera localization. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recog-\nnition (CVPR) (pp. 2616–2625).\nBudvytis, I., Teichmann, M., V ojir, T. & Cipolla, R. (2019). Large scale\njoint semantic re-localisation and scene understanding via glob-\nally unique instance coordinate regression. In Proceedings of the\nBritish machine vision conference (BMVC)\nBui, M., Albarqouni, S., Ilic, S. & Navab, N. (2018). Scene coordi-\nnate and correspondence learning for image-based localization. In\nProceedings of the British machine vision conference (BMVC)\nCalonder, M., Lepetit, V ., Strecha, C. & Fua, P . (2010). BRIEF: Binary\nrobust independent elementary features. In Proceedings of the\nEuropean conference on computer vision (ECCV) (pp. 778–792).\nSpringer Berlin Heidelberg\nCavallari, T., Bertinetto, L., Mukhoti, J., Torr, P . & Golodetz, S. (2019).\nLet’s take this online: Adapting scene coordinate regression net-\nwork predictions for online RGB-D camera relocalisation. In:\nInternational conference on 3D vision (3DV) (pp. 564–573).\nCavallari, T., Golodetz, S., Lord, N., V alentin, J., Prisacariu, V ., Di\nStefano, L., & Torr, P . H. (2020). Real-time RGB-D camera\npose estimation in novel scenes using a relocalisation cascade.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n42(10), 2465–2477.\nCavallari, T., Golodetz, S., Lord, N.A., V alentin, J., Di Stefano, L. &\nTorr, P .H. (2017). On-the-ﬂy adaptation of regression forests for\nonline camera relocalisation. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition (CVPR) (pp.\n4457–4466).\nChen, S., Li, X., Wang, Z. & Prisacariu, V . (2022). Dfnet: Enhance\nabsolute pose regression with direct feature matching. In Proceed-\nings of the European conference on computer vision (ECCV) (pp.\n1–17). Springer Nature Switzerland.\nChen, S., Wang, Z. & Prisacariu, V . (2021). Direct-posenet: Absolute\npose regression with photometric consistency. In International\nconference on 3D vision (3DV) (pp. 1175–1185).\nDeTone, D., Malisiewicz, T. & Rabinovich, A. (2018). Superpoint: Self-\nsupervised interest point detection and description. In Proceedings\nof the IEEE conference on computer vision and pattern recognition\nworkshops (pp. 224–236).\nDing, M., Wang, Z., Sun, J., Shi, J. & Luo, P . (2019). CamNet: Coarse-\nto-ﬁne retrieval for camera re-localization. In Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV)\n(pp. 2871–2880).\nDusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A.\n& Sattler, T. (2019). D2-Net: A trainable CNN for joint detection\nand description of local features. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (CVPR)\n(pp. 8092–8101).\nFischler, M. A., & Bolles, R. C. (1981). Random sample consensus:\nA paradigm for model ﬁtting with applications to image analysis\nand automated cartography. Communications of the ACM, 24 (6),\n381–395.\nGuan, P ., Cao, Z., Y u, J., Zhou, C., & Tan, M. (2021). Scene coor-\ndinate regression network with global context-guided spatial\nfeature transformation for visual relocalization. IEEE Robotics and\nAutomation Letters, 6 (3), 5737–5744.\nGuzmán-Rivera, A., Kohli, P ., Glocker, B., Shotton, J., Sharp, T.,\nFitzgibbon, A.W. & Izadi, S. (2014). Multi-output learning for\ncamera relocalization. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition (CVPR) (pp.\n1114–1121).\nHan, X., Leung, T., Jia, Y ., Sukthankar, R. & Berg, A.C. (2015).\nMatchnet: Unifying feature and metric learning for patch-based\nmatching. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR) (pp. 3279–3286).\nHuang, Z., Zhou, H., Li, Y ., Yang, B., Xu, Y ., Zhou, X., Bao, H., Zhang,\nG. & Li, H. (2021). VS-Net: V oting with segmentation for visual\nlocalization. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR) (pp. 6101–6111).\nJiang, W., Trulls, E., Hosang, J., Tagliasacchi, A. & Yi, K.M. (2021).\nCOTR: Correspondence transformer for matching across images.\nIn Proceedings of the IEEE/CVF international conference on com-\nputer vision (ICCV) (pp. 6207–6217).\nKatharopoulos, A., Vyas, A., Pappas, N. & Fleuret, F. (2020). Trans-\nformers are rnns: Fast autoregressive transformers with linear\nattention. In Proceedings of the 37th international conference on\nmachine learning (ICML) (pp. 5156–5165). JMLR\nKendall, A. & Cipolla, R. (2016). Modelling uncertainty in deep\nlearning for camera relocalization. In Proceedings of the IEEE\ninternational conference on robotics and automation (ICRA) (pp.\n4762–4769).\nKendall, A., Cipolla, R. (2017). Geometric loss functions for cam-\nera pose regression with deep learning. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (CVPR) (pp. 5974–5983).\nKendall, A., Gal, Y . & Cipolla, R. (2018). Multi-task learning using\nuncertainty to weigh losses for scene geometry and semantics. In\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition (CVPR) (pp 7482–7491).\nKendall, A., Grimes, M. & Cipolla, R. (2015). PoseNet: A convolutional\nnetwork for real-time 6-DoF camera relocalization. In: Proceed-\nings of the IEEE/CVF international conference on computer vision\n(ICCV) (pp 2938–2946).\nKingma, D.P . & Ba, J. (2014). Adam: A method for stochastic opti-\nmization. arXiv:1412.6980\nLaskar, Z., Melekhov, I., Kalia, S. & Kannala, J. (2017). Camera relocal-\nization by computing pairwise relative poses using convolutional\nneural network. In Proceedings of the IEEE/CVF international\nconference on computer vision (ICCV) workshops (pp. 929–938).\nLi, X., Wang, S., Zhao, Y ., V erbeek, J. & Kannala, J. (2020). Hier-\narchical scene coordinate classiﬁcation and regression for visual\nlocalization. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR) (pp. 11,983–11,992).\nLi, X., Ylioinas, J. & Kannala, J. (2018). Full-frame scene coordi-\nnate regression for image-based localization. In Proceedings of\nrobotics: science and systems (RSS)\nLi, X., Ylioinas, J., V erbeek, J. & Kannala, J. (2018). Scene coordinate\nregression with angle-based reprojection loss for camera relocal-\nization. In Proceedings of the European conference on computer\nvision (ECCV) workshops (pp 229–245). Springer International\nPublishing.\nLi, X., Ylioinas, J., V erbeek, J. & Kannala, J. (2018). Scene coordinate\nregression with angle-based reprojection loss for camera relocal-\nization. In Proceedings of the European conference on computer\nvision (ECCV) workshops (pp. 0–0).\nLowe, D. G. (2004). Distinctive image features from scale-invariant\nkeypoints. International Journal of Computer Vision, 60 (2), 91–\n110.\nLuo, Z., Shen, T., Zhou, L., Zhang, J., Yao, Y ., Li, S., Fang, T. & Quan,\nL. (2019). Contextdesc: Local descriptor augmentation with cross-\n123\nInternational Journal of Computer Vision (2024) 132:2530–2550 2549\nmodality context. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition (CVPR) (pp 2527–2536).\nMassiceti, D., Krull, A., Brachmann, E., Rother, C., & Torr, P .H. (2017).\nRandom forests versus neural networks—What’s best for camera\nlocalization? In Proceedings of the IEEE International Conference\non Robotics and Automation (ICRA) (pp. 5118–5125).\nMelekhov, I., Brostow, G.J., Kannala, J. & Turmukhambetov, D.\n(2020). Image stylization for robust features. ArXiv preprint\narXiv:2008.06959.\nMelekhov, I., Kannala, J. & Rahtu, E. (2017). Image patch match-\ning using convolutional descriptors with euclidean distance. In\nProceedings of the Asian conference on computer vision (ACCV)\nworkshops (pp. 638–653). Springer.\nMelekhov, I., Laskar, Z., Li, X., Wang, S. & Juho, K. (2021). Digging\ninto self-supervised learning of feature descriptors. In: Interna-\ntional conference on 3D vision (3DV) ( pp. 1144–1155).\nMelekhov, I., Ylioinas, J., Kannala, J. & Rahtu, E. (2017). Image-\nbased localization using hourglass networks. In: Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV)\nWorkshops (pp. 879–886).\nMeng, L., Chen, J., Tung, F., Little, J.J., V alentin, J. & de Silva,\nC.W. (2017). Backtracking regression forests for accurate cam-\nera relocalization. In Proceedings of the IEEE/RSJ international\nconference on intelligent robots and systems (IROS) (pp. 6886–\n6893).\nMeng, L., Tung, F., Little, J.J., V alentin, J. & de Silva, C.W. (2018).\nExploiting points and lines in regression forests for RGB-D cam-\nera relocalization. In Proceedings of the IEEE/RSJ international\nconference on intelligent robots and systems (IROS) (pp. 6827–\n6834).\nMishchuk, A., Mishkin, D., Radenovic, F. & Matas, J. (2017). Working\nhard to know your neighbor’s margins: Local descriptor learn-\ning loss. In Advances in Neural Information Processing Systems\n(NIPS)( vol. 30, pp. 4826–4837). Curran Associates, Inc.\nMoreau, A., Piasco, N., Tsishkou, D., Stanciulescu, B. & de La Fortelle,\nA. (2021). LENS: Localization enhanced by neRF synthesis. In\nAnnual conference on robot learning\nPerez, E., Strub, F., De Vries, H., Dumoulin, V ., & Courville, A. (2018).\nFilm: Visual reasoning with a general conditioning layer. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, 32 (1),\n3942–3951.\nRadenovi´c, F., Tolias, G.& Chum, O. (2016). CNN image retrieval\nlearns from BoW: Unsupervised ﬁne-tuning with hard examples.\nIn Proceedings of the European conference on computer vision\n(ECCV) (pp. 3–20). Springer International Publishing.\nRadwan, N., V alada, A., & Burgard, W. (2018). VLocNet++: Deep\nmultitask learning for semantic visual localization and odometry.\nIEEE Robotics and Automation Letters, 3 (4), 4407–4414.\nRevaud, J., De Souza, C., Humenberger, M. & Weinzaepfel, P .\n(2019). R2D2: Reliable and repeatable detector and descriptor.\nIn: Advances in neural information processing systems (NeurIPS)\n(V ol. 32, pp. 12,405–12,415). Curran Associates, Inc.\nRogez, G., Weinzaepfel, P . & Schmid, C. (2017). LCR-Net:\nLocalization-classiﬁcation-regression for human pose. In: Pro-\nceedings of the IEEE/CVF conference on computer vision and\npattern recognition (CVPR) (pp. 3433–3441).\nRogez, G., Weinzaepfel, P ., & Schmid, C. (2019). LCR-Net++: Multi-\nperson 2D and 3D pose detection in natural images. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 42 (5),\n1146–1161.\nRublee, E., Rabaud, V ., Konolige, K. & Bradski, G.R. (2011). ORB:\nAn efﬁcient alternative to SIFT or SURF. In: Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV)\n(pp. 2564–2571).\nSaha, S., V arma, G. & Jawahar, C. (2018). Improved visual relocaliza-\ntion by discovering anchor points. In Proceedings of the British\nmachine vision conference (BMVC)\nSarlin, P .E., Cadena, C., Siegwart, R. & Dymczyk, M. (2019). From\ncoarse to ﬁne: Robust hierarchical localization at large scale. In:\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition (CVPR) (pp 12,716–12,725).\nSarlin, P .E., DeTone, D., Malisiewicz, T. & Rabinovich, A. (2020).\nSuperglue: Learning feature matching with graph neural networks.\nIn Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition (CVPR) (pp 4938–4947).\nSarlin, P .E., Unagar, A., Larsson, M., Germain, H., Toft, C., Larsson, V .,\nPollefeys, M., Lepetit, V ., Hammarstrand, L., Kahl, F., & Sattler,\nT. (2021). Back to the feature: Learning robust camera localization\nfrom pixels to pose. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition (CVPR) (pp. 3247–\n3257).\nSattler, T., Leibe, B., & Kobbelt, L. (2011). Fast image-based local-\nization using direct 2d-to-3d matching. In Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV)\n(pp. 667–674).\nSattler, T., Leibe, B., & Kobbelt, L. (2012). Improving image-based\nlocalization by active correspondence search. In Proceedings of\nthe European Conference on computer vision (ECCV) (pp. 752–\n765). Springer International Publishing.\nSattler, T., Leibe, B., & Kobbelt, L. (2016). Efﬁcient & effective pri-\noritized matching for large-scale image-based localization. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 39 (9),\n1744–1756.\nSattler, T., Leibe, B., & Kobbelt, L. (2016). Efﬁcient & effective pri-\noritized matching for large-scale image-based localization. IEEE\nTransactions on Pattern Analysis And Machine Intelligence, 39 (9),\n1744–1756.\nSattler, T., Maddern, W., Toft, C., Torii, A., Hammarstrand, L., Sten-\nborg, E., Safari, D., Okutomi, M., Pollefeys, M., Sivic, J., Kahl,\nF., Pajdla, T. (2018). Benchmarking 6DoF outdoor visual local-\nization in changing conditions. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (CVPR)\n(pp. 8601–8610).\nSattler, T., Zhou, Q., Pollefeys, M. & Leal-Taixe, L. (2019). Understand-\ning the limitations of CNN-based absolute camera pose regression.\nIn Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition (CVPR) (pp. 3302–3312).\nSchönberger, J.L., Zheng, E., Pollefeys, M. & Frahm, J.M. (2016).\nPixelwise view selection for unstructured multi-view stereo. In\nProceedings of the European conference on computer vision\n(ECCV)\nShavit, Y ., Ferens, R. & Keller, Y . (2021). Learning multi-scene abso-\nlute pose regression with transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV)\n(pp 2733–2742).\nShavit, Y . & Keller, Y . (2022). Camera pose auto-encoders for improv-\ning pose regression. In Proceedings of the European conference\non computer vision (ECCV) (pp. 140–157). Springer International\nPublishing\nShotton, J., Glocker, B., Zach, C., Izadi, S., Criminisi, A., & Fitzgib-\nbon, A. (2013). Scene coordinate regression forests for camera\nrelocalization in RGB-D images. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (CVPR)\n(pp. 2930–2937).\nSimo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P ., & Moreno-\nNoguer, F. (2015). Discriminative learning of deep convolutional\nfeature point descriptors. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision (ICCV) (pp. 118–126).\nSun, J., Shen, Z., Wang, Y ., Bao, H. & Xiaowei, Z. (2021). LoFTR:\nDetector-free local feature matching with transformers. In Pro-\n123\n2550 International Journal of Computer Vision (2024) 132:2530–2550\nceedings of the IEEE/CVF conference on computer vision and\npattern recognition (CVPR) (pp. 8922–8931).\nTaira, H., Okutomi, M., Sattler, T., Cimpoi, M., Pollefeys, M., Sivic,\nJ., Pajdla, T., & Torii, A. (2018). Inloc: Indoor visual localization\nwith dense matching and view synthesis. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) (pp. 7199–7209).\nTian, Y ., Fan, B. & Wu, F. (2017). L2-net: Deep learning of discrimi-\nnative patch descriptor in euclidean space. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) (pp. 661–669).\nTyszkiewicz, M., Fua, P ., & Trulls, E. (2020). DISK: Learning local fea-\ntures with policy. In Advances in neural information processing\nsystems (NeurIPS) (V ol. 33, pp. 14,254–14,265). Curran Asso-\nciates, Inc.\nV alada, A., Radwan, N. & Burgard, W. (2018). Deep auxiliary learning\nfor visual localization and odometry. In Proceedings of the IEEE\ninternational conference on robotics and automation (ICRA) (pp.\n6939–6946).\nV alentin, J., Dai, A., Nießner, M., Kohli, P ., Torr, P ., Izadi, S., & Keskin,\nC. (2016). Learning to navigate the energy landscape. In Interna-\ntional conference on 3D vision (3DV) (pp. 323–332).\nV alentin, J., Nießner, M., Shotton, J., Fitzgibbon, A., Izadi, S., &\nTorr, P .H. (2015). Exploiting uncertainty in regression forests for\naccurate camera relocalization. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition (CVPR)\n(pp. 4400–4408).\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need.\nIn Advances in neural information processing systems (NeurIPS)\n(V ol. 30, pp. 5998–6008). Curran Associates, Inc.\nWalch, F., Hazirbas, C., Leal-Taixe, L., Sattler, T., Hilsenbeck, S., &\nCremers, D. (2017). Image-based localization using LSTMs for\nstructured feature correlation. In Proceedings of the IEEE/CVF\ninternational conference on computer vision (ICCV) (pp. 627–\n637).\nWang, Q., Zhou, X., Hariharan, B., & Snavely, N. (2020). Learning\nfeature descriptors using camera pose supervision. In Proceedings\nof the European conference on computer vision (ECCV) (pp. 757–\n774). Springer International Publishing\nWang, S., Laskar, Z., Melekhov, I., Li, X., & Kannala, J. (2021). Contin-\nual learning for image-based camera localization. In Proceedings\nof the IEEE/CVF international conference on computer vision\n(ICCV) (pp. 3252–3262).\nWang, Y ., Ma, X., Chen, Z., Luo, Y ., Yi, J., & Bailey, J. (2019).\nSymmetric cross entropy for robust learning with noisy labels. In\nProceedings of the IEEE/CVF international conference on com-\nputer vision (ICCV) (pp. 322–330).\nWeinzaepfel, P ., Csurka, G., Cabon, Y ., & Humenberger, M. (2019).\nVisual localization by learning objects-of-interest dense match\nregression. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition (CVPR) (pp. 5634–5643).\nXue, F., Wang, X., Yan, Z., Wang, Q., Wang, J., & Zha, H. (2019).\nLocal supports global: Deep camera relocalization with sequence\nenhancement. In Proceedings of the IEEE/CVF international con-\nference on computer vision (ICCV) (pp. 2841–2850).\nXue, F., Wu, X., Cai, S., & Wang, J. (2020). Learning multi-view camera\nrelocalization with graph neural networks. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) (pp. 11,375–11,384).\nZagoruyko, S., & Komodakis, N. (2015). Learning to compare image\npatches via convolutional neural networks. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition\n(CVPR) (pp. 4353–4361).\nZhou, Q., Sattler, T., & Leal-Taixé, L. (2021). Patch2Pix: Epipolar-\nguided pixel-level correspondences. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion (CVPR) (pp. 4669–4678).\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123"
}