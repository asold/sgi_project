{
  "title": "Use of large language models as a scalable approach to understanding public health discourse",
  "url": "https://openalex.org/W4391558736",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2294664728",
      "name": "Laura Espinosa",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2284353421",
      "name": "Marcel Salathé",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2294664728",
      "name": "Laura Espinosa",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A2284353421",
      "name": "Marcel Salathé",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2015058772",
    "https://openalex.org/W2157884187",
    "https://openalex.org/W2637751811",
    "https://openalex.org/W2951014126",
    "https://openalex.org/W2911772671",
    "https://openalex.org/W2939633060",
    "https://openalex.org/W3036887448",
    "https://openalex.org/W3120829418",
    "https://openalex.org/W3173797296",
    "https://openalex.org/W4318344493",
    "https://openalex.org/W2471350540",
    "https://openalex.org/W3133971464",
    "https://openalex.org/W4323665848",
    "https://openalex.org/W2049271634",
    "https://openalex.org/W2962707339",
    "https://openalex.org/W4298086570",
    "https://openalex.org/W2923280274",
    "https://openalex.org/W1970381522",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4390723197",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W4391558736",
    "https://openalex.org/W2169203263",
    "https://openalex.org/W4313567270",
    "https://openalex.org/W3011566912",
    "https://openalex.org/W4318459849",
    "https://openalex.org/W2625815975",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W4388735460",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W3121430869"
  ],
  "abstract": "Abstract Online public health discourse is becoming more and more important in shaping public health dynamics. Large Language Models (LLMs) offer a scalable solution for analysing the vast amounts of unstructured text found on online platforms. Here, we explore the effectiveness of Large Language Models (LLMs), including GPT models and open-source alternatives, for extracting public stances towards vaccination from social media posts. Using an expert-annotated dataset of social media posts related to vaccination, we applied various LLMs and a rule-based sentiment analysis tool to classify the stance towards vaccination. We assessed the accuracy of these methods through comparisons with expert annotations and annotations obtained through crowdsourcing. Our results demonstrate that few-shot prompting of best-in-class LLMs are the best performing methods, and that all alternatives have significant risks of substantial misclassification. The study highlights the potential of LLMs as a scalable tool for public health professionals to quickly gauge public opinion on health policies and interventions, offering an efficient alternative to traditional data analysis methods. With the continuous advancement in LLM development, the integration of these models into public health surveillance systems could substantially improve our ability to monitor and respond to changing public health attitudes. Authors summary We examined how Large Language Models (LLMs), including GPT models and open-source versions, can analyse online discussions about vaccination from social media. Using a dataset with expert-checked posts, we tested various LLMs and a sentiment analysis tool to identify public stance towards vaccination. Our findings suggest that using LLMs, and prompting them with labelled examples, is the most effective approach. The results show that LLMs are a valuable resource for public health experts to quickly understand the dynamics of public attitudes towards health policies and interventions, providing a faster and efficient option compared to traditional methods. As LLMs continue to improve, incorporating these models into digital public health monitoring could greatly improve how we observe and react to dynamics in public health discussions.",
  "full_text": "Useoflargelanguagemodelsasa\nscalableapproachtounderstanding\npublichealthdiscourse\nShorttitle:\nLargelanguagemodelsforunderstandingpublichealth\ndiscourse\nLaura Espinosa1*, Marcel Salathé1\n1 DigitalEpidemiologyLab,SchoolofLifeSciences,SchoolofComputerand\nCommunicationSciences,EPFL,Switzerland\n*Correspondingauthor.Email: laura.espinosamontalban@epﬂ.ch\nAbstract\nOnlinepublichealthdiscourseisbecomingmoreandmoreimportantinshapingpublic\nhealth dynamics. Large Language Models(LLMs)offerascalablesolutionforanalysing\nthevastamountsofunstructuredtextfoundononlineplatforms.Here,weexplorethe\neffectivenessofLargeLanguageModels(LLMs),includingGPTmodelsandopen-source\nalternatives,forextractingpublicstancestowardsvaccinationfromsocialmediaposts.\nUsing an expert-annotated dataset of social media posts related to vaccination, we\napplied various LLMs and a rule-based sentiment analysis tool to classify the stance\ntowardsvaccination.Weassessedtheaccuracyofthesemethodsthroughcomparisons\nwith expertannotationsandannotationsobtainedthroughcrowdsourcing.Ourresults\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\ndemonstrate that few-shot prompting of best-in-class LLMs are the best performing\nmethods, and that allalternativeshavesigniﬁcantrisksofsubstantialmisclassi ﬁcation.\nThe study highlights the potential of LLMs as a scalable tool for public health\nprofessionals to quickly gauge public opinion on health policies and interventions,\noffering an efﬁcient alternative to traditional data analysis methods. With the\ncontinuous advancement in LLM development, the integration of these models into\npublic health surveillance systems could substantially improve our ability to monitor\nandrespondtochangingpublichealthattitudes.\nAuthorssummary\nWe examined how Large Language Models (LLMs), including GPT models and\nopen-source versions, can analyse online discussions about vaccination from social\nmedia. Using a dataset with expert-checked posts, we tested various LLMs and a\nsentiment analysis tool to identify public stance towards vaccination. Ourﬁndings\nsuggest that using LLMs, and prompting them with labelled examples, is the most\neffectiveapproach.TheresultsshowthatLLMsareavaluableresourceforpublichealth\nexperts to quicklyunderstandthedynamicsofpublicattitudestowardshealthpolicies\nand interventions, providing a faster and efﬁcient option compared to traditional\nmethods. As LLMs continue to improve,incorporatingthesemodelsintodigitalpublic\nhealth monitoring could greatly improve how we observe and react to dynamics in\npublichealthdiscussions.\nIntroduction\nPublic health is “the art and science of preventing disease, prolonging life and\npromotinghealththroughtheorganizedeffortsofsociety”[1].Theoverallaimofpublic\nhealth is to sustainably promote the health and wellbeing of the entire population,\nstrengthening public health services and reducing inequalities [2], including\nsurveillance,preventionandcontrolofinfectiousdiseases,promotinghealthylifestyles,\nresearchingdisease,andpreventinginjury.Designatedpopulationscanvary,depending\noncontext,fromsmallneighbourhoodstoglobalregions[3].\nOne of the greatest achievements of public health in the 20th century is vaccination\nagainstinfectiousdiseases,whichsaveslives,improvesindividualandpopulationhealth,\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nand reduces health care costs by billions of dollars annually. Despite the success of\nvaccines, uptake of vaccination is still inadequate, often delayed, and unstable [4].\nUnderstandingthefactorsthataffectvaccineacceptanceorrefusaliskeytoincreasing\nvaccination uptake. In October 2014, World Health Organization (WHO) Strategic\nAdvisory Group of Experts on Immunization (SAGE) published a report of the SAGE\nWorkingGrouponVaccineHesitancyoutlyingseveralfactorsthatcanin ﬂuencevaccine\nacceptanceorrefusalgroupedinfourcategories:populations,subgroups,communities,\nandindividuals[5].\nSocial listening is “an active process of attending to, observing, interpreting, and\nrespondingtoavarietyofstimulithroughmediated,electronic,andsocialchannels”[6].\nIthelpspublichealthexpertstobetterengagewiththepopulation[7].Socialmediahas\nproven to be useful in public health, for example, for understanding the public\nperceptionofanepidemic[8,9].InthecontextofCOVID-19pandemic,ascopingreview\nanalysed81studiesonsocialmediafrom2020;fromtheseandusingamodi ﬁedversion\noftheframeworkSocialMediaandPublicHealthEpidemicandResponse(SPHERE)[10],\n48 studies were categorised in the public attitudes theme [11]. A systematic review\nperformedin2019onexistingopinionminingapproachesusingdatafromsocialmedia\nplatforms found that 94% of the 461 included studies use data only from one social\nmedia platform and thevastmajorityofthestudiesuseddatafromX(formerlyknown\nas Twitter), followed by Sina Weibo and Facebook [12]. Furthermore, in another\nsystematic review of techniques for stance detection published in 2023 [13], social\nmediawasthedatasourceof72%ofthe96includedarticlesinthestudy.\nSeveralstudieshaveusednaturallanguageprocessingalgorithmstoextractthegeneral\nsentimentofatext[14],andmachinelearningapproacheswithlabelleddatainorderto\ninfer public health opinion from social media data. Inaddition,thereareopen-source\ntools available to support suchprocesses,suchasCrowdbreaks[15]andepitweetr[16]\nwhich use machine learning methods and labelled data to monitor Xforextractionof\nstancetowardspublichealthinterventionsandearlydetectionofpublichealththreats.\nHowever, large annotated datasets are required to have enough representation ofthe\nspeciﬁc tasks for ensuring proper learning and to have enough domain-speciﬁc\nexamples when applying machine or deep learning techniques to extract or infer the\nstance from a text, especially in unstructured data such as social media [17].\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nFurthermore,havingnon-expertsannotatorswillrequireahighernumberofannotators\nto equal the results fromtheexpertannotators,andincreasecosts[18].Theseaspects\nmaybeunderestimatedornotproperlyconsideredwhenconceptualisingsuchstudies.\nIn recent years, the emergence of Large Language Models (LLMs) haverevolutionised\nthe ﬁeld of natural language (NLP) processing. The automated identiﬁcation of a text\nauthor’s position towards aspeciﬁctopic(stancedetection)iskeyinpublichealthand\nother ﬁelds. LLMs, thanks to their vast training data and enormous model size with\noftenbillionsofparameters,havedemonstratedremarkablepro ﬁciencyinthisdomain.\nA study from Brown et al., 2020, illustrated the effectiveness of models like GPT-3 in\naccurately determining stance from text,outperformingNLPmodelsinbothprecision\nand recall [19]. Furthermore, the adaptability of LLMs to diverse datasets and their\ncapacity to understand nuanced expressions have made them invaluable tools for\nresearchers and practitioners alike. For instance, Devlin et al., 2018, highlighted the\nversatility of theBidirectionalEncoderRepresentationsfromTransformersinhandling\ncontext-dependent stance detection tasks, setting aprecedentforthemoreadvanced\nLLMs[20].\nMore recently, LLMs have entered the public debate with the launch of ChatGPT, a\nchatbot originally based on the GPT-3.5 model, and more recently the GPT-4 model.\nTheseLLMsbyOpenAIhavetriggeredunprecedentedpublicdiscoursearoundarti ﬁcial\nintelligenceacrossalldomains.ShortlyafterthelaunchofChatGPT,the ﬁrstopenLLMs\nwerereleased,notablytheLlamamodelsbyMeta(e.g.Llama2[CITE]).Duringtheyear\nof 2023, new incumbents such as Mistral AI released a number of highly performant\nopen LLMs as a free alternative to commercially available models such as the GPT\nmodels. Early results have shown the incredible power in text understanding across\ndomains that these models provided out of the box, pre-trained. This opened up the\nopportunity for the use of such models for large-scale social media analysis in many\ndomains, including public health, without the resource-intensive steps of data\nannotationandmodeltrainingforeveryindividualusecase.\nThe objective of this study was to assess how these new methods would perform for\nextracting thepublic’sstancetowardsvaccinationusingsocialmediaposts.Thetheme\nofvaccinationwaschosenforitsrelevancetopublichealth,aswellforitsimportancein\nthepublicdiscourseandthebroadrangeofviewstherein.Further,ourfocusonstance,\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nrather than just sentiment, is based on the notion that stance is a better measure of\nvaccination intention. However, stance can bemoredifﬁculttoassess,becausestance\ncan be very different from sentiment (for example, the sentence “I am angry that I\nhaven’t been vaccinated yet” expresses a negative sentiment but a positive stance\ntowards vaccination). Finally, our choice of methodswasbasedonpracticalutilityand\nmethods broadly used (or usable in the future) in various domains: annotation by\nexperts, crowds, lexicon- and rule-based sentiment analysis tools, and large language\nmodels(LLMs).\nMaterialsandmethods\nDatacollection\nCrowdbreaks was a platform that collected andﬁltered Twitter data (which was\nrenamedX).Crowdbreaksusedcrowdsourcingannotationandtrainedmachinelearning\nclassiﬁerstodetermineatweet’sstancetowardsaspeci ﬁctopic(e.g.,vaccination)[15].\nWe sampled 1,000 random English-language tweets on vaccination from tweets that\nCrowdbreaks collected between 2nd of December, 2019 and 11th of March, 2022. We\nused the following keywords: vaccine, vaccination, vaxxer, vaxxed, vaccinated,\nvaccinating,vaccine,overvaccinate,undervaccinate,andunvaccinated.\nWeusedsixmethodstoclassifytheuser’sstancetowardsvaccinationfromthose1,000\ntweets: experts, crowdsourcing, and the four large language models GPT (Generative\nPre-training Transformer, versions 3.5 and 4) [21,22], Mistral 7B Dense Transformer\n(Mistral) [23] and Mixtral 8x7B (Mixtral) [24]. In addition, we used a lexicon and\nrule-basedfeelinganalysisinstrumentnamedValenceAwareDictionaryandsEntiment\nReasoner(VADER)[25]toclassifythegeneralsentimentfromthesame1,000tweetsas\nanautomated,non-LLMbaseline.\nClassifyingthevaccinestanceusingexperts\nParticipants\nWeaskedaconveniencesampleoffourexpertsfromtheEPFLDigitalEpidemiologyLab\ntoidentifythegeneralsentimentandstancetowardsvaccinationofTweets:twopublic\nhealthexperts(co-authorsofthispublication),oneexpertannotator,andoneexpertin\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nadministration. These experts were selected based on their knowledge in theﬁeld\n(publichealth),ontheirknowledgeinthetechnique(annotation)andasacomparisonto\nthe crowdsourcing (expert in administration) with whom we could discuss in case of\ndisagreements.\nDatacollection\nEach of the four experts individually classiﬁed the tweets according to the general\nsentimentandthestanceoftheusertowardsvaccinationbasedsolelyonthetextofthe\nsamelistoftweets.\nEach expert classiﬁed the tweets on a tab of a Googlesheet(onetabforeachexpert)\ncontaining three variables: tweet text, general sentiment of the tweet, and stance\ntowardsvaccinationofthetweet.The ﬁlewassavedinarestrictedfolderinwhichonly\ntheselectedexpertscouldaccessit.\nEach expert classiﬁed independentlythetweetsaseithernegative,neutral,orpositive\nregardingthegeneralsentimentandstancetowardsvaccination.\nDataanalysis\nWeusedR4.1.3andRStudioversion2023.06.1foranalysingtheindividualclassi ﬁcations\nof all tweets [26]. A new dataset was created with the following variables: tweet text,\nclassiﬁcation of the general sentiment and stance toward vaccination by each expert,\nagreement(binaryvariablewith1,ifmorethanthreeexpertslabelledequallythetweet\nfor eachcategory,or0ifitwasotherwise),percentageofeachclass(negative,neutral,\nand positive) per class(generalsentimentandstancetowardsvaccination),andagreed\nclass(incaseofdisagreement,weselectedtheclass“neutral”).\nClassifyingthevaccinestanceusingcrowdsourcing\nParticipants\nWe used crowd annotatorsfromtheplatformAmazonMechanicalTurk(Mturk)[27]to\nclassifythesamelistoftweetsaccordingtothestancetowardsvaccination.Onlycrowd\nannotatorswithatleast5,000previousapprovedtasksandatleast98%ofapprovalrate\nintheplatformhadaccesstothetask.Inaddition,black-listedcrowdannotatorsonthe\nplatformwereexcluded.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nDatacollection\nWe published the job inAmazonMechanicalTurk(Mturk)withseveralinformationfor\nthe crowd annotators. We included the description of the task, expected time to\ncompletethetask,aquestiontoclassifythetweetbasedonthe“attitudeoftheauthor\nregarding vaccines” (i.e.stance),twoexamplesforeachoftheclasses(positive,neutral\nand negative) and the indication to classifyatweetas“neutral”ifthetweetwasnews,\nfactual,objectiveorgenerallyambiguous.\nEachcrowdannotatorcoulddecidehowmanyandwhichtweetstoclassify.Eachcrowd\nannotator could classify a maximum of 1,000 tweets, had to wait at least 2 seconds\nbeforeprovidingtheclasstoavoidautomaticselectionbyamachine,andwasrewarded\nwith 0.04USDpertweet.Attheend,eachtweetwasclassi ﬁedbyatleastthreecrowd\nannotators.\nDataanalysis\nForeachtweet,wehadthefollowingvariables:identi ﬁerofthespeci ﬁctaskinAmazon\nMturk, tweet identiﬁer, date of the tweet, crowd annotator identiﬁer, tweet text, and\ntotaldurationanddateoftheclassi ﬁcationpertweet.\nClassifyingthevaccinestanceusingLLMs\nSoftware\nWeusedJupyterNotebook6.5.2andPython3.9.12toclassifythesametweetsaccording\nto the stance towardsvaccinationusingGPTversions3.5(GPT-3.5)and4(GPT-4),and\nOllama 0.1.17 and Python 3.11.6 to classify the same tweets according to the stance\ntowards vaccination using Mistral and Mixtral. The speciﬁc scriptsareavailableinthe\nrepository [26]. These LLMs were selected based on its popularity of use (GPT-3.5),\nexpected good performance (GPT-4), and usability with low computingpowerneeded\nandnocost(Mistral)andexpectedgoodperformance(Mixtral).\nPromptengineering\nWe used different prompts to classify the tweets based on instructions andexamples\nprovided[S1Promptsusedforlargelanguagemodels].Prompts1and2werezero-shot\nprompts, i.e. withoutlabelledexamples,andthusthesimplestinstructionsprovidedto\ntheseLLMaskingforthesentimentregardingvaccination.Theremainingprompts3to\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \n8 used few-shot classiﬁcation, i.e., few labelledexamplesforeachclasswereprovided\nfor the model to extract the features for each class and usetheselearnedfeaturesto\npredict the class of new tweets [19,28,29]. Prompt 3 contained the same examples\nprovided to crowd annotators (i.e., two examples per class). Prompts4to7werebuilt\nbased on the previous prompts using a random sample of 100 tweets. Prompts were\ntested in this sample and the wronglyclassiﬁedtweetswereusedasexamplesforthe\nfollowingprompt.Forexample,ifatweetconsideredbyexpertstohaveanegativeclass\nwasclassiﬁedotherwise,thistweetwasincludedinthenextpromptasanexamplefor\nthenegativeclass.Furthermore,examplesoftweetsmisclassi ﬁedasneutralwereadded\nasexceptionsforthisclass.Allpromptsexceptfromprompt1requestedtheLLMmodel\ntoprovideanexplanationontheclassi ﬁcation.\nThesameparameterswereusedforallpromptswithineachLLM.ForGPT-3.5,weused\nthe following parameters: text-davinci-003 as model, 0.8 as temperature, and 0 as\nfrequencyandpresencepenalty.ForGPT-4,weusedthefollowingparameters:gpt-4as\nmodel,“user”asrole,0.8astemperature,and0asfrequencyandpresencepenalty.For\nMistralandMixtral,weused0.8astemperature.\nClassifying the general sentiment using natural language\nprocessingalgorithms\nSoftware\nWeusedJupyterNotebook6.5.2andPython3.9.12toclassifythesametweetsaccording\ntothegeneralsentimentexpressedinthetweetusingVADER.\nDatacollection\nVADER assigned to each tweetaprobabilityforhavinganegative,neutralandpositive\nsentiment in the text and an overall score between -1 (most negative) and +1 (most\npositive).\nDataanalysis\nWe used R 4.1.3 and RStudio version 2023.06.1 forclassifyingthetweetsintonegative,\npositive or neutral according to the overall score provided by VADER. Tweets with\nscores between -0.5 and 0.5 were classiﬁed as neutral, more than 0.5 as positiveand\nlessthan-0.5asnegative.analysingtheindividualclassi ﬁcationsofalltweets\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nComparisonoftheclassi ﬁcationmethods\nStatisticalanalysis\nWeusedR4.1.3andRStudioversion2023.06.1tocomparetheclassesforthesamelistof\ntweetsprovidedbyexperts,crowdsourcing,LLMsandVADER.Expertswereconsidered\nthegoldstandardforthecomparison.\nWedidtwocomparisonsbasedonthelevelofagreementbytheexperts:partialandfull\nagreementinstancetowardsvaccination.Tweetswithatleastthreeoutoffourexperts\nassigningthesameclasswereconsideredtohaveapartialagreement.\nWecalculatedaconfusionmatrixwiththreeclasses(negative,neutral,andpositive)for\neach pair of comparisons: experts and crowdsourcing, experts and GPT, experts and\nMistral,expertsandMixtral,andexpertsandVADER.Moreover,aconfusionmatrixwith\nthree classes was also calculated for the comparison of experts and selecting the\nmajority class (i.e., neutral) for all tweets which was considered the baseline. We\ncalculated the overall accuracy with a 95% conﬁdence interval and a one-sidetestto\nsee if the accuracy was better than the fourth confusion matrix considering that the\nsystemwouldalwaysselectthatmajorityclassintheclassi ﬁcationprocess.\nFurthermore, for each class we calculated the sensitivity,speciﬁcity,andF1score.For\nthis,weusedtheexpertsclassesasthegoldstandardandthecrowdsourcingandLLM\nas the comparison method separately. Moreover, the baseline classifying all tweets as\nneutralwasalsoappliedhere.\nEthics\nThis work has been approved by the EPFL Human Research Ethics Committee (HREC\nNo.012.2018/05.03.2018).\nCrowdbreaks anonymises all tweets’ text changing the mention or reference to\nusernames to “@username” and the URLs to “url”, and complies with the X terms of\nservice.\nWe have implemented mitigation measures to ensure an ethical use of the Amazon\nMturk platform and avoid any abuse of the system. We providedanoverestimationof\nthetimerequiredforthetask,provideacommunicationchannelviaemailincasecrowd\nannotators wanted to contact us, had a prompt communication in case of being\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \ncontacted, accepting all jobs in maximumoneday,providingcomprehensiveandclear\ndescription of the research context and instructions and job description, providing a\nmonetarycompensationsubstantiallyhigherthantheUSfederalgovernmentminimum\nwageinwhichtheplatformisbased.Inaddition,qualitycriteriaforselectingthecrowd\nannotators was establishedtominimisetheriskofhavinginappropriateusersorusers\nmisusingtheplatform.\nResults\nExtractingthevaccinestanceusingexperts\nSeventy-six percent of the 1,000 tweetshadapartialagreementbetweenexperts(i.e.,\nthree outoffourexpertsagreedontheclasspertweet).Fromthese756tweets,74.1%,\n19.4%, and 6.5% were classiﬁed as neutral, positive, and negative stance towards\nvaccination,respectively.\nThirty-sevenpercentofthose756tweetshadafullagreementbetweenexperts(i.e.,all\nexperts agreed on the class per tweet). From these, 77.3%, 17.6%, and 5.0% were\nclassiﬁedasneutral,positive,andnegativestancetowardsvaccination,respectively.\nOnly the vaccine stance from the tweets with at least partial agreement among EPFL\nexpertswasalsoextractedusingcrowdsourcingandLLMs.\nExtractingthevaccinestanceusingcrowdsourcing\nAtotalof467crowdannotatorsclassi ﬁedthetweetswiththemajorityoftweets(64.5%)\nbeing classiﬁed by threedifferentcrowdannotators,followedby14.7%and7.4%ofthe\ntweetsbeingclassi ﬁedby10andfourcrowdannotators,respectively.\nThe majority of the 756 tweets had an agreement:85.6%hadapartialagreement(i.e.,\n75%ofthecrowdannotatorsagreedonthestancepertweet)and,fromthese,91.2%had\nfullagreement.\nThe stance towards vaccination of tweets with partial agreement was distributed as\nfollows:89.5%asneutraland10.5%aspositive,withnotweetclassi ﬁedasnegative.The\nstance towards vaccination of tweets with full agreement were 98.1% classiﬁed as\nneutraland1.9%classi ﬁedaspositive,withnotweetclassi ﬁedasnegative(Fig1).\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nFigure1. Distributionofthetweetsperstance,promptandlargelanguagemodel(percentageof\ntotalnumberoftweets).Thedashedlinesrepresentthestancedistributionofpositive,neutral\nandnegativetweetsaccordingtotheexperts’fullandpartialagreement.\nExtractingthevaccinestanceusingLLMs\nThe majority class was neutral for all prompts, LLMs and tweets with partial or full\nagreement by experts, except for prompts 1 and 2 of GPT-3.5 for which the majority\nclasswaspositiveandnegative,respectively(Fig1).\nExtractingthegeneralsentimentusingVADER\nThe majority class was neutral (Fig 1)withanoverallscorerangingfrom-0.97to0.99,\nmedianof0andinterquartilerangeof-0.30and0.01.\nComparisonoftheextractionmethods\nOverallperformancefortweetswithpartialandfullagreement\nTheoverallaccuracyforallmethodswasworsethanjustselectingthemajorityclassfor\nall tweets, except for prompts 2-8 of GPT-4 in tweets with partial agreement among\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nexpertsandforallpromptsofGPT-4,prompt3,4and6ofGPT-3.5andprompts5,7and\n8ofMixtralintweetswithfullagreementamongexperts(Fig2).\nIn the tweets with partial agreement among experts, the accuracy was 0.73 for\ncrowdsourcing and 0.66 for VADER, and ranged between 0.80 and 0.72 for GPT-4,\nbetween0.74and0.43forGPT-3.5,between0.68and0.54forMistralandbetween0.76\nand0.52forMixtral.Inthetweetswithfullagreementamongexperts,theaccuracyhad\nsimilar ranges with a higher upper value for GPT version 4 (0.91), GPT-3.5 (0.83) and\nMixtral (0.84), and higher values for Mistral with a range between 0.79 and 0.62 and\ncrowdsourcingwithavalueof0.81.\nAllpromptsofGPT-4exceptforprompt1,halfoftheMixtralprompts(4,5,7and8)and\nprompt 3 of GPT-3.5 performed better than crowd annotators when comparing the\nresultswiththeexperts’classi ﬁcationintweetswithpartialagreementamongexperts.\nIn addition to those, prompt 1 ofGPT-4andprompts4,5and6ofGPT-3.5performed\nbetterthancrowdannotatorsintweetswithfullagreementamongexperts.\nSome of the promptsofGPT-3.5,MistralandMixtralperformedworsethanextracting\nthegeneralsentimentofthetweetusingVADER.\nOverall, Mistral’s performance was lower than that of crowdannotators.Interestingly,\nprompts 1 and 2 had the best accuracy with Mistral in tweets with partial and full\nagreementamongexperts,differentlytotheothermodels.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nFigure2. Accuracyand95%con ﬁdenceintervalperprompt,modelandexperts’agreement.\nPerformanceperclassforpartialandfullagreement\nTheperformancemetricsperclassandmethodshowedhighvariabilityaccordingtothe\nexperts’agreementonstancetowardsvaccinationwithanoverallhighestperformance\nforneutraltweetsbasedontheF1scoreandsensitivity(Fig3).\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nFigure3. F1score,sensitivityandspeci ﬁcitypermodel,prompt(ifapplicable),stanceand\nexperts’agreement.\nFor the tweets with partial agreement, the performance of crowd annotators wasthe\nlowestfortheminorityclass(negativestancetowardsvaccination)havingthelowestF1\nscore and sensitivity (0.08); however, crowd annotators had the highest speciﬁcity\n(0.99). Furthermore, for the minority class, all prompts except for prompt 4 of GPT-4\nand prompts 4, 5 and 8 of Mixtral were thetop10bestperformingmethodsbasedon\ntheF1score;andmostofthepromptsofMistral(3,4,5,6,and8),prompts1,2and3of\nMixtral, and prompts 2 and 7 of GPT-3.5 had the highest sensitivity. For the majority\nclass (neutral stance towards vaccination), the performance was variable accordingto\nthe method used with all prompts except for prompt 1 of GPT-4, prompts 5 and 8 of\nMixtral, and prompt 3 of GPT-3.5 withthehighestF1score;andprompts1,2,and7of\nGPT-3.5, prompts 1 and 2 of Mixtral, prompts 4, 5 and 7 of Mistral with the highest\nspeciﬁcity.Forthepositiveclass,allpromptsexceptprompt3ofGPT-4hadthehighest\nF1 scores followed by prompts 4, 7 and 8 of Mixtral; and prompts 3, 4, 5, 6 and 7 of\nMixtralandprompt3ofGPT-4andGPT-3.5hadthehighestspeci ﬁcity.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nWhen comparing the results of the tweets with full agreementwiththeresultsofthe\ntweets with the partial agreement, there were some commonalities and some\ndifferences. The differences for the negative class were that prompt 1 of GPT-4 and\nprompts 4 and 8 of Mixtral were not among the top 10 methods with the highest F1\nscorewhichweresubstitutedinthatrankingbyprompt4ofGPT-4,andprompts3and\n6ofGPT-3.5.ThedifferencesfortheneutralclasswerethatallpromptsofGPT-4were\ninthetop10ofmethodswiththehighestF1score;andprompts4and5ofMistralwere\nnotinthetopmethodswiththehighestspeci ﬁcity,whichweresubstitutedbyprompt2\nof GPT-4 and prompt 1 of Mistral. The differences for the positive class were that\nprompt4ofMixtralwasnotamongthetop10methodswiththehighestF1score,which\nwas substituted by prompt 3 of GPT-4; and prompt 3 of GPT-3.5 was not among the\nmodelswiththehighestspeci ﬁcitywhichwassubstitutedbyprompt2ofGPT-4.\nDiscussion\nUnderstanding the population's perception of public health interventions, such as\nvaccination, and current public health events is critical for public health experts and\nepidemiologists. This understanding helps them adjust risk communication and\ncommunity engagement activities. Achieving this is challengingandrequiresaholistic\napproach. Our study concentrates on using social media to assess users' stances\ntowards vaccination. However, our method could be easily extended to cover other\npublic health interventions and objectives, as well as other platforms or sources of\ninformation. With the continuous advancement in LLM technology and better\nunderstanding of their performance in different studies and contexts,theirusageand\nintegration into public healthsurveillancesystemscouldgreatlyimproveourabilityto\nmonitorandrespondtochangingpublichealthattitudes.\nOur study provides an analysis of attitudes towards vaccination, derived from 1,000\nrandomly selected English-language tweets collected between 2019 and 2022. This\nanalysis is based on a consensus amongfourexpertsregardingthevaccinationstance\nexpressed in each tweet. Moreover, we extracted vaccination stances using six\nalternative methods: crowd annotators, a rule-based sentiment analysis tool,andfour\nlargelanguagemodels(LLMs),speci ﬁcallyfocusingontweetswhereatleastthreeoutof\nfour experts agreed on the tweet's classiﬁcation(i.e.,partialexpertagreement).These\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nsixmethodswerechosenbecauseeachoffersuniqueadvantages.Crowdannotatorsare\nconsidered the gold standardforscalablehumanannotation,butarenonethelessslow\nand costly. Rule-based sentiment analysis, while not incorporating the latest machine\nlearningtechnologies,isvaluedforitssimplicity,speed,andtheabilitytoautomateand\nrunonlocalcomputers.\nThe four LLMs, which all offer automation and speed, were selected because they\nrepresenttheleadingedgeintheir ﬁeld.ThetwoGPTmodelsfromOpenAI,knownfor\nunderpinning ChatGPT, are among the most recognized and utilised. GPT-3.5, the\nfoundation of the free version of ChatGPT, and GPT-4, the most advanced model\npublicly available via the OpenAI platform and the basis for ChatGPT Plus, are\nconsideredstate-of-the-artinLLMtechnologyasofthiswriting.Accesstobothmodels\nis restricted to OpenAI's platform. Mistral andMixtral,fromMistralAI,areconsidered\nstate-of-thep-art for open models. The Mistral 7B model, smaller with 7 billion\nparameters, can operate on a personal computer, such as a laptop. Mixtral, a more\ncomplex“mixtureofexperts”model,maynotrunonmostpersonalcomputersbutcan\nbeinstalledonlocalserverenvironments.ThisversatilityimpliesthatMistralAI'sopen\nmodelsprovideprivacybene ﬁts,astheyallowdatatostayon-premises.\nWe included a lexicon and rule-based sentiment analysis tool,speciﬁcallydesignedto\nextractonlythegeneralsentimentofatext,asabaselineforcomparisonwiththemore\nadvancedLLMs.Our ﬁndingsindicatedthatthismethodachievedhigheraccuracythan\nsome LLMs, particularly those utilising zero-shot prompting. This suggests that\ninadequatepromptengineeringcanresultinpooreroutcomesthansimplydetermining\nthegeneralsentimentwithastraightforwardtoollikeVADER.Similartocrowdsourcing,\nVADERdemonstratedhighspeci ﬁcitybutlowsensitivityforminorityclasses,indicating\nthat whilethesemethodsarelesseffectiveinidentifyingminorityclasses,theybene ﬁt\nfromareducedrateoffalsepositivedetections.\nWe observed a notable class imbalance among the three identiﬁed stances or classes\n(negative, neutral and positive), similar to what has been found in previous studies\n[30,31]. The majority were neutral, followed by positive, with negative being the least\ncommon stance. A high proportion of tweets (76% of the 1,000 annotated tweets)\nachieved a partial agreement among experts,butfullagreementwaslimitedto37%of\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nthese. This pattern was consistent acrossalltheothermethods,withtheexceptionof\noneGPT-3.5promptshowingamorebalancedclassdistribution.\nIntheanalysisofthetweetsclassi ﬁedusingcrowdsourcing,86%ofthetweetsshowed\npartial agreement among annotators, similar to the experts pattern. However, a\nsigniﬁcantly higher 91% of these reached full agreement. This might be due to the\nhomogeneity of Amazon Mturk annotators. There was no partial or full agreement in\nnegativetweets,withallneutrally-classi ﬁedtweetsreachingfullagreement.Thiscould\nbe attributed to our methodological choice of not including a category forunclearor\nirrelevant tweets as in other studies [12,13], and instructing annotators to opt for\nneutralityincasesofuncertainty.\nIn the analysis of the tweets classiﬁed using LLMs, the predominant prediction was\nneutral,exceptinGPT-3.5zero-shotpromptswhereamoreevenclassdistributionwas\nfound, which could be a consequence of the simplicity of these prompts. GPT-4\ndemonstratedaconsistentclassdistributionacrossprompts,asdidMistralandMixtral\nfor few-shot prompts, suggesting less prompt-dependency. This is a notable\nimprovement over GPT-3.5, where simpler prompts showed a signiﬁcantly different\nclassdistributioncomparedtomorecomplexones.\nAll methods showedlowerperformanceonthetweetswithpartialexperts’agreement,\nindicating a common challenge in resolving ambiguity or unclarity. In general, this\nambiguity is caused by having single words with multiple meanings, i.e., polysemy,\nwhichismorenoticeableinshorttextsassocialmediapostsinwhichthereisnotmuch\ninformation on the context [32]. Other factors, such as humour, sarcasm, or general\nmisunderstandings, indicate that a single objective assessment can be challenging,\ndepending as much on the reader's interpretation as on the content itself. These\nconsiderations are important when employing scalable methods and evaluating the\npotentialrisksofmisclassi ﬁcation.\nGenerally, the highest performing prompts of GPT-3.5 and GPT-4 surpassed\ncrowdsourcing in the overall accuracy, as supported by previous studies [33–36].\nFurthermore, the overall accuracy of the best performing prompts of Mixtral and\nspeciﬁc accuracy for the minority class of the best performing promptofMistralalso\noutperformed crowdsourcing. The cost beneﬁt and time effectiveness of LLMs over\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \ncrowd annotators, despite usage fees, has been previously highlighted [33].\nFurthermore,astudyhasshownthatupto46%ofcrowdworkersresortedtoLLMsfor\ntaskcompletion[37].\nEach method showed varying performance per class. Some prompts of GPT-4 and\nMixtral stood out with the highest overall accuracy and statistically signiﬁcant better\nperformance than just selecting the majority class for all tweets. They also had the\nhighest F1 scores across all classes, indicating a reduced impact from the class\nimbalance. GPT-3.5 had the greatestvariabilityinperformancebyclass,particularlyin\nthe tweets withfullagreement,showinghigherdependencyonthepromptthanother\nLLMs. Crowdsourcingshoweditsweakestperformanceinthenegativeclass,beingthe\nmethod with the lowest performance, suggestinganeedformorediverseexamplesin\nthe dataset. These differences in performance per class must be considered to use\ncomplementary models depending on their performance and the objective of the\nannotationassuggestedinapreviousstudy[35].\nMixtral, an open-source “mixture of experts” model, demonstrated a performance\ncomparable to GPT-4, a strong result given its accessibility and lower computational\nrequirements.Itshowedconsistentperformanceacrossdifferentprompts,withsimpler\nprompts often yielding better results. The accuracy of Mixtral in tweets with full\nexperts’ agreement was statistically better than merely selecting the majority class, a\ntrend also observed for many GPT-4 prompts, and some GPT 3.5 prompts. Given the\nopen nature ofMixtralanditsabilitytobeinstalledinalocalcomputingenvironment,\nthismakesMixtralaveryinterestingmodelforuseindata-sensitivesituations.\nWedidnotanalysetheexplanationsprovidedbyLLMs,norcollectedthesefromcrowd\nannotators,whichcouldofferinsightsintothediscrepancieswithexpertclassi ﬁcation.\nA study by Huang et al., 2023, suggests that GPT-3.5 provides statistically clearer\nexplanations than human-written ones [34], indicating a potential area for future\nresearch. Additionally, while our study was limited to English tweets, a previous\nresearchindicatesthatGPT’sperformanceisconsistentinnon-Englishlanguageswhen\npromptsremaininEnglish[35],allowingformultilingualstudies.\nInsummary,GPT-4andMixtralshowedaconsistenthighperformanceacrossprompts,\nwhile GPT3.5 and Mistral showed more variability andpromptdependency,showinga\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nhigher relevance of the prompt engineering for these models. In general, LLMs,\nparticularly in the minority negative class, outperformed crowdsourcing, highlighting\ntheir efﬁcacy even with limited class-speciﬁc data. Overall, the use of best-in-class\nmodels like the open Mixtral model or the closed GPT-4 model with few-shot\nprompting are the best methods. All alternatives have signiﬁcant risks of substantial\nmisclassiﬁcation.\nDatasharing\nThe data (list of tweets identiﬁers and summarised anonymised datasets) and R and\nPython code used can be found in the online repository at\nhttps://github.com/digitalepidemiologylab/llm_crowd_experts_annotation.\nAcknowledgements\nWewouldliketothankthecrowdandexpertsfortheirvaluableinputsintheannotation\nofthetweets.ThisworkwassupportedbyFondationBotnarandtheEuropeanUnion's\nHorizonH2020grantVEO(874735).\nReferences\n1. AchesonED.Onthestateofthepublichealth[ThefourthDuncanlecture].Public\nHealth.1988;102\u0000431–437.doi\u000010.1016/S0033-3506(88)80080-5\n2. WorldHealthOrganization.Publichealthservices.Available:\nhttps://www.euro.who.int/en/health-topics/Health-systems/public-health-servi\nces/public-health-services\n3. CDCFoundation.WhatisPublicHealth?2021[cited20Sep2021].Available:\nhttps://www.cdcfoundation.org/what-public-health\n4. ArmstrongD.OriginsoftheProblemofHealth-relatedBehaviours:AGenealogical\nStudy.SocStudSci.2009;39 \u0000909–926.doi\u000010.1177/0306312709104258\n5. WorldHealthOrganisation.ReportoftheSAGEworkinggrouponvaccine\nhesitancy.[cited3Nov2023].Available:\nhttps://cdn.who.int/media/docs/default-source/immunization/sage/2014/octob\ner/sage-working-group-revised-report-vaccine-hesitancy.pdf?sfvrsn=240a7c1c_4\n6. StewartMC,ArnoldCL.De ﬁningSocialListening:RecognizinganEmerging\nDimensionofListening.InternationalJournalofListening.2018;32 \u000085–100.\ndoi\u000010.1080/10904018.2017.1330656\n7. PomputiusA.CanYouHearMeNow?SocialListeningasaStrategyfor\nUnderstandingUserNeeds.MedRefServQ.2019;38 \u0000181–186.\ndoi\u000010.1080/02763869.2019.1588042\n8. DangT,NguyenNVT,PhamV.HealthTvizer:ExploringHealthAwarenessinTwitter\nDatathroughCoordinatedMultipleViews.2018IEEEInternationalConferenceon\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \nBigData(BigData).2018.pp.3647–3655.doi \u000010.1109/BigData.2018.8622445\n9. KannanR,GovindasamyMA,SoonL-K,RamakrishnanK.SocialMediaAnalyticsfor\nDengueMonitoringinMalaysia.20188thIEEEInternationalConferenceonControl\nSystem,ComputingandEngineering(ICCSCE).2018.pp.105–108.\ndoi\u000010.1109/ICCSCE.2018.8685028\n10. SchillingerD,ChittamuruD,RamírezAS.From“Infodemics”toHealthPromotion:A\nNovelFrameworkfortheRoleofSocialMediainPublicHealth.AmJPublicHealth.\n2020;110\u00001393–1396.doi\u000010.2105/AJPH.2020.305746\n11. TsaoS-F,ChenH,TisseverasingheT,YangY,LiL,ButtZA.Whatsocialmediatold\nusinthetimeofCOVID-19 \u0000ascopingreview.LancetDigitHealth.2021;3 \u0000\ne175–e194.doi\u000010.1016/S2589-7500(20)30315-0\n12. CortisK,DavisB.Overadecadeofsocialopinionmining:asystematicreview.Artif\nIntellRev.2021;54 \u00004873–4965.doi\u000010.1007/s10462-021-10030-2\n13. AlturayeifN,LuqmanH,AhmedM.Asystematicreviewofmachinelearning\ntechniquesforstancedetectionanditsapplications.NeuralComputAppl.2023;35 \u0000\n5113–5144.doi\u000010.1007/s00521-023-08285-7\n14. RibeiroFN,AraújoM,GonçalvesP,AndréGonçalvesM,BenevenutoF.SentiBench-\nabenchmarkcomparisonofstate-of-the-practicesentimentanalysismethods.EPJ\nDataSci.2016;5 \u000023.doi\u000010.1140/epjds/s13688-016-0085-1\n15. MüllerMM,SalathéM.Crowdbreaks:TrackingHealthTrendsUsingPublicSocial\nMediaDataandCrowdsourcing.FrontPublicHealth.2019;7 \u000081.\ndoi\u000010.3389/fpubh.2019.00081\n16. EspinosaL,WijermansA,OrchardF,HöhleM,CzernichowT,ColettiP,etal.\nEpitweetr:EarlywarningofpublichealththreatsusingTwitterdata.\nEurosurveillance.2022;27\u00002200177.doi\u000010.2807/1560-7917.ES.2022.27.39.2200177\n17. WangR,ZhouD,JiangM,SiJ,YangY.ASurveyonOpinionMining:FromStanceto\nProductAspect.IEEEAccess.2019;7 \u000041101–41124.doi\u000010.1109/ACCESS.2019.2906754\n18. SnowR,O’ConnorB,JurafskyD,NgA.CheapandFast–ButisitGood?Evaluating\nNon-ExpertAnnotationsforNaturalLanguageTasks.In:LapataM,NgHT,editors.\nProceedingsofthe2008ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing.Honolulu,Hawaii:AssociationforComputationalLinguistics;2008.pp.\n254–263.Available:https://aclanthology.org/D08-1027\n19. BrownT,MannB,RyderN,SubbiahM,KaplanJD,DhariwalP,etal.Language\nModelsareFew-ShotLearners.In:LarochelleH,RanzatoM,HadsellR,BalcanMF,\nLinH,editors.AdvancesinNeuralInformationProcessingSystems.Curran\nAssociates,Inc.;2020.pp.1877–1901.Available:\nhttps://proceedings.neurips.cc/paper_ﬁles/paper/2020/ﬁle/1457c0d6bfcb496741\n8bfb8ac142f64a-Paper.pdf\n20. DevlinJ,ChangM-W,LeeK,ToutanovaK.BERT:Pre-trainingofDeepBidirectional\nTransformersforLanguageUnderstanding.arXiv;2019.Available:\nhttp://arxiv.org/abs/1810.04805\n21. OpenAI.GPT-3.5.2022.Available:https://openai.com/\n22. OpenAI.GPT-4.2023.Available:https://openai.com/\n23. JiangAQ,SablayrollesA,MenschA,BamfordC,ChaplotDS,CasasDdelas,etal.\nMistral7B.2023[cited5Feb2024].doi \u000010.48550/ARXIV.2310.06825\n24. JiangAQ,SablayrollesA,RouxA,MenschA,SavaryB,BamfordC,etal.Mixtralof\nExperts.2024[cited5Feb2024].doi \u000010.48550/ARXIV.2401.04088\n25. HuttoC,GilbertE.VADER:AParsimoniousRule-BasedModelforSentiment\nAnalysisofSocialMediaText.ICWSM.2014;8 \u0000216–225.\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint \ndoi\u000010.1609/icwsm.v8i1.14550\n26. EspinosaL,SalathéM.Useoflargelanguagemodelsasascalableapproachto\nunderstandingpublichealthdiscourse.In:GitHubrepository[Internet].[cited5\nFeb2024].Available:\nhttps://github.com/digitalepidemiologylab/llm_crowd_experts_annotation\n27. CrowstonK.AmazonMechanicalTurk:AResearchToolforOrganizationsand\nInformationSystemsScholars.In:BhattacherjeeA,FitzgeraldB,editors.Shaping\ntheFutureofICTResearchMethodsandApproaches.Berlin,Heidelberg:Springer;\n2012.pp.210–221.doi \u000010.1007/978-3-642-35142-6_14\n28. ZhangR,LiuQ.Learningwithfewsamplesindeeplearningforimageclassi ﬁcation,\namini-review.FrontComputNeurosci.2023;16 \u00001075294.\ndoi\u000010.3389/fncom.2022.1075294\n29. SuH,KasaiJ,WuCH,ShiW,WangT,XinJ,etal.SelectiveAnnotationMakes\nLanguageModelsBetterFew-ShotLearners.arXiv;2022.Available:\nhttp://arxiv.org/abs/2209.01975\n30. KunnemanF,LambooijM,WongA,BoschAvanden,MollemaL.Monitoringstance\ntowardsvaccinationinTwittermessages.arXiv;2019.Available:\nhttp://arxiv.org/abs/1909.00338\n31. MartínezRY,BlancoG,LourençoA.SpanishCorporaoftweetsaboutCOVID-19\nvaccinationforautomaticstancedetection.InformationProcessing&\nManagement.2023;60\u0000103294.doi\u000010.1016/j.ipm.2023.103294\n32. DengS,SinhaAP,ZhaoH.ResolvingAmbiguityinSentimentClassi ﬁcation:The\nRoleofDependencyFeatures.ACMTransManageInfSyst.2017;8 \u00001–13.\ndoi\u000010.1145/3046684\n33. GilardiF,AlizadehM,KubliM.ChatGPTOutperformsCrowd-Workersfor\nText-AnnotationTasks.ProcNatlAcadSciUSA.2023;120 \u0000e2305016120.\ndoi\u000010.1073/pnas.2305016120\n34. HuangF,KwakH,AnJ.IsChatGPTbetterthanHumanAnnotators?Potentialand\nLimitationsofChatGPTinExplainingImplicitHateSpeech.Companion\nProceedingsoftheACMWebConference2023.AustinTXUSA:ACM;2023.pp.\n294–297.doi\u000010.1145/3543873.3587368\n35. KuzmanT,MozetičI,LjubešićN.ChatGPT:BeginningofanEndofManual\nLinguisticDataAnnotation?UseCaseofAutomaticGenreIdenti ﬁcation.arXiv;\n2023.Available:http://arxiv.org/abs/2303.03953\n36. TörnbergP.ChatGPT-4OutperformsExpertsandCrowdWorkersinAnnotating\nPoliticalTwitterMessageswithZero-ShotLearning.arXiv;2023.Available:\nhttp://arxiv.org/abs/2304.06588\n37. VeselovskyV,RibeiroMH,WestR.Arti ﬁcialIntelligence:CrowdWorkersWidely\nUseLargeLanguageModelsforTextProductionTasks.arXiv;2023.\ndoi\u000010.48550/arXiv.2306.07899\n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 6, 2024. ; https://doi.org/10.1101/2024.02.06.24302383doi: medRxiv preprint ",
  "topic": "Public health",
  "concepts": [
    {
      "name": "Public health",
      "score": 0.6166344285011292
    },
    {
      "name": "Social media",
      "score": 0.5549703240394592
    },
    {
      "name": "Public opinion",
      "score": 0.5124297738075256
    },
    {
      "name": "Public relations",
      "score": 0.4084984362125397
    },
    {
      "name": "Computer science",
      "score": 0.3706655502319336
    },
    {
      "name": "Data science",
      "score": 0.36256182193756104
    },
    {
      "name": "Political science",
      "score": 0.29857367277145386
    },
    {
      "name": "Medicine",
      "score": 0.27594029903411865
    },
    {
      "name": "World Wide Web",
      "score": 0.13648682832717896
    },
    {
      "name": "Pathology",
      "score": 0.09838014841079712
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    }
  ],
  "cited_by": 7
}