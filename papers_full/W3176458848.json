{
    "title": "Styleformer: Transformer based Generative Adversarial Networks with Style Vector",
    "url": "https://openalex.org/W3176458848",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4366400725",
            "name": "Park, Jeeseung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1571962178",
            "name": "Kim Young-Geun",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3035170495",
        "https://openalex.org/W2888531725",
        "https://openalex.org/W2603777577",
        "https://openalex.org/W2962760235",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W2989221291",
        "https://openalex.org/W3092033429",
        "https://openalex.org/W1710476689",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W3135404760",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2950274562",
        "https://openalex.org/W3146091044",
        "https://openalex.org/W3034918919",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W2963373786",
        "https://openalex.org/W2545656684",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W2963525668",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2771033904",
        "https://openalex.org/W3104876213",
        "https://openalex.org/W2893749619",
        "https://openalex.org/W2879390606",
        "https://openalex.org/W2593414223",
        "https://openalex.org/W2118858186",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2963836885",
        "https://openalex.org/W1834627138",
        "https://openalex.org/W3152912388",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3094502228"
    ],
    "abstract": "We propose Styleformer, which is a style-based generator for GAN architecture, but a convolution-free transformer-based generator. In our paper, we explain how a transformer can generate high-quality images, overcoming the disadvantage that convolution operations are difficult to capture global features in an image. Furthermore, we change the demodulation of StyleGAN2 and modify the existing transformer structure (e.g., residual connection, layer normalization) to create a strong style-based generator with a convolution-free structure. We also make Styleformer lighter by applying Linformer, enabling Styleformer to generate higher resolution images and result in improvements in terms of speed and memory. We experiment with the low-resolution image dataset such as CIFAR-10, as well as the high-resolution image dataset like LSUN-church. Styleformer records FID 2.82 and IS 9.94 on CIFAR-10, a benchmark dataset, which is comparable performance to the current state-of-the-art and outperforms all GAN-based generative models, including StyleGAN2-ADA with fewer parameters on the unconditional setting. We also both achieve new state-of-the-art with FID 15.17, IS 11.01, and FID 3.66, respectively on STL-10 and CelebA. We release our code at https://github.com/Jeeseung-Park/Styleformer.",
    "full_text": "Styleformer: Transformer based Generative Adversarial Networks with Style\nVector\nJeeseung Park∗\nmAy-I Inc.\njspark@may-i.io\nYounggeun Kim*\nMINDsLab Inc.\nyounggeun@mindslab.ai\nAbstract\nWe propose Styleformer, a generator that synthesizes im-\nage using style vectors based on the Transformer structure.\nIn this paper, we effectively apply the modified Transformer\nstructure (e.g., Increased multi-head attention and Pre-\nlayer normalization) and introduce novel Attention Style\nInjection module which is style modulation and demodu-\nlation method for self-attention operation. The new gen-\nerator components have strengths in CNN’s shortcomings,\nhandling long-range dependency and understanding global\nstructure of objects. We present two methods to generate\nhigh-resolution images using Styleformer. First, we apply\nLinformer in the field of visual synthesis (Styleformer-L),\nenabling Styleformer to generate higher resolution images\nand result in improvements in terms of computation cost and\nperformance. This is the first case using Linformer to im-\nage generation. Second, we combine Styleformer and Style-\nGAN2 (Styleformer-C) to generate high-resolution compo-\nsitional scene efficiently, which Styleformer captures long-\nrange dependencies between components. With these adap-\ntations, Styleformer achieves comparable performances to\nstate-of-the-art in both single and multi-object datasets.\nFurthermore, groundbreaking results from style mixing and\nattention map visualization demonstrate the advantages\nand efficiency of our model.\n1. Introduction\nGenerative Adversarial Network (GAN) [21] is one of\nthe widely used generative model. Since the appear of DC-\nGAN [43], convolution operations have been considered\nessential for high-resolution image generation and stable\ntraining. Convolution operations are created under the as-\nsumption of the locality and stationarity of the image (i.e.,\ninductive bias), which is advantageous for image process-\ning [37]. Through convolution neural networks (CNNs)\nwith this strong inductive bias, GAN have efficiently gen-\n*Equal contribution. The order was determined randomly.\nerated realistic, high-fidelity images.\nHowever, drawbacks of CNNs clearly exist. Local re-\nceptive field of CNNs makes model difficult to capture long-\nrange dependency and understanding global structure of ob-\nject. Stacking multiple layers can solve this problem, but\nthis leads to another problem of losing spatial information\nand fine details [55]. Moreover, sharing kernel weights\nacross locations leads to unstable training when the pattern\nor styles differ by location in the image [56]. This is also\nrelated to the poor quality of generated structured images\nor compositional scenes (e.g., outdoor scenes), unlike the\ngeneration of a single object (e.g., faces)\nIn this paper, we propose Styleformer, a generator that\nuses style vectors based on the Transformer structure. Un-\nlike CNNs, Styleformer utilizes self-attention operation\nto capture long-range dependency and understand global\nstructure of objects efficiently. Furthermore, we overcome\ncomputation problem of Transformer and show superior\nperformance not only in low-resolution but also in high res-\nolution images. Specifically, we introduce the following\nthree models:\n1) Styleformer - The basic block of Styleformer is based\non Transformer encoder, so we introduce components that\nneed to be changed for stable learning. Inspired by Mo-\nbileStyleGAN [3], we enhance the multi-head attention in\noriginal Transformer by increasing the number of heads, al-\nlowing model to generate image efficiently. We also modify\nlayer normalization, residual connection, and feed-forward\nnetwork (Section 3.2). Moreover, we introduce novelatten-\ntion style injection module, suitable style modulation, and\ndemodulation method for self-attention operation (Section\n3.3). This design allows Styleformer to generate image sta-\nbly, and enables model to handle long-range dependency\nand understand global structures.\n2) Styleformer-L - We sidestep scalability limitation\narising from the quadratic mode of attention operation\nby applying Linformer [50] (Styleformer-L). As such,\nStyleformer-L can generate high-resolution images with\nlinear computational costs. This paper is the first case to ap-\nply Linformer in the field of visual synthesis (Section 3.4).\n1\n3) Styleformer-C - We further combine Styleformer and\nStyleGAN2, applying Styleformer at low resolution and\nstyle block of StyleGAN2 at high resolution (Styleformer-\nC). As can be seen from our experiments and analysis (e.g.,\nstyle mixing and visualizing attention map), we show that\nStyleformer-C with the structure above can generate com-\npositional scenes efficiently, and showing flexibility of our\nmodel. In detail, we prove that Styleformer in low resolu-\ntion help model to capture long-range dependency between\ncomponents, and style block in high resolution help model\nto refine the details of each components such as color or\ntexture. This novel blending structure enables fast training,\nwhich is the advantage of StyleGAN2, while maintaining\nthe advantages of Styleformer that can generate structured\nimages.(Section 4).\nStyleformer achieves comparable performances to state-\nof-the-art in both single and multi-object datasets. We\nrecord FID 2.82 and IS 10.00 at the unconditional setting\non CIFAR-10. These results outperform all GAN-based\nmodels including StyleGAN2-ADA [32] which recently\nrecorded state-of-the-art. As can be expected, Styleformer\nshow strength especially in multi-object images or com-\npositional scenes generation (e.g., CLEVR, Cityscapes).\nStyleformer-C records FID 11.67, IS 2.27 in CLEVR, and\nFID 5.99, IS 2.56 in Cityscapes, showing better perfor-\nmance than pure StyleGAN2.\n2. Related Work\nAfter origion of GAN [21], various methods [2, 31, 41,\n42] have been proposed to enhance its training stability and\nperformance. As a result, fidelity and diversity of the gen-\nerated images have dramatically improved. In addition to\nimage synthesis task, GAN has been widely adopted in vari-\nous tasks, such as image-to-image translation [27,58], super\nresolution [38], image editing [55], and style transfer [7].\nIn particular, StyleGAN-based architectures have been ap-\nplied for various applications [16,59,60]. However, since all\nof these models are based on convolution backbones, they\nhave met with only limited success on generating complex\nor compositinal scenes [29].\nTransformer [49] was first introduced to the natural lan-\nguage processing(NLP) domain, achieving a significant ad-\nvance in NLP. Recently, there were efforts to utilize Trans-\nformer in the computer vision field [4, 12, 57]. Using huge\namounts of data and a transformer module, ViT [12] ob-\ntains comparable result with state-of-the-art model in the\nexisting CNN based image classification model [35,47]. In-\nspired by [12], various models such as [22, 39, 53] emerges\nbased on this structure. There have also been attempts to\nutilize transformer for tasks such as video understanding [4]\nand segmentation [57] as well as image classification. Even\nin GAN, there have been attempts to utilize Transformer:\nGANformer [26] proposes a bipartite Transformer struc-\nFigure 1. High-resolution compositional scenes generated by\nStyleformer.\nture and applies it to StyleGAN [33, 34]. With this struc-\nture, GANformer successfully advance the generative mod-\neling of structured images and scenes, which have been\nchallenging in existing GAN. However, they use a bipartite\nattention, differ from the self-attention operation. Trans-\nGAN [28] demonstrates a convolution-free generator based\non the structure of vanilla GAN, which doesn’t show good\nperformance compared to state-of-the-art model.\nUnlike these studies, Styleformer generate images with\nself-attention operation using style vector and showing\ncomparable performance state-of-the-art models [33, 34].\nPrevious methods (TransGAN) mainly use pre-defined\nsparse attention patterns for efficient attention mechanism,\nbut we explore the low-rank property in self-attention. Our\nmodel can generate high resolution images ( 512 × 512)\nwith reduced computation complexity, while GANformer\nand TransGAN show a maximum of 256 × 256 image syn-\nthesis.\n3. Styleformer\n3.1. Styleformer Architecture\nFigure 2a shows the overall architecture of Styleformer,\nand in Figure 2b we show Styleformer encoder network,\nthe basic block of Styleformer. Like existing synthesis net-\nwork of StyleGAN, our generator is conditioned on a learn-\nable constant input. The difference is that the constant input\n(8 × 8) is flattened (64) to enter the Transformer-based en-\ncoder. Then the input which is combined with learnable po-\nsitional encoding passes through the Styleformer encoder.\nStyleformer encoder is based on Transformer encoder, but\nthere are several changes to generate an image efficiently,\nwhich will be discussed in Section 3.2.\nAfter passing several encoder blocks in each resolution,\nwe proceed bilinear upsample operation by reshaping en-\ncoder output to the form of a square feature map. After\nupsampling, flatten process is carried out again to match\nthe input form of the Styleformer encoder. This process is\nrepeated until the feature map resolution reaches the target\n2\nPositional \nEncoding\nUnﬂatten\n+\nConst (8✕8)✕C (ﬂattened)\nStyleformer \nEncoder\n8✕8\nStyleformer \nEncoder\n16✕16\n✕\u0001.\nStyleformer \nEncoder\n32✕32\n✕ N \nUpsample\nFlatten\nPositional \nEncoding +\nUnﬂatten\nUpsample\nFlatten\nPositional \nEncoding +\nUnﬂatten\ntRGB+\nUpsample\n+\nUpsample\n+\ntRGB\ntRGB\n…\n…\n✕\u0001-\n                                                                    \n                                                                             \n                                                                                \n                 \n                                                                              \n                                                                                \n                                                                    \n                                                                             \n…\nMod Input\nPre-Layernorm\nStyle Input\nDemod Demod Demod\nMod Value Style Value\nDemod\nInputs   \nQuery Key Value\nIncreased .ulti-head\nSelf-Attention\nMulti-Head\nIntegration\nAdd\nDemod\n+Bias Noise\nModiﬁed Residual\n\tB\n   \tC\n   \nFigure 2. (a) Overall Architecture of Styleformer. (b) Styleformer encoder structure, which is the basic block of Styleformer.\nimage resolution. For each resolution, the number of the\nStyleformer encoder and hidden dimension size can be cho-\nsen as hyperparameters.\n3.2. Styleformer Components from Transformer\nIncreased Multi-Head Attention Modern vision archi-\ntectures allow communications between different channels\nand different spatial locations (i.e., pixels) [48]. Con-\nventional CNNs perform the above two communications\nat once, but these communications can be clearly sepa-\nrated like depthwise separable convolutions [24]. We also\nseparate the pixel-communication (self-attention), channel-\ncommunication operations (multi-head integration) in the\nTransformer encoder. However, in depthwise separable\nconvolutions, distinct convolution kernels are applied to\neach channel, unlike the self-attention operation share only\none huge kernel A (i.e., attention map). With same kernel\napplied to each channel, diversity in generated image can be\ndecreased.\nWe overcome this problem by increasing the number of\nheads of multi-head attention (Increased multi-head atten-\ntion). Then, the created attention map will be different for\neach head, and so the kernel applying operation. Then the\nattention maps will be created for each head, making the\nchannels in each head meet different kernels. However, in-\ncreasing the number of heads too much may cause atten-\ntion map to not be properly created, resulting in poor per-\nformance. We demonstrate experimentally that increasing\nthe number of heads improves performance only when the\ndepth is at least 32, as shown in Figure 3. Therefore, we fix\nthe depth to 32 for all future experiment. More details about\nincreased multi-head attention can be found in AppendixC.\nPre-Layer Normalization We change the position of\nlayer normalization in Transformer encoder. The layer nor-\nmalization of the existing Transformer comes after a lin-\near layer that integrates multi-heads (Post-Layer normaliza-\ntion). We hypothesis that the role of layer normalization\nin a Transformer is the preparation of generating an atten-\ntion map. If we perform layer normalization at the end of\nStyleformer encoder (Layernorm B in Figure 4), style mod-\nulation is applied before making query and key, which can\ndisturb learning attention map. This is supported by ab-\nlation study and attention map analysis in Table 1 and Ap-\npendix B, respectively. Therefore, to solve this problem, we\nproceed layer normalization before operation making query,\nkey and value (Pre-Layernorm in Figure 2b)\n3\nFigure 3. It shows FID on CIFAR-10 with one layer Styleformer,\nwhich hidden dimension size is fixed as 256 and 32, respectively.\nBoth experiments show the best result when the depth is 32.\n                                                                   \n                                                            \n                                                                   \n                                                            \n…\nMod Input\nLayernorm C\nStyle1\nDemod\nMod ValueStyle2\nDemod\nInputs   \nValue\nIncreased Multi-Head\nSelf-Attention\nMulti-Head\nIntegration\nAdd\nDemod\n+Bias Noise\nLayernorm A\nLayernorm B\n…\nResidual \n C\nResidual \nA\nResidual \n B\nFigure 4. Styleformer encoder structure for ablation study, includ-\ning residual connection, layer normalization, attention style injec-\ntion.\nModified Residual Connection Unlike the Transformer\nencoder, input feature map is scaled by style vector (Mod in-\nput in Figure 2b) in Styleformer encoder. We hence find the\nresidual connection suitable for scaled input. After ablation\nstudy, we apply residual connection like Modified Residual\nin Figure 2b. Demodulation operation is additionally per-\nformed in residual connection, which will be described in\nSection 3.3. Table 1 presents ablation details of residual\nconnection.\nEliminating Feed-Forward Network As can be seen Ta-\nble 1, we remove the feed-forward structure because elimi-\nnating feed forward structure makes the model perform bet-\nter and more efficient.\n3.3. Attention Style Injection\nUnlike vanilla GAN, StyleGAN generates an image with\nlayer-wise style vectors as inputs, enabling controllable\ngeneration via style vectors, i.e., scale-specific control.\nSpecifically, style vector scales the input feature map for\neach layer, i.e., style modulation, amplifying certain fea-\nture maps. For scale-specific control, this amplified effect\nmust be removed before entering the next layer. StyleGAN\nallows scale-specific control through a normalization oper-\nation called AdaIN operation [13, 14, 18, 25], which nor-\nmalizes each feature map separately, then scale and bias\neach feature map with style vector. StyleGAN2 is an ad-\nvanced form of StyleGAN and addresses the artifact prob-\nlem caused by the AdaIN operation, solving it by demod-\nulation operation. While the AdaIN operation normalize\nthe output feature map directly, demodulation operation\nis based on statistical assumptions about the input feature\nmap. For details, similar to the goal of normalization oper-\nation, demodulation operation aims to have an output fea-\nture map with a unit standard deviation while assuming that\nthe input feature maps have a unit standard deviation, i.e.,\nstatistical assumption. Our goal is to design a Transformer-\nbased generator that generates images through style vector\nwhile enabling scale specific control. Therefore, we pro-\npose style modulation, demodulation method for the self-\nattention operation, i.e., Attention style injection.\nModulation for Self-Attention Just as the input feature\nmap is scaled by style vector in the style block of Style-\nGAN2, the input feature map in the Styleformer encoder is\nalso scaled by style vector ( Mod Input in Figure 2b). But\nunlike convolution operation in StyleGAN2, there are two\nsteps in self-attention operation: dot product of query and\nkey to create an attention map (i.e. kernel), weighted sum of\nthe value with calculated attention map. We hypothesis that\nthe style vector applied to the operation in each step should\nbe different. Therefore, we perform style modulation twice\nas in Figure 2b ( Mod Input, Mod Value). This hypothesis\nis supported in Table 1. In Figure 2b, Style Input is a style\nvector for input, and Style Value is a style vector only for\nvalue. Two style vectors are created through common map-\nping networks as in StyleGAN but different learned affine\ntransformations.\nDemodulation for Query, Key, Value As shown in Fig-\nure 2b, Styleformer encoder creates query (Q), key (K), and\nvalue (V ) through linear operation to the input feature map\nscaled with Style Input vector. After that, V will be mod-\nulated with Style Value vector additionally, so the demodu-\nlation operation for removing scaled effect of Style Input is\nclearly required. Also, we observe that when an attention\nmap is created with Q, K from input scaled by Style In-\n4\nMethod Style1 Style2 Style1=Style2 Residual A Residual B Residual C Layernorm A Layernorm B Layernorm C Feed-Forward FID\nBaseline O O X X X O X X O X 8.56\nAttention Style Injection\nO X - X X O X X O X 11.01\nX O - X X O X X O X 11.40\nO O O X X O X X O X 10.27\nResidual Connection\nO O X X X X X X O X 19.09\nO O X O X X X X O X 14.70\nO O X X O X X X O X 9.94\nLayer NormalizationO O X X X O O X X X 9.00\nO O X X X O X O X X 10.96\nFeed-Forward O O X X X O X X O O 14.75\nTable 1. Ablation details of Styleformer components. Ablation study was conducted using small version of Styleformer with CIFAR-10\ndataset, trained for 20M images. See Appendix A for further implementation details.\nput, specific value in the attention map becomes very large,\ndemonstrated in Appendix B. This prevents the attention\noperation from working properly. We sidestep this prob-\nlem with demodulation operation to Q, K, before creating\nattention map. Eventually, demodulation operation is all re-\nquired for Q, K, and V .\nLet’s first look at the style modulation to the input, i.e.,\nMod Input . Each flattened input feature map is scaled\nthrough a style vector, which is equivalent to scaling the\nlinear weight:\nw′\nij = si · wij, (1)\nwhere w is original linear weight to make ( Q, K, V ) from\nflattened input feature map, and w′ is modulated linear\nweight. si is ith component of style vector, which scales\nith flattened input feature map, and j means the dimen-\nsion of ( Q, K, V ). Assuming that flattened input feature\nmaps have unit standard deviation (i.e., statistical assump-\ntion of demodulation), after passing style modulation and\nlinear operation, a standard deviation of output is as fol-\nlows:\nσj =\nsX\ni\nw\n′\nij\n2. (2)\nWe scale output activations for each dimension of Q, K,\nand V by 1/σj(i.e., demodulation), making Q, K, and V\nback to unit standard deviation.\nDemodulation for Encoder Output After demodulation\noperation to Q, K, and V , Styleformer encoder performs\nstyle modulation to V (Mod Value), weighted sum of V\nwith attention map ( Increased Multi-head Self-attention ),\nand then performs linear operation ( Multi-Head Integra-\ntion), as shown in Figure 2b. Encoder output will be input\nfor next encoder, so demodulation operation is necessary.\nWe show in Appendix D that, assuming V has a unit stan-\ndard deviation (This can be assumed because of demodula-\ntion for V ), the standard deviation of Styleformer encoder\noutput can be derived as follows:\nσ\n′\nlk =\nsX\n·\nAl·2 ·\nX\nj\nw\n′\njk\n2, (3)\nwhere w′\njk = sj · wjk, i.e., modulated linear weight. sj\nscales jth feature map of V , and k enumerates the flattened\noutput feature map. Attention map A is computed same as\nexisting Transformer: dot products ofQ and K, divide each\nby square root of depth, and softmax function. Al· denotes\nattention score vector for lth pixel.\nHowever, there are two problems with demodulation by\nsimply scaling each flattened output feature map k with\n1/σ\n′\nlk (Equation 3). First, scaling output feature map k with\n1/σ\n′\nlk will normalize each pixel as a unit, different from\nAdaIN operation which normalizes each feature map as a\nunit. Second, the attention map, which is a matrix derived\nfrom Q and K, is dependent on the input. With input depen-\ndent variables, demodulation operations based on statistical\nassumptions can not be applied as in [19]. Therefore we\nscale the flattened output feature map k with 1/σ\n′′\nk where\nσ\n′′\nk =\nqP\nj w\n′\njk\n2, normalizing each feature map as a unit,\nand excluding input dependent variables Al. Then the stan-\ndard deviation of output activations will be\nσlk = σ\n′\nlk\nσ\n′′\nk\n=\nsX\n·\nAl·2. (4)\nHowever in this way, standard deviation of output is not\nunit, rather approaching to zero when the numbers of pixels\nincrease, as detailed in Appendix D. To prevent this effect,\nwe have applied modified residual connection likeModified\nResidual in Figure 2b. More specifically, we perform linear\noperation to Mod Value, then perform demodulation oper-\nation (same as demodulation for query, key, value). With\nthese modulation and demodulation operations in residual\nconnection, variables with unit standard deviation are added\nto the output. Therefore it helps to keep the final output ac-\n5\ntivation having unit standard deviation, whenσjk is close to\nzero.\n3.4. High Resolution Synthesis with Styleformer\nThe main problem in applying Transformer to image\ngeneration is the efficiency problem with image resolution.\nIn this section, we introduce two different techniques in\nStyleformer that can generate high resolution images. We\nshow a method of applying Linformer, making computa-\ntion complexity to linear. Then, we introduce a method of\ncombining Styleformer and StyleGAN2, which can obtain\nthe advantages of both models.\nApplying Linformer to Styleformer For high-resolution\nimages, input sequence length of the Styleformer encoder\nincreases quadratically, and the standard self-attention\nmechanism requires a complexity of O(n2) with respect\nto the sequence length. It means attending to all pixels\nfor each layer is almost impossible for high-resolution im-\nage generation. Therefore, we apply Linformer [50] to our\nmodel, which projects key and value to the k dimension\nwhen applying self-attention, reducing the time and space\ncomplexity from O(n2) to O(nk). We fix k to 256 and\napply Linformer to the encoder block above32×32 resolu-\ntion, only when n is 1024 or higher. We call this model as\nStyleformer-L.\n[50] explains that this new self-attention mechanism\nsucceeds because the attention map matrix is low-rank. We\nobserve this can be applied equally to the attention map\nmatrix in the image: in the case of images, the pixel that\nneeds to attend is often in a particular location, not all pix-\nels(e.g. where objects are located in the image), which re-\nsults in low-rank attention map matrix. Applying Linformer\ncreates a more dense attention map, and also reduces com-\nputation. This is proved by spectrum analysis of attention\nmap in Section 4.2. See Appendix E for more details about\nStyleformer-L.\nCombining Styleformer and StyleGAN2 Even with ap-\nplying Linformer, it is difficult to generate an image for ex-\ntremely high resolution like 512 × 512 using only Trans-\nformer. We solve this problem by combining Styleformer\nand StyleGAN2 to generate a high-resolution image, and\nwe call this model Styleformer-C. Styleformer-C is com-\nposed of Styleformer at low resolution, and style block of\nStyleGAN2 at high resolution. As demonstrated in 4.1,\nStyleformer encoder in low resolution help model to cap-\nture long-range dependency between components or global\nshape of object, and style block in high resolution help\nmodel to refine the details of each components or objects.\nIn other words, model can capture global interactions ef-\nficiently using Styleformer only at low resolution, which\nleads to fast training speed. The overall architecture and\ndetails of Styleformer-C are described in Appendix F.\n4. Experiments\nWe only change the architecture of the generator in\nStyleGAN2-ADA, i.e., synthesis network, while maintain-\ning the discriminator architecture and loss function. We use\nFr´echet Inception Distance (FID) [23] and Inception Score\n(IS) [44], evaluation metrics mainly used in the field of im-\nage generation. We compare our model with top GAN mod-\nels such as StyleGAN2-ADA [32], and model related to our\nresearch such as TransGAN. In Section 4.1, we show per-\nformance results of Styleformer in low-resolution dataset.\nSection 4.2 provide evidence for a successful application of\nLinformer, including performance of Styleformer-L. In Sec-\ntion 4.3, we show high performance of Styleformer-C and\nprove the advantage and efficiency of our model by style\nmixing, and attention map visualization.\n4.1. Low-Resolution Synthesis with Styleformer\nStyleformer achieves comparable performance to state-\nof-the-art in various low-resolution single-object datasets,\nincluding CIFAR-10 (32 × 32) [36], STL-10 (48 × 48) [9],\nand CelebA (64 × 64) [40].\nAs shown in Table 2, Styleformer outperforms prior\nGAN-based models, in terms of FID and IS. Especially\nin CIFAR-10, Styleformer records FID 2.82, and IS 10.00,\nwhich is comparable to current state-of-the-art and outper-\nforming StyleGAN2-ADA-tuning. These results indicates\nthat the Styleformer encoder has been modified to generate\nimage successfully. Implementation details are in Appendix\nA.\n4.2. Applying Linformer to Styleformer\nWe experiment our method at Section 3.4 which ap-\nplies Linformer to Styleformer (Styleformer-L) on CelebA,\n64 × 64 resolution, and LSUN-Church [54] dataset resized\nto 128×128 resolution. As shown in Table 3, we find signif-\nicant improvements in speed and memory and better perfor-\nmance than conventional Styleformer on CelebA. Memory\nperformance is approximately three times more effective\nand speed performance is 1.3 times better in Styleformer-L.\nWe also succeed in generating images of 128 × 128 resolu-\ntion with the LSUN-Church dataset, which is difficult with\npure Styleformer due to expensive memory.\nIn addition, in the CelebA dataset, Styleformer-L shows\nhigher performance in terms of FID than Styleformer, im-\nproving FID scores from 3.92 to 3.36. To analyze this phe-\nnomenon, we extract an attention map from Styleformer for\ngenerated CelebA images. As in [50], we apply singular\nvalue decomposition into attention map matrix, and plot the\nnormalized cumulative singular value averaged over 1k gen-\nerated images. As shown in Figure 6, most of the informa-\n6\nFigure 5. From the left, results generated by Styleformer on CIFAR-10 and STL-10, results generated by Styleformer-L on CelebA and\nLSUN-church and results generated by Styleformer-C on AFHQ-Cat. For more generated samples, please see Appendix G.\nCifar-10 STL-10 CelebA\nMethod FID↓ IS↑ Method FID↓ IS↑ Method FID↓\nProgressive-GAN [31] 15.52 8.80 ±0.05 SN-GAN [42] 40.1 9.16 ±0.12 PAE [5] 49.2\nAutoGAN [20] 12.42 8.55 ±0.10 Improving MMD-GAN [51] 37.64 9.23±0.08 BEGAN-CS [6] 34.14\nStyleGAN V2 [34] 11.07 9.18 AutoGAN [20] 31.01 9.16 ±0.12 PeerGAN [52] 13.95\nAdversarial NAS-GAN [20] 10.87 8.74±0.07 Adversarial NAS-GAN [17] 26.98 9.63±0.19 TransGAN-XL [28] 12.23\nTransGAN-XL [28] 9.26 9.02 ±0.11 TransGAN-XL [28] 18.28 10.43 ±0.17 HDCGAN [11] 8.77\nStyleGAN2-ADA [32] 2.92 9.83 ±0.04 SNGAN-DCD [46] 17.68 9.33 NCP-V AE [1] 5.25\nStyleformer 2.82 10.00 ±0.12 Styleformer 15.17 11.01 ±0.15 Styleformer 3.92\nTable 2. Comparison results between Styleformer and other GAN models on low-resolution datasets. Results of other GAN models are\ncollected from papers that reports their best results. We compute FID, IS in the same way as StyleGAN2-ADA, generating 50k images\nand compare their statistics against the 50k images from the training set for FID, computing the mean over 10 dependent trials using 5k\ngenerated images per trial for IS.\nDataset Model FID↓ Memory per GPU↓ Speed↓\nCelebA Styleformer 3.92 14668MiB 6.46\nStyleformer-L3.36 5316MiB 4.93\nLSUN churchStyleformer - OOM -\nStyleformer-L7.99 8118MiB 9.81\nTable 3. Results on Styleformer-L which applies Linformer.\n“Memory” is measured on 4 Titan-RTX with 16 batch size per\nGPU and “Speed” means seconds for processing 1k images\n(sec/1kimg). We use the same hidden dimension and the number\nof layers in Styleformer and Styleformer-L.\nFigure 6. Spectrum analysis of attention map matrix at 32, 64\nresolution.We use pretrained Styleformer with CelebA dataset.\ntion in the attention map matrix can be recovered from the\nfew large singular value, which means that the rank of at-\ntention map matrix is low. With low rank attention map,\nMethod CLEVR Cityscapes\nFID↓ IS↑ FID↓ IS↑\nGAN [21] 25.02 2.17 11.57 1.63\nk-GAN [45] 28.09 2.21 51.08 1.66\nSAGAN [56] 26.04 2.17 12.81 1.68\nStyleGAN2 [34] 16.05 2.15 8.35 1.70\nVQGAN [15] 32.60 2.03 173.80 2.82\nStyleformer-C 11.67 2.27 5.99 2.56\nTable 4. Comparison between popular CNN based GAN models\nand Styleformer-C on CLEVR and Cityscapes. We use the results\nin [26] for the performance of other models.\nLinformer can be applied more efficiently [50].\nTherefore, we show the possibility that when applying a\nself-attention operation for high-resolution images, it is not\nnecessary to apply attention to all pixels and provide scala-\nbility to generate high-resolution images using Styleformer-\nL. See Appendix E for implementation details.\n4.3. Styleformer can Capture Global Interaction\nWe experiment our method at Section 3.4 which com-\nbines Styleformer and StyleGAN2 (Styleformer-C) on\nCLEVR(256×256) [30] and Cityscapes (256×256) [10] for\nmulti-object images and compositional scenes, AFHQ CAT\n(512 × 512) [8] for high-resolution single-object images.\n7\n4UZMF(\"/\u0013\u0001TPVSDF4UZMFGPSNFS\u0001TPVSDF\nFigure 7. Style mixing experiment with Styleformer-C on CLEVR dataset. The images on the x-axis and y-axis were generated from their\nrespective latent codes (StyleGAN2 source and Styleformer source, respectively); the rest of the images were generated by applying styles\nfrom Styleformer source to Styleformer at low resolution and applying styles from StyleGAN2 source to StyleGAN2 at high resolution [33].\nFigure 8. Visualizing attention map in generated CLEVR images.\nAs shown in Table 4, Styleformer-C records FID 11.67,\nIS 2.27 in CLEVR, and FID 5.99, IS 2.56 in Cityscapes\nwhich is comparable performance to current state-of-the-\nart, and showing better performance than StyleGAN2 in\nmulti-object images and compositional scenes. This indi-\nrectly shows that Styleformer helps model to handle long-\nrange dependency between components.\nTo show more solid evidence that Styleformer cap-\ntures global interaction, We conduct style mixing [33] in\nStyleformer-C. In detail, when generating new image from\nCLEVR dataset, we use two different latent codesz1, z2 and\napplying z1 to Styleformer at low resolution andz2 to Style-\nGAN2 at high resolution. As shown in Figure 7, style cor-\nresponding to Styleformer (low-resolution) brings the ba-\nsis for structural generation such as the location and struc-\nture of objects, while all colors or textures remain same.\nOn the contrary, style corresponding to StyleGAN2 (high-\nresolution) brings the color and texture change, while main-\ntaining location and shape of objects. This results directly\nprove that Styleformer controls global structure between\nobjects, and handles long-range dependency.\nIn addition, we visualize the attention map to provide\nmore insight into the model’s generating process. Figure 8\nshows the concentration of attention to the position where\nthe object exists. These visualizations show that the self-\nattention operation worked efficiently, enabling the model\nto perform long-range interaction, overcome the shortcom-\ning of convolution operation.\n5. Conclusion\nWe propose Styleformer, a Transformer-based genera-\ntive network that is novel and effective. We propose a\nmethod to efficiently generate images with self attention op-\neration and achieve SOTA performance on various datasets.\nFurthermore, we propose Styleformer-L, which reduces\nthe complex computation to linear, enabling to generate\nhigh-resolution images. We also present a method of ef-\nficiently generating a compositional scene while capturing\nwith long-range dependency through Styleformer-C. There\nstill seems to be room for improvement, such as reducing\ncomputation cost, but we hope that our work will speed up\nthe application of Transformers to the field of computer vi-\nsion, helping the development of the computer vision field.\nHowever, development of the generative model can create\nfake media data using synthesized face images (e.g. deep-\nfake), so particular attention should be paid in the future.\n8\nReferences\n[1] Jyoti Aneja, Alexander Schwing, Jan Kautz, and Arash Vah-\ndat. Ncp-vae: Variational autoencoders with noise con-\ntrastive priors, 2020. 7\n[2] Martin Arjovsky, Soumith Chintala, and L ´eon Bottou.\nWasserstein gan, 2017. 2\n[3] Sergei Belousov. Mobilestylegan: A lightweight convolu-\ntional neural network for high-fidelity image synthesis, 2021.\n1, 12\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?,\n2021. 2\n[5] Vanessa B ¨ohm and Uroˇs Seljak. Probabilistic auto-encoder,\n2020. 7\n[6] Chia-Che Chang, Chieh Hubert Lin, Che-Rung Lee, Da-\nCheng Juan, Wei Wei, and Hwann-Tzong Chen. Escaping\nfrom collapsing modes in a constrained space, 2018. 7\n[7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,\nSunghun Kim, and Jaegul Choo. Stargan: Unified genera-\ntive adversarial networks for multi-domain image-to-image\ntranslation, 2018. 2\n[8] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains,\n2020. 7\n[9] Adam Coates, Andrew Ng, and Honglak Lee. An analysis\nof single-layer networks in unsupervised feature learning. In\nGeoffrey Gordon, David Dunson, and Miroslav Dud ´ık, ed-\nitors, Proceedings of the Fourteenth International Confer-\nence on Artificial Intelligence and Statistics , volume 15 of\nProceedings of Machine Learning Research, pages 215–223,\nFort Lauderdale, FL, USA, 11–13 Apr 2011. PMLR. 6\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding, 2016. 7\n[11] J. D. Curt ´o, I. C. Zarza, Fernando de la Torre, Irwin King,\nand Michael R. Lyu. High-resolution deep convolutional\ngenerative adversarial networks, 2020. 7\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2020. 2\n[13] Vincent Dumoulin, Ethan Perez, Nathan Schucher, Flo-\nrian Strub, Harm de Vries, Aaron Courville, and Yoshua\nBengio. Feature-wise transformations. Distill, 2018.\nhttps://distill.pub/2018/feature-wise-transformations. 4\n[14] Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.\nA learned representation for artistic style, 2017. 4\n[15] Patrick Esser, Robin Rombach, and Bj ¨orn Ommer. Taming\ntransformers for high-resolution image synthesis, 2021. 7\n[16] Aviv Gabbay and Yedid Hoshen. Style generator inversion\nfor image enhancement and animation, 2019. 2\n[17] Chen Gao, Yunpeng Chen, Si Liu, Zhenxiong Tan, and\nShuicheng Yan. Adversarialnas: Adversarial neural archi-\ntecture search for gans, 2020. 7\n[18] Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent\nDumoulin, and Jonathon Shlens. Exploring the structure of a\nreal-time, arbitrary neural artistic stylization network, 2017.\n4\n[19] Xavier Glorot and Yoshua Bengio. Understanding the dif-\nficulty of training deep feedforward neural networks. In\nYee Whye Teh and Mike Titterington, editors, Proceedings\nof the Thirteenth International Conference on Artificial In-\ntelligence and Statistics , volume 9 of Proceedings of Ma-\nchine Learning Research, pages 249–256, Chia Laguna Re-\nsort, Sardinia, Italy, 13–15 May 2010. PMLR. 5\n[20] Xinyu Gong, Shiyu Chang, Yifan Jiang, and Zhangyang\nWang. Autogan: Neural architecture search for generative\nadversarial networks, 2019. 7\n[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks, 2014. 1,\n2, 7\n[22] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre\nStock, Armand Joulin, Herv ´e J ´egou, and Matthijs Douze.\nLevit: a vision transformer in convnet’s clothing for faster\ninference, 2021. 2\n[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium, 2018. 6\n[24] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications, 2017.\n3\n[25] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization, 2017. 4\n[26] Drew A. Hudson and C. Lawrence Zitnick. Generative ad-\nversarial transformers, 2021. 2, 7\n[27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.\nEfros. Image-to-image translation with conditional adver-\nsarial networks, 2018. 2\n[28] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan, 2021. 2, 7, 11\n[29] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image genera-\ntion from scene graphs, 2018. 2\n[30] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,\nLi Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr:\nA diagnostic dataset for compositional language and elemen-\ntary visual reasoning, 2016. 7\n[31] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation, 2018. 2, 7\n[32] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine,\nJaakko Lehtinen, and Timo Aila. Training generative adver-\nsarial networks with limited data, 2020. 2, 6, 7\n[33] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks,\n2019. 2, 8\n[34] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan, 2020. 2, 7, 14\n9\n[35] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning,\n2020. 2\n[36] Alex Krizhevsky. Learning multiple layers of features from\ntiny images. Technical report, 2009. 6\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.\nWeinberger, editors, Advances in Neural Information Pro-\ncessing Systems, volume 25. Curran Associates, Inc., 2012.\n1\n[38] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe\nShi. Photo-realistic single image super-resolution using a\ngenerative adversarial network, 2017. 2\n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows, 2021. 2\n[40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In Proceedings of\nInternational Conference on Computer Vision (ICCV) , De-\ncember 2015. 6\n[41] Xudong Mao, Qing Li, Haoran Xie, Raymond Y . K. Lau,\nZhen Wang, and Stephen Paul Smolley. Least squares gen-\nerative adversarial networks, 2017. 2\n[42] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\nYuichi Yoshida. Spectral normalization for generative ad-\nversarial networks, 2018. 2, 7\n[43] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\nvised representation learning with deep convolutional gener-\native adversarial networks, 2016. 1\n[44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans, 2016. 6\n[45] Samarth Sinha, Zhengli Zhao, Anirudh Goyal, Colin Raffel,\nand Augustus Odena. Top-k training of gans: Improving gan\nperformance by throwing away bad samples, 2020. 7\n[46] Yuxuan Song, Qiwei Ye, Minkai Xu, and Tie-Yan Liu. Dis-\ncriminator contrastive divergence: Semi-amortized gener-\native modeling by exploring energy of the discriminator,\n2020. 7\n[47] Mingxing Tan and Quoc V . Le. Efficientnet: Rethinking\nmodel scaling for convolutional neural networks, 2020. 2\n[48] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario\nLucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-\nchitecture for vision, 2021. 3, 12\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need, 2017. 2\n[50] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity,\n2020. 1, 6, 7\n[51] Wei Wang, Yuan Sun, and Saman Halgamuge. Improving\nmmd-gan training with repulsive loss function, 2019. 7\n[52] Jiaheng Wei, Minghao Liu, Jiahao Luo, Qiutong Li, James\nDavis, and Yang Liu. Peergan: Generative adversarial net-\nworks with a competing peer discriminator, 2021. 7\n[53] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers, 2021. 2\n[54] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans\nin the loop, 2016. 6\n[55] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S. Huang. Generative image inpainting with contex-\ntual attention, 2018. 1, 2\n[56] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-\ntus Odena. Self-attention generative adversarial networks,\n2019. 1, 7\n[57] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers, 2021. 2\n[58] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.\nEfros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks, 2020. 2\n[59] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka.\nSean: Image synthesis with semantic region-adaptive nor-\nmalization. 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun 2020. 2\n[60] Zhen Zhu, Zhiliang Xu, Ansheng You, and Xiang Bai. Se-\nmantically multi-modal image synthesis, 2020. 2\n10\nA. Implementation Details of Styleformer\nWe implemented our Styleformer on top of the\nStyleGAN2-ADA Pytorch implementation 1. Most of the\ndetails have not changed except generator architecture. We\nused CIFAR-10 tuning version of StyleGAN2-ADA, which\nmeans disabling style mixing regularization, path length\nregularization, and residual connections in D when training.\nWe also fixed mapping network’s depth to 2. We used bilin-\near filtering in all upsampling layer used in Styleformer. We\nuse data augmentation pipeline suggested in StyleGAN2-\nADA, and did not use mixed-precision training for all ex-\nperiments.\nFor Styleformer encoder, we add bias and noise at the\nend of the encoder block, then performing leaky RELU\nwith α = 0 .2. After passing several encoder blocks, we\nreshaped it to the form of square feature map, i.e., Unflat-\nten. We then proceed bilinear upsample operation as we\nsaid in the Section 3.1, but also convert these reshaped out-\nput for each resolution into an RGB channel, using ToRGB\nlayer. We upsample each of RGB output and add to each\nother, creating an output-skip connection generator, similar\nto StyleGAN2 generator. Originally ToRGB layer converts\nhigh-dimensional per pixel data into RGB per pixel data via\n1×1 convolution operation, which we replace it to same op-\neration, linear operation. We initialize all weights in Style-\nformer encoder using same method used in Pytorch linear\nlayer. Unlike StyleGAN2-ADA, we employ weight demod-\nulation also in ToRGB layer. We perform all experiments\nwith 4 Titan-RTX using Pytorch 1.7.1. All of our experi-\nments presented in the paper including failure spent about\nfour months.\nAs in Section 4.1, the number of the Styleformer encoder\nand hidden dimension size for each resolution can be chosen\nas hyperparameters. We call these two hyperparameters as\n”Layers” and ”Hidden size”, respectively.\nLow-Resolution Synthesis with Styleformer For low-\nresolution synthesis experiment in Section 4.1, we use pure\nStyleformer. Each Layers and Hidden size used for CIFAR-\n10, STL-10, and CelebA are shown in Table 5. We trained\nStyleformer for 65M, 92M, 25M at CIFAR-10, STL-10 and\nCelebA, respectively.\nFor CIFAR-10 experiment, we use 50K images ( 32 ×\n32) at the training set, without using the label. For STL-10\nexperiment, we resize96×96 image datasets to48×48, and\nusing 5k training images, 100k unlabeled images together\nas in [28]. We change the size of the constant input from\n8×8 to 12×12 to generate an image with a size of48×48.\nFor CelebA dataset, we use 200k unlabeled face images of\nthe Align and Cropped version, which we resize to 64 × 64\n1https : / / github . com / NVlabs / stylegan2 - ada -\npytorch\nTable 5. Details of Styleformer hyperparameters at low resolution\nsynthesis. This model setting match with performance result at\nTable 2 in paper.\nDatasets Layers Hidden size FID\nCIFAR-10 {1,3,3} { 1024,512,512} 2.82\nSTL-10 {1,2,2} { 1024,256,64} 15.17\nCelebA {1,2,1,1} {1024,256,64,64} 3.92\nresolution as in [28]. We start at 8 × 8 constant, as we train\nCIFAR-10 dataset.\nAblation study details As we said in paper, we use small\nversion of Styleformer with CIFAR-10 dataset for ablation\nstudy. In more detail, we use 1,2,2 for Layers, 256, 64, 16\nfor Hidden size, trained for 20M images.\nNumber of head experiment We conduct experiment\nabout number of heads effect using one layer Styleformer,\nas said in Figure 3. One Layer Styleformer is a model that\nstarts with 32×32 learned constant and only have one Style-\nformer encoder with hidden dimension size 256. We trained\nthe model for 20M images, same as ablation study.\nB. Attention map analysis\nPost-Layer normalization We analyze the results of the\nattention map experiment on layer normalization. We pro-\npose in Section 3.2 that if we perform layer normalization at\nthe end of Styleformer encoder, it breaks the attention map.\nFigure 9 shows that if layer normalization is located at the\nend of the encoder, the attention map has not been properly\nlearned. Therefore, we position layer normalization to the\nfront of the encoder (i.e. after applying style modulation)\nso that the attention map can effectively learn the relation-\nship between pixels. The experiment is all conducted on\nCIFAR-10, using small version of Styleformer, same as ab-\nlation study.\nDemodulation for Query and Key As in Section 3.3, we\ndemonstrate that when an attention map is created with Q,\nK from input scaled by style vector, specific value in atten-\ntion map becomes very large. We show the attention map\nwithout demodulation operation to Q and K in Styleformer\nat Figure 9. We can see that without demodulation opera-\ntion, attention is heavily concentrated on a particular pixel,\npreventing the attention operation from working properly.\nWe overcome this problem with demodulation operation to\nQ, K, before creating attention map.\n11\nOriginal\n16✕16 16✕16 32✕32 32✕328✕8\nEnd of the \nblock\nFigure 9. Comparison of attention map experimented based on Layer Normalization\n8✕8 16✕16 16✕16 32✕32 32✕32\nWithout \nDemodulation \nQuery and Key\nFigure 10. Attention map without demodulation operation to query and key. Comparing with Figure 9 upper row, we can see specific large\nvalue in attention map.\nC. Intuition of Increased Multi-Head Attention\nTransformer is designed for natural language process-\ning (NLP). It is difficult to use original Transformer in the\nfield of image generation. A convolution network can ef-\nficiently generate images, unlike a linear layer, because it\nproceeds both operations between pixels using kernels and\nbetween channels. Although not a convolution network\nstructure, [48] has shown that architecture with pixel-to-\npixel and channel-to-channel operations can be efficient in\nlearning information about images.\nUnlike the original convolution operation, in Mo-\nbileStyleGAN [3], they show good performance in image\ngeneration even by separating the interpixel and interchan-\nnel operations with depthwise separable convolution.\nMain operation of Transformer can be divided into a in-\nterpixel operation (i.e. pixel section) and a interchannel op-\neration (i.e. channel section) and it can be expressed by the\nfollowing formula.\nAi = softmax(QiKT\ni√dk\n), (5)\nheadi = AiVi, (6)\nMultihead(Q, K, V) =Concat(head1, . . . , headk)WO,\n(7)\nIn above formulation, Qi, Ki, Vi is query, key, value for\neach head. d is the dimension of ( query, key, value), k is\nthe number of heads, and dk is d/k. Ai is attention map\nand WO is the parameters of a linear layer that integrates\nmulti-heads.\nPixel section corresponds for the self-attention operation\nbetween pixels (i.e., Equation 6), and channel section corre-\nsponds for integration of multi-head with linear layer, which\noperates between channels (i.e., Equation 7). In a Trans-\nformer, the pixel section is slightly different from depthwise\nconvolution. In depthwise convolution, kernel weights exist\nfor each channel, but in Transformer, attention map A acts\nlike one huge kernel, which means applying equal kernel\nweight to all different channels in V . It is difficult to cre-\nate a powerful generator using a Transformer because the\nsame attention kernel is applied for each channel, unlike the\ngenerator using depthwise separable convolution.\nUsing increased multi-head attention, this problem could\nbe overcome. We can generate various attention maps (i.e.,\nkernels) by increasing the number of heads. However, in-\ncreasing the number of heads inevitably leads to smaller\ndepth, where depth is hidden channel dimension divided by\nthe number of heads. Since depth is a dimension used to\ncreate the attention map, there exists a minimum depth re-\nquired. However, due to differences in the properties of pix-\nels and tokens, the required depth size in computer vision is\nsmaller than NLP.\nIn the field of NLP, a single token has a lot of informa-\ntion, so the required depth dimension, which represents the\ntoken, must be large, but one pixel in an image has less in-\nformation than a token, which means that the required depth\nis smaller than a traditional Transformer. As described in\nthe caption of Figure 3, the left graph shows a hidden di-\nmension of 256, and the right graph shows 32. In the left\ngraph, until the number of heads is 8 (depth is 32), per-\n12\nformance increases when the number of heads grows up,\nbut after that, even if the number of heads is increased, the\nperformance decreases because the depth is less than 32.\nSimilarly, in the right graph, performance is best when the\nnumber of heads is 1 (depth is 32), and after that, perfor-\nmance degrades because the depth is less than 32. From\nthese results, we demonstrate that increasing the number of\nheads too much results in poor performance, and fixing the\ndepth to 32.\nMeanwhile, the channel section is a layer to integrate\nthe multi-head together, where the linear layer is exactly\nthe same operation as the 1 × 1 convolution, i.e., point-\nwise convolution. Unlike convolution, the attention kernel\nis a kernel generated by the input itself, so it can create a\nmore dense kernel, and it is advantageous to capture global\nfeatures because it considers the relationship between all\npixels. Therefore, by enhancing multi-head attention, the\nStyleformer can play a more powerful role as depthwise\nseparable convolution, enabling generate high-quality im-\nages.\nD. Demodulation for Encoder Output\nAs described in Section 3.3, self-attention conducts more\noperations than the convolution operation, making the de-\nmodulation process more complex. We show how standard\ndeviation of encoder output can be derived (Demodulation\nfor Encoder Output at Section 3.3), and the effect of the\nnumber of pixel in output standard deviation.\nDerivation of encoder output standard deviation After\ndemodulation operation to Q, K and V , Styleformer en-\ncoder performs style modulation to input V , weighted sum\nof V with attention map, and then performs linear opera-\ntion. Let’s consider the attention map matrix as A, and the\nweight matrix of linear operation as w. Since the matrix\nmultiplication is associative, statistics of the output are the\nsame even if the linear operation is calculated before multi-\nplication with attention map :\noutput = [A(sj · V )]w = A[(sj · V )w]. (8)\nFrom now on, we think the linear operation is conducted\nbefore the multiplication with the attention map. Therefore,\nsame as demodulation for query, key, and value, style mod-\nulation to V can be replaced to scaling linear weights:\nw′\njk = sj · wjk, (9)\nwhere sj scales jth feature map of V , and k enumerates the\nflattened output feature map. Assuming that input V have\na unit standard deviation, the standard deviation of output\nafter linear operation can be derived as follows:\nσk =\nsX\nj\nw\n′\njk\n2. (10)\nFinally, this output is multiplied with the attention map ma-\ntrix A. When the attention score vector for lth pixel is ex-\npressed as Al·, standard deviation of encoder output is as\nfollows:\nσ\n′\nlk =\nsX\n·\nAl·2 ·\nX\nj\nw\n′\njk\n2. (11)\nEffect of the number of pixel Scaling the encoder out-\nput feature map k with 1/σ\n′′\nk where σ\n′′\nk =\nqP\nj w\n′\njk\n2, the\nstandard deviation of output activations will be\nσlk =\nsX\n·\nAl·2. (12)\nSince the attention mapA is matrix after softmax operation,P\n· Al· = 1. Assuming Al· are i.i.d random variables from\nnormal distribution with mean 1\nn and variance 1\nn2, and n\ndenotes the number of pixel,σlk2 can be derived as follows:\nσlk\n2 =\nX\n·\nAl·\n2 =\nX\n·\n(Al· − 1\nn)2 + 1\nn, (13)\nusing P\n· Al· = 1. Then (Al·− 1\nn) become random variables\nfrom normal distribution with zero mean and variance1/n2.\nBased on the property of normal distribution,P\n· (Al· − 1\nn)\nfollows gamma distribution with shape parameter n\n2 and\nscale parameter 2\nn2. Therefore, using Chebyshev inequal-\nity, we have\nPr(|\nX\n·\n(Al· − 1\nn)2 − 1\nn| ≤1\nn) ≥ 1 − 2\nn, (14)\nmeaning σlk approaches zero when the number of pix-\nels(i.e., n) increase.\nE. Implementation details of Styleformer-L\nWe introduce Styleformer-L, a model with Linformer ap-\nplied to self-attention operation of Styleformer. When ap-\nplying Linformer, we fix k (i.e., projection dimension for\nkey, value) to 256, and apply to the encoder block above\n32 ×32 resolution. For projection parameter sharing effect,\nwe used Key-value sharing. It means we create single E\nprojection matrix for each layer that applies equally to the\nkey, value of each head. We project key and value to k di-\nmension after demodulation for key and value, i.e., before\ncreating attention map and weight sum of value. When us-\ning Linformer, to prevent augmentation leaking, we clamp\naugmentation probability to 0.7.\n13\nCelebA We conduct CelebA experiment using\nStyleformer-L in the same setting as CelebA using\npure Styleformer for fair comparision (Table 3 in paper).\nWe use {1, 2, 1, 1} for Layers, {1024, 256, 64, 64} for\nHidden size, and training for 25M images.\nLSUN-Church We use {1, 2, 1, 1, 1} for Layers,\n{1024, 256, 64, 64, 64} for Hidden size, training for 40M\nimages.\nF. Implementation Details of Styleformer-C\nWe introduce Styleformer-C, which combines Style-\nformer and StyleGAN2. We generate low-resolution parts\n(up to 32×32) of image using Styleformer Encoder, and the\nrest of the resolutions parts of image are generated by ap-\nplying StyleGAN2 block. Fundamentally, in Styleformer-\nC, the detail of Styleformer part is the same as that of\nAppendix A, and the StyleGAN2 part is the same as that\nof StyleGAN2 [34] implementation. We experiment with\nCLEVR and Cityscapes which is high-resolution multi-\nobject or compositional scene datasets with a image size\nof 256 × 256, and we also experiment AFHQ-Cat which is\na high-resolution single-object dataset with a image size of\n512 × 512. We performed all training runs on NVIDIA 2\nTesla V100 GPUs.\nCLEVR and Cityscapes In CLEVR, we start at 8 × 8\nlearned constant input just like the default setting and used\n{1, 2, 1, 1, 1, 1} for Layers, {1024, 256, 256, 256, 256, 128}\nfor Hidden size, training for 10M images. Here, what Lay-\ners means in StyleGAN2 is a block which has two convolu-\ntion operations and what Hidden size means in StyleGAN2\nis the number of channels. In Cityscapes, we apply Style-\nGAN2 in the same way as CLEVR. We also set Layers and\nHidden size same as CLEVR experiment. Furthermore, we\ntrain Styleformer-C for 36M images.\nAFHQ-Cat Likewise, we start with 8 × 8 learned con-\nstant input and generate an image of 512 × 512 size.\nWe set Layers to {1, 2, 1, 1, 1, 1} and Hidden size to\n{1024, 256, 256, 256, 256, 64} to train Styleformer-C for\n9M images.\nG. Visual Samples\nWe show various samples generated with Styleformer,\nStyleformer-L and Styleformer-C in this section.\n14\nFigure 11. High-resolution samples generated by Styleformer-C on AFHQ-Cat.\n15\nFigure 12. High-resolution samples generated by Styleformer-C on CLEVR.\n16\nFigure 13. High-resolution samples generated by Styleformer-C on Cityscapes.\n17\nFigure 14. High-resolution samples generated by Styleformer-L on CelebA.\n18\nFigure 15. High-resolution samples generated by Styleformer-L on LSUN-church.\n19\nFigure 16. Samples generated by Styleformer on CIFAR-10.\n20"
}