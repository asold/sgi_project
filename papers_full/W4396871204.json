{
    "title": "Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning",
    "url": "https://openalex.org/W4396871204",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2006518923",
            "name": "Ning Wang",
            "affiliations": [
                "Baidu (China)",
                "City University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A1985857611",
            "name": "Jiang Bian",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2106373672",
            "name": "Yuchen Li",
            "affiliations": [
                "Baidu (China)",
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2127806682",
            "name": "Xuhong Li",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2127294316",
            "name": "Shahid Mumtaz",
            "affiliations": [
                "Nottingham Trent University",
                "Silesian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2144474767",
            "name": "Ling-He Kong",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2336297545",
            "name": "Haoyi Xiong",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2006518923",
            "name": "Ning Wang",
            "affiliations": [
                "City University of Hong Kong",
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1985857611",
            "name": "Jiang Bian",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2106373672",
            "name": "Yuchen Li",
            "affiliations": [
                "Baidu (China)",
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2127806682",
            "name": "Xuhong Li",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2127294316",
            "name": "Shahid Mumtaz",
            "affiliations": [
                "Nottingham Trent University",
                "Silesian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2144474767",
            "name": "Ling-He Kong",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2336297545",
            "name": "Haoyi Xiong",
            "affiliations": [
                "Baidu (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2177784250",
        "https://openalex.org/W2233603983",
        "https://openalex.org/W3005769002",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W3215060724",
        "https://openalex.org/W3127238141",
        "https://openalex.org/W4362700579",
        "https://openalex.org/W4382490702",
        "https://openalex.org/W4225763451",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W4383506054",
        "https://openalex.org/W4290546063",
        "https://openalex.org/W2089766336",
        "https://openalex.org/W3095979265",
        "https://openalex.org/W2070136856",
        "https://openalex.org/W2041344749",
        "https://openalex.org/W2058276221",
        "https://openalex.org/W4307079438",
        "https://openalex.org/W2993894543",
        "https://openalex.org/W2890017579",
        "https://openalex.org/W4213095938",
        "https://openalex.org/W4388539614",
        "https://openalex.org/W2740619578",
        "https://openalex.org/W2108230379",
        "https://openalex.org/W2806518291",
        "https://openalex.org/W2884170967",
        "https://openalex.org/W2120456037",
        "https://openalex.org/W2154085311",
        "https://openalex.org/W4366986661",
        "https://openalex.org/W2134629862",
        "https://openalex.org/W2407848977",
        "https://openalex.org/W3212533323",
        "https://openalex.org/W2759571676",
        "https://openalex.org/W2536860838",
        "https://openalex.org/W2951298881",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4211254980",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W4286669150",
        "https://openalex.org/W2331530999",
        "https://openalex.org/W2061657172",
        "https://openalex.org/W4283362091",
        "https://openalex.org/W3135697420",
        "https://openalex.org/W3126773939",
        "https://openalex.org/W2025763720",
        "https://openalex.org/W3111061871",
        "https://openalex.org/W6911256845",
        "https://openalex.org/W6930188195",
        "https://openalex.org/W6967137381",
        "https://openalex.org/W1967102258",
        "https://openalex.org/W3013450562",
        "https://openalex.org/W4293194795",
        "https://openalex.org/W3082320051",
        "https://openalex.org/W3180037578",
        "https://openalex.org/W2098571862",
        "https://openalex.org/W1966857768",
        "https://openalex.org/W2086561953",
        "https://openalex.org/W2141157874",
        "https://openalex.org/W2954102902",
        "https://openalex.org/W4300861274",
        "https://openalex.org/W2102017611",
        "https://openalex.org/W2031352989"
    ],
    "abstract": "Abstract Pretrained language models have shown promise in analysing nucleotide sequences, yet a versatile model excelling across diverse tasks with a single pretrained weight set remains elusive. Here we introduce RNAErnie, an RNA-focused pretrained model built upon the transformer architecture, employing two simple yet effective strategies. First, RNAErnie enhances pretraining by incorporating RNA motifs as biological priors and introducing motif-level random masking in addition to masked language modelling at base/subsequence levels. It also tokenizes RNA types (for example, miRNA, lnRNA) as stop words, appending them to sequences during pretraining. Second, subject to out-of-distribution tasks with RNA sequences not seen during the pretraining phase, RNAErnie proposes a type-guided fine-tuning strategy that first predicts possible RNA types using an RNA sequence and then appends the predicted type to the tail of sequence to refine feature embedding in a post hoc way. Our extensive evaluation across seven datasets and five tasks demonstrates the superiority of RNAErnie in both supervised and unsupervised learning. It surpasses baselines with up to 1.8% higher accuracy in classification, 2.2% greater accuracy in interaction prediction and 3.3% improved F1 score in structure prediction, showcasing its robustness and adaptability with a unified pretrained foundation.",
    "full_text": "Nature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557 548\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-024-00836-4\nMulti-purpose RNA language modelling with \nmotif-aware pretraining and type-guided \nfine-tuning\nNing Wang‚Äâ  ‚Äâ1,2,6, Jiang Bian1,6, Yuchen Li1,3, Xuhong Li1, Shahid Mumtaz‚Äâ  ‚Äâ4,5, \nLinghe Kong‚Äâ  ‚Äâ3  & Haoyi Xiong‚Äâ  ‚Äâ1,6 \nPretrained language models have shown promise in analysing nucleotide \nsequences, yet a versatile model excelling across diverse tasks with a single \npretrained weight set remains elusive. Here we introduce RNAErnie, an \nRNA-focused pretrained model built upon the transformer architecture, \nemploying two simple yet effective strategies. First, RNAErnie enhances \npretraining by incorporating RNA motifs as biological priors and introducing \nmotif-level random masking in addition to masked language modelling at \nbase/subsequence levels. It also tokenizes RNA types (for example, miRNA, \nlnRNA) as stop words, appending them to sequences during pretraining. \nSecond, subject to out-of-distribution tasks with RNA sequences not seen \nduring the pretraining phase, RNAErnie proposes a type-guided fine-tuning \nstrategy that first predicts possible RNA types using an RNA sequence and \nthen appends the predicted type to the tail of sequence to refine feature \nembedding in a post hoc way. Our extensive evaluation across seven datasets \nand five tasks demonstrates the superiority of RNAErnie in both supervised \nand unsupervised learning. It surpasses baselines with up to 1.8% higher \naccuracy in classification, 2.2% greater accuracy in interaction prediction and \n3.3% improved F1 score in structure prediction, showcasing its robustness \nand adaptability with a unified pretrained foundation.\nRNA is a critical molecule in the central dogma of molecular biology, \nwhich describes the flow of genetic information from DNA to RNA to \nprotein.\nRNA molecules play a crucial role in various cellular processes, \nincluding gene expression, regulation and catalysis. Given the impor-\ntance of RNA in biological systems, there is a growing demand for \nefficient and accurate methods to analyse RNA sequences. The analysis \nof RNA sequences has traditionally been performed using experimen-\ntal techniques such as RNA sequencing and microarrays 1,2. However, \nthese methods are often expensive and time-consuming and require \nlarge amounts of input RNA. In recent years, there has been increasing \ninterest in using computational methods based on machine learning \nmodels to analyse RNA sequences.\nPretrained language models, on the other hand, have shown \ngreat success in various natural language processing tasks, including \ntext classification3, question answering 4 and language translation 5. \nAdvancements in the field of natural language processing have led to \nthe successful adoption of pretrained language models like BERT\n6 to \nmodel and analyse nucleotides (nts) and ribonucleotides from trillions \nof DNA/RNA sequences. For example, preMLI 7 employs rna2vec to \nReceived: 19 August 2023\nAccepted: 10 April 2024\nPublished online: 13 May 2024\n Check for updates\n1Big Data Lab, Baidu Inc., Beijing, China. 2Department of Computer Science, City University of Hong Kong, Hong Kong, China. 3Department of Computer \nScience and Engineering, Shanghai Jiao Tong University, Shanghai, China. 4Department of Computer Science, Nottingham Trent University, Nottingham, \nUK. 5Department of Applied Informatics, Silesian University of Technology, Gliwice, Poland. 6These authors contributed equally: Ning Wang, Jiang Bian, \nHaoyi Xiong. ‚Äâe-mail: linghe.kong@sjtu.edu.cn; haoyi.xiong.fr@ieee.org\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557\n 549\nArticle https://doi.org/10.1038/s42256-024-00836-4\neffectively captures both subsequence and motif-level knowledge17‚Äì19, \nenriching the representation of RNA sequences as illustrated in Fig. 2a. \nAdditionally, RNAErnie tokenizes coarse-grained RNA types as special \nvocabularies and appends the tokens of coarse-grained RNA types at the \nend of every RNA sequence during pretraining. By doing so, the model \ngains the potential to discern the distinct characteristics of various \nRNA types, facilitating domain adaption to various downstream tasks.\nSpecifically, a type-guided fine-tuning strategy is employed, incor-\nporating the predicted RNA types as ‚Äòauxiliary information‚Äô within \na stacking architecture, as shown in Fig. 2b . Upon receiving an RNA \nsequence as input, the model first employs a pretrained RNAErnie \nblock to generate output embeddings. Subsequently, it predicts the \npotential coarse-grained RNA types based on these embeddings. The \nsequence and the predicted RNA types are then fed into a downstream \nnetwork, which consists of RNAErnie blocks and task-specific heads. \nThis approach enables the model to accommodate a diverse range of \nRNA types and enhances its utility in a broad spectrum of RNA analytical \ntasks. More specifically, to adapt the distribution shifts between pre-\ntraining datasets and target domains, RNAErnie leverages domain adap-\ntation20 that composites the pretrained backbone with downstream \nmodules in three neural architectures: frozen backbone with trainable \nhead (FBTH), trainable backbone with trainable head (TBTH) and stack-\ning for type-guided fine-tuning (STACK). In this way, the proposed \nmethod can either end-to-end optimize the backbone and task-specific \nheads or fine-tune task-specific heads with embeddings extracted from \nthe frozen backbone, subject to the downstream applications.\nThe conducted experiments highlight the immense potential of \nRNAErnie in advancing RNA analysis. The model demonstrates strong \nproduce RNA word vector representations. The RNA sequence fea -\ntures are then mined independently, and the two feature vectors are \nconcatenated as the input for the prediction task. DNABERT8 has been \nproposed to extract features from DNA sequences via the pretrained \nlanguage model BERT-alike, and its derivatives9,10 with task-agnostic \nextensions have been studied to solve DNA analytical tasks in an ad \nhoc manner11. Moreover, based on T5 (ref. 12), Rm-LR13 integrates two \nlarge-scale RNA language pretrained models to learn local key features \nand collect discriminative sequential information. A bilinear attention \nnetwork is then used to integrate the learned features. However, there \nis still some work focusing on generic models that performs well on \nvarying downstream tasks derived from one set of pretrained weights. \nRNA-FM14 trains a foundation model for the community to fit all the \nncRNA sequences, although it only uses naive token masking as a pre-\ntraining strategy, which may lose high-density information hidden in \ncontinuous RNA subsequences. This problem is further compounded \nby the fact that RNA is a more complex molecule than DNA15, due to the \npresence of additional modifications and higher-order structures, and \nexisting pretrained models are not optimized for RNA analysis.\nIn response to this challenge, we have developed a pretrained RNA \nlanguage model: RNAErnie. As shown in Fig. 1, this model is built upon \nthe Enhanced Representation through Knowledge Integration (ERNIE) \nframework and incorporates multilayer and multihead transformer \nblocks, each having a hidden state dimension of 768. Pretraining is \nconducted using an extensive corpus consisting of approximately \n23‚Äâmillion RNA sequences meticulously curated from RNAcentral16. The \nproposed motif-aware pretraining strategy involves base-level masking, \nsubsequence-level masking and motif-level random masking, which \nA‚Äì45%\nU‚Äì15%\nC‚Äì12%\nG‚Äì11%\nN‚Äì10%\nVocabulary\nprobability\nMotif-aware pretraining\nType-guided fine-tuning\nAUCGACGCU... GU\nN C UC A U U G A ... A G\nUCAUGAUAG... NU\nUnlabelled RNA sequences\nCross-\nentropy\nA U C G A C G C U ... G U\nN C U C A U U G A ... A G\nU C A U G A U A G ... N U\nMultilevel\nmasking\nRNAE rnie\nInput embedding Output embedding\nMLM head\nMultihead self-attention\nFeed-forward network\nAdd and norm.\nAdd and norm.\n√ó12 Downstream modules\nRNA type\npredictionUnknown RNA sequence\nA U C G U G N N A ... N G\nHelix \nclosing \nSecondary structure prediction\nUnpaired\nregion\nACAACUGUGUGAUAUU\nFolding\nHelix\nstacking \nHelix\nopening\nZuker-style dynamic programming \nEnsemble K\nRNAEr n i e\nP r o j ect i o n\nOne-hot\nRNA sequence\nclassification\nRNAEr n i e\nGAUCCAGUGAN\nRNAEr n i e\nGAUCCAGN\nAUCGUGNN\nCNUCNCAG\nEnsemble\nLong-sequence\nclassification\nForward w/ gradient\nForward w/o gradient\nPretraining\nDownstream tasks\nFlow legend\nRNA‚ÄìRNA interaction prediction \nRNAEr n i e\nP r o j ect i o n\nO n e- hot\n1D-CNN\nBi-LSTM\nClsHead\nAUCGUGNN GAUCCAGN\ni+1 j‚Äì1 j‚Äì1 j‚Äì1j+1j+1\ni i\ni\nij j\nj\nj\nFig. 1 | Overview of the design of the proposed model and its applications. The \nRNAErnie model consists of 12 transformer layers. In the motif-aware pretraining \nphase, RNAErnie is trained on a dataset of approximately 23‚Äâmillion sequences \nextracted from the RNAcentral database using self-supervised learning with \nmotif-aware multilevel random masking. In the type-guided fine-tuning phase, \nRNAErnie first predicts the possible coarse-grained RNA types using output \nembeddings and then leverages the predicted types as auxiliary information \nfor fine-tuning the model with task-specific heads. w/, with; w/o, without; MLM, \nmasked language modelling; norm., normalization.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557 550\nArticle https://doi.org/10.1038/s42256-024-00836-4\nperformance across diverse downstream tasks, showcasing its versatil-\nity and effectiveness as a generic solution. Additionally, the innovative \nstrategies employed in RNAErnie show promise in enhancing the per-\nformance of other pretrained models in RNA analysis. These findings \nposition RNAErnie as a valuable asset, empowering researchers with a \npowerful tool to unravel the complexities of RNA-related investigations.\nResults\nIn this section, we present the experiment results for RNAErnie evalu-\nation on both unsupervised learning (RNA grouping) and supervised \nlearning (RNA sequence classification, RNA‚ÄìRNA interaction predic-\ntion and RNA secondary structure prediction) tasks. For additional \nexperiment settings and results (such as long-sequence classification, \nSARS-CoV-2 variant evolutionary path visualization and so on), please \nrefer to Supplementary Information Section C.\nUnsupervised clustering of RNAErnie-extracted features\nVarious types of RNA exhibit distinct functions and structures, and it \nis expected that these characteristics are captured within the embed-\ndings generated by our proposed model (RNAErnie) using raw RNA \nsequences. T o examine the patterns within the known RNA repertoire, \nwe utilize the suggested encoder to establish scatter plots of RNA \nsequences. Dimension reduction using PHATE21 is then employed to \nmap the embeddings onto a two-dimensional plane. We evaluate the \nimpact of the learning process by considering both pretrained and \nrandomly initialized RNAErnie embeddings, as well as 3mer statistical \nembeddings22 for visualization.\nFigure 3a shows the results, where the pretrained RNAErnie \nembedding space effectively organizes RNA types into distinct clus -\nters based on their structural and functional properties. We also use \na random model for comparing encoding effects, establishing a base-\nline for comparison with other encoding methods. This comparison \nallows us to evaluate the effectiveness of each method in enhancing the \nencoding process. The random model exhibits a less-defined clustering \nstructure, and the 3mer embeddings lack distinguishable features. This \nindicates that RNAErnie captures structural and functional informa -\ntion beyond the primary structure of RNA, enabling grouping based \non similar properties. T o investigate the diversity of non-coding RNAs \n(ncRNAs), we categorize them using sequence ontology at various lev-\nels. Figure 3b illustrates selected classes of ncRNA, such as ribosomal \nRNA (rRNA), long ncRNA (lncRNA) and small ncRNA (sncRNA). Figure 3c \nshows the high-level ontology relationships between ncRNA, tran-\nscript, messenger RNA (mRNA) and intron RNA. Figure 3d represents \nthe low-level ontology of small regulatory ncRNA. RNAErnie effectively \ndiscriminates between classes at different ontology levels, while the \n3mer statistical embeddings struggle to separate them. This suggests \nthat RNAErnie captures structural or functional similarities rather than \nrelying solely on the length of ncRNAs. Note that the random approach \nseems to outperform RNAErnie in differentiating between classes \nacross various ontology levels. This finding suggests that RNAErnie \nmight be less effective in capturing the ontology patterns of low-level, \nsmall regulatory ncRNA classes. We believe that this limitation in iden-\ntifying low-level ontology patterns may stem from several factors, \nincluding the complexity and heterogeneity of classes at this level or \npotential biases in our training dataset. Further research and detailed \nanalysis are needed to identify the specific causes behind RNAErnie‚Äôs \nreduced efficacy in discerning patterns in low-level ontology.\nIn total, these findings demonstrate that RNAErnie constructs \nscatter plots by capturing the structural and functional characteristics \nof ncRNAs, going beyond nucleic acid statistics alone.\nSupervised domain adaptation on downstream tasks\nIn this section, we demonstrate the effectiveness of RNAErnie in three \nessential supervised learning tasks: RNA sequence classification, RNA‚Äì\nRNA interaction and RNA secondary structure prediction.\nT o reveal the effectiveness of the designs in RNAErnie, we con -\nducted a series of ablation studies using variant models derived \nfrom RNAErnie. These models vary in complexity, beginning with \nErnie-base, which lacks RNA-specific pretraining and includes standard \nfine-tuning. RNAErnie‚àí‚àí employs base-level masking during pretrain-\ning, and RNAErnie‚àí adds subsequence-level masking to the mix. The \ncomplete RNAErnie model further integrates motif-level masking \nand is fine-tuned using either TBTH or FBTH architectures. Extending \nthis, RNAErnie\n+ represents the apogee of complexity within this fam-\nily, including all three levels of masking and a STACK architecture for \npretraining. Lastly, the RNAErnie without chunk model is tailored for \nlong RNA sequences by truncating and discarding segments to contend \nwith computational constraints, aimed at the efficient classification of \nlong non-coding and protein-encoding transcripts.\nIn addition, we also bring pretrained models from existing litera-\nture, including RNABERT23, RNA-MSM24 and RNA-FM14 for comparison.\nRNA sequence classification.  We evaluate the performance of our \nproposed sequence-classification models on the benchmark nRC 25. \nThis dataset consists of ncRNA sequences selected from the Rfam \ndatabase release 12 (ref. 26 ). nRC is composed of a balanced collec -\ntion of sequences, with 20% non-redundant samples for each of the 13 \nclasses. It has 6,320 training sequences and 2,600 testing sequences \nlabelled with 13 classes.\nTable 1 presents the sequence-classification results for RNAErnie \non the nRC dataset. The table includes several baseline methods as well \nas different variants of the RNAErnie models. The baseline values are \nall taken from cited literature except the pretrained models: RNABERT, \nRNA-MSM and RNA-FM. Analysing the performance of the models, we \nobserve that the baseline methods achieve varying levels of accuracy. \nNotably, ncRDense demonstrates decent performance, achieving high \naccuracy, recall, precision, F1 score and Matthews correlation coef-\nficient (MCC) values. Turning our attention to the RNAErnie variants, \nwe can see that they consistently outperform most of the baseline \nmodels across all evaluation metrics. Although ncRDense can beat the \nTransformer\n(12 layers, 768 dimensions)\nBase\nlevel\nSubsequence\nlevel\n[IND]\nMotif\nlevel\nMotif databases\na\nRNAErnie transformer\n(pretrained)\nClsHead\nDownstream module\n(FBTH/TBTH)\nEnsemble\n(STACK)\nConcatate\nNetwork module\nw/ gradient\nw/o gradient\nTop K\nq\n&\n&\nqk\nœÉk\nShared parameters\nFlow legend\nb\n[IND]\nFig. 2 | Motif-aware pretraining and type-guided fine-tuning strategies.  \na, Motif-aware multilevel masking strategy for RNAErnie pretraining. Built upon \nERNIE transformer blocks, the design incorporates three levels of masking: base, \nsubsequence and motif. All masking levels are applied during the pretraining \nphase. b, Type-guided fine-tuning for downstream tasks. The RNAErnie approach \nfirst leverages an RNAErnie basic block to predict the top-K most possible coarse-\ngrained RNA types. Then, it stacks an additional layer of K downstream modules \nwith shared parameters for fine-tuning and outputs the ensemble result for tasks.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557\n 551\nArticle https://doi.org/10.1038/s42256-024-00836-4\nfirst two (that is, Ernie-base and RNAErnie‚àí‚àí), RNAErnie‚àí, RNAErnie and \nRNAErnie+ show better performance in all five dimensions.\nIn the hierarchy of the RNAErnie model family, performance met-\nrics improve incrementally with complexity of design. The foundational \nmodel, Ernie-base, establishes a baseline that is modestly surpassed by \nRNAErnie‚àí‚àí through the introduction of base-level masking in pretrain-\ning. Furthermore, RNAErnie‚àí incorporates subsequence-level masking \nand delivers notably enhanced accuracy, recall, precision, F1 score and \nMCC values, endorsing the value of a more comprehensive masking \nstrategy. The full RNAErnie model integrates base, subsequence and \nmotif-level masking, achieving superior performance over its pre -\ndecessors across all metrics and illustrating the cumulative benefits \nof multilevel masking. The apex model, RNAErnie +, which employs \nan exhaustive masking regimen in conjunction with a two-stage \nfine-tuning architecture, outperforms all variants in our experiments.\nRNA‚ÄìRNA interaction. We evaluate the performance of our model on \none of the most representative benchmark datasets, DeepMirTar27,28, \nwhich is used for predicting the interaction between microRNAs (miR-\nNAs) and mRNAs. This dataset consists of 13,860 positive pairs and \n13,860 negative pairs. The miRNA sequences in DeepMirTar are all \nshorter than 26‚Äânts, and the mRNA sequences are shorter than 53‚Äânts. \nBecause most of the target sites are believed to be located at the 3‚Ä≤ \nuntranslated region, DeepMirTar only considers them. Furthermore, \ntwo seeds were taken into consideration: the non-canonical seed, \nwhich pairs at position 2-7 or 3-8, permitting G-U couplings and up \nlncRNA\nRNAEr n ie 3 m e rRandom\nb\na\nc\nd\nsiRNAY  RNA C / D  box snoRNA t RNA Ribozym e m iRNA t m RNA\nRNase  P RNA Pre-miRNA RNase MRP RNA I n tron RNA\nAnt i sense\nS R P  RNA piRNA\nsnRNA r RNA mRNA Hammerhead ribozym e H /AC A  box snoRNA\nr RNA lncRNA RNase P RNA Ribozym esncRNA S R P  RNA Y  RNA\nAnt i sense\nr RNA T ranscript mRNA I n tron RNA\npiRNA m iRNA t m RNA siRNA\nFig. 3 | RNAErnie captures multilevel ontology patterns. Left, RNAErnie \nembeddings. Middle, randomly initialized embeddings. Right, 3mer statistical \nembeddings. a, Scatter plot showcasing all ncRNA types within a subset of \nRNAcentral, utilizing different embedding methods. b, Embedding projections \nof selected ncRNA classes, including rRNA, lncRNA, RNase P RNA, ribozyme, \nsncRNA, signal recognition particle RNA (SRP RNA), antisense RNA and Y \nRNA. c, Distribution of embeddings based on high-level sequence ontology, \nencompassing ncRNA, transcript, mRNA and intron RNA. d, Detailed distribution \nof embeddings for low-level small regulatory ncRNA classes, such as Piwi-\ninteracting RNA (piRNA), miRNA, transfer-messenger RNA (tmRNA) and small \ninterfering RNA (siRNA).\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557 552\nArticle https://doi.org/10.1038/s42256-024-00836-4\nto one bulged or mismatched nt; and the canonical seed, which is the \nprecise W-C pairing of 2-7 or 3-8‚Äânts of the miRNA. Given that RNA types \n(miRNA, mRNA) are fixed here, we do not test RNAErnie+ version which \nuses a two-stage pipeline here.\nTable 2 presents the performance comparison between the pro-\nposed RNAErnie models and baseline methods from existing literature, \nsuch as Miranda29, RNAhybrid30, PITA31, TargetScan v.7.0 (ref. 32), TarP-\nmiR33 and DeepMirTar27. The baseline values are all taken from cited \nliterature except the pretrained models: RNABERT, RNA-MSM and \nRNA-FM. These are evaluated on the RNA‚ÄìRNA interaction prediction \ntask using the DeepMirTar dataset. DeepMirTar emerges as a strong \nbaseline, exhibiting high scores across all metrics. The Ernie-base \nmodel and the RNAErnie variations are then assessed, with the RNAEr-\nnie model demonstrating superior performance and particularly \nexcelling in accuracy, precision, F1 score and area under the curve \n(AUC). This variation achieves an impressive accuracy score of 0.9872, \na competitive precision score of 0.9901, an F1 score of 0.9873 and the \nhighest AUC score of 0.9976, indicating excellent overall performance \nand discriminative power.\nOverall, the results suggest that the RNAErnie model, particularly \nthe RNAErnie variation, outperforms the existing methods and the \nErnie-base model in the RNA‚ÄìRNA interaction prediction task. These \nfindings highlight the potential of the RNAErnie model in accurately \npredicting RNA‚ÄìRNA interactions.\nRNA secondary structure prediction. This section presents a com-\nprehensive comparison between our pretrained RNAErnie model and \nseveral baseline models, including the state-of-the-art UFold model34, \nin the context of RNA secondary structure prediction tasks. The experi-\nments are conducted using commonly used benchmarks employed in \nstate-of-the-art models. These benchmarks include:\n‚Ä¢\t RNAStralign35: This dataset comprises 37,149 RNA structures from \neight RNA families, with lengths ranging from approximately 100 \nto 3,000 base pairs (bp).\n‚Ä¢\t ArchiveII36: This dataset consists of 3,975 RNA structures from \nten RNA families, with lengths ranging from approximately 100 \nto 2,000‚Äâbp.\n‚Ä¢\t bpRNA-1m37: This dataset contains 13,419 RNA structures from \n2,588 RNA families, with sequence similarity removed using an \n80% sequence-identity cut-off. The lengths of the sequences  \nrange from approximately 100 to 500‚Äâbp. The dataset is randomly \nsplit into three subsets: TR0 (10,814 structures) for training, TV0 \n(1,300 structures) for validation and TS0 (1,305 structures) for \ntesting.\nWe train our model on the entire RNAStralign dataset, as well as \nthe TR0 subset and other augmented mutated datasets, following the \napproach used in UFold. Subsequently, we evaluate performance on \nthe ArchiveII600 dataset, which is a subset of ArchiveII with lengths \nless than 600‚Äâbp, and the TS0 dataset.\nTable 3 presents a comparative analysis of the performance of \nvarious methods on the RNA secondary structure prediction task \nusing the ArchiveII and TS0 datasets. The table presents the results \nof several baseline methods, including RNAstructure, RNAsoft, RNA -\nfold, MXfold2, Mfold, LinearFold, Eternafold, E2Efold, Contrafold \nand Contextfold. Each method is assessed based on its precision,  \nrecall and F1 score for both the ArchiveII600 and TS0 datasets. \nThe baseline values are all taken from cited literature except the \npretrained models: RNABERT, RNA-MSM and RNA-FM. Among the \nRNAErnie variations, RNAErnie + achieves the highest scores in pre -\ncision, recall and F1 score, indicating its superior performance in \nRNA secondary structure prediction. Notably, RNAErnie + achieves a \nremarkable precision score of 0.886, a high recall score of 0.870 and \nan impressive F1 score of 0.875 on the ArchiveII600 dataset. These \nresults highlight the effectiveness of RNAErnie+ in accurately predict-\ning RNA secondary structures.\nDiscussion\nOur method, RNAErnie, outperforms existing advanced techniques \nacross seven RNA sequence datasets encompassing over 17,000 major \nRNA motifs, 20 RNA classes/types and 50,000 RNA sequences. Evalu-\nation using 30 mainstream RNA sequence technologies confirms the \ngeneralization and robustness of RNAErnie. We employed accuracy, \nprecision, recall, F1 score, MCC and AUC as evaluation metrics to ensure \na fair comparison of RNA sequence-analysis methods. Currently, little \nresearch exists on applying transformer architectures with enhanced \nexternal knowledge to RNA sequence data analysis. Our from-scratch \nRNAErnie framework integrates RNA sequence embedding and a \nself-supervised learning strategy, resulting in superior performance, \nTable 1 | Performance of RNAErnie on sequence \nclassification for the nRC dataset\nMethod Accuracy Recall Precision F1 MCC\nRNAcon53 0.3737 0.3732 0.4497 0.3505 0.3341\nnRC25 0.6960 0.6889 0.6878 0.6878 0.6627\nncRFP54 0.7972 0.7878 0.7904 0.7883 0.7714\nRNAGCN55 0.8573 0.8609 0.9882 0.8561 0.8459\nncRDeep56 0.8804 0.8842 0.8913 0.8858 0.8801\nncRDense57 0.9510 0.9510 0.9529 0.9512 0.9470\nRNABERT23 0.7142 0.7142 0.7155 0.7155 0.6911\nRNA-MSM24 0.9027 0.9027 0.9045 0.9023 0.8948\nRNA-FM14 0.9656 0.9656 0.9672 0.9664 0.9635\nErnie-base 0.9262 0.9257 0.9371 0.9314 0.9214\nRNAErnie‚àí‚àí 0.9354 0.9354 0.9367 0.9352 0.9301\nRNAErnie‚àí 0.9604 0.9604 0.9611 0.9605 0.9571\nRNAErnie 0.9638 0.9638 0.9641 0.9639 0.9608\nRNAErnie+ 0.9688 0.9688 0.9691 0.9687 0.9662\nBold formatting indicates the best results on the metrics.\nTable 2 | Performance of RNAErnie on RNA‚ÄìRNA interaction \nprediction task using the DeepMirTar dataset\nMethod Accuracy Recall Precision F1 AUC\nMiranda29 0.6592 0.6522 0.6662 0.6591 0.6874\nRNAhybrid30 0.6988 0.6446 0.7535 0.6948 0.7585\nPITA31 0.4981 0.5872 0.4082 0.4816 ‚Äì\nTargetScan v.7.032 0.5801 0.6023 0.5922 0.5972 0.6725\nTarPmiR33 0.7446 0.7368 0.7656 0.7509 0.8021\nDeepMirTar27 0.9348 0.9235 0.9479 0.9245 0.9793\nRNABERT23 0.8375 0.8372 0.8378 0.8373 0.9160\nRNA-MSM24 0.8205 0.8203 0.8207 0.8204 0.9048\nRNA-FM14 0.9208 0.9208 0.9208 0.9208 0.9741\nErnie-base 0.9262 0.9257 0.9371 0.9314 0.9674\nRNAErnie‚àí‚àí 0.9537 0.9547 0.9533 0.9540 0.9801\nRNAErnie‚àí 0.9524 0.9539 0.9514 0.9526 0.9811\nRNAErnie 0.9570 0.9576 0.9571 0.9573 0.9876\nBold formatting indicates the best results on the metrics. The AUC value for the PITA model \nhas been omitted, as it was not available in ref. 31.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557\n 553\nArticle https://doi.org/10.1038/s42256-024-00836-4\ninterpretability and generalization potential for downstream RNA tasks. \nAdditionally, RNAErnie is adaptable to other tasks through modifica-\ntion of the output and supervision signals. RNAErnie is publicly avail-\nable and serves as an effective tool for understanding type-guided RNA \nanalysis and advanced applications.\nThe RNAErnie model, despite its innovations in RNA sequence \nanalysis, confronts several challenges. First, the model is constrained \nby the size of the RNA sequences it can analyse, as sequences longer \nthan 512‚Äânts are dropped, potentially omitting vital structural and \nfunctional information. The chunking method developed to han -\ndle longer sequences might result in the further loss of information \nabout long-range interactions. Second, the focus of this study is nar-\nrow, centred only on the RNA domain and not extending to tasks like \nRNA-protein prediction or binding-site identification. Additionally, the \nmodel encounters difficulties in considering three-dimensional struc-\ntural motifs of RNAs, such as loops and junctions, which are essential \nfor understanding RNA functions.\nMore importantly, the existing post hoc architectural design has \npotential limitations, including heightened inference overhead. An \nalternative approach involves designing a specialized loss function \nthat incorporates RNA type information and pretraining the model \nin an end-to-end fashion. We have experimented with this concept \nand engaged in preliminary pretraining. Our findings indicate that \nalthough this method proves beneficial for discriminative tasks such \nas sequence classification, it unfortunately leads to suboptimal token \nrepresentations with performance degradation in reconstruction of \nstructures. Detailed information is provided Supplementary Informa-\ntion Section C.6. Our future work will go deeper into this issue and \nexplore solutions.\nMethods\nThis section provides a comprehensive overview of the design features \nassociated with each component of RNAErnie. We will explore the spe-\ncific characteristics of each element and discuss their collaborative func-\ntionality in enabling the accomplishment of diverse downstream tasks.\nOverall design\nIn this work, we present RNAErnie, an approach for large-scale pre -\ntraining of RNA sequences based on the ERNIE framework 38, which \nincorporates multilayer and multihead transformer blocks39.\nRNAErnie transformer. The basic block of the RNAErnie transformer \nshares the same architectural configuration as ERNIE 38, employing a \n12-layer transformer and a hidden state dimension of Dh‚Äâ=‚Äâ768. Consider \nan input RNA sequence denoted as x ‚Äâ=‚Äâ(x1,‚Äâx2,‚Äâ‚ãØ‚Äâ,‚ÄâxL), where each ele-\nment xi‚Äâ‚àà‚Äâ{‚Äò A‚Äô , ‚ÄòU‚Äô , ‚ÄòC‚Äô , ‚ÄòG‚Äô} and L represents the length of the sequence. \nAn RNAErnie block first tokenizes RNA bases in the sequence and \nsubsequently feeds them into the transformer. This process enables \nus to extract token embeddings h =( h1,h2,‚ãØ, hL)‚àà‚Ñù L√óDh , where D h \nrepresents the dimension of the hidden representations for the tokens. \nGiven the embeddings for every token in the RNA sequence, the RNAEr-\nnie basic block transforms the series of token embeddings into a \nlower-dimensional vector (that is, 768 dimensions) using trainable \nparameters38 and then outputs the embedding of the RNA sequence. \nThe total number of trainable parameters in RNAErnie is approximately \n105‚Äâmillion.\nPretraining datasets.  Basically, like many other pretraining based \napproaches, the RNAErnie approach is structured into two main phases: \npretraining and fine-tuning. In the pretraining phase, which is agnostic \nto any specific task, RNAErnie is meticulously trained on a vast corpus \nof 23‚Äâmillion ncRNA sequences obtained from the RNAcentral data -\nbase16. This self-supervised autoregressive training phase allows RNAEr-\nnie to capture sequential distributions and patterns within the RNA \nsequences, thereby acquiring a comprehensive understanding of their \nstructural and functional information. In the subsequent task-specific \nfine-tuning phase, the pretrained RNAErnie model is either fine-tuned \nwith downstream modules or used to generate sequence embeddings \n(features) that complement a lightweight prediction layer. Regarding \nthe tokenization of RNA bases, the sequences are tokenized to represent \n‚Äò A‚Äô , ‚ÄòT/U‚Äô , ‚ÄòC‚Äô and ‚ÄòG‚Äô , with the initial token of each sequence reserved for \nTable 3 | Performance of RNAErnie on RNA secondary structure prediction task using the ArchiveII600 and TS0 datasets\nMethods\nArchiveII600 TS0\nPrecision Recall F1 Precision Recall F1\nRNAstructure58 0.563 0.615 0.585 0.494 0.622 0.533\nRNAsoft59 0.665 0.594 0.622 0.497 0.626 0.535\nRNAfold60 0.565 0.627 0.592 0.494 0.631 0.536\nMXfold247 0.788 0.760 0.768 0.519 0.646 0.558\nMfold61 0.428 0.383 0.401 0.501 0.627 0.538\nLinearFold62 0.641 0.617 0.621 0.561 0.581 0.550\nEternafold63 0.667 0.622 0.636 0.516 0.666 0.563\nE2Efold64 0.738 0.665 0.690 0.140 0.129 0.130\nContrafold65 0.607 0.679 0.638 0.528 0.655 0.567\nContextfold66 0.873 0.821 0.842 0.529 0.607 0.546\nRNABERT23 0.634 0.649 0.641 0.435 0.527 0.477\nRNA-MSM24 0.664 0.648 0.656 0.448 0.540 0.490\nRNA-FM14 0.752 0.737 0.744 0.518 0.620 0.564\nErnie-base 0.875 0.839 0.851 0.582 0.666 0.607\nRNAErnie‚àí‚àí 0.855 0.844 0.846 0.569 0.666 0.602\nRNAErnie‚àí 0.848 0.854 0.848 0.579 0.667 0.607\nRNAErnie 0.884 0.869 0.873 0.576 0.668 0.608\nRNAErnie+ 0.886 0.870 0.875 0.575 0.678 0.622\nBold formatting indicates the best results on the metrics.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557 554\nArticle https://doi.org/10.1038/s42256-024-00836-4\nthe special classification embedding ([CLS]). Additionally, an indication \nembedding ([IND]) is appended to each RNA sequence, followed by indi-\ncation classes (for example, ‚ÄòmiRNA‚Äô , ‚ÄòmRNA‚Äô , ‚ÄòlnRNA‚Äô) derived from the \nRNAcentral database, as depicted in Extended Data Fig. 1. The inclusion \nof the indication embedding encourages the model to cluster similar \nRNA sequences in a latent space, facilitating retrieval-based learning40.\nMotif-aware pretraining strategies\nT o integrate both subsequence and motif-level knowledge into the \nrepresentation of RNA sequences, we introduce a motif-ware multilevel \nmasking strategy to pretrain the RNAErnie basic block, as opposed to \ndirectly incorporating motif embedding. In addition, the RNAErnie \napproach follows the standard routine of pretraining with all three \nlevels of masking tasks, learning to predict the masked tokens and also \ncapture contextualized representations of the input RNA sequence. \nSpecifically, the procedure of RNAErnie pretraining with motif-aware \nmultilevel masking strategies is as follows.\nBase-level masking.  In the initial stage of the learning process, we \nemploy base-level masking as a crucial component. Specifically, we \nrandomly mask 15% of the nucleobases within an RNA sequence. Among \nthe masked positions, 10% are preserved without any alterations, and \nthe remaining 10% are replaced with other nucleobases. The model \ntakes the remaining nucleobases as input and is tasked with predict -\ning the masked positions. This stage primarily focuses on acquiring \nfundamental token representations; capturing intricate higher-level \nbiological insights proves to be a challenging endeavour.\nSubsequence-level masking. Next, we incorporate the masking of \nrandom subsequences, which are short and contiguous segments of \nnucleobases within an RNA sequence. Previous studies, such as refs. 41 \nand 42, have demonstrated the efficacy of contiguous token masking \nin enhancing pretrained models for span-selection tasks. Additionally, \nit is important to consider that the functionality of nucleobases often \nmanifests within the context of sequential arrangements. By predicting \nthese subsequences as a whole, we encourage the model to capture a \ndeeper understanding of the biological information inherent in the \nrelationships between consecutive nucleobases. In our research, we \nspecifically mask subsequences with lengths ranging from 4 to 8‚Äâbp.\nMotif-level masking.  In the final stage of pretraining, we employ \nmotif-level masking as part of our approach. RNA motifs, character -\nized as recurrent structural elements with a high concentration of \ninformation, have been extensively observed in atomic-resolution \nRNA structures17. These motifs are widely recognized for their crucial \ninvolvement in various biological activities, such as the formation of \nRNA tertiary structures 19, interaction with dsRNA-binding proteins \n(RBPs) and participation in complex formation with proteins 18.  To \nincorporate these motifs into our model as so-called biological priors, \nwe gather them from multiple sources:\n‚Ä¢\t ATtRACT43: This resource provides comprehensive information \non 370 RBPs and 1,583 RBP consensus binding motifs. The data \nis extracted and carefully curated from experimentally validated \nsources such as CISBP-RNA, SpliceAid-F and RBPDB databases.\n‚Ä¢\t SpliceAid\n44: We gather information from SpliceAid, which \nencompasses 2,220 target sites associated with 62 human splic-\ning proteins. Additionally, it includes expression data from 320 \ntissues per cell.\n‚Ä¢\t We also extract the most frequently occurring contiguous \nnucleobase sequences, ranging from 4 to 8‚Äâbp, by scanning the \nentirety of the RNAcentral database.\nBy incorporating motifs from these diverse sources, we aim to capture \na comprehensive representation of RNA structural elements for our \nanalysis.\nType-guided fine-tuning strategy\nGiven the RNAErnie basic block pretrained with motif-aware mul -\ntilevel masking strategies, we need to combine the basic blocks of \nthe RNAErnie transformer with task-specific heads‚Äîfor example, a \nfully connected layer for RNA classification‚Äîinto a neural network \nfor the downstream task and further train the neural network sub -\nject to labelled datasets for the downstream application in a super -\nvised learning manner. Here, we introduce our proposed type-guided \nfine-tuning strategy in two parts: neural architectures for tasks and \ndomain-adaptation strategies.\nNeural architectures for fine-tuning. T o adapt various downstream \ntasks, the RNAErnie approach follows the surgical fine-tuning strate-\ngies20 and offers three sets of neural network architectures as follows.\nFBTH. In the FBTH architecture, given RNA sequences and their \nlabels for a downstream task, the RNAErnie approach simply extracts \nembeddings of RNA sequences from a pretrained RNAErnie basic \nblock and then leverages the embeddings as inputs to train a separate \ntask-specific head subject to the downstream tasks. In this way, the \nparameters in the RNAErnie backbone are frozen, while the head is \ntrainable. According to ref. 20, this architecture would work well when \nthe downstream tasks are out-of-distribution of pretraining datasets.\nTBTH. In the TBTH architecture, the RNAErnie approach directly com-\nbines the RNAErnie basic block and the task-specific head to construct \nan end-to-end neural network for downstream tasks and then trains \nthe neural network using the labelled datasets in a supervised learning \nmanner. In this way, the parameters in both the RNAErnie backbone and \nthe head are trainable. According to ref. 20, this architecture would \nwork well when the downstream tasks and pretraining datasets are in \nthe same distribution.\nSTACK. In the STACK architecture, the RNAErnie approach first lev -\nerages an RNAErnie basic block to predict the top-K  most possible \ncoarse-grained RNA types (that is, the K coarse-grained RNA types with \nthe highest probabilities) using the input RNA sequence. Then it stacks \nan additional layer of K downstream modules with shared parameters \nfor fine-tuning, where every downstream module refers to a TBTH/\nFBTH network and is fed with the RNA sequence and a predicted RNA \ntype for the downstream task. The K  downstream modules output K \nprediction results, and the RNAErnie approach outputs the ensemble \nof K results as the final outcome.\nMore specifically, in the STACK architecture, the RNAErnie basic \nblock first predicts the indication of an RNA sequence following the [IND] \nmarker by estimating the probability of the masked indication token, \ndenoted as p(xIND‚à£x;‚ÄâŒ∏). From these predictions, the RNAErnie approach \nselects the top-K indications, denoted as Ik ‚àà‚Ñê  for k‚Äâ=‚Äâ1,‚Äâ‚ãØ‚Äâ,‚ÄâK, along with \ntheir corresponding probabilities œÉ1,‚Äâ‚Ä¶,‚ÄâœÉK. Each selected indication is \nthen appended to the end of the RNA sequence, resulting in K parallel \ninputs to the downstream module. Then the downstream module takes \nthe K parallel inputs simultaneously, enabling ensemble learning through \nsoft majority voting. Specifically, the RNAErnie approach calculates the \nweighted sum for soft majority voting as follows:\nÃÑq=\nK\n‚àë\nk=1\nœÉkqk, (1)\nwhere qk could be either scalar, vector or matrix outputs from the \ndownstream module for various downstream tasks (for example, logit \nvectors for classification tasks or pair-wise feature maps for structural \nanalysis), while ÃÑq refers to the weight sum.\nNote that although we consider the stacking architecture part \nof our key contributions, FBTH and TBTH sometimes deliver better \nperformance.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557\n 555\nArticle https://doi.org/10.1038/s42256-024-00836-4\nDomain adaptation to downstream tasks. Upon completion of the \npretraining phase, the RNAErnie basic block is prepared for type-guided \nfine-tuning, enabling its application to various downstream tasks. It is \nimportant to emphasize that RNAErnie has the potential to accommo-\ndate a diverse array of tasks, extending beyond the examples provided \nbelow, through appropriate FBTH, TBTH and STACK architectures.\nRNA sequence classification. RNA sequence classification is a pivotal \ntask that assigns RNA sequences to specific categories. In other words, \nit maps an RNA sequence x of length L to scalar labels, which refer to \ndifferent categories. RNA sequence classification is crucial for under-\nstanding their functions and their roles in various biological processes. \nAccurate classification of RNA sequences enables researchers to iden-\ntify ontology and predict functions, which facilitates the development \nof new therapies and treatments for RNA-related diseases.\nOur work leverages STACK with TBTH to classify RNA sequences. \nIt stacks K classification modules: the RNAErnie basic block combined \nwith a trainable MLP as a prediction head. However, the computational \ncomplexity of transformers, which exhibit a quadratic time complexity \nof ùí™ùí™(n2d), where n denotes the sequence length, posed challenges when \nprocessing excessively long RNA sequences. T o discern lncRNA amidst \nprotein-coding transcripts, we employed a chunk strategy. This strat-\negy entails the division of lengthy RNA sequences into more manage-\nable segments, which are independently fed into the RNAErnie \napproach. Subsequently, we aggregate the segment-level logits to \nobtain the sequence-level logit and employ an MLP for classification \npurposes.\nRNA‚ÄìRNA interaction prediction. RNA‚ÄìRNA interaction prediction \nrefers to the estimation of interactions between two RNA sequences, \nsuch as miRNA and mRNA, circular RNA and lncRNA. This task maps \ntwo RNA sequences, xa of length L1 and xb of length L2, to binary labels \n0/1, where 0 indicates no interaction between the two RNA sequences \nand 1 indicates interaction. Accurate prediction of RNA‚ÄìRNA interac-\ntions can provide valuable insights into RNA-mediated regulatory \nmechanisms and enhance our understanding of biological processes, \nincluding gene expression, splicing and translation45.\nOur work employs a TBTH architecture, which combines the \nRNAErnie basic block with a hybrid neural network inspired by ref. 46. \nThis hybrid neural network acts as the interaction prediction head, \nsequentially incorporating several components: a convolutional neural \nnetwork, a bidirectional long short-term memory network and a MLP. \nBecause the types of interacting RNA are fixed, it is unnecessary to \nemploy the STACK architecture for the purpose of RNA‚ÄìRNA interac-\ntion analysis.\nRNA secondary structure prediction. RNA secondary structure pre -\ndiction determines the probable arrangement of bp within an RNA \nsequence, which can fold back onto itself and form specific pairings. \nIt maps an RNA sequence x of length L to a 0/1 matrix with shape L‚Äâ√ó‚ÄâL, \nwhere element i,‚Äâj means whether nt i forms bp with nt j. The secondary \nstructure of RNA plays a critical role in understanding its interactions \nwith other molecules and its functional importance. This prediction \ntechnique is a valuable tool in molecular biology, aiding in the identi-\nfication of potential targets for drug design and enhancing our under-\nstanding of gene expression and regulation mechanisms.\nOur work utilizes the STACK architecture with FBTH to fold RNA \nsequences. We combined the RNAErnie basic block with a folding neural \nnetwork inspired by the methodology described in ref. 47. It computes \nfour distinct folding scores‚Äîhelix stacking, unpaired region, helix \nopening and helix closing‚Äîfor each pair of nt bases. Subsequently, we \nutilize a Zuker-style dynamic programming approach48 to predict the \nmost favourable secondary structure. This is achieved by maximizing \nthe cumulative scores of adjacent loops, following a systematic and \nrigorous computational procedure. T o facilitate the training of our \ndeep neural network, we adopt the max-margin framework. Within this \nframework, the network minimizes the structured hinge loss function \nwhile incorporating thermodynamic regularization.\nHyperparameters and configurations\nDuring the pretraining phase, our model underwent approximately \n2,580,000 steps of training, with a batch size set to 50 and a maxi -\nmum sequence length for ERNIE limited to 512. We utilized the AdamW \noptimizer, which was regulated by a learning-rate schedule involving \nanneal warm-up and decay. The initial learning rate was set at 1‚Äâ√ó‚Äâ10\n‚àí4, \nwith a minimum learning rate of 5‚Äâ√ó‚Äâ10‚àí5. The learning-rate scheduler \nwas designed to warm up during the first 5% of the steps and then \ndecay in the final 5% of the steps. In terms of masking strategies, we \nmaintained a proportion of 1:1:1 across the three different masking \nlevels, with the training algorithm randomly selecting one strategy for \neach training session. The pretraining was conducted on four Nvidia \nT esla V100 32‚ÄâGB graphics processing units, taking around 250‚Äâhours \nto reach convergence.\nHere, in additional to the hyperparameters for pretraining, we \nintroduce the configurations of variant pretrained models derived \nfrom RNAErnie and used in experiments:\n‚Ä¢\t Ernie-base: this model represents the vanilla ERNIE architecture \nwithout any pretraining on RNA sequence datasets. It under-\nwent standard fine-tuning.\n‚Ä¢\t RNAErnie\n‚àí‚àí: in this model, only base-level masking was \nemployed during the pretraining phase of the RNAErnie family. \nIt was then fine-tuned using the standard approach.\n‚Ä¢\t RNAErnie\n‚àí: the RNAErnie family model with both base and \nsubsequence-level masking during pretraining, followed by \nstandard fine-tuning.\n‚Ä¢\t RNAErnie: this model encompasses the complete set of masking \nstrategies, including base, subsequence and motif-level masking \nduring pretraining. It was fine-tuned using the TBTH or FBTH \narchitecture.\n‚Ä¢\t RNAErnie\n+: the most comprehensive model in the RNAErnie fam-\nily, incorporating all three levels of masking during pretraining \nand the STACK architecture.\n‚Ä¢\t RNAErnie without chunk: this model truncates RNA sequences \nand discards any remaining segments when classifying long \nRNA sequences, specifically lncRNA (for example, in lnRC_H and \nlnRC_M datasets) alongside protein-encoding transcripts.\nData availability\nThe datasets used for pretraining and fine-tuning are all derived from \nprevious studies. Here we include the official links. Note that the \nlncRNA_H and lncRNA_M datasets are used for long-sequence classifica-\ntion in the Supplementary Information. RNAcentral16: https://ftp.ebi.\nac.uk/pub/databases/RNAcentral/releases/21.0/; ATtRACT43: https://\nattract.cnic.es/download; SpliceAid44: http://193.206.120.249/cgi-bin/\nSpliceAid.pl?sites=Download; nRC25: http://tblab.pa.icar.cnr.it/public/\nnRC/paper_dataset/; lncRNA_H 49: https://www.gencodegenes.org/\nhuman/release_25.html; lncRNA_M49: https://www.gencodegenes.org/\nmouse/; DeepMirTar27: https://github.com/tjgu/miTAR/tree/master/\nscripts_data_models; ArchiveII36: https://rna.urmc.rochester.edu/pub-\nlications.html; RNAStrAlign35: https://github.com/mxfold/mxfold2/\nreleases/tag/v0.1.0; bpRNA 37: https://bprna.cgrb.oregonstate.edu/\ndownload.php#bpRNA. Source data are provided with this paper.\nCode availability\nWe built RNAErnie using Python and the PaddlePaddle deep learning \nframework. The code repository of RNAErnie, readme files and tutorials \nare all available at ref. 50. A docker image with configured environments \nand dependent libraries is available for download at ref. 51. T o compare \npretrained RNA language baselines, see the code repository at ref. 52.\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557 556\nArticle https://doi.org/10.1038/s42256-024-00836-4\nReferences\n1. Kukurba, K. & Montgomery, S. RNA sequencing and analysis. Cold \nSpring Harb. Protoc. 2015, pdb‚Äìtop084970 (2015).\n2. Conesa, A. et al. A survey of best practices for RNA-seq data \nanalysis. Genome Biol. 17, 1‚Äì19 (2016).\n3. Dharmadhikari, S., Ingle, M. & Kulkarni, P. Empirical studies on \nmachine learning based text classification algorithms. Adv. \nComput. 2, 161 (2011).\n4. Zheng, S., Li, Y., Chen, S., Xu, J. & Yang, Y. Predicting drug-protein \ninteraction using quasi-visual question answering system. Nat. \nMach. Intell. 2, 134‚Äì140 (2020).\n5. Min, B. et al. Recent advances in natural language processing via \nlarge pre-trained language models: a survey. ACM Comput. Surv. \n56, 1‚Äì40 (2021).\n6. Kenton, J. & Toutanova, L. BERT: pre-training of deep bidirectional \ntransformers for language understanding. In Proc. 2019 \nConference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies \n(eds Burstein, J. et al.) 4171‚Äì4186 (Association for Computational \nLinguistics, 2019).\n7. Yu, X., Jiang, L., Jin, S., Zeng, X. & Liu, X. preMLI: a pre-trained \nmethod to uncover microRNA-lncRNA potential interactions. \nBrief. Bioinform. 23, bbab470 (2022).\n8. Ji, Y., Zhou, Z., Liu, H. & Davuluri, R. DNABERT: pre-trained \nbidirectional encoder representations from transformers model \nfor DNA-language in genome. Bioinformatics 37, 2112‚Äì2120 (2021).\n9. Leksono, M. & Purwarianti, A. Sequential labelling and DNABERT \nFor splice site prediction in Homo Sapiens DNA. Preprint at \nhttps://arXiv.org/quant-ph/2212.07638 (2022).\n10. Zhou, Z. et al. DNABERT-2: efficient foundation model and \nbenchmark for multi-species genome. In Twelfth International \nConference on Learning Representations (2024).\n11. Altenburg, T., Giese, S., Wang, S., Muth, T. & Renard, B. Ad hoc \nlearning of peptide fragmentation from mass spectra enables \nan interpretable detection of phosphorylated and cross-linked \npeptides. Nat. Mach. Intell. 4, 378‚Äì388 (2022).\n12. Raffel, C. et al. Exploring the limits of transfer learning with a \nunified text-to-text transformer. J. Mach. Learn. Res. 21, 5485‚Äì5551 \n(2020).\n13. Liang, S. et al. Rm-LR: a long-range-based deep learning model \nfor predicting multiple types of RNA modifications. Comput. Biol. \nMed. 164, 107238 (2023).\n14. Chen, J. et al. Interpretable RNA foundation model from \nunannotated data for highly accurate RNA structure \nand function predictions. Preprint at bioRxiv https://doi.\norg/10.1101/2022.08.06.503062 (2022).\n15. Holbrook, S. RNA structure: the long and the short of it. Curr. \nOpin. Struct. Biol. 15, 302‚Äì308 (2005).\n16. Sweeney, B. et al. RNAcentral 2021: secondary structure \nintegration, improved sequence search and new member \ndatabases. Nucleic Acids Res. 49, D212‚ÄìD220 (2021).\n17. Leontis, N., Lescoute, A. & Westhof, E. The building blocks and motifs \nof RNA architecture. Curr. Opin. Struct. Biol. 16, 279‚Äì287 (2006).\n18. Fierro-Monti, I. & Mathews, M. Proteins binding to duplexed RNA: \none motif, multiple functions. Trends Biochem. Sci. 25, 241‚Äì246 \n(2000).\n19. Butcher, S. & Pyle, A. The molecular interactions that stabilize \nRNA tertiary structure: RNA motifs, patterns, and networks. Acc. \nChem. Res. 44, 1302‚Äì1311 (2011).\n20. Lee, Y. et al. Surgical fine-tuning improves adaptation to \ndistribution shifts. In Eleventh International Conference on \nLearning Representations (2023).\n21. Moon, K. et al. Visualizing structure and transitions in \nhigh-dimensional biological data. Nat. Biotechnol. 37, 1482‚Äì1492 \n(2019).\n22. Kirk, J. et al. Functional classification of long non-coding RNAs by \nk-mer content. Nat. Genet. 50, 1474‚Äì1482 (2018).\n23. Akiyama, M. & Sakakibara, Y. Informative RNA base embedding for \nRNA structural alignment and clustering by deep representation \nlearning. NAR Genom. Bioinform. 4, lqac012 (2022).\n24. Zhang, Y. et al. Multiple sequence alignment-based RNA language \nmodel and its application to structural inference. Nucleic Acids \nRes. 52, e3‚Äìe3 (2024).\n25. Fiannaca, A., La Rosa, M., La Paglia, L., Rizzo, R. & Urso, A. nRC: \nnon-coding RNA classifier based on structural features. BioData \nMin. 10, 1‚Äì18 (2017).\n26. Nawrocki, E. et al. Rfam 12.0: updates to the RNA families \ndatabase. Nucleic Acids Res. 43, D130‚ÄìD137 (2015).\n27. Wen, M., Cong, P., Zhang, Z., Lu, H. & Li, T. DeepMirTar: a \ndeep-learning approach for predicting human miRNA targets. \nBioinformatics 34, 3781‚Äì3787 (2018).\n28. Pla, A., Zhong, X. & Rayner, S. miRAW: a deep learning-based \napproach to predict microRNA targets by analyzing whole \nmicroRNA transcripts. PLoS Comput. Biol. 14, e1006185 (2018).\n29. Enright, A. et al. MicroRNA targets in Drosophila. Genome Biol. 4, \n1‚Äì27 (2003).\n30. Kr√ºger, J. & Rehmsmeier, M. RNAhybrid: microRNA target \nprediction easy, fast and flexible. Nucleic Acids Res. 34,  \nW451‚ÄìW454 (2006).\n31. Pita, T., Feliciano, J. & Leit√£o, J. Identification of Burkholderia \ncenocepacia non-coding RNAs expressed during Caenorhabditis \nelegans infection. Appl. Microbiol. Biotechnol. 107, 3653‚Äì3671 \n(2023).\n32. Agarwal, V., Bell, G., Nam, J. & Bartel, D. Predicting effective \nmicroRNA target sites in mammalian mRNAs. eLife 4, e05005 \n(2015).\n33. Ding, J., Li, X. & Hu, H. TarPmiR: a new approach for microRNA \ntarget site prediction. Bioinformatics 32, 2768‚Äì2775 (2016).\n34. Fu, L. et al. UFold: fast and accurate RNA secondary structure \nprediction with deep learning. Nucleic Acids Res. 50, e14‚Äìe14 \n(2022).\n35. Tan, Z., Fu, Y., Sharma, G. & Mathews, D. TurboFold II: RNA \nstructural alignment and secondary structure prediction \ninformed by multiple homologs. Nucleic Acids Res. 45, 11570‚Äì\n11581 (2017).\n36. Sloma, M. & Mathews, D. Exact calculation of loop formation \nprobability identifies folding motifs in RNA secondary structures. \nRNA 22, 1808‚Äì1818 (2016).\n37. Danaee, P. et al. bpRNA: large-scale automated annotation and \nanalysis of RNA secondary structure. Nucleic Acids Res. 46, \n5381‚Äì5394 (2018).\n38. Sun, Y. et al. Ernie 2.0: a continual pre-training framework for \nlanguage understanding. In Proc. AAAI Conference on Artificial \nIntelligence 34 (eds Wooldridge, M., Dy, J. & Natarajan, S.) \n8968‚Äì8975 (AAAI, 2020).\n39. Vaswani, A. et al. Attention is all you need. In Proc. Advances in \nInformation Processing Systems 30 (eds Guyon, I. et al.) 5999‚Äì\n6009 (NeurIPS, 2017).\n40. Karpicke, J. D., Lehman, M. & Aue, W. R. Retrieval-based learning: \nan episodic context account. In Psychology of Learning and \nMotivation Vol. 61, 237‚Äì284 (Academic Press, 2014).\n41. Joshi, M. et al. SpanBERT: improving pre-training by representing \nand predicting spans. Trans. Assoc. Comput. Linguist. 8, 64‚Äì77 \n(2020).\n42. Wu, R. et al. High-resolution de novo structure prediction \nfrom primary sequence. Preprint at bioRxiv https://doi.\norg/10.1101/2022.07.21.500999 (2022).\n43. Giudice, G., S√°nchez-Cabo, F., Torroja, C. & Lara-Pezzi, E. \nATtRACT‚Äîa database of RNA-binding proteins and associated \nmotifs. Database 2016, baw035 (2016).\nNature Machine Intelligence | Volume 6 | May 2024 | 548‚Äì557\n 557\nArticle https://doi.org/10.1038/s42256-024-00836-4\n44. Piva, F., Giulietti, M., Burini, A. & Principato, G. SpliceAid 2: a \ndatabase of human splicing factors expression data and RNA \ntarget motifs. Hum. Mutat. 33, 81‚Äì85 (2012).\n45. Fang, Y., Pan, X. & Shen, H. Recent deep learning methodology \ndevelopment for RNA-RNA interaction prediction. Symmetry 14, \n1302 (2022).\n46. Gu, T., Zhao, X., Barbazuk, W. & Lee, J. miTAR: a hybrid deep \nlearning-based approach for predicting miRNA targets. BMC \nBioinform. 22, 1‚Äì16 (2021).\n47. Sato, K., Akiyama, M. & Sakakibara, Y. RNA secondary structure \nprediction using deep learning with thermodynamic integration. \nNat. Commun. 12, 1‚Äì9 (2021).\n48. Zuker, M. & Stiegler, P. Optimal computer folding of large RNA \nsequences using thermodynamics and auxiliary information. \nNucleic Acids Res. 9, 133‚Äì148 (1981).\n49. Frankish, A. et al. GENCODE 2021. Nucleic Acids Res. 49, D916‚Äì\nD923 (2021).\n50. Ning, W. CatIIIIIIII/RNAErnie: v.1.0. Zenodo https://doi.org/10.5281/\nzenodo.10847621 (2024).\n51. Ning, W. RNAErnie docker. Zenodo https://doi.org/10.5281/\nzenodo.10847856 (2024).\n52. Ning, W. CatIIIIIIII/RNAErnie_baselines: v.1.0.0. Zenodo https://doi.\norg/10.5281/zenodo.10851577 (2024).\n53. Panwar, B., Arora, A. & Raghava, G. Prediction and classification \nof ncRNAs using structural information. BMC Genomics 15, 1‚Äì13 \n(2014).\n54. Wang, L. et al. ncRFP: a novel end-to-end method for non-coding \nRNAs family prediction based on deep learning. IEEE/ACM Trans. \nComput. Biol. Bioinform. 18, 784‚Äì789 (2020).\n55. Deng, C. et al. RNAGCN: RNA tertiary structure assessment with a \ngraph convolutional network. Chin. Phys. B 31, 118702 (2022).\n56. Chantsalnyam, T., Lim, D., Tayara, H. & Chong, K. ncRDeep: \nnon-coding RNA classification with convolutional neural network. \nComput. Biol. Chem. 88, 107364 (2020).\n57. Chantsalnyam, T., Siraj, A., Tayara, H. & Chong, K. ncRDense: a \nnovel computational approach for classification of non-coding \nRNA family by deep learning. Genomics 113, 3030‚Äì3038 (2021).\n58. Reuter, J. & Mathews, D. RNAstructure: software for RNA secondary  \nstructure prediction and analysis. BMC Bioinform. 11, 1‚Äì9 (2010).\n59. Andronescu, M., Aguirre-Hernandez, R., Condon, A. & Hoos, \nH. RNAsoft: a suite of RNA secondary structure prediction and \ndesign software tools. Nucleic Acids Res. 31, 3416‚Äì3422 (2003).\n60. Lorenz, R. et al. ViennaRNA package 2.0. Algorithms Mol. Biol. 6, \n1‚Äì14 (2011).\n61. Zuker, M. Mfold web server for nucleic acid folding and \nhybridization prediction. Nucleic Acids Res. 31, 3406‚Äì3415 (2003).\n62. Huang, L. et al. LinearFold: linear-time approximate RNA \nfolding by 5‚Ä≤-to-3‚Ä≤ dynamic programming and beam search. \nBioinformatics 35, i295‚Äìi304 (2019).\n63. Wayment-Steele, H. K. et al. RNA secondary structure packages \nevaluated and improved by high-throughput experiments. Nat. \nMethods 19, 1234‚Äì1242 (2022).\n64. Chen, X., Li, Y., Umarov, R., Gao, X. & Song, L. RNA secondary \nstructure prediction by learning unrolled algorithms. In \nInternational Conference on Learning Representations (2020).\n65. Do, C., Woods, D. & Batzoglou, S. CONTRAfold: RNA secondary \nstructure prediction without physics-based models. \nBioinformatics 22, e90‚Äìe98 (2006).\n66. Zakov, S., Goldberg, Y., Elhadad, M. & Ziv-Ukelson, M. Rich \nparameterization improves RNA structure prediction. J. Comput. \nBiol. 18, 1525‚Äì1542 (2011).\nAcknowledgements\nThis work is kindly supported by the National Science and Technology \nMajor Project under grant no. 2021ZD0110303 (N.W., J.B., X.L. and \nH.X.) and the National Science Foundation of China under grant no. \n62141220 (Y.L. and L.K.).\nAuthor contributions\nAll authors made contributions to this paper. N.W. and J.B. conducted \nexperiments and wrote part of the paper. Y.L., X.L. and S.M. were \ninvolved in the discussion and wrote part of the paper. L.K. oversaw \nthe research progress, was involved in the discussion and wrote part \nof the paper. H.X. oversaw the research progress, designed the study \nand experiments, was involved in the discussion and wrote the paper. \nH.X. is the senior author, and L.K. is the co-senior contributor.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-024-00836-4.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-024-00836-4.\nCorrespondence and requests for materials should be addressed to \nLinghe Kong or Haoyi Xiong.\nPeer review information Nature Machine Intelligence thanks Xiangfu \nZhong and the other, anonymous, reviewer(s) for their contribution to \nthe peer review of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher‚Äôs note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article‚Äôs Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article‚Äôs Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n¬© The Author(s) 2024\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-024-00836-4\nExtended Data Fig. 1 | The figure illustrates the use of a special ‚Äò[IND]‚Äô token followed by the RNAcentral instance type as an indicator. During the pre-training \nphase, the instance type is masked out and RNAErnie attempts to predict it. In downstream tasks, a two-stage pipeline is employed, which aggregates the top-K \npredicted indicators to improve performance."
}