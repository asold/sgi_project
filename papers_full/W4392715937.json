{
  "title": "Using Large Language Models to Augment (Rather Than Replace) Human Feedback in Higher Education Improves Perceived Feedback Quality",
  "url": "https://openalex.org/W4392715937",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2130385562",
      "name": "Thomas Schultze",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A5113139914",
      "name": "Varun Suresh Kumar",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A4279298783",
      "name": "Gary John McKeown",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A4369671287",
      "name": "Patrick Aaron O'Connor",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": "https://openalex.org/A1992743810",
      "name": "Magdalena Rychlowska",
      "affiliations": [
        "Queen's University Belfast"
      ]
    },
    {
      "id": null,
      "name": "Kristina Sparemblek",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2154470526",
    "https://openalex.org/W2054788835",
    "https://openalex.org/W3143756750",
    "https://openalex.org/W4387162877",
    "https://openalex.org/W3125633690",
    "https://openalex.org/W2560140854",
    "https://openalex.org/W2414190984",
    "https://openalex.org/W2933483998",
    "https://openalex.org/W4288907224",
    "https://openalex.org/W3024435124",
    "https://openalex.org/W2075134446",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W2105456679",
    "https://openalex.org/W2800068874",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2164668300",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W2570282904",
    "https://openalex.org/W2091254070",
    "https://openalex.org/W2154041216",
    "https://openalex.org/W2085105383",
    "https://openalex.org/W1983069604",
    "https://openalex.org/W1993985858",
    "https://openalex.org/W2744982537",
    "https://openalex.org/W4399551981",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1810028515",
    "https://openalex.org/W4399570526",
    "https://openalex.org/W4205184268",
    "https://openalex.org/W3034730645",
    "https://openalex.org/W4399650694",
    "https://openalex.org/W2030441548",
    "https://openalex.org/W4399648513"
  ],
  "abstract": "Formative feedback on assignments such as essays or theses is deemed necessary for students’ academic development in higher education. However, providing high quality feedback can be time-intensive and challenging, and students frequently report dissatisfaction with feedback quality. Here we explore a possible solution, namely using large language models (LLMs) to augment feedback provided by instructors. One potential obstacle to using LLM-augmented feedback is algorithm aversion, which might lead students to deprecate LLM-augmented feedback. Therefore, we examined students’ perceptions of human versus LLM-augmented feedback. In a pre-registered study, participants (N = 112) evaluated original human-generated versus LLM-augmented feedback on a previous assignment. Our results show evidence against algorithm aversion. Furthermore, participants rated the quality of LLM-augmented feedback substantially higher and strongly preferred it over the human-generated original. Our findings demonstrate the potential of LLMs to solve the persistent problem of low perceived feedback quality in higher education.",
  "full_text": "LLM-AUGMENTED FEEDBACK \n \n \nUsing Large Language Models to Augment (Rather Than Replace) Human Feedback in \nHigher Education Improves Perceived Feedback Quality \n \n \nThomas Schultze1, Varun Suresh Kumar2, Gary McKeown1, Patrick A. O'Connor1, \nMagdalena Rychlowska1, and Kristina Šparemblek1 \n \n1School of Psychology, Queen’s University Belfast, Belfast, United Kingdom \n2School of Maths and Physics, Queen's University Belfast \n \n \n \n \n \n \n \nAuthor Note \nThe study was preregistered. The preregistration, study materials, data, and analysis \ncode can be obtained from \nhttps://osf.io/h8yxg/?view_only=2ebc16d82adf44c3aa06c55d6dcf1a1f.  \nCorrespondence concerning this article should be addressed to Thomas Schultze, \nQueen’s University Belfast, School of Psychology, 18-30 Malone Road, Belfast BT9 5BN, \nUnited Kingdom. E-mail: t.schultze@qub.ac.uk \n \n \nManuscript submitted for publication, draft version 2, 2024-04-12. This is a \npreprint, which has not yet undergone peer review. That means, the manuscript \nhas not been finalized, might contain errors, and report information that has not \nyet been accepted or endorsed by the scientific community. If you cite this \npreprint or use the methods described here, please do so with caution. \nLLM-AUGMENTED FEEDBACK \n \n \nAbstract \n \nFormative feedback on assignments such as essays or theses is deemed necessary for \nstudents’ academic development in higher education. However, providing high quality \nfeedback can be time-intensive and challenging, and students frequently report dissatisfaction \nwith feedback quality. Here we explore a possible solution, namely using large language \nmodels (LLMs) to augment feedback provided by instructors. One potential obstacle to using \nLLM-augmented feedback is algorithm aversion, which might lead students to deprecate \nLLM-augmented feedback. Therefore, we examined students’ perceptions of human versus \nLLM-augmented feedback. In a pre-registered study, participants (N = 112) evaluated \noriginal human-generated versus LLM-augmented feedback on a previous assignment. Our \nresults show evidence against algorithm aversion. Furthermore, participants rated the quality \nof LLM-augmented feedback substantially higher and strongly preferred it over the human-\ngenerated original. Our findings demonstrate the potential of LLMs to solve the persistent \nproblem of low perceived feedback quality in higher education. \n \n \nKeywords: AI; large language models; feedback; higher education; algorithm aversion \nLLM-AUGMENTED FEEDBACK \n \n \nUsing Large Language Models to Augment (Rather Than Replace) Human Feedback in \nHigher Education Improves Perceived Feedback Quality \n1. Introduction \nOne of the pillars of higher education is formative assessment – informing students \nhow to improve – with instructor feedback being a central component (Sadler, 1989). \nFormative feedback contributes substantially to students’ progress (Hattie & Timperley, \n2007; Kluger & DeNisi, 1996), particularly, when it includes explanations of the correct \nresponse and provides insights about what to improve and how (Heckler & Mikula, 2016; \nLipnevich & Smith, 2009; Petrović et al., 2017). Formative feedback is particularly important \nfor complex assignments such as essays or theses. However, providing quality formative \nfeedback requires time. With growing student numbers and increasing academic workloads \nproviding high-quality feedback is challenging, and a perceived lack of feedback quality \namong students is a recurring issue in higher education (Henderson et al., 2019; Mulliner & \nTucker, 2017). For example, in the most recent UK’s National Student Survey, students \nexpressed dissatisfaction with the quality of the feedback they received (UK Office for \nStudents, 2023), and this dissatisfaction is not a recent phenomenon (Price et al., 2011). \nOne way to address the problem of feedback quality in higher education is to use AI. \nAI and statistical systems have long been used in assessment, usually with a focus on \nautomated grading. Page (1966) started the endeavour with Project Essay Grade, a \nregression-based grading system for high-school English essays. Later, systems such as \nAutotutor (Graesser et al., 2000) and the Intelligent Essay Assessor (Landauer et al., 2003) \nsuccessfully used Latent Semantic Analysis to grade open responses based on their semantic \nsimilarity to ideal answers. Most work in automated essay scoring focused on style features \nlike grammar, sentence structure, and punctuation. The development of techniques such as \nWord2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) allowed some \nLLM-AUGMENTED FEEDBACK \n \n \nimprovement in content-based assessment. Automated essay scoring continued to improve \nwith advancements in natural language processing. The first essay grading tools using large \nlanguage models (LLMs) appeared in 2019, and these models can slightly outperform human \nraters (Rodriguez et al., 2019). In sum, AI-based systems are increasingly able to provide \nvalid grades, especially when assignments focus on well-defined scientific concepts and \nsufficient training data exists (Ariely et al., 2023).  \nThere are also attempts to use AI to provide formative feedback. Revision Assistant \n(Woods et al., 2017) uses predictive scoring to identify weak sentences in essays based on \nlanguage, text organization, and support of arguments. Based on weaknesses identified, it \nthen selects formative feedback from a set of statements pre-written by human experts. \nHowever, while good use of language is important in complex assignments such as essays, \nformative feedback on such assignments is ideally higher-level and content based. AI is \nunfortunately not (yet) able to provide valid evaluations or feedback on the classic essay \nformat. For example, Dai et al. (2023) used an LLM to generate feedback on a student \nassignment and then assessed the validity of the LLM-generated feedback by comparing it to \nfeedback from human experts. While the LLM-generated feedback outperformed human \nfeedback in terms of readability, its validity was generally poor.  \nA possible solution to this problem is to use LLMs to augment rather than replace \nhuman feedback. We define LLM-augmented feedback as feedback generated by a human \nexpert that is then processed by an LLM. Consider a human instructor who reads a student \nassignment and comments on its strengths and weaknesses. This expert then feeds those \ncomments into an LLM, instructing it to make the text constructive and encouraging. Having \nan LLM augment the feedback might increase its perceived quality due to being more \nmotivating. In addition, its perceived quality should be higher due to improved readability \n(Dai et al., 2023).  \nLLM-AUGMENTED FEEDBACK \n \n \nOne potential obstacle to using LLM-augmented feedback is algorithm aversion, an \nirrational tendency to reject feedback generated using algorithms (Dietvorst et al., 2015). \nAlgorithm aversion towards LLM-augmented feedback (compared to human feedback) might \nnegatively impact students’ learning trajectories. While there are no studies investigating \nstudents’ acceptance of LLM-augmented – or LLM-generated – feedback, we can draw from \nresearch on advice taking. This research suggests that incorporating human judgements into \nalgorithmic advice – similar to our idea of augmented feedback – moderates the degree of \nalgorithm aversion, but there is still a controversy around the direction of this moderating \neffect (Jussupow et al., 2020). While some studies suggest that people derogate advisors who \nuse algorithmic decision aids (Arkes et al., 2007; Shaffer et al., 2013; Wolf, 2014), others \nsuggest that advice combining human and algorithmic judgment can reduce or even eliminate \nalgorithm aversion (Himmelstein & Budescu, 2023; Palmeira & Spassova, 2015). In addition, \nsome researchers argue that people prefer algorithmic to human advice, thus showing \nalgorithm appreciation (Logg et al., 2019). \nGiven the mixed evidence concerning algorithm aversion and appreciation it is \nessential to assess how students perceive the quality of LLM-augmented feedback before we \ncan consider employing it in higher education. Using LLM-augmented feedback might be \ncounter-indicated if students show algorithm aversion toward it. If, however, there is \nevidence against algorithm aversion or for algorithm appreciation, then LLM-augmented \nfeedback is viable tool in higher education. The present research aimed to test empirically \nwhether students would show algorithm aversion or algorithm appreciation towards LLM-\naugmented feedback. \nIn a pre-registered study, we asked students to evaluate feedback on one of their \nprevious assignments. We presented students human feedback they had received on that \nassignment (original feedback) along with an LLM-augmented alternative version. Since \nLLM-AUGMENTED FEEDBACK \n \n \nalgorithm aversion or appreciation can only occur when students are aware that the \nalternative feedback was created using an LLM, we manipulated whether students were \ninformed about or naïve to the use of an LLM when rating the feedback. Statistically, both \nalgorithm aversion and algorithm appreciation should manifest as interactions between \nfeedback type and awareness of the involvement of an LLM (awareness of the LLM’s \ninvolvement should affect ratings of the augmented feedback while leaving ratings of the \noriginal feedback unchanged). This leads to two competing hypotheses. \nH1a (algorithm aversion): When participants know how the augmented feedback was \ngenerated prior to rating it, their evaluation of the augmented feedback relative to the original \nfeedback is more negative than that of participants who only learn about the generation \nprocess after the ratings. \nH1b (algorithm appreciation): When participants know how the augmented feedback \nwas generated prior to rating it, their evaluation of the augmented feedback relative to the \noriginal feedback is more positive than that of participants who only learn about the \ngeneration process after the ratings. \nIn addition to testing these interaction hypotheses, we explored how students \nperceived the overall quality of LLM-augmented feedback relative to its human-generated \noriginal (main effect of feedback type). \n2. Method \n2.1 Participants and Design \nParticipants were 112 second-year undergraduate psychology students at a UK \nuniversity. Students were eligible to participate if they had been enrolled as first-year students \nin the same programme in the preceding academic year and had submitted a specific essay in \nthat academic year (we used feedback students received on this essay as the stimulus \nmaterials). This limited the pool of potential participants to 193 people, which translates to a \nLLM-AUGMENTED FEEDBACK \n \n \nresponse rate of 58%. We did not collect demographic data, but as with most UK universities, \nthe psychology student population mainly consists of UK nationals with notable proportions \nof foreign students from South Asia and Southeast Asia. The study used a 2 (information \nabout LLM: after rating vs. before rating) × 2 (feedback type: original vs. augmented) mixed \ndesign with information about the LLM as a between-subjects variable and feedback type as a \nwithin-subjects variable. Participants were compensated with online vouchers worth £15. \n2.2 Materials \nStimulus materials were developed using feedback participants had received from \ntheir instructors on an essay on individual differences submitted as part of the coursework of \ntheir first academic year. This feedback was provided by academic staff using three rubrics: \nstrengths of the piece of work, what could be improved, and how to make improvements. Due \nto university regulations, feedback was standardized such that there was at least one point for \neach rubric, and the total number of points raised across rubrics was capped at nine. From this \nhuman-generated original feedback, we created an alternative version by feeding the \ncomments into a localised instance of ChatGPT 3.5 (OpenAI, 2023). We instructed ChatGPT \nto make the original feedback constructive and encouraging.  \n2.3 Procedure \nParticipants were recruited for the online study via a mailing list, adverts in their \nlectures, and personalised email invitations containing a link to the study. Only eligible \nstudents could participate, and each person could take the survey only once. Participants were \nassigned to one of the two between-subjects conditions using an odd-even method. Every odd \nparticipant was assigned to the condition where information about the use of an LLM in the \ngeneration of the alternative feedback was provided before participants rated the feedback’s \nquality. Even participants were assigned to the condition where we provided information \nabout the LLM only after the quality ratings. \nLLM-AUGMENTED FEEDBACK \n \n \nAfter reading the study information and providing informed consent, participants \nwere directed to an instruction page. On this page, we informed participants that they would \nsee the feedback they had received on their essay on individual differences in their first study \nyear. We further told them that, in addition to the actual feedback they had received, we \nwould show them an alternative version of that feedback. They would then be asked to read \nboth versions of the feedback, rate their quality. In the condition providing the information \nabout the LLM before ratings, the study instructions included the following paragraph: \nWe would like to briefly explain how we created the alternative version of the \nfeedback. To create it, we took the original feedback provided by your tutor and fed it into an \nAI, more specifically, a large language model (LLM). The LLM we used was ChatGPT, \nwhich you might be familiar with. We instructed the AI to take the original feedback and \nmake it constructive and encouraging. The result is what we call AI-augmented feedback. \nAI-augmented feedback differs from AI-generated feedback in that it is based on human \nevaluation of your essay instead of an AI attempting to evaluate and provide feedback on its \nown. \n  Next, participants were shown both the original and alternative feedback and were \nasked to rate the quality of each version. They then indicated which version of the feedback \nthey preferred and explained their preference using an open-response format. Upon \ncompleting the ratings, participants were shown a debriefing page explaining the purpose of \nthe study. For participants who received information about the LLM only after rating \nfeedback quality, the debriefing included the paragraph shown above detailing how we \ngenerated the alternative feedback using an LLM. Following the debriefing, participants were \nthanked for taking the survey and received their online vouchers.  \nLLM-AUGMENTED FEEDBACK \n \n \n2.4 Measures \n2.4.1 Feedback Quality \nWe measured feedback quality using the following four statements: “The feedback \nwas clear and easy to understand”, “The feedback provided specific suggestions for \nimprovement”, “Overall, I found the feedback helpful”, and “I am satisfied with the quality \nof the feedback”. Participants indicated their agreement with these statements using sliders \nranging from 0 (complete disagreement) to 100 (complete agreement). We preregistered that \nwe would aggregate the responses to the four items by averaging them if their internal \nconsistency measured as Cronbach’s α exceeded .70. This was the case for both the ratings of \nthe original feedback (α = .90) and the alternative feedback (α = .80). \n2.4.2 Feedback Preference \nOur measure of feedback preference was a simple 2-alternative forced choice \nresponse. Participants could indicate either that they preferred the original or the alternative \nfeedback. They were also asked to explain their preference using an open-response format.  \n3. Research Transparency Statement \nHypotheses, method, and analysis plan were preregistered on 12th December 2024 \n(https://osf.io/h8yxg/?view_only=092c4819de7f473c970292de30c12702). We deviated from \nthe preregistration in one way: originally, we planned to stop collecting data at the end of \nNovember 2023. However, this would have left us with only 30 responses. Therefore, we \ndecided to re-advertise our study using personalised emails and to continue collecting data \nuntil the end of December 2023. Study materials, data, and analysis scripts are publicly \navailable (https://osf.io/h8yxg/?view_only=2ebc16d82adf44c3aa06c55d6dcf1a1f). All \nauthors declare that they have no conflicts of interest. The study was supported by the \nInnovation in Teaching Fund of the Faculty of XXXX at XXXX. Our study was approved by \nthe Research Ethics Committee of the Faculty of XXXX at XXXX on November 9th 2023 \nLLM-AUGMENTED FEEDBACK \n \n \n(reference number: EPS 23_394).  \n4. Results \nWe analysed the data using R version 4.3.1 (R Core Team, 2023) with the following \npackages: BayesFactor 0.9.12-4.6 (Morey & Rouder, 2023), dplyr 1.1.3 (Wickham, François, \net al., 2023), ggplot2 3.4.4 (Wickham, 2016), gridExtra 2.3 (Auguie, 2017), psych 2.3.12 \n(Revelle, 2023), and tidyr 1.3.0 (Wickham, Vaughan, et al., 2023). We used Bayesian \nanalyses with the Bayes factor (BF) as the inference criterion. As preregistered, we treated a \nBF > 3 as evidence for an effect, a BF < 1/3 as evidence against it, and BFs within this range \nas inconclusive. \n4.1 Preliminary Analyses \nWe compared the original and the LLM-augmented feedback using descriptive \nstatistics. First, we looked at the length of the feedback, operationalized as its word count. As \ncan be seen from Figure 1 (upper panels), the LLM-augmented feedback was, on average, \nabout 100 words longer than the original feedback (M = 376.39, SD = 110.19 vs. M = 258.35, \nSD = 122.87), and this increase in feedback length was largely due to the LLM increasing the \nlength of originally shorter feedback.  \nWe then analysed the tonality of the feedback using another LLM, namely \nFacebook’s bart-mnli model (Lewis et al., 2019). This model allows zero-shot classification \nof text, which means that it computes the probability of a piece of text to fall into certain \nuser-defined categories. For the bart-mnli analysis, we split the feedback into individual \nsentences and then prompted the model to compute the probability of each sentence to fall \ninto one of five categories. Categories were defined using the following labels: instructional, \ncritical, encouraging, informational, and question. We then assigned to each sentence the \nlabel with the highest probability. The lower panel of Figure 1 shows the relative frequency \nof the five categories for each type of feedback. The most notable difference is that the \nLLM-AUGMENTED FEEDBACK \n \n \ntonality of the augmented feedback is less critical and more encouraging than the original \nfeedback, whereas the relative frequencies of the remaining categories are relatively similar. \nThis shows that our instruction to ChatGPT to make the original feedback constructive and \nencouraging worked as intended. \n  \nLLM-AUGMENTED FEEDBACK \n \n \nFigure 1 \nComparison of the Original and the LLM-augmented Feedback \n \nNote. The top left panel shows the word count by feedback type. Black diamonds and error \nbars represent means and 95% confidence intervals. The width of the violins represents the \ndensity of the data at the respective point of the y-axis. The top right panel shows how the \nlength of the original feedback relates to the length of the augmented version. The line and \nbold line and grey band represent a linear regression line and its 95% confidence band. The \nbottom panel shows the results of a classification of each type of feedback using a bart-mnli \nmodel. Bars show the relative frequency of a sentence falling into a category.  \n\nLLM-AUGMENTED FEEDBACK \n \n \n4.2 Feedback Quality \nThe mean quality ratings by condition are shown in Figure 2. To test Hypotheses 1a \nand 1b, we ran a Bayesian mixed 2×2 ANOVA feedback quality. We used the default priors \nas implemented in the BayesFactor package for R (Morey & Rouder, 2023). In the Bayesian \nANOVA, a null model, which only contains participants as random effects, is compared to \nfour models: a model containing only the main effect of feedback type, a model containing \nonly the main effect of information about the LLM, a model containing both main effects, \nand the full model with the main effects and their interaction. Each of these comparisons \nyields a BF (see Table 1).  \n  \nLLM-AUGMENTED FEEDBACK \n \n \nTable 1 \nResults of the Bayesian ANOVA on feedback quality \nModel BF \nID + info about LLM 0.18 \nID + feedback type 8.86 × 10 7 \nID + info about LLM + feedback type 1.71 × 10 7 \nID + info about LLM + feedback type + info about LLM × feedback type 4.33 × 106 \n \nNote. ID represents participants as random effects. \n \nSince both hypotheses predict an interaction of feedback type and information about \nthe LLM, we first tested for this interaction by comparing the full model (both main effects \nand the interaction effect) with the main-effects only model. The comparison yielded \nevidence against an interaction, BF = 0.25, that is, students’ ratings were unaffected by the \ntime at which they learned about how we generated the alternative feedback. In other words, \nthere was neither algorithm aversion nor algorithm appreciation, which leads us to reject both \nH1a and H1b. We preregistered running simple effects analysis using Bayesian t-tests \n(scaling factor of r = 0.50 for the Cauchy priors on the effect size) only in case there was \nevidence for an interaction. However, for the sake of completeness, the results of these \nsimple effects tests are included in Figure 2. \nHaving found evidence against an interaction effect, we next explored the main \neffects. Absent an interaction, the main effects can be tested by comparing the model \ncontaining only the main effect of interest with the null model. Feedback type had a strong \nimpact on the quality ratings with the alternative feedback receiving much more favourable \nratings than the human-generated original (M = 80.80, SD = 14.50 vs. M = 66.20, SD = \nLLM-AUGMENTED FEEDBACK \n \n \n22.00). As can be seen in Table 1, the feedback type model not only fit the data better than \nthe null model, but it also outperformed all other models (all BFs > 5.19). Information about \nthe LLM had no effect on the quality ratings, BF = 0.18. \nOne simple explanation for LLM-augmented feedback receiving higher quality rating \nis that it is longer than the original feedback. To test this simple explanation, we ran a set of \nexploratory analyses. First, we tested within each feedback type whether word count was \ncorrelated with the quality ratings using the default Bayesian test of correlation against. We \nfound evidence against a correlation of feedback length and quality for both the original \nfeedback (r = .01, BF = 0.22) and the augmented feedback (r = .07, BF = 0.29). Furthermore, \nwe tested whether word count predicted quality ratings over and above feedback type, \ninformation about the LLM, and their interaction. Since the Bayesian ANOVA does not \nallow including covariates, we ran two Bayesian regression models using default priors. The \nfirst model is the equivalent to the full model Bayesian ANOVA. It includes the two main \neffects and the interaction as fixed effects, and participants as random effect. The second \nmodel also includes word count as an additional predictor. Consistent with the absence of \ncorrelations within each feedback type, the model comparison yielded evidence against an \neffect of feedback length on its perceived quality, BF = 0.29. \n \n \n  \nLLM-AUGMENTED FEEDBACK \n \n \nFigure 2 \nRatings of Feedback Quality by Feedback Type and Information About the LLM \n \nNote. Black diamonds and error bars represent means and 95% confidence intervals. The \nwidth of the violins represents the density of the data at the respective point of the y-axis. The \nsemi-transparent points represent individual quality ratings. Results of the Bayesian ANOVA \nare shown in the bottom left. Bayes factors for the simple effects analyses of feedback type \nand information about the LLM are displayed above the respective vertical brackets. \n  \n\nLLM-AUGMENTED FEEDBACK \n \n \n4.3 Feedback Preference \nWe ran an exploratory analysis on participants’ feedback preferences to test whether \nthey covaried systematically with information about the use of an LLM in the generation of \nthe alternative feedback. To this end, we ran the default Bayesian test of non-independence \nimplemented in the BayesFactor package on a 2 (info about LLM: after rating vs. before \nrating) × 2 (preferred feedback: original vs. augmented) contingency table. This test yielded \nevidence against non-independence, BF = 0.20. Among participants who received \ninformation about the LLM before rating it, 45 (79%) preferred the alternative feedback \nwhile only 12 (21%) indicated a preference for the original version. Numbers were \ncomparable for participants who only learned about the use of an LLM after the ratings with \n45 (82%) preferring the augmented feedback and 10 (18%) preferring the original. Finally, \nwe tested whether participants preferred the alternative feedback above chance level using a \ndefault binomial test as implemented in the BayesFactor package. Consistent with the quality \nratings, the test showed that participants systematically preferred the LLM-augmented \nfeedback over the human-generated original, BF = 2.76 × 108.  \n4.4 Open Responses \nTo gain more insights into students’ reasons for preferring either the original or LLM-\naugmented feedback, we inspected their responses to the question asking them to explain \ntheir stated preference. For students who preferred the augmented feedback, three dominant \nthemes emerged (a student’s explanation could include multiple themes). The most frequent \ntheme was that the alternative feedback was more positively phrased (76% of cases). It is \nimportant to note, here, that mentions of feedback positivity went beyond students preferring \npraise over criticism. In about half of the cases where students mentioned feedback positivity \nas a reason for preferring the augmented feedback, they added that – due to is more positive \ntone – they felt more motivated to engage with the feedback or more confident in their ability \nLLM-AUGMENTED FEEDBACK \n \n \nto improve. As one student put it:  \n“Personally, regardless of the grade I would feel more confident in my ability to \nimprove my next essays with the AI feedback because it is more uplifting and encouraging.” \nThe second most prevalent reason for preferring the alternative feedback was that it \nwas more detailed or clear on ways to improve (34% of cases), for example: \n“I found that the alternative feedback was a lot more constructive. I felt like the \nimprovements section was elaborated a lot more than the first one.” \nThe third recurring motive was better readability (27% of cases) as, for example, \napparent from this comment: “I feel that the alternative feedback is easier to follow and has a \nbetter flow while providing encouragement on the work and clearly describing what could be \ndone better.” \nAmong students who preferred the original instructor feedback, the three dominant \nthemes were that the original feedback seemed more realistic (36% of cases), that students \ndid not need ego-brushing (36% of cases), and that the original feedback was more concise \n(32% of cases). The following examples illustrate these recurring motives: \n“It isn’t trying to protect your ego and make you feel better about yourself telling you \nit is super when there is [sic] actual improvements to be made. You trust it more as it’s \nhonest.” \n“I rather it be more concise, the second one seems as though its [sic] trying harder to \nnot offend me.” \n“The original feels more genuine and doesn't try to curry favour by exclaiming that \nthe work was impressive.” \n \n \nLLM-AUGMENTED FEEDBACK \n \n \n5. Discussion  \n We investigated how students in higher education perceive the quality of LLM-\naugmented formative feedback compared to human-generated feedback. We were particularly \ninterested in whether students would show algorithm aversion or algorithm appreciation \ntowards LLM-augmented feedback. Answering this question is essential before we can \nconsider using AI to augmented feedback. The results of our study provide evidence against \nboth algorithm aversion and algorithm appreciation. Students did not like the augmented \nfeedback less or more just because they knew an AI was involved in its generation. The \nabsence of algorithm aversion is critical because it is a license to explore the possibilities of \nLLM-augmented feedback in higher education further.  \nIn addition, students rated the quality of the LLM-augmented feedback much higher \nthan that of the original human feedback, and most of them preferred the augmented \nfeedback. At first glance, this might seem trivial because we instructed ChatGPT to make the \noriginal feedback constructive and encouraging, and students might simply prefer feedback \nthat praises them. However, students’ open responses paint a more complete picture. The \naugmented feedback’s more positive tone was associated with a greater reported motivation \nto engage with it and a bolstering effect on students’ academic self-efficacy. Participants also \nfelt that the AI-augmented feedback was easier to parse and more detailed regarding ways to \nimprove. This finding is highly relevant considering the perceived lack of feedback quality in \nhigher education mentioned above (Henderson et al., 2019; Mulliner & Tucker, 2017). \nFeedback can only be effective if students engage with it, and the greater perceived quality of \nLLM-augmented feedback along with its more encouraging tone might contribute to greater \nstudent engagement and better learning outcomes. Using LLM-augmented feedback might \nprove an effective remedy to the persistent problem of feedback quality in higher education.  \nWe need to emphasise that LLM-augmented feedback is not a “one-size-fits-all” \nLLM-AUGMENTED FEEDBACK \n \n \nsolution. A notable minority of students preferred the original feedback because they felt it to \nbe more authentic, more concise, or because they felt that they did not need their teachers to \nbrush their egos. Those students might be more likely to disengage from LLM-augmented \nthan from human feedback. We cannot say, based on our data, what differentiates students \nwho preferred the original feedback from those who preferred the augmented version. \nUnderstanding the individual differences underlying these preferences might allow using a \ntailored approach where student receive the type of feedback that is most effective for their \nimprovement.   \nOne interesting question is why students in our study neither showed algorithm \nappreciation nor algorithm aversion. One possibility is that these phenomena are largely \nrestricted to the realm of judgement and decision-making. Another explanation, which we \nconsider more realistic, is the involvement of the human element in AI-augmented feedback. \nIn this regard, our findings are consistent with those of previous studies who found no \nevidence of algorithm aversion – or algorithm appreciation – toward hybrid advice \n(Lipnevich & Smith, 2009; Palmeira & Spassova, 2015). A recent study by (Himmelstein & \nBudescu, 2023) found algorithm aversion for political issues and appreciation for economic \nquestions, but only for the a priori expectations of advice quality. When presented with the \nactual advice, there were no differences in advice taking. Perhaps, as a recent study \ninsinuates, algorithm aversion is a question of hypotheticals whereas it disappears or even \nturns into algorithm appreciation when the advice is received (Logg & Schlund, 2024). Our \nfindings, too, point toward the possibility that the perceived quality of feedback matters more \nthan labels, at least in contexts where the recipient of the feedback can reasonably assess its \nquality or usefulness. \n5.1 Limitations and Directions for Future Research \nWe would like to address three limitations of our study. First, our study used self-\nLLM-AUGMENTED FEEDBACK \n \n \nreport data from a single cohort of psychology undergraduates at a single university in the \nUK. Arguably, the characteristics of the students, the subject, the institution, or even the \ncountry or region a learning institution is placed in could influence students’ acceptance of \nLLM-augmented feedback. Whether our findings generalise to other higher education \ncontexts is an open empirical question. However, our study can serve as a blueprint for others \nwho wish study the effectiveness of LLM-augmented feedback. \nSecond, while we found that students overwhelmingly preferred the AI-augmented \nfeedback due to its more motivating tone, better readability, and more detailed instructions, \nwe do not yet know whether LLM-augmented feedback also leads to better learning \noutcomes. Accordingly, future research should test the impact of LLM-augmented versus \nhuman-generated feedback on student engagement with the feedback as well as student \nprogression and academic performance. \nFinally, we cannot say, based on our results, to what extent the use of AI to augment \nhuman feedback could also be leveraged for greater efficiency. In our study, the original \nfeedback comprised elaborate written responses to three rubrics. One interesting question is \nwhether LLMs can generate high-quality feedback from more sparse human input, for \nexample, by reducing that input to bullet points or – for recurring mistakes – predefined \ncomments. Again, this is an empirical question that could be tested by comparing elaborate \nhuman feedback with LLM-augmented feedback generated from sparse human input instead. \nIf the quality of LLM-augmented feedback based on sparse human input matches or even \nexceeds that of elaborate human feedback, then it could contribute to more sustainable \nacademic workloads or free up valuable time for additional teaching opportunities. \n  \nLLM-AUGMENTED FEEDBACK \n \n \nReferences \nAriely, M., Nazaretsky, T., & Alexandron, G. (2023). Machine learning and hebrew NLP for \nautomated assessment of open-ended questions in biology. International Journal of \nArtificial Intelligence in Education, 33(1), 1–34. https://doi.org/10.1007/s40593-021-\n00283-x \nArkes, H. R., Shaffer, V. A., & Medow, M. A. (2007). Patients derogate physicians who use a \ncomputer-assisted diagnostic aid. Medical Decision Making: An International Journal \nof the Society for Medical Decision Making, 27(2), 189–202. \nhttps://doi.org/10.1177/0272989X06297391 \nAuguie, B. (2017). gridExtra: Miscellaneous functions for ‘grid’ graphics. https://CRAN.R-\nproject.org/package=gridExtra \nDai, W., Lin, J., Jin, H., Li, T., Tsai, Y.-S., Gašević, D., & Chen, G. (2023). Can large \nlanguage models provide feedback to students? A case study on ChatGPT. 2023 IEEE \nInternational Conference on Advanced Learning Technologies (ICALT), 323–325. \nhttps://doi.org/10.1109/ICALT58122.2023.00100 \nDietvorst, B. J., Simmons, J. P., & Massey, C. (2015). Algorithm aversion: People \nerroneously avoid algorithms after seeing them err. Journal of Experimental \nPsychology: General, 144, 114–126. https://doi.org/10.1037/xge0000033 \nGraesser, A. C., Wiemer-Hastings, P., Wiemer-Hastings, K., Harter, D., Tutoring Research \nGroup, T. R. G., & Person, N. (2000). Using latent semantic analysis to evaluate the \ncontributions of students in AutoTutor. Interactive Learning Environments, 8(2), 129–\n147. https://doi.org/10.1076/1049-4820(200008)8:2;1-B;FT129 \nHattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, \n77(1), 81–112. https://doi.org/10.3102/003465430298487 \nLLM-AUGMENTED FEEDBACK \n \n \nHeckler, A. F., & Mikula, B. D. (2016). Factors affecting learning of vector math from \ncomputer-based practice: Feedback complexity and prior knowledge. Physical Review \nPhysics Education Research, 12(1), 010134. \nhttps://doi.org/10.1103/PhysRevPhysEducRes.12.010134 \nHenderson, M., Ryan, T., & Phillips, M. (2019). The challenges of feedback in higher \neducation. Assessment & Evaluation in Higher Education, 44(8), 1237–1252. \nhttps://doi.org/10.1080/02602938.2019.1599815 \nHimmelstein, M., & Budescu, D. V. (2023). Preference for human or algorithmic forecasting \nadvice does not predict if and how it is used. Journal of Behavioral Decision Making, \n36(1), e2285. https://doi.org/10.1002/bdm.2285 \nJussupow, E., Benbasat, I., & Heinzl, A. (2020). Why are we averse towards algorithms? A \ncomprehensive literature review on  algorithm aversion. ECIS 2020 Research Papers. \nhttps://aisel.aisnet.org/ecis2020_rp/168 \nKluger, A. N., & DeNisi, A. (1996). The effects of feedback interventions on performance: A \nhistorical review, a meta-analysis, and a preliminary feedback intervention theory. \nPsychological Bulletin, 119(2), 254–284. https://doi.org/10.1037/0033-\n2909.119.2.254 \nLandauer, T. K., Laham, D., & Foltz, P. (2003). Automatic essay assessment. Assessment in \nEducation: Principles, Policy & Practice, 10(3), 295–308. \nhttps://doi.org/10.1080/0969594032000148154 \nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., & \nZettlemoyer, L. (2019). BART: Denoising sequence-to-sequence pre-training for \nnatural language generation, translation, and comprehension (arXiv:1910.13461). \narXiv. https://doi.org/10.48550/arXiv.1910.13461 \nLLM-AUGMENTED FEEDBACK \n \n \nLipnevich, A. A., & Smith, J. K. (2009). Effects of differential feedback on students’ \nexamination performance. Journal of Experimental Psychology: Applied, 15(4), 319–\n333. https://doi.org/10.1037/a0017841 \nLogg, J. M., Minson, J. A., & Moore, D. A. (2019). Algorithm appreciation: People prefer \nalgorithmic to human judgment. Organizational Behavior and Human Decision \nProcesses, 151, 90–103. https://doi.org/10.1016/j.obhdp.2018.12.005 \nLogg, J., & Schlund, R. (2024). A simple explanation reconciles “algorithm aversion” and \n“algorithm appreciation”: Hypotheticals vs. real judgments (SSRN Scholarly Paper \n4687557). https://papers.ssrn.com/abstract=4687557 \nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word \nrepresentations in vector space (arXiv:1301.3781). arXiv. \nhttps://doi.org/10.48550/arXiv.1301.3781 \nMorey, R. D., & Rouder, J. N. (2023). BayesFactor: Computation of Bayes factors for \ncommon designs. https://CRAN.R-project.org/package=BayesFactor \nMulliner, E., & Tucker, M. (2017). Feedback on feedback practice: Perceptions of students \nand academics. Assessment & Evaluation in Higher Education, 42(2), 266–288. \nhttps://doi.org/10.1080/02602938.2015.1103365 \nOpenAI. (2023). OpenAI GPT-3.5. https://www.openai.com \nPage, E. B. (1966). The imminence of... Grading essays by computer. The Phi Delta Kappan, \n47(5), 238–243. \nPalmeira, M., & Spassova, G. (2015). Consumer reactions to professionals who use decision \naids. European Journal of Marketing, 49(3/4), 302–326. https://doi.org/10.1108/EJM-\n07-2013-0390 \nLLM-AUGMENTED FEEDBACK \n \n \nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for word \nrepresentation. Proceedings of the 2014 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP), 1532–1543. \nPetrović, J., Pale, P., & Jeren, B. (2017). Online formative assessments in a digital signal \nprocessing course: Effects of feedback type and content difficulty on students learning \nachievements. Education and Information Technologies, 22(6), 3047–3061. \nhttps://doi.org/10.1007/s10639-016-9571-0 \nPrice, M., Handley, K., & Millar, J. (2011). Feedback: Focusing attention on engagement. \nStudies in Higher Education, 36(8), 879–896. \nhttps://doi.org/10.1080/03075079.2010.483513 \nR Core Team. (2023). R: A language and rnvironment for statistical computing. R \nFoundation for Statistical Computing. https://www.R-project.org/ \nRevelle, W. (2023). psych: Procedures for psychological, psychometric, and personality \nresearch. Northwestern University. https://CRAN.R-project.org/package=psych \nSadler, D. R. (1989). Formative assessment and the design of instructional systems. \nInstructional Science, 18(2), 119–144. https://doi.org/10.1007/BF00117714 \nShaffer, V. A., Probst, C. A., Merkle, E. C., Arkes, H. R., & Medow, M. A. (2013). Why do \npatients derogate physicians who use a computer-based diagnostic support system? \nMedical Decision Making: An International Journal of the Society for Medical \nDecision Making, 33(1), 108–118. https://doi.org/10.1177/0272989X12453501 \nUK Office for Students. (2023). National Student Survey 2023. \nhttps://www.officeforstudents.org.uk/data-and-analysis/national-student-survey-\ndata/student-characteristics-data/ \nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New \nYork. https://ggplot2.tidyverse.org \nLLM-AUGMENTED FEEDBACK \n \n \nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). dplyr: A grammar \nof data manipulation. https://CRAN.R-project.org/package=dplyr \nWickham, H., Vaughan, D., & Girlich, M. (2023). tidyr: Tidy messy data. https://CRAN.R-\nproject.org/package=tidyr \nWolf, J. R. (2014). Do IT students prefer doctors who use IT? Computers in Human \nBehavior, 35, 287–294. https://doi.org/10.1016/j.chb.2014.03.020 \nWoods, B., Adamson, D., Miel, S., & Mayfield, E. (2017). Formative essay feedback using \npredictive scoring models. Proceedings of the 23rd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, 2071–2080. \nhttps://doi.org/10.1145/3097983.3098160 \n ",
  "topic": "Formative assessment",
  "concepts": [
    {
      "name": "Formative assessment",
      "score": 0.8144629001617432
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.6451939344406128
    },
    {
      "name": "Higher education",
      "score": 0.58888840675354
    },
    {
      "name": "Perception",
      "score": 0.546109676361084
    },
    {
      "name": "Computer science",
      "score": 0.5201666355133057
    },
    {
      "name": "Peer feedback",
      "score": 0.48858770728111267
    },
    {
      "name": "Psychology",
      "score": 0.41319456696510315
    },
    {
      "name": "Mathematics education",
      "score": 0.29951101541519165
    },
    {
      "name": "Political science",
      "score": 0.06965649127960205
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126231945",
      "name": "Queen's University Belfast",
      "country": "GB"
    }
  ]
}