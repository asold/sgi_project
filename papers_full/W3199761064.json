{
  "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
  "url": "https://openalex.org/W3199761064",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5023594937",
      "name": "Runxin Xu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5019525050",
      "name": "Fuli Luo",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5015455705",
      "name": "Zhiyuan Zhang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5065907658",
      "name": "Chuanqi Tan",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5021459300",
      "name": "Baobao Chang",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5047856952",
      "name": "Songfang Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5101488344",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6629161641",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2396585843",
    "https://openalex.org/W2756386045",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2962885042",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3146885639",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2962965870",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W4287866129",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3176123145",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3034774187",
    "https://openalex.org/W3130089296",
    "https://openalex.org/W2963959597",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2995463996",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3005700362",
    "https://openalex.org/W3007027445",
    "https://openalex.org/W2764043458"
  ],
  "abstract": "Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9514‚Äì9528\nNovember 7‚Äì11, 2021.c‚Éù2021 Association for Computational Linguistics\n9514\nRaise a Child in Large Language Model:\nTowards Effective and Generalizable Fine-tuning\nRunxin Xu1‚àó, Fuli Luo2‚àó, Zhiyuan Zhang1, Chuanqi Tan2,\nBaobao Chang1‚Ä†, Songfang Huang2‚Ä†, Fei Huang2\n1Key Laboratory of Computational Linguistics, Peking University, MOE, China\n2Alibaba Group\nrunxinxu@gmail.com, {zzy1210,chbb}@pku.edu.cn\n{lfl259702,chuanqi.tcq,songfang.hsf,f.huang}@alibaba-inc.com\nAbstract\nRecent pretrained language models extend\nfrom millions to billions of parameters. Thus\nthe need to Ô¨Åne-tune an extremely large pre-\ntrained model with a limited training corpus\narises in various downstream tasks. In this pa-\nper, we propose a straightforward yet effec-\ntive Ô¨Åne-tuning technique, C HILD -TUNING ,\nwhich updates a subset of parameters (called\nchild network) of large pretrained models via\nstrategically masking out the gradients of the\nnon-child network during the backward pro-\ncess. Experiments on various downstream\ntasks in GLUE benchmark show that C HILD -\nTUNING consistently outperforms the vanilla\nÔ¨Åne-tuning by 1.5 ‚àº8.6 average score among\nfour different pretrained models, and surpasses\nthe prior Ô¨Åne-tuning techniques by 0.6 ‚àº\n1.3 points. Furthermore, empirical results on\ndomain transfer and task transfer show that\nCHILD -TUNING can obtain better generaliza-\ntion performance by large margins.\n1 Introduction\nPretrained Language Models (PLMs) have had a\nremarkable effect on the natural language process-\ning (NLP) landscape recently (Devlin et al., 2019;\nLiu et al., 2019; Clark et al., 2020). Pretraining and\nÔ¨Åne-tuning have become a new paradigm of NLP,\ndominating a large variety of tasks.\nDespite its great success, how to adapt such\nlarge-scale pretrained language models with mil-\nlions to billions of parameters to various scenarios,\nespecially when the training data is limited, is still\nchallenging. Due to the extremely large capac-\nity and limited labeled data, conventional trans-\nfer learning tends to aggressive Ô¨Åne-tuning (Jiang\net al., 2020), resulting in: 1) degenerated results\non the test data due to overÔ¨Åtting (Devlin et al.,\n2019; Phang et al., 2018; Lee et al., 2020), and 2)\n‚àóEqual Contribution. Joint work between Alibaba and\nPeking University.\n‚Ä†Corresponding authors.\n+ =Forward\nVanilla\t\nBackward\nCHILD-TUNING\nBackward\nùë§! ‚àÜùë§! ùë§\"\n+ =\nTask-Free or\tTask-Driven\nGradients\tMask\nPretrained\nWeights\nWeights\tat\n1-th\tIteration\nùë§! ‚àÜùë§! ùë§\"\nFigure 1: The illustration of C HILD -TUNING . Left:\nIt forwards on the whole network while backwarding\non a subset of network (i.e., child network). Right:\nTo achieve this, a task-free or task-driven mask is per-\nformed on the gradients of the non-child network, re-\nsetting them to zero (grey diagonal grids).\npoor generalization ability in transferring to out-of-\ndomain data or other related tasks (Mahabadi et al.,\n2021; Aghajanyan et al., 2021).\nPreventing the Ô¨Åne-tuned models to deviate too\nmuch from the pretrained weights (i.e., with less\nknowledge forgetting), is proved to be effective to\nmitigate the above challenges (Gouk et al., 2020).\nFor instance, RecAdam (Chen et al., 2020) intro-\nduces L2 distance penalty between the Ô¨Åne-tuned\nweights and their pretrained weights. In addition,\nMixout (Lee et al., 2020) randomly replaces part of\nthe model parameters with their pretrained weights\nduring Ô¨Åne-tuning. The core idea behind them is\nto utilize the pretrained weights to regularize the\nÔ¨Åne-tuned model.\nIn this paper, we propose to mitigate the aggres-\nsive Ô¨Åne-tuning problem from a new perspective.\nBased on the observation that it is unnecessary to\nupdate all the parameters within the large-scale\nmodel during Ô¨Åne-tuning, we propose an effec-\ntive Ô¨Åne-tuning technique, CHILD -TUNING , which\nstraightforwardly updates a subset of parameters\n(called child network) via strategically masking\nout the gradients of non-child network in the back-\nward process, as illustrated in Figure 1. Note that\n9515\nit is different from model pruning, since it still for-\nwards on the whole network, thus making the full\nuse of knowledge hidden in the pretrained weights.\nIn detail, we propose two variants, CHILD -\nTUNING F and CHILD -TUNING D, which respec-\ntively detect the child network in a task-free and\na task-driven way. CHILD -TUNING F chooses out\nthe child network in the absence of task data via a\nBernoulli distribution. It introduces noise to the full\ngradients, playing a role of regularization, hence\npreventing overÔ¨Åtting to small datasets and lead-\ning to better generalization. Furthermore, CHILD -\nTUNING D utilizes the downstream task data to de-\ntect the most task-related parameters as the child\nnetwork and freezes the parameters in non-child\nnetwork to their pretrained weights. It decreases the\nhypothesis space of the model via a task-speciÔ¨Åc\nmask applied to the full gradients, helping to ef-\nfectively adapt the large-scale pretrained model to\nvarious tasks and meanwhile greatly maintain its\noriginal generalization ability.\nOur extensive experiments on the GLUE\nbenchmark show that CHILD -TUNING can be\nmore excellent at Ô¨Åne-tuning different PLMs,\nwith up to 8.60 average score improvement\non CoLA/RTE/MRPC/STS-B tasks compared to\nvanilla Ô¨Åne-tuning (Section. 3.3). Moreover, it\nachieves better generalization ability in transferring\nto out-of-domain data and other related tasks (Sec-\ntion. 3.4). Experimental results also demonstrate\nthat CHILD -TUNING yields consistently greater im-\nprovements than state-of-the-art Ô¨Åne-tuning meth-\nods. More importantly, since CHILD -TUNING\nis orthogonal to these prior methods, integrating\nCHILD -TUNING with them can even lead to further\nimprovements (Section. 4.1).\nIn summary, our contributions are three-fold:\n‚Ä¢ We propose CHILD -TUNING , a straightfor-\nward yet effective Ô¨Åne-tuning technique that\nonly updates the parameters in the child net-\nwork. We explore to detect the child network\nin both task-free and task-driven ways.\n‚Ä¢ CHILD -TUNING can effectively adapt the\nlarge-scale pretrained model to various down-\nstream scenarios, from in-domain to out-of-\ndomain, and cross-task transfer learning.\n‚Ä¢ Since CHILD -TUNING is orthogonal to prior\nÔ¨Åne-tuning methods, integrating CHILD -\nTUNING with them can further boost the Ô¨Åne-\ntuning performance.\n2 Methodology\nTo better adapt large-scale pretrained language\nmodel to various downstream tasks, we propose a\nsimple yet effective Ô¨Åne-tuning technique, CHILD -\nTUNING . We Ô¨Årstly introduce a gradient mask in\nthe backward process to achieve the aim of updat-\ning a subset of parameters (i.e., child network),\nwhile still utilizing the knowledge of the whole\nlarge model in the forward process (Section 2.1).\nThen, we explore two ways to detect the child\nnetwork (i.e., generate different gradient masks):\nCHILD -TUNING F that are in a task-free way (Sec-\ntion 2.2), and CHILD -TUNING D that are in a task-\ndriven way ( Section 2.3).\n2.1 Overview of C HILD -TUNING\nWe start the introduction of CHILD -TUNING by\ngiving a general formulation of the back propaga-\ntion during the vanilla Ô¨Åne-tuning. We denote the\nparameters of the model at the t-th iteration as wt\n(w0 refers to the pretrained weights). The vanilla\nÔ¨Åne-tuning computes the gradient of the lossL(wt)\nand then applies gradient descent to all parameters,\nwhich can be formulated as:\nwt+1 = wt ‚àíŒ∑‚àÇL(wt)\n‚àÇwt\n(1)\nwhere ‚àÇL(wt)\n‚àÇwt are the gradients corresponding to\nthe model parameters wt, Œ∑is the learning rate.\nCHILD -TUNING also backwardly computes the\ngradients of all trainable parameters like standard\nÔ¨Åne-tuning. However, the key difference is that\nCHILD -TUNING determines a child network Ct at\nthe t-th iteration, and only updates this part of pa-\nrameters. To achieve this, we Ô¨Årstly deÔ¨Åne a 0-1\nmask that is the same-sized as w as follows:\nM(i)\nt =\n{\n1, w(i)\nt ‚ààCt\n0, w(i)\nt /‚ààCt\n(2)\nwhere M(i)\nt and w(i)\nt denote the i-th element of\nthe mask Mt and parameters wt at the t-th training\niteration, respectively.\nThen, we formally deÔ¨Åne CHILD -TUNING tech-\nnique by simply replacing Eq. 1 with the following\nequation:\nwt+1 = wt ‚àíŒ∑‚àÇL(wt)\n‚àÇwt\n‚äôMt (3)\nAlgorithm 1 provides the pseudo-code of\nCHILD -TUNING when applied to widely used\nAdam (Kingma and Ba, 2015) optimizer. The main\ndifference is the insertion of line 5-7.\n9516\nAlgorithm 1 CHILD -TUNING for Adam Optimizer\nRequire: w0: initial pretrained weights; L(w):\nstochastic objective function with parameters\nw; Œ∑: learning rate; Œ≤1,Œ≤2 ‚àà[0,1): exponen-\ntial decay rates for the moment estimates;\n1: initialize timestep t‚Üê0, Ô¨Årst moment vector\nm0 ‚Üê0, second moment vector v0 ‚Üê0\n2: while not converged do\n3: t‚Üêt+ 1\n// Get gradients\n4: gt ‚Üê‚àÇL(wt)\n‚àÇwt\n// Get task-free/task-driven child network\n5: Ct ‚ÜêGetChildNetwork()\n// Generate a corresponding gradient mask\n6: Mt ‚ÜêGenerateMask(Ct)\n// Employ mask for gradients\n7: gt ‚Üêgt ‚äôMt\n8: mt ‚ÜêŒ≤1 ¬∑mt‚àí1 + (1 ‚àíŒ≤1) ¬∑gt\n9: vt ‚ÜêŒ≤2 ¬∑vt‚àí1 + (1 ‚àíŒ≤2) ¬∑g2\nt\n// Bias correction\n10: ÀÜmt ‚Üêmt/(1 ‚àíŒ≤t\n1)\n11: ÀÜvt ‚Üêvt/(1 ‚àíŒ≤t\n2)\n// Update weights\n12: wt ‚Üêwt‚àí1 ‚àíŒ∑¬∑ÀÜmt/(‚àöÀÜvt + œµ)\n13: end while\n14: return wt\n2.2 Task-Free Variant: C HILD -TUNING F\nIn this section, we Ô¨Årstly explore the choice of\nthe child network that does not require any down-\nstream task data, i.e., a task-free technique called\nCHILD -TUNING F. SpeciÔ¨Åcally, CHILD -TUNING F\ngenerates a 0-1 mask Mt at the t-th iteration drawn\nfrom a Bernoulli distribution with a probability pF:\nMt ‚àºBernoulli(pF) (4)\nThe higher the pF is, the larger the child network\nis, and hence more parameters are updated. When\npF = 1 , CHILD -TUNING F degenerates into the\nvanilla Ô¨Åne-tuning method. Note that we also en-\nlarge the reserved gradients by 1\npF\nto maintain the\nexpectation of the gradients.\nWe theoretically justify the effectiveness of\nCHILD -TUNING F. We denote ‚àÜw as the update\nat each iteration:\n‚àÜw = Œ∑‚àÇL(w)\n‚àÇw ‚äôM (5)\nIntuitively, Theorem 1 shows the variance of gradi-\nents is a strictly decreasing function of pF. Thus,\nCHILD -TUNING F improves the variance of the\ngradients, and the trade-off between exploration\nand exploitation can be controlled by adjusting pF.\nAs illustrated in Theorem 2, with higher variance,\nthe model can converge to more Ô¨Çat local minima\n(smaller œÅin Theorem 2). Inspired by studies that\nshow Ô¨Çat minima tends to generalize better (Keskar\net al., 2017; Sun et al., 2020; Foret et al., 2021), we\ncan further prove CHILD -TUNING F decreases the\ngeneralization error bound.\nTheorem 1. Suppose Ldenotes the loss function\non the parameter w, the gradients obey a Gaussian\ndistribution N( ‚àÇL\n‚àÇw,œÉ2\ngIk), and SGD with learning\nrate Œ∑is used. For a randomly sampled batch B, if\nGradMask reserves gradients with probability pF,\nthe mean and covariance of the update ‚àÜw are,\nE[‚àÜw] = ‚àíŒ∑‚àÇL\n‚àÇw (6)\nŒ£[‚àÜw] = Œ∑2œÉ2\ngIk\npF|B| + (1 ‚àípF)Œ∑2diag{‚àÇL\n‚àÇw}2\npF\n(7)\nSpecially, when w is a local minima, E[‚àÜw] =\n0k,Œ£[‚àÜw] = œÉ2Ik and œÉ2 =\nŒ∑2œÉ2\ng\npF|B| is a strictly\ndecreasing function of pF.\nTheorem 2. Suppose w0 denotes the pretrained\nparameter; kis the number of parameters; w de-\nnotes the local minima the algorithm converges\nto; œÅ is the greatest eigenvalue of the Hessian\nmatrix on w, which indicates the sharpness. If\n‚àÜw ‚àºN(0k,œÉ2Ik), when the following bound\nholds, the algorithm can converge to the local min-\nima w with high probability,\nœÅ‚â§O\n(1\nœÉ2\n)\n(8)\nSuppose the prior over parameters after training\nis P = N(w0,œÉ2\n0Ik), the following generalization\nerror bound holds with high probability,\nbound (w) ‚â§O\n(kœÉ2\n0 ‚àí‚à•w‚àíw0 ‚à•2\nœÉ2\n)\n+ R (9)\nwhere Ris a term not determined by œÉ.\nThus, CHILD -TUNING F can be viewed as a\nstrong regularization for the optimization process.\nIt enables the model to skip the saddle point in the\nloss landscape and encourages the model to con-\nverge to a more Ô¨Çat local minima. Please refer to\nAppendix E for more details about stated theorems\nand proofs.\n9517\n2.3 Task-Driven Variant: C HILD -TUNING D\nTaking the downstream labeled data into considera-\ntion, we propose CHILD -TUNING D, which detects\nthe most important child network for the target\ntask. SpeciÔ¨Åcally, we adopt the Fisher informa-\ntion estimation to Ô¨Ånd the highly relevant subset\nof the parameters for a speciÔ¨Åc downstream task.\nFisher information serves as a good way to provide\nan estimation of how much information a random\nvariable carries about a parameter of the distribu-\ntion (Tu et al., 2016a,b). For a pretrained model,\nFisher information can be used to measure the rel-\native importance of the parameters in the network\ntowards the downstream tasks.\nFormally, the Fisher Information Matrix (FIM)\nfor the model parameters w is deÔ¨Åned as follows:\nF (w) = E\n[\n( ‚àÇlog p(y|x;w)\n‚àÇw )( ‚àÇlog p(y|x;w)\n‚àÇw )‚ä§\n]\nwhere x and y denote the input and the output\nrespectively. It can be also viewed as the covariance\nof the gradient of the log likelihood with respect\nto the parameters w. Following Kirkpatrick et al.\n(2016), given the task-speciÔ¨Åc training data data\nD, we use the diagonal elements of the empirical\nFIM to point-estimate the task-related importance\nof the parameters. Formally, we derive the Fisher\ninformation for the i-th parameter as follows:\nF(i) (w) = 1\n|D|\n|D|‚àë\nj=1\n(‚àÇlog p(yj|xj; w)\n‚àÇw(i)\n)2\n(10)\nWe assume that the more important the parame-\nter towards the target task, the higher Fisher infor-\nmation it conveys. Hence the child network Cis\ncomprised of the parameters with the highest infor-\nmation. The child network ratio is pD = |C|\n|C|+|C|‚àà\n(0,1], where Cdenotes the non-child network. As\npD rises, the scale of the child network also in-\ncreases, and when pD = 1 it degenerates into the\nvanilla Ô¨Åne-tuning strategy.\nSince the overhead of obtaining the task-driven\nchild network is heavier than that of the task-\nfree one, we simply derive the child network for\nCHILD -TUNING D at the beginning of Ô¨Åne-tuning,\nand keep it unchanged during the Ô¨Åne-tuning, i.e.,\nC0 = C1 = ¬∑¬∑¬∑ = CT. In this way, CHILD -\nTUNING D dramatically decreases the hypothesis\nspace of the large-scale models, thus alleviating\noverÔ¨Åtting. Meanwhile, keeping the non-child net-\nwork freezed to their pretrained weights can sub-\nstantially maintain the generalization ability.\n3 Experiments\n3.1 Datasets\nGLUE benchmark Following previous stud-\nies (Lee et al., 2020; Dodge et al., 2020), we con-\nduct experiments on various datasets from GLUE\nleaderboard (Wang et al., 2019), including linguis-\ntic acceptability (CoLA), natural language infer-\nence (RTE, QNLI, MNLI), paraphrase and similar-\nity (MRPC, STS-B, QQP), and sentiment classiÔ¨Åca-\ntion (SST-2). CoLA and SST-2 are single-sentence\nclassiÔ¨Åcation tasks and the others are involved with\na pair of sentences. The detailed statistics and met-\nrics are provided in Appendix A. Following most\nprevious works (Phang et al., 2018; Lee et al., 2020;\nDodge et al., 2020), we Ô¨Åne-tune the pretrained\nmodel on the training set and directly report results\non the dev set using the last checkpoint, since the\ntest results are only accessible by the leaderboard\nwith a limitation of the number of submissions.\nNLI datasets In this paper, we also conduct\nexperiments to explore the generalization ability\nof the Ô¨Åne-tuned model based on several Natu-\nral Language Inference (NLI) tasks. SpeciÔ¨Åcally,\nwe additionally introduce three NLI datasets, i.e.,\nSICK (Marelli et al., 2014), SNLI (Bowman et al.,\n2015) and SciTail (Khot et al., 2018). We also\nreport results on the dev set consistent with GLUE.\n3.2 Experiments Setup\nWe use the pretrained models and codes pro-\nvided by HuggingFace 1 (Wolf et al., 2020), and\nfollow their default hyperparameter settings un-\nless noted otherwise. Appendix B provides\ndetailed experimental setups (e.g., batch size,\ntraining steps, and etc.) for BERT LARGE (De-\nvlin et al., 2019), XLNet LARGE (Yang et al.,\n2019), RoBERTaLARGE (Liu et al., 2019), and\nELECTRALARGE (Clark et al., 2020). We report\nthe averaged results over 10 random seeds.2\n1https://github.com/huggingface/\ntransformers\n2Our code is available at https://github.com/\nalibaba/AliceMind/tree/main/ChildTuning\nand https://github.com/PKUnlp-icler/\nChildTuning.\n9518\nMethod BERT XLNet\nCoLA RTE MRPC STS-B Avg CoLA RTE MRPC STS-B Avg\nVanilla Fine-tuning 63.13 70.18 90.77 89.61 78.42 47.14 77.62 91.90 91.77 77.11\nCHILD-TUNINGF 63.71 72.06 91.22 90.18 79.29 52.07 78.05 92.29 91.81 78.56\nCHILD-TUNINGD 64.92 73.14 91.42 90.18 79.92 51.54 80.94 92.46 91.82 79.19\nMethod RoBERTa ELECTRA\nCoLA RTE MRPC STS-B Avg CoLA RTE MRPC STS-B Avg\nVanilla Fine-tuning 66.10 85.20 92.62 92.04 83.99 47.42 88.23 92.95 81.86 77.62\nCHILD-TUNINGF 65.99 84.80 92.66 92.15 83.90 62.31 88.41 93.09 91.73 83.89\nCHILD-TUNINGD 66.71 86.14 92.78 92.36 84.50 70.62 88.90 93.32 92.02 86.22\nTable 1: Comparison between CHILD -TUNING and vanilla Ô¨Åne-tuning applied to four widely used large-scale\nPretrained Language Models (PLMs). Average scores on all tasks are underlined . The best results are bold. It\nshows that CHILD -TUNING yields consistent improvements across all tasks among different PLMs, especially for\nCHILD -TUNING D that detects the child network in a task-driven way.\nDatasets MNLI SNLI\nVanilla C.TUNINGF ‚àÜF C.TUNINGD ‚àÜD Vanilla C.TUNINGF ‚àÜF C.TUNINGD ‚àÜD\nMNLI 75.30 75.95 +0.65 76.61 +1.31 65.80 66.01 +0.21 66.82 +1.02\nMNLI‚Äìm 76.50 77.79 +1.29 77.98 +1.48 67.71 67.27 ‚Äì0.44 68.48 +0.77\nSNLI 69.61 70.35 +0.74 71.17 +1.56 82.90 83.17 +0.27 83.66 +0.76\nSICK 48.25 49.13 +0.88 50.15 +1.90 51.50 51.16 ‚Äì0.34 51.42 ‚Äì0.08\nSciTail 73.65 75.42 +1.77 75.08 +1.43 69.35 70.74 +1.39 71.10 +1.75\nQQP 71.37 72.24 +0.87 72.67 +1.30 70.60 71.52 +0.92 71.19 +0.59\nAvg‚àó 67.88 68.99 +1.11 69.41 +1.53 64.99 65.34 +0.35 65.80 +0.81\nTable 2: Probing domain generalization. The models are trained on MNLI/SNLI and tested on out-of-domain\ndata. ‚àÜF and ‚àÜD denotes the improvement of C.T UNING F and C.T UNING D compared with vanilla Ô¨Åne-tuning.\nAverage scores (marked with‚àó) is computed excluding in-domain results (underlined). Positive transfer results are\nhighlighted in blue. C HILD -TUNING can better maintain the out-of-domain generalization ability of the model.\n3.3 Results on GLUE Benchmark\nIn this section, we show the results of four widely\nused large PLMs on four GLUE tasks: CoLA, RTE,\nMRPC, and STS-B, following Lee et al. (2020).\nBesides vanilla Ô¨Åne-tuning, we also report the re-\nsults of two variants of CHILD -TUNING , including\nboth CHILD -TUNING F (pF = 0.2,0.3,0.4) and\nCHILD -TUNING D (pD = 0.1,0.2,0.3).\nAs Table 1 illustrates, CHILD -TUNING outper-\nforms vanilla Ô¨Åne-tuning by a large gain across\nall the tasks on different PLMs . For instance,\nCHILD -TUNING yields an improvement of up to\n2.08 average score on XLNet, and 8.60 average\nscore on ELECTRA. Besides, the straightforward\ntask-free variant, CHILD -TUNING F, can still pro-\nvide an improvement of 0.87 average score on\nBERT and 6.27 on ELECTRA. CHILD -TUNING D,\nwhich detects child network in a task-driven way,\nis more aware of the unique characteristics of the\ndownstream task, and therefore achieves the best\nperformance, with up to 1.50 and 8.60 average\nscore improvement on BERT and ELECTRA. In\nsummary, we can come to a conclusion thatCHILD -\nTUNING is model-agnostic and can consistently\noutperform vanilla Ô¨Åne-tuning on different PLMs.\n3.4 Probing Generalization Ability of the\nFine-tuned Model\nTo measure the generalization properties of various\nÔ¨Åne-tuning methods, in this section, we conduct\nprobing experiments from two aspects, that is, do-\nmain generalization and task generalization.\n3.4.1 Domain Generalization\nBesides boosting performance on the target down-\nstream task, we also expect CHILD -TUNING can\nhelp the Ô¨Åne-tuned model achieve better general-\nization ability towards out-of-domain data.\nWe evaluate how well the Ô¨Åne-tuned model gen-\neralizes to out-of-domain data based on several\nNatural Language Inference (NLI) tasks. In detail,\n9519\n35.17\n38.68\n39.90\n(a) CoLA (Matthews Corr)\n70.57\n72.19\n78.74 (b) STS-B (Spearman Corr)\n69.45\n75.26\n77.60 (c) QNLI (Accuracy)\n60.61\n67.99 67.80 (d) QQP (F1)\nFigure 2: Probing task generalization. The model is Ô¨Åne-tuned on MRPC task and transferred to four different\ntasks. C HILD -TUNING can maintain more generalizable representations compared with vanilla Ô¨Åne-tuning.\nwe Ô¨Åne-tune BERTLARGE with different strategies\non 5k subsampled MNLI and SNLI datasets re-\nspectively, and directly test the accuracy of the Ô¨Åne-\ntuned models on other NLI datasets in different do-\nmains, including MNLI, MNLI-mismatch3, SNLI,\nSICK, SciTail, and QQP4. As Table 2 illustrates,\nCHILD -TUNING outperforms vanilla Ô¨Åne-tuning\nacross different out-of-domain datasets. Specif-\nically, CHILD -TUNING F improves 1.11/0.35 av-\nerage score for models trained on MNLI/SNLI,\nwhile CHILD -TUNING D improves up to 1.53/0.81\naverage score. In particular, CHILD -TUNING D\nachieves 1.90 score improvement on SICK task and\n1.56 on SNLI task for models trained on MNLI.\nThe results suggest that CHILD -TUNING encour-\nages the model to learn more general semantic fea-\ntures during Ô¨Åne-tuning, rather than some super-\nÔ¨Åcial features unique to the training data. Hence,\nthe Ô¨Åne-tuned model can well generalize to differ-\nent datasets, even though their domains are quite\ndifferent from the dataset the model is trained on.\n3.4.2 Task Generalization\nTo justify the generalization ability of the model\nfrom another perspective, we follow the probing\nexperiments from Aghajanyan et al. (2021), which\nÔ¨Årst freezes the representations from the model\ntrained on one task and then only trains a linear\nclassiÔ¨Åer on top of the model for another task.\nIn particular, we Ô¨Åne-tune BERT LARGE on\nMRPC task, and transfer to four other GLUE\ntasks, i.e., CoLA, STS-B, QNLI, and QQP. As Fig-\nure 2 shows, CHILD -TUNING consistently outper-\nforms vanilla Ô¨Åne-tuning on different transferred\ntasks. Compared with vanilla Ô¨Åne-tuning, CHILD -\n3MNLI-m has different domain from MNLI training data.\n4The target tasks may have different label spaces and we\nintroduce the label mapping in Appendix D.\nTUNING F improves 4.58 average score (58.95 ‚Üí\n63.53), while CHILD -TUNING D even gains up to\n7.06 average score improvement (58.95 ‚Üí66.01).\nIn summary, Ô¨Åne-tuning with CHILD -TUNING\ngains better performance when the Ô¨Åne-tuned\nmodel is transferred to another task, demonstrating\nthat CHILD -TUNING can maintain more general-\nizable representations produced by the model than\nvanilla Ô¨Åne-tuning.\n4 Analysis and Discussion\n4.1 Comparison with Prior Methods\nIn this section, we review and compare prior stud-\nies towards effective Ô¨Åne-tuning: 1) Weight De-\ncay (Daum√© III, 2007), which adds theŒª‚à•w‚àíw0‚à•2\npenalty to the loss function, where w0 denotes\nthe pretrained weights; 2) Top-K Tuning, which\nonly Ô¨Åne-tune the top-Klayers of the model with\nother layers freezed. Houlsby et al. (2019) uses\nit as a strong baseline; 3) Mixout (Lee et al.,\n2020), which randomly replaces the parameters\nwith their pretrained weights; 4) RecAdam (Chen\net al., 2020), which is similar to Weight Decay\nwhile its loss weights Œªkeeps changing during Ô¨Åne-\ntuning; 5) Robust Representations through Regular-\nized Finetuning (R3F) (Aghajanyan et al., 2021),\nwhich is rooted in trust region theory. Appendix C\nshows detailed hyperparameter settings.\nWe compare CHILD -TUNING with these meth-\nods based on BERT LARGE, and report the mean\n(max) score results in Table 3, following Lee et al.\n(2020). While all the Ô¨Åne-tuning methods can bring\nimprovements across four different tasks compared\nwith vanilla Ô¨Åne-tuning, CHILD -TUNING achieves\nthe best performance. In detail, among prior Ô¨Åne-\ntuning methods, Mixout and R3F yield the highest\nimprovement with 0.84 and 0.88 average score re-\n9520\nMethods CoLA RTE MRPC STS-B Avg ‚àÜ\nVanilla Fine-tuning‚Ä† 60.60 ( ‚Äì ) 70.40 ( ‚Äì ) 88.00 ( ‚Äì ) 90.00 ( ‚Äì ) 77.25 ‚Äì\nVanilla Fine-tuning 63.13 (64.31) 70.18 (72.56) 90.77 (91.42) 89.61 (90.12) 78.42 0.00\nWeight Decay (Daum√© III, 2007) 63.63 (64.56) 71.99 (74.37) 90.93 (91.70) 89.82 (90.29) 79.09 +0.67\nTop-KTuning (Houlsby et al., 2019) 62.63 (64.06) 70.90 (74.73) 91.09 (92.20) 89.97 (90.15) 78.65 +0.23\nMixout (Lee et al., 2020) 63.60 (64.82) 72.15 (75.45) 91.29 (91.85) 89.99 (90.13) 79.26 +0.84\nRecAdam (Chen et al., 2020) 64.33 (65.33) 71.63 (73.29) 90.85 (92.01) 89.86 (90.42) 79.17 +0.75\nR3F (Aghajanyan et al., 2021) 64.13 (66.32) 72.28 (74.73) 91.18 (91.57) 89.61 (90.12) 79.30 +0.88\nCHILD-TUNINGF 63.71 (66.06) 72.06 (74.73) 91.22 (91.85)90.18 (90.92) 79.29 +0.87\nCHILD-TUNINGD 64.92 (66.03) 73.14 (76.17) 91.42 (92.17) 90.18 (90.64) 79.92 +1.50\nCHILD-TUNINGD+ R3F 65.18 (66.03) 73.43 (76.17) 92.23 (92.65) 90.18 (90.64)‚àó 80.26 +1.84\nTable 3: Comparison between CHILD -TUNING with other Ô¨Åne-tuning methods. We report the mean (max) re-\nsults of 10 random seeds. Results with ‚Ä†are taken from Yang et al. (2019), and others are from our implementation.\nThe task-driven variant, CHILD -TUNING D, achieves the best performance compared with other methods. Integrat-\ning C HILD -TUNING D with other Ô¨Åne-tuning methods like R3F can yield further improvements. Note that since\nR3F is not applicable to regression task, the result on STS-B (marked with ‚àó) is the same as CHILD -TUNING D.\nDataset Vanilla C.T UNINGF C.TUNINGD\nCoLA 47.48 48.44 50.37\nRTE 65.09 65.52 68.09\nMRPC 84.91 85.44 86.49\nSTS-B 81.86 82.25 82.76\nSST2 90.25 90.34 90.39\nQNLI 81.68 83.09 83.42\nQQP 71.30 72.15 71.79\nMNLI 55.72 62.47 62.93\nAvg 72.29 73.71 74.53\nTable 4: Results in low-resource scenarios . C HILD -\nTUNING is better than vanilla Ô¨Åne-tuning in alleviating\noverÔ¨Åtting problems.\nspectively. CHILD -TUNING F has performance on\npar with Mixout and R3F, while CHILD -TUNING D\nachieves 1.50 average score improvement in total.\nMore importantly, CHILD -TUNING is Ô¨Çexible and\northogonal to most Ô¨Åne-tuning methods. Thus, in-\ntegrating CHILD -TUNING with other methods can\nfurther boost the performance. For instance, com-\nbining CHILD -TUNING D with R3F leads to a 1.84\naverage score improvement in total.\nIn short, compared with prior Ô¨Åne-tuning meth-\nods, we Ô¨Ånd that 1) CHILD -TUNING is more effec-\ntive in adapting PLMs to various tasks, especially\nfor the task-driven variant CHILD -TUNING D, and\n2) CHILD -TUNING has the advantage that it is Ô¨Çex-\nible enough to integrate with other methods to po-\ntentially achieve further improvements.\n4.2 Results in Low-resource Scenarios\nFine-tuning a large pretrained model on extremely\nsmall datasets can be very challenging since the\nrisk of overÔ¨Åtting rises (Dodge et al., 2020). Thus,\nin this section, we explore the effect of CHILD -\nTUNING with only a few training examples. To this\nend, we downsample all datasets in GLUE to 1k\ntraining examples and Ô¨Åne-tune BERT LARGE on\nthem.\nAs Table 4 demonstrates, compared with vanilla\nÔ¨Åne-tuning, CHILD -TUNING F improves the aver-\nage score by 1.42, and the improvement is even\nlarger for CHILD -TUNING D, which is up to 2.24.\nIt suggests that although overÔ¨Åtting is quite se-\nvere when the training data is in extreme low-\nresource scenarios, CHILD -TUNING can still ef-\nfectively improve the model performance, espe-\ncially for CHILD -TUNING D since it decreases the\nhypothesis space of the model.\n4.3 What is the Difference Between\nCHILD -TUNING and Model Pruning?\nCHILD -TUNING D detects the most important child\nnetwork in a task-driven way, and only updates\nthis parameters within the child network during\nthe Ô¨Åne-tuning with other parameters freezed. It is\nvery likely to be confused with model pruning (Li\net al., 2017; Zhu and Gupta, 2018; Lin et al., 2020),\nwhich also detects a subnetwork within the model\n(but then removes the other parameters).\nActually, CHILD -TUNING and model pruning\nare different in both the objectives and methods .\nRegarding objectives, model pruning aims at im-\nproving the inference efÔ¨Åciency and maintaining\n9521\nMethods CoLA RTE MRPC STS-B\nVanilla 63.13 70.18 90.77 89.61\nPrune 0.00 51.12 81.40 45.63\nRandom 63.23 70.69 90.83 89.67\nLowest Info. 60.33 59.86 83.82 88.52\nC.TUNINGD 64.92 72.78 91.26 90.18\nTable 5: Ablation study of CHILD -TUNING D. Prune:\nAbandon parameters out of the child network. Random:\nRandomly choose a child network and keep it un-\nchanged during Ô¨Åne-tuning. Lowest Info. : Detect a\nchild network with lowest Fisher information instead.\nthe performance at the same time, while CHILD -\nTUNING is proposed to address the overÔ¨Åtting prob-\nlem and improve the generalization ability for large-\nscale language models during Ô¨Åne-tuning. Regrad-\ning methods, model pruning abandons the unimpor-\ntant parameters during inference, while the param-\neters that do not belong to the child network are\nstill reserved for CHILD -TUNING during training\nand inference. In this way, the knowledge of the\nnon-child network hidden in the pretrained weights\nwill be fully utilized.\nTo better illustrate the effectiveness of CHILD -\nTUNING D compared to model pruning, we set all\nthe parameters not belonging to the child network\nto zero, which is referred to as Prune in Table 5.\nIt shows that, once we abandon parameters out\nof the child network, the score dramatically de-\ncreases by 33.89 points averaged on four tasks\n(CoLA/RTE/MRPC/STS-B), and the model even\ncollapses on CoLA task. It also suggests that be-\nsides parameters in child network, those in the non-\nchild network are also necessary since they can\nprovide general knowledge learned in pretraining.\n4.4 Is the Task-Driven Child Network Really\nthat Important to the Target Task?\nCHILD -TUNING D detects the task-speciÔ¨Åc child\nnetwork by means of choosing parameters with the\nhighest Fisher information towards the downstream\ntask data. In this section, we exlore whether the\ndetected task-driven child network is really that\nimportant to the task.\nTo this end, we introduce two ablation studies\nfor CHILD -TUNING D: 1) Random: We randomly\nchoose a child network and keep it unchanged dur-\ning Ô¨Åne-tuning; 2) Lowest Info.: We choose those\nparameters with lowest Fisher information as the\nchild network, contrasted to the highest Fisher in-\nCoLA RTE MRPCSTS-BMNLI QNLI QQP\nRTE\nMRPC\nSTS-B\nMNLI\nQNLI\nQQP\nSST2\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nFigure 3: The overlapping ratio among task-driven\nchild networks among GLUE tasks.\nformation adopted in CHILD -TUNING D.\nAs shown in Table 5, choosing the child network\nrandomly can even outperform vanilla Ô¨Åne-tuning,\nwith 0.18 average score improvement. It supports\nour claim that there is no need to update all pa-\nrameters of the large PLMs, and decreasing the\nhypothesis space can reduce the risk of overÔ¨Åtting.\nHowever, it is still worth Ô¨Ånding a proper child\nnetwork to further boost the performance. If we\nchoose parameters with the lowest Fisher infor-\nmation (Lowest Fisher), the average score is dra-\nmatically decreased by 6.65 compared with choos-\ning with the highest Fisher information adopted in\nCHILD -TUNING D. Hence, we can conclude that\nthe child network detected by CHILD -TUNING D is\nindeed important to the downstream task.\n4.5 What is the Relationship among Child\nNetworks for Different Tasks?\nAs the task-driven child networks are correlated\nwith the tasks, we further explore the relationship\namong child networks for different tasks. To this\nend, we visualize the overlapping rate among dif-\nferent task-driven child networks, where we use the\nJaccard similarity coefÔ¨Åcient, |Ci‚à©Cj|\n|Ci‚à™Cj|, to calculate\nthe overlapping rate between task iand j.\nFigure 3 shows the overlap among GLUE tasks.\nAs we expected, similar tasks tend to have higher\noverlapping ratios of child network. For example,\nthe overlapping ratio among NLI tasks is remark-\nably higher than others, such as RTE and QNLI,\nQNLI and MNLI. For different kinds of tasks, their\noverlapping ratio is relatively lower, such as CoLA\nand MRPC. It is also interesting to Ô¨Ånd that the\ntask-driven child network for SST2 overlaps less\nwith other tasks except CoLA, even though SST2\n9522\nand CoLA is not so similar. The reason may be that\nboth SST2 and CoLA belongs to a single sentence\nclassiÔ¨Åcation task, while others are in a different\nformat of sentence-pair classiÔ¨Åcation tasks.\n5 Related Work\nExplosion of PLMs. There has been an explo-\nsion of studies on Pretrained Language Models\n(PLMs). Devlin et al. (2019) propose BERT that is\npretrained on large quantities of unannotated cor-\npus with self-supervised tasks. Many PLMs also\nemerged such as GPT-2 (Radford et al., 2018), GPT-\n3 (Brown et al., 2020), ELECTRA (Clark et al.,\n2020), XLNet (Yang et al., 2019), RoBERTa (Liu\net al., 2019), and BART (Lewis et al., 2020). The\nnumber of parameters of PLMs also explodes.\nBERTLARGE has 340 millions of parameters, and\nthe number for GPT-3 is even up to 175 billions.\nEffective and generalizable Ô¨Åne-tuning. With\na mass of parameters, Ô¨Åne-tuning large PLMs tend\nto achieve degenerated performance due to overÔ¨Åt-\nting and have poor generalization ability, especially\non small datasets (Devlin et al., 2019; Phang et al.,\n2018; Lee et al., 2020). Therefore, different Ô¨Åne-\ntuning techniques have been proposed. Some of\nthem utilize the pretrained weights to regularize the\ndeviation of the Ô¨Åne-tuned model (Lee et al., 2020;\nDaum√© III, 2007; Chen et al., 2020), while others\ncompress the output information (Mahabadi et al.,\n2021) or injects noise into the input (Jiang et al.,\n2020; Aghajanyan et al., 2021). Moreover, Zhang\net al. (2021) and Mosbach et al. (2021) point out\nthat the omission of bias correction in the Adam\noptimizer used in Devlin et al. (2019) is also re-\nsponsible for the degenerated results.\nOrthogonal to these methods, CHILD -TUNING\naddress the problems by detecting the child net-\nwork within the model in a task-free or task-driven\nway. It only updates parameters within the child\nnetwork via a gradient mask, which is proved to be\neffective in adapting large PLMs to various tasks,\nalong with better generalization ability.\nParameter-efÔ¨Åcient Fine-tuning. There are\nalso studies focusing on parameter-efÔ¨Åcient\nÔ¨Åne-tuning, for example, the adapter-based\nmethods (Houlsby et al., 2019; Pfeiffer et al.,\n2020; Karimi Mahabadi et al., 2021), and the\nDiff-Pruning method (Guo et al., 2021). However,\nour CHILD -TUNING is different from this line\nof works. Firstly, they aim at Ô¨Åne-tuning as few\nas possible parameters to maintain performance,\nwhile we target effective and generalizable\nÔ¨Åne-tuning. Secondly, Diff-Pruning sparsiÔ¨Åes\ndiff-vector with gradient estimators, and adapter-\nbased methods Ô¨Åne-tune new added module\nduring training, while we detect the child network\ninside the model without extra parameters and\nonly need to calculate the FIM before training\nfor CHILD -TUNING D. Finally, we consistently\noutperform vanilla Ô¨Åne-tuning by a large margin,\nwhile they achieve competitive performance with\nfull model training.\n6 Conclusion\nTo mitigate the overÔ¨Åtting problem and improve\ngeneralization for Ô¨Åne-tuning large-scale PLMs, we\npropose a straightforward yet effective Ô¨Åne-tuning\ntechnique, CHILD -TUNING , which only updates\nthe child network during Ô¨Åne-tuning via strategi-\ncally masking out the gradients of the non-child\nnetwork. Two variants are introduced, CHILD -\nTUNING F and CHILD -TUNING D, which detect the\nchild network in a task-free and task-driven way, re-\nspectively. Extensive experiments on various down-\nstream tasks show that both of them can outperform\nvanilla Ô¨Åne-tuning and prior works by large gains\namong four different pretrained language models,\nand meanwhile largely enhance the generalization\nability of the Ô¨Åne-tuned models. Since CHILD -\nTUNING is orthogonal to most prior Ô¨Åne-tuning\ntechniques, integrating CHILD -TUNING with them\ncan further boost the performance.\nAcknowledgments\nThis paper is supported by the National Key\nResearch and Development Program of China\nunder Grant No. 2020AAA0106700, the Na-\ntional Science Foundation of China under Grant\nNo.61936012 and 61876004.\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021. Better Ô¨Åne-tuning by reducing representa-\ntional collapse. In International Conference on\nLearning Representations (ICLR).\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\n9523\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems (NeurIPS).\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations (ICLR).\nHal Daum√© III. 2007. Frustratingly easy domain adap-\ntation. In Proceedings of the 45th Annual Meeting of\nthe Association of Computational Linguistics (ACL).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL-HLT).\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah A. Smith.\n2020. Fine-tuning pretrained language models:\nWeight initializations, data orders, and early stop-\nping. arXiv preprint, arXiv:2002.06305.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and\nBehnam Neyshabur. 2021. Sharpness-aware mini-\nmization for efÔ¨Åciently improving generalization. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nYichen Gong, Heng Luo, and Jian Zhang. 2018. Nat-\nural language inference over interaction space. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nHenry Gouk, Timothy M. Hospedales, and Massim-\niliano Pontil. 2020. Distance-based regularisation\nof deep networks for Ô¨Åne-tuning. arXiv preprint ,\narXiv:2002.08253.\nDemi Guo, Alexander Rush, and Yoon Kim. 2021.\nParameter-efÔ¨Åcient transfer learning with diff prun-\ning. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics\n(ACL).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efÔ¨Åcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning (ICML).\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xi-\naodong Liu, Jianfeng Gao, and Tuo Zhao. 2020.\nSMART: Robust and efÔ¨Åcient Ô¨Åne-tuning for pre-\ntrained natural language models through principled\nregularized optimization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021. Parameter-\nefÔ¨Åcient multi-task Ô¨Åne-tuning for transformers via\nshared hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nNitish Shirish Keskar, Dheevatsa Mudigere, Jorge No-\ncedal, Mikhail Smelyanskiy, and Ping Tak Peter\nTang. 2017. On large-batch training for deep learn-\ning: Generalization gap and sharp minima. In 5th\nInternational Conference on Learning Representa-\ntions (ICLR).\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Proceedings of the Thirty-\nSecond Conference on ArtiÔ¨Åcial Intelligence (AAAI).\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR).\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A. Rusu, Kieran Milan, John Quan, Tiago Ra-\nmalho, Agnieszka Grabska-Barwinska, Demis Hass-\nabis, Claudia Clopath, Dharshan Kumaran, and Raia\nHadsell. 2016. Overcoming catastrophic forgetting\nin neural networks. In Proceedings of the National\nAcademy of Sciences (PNAS).\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to Ô¨Ånetune\nlarge-scale pretrained language models. In 8th Inter-\nnational Conference on Learning Representations\n(ICLR).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL).\nHao Li, Asim Kadav, Igor Durdanovic, Hanan Samet,\nand Hans Peter Graf. 2017. Pruning Ô¨Ålters for ef-\nÔ¨Åcient convnets. In International Conference on\nLearning Representations (ICLR).\n9524\nTao Lin, Sebastian U. Stich, Luis Barba, Daniil\nDmitriev, and Martin Jaggi. 2020. Dynamic model\npruning with feedback. In International Conference\non Learning Representations (ICLR).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint, arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations (ICLR).\nRabeeh Karimi Mahabadi, Yonatan Belinkov, and\nJames Henderson. 2021. Variational information\nbottleneck for effective low-resource Ô¨Åne-tuning. In\nInternational Conference on Learning Representa-\ntions (ICLR).\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of com-\npositional distributional semantic models. In Pro-\nceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC).\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of Ô¨Åne-tuning\nBERT: Misconceptions, explanations, and strong\nbaselines. In International Conference on Learning\nRepresentations (ICLR).\nJonas Pfeiffer, Ivan Vuli ¬¥c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nJason Phang, Thibault F√©vry, and Samuel R. Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint, arXiv:1811.01088.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nXu Sun, Zhiyuan Zhang, Xuancheng Ren, Ruixuan\nLuo, and Liangyou Li. 2020. Exploring the vulnera-\nbility of deep neural networks: A study of parameter\ncorruption. arXiv preprint, arXiv:2006.05620.\nM. Tu, V . Berisha, Y . Cao, and J. Seo. 2016a. Reduc-\ning the model order of deep neural networks using\ninformation theory. In 2016 IEEE Computer Soci-\nety Annual Symposium on VLSI (ISVLSI).\nM. Tu, V . Berisha, M. Woolf, J. Seo, and Y . Cao. 2016b.\nRanking the parameters of deep neural networks us-\ning the Ô¨Åsher information. In 2016 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations (EMNLP).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems (NeurIPS).\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample BERT Ô¨Åne-tuning. In International Confer-\nence on Learning Representations (ICLR).\nMichael Zhu and Suyog Gupta. 2018. To prune, or\nnot to prune: Exploring the efÔ¨Åcacy of pruning for\nmodel compression. In International Conference on\nLearning Representations (ICLR).\nA GLUE Benchmark Introduction\nIn this paper, we conduct experiments on8 datasets\nin GLUE benchmark (Wang et al., 2019) as shown\nin Table 6, including single-sentence tasks, infer-\nence tasks, and similarity and paraphrase tasks.\nNote that the original GLUE benchmark includes 9\ndifferent datasets in total. However, there are some\nissues with the construction of the WNLI dataset5.\nTherefore most studies exclude this dataset (De-\nvlin et al., 2019; Dodge et al., 2020) and we follow\nthem. The metrics we report for each dataset are\nalso illustrated in Table 6.\nB Settings for Different Pretrained\nLanguage Models\nIn this paper, we Ô¨Åne-tune different large pretrained\nlanguage models with CHILD -TUNING , including\nBERTLARGE6, XLNetLARGE7, RoBERTaLARGE8,\n5https://gluebenchmark.com/faq\n6https://huggingface.co/\nbert-large-cased/tree/main\n7https://huggingface.co/\nxlnet-large-cased/tree/main\n8https://huggingface.co/roberta-large/\ntree/main\n9525\nDataset #Train #Dev Metrics\nSingle-sentence Tasks\nCoLA 8.5k 1.0k Matthews Corr\nSST-2 67k 872 Accuracy\nInference\nRTE 2.5k 277 Accuracy\nQNLI 105k 5.5k Accuracy\nMNLI 393k 9.8k Accuracy\nSimilarity and Paraphrase\nMRPC 3.7k 408 F1\nSTS-B 5.7k 1.5k Spearman Corr\nQQP 364k 40k F1\nTable 6: Statistics and metrics of eight datasets used in\nthis paper form GLUE benchmark.\nand ELECTRALARGE9. The training epochs/steps,\nbatch size, and warmup steps are listed in Ta-\nble 7. We use AdamW (Loshchilov and Hutter,\n2019) optimizer, and set Œ≤1 = 0 .9, Œ≤2 = 0 .999,\nœµ = 1 e-6. We clip the gradients with a maxi-\nmum norm of 1, and the maximum sequence length\nis set as 128. For CHILD -TUNING F, we uses\npF = {0.2,0.3,0.4}and re-scale the gradients to\nensure the gradients after CHILD -TUNING F are\nunbiased. For CHILD -TUNING D, we use pD =\n{0.1,0.2,0.3}. We use grid search for learning\nrate from {1e-5,2e-5,..., 1e-4}. We conduct all\nthe experiments on a single GTX-3090 GPU.\nThese pretrained models are all Transformer-\nbased. XLNet (Yang et al., 2019) is an autore-\ngressive pretrained language model with token per-\nmutations. It generates tokens in an autoregressive\nway while can still capture bidirectional context\ninformation. RoBERTa (Liu et al., 2019) is a ro-\nbustly optimized version of BERT. It uses a dy-\nnamic masking mechanism, larger batch size, and\nlonger training times, and it also abandons the next\nsentence prediction task. ELECTRA (Clark et al.,\n2020) pretrains the model with a generator and\na discriminator. The discriminator is trained to\ndistinguish whether the token is generated by the\ngenerator or the original token.\n9https://huggingface.co/google/\nelectra-large-discriminator/tree/main\nC Settings for Other Fine-tuning\nMethods\nWe compare Child-tuning with several other regu-\nlarization approaches in our paper. In this section,\nwe simply introduce these approaches and their\nhyperparameters settings.\nWeight Decay Daum√© III (2007) proposes to\nadds a penalty item to the loss function to regu-\nlate the L2 distance between Ô¨Åne-tuned models and\nthe pretrained models. Therefore, the loss function\nis as follows:\nL(w) = LCE(w) + ŒªWD‚à•w ‚àíw0‚à•2]\nWe grid search the optimal ŒªWD from{\n10,1,10‚àí1,10‚àí2,10‚àí3,10‚àí4}\n.\nTop-K Fine-tuning Top-K Fine-tuning is a\ncommon method and Houlsby et al. (2019) uses it\nas a strong baseline. Top-KFine-tuning only updat-\ness the top K layers along with the classiÔ¨Åcation\nlayer, while freezing all the other bottom layers.\nWe grid search the optimal Kfrom {0,3,6,12}in\nour paper.\nMixout Lee et al. (2020) randomly replace\nthe parameters with its pretrained weights with a\ncertainly probability pduring Ô¨Åne-tuning, which\naims to minimize the deviation of the Ô¨Åne-\ntuned model towards the pretrained weights. In\nour paper, we grid search the optimal p from\n{0.1,0.2,..., 0.8}. We use the implementa-\ntion in https://github.com/bloodwass/\nmixout.\nRecAdam Chen et al. (2020) proposes a new\noptimizer RecAdam for Ô¨Åne-tuning, which can be\nconsidered as an advanced version of Weight De-\ncay, because the coefÔ¨Åcient of two different loss\nitems are changed as the training progresses. The\nfollowing equations demonstrate the new loss func-\ntion, where kand t0 are controlling hyperparame-\nters and tis the current training step.\nL(w) = ŒªRec(t)LCE(w)\n+ (1 ‚àíŒªRec(t))‚à•w ‚àíw0‚à•2\nŒªRec(t) = 1\n1 + exp(‚àík¬∑(t‚àít0))\nWe grid search thekfrom {0.05,0.1,0.2,0.5,1.0},\nand t0 from {50,100,250,500}. We use the\nimplementation in https://github.com/\nSanyuan-Chen/RecAdam.\n9526\nModel Dataset Batch Size Training Epochs/Steps Warmup Ratio/Steps\nBERT all 16 3 epochs 10%\nXLNet\nCoLA 128 1200 steps 120 steps\nRTE 32 800 steps 200 steps\nMRPC 32 800 steps 200 steps\nSTS-B 32 3000 steps 500 steps\nRoBERTa\nCoLA 16 5336 steps 320 steps\nRTE 16 2036 steps 122 steps\nMRPC 16 2296 steps 137 steps\nSTS-B 16 3598 steps 214 steps\nELECTRA\nCoLA 32 3 epochs 10%\nRTE 32 10 epochs 10%\nMRPC 32 3 epochs 10%\nSTS-B 32 10 epochs 10%\nTable 7: Hyperparameters settings for different pretrained models on variant tasks. These settings are reported in\nthe their ofÔ¨Åcial repository for best practice.\nRobust Representations through Regularized\nFine-tuning (R3F) Aghajanyan et al. (2021)\npropose R3F for Ô¨Åne-tuning based on trust region\ntheory, which adds noise into the sequence input\nembedding and tries to minimize the symmetrical\nKL divergence between probability distributions\ngiven original input and noisy input. The loss func-\ntion of R3F is as follows:\nL(w) = LCE(w) + ŒªR3FKLS(f(x)||f(x+ z))\ns.t. z ‚àºN(0,œÉ2I) or z ‚àºU(‚àíœÉ,œÉ)\nwhere f(¬∑) denotes the model and z denotes\nthe noise sampled from either normal distribu-\ntion or uniform distribution controlled by hyper-\nparameter œÉ, and KLS(x||y) = KL(x||y) +\nKL(y||x). We use both normal and unform\ndistribution, ŒªR3F = 1 , and grid search the œÉ\nfrom {0.1,0.5,1.0,5.0}. We use the implemen-\ntation in https://github.com/pytorch/\nfairseq/tree/master/examples/rxf.\nD Label Mapping in Task Generalization\nMNLI and SNLI datasets contain three labels, i.e.,\nentailment, neutral, and contradiction. For SciTail,\nit only has two labels, entailment and neutral, and\ntherefore we map both neutral and contradiction in\nsource label space to neutral in target label space\nfollowing Mahabadi et al. (2021). For QQP, it\nhas two labels, duplicate and not duplicate, and\nGong et al. (2018) interpret them as entailment and\nneutral respectively. We follow Gong et al. (2018)\nand use the same mapping strategy as SciTail.\nE Theoretical Details\nWe theoretically justify the effectiveness ofCHILD -\nTUNING F. Assume CHILD -TUNING F reserves\ngradients with probability pF ‚àà(0,1], and we sim-\nply use p to denote pF in the following content.\nTheorem 1 shows the variance of gradients is a\nstrictly decreasing function ofp. When p= 1, it de-\ngenerates into normal Ô¨Åne-tuning methods. There-\nfore, CHILD -TUNING F can improve the variance\nof the gradients of the model. Next, Theorem 2\nshows that with higher variance, the model can\nconverge to more Ô¨Çat local minima (smaller œÅin\nTheorem 2). Inspired by studies that show Ô¨Çat min-\nima tends to generalize better (Keskar et al., 2017;\nSun et al., 2020; Foret et al., 2021), we can further\nprove CHILD -TUNING F decreases the generaliza-\ntion error bound.\nTheorem 1. Suppose Ldenotes the loss function\non the parameter w, for multiple data instances in\nthe training set x ‚àºS, the gradients obey a Gaus-\nsian distribution N( ‚àÇL\n‚àÇw,œÉ2\ngIk). For a randomly\nsampled batch B ‚àºS, when the learning algo-\nrithm is SGD with learning rate Œ∑, the reserving\nprobability of the CHILD -TUNING F is p, then the\n9527\nmean and covariance of the update ‚àÜw are,\nE[‚àÜw] = ‚àíŒ∑‚àÇL\n‚àÇw (11)\nŒ£[‚àÜw] = Œ∑2œÉ2\ngIk\np|B| + (1 ‚àíp)Œ∑2diag{‚àÇL\n‚àÇw}2\np\n(12)\nwhere Œ£ is the covariance matrix and diag (x) is\nthe diagonal matrix of the vector x.\nSpecially, when w is a local minima, E[‚àÜw] =\n0k,Œ£[‚àÜw] = œÉ2Ik and œÉ2 =\nŒ∑2œÉ2\ng\np|B| is a strictly\ndecreasing function of p.\nTheorem 2. Suppose Ldenotes the expected error\nrate loss function; w0 denotes the pretrained pa-\nrameter; kis the number of parameters; w denotes\nthe local minima the algorithm converges to; H is\nthe Hessian matrix on w and œÅis its greatest eigen-\nvalue; Fk is the cumulative distribution function of\nthe œá2(k) distribution.\nIf the next update of the algorithm ‚àÜw ‚àº\nN(0k,œÉ2Ik) and the training loss increases more\nthan œµwith probability Œ¥, we assume the algorithm\nwill escape the local minima w. When the follow-\ning bound holds, the algorithm can converge to the\nlocal minima w, with higher order inÔ¨Ånity omitted,\nœÅ‚â§ 2œµ\nF‚àí1\nk (1 ‚àíŒ¥)œÉ2 (13)\nSuppose the prior over parameters after training\nis P = N(w0,œÉ2\n0Ik), the following generalization\nerror bound holds with probability 1- Œ¥ over the\nchoice of training set S‚àºD ,\nbound(w) ‚â§(kœÉ2\n0 ‚àí‚à•w ‚àíw0‚à•2)œµ\nkF‚àí1\nk (1 ‚àíŒ¥)œÉ2 + R (14)\nwhere bound(w) = LS(w) ‚àíLD(w), R =‚àö\nklog\n(\n1+\nk‚à•w‚àíw0‚à•2\n2\nkœÉ2\n0‚àí‚à•w‚àíw0‚à•2\n(\n1+\n‚àö\nlog |S|\nk\n)2)\n+4 log|S|\nŒ¥\n2(|S|‚àí1) ,\nwith higher order inÔ¨Ånity omitted.\nE.1 Proof of Theorem 1\nProof. Suppose g(i) is the gradient of data instance\nx(i),(1 ‚â§ i ‚â§ |B|), then g(i) ‚àº N( ‚àÇL\n‚àÇw,œÉ2\ngIk).\nThen, deÔ¨Åne g =\n|B|‚àë\ni=1\ng(i)\n|B|, we have\n‚àÜw = ‚àíŒ∑\n|B|‚àë\ni=1\ng(i)\n|B|‚äôM = ‚àíŒ∑g ‚äôM (15)\nConsider g, we have\nE[g] = ‚àÇL\n‚àÇw,Œ£[g] = œÉ2\ngIk\n|B| (16)\nSuppose ÀÜ g= g\np ‚äôM, therefore,\nE[ÀÜ g] = p\np √ó‚àÇL\n‚àÇw = ‚àÇL\n‚àÇw (17)\nSuppose ÀÜgi,gi are the i-th dimension of ÀÜ g,g, we\nhave\nD[ÀÜgi] = E[ÀÜg2\ni] ‚àí(E[ÀÜgi])2 (18)\n= pE[(gi\np)2] ‚àí(E[ÀÜgi])2 (19)\n= E[g2\ni]\np ‚àí(E[ÀÜgi])2 (20)\n= (E[gi])2 + D[gi]\np ‚àí(E[ÀÜgi])2 (21)\n= D[gi]\np + (1 ‚àíp)(E[ÀÜgi])2\np (22)\nTherefore,\nŒ£[ÀÜ g] = œÉ2\ngIk\np|B|+ (1 ‚àíp)diag{E[g]}2\np (23)\nTherefore,\nE[‚àÜw] = ‚àíŒ∑‚àÇL\n‚àÇw (24)\nŒ£[‚àÜw] = Œ∑2œÉ2\ngIk\np|B| + (1 ‚àíp)Œ∑2diag{‚àÇL\n‚àÇw}2\np\n(25)\nSpecially, when w is a local minima, ‚àÇL\n‚àÇw =\n0k. Therefore, E[‚àÜw] = 0k,Œ£[‚àÜw] = œÉ2Ik\nand œÉ2 =\nŒ∑2œÉ2\ng\np|B| is a strictly decreasing function of\np.\nE.2 Proof of Theorem 2\nProof. We Ô¨Årst prove Eq. 13. Apply a Taylor ex-\npansion on training loss L, notice that ‚àáwL(w) =\n0k since w is a local minima. When the algorithm\ncan escape the local minima w, with higher order\ninÔ¨Ånity omitted, we have,\nœµ‚â§L(w + v) ‚àíL(w) (26)\n=vT‚àáwL(w) + 1\n2vTHv + o(‚à•v‚à•2\n2) (27)\n‚â§œÅ‚à•v‚à•2\n2\n2 + o(‚à•v‚à•2\n2) = œÅ‚à•v‚à•2\n2\n2 (28)\n9528\nIf the probability of escaping, Pesc, we have\nPesc = P(L(w + ‚àÜw) ‚àíL(w) ‚â•œµ) (29)\n‚â§P(œÅ‚à•‚àÜw‚à•2\n2\n2 ‚â•œµ) (30)\n= P(‚à•‚àÜw\nœÉ ‚à•2\n2 ‚â• 2œµ\nœÅœÉ2 ) (31)\nnamely, P(‚à•‚àÜw\nœÉ ‚à•2\n2 ‚â§ 2œµ\nœÅœÉ2 ) ‚â§1 ‚àíPesc.\nSince ‚àÜw\nœÉ ‚àºN(0k,Ik), ‚à•‚àÜw\nœÉ ‚à•2\n2 ‚àºœá2(k), we\nhave,\nP(‚à•‚àÜw\nœÉ ‚à•2\n2 ‚â§ 2œµ\nœÅœÉ2 ) = Fk( 2œµ\nœÅœÉ2 ) (32)\nwhen Eq. 13 holds,\nP(‚à•‚àÜw\nœÉ ‚à•2\n2 ‚â§ 2œµ\nœÅœÉ2 ) = Fk( 2œµ\nœÅœÉ2 ) (33)\n‚â•Fk(F‚àí1\nk (1 ‚àíŒ¥)) = 1 ‚àíŒ¥ (34)\nTherefore, Pesc ‚â§1 ‚àíP(‚à•‚àÜw\nœÉ ‚à•2\n2 ‚â§ 2œµ\nœÅœÉ2 ) ‚â§Œ¥.\nThe algorithm will not escape the local minima w\nand can converge to the local minima w.\nTo prove Eq. 14, we introduce Lemma 1 in pa-\nper Foret et al. (2021), which is Theorem 2 in the\npaper.\nLemma 1. Suppose d > 0, the prior over pa-\nrameters is P = N(wP,œÉ2\nPIk) and œÉ2\nP = d2 +\n‚à•w‚àíwP‚à•2\nk , the following bound holds with proba-\nbility 1-Œ¥over the choice of training set S‚àºD ,\nLD(w) ‚â§ max\n‚à•‚àÜw‚à•2‚â§d\nLS(w + ‚àÜw) + R (35)\nwhere k denotes the number of parameters and\nR =\n‚àö\nklog\n(\n1+\n‚à•w‚àíwP‚à•2\n2\nd2\n(\n1+\n‚àö\nlog |S|\nk\n)2)\n+4 log|S|\nŒ¥\n2(|S|‚àí1) ,\nwith higher order inÔ¨Ånity omitted.\nIn Lemma 1, when we set wP = w0 and\nœÉP = œÉ0, we have d2 = œÉ2\n0 ‚àí‚à•w‚àíw0‚à•2\nk and R=‚àö\nklog\n(\n1+\nk‚à•w‚àíw0‚à•2\n2\nkœÉ2\n0‚àí‚à•w‚àíw0‚à•2\n(\n1+\n‚àö\nlog |S|\nk\n)2)\n+4 log|S|\nŒ¥\n2(|S|‚àí1) .\nWith higher order inÔ¨Ånity omitted, we have\nmax\n‚à•‚àÜw‚à•2‚â§d\nLS(w + ‚àÜw) = LS(w) + œÅd2\n2 (36)\n‚â§(kœÉ2\n0 ‚àí‚à•w ‚àíw0‚à•2)œµ\nkF‚àí1\nk (1 ‚àíŒ¥)œÉ2 (37)\nTherefore, the following generalization error\nbound holds,\nbound(w) ‚â§(kœÉ2\n0 ‚àí‚à•w ‚àíw0‚à•2)œµ\nkF‚àí1\nk (1 ‚àíŒ¥)œÉ2 + R (38)\nwhere higher order inÔ¨Ånity is omitted and R=‚àö\nklog\n(\n1+\nk‚à•w‚àíw0‚à•2\n2\nkœÉ2\n0‚àí‚à•w‚àíw0‚à•2\n(\n1+\n‚àö\nlog |S|\nk\n)2)\n+4 log|S|\nŒ¥\n2(|S|‚àí1) .",
  "topic": "Fine-tuning",
  "concepts": [
    {
      "name": "Fine-tuning",
      "score": 0.832921028137207
    },
    {
      "name": "Computer science",
      "score": 0.8155754208564758
    },
    {
      "name": "Generalization",
      "score": 0.7168121337890625
    },
    {
      "name": "Language model",
      "score": 0.667386531829834
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6438398361206055
    },
    {
      "name": "Task (project management)",
      "score": 0.607032060623169
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5953903198242188
    },
    {
      "name": "Process (computing)",
      "score": 0.5626585483551025
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5204811096191406
    },
    {
      "name": "Machine learning",
      "score": 0.467204749584198
    },
    {
      "name": "Transfer of learning",
      "score": 0.4220658540725708
    },
    {
      "name": "Speech recognition",
      "score": 0.3630982041358948
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ]
}