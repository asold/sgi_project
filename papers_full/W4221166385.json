{
  "title": "TubeDETR: Spatio-Temporal Video Grounding with Transformers",
  "url": "https://openalex.org/W4221166385",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2971414680",
      "name": "Antoine Yang",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A2677206639",
      "name": "Antoine Miech",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A825683998",
      "name": "Josef Sivic",
      "affiliations": [
        "Czech Technical University in Prague",
        "Institute of Informatics of the Slovak Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2170872680",
      "name": "Ivan Laptev",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    },
    {
      "id": "https://openalex.org/A2111851554",
      "name": "Cordelia Schmid",
      "affiliations": [
        "Institut national de recherche en informatique et en automatique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6771626834",
    "https://openalex.org/W6789753369",
    "https://openalex.org/W2963735856",
    "https://openalex.org/W2558535589",
    "https://openalex.org/W2891456603",
    "https://openalex.org/W2963017553",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W6775188310",
    "https://openalex.org/W2798708692",
    "https://openalex.org/W3174004334",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964232540",
    "https://openalex.org/W3206836360",
    "https://openalex.org/W3182683290",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6780924892",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2975706270",
    "https://openalex.org/W6784761581",
    "https://openalex.org/W3035590142",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3179041377",
    "https://openalex.org/W3172908893",
    "https://openalex.org/W2505639562",
    "https://openalex.org/W2935563816",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6694395031",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2952524542",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W2890502146",
    "https://openalex.org/W3034772468",
    "https://openalex.org/W6775970589",
    "https://openalex.org/W2779827764",
    "https://openalex.org/W2998355566",
    "https://openalex.org/W3127384563",
    "https://openalex.org/W3186024896",
    "https://openalex.org/W3105232955",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2782140824",
    "https://openalex.org/W6785783668",
    "https://openalex.org/W3166712493",
    "https://openalex.org/W2904910963",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2998495542",
    "https://openalex.org/W3178418424",
    "https://openalex.org/W2963095467",
    "https://openalex.org/W6774054309",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W3025323587",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2982651953",
    "https://openalex.org/W3204090293",
    "https://openalex.org/W6767211374",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6793736971",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2962764817",
    "https://openalex.org/W2962869524",
    "https://openalex.org/W3035640828",
    "https://openalex.org/W6797109355",
    "https://openalex.org/W2770129969",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2963393391",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W2795840542",
    "https://openalex.org/W4214516465",
    "https://openalex.org/W2948958195",
    "https://openalex.org/W3034743747",
    "https://openalex.org/W2997429269",
    "https://openalex.org/W2903901502",
    "https://openalex.org/W6803039797",
    "https://openalex.org/W2904824998",
    "https://openalex.org/W6639432524",
    "https://openalex.org/W6776721752",
    "https://openalex.org/W2963521717",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W2963843782",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2963042258",
    "https://openalex.org/W2799263800",
    "https://openalex.org/W2960655175",
    "https://openalex.org/W4214490042",
    "https://openalex.org/W3197457832",
    "https://openalex.org/W6773226109",
    "https://openalex.org/W2571175805",
    "https://openalex.org/W2986755220",
    "https://openalex.org/W6781547866",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3094751268",
    "https://openalex.org/W4287122452",
    "https://openalex.org/W2964284374",
    "https://openalex.org/W3214642103",
    "https://openalex.org/W3002552512",
    "https://openalex.org/W3110435696",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3040350016",
    "https://openalex.org/W3006683367",
    "https://openalex.org/W4226163860",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3168640669",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W4287812455",
    "https://openalex.org/W2997591391",
    "https://openalex.org/W3101429639",
    "https://openalex.org/W4287255256",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2979933490",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W4287125738",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962703144",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W4287757777",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query. This is a challenging task that requires the joint and efficient modeling of temporal, spatial and multi-modal interactions. To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection. Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and (ii) a space-time decoder that jointly performs spatio-temporal localization. We demonstrate the advantage of our proposed components through an extensive ablation study. We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging VidSTG and HC-STVG benchmarks. Code and trained models are publicly available at https://antoyang.github.io/tubedetr.html.",
  "full_text": "TubeDETR: Spatio-Temporal Video Grounding with Transformers\nAntoine Yang1,2, Antoine Miech3, Josef Sivic4, Ivan Laptev1,2, Cordelia Schmid1,2\n1Inria Paris 2DÂ´epartement dâ€™informatique de lâ€™ENS, CNRS, PSL Research University 3DeepMind 4CIIRC CTU Prague\nhttps://antoyang.github.io/tubedetr.html\nNote that vIoU results are updated compared to the CVPRâ€™22 camera-ready version.\nAbstract\nWe consider the problem of localizing a spatio-temporal\ntube in a video corresponding to a given text query. This is\na challenging task that requires the joint and efficient mod-\neling of temporal, spatial and multi-modal interactions. To\naddress this task, we propose TubeDETR, a transformer-\nbased architecture inspired by the recent success of such\nmodels for text-conditioned object detection. Our model\nnotably includes: (i) an efficient video and text encoder\nthat models spatial multi-modal interactions over sparsely\nsampled frames and (ii) a space-time decoder that jointly\nperforms spatio-temporal localization. We demonstrate the\nadvantage of our proposed components through an exten-\nsive ablation study. We also evaluate our full approach on\nthe spatio-temporal video grounding task and demonstrate\nimprovements over the state of the art on the challenging\nVidSTG and HC-STVG benchmarks.\n1. Introduction\nGrounding natural language in visual content is a fun-\ndamental skill to build powerful and explainable vision and\nlanguage models. In particular, understanding the associa-\ntion of language with spatial regions and temporal bound-\naries in videos is particularly important to analyze and im-\nprove multi-modal video models. This goes beyond associ-\nating a global visual representation with a textual represen-\ntation [57,62], as it requires to reason about detailed spatio-\ntemporal visual representations and their association with\nnatural language, as illustrated in Figure 1.\nSpatio-temporal video grounding, recently introduced\nin [102], is an interesting and challenging task that lies at\nthe intersection of visual grounding [33, 59, 74] and tem-\nporal localization [9, 25, 30]. Given an untrimmed video\nand a textual description of an object, spatio-temporal video\ngrounding aims at localizing a spatio-temporal tube ( i.e., a\nsequence of bounding boxes) for the target object described\n4Czech Institute of Informatics, Robotics and Cybernetics at the Czech\nTechnical University in Prague.\nInput textquery: Whatdoesthe adultride in the playground?\n... ...\ntstart tend\n...\nOutput spatio-temporaltube:\nFigure 1. Spatio-temporal video grounding requires reasoning\nabout space, time, and language.\nby the input text. This task is particularly challenging as\nvideos are highly diverse and often present challenging sce-\nnarios where different entities have similar appearance or\nperform similar actions within one scene.\nThe success of attention-based models in natural lan-\nguage processing [21, 75] has recently inspired approaches\nto integrate transformers into computer vision tasks, such\nas image classification [22], object detection [8], semantic\nsegmentation [52] or action recognition [3, 7, 60, 100]. No-\ntably, with DETR [8], transformers have shown competitive\nperformance on object detection while removing the need\nof multiple hand-designed components encoding a prior\nknowledge about this task. More recently, MDETR [37] has\nextended this framework for various text-conditioned object\ndetection tasks in the image domain, such as phrase ground-\ning, referring expression comprehension and segmentation.\nInspired by these works, and the fact that attention-\nbased architectures are an intuitive choice for modelling\nmulti-modal and spatio-temporal contextual relationships in\nvideos, we develop a transformer encoder-decoder model\nfor spatio-temporal video grounding, as illustrated in Fig-\nure 2. While existing approaches for this task rely on pre-\nextracted object proposals [102], tube proposals [72] or up-\nsampling layers [68], our architecture simply reasons about\nabstractions called time queries to jointly perform temporal\nlocalization and visual grounding. Our framework enables\nto use the same representations for both subtasks in order to\nlearn powerful contextualized representations.\n1\nMore specifically, our architecture includes key compo-\nnents to jointly model temporal, spatial and multi-modal in-\nteractions. Our video-text encoder efficiently encodes spa-\ntial and multi-modal interactions by computing these inter-\nactions over sparsely sampled frames, and separately re-\ncovers temporally local information with a lightweight fast\nbranch. Our space-time decoder models temporal interac-\ntions with temporal self-attention layers, and spatial and\nmulti-modal interactions with time-aligned cross-attention\nlayers. Spatio-temporal video grounding is then tackled\nwith multiple heads on top of the decoder outputs, which\npredict the object boxes and temporal start and end prob-\nabilities. We conduct various ablation studies, where we\nnotably show the benefit of our video-text encoder in terms\nof performance-memory trade-off, and the efficiency of our\nspace-time decoder in terms of spatio-temporal grounding\nresults. Finally, we show that our method significantly im-\nproves over state-of-the-art methods on two benchmarks,\nVidSTG [102] and HC-STVG [72].\nIn summary, our contributions are three-fold: (i) We pro-\npose a novel architecture for spatio-temporal video ground-\ning that performs this task with a space-time transformer\ndecoder. (ii) We propose a dual-stream encoder that effi-\nciently encodes spatial and multi-modal interactions, based\non a slow multi-modal stream and a lightweight fast visual\nstream. (iii) We conduct comprehensive experiments on\ntwo benchmarks, VidSTG and HC-STVG, showing the ef-\nfectiveness of our framework for the spatio-temporal video\ngrounding task. Our approach, referred to as TubeDETR,\noutperforms all state-of-the-art methods by a large margin.\nCode and trained models are publicly available at [1].\n2. Related Work\nSpatio-temporal video grounding.Visual grounding con-\nsists in spatially localizing an object given a referring ex-\npression, and has been an active area of research both in\nthe image domain [18, 32, 33, 51, 59, 77, 83, 91, 97, 107] and\nthe video domain [35,66,74]. A standard paradigm consists\nin using pre-extracted object proposals [48, 49, 78, 84, 86,\n87, 90], while some recent works [19, 34, 37, 46, 56, 88, 89]\nhave proposed one-stage approaches which do not rely on\nsuch proposals. Our work follows the one-stage framework\nof MDETR [37], but extends it to spatio-temporal video\ngrounding with temporal localization losses (see Equa-\ntion 1), slow-fast encoding (see Figure 3), and space-time\ndecoding (see Figure 4).\nA separate line of work focuses on temporally localizing\nmoments in a video given a natural language query [9, 10,\n12, 25, 27, 30, 31, 47, 58, 64, 76, 80, 92, 95, 96, 98, 99, 101].\nThese works build architectures that reason about time but\ndo not preserve spatial information. Spatio-temporal video\ngrounding lies at the intersection of temporal localization\nand visual grounding. While some approaches [15, 72, 84]\nrely on pre-extracted tube proposals, or object propos-\nals [102], our method does not require any pre-extracted\nproposals. A recent work [68] proposes STVGBert, a one-\nstage approach that extends the VilBERT model [54] pre-\ntrained on Conceptual Captions [65] to this task. STVG-\nBert uses deconvolutions to perform visual grounding, and\nsymmetrically models temporal and spatial interactions. In\ncontrast, our architecture performs visual grounding with a\ntransformer decoder, and separately reasons about the tem-\nporal and spatial dimensions.\nTemporal modeling for video understanding.The rise of\npowerful models for image understanding such as ViT [22]\nor DETR [8] has fostered research extending these models\nto the video domain [3,7,29,41,60,100]. In particular, Leiet\nal. [41] propose an architecture that views moment retrieval\nas a direct set prediction problem, but is unsuitable to vi-\nsual grounding as it does not preserve spatial information.\nHe et al. [29] extend the DETR framework to videos, and\npropose an architecture built with sequentially added mod-\nules on top of Deformable DETR [106], while ours is built\non inner modifications of a pretrained encoder and decoder\nand also reasons about language. Our dual-branch encoder\nis also related to SlowFast networks [23,82] which combine\nfast and slow video streams. In contrast, in our case, both\nstreams operate on features extracted from the same back-\nbone, and our dual-stream architecture is motivated by the\ncomputational complexity related to multi-modal modeling.\nVision and language. Transformer-based architectures\nhave become ubiquitous in various vision and language\ntasks [11, 14, 17, 20, 36, 38, 43, 45, 54, 55, 69, 71, 103]. Most\nvideo-text transformers rely either on pre-extracted object\nfeatures [105], or spatially pooled features [24, 26, 44, 70,\n85, 104], which do not preserve detailed spatial informa-\ntion. In contrast, our architecture is designed to preserve\nspatial information to perform visual grounding. Some re-\ncent works propose transformer-based architectures reason-\ning on videos and text that do preserve spatial informa-\ntion [2, 5, 42, 94]. However, these works typically aim to\nlearn global video representations to tackle video-level pre-\ndiction tasks, while we focus on learning detailed frame-\nlevel representations to address a dense prediction task re-\nquiring spatial and temporal localization.\n3. Method\nWe first give an overview of our model in Section 3.1.\nNext, we describe in detail the two main components of our\nmodel, the video-text encoder (Section 3.2) and the space-\ntime decoder (Section 3.3). Then in Section 3.4 we explain\nthe loss used to train our model. Finally in Section 3.5 we\npresent how we initialize our model weights.\n2\nVideo-TextEncoder\nSpace-TimeDecoder\nText featuresð‘¦!ð‘  Predictedspatio-temporaltube\ntime queries{ð‘ž\"}\"#$%\nPredicted start&ð‘¡&Predictedend &ð‘¡'\nâ€œWhatdoesthe adultride in the playground?â€\nLinear\ntime\n1\nT\nk\nâ€¦\n2D PositionEncodingLinearVisualBackbone\nFrames featuresð‘¥!ð‘£TextEncoder\nâ€¦â€¦â€¦\nVideo-text featuresð¹(ð‘£,ð‘ )\n1 Ttime\nClip M â€¦â€¦\nâ€¦â€¦â€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦ â€¦â€¦\nâ€¦\nâ€¦â€¦\ntime\n1\nT\nkâ€¦â€¦â€¦\ntime\n1\nT\nkâ€¦â€¦â€¦\n{.ð‘\"}\"#(\"!)\"\"\nSentence s\nVideo{ð‘£\"}\"#$% MLPs\nâ€¦â€¦â€¦\ntime\n1\nT\nkâ€¦â€¦â€¦\nVisualBackbone\nVisualBackbone\nVisualBackbone\nLinearLinearLinear\n2D PositionEncoding2D PositionEncoding2D PositionEncoding\nVideo-TextEncoder\nClip 1\nâ€¦â€¦â€¦ â€¦â€¦â€¦ â€¦â€¦â€¦\nâ€¦â€¦â€¦ â€¦â€¦â€¦\nâ€¦\nFigure 2. TubeDETR model overview. All input video frames vt and the sentence s are first processed with a Visual Backbone and a Text\nEncoder. The resulting text and video features y0(s) and x0(v) are then jointly encoded with a Video-Text Encoder that computes spatial\nand multi-modal interactions for M short clips of k frames (about 1 second). The resulting video-text features F(v, s) are then decoded\ninto the output spatio-temporal tube Ë†b using a Space-Time Decoder that jointly reasons about time, space and text over the entire video.\n3.1. Overview\nOur objective is, given a video and a language query, to\noutput a spatio-temporal tube, i.e. a sequence of bounding\nboxes with temporal boundaries, grounding the language\nquery in the video. This is challenging as it requires mod-\nelling long-range spatial and temporal interactions between\nthe language query and the video where the video may have\nhundreds of frames represented by tens of thousands spatio-\ntemporal video features. Hence efficiency is a major chal-\nlenge. To address this issue we design an encoder-decoder\narchitecture, illustrated in Figure 2, that enables accurate\nyet efficient modelling of video-language spatial and tem-\nporal interactions across the entire video. In particular, our\ntwo-stream video-text encoder (Section 3.2) models video-\nlanguage interactions only over short clips of about one sec-\nond but allows for detailed spatial localization. Our space-\ntime decoder (Section 3.3) then models long-range tempo-\nral interaction over the entire video to produce a temporally\nconsistent output and accurate predictions of the start and\nend times of the output spatio-temporal tube.\n3.2. Video-Text Encoder\nOur encoder is illustrated in Figure 3 and described\nnext. Its objective is to model spatial and multi-modal\ninteractions between the language query and the video to\naccurately spatially ground the query in each frame. To\nachieve this, we leverage the ability of the self-attention\nlayers to jointly model spatial and visual-linguistic inter-\nactions [36, 37, 42]. However, computing self-attention be-\ntween visual features and textual features for every frame is\ncomputationally expensive. For this reason, we propose to\ncompute spatial and multi-modal interactions only for every\nk-th frame. We denote the resulting stream as slow multi-\nmodal branch. We use a separate lightweight fast visual-\nonly branch that preserves the original frame rate and allows\nus to recover some of the high frequency spatio-temporal\ndetails lost by the sparse sampling in the slow branch.\nFormally, our encoder takes as input a set of 2D flattened\nimage features x0(v) âˆˆ RTÃ—HW Ã—d from the visual back-\nbone for all T frames of the input video together with a set\nof L text features y0(s) âˆˆ RLÃ—d extracted by the text en-\ncoder from the query sentence, and outputs a set of video-\ntext features F(v, s) âˆˆ RTÃ—(HW +L)Ã—d, one for each frame.\nNext we give the details of the Slow and Fast branches, and\nthe final feature aggregation module.\nSlow multi-modal branch. The goal of this branch (see\ntop of Figure 3) is to model interactions between visual and\ntextual representations. This branch first samples features\nfrom one frame for a short clip of k consecutive frames. A\ntypical clip length is one second, i.e. k = 5with a standard\nframe rate of 5 frames per second [102]. Formally, the re-\nsulting feature map is written as xp âˆˆ RMÃ—HW Ã—d where\nM = âŒˆT\nk âŒ‰ is the number of clips, k is the length of the clip\nand T is the length of the entire video. We then concate-\nnate, for each clip m, its visual features xp\nm with text fea-\ntures y0(s) and forward it to a N-layer transformer encoder.\nThe outputs are contextualized visual-text representations\nhp(v, s) âˆˆ RMÃ—(HW +L)Ã—d, which effectively combine in-\nformation from the input video v and the query sentence s.\nFast visual-only branch.The previously explained tempo-\nral sparse sampling scheme reduces significantly the mem-\nory requirements of the video-text encoder but results in\na loss of spatio-temporal details which are important for\nspatio-temporal video grounding. To alleviate this issue,\nwe introduce module f (see bottom of Figure 3) which op-\nerates on 2D flattened image features for all frames . For-\n3\nkÃ—ð»ð‘ŠFrames featuresð‘¥!ð‘£ ConcatTransformer Encoder\nf\nmk+1\nð¿Textfeaturesð‘¦!ð‘ \nFastvisual-onlybranch\nâ€¦\ntime\nð»ð‘ŠSampled Featuresð‘¥\"# Multi-modalslowfeaturesVisual-onlyfastfeaturesð‘˜Ã—ð»ð‘Š+ð¿Final features\ngTemporal sampling\nâ€¦Temporal replication\nâ€¦\nâ€¦Slow multi-modalbranch\ntime\nâ„Ž#ð‘£,ð‘ ð‘“(ð‘£) ð¹(ð‘£,ð‘ )\nmk+k\nmk+1\nmk+k\nFigure 3. Video-Text Encodertakes as input a set of 2D flattened image features x0(v) together with a set of text features y0(s) from the\nquery sentence, and outputs a set of video-text features F(v, s), one for each frame. Top: the Slow multi-modal branch first samples video\nfeatures xp\nm, one from every k frames. Then it computes multi-modal interactions between the sampled features xp\nm and text features y0\nusing a transformer encoder. The temporal sampling reduces the number of video features in order to efficiently compute the attention-\nbased interactions. Bottom: lightweight â€œFast visual-onlyâ€ branch f processes features from all frames but without any attention layers for\nincreased efficiency. Features from both branches are then combined in module g into the final set of per-frame features F(v, s).\nmally, given feature map x0(v), this module outputs visual\nfeatures f(v) âˆˆ RTÃ—HW Ã—d. This fast branch preserves the\nspatial and temporal resolution of the features but is com-\nputationally light as it does not compute any multi-modal\nor spatial interactions. For additional efficiency, at training\ntime, this branch does not back-propagate gradients to the\nvisual backbone. Furthermore, we show in Section 4.2 that\nit is able, when combined with the temporally sparse fea-\ntures obtained from the slow branch, to recover some of the\ntemporal information lost during the temporal sampling.\nSlow-Fast feature aggregation. We now describe the\nslow and fast branches aggregation module (see Figure 3,\nright), which fuses information from both branches and\noutputs final video-text features. To match the temporal\ndimension of the output from the fast branch f(v), the\noutput of the slow multi-modal branch hp(v, s) is tempo-\nrally replicated k times for each clip resulting in video-\ntext encodings h(v, s) âˆˆ RTÃ—(HW +L)Ã—d. These encodings\nare a concatenation of text-contextualized visual encodings\nhv(v, s) âˆˆ RTÃ—HW Ã—d and visually-contextualized textual\nencodings hs(v, s) âˆˆ RTÃ—LÃ—d. The text-contextualized\nvisual encodings hv(v, s) are combined with the outputs\nof the fast branch with an additional aggregation module\ng and a residual connection, resulting in aggregated vi-\nsual encodings Fv(v, s) = g(hv(v, s), f(v)) +hv(v, s).\nThe final output of our video-text encoder is obtained by\nconcatenating these aggregated visual encodings with the\nvisually-contextualized textual encodings i.e. F(v, s) =\n[Fv(v, s), hs(v, s)] âˆˆ RTÃ—(HW +L)Ã—d. In detail, the mod-\nule g is implemented as a sum followed by a linear layer,\ni.e. g(hv(v, s), f(v)) =Linear(hv(v, s) +f(v)).\n3.3. Space-Time Decoder\nOur decoder is illustrated in Figure 4 and detailed next.\nIts objective is to model the temporal interactions within\nthe entire video of T frames and decode the multi-modal\nfeatures from the encoder into a temporally coherent out-\nput tube with accurate start and end times. This is achieved\nby an efficient decoder architecture that alternates (i) tem-\nporal self-attention layers, which model temporal interac-\ntions across the entire video, with (ii)time-aligned cross at-\ntention layers, which efficiently incorporate the video-text\nfeatures for individual frames obtained from the encoder.\nIn detail, the decoder operates on T positional encodings\n{qt}T\nt=1, one per frame, referred to as time queries. The ini-\ntial encoding of each time query is obtained by summing a\nlearnt object encoding common to all frames, and a frozen\nsinusoidal time encoding. The decoder also takes as input\nT Ã— (HW + L) video-language embeddings F(v, s) out-\nput from the video-text encoder. The decoder is a suc-\ncession of N decoder blocks. Each block is composed\nof temporal self-attention, time-aligned cross-attention, and\nfeed-forward layers, interleaved with normalization [4], as\nshown in Figure 4. The decoder outputs refined time queries\n{Qt}T\nt=1, which are contextualized across all frames in the\nvideo together with video-text features produced by the en-\ncoder. The refined time queries are then jointly used for\noutputting the spatio-temporal video tube that grounds the\ninput sentence in the video. The individual layers are de-\nscribed in detail next.\nTemporal self-attention. The T input time queries qt at-\ntend to each other using the temporal self-attention layer.\nThis layer is in each of the N blocks of the decoder and\nis responsible for modelling the long-range temporal inter-\nactions in the entire video. This is possible because of the\nrelatively low complexity of this layer, which does not de-\npend on the spatial resolution of the input video.\nTime-aligned cross-attention. Allowing each time query\nto cross-attend to all T Ã— (HW + L) video-text features\ncan be highly computationally expensive due to the large\nnumber of video frames T and a large spatial resolution\nHW of the video features. Instead, in our cross-attention\nmodule, each time query qt only cross-attends to its tem-\nporally corresponding multi-modal features F(v, s)[t] at\nframe t. Note that with our time-aligned cross-attention for-\nmulation, the time encoding and the temporal self-attention\n4\nTime-AlignedCross-Attention Mask ð‘µÃ—time1 T\nL 1T\ntime\nTemporal Self-AttentionAdd& NormTime-AlignedCross-AttentionFeed-Forward\nVideo-TextFeaturesð¹(ð‘£,ð‘ )Add& Norm Add& Norm+Time encodingTime queries{ð‘ž!}!\"#$\nObject query HW\nPredictedboxes, startand end probabilities{ð‘„!}!\"#$â€¦ MLPs+ð‘#Ì‚ðœ%!Ì‚ðœ&!+ð‘!!Ì‚ðœ%\"!Ì‚ðœ&\"!\n+ð‘!\"Ì‚ðœ%\"\"Ì‚ðœ&\"\"\n+ð‘$Ì‚ðœ%#Ì‚ðœ&#{/ð‘!}!\"'!$'!%\nPredictedspatio-temporaltubeMLPsMLPsMLPs\nâ€¦â€¦â€¦\nâ€¦â€¦â€¦\nâ€¦â€¦â€¦â€¦â€¦â€¦\nâ€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦\nâ€¦\nâ€¦\nFigure 4. Space-Time Decoder. The decoder is composed of N repeated blocks. In each block, time queries qt successively attend to\neach other via temporal self-attention and to their respective time-aligned video-text featuresF(v, s) via time-aligned cross-attention. The\ncross-attention mask (bottom) indicates the non-zero weights (white) between the inputHW +L video-language features for each of theT\ninput frames (x-axis) and T time queries (y-axis). The cross-attention mask ensures that each time queryqt only cross-attends to video-text\nfeatures F(v, s) at the corresponding frame t, which significantly increases efficiency of the decoder and enables decoding entire videos\nof T frames. The temporal modelling over the entire length of the video is ensured by the temporal self-attention layers.\nlayers are all the more important, as they are responsible\nfor the temporal modelling across the entire video. Without\nthem, our decoder would be decoding each frame indepen-\ndently. Their importance is ablated in Section 4.2.\nPrediction heads. The output of the decoder is a set of\nrefined time queries {Qt}T\nt=1. They are jointly used for vi-\nsual grounding and temporal localization to simultaneously\nobtain predictions for all frames of the video . In detail,\nnormalized coordinates of all bounding boxes (2D center\nand size) Ë†b âˆˆ [0, 1]TÃ—4 are predicted with a 3-layer MLP.\nProbabilities of the start and the end of the output video\ntube, Ë†Ï„s âˆˆ [0, 1]T and Ë†Ï„e âˆˆ [0, 1]T , respectively, are pre-\ndicted with 2-layer MLPs. At inference time, the start and\nend times of the output tube, Ë†ts and Ë†te, are computed by\nchoosing the maximum of the joint start and end proba-\nbility distribution (Ë†Ï„s, Ë†Ï„e) âˆˆ [0, 1]TÃ—T with invalid com-\nbinations where Ë†te â‰¤ Ë†ts masked out. The predicted spatio-\ntemporal tube {Ë†bt}\nË†te\nt=Ë†ts\nis composed from bounding boxes\nË†bt predicted within the chosen start and end times Ë†ts and Ë†te.\n3.4. Training loss\nThe input training data is in the form of a set of videos,\nwhere each video is annotated with a query sentence s and\nthe corresponding video tube b composed of a set of bound-\ning boxes and corresponding start and end times, ts and te.\nInspired by [64], we construct a target start (respectively\nend) distribution Ï„s âˆˆ [0, 1]T (respectively Ï„e) which fol-\nlows a quantized Gaussian centered at ts âˆˆ [0, Tâˆ’ 1] (re-\nspectively te) with standard deviation 1. We train our archi-\ntecture with a linear combination of four losses\nL = Î»L1 LL1 (Ë†b, b) +Î»gIoU LgIoU (Ë†b, b)\n+Î»KLLKL(Ë†Ï„s, Ë†Ï„e, Ï„s, Ï„e) +Î»attLatt(A) (1)\nwhere b âˆˆ [0, 1]4(teâˆ’ts+1) denotes the normalized ground\ntruth box coordinates and Ë†b the predicted bounding boxes\nand A âˆˆ [0, 1]TÃ—T denotes the temporal self-attention ma-\ntrix. Finally, different Î»â€¢ are scalar weights of the indi-\nvidual losses. LL1 is a L1 loss on bounding box coor-\ndinates. LgIoU is a generalized â€œintersection over unionâ€\n(IoU) loss [63] on the bounding boxes. Both L1 and LgIoU\nare used for spatial grounding. LKL(Ë†Ï„s, Ë†Ï„e, Ï„s, Ï„e) is the\nKullback-Leibler divergence loss measuring the distance\nbetween the predicted and the target start distribution as\nwell as the distance between the predicted and the target\nend distribution [64]. Latt(A) is a guided attention loss [64]\nthat encourages weights corresponding to time queries out-\nside of the temporal boundaries to be lower than the weights\ninside these boundaries. LKL and Latt(A) are both used for\ntemporal grounding. Losses are computed at each layer of\nthe decoder following [8].\n3.5. Weight initialization\nWe initialize our architecture with weights from\nMDETR [37] pretrained on Flickr30k [61], MS COCO [13]\nand Visual Genome [40]. In detail, weights of our video-\ntext encoder are initialized from the MDETR multi-modal\nencoder, except for the fast and aggregation modules. We\nalso use the weights from the MDETR single-image multi-\nobject decoder to initialize our multi-frame single-object\nspace-time decoder, except for the temporal localization\nhead. We show the benefit of this initialization notably\nby comparing it to an ImageNet initialization, i.e. using a\nvisual backbone pretrained on ImageNet with a randomly\ninitialized transformer, in Section 4.2. We also evaluate a\nMDETR-equivalent baseline in Section 4.2.\n4. Experiments\nThis section demonstrates the effectiveness of our archi-\ntecture and compares our method to the state of the art. We\nfirst introduce the datasets, evaluation metrics and imple-\n5\nmentation details in Section 4.1. We then present ablation\nstudies in Section 4.2. The comparison to the state of the art\nin spatio-temporal video grounding is given in Section 4.3.\nFinally, we show qualitative results in Section 4.4.\n4.1. Experimental setup\nDatasets. We evaluate our approach on the VidSTG [102]\nand HC-STVG [72] datasets. Both are annotated with\nspatio-temporal tubes corresponding to text queries. Vid-\nSTG consists of 99,943 sentence descriptions with 44,808\ndeclarative sentences and 55,135 interrogative sentences de-\nscribing 79 types of objects appearing in 10,303 different\nvideos. The dataset is divided into training, validation and\ntest subsets with 80,684, 8,956 and 10,303 distinct sen-\ntences respectively, and 5,436, 602 and 732 distinct videos\nrespectively. HC-STVG consists of videos in multi-person\nscenes, each annotated with one sentence referring to a per-\nson. For ablation, we use the second improved version of\nthe dataset HC-STVG2.0 which is divided into training and\nvalidation subsets with 10,131 and 2,000 video-sentence\npairs, respectively. The test set is not publicly available at\nthe time of writing. To compare with prior work, we use\nthe first version of the datasetHC-STVG1 which is divided\ninto training and test subsets with 4,500 and 1,160 video-\nsentence pairs, respectively.\nEvaluation metrics. We follow [102] and define vIoU\nas vIoU = 1\n|Su|\nP\ntâˆˆSi IoU (Ë†bt, bt) where Su (respectively\nSi) is the set of frames in the union (respectively inter-\nsection) between the ground truth (GT) and the predicted\ntimestamps. Ë†bt (respectively bt) are the predicted (respec-\ntively GT) boxes at time t. To evaluate spatio-temporal\nvideo grounding, we use m vIoU , which is the average of\nvIoU . We also use vIoU @R, the proportion of samples for\nwhich vIoU > R. To isolate the evaluation of temporal\nlocalization, we use m tIoU which is the average of tem-\nporal IoU between the GT start and end and the predicted\nstart and end. Likewise, to evaluate spatial grounding only,\nwe use m sIoU , which is computed by using the GT start\nand end times. For ablations we report results averaged over\nall samples. More detailed ablation results presented sepa-\nrately for declarative and interrogative sentences in VidSTG\nare reported in Appendix Section C. We also report peak\nGPU memory usage during training (Mem.) to measure the\nmemory footprint of alternative models.\nImplementation details. The visual backbone is ResNet-\n101 [28], the text encoder is RoBERTa [50] and the fast\nmodule f is a linear layer. Following [102], we sample 5\nframes per second for videos, and for videos with more than\n200 sampled frames we uniformly sample 200 frames. We\nuse hyper-parameters T = 200, N = 6, d = 256, Î»L1 = 5,\nÎ»giou = 2, Î»KL = 10and Î»att = 1. We train our networks\nfor 10, 20 and 40 epochs on VidSTG, HC-STVG2.0 and\nHC-STVG1, respectively. The final model is selected based\nTime\nEncoding\nSelf\nAttention m tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU\n1. âœ— - 23.9 12.2 15.3 6.1 47.0\n2. âœ— Temporal 25.2 13.0 16.9 6.5 47.3\n3. âœ“ - 41.7 21.3 28.7 17.4 46.5\n4. âœ“ Temporal 45.9 24.3 33.2 22.0 47.7\nTable 1. Effect of the time encoding and the temporal self-\nattention in our space-time decoder on the VidSTG validation set.\nPre-\nTraining\nDecoder Self-\nAttention Transferm tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU\n1. âœ— âœ— 42.8 18.8 25.1 15.6 38.5\n2. âœ“ âœ— 43.8 22.4 29.9 19.1 46.5\n3. âœ“ Temporal 45.9 24.3 33.2 22.0 47.7\nTable 2. Effect of the weight initialization for our model on the\nVidSTG validation set.\non the best spatio-temporal video grounding performance\non the validation set. For the largest dataset VidSTG, the\noptimization takes 2 days on 16 Tesla V100 GPUs. Further\ndetails are included in Appendix Section B.\n4.2. Ablation studies\nIn this section, we ablate the hyper-parameters of our\nmodel and evaluate alternative design choices of the en-\ncoder and decoder. Unless stated otherwise, we use spatial\nframe resolution of 224 pixels and temporal stride k = 5.\nSpace-time decoder. We first ablate the design choices\nof the proposed space-time decoder. We compare our full\ndecoder model with variants without time encoding, with-\nout temporal self-attention and without both. The variant\nwithout both corresponds to a space-only decoder, similar\nto MDETR [37] applied independently to every frame. Ta-\nble 1 shows that there is a substantial improvement over\nthe space-only decoder when using both time encoding\nand temporal self-attention (+17.9% onvIoU @0.3 between\nrows 1 and 4). The gain comes mostly from the temporal lo-\ncalization (+22.0% on m tIoU ), while the spatial ground-\ning moderately increases (+0.7% in m sIoU ). Further-\nmore, we can observe that the time encoding brings most\nof the gain (+13.4% on vIoU @0.3 between rows 1 and 3).\nFinally, the temporal self-attention results in an additional\nimprovement (+4.5% on vIoU @0.3 between rows 3 and 4)\nover using time encoding only.\nInitialization. We now ablate the importance of initializ-\ning our model with pretrained MDETR [37] weights. In\nTable 2, we compare this initialization to ImageNet ini-\ntialization, and a variant that does not transfer the spatial\nself-attention weights from MDETR decoder to the tempo-\nral self-attention in our space-time decoder. At pretraining\ntime, this self-attention was used to model spatial relation-\nships between different objects in the same image, while\nthe temporal self-attention in our decoder models temporal\nrelationships between the same object in different frames\nof a video. We find that pretraining is highly beneficial\n(+8.1% on vIoU @0.3 between rows 1 and 3), especially\n6\n(a) VidSTG\nFast Res.Temp.\nStridem tIoU mvIoU vIoU@0.3 vIoU@0.5 msIoUMem.\n(GB)\n1. â€” 224 1 46.5 25.2 34.1 23.0 49.1 23.9\n2. âœ“ 224 2 46.0 25.0 34.3 22.9 49.0 16.2\n3. âœ“ 224 5 45.9 24.3 33.2 22.0 47.7 11.8\n4. âœ“ 288 2 46.4 25.9 35.0 23.9 50.5 23.7\n5. âœ“ 320 3 46.4 25.9 35.7 23.7 50.7 23.6\n6. âœ“ 352 4 46.9 26.2 36.1 24.1 50.7 24.4\n7. âœ— 352 4 46.6 24.8 34.0 21.6 48.3 18.1\n8. âœ“ 384 5 46.8 26.0 35.5 24.0 50.4 26.1\n(b) HC-STVG2.0\nFast Res.Temp.\nStridem tIoU mvIoU vIoU@0.3 vIoU@0.5 msIoUMem.\n(GB)\n1. â€” 224 1 52.8 35.0 55.3 28.3 63.9 14.3\n2. âœ“ 224 2 53.7 35.8 56.7 29.6 64.3 10.2\n3. âœ“ 224 5 53.2 35.0 54.5 29.0 63.2 8.0\n4. âœ“ 288 2 53.9 36.4 58.1 30.7 65.4 13.9\n5. âœ“ 320 3 53.6 36.2 57.5 30.4 65.2 13.8\n6. âœ“ 352 4 53.9 36.4 58.8 30.6 64.9 14.3\n7. âœ— 352 4 53.1 34.7 55.9 27.4 63.0 11.3\n8. âœ“ 384 5 53.6 36.3 57.5 30.4 65.3 15.2\nTable 3. Comparison of performance-memory trade-off with various temporal strides k, spatial resolutions (Res.), with or without the fast\nbranch in our video-text encoder, on the VidSTG validation set (left, Table 3a) and the HC-STVG2.0 validation set (right, Table 3b).\nMethod Pretraining\nData\nVidSTG HC-STVG1\nDeclarative Sentences Interrogative Sentences\nm tIoU mvIoUvIoU@0.3 vIoU@0.5m tIoU mvIoUvIoU@0.3 vIoU@0.5m vIoU vIoU@0.3 vIoU@0.5\n1. STGRN [102] Visual Genome 48.5 19.8 25.8 14.6 47.0 18.3 21.1 12.8 â€” â€” â€”\n2. STGVT [72] Visual Genome +\nConceptual Captions â€” 21.6 29.8 18.9 â€” â€” â€” â€” 18.2 26.8 9.5\n3. STVGBert [68]ImageNet + Visual Genome +\nConceptual Captions â€” 24.0 30.9 18.4 â€” 22.5 26.0 16.0 20.4 29.4 11.3\n4. TubeDETR (Ours) ImageNet 43.1 22.0 29.7 18.1 42.3 19.6 26.1 14.9 21.2 31.6 12.2\n5. TubeDETR (Ours)ImageNet + Visual Genome +\nFlickr + COCO 48.1 30.4 42.5 28.2 46.9 25.7 35.7 23.2 32.4 49.8 23.5\nTable 4. Comparison to the state of the art on the VidSTG test set and the HC-STVG1 test set.\nfor the spatial grounding performance (+9.2% onm sIoU ).\nAdditionally, we observe the benefit of using the spatial\nself-attention weights from the MDETR decoder to initial-\nize the temporal self-attention in our decoder (+3.3% on\nvIoU @0.3 between rows 2 and 3).\nImpact of spatial resolution and temporal stridek. In\nthis section, we analyze the impact of the frame resolution\nand the temporal stride k. In Table 3, we show that increas-\ning the resolution is an important factor of performance\nfor spatio-temporal video grounding, on both the VidSTG\nand HC-STVG2.0 datasets (see rows 2 and 4). However, it\nalso results in significantly higher memory usage (16.2GB\nvs 23.7GB). As a consequence, the variant using temporal\nstride k = 1is challenging to train on VidSTG with a res-\nolution higher than 224 on a Tesla V100 32GB GPU. At a\nfixed 224 resolution, increasing the temporal stride k to 2\nor 5 reduces the peak memory usage by 7.7GB or 12.1GB,\nrespectively (see row 1 vs 2 or 3, respectively). Our pro-\nposed video-text encoder enables us to train on higher res-\nolutions at a given memory usage. This leads to a better\nperformance-memory trade-off (rows 4, 5, 6, 8) than the\nbaseline variant with temporal stride k = 1 (row 1). In\nparticular, the best spatio-temporal video grounding results\n(m vIoU and vIoU @R) over the two datasets are obtained\nwith temporal stride k = 4and resolution 352 (row 6).\nWe note that as the resolution increases, performance\ngains obtained by its further increase are expected to be\nlower as they are limited by the original video resolution.\nFor instance, the average video pixel height in VidSTG and\nHCSTVG2.0 is 440 and 490 pixels, respectively.\nImpact of the fast branch.Finally, we validate the im-\nportance of our fast branch by comparing, for the best vari-\nant, temporal stride k = 4and resolution 352, our slow-fast\nvideo-text encoder to a slow-only variant that corresponds\nto f = 0and g = 0. In this case the video-text features are\nthe slow video-text features. By comparing rows 6 and 7 in\nTable 3, our fast branch significantly improves the spatio-\ntemporal video grounding performance (+2.1% vIoU @0.3\non VidSTG and +2.9% vIoU @0.3 on HC-STVG2.0) with\nlow computational memory overhead. This shows that the\nfast branch recovers useful spatio-temporal details lost by\nthe temporal sampling operation in the slow branch. We\nfurther ablate the design of the fast and aggregation mod-\nules f and g in Appendix Section D.\n4.3. Comparison to the state of the art\nIn this section, we compare our approach to state-of-the-\nart methods in spatio-temporal video grounding. We report\nresults for the model achieving the best validation results in\nthe previous ablation studies, i.e., our space-time decoder\nwith time encoding and temporal self-attention, temporal\nstride k = 4and resolution 352. The focus of our work is on\nthe spatio-temporal video grounding metrics (m vIoU and\nvIoU @R). As shown in Table 4, only using ImageNet to\ninitialize the visual backbone (row 4), our TubeDETR per-\nforms competitively despite using less annotations. Further-\nmore, if we use MDETR initialization (row 5), our Tube-\nDETR outperforms by a large margin all previous methods\n(rows 1, 2 and 3) on both datasets. STGRN [102] achieves\nsimilar m tIoU (measuring only temporal localization), but\nit defines a handcrafted set of possible window widths to\ntackle temporal localization, while we consider all possible\nwindows, i.e. any starting frame i and ending frame j with\ni < j. These results demonstrate the excellent performance\nof our architecture for spatio-temporal video grounding.\n7\nQuery: Whatbites the adulton the ground?\nGT\nQuery: An adultgrabsa sports balloutdoors.\nGT\nQuery: Whatisbeneaththe adultin the snow?\nGTQuery: There isa ball/sports ballnextto the littlechild.\nGTTubeDETR\nTubeDETR\nTubeDETR\nTubeDETR\nFigure 5. Qualitative examples of spatio-temporal tubes predicted by our model (light yellow), compared with ground truth (light green),\non the VidSTG test set. The first three examples illustrate successful predictions of our method. In the last example the method confuses\nthe small sports ball in the background with a balloon.\n4.4. Qualitative examples\nWe show qualitative examples of our predictions on the\nVidSTG test set in Figure 5. These examples show that our\nmodel is able to predict meaningful and accurate spatio-\ntemporal tubes associated with the input text queries. In\nparticular, in the first example, our model correctly de-\ntects the temporal moment corresponding to the cat biting\nthe adult. In the second example, our model localizes the\nspatio-temporal tube corresponding to a man quickly grab-\nbing a very small sports ball and in the third example it is\nable to localize the skis under the adult while skiing. How-\never, as shown in the last example, it may fail to understand\nfine details in the query and the video. Note that the balloon\nand the ball are visually and semantically similar. A careful\nanalysis is required to understand the difference. Further-\nmore, we provide visualizations of the different attention\nmechanisms of TubeDETR in Appendix Section A.\n5. Conclusion\nWe have proposed TubeDETR, a novel transformer-\nbased architecture for spatio-temporal video grounding.\nTubeDETR tackles this task with a space-time transformer\ndecoder combined with a video-text encoder that efficiently\nencodes spatial and multi-modal interactions. We have\ndemonstrated the effectiveness of our space-time decoder,\nand the benefits of our video-text encoder in terms of\nperformance-memory trade-off. Finally, our approach out-\nperforms state-of-the-art methods on two benchmarks, Vid-\nSTG and HC-STVG. Future work could extend our space-\ntime decoder to detect multiple objects per frame or mul-\ntiple events per video. Investigating more efficient alterna-\ntives to self-attention, such as the ones studied for natural\nlanguage [6, 16, 39, 73, 79, 81, 93], is another promising di-\nrection for future research.\nAcknowledgements. This work was granted access to the HPC resources\n8\nof IDRIS under the allocation 2021-AD011011670R1 made by GENCI.\nThe work was funded by a Google gift, the French government under man-\nagement of Agence Nationale de la Recherche as part of the â€Investisse-\nments dâ€™avenirâ€ program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA\nInstitute), the Louis Vuitton ENS Chair on Artificial Intelligence, the\nEuropean Regional Development Fund under project IMPACT (reg. no.\nCZ.02.1.01/0.0/0.0/15 003/0000468). We thank S. Chen and J. Chen for\nhelpful discussions and O. Bounou and P.-L. Guhur for proofreading.\nReferences\n[1] TubeDETR project webpage. https://antoyang.\ngithub.io/tubedetr.html. 2\n[2] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\nV ATT: Transformers for multimodal self-supervised learn-\ning from raw video, audio and text. arXiv preprint\narXiv:2104.11178, 2021. 2\n[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu Ë‡ciÂ´c, and Cordelia Schmid. ViViT: A video\nvision transformer. In ICCV, 2021. 1, 2\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 4\n[5] Max Bain, Arsha Nagrani, G Â¨ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 2\n[6] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020. 8\n[7] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\nIn ICML, 2021. 1, 2\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. In ECCV,\n2020. 1, 2, 5\n[9] Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and\nTat-Seng Chua. Temporally grounding natural sentence in\nvideo. In EMNLP, 2018. 1, 2\n[10] Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and\nJiebo Luo. Localizing natural language in videos. In AAAI,\n2019. 2\n[11] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and\nIvan Laptev. History aware multimodal transformer for\nvision-and-language navigation. In NeurIPS, 2021. 2\n[12] Shaoxiang Chen and Yu-Gang Jiang. Semantic proposal for\nactivity localization in videos via sentence query. In AAAI,\n2019. 2\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Doll Â´ar, and C Lawrence\nZitnick. Microsoft COCO captions: Data collection and\nevaluation server. arXiv preprint arXiv:1504.00325, 2015.\n5\n[14] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\nFaisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUNITER: Universal image-text representation learning. In\nECCV, 2020. 2\n[15] Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee K\nWong. Weakly-supervised spatio-temporally grounding\nnatural sentence in video. In ACL, 2019. 2\n[16] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. In ICLR, 2021.\n8\n[17] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and\nRita Cucchiara. Meshed-memory transformer for image\ncaptioning. In CVPR, 2020. 2\n[18] Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan Lyu,\nand Mingkui Tan. Visual grounding via accumulated atten-\ntion. In CVPR, 2018. 2\n[19] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang\nZhou, and Houqiang Li. TransVG: End-to-end visual\ngrounding with transformers. In ICCV, 2021. 2\n[20] Karan Desai and Justin Johnson. VirTex: Learning visual\nrepresentations from textual annotations. In CVPR, 2021. 2\n[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. InNAACL-HLT, 2019.\n1\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. In ICLR, 2021. 1, 2\n[23] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nICCV, 2019. 2\n[24] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid. Multi-modal transformer for video retrieval. In\nECCV, 2020. 2\n[25] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.\nTALL: Temporal activity localization via language query.\nIn ICCV, 2017. 1, 2\n[26] Simon Ging, Mohammadreza Zolfaghari, Hamed Pirsi-\navash, and Thomas Brox. COOT: Cooperative hierarchi-\ncal transformer for video-text representation learning. In\nNeurIPS, 2020. 2\n[27] Dongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao\nLiu, and Shilei Wen. Read, watch, and move: Reinforce-\nment learning for temporally grounding natural language\ndescriptions in videos. In AAAI, 2019. 2\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In CVPR,\n2016. 6\n[29] Lu He, Qianyu Zhou, Xiangtai Li, Li Niu, Guangliang\nCheng, Xiao Li, Wenxuan Liu, Yunhai Tong, Lizhuang\nMa, and Liqing Zhang. End-to-end video object detection\nwith spatial-temporal transformers. Proceedings of the 29th\nACM International Conference on Multimedia, 2021. 2\n9\n[30] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing mo-\nments in video with natural language. ICCV, 2017. 1, 2\n[31] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef\nSivic, Trevor Darrell, and Bryan Russell. Localizing mo-\nments in video with temporal language. In EMNLP, 2018.\n2\n[32] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor\nDarrell, and Kate Saenko. Modeling relationships in ref-\nerential expressions with compositional modular networks.\nIn CVPR, 2017. 2\n[33] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng,\nKate Saenko, and Trevor Darrell. Natural language object\nretrieval. In CVPR, 2016. 1, 2\n[34] Binbin Huang, Dongze Lian, Weixin Luo, and Shenghua\nGao. Look before you leap: Learning landmark features\nfor one-stage visual grounding. In CVPR, 2021. 2\n[35] De-An Huang, Shyamal Buch, Lucio Dery, Animesh Garg,\nLi Fei-Fei, and Juan Carlos Niebles. Findingâ€ itâ€: Weakly-\nsupervised reference-aware visual grounding in instruc-\ntional videos. In CVPR, 2018. 2\n[36] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu,\nand Jianlong Fu. Pixel-BERT: Aligning image pixels with\ntext by deep multi-modal transformers. arXiv preprint\narXiv:2004.00849, 2020. 2, 3\n[37] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel\nSynnaeve, Ishan Misra, and Nicolas Carion. MDETR\n- modulated detection for end-to-end multi-modal under-\nstanding. In ICCV, 2021. 1, 2, 3, 5, 6\n[38] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-\nand-language transformer without convolution or region su-\npervision. In ICML, 2021. 2\n[39] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efficient transformer. In ICLR, 2020. 8\n[40] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\nLi Fei-Fei. Visual Genome: Connecting language and vi-\nsion using crowdsourced dense image annotations. IJCV,\n2016. 5\n[41] Jie Lei, Tamara L Berg, and Mohit Bansal. QVHighlights:\nDetecting moments and highlights in videos via natural lan-\nguage queries. In NeurIPS, 2021. 2\n[42] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg,\nMohit Bansal, and Jingjing Liu. Less is more: Clipbert for\nvideo-and-language learning via sparse sampling. InCVPR,\n2021. 2, 3\n[43] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang,\nand Ming Zhou. Unicoder-VL: A universal encoder for vi-\nsion and language by cross-modal pre-training. In AAAI,\n2020. 2\n[44] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng\nYu, and Jingjing Liu. HERO: Hierarchical encoder\nfor video+language omni-representation pre-training. In\nEMNLP, 2020. 2\n[45] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\nFuru Wei, et al. Oscar: Object-semantics aligned pre-\ntraining for vision-language tasks. In ECCV, 2020. 2\n[46] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen\nQian, and Bo Li. A real-time cross-modality correlation\nfiltering method for referring expression comprehension. In\nCVPR, 2020. 2\n[47] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng\nLiu. Weakly-supervised video moment retrieval via seman-\ntic completion network. In AAAI, 2020. 2\n[48] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring\nexpression generation and comprehension via attributes. In\nICCV, 2017. 2\n[49] Xihui Liu, Zihao Wang, Jing Shao, Xiaogang Wang, and\nHongsheng Li. Improving referring expression grounding\nwith cross-modal attention-guided erasing. In CVPR, 2019.\n2\n[50] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. RoBERTa: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019. 6\n[51] Yongfei Liu, Bo Wan, Lin Ma, and Xuming He. Relation-\naware instance refinement for weakly supervised visual\ngrounding. In CVPR, 2021. 2\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 1\n[53] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 13\n[54] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-\nBERT: Pretraining task-agnostic visiolinguistic representa-\ntions for vision-and-language tasks. In NeurIPS, 2019. 2\n[55] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 12-in-1: Multi-task vision and lan-\nguage representation learning. In CVPR, 2020. 2\n[56] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao,\nChenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task\ncollaborative network for joint referring expression com-\nprehension and segmentation. In CVPR, 2020. 2\n[57] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan\nLaptev, Josef Sivic, and Andrew Zisserman. End-to-end\nlearning of visual representations from uncurated instruc-\ntional videos. In CVPR, 2020. 1\n[58] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K\nRoy-Chowdhury. Weakly supervised video moment re-\ntrieval from text queries. In CVPR, 2019. 2\n[59] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.\nModeling context between objects for referring expression\nunderstanding. In ECCV, 2016. 1, 2\n[60] Mandela Patrick, Dylan Campbell, Yuki M Asano, Is-\nhan Misra Florian Metze, Christoph Feichtenhofer, Andrea\nVedaldi, Jo Henriques, et al. Keeping your eye on the ball:\nTrajectory attention in video transformers. In NeurIPS,\n2021. 1, 2\n[61] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-\n10\nnik. Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models. In ICCV,\n2015. 5\n[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020, 2021. 1\n[63] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union. In CVPR, 2019. 5\n[64] Cristian Rodriguez, Edison Marrese-Taylor, Fatemeh Sadat\nSaleh, HONGDONG LI, and Stephen Gould. Proposal-free\ntemporal moment localization of a natural-language query\nin video using guided attention. In WACV, 2020. 2, 5\n[65] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual Captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. InACL,\n2018. 2\n[66] Jing Shi, Jia Xu, Boqing Gong, and Chenliang Xu. Not all\nframes are equal: Weakly-supervised video grounding with\ncontextual similarity and visual clustering losses. In CVPR,\n2019. 2\n[67] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple\nway to prevent neural networks from overfitting. JMLR,\n2014. 13\n[68] Rui Su, Qian Yu, and Dong Xu. STVGBert: A visual-\nlinguistic transformer based framework for spatio-temporal\nvideo grounding. In ICCV, 2021. 1, 2, 7\n[69] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. VL-BERT: Pre-training of generic\nvisual-linguistic representations. In ICLR, 2019. 2\n[70] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,\nand Cordelia Schmid. VideoBERT: A joint model for video\nand language representation learning. In ICCV, 2019. 2\n[71] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. In\nEMNLP, 2019. 2\n[72] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin,\nHongxu Jiang, Qian Yu, and Dong Xu. Human-centric\nspatio-temporal video grounding with visual transformers.\nIEEE Transactions on Circuits and Systems for Video Tech-\nnology, 2021. 1, 2, 6, 7\n[73] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. Long range arena: A bench-\nmark for efficient transformers. In ICLR, 2021. 8\n[74] Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool.\nObject referring in videos with language and human gaze.\nIn CVPR, 2018. 1, 2\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017. 1\n[76] Jingwen Wang, Lin Ma, and Wenhao Jiang. Tempo-\nrally grounding language queries in videos by contextual\nboundary-aware prediction. In AAAI, 2020. 2\n[77] Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan\nYang, and Dong Yu. Improving weakly supervised visual\ngrounding by contrastive knowledge distillation. In CVPR,\n2021. 2\n[78] Peng Wang, Qi Wu, Jiewei Cao, Chunhua Shen, Lianli Gao,\nand Anton van den Hengel. Neighbourhood watch: Refer-\nring expression comprehension via language-guided graph\nattention networks. In CVPR, 2019. 2\n[79] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. Linformer: Self-attention with linear com-\nplexity. arXiv preprint arXiv:2006.04768, 2020. 8\n[80] Weining Wang, Yan Huang, and Liang Wang. Language-\ndriven temporal activity localization: A semantic matching\nreinforcement learning model. In CVPR, 2019. 2\n[81] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention. In\nICLR, 2020. 8\n[82] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra\nMalik, and Christoph Feichtenhofer. Audiovisual slow-\nfast networks for video recognition. arXiv preprint\narXiv:2001.08740, 2020. 2\n[83] Fanyi Xiao, Leonid Sigal, and Yong Jae Lee. Weakly-\nsupervised visual grounding of phrases with linguistic\nstructures. In CVPR, 2017. 2\n[84] Masataka Yamaguchi, Kuniaki Saito, Yoshitaka Ushiku,\nand Tatsuya Harada. Spatio-temporal person retrieval via\nnatural language queries. In ICCV, 2017. 2\n[85] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and\nCordelia Schmid. Just ask: Learning to answer questions\nfrom millions of narrated videos. In ICCV, 2021. 2\n[86] Sibei Yang, Guanbin Li, and Yizhou Yu. Cross-modal re-\nlationship inference for grounding referring expressions. In\nCVPR, 2019. 2\n[87] Sibei Yang, Guanbin Li, and Yizhou Yu. Dynamic graph\nattention for referring expression comprehension. In ICCV,\n2019. 2\n[88] Zhengyuan Yang, Tianlang Chen, Liwei Wang, and Jiebo\nLuo. Improving one-stage visual grounding by recursive\nsub-query construction. In ECCV, 2020. 2\n[89] Zhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing\nHuang, Dong Yu, and Jiebo Luo. A fast and accurate one-\nstage approach to visual grounding. In ICCV, 2019. 2\n[90] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. MAttNet: Modular at-\ntention network for referring expression comprehension. In\nCVPR, 2018. 2\n[91] Licheng Yu, Hao Tan, Mohit Bansal, and Tamara L Berg. A\njoint speaker-listener-reinforcer model for referring expres-\nsions. In CVPR, 2017. 2\n[92] Yitian Yuan, Tao Mei, and Wenwu Zhu. To find where you\ntalk: Temporal sentence localization in video with attention\nbased location regression. In AAAI, 2019. 2\n[93] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big\nbird: Transformers for longer sequences. InNeurIPS, 2020.\n8\n11\n[94] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,\nJae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\nMERLOT: Multimodal neural script knowledge models. In\nNeurIPS, 2021. 2\n[95] Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen,\nMingkui Tan, and Chuang Gan. Dense regression network\nfor video grounding. In CVPR, 2020. 2\n[96] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and\nLarry S Davis. MAN: Moment alignment network for nat-\nural language moment retrieval via iterative graph adjust-\nment. In CVPR, 2019. 2\n[97] Hanwang Zhang, Yulei Niu, and Shih-Fu Chang. Ground-\ning referring expressions in images by variational context.\nIn CVPR, 2018. 2\n[98] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.\nSpan-based localizing network for natural language video\nlocalization. In ACL, 2020. 2\n[99] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo\nLuo. Learning 2d temporal adjacent networks for moment\nlocalization with natural language. In AAAI, 2020. 2\n[100] Yanyi Zhang, Xinyu Li, Chunhui Liu, Bing Shuai, Yi Zhu,\nBiagio Brattoli, Hao Chen, Ivan Marsic, and Joseph Tighe.\nVidTr: Video transformer without convolutions. In ICCV,\n2021. 1, 2\n[101] Zhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao.\nCross-modal interaction networks for query-based moment\nretrieval in videos. In SIGIR, 2019. 2\n[102] Zhu Zhang, Zhou Zhao, Yang Zhao, Qi Wang, Huasheng\nLiu, and Lianli Gao. Where does it exist: Spatio-temporal\nvideo grounding for multi-form sentences. In CVPR, 2020.\n1, 2, 3, 6, 7, 13\n[103] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\nson J Corso, and Jianfeng Gao. Unified vision-language\npre-training for image captioning and VQA. InAAAI, 2020.\n2\n[104] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard\nSocher, and Caiming Xiong. End-to-end dense video cap-\ntioning with masked transformer. In CVPR, 2018. 2\n[105] Linchao Zhu and Yi Yang. ActBERT: Learning global-local\nvideo-text representations. In CVPR, 2020. 2\n[106] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: Deformable transform-\ners for end-to-end object detection. In ICLR, 2021. 2\n[107] Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, and Anton\nVan Den Hengel. Parallel attention: A unified framework\nfor visual object discovery through dialogs and queries. In\nCVPR, 2018. 2\n12\nAppendix\nIn this Appendix, we present additional visualizations of\nthe different attention mechanisms in our space-time de-\ncoder in Section A. Section B provides additional imple-\nmentation details. We then give detailed results for abla-\ntions in Section 4.2 on the VidSTG dataset [102] split by\nsentence type in Section C. Next we present an ablation of\nour fast and aggregation modules in Section D. Finally we\ndiscuss broader impact in Section E.\nA. Visualization of space, time and language\nattention patterns in the decoder\nThis section illustrates attention mechanisms of our\nspace-time decoder over space, language and time for the\nspatio-temporal video grounding example presented in Fig-\nure 7. For this example the time-aligned cross-attention for\nthe visual modality is also shown in Figure 7. We note that\nspatially, attention at each timestep is particularly focused\non humans that are receiving the sports ball and gesturing.\nAdditionally, the time-aligned cross-attention for the tex-\ntual modality is illustrated in Figure 6. We observe that\nthe words adult and grabs are the most attended overall,\nand that attention weights on the different words (e.g. sports\nand ball) vary over time. Ë†ts and Ë†te in Figure 6 denote the\npredicted start and end times of the output tube. Next, the\ntemporal self-attention is illustrated in Figure 8. We notice\nlong-range temporal interactions: a certain number of time\nqueries attend to various temporally distant time queries,\ne.g. time queries located around the start of the video be-\ntween the eighth and sixteenth frames.\nB. Additional implementation details\nIn our transformer, the number of heads is 8 and the hid-\nden dimension of the feed-forward layers is 2048. We set\nthe initial learning rates to 1eâˆ’5 for the visual backbone,\nand 5eâˆ’5 for the rest of the network. The learning rate fol-\nlows a linear schedule with warm-up for the text encoder\nand the learning rate is constant for the rest of the network.\nWe use the AdamW optimizer [53] and weight-decay1eâˆ’4.\nVideo data augmentation includes spatial random resizing,\nspatial random cropping preserving box annotations, and\ntemporal random cropping preserving the annotated time\ninterval. Dropout [67] with probability 0.1 is applied in our\ntransformer layers, and dropout with probability 0.5 is ap-\nplied in the temporal localization head. We use exponential\nmoving average with a decay rate of 0.9998, and an effec-\ntive batch size of 16 videos. For temporal stride k = 1the\nfast and aggregation modules in the encoder are not active,\nas their goal is to recover local spatial and temporal infor-\nmation when k >1.\nÌ‚ð‘¡ð‘ Ì‚ð‘¡ð‘’\nFigure 6. Time-aligned cross-attention visualization (textual\nmodality). Visualization of the attention weights between the\ntime query (y-axis) and its time-aligned visually-contextualized\ntext features (x-axis) at different times in our space-time decoder.\nThese attention weights are averaged across all 8 heads and all 6\nlayers, and renormalized by the maximum weight at each timestep\n(i.e. each row) for the purpose of visualization. Lighter colors cor-\nrespond to higher attention weights (see the colorbar on the right).\nC. Detailed ablation results\nIn this section, we provide detailed results split by\nsentence type (declarative, interrogative) on the VidSTG\ndataset for the ablation studies presented in Section 4.2.\n13\nQuery: An adultgrabsa sports balloutdoors.\ntime\ntime\ntime\ntime\nFigure 7. Time-aligned cross-attention visualization (visual modality).Top rows: Input frames with the predicted (yellow) and ground\ntruth (green) spatio-temporal tubes overlaid. Bottom rows: Visualization of the attention weights between the time query and its time-\naligned text-contextualized visual features at different times in our space-time decoder. These attention weights are averaged across all\n8 heads and all 6 layers, and renormalized by the maximum weight at each timestep for the purpose of visualization. Attention at each\ntimestep is particularly focused on humans that are receiving the sports ball and gesturing.\n14\nÌ‚ð‘¡ð‘ Ì‚ð‘¡ð‘’\nÌ‚ð‘¡ð‘ Ì‚ð‘¡ð‘’\nFigure 8. Temporal self-attention visualization.Visualization of the attention weights between the different time queries in our space-\ntime decoder. The column t corresponds to the weights of the different time queries for the time query at time t. These attention weights\nare averaged across all 8 heads and all 6 layers, and renormalized by the maximum weight at each timestep ( i.e. each column) for the\npurpose of visualization. Ë†ts and Ë†te denote the predicted start and end times of the output tube. Lighter colors correspond to higher attention\nweights (see the colorbar on the right).\nTime\nEncoding\nSelf\nAttention\nDeclarative Sentences Interrogative Sentences\nm tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU m tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU\n1. âœ— - 24.4 13.6 17.8 7.3 51.9 23.5 11.1 13.3 5.2 43.1\n2. âœ— Temporal 25.3 14.1 18.6 7.3 52.3 25.0 12.1 15.4 5.9 43.3\n3. âœ“ - 42.1 23.2 31.8 19.5 51.3 41.5 19.7 26.2 15.8 42.5\n4. âœ“ Temporal 46.4 26.6 36.1 24.7 52.8 45.6 22.5 30.8 19.8 43.6\nTable 5. Effect of the time encoding and the temporal self-attention in our space-time decoder on the VidSTG validation set.\n15\nPre-\nTraining\nDecoder Self-\nAttention Transfer\nDeclarative Sentences Interrogative Sentences\nm tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU m tIoU m vIoU vIoU\n@0.3\nvIoU\n@0.5 m sIoU\n1. âœ— âœ— 42.9 19.8 26.7 16.8 41.1 42.8 18.0 23.9 14.6 36.5\n2. âœ“ âœ— 44.0 24.5 32.9 21.5 51.5 43.6 20.8 27.5 17.2 42.6\n3. âœ“ âœ“ 46.4 26.6 36.1 24.7 52.8 45.6 22.5 30.8 19.8 43.6\nTable 6. Effect of the weight initialization for our model on the VidSTG validation set.\nFast Res. Temp.\nStride\nDeclarative Sentences Interrogative Sentences Mem.\n(GB)m tIoU m vIoU vIoU@0.3 vIoU@0.5 m sIoU m tIoU m vIoU vIoU@0.3 vIoU@0.5 m sIoU\n1. â€” 224 1 46.9 27.6 37.7 25.7 54.2 46.1 23.3 31.3 20.8 44.9 23.9\n2. âœ“ 224 2 46.6 27.4 38.0 25.7 54.3 45.5 23.0 31.3 20.7 44.7 16.2\n3. âœ“ 224 5 46.4 26.6 36.1 24.7 52.8 45.6 22.5 30.8 19.8 43.6 11.8\n4. âœ“ 288 2 47.0 28.2 38.3 26.3 55.7 46.0 24.1 32.4 22.0 46.3 23.7\n5. âœ“ 320 3 46.9 28.3 39.2 26.4 56.0 45.9 24.0 32.8 21.5 46.4 23.6\n6. âœ“ 352 4 47.2 28.7 39.6 27.1 56.4 46.6 24.2 33.2 21.7 46.2 24.4\n7. âœ— 352 4 47.1 27.1 37.4 24.1 53.7 46.2 22.9 31.3 19.6 44.0 18.1\n8. âœ“ 384 5 47.4 28.4 38.9 27.0 55.3 46.4 24.0 32.8 21.7 45.6 26.1\nTable 7. Comparison of performance-memory trade-off with various temporal strides k, frame spatial resolutions (Res.), with or without\nthe fast branch in our video-text encoder, on the VidSTG validation set.\nSpace-time decoder. We first provide detailed results for\nthe ablation on our space-time decoder. The analysis is sim-\nilar for both declarative and interrogative sentences. In de-\ntail, Table 5 shows that there is a substantial improvement\nover the space-only decoder when using both time encod-\ning and temporal self-attention (+18.3% on vIoU @0.3 for\ndeclarative sentences and +17.5% on vIoU @0.3 for inter-\nrogative sentences between rows 1 and 4). The gain comes\nmostly from the temporal localization (+22.0% on m tIoU\nfor declarative sentences and +22.1% on m tIoU for inter-\nrogative sentences), while the spatial grounding moderately\nincreases (+0.9% in m sIoU for declarative sentences and\n+0.5% in m sIoU for interrogative sentences). Further-\nmore, we can observe that the time encoding brings most\nof the gain (+14.0% onvIoU @0.3 for declarative sentences\nand +12.9% on vIoU @0.3 for interrogatives sentences be-\ntween rows 1 and 3). Finally, the temporal self-attention\nresults in an additional improvement (+4.3% on vIoU @0.3\nfor declarative sentences and +4.6% on vIoU @0.3 for in-\nterrogative sentences between rows 3 and 4) over using time\nencoding only.\nInitialization. We now provide detailed results for the ab-\nlation on our weight initialization. The analysis is simi-\nlar for both declarative and interrogative sentences. In de-\ntail, Table 6 shows that pretraining is highly beneficial for\nspatio-temporal video grounding (+9.4% on vIoU @0.3 for\ndeclarative sentences and +6.9% on vIoU @0.3 for inter-\nrogative sentences between rows 1 and 3). The gain mainly\ncomes from the spatial grounding performance (+11.7% on\nm sIoU for declarative sentences and +7.1% on m sIoU\nfor interrogative sentences). Additionally, we observe the\nbenefit of using the spatial self-attention weights from the\nMDETR decoder to initialize the temporal self-attention in\nour decoder (+3.2% onvIoU @0.3 for declarative sentences\nand +3.3% on vIoU @0.3 for interrogative sentences be-\ntween rows 2 and 3).\nImpact of spatial resolution and temporal stridek. In\nthis section, we provide detailed results on the VidSTG\ndataset for the ablation on the impact of the spatial frame\nresolution and the temporal stride k. The analysis is similar\nfor both declarative and interrogative sentences. In detail,\nTable 7 shows that increasing the resolution is an important\nfactor of performance for spatio-temporal video grounding\n(see rows 2 and 4). Our proposed video-text encoder en-\nables us to train on higher resolutions at a given memory\nusage. This leads to a better performance-memory trade-\noff (rows 4, 5, 6, 8) compared to the baseline variant with\ntemporal stride k = 1(row 1). In particular, the best spatio-\ntemporal video grounding results ( m vIoU and vIoU @R)\nare obtained with temporal stride k = 4and resolution 352\n(row 6).\nImpact of the fast branch.Finally, we provide detailed\nresults on the VidSTG dataset for the ablation on the im-\nportance of our fast branch where we compare, for the best\nvariant, temporal stride k = 4and resolution 352, our slow-\nfast video-text encoder to a slow-only variant. The analysis\nis similar for both declarative and interrogative sentences.\nBy comparing rows 6 and 7 in Table 7, our fast branch\nsignificantly improves the spatio-temporal video grounding\nperformance (+2.2% vIoU @0.3 for declarative sentences\nand +1.9% vIoU @0.3 for interrogative sentences) with low\ncomputational memory overhead. This shows that the fast\nbranch recovers useful spatio-temporal details lost by the\ntemporal sampling operation in the slow branch.\n16\nSlow Spatial\nPool. f g Declarative Sentences Interrogative Sentences\nm tIoU m vIoU vIoU@0.3 vIoU@0.5 msIoU m tIoU m vIoU vIoU@0.3 vIoU@0.5 msIoU\n1. âœ— âœ— Linear Sum + Linear 42.7 18.6 25.0 14.8 39.6 42.5 16.9 22.0 12.9 35.1\n2. âœ“ - 0 0 46.2 24.9 34.4 21.8 49.7 45.1 20.9 28.3 17.9 40.5\n3. âœ“ âœ“ Linear Sum + Linear 45.8 25.0 34.7 22.1 50.2 44.9 21.1 29.2 17.8 40.9\n4. âœ“ âœ— Linear Product + Ïƒ 46.2 26.2 36.0 23.9 52.0 45.4 22.1 30.1 18.8 43.0\n5. âœ“ âœ— Transformer Sum + Linear 46.4 26.4 36.4 23.8 52.8 45.3 22.2 30.2 19.6 43.3\n6. âœ“ âœ— Linear Sum + Linear 46.4 26.6 36.1 24.7 52.8 45.6 22.5 30.8 19.8 43.6\nTable 8. Comparison of designs for the video-text encoder, with or without the slow branch, with or without spatial pooling in the fast\nbranch, with variants of the fast module f and aggregation module g, on the VidSTG validation set.\nD. Additional Experiments\nIn this section, we provide additional ablation studies.\nAs in the ablations presented Section 4.2, unless stated oth-\nerwise, we use spatial frame resolution of 224 pixels and\ntemporal stride k = 5.\nDesign of the fast and aggregation modules.Here we\nfurther ablate the fast and aggregation modules f and g\nused in our dual-branch encoder. We report results in Ta-\nble 8. The comparison between our slow-fast design (row\n6) and the slow-only variant (row 2) is discussed in Sec-\ntion 4.2. Likewise, we compare our slow-fast design to a\nfast-only variant (row 1). The fast-only variant does not\nuse the slow multi-modal branch, in which case the video-\ntext features are the fast visual-only features concatenated\nwith the text features. As shown in Table 8, our slow-fast\ndesign outperforms the fast-only variant, showing the im-\nportance of the slow multi-modal branch. We further com-\npare the design of our fast and aggregation modules f and\ng (row 6) to other alternatives: row 3, a variant with the\nsame primitives f and g but with f operating on features\npooled over the spatial dimension; row 4, a variant which\nuses the same fast module f but a gating aggregation mod-\nule g(hv(v, t), f(v)) = Ïƒ(hv(v, t) âˆ— f(v)) where Ïƒ is the\nsigmoid function; row 5, a variant that uses the same ag-\ngregation module g but a fast temporal transformer module\nf, which models temporal interactions between spatially-\ndetailed features. As shown in Table 8, our design outper-\nforms row 3, showing that preserving spatial information\nfor each frame is crucial for the effectiveness of the fast\nbranch. Additionally, our design slightly improves over row\n4, indicating that further forcing the network to use the slow\nbranch is not helpful. Finally, our design slightly improves\nover row 5, suggesting that additional modeling of temporal\ninteractions in our encoder is not necessarily helpful.\nE. Broader Impact\nThis work is a contribution to spatio-temporal video\ngrounding and its potential positive or negative impacts de-\npend on the application. Such models may be used for video\nsurveillance and hence lead to questionable use. On the\nother hand, we believe that such methods could improve ex-\nplainability of vision and language models which may help\nto understand some of their biases. This work also ablates\nmemory usage when learning such models and thus could\nhelp promote development of lighter models with a reduced\nimpact on the environment.\n17",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8498203754425049
    },
    {
      "name": "Encoder",
      "score": 0.6762343645095825
    },
    {
      "name": "Transformer",
      "score": 0.6245131492614746
    },
    {
      "name": "Modal",
      "score": 0.4912552237510681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.471873939037323
    },
    {
      "name": "Task (project management)",
      "score": 0.4502812623977661
    },
    {
      "name": "Real-time computing",
      "score": 0.3920426666736603
    },
    {
      "name": "Computer vision",
      "score": 0.3438693583011627
    },
    {
      "name": "Voltage",
      "score": 0.08474856615066528
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210161954",
      "name": "DÃ©partement d'Informatique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1326498283",
      "name": "Institut national de recherche en sciences et technologies du numÃ©rique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I2746051580",
      "name": "UniversitÃ© Paris Sciences et Lettres",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I44504214",
      "name": "Czech Technical University in Prague",
      "country": "CZ"
    },
    {
      "id": "https://openalex.org/I4210152232",
      "name": "Institute of Informatics of the Slovak Academy of Sciences",
      "country": "SK"
    }
  ],
  "cited_by": 81
}