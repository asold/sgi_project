{
  "title": "Cache-based Statistical Language Models of English and Highly Inflected Lithuanian",
  "url": "https://openalex.org/W1713571952",
  "year": 2006,
  "authors": [
    {
      "id": "https://openalex.org/A2408646837",
      "name": "Airenas Vaičiūnas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208044182",
      "name": "Gailius Raškinis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1549285799",
    "https://openalex.org/W1665921526",
    "https://openalex.org/W18904795",
    "https://openalex.org/W6636890138",
    "https://openalex.org/W6676699771",
    "https://openalex.org/W2121847905",
    "https://openalex.org/W7071444332",
    "https://openalex.org/W6640281019",
    "https://openalex.org/W6682297131",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2968047920",
    "https://openalex.org/W158414620",
    "https://openalex.org/W6884717427",
    "https://openalex.org/W194103849",
    "https://openalex.org/W1553387275",
    "https://openalex.org/W1495314497",
    "https://openalex.org/W7043716560",
    "https://openalex.org/W1594264430",
    "https://openalex.org/W1644652583",
    "https://openalex.org/W2151315616",
    "https://openalex.org/W2112874453",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1996903695",
    "https://openalex.org/W1974094162",
    "https://openalex.org/W2072223048",
    "https://openalex.org/W2111305191"
  ],
  "abstract": "This paper investigates a variety of statistical cache-based language models built upon three corpora: English, Lithuanian, and Lithuanian base forms.The impact of the cache size, type of the decay function, including custom corpus derived functions, and interpolation technique (static vs. dynamic) on the perplexity of a language model is studied.The best results are achieved by models consisting of 3 components: standard 3-gram, decaying cache 1-gram and decaying cache 2-gram that are joined together by means of linear interpolation using the technique of dynamic weight update.Such a model led up to 36% and 43% perplexity improvement with respect to the 3-gram baseline for Lithuanian words and Lithuanian word base forms respectively.The best language model of English led up to a 16% perplexity improvement.This suggests that cache-based modeling is of greater utility for the free word order highly inflected languages.",
  "full_text": "INFORMATICA, 2006, V ol. 17, No. 1, 111–124 111\n 2006 Institute of Mathematics and Informatics, Vilnius\nCache-based Statistical Language Models of\nEnglish and Highly Inﬂected Lithuanian\nAirenas V AIˇCI ¯UNAS, Gailius RAŠKINIS\nDepartment of Applied Informatics, Vytautas Magnus University\nVileikos 8, LT-44404 Kaunas, Lithuania\ne-mail: airenas@freemail.lt, g.raskinis@if.vdu.lt\nReceived: August 2005\nAbstract.This paper investigates a variety of statistical cache-based language models built upon\nthree corpora: English, Lithuanian, and Lithuanian base forms. The impact of the cache size, type of\nthe decay function, including custom corpus derived functions, and interpolation technique (static\nvs. dynamic) on the perplexity of a language model is studied. The best results are achieved by\nmodels consisting of 3 components: standard 3-gram, decaying cache 1-gram and decaying cache\n2-gram that are joined together by means of linear interpolation using the technique of dynamic\nweight update. Such a model led up to 36% and 43% perplexity improvement with respect to\nthe 3-gram baseline for Lithuanian words and Lithuanian word base forms respectively. The best\nlanguage model of English led up to a 16% perplexity improvement. This suggests that cache-based\nmodeling is of greater utility for the free word order highly inﬂected languages.\nKey words:language models,n-grams, cache models, dynamic interpolation, perplexity reduction,\ninﬂected language, free word order language, Lithuanian.\n1. Introduction\nStatistical language models (LM) have become key components for large vocabulary con-\ntinuous speech recognition (LVCSR) systems. These models provide prior probabilities\nthat are used to rate hypothesized sentences and to disambiguate their acoustical similar-\nities.\nDuring the last few decades, much experimental work has been done in the ﬁeld of sta-\ntistical language modeling covering widespread world languages such as English, French,\nand German. The most popular modeling techniques developed for those languages are\nknown asn-grams. Althoughn-grams have shown a good performance, they are far from\noptimal because of false word independency assumption.\nLithuanian language modeling has started since 2002. Lithuanian has free word order\nand is highly inﬂected, i.e., new words are easily formed by inﬂectional afﬁxation. These\nproperties of a language result in difﬁculties of statistical modeling known as huge vocab-\nulary size, model sparseness, high perplexity, and a high out-of-vocabulary (OOV) word\nrate. The attempts to overcome the abovementioned difﬁculties of Lithuanian included\nword parsing into stems and endings (Vaiˇci¯unas and Raškinis, 2003) as well as class-\nbased modeling and modeling by morphological decomposition (Vaiˇci¯unas and Raškinis,\n112 A. Vaiˇci¯unas, G. Raškinis\n2004). In this paper, we investigate an alternative cache-based modeling. To our knowl-\nedge, the cache-based modeling of highly inﬂected free word order languages has not\nbeen attempted. The cache-based models presented in this paper are interesting in two\nrespects. They are able to adapt dynamically to the text under investigation and they have\nthe potential of catching dependencies spanning longer word sequences thann-grams\ndo. The impact of the model architecture, cache size, type of the decay function, includ-\ning custom corpus derived functions, and interpolation technique (static vs. dynamic) on\nthe perplexity of a language model is studied. Cache language models of Lithuanian are\ncompared to the corresponding English ones.\n2. Related Work\nCache-basedn-gram model for linguistic applications was ﬁrst introduced by Kuhn and\nDe Mori, (1988, 1990). It can be thought of as a usualn-gram Markov model trained on\na relatively short history of recent words of some particular wordw\ni.\nLet wi be thei−th word of a text and leth = wi−K ,...,w i−1 denote the cache or\nhistory ofwi, whereK is the size of the cache.\nLet C(h) ⩽ K be the number of words withinh belonging to the chosen vocabula-\nryV .\nLetC(wi,h ) be the number of occurrences of a wordwi withinh.\nLetC(wi−1,wi,h ) be the number of word pairswi−1,wi withinh.\nFinally, letI(condition) denote the indicator function taking the value 1 ifcondition\nis true and 0 otherwise.\nThen, the conditional probabilities of 1-gram and 2-gram cache language models can\nbe estimated by formulas (1) and (2) respectively:\nˆPH (wi|h)= C(wi,h )\nC(h) =\n∑ i−1\nj=i−K I(wi = wj )\n∑ i−1\nj=i−K I(wj ∈ V )\n, (1)\nˆPH2 (wi|wi−1,h )= C(wi−1wi,h )\nC(wi−1,h ) =\n∑ i−2\nj=i−K I(wi−1 = wj ∧ wi = wj+1)\n∑ i−2\nj=i−K I(wi−1 = wj )\n. (2)\nConditional probabilities of a 3-gram cache LM can be estimated in a similar way.\nJelineket al.(1991) showed that 2-gram and 3-gram cache outperformed 1-gram cache\nin terms of LM perplexity1. Rosenfeld (1996) and Goodman (2001) reported just minor\nimprovements of 3-gram cache over the 2-gram cache. Nevertheless, 1-gram cache lan-\nguage models are often used because of the problem of LM sparseness arising due to the\nlimited cache sizeK.\nClarkson and Robinson (1997) suggested an improvement to (1) and (2) based on the\nexperimental evidence that the probability of a word reoccurrence in a text exponentially\n1Perplexity refers to how many different equally probable words a statistical LM expects to appear in\naverage for a particular type of a context. It is estimated on the test subset of the corpora.\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian113\ndecays as the distance to that word increases. Otherwise stated, recent wordswj have\ngreater inﬂuence on probability distribution of the current wordwi. The inﬂuence dimin-\nishes as the distancei − j increases. The decay cache is used to model this phenomenon:\nˆPd(H)(wi|h)=\n∑ i−1\nj=i−K [I(wi = wj ) · d(i − j)]\n∑ i−1\nj=i−K d(i − j)\n, (3)\nˆPd(H2)(wi|wi−1,h)=\n∑ i−2\nj=i−K [I(wi−1 = wj ∧ wi = wj+1) · d(i − j)]\n∑ i−2\nj=i−K [I(wi−1 = wj ) · d(i − j)]\n, (4)\nwhere d(x) is the decay function that tends to zero as the distancex increases. Two\nexponentially decaying functions are often used:d(x)=e −bx and d(x)= ae−bx +c.T h e\ndecay speedb as well as parametersa and c are chosen experimentally or estimated by\napproximating the function of word reoccurrence, i.e., the actual histogram of distances\nbetween the two consecutive repetitions of the same word.\nBecause of a very limited cache size standalone cache models (1)–(4) are sparse and\nshould be used in combination withn-gram models built upon larger text corpora. Lin-\near interpolation is the most popular method of such combination (Jelineket al., 1991;\nIyer and Ostendorf, 1999; Clarkson, 1999; Tillmann and Ney, 1996). 1-gram and 2-\ngram cache models can be linearly interpolated with the standard word 3-gram model\nˆPW 3 (wi|wi−2,wi−1) in a way shown below:\nˆPW 3+H (wi|wi−2,wi−1)= λ · ˆPW 3 (wi|wi−2,wi−1)\n+( 1− λ) · ˆPH (wi|h), 0 ⩽ λ ⩽ 1, (5)\nˆPW 3+H+H2 (wi|wi−2,wi−1)= λW 3 ˆPW 3 (wi|wi−2,wi−1)\n+ λH ˆPH (wi|h)+ λH2 ˆPH2 (wi|wi−1,h ), (6)\n0 ⩽ λW 3 ,λ H ,λ H2 ⩽ 1,λ W 3 + λH + λH2 =1 ,\nHere,λ′s are interpolation weights optimized on the validation corpus.\nSometimes conditional interpolation formula is used (Goodman, 2001):\nˆPW 3+H+[H2](wi|wi−2,wi−1)=\n{\nˆPW 3+H+H2 (wi|wi−2,wi−1), ifwi−1 ∈ h,\nˆPW 3+H (wi|wi−2,wi−1), otherwise. (7)\nBesides the standard word 3-gram models, cache models can be interpolated with\nclass-based, skip, sentence mixture models (Goodman, 2001), topic mixture models\n(Kneser and Steinbiss, 1993; Iyer and Ostendorf, 1999), and trigger pair models (Till-\nmann and Ney, 1996).\nModels (5)–(7) are based on the static interpolation weightsλM . Dynamic model\ninterpolation weightsλM (i, hD) can also be used (Kneser and Steinbiss, 1993). The basic\nidea of this approach is that if the cache-hit2 is small in recent history the weight of the\ngeneral 3-gram component should be increased and vice versa.\n2The percentage of wordswi of the test corpus such thatC(wi,h ) > 0.\n114 A. Vaiˇci¯unas, G. Raškinis\nDynamic weights may be adapted on a word by word basis by optimizing perplexity\non the recent word historyhD = wi−D,...,w i−1:\nˆPW 3⊕H⊕H2 (wi|wi−2,wi−1)= λW 3 (i, hD) ˆPW 3 (wi|wi−2,wi−1)\n+ λH (i, hD) ˆPH (wi|h)+ λH2 (i, hD) ˆPH2 (wi|wi−1,h ), (8)\nwhere λW 3 (i, hD)+ λH (i, hD)+ λH2 (i, hD)=1 for alli, andD represents the length of\nan empirically chosen word history. TheλM (i, hD) can be estimated by an expectation\nmaximization algorithm (see (Kneser and Steinbiss, 1993; Martinet al., 1997) or (Gotoh\nand Renals, 1997)) before estimating the combined probability estimate (8). Dynamic in-\nterpolation was previously introduced in topic mixture models of highly inﬂected Slove-\nnian (Maucec and Kacic, 2001) and Finish (Siivolaet al., 2001). Some other attempts to\navoid using static interpolation weights include the deﬁnition of interpolation weights as\nthe function of a cache sizeK (Goodman, 2001) and the use of distinct weightsλ(i) for\nclasses of topic-speciﬁc and general purpose words (Federico and Bertoldi, 2001; Gotoh\nand Renals, 1997; Martinet al., 1997; Seymoreet al., 1998).\nThere is no consensus about the efﬁciency of the cache-based LM embedded in a\nspeech recognition system. Jelineket al.(1991), Rosenfeld (1996), Tillmann and Ney\n(1996) reported WER\n3 reduction, while Clarkson (1999) and Goodman (2001) reported\nWER degradation due to the use of a cache-based LM. In all those cases, the perplexity\nof the cache-based LM was signiﬁcantly better than the perplexity of a word 3-gram LM.\n3. Resources and Tools\nOur investigations were based on three corpora. The main corpus was the Lithuanian\ntext corpus compiled by the Center of Computational Linguistics at Vytautas Magnus\nUniversity (Marcinkeviˇcien˙e, 2000) containing 84 202 576 word tokens (henceforth LT\ncorpus). This corpus represented a great variety of genres and topics of the present day\nwritten Lithuanian. It was used for the investigation of cache based language modeling\nphenomena of inﬂected Lithuanian. Two auxiliary corpora were the corpus of Lithuanian\nbase forms (LTBF) and “The Sunday Times” English corpus of the year 1995 (EN). The\nLTBF corpus was derived from the LT corpus by replacing each word with its base form\n4.\nAuxiliary corpora served for inﬂected/non-inﬂected and Lithuanian/English comparison\npurposes.\nAll corpora were divided into training, validation and test subsets constituting 98%,\n1%, and 1% of the original corpora respectively. The same proportions of text genres were\nkept within all subsets. We used some text clearing: punctuation was removed, numbers\nand out-of-vocabulary (OOV) words (i.e., words found in the test subset but absent from\nthe vocabularyV ) where replaced by tags⟨num⟩ and ⟨oov⟩, respectively.\n3Word Error Rate is the standard measure of accuracy of a speech recognition system.\n4Base forms (the inﬁnitive for verbs, the singular nominative case for nouns, etc.) were obtained with the\nmorphological lemmatizer of Lithuanian (Zinkeviˇcius, 2000). In case of morphological ambiguity, the ﬁrst base\nform out of the list of possible base forms was selected.\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian115\nTable 1\nSummary of corpora characteristics\nWord tokens\nCorpus Word types\n(vocabulary) Articles Average words\nper articletraining validation testing\nLT 1158k\n84 202 k 853 k 713 k 1996 42185\nLTBF 371k\nEN 235k 40 525 k 409 k 400 k 91167 445\nMajority of our investigations were carried out using locally developed cache-based\nlanguage modeling tools. Simplen-grams were built using CMU-Cambridge Statistical\nLanguage Modeling Toolkit (Clarkson and Rosenfeld, 1997) that was extended to handle\nvocabularies of more than 65k words.\n4. Experimental Results\nWe have investigated cache-based models in order of increasing complexity. First, the\nsimple 1-gram cache, 1-gram decaying cache, and 1-gram decaying cache using dynamic\nweight adaptation were investigated. Thereafter, the best performing 1-gram cache mod-\nels were complemented with the components of 2-gram cache, 2-gram decaying cache,\nand 2-gram decaying cache using dynamic weight adaptation. Throughout all experi-\nments, cache-based LMs were compared on the basis of perplexity and perplexity im-\nprovement with respect to the baseline. The results are brieﬂy summarized in the Table 2.\nMore detailed description of our investigations is presented in the subsections that follow.\nTable 2\nSummary of cache-based language modeling experiments\nLanguage model LT, 1157k LTBF, 371k EN, 235k\nPerplexity\nˆPW3 (3-gram baseline, Kneser-Ney) 1027.21 451.27 259.46\nPerplexity improvement, %\nˆPW3+H (+1-gram cache) 24.72 30.64 12.24\nˆPW3+d(H) (+1-gram decaying cache) 28.20 34.24 13.49\nˆPW3⊕d(H) (+ dynamic weight adaptation) 29.62 35.94 13.71\nˆPW3+d(H)+[H2] (+2-gram cache, static weights) 33.51 39.55 15.69\nˆPW3+d(H)+[d(H2)] (+2-gram decaying cache) 33.32 40.13 16.02\nˆPW3⊕d(H)⊕[d(H2)] (+ dynamic weight adaptation) 36.21 43.03 16.20\n116 A. Vaiˇci¯unas, G. Raškinis\nTable 3\nPerplexities and OOV rates of 3-gram language models obtained with Kneser-Ney and Good-Turing smoothing\ntechniques\nPerplexity ofˆPW3\nCorpus V ocabulary\nsize OOV , %\nKneser-Ney\nsmoothing\nGood-Turing\nsmoothing\nLT 1157k 1027.21 1117.42 1.73\nLTBF 371k 451.27 478.68 1.15\nEN 235k 259.46 276.76 0.31\nAll experiments were carried out without cache ﬂushing5, as we wanted to investigate\nthe ability of LMs to adapt to the changes in text topics. OOV handling was realized\nin the following way: terms of typeˆP(⟨oov⟩|h) and ˆP(⟨oov⟩|wi,h ) were skipped, but\nˆP(wi|⟨oov⟩,h ), were included into perplexity calculations.\n4.1.Choice of the Baseline Language Model\nWe have chosen the conventional word-based 3-gramˆPW 3 (wi|wi−2,wi−1) including all\nsingleton 3-grams and smoothed using Kneser-Ney (Kneser and Ney, 1995) smoothing\ntechnique as our baseline model. Kneser-Ney smoothing systematically outperformed\nKatz backoff technique (Jelinek, 2001) coupled with Good-Turing smoothing (see Ta-\nble 3).\n4.2.1-gram Cache-based Models\nWe have constructed a series of 1-gram cache-based modelsˆPW 3+H for cache sizesK\nranging from 50 to 1000. Perplexity improvement and cache hit for eachK was mea-\nsured. The obtained results are summarized by Fig. 1 and Fig. 2.\n1-gram cache-based model signiﬁcantly improved perplexity with respect to baseline\nˆPW 3 by 12% (EN), 25% (LT) and 31% (LTBF). Perplexity improvement showed similar\ncache size dependency curves for both languages. The best improvements were achieved\natK = 300 (EN and LTBF) andK = 500 (LT). Cache hit estimates conﬁrmed our in-\ntuition that Lithuanian words were less used by the cache because of a bigger inﬂected\nvocabulary. Cache hit curve for LTBF was similar to that of EN, but the perplexity im-\nprovement for EN was much lower.\n4.3.1-gram Decaying Cache-based Models\nWe have investigated 1-gram cache-based modelsˆP\nW 3+d(H) with four types of decay\nfunctions.\n5The term “cache ﬂushing” means that all words are removed from the cache at the end of an article.\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian117\nFig. 1. Impact of the 1-gram cache size on the perplexity improvement.\nFig. 2. Impact of the 1-gram cache size on the cache hit.\nExponential decay function dexp\nb (x)=e −bx.\nLinear decay function6 dlinear\na (x)=m a x (a − x,0).\nGamma-like decay function7 dgamma\na;b (x)= xa−1e−bx.\nCorpus-derived decay functiondcorpus\no (x)= ∑ N\ni=x+1 I(wi = wi−x\n∧Occ(i, x)= o).\n(9)\n(10)\n(11)\n(12)\nHere,N is the size of thecorpusand Occ(i, x)= ∑ i−1\nj=i−x+1 I(wi = wj ).T h e\nexpression(wi = wi−x ∧ Occ(i, x)= o) is true if and only ifwi = wi−x and there\nis exactlyo occurrences of the same word in betweenwi and wi−x. Thus, the functions\ndcorpus\n0 (x) and dcorpus\n1 (x) represent respectively the histograms of distances between\ntwo consecutive and two next to consecutive repetitions of the same word8.\n6The linear decay function was included for comparison purposes only.\n7The popular exponential decay function has maximum at the position one. But this contradicts empirical\ndata as identical words rarely follow one another. Empirical evidence suggests that the probability of the word\nto reoccur grows from the start and then starts decaying after some position.\n8In all decaying cache experiments, we used a discrete array implementation for storing function values.\nThe maximum cache sizeK was truncated atK = 1000 for speed-up purposes. The values ofd(K) are\nrelative small forK> 1000.\n118 A. Vaiˇci¯unas, G. Raškinis\nOptimum parameters for the decay functions (9, 11) were found experimentally, by\noptimizing perplexity on the validation data set. Corpus-derived decay functions were\nindividually estimated on training subsets of LT, LTBF and EN corpora. Adding decay\nto 1-gram cache resulted in an improvement of about 3.5% for Lithuanian and 1.3% for\nEnglish models.\nThe optimum cache size and decay speed were inversely related. Thus, slower decay-\ning functions were used for the LT task as it had longer caches. It is interesting to note\nthatd\nexp\n0.01(x) was one of the best decay functions for EN corpus and actually had a decay\nspeed different from the decay speed of the distribution of word reoccurrencesdEN\n0 (x)\n(Fig. 3a, 3d). Nevertheless, taking into account the second reoccurrence of the word could\nbe of some help for both languages (Table 4, last line). It is also interesting to note that\nFig. 3. Sample decay functions normalized by dividing by the maximum.\nTable 4\nPerplexity improvements obtained with various decay function types of 1-gram cache\nPerplexity ofˆPW3+d(H)\nDecay function,d(x)\nLT (1157k) LTBF (371k) EN (235k)\nNone (best cache size) 773.31 (500) 313.00 (300) 227.71 (300)\ndlinear\n500 (x) 757.52 305.20 225.90\ndexp\n0.015(x) 756.83 302.84 225.79\ndexp\n0.01(x) 746.70 299.23 224.60\ndexp\n0.005(x) 742.13 299.15 224.99\ndexp\n0.0025(x) 750.90 305.06 227.40\ndgamma\n1.05;0.005(x) 743.22 299.88 225.19\ndgamma\n1.10;0.01(x) 746.23 299.25 224.47\ndcorpus\n0 (x) 737.56 296.07 225.17\ndcorpus\n0 (x)+ dcorpus\n1 (x) 737.49 296.14 224.46\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian119\nthe distribution of word reoccurrencesdEN\n0 (x) and dLT\n0 (x) of English and Lithuanian\nappeared to be very similar. This distribution seems to be a language independent param-\neter.\n4.4.2-gram Cache-based Models\nWe have constructed a series of 2-gram cache-based modelsˆPW 3+d(H)+[H2](7) for cache\nsizesK ranging from 50 to 50000. 2-gram cache-based modelˆPW 3+d(H)+[H2] signiﬁ-\ncantly outperformed 1-gram modelˆPW 3+d(H) (havingd(x)= dcorpus\n0 (x)+ dcorpus\n1 (x))\nby 5% (LT, LTBF), and 2% (EN). The optimumK value was about 30000, 2000 and 500\nwords for LT, LTBF and EN corpora respectively. Important differences in optimumK\nvalues could be explained by the fact that the average article size is more than 40k words\nin LT and only 445 words in EN corpus.\nAdding decay to the cache 2-gram improved LTBF and EN models, but not the LT\nmodel (Table 5). This can be explained by the fact that decay functions used “truncated”\ncache size ofK = 1000, much less than the optimum cache size for LT models. Corpus\nderived decay functions analogous to (12) seem to be best suited for Lithuanian corpora\nand exponential decay works best for the English corpus.\nAn interesting fact is that 2-gram cache hit on LTBF and even on LT was larger than\non EN texts (see Fig. 4). This can probably explain why 2-gram cache improves English\nLMs not as much as Lithuanian LMs.\n4.5.Dynamic Adaptation of Component Weights\nWe have constructed a series of 1-gram and 2-gram cache-based models of type\nˆPW 3⊕d(H) and ˆPW 3⊕d(H)⊕[d(H2)](8) forD ranging from 20 to 500. As it was ex-\npected, dynamic weight adaptation outperformed static weight optimization. The model\nTable 5\nPerplexity improvements obtained with various decay function types of 2-gram cache\nPerplexity ofˆPW3+d(H)+[d2(H2)]\nDecay function,d(x)\nLT (1157k) LTBF (371k) EN (235k)\nNone (best cache size) 683.04(30000) 272.81 (2000) 218.76 (500)\ndlinear\n1000 (x) 687.06 271.32 218.22\ndexp\n0.015(x) 688.71 272.30 218.23\ndexp\n0.01(x) 687.01 271.29 217.88\ndexp\n0.005(x) 685.48 270.51 217.67\ndexp\n0.0025(x) 686.10 270.79 217.99\ndgamma\n1.05;0.005(x) 685.55 270.54 217.69\ndgamma\n1.10;0.01(x) 686.85 271.29 217.83\ndcorpus\n0 (x) 685.11 270.28 217.76\ndcorpus\n0 (x)+ dcorpus\n1 (x) 684.97 270.19 217.89\n120 A. Vaiˇci¯unas, G. Raškinis\nFig. 4. Impact of the 2-gram cache size on the cache hit.\nFig. 5. Impact of the size of interpolation optimization historyD on the perplexity ofˆPW3⊕d(H).\nˆPW 3⊕d(H)⊕[d(H2)] added about 3% (LT, LTBF) and 0.2% (EN) of improvement. Tiny im-\nprovement of LMs built over EN corpus was probably due to the shortness of EN articles.\nThe 2-gram cache component of EN models had its utility as well as its average weight\nreduced. Thus, weight adaptation procedure could bring little gain over static weights.\nIn contrary, 2-gram cache component was extremely useful for some articles of LT and\nLTBF corpora. In this case, the dynamic weight adaptation boosted LM performance.\nThough short interpolation optimization historieshD had the potential of better adap-\ntation to the changes in article or text topic, there were no perplexity improvements for\nshort histories (D< 50 words) because of small reliability of such short histories. The\noptimum history size was found to beD = 200 for both 1-gram and 2-gram models for\nall three corpora. The perplexity grew slowly forD> 200 (see Fig. 5).\nIt is interesting to note that LMs using dynamic weight adaptation had different aver-\nage component weights for texts belonging to different stylistic categories (Table 6). For\ninstance, legal documents had averageλH2 (i, hD)=0 .2 indicating repeated usage of\nword-pair collocations.\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian121\nTable 6\nAverage weights ofˆPW3⊕d(H)⊕d(H2) components per text category of LT corpus\nAverage weights ofˆPW3⊕d(H)⊕[d(H2)](8) components\nText category\nλW3 (i, hD) λH(i, hD) λH2 (i, hD)\nNational newspapers 0.88 0.11 0.01\nTranslated philosophy 0.70 0.23 0.07\nLegal documents 0.75 0.05 0.20\n4.6.Other Approaches Related Cache-based Modeling\nRosenfeld (1996) found that the reduction of the perplexity could be achieved by using\nthe cache for rare words only. Such cache usage appeared not to be useful to Lithuanian.\nHowever, we found that some perplexity reduction could be gained by omitting certain\n“unpromising” words of the validation corpus from the cache (boosting the probabilities\nof remaining words). The unpromising words were deﬁned as those having average prob-\nability estimate given byˆP\nW 3+H lower thanˆPW 3 , i.e., wordsw havingperf(w) less than\nsome negative constant, where\nperf(w)=\nNVa l∑\ni=1\n(\nlog2 ˆPW 3+H (wi|wi−2,wi−1)\n− log2 ˆPW 3 (wi|wi−2,wi−1)\n)\n· I(wi = w)\nand the sum is over validation corpus. This approach resulted in some though negligible\nimprovement.\n5. Conclusions\nIn this paper, we described a number of experiments with the cache-based LMs of Lithua-\nnian and English. Our work conﬁrmed that signiﬁcant reduction of perplexity (43.03%,\n36.21% and 16.20% for LTBF, LT, and EN corpora respectively) could be achieved by the\nuse of the cache-based modeling. Improvements over the baseline are higher than twice\nfor Lithuanian with respect to English. English 3-gram baseline model performs relatively\nwell and is hard to improve as English has a strict word order. Simplistic claim that “worse\nmodels can be better improved” cannot explain this difference. Actually, we repeated the\nwhole set of experiments by replacing Kneser-Ney smoothed 3-grams with worse Good-\nTuring smoothed 3-grams. Perplexity improvement obtained with those worse models\nwas the same as with the better ones through the whole set of experiments. This suggests\nthat cache-based modeling brings more beneﬁts to the free word order languages by being\ncapable of capturing some dependencies that lie besides the strict word order.\n122 A. Vaiˇci¯unas, G. Raškinis\nCache improved LMs of Lithuanian word base forms (LTBF) better than LMs of\nLithuanian words (LT). This suggests that additional efﬁciency can be brought into lan-\nguage modeling of Lithuanian by methods that are able to cope with the highly inﬂected\nnature of Lithuanian.\nThe impact of different modeling techniques had similar tendencies in case of both\nlanguages. Adding 1-gram cache component to the 3-gram model brought the greatest\npart of improvement. Additional improvement was gained by adding a 2-gram cache\ncomponent, by selecting an appropriate decay function, and by replacing static compo-\nnent interpolation weights with the procedure of dynamic weight update.\nIt was found that optimal decaying function differs from the distribution of the dis-\ntances of the word reoccurrence, in general. However it is possible to construct better\ndecay functions by analyzing longer relations, for example distribution of the distance to\nthe second reoccurrence. It appeared that the best 1-gram cache size is independent of\nlanguage, i.e., it is the same for EN and LTBF tasks. Experiments conﬁrmed that longer\ncache size should be used in the 2-gram cache case. These last ﬁndings should be regarded\nwith care because of the differences in article size in Lithuanian and English corpora.\nThis research conﬁrms that cache-based modeling signiﬁcantly improves LM perple-\nxity. Our next task is to integrate them into a speech recognition system ant to investigate\ntheir impact on a speech recognition accuracy.\nReferences\nClarkson, P. (1999).Adaptation of Statistical Language Models for Automatic Speech Recognition. PhD thesis,\nCambridge University Engineering Department, Cambridge.\nClarkson, P., and R. Rosenfeld (1997). Statistical language modeling using the CMU-Cambridge Toolkit. In\nProceedings of 5th European Conference on Speech Communication and Technology. pp. 2707–2710.\nClarkson, P., and A. Robinson (1997). Language model adaptation using mixtures and an exponentially decay-\ning cache. InProceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing.\npp. 799–802.\nFederico, M., and N. Bertoldi (2001). Broadcast news LM adaptation using contemporary texts. InProceedings\nof 7th European Conference on Speech Communication and Technology, vol. A42. pp. 239–242.\nGildea, D., and T. Hofmann (1999). Topic-based language models using EM. InProceedings of 6th European\nConference on Speech Communication and Technology. pp. 2167–2170.\nGoodman, J.T. (2001). A bit of progress in language modeling.Computer Speech and Language,15(4), 403–\n434.\nGotoh, Y ., and S. Renals (1997). Document space models using latent semantic analysis. InProceedings of 5th\nEuropean Conference on Speech Communication and Technology. pp. 1443–1446.\nIyer, R., and M. Ostendorf (1999). Modeling long distance dependence in language: topic mixture vs. dynamic\ncache models. InProceedings of the IEEE Transactions on Speech and Audio Processing IEEE-SAP,v o l .7 .\npp. 30–39.\nJelinek, F. (2001).Statistical Methods for Speech Recognition. Massachusetts Institute of Technology, Cam-\nbridge.\nJelinek, F., B. Merialdo, S. Roukos and M. Strauss (1991). A dynamic LM for speech recognition. InProceed-\nings of the ARPA Workshop on Speech and Natural Language. pp. 293–295.\nKneser, R., and H. Ney (1995). Improved backing-off form-gram language modeling. InProceedings of Inter-\nnational Conference on Acoustics, Speech and Signal Processing. pp. 181–184.\nKneser, R., and V . Steinbiss (1993). On the dynamic adaptation of stochastic language models. InProceedings\nof International Conference on Acoustics, Speech and Signal Processing. pp. 586–589.\nCache-based Statistical Language Models of English and Highly Inﬂected Lithuanian123\nKuhn, R. (1988). Speech recognition and the frequency of recently used words: a modiﬁed Markov model for\nnatural language. InProceedings of 12th International Conference on Computational Linguistics. pp. 348–\n350.\nKuhn, R., and R. De Mori (1990). A cache-based natural language model for speech recognition.IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,12(6), 570–583.\nMarcinkeviˇcien˙e, R. (2000). Corpus linguistics in theory and practice.Darbai ir Dienos,24, 7–64 (in Lithua-\nnian).\nhttp://donelaitis.vdu.lt/\nMartin, S.C., J. Liermann and H. Ney (1997). Adaptive topic-dependent language modelling using word-\nbased varigrams. InProceedings of 5th European Conference on Speech Communication and Technology.\npp. 1447–1450.\nRosenfeld, R. (1996). A maximum entropy approach to adaptive statistical language modeling.Computer\nSpeech and Language,10, 187–228.\nSepesy Maucec, M., and Z. Kacic (2001). Topic detection for language model adaptation of highly-inﬂected\nlanguages by using a fuzzy comparison function. InProceedings of 7th European Conference on Speech\nCommunication and Technology, vol. A42. pp. 243–247.\nSeymore, K., S. Chen and R. Rosenfeld (1998). Nonlinear interpolation of topic models for language model\nadaptation. InProceedings of ICSLP-98.\nSiivola, V ., M. Kurimo and K. Lagus (2001). Large vocabulary statistical language modeling for continuous\nspeech recognition in Finnish. InProceedings of 7th European Conference on Speech Communication and\nTechnology, vol. B25. pp. 737–741.\nThe Lithuanian Language Corpus at Vytautas Magnus University.\nhttp://donelaitis.vdu.lt/\nTillmann, Ch., and H. Ney (1996). Statistical language modeling and word triggers. InProceedings of the\nSPECOM 96 . pp. 22–27.\nVaiˇci¯unas, A., G. Raškinis and V . Kaminskas (2004). Statistical Language Models of Lithuanian based on word\nclustering and morphological decomposition.Informatica,15(4), 565–580.\nVaiˇci¯unas, A., and G. Raškinis (2003). Statistical modeling of Lithuanian language. InProceedings of the\nConference “Information Technologies 2003”, KTU, Kaunas. pp. IX 35–40 (in Lithuanian).\nZinkeviˇcius, V . (2000). Lemuoklis – tool for morphological analysis.Darbai ir dienos, 245–274 (in Lithuanian).\nA. Vaiˇci¯unas (born in 1976) received his MSc degree in computer science from the Vy-\ntautas Magnus University in Kaunas in 2000. Presently, he is a PhD student at the same\nuniversity. His research interests are natural language modeling and speech recognition.\nG. Raškinis(born in 1972) received his MSc degree in artiﬁcial intelligence and pattern\nrecognition from the University of Pierre et Marie Curie in Paris in 1995. He received doc-\ntor’s degree in the ﬁeld of informatics (physical sciences) in 2000. Presently, he works at\nthe Center of Computational Linguistics and teaches at the Department of Applied Infor-\nmatics of VMU. His research interests include application of machine learning techniques\nto human language processing.\n124 A. Vaiˇci¯unas, G. Raškinis\nStatistiniai kalbos modeliai, naudojantys trumpalaik↪ea t m i n t↪i, angl↪u\nir lietuvi↪u kalboms\nAirenas V AIˇCI ¯UNAS, Gailius RAŠKINIS\nŠiame straipsnyje aprašomi statistini↪u kalbos modeli↪u, naudojanˇci↪u trumpalaik↪e atmint↪i, tyri-\nmai. Modeliai↪ivertinami naudojant tris skirtingus tekstynus: anglišk↪a, lietuvišk↪a ir lietuvišk↪ap a -\ngrindini↪uf o r m↪ut e k s t y n↪a. Darbe pateikiama ši↪u modeli↪u maišaties priklausomyb˙e nuo atminties\nžodži↪u kiekio, slopinanˇcios funkcijos tipo, bei modeli↪u interpoliavimo b¯udo (statinis arba di-\nnaminis). Geriausi rezultatai buvo gauti naudojant dinamiškai interpoliuotus standartin↪i 3-gramos,\nnetolimos praeities 1-gramos ir 2-gramos (su slopinanˇciomis funkcijomis) modelius. Naudojant\ntrumpalaik˙es atminties modelius maišatis sumaž˙ejo (lyginant su standartine 3-grama) 36% lietu-\nviškam, 43% lietuviškam pagrindini↪uf o r m↪u bei 16% angliškam tekstynui.",
  "topic": "Lithuanian",
  "concepts": [
    {
      "name": "Lithuanian",
      "score": 0.9387634992599487
    },
    {
      "name": "Computer science",
      "score": 0.7091965675354004
    },
    {
      "name": "Cache",
      "score": 0.5355142951011658
    },
    {
      "name": "Natural language processing",
      "score": 0.48279058933258057
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34805673360824585
    },
    {
      "name": "Linguistics",
      "score": 0.30367371439933777
    },
    {
      "name": "Parallel computing",
      "score": 0.15429335832595825
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I188730046",
      "name": "Vytautas Magnus University",
      "country": "LT"
    }
  ],
  "cited_by": 6
}