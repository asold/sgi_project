{
  "title": "GCNFORMER: graph convolutional network and transformer for predicting lncRNA-disease associations",
  "url": "https://openalex.org/W4390500642",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5006789705",
      "name": "Dengju Yao",
      "affiliations": [
        "Harbin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5041415152",
      "name": "Bailin Li",
      "affiliations": [
        "Harbin University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5109549118",
      "name": "Xiaojuan Zhan",
      "affiliations": [
        "Harbin University of Science and Technology",
        "Heilongjiang Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5101986042",
      "name": "Xiaorong Zhan",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5087576334",
      "name": "Liyang Yu",
      "affiliations": [
        "Harbin University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2166810745",
    "https://openalex.org/W2079046638",
    "https://openalex.org/W2318370297",
    "https://openalex.org/W2894088602",
    "https://openalex.org/W2462108616",
    "https://openalex.org/W611910056",
    "https://openalex.org/W2331900771",
    "https://openalex.org/W2079983771",
    "https://openalex.org/W2070777085",
    "https://openalex.org/W2802616725",
    "https://openalex.org/W2765950793",
    "https://openalex.org/W4294376371",
    "https://openalex.org/W4297183296",
    "https://openalex.org/W2136550485",
    "https://openalex.org/W2152970345",
    "https://openalex.org/W2408796111",
    "https://openalex.org/W1158178075",
    "https://openalex.org/W2739280501",
    "https://openalex.org/W2512308096",
    "https://openalex.org/W4312094581",
    "https://openalex.org/W3081960698",
    "https://openalex.org/W2772618228",
    "https://openalex.org/W2801501532",
    "https://openalex.org/W3162698679",
    "https://openalex.org/W2912626494",
    "https://openalex.org/W2531500048",
    "https://openalex.org/W3014726572",
    "https://openalex.org/W2969832535",
    "https://openalex.org/W3001440478",
    "https://openalex.org/W3013787003",
    "https://openalex.org/W2942942195",
    "https://openalex.org/W2971132843",
    "https://openalex.org/W2970249540",
    "https://openalex.org/W3136042383",
    "https://openalex.org/W2128983843",
    "https://openalex.org/W2113868616",
    "https://openalex.org/W2158135353",
    "https://openalex.org/W2153234291",
    "https://openalex.org/W4295242659",
    "https://openalex.org/W3209819349",
    "https://openalex.org/W1977878983",
    "https://openalex.org/W3175544068",
    "https://openalex.org/W3087498435",
    "https://openalex.org/W3145754360",
    "https://openalex.org/W4385475774",
    "https://openalex.org/W4280587098",
    "https://openalex.org/W4200042293",
    "https://openalex.org/W2889646458",
    "https://openalex.org/W3193448903",
    "https://openalex.org/W4283462655",
    "https://openalex.org/W3104684621",
    "https://openalex.org/W4292168331",
    "https://openalex.org/W2111632848",
    "https://openalex.org/W2594727603",
    "https://openalex.org/W1998356253",
    "https://openalex.org/W4206226061",
    "https://openalex.org/W4210805422"
  ],
  "abstract": "Abstract Background A growing body of researches indicate that the disrupted expression of long non-coding RNA (lncRNA) is linked to a range of human disorders. Therefore, the effective prediction of lncRNA-disease association (LDA) can not only suggest solutions to diagnose a condition but also save significant time and labor costs. Method In this work, we proposed a novel LDA predicting algorithm based on graph convolutional network and transformer, named GCNFORMER. Firstly, we integrated the intraclass similarity and interclass connections between miRNAs, lncRNAs and diseases, and built a graph adjacency matrix. Secondly, to completely obtain the features between various nodes, we employed a graph convolutional network for feature extraction. Finally, to obtain the global dependencies between inputs and outputs, we used a transformer encoder with a multiheaded attention mechanism to forecast lncRNA-disease associations. Results The results of fivefold cross-validation experiment on the public dataset revealed that the AUC and AUPR of GCNFORMER achieved 0.9739 and 0.9812, respectively. We compared GCNFORMER with six advanced LDA prediction models, and the results indicated its superiority over the other six models. Furthermore, GCNFORMER's effectiveness in predicting potential LDAs is underscored by case studies on breast cancer, colon cancer and lung cancer. Conclusions The combination of graph convolutional network and transformer can effectively improve the performance of LDA prediction model and promote the in-depth development of this research filed.",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nYao et al. BMC Bioinformatics            (2024) 25:5  \nhttps://doi.org/10.1186/s12859-023-05625-1\nBMC Bioinformatics\nGCNFORMER: graph convolutional network \nand transformer for predicting lncRNA-disease \nassociations\nDengju Yao1*, Bailin Li1, Xiaojuan Zhan1,2, Xiaorong Zhan3 and Liyang Yu1 \nAbstract \nBackground: A growing body of researches indicate that the disrupted expression \nof long non-coding RNA (lncRNA) is linked to a range of human disorders. Therefore, \nthe effective prediction of lncRNA-disease association (LDA) can not only suggest solu-\ntions to diagnose a condition but also save significant time and labor costs.\nMethod: In this work, we proposed a novel LDA predicting algorithm based on graph \nconvolutional network and transformer, named GCNFORMER. Firstly, we integrated \nthe intraclass similarity and interclass connections between miRNAs, lncRNAs and dis-\neases, and built a graph adjacency matrix. Secondly, to completely obtain the features \nbetween various nodes, we employed a graph convolutional network for feature \nextraction. Finally, to obtain the global dependencies between inputs and outputs, \nwe used a transformer encoder with a multiheaded attention mechanism to forecast \nlncRNA-disease associations.\nResults: The results of fivefold cross-validation experiment on the public data-\nset revealed that the AUC and AUPR of GCNFORMER achieved 0.9739 and 0.9812, \nrespectively. We compared GCNFORMER with six advanced LDA prediction models, \nand the results indicated its superiority over the other six models. Furthermore, GCN-\nFORMER’s effectiveness in predicting potential LDAs is underscored by case studies \non breast cancer, colon cancer and lung cancer.\nConclusions: The combination of graph convolutional network and transformer can \neffectively improve the performance of LDA prediction model and promote the in-\ndepth development of this research filed.\nKeywords: LncRNA-disease association prediction, Graph convolutional network, \ntransformer, Machine learning, Multiheaded attention mechanism\nIntroduction\nThe majority of transcribed sequences, classified as non-coding RNAs, do not pos -\nsess the coding capacity for proteins. Specifically, we designate those non-coding \nRNAs exceeding 200 nucleotides in length as long non-coding RNAs (lncRNAs) [1–3]. \nFor much of the past, lncRNAs were mistakenly thought of as transcription noise [4]. \n*Correspondence:   \nydkvictory@hrbust.edu.cn\n1 School of Computer \nScience and Technology, \nHarbin University of Science \nand Technology, Harbin 150080, \nChina\n2 College of Computer Science \nand Technology, Heilongjiang \nInstitute of Technology, \nHarbin 150050, China\n3 Department of Endocrinology \nand Metabolism, Hospital \nof South, University \nof Science and Technology, \nShenzhen 518055, China\nPage 2 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nHowever, in recent times, researchers worldwide have shown a notable increase in curi -\nosity regarding lncRNAs. Thanks to advancements in both experimental methodologies \nand computational prediction algorithms, the identification of thousands of lncRNAs \nhas rapidly expanded across eukaryotic organisms, encompassing organisms from \nnematodes to humans. A mounting body of researches have underscored the pervasive \ninvolvement of lncRNAs throughout the cellular life cycle, operating through diverse \nmechanisms and exerting crucial influences on various essential biological processes [5], \nsuch as regulation of gene expression, species evolution, embryonic development, mate -\nrial metabolism, and tumorigenesis [6]. For example, the lncRNA MEG8 in the PANC-1 \ncell line in pancreatic cancer is overexpressed and represses miRNA-34a and miRNA-\n203 genes, leading to the upregulation of SNAIL family transcription factors and pro -\nmoting the expression of calmodulin causing EMT [7–9].\nIn the last decade, researchers have proposed various methods for predicting poten -\ntial lncRNA-disease associations (LDAs), and these approaches have demonstrated com-\nmendable performance [10]. LncRNAs and miRNAs stand out as separate classes in the \ndomain of non-coding RNAs, each serving unique functions within the cell. Despite \ntheir divergent roles, these two RNA types exhibit intricate interconnections with one \nanother [11]. Chen et al. delve into the advancements in addressing challenges related to \nthe accurate prediction of miRNA-disease associations (MDAs) since 2017 [12]. Huang \nand Chen, along with their collaborators, conducted an extensive examination of 29 cut-\nting-edge models designed for predicting MDAs. They propose a practical evaluation \nframework that can be universally applied to ensure an impartial and systematic assess -\nment of predictive capabilities for any future models in this domain [13]. These works \nprovide useful references for designing more effective LDA prediction models. In gen -\neral, current LDA prediction methods fall under three classifications:\nThe first type of LDA prediction approach is based on biological networks with the \npremise that lncRNAs with equivalent functions are frequently connected to similar dis -\neases [14]. LRLSLDA is the first computational model in this field, which strategically \nincorporates a Laplace regularization term to constrain model parameters, preventing \nthem from becoming excessively large or small. This enhances the stability and robust -\nness of the model, particularly in the presence of noise and data perturbations. Con -\nsequently, LRLSLDA exhibits improved performance and reliability in inferring LDAs. \nThe introduction of LRLSLDA signifies innovative thinking and experimentation within \nthis research field, serving as a cornerstone for subsequent developments and investi -\ngations into related models [15]. Later, Ping and colleagues devised a binary network \nutilizing established LDAs and inferred potential LDAs by analyzing the nature of the \ndichotomous network [10]. The KATZLDA model, introduced by Chen et  al., first \nintegrates Gaussian interaction profile kernel similarity, lncRNA expression similarity, \nlncRNA functional similarity between lncRNA and diseases, and lncRNA-disease con -\nnection networks, and then applies the KATZ algorithm to forecast lncRNAs and dis -\neases [16]. The HGLDA establishes lncRNA-miRNA and miRNA-disease relationships \nand makes LDA predictions based on hypergeometric distribution tests [17]. Yu et  al. \ncombined lncRNA-miRNA, miRNA-diseases, and lncRNA-diseases associations and \npredicted LDAs by a double random walk model [18]. Chen et al. performed prediction \nby integrating the expression and semantic similarity of lncRNA and disease and using \nPage 3 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \na modified version of random walk in IRWRLDA [19]. Liu et al. discovered the link by \nincorporating lncRNA tissue specificity and representation of both genes and lncRNAs \n[20]. The LNSUBRW makes predictions of potential lncRNAs with candidate diseases \nbased on imbalanced double random walking and linear neighborhood similarity [21].\nThe second type of LDA prediction method employs matrix decomposition. The \nMFLDA is a matrix decomposition-based LDA prediction approach presented by Fu \net al. [22]. This method firstly works by splitting the adjacency matrix triple factoriza -\ntion of combining disparate data sources into low-rank matrices, then optimizes the \nweight matrix by integrating heterogeneous data sources and weighting them differently \nto the adjacency matrix, and finally makes use of the improved low-rank. The SIMCLDA \nmodel firstly uses PCA to further extract characteristics from the similarity matrix, fol -\nlowed by induction matrix complementation to make predictions for LDA pairs [23]. \nFurthermore, Liu et  al. suggested a double sparse cooperative matrix decomposition \ntechnique based on the Gaussian kernel function to forecast LDAs [24]. Xuan et al. pro -\nposed the PMFILDA which applied the probability distribution matrix for forecasting \nLDA pairs [25].\nThe third strategy is based on machine learning. Machine learning-based approaches \npredict LDAs by extracting features of lncRNAs and diseases, for example, the LDAP \nmodel, formulated on SVM principles, was conceptualized by Lan and collaborators \n[26]. To address the difficulty of learning putative representations of lncRNAs and dis -\neases, the DMFLDA model employed cascading hidden layers [27]. To solve the dif -\nficulty of lacking negative samples, Chen and colleagues were instrumental in the \ndevelopment of the LRLSLDA model, a semisupervised learning method, to find the link \nbetween lncRNAs and diseases by using two classifiers without the need for negative \nsamples. Although the LRLSLDA reduces the prerequisites for prediction, the selec -\ntion of parameters for the classifiers remains to be considered [6]. Chen et al. also pro -\nposed the ILDMSF model under the premise of fusing lncRNA similarity and disease \nsimilarity [28]. In general, machine learning-based algorithms have produced promis -\ning findings in predicting LDAs. Recently, ensemble learning strategies have also pro -\nduced positive results. Zhao et  al. designed the ABDA model, which predicted LDAs \nby using an adaptive augmentation algorithm that continuously adjusts the weighting \ncoefficients of the residual samples to make the residual samples better trained, thus \nachieving better results. Zhou and colleagues employed a fusion approach, integrating \ngradient-augmented decision trees with logistic regression, abbreviated as GBDT-LR, \nfor the prediction of LDAs [29]. Yao and colleagues employed a random forest approach \nto identify and select 100 noteworthy features, subsequently utilizing these features for \nthe prediction of LDAs [30]. Recently, deep learning has also made significant break -\nthroughs in this area. Xuan et  al. designed multiple LDA prediction models based on \nconvolutional neural network (CNN), such as CNNLDA [31], LDAPred [22], GCNLDA \n[32], and CNNDLP [33]. In addition, the VGAELDA predicted LDAs by combining vari-\national inference with a graph self-encoder [34].\nIn this paper, we proposed a novel LDA prediction model based on graph convolu -\ntional network and transformer, named GCNFORMER. Firstly, based on the correlation \nand similarity between lncRNAs, miRNAs and diseases, we integrated intraclass similar -\nity and interclass correlation between them to build a graph relational adjacency matrix. \nPage 4 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nSecondly, to completely obtain the features between nodes, we used a graph convolu -\ntional network for feature extraction. Finally, to obtain the global dependencies between \ninputs and outputs, we used a transformer with the multiheaded attention mechanism \nto predict potential LDAs. Under fivefold cross-validation, both the AUC (area under \nthe ROC curve) and the AUPR (area under the precision-recall curve) reveal the GCN -\nFORMER outperforms six other LDA prediction models. Additionally, in case studies \ninvolving breast cancer, colon cancer, and lung cancer, the GCNFORMER consistently \ndemonstrates strong performance.\nMaterials and methods\nDatasets\nDataset1 is from the work of Fu et al., which includes 240 lncRNAs, 495 miRNAs, and \n412 diseases [ 22]. In dataset1, 2697 experimentally validated LDAs were obtained from \nthe LncRNADisease [ 35] and Lnc2Cancer databases [ 36]. Meanwhile, 13,562 MDAs \nwere gained from the HMDD database [37], and 1002 lncRNA-miRNA interactions were \ngot from the starBase database [ 38]. Dataset2 is from LDAformer [ 39], which contains \n665 lncRNAs, 316 diseases, 295 miRNAs, 3833 LDAs, 2108 lncRNA-miRNA interac -\ntions, and 8540 MDAs. Dataset3 is from SVDNVLDA [ 40], which contains 861 lncR -\nNAs, 431 diseases, 437 miRNAs, 4518 LDAs, 4189 MDAs, and 8172 lncNRA-miRNA \ninteractions.\nDisease semantic similarity\nDisease Ontology (DO) provide downloadable ontology for integrating biological data \nrelated to human diseases. The terms in DO are organized in directed acyclic graphs \n(DAGs) as diseases or concepts associated with diseases [ 41]. DAGs have been widely \nused in computing disease similarity. Disease d i is defined as DAG(d)  = (Col(d), E(d)), \nwhere Col(d) denotes the node-set, which consists of both the current node and its \nancestor nodes, and E(d) signifies the collection of edges connecting parent and child \nnodes. The contribution of disease d to the ontology worth of disease W, can be deter -\nmined in two phases, as follows:\nwhere the semantic decay factor is Δ, which usually takes the value of 0.5 so that the \nsimilarity of diseases d i and d j can be calculated by the following equation:\n(1)\n{ DW (d) = 1 if d= W\nD W (d) = max\n{\n� ∗ D T\n(\nd′)\n| d′ ∈ chidren ofd\n}\nif d�=W\n(2)DV(W ) =\nd∈Col(d)\nDW (d)\n(3)DS\n(\ndi,dj\n)\n=\n∑\nt∈Col(di)∩Col(dj)\n(\nD di(d) + D dj(d)\n)\nDV (di) + DV\n(\ndj\n)\nPage 5 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nLncRNA/miRNA functional similarity\nIn terms of functionality, lncRNAs/miRNAs that share similarities are typically linked \nto comparable diseases [42]. Based on the previous work, we assume that lncRNAs or \nmiRNAs z1 and z2 , are associated with p and q diseases, respectively. One of them can be \nregarded as d i (1 ≤ i ≤ p) and d j (1 ≤ j ≤ q). As a result, the functional similarity between \nz1 and z2 can be determined using the below equation:\nModel framework\nThis paper introduced a novel LDA prediction model, GCNFORMER, with its construc -\ntion outlined in Fig.  1. Firstly, we constructed a graph relationship adjacency matrix \nbased on the intraclass and interclass relationships between lncRNA, miRNA and dis -\nease. Secondly, according to the above graph adjacency matrix, the features between the \nthree entities are further extracted by the GCN. Finally, we adopt the encoder part of \nthe transformer with its own multiheaded attention mechanism to forecast associations \nbetween lncRNAs and diseases.\n(4)Sim (z1,z2) = 1\np+q\n[∑ p\ni=1 max\n1≤j≤q\n(\nDS\n(\ndi,dj\n))\n+∑ q\nj=1 max\n1≤i≤p\n(\nDS\n(\ndj,di\n))]\n1\u00031\u0003\n1\u0003\nFig. 1 The flowchart of constructing the GCNFORMER model\nPage 6 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nGraph convolutional network\nDeep learning has grown in popularity in computational biology in recent years, in which \nthe graph convolutional network (GCN), in essence, is a feature extractor. GCN has \nexcellent graph data processing recognition ability, and it can identify node information \nand the relationship between the nodes [43]. In this work, we used a GCN for feature \nextraction. We constructed a graph network consisting of six types of graphs, including \nintraclass similarity of nodes between lncRNA and lncRNA, miRNA and miRNA, dis -\nease and disease, and interclass associations between lncRNA and disease, lncRNA and \nmiRNA, miRNA and disease. Specifically, a weighted complex graph Bcomplex = (V ,E) \nconstructed, where V is the set of nodes consisting of lncRNA, miRNA, and disease \nnodes, E is the set of edges between nodes. We define Xcomplex = (Z,S) ∈ RN t×N t , \nN t = Nl+ N d + N m  , as the adjacency matrix of Bcomplex , where S is the similarity asso -\nciation matrix of the same nodes, while Z is the association matrix of different types of \nnodes.\nwhere S denotes intraclass similarity including matrices of miRNA-miRNA similarity, \ndisease-disease similarity, and lncRNA-lncRNA similarity; Z denotes interclass associa -\ntion matrix, if there is an association, it is set to 1, otherwise, it is set to 0; ZT denotes \nthe transpose matrix of the Z matrix. After that, we set the row normalized adjacency \nmatrix Xcomplex as the feature matrix X feature.\nwhere X feature is an N t×N t matrix where each row is the eigenvector of a node in t. Firstly, \ndefine the following matrix as the adjacency matrix with self-connections ˆXcomplex , \nwhere I is the unit matrix:\nThen symmetric Laplace normalisation of ˆXcomplex yields ˜Xcomplex∈RN ×N:\nIn the above equation, E∈RN ×N is the diagonal matrix, and the matrix E is actually the \ndegree matrix of ˆXcomplex , similar to the following equation:\nThe matrix ˜Xcomplex as well as the feature matrix Xfeature are used as inputs to the graph \nconvolutional network, through which the network representation between lncRNA, \nmiRNA and disease is obtained:\n(5)X complex =\n\n\nSlncRNA−lncRNA ZlncRNA−diseaseZlncRNA−miRNA\nZT\nlncRNA−disease Sdisease−diseaseZT\nmiRNA −disease\nZT\nlncRNA−miRNA ZmiRNA−disease SmiRNA−miRNA\n\n\n(6)Xfeature=\n\n\nXl\nX d\nX m\n\n\n(7)ˆX complex = X complex + I\n(8)˜Xcomplex = E−1\n2 ˆXcomplexE−1\n2\n(9)Eii=\n∑\nj\nˆXcomplex ij\nPage 7 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nwhere W is a weight matrix, n is a hyperparameter and the operation of multiplying \nthe matrix ˜Xcomplex Xfeature can be interpreted as the integration of spatial information. \nAssuming that K = ˜Xcomplex Xfeature∈RN×N , where Ki ∈ R N , the ith row of matrix K can \nbe understood as the feature vector of the ith node. By multiplying K with the weight \nmatrix W, the node can be mapped to a low-dimensional vector Z i∈Rn , similar to Fig.  1, \nwhere the second row Z2 as well as the third row Z3 are representations of lncRNA l2 as \nwell as the disease d1 , respectively.\nTransformer\nInspired by Zhou et al., we used a transformer for the final prediction [39]. Transformer \nis a model that uses the attention mechanism to expedite model training. It performs \nwell in parallelizing the computation and understanding the relationship of data. Trans -\nformer does away with conventional CNN and RNN, and the entire network is made \nup of attention mechanisms. Transformer adds the concept of a multiheaded atten -\ntion mechanism to further improve the performance of the self-attentive layer and to \naddress the gradient vanishing issue. The transformer also uses a residual neural net -\nwork structure.\nMultihead attention\nWhen given the same set of queries, keys, and values, multiheaded attention is a design \nthat allows the model to learn several behaviors based on the same attention process, \nand then combine them. There are three inputs for the scaled dot product attention: Q, \nK, and V, i.e., three multiheads, which are finally spliced. Given the query Q ∈ Rdq、key \nK ∈ Rdk、value V ∈ Rdv , each attention header X i (i = 1,….,X) is calculated as follows:\nThe parameters that can be learned are w (q)\ni ∈ Rpq×dq , w (k)\ni ∈ Rpk×dk , w (v)\ni ∈ R\npv×dv\n . \nThe output of multiheaded attention must undergo additional linear transformation to \ncorrelate to the outcome of X head splicing. The learnable parameters are W 0 ∈ Rp0×hpv:\nAdd and norm\nThe Add and Norm operations are utilized in the transformer’s encoder layers, i.e., the \nresidual join and the layer normalization operations. Residual concatenation means add-\ning the inputs and outputs of the network, i.e.:\n(10)Z = f\n(\nX feature, ˜X complex\n)\n= Softmax( ˜X complex X featureW )\n(11)X i = f\n(\nw (q)\ni q, w(k)\ni k, w(v)\ni v\n)\n∈ R pv\n(12)W 0\n\n\nx1\nx2\nx3\n.\n.\n.\nxx\n\n\n∈ Rp0\nPage 8 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nWhen the network structure is deep, the gradient of the network backpropagation \nwhen updating the parameters, easily causes the problem of gradient disappearance, and \neach layer’s output plus x, in the derivative of every layer adds a constant, effectively \nsolving the problem of gradient disappearance. Compared with BatchNorm, we use Lay-\nerNorm here, which can normalize all features of each sample, and finally we can obtain:\nFeedforward network\nAlthough the multiheaded attention process is used to learn to articulate features, the \nresults achieved may not be particularly good. Normalization after the attention layer, in \ncombination with the activation function, can be better learned. The essence of feedfor -\nward neural networks is the ReLU activation function, namely:\nPrediction\nThe prediction score is calculated using the sigmoid activation function, and the loss \nfunction is binary cross-entropy, as shown below:\nwhere p is the prediction score, sigmoid is the activation function, W denotes the \nweights, and b denotes the bias. If the collection contains experimental records of lncR -\nNAs associated with disease, y = 1, otherwise y = 0.\nExperiments and results\nFivefold cross‑validation\nBecause of limited known LDA information and lack of unknown information, there is \nan imbalance problem in LDA prediction. By using cross-validation, model performance \nevaluation can be performed on different training and validation sets, thus minimizing \nthe impact of imbalance. In order to objectively evaluate the performance of LDA pre -\ndiction model, each fivefold cross validation experiment was performed 10 times. This \nis particularly important for dealing with imbalanced data because it reduces the eval -\nuation bias caused by random sampling and ensures that the model performs consist -\nently on multiple training and validation sets. Furthermore, the use of evaluation metrics \nappropriate for unbalanced data, such as AUPR, enables a thorough evaluation of the \nmodel’s performance across diverse classes, thus reducing the problems associated with \nsample imbalance.\nIn fivefold cross-validation, 20% of the samples were individually taken out as test set \nwhich will not be involved in the training and validation of the model but will be used for \n(13)F(x) = f(x) + x\n(14)F(x) = LayerNorm(f(x) + x)\n(15)FNN (x) = ReLU(0, xw1 + b1)w 2 + b2\n(16)Loss =− �[ylog(p) +\n(\n1 − y\n)\nlog(1 − p)]\n(17)P = sigmoid(WX + b)\nPage 9 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nthe final evaluation of the model’s performance. The remaining 80% of the samples were \nused as training set. In fivefold cross validation, these samples is divided into 5 equal \nparts. In each fold, four of these parts are used in turn to train LDA prediction model, \nand the remaining part is used as validation set. The performance metrics of the model \nwere computed and recorded. This process is repeated 5 times, and the average value of \nfivefold is used as the final prediction result of the model. Finally, the predicted results \nof the trained model on the test set are used as a basis for evaluating the model’s perfor -\nmance. Such a cross-validation approach helps in assessing the model’s performance in \na more comprehensive manner and reduces the effect of chance due to improper data \nsegmentation. Additionally, to avoid over-fitting, we fine-tune the model’s complexity by \nadjusting the number of network layers and reducing the number of units in each layer. \nIn addition, the attention mechanism assists the model in enhancing its concentration \non important parts when processing the data to prevent over-fitting.\nEvaluation indicators\nBy adopting fivefold cross-validation, the various evaluation indicators of LDA predic -\ntion models can be calculated. First, the receiver operating characteristic (ROC) curve \ncan be obtained by graphing the true positive rate (TPR) and the false positive rate (FPR) \nat various thresholds. The TPR and FPR were determined as follows, with a tighter area \nunder the curve near 1 indicating higher model performance:\nwhere true positive (TP) consists of instances that are both positive and projected to be \npositive, false positive (FP) refers to situations that are negative but are projected to be \npositive, true negative (TN) refers to situations that are negative and are projected to \nbe negative, and instances that are positive but are projected to be negative are referred \nto as false negatives (FN). In addition to the area the under ROC curve (AUC), the area \nunder the PR curve (AUPR), accuracy (Acc), F1-score (F1), and Marrs correlation coef -\nficient (Mcc) are also used to evaluate the model’s performance, shown as Eqs.  20–24, \nwhere Recall represents the recall rate and Presession represents the precision rate:\n(18)FPR= FP\nFP+ TN\n(19)TPR = TP\nTP + FN\n(20)Precision= TP\nTP + FP\n(21)Recall= TP\nTP + FN\n(22)Acc= TP + TN\nTP + FN + FP+ TN\nPage 10 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nEvaluation results\nTo evaluate the GCNFORMER’s performance, we conducted a comparison with six \ncontemporary methods for LDAs prediction, including IPCARF [44], GCLMTP [45], \nMAGCNSE [ 46], LR-GNN [47], VGAELDA [34], and SIMCLDA [23]. These methods \ninclude matrix decomposition-based methods, machine learning-based methods, and \ngraph neural network-dependent methods. Specifically, GCLMTP proposes a graph \ncomparison learning for multi-task prediction, MAGCNSE employs a multi-view graph \nconvolutional neural network, LR-GNN is based on graph neural networks for discov -\nering biologically significant molecular relationships, and VGAELDA integrates vari -\national inference and graph autoencoder. In addition, IPCARF combines incremental \nprincipal component analysis and random forest algorithms. SIMCLDA is a matrix \ndecomposition-based approach.\nIn order to make a fair comparison, we determined the hyperparameters of the \ncompared methods based on the values in the relevant literature. For IPCARF, n_esti -\nmators = 1500; for GCLMTP , the number of GCN layers was set to 2 and the node \nembedding dimension was set to 256; for MAGCNSE, the number of GCN layers was \nset to 2, the number of GCN embedding layers was set to 128, and the CNN embedding \nlayer was set to 128; for LR-GNN, the number of GCN layers was set to 3, and embed -\nding size is set to 64; for VGAELDA, the dimension of the output vector is 256; for SIM -\nCLDA αl is set to 0.8, αd is set to 0.6, and λ is set to 1. Figures  2 and 3 display the AUC \nvalues and AUPR values obtained by all seven LDA prediction models under fivefold \ncross-validation, respectively. Furthermore, Table 1 lists more performance measures for \nthe seven models involved in the comparison.\n(23)F1 = 2 × Precision× Recall\nPrecision+ Recall\n(24)Mcc= TP × TN − FP× FN√(TP + FN) × (TP + FP) × (TN + FP) × (TN + FN)\nFig. 2 ROC curves of seven LDA prediction models on dataset 1\nPage 11 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nAs one can see from Figs. 2, 3 and Table 1, the average AUC and AUPR of the IPCARF \nmodel are lower than that of the GCNFORMER by 0.5% and 1.66%; the average AUC \nand AUPR of the GCLMTP model are 0.91% and 1.96% lower than that of the GCN -\nFORMER; the average AUC and AUPR of the MAGCNSE are 2.07% and 2.19% lower \nthan that of the GCNFORMER; the average AUC and AUPR of the LR-GNN are 5.5% \nand 79.37% lower than that of the GCNFORMER; the average AUC and AUPR of the \nVGAELDA model are 6.13% and 21.54% lower than that of the GCNFORMER; the aver -\nage AUC and AUPR of the SIMCLDA are 22.84% and 89.13% lower than that of the \nGCNFORMER. These results indicate that the GCNFORMER has an excellent ability to \npredict LDAs.\nFig. 3 AUPR curves of seven LDA prediction models on dataset 1\nTable 1 The performance of seven LDA prediction models\nMethod AUC AUPR ACC F1 Mcc\nGCNFORMER 0.9739 0.9812 0.9726 0.9693 0.9461\nIPCARF 0.9689 0.9646 0.9093 0.9168 0.8403\nGCLMTP 0.9648 0.9616 0.8992 0.9006 0.8069\nMAGCNSE 0.9532 0.9593 0.9526 0.9544 0.8965\nLR-GNN 0.9189 0.1875 0.8596 0.5689 0.5598\nVGAELDA 0.9126 0.7658 0.9718 0.5863 0.6456\nSIMCLDA 0.7455 0.0899 0.7834 0.2748 0.2376\nTable 2 Performance of GCNFORMER on three datasets\nDataset AUC AUPR ACC F1 Mcc\nDataset1 0.9739 0.9812 0.9726 0.9693 0.9461\nDataset2 0.9642 0.9616 0.9196 0.9204 0.8379\nDataset3 0.9681 0.9623 0.9203 0.9289 0.8605\nPage 12 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nIn order to prove the generalization ability of the model, we tested the GCNFORMER \non three datasets separately, and the test results are shown in Table  2, which proves that \nthe model has good generalization ability.\nWe also employed a two-tailed equal variance t-test to assess the performance differ -\nences between GCNFORMER and the other methods. The two-tailed equal variance \nt-test is a hypothesis test in statistics, which is usually used to compare whether there is \na significant difference between two groups of sample means. As can be seen in Table  3, \nGCNFORMER outperforms the current state-of-the-art methods in both AUC and \nAUPR.\nGCN parameter analysis\nAs an important module of LDA prediction, the hyperparameters of the GCN have a \ngreat influence on the prediction, and poor or too many parameter settings will affect \nthe model’s performance. Therefore, experiments are used to fine-tune the model’s \nparameters. Figure  4 shows the evaluation of AUC values for various GCN layers and \ndifferent GCN embedding sizes, which demonstrates that the model performs best when \nthe GCN embedding size is 128 and the number of GCN layers is 2.\nAblation study\nFor GCNFORMER, the interclass graph Z and the intraclass graph S contain compre -\nhensive and detailed relationships, and we further conducted cauterization experi -\nments to validate the importance of both interclass and intraclass similarity graphs, and \neliminated modules from the transformer one by one to validate the importance of each \nTable 3 Differences of AUC and AUPR between GCNFORMER and the other methods via t-tests\nAUC AUPR\nIPCARF 2.65847E−10 5.48979E−17\nGCLMTP 5.22752E−29 1.07083E−34\nMAGCNSE 1.97778E−35 1.45233E−35\nLR-GNN 2.23227E−39 1.23987E−63\nVGAELDA 4.54622E−43 1.94547E−53\nSIMCLDA 3.3708E−54 1.53726E−64\nFig. 4 AUC results are compared for various GCN embedding sizes and layer counts\nPage 13 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nmodule. Tables  4 and 5 present the outcomes of the burn-in experiments, where we \nobserve that the best results are obtained by using both interclass association graphs and \nintraclass similarity graphs, and effectively validate the importance of the Add, Norm, \nand feed-forward network modules in the transformer. This may be because the fact that \nthe interclass association graph better encompasses the interconnections between lncR -\nNAs and diseases, while the intraclass similarity graph better describes the relationships \nbetween nodes, which has a crucial impact on the performance.\nCase studies\nTo conduct a more in-depth assessment of the model’s effectiveness, we validated three \nrelatively common cancers: colorectal, breast, and lung cancer using the LncRNADis -\nease v2.0 and Lnc2Cancer v3.0 datasets and some published literature data. First, known \nLDAs were used as positive samples and the same negative samples were randomly \nselected from unknown LDAs. Next, all unknown pairs of lncRNAs associated with a \nspecific disease were used as test samples. Finally, after training with the positive–nega -\ntive samples, scores were obtained and ranked using the test samples, and evidence was \nsought from relevant databases.\nColon cancer stands out as one of the deadliest malignancies affecting the digestive \nsystem [48], which is a type of malignant tumor that grows in the colon, and tends to \noccur at the junction of the rectum and sigmoid colon. We used GCNFORMER to pre -\ndict the lncRNAs linked to colon cancer, 19 of which have been supported by published \nresearch. For example, it was demonstrated that human colorectal cancer (CRC) exhib -\nits abnormal production of long noncoding RNA cell cycle protein-dependent kinase \ninhibitor 2B antisense RNA1 (CDKN2B-AS1) [49]. Increased PVT1 expression is linked \nto colon cancer incidence, disease remission, and distant metastasis. It is also linked to \nincreased expression of poor prognostic metastatic markers [50]. Table  6 shows the 20 \nlncRNAs predicted to be linked to colon cancer:\nThe most common primary malignant lung tumor is lung cancer. In the past 50 years, \nlung cancer incidence and mortality rates have been significantly rising worldwide, espe-\ncially in industrially developed countries, and lung cancer has taken first place among \nTable 4 Influence of the final result of the cauterization experiment\nInterclass association Z Intraclass similarity S AUC AUPR\n✓  × 0.9713 0.9785\n × ✓ 0.9683 0.9762\n✓ ✓ 0.9739 0.9812\nTable 5 Influence of the final result of the cauterisation experiment\nAdd Norm FN AUC AUPR\n× ✓ ✓ 0.9562 0.9654\n✓ × ✓ 0.9689 0.9769\n✓ ✓ × 0.9708 0.9776\n✓ ✓ ✓ 0.9739 0.9812\nPage 14 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nmale patients who died of cancer. Lung cancer, encompassing both non-small cell lung \ncancer (NSCLC) and small cell lung cancer (SCLC), is increasingly emerging as a lead -\ning contributor to global cancer-related mortality [51]. Table  7 summarizes the sources \nTable 6 Twenty predicted lncRNAs linked to colon cancer\nRank LncRNAname Evidence\n1 CDKN2B-AS1 LncRNADiseasev2.0Lnc2Cancerv3.0\n2 SNHG4 LncRNADiseasev2.0Lnc2Cancerv3.0\n3 AFAP1-AS1 LncRNADiseasev2.0Lnc2Cancerv3.0\n4 GAS5 LncRNADiseasev2.0Lnc2Cancerv3.0\n5 HNF1A-AS1 Lnc2Cancerv3.0\n6 KCNQ1OT1 Lnc2Cancerv3.0\n7 BANCR LncRNADiseasev2.0Lnc2Cancerv3.0\n8 NRON LncRNADiseasev2.0\n9 TUG1 LncRNADiseasev2.0Lnc2Cancerv3.0\n10 SPRY4-IT1 LncRNADiseasev2.0Lnc2Cancerv3.0\n11 H19 LncRNADiseasev2.0Lnc2Cancerv3.0\n12 BCYRN1 Lnc2Cancerv3.0\n13 PRNCR1 LncRNADiseasev2.0Lnc2Cancerv3.0\n14 CASC16 Unknown\n15 PVT1 Literature\n16 UCA1 LncRNADiseasev2.0Lnc2Cancerv3.0\n17 XIST LncRNADiseasev2.0Lnc2Cancerv3.0\n18 TP53TG1 LncRNADiseasev2.0Lnc2Cancerv3.0\n19 TUSC7 LncRNADiseasev2.0Lnc2Cancerv3.0\n20 DANCR LncRNADiseasev2.0\nTable 7 Twenty predicted lncRNAs linked to lung cancer\nRank LncRNAname Evidence\n1 CRNDE LncRNADiseasev2.0\n2 H19 LncRNADiseasev2.0Lnc2Cancerv3.0\n3 DLEU2 LncRNADiseasev2.0Lnc2Cancerv3.0\n4 HOTAIR LncRNADiseasev2.0Lnc2Cancerv3.0\n5 AFAP1-AS1 LncRNADiseasev2.0Lnc2Cancerv3.0\n6 NEAT1 LncRNADiseasev2.0Lnc2Cancerv3.0\n7 ZFAS1 Literature\n8 LINC-PINT Lnc2Cancerv3.0\n9 BCAR4 LncRNADiseasev2.0\n10 TINCR LncRNADiseasev2.0Lnc2Cancerv3.0\n11 NPSR1-AS1 Lnc2Cancerv3.0\n12 PANDAR LncRNADiseasev2.0Lnc2Cancerv3.0\n13 SOX2-OT LncRNADiseasev2.0Lnc2Cancerv3.0\n14 MEG3 LncRNADiseasev2.0Lnc2Cancerv3.0\n15 UCA1 LncRNADiseasev2.0Lnc2Cancerv3.0\n16 KIRREL3-AS3 Unknown\n17 CASC16 LncRNADiseasev2.0Lnc2Cancerv3.0\n18 RMST Unknown\n19 EWSAT1 LncRNADiseasev2.0Lnc2Cancerv3.0\n20 CBR3-AS1 Lnc2Cancerv3.0\nPage 15 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nof evidence for lncRNAs linked to lung cancer, 18 of which have been confirmed in the \nliterature. As an example, as determined by qPCR and protein blotting analysis, the \nexpression level of lncRNAH19 was significantly increased in hypoxic circumstances \nand the invasive capacity of lung cancer was greatly increased [52]. Loss-of-function \nassays showed that knockdown of ZFAS1-inhibited NSCLC cell proliferation and inva -\nsive potentials increased the rate of apoptosis of NSCLC cells in  vitro and attenuated \ntumour growth of NSCLC cells in nude mice [53].\nDespite notable strides in cancer research, breast cancer persists as a critical health \nconcern and continues to be a prominent subject of scientific investigation. Breast can -\ncer is the most frequent cancer in women around the world, and its prevalence and fatal-\nity rates are predicted to rise further [54]. Table  8 shows the origin of the evidence for \nlncRNAs linked to breast cancer, and the relevant literature has confirmed 18 of them. \nFor example, BCAR4 expression was driven in human ZR-75–1 and MCF7 breast cancer \ncells, which resulted in cell proliferation [55]. Thus, a case study of colon and lung cancer \nand breast cancer showed that GCNFORMER has good performance in predicting rel -\nevant lncRNAs.\nDiscussion\nGraph convolutional network extends convolutional operations from traditional data to \ngraph data by learning a mapping of functions through which a node can aggregate its \nfeatures with those of its neighbors to achieve a more complex network, so graph convo-\nlutional network has a superior ability to process graph data. Today, the attention mech-\nanism is frequently used in a variety of tasks, and its advantage is its ability to amplify \nthe impact of important parts of the data. Transformer itself is a model that uses the \nTable 8 Twenty predicted lncRNAs linked to breast cancer\nRank LncRNAname Evidence\n1 BCAR4 LncRNADiseasev2.0Lnc2Cancerv3.0\n2 XIST LncRNADiseasev2.0Lnc2Cancerv3.0\n3 UCA1 LncRNADiseasev2.0Lnc2Cancerv3.0\n4 SOX2-OT LncRNADiseasev2.0\n5 HOTAIR LncRNADiseasev2.0Lnc2Cancerv3.0\n6 LINC01133 LncRNADiseasev2.0Lnc2Cancerv3.0\n7 AFAP1-AS1 LncRNADiseasev2.0Lnc2Cancerv3.0\n8 LINC00961 LncRNADiseasev2.0\n9 MEG3 LncRNADiseasev2.0Lnc2Cancerv3.0\n10 EGOT LncRNADiseasev2.0Lnc2Cancerv3.0\n11 HULC LncRNADiseasev2.0\n12 GAS5 LncRNADiseasev2.0Lnc2Cancerv3.0\n13 CRNDE Lnc2Cancerv3.0\n14 CCAT2 LncRNADiseasev2.0Lnc2Cancerv3.0\n15 PVT1 LncRNADiseasev2.0Lnc2Cancerv3.0\n16 CDKN2B-AS1 LncRNADiseasev2.0\n17 TP53COR1 Unknown\n18 HOXA11-AS LncRNADiseasev2.0Lnc2Cancerv3.0\n19 MIR155HG Unknown\n20 PRNCR1 LncRNADiseasev2.0Lnc2Cancerv3.0\nPage 16 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \nattention mechanism to improve its effectiveness, and its multiheaded attention mecha -\nnism further refines the attention layer by enhancing the model’s capacity to focus on \nvarious locations as well as giving multiple representation subspaces to increase model \nperformance. In GCNFORMER model, graph convolutional network can effectively \ncapture the topology and interactions in lncRNA-disease association network, while \ntransformer can extract the contextual information under the complex relationships. \nTherefore, combining graph convolutional network with transformer can help to learn \nricher and more efficient feature representations, improve the ability to identify and \nmine key features in lncRNA-disease associations, and provide more accurate predic -\ntion. Taken together, the combination of graph convolutional network and transformer \nbrings new ideas and technical means to lncRNA-disease association prediction field, \nimproves the accuracy and explanatory ability of lncRNA-disease association prediction \nmodel, and can promote the in-depth development of the related research.\nConclusion\nIn this work, we proposed a graph convolutional network and transformer-based LDA \nprediction method (GCNFORMER). First, we constructed graph relational adjacency \nmatrices by combining intraclass similarity and interclass associations between lncR -\nNAs, diseases, and miRNAs. Second, we employed a graph convolutional network to \nfully extract the characteristics among the nodes. Finally, we implemented a transformer \nencoder to forecast potential lncRNA-disease associations. The AUC, AUPR, and some \nother evaluation indicators under fivefold cross validation outperform six other state-\nof-art lncRNA-disease association prediction models. The case study on three cancers \ndemonstrate that GCNFORMER is a useful LDA prediction model with good predic -\ntion performance. Of course, there are still some aspects which can be further improved. \nFirst, only lncRNA, miRNA, and disease information were used in current GCN -\nFORMER model. To improve the model’s effectiveness in predicting LDAs, our next step \nwill introduce more biological information into GCNFORMER. Specifically, large-scale \nmulti-omics data, including genomics, transcriptomics, proteomics, and clinical data, \ncan be further integrated, and comprehensive data analyses can be carried out to iden -\ntify potential LDAs. Second, in addition to LDA prediction, further in-depth studies on \nthe function and mechanism of lncRNAs can be carried out in the future to explore their \nspecific roles in the process of disease development, thus revealing their importance in \nthe mechanism of disease occurrence.\nAcknowledgements\nNot applicable.\nAuthor contributions\nBLL conceived and implemented the model, performed the experiments, and wrote the paper. DJY directed the research \nand revised the paper. XJZ and XRZ analyzed the experimental results and revised the paper. LYY performed the experi-\nments. All authors have read and approved the final manuscript.\nFunding\nThis work is supported by the National Natural Science Foundation of China (Grant No. 62172128). The funding body \ndid not play any roles in the design of the study and collection, analysis, and interpretation of data and in writing the \nmanuscript.\nAvailability of data and materials\nThe data and materials are available from https:// github. com/ ydkvi ctory/ GCNFO RMER.\nPage 17 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 22 October 2023   Accepted: 18 December 2023\nReferences\n 1. Djebali S, Davis CA, Merkel A, Dobin A, Lassmann T, Mortazavi A, Tanzer A, Lagarde J, Lin W, Schlesinger F, Xue C, Marinov \nGK, Khatun J, Williams BA, Zaleski C, Rozowsky J, Röder M, Kokocinski F, Abdelhamid RF, Alioto T, Gingeras TR. Landscape \nof transcription in human cells. Nature. 2012;489(7414):101–8. https:// doi. org/ 10. 1038/ natur e11233.\n 2. Pennisi E. Shining a light on the genome’s “dark matter.” Science. 2010;330(6011):1614. https:// doi. org/ 10. 1126/ scien ce. \n330. 6011. 1614.\n 3. Zhou S, Ding F, Gu X. Non-coding RNAs as emerging regulators of neural injury responses and regeneration. Neurosci \nBull. 2016;32(3):253–64. https:// doi. org/ 10. 1007/ s12264- 016- 0028-7.\n 4. Sun W, Shi Y, Wang Z, Zhang J, Cai H, Zhang J, Huang D. Interaction of long-chain non-coding RNAs and important \nsignaling pathways on human cancers (Review). Int J Oncol. 2018;53(6):2343–55. https:// doi. org/ 10. 3892/ ijo. 2018. 4575.\n 5. Chen X, Yan CC, Zhang X, You ZH. Long non-coding RNAs and complex diseases: from experimental results to computa-\ntional models. Brief Bioinform. 2017;18(4):558–76. https:// doi. org/ 10. 1093/ bib/ bbw060.\n 6. Chen X, Yan CC, Luo C, Ji W, Zhang Y, Dai Q. Constructing lncRNA functional similarity network based on lncRNA-disease \nassociations and disease semantic similarity. Sci Rep. 2015;5:11338. https:// doi. org/ 10. 1038/ srep1 1338.\n 7. Mohanty V, Gökmen-Polar Y, Badve S, Janga SC. Role of lncRNAs in health and disease-size and shape matter. Brief Funct \nGenomics. 2015;14(2):115–29. https:// doi. org/ 10. 1093/ bfgp/ elu034.\n 8. Mercer TR, Mattick JS. Structure and function of long noncoding RNAs in epigenetic regulation. Nat Struct Mol Biol. \n2013;20(3):300–7. https:// doi. org/ 10. 1038/ nsmb. 2480.\n 9. Esteller M. Non-coding RNAs in human disease. Nat Rev Genet. 2011;12(12):861–74. https:// doi. org/ 10. 1038/ nrg30 74.\n 10. Ping P , Wang L, Kuang L, Ye S, Iqbal MFB, Pei T. A Novel Method for LncRNA-disease association prediction based on an \nlncRNA-disease association network. IEEE/ACM Trans Comput Biol Bioinf. 2019;16(2):688–93. https:// doi. org/ 10. 1109/ \nTCBB. 2018. 28273 73.\n 11. Chen X, Xie D, Zhao Q, You ZH. MicroRNAs and complex diseases: from experimental results to computational models. \nBrief Bioinform. 2019;20(2):515–39. https:// doi. org/ 10. 1093/ bib/ bbx130.\n 12. Huang L, Zhang L, Chen X. Updated review of advances in microRNAs and complex diseases: taxonomy, trends and \nchallenges of computational models. Brief Bioinform. 2022;23(5):bbac358. https:// doi. org/ 10. 1093/ bib/ bbac3 58.\n 13. Huang L, Zhang L, Chen X. Updated review of advances in microRNAs and complex diseases: towards systematic evalu-\nation of computational models. Brief Bioinform. 2022;23(6):bbac407. https:// doi. org/ 10. 1093/ bib/ bbac4 07.\n 14. Clark MB, Johnston RL, Inostroza-Ponta M, Fox AH, Fortini E, Moscato P , Dinger ME, Mattick JS. Genome-wide analysis of \nlong noncoding RNA stability. Genome Res. 2012;22(5):885–98. https:// doi. org/ 10. 1101/ gr. 131037. 111.\n 15. Chen X, Yan GY. Novel human lncRNA-disease association inference based on lncRNA expression profiles. Bioinformatics. \n2013;29(20):2617–24. https:// doi. org/ 10. 1093/ bioin forma tics/ btt426.\n 16. Chen X. KATZLDA: KATZ measure for the lncRNA-disease association prediction. Sci Rep. 2015;5:16840. https:// doi. org/ \n10. 1038/ srep1 6840.\n 17. Chen X. Predicting lncRNA-disease associations and constructing lncRNA functional similarity network based on the \ninformation of miRNA. Sci Rep. 2015;5:13186. https:// doi. org/ 10. 1038/ srep1 3186.\n 18. Yu G, Fu G, Lu C, Ren Y, Wang J. BRWLDA: bi-random walks for predicting lncRNA-disease associations. Oncotarget. \n2017;8(36):60429–46. https:// doi. org/ 10. 18632/ oncot arget. 19588.\n 19. Chen X, You ZH, Yan GY, Gong DW. IRWRLDA: improved random walk with restart for lncRNA-disease association predic-\ntion. Oncotarget. 2016;7(36):57919–31. https:// doi. org/ 10. 18632/ oncot arget. 11141.\n 20. Li M, Zhao B, Yin R, Lu C, Guo F, Zeng M. GraphLncLoc: long non-coding RNA subcellular localization prediction using \ngraph convolutional networks based on sequence to graph transformation. Brief Bioinform. 2023;24(1):bbac565. https:// \ndoi. org/ 10. 1093/ bib/ bbac5 65.\n 21. Xie G, Jiang J, Sun Y. LDA-LNSUBRW: lncRNA-Disease association prediction based on linear neighborhood similarity and \nunbalanced bi-random walk. IEEE/ACM Trans Comput Biol Bioinf. 2022;19(2):989–97. https:// doi. org/ 10. 1109/ TCBB. 2020. \n30205 95.\n 22. Fu G, Wang J, Domeniconi C, Yu G. Matrix factorization-based data fusion for the prediction of lncRNA-disease associa-\ntions. Bioinformatics. 2018;34(9):1529–37. https:// doi. org/ 10. 1093/ bioin forma tics/ btx794.\n 23. Lu C, Yang M, Luo F, Wu FX, Li M, Pan Y, Li Y, Wang J. Prediction of lncRNA-disease associations based on inductive matrix \ncompletion. Bioinformatics. 2018;34(19):3357–64. https:// doi. org/ 10. 1093/ bioin forma tics/ bty327.\n 24. Liu JX, Gao MM, Cui Z, Gao YL, Li F. DSCMF: prediction of LncRNA-disease associations based on dual sparse collabora-\ntive matrix factorization. BMC Bioinform. 2021;22(Suppl 3):241. https:// doi. org/ 10. 1186/ s12859- 020- 03868-w.\nPage 18 of 19Yao et al. BMC Bioinformatics            (2024) 25:5 \n 25. Xuan Z, Li J, Yu J, Feng X, Zhao B, Wang L. A probabilistic matrix factorization method for identifying lncRNA-disease \nassociations. Genes. 2019;10(2):126. https:// doi. org/ 10. 3390/ genes 10020 126.\n 26. Lan W, Li M, Zhao K, Liu J, Wu FX, Pan Y, Wang J. LDAP: a web server for lncRNA-disease association prediction. Bioinfor-\nmatics. 2017;33(3):458–60. https:// doi. org/ 10. 1093/ bioin forma tics/ btw639.\n 27. Zeng M, Lu C, Fei Z, Wu FX, Li Y, Wang J, Li M. DMFLDA: a deep learning framework for predicting lncRNA-disease asso-\nciations. IEEE/ACM Trans Comput Biol Bioinf. 2021;18(6):2353–63. https:// doi. org/ 10. 1109/ TCBB. 2020. 29839 58.\n 28. Chen Q, Lai D, Lan W, Wu X, Chen B, Liu J, Chen YP , Wang J. ILDMSF: inferring associations between long non-coding RNA \nand disease based on multi-similarity fusion. IEEE/ACM Trans Comput Biol Bioinf. 2021;18(3):1106–12. https:// doi. org/ 10. \n1109/ TCBB. 2019. 29364 76.\n 29. Zhou S, Wang S, Wu Q, Azim R, Li W. Predicting potential miRNA-disease associations by combining gradient boosting \ndecision tree with logistic regression. Comput Biol Chem. 2020;85: 107200. https:// doi. org/ 10. 1016/j. compb iolch em. \n2020. 107200.\n 30. Yao D, Zhan X, Zhan X, Kwoh CK, Li P , Wang J. A random forest based computational model for predicting novel lncRNA-\ndisease associations. BMC Bioinform. 2020;21(1):126. https:// doi. org/ 10. 1186/ s12859- 020- 3458-1.\n 31. Xuan P , Cao Y, Zhang T, Kong R, Zhang Z. Dual Convolutional neuralnetworks with attention mechanisms based method \nfor predicting disease-related lncRNA genes. Front Genet. 2019;10:416. https:// doi. org/ 10. 3389/ fgene. 2019. 00416.\n 32. Xuan P , Pan S, Zhang T, Liu Y, Sun H. Graph convolutional network and convolutional neural network based method for \npredicting lncRNA-disease associations. Cells. 2019;8(9):1012. https:// doi. org/ 10. 3390/ cells 80910 12.\n 33. Xuan P , Sheng N, Zhang T, Liu Y, Guo Y. CNNDLP: a method based on convolutional autoencoder and convolutional \nneural network with adjacent edge attention for predicting lncRNA-disease associations. Int J Mol Sci. 2019;20(17):4260. \nhttps:// doi. org/ 10. 3390/ ijms2 01742 60.\n 34. Shi Z, Zhang H, Jin C, Quan X, Yin Y. A representation learning model based on variational inference and graph \nautoencoder for predicting lncRNA-disease associations. BMC Bioinform. 2021;22(1):136. https:// doi. org/ 10. 1186/ \ns12859- 021- 04073-z.\n 35. Chen G, Wang Z, Wang D, Qiu C, Liu M, Chen X, Zhang Q, Yan G, Cui Q. LncRNADisease: a database for long-non-coding \nRNA-associated diseases. Nucleic Acids Res. 2013;41(Database issue):D983–6. https:// doi. org/ 10. 1093/ nar/ gks10 99.\n 36. Ning S, Zhang J, Wang P , Zhi H, Wang J, Liu Y, Gao Y, Guo M, Yue M, Wang L, Li X. Lnc2Cancer: a manually curated \ndatabase of experimentally supported lncRNAs associated with various human cancers. Nucleic Acids Res. \n2016;44(D1):D980–5. https:// doi. org/ 10. 1093/ nar/ gkv10 94.\n 37. Li Y, Qiu C, Tu J, Geng B, Yang J, Jiang T, Cui Q. HMDD v2.0: a database for experimentally supported human microRNA \nand disease associations. Nucleic Acids Res. 2014;42(Database issue):D1070–4. https:// doi. org/ 10. 1093/ nar/ gkt10 23.\n 38. Yang JH, Li JH, Shao P , Zhou H, Chen YQ, Qu LH. starBase: a database for exploring microRNA-mRNA interaction maps \nfrom Argonaute CLIP-Seq and Degradome-Seq data. Nucleic Acids Res. 2011;39(Database issue):D202–9. https:// doi. \norg/ 10. 1093/ nar/ gkq10 56.\n 39. Zhou Y, Wang X, Yao L, Zhu M. LDAformer: predicting lncRNA-disease associations based on topological feature extrac-\ntion and Transformer encoder. Brief Bioinform. 2022;23(6):bbac370. https:// doi. org/ 10. 1093/ bib/ bbac3 70.\n 40. Li J, Li J, Kong M, Wang D, Fu K, Shi J. SVDNVLDA: predicting lncRNA-disease associations by Singular Value Decomposi-\ntion and node2vec. BMC Bioinform. 2021;22(1):538. https:// doi. org/ 10. 1186/ s12859- 021- 04457-1.\n 41. Li J, Gong B, Chen X, Liu T, Wu C, Zhang F, Li C, Li X, Rao S, Li X. DOSim: an R package for similarity between diseases \nbased on Disease Ontology. BMC Bioinform. 2011;12:266. https:// doi. org/ 10. 1186/ 1471- 2105- 12- 266.\n 42. Yang Q, Li X. BiGAN: LncRNA-disease association prediction based on bidirectional generative adversarial network. BMC \nBioinform. 2021;22(1):357. https:// doi. org/ 10. 1186/ s12859- 021- 04273-7.\n 43. Barr WA, Sheth RB, Kwon J, Cho J, Glickman JW, Hart F, Chatterji OK, Scopino K, Voelkel-Meiman K, Krizanc D, Thayer KM, \nWeir MP . GCN sensitive protein translation in yeast. PLoS ONE. 2020;15(9): e0233197. https:// doi. org/ 10. 1371/ journ al. \npone. 02331 97.\n 44. Zhu R, Wang Y, Liu JX, Dai LY. IPCARF: improving lncRNA-disease association prediction using incremental principal \ncomponent analysis feature selection and a random forest classifier. BMC Bioinform. 2021;22(1):175. https:// doi. org/ 10. \n1186/ s12859- 021- 04104-9.\n 45. Sheng N, Wang Y, Huang L, Gao L, Cao Y, Xie X, Fu Y. Multi-task prediction-based graph contrastive learning for inferring \nthe relationship among lncRNAs, miRNAs and diseases. Brief Bioinform. 2023;24(5):bbad276. https:// doi. org/ 10. 1093/ \nbib/ bbad2 76.\n 46. Liang Y, Zhang ZQ, Liu NN, Wu YN, Gu CL, Wang YL. MAGCNSE: predicting lncRNA-disease associations using multi-view \nattention graph convolutional network and stacking ensemble model. BMC Bioinform. 2022;23(1):189. https:// doi. org/ \n10. 1186/ s12859- 022- 04715-w.\n 47. Kang C, Zhang H, Liu Z, Huang S, Yin Y. LR-GNN: a graph neural network based on link representation for predicting \nmolecular associations. Brief Bioinform. 2022;23(1):bbab513. https:// doi. org/ 10. 1093/ bib/ bbab5 13.\n 48. Bray F, Ferlay J, Soerjomataram I, Siegel RL, Torre LA, Jemal A. Global cancer statistics 2018: GLOBOCAN estimates of inci-\ndence and mortality worldwide for 36 cancers in 185 countries. CA Cancer J Clin. 2018;68(6):394–424. https:// doi. org/ 10. \n3322/ caac. 21492.\n 49. Pan J, Lin M, Xu Z, Xu M, Zhang J, Weng Z, Lin B, Lin X. CDKN2B antisense RNA 1 suppresses tumor growth in human \ncolorectal cancer by targeting MAPK inactivator dual-specificity phosphatase 1. Carcinogenesis. 2021;42(11):1399–409. \nhttps:// doi. org/ 10. 1093/ carcin/ bgab0 77.\n 50. Luo Z, Chen R, Hu S, Huang X, Huang Z. PVT1 promotes resistance to 5-FU in colon cancer via the miR-486-5p/CDK4 \naxis. Oncol Lett. 2022;24(2):280. https:// doi. org/ 10. 3892/ ol. 2022. 13400.\n 51. Wu F, Wang L, Zhou C. Lung cancer in China: current and prospect. Curr Opin Oncol. 2021;33(1):40–6. https:// doi. org/ 10. \n1097/ CCO. 00000 00000 000703.\n 52. Li H, Wang J, Jin Y, Lin J, Gong L, Xu Y. Hypoxia upregulates the expression of lncRNA H19 in non-small cell lung cancer \ncells and induces drug resistance. Transl Cancer Res. 2022;11(8):2876–86. https:// doi. org/ 10. 21037/ tcr- 22- 1812.\nPage 19 of 19\nYao et al. BMC Bioinformatics            (2024) 25:5 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 53. Fernandez-Cuesta L, Thomas RK. Molecular pathways: targeting NRG1 fusions in lung cancer. Clin Cancer Res. \n2015;21(9):1989–94. https:// doi. org/ 10. 1158/ 1078- 0432. CCR- 14- 0854.\n 54. Anastasiadi Z, Lianos GD, Ignatiadou E, Harissis HV, Mitsis M. Breast cancer in young women: an overview. Updat Surg. \n2017;69(3):313–7. https:// doi. org/ 10. 1007/ s13304- 017- 0424-1.\n 55. Godinho M, Meijer D, Setyono-Han B, Dorssers LC, van Agthoven T. Characterization of BCAR4, a novel oncogene caus-\ning endocrine resistance in human breast cancer cells. J Cell Physiol. 2011;226(7):1741–9. https:// doi. org/ 10. 1002/ jcp. \n22503.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6447025537490845
    },
    {
      "name": "Transformer",
      "score": 0.5263981819152832
    },
    {
      "name": "Adjacency matrix",
      "score": 0.5153409838676453
    },
    {
      "name": "Graph",
      "score": 0.4885495603084564
    },
    {
      "name": "Data mining",
      "score": 0.4015711545944214
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33601564168930054
    },
    {
      "name": "Theoretical computer science",
      "score": 0.12324872612953186
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I100188998",
      "name": "Harbin University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210161462",
      "name": "Heilongjiang Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3045169105",
      "name": "Southern University of Science and Technology",
      "country": "CN"
    }
  ],
  "cited_by": 13
}