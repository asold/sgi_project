{
  "title": "Leveraging pretrained language models for seizure frequency extraction from epilepsy evaluation reports",
  "url": "https://openalex.org/W4409410170",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2774962879",
      "name": "Rashmie Abeysinghe",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2119318853",
      "name": "Shiqiang Tao",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2334402204",
      "name": "Samden D. Lhatoo",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2302686400",
      "name": "Guo-Qiang Zhang",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2147120483",
      "name": "Licong Cui",
      "affiliations": [
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2774962879",
      "name": "Rashmie Abeysinghe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119318853",
      "name": "Shiqiang Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2334402204",
      "name": "Samden D. Lhatoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2302686400",
      "name": "Guo-Qiang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147120483",
      "name": "Licong Cui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1967758872",
    "https://openalex.org/W4323806306",
    "https://openalex.org/W4319322326",
    "https://openalex.org/W2047168771",
    "https://openalex.org/W2646536716",
    "https://openalex.org/W1524327358",
    "https://openalex.org/W144720917",
    "https://openalex.org/W2089125052",
    "https://openalex.org/W2790138787",
    "https://openalex.org/W2212049882",
    "https://openalex.org/W4292299650",
    "https://openalex.org/W2418920503",
    "https://openalex.org/W4310640599",
    "https://openalex.org/W4286255110",
    "https://openalex.org/W4286376081",
    "https://openalex.org/W4210898640",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W3116477623",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W1623072288",
    "https://openalex.org/W6601375367",
    "https://openalex.org/W4282983782"
  ],
  "abstract": "Abstract Seizure frequency is essential for evaluating epilepsy treatment, ensuring patient safety, and reducing risk for Sudden Unexpected Death in Epilepsy. As this information is often described in clinical narratives, this study presents an approach to extracting structured seizure frequency details from such unstructured text. We investigated two tasks: (1) extracting phrases describing seizure frequency, and (2) extracting seizure frequency attributes. For both tasks, we fine-tuned three BERT-based models (bert-large-cased, biobert-large-cased, and Bio_ClinicalBERT), as well as three generative large language models (GPT-4, GPT-3.5 Turbo, and Llama-2-70b-hf). The final structured output integrated the results from both tasks. GPT-4 attained the best performance across all tasks with precision, recall, and F1-score of 86.61%, 85.04%, and 85.79% respectively for frequency phrase extraction; 90.23%, 93.51%, and 91.84% for seizure frequency attribute extraction; and 86.64%, 85.06%, and 85.82% for the final structured output. These findings highlight the potential of fine-tuned generative models in extractive tasks from limited text strings.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01592-4\nLeveraging pretrained language models\nfor seizure frequency extraction from\nepilepsy evaluation reports\nCheck for updates\nRashmie Abeysinghe1,2, Shiqiang Tao1,2,S a m d e nD .L h a t o o1,2, Guo-Qiang Zhang1,2,3 &L i c o n gC u i2,3\nSeizure frequency is essential for evaluating epilepsy treatment, ensuring patient safety, and reducing\nrisk for Sudden Unexpected Death in Epilepsy. As this information is often described in clinical\nnarratives, this study presents an approach to extracting structured seizure frequency details from\nsuch unstructured text. We investigated two tasks: (1) extracting phrases describing seizure\nfrequency, and (2) extracting seizure frequency attributes. For both tasks, weﬁne-tuned three BERT-\nbased models (bert-large-cased, biobert-large-cased, and Bio_ClinicalBERT), as well as three\ngenerative large language models (GPT-4, GPT-3.5 Turbo, and Llama-2-70b-hf). Theﬁnal structured\noutput integrated the results from both tasks. GPT-4 attained the best performance across all tasks\nwith precision, recall, and F1-score of 86.61%, 85.04%, and 85.79% respectively for frequency\nphrase extraction; 90.23%, 93.51%, and 91.84% for seizure frequency attribute extraction; and\n86.64%, 85.06%, and 85.82% for theﬁnal structured output. Theseﬁndings highlight the potential of\nﬁne-tuned generative models in extractive tasks from limited text strings.\nEpilepsy affects more than 3.4 million people in the United States and 65\nmillion globally. Even with the availability of various treatments, 4 out of 10\nindividuals with epilepsy will continue to have uncontrolled seizures\n(https://www.epilepsy.com/stories/number-people-epilepsy-united-states-\nall-time-high-cdc-reports). Sudden Unexpected Death in Epilepsy\n(SUDEP) refers to the sudden and unexpected death of someone with\nepilepsy who was otherwise healthy, where no other cause of death can be\nfound during an autopsy\n1. SUDEP is the leading cause of death in patients\nhaving uncontrolled seizures. Annually, more than 1 in 1000 epilepsy\npatients die from SUDEP (https://www.cdc.gov/epilepsy/sudep/index.html#:\n~:text=Causes%20of%20SUDEP&text=Heart%20problems%E2%80%94a%\n20seizure%20may,may%20also%20contribute%20to%20SUDEP). However,\nmechanisms behind SUDEP remain an active area of research\n2–4.\nA number of risk factors have been identiﬁed for SUDEP5–9.F o r\ninstance, SUDEP-7 Inventory comprises seven risk factors for assessing an\nindividual’s SUDEP risk10. Notably, four of these risk factors are associated\nwith seizure frequency, such as“More than three tonic-clonic seizures in last\nyear,”“O n eo rm o r et o n i c - c l o n i cs e i z u r e si nl a s ty e a r ,” and “One or more\nseizures of any type over the last 12 months.” Other sources such as the\nCenter for Disease Control (CDC) also recognize uncontrolled or frequent\nseizures as one of the main risk factors for SUDEP (https://www.cdc.gov/\nepilepsy/sudep/index.html#:~:text=Causes%20of%20SUDEP&text=Heart\n%20problems%E2%80%94a%20seizure%20may,may%20also%\n20contribute%20to%20SUDEP). Therefore, it is imperative to track seizure\nfrequencies of epilepsy patients to inform care planning and minimize\nSUDEP risk.\nThe Center for SUDEP Research (CSR), funded by the National\nInstitute of Neurological Disorders and Stroke, has collected clinical data\nfrom over 2,700 epilepsy patients across seven institutions in the United\nStates and Europe, to better understand risk factors and brain\nmechanisms of SUDEP. The CSR dataset encompasses a range of\nmodalities, including evaluation reports from epilepsy monitoring units\n(EMUs) and electrophysiological signals\n11–13. The EMU evaluation\nreports often contain a section detailing seizure frequency. However, this\ninformation is often presented in a narrative format (or free-text), posing\nchallenges for automated SUDEP risk assessment. These narratives vary\nwidely in terms of their content, with some segments containing single\nor multiple seizure frequencies, while others lacking any explicitly\ndeﬁned seizure frequency information. Therefore, there is a pressing\nneed to extract structured seizure frequency data from EMU evaluation\nreports automatically to assist in assessing individual SUDEP risk for\nepilepsy patients.\nAlthough natural language processing (NLP) techniques have been\nemployed in the epilepsy domain for patient identi ﬁcation, risk\n1Department of Neurology, McGovern Medical School, The University of Texas Health Science Center at Houston, Houston, TX, USA.2Texas Institute for\nRestorative Neurotechnologies, The University of Texas Health Science Center at Houston, Houston, TX, USA.3McWilliams School of Biomedical Informatics, The\nUniversity of Texas Health Science Center at Houston, Houston, TX, USA.e-mail: licong.cui@uth.tmc.edu\nnpj Digital Medicine|           (2025) 8:208 1\n1234567890():,;\n1234567890():,;\nstratiﬁcation, and prediction14, approaches to extracting seizure frequencies\nfrom clinical text are relatively rare. In one such work, Decker et al. devel-\noped a rule-based NLP pipeline to extract seizure types and frequencies from\nclinical notes\n15. Their algorithm leveraged pattern matching and regular\nexpressions to scan a given note for seizure frequency phrases and extract\nthe seizure event type and the quantitative frequency. Xie et al. explored the\nﬁne-tuning of pre-trained Bidirectional Encoder Representations from\nTransformers (BERT)-based models toclassify seizure freedom, extract\nseizure frequency, and extract the date of the last seizure\n16. They framed the\ntask of seizure frequency extraction as an extractive question-answering\ntask. However, the potential of employing more recent generative large\nlanguage models (LLMs) for extracting structured seizure frequencies from\nclinical text remains to be explored.\nThe goal of this work is to develop an automated approach for seizure\nfrequency extraction from EMU evaluation reports to facilitate SUDEP risk\nassessment. To this end, we investigated two speciﬁc tasks: (1) extracting\nphrases describing seizure frequency; and (2) extracting detailed seizure\nfrequency attributes such as seizure events and quantities. Weﬁne-tuned\nand compared different pre-trained language models, including BERT-\nbased models and generative LLMs forboth tasks. Combining the results\nfrom these tasks produces a structured representation of seizure frequency\ninformation. To the best of our knowledge, this is theﬁrst study to explore\nthe potential of LLMs for extracting structured seizure frequency details\nfrom clinical text.\nResults\nWe conducted our experiments on a CentOS Linux 7 server equipped with 8\nNVIDIA A100 GPUs, each with a memory capacity of 80GB for all models\nexcept GPT-3.5 Turbo and GPT-4. For the GPT models, we utilized the\nMicrosoft Azure OpenAI Service ( https://azure.microsoft.com/en-us/\nproducts/ai-services/openai-service). Forﬁne-tuning BERT-based models,\nwe performed 100 hyperparameter tuning trials utilizing Optuna, a tool for\nautomatic hyperparameter optimization\n17.T h eﬁnal hyperparameters for all\nthe models can be found in Supplementary Information (Supplementary\nTables 1–5).\nModel performance for seizure frequency phrase extraction\nTable 1 presents the performance of variousﬁne-tuned models for seizure\nfrequency phrase extraction on the test set in terms of precision, recall, and\nF1-score. Among the sixﬁne-tuned models, the GPT-4 model achieved the\nhighest mean precision (86.61%) and mean recall (85.04%), outperforming\nthe other models in terms of mean F1-score (85.79%). This higher mean F1-\nscore of GPT-4 compared to Llama-2-70b-hf, which obtained the second\nhighest F1-score, was foundto be statistically signiﬁcant.\nModel performance for seizure frequency attribute extraction\nTable 2 shows the performance metrics of the sixﬁne-tuned models for the\nseizure frequency attribute extraction task. Among these models, GPT-4\nyielded the highest mean precision (90.23%) and mean recall (93.51%),\noutperforming other models in terms of mean F1-score (91.84%). The\nhigher mean F1-score of GPT-4 compared with bert-large-cased which\nobtained the second highest F1-score, was found to be statistically\nsigniﬁcant.\nModel performance for structured seizure frequency extraction\nTable 3 provides a pairwise comparison of various model combinations for\nseizure frequency phrase extractionand attribute extraction to obtain\nstructured seizure frequency details, with all scores reported as F1-scores.\nThe results indicate that using GPT-4 for both seizure frequency phrase and\nattribute extraction yielded the highest performance for structured seizure\nfrequency extraction, with a mean F1-score of 85.82% (mean precision:\n86.64%, mean recall: 85.06%). The combination of GPT-3.5 Turbo for\nseizure frequency phrase extraction and GPT-4 for seizure frequency\nattribute exaction performed similarly with a mean F1-score of 85.25%. The\nperformance difference between the former and the latter was not found to\nbe statistically signiﬁcant.\nDiscussion\nIn this paper, we investigated approaches to extract structured seizure fre-\nquency information from selected small portions of unstructured clinical\ntext. The primary objective of this work was to facilitate accurate SUDEP\nrisk assessment. Given EMU reports with segments like those analyzed in\nthis work continue to be generated, our approach provides a crucial step\ntoward automating the extraction of seizure frequency information to\nfacilitate SUDEP risk assessment. However, the implications of this work\nextend far beyond SUDEP risk stratiﬁcation, providing a broader impact on\ncapturing seizure outcomes in clinicalsettings, including evaluating treat-\nment efﬁcacy and planning disease progression. By automating the\nextraction of seizure frequencies, our approach can contribute to more\nstandardized and scalable methods for tracking seizure patterns, evaluating\ntreatment efﬁcacy, disease progression, and planning of care. Since the EMU\nreports used in this work are generated by a bespoke Electronic Health\nRecord (EHR) system\n18, a tool based on this work could be integrated into\nthe system, allowing clinicians to view and conﬁrm structured seizure fre-\nquencies in real-time as they document the notes.\nTo achieve our objectives, we explored various pre-trained models to\nextract seizure frequency phrases and seizure frequency attributes from free-\ntext. We aimed to integrate the output of such phrase extraction and\nattribute extraction models to come up with structured seizure frequencies.\nUntil recently, sequence labeling approaches based on encoder-only models\nsuch as BERT and its derivatives were the go-to models for such natural\nlanguage understanding (NLU) tasks\n19. Our aim in this work was not only to\ncome up with an effective method for obtaining structured seizure fre-\nquencies to facilitate the SUDEP risk assessment of epilepsy patients, but\nalso to provide a comparison of the performance of a number of popular\nBERT-based models and more recent generative models. Interestingly, our\nresults revealed that GPT-4 outperforms all BERT-based models as well as\nLlama-2 and GPT-3.5 Turbo for both seizure frequency phrase and attribute\nextraction. However, the bert-large-cased model came in second for seizure\nTable 1 | Performance of variousﬁne-tuned models for seizure\nfrequency phrase extraction on the test set\nModel Precision (%) Recall (%) F1-score (%)\nbert-large-cased 77.33 ± 4.24 71.95 ± 4.65 74.51 ± 4.2\nbiobert-large-cased 78.83 ± 4.06 75.43 ± 4.27 77.06 ± 3.86\nBio_ClinicalBERT 70.12 ± 4.79 65.8 ± 4.58 67.84 ± 4.29\nLlama-2-70b-hf 80.72 ± 4.16 80.69 ± 3.65 80.68 ± 3.58\nGPT-3.5 Turbo 84.53 ± 3.85 77.13 ± 4.15 80.64 ± 3.81\nGPT-4 86.61 ± 4.28 85.04 ± 3.51 85.79 ± 3.59\nData are shown as mean ± standard deviation. Highest scores are highlighted.\nTable 2 | Performance of differentﬁne-tuned models for\nseizure frequency attribute extraction on the test set\nModel Precision (%) Recall (%) F1-score (%)\nbert-large-cased 87.19 ± 1.74 90.9 ± 1.35 89 ± 1.43\nbiobert-large-cased 87.45 ± 1.68 90.28 ± 1.34 88.84 ± 1.38\nBio_ClinicalBERT 83.98 ± 1.97 88.05 ± 1.56 85.96 ± 1.67\nLlama-2-70b-hf 84.64 ± 2.68 85.83 ± 2.17 85.23 ± 2.33\nGPT-3.5 Turbo 88.99 ± 1.62 90.23 ± 1.7 87.91 ± 1.61\nGPT-4 90.23 ± 1.7 93.51 ± 1.21 91.84 ± 1.36\nData are shown as mean ± standard deviation. Highest scores are highlighted.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 2\nfrequency attribute extraction, while Llama-2 and GPT-3.5 Turbo followed\nGPT-4 for seizure frequency phrase extraction.\nBuilding on theseﬁndings, we also evaluated GPT-4’s ability to handle\ninstances lacking explicitly deﬁned seizure frequency information. When\nGPT-4 was applied to the test set, it identiﬁed 104 instances as lacking\nexplicitly deﬁned seizure frequency information, 98 of which were correct\n(precision of 94.23%). On the other hand, the test set contained 99 instances\nlacking explicitly deﬁned seizure frequency information, from which the\nmodel correctly identiﬁed 98 (a recall of 98.99%). This resulted in an F1-\nscore of 96.55% for correctly detecting instances lacking explicitly deﬁned\nseizure frequency information.\nIn this work, the structured seizure frequencies were obtained by\nintegrating the individual outputs from phrase extraction and attribute\nextraction models. An alternative approach is to extract frequency phrases\nﬁrst and then perform attribute extraction within these phrases. Testing\nthis approach with GPT-4 resulted in a mean precision of 84.9% ± 4.47, a\nmean recall of 83.36% ± 3.75, and a mean F1-score of 84.1% ± 3.84.\nThough this F1-score is lower than the model with the highest F1-score for\nstructured frequency extraction (leveraging GPT-4 for both frequency\nphrase and attribute extraction), their difference was not found to be\nstatistically signiﬁcant. As an example, consider the text“automotor sei-\nzure lasting 1–2 minutes happening up to 10 times per day. Progress to\ntonic-clonic seizures (once every 3–4 months).” This alternative approach\nincorrectly obtained the structured frequency [Event =“automotor sei-\nzure”, Minimum duration =“1”, Maximum duration =“2”, Quantity =\n“10”, Temporal unit =“day”], while our original approach obtained the\ncorrect structured frequency [Event =“automotor seizure”, Quantity =\n“10”, Temporal unit =“day”]. Both approaches correctly identiﬁed the\nother structured frequency in this text: [Event =“tonic-clonic seizures”,\nQuantity =“once”, Minimum duration =“3”, Maximum duration =“4”,\nTemporal unit =\n“months”].\nOff-the-shelf use of modern LLMshas gained attention for their\npotential to perform tasks without any task-speciﬁc ﬁne-tuning. However,\nour ﬁndings indicate that such approaches are not as effective in extracting\nstructured seizure frequencies from unstructured text. We employed GPT-\n4o in a few-shot setting to assess its effectiveness in this task. A tailored\nprompt containing multiple examples covering different types of attributes\nused in the work was developed (see Supplementary Note 1 for the prompt).\nThe prompt guides GPT-4o through examples to respond with structured\nf r e q u e n c i e si nag i v e nJ S O N - l i k ef o r m a t .W ea p p l i e dt h i sa p p r o a c ht ot h e\ntest set and observed that this few-shot strategy only achieved a mean\nprecision of 49.55% ± 5.44, a mean recall of 47.34% ± 4.91, and a mean F1-\nscore of 48.36% ± 4.86 across 10,000 bootstrap trials. This underscores the\nimportance as well as the necessity ofﬁne-tuning such models for optimal\nperformance.\nThe approaches discussed in this paper were developed for extracting\nseizure frequency information from free-text segments in a speciﬁcs u b -\nsection of the EMU reports reserved for holding seizure frequency infor-\nmation. We performed an initial investigation into how such models would\nperform when presented with texts that are not speciﬁcally focused on\nseizure frequencies. We applied the GPT-4 frequency phrase extraction and\nattribute extraction models on the texts under the patient history section of\n30 randomly picked EMU reports which were on average 274 words long.\nAmong these, 9 reports contained 15 seizure frequencies. However, the\npreliminary results indicated that the approach only correctly identiﬁed a\nsingle structured seizure frequency while incorrectly identifying 2. We\nfurther experimented with splitting the longer passages into sentences and\nthen applying the approach to extract structured seizure frequencies indi-\nvidually on these sentences. This strategy led to the model accurately\nextracting 13 out of the 15 seizure frequencies. However, the approach also\nincorrectly extracted 17 structured frequencies. Therefore, we believe that\nthe appropriate course of action to apply the approach for general text is to\nretrain the models on a dataset containing such text. The same pipeline that\nwas used in this work could be adapted with a dataset containing\ngeneral text.\nIn order to understand how the frequency values in structured fre-\nquencies extracted by different models compare with the actual frequency\nvalues, we further performed an analysis using Mean Absolute Error\n(MAE). To facilitate comparison, we normalized all extracted structured\nfrequencies to a“per day”basis. Whenever a structured frequency extracted\nby a model had missing data that would hinder normalization (such as\nmissing a temporal unit), we assigned the average seizure frequency for\nstructured frequencies across the test set as a placeholder frequency for those\ninstances. We ran 10,000 bootstrap trials using the same bootstrap samples\nfrom the earlier performance evaluation. For each trial, we computed the\nMean Absolute Error (MAE) between the frequencies extracted by the\nmodel and the gold standard annotations. The resulting mean MAEs and\nstandard deviations across all 10,000 trials are presented in Table4.A s\nshown, GPT-4 when used for both seizure frequency phrase and attribute\nextraction produced the lowest MAE of 0.0594. In comparison, using GPT-4\nfor phrase extraction and bert-large-case for attribute extraction resulted in a\nsimilar MAE of 0.0693, with the difference being statistically insigniﬁcant.\nTo understand and analyze the impact of theﬁndings on SUDEP risk\nassessment, we computed the SUDEP-7 scores based on the structured\nseizure frequencies extracted by different models from the test set, as well as\ntheir gold standard manual annotation. These scores were based on theﬁrst\nfour risk factors in the SUDEP-7 inventory, as they directly correspond to\nseizure frequency\n10. We also performed 10,000 bootstrap trials on the same\nbootstrap samples obtained earlier. In each trial, we computed the MAE\nbetween the SUDEP-7 score calculated by a model’s structured seizure\nfrequency extraction and the gold standard annotation. The mean MAEs\nand standard deviations computed across 10,000 bootstrap trials are given in\nTable 5. As can be seen, using GPT-4 for both phrase extraction and\nattribute extraction provided the lowest mean MAE of 0.1101 while using\nGPT-4 for phrase extraction and bert-large-cased for attribute extraction\nperformed similarly with an MAE of 0.12. Their difference was not found to\nbe statistically signiﬁcant. Note that erroneous extraction of structured\nseizure frequencies may not always lead to inaccurate SUDEP-7 risk scores.\nFor example, for the string“0.5 per month,” using GPT-4 for both phrase\nextraction and attribute extraction correctly obtains the structured fre-\nquency [Quantity =“0.5”, Temporal unit =“month”] while using GPT-4 for\nphrase extraction and biobert-large cased for attribute extraction incorrectly\nTable 3 | Pairwise comparison of model combinations for extracting structured seizure frequency details\nAttributes Phrase bert-large-cased biobert-large-cased Bio_ClinicalBERT Llama-2-70b-hf GPT-3.5 Turbo GPT-4\nbert-large-cased 73.34 ± 4.32 79.6% ± 3.59 69.67 ± 4.15 80.68 ± 3.26 81.62 ± 3.65 83.17 ± 3.67\nbiobert-large-cased 71.49 ± 4.24 75.04 ± 3.85 69.4 ± 4.18 78.06 ± 3.63 77.03 ± 3.91 80.52 ± 3.85\nBio_ClinicalBERT 68.44 ± 4.5 76.86 ± 3.78 66.91 ± 4.18 78.05 ± 3.42 78.86 ± 3.84 78.73 ± 3.91\nLlama-2-70b-hf 70.59 ± 4.44 78.13 ± 3.82 67.83 ± 4.22 81.04 ± 3.31 80.7 ± 3.73 83.15 ± 3.72\nGPT-3.5 Turbo 70.93 ± 4.17 78.13 ± 3.72 69.72 ± 4.1 81.05 ± 3.47 82.52 ± 3.63 83.53 ± 3.73\nGPT-4 74.84 ± 4.12 77.78 ± 3.75 70.01 ± 4.06 81.93 ± 3.26 85.25 ± 3.39 85.82 ± 3.56\nThe scores listed are all F1-scores (%). Data are shown as mean ± standard deviation. The highest scores are highlighted.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 3\nobtains [Quantity =“5”,T e m p o r a lu n i t=“month”]. However, both of these\nonly satisfy the third risk fact o ri nt h eS U D E P - 7I n v e n t o r y :“One or more\nseizures of any type over the last 12 months,” both leading to the accurate\nSUDEP-7 risk score of 1. It must benoted that while such incorrect\nextractions may not impact the analysis of SUDEP risk by SUDEP-7\ninventory, as the extracted seizure frequencies are inherently wrong, they\nmay still affect other tasks leveragingthese structured frequencies, such as\nclinical decision-making and downstream data analysis for research.\nOur initial training set consisted of 470 instances. To understand the\neffect of training set size on the performance of models for extracting seizure\nfrequency phrases and attributes, we experimented with smaller sets of 370,\n270, 170, and 70 training instances, randomly chosen from the original\ntraining set. Figure1 depicts the F1-scores for seizure frequency phrase\nextraction models trained on these different sizes. Both GPT models’F1-\nscores took a notable drop at 170 traininginstances. The biobert-large-cased\nmodel, which had the 4th best performance with 470 training instances,\nexperienced a notable decline at smaller training instances. Other models\nmaintained reasonable performance down to 170 instances compared to\ntheir full training set performance. Figure2 presents the F1-scores for the\nseizure frequency attribute extraction models across various training set\nsizes. Similar to the seizure frequency phrase extraction, both GPT models\nagain showed a notable decrease in performance at 170 training instances.\nOther models experienced a notable drop in performance when the training\nset was reduced from 170 to 70 instances, although the biobert-large-cased\nand bert-large-cased models maintained good F1-scores at 170 instances.\nThere are instances where the model makes extraction errors. In some\ncases, the models may incorrectly identify an entity. For example, in\nextracting seizure frequency attributes from the text“5 times every morn-\ning,” GPT-4 incorrectly extracted the word“every” as a Duration. In other\nscenarios, the models may only partially identify an entity. For instance, in\nextracting seizure frequency phrases from the text“Once daily for automotor\ns za n d2p e ry e a rf o rG T C,”GPT-3.5 Turbo extracted“Once daily”and“2p e r\nyear”as seizure frequency phrases but missed the seizure events“automotor\nsz”and“GTC”respectively. The models may alsofail to identify entities. For\nexample, in extracting seizure frequency phrases from the text“Multpkle\nover past two yeasr (5-10 sec each)”[sic], GPT-4 failed to identify a frequency\nphrase within this text. All such scenarios are considered incorrect extrac-\ntions in our work. However, in some variations of the partial identiﬁcation\ncase above, though we consider the extracted seizure frequency to be\nincorrect, the structured seizure frequency extracted by combining the\nextracted seizure frequency with the attributes may still be accurate. Take the\ntext“Few per hour.”as an example, our manual frequency phrase annotation\nwas “Few per hour”(without the period), while GPT-4 extracted the entire\ntext “Few per hour.” (with the period) leading this to be classiﬁed as an\nincorrect extraction. The GPT-4 model, however, correctly identiﬁed the\nattributes“few” as a Quantity and“hour” as a Temporal unit in this text.\nCombining the outputs of the two models, the structured frequency\nobtained was: [Quantity =“Few”, Temporal unit =“hour”], which is accu-\nrate. This is the reason why some structured seizure extraction models\noutperformed their underlying seizure frequency extraction models.\nFew studies have explored NLP approaches for extracting seizure\nfrequency information from clinical notes. Decker et al. presented a rule-\nbased system achieving an F1-score of 0.82 on their test set\n15.C o m p a r e dt o\nt h e i ra p p r o a c h ,o u rm o d e lc a ne x t r act more complex scenarios involving\nranges (minimums and maximums) of seizure quantities and durations, as\nwell as additional temporal expression types such as ages, speciﬁct i m e s ,\nrelative times, and relative time periods. Decker et al.’s results indicated that\nrule-based systems do not generalize well across data from different insti-\ntutions, whereas our data is sourced from six different institutions. More-\nover, rule-based systems require extensive time and effort to develop rules,\nhindering their quick adaptation to different institutional data. Xie et al.’s\ntransfer learning approach was targeted towards classifying seizure freedom\nand extracting seizure frequency and date of last seizure\n16. For extracting\nseizure frequencies, in contrast to our token-classiﬁcation approach, they\nexplored extractive question-answering models, with the best model\nachieving an F1-score of 0.845. However, their approach could only extract\nseizure frequency phrases, necessitating additional steps to extract detailed\nseizure frequency attributes.\nSeveral limitations of this study warrant discussion. One limitation is\nthat we randomly selected segments for our dataset as well as the splits\nTable 4 | Mean Absolute Error (MAE) scores for seizure frequencies (in number of seizures per day), obtained using different\nseizure frequency phrase and attribute models\nPhrase Attributes bert-large-cased biobert-large-cased Bio_ClinicalBERT Llama-2-70b-hf GPT-3.5 Turbo GPT-4\nbert-large-cased 0.2555 ± 0.1004 0.125 ± 0.0493 0.2941 ± 0.106 0.0724 ± 0.0203 0.1221 ± 0.0512 0.0603 ± 0.0221\nbiobert-large-cased 0.3062 ± 0.1097 0.2331 ± 0.0773 0.2998 ± 0.1067 0.1925 ± 0.0715 0.248 ± 0.0835 0.1704 ± 0.0711\nBio_ClinicalBERT 0.2713 ± 0.1028 0.1808 ± 0.0609 0.3038 ± 0.105 0.0919 ± 0.0222 0.129 ± 0.0512 0.0857 ± 0.0241\nLlama-2-70b-hf 0.2965 ± 0.1035 0.1643 ± 0.0568 0.3408 ± 0.1084 0.1204 ± 0.036 0.1547 ± 0.0586 0.0953 ± 0.0362\nGPT-3.5 Turbo 0.3545 ± 0.1197 0.2263 ± 0.0855 0.3964 ± 0.1251 0.1745 ± 0.0747 0.1793 ± 0.0762 0.162 ± 0.0747\nGPT-4 0.2549 ± 0.1046 0.1741 ± 0.0617 0.2952 ± 0.1065 0.0737 ± 0.0209 0.11 ± 0.0515 0.0594 ± 0.022\nResults are presented as mean ± standard deviation, with the lowest error score highlighted.\nTable 5 | Mean absolute error (MAE) scores for SUDEP-7 risk scores were obtained using different combinations of seizure\nfrequency phrase and attribute models\nPhrase Attributes bert-large-cased biobert-large-cased Bio_ClinicalBERT Llama-2-70b-hf GPT-3.5 Turbo GPT-4\nbert-large-cased 0.2207 ± 0.0464 0.1798 ± 0.0436 0.2551 ± 0.0521 0.13 ± 0.0357 0.1401 ± 0.0346 0.12 ± 0.0342\nbiobert-large-cased 0.2607 ± 0.0517 0.2101 ± 0.0462 0.2651 ± 0.0529 0.1848 ± 0.0438 0.1805 ± 0.0407 0.1604 ± 0.0405\nBio_ClinicalBERT 0.2706 ± 0.0502 0.2049 ± 0.0454 0.2752 ± 0.0533 0.145 ± 0.0372 0.1651 ± 0.0388 0.1601 ± 0.0397\nLlama-2-70b-hf 0.2506 ± 0.0506 0.1898 ± 0.0444 0.2602 ± 0.0525 0.1349 ± 0.0373 0.1452 ± 0.0364 0.1302 ± 0.037\nGPT-3.5 Turbo 0.2503 ± 0.0498 0.1898 ± 0.0439 0.255 ± 0.0514 0.14 ± 0.0362 0.1551 ± 0.0377 0.145 ± 0.0377\nGPT-4 0.2007 ± 0.0444 0.1848 ± 0.0438 0.2449 ± 0.0512 0.135 ± 0.0374 0.1252 ± 0.0332 0.1101 ± 0.0329\nResults are presented as mean ± standard deviation, with the lowest scores highlighted.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 4\nwithout stratifying by institution.There could be institution-speciﬁcj a r g o n ,\ncustoms, and reporting styles which may introduce slight biases impacting\ngeneralizability. Therefore, we intend to perform a comprehensive analysis\nof institution-speciﬁc biases in the future so that the robustness of the\nmodels against potential institution-speciﬁc biases can be identiﬁed. In\naddition, since EMUs often capture data primarily from individuals who\nmay not respond to medications or are undergoing evaluation to determine\nan epilepsy diagnosis, our dataset may lack adequate representation of\nseizure-free cases and may limit the model’s generalizability to broader\nclinical populations. Additionally, the model was trained to extract explicit\nFig. 1 | The performance of the seizure frequency phrase extraction models across\ndifferent training set sizes.This ﬁgure displays the F1-scores achieved by the\nseizure frequency phrase extraction models as a function of varying training set sizes.\nIt shows that GPT-4 consistently performs best across all training set sizes. Overall,\nall models beneﬁtted from increased training set size.\nFig. 2 | The performance of the seizure frequency attribute extraction models for\ndifferent training set sizes.This ﬁgure demonstrates the F1-scores achieved by the\nseizure frequency attribute models as a function of different training set sizes. It\nshows that GPT models follow a similar trend with a notable increase in the F1-score\nwhen the training set size is increased from 170 to 270. The BERT models, together\nwith Llama-2, also follow a similar trend with a notable increase in performance\nwhen the training dataset is increased from 70 to 170.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 5\nseizure frequency information; thus, if no seizure frequency information is\nextracted, it only reﬂects a lack of explicitly deﬁned seizure frequency\ninformation and remains ambiguous regarding the status of“no seizures”or\n“seizure freedom.”Another limitation is that our evaluation is too rigid since\nit requires the starting and ending position of any entity extracted to exactly\nmatch the manual annotations to be considered an accurate extraction. In\nfuture work, we expect to investigate moreﬂexible evaluation approaches\nthat take into account partial matches enhancing the robustness of the\nevaluation and better reﬂecting the real-world performance of the models.\nThe current approach requires 17 different attribute types to represent\ndifferent seizure frequencies. We plan to investigate leveraging the reasoning\npower of LLMs to reduce some of these attributes so that a more concise set\nof attributes can be used, thereby reducing the harmonization and nor-\nmalization efforts. For instance, anLLM would be potentially able to infer\nthat “2003 to 2010”represents 7 years, therefore, not requiring the“Interval\nstart” and “Interval end” attribute types.\nIn this work, we investigated various models for two tasks: extracting\nseizure frequency phrases and extracting seizure frequency attributes. We\nthen combined their outputs to obtainstructured details on seizure fre-\nquency. Although the results are promising, there is room for improvement.\nCurrently, the models for extracting seizure frequency phrases and attri-\nbutes operate independently. However, the output from one model could\npotentially inform the other. For example, the seizure frequency attributes of\na given text may be useful in identifying its seizure frequency phrase.\nTherefore, we plan to investigate how the outputs of one model can be\nintegrated into another to enhance prediction performance. In addition, for\nLLMs, we only investigated GPT-4, GPT-3.5 Turbo, and Llama-2 models.\nOur work demonstrates the potential of generative models for this text\nextraction task, which has traditionally been dominated by encoder-only\nmodels like BERT. Future work will involve more comprehensive com-\nparisons to investigate if there are other generative LLMs better suited for\nthis task.\nIn this paper, we presented an automated approach for extracting\nstructured seizure frequency details from clinical text. Our approach\ninvolved tackling two key extraction tasks: seizure frequency phrase\nextraction and seizure frequency attribute extraction. For both tasks, we\nexperimented withﬁne-tuning three pre-trained BERT models (bert-large-\ncased, biobert-large-cased, and Bio_ClinicalBERT), and instruction tuning\nthree generative large language models (GPT-4, GPT-3.5 turbo, and Llama-\n2-70b-hf). Theﬁnal structured seizure frequency details were derived by\ncombining the outputs from both tasks. Our experiments showed that by\nusing GPT-4 model for both seizure frequency phrase and attribute\nextraction achieved the best performance with a precision of 86.64%, recall\nof 85.06%, and F1-score of 85.82%. These results highlight the potential of\ngenerative large language models for extractive tasks.\nMethods\nIn this work, we utilized EMU reportsin PDF format from the CSR dataset\nto develop an automated method for seizure frequency extraction. Our\napproach involves two main tasks: (1) extraction of seizure frequency\nphrases; and (2) extraction of detailed seizure frequency attributes. For each\ntask, weﬁne-tuned different pre-trained language models including three\nBERT-based models and three generative LLMs. By merging the output of\nthe two tasks, we can systematically extract structured seizure frequency\ndetails for each instance mentioned in the text. Figure3 shows the overall\nworkﬂow of our approach.\nDataset preparation\nIn this work, we utilized EMU reports in PDF format from the CSR\ndataset, which were parsed and converted into plain textﬁles with the\npdftotext tool (https://www.xpdfreader.com/pdftotext-man.html). The\nEMU reports contain a subsection reserved for holding seizure frequency\ninformation. From these subsections, we extracted 2242 free-text seg-\nments through a Python script. These were found to be originating from\n6 different institutions. To train and evaluate various models, we ran-\ndomly selected 800 instances as our dataset for manual annotation. Table\n6 presents a summary of characteristics of our dataset, including the\nnumber of free-text segments based on originating institutions, statistics\nabout the number of words in these free-text segments, and the number\nof seizure frequencies in the free-text segments. Note that out of the 800\nsegments, 377 contained one frequency, 39 contained two frequencies,\n2 segments contained three frequencies, 1 segment contained four\nFig. 3 | The seizure frequency extraction workﬂow. This ﬁgure demonstrates the\nworkﬂow followed to extract structured seizure frequencies from EMU PDF reports.\nAs shown, the PDF reports are parsed and frequency phrases and attributes are\nannotated. Separate pre-trained models areﬁne-tuned for frequency phrase\nextraction and attribute extraction. Their outputs are combined to obtain structured\nseizure frequencies.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 6\nfrequencies, and 381 did not contain explicitly deﬁned seizure frequency\ninformation.\nThe seizure frequency phrases and detailed attributes in these 800\ninstances were manually annotated by authors RA and LC (who have prior\nexperience in information extraction from biomedical text) using the open-\nsource annotation tool Doccano (https://github.com/doccano/doccano).\nFor example, the bolded phrases in the following description are mentions of\ntwo distinct seizure frequencies:\n“Aura- 1–2 per week(unclear if it is related to sensation of muscle\ncontraction at onset of clonic seizure),GTC 1 every 5–6m o n t h s”\nAs can be seen from the above example, a seizure frequency phrase may\ninvolve various types of attributes such as seizure event, quantity, duration,\nand temporal unit. Table7 shows a list of all the attribute types used in manual\nannotation, alongside example phrases illustrating each attribute type.\nThis dataset of 800 instances was further split into three sets: 400 for\ntraining, 200 for validation, and the remaining 200 for testing. We observed\nthat some attribute types (Interval start, Relative time period, Age, Age start,\nand Age end) had few instances in the training set. For such cases, we\nemployed a text augmentation strategy leveraging ChatGPT to generate\nadditional training instances based on the existing instances (https://\nchatgpt.com/) .T h ep r o m p tt h a tw ee n g i n e e r e df o rt h i si sg i v e na sf o l l o w s :\n“Working as a clinical text augmentation tool that generates addi-\ntional training instances for training an NLP model, provide <the\nnumber of augmented instances needed>a u g m e n t e dt e x tf o rt h e\ninput clinical text. The augmented text should be approximately\nsimilar in length to the input. The numerical values and units in the\ntext must be augmented as well. Input: “<the input training\ninstance >“.”\nSeizure frequency phrase and attribute extraction models\nThe task of seizure frequency phrase extraction involves the automatic\ndetection and extraction of phrases describing seizure frequencies within the\ngiven text. Our dataset may include three types of text: (1) text with a single\nseizure frequency phrase, (2) text with multiple seizure frequency phrases,\nand (3) text lacking explicitly deﬁned seizure frequency information. For\nexample, the text“Aura- 1–2 per week”contains a single seizure frequency\nand the text“Right arm clonic 1 every 2 weeks, GTC one every 2 months”\ncontains two seizure frequency phrases. The texts lacking explicitly deﬁned\nseizure frequency information include instances with missing temporal\ncontext (e.g.,“Happened only once”), cases where seizure frequency cannot\nbe clearly determined (e.g.,“Uncertain”; “Variable”;o r“Patient was unable\nto tell the frequency, she said theybecame less after AEDs were started”), and\ninstances indicating seizure remission (e.g., “No seizures in the last\n20 years”).\nThe task of seizure frequency attribute extraction is to identify speciﬁc\ndetails of a seizure frequency, such as the seizure event, its quantity, and the\ntemporal unit (see Table1 for a comprehensive list of potential compo-\nnents). For instance, the text“1G T Cs e i z u r ee v e r y6m o n t h s” mentions a\nseizure event“GTC seizure,” aq u a n t i t yo f“1,” a duration of“6,” and a\ntemporal unit of“months.”\nFor both seizure frequency phrase extraction and attribute extraction\ntasks, we investigated three pre-trained BERT models and three pre-trained\ngenerative LLMs. Despite beingﬁne-tuned on different data, both tasks\nshared a common pipeline for identifying text spans (start and end\npositions).\nTo begin, we examined BERT, which isa popular pre-trained encoder-\nonly language model developed by Google that has revolutionized Natural\nLanguage Processing\n20. Various models based on the BERT architecture\nhave since been developed. We experimented with three BERT-based\nmodels: bert-large-cased (https://huggingface.co/google-bert/bert-large-\ncased), biobert-large-cased21 (https://huggingface.co/dmis-lab/biobert-\nlarge-cased-v1.1),a n dB i o _ C l i n i c a l B E R T(https://huggingface.co/\nemilyalsentzer/Bio_ClinicalBERT) for a token-classiﬁcation task, aimed at\nassigning a label to each token in a given text (https://huggingface.co/tasks/\ntoken-classiﬁcation). To accomplish this, we converted the original manual\nannotations of training instances to token-level tags using the IOB2 tagging\nscheme22, which enabled the assignment of a speciﬁcr o l et oe a c ht o k e n\nwithin a sequence. For instance, in the text“Right arm clonic 1 every 2 weeks,\nGTC one every 2 months,” the tokens“Right,”“arm,” and “clonic” were\nassigned IOB2 tags of B-Event, I-Event, and I-Event, respectively. This\nindicates that“Right”marks the beginning token of the seizure event“Right\narm clonic,” while “arm” and “clonic” denote inside tokens. The models\nwere thenﬁne-tuned to predict the IOB2 tags for each token, which would\nultimately be converted back to start and end string positions.\nTable 6 | Summary of characteristics of the dataset\nNumber of free-text segments from\ndifferent institutions\nUniversity Hospitals Cleveland\nMedical Center\n334\nNorthwestern University 194\nUniversity of Iowa 86\nUniversity College London 78\nThomas Jefferson University 56\nNew York University 52\nNumber of words in free-text\nsegments\nMean 8.72\nMedian 6\nMinimum 1\nMaximum 54\nNumber of seizure frequencies in\nfree-text segments\nMean 0.58\nMedian 1\nMinimum 0\nMaximum 4\nTable 7 | Attribute types used for annotating seizure frequency\nphrases and illustrating examples\nAttribute type Example\nEvent 3 aphasic seizures per week and 1GTC seizure\nevery 6 months\nQuantity 3 aphasic seizures per week and1 GTC seizure\nevery 6 months\nTemporal unit 3 aphasic seizures per week and 1 GTC seizure\nevery 6months\nMinimum quantity Aura- 1– 2 per week\nMaximum quantity Aura- 1– 2 per week\nInterval start 5 seizures from 2003 to 2013\nInterval end 5 seizures from 2003 to 2013\nTime total of 3 events in the month of June, 2018\nRelative time Last known 2 years ago\nRelative time period Once per month in the last 10 months\nDuration 3 every 4 months\nMinimum duration one to two seizures every 4 – 5 months\nMaximum duration\ndduration\none to two seizures every 4– 5 months\nAge Started at 7 months of age\nAge start started at 7 y/o, stopped around age 8\nAge end started at 7 y/o, stopped around age 8\nPeriodic Daily since 2012\nThe attribute values are in bold and underlined.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 7\nNext, we focused on three generative LLMs: Llama-2-70b-hf (https://\nllama.meta.com/llama2/), GPT-3.5 Turbo, and GPT-4 (https://platform.\nopenai.com/docs/models), to explore a different strategy for extracting\nseizure frequency phrases and attributes. This strategy involvesﬁne-tuning\nthe models to generate HTML-like tags surrounding the relevant entities.\nThe training instances were accordingly converted to a tagged format. To\nillustrate, the text“Right arm clonic 1 every 2 weeks, GTC one every 2 months”\nwas converted to\n“<FREQ>Right arm clonic 1 every 2 weeks<\\FREQ>,<FREQ>GTC\none every 2 months<\\FREQ>”\nfor the purpose of extracting seizure frequency phrases. While for\nfrequency attribute extraction, the text was converted to\n“<EVNT>Right arm clonic<\\EVNT><QNT>1<\\QNT>every<DUR>\n2<\\DUR><UNT>weeks<\\UNT>,<EVNT>GTC<\\EVNT><QNT>\none<\\QNT>every<DUR>2<\\DUR><UNT>months<\\UNT>.”\nHere EVNT, QNT, DUR, and UNT represent the seizure attribute\ntypes Event, Quantity, Duration, and Temporal unit, respectively. As can be\nseen, each entity is surrounded by an opening tag (e.g., <EVNT>) and a\nclosing tag (e.g., <\\EVNT>) to indicate the starting and ending positions.\nWe ﬁne-tuned Llama-2 using instruction tuning, which involves\nadditional training of a generative LLM with a dataset of instructions and the\ndesired outputs\n23. This supervised approach ensures theﬁne-tuned model\nproduces annotations in a customized tagged format. For the seizure fre-\nquency phrase extraction, each instance of the training set was converted to\nt h et e m p l a t es h o w ni nF i g .4. For the detailed seizure frequency attribute\nextraction, the same template was used, with the“Response” section con-\ntaining the attribute annotations. When applying theﬁne-tuned model to\nthe validation set and testing set, the template remained the same, but the\n“Response” section was left blank. For instruction tuning, we leveraged a\ntechnique called Parameter-Efﬁcient Fine-Tuning (PEFT) that requires\nﬁne-tuning a smaller number of parameters, greatly reducing the compu-\ntational cost associated withﬁne-tuning the entire model\n24.\nFor GPT-3.5 Turbo (0613 snapshot) and GPT-4 (0613 snapshot), we\nexplored ﬁne-tuning them through the Microsoft Azure OpenAI Service.\nEach training instance was formatted according to OpenAI’s requirements\n(https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/ﬁne-\ntuning). Figure5 and 6 show example inputs for seizure frequency phrase\nextraction and attribute extraction, respectively. When utilizing theﬁne-\ntuned model to extract frequency phrases or attributes from the validation\nset and testing set, the same template was used without the response by the\nrole “assistant.”\nNote that since Llama-2 and both GPT models produced their output\nin the tagged format, further post-processing was needed to pinpoint the\nstart and end positions within the original text.\nStructured seizure frequency extraction\nSince the pre-trained models were independentlyﬁne-tuned for extracting\nseizure frequency phrases and attributes, we investigated pairwise model\ncombinations to obtain structured seizure frequency details. Given a seizure\nfrequency phrase extraction model and a seizure frequency attribute\nextraction model, theﬁnal output of structured seizure frequency details for\nan input text was obtained as follows. Let the output of the seizure frequency\nphrase extraction model beP ={ (p\ni,s,p i,e)| 0 ≤ i ≤ m and pi,e < pi+1,s}, where\npi,s and pi,e represent the start and end positions of thei-th seizure frequency\nphrase. Similarly, let the output of the seizure frequency attribute extraction\nmodel beA ={ (a\nj,s,a j,e)| 0 ≤ j ≤ n and aj,e < aj+1,s}, whereaj,s and aj,e\nrepresent the start and end positions of thej-th seizure frequency attribute.\nA structured seizure frequency is deﬁned as a subset ofA,d e n o t e da s\nS ={ (ak,s,a k,e), (ak+1,s,a k+1,e), … ,( al,s,a l,e)| 0 ≤ k ≤ l ≤ n}, such that there\nexists(pq,s,p q,e) in P such thatpq,s ≤ah,s and pq,e ≥ah,e for each element(ah,s,\nah,e) in S.\nFor example, consider the text“Right arm clonic 1 every 2 weeks, GTC\none every 2 months.” Assume a seizure frequency phrase extraction model\noutputs {(0, 31), (34, 55)}, where (0, 31) represents the phrase“Right arm\nclonic 1 every 2 weeks”and (34, 55) represents the phrase“GTC one every\nFig. 4 | The Llama-2 input template for seizure frequency phrase extraction.This\nﬁgure shows the input template used toﬁne-tune Llama-2 for seizure frequency\nphrase extraction. The“Instruction” introduces the task while the“Input” contains\nthe segment. The“Response” contains the expected output. The“Response” is left\nempty at the prediction stage when theﬁne-tuned model is used to identify fre-\nquency phrases.\nFig. 5 | The GPT input template for seizure frequency phrase extraction.This\nﬁgure shows the input template used toﬁne-tune GPT models for seizure frequency\nphrase extraction. The“system” role provides a context for the task at hand while\n“user” role contains the segment. The expected output will be provided in the\n“assistant” role. When using aﬁne-tuned model to extract seizure frequency\nphrases, the“assistant” role will not be included.\nFig. 6 | The GPT input template for seizure frequency attribute extraction.This\nﬁgure displays the input template used toﬁne-tune GPT models for seizure fre-\nquency attribute extraction. The“system”role provides a context for the task, while\nthe “user” role includes the segment. The expected output will be provided in the\n“assistant” role. When using aﬁne-tuned model to extract seizure frequency attri-\nbutes, the“assistant” role will not be included.\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 8\n2m o n t h s .”Suppose a seizure frequency attribute extraction model outputs\n{(0, 15), (17,17), (25,25), (27, 31), (34, 36), (38, 40), (48,48), (50,55)}\nrepresenting “Right arm clonic” (event), “1” (quantity), “2” (duration),\n“weeks” (temporal unit),“GTC” (event), “one” (quantity),“2” (duration),\n“months”(temporal unit), respectively. Since {(0, 15), (17,17), (25,25), (27,\n31)} lies within the boundary of the phrase“Right arm clonic 1 every\n2 weeks”and {(34, 36), (38, 40), (48,48), (50,55)} lies within the boundary of\nthe phrase“GTC one every 2 months,” two structured seizure frequencies\ncan be extracted from this example: [event =“Right arm clonic”,q u a n -\ntity =“1”,d u r a t i o n =“2”, temporal unit =“weeks”]a n d[ e v e n t =“GTC”,\nquantity =“one”,d u r a t i o n=“2”, temporal unit =“months”].\nPerformance evaluation\nWe evaluated theﬁne-tuned models on the test set, comparing their per-\nformance across different extraction tasks. The evaluation metrics included\nprecision, recall, and F1-score. For both the seizure frequency phrase\nextraction and attribute extraction tasks, precision was calculated as the\nnumber of correctly extracted entities divided by the total number of\nextracted entities. The recall was calculated as the number of correctly\nextracted entities divided by the total number of manually annotated entities\nin the test set. An extraction was deemed correct if the span (start and end\npositions) exactly matched the manual annotation.\nFor structured seizure frequency extraction, the precision was deﬁned\nas the ratio of correctly extracted structured frequencies to all extracted\nstructured frequencies, while recall was the ratio of correctly extracted\nstructured frequencies to the total manually annotated structured fre-\nquencies in the test set. A structured seizure frequency was considered\ncorrectly extracted only if all its seizure frequency attributes were accurately\nextracted.\nAll metrics are computed based onbootstrap trials across 10,000\nbootstrapped samples from the test set, following the methods described in\nYan et al. and Koehn et al.\n25,26. For each bootstrapped sample, we randomly\nselected 200 instances with replacements from the test set. Precision, Recall,\nand F1-scores are computed for each bootstrapped sample, and their mean,\nas well as the standard deviation, are reported. If the F1-score of one model\nexceeds that of another model in more than 95% of the 10,000 trials, we\nconsider the performance of the earlier model to be statistically signiﬁcantly\nbetter than the latter\n25,26.\nData availability\nThe de-identiﬁed version of the text segments used in this study is available\nin the GitHub repository:https://github.com/rashmie/SZFreqExtract.\nCode availability\nThe underlying code developed for thet a s k si nt h i sp a p e ri sa v a i l a b l ei nt h e\nGitHub repository:https://github.com/rashmie/SZFreqExtract\nReceived: 8 August 2024; Accepted: 28 March 2025;\nReferences\n1. Annegers, J. F. & Coan, S. P. SUDEP: overview of deﬁnitions and\nreview of incidence data.Seizure 8, 347– 352 (1999).\n2. Giussani, G. et al. Sudden unexpected death in epilepsy: a critical view\nof the literature.Epilepsia Open8, 728– 757 (2023).\n3. Sun, X., Lv, Y. & Lin, J. The mechanism of sudden unexpected death in\nepilepsy: a mini review.Front. Neurol.14, 1137182 (2023).\n4. Massey, C. A., Sowers, L. P., Dlouhy, B. J. & Richerson, G. B. SUDEP\nmechanisms: the pathway to prevention.Nat. Rev. Neurol.10, 271\n(2014).\n5. Harden, C. et al. Practice guideline summary: Sudden unexpected\ndeath in epilepsy incidence rates and risk factors: report of the\nGuideline Development, Dissemination, and Implementation\nSubcommittee of the American Academy of Neurology and the\nAmerican Epilepsy Society.Neurology 88, 1674– 1680 (2017).\n6. Hesdorffer, D. C. et al. Combined analysis of risk factors for SUDEP.\nEpilepsia 52, 1150– 1159 (2011).\n7. Monté, C. P. J. A. et al. Sudden unexpected death in epilepsy patients:\nrisk factors: a systematic review.Seizure 16,1 – 7 (2007).\n8. Tomson, T., Walczak, T., Sillanpaa, M. & Sander, J. W. A. S. Sudden\nunexpected death in epilepsy: a review of incidence and risk factors.\nEpilepsia 46,5 4– 61 (2005).\n9. Watkins, L., Shankar, R. & Sander, J. W. Identifying and mitigating\nSudden Unexpected Death in Epilepsy (SUDEP) risk factors.Expert\nRev. Neurother.18, 265– 274 (2018).\n10. Novak, J. L., Miller, P. R., Markovic, D., Meymandi, S. K. & DeGiorgio,\nC. M. Risk assessment for sudden death in epilepsy: the SUDEP-7\ninventory. Front. Neurol.6, 252 (2015).\n11. Zhang, G. -Q., Cui, L., Lhatoo, S., Schuele, S. U. & Sahoo, S. S.\nMEDCIS: multi-modality epilepsy data capture and integration\nsystem. Amia. Annu. Symp. Proc.2014, 1248– 1257 (2014).\n12. Tao, S., Cui, L., Chou, W. -C., Lhatoo, S. & Zhang, G. -Q. DaT3M: a\ndata tracker for multi-faceted management of multi-site clinical\nresearch data submission, curation, master inventorying, and sharing.\nAMIA Summits Transl. Sci. Proc.2022, 466– 475 (2022).\n13. Li, X. et al. A multimodal clinical data resource for personalized risk\nassessment of sudden unexpected death in epilepsy.Front. Big Data\n5, 965715 (2022).\n14. Yew, A. N. J., Schraagen, M., Otte, W. M. & van Diessen, E.\nTransforming epilepsy research: A systematic review on natural\nlanguage processing applications.Epilepsia 64, 292– 305 (2023).\n15. Decker, B. M. et al. Development of a natural language processing\nalgorithm to extract seizure types and frequencies from the electronic\nhealth record.Seizure Eur. J. Epilepsy101,4 8– 51 (2022).\n16. Xie, K. et al. Extracting seizure frequency from epilepsy clinic notes: a\nmachine reading approach to natural language processing.\nJ. Am.\nMed. Inform. Assoc.29, 873– 881 (2022).\n17. Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: a next-\ngeneration hyperparameter optimization framework. In:Proceedings\nof the 25th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining2623– 2631 (Association for Computing\nMachinery, New York, NY, USA, 2019).https://doi.org/10.1145/\n3292500.3330701.\n18. Tao, S., Lhatoo, S., Hampson, J., Cui, L. & Zhang, G. -Q. A bespoke\nelectronic health record for epilepsy care (epitome): development and\nqualitative evaluation.J. Med. Internet Res.23, e22939 (2021).\n19. Deußer, T., Hillebrand, L., Bauckhage, C. & Sifa, R. Informed named\nentity recognition decoding for generative language models.arXiv.org\nhttps://arxiv.org/abs/2308.07791v1 (2023).\n20. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of\ndeep bidirectional transformers for language understanding. In:\nProceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers)(eds. Burstein, J., Doran,\nC. & Solorio, T.) 4171–4186 (Association for Computational Linguistics,\nMinneapolis, Minnesota, 2019).https://doi.org/10.18653/v1/N19-1423.\n21. Lee, J. et al. BioBERT: a pre-trained biomedical language\nrepresentation model for biomedical text mining.Bioinformatics 36,\n1234– 1240 (2020).\n22. Ramshaw, L. A. & Marcus, M. P. Text chunking using transformation-\nbased learning. In:Natural Language Processing Using Very Large\nCorpora (eds. Armstrong, S. et al.) 157– 176 (Springer Netherlands,\nDordrecht, 1999).https://doi.org/10.1007/978-94-017-2390-9_10.\n23. Zhang, S. et al. Instruction tuning for large language models: a survey.\nhttp://arxiv.org/abs/2308.10792 (2023).\n24. Xu, L., Xie, H., Qin, S.-Z. J., Tao, X. & Wang, F. L. Parameter-efﬁcient\nﬁne-tuning methods for pretrained language models: a critical review\nand assessment.http://arxiv.org/abs/2312.12148 (2023).\n25. Yan, A. et al. RadBERT: adapting transformer-based language\nmodels to radiology.Radiol. Artif. Intell.4, e210258 (2022).\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 9\n26. Koehn, P. Statistical signiﬁcance tests for machine translation\nevaluation. In:Proceedings of the 2004 Conference on Empirical\nMethods in Natural Language Processing(eds. Lin, D. & Wu, D.)\n388– 395 (Association for Computational Linguistics, Barcelona,\nSpain, 2004).\nAcknowledgements\nThis work was supported by the National Institute of Neurological Disorders\nand Stroke of the National Institutes of Health through grants R01NS116287\nand R01NS126690. The funders played no role in study design, data\ncollection, analysis and interpretation of data, or the writing of this\nmanuscript. The content is solely the responsibility of the authors and does\nnot necessarily represent the ofﬁcial views of the National Institutes of\nHealth.\nAuthor contributions\nL.C. and R.A. conceptualized and designed this study. S.T., S.D.L. and\nG.Q.Z. prepared the data used for developing and testing the automated\napproach for extracting structured seizure frequency details. R.A. developed\nthe automated seizure frequency extraction approach, performed the\nexperiments forﬁne-tuning the pre-trained models, and generated the\nresults. R.A. and L.C. performed manual annotation and analyzed the\nresults. R.A. and L.C. wrote the manuscript with inputs from S.T., S.D.L.\nand G.Q.Z.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01592-4\n.\nCorrespondenceand requests for materials should be addressed to\nLicong Cui.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01592-4 Article\nnpj Digital Medicine|           (2025) 8:208 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7082501649856567
    },
    {
      "name": "Epilepsy",
      "score": 0.6411253213882446
    },
    {
      "name": "Recall",
      "score": 0.5900126099586487
    },
    {
      "name": "Phrase",
      "score": 0.5409986972808838
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4969921410083771
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4822293221950531
    },
    {
      "name": "Natural language processing",
      "score": 0.4598328769207001
    },
    {
      "name": "Speech recognition",
      "score": 0.4163861572742462
    },
    {
      "name": "Machine learning",
      "score": 0.33181333541870117
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3208155035972595
    },
    {
      "name": "Psychology",
      "score": 0.18257761001586914
    },
    {
      "name": "Cognitive psychology",
      "score": 0.12074840068817139
    },
    {
      "name": "Neuroscience",
      "score": 0.10298749804496765
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    }
  ]
}