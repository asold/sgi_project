{
    "title": "Language Modelling for Sound Event Detection with Teacher Forcing and Scheduled Sampling",
    "url": "https://openalex.org/W2964264308",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2530999682",
            "name": "Drossos, Konstantinos",
            "affiliations": [
                "Tampere University"
            ]
        },
        {
            "id": "https://openalex.org/A4289097344",
            "name": "Gharib, Shayan",
            "affiliations": [
                "Tampere University"
            ]
        },
        {
            "id": "https://openalex.org/A4224605519",
            "name": "Magron, Paul",
            "affiliations": [
                "Tampere University"
            ]
        },
        {
            "id": "https://openalex.org/A2755654187",
            "name": "Virtanen, Tuomas",
            "affiliations": [
                "Tampere University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6764450229",
        "https://openalex.org/W2292996718",
        "https://openalex.org/W2165320399",
        "https://openalex.org/W2132920253",
        "https://openalex.org/W1529830495",
        "https://openalex.org/W1587644769",
        "https://openalex.org/W2341412280",
        "https://openalex.org/W2746554716",
        "https://openalex.org/W2591013610",
        "https://openalex.org/W2594627932",
        "https://openalex.org/W2618269622",
        "https://openalex.org/W2899579490",
        "https://openalex.org/W7027429494",
        "https://openalex.org/W2016589492",
        "https://openalex.org/W2950304420",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W6639657675",
        "https://openalex.org/W2566935005",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W1844944916",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2760145945",
        "https://openalex.org/W2963099423",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2127141656",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W648786980",
        "https://openalex.org/W2950773293"
    ],
    "abstract": "A sound event detection (SED) method typically takes as an input a sequence of audio frames and predicts the activities of sound events in each frame. In real-life recordings, the sound events exhibit some temporal structure: for instance, a \"car horn\" will likely be followed by a \"car passing by\". While this temporal structure is widely exploited in sequence prediction tasks (e.g., in machine translation), where language models (LM) are exploited, it is not satisfactorily modeled in SED. In this work we propose a method which allows a recurrent neural network (RNN) to learn an LM for the SED task. The method conditions the input of the RNN with the activities of classes at the previous time step. We evaluate our method using F1 score and error rate (ER) over three different and publicly available datasets; the TUT-SED Synthetic 2016 and the TUT Sound Events 2016 and 2017 datasets. The obtained results show an increase of 9% and 2% at the F1 (higher is better) and a decrease of 7% and 2% at ER (lower is better) for the TUT Sound Events 2016 and 2017 datasets, respectively, when using our method. On the contrary, with our method there is a decrease of 4% at F1 score and an increase of 7% at ER for the TUT-SED Synthetic 2016 dataset.",
    "full_text": "Detection and Classiﬁcation of Acoustic Scenes and Events 2019 25–26 October 2019, New York, NY , USA\nLANGUAGE MODELLING FOR SOUND EVENT DETECTION WITH TEACHER FORCING\nAND SCHEDULED SAMPLING\nKonstantinos Drossos, Shayan Gharib, Paul Magron, and Tuomas Virtanen\nAudio Research Group, Tampere University, Tampere, Finland\n{ﬁrstname.lastname}@tuni.ﬁ\nABSTRACT\nA sound event detection (SED) method typically takes as an input a\nsequence of audio frames and predicts the activities of sound events\nin each frame. In real-life recordings, the sound events exhibit some\ntemporal structure: for instance, a “car horn” will likely be followed\nby a “car passing by”. While this temporal structure is widely ex-\nploited in sequence prediction tasks (e.g., in machine translation),\nwhere language models (LM) are exploited, it is not satisfactorily\nmodeled in SED. In this work we propose a method which allows a\nrecurrent neural network (RNN) to learn an LM for the SED task.\nThe method conditions the input of the RNN with the activities of\nclasses at the previous time step. We evaluate our method using F1\nscore and error rate (ER) over three different and publicly available\ndatasets; the TUT-SED Synthetic 2016 and the TUT Sound Events\n2016 and 2017 datasets. The obtained results show an increase of\n6% and 3% at the F1 (higher is better) and a decrease of 3% and\n2% at ER (lower is better) for the TUT Sound Events 2016 and\n2017 datasets, respectively, when using our method. On the con-\ntrary, with our method there is a decrease of 10% at F1 score and\nan increase of 11% at ERfor the TUT-SED Synthetic 2016 dataset.\nIndex Terms— sound event detection, language modelling, se-\nquence modelling, teacher forcing, scheduled sampling\n1. INTRODUCTION\nSound event detection (SED) consists in detecting the activity of\nclasses (onset and offset times) in an audio signal, where the classes\ncorrespond to different sound events. (e.g., “baby cry”, “glass shat-\nter”). This task ﬁnds applications in many areas related to ma-\nchine listening, such as audio surveillance for smart industries and\ncities [1, 2], smart meeting room devices for enhanced telecom-\nmunications [3, 4], or bio-diversity monitoring in natural environ-\nments [5, 6]. SED is a challenging research task since the sound\nevents are of very diverse nature, which might be unknown a pri-\nori in real-life recordings. Besides, they often overlap in time, a\nproblem termed as polyphonic SED. Signiﬁcant advances in SED\nwere made recently thanks to the advent of deep learning [7]. The\nrecurrent neural network (RNN) have proven particularly promis-\ning [8, 9] as they are able to model the temporal discriminant repre-\nsentations for sound events. More recently, these have been stacked\nwith convolutional layers, resulting in convolutional recurrent neu-\nral networks (CRNN) which yield state-of-the-art results [10, 11].\nIn real-life recordings, the various sound events likely tempo-\nral structures within and across events. For instance, a “footsteps”\nevent might be repeated with pauses in between (intra-event struc-\nture). On the other hand, “car horn” is likely to follow or precede\nthe “car passing by” sound event (inter-events structure). Although\nthese temporal structures vary with the acoustic scene and the ac-\ntual sound events classes, they exist and can be exploited in the\nSED task. Some previous studies focus on exploiting these tempo-\nral structures. For example, in [9], the authors propose to use hidden\nMarkov models (HMMs) to control the duration of each sound event\npredicted with a deep neural network (DNN). Although the results\nshow some improvement with the usage of HMMs, the approach is\na hybrid one and it requires a post processing step, which might be\nlimited compared to an non-hybrid, DNN-based approach. In [12]\nand [13], the connectionist temporal classiﬁcation (CTC) [14] loss\nfunction is used for SED: the output of the DNN is modiﬁed in or-\nder to be used with the CTC. Although the usage of CTC seems\nto be promising, CTC needs modiﬁcation in order to be used for\nSED, it is a complicated criterion to employ, and it was developed\nto solve the problem where there is no frame-to-frame alignment\nbetween the input and output sequences [14]. Thus, there might be\nthe case that using a different method for SED language modelling\ncould provide better results than CTC. Finally, in [13], the authors\nalso employ N-grams, which require pre and post processing stages,\nand use the class activities as extra input features. However, the lat-\nter approach did not perform better than a baseline which did not\nemploy any language model.\nIn this paper we propose an RNN-based method for SED that\nexploit the temporal structures within and across events of audio\nscenes without the aforementioned drawbacks of the previous ap-\nproaches. This method is based on established practices from other\nscientiﬁc disciplines that deal with sequential data (e.g., machine\ntranslation, natural language processing, speech recognition). It\nconsists in using the output of the classiﬁer as an extra input to\nthe RNN in order to learn a model of the temporal structures of the\noutput sequence (referred to as language model), a technique called\nteacher forcing[15]. Besides, this extra input of the RNN is chosen\nas a combination of the ground truth and predicted classes. This\nstrategy, known as schedule sampling [16], consists in ﬁrst using\nthe ground truth activities and further replacing them by the pre-\ndictions. This allows the RNN to learn a robust language model\nfrom clean labels, without introducing any mismatch between the\ntraining and inference processes.\nThe rest of the paper is organized as follows. In Section 2 we\npresent our method. Section 3 details the experimental protocol and\nSection 4 presents the results. Section 5 concludes the paper.\n2. PROPOSED METHOD\nWe propose a system that consists of a DNN acting as a feature\nextractor, an RNN that learns the temporal structures withing and\nacross events (i.e. a language model), and a feed-forward neural\nnetwork (FNN) acting as a classiﬁer. Since we focus on designing\nhttps://doi.org/10.33682/1dze-8739\n59\nDetection and Classiﬁcation of Acoustic Scenes and Events 2019 25–26 October 2019, New York, NY , USA\nan RNN that is able to learn a language model over the sound events,\nthe RNN takes as inputs the outputs of both the DNN and the FNN.\nThe code for our method can be found online1.\n2.1. Baseline system\nThe DNN takes as an input a time-frequency representation of an\naudio signal denoted X ∈RT×F\n≥0 , where T and F respectively de-\nnote the number of time frames and features. It outputs a latent\nrepresentation:\nH = DNN(X), (1)\nwhere H ∈RT×F′\nis the learned representation with F′features.\nThen, the RNN operates over the rows of H as\nh′\nt = RNN(ht,h′\nt−1), (2)\nwhere t = 1,2,...,T , h′\n0 = {0}F′′\n, h′\nt ∈[−1,1]F′′\n, and F′′\nis the amount of features that the RNN outputs at each time-step.\nFinally, the FNN takes h′\nt as an input and outputs the prediction ˆyt\nfor the time-step tas:\nˆyt = σ(FNN(h′\nt)), (3)\nwhere σis the sigmoid function, and ˆyt ∈[0,1]C is the predicted\nactivity of each of the Cclasses.\nThe DNN, the RNN, and the FNN are simultaneously opti-\nmized by minimizing the loss L( ˆY,Y) =∑\ntLt(ˆyt,yt) with:\nLt(ˆyt,yt) =\nC∑\nc=1\nyt,clog(ˆyt,c) + (1−yt,c) log(1−ˆyt,c), (4)\nwhere yt,c and ˆyt,c are the ground truth and predicted activities,\nrespectively, of the c-th class at the t-th time-step.\n2.2. Teacher forcing\nThe modeling in Eq. (2) shows that the RNN learns according to\nits input and its previous state [15, 16]. In order to allow the RNN\nto learn a language model over the output (i.e. the sound events),\nwe propose to inform the RNN of the activities of the classes of the\nsound events at the time step t−1. That is, we condition the input\nto the RNN as:\nh′\nt = RNN(ht,h′\nt−1,y′\nt−1), (5)\nwhere y′\nt−1 is the vector with the activities of the classes of the\nsound events at time step t−1, and y′\n0 = {0}C. This technique is\ntermed as teacher forcing [15], and is widely employed in sequence\nprediction/generation tasks where the output sequence has an in-\nherent temporal model/structure (e.g., machine translation, image\ncaptioning, speech recognition) [17, 18, 19]. By using this condi-\ntioning of the RNN, the RNN can learn a language model over the\noutput tokens of the classiﬁer [15, 16]. In SED, this results in letting\nthe RNN learn a language model over the sound events, e.g., which\nsound events are more likely to happen together and/or in sequence,\nor how likely is a sound event to keep being active, given the pre-\nvious activity of the sound events. Teacher forcing is different from\nwhat was proposed in [13], as the latter approach conditioned the\nDNN (not the RNN) with the class activities: such an approach\nyielded poor results, intuitively explained by having y′\nt−1 domi-\nnated by the information in X through the sequence of the CNN\nblocks.\n1https://github.com/dr-costas/SEDLM\nRNN\nFNN\nFigure 1: Proposed method of teacher forcing with scheduled sam-\npling.\n2.3. Scheduled sampling\nThe activity vector y′\nt−1 can be either the ground truth data (i.e.,\nyt−1), or the predictions of the classiﬁer (i.e., ˆyt−1). In the for-\nmer case, the RNN is likely to start learning the desired language\nmodel from the ﬁrst updates of the weights. At each time step t, the\nRNN will take as input the ground truth activities of the classes, thus\nbeing able to exploit this information from the very ﬁrst weight up-\ndates. However, these ground truth values are not available at the in-\nference stage: these would be replaced by the estimatesˆyt−1, which\nwould create a mismatch between the training and testing processes.\nBesides, an RNN trained using only the true class activities is very\nlikely to be sensitive to the prediction errors in ˆyt−1. Finally, we\nempirically observed that using yt−1 with the SED datasets, which\nare of relatively small size, results in a very poor generalization of\nthe SED method.\nA countermeasure to the above is to use the predictions ˆyt−1\nas y′\nt−1, which allows the RNN to compensate for the prediction\nerrors. However, during the ﬁrst weight updates, the predictedˆyt−1\nis very noisy and any error created at a time steptis propagated over\ntime, which results in accumulating more errors down the line of the\noutput sequence. This makes the training process very unstable and\nis likely to yield a poor SED performance.\nTo exploit the best of both approaches, we propose to use the\nscheduled sampling strategy [16]: the ground truth class activities\nare used during the initial epochs, and they are further gradually\nreplaced by the predicted class activities. This gradual replacement\nis based on a probability pTF of picking yt−1 over ˆyt−1 as y′\nt−1\nthat decreases over epochs. Different functions can be used for the\ncalculation of pTF (e.g., exponential, sigmoid, linear). Here, we\nemploy a model of exponential decrease of pTF:\npTF = min(pmax,1 −min(1 −pmin, 2\n1 +eβ −1)), (6)\nwhere β = −iγN−1\nb , iis the index of the weight update (i.e., how\nmany weight updates have been performed), Nb is the amount of\nbatches in one epoch, and pmax, pmin, and γ are hyper-parameters\nto be tuned. pmax and pmin are the maximum and minimum prob-\n60\nDetection and Classiﬁcation of Acoustic Scenes and Events 2019 25–26 October 2019, New York, NY , USA\nabilities of selecting ˆyt, and γ controls the slope of the curve of\npTF for a given Nb and as iincreases. We use a minimum prob-\nability pmin because we experimentally observed that if we solely\nuse yt−1 as y′\nt−1 even in the ﬁrst initial weight updates, then the\nSED method overﬁts. The usage of pmin counters this fact. On the\nother hand, we use a maximum probability pmax in order to allow\nthe usage of yt−1 as y′\nt−1 at the later stages of the learning process.\nWe do this because the length of a sequence in SED is usually over\n1000 time-steps and any error in ˆyt is accumulated in this very long\nsequence, resulting in hampering the learning process. The usage\nof pmax offers a counter measure to this, by allowing the usage of\nground truth values yt instead of predicted and noisy values. This\nmethod is illustrated in Figure 1.\n3. EV ALUATION\nWe evaluate our method using the CRNN from [10], and we employ\nsynthetic and real-life recordings datasets to illustrate the impact of\nthe language model learned by our method.\n3.1. Data and feature extraction\nThe synthetic dataset is the TUT-SED Synthetic 2016, used in [10],\nand consisting of 100 audio ﬁles which are synthetically created\nout of isolated sound events of 16 different classes. These classes\nare: alarms and sirens, baby crying, bird singing, bus, cat meow-\ning, crowd applause, crowd cheering, dog barking, footsteps, glass\nsmash, gun shot, horse walk, mixer, motorcycle, rain, and thunder.\nEach audio ﬁle contains a maximum of N number of randomly se-\nlected target classes, where N is sampled from the discrete uniform\ndistribution U(4,9), and the maximum number of simultaneously\nactive (polyphony) sound events is 5. The audio ﬁles do not con-\ntain any background noise. The audio ﬁles amount to a total of 566\nminutes of audio material, and according to the splits introduced\nby [10], roughly 60% of the data are dedicated to training, 20% to\nvalidation, and 20% to testing split. More information about the\ndataset can be found online2.\nWe employ two real-life recording datasets, which were part\nof the Detection and Classiﬁcation of Acoustic Scenes and Events\n(DCASE) challenge datasets for SED in real life audio task: the\nTUT Sound Events 2016 and the TUT Sound Events 2017 [20]. The\nTUT Sound Events 2016 dataset contains sound events recorded in\ntwo environments: home (indoor), which contains 11 classes, and\nresidential area (outdoor), which contains 7 classes. The classes for\nthe home environment are:(object) rustling, (object) snapping, cup-\nboard, cutlery, dishes, drawer, glass jingling, object impact, people\nwalking, washing dishes, and water tap running. The classes for\nthe residential area environment are: (object) banging, bird singing,\ncar passing by, children shouting, people speaking, people walking,\nand wind blowing. The TUT Sound Events 2017 dataset contains\nrecordings in a street environment and contains 6 different classes.\nThese classes are: brakes squeaking, car, children, large vehicle,\npeople speaking, and people walking. For both datasets, we use the\ncross-fold validation split proposed in the DCASE 2016 and 2017\nchallenges. More information about the classes, the cross-fold set-\nting, and the recordings of the datasets can be found online3,4 .\n2http://www.cs.tut.fi/sgn/arg/taslp2017-crnn-\nsed/tut-sed-synthetic-2016\n3http://www.cs.tut.fi/sgn/arg/dcase2016\n4http://www.cs.tut.fi/sgn/arg/dcase2017/\nchallenge\n0 1000 2000 3000 4000\nWeight updates\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0pTF\nFigure 2: The value of pTF with consecutive weight updates with\npmin = 0.05, pmax = 0.95, and Nb = 44. The vertical dashed\nlines indicate steps of 25 epochs (i.e. 25, 50, 75 epochs).\nThe synthetic dataset has randomly selected and placed sound\nevents, therefore not exhibiting any underlying temporal structure\nof sound events. We thus expect the performance of our method to\nbe similar to a method without language modeling on the synthetic\ndataset. Contrarily, the real-life recording datasets exhibit some un-\nderlying temporal structures in the sound events, therefore we ex-\npect our method to perform better than a method without language\nmodelling on these datasets.\nAs input features X we use non-overlapping sequences of\nT = 1024feature vectors. These consist of F = 40log mel-bands,\nextracted using a short-time Fourier transform using a 22 ms Ham-\nming window, 50% overlap and no zero padding. We normalize the\nextracted feature vectors from each dataset to have zero mean and\nunit variance, employing statistics calculated on the training split of\neach corresponding dataset.\n3.2. System and hyper-parameters\nAs our DNN we use the three convolutional neural network (CNN)\nblocks from the system in [10], each consisting of a CNN, a batch\nnormalization function, a max-pooling operation, a dropout func-\ntion, and a rectiﬁed linear unit (ReLU). The kernels of the CNNs\nare square with a width of 5, a stride of 1, and a padding of 2 in\nboth directions. There are 128 ﬁlters for each CNN. The kernel and\nthe stride for the ﬁrst max-pooling operation are {1,5}, for the sec-\nond {1,4}, and for the third {1,2}. These result in F′ = 128for\nH. All CNN blocks use a dropout of 25% at their input, and the last\nCNN block also uses a dropout of 25% at its output. As our RNN\nwe use a gated recurrent unit (GRU) with F′′= 128and our FNN\nis a single-layer feed-forward network with the output size deﬁned\naccording to the amount of classes in each dataset: C = 16 for\nTUT-SED Synthetic 2016, C = 11 and C = 7 for the home and\nresidential area scenes of the TUT Sound Events 2016, and C = 6\nfor the TUT Sound Events 2017. To optimize the weights we em-\nployed the Adam optimizer [21] with default values. We employ a\nbatch size of 8 and we stop the training when the loss for the vali-\ndation data is not decreasing for 50 consecutive epochs. Finally, we\nset the hyper-parameters for pTF at γ = 12−1, pmin = 0.05, and\npmax = 0.9. In Figure 2 is the value of pTF for consecutive weight\nupdates of Nb = 44and for 100 epochs.\nEmpirically we observed that when using the TUT Sound\n61\nDetection and Classiﬁcation of Acoustic Scenes and Events 2019 25–26 October 2019, New York, NY , USA\nEvents 2017, there are some irregular spikes of relatively high gra-\ndients in different batches during training. To alleviate this issue, we\nclipped the ℓ2-norm of the gradient of all weights in each layer of\nour system to a value of 0.5. Additionally, we also observed that for\nthe TUT Sound Events 2017 and TUT-SED Synthetic 2016 datasets,\nour method performed signiﬁcantly better when we decreased the\nlearning rate of the optimizer to 5e−4. Therefore, we employed\nthe above mentioned gradient clipping and modiﬁed learning rate\nfor our method, when using the aforementioned datasets. Finally,\nfor the TUT Sound Events 2016, we employed a binarized version\nof y′denoted y′′, such that y′′\nt,c = 1 if y′\nt,c ≥0.5, and y′′\nt,c = 0\notherwise.\nAll the above hyper-parameters were tuned using the cross val-\nidation set up for the TUT Sound Events 2016 and 2017 datasets\nprovided by DCASE challenges, and the validation split provided\nin [10] for the TUT-SED Synthetic 2016 dataset.\n3.3. Metrics\nWe measure the performance of our method using the frame based\nF1 score and the error rate (ER), according to previous studies and\nthe DCASE Challenge directions [10, 13]. For the real-life datasets,\nthe F1 and ER are the averages among the provided folds (and\namong the different acoustic scenes for the 2016 dataset), while for\nthe synthetic dataset theF1 and ERare obtained on the testing split.\nFinally, we repeat four times the training and testing process for all\ndatasets, in order to obtain a mean and standard deviation (STD) for\nF1 and ER.\n3.4. Baseline\nAs a baseline we employ the system presented in [10], that does\nnot exploit any language model. We do not apply any data augmen-\ntation technique during training and we use the hyper-parameters\npresented in the corresponding paper. This system is referred to as\n“Baseline”.\nWhen using our method with the TUT Sound Events 2017 and\nTUT-SED Synthetic 2016 datasets, we employ a modiﬁed learning\nrate for the optimizer and we clip the ℓ2-norm of the gradient for\nall weights. To obtain a thorough and fair assessment of the perfor-\nmance of our method, we utilize a second baseline for this dataset:\nwe use again the system presented in [10], but we employ the above-\nmentioned gradient clipping and modiﬁed learning rate. We denote\nthis modiﬁed baseline as “modBaseline”.\nFinally, we compare our method to the best results presented\nin [13] which are obtained by employing N-grams as a post-\nprocessing to learn a language model. We report the results of this\nmethod on the TUT Sound Events 2016 datasets, as these are the\nonly ones in the corresponding paper that are based on a publicly\navailable dataset. It must be noted that in [13] was proposed the\nusage of y′\nt−1 as extra input features and the usage of CTC, but\nthe results were inferior to the N-grams approach. Speciﬁcally, the\nper frame F1 score was 0.02 and 0.04 lower and ERwas 0.02 and\n0.15 higher with the usage of y′\nt−1 as an extra input feature and the\nusage of CTC, respectively, compared to the N-grams approach.\n4. RESULTS & DISCUSSION\nIn Table 1 are the obtained results for all the employed datasets. We\nremark that using the proposed language model improves the per-\nformance of SED in the real-life datasets. Speciﬁcally, for the TUT\nTable 1: Mean and STD (Mean/STD) of F1 (higher is better) and\nER (lower is better). For the method [13] only the mean is avail-\nable.\nBaseline modBaseline [13] Proposed\nTUT Sound Events 2016 dataset\nF1 0.28/0.01 – 0.29 0 .37/0.02\nER 0.86/0.02 – 0.94 0 .79/0.01\nTUT Sound Events 2017 dataset\nF1 0.48/0.01 0 .49/0.01 – 0.50/0.02\nER 0.72/0.01 0 .70/0.01 – 0.70/0.01\nTUT-SED Synthetic 2016 dataset\nF1 0.58/0.01 0 .62/0.01 – 0.54/0.01\nER 0.54/0.01 0 .49/0.01 – 0.61/0.02\nSound Events 2016 dataset there is an improvement of 0.09 in the\nF1 score and 0.07 for the ER. For the TUT Sound Events 2017,\nthere is a 0.02 improvement in F1 and 0.02 improvement in ER.\nThese results clearly show that the employment of language mod-\nelling was beneﬁcial for the SED method, when a real life datset was\nused. This is expected, since in a real life scenario the sound events\nexhibit temporal relationships. For example, “people speaking” and\n“people walking” or “washing dishes” and “water tap running” are\nlikely to happen together or one after the other.\nOn the contrary, from Table 1 we observe that there is a\ndecrease in performance with our method on the synthetic data.\nSpeciﬁcally, there is a 0.04 (or 0.08 when compared to modBase-\nline) decrease in F1 and 0.07 (or 0.12 when compared to modBase-\nline) increase in ER. This clearly indicates that using a language\nmodel has a negative impact when the synthetic dataset is used. The\nsound events in the synthetic dataset do not exhibit any temporal re-\nlationships and, thus, the language model cannot provide any beneﬁt\nto the SED method. We suggest that in such a scenario, the network\nfocuses on learning a language model that does not exist in the data\ninstead of solely trying to accurately predict the events on a frame-\nwise basis: this explains the drop in performance compared to the\nbaseline method. Overall, this difference in performance between\nthe two types of datasets strongly suggests that our method learns a\nlanguage model over the activities of the sound events.\nFinally, our system signiﬁcantly outperforms the previous\nmethod [13] on the TUT Sound Events 2016 dataset. This shows\nthat learning a language model is more powerful than crafting it as\na post-processing.\n5. CONCLUSIONS\nIn this paper we presented a method for learning a language model\nfor SED. Our method focuses on systems that utilize an RNN before\nthe the last layer of the SED system, and consists of conditioning\nthe RNN at a time step twith the activities of sound events at the\ntime step t−1. As activities for t−1 we select the ground truth\nearly on the training process, and we gradually switch to the predic-\ntion of the classiﬁer as the training proceeds over time. We evaluate\nour method with three different and publicly available datasets, two\nfrom real life recordings and one synthetic dataset. The obtained re-\nsults indicate that with our method, the utilized SED system learned\na language model over the activities of the sound events, which is\nbeneﬁcial when used on real life datasets.\nIn future work, we will conduct a more in-depth analysis of the\nlearned language model and of the SED performance per class.\n62\nDetection and Classiﬁcation of Acoustic Scenes and Events 2019 25–26 October 2019, New York, NY , USA\n6. REFERENCES\n[1] M. Crocco, M. Cristani, A. Trucco, and V . Murino, “Au-\ndio surveillance: A systematic review,” ACM Comput. Surv.,\nvol. 48, no. 4, pp. 52:1–52:46, Feb. 2016.\n[2] P. Foggia, N. Petkov, A. Saggese, N. Strisciuglio, and\nM. Vento, “Audio surveillance of roads: A system for de-\ntecting anomalous sounds,” IEEE Transactions on Intelligent\nTransportation Systems, vol. 17, no. 1, pp. 279–288, Jan 2016.\n[3] T. Butko, F. G. Pla, C. Segura, C. Nadeu, and J. Hernando,\n“Two-source acoustic event detection and localization: Online\nimplementation in a smart-room,” in Proc. European Signal\nProcessing Conference, Aug 2011.\n[4] C. Busso, S. Hernanz, Chi-Wei Chu, Soon-il Kwon, Sung\nLee, P. G. Georgiou, I. Cohen, and S. Narayanan, “Smart\nroom: participant and speaker localization and identiﬁcation,”\nin Proc. IEEE International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP), March 2005.\n[5] B. J. Furnas and R. L. Callas, “Using automated recorders\nand occupancy models to monitor common forest birds across\na large geographic region,” The Journal of Wildlife Manage-\nment, vol. 79, no. 2, pp. 325–337, 2015.\n[6] T. A. Marques, L. Thomas, S. W. Martin, D. K. Mellinger,\nJ. A. Ward, D. J. Moretti, D. Harris, and P. L. Tyack, “Es-\ntimating animal population density using passive acoustics,”\nBiological Reviews, vol. 88, no. 2, pp. 287–309, 2013.\n[7] E. Benetos, D. Stowell, and M. D. Plumbley, Approaches to\nComplex Sound Scene Analysis. Springer International Pub-\nlishing, 2018, pp. 215–242.\n[8] G. Parascandolo, H. Huttunen, and T. Virtanen, “Recurrent\nneural networks for polyphonic sound event detection in real\nlife recordings,” in Proc. IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), March\n2016.\n[9] T. Hayashi, S. Watanabe, T. Toda, T. Hori, J. Le Roux, and\nK. Takeda, “Duration-controlled LSTM for polyphonic sound\nevent detection,” IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 25, no. 11, pp. 2059–2070,\nNovember 2017.\n[10] E. C ¸ akir, G. Parascandolo, T. Heittola, H. Huttunen, and\nT. Virtanen, “Convolutional recurrent neural networks for\npolyphonic sound event detection,” IEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 25, no. 6,\npp. 1291–1303, June 2017.\n[11] S. Adavanne, P. Pertil ¨a, and T. Virtanen, “Sound event detec-\ntion using spatial features and convolutional recurrent neural\nnetwork,” in Proc. IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), March 2017.\n[12] Y . Wang and F. Metze, “A ﬁrst attempt at polyphonic sound\nevent detection using connectionist temporal classiﬁcation,”\nin Proc. IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), March 2017.\n[13] G. Huang, T. Heittola, and T. Virtanen, “Using sequential\ninformation in polyphonic sound event detection,” in 2018\n16th International Workshop on Acoustic Signal Enhancement\n(IWAENC), Sep. 2018, pp. 291–295.\n[14] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber,\n“Connectionist temporal classiﬁcation: Labelling unseg-\nmented sequence data with recurrent neural networks,”\nin Proceedings of the 23rd International Conference on\nMachine Learning , ser. ICML ’06. New York, NY ,\nUSA: ACM, 2006, pp. 369–376. [Online]. Available:\nhttp://doi.acm.org/10.1145/1143844.1143891\n[15] R. J. Williams and D. Zipser, “A learning algorithm for contin-\nually running fully recurrent neural networks,” Neural Com-\nputation, vol. 1, no. 2, pp. 270–280, June 1989.\n[16] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Sched-\nuled sampling for sequence prediction with recurrent\nneural networks,” in Proceedings of the 28th Interna-\ntional Conference on Neural Information Processing Systems\n- Volume 1 , ser. NIPS’15. Cambridge, MA, USA:\nMIT Press, 2015, pp. 1171–1179. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=2969239.2969370\n[17] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” in Interna-\ntional Conference on Learning Representations (ICLR), 2015.\n[18] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to\nsequence learning with neural networks,” in Advances in\nNeural Information Processing Systems 27, Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q.\nWeinberger, Eds. Curran Associates, Inc., 2014, pp. 3104–\n3112. [Online]. Available: http://papers.nips.cc/paper/5346-\nsequence-to-sequence-learning-with-neural-networks.pdf\n[19] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and\ntell: A neural image caption generator,” in 2015 IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR),\nJune 2015, pp. 3156–3164.\n[20] A. Mesaros, T. Heittola, and T. Virtanen, “TUT database\nfor acoustic scene classiﬁcation and sound event detection,”\nin 24th European Signal Processing Conference 2016 (EU-\nSIPCO 2016), Budapest, Hungary, 2016.\n[21] D. Kingma and J. Ba, “Adam: A method for stochastic opti-\nmization,” in 3rd International Conference for Learning Rep-\nresentations, May 2015.\n63"
}