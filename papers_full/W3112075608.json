{
  "title": "Confidence-aware Non-repetitive Multimodal Transformers for TextCaps",
  "url": "https://openalex.org/W3112075608",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5060872432",
      "name": "Zhaokai Wang",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A5029434214",
      "name": "Renda Bao",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5060958969",
      "name": "Qi Wu",
      "affiliations": [
        "The University of Adelaide"
      ]
    },
    {
      "id": "https://openalex.org/A5100330138",
      "name": "Si Liu",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2053317383",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2927058921",
    "https://openalex.org/W6760808182",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W6929439978",
    "https://openalex.org/W2994470118",
    "https://openalex.org/W2982834534",
    "https://openalex.org/W6766818547",
    "https://openalex.org/W2784050770",
    "https://openalex.org/W2973091507",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W3007389333",
    "https://openalex.org/W2810028092",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W2298368322",
    "https://openalex.org/W6761041305",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W6725207838",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2937725239",
    "https://openalex.org/W3035170495",
    "https://openalex.org/W2967045987",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W3101411491",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3034336960",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034772468",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W3034792612",
    "https://openalex.org/W2967615747",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W639708223",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2968549119",
    "https://openalex.org/W2963517393",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2809273748",
    "https://openalex.org/W3004846386"
  ],
  "abstract": "When describing an image, reading text in the visual scene is crucial to understand the key information. Recent work explores the TextCaps task, i.e. image captioning with reading Optical Character Recognition (OCR) tokens, which requires models to read text and cover them in generated captions. Existing approaches fail to generate accurate descriptions because of their (1) poor reading ability; (2) inability to choose the crucial words among all extracted OCR tokens; (3) repetition of words in predicted captions. To this end, we propose a Confidence-aware Non-repetitive Multimodal Transformers (CNMT) to tackle the above challenges. Our CNMT consists of a reading, a reasoning and a generation modules, in which Reading Module employs better OCR systems to enhance text reading ability and a confidence embedding to select the most noteworthy tokens. To address the issue of word redundancy in captions, our Generation Module includes a repetition mask to avoid predicting repeated word in captions. Our model outperforms state-of-the-art models on TextCaps dataset, improving from 81.0 to 93.0 in CIDEr. Our source code is publicly available.",
  "full_text": "Conﬁdence-aware Non-repetitive Multimodal Transformers for TextCaps\nZhaokai Wang1, Renda Bao2, Qi Wu3, Si Liu1*\n1 Beihang University, Beijing, China\n2 Alibaba Group, Beijing, China\n3 University of Adelaide, Australia\nfwzk1015, liusig@buaa.edu.cn, renda.brd@alibaba-inc.com, qi.wu01@adelaide.edu.au\nAbstract\nWhen describing an image, reading text in the visual scene\nis crucial to understand the key information. Recent work ex-\nplores the TextCaps task, i.e. image captioning with reading\nOptical Character Recognition (OCR) tokens, which requires\nmodels to read text and cover them in generated captions. Ex-\nisting approaches fail to generate accurate descriptions be-\ncause of their (1) poor reading ability; (2) inability to choose\nthe crucial words among all extracted OCR tokens; (3) rep-\netition of words in predicted captions. To this end, we pro-\npose a Conﬁdence-aware Non-repetitive Multimodal Trans-\nformers (CNMT) to tackle the above challenges. Our CNMT\nconsists of a reading, a reasoning and a generation modules,\nin which Reading Module employs better OCR systems to en-\nhance text reading ability and a conﬁdence embedding to se-\nlect the most noteworthy tokens. To address the issue of word\nredundancy in captions, our Generation Module includes a\nrepetition mask to avoid predicting repeated word in captions.\nOur model outperforms state-of-the-art models on TextCaps\ndataset, improving from 81.0 to 93.0 in CIDEr. Our source\ncode is publicly available 1.\nIntroduction\nImage Captioning has emerged as a prominent area at the\nintersection of vision and language. However, current Image\nCaptioning datasets (Chen et al. 2015; Young et al. 2014)\nand models (Anderson et al. 2018; Huang et al. 2019) pay\nfew attention to reading text in the image, which is crucial to\nscene understanding and its application, such as helping vi-\nsually impaired people understand the surroundings. For ex-\nample, in Figure 1, Ushahidi on the screen tells the user the\nwebsite he is browsing. To address this drawback, Sidorov\net al. has introduced TextCaps (Sidorov et al. 2020) dataset,\nwhich requires including text in predicted captions.\nIn order to generate captions based on text from images,\nthe model needs to (1) recognize text in the image with\nOptical Character Recognition (OCR) methods; (2) capture\nthe relationship between OCR tokens and visual scenes;\n(3) predict caption tokens from ﬁxed vocabulary and OCR\ntokens based on previous features. Current state-of-the-art\n*Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1https://github.com/wzk1015/CNMT\nFigure 1: Our model extracts text in image with better OCR\nsystems and records their recognition conﬁdence as conﬁ-\ndence embedding, which represents semantic importance of\nOCR tokens. After reasoning with objects and text features,\nit predicts caption tokens with a repetition mask to avoid re-\ndundancy.\nmodel M4C-Captioner (Sidorov et al. 2020), adapted to\nthe TextCaps task from M4C (Hu et al. 2020), fuses vi-\nsual modality and text modality by embedding them into a\ncommon semantic space and predicts captions with multi-\nword answer decoder based on features extracted from mul-\ntimodal transformers (Vaswani et al. 2017).\nWhile M4C-Captioner manages to reason over text in im-\nages, it is originally designed for TextVQA (Singh et al.\n2019), and thus fails to ﬁt into Image Captioning task.\nIt mainly has three problems. Firstly, its OCR system\nRosetta (Borisyuk, Gordo, and Sivakumar 2018) is not ro-\nbust enough, making it suffer from bad recognition results.\nAs words in captions come from either pre-deﬁned vocab-\nulary (common words) or OCR tokens, even a tiny error in\nrecognizing uncommon words can lead to missing key in-\nformation of the image.\nSecondly, compared with answers in TextVQA where\nall OCR tokens can be queried in questions, captions in\nTextCaps should only focus on the most important OCR to-\nkens in the image. In Figure 1, the key OCR tokens should\nbe nokia and ushahidi, while others like location, descrip-\ntion are dispensable. In fact, having these words in captions\nmakes them verbose and even has negative effects. However,\nM4C-Captioner simply feeds all the OCR tokens into trans-\nformers without paying attention to their semantic signiﬁ-\ncance, so irrelevant OCR tokens can appear in captions.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n2835\nThirdly, due to the use of Pointer Network (Vinyals, For-\ntunato, and Jaitly 2015) which directly copies input OCR\ntokens to output, M4C-Captioner’s decoding module tends\nto predict the same word multiple times (e.g. describe one\nobject or OCR token repeatedly) in captions, like describing\nthe image in Figure 1 as a nokia phone saying nokia. This\nredundancy leads to less natural captions which also misses\nkey information ushahidi, thus it should be avoided.\nIn this paper, we address these limitations with our new\nmodel Conﬁdence-aware Non-repetitive Multimodal Trans-\nformers (CNMT), as shown in Figure 1. For the ﬁrst issue,\nwe employ CRAFT (Baek et al. 2019b) and ABCNet (Liu\net al. 2020) for text detection, and four-stage STR (Baek\net al. 2019a) for text recognition. These new OCR systems\nhelp to improve reading ability of our model.\nFor the second issue, we record recognition conﬁdence\nof each OCR token as a semantic feature, based on the in-\ntuition that OCR tokens with higher recognition conﬁdence\nare likely to be crucial and should be included in captions,\nas they are frequently more conspicuous and recognizable.\nFor instance, among all the OCR tokens in Figure 1, tokens\nwith high recognition conﬁdence (nokia, ushahidi) are con-\nsistent with our analysis on the key information in the image,\nwhile less recognizable ones (location, description) match\ndispensable words. Besides, tokens with lower conﬁdence\nare more likely to have spelling mistakes. Therefore, we use\nrecognition conﬁdence to provide conﬁdence embedding of\nOCR tokens. In Reasoning Module, OCR tokens and their\nrecognition conﬁdence are embedded together with multi-\nple OCR token features, and fed into the multimodal trans-\nformers with object features of the image, fusing these two\nmodalities in a common semantic space.\nFor the third issue, we apply a repetition mask on the orig-\ninal pointer network (Vinyals, Fortunato, and Jaitly 2015)\nin the decoding step, and predict caption tokens iteratively.\nRepetition mask helps our model avoid repetition by mask-\ning out words that have appeared in previous time steps. We\nensure that the repetition mask ignores common words such\nas a, an, the, of, says, for they act as an auxiliary role in\ncaptions and are essential for ﬂuency. As shown in Figure 1,\nat decoding step t, predicted score of nokia is masked out\nas it appeared at step 2, allowing our model to generate the\ncorrect caption a nokia phone saying ushahidi without rep-\netition. Meanwhile, previously predicted common words a,\nsaying is not affected in case of necessary repetition of them.\nIn summary, our contributions are threefold: (1) We\npropose our Conﬁdence-aware Non-repetitive Multimodal\nTransformers (CNMT) model, which employs better OCR\nsystems to improve reading ability, and uses conﬁdence em-\nbedding of OCR tokens as representation of semantic signif-\nicance to select the most important OCR tokens; (2) With the\nrepetition mask, our model effectively avoids redundancy in\npredicted captions, and generates more natural captions; (3)\nOur model signiﬁcantly outperforms current state-of-the-art\nmodel of TextCaps dataset by 12.0 in CIDEr on test set, im-\nproving from 81.0 to 93.0.\nRelated Work\nText based Image Captioning.In recent years, many works\nhave focused on vision or language tasks (Zheng et al. 2019;\nGao et al. 2020; Liao et al. 2020). Conventional Image Cap-\ntioning datasets (Chen et al. 2015; Young et al. 2014) aim to\ndescribe each image with a caption, but they tend to ignore\ntext in the images as another modality, which is of great im-\nportance when describing the key information in the image.\nRecently TextCaps (Sidorov et al. 2020) dataset has been in-\ntroduced, which requires reading text in the images. State-\nof-the-art models for conventional Image Captioning like\nBUTD (Anderson et al. 2018), AoANet (Huang et al. 2019)\nfail to describe text in TextCaps images. M4C-Captioner\n(Sidorov et al. 2020), adapted from TextVQA (Singh et al.\n2019) benchmark model M4C (Hu et al. 2020), is proposed\nto fuse text modality and image modality to make predic-\ntions. It employs multimodal transformers (Vaswani et al.\n2017) to encode image and text and predicts captions with an\niterative decoding module. However, its performance is lim-\nited by poor reading ability and its inability to select the most\nsemantically important OCR token in the image. Besides,\nits decoding module, originally designed for TextVQA task,\nshows redundancy in predicted captions. In this paper, we\npropose our CNMT model, which applies conﬁdence em-\nbedding, better OCR systems and a repetition mask to ad-\ndress these limitations.\nOptical Character Recognition (OCR). OCR helps to\nread text in images, which is crucial to TextCaps task.\nOCR involves two steps: detection (ﬁnd text regions in\nthe image) and recognition (extract characters from text re-\ngions). One way of text detection method is to use box re-\ngression adapted from popular object detectors (Liao, Shi,\nand Bai 2018). Another method is based on segmenta-\ntion (Long et al. 2018). For text detection, CRAFT (Baek\net al. 2019b) effectively detects text regions by exploring\ncharacter-level afﬁnity. Recent work ABCNet (Liu et al.\n2020) presents a way to ﬁt arbitrarily-shaped text by using\nAdaptive Bezier-Curve. For scene text recognition (STR),\nexistent approaches have beneﬁted from the combination\nof convolutional neural networks and recurrent neural net-\nworks (Shi, Bai, and Yao 2016) and employment of trans-\nformation modules for text normalization such as thin-plate\nspline (Shi et al. 2016). Baek et al. introduce four-stage STR\nframework for text recognition. As for the TextCaps task,\nM4C-Captioner uses Rosetta (Borisyuk, Gordo, and Sivaku-\nmar 2018) as OCR processor, but it is not robust enough to\nread text correctly. To solve this problem, our model adapts\nCRAFT (Baek et al. 2019b) and ABCNet (Liu et al. 2020)\nas the detection module, and four-stage STR (Baek et al.\n2019a) as the recognition module.\nMethods\nPipeline Overview\nOur CNMT is composed of three modules as shown in Fig-\nure 2. The input image is ﬁrst fed into Reading Module to\nextract OCR tokens along with their recognition conﬁdence,\nas the token-conﬁdence table on the top right part. Then Rea-\nsoning Module extracts object features of the image, embeds\n2836\nFigure 2: Overview of our CNMT model. In Reading Module, we extract OCR tokens with better OCR systems, and record their\nrecognition conﬁdence; Then Reasoning Module fuses OCR token features and object features with multimodal transformers,\nand Generation Module predicts caption tokens iteratively from a ﬁxed vocabulary OCR tokens based on pointer network. A\nrepetition mask is employed to avoid repetition in predictions.\nobjects and OCR features into a common semantic space,\nand fuses them and previous output embedding with mul-\ntimodal transformers. Finally, Generation Module uses out-\nput of the multimodal transformers and predicts caption to-\nkens iteratively based on Pointer Network and the repetition\nmask, like predicting LG at current step.\nReading Module\nAs shown in the top part of Figure 2, Reading Module de-\ntects text regions in the image, and extract OCR tokens from\nthese regions, jointly with conﬁdence features of tokens.\nOCR systems. We use two models for text detection,\nCRAFT (Baek et al. 2019b) and ABCNet (Liu et al. 2020).\nText regions detected separately by CRAFT and ABCNet\nare combined together and fed into the text recognition part,\nas the four blue OCR boxes in the top part of Figure 2. For\ntext recognition, we use deep text recognition benchmark\nbased on four-stage STR framework (Baek et al. 2019a). We\ncombine OCR tokens extracted from our new OCR systems\nwith the original Rosetta OCR tokens, and feed them into\nReasoning Module.\nConﬁdence embedding. Our previously mentioned intu-\nition is that OCR tokens with higher recognition conﬁdence\ntend to be crucial that should be included in captions, as\nthey are frequently more conspicuous, recognizable and less\nlikely to have spelling mistakes. Based on this, we record\nrecognition conﬁdence xconf of each OCR token from our\ntext recognition system STR, where xconf is between 0 and\n1. We then feed these conﬁdence features into the next mod-\nule to provide conﬁdence embedding. As original Rosetta\ntokens do not include recognition conﬁdence, OCR tokens\nthat only appear in Rosetta recognition result are recorded\nby a default conﬁdence value cdefault . As shown in the top\nright part of Figure 2, we get several token-conﬁdence pairs\nas the result of Reading Module.\nReasoning Module\nFor Reasoning Module we mainly follow the design of\nM4C-Captioner (Sidorov et al. 2020), but with better OCR\ntoken embedding. As shown in the bottom left part of Fig-\nure 2, object features and OCR token features are jointly\nprojected to a d-dimensional semantic space, and extracted\nby multimodal transformers.\nObject embedding. To get object embedding, we ap-\nply pretrained Faster R-CNN (Ren et al. 2015) as the\ndetector to extract appearance feature xfr\nm of each ob-\nject m. In order to reason over spatial information of\neach object, we denote its location feature by xb\nm =\n[xmin=W; ymin=H; xmax=W; ymax=H]. The ﬁnal object\nembedding xobj\nm is projected to a d-dimensional vector as\nxobj\nm = LN(W1xfr\nm ) +LN(W2xb\nm) (1)\n, where W1 and W2 are learnable parameters, and LN de-\nnotes layer normalization.\nOCR token embedding. To get rich representation of\nOCR tokens, we use FastText (Bojanowski et al. 2017),\nFaster R-CNN, PHOC (Almaz´an et al. 2014) to extract sub-\nword feature xft , appearance featurexfr and character-level\nfeature xp respectively. Location feature is represented as\nxb\ni = [xmin=W; ymin=H; xmax=W; ymax=H]. Then we add\nthe conﬁdence feature xconf , based on the intuition that our\nmodel should focus more on tokens with higher recognition\nconﬁdence. The ﬁnal OCR token embedding xocr\ni is a list of\nd-dimensional vectors\n2837\nxocr\ni = LN(W3xft\ni + W4xfr\ni + W5xp\ni )\n+LN(W6xb\ni) +LN(W7xconf\ni )\n(2)\nwhere W3, W4, W5, W6 and W7 are learnable parameters,\nand LN denotes layer normalization.\nMultimodal transformers. After extracting object em-\nbedding and OCR token embedding, a stack of transformers\n(Vaswani et al. 2017) are applied to these two input modali-\nties, allowing each entity to attend to other entities from the\nsame modality or the other one. Decoding output of previous\nstep is also embedded and fed into the transformers, like pre-\nvious output says in Figure 2. Previous decoding outputxdec\nt\u00001\nis the corresponding weight of the linear layer in Generation\nModule (if previous output is from vocabulary), or OCR to-\nken embedding xocr\nn (if from OCR tokens). The multimodal\ntransformers provide a list of feature vectors as output:\n[zobj; zocr; zdec\nt\u00001] =mmt([xobj; xocr; xdec\nt\u00001]) (3)\nwhere mmt denotes multimodal transformers.\nGeneration Module\nGeneration Module takes output of multimodal transformers\nin Reading Module as input, predicts scores of each OCR\ntoken and vocabulary word, employs the repetition mask,\nand selects the predicted word of each time step, as shown\nin the bottom right part of Figure 2.\nPredicting scores. Each token in the predicted caption\nmay come from ﬁxed vocabulary words fwvoc\nn gor OCR to-\nkens fwocr\ni g. Following the design of M4C-Captioner, we\ncompute scores of these two sources based on transformer\noutput zdec\nt\u00001 (corresponding to input xdec\nt\u00001). Scores of ﬁxed\nvocabulary words and OCR tokens are calculated with a lin-\near layer and Pointer Network (Vinyals, Fortunato, and Jaitly\n2015) respectively. Pointer Network helps to copy the input\nOCR token to output. Linear layer and Pointer Network gen-\nerate a V dimensional OCR score yocr\nt and a N dimensional\nvocabulary score yvoc\nt . Here V is the number of words in the\nﬁxed vocabulary and N is pre-deﬁned max number of OCR\ntokens in an image. This process can be shown as:\nyocr\nt = PN (zdec\nt\u00001) (4)\nyvoc\nt = Wz dec\nt\u00001 + b (5)\nwhere PN denotes Pointer Network. W and b are learnable\nparameters.\nPrevious approaches consider scores of OCR tokens and\nvocabulary separately even if one word appears in both\nsources. However, this may lead to two sources competing\nwith each other and predicting another inappropriate word.\nTherefore, we add scores of one word from multiple sources\ntogether to avoid competition. Adding scores of n-th vocab-\nulary word can be described as:\nyadd\nt;n = yvoc\nt;n +\nP\ni:wocr\ni =wvoc\nn\nyocr\nt;i (6)\nThen the ﬁnal scores are the concatenation of added vo-\ncabulary scores and OCR scores:\nyt = [yadd\nt ; yocr\nt ] (7)\nRepetition mask. As we have mentioned in Section 1,\nrepetition in captions brings negative effects on their ﬂuency.\nIn order to avoid repetition, we apply a repetition mask in\nGeneration Module. At step t of inference, the N + V di-\nmensional concatenated scores yt is added by a mask vector\nMt 2RN+V , where the i-th element of Mt is\nMt;i =\n\u001a\n\u00001 if wordi appeared in previous steps\n0 otherwise\n(8)\nm is set to a minimum value. This helps to minimize the\nscores of elements that have appeared in previous steps, like\nthe masked word billboard in Figure 2.\nNote that M is applied only during inference. It focuses\non repeating words, so when one word appears in both ﬁxed\nvocabulary and OCR tokens or in multiple OCR tokens,\nall the sources will be masked out together. In addition,\nwe ignore common words when applying mask, consider-\ning words like a, an, of, says, on are indispensable to the\nthe ﬂuency of captions. Common words are deﬁned as top-\nC frequency words in ground-truth captions of training set,\nwhere C is a hyper-parameter.\nIn Figure 3 we show an illustration of the repetition mask.\nEach row shows outputs(left) and predicted scores(right) at\neach decoding step. Since nokia is predicted at step 2, its\nscore is masked out from step 3 to the end (marked as grey).\nScores of phone are masked out from step 4. Common words\na and saying are not masked. This mask prevents our model\nfrom predicting nokia at the last step.\nTherefore, the output word at step t is calculated as\noutputt = argmax(yt + Mt) (9)\nOur model iteratively predicts caption tokens through\ngreedy search, starting with begin token hsi. Decoding ends\nwhen hnsiis predicted.\nFigure 3: Illustration of the repetition mask. We show scores\nof words and predicted word at each step. Grey indicates\nmasked word. Common words like a, saying are ignored for\ntheir essentiality.\n2838\n# Method BLEU-4 METEOR ROUGE L SPICE CIDEr\n1 BUTD (Anderson et al. 2018) 20.1 17.8 42.9 11.7 41.9\n2 AoANet (Huang et al. 2019) 20.4 18.9 42.9 13.2 42.7\n3 M4C-Captioner (Sidorov et al. 2020) 23.3 22.0 46.2 15.6 89.6\n4 CNMT (ours) 24.8 23.0 47.1 16.3 101.7\nTable 1: Evaluation on TextCaps validation set. We provide a comparison with prior works. Beneﬁting from better OCR systems,\nrecognition conﬁdence embedding and the repetition mask, our model outperforms state-of-the-art approach by a signiﬁcant\namount.\n# Method BLEU-4 METEOR ROUGE L SPICE CIDEr\n1 BUTD (Anderson et al. 2018) 14.9 15.2 39.9 8.8 33.8\n2 AoANet (Huang et al. 2019) 15.9 16.6 40.4 10.5 34.6\n3 M4C-Captioner (Sidorov et al. 2020) 18.9 19.8 43.2 12.8 81.0\n4 CNMT (ours) 20.0 20.8 44.4 13.4 93.0\n5 Human (Sidorov et al. 2020) 24.4 26.1 47.0 18.8 125.5\nTable 2: Evaluation on TextCaps test set. Our model achieves state-of-the-art performance on all of the TextCaps metrics,\nnarrowing the gap between models and human performance.\nExperiments\nWe train our model on TextCaps dataset, and evaluate its\nperformance on validation set and test set. Our model out-\nperforms previous work by a signiﬁcant margin. We also\nprovide ablation study results and qualitative analysis.\nImplementation Details\nFor text detection, we use pretrained CRAFT (Baek et al.\n2019b) model and ABCNet (Liu et al. 2020) model with\n0.7 conﬁdence threshold. Afﬁne transformation is applied\nto adjust irregular quadrilateral text regions to rectangular\nbounding box. We use pretrained four-stage STR framework\n(Baek et al. 2019a) for text recognition. For OCR tokens\nthat only appear in Rosetta results, we set default conﬁdence\ncdefault = 0:90. We set the max OCR number N = 50,\nand apply zero padding to align to the maximum number.\nThe dimension of the common semantic space is d = 768.\nGeneration Module uses 4 layers of transformers with 12 at-\ntention heads. The other hyper-parameters are the same with\nBERT-BASE (Devlin et al. 2018). The maximum number of\ndecoding steps is set to 30. Words that appear \u001510 times in\ntraining set ground-truth captions are collected as the ﬁxed\nvocabulary, together with hpadi, hsiand hnsitokens. The\ntotal vocabulary size V = 6736. Common word ignoring\nthreshold C of the repetition mask is set to 20.\nThe model is trained on the TextCaps dataset for 12000 it-\nerations. The initial learning rate is 1e-4. We multiply learn-\ning rate by 0.1 at 5000 and 7000 iterations separately. At\nevery 500 iterations we compute the BLEU-4 metric on val-\nidation set, and select the best model based on all of them.\nThe entire training takes approximately 12 hours on 4 RTX\n2080 Ti GPUs. All of our experimental results are generated\nby TextCaps online platform submissions.\nComparison with SoTA\nWe measure our model’s performance on TextCaps dataset\nusing BLEU (Papineni et al. 2002), METEOR (Banerjee and\nLavie 2005), ROUGE\nL (Lin 2004), SPICE (Anderson et al.\n2016) and CIDEr (Vedantam, Lawrence Zitnick, and Parikh\n2015), and mainly focus on CIDEr when comparing differ-\nent methods, following the original TextCaps paper (Sidorov\net al. 2020).\nWe evaluate our model on TextCaps validation set and test\nset, and compare our results with TextCaps baseline mod-\nels BUTD (Anderson et al. 2018), AoANet (Huang et al.\n2019) and state-of-the-art model M4C-captioner(Sidorov\net al. 2020), as shown in Table 1 and Table 2. Our proposed\nmodel outperforms state-of-the-art models on all ﬁve met-\nrics by a large margin, improving by around 12 CIDEr scores\non both validation set and test set. While the original gap\nbetween human performance and M4C-Captioner is 44.5 in\nCIDEr, our model narrows this gap by 27% relative.\nAblation Study\nWe conduct ablation study on OCR systems, conﬁdence em-\nbedding and the repetition mask on validation set, and prove\ntheir effectiveness.\nAblation on OCR systems. We ﬁrst examine our new\nOCR systems through ablation study. We extract new OCR\ntokens with CRAFT and ABCNet and use four-stage STR\nfor recognition, combine them with the original Rosetta\nOCR tokens, and extract their sub-word, character, appear-\nance and location features. To focus on OCR system im-\nprovement, other parts of the model are kept consistent with\nM4C-Captioner. The result is shown in Table 3. Compared\nwith only using Rosetta-en, the model improves by around 3\nCIDEr scores after employing CRAFT, and another 3 CIDEr\n2839\nOCR system(s) CIDEr\nRosetta 89.6 -\nRosetta + CRAFT 92.7 (+3.1)\nRosetta + CRAFT + ABCNet 95.5 (+5.9)\nTable 3: OCR systems experiment on TextCaps validation\nset. We keep other parts of the same conﬁguration as M4C-\nCaptioner in order to focus on OCR improvements. Our two\ndetection modules CRAFT and ABCNet both bring signiﬁ-\ncant improvements.\nOCR system(s) # Total tokens # In GT tokens\nRosetta-en 40.8k 5.5k\nRosetta-en +\nCRAFT + ABCNet 117.5k 10.0k\nTable 4: OCR tokens analysis on validation set. We compare\nthe original OCR system with our new ones, and demon-\nstrate that both number of total OCR tokens and number of\ntokens that appear in ground truth captions have increased\nby a large amount.\nscores after jointly employing ABCNet and CRAFT.\nAnother analysis can be seen in Table 4, where we com-\npute number of all OCR tokens and tokens that appear in\nground truth captions to evaluate our OCR system improve-\nment. After employing new OCR systems, total OCR tokens\nnearly tripled, and tokens that appear in ground truth cap-\ntions nearly doubled, indicating our model’s stronger read-\ning ability. Jointly analyzing Table 3 and Table 4, we con-\nclude that better OCR systems lead to a larger amount of\nOCR tokens and thus higher probability to predict the cor-\nrect word.\nAblations on conﬁdence embedding. We evaluate the\nperformance of OCR conﬁdence embedding by ablating\nrecognition conﬁdence, as shown in Table 5. Comparing line\n1 and 3, we ﬁnd that conﬁdence embedding helps to improve\nperformance by around 2.0 in CIDEr. This validates our in-\ntuition that recognition conﬁdence serves as a way to un-\nderstand semantic signiﬁcance of OCR tokens and select the\nmost important one when generating captions.\nWe compare our embedding method with a rather simple\none: simply multiply recognition conﬁdence (scalar between\n0 and 1) to the ﬁnal OCR token embedding xocr\ni . Through\nthis way, an OCR token is nearly a padding token (all ze-\nros) if its conﬁdence is small. However, as shown in line 2,\nthis method actually brings negative effects, because it dis-\nturbs the original rich OCR token embedding. It also lacks\nlearnable parameters, so the model is unable to decide the\nimportance of conﬁdence on its own.\nAblations on the repetition mask. In Table 6 we pro-\nvide ablation study on the repetition mask. It can be seen\nthat the repetition mask improve performance by a relatively\nlarge amount of 3.6 in CIDEr. This proves our model’s abil-\nity to predict more ﬂuent and natural captions after remov-\nMethod CIDEr\nCNMT (w/o conﬁdence) 99.7 -\nCNMT (multiply conﬁdence) 98.9 (-0.8)\nCNMT (conﬁdence embedding) 101.7 (+2.0)\nTable 5: Ablation of conﬁdence embedding on validation\nset. Conﬁdence embedding brings improvement on perfor-\nmance, while simply multiplying conﬁdence to OCR token\nembedding leads to negative results.\nMethod Ignoring threshold C CIDEr\nCNMT (w/o mask) - 98.1\nCNMT 0 92.6\nCNMT 10 101.6\nCNMT 20 101.7\nCNMT 50 99.4\nTable 6: Ablation of the repetition mask on validation set.\nRepetition mask helps to improve performance signiﬁcantly.\nExperiment on hyper-parameter C indicates that a small ig-\nnoring threshold has negative effects because of the essen-\ntial auxiliary effects of these common words, while a large\nthreshold limits the scope of the repetition mask.\ning repeating words, which solves an existing problem of\nprevious approaches. Qualitative examples of therepetition\nmask can be found in Figure 4 (a,c,g) where we give predic-\ntions of M4C-Captioner and our CNMT model, and prove\nour model’s ability to avoid repetition effectively.\nTo prove the essentiality of ignoring common words when\napplying the repetition mask, we evaluate our model with an\nindiscriminate repetition mask, i.e. all words include words\nlike a, an, says are masked out once they appear in previ-\nous steps. The result is shown in line 2 of Table 6, where\nwe ﬁnd a large decrease in CIDEr, demonstrating the impor-\ntance of ignoring common words. In fact, we ﬁnd indiscrim-\ninate mask often generating unnatural captions, such as a\nposter for movie called kaboom with man holding gunwhere\narticles a are masked out, or a coin from 1944 next to other\nmonedas where money is replaced with rarely used synonym\nmonedas. Such examples indicate that it is necessary to al-\nlow repetition of common words.\nWe conduct further experiments on hyper-parameter C,\nwhich is shown in Table 6. When C is set to a relatively\nsmall value, the repetition mask is applied on more com-\nmonly appeared words, and becomes indiscriminate when\nC = 0. On the contrary, when C is set to a large value,\nthe scope of the repetition mask is limited, which brings\nnegative effects. We observe that the best performance is\nachieved when C is set to 20.\nQualitative Analysis\nIn Figure 4 we provide example images of validation set\nand predictions from our model and M4C-Captioner. In\nFigure 4 (e), with the help of conﬁdence embedding, our\nmodel chooses the most recognizable OCR token 21 instead\n2840\na b c d\nM4C-Captioner: a plate of\nfood is on a table witha plate\nof foodand a plate ofhonghe\non it.\nOurs: a plate of food is on a\ntable with a box that says\nhonghe.\nHuman: a plate of skewed\nmeat sits on a table next to a\npack of honghe cigarettes.\nM4C-Captioner: a bottle of\ndouble ipa is next to a\nglass.\nOurs: a bottle ofindia pale\nale is next to a glass.\nHuman: a bottle of india\npale ale is next to a glass of\nbeer.\nM4C-Captioner: a sign for\ndog doghangs on the side\nof a building.\nOurs: a billboard fordog\njanitor hangs on a street.\nHuman: a billboard for dog\njanitor is on a pole next to\na building.\nM4C-Captioner: a sign for\nthe wrigley field of chicago\ncubs.\nOurs: a sign for thewrigley\nhome of chicago field.\nHuman: the digital sign at\nwrigley field states \"welcome\nto wrigley field\".\ne f g h\nM4C-Captioner: a baseball\nfield with a banner that says\ncket.\nOurs: a baseball player with\nthe number21 on his jersey is\nstanding on the field.\nHuman: a player on a field\nwith the number 21 on their\nback.\nM4C-Captioner: a wooden\nbox with a sign that says\nthe urban ketplace.\nOurs: a wooden door with a\nsign that says the urban\nwood marketplace.\nHuman: wooden wall with\na yellow sign that says \"the\nurban wood marketplace\".\nM4C-Captioner: a bottle\nof gireau gireau pure\nfrench gin.\nOurs: a bottle of gireau\ngin is on a wooden shelf.\nHuman: a bottle of gireau\ngin is sitting on a wooden\nshelf.\nM4C-Captioner: a paper\nthat says opt-out!! on it.\nOurs: a paper that saysstop\nbefore all schoolon it.\nHuman: a piece of paper on\na wall informs of a deadline\nof oct. 1.\nFigure 4: Qualitative examples on TextCaps validation set. Yellow indicates words from OCR tokens. Italic font indicates\nrepetitive words. Compared to with previous work, our model has better reading ability, and can select the most important\nwords from OCR tokens with conﬁdence embedding. It also avoids repetition in predictions compared with M4C-Captioner.\nof out-of-region word CKET which is predicted by M4C-\nCaptioner. Figure 4 (b,f) shows our model’s robust reading\nability towards curved text and unusual font text. From Fig-\nure 4 (a,c,g) we can see that our model signiﬁcantly avoids\nrepetition of words from both vocabulary and OCR tokens,\nand generates more ﬂuent captions. While our model can\ndetect multiple OCR tokens in the image, it is not robust\nenough to combine these tokens correctly, as shown in Fig-\nure 4 (d) where our model puts the tokenﬁeld in a wrong po-\nsition. In Figure 4 (h), our model fails to inferdeadline from\ntext before Oct. 1. As it requires more than simply reading,\nreasoning based on text remains a tough issue of predicting\ncaptions on the TextCaps dataset.\nConclusion\nIn this paper we introduce CNMT, a novel model for\nTextCaps task. It consists of three modules: Reading mod-\nule which extracts text and recognition conﬁdence, Rea-\nsoning Module which fuses object features with OCR to-\nken features, and Generation Module which predicts cap-\ntions based on output of Reading module. With recogni-\ntion conﬁdence embedding of OCR tokens and better OCR\nsystems, our model has stronger reading ability compared\nwith previous models. We also employ a repetition mask to\navoid redundancy in predicted captions. Experiments sug-\ngest that our model signiﬁcantly outperforms current state-\nof-the-art model of TextCaps dataset by a large margin. We\nalso present a qualitative analysis of our model. Further re-\nsearch on avoiding repetition may include making the model\nlearn by itself with reinforcement learning approach. As for\nsemantic signiﬁcance of OCR tokens, other features besides\nrecognition conﬁdence can be explored. We leave these as\nfuture work.\nAcknowledgments\nThis work was partially supported by the National Natu-\nral Science Foundation of China (Grant 61876177), Beijing\nNatural Science Foundation (Grant 4202034), Fundamental\nResearch Funds for the Central Universities and Zhejiang\nLab (No. 2019KD0AB04).\n2841\nReferences\nAlmaz´an, J.; Gordo, A.; Forn ´es, A.; and Valveny, E. 2014.\nWord spotting and recognition with embedded attributes.\nIEEE transactions on pattern analysis and machine intel-\nligence 36(12): 2552–2566.\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. Spice: Semantic propositional image caption evalu-\nation. In European Conference on Computer Vision , 382–\n398. Springer.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 6077–6086.\nBaek, J.; Kim, G.; Lee, J.; Park, S.; Han, D.; Yun, S.; Oh,\nS. J.; and Lee, H. 2019a. What is wrong with scene text\nrecognition model comparisons? dataset and model analy-\nsis. In Proceedings of the IEEE International Conference\non Computer Vision, 4715–4723.\nBaek, Y .; Lee, B.; Han, D.; Yun, S.; and Lee, H. 2019b.\nCharacter region awareness for text detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 9365–9374.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, 65–72.\nBojanowski, P.; Grave, E.; Joulin, A.; and Mikolov, T. 2017.\nEnriching word vectors with subword information. Trans-\nactions of the Association for Computational Linguistics 5:\n135–146.\nBorisyuk, F.; Gordo, A.; and Sivakumar, V . 2018. Rosetta:\nLarge scale system for text detection and recognition in im-\nages. In Proceedings of the 24th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data Mining,\n71–79.\nChen, X.; Fang, H.; Lin, T.-Y .; Vedantam, R.; Gupta, S.;\nDoll´ar, P.; and Zitnick, C. L. 2015. Microsoft coco cap-\ntions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325 .\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805 .\nGao, C.; Chen, Y .; Liu, S.; Tan, Z.; and Yan, S. 2020. Ad-\nversarialnas: Adversarial neural architecture search for gans.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 5680–5689.\nHu, R.; Singh, A.; Darrell, T.; and Rohrbach, M. 2020. Iter-\native answer prediction with pointer-augmented multimodal\ntransformers for textvqa. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n9992–10002.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X.-Y . 2019. At-\ntention on attention for image captioning. In Proceedings\nof the IEEE International Conference on Computer Vision ,\n4634–4643.\nLiao, M.; Shi, B.; and Bai, X. 2018. Textboxes++: A single-\nshot oriented scene text detector. IEEE transactions on im-\nage processing 27(8): 3676–3690.\nLiao, Y .; Liu, S.; Li, G.; Wang, F.; Chen, Y .; Qian, C.; and Li,\nB. 2020. A Real-Time Cross-Modality Correlation Filtering\nMethod for Referring Expression Comprehension. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR).\nLin, C.-Y . 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74–81.\nLiu, Y .; Chen, H.; Shen, C.; He, T.; Jin, L.; and Wang,\nL. 2020. ABCNet: Real-time Scene Text Spotting with\nAdaptive Bezier-Curve Network. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9809–9818.\nLong, S.; Ruan, J.; Zhang, W.; He, X.; Wu, W.; and Yao,\nC. 2018. Textsnake: A ﬂexible representation for detecting\ntext of arbitrary shapes. In Proceedings of the European\nconference on computer vision (ECCV), 20–36.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: a method for automatic evaluation of machine trans-\nlation. In Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311–318.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn:\nTowards real-time object detection with region proposal net-\nworks. In Advances in neural information processing sys-\ntems, 91–99.\nShi, B.; Bai, X.; and Yao, C. 2016. An end-to-end trainable\nneural network for image-based sequence recognition and\nits application to scene text recognition. IEEE transactions\non pattern analysis and machine intelligence 39(11): 2298–\n2304.\nShi, B.; Wang, X.; Lyu, P.; Yao, C.; and Bai, X. 2016. Ro-\nbust scene text recognition with automatic rectiﬁcation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 4168–4176.\nSidorov, O.; Hu, R.; Rohrbach, M.; and Singh, A. 2020.\nTextCaps: a Dataset for Image Captioningwith Reading\nComprehension .\nSingh, A.; Natarjan, V .; Shah, M.; Jiang, Y .; Chen, X.;\nParikh, D.; and Rohrbach, M. 2019. Towards VQA Mod-\nels That Can Read. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 8317–8326.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nVedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.\nCider: Consensus-based image description evaluation. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 4566–4575.\n2842\nVinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer net-\nworks. In Advances in neural information processing sys-\ntems, 2692–2700.\nYoung, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.\nFrom image descriptions to visual denotations: New simi-\nlarity metrics for semantic inference over event descriptions.\nTransactions of the Association for Computational Linguis-\ntics 2: 67–78.\nZheng, Z.; Wang, W.; Qi, S.; and Zhu, S.-C. 2019. Reason-\ning visual dialogs with structural and partial observations.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 6669–6678.\n2843",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8472119569778442
    },
    {
      "name": "Closed captioning",
      "score": 0.830653190612793
    },
    {
      "name": "Transformer",
      "score": 0.674562931060791
    },
    {
      "name": "Natural language processing",
      "score": 0.5731439590454102
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5436595678329468
    },
    {
      "name": "Repetition (rhetorical device)",
      "score": 0.5321200489997864
    },
    {
      "name": "Optical character recognition",
      "score": 0.5318161249160767
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.5169641375541687
    },
    {
      "name": "Reading (process)",
      "score": 0.4929027557373047
    },
    {
      "name": "Word recognition",
      "score": 0.46018773317337036
    },
    {
      "name": "Speech recognition",
      "score": 0.4591182768344879
    },
    {
      "name": "Word (group theory)",
      "score": 0.41919443011283875
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2623897194862366
    },
    {
      "name": "Linguistics",
      "score": 0.1470455825328827
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}