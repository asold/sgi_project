{
  "title": "Large Language Models Meet NL2Code: A Survey",
  "url": "https://openalex.org/W4385572142",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5021771488",
      "name": "Daoguang Zan",
      "affiliations": [
        "Institute of Software",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5114950103",
      "name": "Bei Chen",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5085902267",
      "name": "Fengji Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5025153376",
      "name": "Dianjie Lu",
      "affiliations": [
        "Shandong Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5012208597",
      "name": "Bingchao Wu",
      "affiliations": [
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A5111019883",
      "name": "Bei Guan",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A5064567218",
      "name": "Yongji Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A5025118710",
      "name": "Jian–Guang Lou",
      "affiliations": [
        "Microsoft Research Asia (China)",
        "University of Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224060952",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2970004377",
    "https://openalex.org/W4312091028",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3005855585",
    "https://openalex.org/W3161997752",
    "https://openalex.org/W3170092793",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W1973831620",
    "https://openalex.org/W4282830668",
    "https://openalex.org/W3006127095",
    "https://openalex.org/W4297819591",
    "https://openalex.org/W2963617989",
    "https://openalex.org/W3161457214",
    "https://openalex.org/W4289792856",
    "https://openalex.org/W4282968607",
    "https://openalex.org/W4308643152",
    "https://openalex.org/W4309804142",
    "https://openalex.org/W3126675481",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W4361866100",
    "https://openalex.org/W4362679230",
    "https://openalex.org/W3183541766",
    "https://openalex.org/W4283210053",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2135341757",
    "https://openalex.org/W4308627645",
    "https://openalex.org/W4226305955",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4300989059",
    "https://openalex.org/W4385571167",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4389519979",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4385574207",
    "https://openalex.org/W4309986599",
    "https://openalex.org/W4382239980",
    "https://openalex.org/W3003678146",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1994573369",
    "https://openalex.org/W4285069927",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4226485558",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W2616318524",
    "https://openalex.org/W4323033814",
    "https://openalex.org/W2759781493",
    "https://openalex.org/W4291476001",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4307479317",
    "https://openalex.org/W4306881916",
    "https://openalex.org/W4288804596",
    "https://openalex.org/W3188227658",
    "https://openalex.org/W4309428217",
    "https://openalex.org/W4302012631",
    "https://openalex.org/W4315706637",
    "https://openalex.org/W4313212241",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2134734244",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4284664028",
    "https://openalex.org/W2963935794",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4298186479",
    "https://openalex.org/W4312438588",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W4226053975",
    "https://openalex.org/W4394638297",
    "https://openalex.org/W4221166113",
    "https://openalex.org/W2601394446",
    "https://openalex.org/W2122845366",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4386576744",
    "https://openalex.org/W4283799640",
    "https://openalex.org/W2116272605",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4306179830",
    "https://openalex.org/W2963868406",
    "https://openalex.org/W2964315653",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385571808"
  ],
  "abstract": "Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, Jian-Guang Lou. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7443–7464\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models Meet NL2Code: A Survey\nDaoguang Zan1,2∗, Bei Chen3, Fengji Zhang3, Dianjie Lu4, Bingchao Wu1,\nBei Guan5, Yongji Wang5, Jian-Guang Lou3\n1Cooperative Innovation Center, Institute of Software, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences;\n3Microsoft Research Asia; 4Shandong Normal University\n5Integrative Innovation Center, Institute of Software, Chinese Academy of Sciences\n{daoguang@, bingchao2017@, guanbei@, ywang@itechs.}iscas.ac.cn;\n{beichen, v-fengjzhang, jlou}@microsoft.com; Ludianjie@sdnu.edu.cn\nAbstract\nThe task of generating code from a natural lan-\nguage description, or NL2Code, is considered\na pressing and significant challenge in code in-\ntelligence. Thanks to the rapid development\nof pre-training techniques, surging large lan-\nguage models are being proposed for code,\nsparking the advances in NL2Code. To facil-\nitate further research and applications in this\nfield, in this paper, we present a comprehen-\nsive survey of 27 existing large language mod-\nels for NL2Code, and also review benchmarks\nand metrics. We provide an intuitive compari-\nson of all existing models on the HumanEval\nbenchmark. Through in-depth observation and\nanalysis, we provide some insights and con-\nclude that the key factors contributing to the\nsuccess of large language models for NL2Code\nare \"Large Size, Premium Data, Expert Tun-\ning\". In addition, we discuss challenges and\nopportunities regarding the gap between mod-\nels and humans. We also create a website\nhttps://nl2code.github.io to track the lat-\nest progress through crowd-sourcing. To the\nbest of our knowledge, this is the first survey of\nlarge language models for NL2Code, and we\nbelieve it will contribute to the ongoing devel-\nopment of the field.\n1 Introduction\nIs it possible for novice programmers, even those\nwithout any programming experience, to create\nsoftware simply by describing their requirements\nin natural language? This is a long-standing fasci-\nnating question, which poses challenges to research\nareas like software engineering, programming lan-\nguage, and artificial intelligence. Realizing this\nscenario would have an unprecedented impact on\nour lives, education, economy, and labour mar-\nket, as it would change the centralized software\ndevelopment and operation paradigm. Due to its\n∗ This work was done before October 2022 when the\nauthor, Daoguang Zan, was an intern at Microsoft Research\nAsia.\npromising and intriguing future, natural-language-\nto-code (NL2Code) has been proposed as a re-\nsearch task that has attracted widespread interest in\nboth academia and industry, with the goal of gener-\nating code from natural language descriptions.\nEarly studies on NL2Code were mainly based on\nheuristic rules or expert systems, such as probabilis-\ntic grammar-based methods (Joshi and Rambow,\n2003; Cohn et al., 2010; Allamanis and Sutton,\n2014) and those focusing on domain-specific lan-\nguages (de Moura and Bjørner, 2008; Gulwani,\n2010; Jha et al., 2010), which are inflexible and\nnot scalable. Other studies utilized static lan-\nguage models, like n-gram (Nguyen et al., 2013;\nRaychev et al., 2014; Devanbu, 2012) and Hid-\nden Markov (Sutskever et al., 2008), which have\nsparse vector representations and cannot model\nlong-term dependencies. Subsequently, neural net-\nworks, including CNN (Liu et al., 2016; Sun et al.,\n2018), RNN (Iyer et al., 2016; Wan et al., 2018),\nand LSTM (Eriguchi et al., 2016; Yin and Neu-\nbig, 2017), were employed to model the relation-\nship between NL and code. In 2017, the Trans-\nformer (Vaswani et al., 2017) model was intro-\nduced for machine translation and later applied to\nthe NL2Code task (Mastropaolo et al., 2021; Shah\net al., 2021). However, these deep learning models\nrequire a significant amount of labelled pairs of NL\nand code for training, and have limited capabilities\nfor the NL2Code task.\nRecently, a growing number of large language\nmodels (LLMs) with Transformer architecture have\nbeen trained on large-scale unlabelled code cor-\npus. These models have the ability to generate\ncode in a zero-shot manner and have achieved\nimpressive results in the NL2Code task. As a\nmilestone, Codex (Chen et al., 2021) has shown\nthat an LLM with 12 billion parameters is able\nto solve 72.31% of challenging Python program-\nming problems created by humans. More encourag-\ningly, Codex has been used to power a commercial\n7443\nProblemDescription\nCodeSolutionTestCases\nfromcollections importCounterdefMostCommon(lst):'''Find the most common element fromlst.'''data = Counter(lst)returndata.most_common(1)[0][0]defcheck():assertMostCommon([1, 2, 1]) == 1assertMostCommon([4, 0, 0]) == 0...\nFigure 1: A simple example of the NL2Code task. The\ncode blocks marked in grey, green, and yellow represent\nthe natural language problem description, the predicted\ncode solution, and the test cases, respectively.\nproduct1 and improve coding efficiency in prac-\ntice (Sobania et al., 2022a; Barke et al., 2023).\nFollowing Codex’s success, various LLMs for the\nNL2Code task have emerged, with model sizes\nranging from millions to billions of parameters. Ex-\namples include AlphaCode (Li et al., 2022b), which\naims to solve competitive-level programming prob-\nlems, and InCoder (Fried et al., 2023), which sup-\nports filling code in arbitrary positions using bidi-\nrectional contexts. Other models such as Code-\nGen (Nijkamp et al., 2023), PaLM-Coder (Chowd-\nhery et al., 2022), PanGu-Coder (Christopoulou\net al., 2022), CodeGeeX (Zheng et al., 2023), and\nSantaCoder (Allal et al., 2023) have also gained\ngreat attention. As the model size increases, LLMs\nhave been shown to exhibit some emergent capa-\nbilities such as human-like programming and de-\nbugging (Zhang et al., 2022; Saunders et al., 2022;\nKang et al., 2023).\nLarge language models have kindled hope for the\nNL2Code task due to their impressive power and\npotential value. Despite the significant progress,\nthere are still numerous challenges and opportu-\nnities, calling for more advanced and innovative\nfuture work. Currently, considering the variety\nof techniques and applications, there is a grow-\ning need for a comprehensive survey to provide\na systematic overview of this field and identify\ncritical challenges. To this end, in this paper,\nwe carefully investigate 27 advanced LLMs for\nNL2Code (§2), and also review benchmarks and\nmetrics (§4). We conduct an intuitive comparison\nof all the existing LLMs on the HumanEval bench-\nmark, perform a thorough analysis, and eventu-\nally attribute the success of these LLMs to \"Large\nSize, Premium Data, Expert Tuning \" (§3). This\n1https://github.com/features/copilot\nModel Size L. A. H. P.\nDecoder\nGPT-C (2020) 366M 24 16 1 ,024 ×\nCodeGPT (2021) 124M 12 12 768 ✓\nGPT-Neo (2021) 125M~2.7B 32 20 2 ,560 ✓\nGPT-J (2021) 6B 28 16 4 ,096 ✓\nCodex (2021) 12M~12B 40 40 5 ,140 ×\nGPT-CC (2021) 125M~1.3B 24 16 2 ,048 ✓\nCodeParrot (2021)110M~1.5B 48 25 1 ,600 ✓\nLaMDA (2022) 2B~137B 64 128 8,192 ×\nPolyCoder (2022)160M~2.7B 32 32 2 ,560 ✓\nCodeGen (2023) 350M~16.1B 34 24 6 ,144 ✓\nInCoder (2023) 1.3B~6.7B 32 32 4 ,096 ✓\nGPT-NeoX (2022)20B 44 64 6 ,144 ✓\nPaLM-Coder (2022)8B~540B 118 48 18,432 ×\nPanGu-Coder (2022)317M~2.6B 32 32 2 ,560 ×\nFIM (2022) 50M~6.9B 32 32 4 ,096 ×\nPyCodeGPT (2022b)110M 12 12 768 ✓\nCodeGeeX (2023)13B 39 40 5 ,120 ✓\nBLOOM (2022) 560M~176B 70 112 14,336 ✓\nSantaCoder (2023)1.1B 24 16 2 ,048 ✓\nEncoder-Decoder\nPyMT5 (2020) 374M 12 16 1 ,472 ×\nPLBART (2021) 140M~406M 24 16 1 ,024 ✓\nCodeT5 (2021) 60M~770M 48 16 1 ,024 ✓\nJuPyT5 (2022a) 350M 12 16 1 ,472 ×\nAlphaCode (2022b)284M~41.1B 64 128 6,144 ×\nCodeRL (2022) 770M 48 16 1 ,024 ✓\nCodeT5Mix (2022)220M~770M 48 16 1 ,024 ✓\nERNIE-Code (2022)560M 24 12 768 ✓\nTable 1: Summary of 27 existing LLMs for NL2Code.\nWe show L. (number of layers), A. (number of atten-\ntion heads), H. (hidden dimensions), and P. (model\nweights public or not) for the largest size version of\neach model. Note that some models, such as GPT-Neo,\nGPT-J, LaMDA, GPT-NeoX, FIM, and BLOOM, are\nnot exclusively trained for code.\nmeans large model and data size, high-quality train-\ning data and expert hyper-parameter tuning. We\nalso discuss the challenges and opportunities re-\ngarding the ability gap between LLMs and Hu-\nmans (§5). In addition, we have built a web-\nsite https://nl2code.github.io to keep track\nof the latest progress and support crowd-sourcing\nupdates. To the best of our knowledge, this is the\nfirst survey of LLMs for NL2Code2, and we hope\nit will contribute to the ongoing development of\nthis exciting field.\n2 Large Language Models for NL2Code\nGiven a natural language problem description, the\nNL2Code task aims to automatically generate the\ndemanded code. To illustrate this task visually,\nwe provide a Python programming problem as an\nexample in Figure 1, while different NL2Code\nbenchmarks may vary in terms of language or\n2We summarize the related surveys in Appendix A.\n7444\nBLOOM560M~176B\nMayOct.Feb.Mar.MayJul.Sep.Oct.Nov.Jan.Feb.Mar.Apr.Jun.Jul.Sep.Dec.2020 2021 2022\nGPT-C366M\nPyMT5374M\nCodeGPT124M\nGPT-Neo125M~2.7B\nPLBART140M~406M\nGPT-J6BCodex12M~12B\nCodeT560M~770M\nGPT-CC125M~1.3B\nCodeParrot110M~1.5B\nJuPyT5350M\nLaMDA2B~137B\nAlphaCode284M~41.1B\nPolyCoder160M~2.7B\nCodeGen350M~16.1B\nGPT-NeoX20B\nInCoder1.3B~6.7B\nPaLM-Coder8B~540B\nPanGu-Coder317M~2.6BCodeRL770M\nFIM50M~6.9B\nPyCodeGPT110M\nCodeGeeX13B\nERNIE-Code560M\nSantaCoder1.1B\n100M\n1B\n10B\n100B\nCodeT5Mix200M~770M\nFigure 2: The timeline of LLMs for NL2Code, with only the largest model sizes plotted for visual clarity.\nproblem domain. Existing large language models\nfor the NL2Code task are usually based on Trans-\nformer (Vaswani et al., 2017) and are trained on\na large-scale code related unlabelled corpus. For\nbetter code generation performance, most LLMs,\nno matter encoder-decoder or decoder-only models,\nemploy the causal language modeling objective for\ntraining, which is to predict the token following a\nsequence of tokens. During inference, an LLM can\ntackle NL2Code problems in a zero-shot manner\nwithout fine-tuning its parameters. There are also\nstudies employing few-shot (Austin et al., 2021) or\nin-context learning (Nijkamp et al., 2023) to further\nboost the performance.\nWe conduct a comprehensive investigation of27\nrepresentative LLMs for the NL2Code task. De-\ntails of each model are summarized in Table 1,\nwhere models vary in architecture, size, and acces-\nsibility. For better visualization, we present these\nmodels in chronological order in Figure 2, plot-\nting the largest model sizes. One trend observed is\nthat these large language models are consistently\ngrowing in size as the research field advances. Ad-\nditionally, the decoder-only architecture is favoured\nfor pre-trained models with larger sizes.\nEarly works, such as GPT-C (Svyatkovskiy\net al., 2020), PyMT5 (Clement et al., 2020), and\nPLBART (Ahmad et al., 2021), have relatively\nsmall numbers of parameters and do not demon-\nstrate strong capabilities in zero-shot code genera-\ntion. Conversely, large-scale models such as GPT-\nNeo (Black et al., 2021) and GPT-J (Wang and Ko-\nmatsuzaki, 2021), despite their billion-level param-\neter scale, have been found to have limited power\nin the NL2Code task due to the small amount of\ncode in their training corpus. Recently, a number of\npowerful LLMs have been proposed for NL2Code,\nsuch as Codex (Chen et al., 2021), AlphaCode (Li\net al., 2022b), and PaLM-Coder (Chowdhery et al.,\n2022), which possess massive parameter scales and\nhigh-quality training corpus with code. While they\nshow surprisingly good performance on NL2Code,\nmost of them are not readily accessible. At\npresent, a number of excellent open-source mod-\nels have also been proposed, including CodePar-\nrot (Huggingface, 2021), PolyCoder (Xu et al.,\n2022), GPT-NeoX (Black et al., 2022), and San-\ntaCoder (Allal et al., 2023), which contribute to\nthe thriving of LLMs for NL2Code. Besides, re-\ncent studies have proposed various approaches to\naddress specific NL2Code scenarios. For exam-\nple, JuPyT5 (Chandel et al., 2022a) is designed\nto work within Jupyter Notebooks, while ERNIE-\nCode (Chai et al., 2022), CodeGeeX (Zheng et al.,\n2023), and BLOOM (Scao et al., 2022) are trained\nto support multiple natural or programming lan-\nguages. Additionally, InCoder (Fried et al., 2023),\nFIM (Bavarian et al., 2022), and SantaCoder (Al-\nlal et al., 2023) not only support left-to-right code\nprediction, but also allow for infilling arbitrary re-\ngions of code. As LLMs for NL2Code are evolving\nrapidly, we created a website to keep up-to-date\n7445\nwith the latest advances by crowd-sourcing. De-\ntails of the website can be found in Appendix B.\nThese models are not only attractive in\nacademia (Chen et al., 2021; Nijkamp et al., 2023;\nLi et al., 2022b), but also applied in real-world prod-\nucts to improve programming efficiency (Sobania\net al., 2022a; Barke et al., 2023). One example\nis GitHub and OpenAI’s Copilot, a programming\nassistance tool that utilizes Codex to provide real-\ntime code suggestions. Other notable products in-\nclude CodeGeeX3 and CodeWhisperer4. A sum-\nmary of 10 products can be found in Appendix Ta-\nble 5. Recent studies (Sobania et al., 2022b; Pearce\net al., 2022; Nguyen and Nadi, 2022) have shown\nthat these products can provide helpful recommen-\ndations, while they also introduce minor bugs that\ncan cause issues for users. There is still room for\nimprovement before LLMs can be fully practical\nand capable of coding like humans.\n3 What makes LLMs successful?\nWe have summarized the existing large language\nmodels for NL2Code. These LLMs vary in terms of\narchitecture, size, and other characteristics, making\nit difficult to establish a completely fair comparison.\nWe evaluate these LLMs on the HumanEval bench-\nmark (Chen et al., 2021) in a zero-shot manner to\nprovide an intuitive comparison. HumanEval, pro-\nposed along with Codex, is one of the most popular\nbenchmarks for the NL2Code task and consists of\n164 hand-written Python programming problems.\nTest cases are provided for each programming prob-\nlem to evaluate the correctness of generated code.\npass@k is used as the evaluation metric 5, which\ncalculates the proportion of problems that can be\ncorrectly answered with k tries. Table 2 shows the\nresults of different LLMs organized by the model\nsize. Implementation details and the evaluation on\nthe MBPP benchmark (Austin et al., 2021) can be\nfound in Appendix C.2.\nIt can be observed from Table 2 that the per-\nformance of existing LLMs varies widely on Hu-\nmanEval, even for those with similar model sizes.\nSpecifically, Codex (Chen et al., 2021) holds the\nleading position in various model sizes, while a\nrelatively small model, PyCodeGPT 110M (Zan\net al., 2022b), achieves comparable results to\nCodex 85M. Other larger models such as Alpha-\n3https://keg.cs.tsinghua.edu.cn/codegeex\n4https://aws.amazon.com/cn/codewhisperer\n5The details of pass@k can be found in Appendix C.1.\nModel Size pass@k (%)\nk=1 k=10 k=100\nModel Size: ~100M\nGPT-Neo 125M 0.75 1 .88 2 .97\nCodeParrot 110M 3.80 6 .57 12 .78\nPyCodeGPT 110M 8.33 13 .36 19.13\nPolyCoder 160M 2.13 3 .35 4 .88\nCodex 12M 2.00 3 .62 8 .58\nCodex 25M 3.21 7 .1 12 .89\nCodex 42M 5.06 8 .8 15 .55\nCodex 85M 8.22 12 .81 22.40\nAlphaCode(dec) 13M 1.5 3 .6 8 .6\nAlphaCode(dec) 29M 3.4 5 .8 11 .2\nAlphaCode(dec) 55M 4.2 8 .2 16 .9\nAlphaCode(dec) 89M 4.3 12 .2 20 .0\nModel Size: ~500M\nCodeT5† 770M 12.09 19 .24 30 .93\nPolyCoder 400M 2.96 5 .29 11 .59\nJuPyT5 300M 5.40 15 .46 25 .60\nBLOOM 560M 0.82 3 .02 5 .91\nCodex 300M 13.17 20 .37 36 .27\nCodex 679M 16.22 25.70 40.95\nAlphaCode(dec) 302M 11.6 18 .8 31 .8\nAlphaCode(dec) 685M 14.2 24 .4 38 .8\nCodeGen-Mono 350M 12.76 23 .11 35 .19\nPanGu-Coder 317M 17.07 24.05 34 .55\nModel Size: ~1B\nGPT-Neo 1.3B 4.79 7 .47 16 .30\nCodeParrot 1.5B 3.99 8 .69 17 .88\nBLOOM 1.1B 2.48 5 .93 9 .62\nBLOOM 1.7B 4.03 7 .45 12 .75\nInCoder† 1.3B 11.09 16 .14 24 .20\nAlphaCode(dec) 1.1B 17.1 28 .2 45 .3\nSantaCoder 1.1B 18 29 49\nModel Size: ~5B\nGPT-Neo 2.7B 6.41 11 .27 21 .37\nPolyCoder 2.7B 5.59 9 .84 17 .68\nCodex 2.5B 21.36 35 .42 59 .50\nPanGu-Coder 2.6B 23.78 35 .36 51 .24\nBLOOM 3B 6.48 11 .35 20 .43\nBLOOM 7.1B 7.73 17 .38 29 .47\nCodeGen-Mono 2.7B 23.70 36 .64 57 .01\nCodeGen-Mono 6.1B 26.13 42.29 65.82\nGPT-J 6B 11.62 15 .74 27 .74\nInCoder 6.7B 15.2 27 .8 47 .0\nModel Size: >10B\nCodex 12B 28.81 46 .81 72 .31\nCodeGen-Mono 16.1B 29.28 49 .86 75 .00\nGPT-NeoX 20B 15.4 25 .6 41 .2\nLaMDA 137B 14.0 − 47.3\nBLOOM 176B 15.52 32 .20 55 .45\nPaLM-Coder 540B 36.0 − 88.4\ncode-cushman-001− 33.5 54 .3 77 .4\ncode-davinci-001− 39.0 60 .6 84 .1\ncode-davinci-002− 47.0 74 .9 92 .1\nTable 2: Performance on the HumanEval benchmark. †\ndenotes our reproduced results, while others are cited\nfrom the original papers. AlphaCode(dec) means the\ndecoder-only version. We also compare the Codex mod-\nels (code-cushman and code-davinci) provided by Ope-\nnAI API. We exclude the models that cannot pass any\nproblem in the benchmark.\n7446\n0 5 10 15\nNumber of Parameters (billion)\n0\n5\n10\n15\n20\n25\n30pass@1 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodex\nAlphaCode(dec)\nCodeGen-Mono\nPanGu-Coder\nInCoder\nBLOOM\n(a)\n0 5 10 15\nNumber of Parameters (billion)\n5\n10\n15\n20\n25\n30\n35\n40\n45Syntax Error Rate (%)\nGPT-Neo\nCodeParrot\nPolyCoder\nCodeGen-Mono\nInCoder\nBLOOM (b)\nFigure 3: (a) pass@1 and (b) syntax error rates on the HumanEval benchmark with various model sizes.\nCode (Li et al., 2022b), CodeGen-Mono (Nijkamp\net al., 2023), and PanGu-Coder (Christopoulou\net al., 2022) also exhibit impressive performance.\nNotably, InCoder (Fried et al., 2023) and Santa-\nCoder (Allal et al., 2023), which use the FIM train-\ning method (Bavarian et al., 2022), also obtain re-\nmarkably decent results in the left-to-right gener-\nation setting. The significant variation in perfor-\nmance leads us to the question: What makes LLMs\nsuccessful in NL2Code? Given the diversity of\nthese models in terms of design choices, we per-\nform a thorough analysis and conclude the answer:\nLarge Size, Premium Data, Expert Tuning. That\nis, large model and data size, high-quality data and\nexpert hyper-parameter tuning are the key factors\nfor the success of LLMs in the NL2Code task. In\nthis section, we detail our observations and insights\nfrom the perspectives of model, data and tuning.\n3.1 Large Model Size\nAs shown in Figure 2 and Table 2, recent LLMs\nfor NL2Code exhibit larger sizes and superior per-\nformance. This is consistent with prior findings\nthat an increased number of model parameters can\nenhance model capabilities (Radford et al., 2019;\nThoppilan et al., 2022; Chowdhery et al., 2022).\nWe further demonstrate the correlation between\nmodel size and performance in Figure 3a, which\ncompares the pass@1 results of 10 representative\nmodels on the HumanEval benchmark. It is clear\nthat larger models generally result in better per-\nformance. Furthermore, we also find that current\nmodels, regardless of size, still have the potential\nfor improvement through further increases in size.\nAdditional results on the HumanEval and MBPP\nbenchmarks can be found in Appendix Figure 7,\nwhich also support this conclusion.\nAdditionally, we conduct an experiment on the\nHumanEval benchmark to examine the syntax er-\nror rates of the code generated by different models\nof varying sizes. Specifically, we make the mod-\nels predict 10 code samples for each programming\nproblem, and then calculate the percentage of code\nsamples that have syntax errors. As shown in Fig-\nure 3b, results indicate that larger models tend to\nhave lower syntax error rates. It is noteworthy that\nthe largest version of the CodeGen-Mono model\nexhibits a remarkably low rate of syntax errors, i.e.,\n6%. However, as evidenced by Figure 3a and Ta-\nble 2, the CodeGen-Mono model with 16 billion\nparameters still has unsatisfactory performance in\nterms of pass@k , e.g., pass@1 to be 29%. This\nhighlights the fact that the current limitation for\nlarge pre-trained models is the generation of se-\nmantically correct code.\n3.2 Large and Premium Data\nAs the sizes of LLMs increase in the field of\nNL2Code, the scale of the corpus used for train-\ning also increases. This highlights the importance\nof selecting and pre-processing high-quality data.\nIn this section, we will discuss various commonly\nused data sources and pre-processing strategies that\nare essential for training LLMs.\nEarly models were trained using manually an-\nnotated data pairs of NL and code, and the data\nsources include CodeSearchNet (Husain et al.,\n2019), CoST (Zhu et al., 2022b), and XL-\nCoST (Zhu et al., 2022a). However, manual an-\nnotation is labour-intensive and time-consuming.\nThere are also models like GPT-3 (Brown et al.,\n2020), GPT-Neo (Black et al., 2021), and GPT-\n7447\nJ (Wang and Komatsuzaki, 2021) that are trained\non the Pile (Gao et al., 2020), a large-scale unsuper-\nvised dataset. However, these models have not yet\ndemonstrated exceptional code generation capabili-\nties due to the limited number of code files in the\ntraining corpus. More recently, with the emergence\nof more powerful LLMs for NL2Code, larger-scale\nunlabelled code datasets have been proposed, in-\ncluding BigQuery (Google, 2016), CodeParrot’s\ncorpus (HuggingFace, 2021a), GitHub-Code (Hug-\ngingFace, 2021b), and the Stack (HuggingFace,\n2022), which are collected from general domain\nopen-source websites like GitHub6 and Stack Over-\nflow7. Furthermore, there are also specialized\ndatasets proposed for different scenarios, for exam-\nple, using Jupyter Notebooks or competition pro-\ngramming problems as a training corpus. Released\ndatasets include Jupyter (HuggingFace, 2021c),\nJuICe (Agashe et al., 2019), APPS (Hendrycks\net al., 2021), and CodeNet (IBM, 2021).\nIn order to ensure the quality of the training cor-\npus, it is common for LLMs to perform data pre-\nprocessing on the significant amount of code in\nthe collected data. We carefully review the data\npre-processing methods of five powerful LLMs, in-\ncluding Codex (Chen et al., 2021), AlphaCode (Li\net al., 2022b), CodeGen (Nijkamp et al., 2023), In-\nCoder (Fried et al., 2023), and PyCodeGPT (Zan\net al., 2022b), and identify several commonalities.\nOne is the removal of likely auto-generated or un-\nfinished code files, as they are deemed to be mean-\ningless. Additionally, specific rules are employed\nto filter out uncommon code files. These rules in-\nclude factors such as the repository star rating, the\nfile size, the line length, and the alphanumeric rate.\nIn summary, the goal of these pre-processing strate-\ngies is to achieve a code corpus that is unduplicated,\ncomplete, correct, clean, and general in nature.\n3.3 Expert Tuning\nTraining an excellent model requires careful con-\nsideration of various design choices and hyper-\nparameters. After reviewing the existing 27 LLMs\n(summary in Appendix Table 6), we have the\nfollowing findings. Firstly, these LLMs share\nsome common settings. For example, we ob-\nserve that the optimizer of the current models is\nalmost all Adam (Kingma and Ba, 2014) or its\nvariants (Loshchilov and Hutter, 2017). We also\n6https://github.com\n7https://stackoverflow.com\n0 5 10 15\nNumber of Parameters (billion)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50Learning Rate (LR)\n1e 3\nCodex\nCodeGen-NL\nCodeGen-Mono\nFIM\nPyCodeGPT\nSantaCoder\nFigure 4: Learning rate of six advanced LLMs in terms\nof various model sizes.\n0.2 0.3 0.4 0.5 0.6 0.7 0.8\nT emperature\n10\n20\n30\n40\n50Pass@k (%)\nCodeGen-Mono 2.7B pass@1\nCodeGen-Mono 2.7B pass@10\nCodeGen-Mono 2.7B pass@100\nInCoder 1.3B pass@1\nInCoder 1.3B pass@10\nInCoder 1.3B pass@100\nFigure 5: pass@k on the HumanEval benchmark with\ndifferent temperatures during model inference.\nfind that initializing with other natural language\nmodels yields no noticeable gain compared to train-\ning from scratch, except for accelerating conver-\ngence (Chen et al., 2021). Furthermore, there are\nseveral hyper-parameters that require expert tun-\ning, such as learning rate, batch size, window size,\nwarmup steps, gradient accumulation steps, and\nsampling temperature. For the learning rate, we\nanalyze its correlation with model size using six\npowerful LLMs, as shown in Figure 4. We ob-\nserve that the learning rate becomes smaller as the\nmodel gets larger. To explore the effects of temper-\nature, in Figure 5, we report the performance of two\nmodels using multiple temperatures on HumanEval.\nOne observation is that higher temperature leads\nto lower pass@1 and higher pass@100, which sug-\ngests that a higher temperature makes LLMs gen-\nerate more diverse predictions and vice versa. Be-\nsides, some studies (erman Arsenovich Arutyunov\nand Avdoshin, 2022) have shown that window size\nis a key factor. An interesting finding is that the\n7448\nBenchmark Num. P. NL S. PL Data Statistics ScenarioT.N. P .C. P .L. S.C. S.L.\nHumanEval (2021) 164 English Python 7.8 450 .6 13 .7 180.9 6 .8 Code Exercise\nMBPP (2021) 974 English Python 3.1 78 .6 1 .0 181 .1 6 .7 Code Exercise\nAPPS (2021) 5,000 English Python 21.0 1743 .4 41.6 473.8 21 .4 Competitions\nCodeContests (2022b) 165 English Multi. 203.7 1989.2 66.4 2239.3 92.1 Competitions\nDS-1000 (2022) 1,000 English Python 1.6 879 .1 31 .6 137.4 5 .0 Data Science\nDSP (2022b) 1,119 English Python 2.1 756 .9 17 .8 226.3 7 .6 Data Science\nMBXP (2022) 974∗ English Multi. 3.1 419 .9 14 .8 − − Multilingual\nMBXP-HumanEval (2022)164∗ English Multi. 7.8 825 .6 30 .0 − − Multilingual\nHumanEval-X (2023) 164∗ English Multi. 7.8 468 .4 15 .5 264.6 12 .1 Multilingual\nMultiPL-HumanEval (2022)164∗ English Multi. 7.8 453 .9 13 .0 − − Multilingual\nMultiPL-MBPP (2022) 974∗ English Multi. 3.1 181 .2 5 .4 − − Multilingual\nPandasEval (2022b) 101 English Python 6.5 244 .5 7 .2 46 .2 1 .3 Public Library\nNumpyEval (2022b) 101 English Python 3.5 222 .9 7 .0 29 .9 1 .1 Public Library\nTorchDataEval (2022a) 50 English Python 1.1 329 .0 8 .6 50 .7 1 .3 Private Library\nMTPB (2023) 115 English Python − 72.7 1 .0 − − Multi-Turn\nODEX (2022c) 945 Multi. Python 1.8 26 .6 2 .0 50 .4 1 .9 Open-Domain\nBIG-Bench (2022) 32 English Python 4.7 341 .8 3 .0 − − Code Exercise\nTable 3: Summary of 17 benchmarks for NL2Code. Num. denotes the number of instances in the benchmark, P.NL\ndenotes Problem description’s Natural Language, S.PL denotes code Solution’s Programming Language, and T.N.\ndenotes the average Number of Test cases. P.C. and P.L. (S.C. and S.L.) stand for the average number ofCharacters\nand Lines in Problem description (code Solution). ∗denotes the number of instances per programming language.\nsmall model with a large window size sometimes\noutperforms the large model with a small window\nsize (details in Appendix D). In addition, power-\nful LLMs usually train a new tokenizer on code\ncorpus primarily using two techniques: Byte-level\nByte-Pair-Encoding (Radford et al., 2019) and Sen-\ntencePiece (Kudo and Richardson, 2018). A new\ntokenizer can be more effective and accurate in\nsplitting code content into tokens. These proven\ntuning techniques will serve as valuable references\nfor training more powerful LLMs.\n4 Benchmarks and Metrics\nTo evaluate the NL2Code task, high-quality bench-\nmarks and reliable metrics are fundamental and es-\nsential. In this section, we provide a brief overview\nof current benchmarks and metrics, as well as our\nobservations and the open challenges.\nWe summarize 17 well-studied NL2Code bench-\nmarks in Table 3, where we can find that each of\nthese benchmarks has its own characteristics re-\ngarding size, language, complexity, and scenario.\nWe observe that most benchmarks contain a limited\nnumber of instances. For example, the widely used\nHumanEval and MBPP have164 and 974 instances,\nrespectively. This is because these benchmarks are\ntypically hand-written to ensure that LLMs have\nnot seen them during training. In the era of large\nlanguage models, it is crucial to avoid data leak-\nage when creating new benchmarks. Additionally,\nmost current benchmarks have their problem de-\nscriptions in English and code solutions in Python.\nRecently, several multi-lingual benchmarks have\nbeen proposed, such as MBXP (Athiwaratkun et al.,\n2022), HumanEvalX (Zheng et al., 2023), and Mul-\ntiPL (Cassano et al., 2022), which cover multi-\nple programming languages, and ODEX (Wang\net al., 2022c), which covers multiple natural lan-\nguages. Details of multi-lingual benchmarks are\nlisted in Appendix Table 7. Furthermore, bench-\nmarks have been proposed for other practical sce-\nnarios, such as data science (Lai et al., 2022), pub-\nlic library (Zan et al., 2022b), private library (Zan\net al., 2022a), multi-turn program synthesis (Ni-\njkamp et al., 2023), and code security (Siddiq and\nmsiddiq, 2022). For execution-based benchmarks,\ncomprehensive test cases with complete coverage\nof the generated program can ensure the trustwor-\nthiness of evaluation results. As a reference, the av-\nerage number of test cases for each benchmark, as\nwell as the length statistics of the problem descrip-\ntions and solutions are also provided in Table 3.\nManually evaluating the generated code is im-\npractical, which calls for the need for automatic\nmetrics. The above mentioned benchmarks all\nprovide test cases for execution-based evaluation,\nwhere metrics such as pass@k (Chen et al.,\n2021), n@k (Li et al., 2022b), test case aver-\n7449\nage (Hendrycks et al., 2021), and execution ac-\ncuracy (Rajkumar et al., 2022) can be used. How-\never, this approach has stringent requirements for\nthe quality of test cases and can only evaluate exe-\ncutable code. For non-executable code, metrics like\nBLEU (Papineni et al., 2002), ROUGE (Lin, 2004),\nand CodeBLEU (Ren et al., 2020) are used, while\nthey can not precisely evaluate the correctness of\nthe code. So far, there are many open challenges\nin designing metrics to evaluate various aspects of\ncode, such as vulnerability, maintainability, clarity,\nexecution complexity, and stability.\n5 Challenges and Opportunities\nOur investigations have revealed that advances in\nLLMs for NL2Code have a considerable impact on\nboth academia and industry. Despite this progress,\nthere are still numerous challenges that need to be\naddressed, offering ample opportunities for further\nresearch and applications. In this section, we ex-\nplore the challenges and opportunities in terms of\nthe ability gap between LLMs and humans.\nUnderstanding Ability The inherent flexibility\nof natural language allows for a variety of expres-\nsions to convey functional requirements. Humans\nare able to understand various descriptions at differ-\nent levels of abstraction. In contrast, current LLMs\ntend to be sensitive to the given context, which may\ncause unexpected performance degradation (Wang\net al., 2022a). In addition, LLMs may struggle\nwhen faced with complex problems that have nu-\nmerous conditions and requirements (Barke et al.,\n2022; Imai, 2022). We believe exploring the un-\nderstanding abilities of LLMs is a crucial research\ndirection. One potential solution is to break down\ncomplex problems into multiple steps, as is com-\nmonly done in reasoning tasks (Wei et al., 2022).\nJudgement Ability Humans have the ability to\ndetermine whether they can solve a programming\nproblem or not. While current models will al-\nways return a solution even if there is no answer\nto the problem, due to the fact that they are trained\nby unsupervised causal language modeling objec-\ntive. This can cause problems in practical applica-\ntions. To improve the judgment ability of LLMs,\nresearchers have employed reinforcement learning\nto leverage user feedback, as seen in models like\nInstructGPT (Ouyang et al., 2022) and ChatGPT8.\nHowever, collecting high-quality feedback for code\n8https://chat.openai.com\nis costly and challenging. There are also ongoing\nstudies (Chen et al., 2023; Key et al., 2022) ex-\nploring the possibility of self-validation for LLMs,\nwhich is also a promising research direction.\nExplanation Ability It is widely acknowledged\nthat human developers possess the ability to inter-\npret the meaning of the code they write, which is\ncrucial for educational purposes and software main-\ntenance. Recent studies showed that LLMs have\nthe potential to automatically generate code expla-\nnations. MacNeil et al. (2022a) proposed using\nLLMs to generate code explanations for students\nduring their learning process, and MacNeil et al.\n(2022b) proposed explaining numerous aspects of a\ngiven code snippet using Copilot. Further research\nand explorations are necessary to fully realize the\npotential of LLMs in this regard.\nAdaptive Learning Ability A fundamental dif-\nference between current large language models and\nhumans is their ability to adapt to new and updated\nknowledge. Human developers possess a unique\nability to quickly search and learn new materials,\nsuch as programming documentation, and adapt\nto changes in APIs with relative ease. However,\nre-training or fine-tuning LLMs requires signifi-\ncant effort and resources. This issue has inspired a\nnumber of recent studies, such as DocCoder (Zhou\net al., 2023) and APICoder (Zan et al., 2022a),\nwhich utilize retrieval-based methods to provide\nextra or updated knowledge during model infer-\nence. Despite these advancements, it remains an\nopen challenge to endow LLMs with the powerful\nlearning capabilities humans possess.\nMulti-tasking Ability Large language models\nhave been applied to a variety of code-related tasks,\nsuch as code repair (Joshi et al., 2022; Prenner and\nRobbes, 2021), code search (Neelakantan et al.,\n2022), and code review (Li et al., 2022c) as well as\nnon-code tasks that can be formatted in a code-like\nmanner, such as mathematics (Drori and Verma,\n2021; Drori et al., 2021) and chemistry (Krenn\net al., 2022; Hocky and White, 2022). However,\nthere are differences between LLMs and human\nabilities in terms of multi-tasking. Humans can\nseamlessly switch between tasks, while LLMs may\nrequire sophisticated prompt engineering (Liu et al.,\n2023). Another evidence is that LLMs lack the\nability to quickly master multiple programming\nlanguages (Zheng et al., 2023) as humans do. These\nlimitations highlight areas for future research.\n7450\n6 Conclusion\nIn this paper, we survey 27 existing large language\nmodels for NL2Code, and draw a thorough analy-\nsis of the underlying reasons for their success. We\nalso provide a detailed review of benchmarks and\nmetrics. Regarding the gap between models and\nhumans, we present ongoing challenges and oppor-\ntunities. In addition, we have developed a website\nto track the latest findings in this field. We hope this\nsurvey can contribute to a comprehensive overview\nof the field and promote its thriving evolution.\nLimitations\nIn this paper, we thoroughly investigate the ex-\nisting large language models for NL2Code, and\nsummarize them from diverse perspectives with\nour own thinking. However, as this field is evolv-\ning so rapidly, there may be aspects that we have\noverlooked, or some new works that we have not\ncovered. To mitigate this issue, we have created a\nwebsite to track the latest progress through crowd-\nsourcing, hoping that it will continually contribute\nto the development of the field. Besides, the exist-\ning LLMs possess their own characteristics in terms\nof model size, architecture, corpus, pre-processing,\ntokenizer, hyper-parameters, and training platforms.\nAlso, some of them are currently not publicly avail-\nable, such as AlphaCode (Li et al., 2022b) and\nPaLM-Coder (Chowdhery et al., 2022). Therefore,\nit is almost impractical to conduct a completely fair\ncomparison. We tried our best to show a kind of\ncomparison on the popular HumanEval and MBPP\nbenchmarks, hoping that it can provide clues to\nthe differences in performance of different LLMs.\nIn addition, evaluating LLMs has a high cost in\ncomputational resources. We thus have made all\nfiles generated by the LLMs publicly available on\nhttps://nl2code.github.io.\nReferences\nRajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer.\n2019. JuICe: A large scale distantly supervised\ndataset for open domain context-based code gener-\nation. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5436–5446.\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2021. Unified pre-training for pro-\ngram understanding and generation. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2655–2668.\naiXcoder. 2018. aiXcoder. https://aixcoder.com.\nAlibaba. 2022. Alibaba. https://github.com/\nalibaba-cloud-toolkit/cosy.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Muñoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlexander Gu, Manan Dey, Logesh Kumar Uma-\npathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier,\nHailey Schoelkopf, Sergey Mikhailovich Troshin,\nDmitry Abulkhanov, Manuel Romero, Michael Franz\nLappert, Francesco De Toni, Bernardo Garc’ia del\nR’io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya,\nTerry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca,\nSourab Mangrulkar, David Lansky, Huu Nguyen,\nDanish Contractor, Luisa Villa, Jia Li, Dzmitry Bah-\ndanau, Yacine Jernite, Sean Christopher Hughes,\nDaniel Fried, Arjun Guha, Harm de Vries, and Le-\nandro von Werra. 2023. SantaCoder: don’t reach for\nthe stars! ArXiv, abs/2301.03988.\nMiltiadis Allamanis, Earl T Barr, Premkumar Devanbu,\nand Charles Sutton. 2018. A survey of machine learn-\ning for big code and naturalness. ACM Computing\nSurveys (CSUR), 51(4):1–37.\nMiltiadis Allamanis and Charles Sutton. 2014. Mining\nidioms from source code. In Proceedings of the 22nd\nacm sigsoft international symposium on foundations\nof software engineering, pages 472–483.\nAmazon. 2022. CodeWhisperer. https://aws.\namazon.com/cn/codewhisperer.\nAnonymous. 2022. CodeT5Mix: A pretrained mixture\nof encoder-decoder transformers for code understand-\ning and generation. In Submitted to The Eleventh In-\nternational Conference on Learning Representations.\nUnder review.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang,\nXiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin\nAhmad, Shiqi Wang, Qing Sun, Mingyue Shang, Su-\njan Kumar Gonugondla, Hantian Ding, Varun Ku-\nmar, Nathan Fulton, Arash Farahani, Siddharth Jain,\nRobert Giaquinto, Haifeng Qian, Murali Krishna Ra-\nmanathan, Ramesh Nallapati, Baishakhi Ray, Parmin-\nder Bhatia, Sudipta Sengupta, Dan Roth, and Bing\nXiang. 2022. Multi-lingual evaluation of code gener-\nation models. ArXiv, abs/2210.14868.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie J. Cai, Michael Terry, Quoc V . Le, and\nCharles Sutton. 2021. Program synthesis with large\nlanguage models. ArXiv, abs/2108.07732.\nShraddha Barke, Michael B. James, and Nadia Polikar-\npova. 2022. Grounded Copilot: How programmers\ninteract with code-generating models. Proceedings\nof the ACM on Programming Languages, 7:85 – 111.\n7451\nShraddha Barke, Michael B James, and Nadia Po-\nlikarpova. 2023. Grounded copilot: How program-\nmers interact with code-generating models. Pro-\nceedings of the ACM on Programming Languages ,\n7(OOPSLA1):85–111.\nMohammad Bavarian, Heewoo Jun, Nikolas A. Tezak,\nJohn Schulman, Christine McLeavey, Jerry Tworek,\nand Mark Chen. 2022. Efficient training of language\nmodels to fill in the middle. ArXiv, abs/2207.14255.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. Neural In-\nformation Processing Systems, 33:1877–1901.\nFederico Cassano, John Gouwar, Daniel Nguyen,\nSy Duy Nguyen, Luna Phipps-Costin, Donald Pinck-\nney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane An-\nderson, Molly Q. Feldman, Arjun Guha, Michael\nGreenberg, and Abhinav Jangda. 2022. A scalable\nand extensible approach to benchmarking nl2code for\n18 programming languages. ArXiv, abs/2208.08227.\nYekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao\nTian, and Hua Wu. 2022. ERNIE-Code: Beyond\nenglish-centric cross-lingual pretraining for program-\nming languages. arXiv preprint arXiv:2212.06742.\nShubham Chandel, Colin B. Clement, Guillermo Ser-\nrato, and Neel Sundaresan. 2022a. Training and\nevaluating a jupyter notebook data science assistant.\nArXiv, abs/2201.12901.\nShubham Chandel, Colin B Clement, Guillermo Serrato,\nand Neel Sundaresan. 2022b. Training and evaluat-\ning a jupyter notebook data science assistant. arXiv\npreprint arXiv:2201.12901.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023.\nCodeT: Code generation with generated tests. In\nThe Eleventh International Conference on Learning\nRepresentations.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde, Jared Kaplan, Harrison Ed-\nwards, Yura Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Fe-\nlipe Petroski Such, David W. Cummings, Matthias\nPlappert, Fotios Chantzis, Elizabeth Barnes, Ariel\nHerbert-V oss, William H. Guss, Alex Nichol, Igor\nBabuschkin, S. Arun Balaji, Shantanu Jain, Andrew\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew M. Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. ArXiv,\nabs/2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinod-\nkumar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. PaLM: Scaling language\nmodeling with pathways. ArXiv, abs/2204.02311.\nFenia Christopoulou, Gerasimos Lampouras, Milan\nGritta, Guchun Zhang, Yinpeng Guo, Zhong-Yi Li,\nQi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu,\nLi yu Yan, Pingyi Zhou, Xin Wang, Yu Ma, Igna-\ncio Iacobacci, Yasheng Wang, Guangtai Liang, Jia\nWei, Xin Jiang, Qianxiang Wang, and Qun Liu. 2022.\nPanGu-Coder: Program synthesis with function-level\nlanguage modeling. ArXiv, abs/2207.11280.\nColin B. Clement, Dawn Drain, Jonathan Timcheck,\nAlexey Svyatkovskiy, and Neel Sundaresan. 2020.\nPyMT5: Multi-mode translation of natural language\nand python code with transformers. In Conference on\nEmpirical Methods in Natural Language Processing.\nCodedotAl. 2021. GPT Code Clippy: The Open Source\nversion of GitHub Copilot. https://github.com/\nCodedotAl/gpt-code-clippy.\n7452\nTrevor Cohn, Phil Blunsom, and Sharon Goldwater.\n2010. Inducing tree-substitution grammars. The\nJournal of Machine Learning Research , 11:3053–\n3096.\nLeonardo Mendonça de Moura and Nikolaj S. Bjørner.\n2008. Z3: An efficient smt solver. In International\nConference on Tools and Algorithms for Construction\nand Analysis of Systems.\nDeepGenX. 2022. CodeGenX. https://docs.\ndeepgenx.com.\nEnrique Dehaerne, Bappaditya Dey, Sandip Halder, Ste-\nfan De Gendt, and Wannes Meert. 2022. Code gener-\nation using machine learning: A systematic review.\nIEEE Access.\nPremkumar T. Devanbu. 2012. On the naturalness of\nsoftware. 2012 34th International Conference on\nSoftware Engineering (ICSE), pages 837–847.\nIddo Drori and Nakul Verma. 2021. Solving lin-\near algebra by program synthesis. arXiv preprint\narXiv:2111.08171.\nIddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard\nTang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda\nChen, Sunny Tran, Newman Cheng, Roman Wang,\nNikhil Singh, Taylor Lee Patti, J. Lynch, Avi Sh-\nporer, Nakul Verma, Eugene Wu, and Gilbert Strang.\n2021. A neural network solves, explains, and gener-\nates university math problems by program synthesis\nand few-shot learning at human level. Proceedings\nof the National Academy of Sciences of the United\nStates of America, 119.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 823–833.\nerman Arsenovich Arutyunov and Sergey Avdoshin.\n2022. Big transformers for code generation. Pro-\nceedings of the Institute for System Programming of\nthe RAS.\nFauxPilot. 2022. FauxPilot. https://github.com/\nmoyix/fauxpilot.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih,\nLuke Zettlemoyer, and Mike Lewis. 2023. InCoder:\nA generative model for code infilling and synthesis.\nIn The Eleventh International Conference on Learn-\ning Representations.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nGitHub. 2021. GitHub Copilot. https://github.\ncom/features/copilot.\nGoogle. 2016. GitHub on BigQuery: Analyze all the\nopen source code. https://cloud.google.com/\nbigquery.\nGoogle. 2022. Big-bench. https://github.com/\ngoogle/BIG-bench.\nSumit Gulwani. 2010. Dimensions in program synthe-\nsis. In Proceedings of the 12th international ACM\nSIGPLAN symposium on Principles and practice of\ndeclarative programming, pages 13–24.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Xiaodong Song,\nand Jacob Steinhardt. 2021. Measuring coding chal-\nlenge competence with apps. In Neural Information\nProcessing Systems.\nGlen M Hocky and Andrew D White. 2022. Natural\nlanguage processing models that automate program-\nming will transform chemistry research and teaching.\nDigital discovery, 1(2):79–83.\nHuggingFace. 2021a. CodeParrot Dataset.\nhttps://huggingface.co/datasets/\ntransformersbook/codeparrot.\nHuggingFace. 2021b. Github-Code. https:\n//huggingface.co/datasets/codeparrot/\ngithub-code.\nHuggingFace. 2021c. GitHub-Jupyter. https:\n//huggingface.co/datasets/codeparrot/\ngithub-jupyter.\nHuggingface. 2021. Training CodeParrot from Scratch.\nhttps://huggingface.co/blog/codeparrot.\nHuggingFace. 2022. The Stack. https://\nhuggingface.co/datasets/bigcode/the-stack.\nHamel Husain, Hongqi Wu, Tiferet Gazit, Miltiadis Al-\nlamanis, and Marc Brockschmidt. 2019. CodeSearch-\nNet Challenge: Evaluating the state of semantic code\nsearch. ArXiv, abs/1909.09436.\nIBM. 2021. CodeNet. https://github.com/IBM/\nProject_CodeNet.\nSaki Imai. 2022. Is github copilot a substitute for human\npair-programming? an empirical study. In Proceed-\nings of the ACM/IEEE 44th International Conference\non Software Engineering: Companion Proceedings,\npages 319–321.\nSrini Iyer, Ioannis Konstas, Alvin Cheung, and Luke\nZettlemoyer. 2016. Summarizing source code using\na neural attention model. Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nSusmit Jha, Sumit Gulwani, Sanjit A. Seshia, and\nAshish Tiwari. 2010. Oracle-guided component-\nbased program synthesis. 2010 ACM/IEEE 32nd\nInternational Conference on Software Engineering,\n1:215–224.\n7453\nAravind Joshi and Owen Rambow. 2003. A formal-\nism for dependency grammar based on tree adjoin-\ning grammar. In Proceedings of the Conference on\nMeaning-text Theory, pages 207–216. MTT Paris,\nFrance.\nHarshit Joshi, José Cambronero, Sumit Gulwani, Vu Le,\nIvan Radicek, and Gust Verbruggen. 2022. Repair is\nnearly generation: Multilingual program repair with\nllms. arXiv preprint arXiv:2208.11640.\nSungmin Kang, Bei Chen, Shin Yoo, and Jian-Guang\nLou. 2023. Explainable automated debugging via\nlarge language model-driven scientific debugging.\narXiv preprint arXiv:2304.02195.\nDarren Key, Wen-Ding Li, and Kevin Ellis. 2022. I\nSpeak, You Verify: Toward trustworthy neural pro-\ngram synthesis. arXiv preprint arXiv:2210.00848.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMario Krenn, Qianxiang Ai, Senja Barthel, Nessa\nCarson, Angelo Frei, Nathan C Frey, Pascal\nFriederich, Théophile Gaudin, Alberto Alexander\nGayle, Kevin Maik Jablonka, et al. 2022. Selfies\nand the future of molecular string representations.\nPatterns, 3(10):100588.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang,\nRuiqi Zhong, Luke Zettlemoyer, Scott Yih, Daniel\nFried, Si yi Wang, and Tao Yu. 2022. DS-1000: A\nnatural and reliable benchmark for data science code\ngeneration. ArXiv, abs/2211.11501.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Sil-\nvio Savarese, and Steven CH Hoi. 2022. CodeRL:\nMastering code generation through pretrained mod-\nels and deep reinforcement learning. arXiv preprint\narXiv:2207.01780, abs/2207.01780.\nTriet HM Le, Hao Chen, and Muhammad Ali Babar.\n2020. Deep learning for source code modeling and\ngeneration: Models, applications, and challenges.\nACM Computing Surveys (CSUR), 53(3):1–38.\nYaoxian Li, Shiyi Qi, Cuiyun Gao, Yun Peng, David\nLo, Zenglin Xu, and Michael R Lyu. 2022a. A closer\nlook into transformer-based code intelligence through\ncode transformation: Challenges and opportunities.\narXiv preprint arXiv:2207.04285.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kush-\nman, Julian Schrittwieser, Rémi Leblond, Tom, Ec-\ncles, James Keeling, Felix Gimeno, Agustin Dal\nLago, Thomas Hubert, Peter Choy, Cyprien de,\nMasson d’Autume, Igor Babuschkin, Xinyun Chen,\nPo-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey, Cherepanov, James Molloy, Daniel Jaymin\nMankowitz, Esme Sutherland Robson, Pushmeet\nKohli, Nando de, Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. 2022b. Competition-level code gener-\nation with alphacode. Science, 378:1092 – 1097.\nZhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh\nJannu, Grant Jenks, Deep Majumder, Jared Green,\nAlexey Svyatkovskiy, Shengyu Fu, et al. 2022c. Au-\ntomating code review activities by large-scale pre-\ntraining. In Proceedings of the 30th ACM Joint Eu-\nropean Software Engineering Conference and Sym-\nposium on the Foundations of Software Engineering,\npages 1035–1047.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text summariza-\ntion branches out, pages 74–81.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nZhiqiang Liu, Yong Dou, Jingfei Jiang, and Jinwei Xu.\n2016. Automatic code generation of convolutional\nneural networks in fpga implementation. In 2016\nInternational conference on field-programmable tech-\nnology (FPT), pages 61–68. IEEE.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey\nSvyatkovskiy, Ambrosio Blanco, Colin B. Clement,\nDawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong\nZhou, Linjun Shou, Long Zhou, Michele Tufano,\nMing Gong, Ming Zhou, Nan Duan, Neel Sundare-\nsan, Shao Kun Deng, Shengyu Fu, and Shujie Liu.\n2021. CodeXGLUE: A machine learning benchmark\ndataset for code understanding and generation.ArXiv,\nabs/2102.04664.\nLechanceux Luhunu and Eugene Syriani. 2017. Survey\non template-based code generation. In ACM/IEEE\nInternational Conference on Model Driven Engineer-\ning Languages and Systems.\nStephen MacNeil, Andrew Tran, Arto Hellas, Joanne\nKim, Sami Sarsa, Paul Denny, Seth Bernstein, and\nJuho Leinonen. 2022a. Experiences from using code\nexplanations generated by large language models in\na web software development e-book. arXiv preprint\narXiv:2211.02265.\nStephen MacNeil, Andrew Tran, Dan Mogil, Seth Bern-\nstein, Erin Ross, and Ziheng Huang. 2022b. Generat-\ning diverse code explanations using the gpt-3 large\nlanguage model. In Proceedings of the 2022 ACM\nConference on International Computing Education\nResearch-Volume 2, pages 37–39.\n7454\nAntonio Mastropaolo, Simone Scalabrino, Nathan\nCooper, David Nader Palacio, Denys Poshyvanyk,\nRocco Oliveto, and Gabriele Bavota. 2021. Study-\ning the usage of text-to-text transfer transformer to\nsupport code-related tasks. In 2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering\n(ICSE), pages 336–347. IEEE.\nMicrosoft. 2019. IntelliCode. https://github.com/\nMicrosoftDocs/intellicode.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\nNhan Nguyen and Sarah Nadi. 2022. An empirical\nevaluation of github copilot’s code suggestions. In\nProceedings of the 19th International Conference on\nMining Software Repositories, pages 1–5.\nTung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh\nNguyen, and Tien N Nguyen. 2013. A statistical se-\nmantic language model for source code. In Proceed-\nings of the 2013 9th Joint Meeting on Foundations of\nSoftware Engineering, pages 532–542.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2023. CodeGen: An open large language\nmodel for code with multi-turn program synthesis. In\nThe Eleventh International Conference on Learning\nRepresentations.\nUniversity of Oxford. 2020. Diffblue Cover. https:\n//www.diffblue.com.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback. ArXiv,\nabs/2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nDipti Pawade, Avani Sakhapara, Sanyogita Parab, Divya\nRaikar, Ruchita Bhojane, and Henali Mamania. 2018.\nLiterature survey on automatic code generation tech-\nniques. i-Manager’s Journal on Computer Science,\n6(2):34.\nHammond Pearce, Baleegh Ahmad, Benjamin Tan,\nBrendan Dolan-Gavitt, and Ramesh Karri. 2022.\nAsleep at the keyboard? assessing the security of\ngithub copilot’s code contributions. In 2022 IEEE\nSymposium on Security and Privacy (SP), pages 754–\n768. IEEE.\nJulian Aron Prenner and Romain Robbes. 2021. Auto-\nmatic program repair with openai’s codex: Evaluat-\ning quixbugs. arXiv preprint arXiv:2111.03922.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabil-\nities of large language models. arXiv preprint\narXiv:2204.00498.\nVeselin Raychev, Martin Vechev, and Eran Yahav. 2014.\nCode completion with statistical language models. In\nProceedings of the 35th ACM SIGPLAN Conference\non Programming Language Design and Implementa-\ntion, pages 419–428.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. CodeBLEU: a method\nfor automatic evaluation of code synthesis. arXiv\npreprint arXiv:2009.10297.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills,\nLong Ouyang, Jonathan Ward, and Jan Leike. 2022.\nSelf-critiquing models for assisting human evaluators.\narXiv preprint arXiv:2206.05802.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. BLOOM: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nMeet Shah, Rajat Shenoy, and Radha Shankarmani.\n2021. Natural language to python source code using\ntransformers. In 2021 International Conference on\nIntelligent Technologies (CONIT), pages 1–4. IEEE.\nTushar Sharma, Maria Kechagia, Stefanos Georgiou,\nRohit Tiwari, and Federica Sarro. 2021. A survey on\nmachine learning techniques for source code analysis.\narXiv preprint arXiv:2110.09610.\nJiho Shin and Jaechang Nam. 2021. A survey of auto-\nmatic code generation from natural language. Jour-\nnal of Information Processing Systems, 17(3):537–\n555.\nMohammed Latif Siddiq and msiddiq. 2022. SecurityE-\nval dataset: mining vulnerability examples to eval-\nuate machine learning-based code generation tech-\nniques. Proceedings of the 1st International Work-\nshop on Mining Software Repositories Applications\nfor Privacy and Security.\nDominik Sobania, Martin Briesch, and Franz Rothlauf.\n2022a. Choose your programming copilot: a compar-\nison of the program synthesis performance of github\ncopilot and genetic programming. In Proceedings of\nthe Genetic and Evolutionary Computation Confer-\nence, pages 1019–1027.\n7455\nDominik Sobania, Martin Briesch, and Franz Rothlauf.\n2022b. Choose your programming copilot: a compar-\nison of the program synthesis performance of github\ncopilot and genetic programming. In Proceedings of\nthe Genetic and Evolutionary Computation Confer-\nence, pages 1019–1027.\nZeyu Sun, Qihao Zhu, Lili Mou, Yingfei Xiong, Ge Li,\nand Lu Zhang. 2018. A grammar-based structural\ncnn decoder for code generation. In AAAI Confer-\nence on Artificial Intelligence.\nIlya Sutskever, Geoffrey E Hinton, and Graham W Tay-\nlor. 2008. The recurrent temporal restricted boltz-\nmann machine. Neural Information Processing Sys-\ntems, 21.\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,\nand Neel Sundaresan. 2020. IntelliCode compose:\ncode generation using transformer. Proceedings of\nthe 28th ACM Joint Meeting on European Software\nEngineering Conference and Symposium on the Foun-\ndations of Software Engineering.\nEugene Syriani, Lechanceux Luhunu, and Houari\nSahraoui. 2018. Systematic mapping study of\ntemplate-based code generation. Computer Lan-\nguages, Systems & Structures, 52:43–62.\ntabnine. 2018. TabNine. https://www.tabnine.com.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. LAMDA: Language models for dialog appli-\ncations. arXiv preprint arXiv:2201.08239.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Neural Information Processing Systems,\n30.\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu,\nHaochao Ying, Jian Wu, and Philip S Yu. 2018. Im-\nproving automatic source code summarization via\ndeep reinforcement learning. In Proceedings of the\n33rd ACM/IEEE international conference on auto-\nmated software engineering, pages 397–407.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nShiqi Wang, Zheng Li, Haifeng Qian, Cheng Yang,\nZijian Wang, Mingyue Shang, Varun Kumar, Sam-\nson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh\nNallapati, Murali Krishna Ramanathan, Dan Roth,\nand Bing Xiang. 2022a. ReCode: Robustness eval-\nuation of code generation models. arXiv preprint\narXiv:2212.10264.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH\nHoi. 2021. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8696–8708.\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu,\nand Graham Neubig. 2022b. MCoNaLa: a bench-\nmark for code generation from multiple natural lan-\nguages. arXiv preprint arXiv:2203.08388.\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Gra-\nham Neubig. 2022c. Execution-based evaluation\nfor open-domain code generation. arXiv preprint\narXiv:2212.10481.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nFrank F. Xu, Uri Alon, Graham Neubig, and Vincent J.\nHellendoorn. 2022. A systematic evaluation of large\nlanguage models of code. Proceedings of the 6th\nACM SIGPLAN International Symposium on Ma-\nchine Programming.\nYichen Xu and Yanqiao Zhu. 2022. A survey on pre-\ntrained language models for neural code intelligence.\narXiv preprint arXiv:2212.10079.\nPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan\nVasilescu, and Graham Neubig. 2018. Learning to\nmine aligned code and natural language pairs from\nstack overflow. 2018 IEEE/ACM 15th International\nConference on Mining Software Repositories (MSR),\npages 476–486.\nPengcheng Yin and Graham Neubig. 2017. A syntactic\nneural model for general-purpose code generation.\narXiv preprint arXiv:1704.01696.\nDaoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji\nWang, and Jian-Guang Lou. 2022a. When language\nmodel meets private library. In Conference on Em-\npirical Methods in Natural Language Processing.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,\nMinsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,\nand Jian-Guang Lou. 2022b. CERT: Continual pre-\ntraining on sketches for library-oriented code genera-\ntion. In International Joint Conference on Artificial\nIntelligence.\nJialu Zhang, José Cambronero, Sumit Gulwani, Vu Le,\nRuzica Piskac, Gustavo Soares, and Gust Ver-\nbruggen. 2022. Repairing bugs in python assign-\nments using large language models. arXiv preprint\narXiv:2209.14876.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan-\nshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and\nJie Tang. 2023. CodeGeeX: A pre-trained model\nfor code generation with multilingual evaluations on\nhumaneval-x. ArXiv, abs/2303.17568.\n7456\nShuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang,\nand Graham Neubig. 2023. DocCoder: Generating\ncode by retrieving and reading docs. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravin-\ndran, Sindhu Tipirneni, and Chandan K. Reddy.\n2022a. XLCoST: A benchmark dataset for cross-\nlingual code intelligence.\nMing Zhu, Karthik Suresh, and Chandan K Reddy.\n2022b. Multilingual code snippets training for pro-\ngram translation.\n7457\nA Related Surveys\nPrevious surveys on the topic of code intelli-\ngence (Allamanis et al., 2018; Le et al., 2020; Li\net al., 2022a; Xu and Zhu, 2022) and code gener-\nation (Pawade et al., 2018; Shin and Nam, 2021;\nDehaerne et al., 2022) have primarily focused on\nearly methodologies such as the use of program-\nming templates (Syriani et al., 2018; Luhunu and\nSyriani, 2017), neural models based on CNN, RNN,\nand LSTM architectures (Allamanis et al., 2018;\nSharma et al., 2021), and small-scale Transformer\nmodels that require labelled data for training (Mas-\ntropaolo et al., 2021; Shah et al., 2021). However,\nwith the advancement of model size, Transformer-\nbased models have demonstrated exceptional per-\nformance in NL2Code tasks and have given rise to\nthe development of more capable code generation\nmodels. In light of this, there exists a clear need for\na comprehensive survey of large language models\nfor NL2Code tasks to bridge this gap in knowledge.\nThis study endeavours to fulfill this need by pro-\nviding a thorough analysis of the successful LLMs\nand a detailed review of NL2Code benchmarks and\nmetrics. We also present the ongoing challenges\nand opportunities regarding the ability gap between\nLLMs and humans.\nFinally, we would like to highlight some criteria\nfor our survey. First, we only refer to official papers\nto investigate the size of the models. For example,\nCodex reported the model with a maximum size of\n12B in the paper, but later trained larger ones. In\nthis case, we only consider the 12B model as the\nlargest one. In addition, the publication dates of the\nmodels in Figure 2 are taken from official papers\nor blogs.\nB An Online Website\nTo keep tracking the latest progress of LLMs for\nNL2Code, we have developed an online real-time\nupdate website at https://nl2code.github.io.\nWe have collected as many of the latest research\nworks as possible on this website. Everyone is\nallowed to contribute to the website by pulling\nrequests on GitHub. This website also includes\nfeatures such as fuzzy search and custom tag cate-\ngories, which will facilitate researchers to find the\npapers they want quickly. We hope this website can\nassist researchers and developers in related fields\nand contribute to its advancement.\nModel Size pass@k\nk=1 k=10 k=100\nModel Size: ~100M\nGPT-Neo† 125M 0.26 2 .15 7 .96\nCodeParrot† 110M 0.48 3 .89 15 .93\nPyCodeGPT† 110M 9.39 28 .37 48.71\nPolyCoder† 160M 1.08 6 .67 18 .97\nModel Size: ~500M\nCodeT5† 770M 15.78 38.63 50 .35\nPolyCoder† 400M 1.31 7 .98 21 .55\nBLOOM† 560M 0.26 2 .04 8 .90\nCodeGen-Mono† 350M 15.44 42.50 64.40\nModel Size: ~1B\nGPT-Neo† 1.3B 3.77 16 .26 29 .51\nCodeParrot† 1.5B 1.29 8 .66 27 .17\nBLOOM† 1.1B 1.90 9 .20 23 .42\nBLOOM† 1.7B 3.16 14 .23 31 .38\nInCoder† 1.3B 10.00 34.02 55.50\nSantaCoder† 1.1B 3.65 21 .33 41 .92\nModel Size: ~5B\nGPT-Neo† 2.7B 5.89 23 .09 44 .26\nPolyCoder† 2.7B 4.39 17 .99 38 .17\nBLOOM† 3B 2.25 13 .58 32 .08\nBLOOM† 7.1B 1.01 7 .91 24 .12\nCodeGen-Mono† 2.7B 28.80 60 .73 75.41\nCodeGen-Mono† 6.1B 33.70 62.70 70.25\nGPT-J† 6B 11.30 35 .62 53 .63\nInCoder 6.7B 21.3 46 .5 66 .2\nModel Size: >10B\nCodeGen-Mono 16.1B 42.4 65 .8 79 .1\ncushman-001 − 45.9 66 .9 79 .9\ndavinci-001 − 51.8 72 .8 84 .1\ndavinci-002 − 58.1 76 .7 84 .5\nTable 4: The performance of LLMs on the MBPP bench-\nmark. †denotes our reproduced results, while others\nare taken from Chen et al. (2023). We omit CodeGPT,\nGPT-CC, and PLBART as their numbers are zero.\nC Experimental Setup\nIn this section, we will first present the definition\nof pass@k , followed by the details of the experi-\nments conducted on two benchmarks, namely Hu-\nmanEval (Chen et al., 2021) (results in Table 2) and\nMBPP (Austin et al., 2021) (results in Table 4).\nC.1 Definition of pass@k\nWe use pass@k as our metric for evaluation. For\neach programming problem, we sample n candi-\ndate code solutions and then randomly pick k of\nthem. If any of the k code solutions pass the given\ntest cases, the problem can be regarded as solved.\nSo pass@k is the proportion of solved problems\nin the benchmark (Chen et al., 2021). Formally,\n7458\nassuming that the number of correct ones in k sam-\nples is c, pass@k = 1 if n −c < k; otherwise,\npass@k = 1 −∏n\ni=n−c+1(1 −k/i). We chose\npass@k as our primary evaluation metric because\nit offers a completely precise evaluation of code\naccuracy by executing test cases, while other met-\nrics mentioned in Section 4 either originate from\npass@k or have lower precision.\nC.2 Implementation Details\nFor HumanEval, we use the original benchmark9.\nMost results in Table 2 are taken from the original\npapers, while we reproduce the results of GPT-CC,\nPLBART, CodeT5, and InCoder 1.3B by strictly\nfollowing the same experimental setup as the other\nmodels. In detail, we set the sample number to 200,\nthe maximum length of newly generated tokens to\n200, and top_p to 0.95. We set the temperature\nfrom 0.1 to 1.0 with an interval of 0.1, and report\nthe best performance across these temperatures.\nFor MBPP, we use the version from Chen et al.\n(2023)10. In Table 4, the results of InCoder 6.7B\nand models larger than 10B are taken from Chen\net al. (2023), while we reproduced other results.\nSpecifically, we set the sample number to 100, the\nmaximum length of newly generated tokens to 200,\ntop_p to 0.95, and the temperature to 0.8.\nFor the two benchmarks above, we employ\nthe same post-processing strategy. Following\nCodex (Chen et al., 2021), we terminate the sam-\npling process when one of the following sequences\nis encountered in the generated code: ‘ \\nclass’,\n‘\\ndef’, ‘\\n#’, ‘\\n@’, ‘\\nif’, and ‘ \\nprint’. In\nour experiments, CodeT5 770M refers to the ver-\nsion11 with the causal language modeling objective.\nFor good reproducibility and further research, we\nhave made our code and the generated results of the\nLLMs on HumanEval and MBPP publicly available\non our website.\nD Context Window vs. Performance\nRecent work (erman Arsenovich Arutyunov and\nAvdoshin, 2022) claimed that the size of the con-\ntext window plays a vital role in enhancing the\nperformance of LLMs for NL2Code. Specifi-\n9https://github.com/openai/human-eval/blob/\nmaster/data/HumanEval.jsonl.gz\n10https://github.com/microsoft/CodeT/blob/\nmain/CodeT/data/dataset/mbpp_sanitized_for_code_\ngeneration.jsonl\n11https://huggingface.co/Salesforce/\ncodet5-large-ntp-py\nIntroductory Interview Competition\n0%\n1%\n2%\n3%Average Passed T est Cases\nGPT-NeoX 165M 2K\nGPT-NeoX 165M 4K\nGPT-NeoX 165M 8K\nGPT-NeoX 20B 2K\nFigure 6: Performance of GPT-NeoX with different\nmodel sizes (165M and 20B) and context windows (2K,\n4K, and 8K) on the APPS benchmark.\ncally, experiments are conducted on the APPS\nbenchmark (Hendrycks et al., 2021) with GPT-\nNeoX (Black et al., 2022), and we visualize the\nresults in Figure 6. It is found that the 165M ver-\nsion model with an 8, 000 context window is com-\nparable to the 20B version model with a 2, 000\ncontext window. This observation illustrates that\nthe context window also needs to be considered\nwhen training the model.\n7459\nProducts Model Supported PLs Supported IDEs\ntabnine (2018) −\nPython, Java, Javascript, TypeScript,\nGo, Ruby, PHP, C#, C, C++, Swift,\nPerl, Rust, CSS, Angular, Dart, React,\nHaskell, HTML, Kotlin, Matlab, Sass,\nNodeJS, Objective C, Scala,\nVS Code, Visual Studio, IntelliJ IDE,\nNeovim, Sublime, PyCharm, Rider,\nWebStorm, Android Studio, Emacs,\nVim, PhpStorm, RubyMine, DataGrip,\nJupyter Notebook, JupyterLab, Clion,\nAppCode, Eclipse, GoLand\naiXcoder (2018) − Python, Java, JavaScript, Typescript,\nGo, PHP, C, C++\nVS Code, IntelliJ IDEA, PyCharm,\nSTS3, WebStorm, Rider, Clion, STS4\nAndroid Studio, PhpStorm, Eclipse,\nGoLand\nIntelliCode (2019) − Python, Java, JavaScript, TypeScript,\nC#, C++, SQL Server, XAML VS Code, Visual Studio\nDiffblue Cover (2020)− Java IntelliJ IDEA, CLI Tool\nCopilot (2021) Codex\nPython, Java, JavaScript, TypeScript,\nGo, Ruby, Julia, PHP, C#, C++, Swift,\nPerl, PowerShell, R, Rust, CSS, SQL,\nJSON, HTML, SCSS, Less, .NET,\nMarkdown, T-SQL\nVS Code, Visual Studio, Neovim,\nJetBrains IDE\nCosy (2022) − Java IntelliJ IDEA\nCodeWhisperer (2022)− Python, Java, JavaScript, TypeScript,\nC#\nVS Code, JetBrains IDE, AWS Cloud9,\nAWS Lambda\nCodeGenX (2022) GPT-J Python VS Code\nCodeGeeX (2023) CodeGeeX\nPython, Java, JavaScript, TypeScript,\nGo, PHP, C#, C, C++, Perl, Rust, CSS,\nSQL, HTML, Kotlin, Shell, R, Cuda,\nObjective C, Objective C++, Pascal,\nTex, Fortran, Lean, Scala\nVS Code, IntelliJ IDEA, PyCharm,\nWebStorm, Android Studio, Rider,\nRubyMine, Clion, AppCode, Aqua,\nDataGrip, GoLand, DataSpell\nFauPilot (2022) CodeGen Python, Java, Javascript, Go, C, C++ −\nTable 5: Summary of products powered by LLMs. PLs and IDEs refer to programming languages and integrated\ndevelopment environments, respectively. The information for these products was recorded on December27, 2022.\n0 5 10 15\n0\n5\n10\n15\n20\n25\n30HumanEval pass@1 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodex\nAlphaCode(dec)\nCodeGen-Mono\nPanGu-Coder\nInCoder\nBLOOM\n0 5 10 15\n0\n10\n20\n30\n40\n50HumanEval pass@10 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodex\nAlphaCode(dec)\nCodeGen-Mono\nPanGu-Coder\nInCoder\nBLOOM\n0 5 10 15\n0\n20\n40\n60HumanEval pass@100 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodex\nAlphaCode(dec)\nCodeGen-Mono\nPanGu-Coder\nInCoder\nBLOOM\n0 5 10 15\nNumber of Parameters (Billion)\n0\n10\n20\n30\n40MBPP pass@1 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodeGen-Mono\nInCoder\n0 5 10 15\nNumber of Parameters (Billion)\n0\n10\n20\n30\n40\n50\n60MBPP pass@10 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodeGen-Mono\nInCoder\n0 5 10 15\nNumber of Parameters (Billion)\n0\n20\n40\n60\n80MBPP pass@100 (%)\nCodeT5\nGPT-Neo\nCodeParrot\nPolyCoder\nCodeGen-Mono\nInCoder\nFigure 7: Performance of LLMs with varying parameter sizes on the HumanEval and MBPP benchmarks.\n7460\nDataModel Hyper-parametersTraining Modelde.token.opti.betasepsbswsgsswplrwddecayprinit.m.\nDecoder\nGPT-C366M × BBPE Adam− − −1,024− −6.25e-5−Cosine−Scratch→\nCodeGPT124M × BBPE Adam− − −768− −5e-5− − −GPT-2→\nGPT-Neo2.7B−BBPE Adam0.9,0.95 1e-8−2,048−3,000−0.1Cosine−Scratch→\nGPT-J6B−BBPE Adam− − −2,048 16 3,000−0.1−BF16−→\nCodex12B✓BBPE Adam0.9,0.95 1e-8 2M4,096−175 1e-4 0.1Cosine−GPT-3→\nGPT-CC1.3B✓BBPE AdaFa− − −1,024−5,000 2e-5 0.1Linear−GPT-Neo→\nCodeParrot1.5B✓BBPE AdamW0.9,0.999 1e-8 524K1,024 16 750 5e-5 0.1Cosine−Scratch→\nLaMDA137B−SP− − −256K− − − − − − − −→\nPolyCoder2.7B✓BBPE AdamW0.9,0.999 1e-8 262K2,048−1,600 1.6e-4−Cosine−Scratch→\nCodeGen16.1B✓BBPE Adam0.9,0.999 1e-8 2M2,048−3,000 0.5e-4 0.1Cosine− −→\nInCoder6.7B✓BBPE Adam0.9,0.98− −2,048−1,500− −PN−Scratch↔\nGPT-NeoX20B✓BBPE ZeRo0.9,0.95 1e-8 3.15M2,048 32−9.7e-5 0.01Cosine FP16 Scratch→\nPaLM-Coder540B✓SP Adafa.− − −2,048− −1e-2− − −PaLM→\nPanGu-Coder2.6B✓SP Adam0.9,0.95− −1,024− − −0.01Cosine−Scratch→\nFIM6.9B✓BBPE Adam− −2M2,048− −2.4e-4− − −Scratch↔\nPyCodeGPT110M✓BBPE AdamW0.9,0.95 1e-8 480K1,024 4 1,000 5e-4 0.1Cosine FP16 Scratch→\nCodeGeeX13B−BBPE ZeRo0.9,0.95− −2,048− − −0.1Cosine FP16−→\nBLOOM176B✓BBPE Adam0.9,0.95− −2,048− −6e-5 0.1Cosine BF16 Scratch→\nSantaCoder1.1B✓BBPE Adam0.9,0.95 1e-8− − − −2e-4 0.1Cosine FP16 Scratch↔\nEncoder-Decoder\nPyMT5374M✓BBPE Adam0.9,0.98 1e-6−2,200−5,000 9.1875e-5 0.01IS FP16−→\nPLBART406M × SP Adam−,0.98 1e-6−768− −5e-5−Linear FP16−→\nCodeT5770M × BBPE AdamW− − − − −1,000 2e-4 0.05Linear FP16 Scratch→\nJuPyT5350M✓BBPE Adam0.9,0.98 1e-6−2,200−5,000 9.1875e-5 0.01IS FP16 PyMT5→\nAlphaCode41.1B✓SP AdamW0.9,0.95− −6,144−1,000 1e-4 0.1Cosine BF16−→\nCodeRL770M−BBPE AdamW− − − − − − − −PN−CodeT5→\nCodeT5Mix770M✓BBPE AdamW− − − − − − −0.1Linear FP16 Scratch→\nERNIE-Code560M × SP AdaFa− − −1,024 15 1,000 1e-4−Linear BF16 mT5→\nTable 6: The details of LLMs for NL2Code. We list the full names of these abbreviations: de-duplication ( de.),\ntokenizer (token.), optimizer (opti.), batch size (bs), window size (ws), gradient accumulation steps (gss), warmup\nsteps (wp), learning rate ( lr), weight decay ( wd), decay schedule ( decay), precision floating point ( pr), model\ninitialization (init.), left-to-right (→), fill-in-the-middle (↔), byte-level byte-pair-encoding (BBPE), SentencePiece\n(SP), polynomial (PN), and inverse square (IS).\n7461\nBenchmark Originate From Multilingual\nMCoNaLa (2022b) CoNaLa (2018) English, Spanish, Japanese, Russian\nODEX (2022c) CoNaLa (2018)\nMCoNaLa (2022b) English, Spanish, Japanese, Russian\nMBXP (2022) MBPP (2021) Python, Java, JavaScript, TypeScript, Go, Ruby,\nKotlin, PHP, C#, Scala, C++, Swift, Perl\nMBXP-HumanEval (2022) HumanEval (2021) Python, Java, JavaScript, Ruby, Kotlin, PHP, Scala,\nSwift, Perl,\nMultiPL-MBPP (2022) MBPP (2021)\nPython, Java, JavaScrpt, TypeScript, Go, Ruby,\nJulia, PHP, C#, Scala, C++, Swift, Perl, D, Bash,\nRacket, Lua, R, Rust\nMultiPL-HumanEval (2022) HumanEval (2021)\nPython, Java, JavaScrpt, TypeScript, Go, Ruby,\nJulia, PHP, C#, Scala, C++, Swift, Perl, D, Bash,\nRacket, Lua, R, Rust\nHumanEval-X (2023) HumanEval (2021) Python, Java, JavaScript, Go, C++\nTable 7: Details of multilingual NL2Code benchmarks. Here we also list MCoNaLa and CoNaLa, which have no\ntest case for evaluation.\n7462\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7: Limitations\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n0: Abstract 6: Conclusion\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nAppendix C: Experimental Setup\n□\u0013 B1. Did you cite the creators of artifacts you used?\nAppendix C: Experimental Setup\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4: Benchmarks and Metrics\nC □\u0013 Did you run computational experiments?\n3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7463\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix C: Experimental Setup\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix C: Experimental Setup\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n7464",
  "topic": "Guan",
  "concepts": [
    {
      "name": "Guan",
      "score": 0.8919677734375
    },
    {
      "name": "Chen",
      "score": 0.64568692445755
    },
    {
      "name": "Zhàng",
      "score": 0.6133978366851807
    },
    {
      "name": "Computer science",
      "score": 0.5428978204727173
    },
    {
      "name": "Computational linguistics",
      "score": 0.4187092185020447
    },
    {
      "name": "Natural language processing",
      "score": 0.27365610003471375
    },
    {
      "name": "Humanities",
      "score": 0.25385504961013794
    },
    {
      "name": "China",
      "score": 0.18447288870811462
    },
    {
      "name": "History",
      "score": 0.17984488606452942
    },
    {
      "name": "Archaeology",
      "score": 0.17948183417320251
    },
    {
      "name": "Art",
      "score": 0.17417776584625244
    },
    {
      "name": "Geology",
      "score": 0.11904612183570862
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I28006308",
      "name": "Shandong Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}