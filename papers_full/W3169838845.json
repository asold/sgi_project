{
    "title": "Multi-Domain Aspect Extraction Using Bidirectional Encoder Representations From Transformers",
    "url": "https://openalex.org/W3169838845",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2994737939",
            "name": "Brucce Neves dos Santos",
            "affiliations": [
                "Universidade de São Paulo"
            ]
        },
        {
            "id": "https://openalex.org/A1984765312",
            "name": "Ricardo Marcondes Marcacini",
            "affiliations": [
                "Universidade de São Paulo"
            ]
        },
        {
            "id": "https://openalex.org/A2154671366",
            "name": "Solange Oliveira Rezende",
            "affiliations": [
                "Universidade de São Paulo"
            ]
        },
        {
            "id": "https://openalex.org/A2994737939",
            "name": "Brucce Neves dos Santos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1984765312",
            "name": "Ricardo Marcondes Marcacini",
            "affiliations": [
                "Institute of Mathematics and Computer Science"
            ]
        },
        {
            "id": "https://openalex.org/A2154671366",
            "name": "Solange Oliveira Rezende",
            "affiliations": [
                "Institute of Mathematics and Computer Science"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2977700584",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6725474846",
        "https://openalex.org/W2251777082",
        "https://openalex.org/W2163459391",
        "https://openalex.org/W2104731083",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W758155932",
        "https://openalex.org/W6731920789",
        "https://openalex.org/W2337835951",
        "https://openalex.org/W2144012961",
        "https://openalex.org/W2252024663",
        "https://openalex.org/W2963264961",
        "https://openalex.org/W2427312199",
        "https://openalex.org/W2765994350",
        "https://openalex.org/W6764456104",
        "https://openalex.org/W6769318315",
        "https://openalex.org/W6684149856",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2253519362",
        "https://openalex.org/W2395579298",
        "https://openalex.org/W2080558111",
        "https://openalex.org/W4248506559",
        "https://openalex.org/W2088622183",
        "https://openalex.org/W2112744748",
        "https://openalex.org/W6734372248",
        "https://openalex.org/W2152571774",
        "https://openalex.org/W2031998113",
        "https://openalex.org/W3003963580",
        "https://openalex.org/W2889366722",
        "https://openalex.org/W2791056613",
        "https://openalex.org/W4211186029",
        "https://openalex.org/W2993843842",
        "https://openalex.org/W2612990922",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2739677115",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6757635932",
        "https://openalex.org/W2790309729",
        "https://openalex.org/W6786526239",
        "https://openalex.org/W6634901647",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2510668267",
        "https://openalex.org/W2982567551",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2908854766",
        "https://openalex.org/W2081375810",
        "https://openalex.org/W2952357537",
        "https://openalex.org/W2574519325",
        "https://openalex.org/W2925618549",
        "https://openalex.org/W1581485226",
        "https://openalex.org/W2163302275",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2593112041",
        "https://openalex.org/W3109375587",
        "https://openalex.org/W2108646579",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2896457183"
    ],
    "abstract": "Deep learning and neural language models have obtained state-of-the-art results in aspects extraction tasks, in which the objective is to automatically extract characteristics of products and services that are the target of consumer opinion. However, these methods require a large amount of labeled data to achieve such results. Since data labeling is a costly task, there are no labeled data available for all domains. In this paper, we propose an approach for aspect extraction in a multi-domain transfer learning scenario, thereby leveraging labeled data from different source domains to extract aspects of a new unlabeled target domain. Our approach, called MDAE-BERT (Multi-Domain Aspect Extraction using Bidirectional Encoder Representations from Transformers), explores neural language models to deal with two major challenges in multi-domain learning: (1) inconsistency of aspects from target and source domains and (2) context-based semantic distance between ambiguous aspects. We evaluated our MDAE-BERT considering two perspectives (1) the aspect extraction performance using F1-Macro and Accuracy measures; and (2) by comparing the multi-domain aspect extraction models and single-domain models for aspect extraction. In the first perspective, our method outperforms the LSTM-based approach. In the second perspective, our approach proved to be a competitive alternative compared to the single-domain model trained in a specific domain, even in the absence of labeled data from the target domain.",
    "full_text": "Received March 11, 2021, accepted May 20, 2021, date of publication June 14, 2021, date of current version July 2, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3089099\nMulti-Domain Aspect Extraction Using\nBidirectional Encoder Representations\nFrom Transformers\nBRUCCE NEVES DOS SANTOS\n, RICARDO MARCONDES MARCACINI\n,\nAND SOLANGE OLIVEIRA REZENDE\nInstitute of Mathematics and Computer Sciences, University of São Paulo, São Carlos 13566-590, Brazil\nCorresponding author: Brucce Neves dos Santos (brucce.neves@usp.br)\nThis work was supported in part by the National Council for Scientiﬁc and Technological Development (CNPq) under\nGrant 426663/2018-7, and in part by the São Paulo Research Foundation (FAPESP) under Grant 2019/25010-5 and Grant 2019/07665-4.\nABSTRACT Deep learning and neural language models have obtained state-of-the-art results in aspects\nextraction tasks, in which the objective is to automatically extract characteristics of products and services\nthat are the target of consumer opinion. However, these methods require a large amount of labeled data to\nachieve such results. Since data labeling is a costly task, there are no labeled data available for all domains.\nIn this paper, we propose an approach for aspect extraction in a multi-domain transfer learning scenario,\nthereby leveraging labeled data from different source domains to extract aspects of a new unlabeled target\ndomain. Our approach, called MDAE-BERT (Multi-Domain Aspect Extraction using Bidirectional Encoder\nRepresentations from Transformers), explores neural language models to deal with two major challenges in\nmulti-domain learning: (1) inconsistency of aspects from target and source domains and (2) context-based\nsemantic distance between ambiguous aspects. We evaluated our MDAE-BERT considering two perspectives\n(1) the aspect extraction performance using F1-Macro and Accuracy measures; and (2) by comparing\nthe multi-domain aspect extraction models and single-domain models for aspect extraction. In the ﬁrst\nperspective, our method outperforms the LSTM-based approach. In the second perspective, our approach\nproved to be a competitive alternative compared to the single-domain model trained in a speciﬁc domain,\neven in the absence of labeled data from the target domain.\nINDEX TERMS Aspect extraction, multi-domain, BERT, transfer learning.\nI. INTRODUCTION\nOpinion Mining is the task of extracting opinions or senti-\nments from unstructured texts using Natural Language Pro-\ncessing (NLP), Text Mining, and Machine Learning. The\nkey idea is to analyze automatically large review datasets to\nclassify them into sentiment polarities (i.e., positive, negative,\nor neutral) [1], [2].\nOpinions are formally deﬁned as a 5-tuple (e i,aij,sijkl ,\nhk ,tl ), where ei is the i-th entity of the opinion, aij is the\nj-th aspect on which the opinion is related, sijkl is the sen-\ntiment contained in the opinion that can be positive, nega-\ntive or neutral, hk is the k-th holder of the opinion, and tl\nis temporal information about the opinion [1]. This deﬁni-\ntion can be applied to different levels of sentiment analysis.\nThe most general level is called the document level and\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Thanh Long Vu\n.\naims to evaluate the overall sentiment of a textual document\n(e.g., text review) [3], [4].\nSentiment analysis can also be performed at the sentence\nlevel, which consists of dividing a document into several\nsentences and evaluating the sentiment of each sentence indi-\nvidually [3], [4]. Finally, aspect-based sentiment analysis is\nthe most granular level aiming to evaluate the sentiment of\nentities and aspects [5]–[7]. Figure 1 presents an example of\naspect-based sentiment analysis, where ‘‘laptop’’ is the entity\nand ‘‘screen’’ is the aspect extracted from the review. Note\nthat sentiment analysis can fail if performed at the document\nor sentence level.\nAspect-based sentiment analysis is the most promising\nscenario for Opinion Mining, providing a detailed analy-\nsis of consumers’ opinions about products and services [8].\nHowever, it is a more complex scenario due to the require-\nment to extract aspects from the reviews before sentiment\nclassiﬁcation.\n91604 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 9, 2021\nB. N. D. Santoset al.: MDAE-BERT\nFIGURE 1. Example of aspect-based sentiment analysis, where ‘‘laptop’’\nis the entity (ei ), ‘‘screen’’ is the aspect (aij ) with positive sentiment, and\n‘‘battery’’ is another aspect (ai(j+1)) with negative sentiment.\nAspect extraction from opinion texts is a crucial step\nin opinion mining. The ﬁrst proposed initiatives are meth-\nods based on linguistic rules, generally used in conjunc-\ntion with Part-of-Speech tools [9]. More recently, machine\nlearning-based methods have been recognized as state-of-\nthe-art due to the automatic learning of complex patterns\nfrom textual features for aspect extraction. In particular, deep\nlearning and neural language models have obtained the best\nresults in aspect extraction tasks [10]–[15]. However, these\nmethods require a large amount of labeled data to achieve\nsuch results. Since data labeling is a costly task, there are\nno labeled datasets available for all domains. A domain in\nthis work is deﬁned as a product category [16], for instance,\nrestaurants, hotels, and smartphones.\nA promising alternative that has been used successfully\nin other areas to deal with the lack of labeled data is the\nmulti-domain knowledge transfer [17]–[21], where labeled\ndata from already known domains is used to learn a model\nto classify data from a new domain [7]. Thus, multi-domain\ntransfer learning methods should only use labeled data from a\nsource domain and do not require labeled data from the target\ndomain.\nIn this paper, we propose an approach for aspect extraction\nin a multi-domain transfer learning scenario. We identiﬁed\ntwo signiﬁcant challenges in this scenario: (1) inconsistency\nof aspects between the target and source domains and (2)\nsemantic distance between aspects. The ﬁrst challenge occurs\nmainly when the target and source domains have very dif-\nferent characteristics. For example, aspects extracted from\nopinions about smartphones (source domain) will be incon-\nsistent with aspects extracted from opinions about hotels\n(target domain). The second challenge is related to the terms\nthat can have different meanings in different domains. For\nexample, ‘‘light’’ can mean ‘‘the lighting of an environ-\nment’’ in a domain or ‘‘the weight of an object’’ in another\ndomain.\nTo address the challenges of aspect extraction with\nmulti-domain transfer learning, our approach explores the\nBERT (Bidirectional Encoder Representations from Trans-\nformers) neural language model [22], which has been obtain-\ning promising results in several NLP tasks. The structure of\nthe BERT neural network is organized in different layers,\nwhere each layer allows to capture information at the level\nof sentence, syntactic, and semantic characteristics [23]–[25].\nWe note that the BERT model is useful in dealing with both\ninconsistent aspects and semantic differences. Our approach,\ncalled MDAE-BERT (Multi-Domain Aspect Extraction using\nBidirectional Encoder Representations from Transformers),\nfocuses more on the linguistic patterns existing in the BERT\nlayers regarding the inconsistency of aspects between source\nand target domains. Regarding semantic differences, our\napproach explores BERT contextual representations, in which\nthe semantic proximity among aspects is determined accord-\ning to the context of the opinion.\nThe results are presented and discussed considering two\nperspectives (1) the aspect extraction performance using\nF1-Macro and accuracy measures; and (2) comparison\nbetween the multi-domain aspect extraction model and the\nsingle-domain model, thereby aiming to evaluate whether\nthe use of multi-domain data for training is a competitive\nalternative if compared to the cost of labeling data for a\nnew domain. Our proposed approach achieved a competitive\nperformance compared to the baseline (LSTM-based aspect\nextraction), obtaining a minimum increase in aspect extrac-\ntion performance of 7.99% for F1-Macro and 10.62% for\naccuracy. Moreover, our MDAE-BERT approach is also a\ncompetitive alternative compared to single-domain models,\neven in the absence of labeled data from the target domain.\nThe rest of the paper is organized as follows. Section 2\npresents background and related studies. The proposed\nMDAE-BERT approach is described in Section 3. In\nSection 4, an experimental evaluation of the proposed method\nis carried out, as well as a comparison with competitive base-\nlines as (1) LSTM-based models for multiple domains and (2)\nBERT-based models for single domains. Section 5 presents\nthe conclusions and directions for future work.\nII. BACKGROUND AND RELATED STUDIES\nOpinion Mining aims to extract opinions from unstructured\ntexts, combining Natural Language Processing and Machine\nLearning techniques. Several levels for sentiment analysis\nhave been proposed since the advent of Opinion Mining.\nFirst, document-level sentiment analysis aims to classify the\nsentiment polarity (e.g., positive, negative, or neutral) of a\nwhole document [1]. Second, sentence-level sentiment anal-\nysis divides the document into sentences, and the polarity\nof each sentence is classiﬁed [3], [4]. Sentence-level sen-\ntiment analysis is analogous to the previous one because a\nsentence can be seen as a short document. Sentiment analysis\nat document or sentence levels failed to deal with phrases\nwith more than one aspect with different sentiments. Thus,\nsentiment analysis at the aspect level emerged, which is the\nmost granular level. Aspect-based sentiment analysis can be\ndivided into two tasks: (1) extract the entities and aspects\nfrom text opinions; and (2) classify the sentiment polarity in\nthe context of each aspect [5]–[7].\nIn aspect extraction tasks, we have to deal with explicit and\nimplicit aspects [26]. Explicit aspects are words or expres-\nsions that directly refer to a technical characteristic of a\nproduct or service. For example, in ‘‘the battery has a good\nlife’’, the aspect ‘‘battery’’ is a technical feature explicitly\nused by the reviewer. On the other hand, implicit aspects are\nindirect references to characteristics of products or services,\nusually through expressions about the behavior of the aspect.\nVOLUME 9, 2021 91605\nB. N. D. Santoset al.: MDAE-BERT\nFor example, in the text ‘‘My photos are too dark’’, the opin-\nion is about the smartphone’s implicit aspect ‘‘camera’’.\nTraditional approaches for aspect extraction are based\non linguistic rules, which usually depend on lexicons,\npart-of-speech tagging, and token dependency relationships.\nThese linguistic resources are often unavailable for multi-\nple domains, as well as being hampered in the presence of\nreviews with typos, slang, and abbreviation obtained from\nsocial platforms [27]–[36]. Moreover, methods based on lex-\nicons, ontologies, or other external resources are useful for\nexplicit aspect extraction, but fail to extract implicit aspects.\nHowever, recent methods based on neural language models\nallow extracting both explicit and implicit aspects by taking\nadvantage of pre-trained models and more complex textual\nrepresentations, such as word embeddings.\nWe focus on recent studies that involve two types of deep\nneural networks for learning neural language models. First,\nwe explore Long Short-Term Memory (LSTM) networks and\ntheir variants that allow dealing with long sequences of tokens\nand used in methods such as ELMO (Embeddings from\nLanguage Models) [37]. Second, we explore Transformers\nnetworks, which in addition to dealing with token sequences,\nalso use attention mechanisms and provide parallelization for\nscalable training from a large textual corpus. Both LSTM\nand Transformers have been successfully explored in the\ncontext of aspect-based sentiment analysis, as discussed in\nthe following subsections.\nA. LSTM-BASED ASPECT EXTRACTION\nLong Short-Term Memory (LSTM) is a type of recurrent\nneural network (RNN) with loops that allows the persistence\nof information from long sequences, such as token sequences\nextracted from reviews. An LSTM unit is called a memory\ncell or LSTM cell, as illustrated in Figure 2, which has a\nvector called the cell state to store information, and its value\nis changed based on the gates. The input gate protects the\nstored information against irrelevant disturbances. The forget\ngate selects which pieces of information in the cell state are\nimportant and which information should be forgotten. The\noutput gate protects other units from disorders caused by\nirrelevant content stored in the cell and decides which parts of\nthe cell state are important to generate the output. These gates\nregulate the ﬂow of information into and out of the cell [38].\nFIGURE 2. Illustration of the LSTM cell.\nThe ﬂow starts with the forget gate using a sigmoid func-\ntion to decide which information should be removed from\nthe cell. In the next step, the input gate determines what\ninformation should be stored in the cell. Finally, the output\ngate decides whether the cell information will be visible to\nother cells.\nIn [11], the authors evaluated different LSTM net-\nwork architectures for aspect extraction and presented\nan experimental comparison involving traditional machine\nlearning-based methods, such as CRF (Conditional Random\nField) models. The results showed that methods based on neu-\nral language models are more efﬁcient for aspect extraction,\neven without hand-crafted features.\nSeveral methods for aspect extraction using LSTM net-\nworks have been proposed recently. In [13], the authors\nincluded a local context strategy based on a window of\nwords surrounding the aspect during the LSTM training.\nAn experimental analysis on bidirectional LSTM for aspect\nextraction tasks was presented by [39], in which LSTM-based\naspect extraction obtained results superior to the top-ranked\nmethods in the 2014 SemEval ABSA aspect extraction con-\ntest.1 Li et al. [40] discussed the impact of attention mech-\nanisms in LSTM networks to reﬁne the process of aspect\nextraction. Ma et al. [41] proposed an approach based on\nLSTM networks for aspect extraction and evaluated the\nimpact of external knowledge resources to improve neural\nlanguage modeling in sentiment analysis. A recent survey by\nNazir et al.[8] presents an overview of aspects-based senti-\nment analysis techniques, where they discussed that LSTM\nmodels are very promising for learning implicit knowledge\nfrom reviews because they work similarly to the human brain\nto understand the importance of each word in the review — a\nuseful resource for identifying aspects in reviews.\nAlthough LSTM-based aspect extraction methods have\nreceived signiﬁcant attention in the literature, their use is\nunderexplored in multi-domain scenarios. Another recog-\nnized limitation is that LSTM has drawbacks related to\nscalability since sequential dependence impairs the par-\nallelization of neural network training. Transformers net-\nworks [42] is an alternative considered state-of-the-art for\nnatural language processing, which surpasses the drawbacks\nof LSTM networks.\nB. TRANSFORMERS-BASED ASPECT EXTRACTION\nTransformers are an evolution of the encoder-decoder archi-\ntecture to handle sequential data, such as text reviews [42].\nUnlike RNN’s where the words are presented one at a time\nin a sequence and the current state depends on the result of\nthe previous state, in the Transformers encoder, words of the\nsentence are processed in parallel to learn text embeddings,\nas illustrated in Figure 3.\nWord order is crucial in natural language processing (NLP)\ntasks since words in different positions may have different\nmeanings. In order to parallelize the processing of textual\ndata, Transformers networks introduce a positional encoder to\nmap the distance between the tokens of the sentence. Figure 4\nshows the complete architecture of the Transformers network.\n1https://alt.qcri.org/semeval2014/task4/\n91606 VOLUME 9, 2021\nB. N. D. Santoset al.: MDAE-BERT\nFIGURE 3. Illustration of the difference between RNN and transformers.\nFIGURE 4. Architecture of the transformers network.\nRegarding the Transformers encoder step, the Input\nEmbedding receives the sentence words and maps them\nto their respective word embedding. A Positional Encod-\ning adds the respective position vector, thereby producing\na contextual word embedding considering the word positions\nof the sentence.\nThe encoder consists of n identical layers. The original\npaper uses n =6. The input of the ﬁrst layer is the contextual\nword embeddings, and the input of subsequent layers is the\noutput of the previous layer. Each layer is described below.\n• Multi-Head Attention: This layer consists of x\nSelf-Attention layers that are processed in parallel. The\nSelf-Attention layers complement each other. The orig-\ninal paper uses x =8. Each word will have x attention\nvectors and these vectors will be concatenated and multi-\nplied by another vector to obtain the ﬁnal result. In short,\nthe input and output of the layer are attention vectors for\neach word, thus indicating the most relevant words in the\nsentence.\n• Feed Forward: It consists of several Feed Forward\nNeural Networks (FFNN), which process each word of\nthe sentence in parallel. Each FFNN is a simple neural\nnetwork with two layers fully connected with a ReLU\nactivation.\n• Add & Normalization:This layer receives the input\nvectors and the output vectors from the previous layer\nto generate residual connections in order to optimize\ndeep network training. Normalization is carried out on\nthese vectors to facilitate optimization and ensure that\nthe Positional Encoder remains stable throughout the\nprocess.\nAfter the encoder step, the decoder step is executed for the\nnext word prediction tasks. The decoder aims to predict one\nword at a time until the special token ‘‘<end>’’ is predicted.\nUnlike the encoder, which is only executed once, the decoder\nhas to be performed k times, where the predicted word will\nfeed the decoder in the next step. In Output Embedding,\nthe decoder receives the words of the sentence that have\nalready been predicted. The ﬁrst word is the special token\n‘‘<start>’’. The Positional Encoding of the decoder is a\nsimilar operation to the encoder. The decoder contains sub-\nlayers, as described below.\n• Masked Multi-Head Attention:It works similarly to\nthe encoder sub-layer ‘‘Multi-Head Attention’’. The\nmain difference is to prevent future words from being\npart of the attention. In other words, it uses only the\nwords that have already been predicted in the attention\nmechanism.\n• Multi-Head Attention:It works in the same way as the\nencoder. However, one of the inputs is the output of the\nlast encoder.\n• Feed Forward:Similar operation to the encoder step.\n• Add & Normalization:Similar operation to the encoder\nstep.\nThe decoder’s output at this point is a numeric vector which\nis processed by two layers:\n• Linear: It is a fully connected neural network that\nreceives the output vector from the last decoder and\nprojects it into a large vector called ‘‘logits vector’’.\nVOLUME 9, 2021 91607\nB. N. D. Santoset al.: MDAE-BERT\nThe dimension of this vector is according to the vocab-\nulary size.\n• Softmax: This layer receives ‘‘vector logits’’ and turns\neach word’s score into probability, thereby selecting the\nword associated with the highest chance. The output\nfrom the softmax layer will be added to the ‘‘Output\nEmbedding’’ until the word ‘‘ <end>’’ is produced.\nDevlin et al.[22] argue that one limitation of Transformers\nis the fact that it is unidirectional, thereby restricting the\nchoice of architectures that can be used during pre-training.\nBERT (Bidirectional Encoder Representations from Trans-\nformers) is a pre-training language representation method\nwith the general purpose of generating a ‘‘language under-\nstanding’’ model. First, BERT is trained through large\nunlabeled text corpus. Second, we can reﬁne the model for\na speciﬁc task, in a task called ﬁne-tuning [22]. BERT is\navailable in two architectures as described in Table 1.\nTABLE 1. BERT’s model architecture.\nBERT consists of a vocabulary of 30000 tokens from\nWordPiece embeddings [43]. If a word is not in the vocab-\nulary, then that word will be divided into sub-words that\nexist in the vocabulary. These sub-words can even be a single\ncharacter. The ﬁrst 1000 tokens are reserved, and except for\nﬁve tokens, the other tokens are in the form ‘‘[unused2]’’.\nThese ﬁve tokens are:\nPAD: Token used for padding, for example, when there\nare sequences of different sizes.\nUNK: Token ‘‘unknown’’ used to represent a token that is\nnot in the vocabulary.\nCLS: Token ‘‘classiﬁer’’ used for sequence classiﬁcation\ninstead of token classiﬁcation. In general, it is the\nﬁrst token in the sequence.\nSEP: Token ‘‘separator’’ is used to construct a sequence\nfrom sub-sequences. For example, two sequences\nfor sentence classiﬁcation or question answering\ntasks. It is also used as the last token in a sequence.\nMASK: Token used to mask values. This token is used\nto replace the word that the model will attempt to\npredict.\nBERT training can be divided into two stages:\n• Pre-Training: BERT is pre-trained using two unsuper-\nvised tasks: (1) ‘‘Masked Language Model (MLM)’’\nwhere a percentage of the input tokens are selected\nat random and replaced with the [MASK] token. The\npurpose of this task is to predict the masked tokens; and\n(2) ‘‘Next Sentence Prediction (NSP)’’ that receives the\nsentences A and B to predict if the sentence B is the next\nsentence of A. This training is important since under-\nstanding the relationship between the two sentences is\nvital in tasks like ‘‘Question Answering (QA)’’ and\n‘‘Natural Language Inference (NLI)’’.\n• Fine-Tuning: This stage uses a model that has been\ntrained in a large text corpus and carries out more\ntraining using the domain-application texts and labels.\nCompared to the Pre-Training stage, Fine-Tuning has a\nlow cost.\nIn [14], BERT is evaluated for sentiment analysis tasks\nto compare the impact of BERT ﬁne-tuning. The results\ndemonstrate that BERT ﬁne-tuning improves the perfor-\nmance of sentiment analysis tasks, including aspect extrac-\ntion. Zhang and Shi [44] recently evaluated BERT ﬁne-tuning\nfor aspect extraction task. In particular, the authors con-\ncluded that aspect extraction using contextual word embed-\ndings from BERT model obtained state-of-the-art results for\nsingle-domain aspect extraction, compared to LSTM and\nCRF-based models. In [15], the authors evaluated the BERT\nﬁne-tuning using labeled reviews from different domains.\nThe promising results provide evidence that BERT-based\nmodels are efﬁcient for classifying whether a given aspect\nbelongs to a speciﬁc domain. Although it is not a speciﬁc\nproposal for multi-domain aspect extraction, these results\nmotivated us to investigate BERT in scenarios in which a\ntarget domain does not have labeled aspects.\nIII. MDAE-BERT: MULTI-DOMAIN ASPECT EXTRACTION\nUSING BERT MODEL\nThe MDAE-BERT innovates by using labeled data from mul-\ntiple domains for ﬁne-tuning a pre-trained language model.\nWe unify labeled aspects from different domains into a token\nclassiﬁcation task. We use the IOB representation for token\nlabels. The IOB is short for inside, outside, and beginning.\nEach token of a text in the training set is labeled with class B,\nwhich indicates that the token represents the beginning of an\naspect; class I, which indicates that the token is inside an\naspect; and class O, which indicates that the token is outside\nan aspect. The IOB representation is illustrated in Tables 2\nand 3 for aspects formed by a single and multiple tokens,\nrespectively.\nTABLE 2. Example of IOB labels for a review with an aspect (screen)\nformed by a single token.\nTABLE 3. Example of IOB labels for a review with an aspect (battery life)\nformed by multiple tokens.\nLet Ds and Dt be two domains, where Ds indicates the\nsource domain and Dt the target domain. In the source\ndomain, Dk\ns ={(x i,xa\ni ,yi)k\ns }Nk\ns\ni=1, where x =(x1,x2,..., xS )\nrepresents the token sequence of size S of a review belonging\n91608 VOLUME 9, 2021\nB. N. D. Santoset al.: MDAE-BERT\nto the k-th source domain, xa =(xa\n1 ,xa\n2 ,..., xa\nM ) represents\nthe aspect, where xa is a subsequence of size S (with 1 ≤\nM ≤S) from x, and y indicates the IOB tag of the token.\nNk\ns indicates the number of reviews with labeled aspects in\nthe k-th source domain. The target domain is composed by\nreviews with unlabeled aspects Dt = {(xt\ni )}Nt\ni=1, where Nt\nindicates the number of reviews in the target domain. The\naspect extraction task can be formulated as a token level\nsequence labeling problem, where given a review of the target\ndomain xt ∈Dt , for each token x ∈xt , the task aims to ﬁnd\na label y ∈{I ,O,B}.\nOur approach uses the BERT model to encode the review\ntoken sequence into a vector representing semantic and lin-\nguistic information. As we discussed in Section II, BERT\nis a pre-trained language model based on deep bidirec-\ntional Transformers using two prediction tasks: the masked\nlanguage model and the next sentence prediction. Below,\nwe present details of how each task is explored in the\nMDAE-BERT approach.\nA. CONTEXTUAL WORD EMBEDDINGS FOR\nMULTI-DOMAIN ASPECT EXTRACTION\nLearning word embeddings for text reviews is a crucial step\nfor aspect extraction. We focus on contextual word embed-\ndings, in which the vector representation of a word is a func-\ntion of the entire text review in which it occurs. Extracting\naspects through their semantic representation is a strategy\nto deal with the limitations presented in the introduction,\nparticularly the inconsistency between aspects from target\nand source domains.\nIn the proposed MDAE-BERT approach, we extend the\nBERT masked-language model to the multi-domain sce-\nnario. Given a token sequence of the review x ∈ Dk\ns ,\nthe BERT model ﬁrst generates a corrupted version ˆx of the\nreview, where approximately 15% of the tokens are randomly\nselected and replaced by a special token called [MASK].\nThus, the objective function is to reconstruct the masked\ntokens ¯x from ˆx, according to Equation 1,\nmax\nθ\nlog p(¯x|ˆx,θ) ≈\n∑\nx∈Dks\nS∑\nt=1\nmt log\n×\n\n exp(hθ(ˆx)⊤\nt e(xt ))\n∑\nx′\nexp(hθ(ˆx)⊤t e(x′))\n\n (1)\nwhere e(xt ) indicates the word embedding vector of the token\nxt ; the hθ(ˆx)t is a sequence of S hidden state vectors according\nto parameters θfrom the Transfomers neural network model;\nand mt =1 indicates when xt is masked.\nNote that we aggregate reviews from all source domains\ninto a word embedding learning strategy based on denois-\ning auto-encoding. By masking tokens considering multiple\ndomains, we force the model to generate word embeddings\nconsidering the context, particularly the review domain in\nwhich the word occurs, since the model tries to predict\na masked word that may occur ambiguously in different\ndomains.\nB. TOKEN CLASSIFICATION FOR ASPECT EXTRACTION\nWhile the previous step focuses on learning contextual word\nembeddings, now we deﬁne the speciﬁc classiﬁcation task\nthat guides the ﬁne-tuning of the pre-trained BERT model.\nIn general, BERT-based classiﬁcation tasks use a special\ntoken called [CLS] at the beginning of each review. The\ncorresponding word embedding of the [CLS] token is used\nas a vector representation of the entire sentence, considering\nboth the semantics of the text and the linguistic structure of\nthe sentence.\nThe same sentence should have different representations in\nthe token classiﬁcation from reviews in IOB tags according to\nthe considered aspect tokens. Thus, we formulated the token\nclassiﬁcation as a sentence pair classiﬁcation task to obtain\nvector representations of reviews according to the aspect,\nsimilar to the next sentence prediction task used in BERT.\nFIGURE 5. Example of MDAE-BERT input for aspect extraction.\nGiven a review x =(x1,x2,..., xS ) and an aspect xa =\n(xa\n1 ,xa\n2 ,..., xa\nM ), we use the special [CLS] token at the begin-\nning of the review and include a special [SEP] token after the\nreview to connect the aspect tokens, as shown in Figure 5.\nThus, the input xr for ﬁne-tuning of the BERT model is as\nfollows:\nxr ={[CLS ],x1,x2,..., xS ,[SEP],xa\n1 ,xa\n2 ,..., xa\nM ,[SEP]}\nNow, we can use BERT to encode the input sequence xr ,\nwhere the vector corresponding to the [CLS] token considers\nboth the review and aspect sequences. Let xr\nCLS =BERT (xr )\nthe corresponding vector of the review-aspect representation,\nwe use xr\nCLS as the input of a classiﬁer composed of a dense\nlayer followed by a softmax layer to classify the aspect in the\nIOB tags, according to Equations 2 and 3, respectively,\nL =tanh(Wx xr\nCLS +bx ) (2)\nˆy =softmax(WL L +bL ) (3)\nVOLUME 9, 2021 91609\nB. N. D. Santoset al.: MDAE-BERT\nFIGURE 6. Overview of the effectiveness (F1-Macro) of each analyzed method for multi-domain aspect\nextraction. The X-axis means which dataset was used for testing.\nFIGURE 7. Overview of the effectiveness (Accuracy) of each analyzed method for multi-domain aspect\nextraction. The X-axis means which dataset was used for testing.\nwhere ˆy is the estimated probability distribution of the aspect\nin relation to the IOB tags, and Wx , WL , bx , bL , are weight\nparameters of the neural network to be optimized.\nFor model training, while tags B and I are directly esti-\nmated from labeled aspect tokens, we sample a subset of\nreview tokens for tag O. In this sampling, we use all tokens\nin a review that are not aspects.\nOur MDAE-BERT model is trained using cross-entropy\nloss, as deﬁned in Equation 4,\nL =−\n∑\nDks ∈Ds\n∑\nxr ∈Dk\ns\nC∑\nc=1\nyc\nxr log(ˆyc\nxr ) (4)\nwhere C indicates integer codes for each IOB tag, ˆyc\nxr is the\npredicted probability for input xr , and yc\nxr is a value of 1 or 0\nthat indicates when the IOB tag was correctly predicted by\nthe model.\nIV. EXPERIMENTAL ANALYSIS AND DISCUSSION\nWhile most works in the literature evaluate aspect extrac-\ntion tasks using two datasets (Laptops and Restaurants\ndatasets from 2014 SemEval ABSA aspect extraction con-\ntext), we collect and provide a repository 2 containing several\nreview datasets of different domains with labeled aspects.\nIn our experimental evaluation, we use 15 datasets (domains)\ncontaining reviews in English, as presented in Table 4. In this\n2https://github.com/BrucceNeves/MDAE-BERT\ntable, O indicates the number of non-aspect words, B indi-\ncates the number of aspects, and I indicates how many words\nare the continuation of an aspect (i.e., IOB tags). The training\nprocess consists of a cross-validation approach at the domain\nlevel, where 14 domains were used to train the model, and\n1 domain is used to evaluate the aspect extraction task.\nIn each dataset, all duplicate sentences were removed.\nA sentence is considered duplicate if another sentence\npresents the same words in the same order, disregarding any\npunctuation. For example, the following sentences are con-\nsidered duplicated: ‘‘The Screen is amazing.’’, ‘‘the screen is\namazing!!!!’’ and ‘‘ThE, ScReEn, Is, AmAzIng!!!...’’. It was\nalso guaranteed that there are no duplicate sentences between\nthe datasets. For example, ‘‘the battery has a great life’’\ncould exist in the Laptop dataset and the MicroMP3 dataset.\nStopwords and punctuations were labeled with the ‘‘O’’ tag.\nOur MDAE-BERT uses BERT base architecture. LSTM-\nbased aspect extraction uses the GloVe 3 word embedding.\nEach input review for training step consists of 3 information\npieces:\n• Sentence with token ti replaced by the marker ‘‘$T$’’.\n• Token ti\n• Token label ti in the IOB format.\nThe results are presented and discussed considering (1)\nthe classiﬁcation performance using the F1-Macro and Accu-\nracy measures, and (2) a comparison of our MDAE-BERT\n3http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n91610 VOLUME 9, 2021\nB. N. D. Santoset al.: MDAE-BERT\nFIGURE 8. Comparison between classification efficiency (F1-Macro) using a model trained in one\ndomain compared to the multi-domain model. The error bars illustrate the standard deviation. The\nX-axis indicates the domain that was used for testing, in the case of the multi-domain.\nFIGURE 9. Comparison between the efficiency (accuracy) of classification using a model trained in one\ndomain compared to the multi-domain model. The error bars illustrate the standard deviation. The\nX-axis indicates the domain that was used for testing, in the case of the multi-domain.\n(which does not use labeled data from the target domain)\nand a BERT-based aspect extraction model (AE-BERT) that\nincorporates labeled data from the target domain.\nFigures 6 and 7 show the experimental results (F1-Macro\nand Accuracy measures) for multi-domain aspect extraction.\nThe proposed MDAE-BERT approach obtained a superior\nperformance compared with the LSTM, obtaining a mini-\nmum increase in the aspect extraction performance of 7.99%\nfor F1-Macro and 10.62% for Accuracy. The best result of\nMDAE-BERT was for the Laptop target domain with 65.86%\nF1-Macro, thereby achieving almost 90% improvement over\nLSTM.\nThe second experiment aims to assess whether using our\nMDAE-BERT is an alternative if compared to the cost of\nlabeling data for a new domain. In this scenario, we use\n90% of labeled aspects of the target domain to train a\nBERT-based aspect extractor (AE-BERT). For a fair com-\nparison, the MDAE-BERT was evaluated using the same\ncross-validation test folds, i.e., using the 10% remaining\nreviews from the target domain. The results are illustrated\nin Figures 8 and 9, which respectively compare F1-Macro and\nAccuracy.\nAs expected, the AE-BERT obtained higher values of\nF1-Macro and Accuracy since it is trained with data from\nthe target domain. However, it is worth noting that in this\nTABLE 4. Overview of the reviews datasets used in the experimental\nevaluation.\nscenario, the AE-BERT is a hypothetical model, thereby indi-\ncating the result obtained if it were possible to label aspects of\nthe target domain. Even without labeled aspects of the target\ndomain, MDAE-BERT obtained competitive results for most\ndomains.\nTable 5 presents the inconsistency between the domains\nused in the training and the target domain. The inconsistency\nVOLUME 9, 2021 91611\nB. N. D. Santoset al.: MDAE-BERT\nTABLE 5. Overview of the inconsistency levels between source and target\ndomain in the experimental evaluation. High values mean high\ninconsistency.\nwas calculated according to the measure proposed in [7] by\ncomputing the number of aspects that the source and target\ndomains have in common. In general, when there is less\ninconsistency between the target and source domains, there\nis also less difference in performance between AE-BERT\nand MDAE-BERT, indicating that MDAE-BERT can learn\nrelated concepts from other domains. When the inconsis-\ntency is high, as in the target domains ‘‘restaurant’’, ‘‘diaper\nchamp’’ and ‘‘hotels ’, the MDAE-BERT presents some limi-\ntations in transferring knowledge among domains. Analyzing\nmulti-domain learning in inconsistent domains motivates fur-\nther research. We argue that pre-training BERT models from\nreview datasets instead of general domain texts is a promising\nway to deal with such knowledge transfer limitations.\nV. CONCLUDING REMARKS\nMulti-domain aspect extraction is a promising solution to\nmitigate the cost of labeling data for aspect-based senti-\nment analysis. The proposed MDAE-BERT proved to be a\ncompetitive alternative for this task since it uses existing\nlabeled data from other domains to train an aspect extrac-\ntion model. MDAE-BERT obtained results superior to the\nLSTM-based aspect extraction for multiple domains. In addi-\ntion, MDAE-BERT was competitive with AE-BERT, which\nconsiders labeled data from the target domain.\nWe note that aspect extraction results can be improved\nwhen exploring the MDAE-BERT conﬁdence scores. For\nexample, if a new target domain has a higher inconsistency\nin relation to source domains, it is possible to increase the\nclassiﬁcation process’s conﬁdence threshold by each aspect.\nIn this case, only aspects classiﬁed with greater conﬁdence\nfor classes B or I would be extracted, thereby increasing the\nprecision (to the detriment of the recall). This strategy will\nbe explored in future works. Another future work to mitigate\ninconsistency between domains is to reﬁne the pre-training\nstage of BERT using a large corpus of reviews instead of a\ngeneral-purpose text corpus.\nFinally, another contribution is a pre-trained multi-domain\naspect extraction model available at https://github.com/\nBrucceNeves/MDAE-BERT. This model can be used in\nEnglish reviews to extract aspects in an unsupervised way.\nIn addition, it is also prepared for a ﬁne-tuning process to\nreﬁne the model for aspect extraction if new labeled data are\navailable.\nVI. ACKNOWLEDGMENT\nThe authors would like to thank the NVIDIA for donating\ncomputer equipment (GPU Grant Academic Program).\nREFERENCES\n[1] B. Liu, ‘‘Sentiment analysis and opinion mining,’’ Synthesis Lect. Hum.\nLang. Technol., vol. 5, no. 1, pp. 1–167, 2012.\n[2] C. C. Aggarwal, ‘‘Opinion mining and sentiment analysis,’’ in Machine\nLearning for Text. Boston, MA, USA: Springer, 2018, pp. 413–434.\n[3] E. Riloff and J. Wiebe, ‘‘Learning extraction patterns for subjective\nexpressions,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.\nStroudsburg, PA, USA: Association for Computational Linguistics, 2003,\npp. 105–112.\n[4] H. Yu and V . Hatzivassiloglou, ‘‘Towards answering opinion questions:\nSeparating facts from opinions and identifying the polarity of opinion\nsentences,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.\nStroudsburg, PA, USA: Association for Computational Linguistics, 2003,\npp. 129–136.\n[5] R. Feldman, ‘‘Techniques and applications for sentiment analysis,’’ Com-\nmun. ACM, vol. 56, no. 4, pp. 82–89, Apr. 2013.\n[6] I. P. Matsuno, R. G. Rossi, R. M. Marcacini, and S. O. Rezende, ‘‘Aspect-\nbased sentiment analysis using semi-supervised learning in bipartite het-\nerogeneous networks,’’ J. Inf. Data Manage., vol. 7, no. 2, p. 141, 2016.\n[7] R. M. Marcacini, R. G. Rossi, I. P. Matsuno, and S. O. Rezende, ‘‘Cross-\ndomain aspect extraction for sentiment analysis: A transductive learning\napproach,’’Decis. Support Syst., vol. 114, pp. 70–80, Oct. 2018.\n[8] A. Nazir, Y . Rao, L. Wu, and L. Sun, ‘‘Issues and challenges of aspect-\nbased sentiment analysis: A comprehensive survey,’’ IEEE Trans. Affect.\nComput., early access, Jan. 30, 2020, doi: 10.1109/TAFFC.2020.2970399.\n[9] A. Yadav and D. K. Vishwakarma, ‘‘Sentiment analysis using deep learning\narchitectures: A review,’’ Artif. Intell. Rev., vol. 53, no. 6, pp. 4335–4385,\nAug. 2020.\n[10] O. Irsoy and C. Cardie, ‘‘Opinion mining with deep recurrent neural\nnetworks,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.\n(EMNLP), 2014, pp. 720–728.\n[11] P. Liu, S. Joty, and H. Meng, ‘‘Fine-grained opinion mining with recurrent\nneural networks and word embeddings,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2015, pp. 1433–1443.\n[12] S. Poria, E. Cambria, and A. Gelbukh, ‘‘Aspect extraction for opinion\nmining with a deep convolutional neural network,’’ Knowl.-Based Syst.,\nvol. 108, pp. 42–49, Sep. 2016.\n[13] J. Yuan, Y . Zhao, B. Qin, and T. Liu, ‘‘Local contexts are effective for\nneural aspect extraction,’’ in Proc. Chin. Nat. Conf. Social Media Process.\nSingapore: Springer, 2017, pp. 244–255.\n[14] H. Xu, B. Liu, L. Shu, and P. Yu, ‘‘BERT post-training for review reading\ncomprehension and aspect-based sentiment analysis,’’ in Proc. Conf. North\nAmer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., vol. 1.\nAssociation for Computational Linguistics, 2019, pp. 2324–2335.\n[15] M. Hoang, O. A. Bihorac, and J. Rouces, ‘‘Aspect-based sentiment analysis\nusing BERT,’’ in Proc. 22nd Nordic Conf. Comput. Linguistics, Turku,\nFinland, Sep./Oct. 2019, pp. 187–196.\n[16] J. Blitzer, M. Dredze, and F. Pereira, ‘‘Biographies, Bollywood, boom-\nboxes and blenders: Domain adaptation for sentiment classiﬁcation,’’ in\nProc. 45th Annu. Meeting Assoc. Comput. Linguistics, 2007, pp. 440–447.\n[17] S. J. Pan and Q. Yang, ‘‘A survey on transfer learning,’’ IEEE Trans. Knowl.\nData Eng., vol. 22, no. 10, pp. 1345–1359, Oct. 2010.\n[18] K. Schouten and F. Frasincar, ‘‘Survey on aspect-level sentiment analysis,’’\nIEEE Trans. Knowl. Data Eng., vol. 28, no. 3, pp. 813–830, Mar. 2016.\n[19] K. Weiss, T. M. Khoshgoftaar, and D. Wang, ‘‘A survey of transfer learn-\ning,’’J. Big data, vol. 3, no. 1, p. 9, 2016.\n91612 VOLUME 9, 2021\nB. N. D. Santoset al.: MDAE-BERT\n[20] P. Zhang, J. Wang, Y . Wang, and Y . Wang, ‘‘A statistical approach to\nopinion target extraction using domain relevance,’’ in Proc. 2nd IEEE Int.\nConf. Comput. Commun. (ICCC), Oct. 2016, pp. 273–277.\n[21] T. A. Rana and Y .-N. Cheah, ‘‘A two-fold rule-based model for aspect\nextraction,’’Expert Syst. Appl., vol. 89, pp. 273–285, Dec. 2017.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,’’\nin Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum.\nLang. Technol., vol. 1. Association for Computational Linguistics, 2019,\npp. 4171–4186.\n[23] I. Tenney, P. Xia, B. Chen, A. Wang, A. Poliak, R. T. McCoy, N. Kim,\nB. Van Durme, S. Bowman, D. Das, and E. Pavlick, ‘‘What do you\nlearn from context? Probing for sentence structure in contextualized word\nrepresentations,’’ in Proc. Int. Conf. Learn. Represent., 2019, pp. 1–17.\n[24] J. Hewitt and C. D. Manning, ‘‘A structural probe for ﬁnding syntax\nin word representations,’’ in Proc. Conf. North Amer. Chapter Assoc.\nComput. Linguistics, Hum. Lang. Technol., vol. 1. Minneapolis, MA, USA:\nAssociation for Computational Linguistics, 2019, pp. 4129–4138.\n[25] G. Jawahar, B. Sagot, and D. Seddah, ‘‘What does BERT learn about\nthe structure of language?’’ in Proc. 57th Annu. Meeting Assoc. Comput.\nLinguistics. Florence, Italy: Association for Computational Linguistics,\n2019, pp. 3651–3657.\n[26] M. Hu and B. Liu, ‘‘Mining opinion features in customer reviews,’’ in Proc.\n19th Nat. Conf. Artif. Intell. (AAAI), vol. 4, 2004, pp. 755–760.\n[27] L. Zhuang, F. Jing, and X.-Y . Zhu, ‘‘Movie review mining and summariza-\ntion,’’ in Proc. 15th ACM Int. Conf. Inf. Knowl. Manage. (CIKM), 2006,\npp. 43–50.\n[28] A.-M. Popescu and O. Etzioni, ‘‘Extracting product features and opinions\nfrom reviews,’’ in Natural Language Processing and Text Mining. London,\nU.K.: Springer, 2007, pp. 9–28.\n[29] S. Blair-Goldensohn, T. Neylon, K. Hannan, G. A. Reis, R. Mcdonald, and\nJ. Reynar, ‘‘Building a sentiment summarizer for local service reviews,’’ in\nProc. WWW Workshop NLP Inf. Explosion Era, 2008, pp. 14–23.\n[30] S. S. Htay and K. T. Lynn, ‘‘Extracting product features and opinion words\nusing pattern knowledge in customer reviews,’’ Sci. World J., vol. 201,\nDec. 2013, Art. no. 394758.\n[31] Z. Hai, K. Chang, J.-J. Kim, and C. C. Yang, ‘‘Identifying features in\nopinion mining via intrinsic and extrinsic domain relevance,’’ IEEE Trans.\nKnowl. Data Eng., vol. 26, no. 3, pp. 623–634, Mar. 2014.\n[32] S. Poria, E. Cambria, L.-W. Ku, C. Gui, and A. Gelbukh, ‘‘A rule-based\napproach to aspect extraction from product reviews,’’ in Proc. 2nd Work-\nshop Natural Lang. Process. Social Media (SocialNLP), 2014, pp. 28–37.\n[33] Q. Liu, B. Liu, Y . Zhang, D. S. Kim, and Z. Gao, ‘‘Improving opinion\naspect extraction using semantic similarity and aspect associations,’’ in\nProc. 30th AAAI Conf. Artif. Intell., 2016, pp. 1–7.\n[34] Q. Liu, Z. Gao, B. Liu, and Y . Zhang, ‘‘Automated rule selection for opinion\ntarget extraction,’’ Knowl.-Based Syst., vol. 104, pp. 74–88, Jul. 2016.\n[35] O. F. Gunes, T. Furche, and G. Orsi, ‘‘Structured aspect extraction,’’\nin Proc. 26th Int. Conf. Comput. Linguistics, Tech. Papers (COLING),\nvol. 2016, 2016, pp. 2321–2332.\n[36] S. M. Jiménez-Zafra, M. T. Martín-Valdivia, E. Martínez-Cámara, and\nL. A. Ureña-López, ‘‘Combining resources to improve unsupervised sen-\ntiment analysis at aspect-level,’’ J. Inf. Sci., vol. 42, no. 2, pp. 213–229,\nApr. 2016.\n[37] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\nL. Zettlemoyer, ‘‘Deep contextualized word representations,’’ in Proc.\nConf. North Amer. Chapter Assoc. Comput. Linguistics: Human Lang.\nTechnol., vol. 1, 2018, pp. 2227–2237.\n[38] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[39] T. U. Tran, H. T.-T. Hoang, and H. X. Huynh, ‘‘Bidirectional independently\nlong short-term memory and conditional random ﬁeld integrated model for\naspect extraction in sentiment analysis,’’ in Frontiers in intelligent comput-\ning: Theory and Applications. Singapore: Springer, 2020, pp. 131–140.\n[40] X. Li, L. Bing, P. Li, W. Lam, and Z. Yang, ‘‘Aspect term extraction with\nhistory attention and selective transformation,’’ in Proc. 27th Int. Joint\nConf. Artif. Intell., Jul. 2018, pp. 4194–4200.\n[41] Y . Ma, H. Peng, T. Khan, E. Cambria, and A. Hussain, ‘‘Sentic LSTM:\nA hybrid network for targeted aspect-based sentiment analysis,’’ Cognit.\nComput., vol. 10, no. 4, pp. 639–650, Aug. 2018.\n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nK. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[43] Y . Wu, M. Schuster, Z. Chen, Q. Le, M. Norouzi, W. Macherey, M. Krikun,\nY . Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu,\nU. Kaiser, S. Gouws, Y . Kato, T. Kudo, H. Kazawa, and J. Dean, ‘‘Google’s\nneural machine translation system: Bridging the gap between human\nand machine translation,’’ 2016, arXiv:1609.08144. [Online]. Available:\nhttps://arxiv.org/abs/1609.08144\n[44] Q. Zhang and C. Shi, ‘‘Exploiting BERT with global-local context and label\ndependency for aspect term extraction,’’ in Proc. IEEE 7th Int. Conf. Data\nSci. Adv. Analytics (DSAA), Oct. 2020, pp. 354–362.\nBRUCCE NEVES DOS SANTOS received the\nmaster’s degree in computer science from the\nFederal University of Mato Grosso do Sul, Brazil.\nHe is currently pursuing the Ph.D. degree in the\nGraduate Program in Computer Science and Com-\nputational Mathematics (PPG-CCMC) with the\nUniversity of São Paulo, Brazil. He has published\narticles in Pattern Recognition Lettersand com-\nputer science conferences. His research interests\ninclude data and text mining and machine learning.\nRICARDO MARCONDES MARCACINIreceived\nthe Ph.D. degree in computer science from the\nInstitute of Mathematics and Computer Science,\nUniversity of São Paulo, Brazil. He is currently\na Professor of computer science at the University\nof São Paulo. He has published articles in a num-\nber of international journals and conferences, such\nas Decision Support Systems,Pattern Recognition\nLetters, Journal of Information and Data Man-\nagement, International Conference on World Wide\nWeb, Web Intelligence Conference, and ACM Symposium on Document\nEngineering. His research interests include machine learning, data clustering,\nand data analytics systems.\nSOLANGE OLIVEIRA REZENDE received the\nPh.D. degree in mechanical engineering from the\nUniversity of São Paulo, Brazil, and the Ph.D.\ndegree in computer science from the University of\nMinnesota, USA. She is a Full Professor of com-\nputer science at the University of São Paulo. She\nhas published articles in a number of international\njournals, such as Pattern Recognition Letters,\nJournal of Information and Data Management,\nKnowledge-Based Systems,Intelligent Data Anal-\nysis, Information Retrieval Journal, andInformation Processing and Man-\nagement. Her research interests include data and text mining, machine\nlearning, and recommendation systems.\nVOLUME 9, 2021 91613"
}