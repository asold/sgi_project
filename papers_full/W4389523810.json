{
  "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
  "url": "https://openalex.org/W4389523810",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2330018016",
      "name": "Siyu Yuan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2934935891",
      "name": "Jiangjie Chen",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2162904798",
      "name": "Xuyang Ge",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131222654",
      "name": "Yanghua Xiao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2108868877",
      "name": "Deqing Yang",
      "affiliations": [
        "Fudan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4241881032",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4385565417",
    "https://openalex.org/W4302011175",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2963270153",
    "https://openalex.org/W3143303760",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4385572719",
    "https://openalex.org/W2294507219",
    "https://openalex.org/W1660519191",
    "https://openalex.org/W4312091865",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W572012434",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2460442863",
    "https://openalex.org/W4381104096",
    "https://openalex.org/W4389009551",
    "https://openalex.org/W4285255684",
    "https://openalex.org/W2950824039",
    "https://openalex.org/W2055666215",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3177100526",
    "https://openalex.org/W3104432410",
    "https://openalex.org/W4376167190",
    "https://openalex.org/W4385573282",
    "https://openalex.org/W4321102425",
    "https://openalex.org/W4389519325",
    "https://openalex.org/W4229954498",
    "https://openalex.org/W3031769198",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2083893165",
    "https://openalex.org/W4385430086"
  ],
  "abstract": "The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2446–2460\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBeneath Surface Similarity: Large Language Models Make Reasonable\nScientific Analogies after Structure Abduction\nSiyu Yuan♡, Jiangjie Chen♠∗, Xuyang Ge♠, Yanghua Xiao♠♣, Deqing Yang♡∗\n♡School of Data Science, Fudan University\n♠Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n♣Fudan-Aishu Cognitive Intelligence Joint Research Center\nsyyuan21@m.fudan.edu.cn, {jjchen19,xyge20,shawyh,yangdeqing}@fudan.edu.cn\nAbstract\nThe vital role of analogical reasoning in human\ncognition allows us to grasp novel concepts by\nlinking them with familiar ones through shared\nrelational structures. Despite the attention pre-\nvious research has given to word analogies, this\nwork suggests that Large Language Models\n(LLMs) often overlook the structures that un-\nderpin these analogies, raising questions about\nthe efficacy of word analogies as a measure\nof analogical reasoning skills akin to human\ncognition. In response to this, our paper intro-\nduces a task of analogical structure abduction,\ngrounded in cognitive psychology, designed to\nabduce structures that form an analogy between\ntwo systems. In support of this task, we estab-\nlish a benchmark called SCAR, containing 400\nscientific analogies from 13 distinct fields, tai-\nlored for evaluating analogical reasoning with\nstructure abduction. The empirical evidence\nunderlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mas-\ntering this task, signifying the need for future\nexploration to enhance their abilities.1\n1 Introduction\nAnalogical reasoning is one of the foundations\nof human cognition, which helps humans under-\nstand complex and unfamiliar concepts by relating\nthem to familiar ones (Gentner, 1983; Hofstadter,\n2001; Hofstadter and Sander, 2013). In cognitive\npsychology, theories like the Structure Mapping\nTheory (SMT) have been proposed to explain the\nunderlying mechanisms behind analogical reason-\ning (Gentner and Markman, 1997). According to\nSMT, individuals gain new knowledge by establish-\ning mapping relations between familiar systems\nto unfamiliar systems ( system analogy) (Bunge,\n1981). As an example in Figure 1, an engineer can\nlearn the eye cross-section by taking the analogy\n∗Corresponding authors.\n1Resources of this paper can be found athttps://github.\ncom/siyuyuan/scar.\nAperture should be mapped to Pupil since both \nare channels for light to enter.\nConcept mappings\nFilm\nDiaphragm\nAperture\nLens\nBlack paint\nRetina\nIris\nPupil\nLens\nChoroid\nSystem A: Camera (Engineering) System B: Eye (Biology)\nMaking an analogy to explain how biologically an eye \nworks to an engineer …\nFigure 1: An example of establishing an analogy be-\ntween two systems across different domains. Based\non the common relational structures, an engineer can\nabduct concept mappings to learn about the cross-\nsection of eye (right) with the help of camera structure\n(left).\nof the camera structure since both of them exhibit\ncommon relational structures.\nIn this paper, we aim to evaluate the analogical\nreasoning ability of language models (LMs) align-\ning with humans. In this regard, previous work on\nanalogical reasoning mainly focuses on word anal-\nogy (e.g., “king is to man as queen is to woman”),\nwhich does not evaluate if LMs reason about the\nanalogy between two systems in a manner akin\nto humans (Turney et al., 2003; Mikolov et al.,\n2013b; Boteanu and Chernova, 2015; Gladkova\net al., 2016; Chen et al., 2022). There has been a\nparadigm shift in the study of analogies, moving\nfrom examining word analogies between phrases\nto exploring analogies between processes (Turney,\n2008; Sultan and Shahaf, 2022), e.g., the process of\nhurricanes can be analogous to the process of vol-\ncano eruptions. However, these researches remain\nlimited to the situations within the same domains,\nleaving cross-domain exploration uncharted, and\nlack benchmarks. Large language models (LLMs,\ne.g., Brown et al., 2020; Ouyang et al., 2022; Ope-\nnAI, 2022, 2023), despite their great abilities in\n2446\nmany tasks including analogy generation (Bhavya\net al., 2022; Webb et al., 2022; Yuan et al., 2023),\nthe evaluation is limited to simple word analogies.\nLittle investigation has been done on system analo-\ngies to align with human cognition.\nIn this paper, we begin by evaluating and ana-\nlyzing the analogical reasoning ability of LLMs\non the word analogy test. Although LLMs, such\nas GPT-4 (OpenAI, 2023), exhibit exceptional per-\nformance in word analogy recognition, they often\nfail at abducing the correct structures when solv-\ning word analogies. To improve the evaluation\nand benchmarking of analogical reasoning for bet-\nter alignment with human cognitive processes, we\ndraw inspiration from SMT and propose an ana-\nlogical structure abduction task. This task aims to\nconstruct the mappings between concepts in two\nsystems based on the relational structure abduction\nto establish a system analogy.\nFor this purpose, we introduce a benchmark of\nSCientific Analogical Reasoning with structure\nabduction, i.e., SCAR, consisting of 400 system\nanalogies across 13 domains with 1600 concept\nmappings, enriched with background knowledge\nfrom Wikipedia and GPT-4 generated explanations.\nOur experiments reveal that LLMs struggle in this\ntask, but can be improved by incorporating back-\nground knowledge and explanations in a chain-of-\nthought (CoT) (Wei et al., 2022) manner.\nOur contributions are summarized as follows:\n• We demonstrate that word analogies do not ad-\nequately reflect the analogical reasoning abil-\nity of LMs to align with human cognition;\n• We propose the analogical structure abduc-\ntion task to evaluate LLMs from a cognitive\nperspective to align with humans;\n• We develop a benchmark of scientific ana-\nlogical reasoning with structure abduction,\ni.e., SCAR, and introduce a CoT prompting\nmethod to enhance model performance on this\ntask.\n2 Analogical reasoning for LLMs\n2.1 A Cognitive Perspective for Analogical\nReasoning\nWe first introduce the cognitive foundations of ana-\nlogical reasoning through the lens of Structure\nMapping Theory (SMT), a psychological frame-\nwork proposed by Gentner and Markman (1997).\nI: Word Analogy Test\nQuery riverbank:bridge\nCandidates: (A) post office:letter\n(B) floor:stairs\n(C) phone:communication\n(D) train:destination\nII: Relational Structure Identification (RSI)\nQuery riverbank:bridge::floor:stairs\nCandidates: (A) separate ( semantic opposite distractor)\n(B) linked by\n(C) link (relational opposite distractor)\n(D) adjacent (similar distractor)\nTable 1: Examples of word analogy and RSI task. We\nalso give the distractor types in RSI task for a better\nunderstanding. The true answers are highlighted.\nSMT suggests that analogy is achieved by identi-\nfying common relational structures between two\nsystems (i.e., system analogy) (Bunge, 1981; Gen-\ntner, 1983). Key components of SMT include\n1. Representation: Structured systems with con-\ncepts and relations;\n2. Mapping: Comparisons between two rep-\nresentations for commonalities, resulting in\nstructure abduction between two systems;\n3. Evaluation: Analogies are evaluated based\non the abduced structures between the two\nrepresentations.\nAn example of SMT is illustrated in Figure 1.\nThe two systems, i.e., camera and eye, can be rep-\nresented into five concepts. Based on the relational\nstructure, aperture should be mapped to pupil since\nboth are channels for light to enter. Adhering to\nthe one-to-one mapping, this process focuses on\nstructure abduction to foster comprehension across\nboth domains (Bartha, 2013).\n2.2 How are LLMs on word analogy test?\nPrevious work adopts word analogy tests ( i.e., A\nis to B as C is to D) to evaluate the analogical rea-\nsoning ability of LMs. As illustrated in Table 1\n(I), this task can be framed as a multiple-choice\nquestion-answering (QA) challenge. We adhere to\nthis paradigm and test the performance of LLMs,\ne.g., InstructGPT series (Ouyang et al., 2022), Chat-\nGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023),\non this test. We also adopt InstructGPT embed-\ndings (text-ada-embedding-002) and follow the\nmethod proposed by Ushio et al. (2021), which con-\nverts word analogies into embeddings and select\n2447\nModel k Word Analogy Test RSI Test on E-KAR\nE-KAR BATS UNIT 2 UNIT 4 Google SAT Mean Accuracy Overlap (↑)\nEmbedding - 30.53 30.24 34.65 33.56 50.40 36.69 36.01 33.25 22.40\nInstructGPT002\n0 32.44 57.78 47.80 46.99 78.40 37.39 50.13 57.50 47.50\n1 38.93 81.90 50.00 52.78 91.60 48.96 60.70 58.50 49.50\nInstructGPT003\n0 39.31 82.77 56.14 58.33 94.40 47.48 63.07 61.50 48.50\n1 41.60 88.99 62.72 63.66 98.20 57.27 68.74 65.50 50.00\nChatGPT 0 41.22 81.71 53.07 52.31 93.80 49.26 61.90 64.30 52.47\n1 44.27 81.59 59.21 55.32 94.80 55.19 65.06 68.48 53.76\nGPT-4 0 53.05 92.42 76.32 71.30 98.80 74.78 77.78 71.69 60.47\n1 60.36 93.97 84.21 81.71 100.00 83.68 83.99 78.50 64.90\nHuman - 77.80 84.85 87.50 66.66 99.41 57.00 78.87 86.43 98.70\nTable 2: Accuracy on the word analogy test and RSI test with k-shot learning. Overlap indicates the ratio of correct\nanswers on both E-KAR and RSI tests. We obtain human performance on word analogy benchmarks from the\noriginal papers. The best results are bolded, and the second best ones are underlined.\nthe answer candidate with the marginal likelihood\nbiased perplexity.\nWe use six benchmarks and design instructions\nfor LLMs to complete the tests. SAT (Turney et al.,\n2003), UNIT2 (Boteanu and Chernova, 2015) and\nUNIT4 (Boteanu and Chernova, 2015) come from\neducational resources. Google (Mikolov et al.,\n2013b) and BATS (Gladkova et al., 2016) are de-\nrived for word embedding evaluation with semantic\nand morphological relations. E-KAR (Chen et al.,\n2022) is from China’s Civil Service Examinations\nwith more complex relations and structures.2\nThe results in Table 2 show that 1) InstructGPT\nembeddings perform poorly on the word analogy\ntest; 2) GPT-4 achieves human-level performance\nand providing examples improves model perfor-\nmance; 3) Despite GPT-4 exceeding human perfor-\nmance on most word analogy benchmarks, it lags\nconsiderably behind humans on E-KAR.\nHowever, analogical reasoning relies on iden-\ntifying shared relational structures between two\nsystems, but the word analogy test does not explic-\nitly evaluate structural abduction. Thus, the word\nanalogy test may not reflect model performance in\nanalogical reasoning aligned with humans.\n2.3 Is the word analogy test aligned with\nhumans?\nTo confirm our hypothesis, we explore the discern-\ning relations between word analogies for LLMs.\nWe analyze word analogies in E-KAR to explore\nwhether LLMs can establish structural relations.\nAs shown in Table 1 (II), we define a rela-\ntional structure identification (RSI) test where\n2Details on the benchmarks and prompt templates are pro-\nvided in Appendix B.\nLLMs select the relation constituting the anal-\nogy from four options. We construct a bench-\nmark using 700 E-KAR test data, where annota-\ntors identify the correct relation of the word anal-\nogy. For distractors, annotators write the relation\nopposite to the golden relation ( relational oppo-\nsite distractor ) and the semantic opposite rela-\ntion (semantic opposite distractor). Besides, we\nconvert golden and Wikidata relations (Vrandeˇci´c\nand Krötzsch, 2014) into InstructGPT embeddings\n(text-ada-embedding-002) and calculate cosine\nsimilarity. Then, annotators select an incorrect re-\nlation with the closest semantics as a distractor\n(similar distractor). Two annotators annotate each\ndata at first, and then we hire a third annotator to\nselect a better one as the distractor. For human\nperformance, we test two undergraduates on the\nRSI test with their results averaged.\nWe evaluate LLMs on the RSI task and calculate\nthe Accuracy and Overlap. Overlap is a metric\nthat represents the ratio of data samples that are\ncorrectly identified in both tasks to the total number\nof samples correctly identified in at least one of\nthe tasks. A higher overlap suggests LMs tend to\nunderstand word analogies based on structure.\nTable 2 shows superior model performance in\nthe RSI test. However, the low overlap reveals\nLLMs doing well in the RSI test may not necessar-\nily succeed in the word analogy test. According to\nSMT, analogical reasoning is based on identifying\ncommon relational structures between two systems.\nSuch discrepancy indicates that the word analogy\ntest is not aligned with humans, and we need a new\nway of evaluating and benchmarking analogical\nreasoning to align with humans.\n2448\n3 S CAR: Scientific Analogical Reasoning\nwith Structure Abduction\n3.1 Schema for Analogies in S CAR\nIn this paper, we aim to explore the analogical\nreasoning ability of LLMs to align with humans.\nInspired by SMT, we focus on the structure abduc-\ntion of system analogy (Bunge, 1981). As shown\nin Figure 1, two systems are analogous based on\ntheir common relational structure. To construct the\nsystem analogy, concepts in System A (e.g., Cam-\nera) can be mapped into corresponding concepts in\nSystem B (e.g., Eye), forming multiple one-to-one\nconcept mappings (e.g., Aperture maps to Pupil).\nThis process facilitates analogical reasoning and\nenables a deeper understanding of both systems.\n3.2 Data Collection\nSystem Analogy Selection Given that analogi-\ncal reasoning is usually used in scientific problem-\nsolving, we construct a benchmark of SCientific\nAnalogical Reasoning with structure abduction,i.e.,\nSCAR. We recruit a team of five undergraduate stu-\ndents with different academic backgrounds to serve\nas annotators for this benchmark.\nThe annotators are provided with guidelines of\nSMT to learn about identifying potential analogies\nbased on the relational structures of systems. To\nassist our annotators, we furnish them with scien-\ntific analogies sourced through online research. 3\nThese resources contain various scientific analo-\ngies with detailed information and thus can prompt\nannotators to create system analogies with concept\nmappings. Overall, annotators manually curate 400\nsystem analogies and define mappings between\nconcepts based on their domain-specific expertise.\nWe also ask the annotators to mark the domains in\neach system analogy. Then, we remove duplicates\nto collate the benchmark and conduct a review pro-\ncess to verify the correctness and plausibility of the\nanalogies in the benchmark.\nBackground Knowledge Retrieval We incorpo-\nrate background knowledge into each system to\nfacilitate the understanding of LMs and streamline\nthe mapping process. To achieve this, we first ex-\ntract the encyclopedia abstracts from Wikipedia 4\nfor each system. Considering that abstracts may\nnot include all relevant concepts and could be\ntoo lengthy, we use ChatGPT (OpenAI, 2022)\n3The online resources are shown in Appendix C.1.\n4https://www.wikipedia.org/\nStatistic Number\nTotal system analogies 400\nSystems 632\nMappings 1615\nConcepts in analogies 3230\nDomain classes 13\nWord Analogy 3159\nDifferent mappings 1555\nDifferent concepts 2046\nAverage mappings in analogies 4.04\nAverage concept length 1.36\nAverage background length 148.81\nAverage explanation length 44.80\nTable 3: Main statistics of SCAR.\nto rewrite each abstract as the background. We\nprompt ChatGPT with human-written instructions\nto ensure that each revised background is limited\nto 500 words and encompasses all concepts in each\nsystem.5\nExplanation Generation As shown in Figure 1,\nFilm maps Retina since both capture light and trans-\nlate it into recognizable information. To rationalize\nanalogical reasoning, we design prompts for GPT-\n4 to generate explanations for each concept map-\nping.6 To ensure the quality of explanations, we\nemploy two annotators to evaluate the accuracy of\neach explanation in SCAR. The annotation results\nindicate that 69.35% of the concept mapping expla-\nnations are accurate, with Fleiss’s κ= 0.93 (Fleiss\net al., 1981). Then we ask the annotators who cre-\nated the dataset to revise the wrong explanations\n(495 in total) with their expertise, thereby guaran-\nteeing the quality of the explanations.\nBilinguality: English and Chinese To broaden\nthe scope of this work, we also develop a Chinese\nversion of SCAR through translation. We employ\nthree native Chinese-speaking annotators to refine\nthe machine translation provided by Google. Fi-\nnally, we have a bilingual SCAR benchmark.7\n3.3 S CAR Analysis\nTable 3 shows the main statistics of SCAR. SCAR\ncontains a total of 400 system analogies, with 632\nsystems and 1,614 concept mappings, indicating a\n5The instruction template to revise backgrounds is shown\nin Appendix C.3.\n6The instruction template to generate explanations is\nshown in Appendix C.4.\n7The data examples of SCAR are provided in Ap-\npendix C.5.\n2449\nBiology\nEconomy\nChemical\nComputer\nPhilosophy\nGeography\nPhysics\nEngineering\nArt\nHistoryLiterature\nFigure 2: Domain transfer in S CAR benchmark\nrich and complex analogy structure that can poten-\ntially challenge LLMs. The benchmark spans 13\ndomains for evaluating the generalizability of mod-\nels across various domains. In addition, the bench-\nmark provides backgrounds and explanations for\nconcept mappings, serving as valuable resources to\nrationalize reasoning.\nComparison with Previous Benchmarks We\ncompare SCAR to existing analogy resources by\ntransforming it into word analogies. As in Fugure 1,\nwe can obtain a word analogy,i.e., Film is to Retina\nas Aperture is to Pupil. Overall, as shown in Ta-\nble 3, there are 3,159 word analogies in SCAR,\nwhich exhibits a larger number of word analogies\nthan previous benchmarks.8\nDomain Analysis We consider the domains of\nthe two systems in each system analogy as a pair,\ne.g., (Engineering, Biology), and calculate the fre-\nquency of each pair in SCAR to derive the do-\nmain transfer distribution of SCAR, as illustrated\nin Figure 2. The Figure highlights interdisciplinary\naspects, with prominent cross-field relationships\nbetween Biology, Engineering, and Physics, em-\nphasizing their inherent inter-connection. SCAR\nshows the prevalence of within-field analogies and\nasserts the significance of promoting interdisci-\nplinary connections to foster collaborative advance-\nments in knowledge acquisition.\n3.4 Probing Task Formulation\nOur task draws inspiration from SMT, which sug-\ngests that analogical reasoning involves drawing\ncorrespondences between two systems, founded on\ntheir shared relational structure. To this end, we\ndefine the analogical structure abduction task to\n8The detailed comparison is shown in Appendix C.6\n/* Task prompt */\nFor two given systems, you are required to create an\nanalogy by matching the concepts in each system with\none another in a one-to-one mapping.\n/* Data */\nSystem A: Camera\nSystem B: Eye\nConcepts in System A:\nFilm, Diaphragm, Aperture, Lens, Black paint\nConcepts in System B:\nIris, Choroid, Lens, Retina, Pupil\n/* Question */\nQuestion: Please establish the mappings between\nconcepts. The format should be a list:\n(Concept1_SystemA, Concept1_SystemB),\n(Concept2_SystemA, Concept2_SystemB), ...\n/* Answer */\nAnswer: (Film, Retina), (Diaphragm, Iris),\n(Aperture, Pupil), (Lens, Lens), (Black paint, Choroid)\nTable 4: An instruction template for LLMs in the analog-\nical structure abduction task. Generated texts by LLMs\nare highlighted.\nexplore the analogical reasoning ability of LLMs.\nGiven two systems:\nSA = {tA\n1 ,tA\n2 ,....,t A\nn },\nSB = {tB\n1 ,tB\n2 ,....,t B\nn },\nthis task involves establish mappings between con-\ncepts {tA\ni }i and {tB\nj }j for two systems to form an\nanalogy between SA and SB. The task requires the\nunderstanding of the relational structures between\nconcepts in both systems and creating a one-to-one\nmapping between them. Table 4 shows an instruc-\ntion for LLMs to generate mappings. Our evalu-\nation assesses the accuracy of concept mappings\nand system analogy. A system analogy is deemed\ncorrect if all concept mappings between the two\nsystems are accurate.\n4 Evaluation\n4.1 Evaluation Settings\nTo minimize the impact of instruction design on\nLLMs, we create 10 different instruction templates\nfor LLMs in this task and select the best one to\nevaluate model performance. Furthermore, we also\nexplore the ability of models to use background\nknowledge and the CoT prompting (Kojima et al.,\n2022; Wei et al., 2022) with explanations in this\ntask. The templates are shown in Table 11.\n2450\nMethod Concept Acc. System Acc. Avg\nAcc.En Zh En Zh\nAlpaca 4.58 0.00 1.75 0.00 1.58\nw/ 1-shot 9.97 14.11 4.50 5.50 8.52\nVicuna 9.04 26.42 4.50 0.12 10.02\nw/ 1-shot 40.93 16.00 21.75 0.00 19.67\nInstructGPTcurie\n001 3.18 0.00 1.75 0.00 1.23\nw/ 1-shot 8.43 5.43 2.75 2.00 4.65\nInstructGPT002 51.18 37.83 36.18 25.37 37.64\nw/ 1-shot 54.71 48.82 40.25 33.50 44.32\nw/ Backg. 51.36 54.86 34.50 41.25 45.49\nw/ 1-shot+Backg. 55.46 54.94 41.30 45.11 49.20\nInstructGPT003 52.49 39.91 36.25 26.50 38.79\nw/ 1-shot 55.36 47.70 40.50 33.00 44.14\nw/ Backg. 54.95 54.76 37.25 41.75 47.18\nw/ 1-shot+Backg. 58.63 53.92 42.60 43.50 49.66\nChatGPT 66.52 66.26 46.61 52.00 57.85\nw/ 1-shot 69.99 70.33 51.25 56.75 62.08\nw/ Backg. 70.78 71.97 52.50 61.25 64.13\nw/ 1-shot+Backg. 72.80 74.03 57.39 59.25 65.87\nGPT-4 73.28 69.77 58.50 58.50 65.01\nw/ 1-shot 71.66 71.96 59.25 61.75 66.16\nw/ Backg. 75.10 73.11 63.00 62.74 68.49\nw/ 1-shot+Backg. 77.17 72.74 64.00 62.50 69.10\nHuman 85.94 88.46 83.37 86.36 86.03\nTable 5: Main Results of different LLMs. We compare\nvanilla LLMs and LLMs with one added example (w/\n1-shot) or background (w/ Backg.). The best results\nare bolded and the second best are underlined.\n4.2 Model Choices\nWe choose Alpaca (Taori et al., 2023) (7B),\nVicuna (Chiang et al., 2023) (7B), In-\nstructGPT series (Ouyang et al., 2022) ( i.e.,\nInstructGPTcurie\n001 (6.7B), InstructGPT002 (≥175B)\nand InstructGPT003 (≥175B)), ChatGPT (OpenAI,\n2022) and GPT-4 (OpenAI, 2023). Alpaca\nand Vicuna are fine-tuned from a 7B LLaMA\nmodel (Touvron et al., 2023) with instructions or\nuser-shared conversations. InstructGPT series are\nvariants of GPT-3 (Brown et al., 2020) fine-tuned\non instructions using reinforcement learning with\nhuman feedback (RLHF). ChatGPT is built on\nInstructGPT and trained on dialogue data using\nRLHF. GPT-4 is the most advanced LLM so far.\n4.3 Overall Performance\nWe first compare LLMs with 0-shot, 1-shot,\nand background knowledge ( w/ Backg. ). Due\nto the limited input length, we excluded back-\nground knowledge from Alpaca, Vicuna, and\nInstructGPTcurie\n001 . For human performance, we test\ntwo graduate students, one in liberal arts and the\n1 2 3 4 5 6 7 8 91020\n30\n40\n50\n60\n70\nTemplate id\nInstructGPT003\nw/ Step\nw/ Expl.\n(a) Acc. on InstructGPT 003.\n1 2 3 4 5 6 7 8 91020\n30\n40\n50\n60\n70\nTemplate id\nChatGPT\nw/ Step\nw/ Expl.\n(b) Acc. on ChatGPT.\n1 2 3 4 5 6 7 8 91020\n30\n40\n50\n60\n70\nTemplate id\nGPT-4\nw/ Step\nw/ Expl.\n(c) Acc. on GPT-4.\n0-shot\n1-shot\nw/ Expl.\nw/ Backg\n35\n40\n45\n50\n(d) Average Acc.\nFigure 3: Subfigures (a-c) show the system accuracy\nof LLMs enhanced with different types of CoT prompt-\ning. The average system accuracy of LLMs ( i.e.,\nInstructGPT003, ChatGPT and GPT-4) on different tem-\nplates is shown in Subfigure (d). All results are from\nthe English version of SCAR.\nother in science, with their results averaged. Result\nin Table 5 shows that: 1) GPT-4 achieves the best\nperformance across both languages, and adding an\nexample can enhance model performance. How-\never, its ability still lags behind that of humans;\n2) Smaller models perform poorly, but training\non dialogue data can improve model performance;\n3) The performance of the InstructGPT series and\nChatGPT in Chinese is improved significantly by\nadding background knowledge, highlighting the\nmodel’s struggle with domain-specific terminology\nin Chinese to affect their performance.\n4.4 Analysis\nWill step-by-step reasoning help solve SCAR?\nWe dig into the effectiveness of the CoT prompting\ntechnique for LLMs in structure abduction. We\nadopt one example with the explanations of con-\ncept mappings to induce the LLMs to generate in-\ntermediate steps in analogical reasoning (w/ Expl.).\nOne instruction template is shown in Table 11 (II).\nWe also add “Let’s think step by step” before each\nanswer (w/ Step), which improves zero-shot rea-\nsoning for LLMs (Kojima et al., 2022). We conduct\nexperiments on ten different templates to mitigate\n2451\nArt\nBiology\nChemical\nComputer\nEconomy\nEngineering\nGeography\nHistory\nLiterature\nMathematics\nPhilosophy\nPhysics\nSport\nArt\nBiology\nChemical\nComputer\nEconomy\nEngineering\nGeography\nHistory\nLiterature\nMathematics\nPhilosophy\nPhysics\nSport\n100.0 42.9 70.0 87.5 50.0 0.0 50.0 50.0 33.3 0.0 50.0 87.5 50.0\n42.9 66.7 100.0 75.0 100.0 81.8 83.3 80.0 37.5 73.9 100.0\n70.0 100.0 100.0 0.0 50.0 50.0 0.0 100.0 50.0 50.0 50.0 100.0\n87.5 75.0 100.0 50.0 42.9 80.0 0.0 100.0 83.3 100.0 0.0\n50.0 100.0 0.0 50.0 100.0 33.3 0.0 100.0 60.0 0.0\n0.0 81.8 50.0 42.9 50.0 50.0 33.3 28.6 100.0\n50.0 83.3 50.0 80.0 33.3 50.0 0.0 50.0 50.0 55.6\n50.0 80.0 0.0 0.0 0.0 50.0 0.0 81.5 33.3 100.0\n33.3 100.0 33.3 50.0 33.3\n0.0 50.0 100.0 50.0 50.0\n50.0 37.5 50.0 83.3 100.0 28.6 50.0 33.3 50.0 64.3 40.0 66.7\n87.5 73.9 50.0 100.0 60.0 100.0 55.6 50.0 40.0 71.4 0.0\n50.0 100.0100.0 0.0 0.0 100.0 66.7 0.0 100.0\n0\n20\n40\n60\n80\n100\nFigure 4: The heatmap depicts the system accuracy of\nGPT-4 across different domains.\nhuman design bias for LLMs. Results in Figure 3\n(a-c) show that: 1) CoT prompting enhances GPT-\n4 performance in structure abduction but harms\nChatGPT and InstructGPT003 performance due to\nflawed reasoning; 2) CoT prompting with expla-\nnations outperforms the “Let’s think step by step”\napproach, highlighting the importance of explana-\ntions in CoT prompting.\nDoes instruction design affect the model perfor-\nmance? To answer this question, we calculate\nthe average system accuracy of LLMs across all\ntemplates. Results in Figure 3(d) show that LLMs\nare sensitive to the instruction design. However,\nproviding an example, additional backgrounds, or\nutilizing CoT prompting can enhance the robust-\nness of LLMs to the instruction design.\nDo models behave differently for analogies\nacross various domains? The heatmap in Fig-\nure 4 shows the system accuracy of GPT-4 across\ndifferent domains.9 We find that: 1) The perfor-\nmance varies considerably for system analogies of\ndifferent domains, indicating limited sensitivity of\nLLMs to domain boundaries; 2) In certain domains,\ne.g., literature, intra-domain system analogies have\nlow accuracy, indicating LLMs may have some\ndifficulties in these fields; 3) System analogies be-\ntween similar domains (i.e., biology and chemistry)\nshow high accuracy, demonstrating the potential\nfor knowledge transfer.\n9We select the best results of GPT-4 on the English version\nof SCAR to draw the heatmap.\n/uni00000036/uni00000048/uni00000051/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni00000036/uni00000028/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000044/uni00000047/uni00000044/uni00000010\n/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000010/uni00000013/uni00000013/uni00000015\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013/uni00000036/uni0000005c/uni00000056/uni00000057/uni00000048/uni00000050/uni00000003/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000028/uni00000051/uni0000004a/uni0000004f/uni0000004c/uni00000056/uni0000004b\n/uni00000036/uni00000048/uni00000051/uni00000057/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000025/uni00000028/uni00000035/uni00000037\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni00000036/uni00000028/uni00000057/uni00000048/uni0000005b/uni00000057/uni00000010/uni00000044/uni00000047/uni00000044/uni00000010\n/uni00000048/uni00000050/uni00000045/uni00000048/uni00000047/uni00000047/uni0000004c/uni00000051/uni0000004a/uni00000010/uni00000013/uni00000013/uni00000015\n/uni00000026/uni0000004b/uni0000004c/uni00000051/uni00000048/uni00000056/uni00000048\n/uni00000030/uni00000044/uni0000005b/uni00000010/uni00000056/uni0000004c/uni00000050/uni0000004c/uni0000004f/uni00000044/uni00000055/uni0000004c/uni00000057/uni0000005c/uni0000002a/uni00000055/uni00000048/uni00000048/uni00000047/uni0000005c/uni0000002e/uni00000058/uni0000004b/uni00000051/uni00000030/uni00000058/uni00000051/uni0000004e/uni00000055/uni00000048/uni00000056\nFigure 5: The system accuracy of different em-\nbedding methods ( i.e., Sentence-BERT (Reimers\nand Gurevych, 2019), SimCSE (Gao et al., 2021),\ntext-ada-embedding-002 (Ouyang et al., 2022)).\nHow do the embedding-mapping algorithms of\nconcepts affect system analogies? We explore\ncreating system analogies based on the embeddings\nof each concept in the systems. To achieve this, we\nimplement three distinct mapping algorithms, lever-\naging the cosine similarity score of embeddings to\nfacilitate the process: 1) Max-Similarity Algorithm:\nThis algorithm maps each concept in System A\nto the concept from System B that exhibits the\nhighest cosine similarity score, implying the same\nconcept from System B can map to multiple con-\ncepts from System A; 2) Greedy Algorithm (Zhang\net al., 2000): This algorithm iteratively maps the\nconcepts with the highest cosine similarity. In each\niteration, the concepts with the highest similarity\nare mapped and excluded from further considera-\ntion, generating one-to-one mappings without over-\nall optimality; 3) Kuhn–Munkres Algorithm (Kuhn,\n1955): This combinatorial optimization algorithm\ngenerates one-to-one mappings, providing a glob-\nally optimized solution.10\nResults in Figure 5 reveal the insufficiency of\nthe max-similarity algorithm in creating viable sys-\ntem analogies in SCAR. It underscores the infer-\nence that human-like analogical reasoning does not\nrely solely on surface-level embedding similarities.\nHowever, both the Greedy and Kuhn–Munkres al-\ngorithms display enhanced performance, suggest-\ning that the lackluster results of the LLMs on the\nstructure abduction task might be attributed to their\nweak mapping capabilities. This observation in-\ndicates that human-like reasoning could employ\nembeddings alongside mapping algorithms as com-\nplementary tools to deduce system analogies.\n10Please refer to Appendix E for an in-depth explanation of\nthe employed embedding similarity methods.\n2452\n/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000013/uni00000013/uni00000016\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000013/uni00000010/uni00000036/uni0000004b/uni00000052/uni00000057\n/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037\n/uni00000013/uni00000013/uni00000016\n/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000014/uni00000010/uni00000036/uni0000004b/uni00000052/uni00000057\n/uni00000033/uni00000055/uni00000048/uni00000046/uni0000004c/uni00000056/uni0000004c/uni00000052/uni00000051/uni00000035/uni00000048/uni00000046/uni00000044/uni0000004f/uni0000004f/uni00000029/uni00000014\nFigure 6: The performance of LLMs in the open ana-\nlogical structure abduction task. All results are from the\nEnglish version of SCAR.\n4.5 Open Analogical Structure Abduction\nIn the above experiments, we evaluate LLMs’ abil-\nity of structure abduction in a close setting, where\nthe concepts of each system are given (as in Ta-\nble 4). However, a more intriguing question arises:\nCan LLMs perform analogical reasoning for sys-\ntems with an open set of concepts, where concepts\nare not explicitly given? In this case, models must\nfirst identify concepts from contexts and then gen-\nerate concept mappings to form a system analogy.\nThis open analogical structure abduction problem\nmore closely simulates the process of humans dis-\ncovering and acquiring knowledge.\nWe provide LLMs with the background descrip-\ntion texts of systems to simulate an open setting.\nLLMs are expected to retrieve concepts that can\nbe used to create mappings from backgrounds and\nthen establish concept mappings to form system\nanalogies.11 For evaluation, we automatically cal-\nculate the recall of concept mappings based on\nSCAR to measure the correctness of newly gener-\nated mappings with annotated mappings (as in the\nclose setting). Since some reasonable mappings\nmay not be included in SCAR, we need to manu-\nally evaluate the precision. We randomly sample\n50 data from SCAR and let LLMs generate con-\ncept mappings in the open setting. Two annotators\nassess the precision of the generated concept map-\npings with Fleiss’s κ = 0.86. Then F1 score can\nbe calculated based on precision and recall.\nThe results in Figure 6 show that LLMs can\nestablish some concept mappings even when con-\ncepts are not directly given. Despite the relatively\nlow recall, higher precision indicates LLMs’ abil-\nity to form new concept mappings, which can be\nutilized to further improve SCAR.\n11One instruction template is shown in the Appendix D.\n5 Related Work\nAnalogical reasoning has been an area of interest\nin the AI community, primarily focusing on word\nanalogies (Mitchell, 2021). Early researches in\nword analogy focus on evaluating the quality of\nword embeddings, which examines linear relations\nbetween words (Turney et al., 2003; Mikolov et al.,\n2013b; Gladkova et al., 2016) and can be effec-\ntively addressed through vector arithmetic for neu-\nral word embeddings such as Word2Vec (Mikolov\net al., 2013a) and GloVe (Pennington et al., 2014).\nIn recent years, some studies explore anal-\nogy understanding of LMs on various bench-\nmarks (Fournier et al., 2020; Ushio et al., 2021)\nand fine-tuned LMs using constructed knowledge\nbases of analogies to improve performance (Li\net al., 2018, 2020; Yuan et al., 2023). New word\nanalogy benchmarks with more complex relational\nstructure (Chen et al., 2022; Czinczoll et al., 2022)\nand with multimodal elements (Zhang et al., 2023)\n(Zhang et al., 2023) are built to evaluate the per-\nformance of multilingual and multimodal models.\nHowever, a gap exists between word analogy for-\nmats and the nature of analogical reasoning in hu-\nman cognition (Hofstadter, 2001; Bartha, 2013;\nGentner and Maravilla, 2017), limiting the word\nanalogy task to reflect the LLMs’ analogical rea-\nsoning ability aligning with humans.\nThere has been a paradigm shift toward explor-\ning analogies between situations (Turney, 2008;\nSultan and Shahaf, 2022). These works are in-\nspired by the SMT (Gentner, 1983), which aims\nto establish mappings between concepts in two\ndomains based on a common relational structure.\nNonetheless, Turney (2008) focuses on simple com-\nmonsense relations. Sultan and Shahaf (2022) ar-\ngue that two processes with similar questioning\nformats can be analogies, which does not address\ncomplex structures and yield unsatisfactory perfor-\nmance on discovering analogies between different\ndomains. Furthermore, some studies also explore\nthe analogy generation of LLMs (Bhavya et al.,\n2022; Yuan et al., 2023; Ding et al., 2023). How-\never, they mostly evaluate word analogies or simple\nanalogies between two sentences, leaving complex\nstructures in analogical reasoning unstudied. Webb\net al. (2022) and Hu et al. (2023) examine the ab-\nstract language-based analogy task and evaluate the\nanalogical reasoning ability of LLMs on this task.\nCompared to their task, our task requires the inten-\nsive involvement of commonsense, encyclopedic,\n2453\nand cultural (e.g., idiom and historical) knowledge.\nRecent researchers study AI alignment to guide\nAI toward achieving human preferences and ethical\nprinciples (Li et al., 2022; Rao et al., 2023; Park\net al., 2023). We explore the analogical reasoning\nability of LLMs with complex structure abduction,\nwhich is more aligned with human cognition.\n6 Conclusion\nIn this paper, we explore the analogical reasoning\nability of LLMs. We highlight word analogies ne-\nglect structures and thus can not evaluate LLMs in\nalignment with human cognition. To better evaluate\nLLMs aligning with humans, we propose an analog-\nical structure abduction task with a new benchmark,\nSCAR. Experiments show that LLMs struggle with\nthis task, but incorporating background knowledge\nand CoT prompting can improve their performance.\nWe hope the SCAR can be a valuable resource to\nadvance the research on analogical reasoning.\nLimitations\nWe only instruct LLMs to establish concept map-\npings between systems in the analogical structure\nabduction task, leaving the discovery of novel\nanalogies unexplored. Such a limitation highlights\nthe potential for future work to adopt structure ab-\nduction to uncover analogies and learn about new\nknowledge.\nAnother limitation of this work is that our eval-\nuation of the analogical structure abduction task\nrelies on concept mappings. Although this crite-\nrion aligns with humans, it remains a challenge for\nthe model. Future studies can consider designing\nmore appropriate evaluation tasks. Additionally,\nalthough we mitigated the impact of templates on\nmodel results by designing ten templates and choos-\ning the best results for evaluation, we believe there\nremains room for improvement in instruction de-\nsign to fully harness the capability of LLMs.\nEthics Statement\nAll authors of this work abide by the provided Code\nof Ethics. Annotators recruited by our institution\nannotate the system analogies in SCAR. The an-\nnotation quality is ensured through a double-check\nstrategy outlined in Section 3. We ensure that the\nprivacy rights of all annotators are respected in the\nannotation process. As described in our paper, all\nannotators are compensated above the local min-\nimum wage and consent to using the SCAR for\nresearch purposes.\nAcknowledgement\nWe thank the anonymous reviewers for their valu-\nable comments, and Yikai Zhang and Shuang Li\nfrom Fudan University for their useful suggestions\nfor the manuscript. This work is supported by the\nChinese NSF Major Research Plan (No.92270121),\nShanghai Science and Technology Innovation Ac-\ntion Plan (No.21511100401) and the Science and\nTechnology Commission of Shanghai Municipality\nGrant (No. 22511105902).\nReferences\nPaul Bartha. 2013. Analogy and analogical reasoning.\nBhavya Bhavya, Jinjun Xiong, and ChengXiang Zhai.\n2022. Analogy generation by prompting large lan-\nguage models: A case study of instructgpt. In Pro-\nceedings of the 15th International Conference on Nat-\nural Language Generation, pages 298–312, Water-\nville, Maine, USA and virtual meeting. Association\nfor Computational Linguistics.\nAdrian Boteanu and Sonia Chernova. 2015. Solving\nand explaining analogy questions using semantic net-\nworks. Proceedings of the AAAI Conference on Arti-\nficial Intelligence, 29(1).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMario Bunge. 1981. Analogy between systems. Inter-\nnational Journal Of General System, 7(4):221–223.\nJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao\nLi, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua\nXiao, and Hao Zhou. 2022. E-KAR: A benchmark\nfor rationalizing natural language analogical reason-\ning. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022 , pages 3941–3955,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\n2454\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nTamara Czinczoll, Helen Yannakoudakis, Pushkar\nMishra, and Ekaterina Shutova. 2022. Scientific\nand creative analogies in pretrained language mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 2094–2100, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nZijian Ding, Arvind Srinivasan, Stephen MacNeil, and\nJoel Chan. 2023. Fluid transformers and creative\nanalogies: Exploring large language models’ capacity\nfor augmenting cross-domain analogical creativity.\narXiv preprint arXiv:2302.12832.\nJoseph L Fleiss, Bruce Levin, Myunghee Cho Paik,\net al. 1981. The measurement of interrater agreement.\nStatistical methods for rates and proportions, 2(212-\n236):22–23.\nLouis Fournier, Emmanuel Dupoux, and Ewan Dun-\nbar. 2020. Analogies minus analogy test: measuring\nregularities in word embeddings. In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning, pages 365–375, Online. Association\nfor Computational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nDedre Gentner. 1983. Structure-mapping: A theoretical\nframework for analogy. Cognitive science, 7(2):155–\n170.\nDedre Gentner and Francisco Maravilla. 2017. Ana-\nlogical reasoning. In The Routledge International\nHandbook of Thinking and Reasoning , pages 186–\n203. Routledge.\nDedre Gentner and Arthur B Markman. 1997. Struc-\nture mapping in analogy and similarity. American\npsychologist, 52(1):45.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpholog-\nical and semantic relations with word embeddings:\nwhat works and what doesn’t. In Proceedings of the\nNAACL Student Research Workshop, pages 8–15, San\nDiego, California. Association for Computational\nLinguistics.\nDouglas R Hofstadter. 2001. Analogy as the core of\ncognition. The analogical mind: Perspectives from\ncognitive science, pages 499–538.\nDouglas R Hofstadter and Emmanuel Sander. 2013.Sur-\nfaces and essences: Analogy as the fuel and fire of\nthinking. Basic books.\nXiaoyang Hu, Shane Storks, Richard Lewis, and Joyce\nChai. 2023. In-context analogical reasoning with\npre-trained language models. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1953–1969, Toronto, Canada. Association for Com-\nputational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nHarold W Kuhn. 1955. The hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2):83–97.\nPeng-Hsuan Li, Tsan-Yu Yang, and Wei-Yun Ma. 2020.\nCA-EHN: Commonsense analogy from E-HowNet.\nIn Proceedings of the Twelfth Language Resources\nand Evaluation Conference, pages 2984–2990, Mar-\nseille, France. European Language Resources Asso-\nciation.\nShen Li, Zhe Zhao, Renfen Hu, Wensi Li, Tao Liu,\nand Xiaoyong Du. 2018. Analogical reasoning on\nChinese morphological and semantic relations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 138–143, Melbourne, Australia.\nAssociation for Computational Linguistics.\nXingxuan Li, Yutong Li, Linlin Liu, Lidong Bing, and\nShafiq Joty. 2022. Is gpt-3 a psychopath? evalu-\nating large language models from a psychological\nperspective. arXiv preprint arXiv:2212.10529.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013a. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nMelanie Mitchell. 2021. Abstraction and analogy-\nmaking in artificial intelligence. Annals of the New\nYork Academy of Sciences, 1505(1):79–101.\nOpenAI. 2022. Chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\n2455\nhuman feedback. In Advances in Neural Information\nProcessing Systems.\nPeter S Park, Philipp Schoenegger, and Chongyang Zhu.\n2023. Artificial intelligence in psychology research.\narXiv preprint arXiv:2302.07267.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nHaocong Rao, Cyril Leung, and Chunyan Miao.\n2023. Can chatgpt assess human personalities?\na general evaluation framework. arXiv preprint\narXiv:2303.01248.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nOren Sultan and Dafna Shahaf. 2022. Life is a circus\nand we are the clowns: Automatically finding analo-\ngies between situations and processes. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3547–3562,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeter D Turney. 2008. The latent relation mapping\nengine: Algorithm and experiments. Journal of Arti-\nficial Intelligence Research, 33:615–655.\nPeter D Turney, Michael L Littman, Jeffrey Bigham,\nand Victor Shnayder. 2003. Combining independent\nmodules in lexical multiple-choice problems. Re-\ncent Advances in Natural Language Processing III:\nSelected Papers from RANLP, 2003:101–110.\nAsahi Ushio, Luis Espinosa Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. BERT is to NLP\nwhat AlexNet is to CV: Can pre-trained language\nmodels identify analogies? In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3609–3624, Online. As-\nsociation for Computational Linguistics.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Communi-\ncations of the ACM, 57(10):78–85.\nTaylor Webb, Keith J Holyoak, and Hongjing Lu. 2022.\nEmergent analogical reasoning in large language\nmodels. arXiv preprint arXiv:2212.09196.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nSiyu Yuan, Jiangjie Chen, Changzhi Sun, Jiaqing Liang,\nYanghua Xiao, and Deqing Yang. 2023. Analogykb:\nUnlocking analogical reasoning of language models\nwith a million-scale knowledge base. arXiv preprint\narXiv:2305.05994.\nNingyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang,\nShumin Deng, and Huajun Chen. 2023. Multimodal\nanalogical reasoning over knowledge graphs. In The\nEleventh International Conference on Learning Rep-\nresentations.\nZheng Zhang, Scott Schwartz, Lukas Wagner, and Webb\nMiller. 2000. A greedy algorithm for aligning dna\nsequences. Journal of Computational biology, 7(1-\n2):203–214.\nA Author Contributions\nSiyu Yuan Lead the project, develop the original\nmethod and original code, lead the curation of the\ndataset, contribute to the original experiments, and\ncontribute to the original manuscript.\nJiangjie Chen Conceptualization of the origi-\nnal idea, supervision of the research activity plan-\nning and execution, contribution to the original\nmanuscript and figures, and acquisition of financial\nsupport for the project.\nXuyang Ge Contribute to the experiments and\ndata curation.\nYanghua Xiao Provide financial support for the\nproject and revise the manuscript.\nDeqing Yang Provide financial support for the\nproject, revise the manuscript, and oversee the re-\nsearch activity execution.\nB Word Analogy Test\nB.1 Benchmark\nWe compare LLMs with human performance in 6\ndifferent analogy benchmarks.\n2456\n• E-KAR (Chen et al., 2022): A knowledge-\nintensive analogical reasoning benchmark\nfrom China’s Civil Service Examinations\n(CSE), including linguistic, commonsense, en-\ncyclopedic, and cultural knowledge.\n• BATS (Gladkova et al., 2016): This bench-\nmark features over 1,000 analogies in four cat-\negories: lexicographic, encyclopedic, deriva-\ntional, and inflectional morphology.\n• UNIT2 (Boteanu and Chernova, 2015): A\nbenchmark using word analogy problems\nfrom an educational resource.\n• UNIT4 (Boteanu and Chernova, 2015): Sim-\nilar to U2, this benchmark comes from an\neducational resource but is more challenging.\n• Google (Mikolov et al., 2013b): A benchmark\nfor intrinsic evaluation of word embeddings,\ncontaining semantic and morphological rela-\ntions.\n• SAT (Turney et al., 2003): A benchmark de-\nrived from a US college admission test with\n374 word analogy problems.\nB.2 Prompt Templates for Large Language\nModels\nAs shown in Table 9, we design the instruction for\nLLMs to generate the answers.\nC Details of S CAR\nC.1 Resource of S CAR\nTo facilitate a more efficient and effective annota-\ntion process, we furnish annotators with scientific\nanalogies sourced through online research:\n• https://homework.\nstudy.com/explanation/\nwhat-is-analogy-in-science.html\n• www.csun.edu/science/ref/analogy/\nanalogy.htm\n• https://science-education-research.\ncom/teaching-science/\nconstructivist-pedagogy/\nmaking-the-unfamiliar-familiar/\nscience-analogies/\n• www.engagefastlearning.com\nData # Analogy Lang. Backg. Expl.\nSAT 374 En ✗ ✗\nGoogle 550 En ✗ ✗\nUNIT 2 252 En ✗ ✗\nUNIT 4 480 En ✗ ✗\nBATS 1998 En ✗ ✗\nE-KAR 1251 En ✗ ✓\nE-KAR 1655 Zh ✗ ✓\nSCAR 3159 En ✓ ✓\nSCAR 3159 Zh ✓ ✓\nTable 6: Comparison between SCAR and previous anal-\nogy data source: numbers of word analogies, language\n(Lang.) and whether the benchmark has background\ninformation (Backg.) and explanations (Expl.).\nFor example, “ When Rutherford (following\nNagoka) conceived of the atom as a miniature solar\nsystem – electrons circling the nucleus as planets\ncircle the sun”. These resources can prompt an-\nnotators to create system analogies with concept\nmappings.\nC.2 Crowd-sourcing Details\nWe have recruited a team of five undergraduates\nwith diverse academic backgrounds in Computer\nScience, History, Physics, Biology, and Chemistry.\nAmong them, the student majoring in Computer\nScience has a minor in Economics, while the stu-\ndent majoring in History has minors in Creative\nWriting and Philosophy. We pay each annotator\n$8/h, exceeding the local minimum wage $5/h.\nC.3 Backgroud Rewriting Template\nAs shown in Table 10 (I), we design the instruc-\ntion for ChatGPT to ensure that each revised back-\nground is limited to 500 words and encompasses\nall concepts relevant to each system.\nC.4 Explanation Generation Template\nTable 10 (II) shows the human-written instruction\nfor GPT-4 to generate the explanation for each map-\nping in SCAR.\nC.5 Data Examples of S CAR\nTable 7 presents some examples of SCAR for a\nbetter understanding.\nC.6 Comparison to Previous Benchmarks\nWe compare SCAR with the resources related to\nthe problem of analogy. As the existing bench-\nmarks are based on word analogies, we transform\nSCAR for appropriate comparisons. We combine\n2457\n1 Limit Modification =⇒ Firewall\n(Biology) (Computer)\nProkaryotes −→ Computer\nExogenous DNA −→ Virus\nRestriction Enzyme −→ Antivirus Software\nCut Off −→ Intercept\nDegradation −→ Clear\n2 Tide =⇒ Lift\n(Geography) (Engineering)\nOcean −→ Platform\nMoon −→ Console\nHigh tide −→ Rise\nEbb and Flow −→ Decline\n3 Sound =⇒ Light\n(Physics) (Physics)\nLow −→ Red\nHigh −→ Violet\nEchoes −→ Reflects\nLoud −→ Bright\nQuiet −→ Dim\nHorn −→ Lens\n4 Computer Systems =⇒ Urban\n(Computer) (Geography)\nOperating System −→ Mayor\nProcess −→ Resident\nResource manager −→ Municipal Facilities\nFile System −→ Architecture\n5 Chemistry =⇒ Cooking\n(Chemical) (Art)\nTemperature −→ Heat\nPressure −→ Firepower\nReactant Concentration −→ Food Size\nReactant −→ Raw Material\nProduct −→ Dishes\nTable 7: Some data examples of SCAR. We give the\nsystem analogies (=⇒) with concept mappings (−→).\nthe two concept mappings within the same system\nanalogy to form a word analogy. For instance, in\nFugure 1, we can obtain a word analogy, i.e., film\nis to retina as aperture is to pupil. As reported in\nTable 6, our method exhibits a larger number of\nword analogies with bilingual language.\nD Details about Analogical Structure\nAbduction Task\nWe show the instructions combining backgrounds\nand the CoT prompting with explanations in Ta-\nble 11 (I) and (II). One human-written instruction\nfor the open analogical structure abduction task is\nshown in Table 8.\nE Embedding Similarity Method\nWe convert concepts in two systems into differ-\nent embeddings with the following strategies and\ncalculate cosine similarity between concepts:\n1. Max-similarity algorithm, which establishes\na mapping with the concept from System B that\n/* Task prompt */\nFor two given systems, you are required to create an\nanalogy by extracting concepts from the backgrounds of\nsystems and matching the concepts in each system with\none another in a one-to-one mapping.\n/* Data */\nSystem A: Camera\nSystem B: Eye\nBackground of System A:\nA camera is a device that captures visual images by...\nBackground of System B:\nThe eye is a remarkable organ that allows us...\n/* Question */\nQuestion: Please extract concepts from the backgrounds\nof systems and establish the mappings between concepts.\nThe format should be a list:\n(Concept1_SystemA, Concept1_SystemB),\n(Concept2_SystemA, Concept2_SystemB), ...\n/* Answer */\nAnswer: (Film, Retina), (Diaphragm, Iris),\n(Aperture, Pupil), (Lens, Lens), (Black paint, Choroid)\nTable 8: An instruction template for LLMs in the open\nanalogical structure abduction task. Generated texts by\nLLMs are highlighted.\nexhibits the highest cosine similarity score. This\nmethod performs mapping with replacement, mean-\ning that a concept from System B can be mapped\nto multiple concepts from System A;\n2. Greedy algorithm (Zhang et al., 2000), which\niteratively maps concepts exhibiting the highest\ncosine similarity in each step. In each round of iter-\nations, we first calculate all cosine similarity scores\nbetween concepts in the two systems. Then, we\nmap the concepts with the highest cosine similarity\nscores, and the concepts that have been mapped\nwill not be considered in the next round of itera-\ntions. This strategy generates one-to-one mappings\nwhile not considering the overall optimality.\n3. Kuhn-Munkres algorithm (Kuhn, 1955), a\ncombinatorial optimization technique for solving\none-to-one mapping problems. Given a matrix C,\nwith each C[i,j] representing the cost of match-\ning vertex i(a “worker”) to vertex j(a “job”), the\nobjective is to find a minimal cost assignment of\nworkers to jobs. Let X be a boolean matrix, with\nC[i,j] = 1if and only if row iis assigned to col-\numn j. The optimal assignment cost is given by:\nmin\n∑\ni\n∑\nj\nCi,jXi,j, (1)\nwhere X is square, each row corresponds to ex-\nactly one column, and each column corresponds to\nexactly one row.\n2458\nI: Word Analogy Test II: Relational Structure Identification (RSI)\n/* Task prompt */\nFind the most analogous candidate answer\nthat follows the relations in the query.\n/* Examples */\nQuestion: broom:dustpan\nChoice:\nA: lock:key\nB: frame:lens\nC: scarf:hat\nD: toothbrush:cup\nPlease choose A, B, C or D.\nAnswer: D\n/* Test data */\nQuestion: admire:respect\nChoice:\nA: like:adore\nB: oppress:exploit\nC: spouse:husband and wife\nD: relatives:neighbors\nPlease choose A, B, C or D.\nAnswer: A\n/* Task prompt */\nWhat relationship is in the given analogy?\n/* Examples */\nQuestion: army:order:band::band leader\nChoice:\nA: govern\nB: violate\nC: obey\nD: cooperate\nPlease choose A, B, C or D.\nAnswer: C\n/* Test data */\nQuery: riverbank:bridge::floor:stairs\nChoice:\nA: separate\nB: linked by\nC: link\nD: adjacent\nPlease choose A, B, C or D.\nAnswer: B\nTable 9: Prompt templates for LLMs in word analogy task and RSI task. Generated texts by LLMs are highlighted.\nI: Background Revision\n/* Task prompt */\nGiven a description of a system with a list of concepts related to the system, please generate a short introduction of\nthe system according to the description and concepts within 500 words.\n/* Data */\nSystem: Biosphere\nDescription: The biosphere, also known as the ecosphere, is the worldwide sum of all ecosystems. It can also...\nConcepts: biology, biodiversity, ecosystem\nIntroduction: The biosphere refers to the sum of all ecosystems on Earth, including the interactions between living\norganisms and their environment...\nII: Explanation Generation\n/* Task prompt */\nGiven two systems, each of which consists of some concepts, establishing mappings between the concepts can make\nthe two systems analogous. Given mappings, please explain why such mappings can form an analogy. Hint: You can\nanswer from a structural point of view.\n/* Data */\nSystem 1: Immune System\nSystem 2: Army\nConcepts in System 1:Immune Cells, Antibody, lymphoid tissue, regulatory organs\nConcepts in System 2:Soldier, Weaponry, base, headquarters\nMappings: [Immune Cells,Soldier],[Antibody,Weaponry],[lymphoid tissue,base],[regulatory organs,headquarters]\nExplanations:\n1. Immune cells correspond to soldiers: In the immune system, immune cells (such as white blood cells) are the soldiers\nresponsible for identifying and eliminating pathogens. They can quickly recognize, attack, and clear invading pathogens.\nIn the military, soldiers are the basic force for executing tasks and defending national security. In this mapping, immune\ncells and soldiers both represent the executors of their respective systems.\n2. Antibodies correspond to weaponry: In the immune system, antibodies are specific proteins produced by immune\ncells. They can recognize and bind to pathogens, helping the immune system clear them. In the military, weaponry is the\ntool soldiers use to execute tasks. Antibodies and weaponry both represent specific tools used to accomplish tasks.\n3. Lymphoid tissue corresponds to bases: In the immune system, lymphoid tissue (such as the spleen and lymph nodes)\nis where immune cells are produced, stored, and exchanged. In the military, bases are where soldiers live, train, and\norganize tasks. In this mapping, lymphoid tissue and bases are both support facilities for their respective systems.\n4. Regulatory organs correspond to headquarters: In the immune system, regulatory organs (such as the thymus and\nbone marrow) are responsible for coordinating the production, development, and function of immune cells, ensuring the\nnormal operation of the immune system. In the military, headquarters are responsible for organizing, commanding, and\ncoordinating military actions. Regulatory organs and headquarters both represent the management and coordination\norganizations of their respective systems.\nTable 10: Prompt templates for background revision (generated by ChatGPT) and explanation generation (generated\nby GPT-4). Generated texts by ChatGPT and GPT-4 arehighlighted.\n2459\nI: Instruction Template with Background\n/* Task prompt */\nFor two given systems, you are required to create an analogy by matching the concepts in each system with one another\nin a one-to-one mapping.\n/* Data */\nSystem A: Camera. A camera is a device that captures images by exposing a photosensitive surface to light...\nSystem B: Eye. The human eye is a complex optical instrument that allows us to perceive the world around...\nConcepts in System A: Film, Diaphragm, Aperture, Lens, Black paint\nConcepts in System B: Iris, Choroid, Lens, Retina, Pupil\nPlease establish the analogies between the concepts.\nThe format should be a list: (Concept1_SystemA, Concept1_SystemB), (Concept2_SystemA, Concept2_SystemB), ...\nAnswer: (Film, Retina), (Diaphragm, Iris), (Aperture, Pupil), (Lens, Lens), (Black paint, Choroid)\nII: Instruction Template with Explanation (CoT)\n/* Task prompt */\nFor two given systems, you are required to create an analogy by matching the concepts in each system with one another\nin a one-to-one mapping.\n/* Example */\nSystem A: Immune System\nSystem B: Army\nConcepts in System A:Immune Cells, Antibody, lymphoid tissue, regulatory organs\nConcepts in System B:base, Weaponry, headquarters, Soldier\nPlease establish the analogies between the concepts.\nThe format should be a list: (Concept1_SystemA, Concept1_SystemB), (Concept2_SystemA, Concept2_SystemB), ...\nAnswer: let’s think step by step.\n1. Immune cells correspond to soldiers: In the immune system, immune cells (such as white blood cells) are the soldiers\nresponsible for identifying and eliminating pathogens. They can quickly recognize, attack, and clear invading pathogens.\nIn the military, soldiers are the basic force for executing tasks and defending national security. In this mapping, immune\ncells and soldiers both represent the executors of their respective systems.\n2. Antibodies correspond to weaponry: In the immune system, antibodies are specific proteins produced by immune\ncells. They can recognize and bind to pathogens, helping the immune system clear them. In the military, weaponry is the\ntool soldiers use to execute tasks. Antibodies and weaponry both represent specific tools used to accomplish tasks.\n3. Lymphoid tissue corresponds to bases: In the immune system, lymphoid tissue (such as the spleen and lymph nodes)\nis where immune cells are produced, stored, and exchanged. In the military, bases are where soldiers live, train, and\norganize tasks. In this mapping, lymphoid tissue and bases are both support facilities for their respective systems.\n4. Regulatory organs correspond to headquarters: In the immune system, regulatory organs (such as the thymus and\nbone marrow) are responsible for coordinating the production, development, and function of immune cells, ensuring the\nnormal operation of the immune system. In the military, headquarters are responsible for organizing, commanding, and\ncoordinating military actions. Regulatory organs and headquarters both represent the management and coordination\norganizations of their respective systems.\nTherefore, the final answer is: (Immune Cells,Soldier), (Antibody,Weaponry), (lymphoid tissue,base),\n(regulatory organs,headquarters)\n/* Test Data */\nSystem A: Camera\nSystem B: Eye\nConcepts in System A: Film, Diaphragm, Aperture, Lens, Black paint\nConcepts in System B: Iris, Choroid, Lens, Retina, Pupil\nPlease establish the analogies between the concepts.\nThe format should be a list: (Concept1_SystemA, Concept1_SystemB), (Concept2_SystemA, Concept2_SystemB), ...\nAnswer: let’s think step by step.\n1. Film corresponds to Retina: ...\nTable 11: Instruction templates for LLMs in the structure mapping abduction task. Generated texts by LLMs are\nhighlighted.\n2460",
  "topic": "Analogy",
  "concepts": [
    {
      "name": "Analogy",
      "score": 0.8828188180923462
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.699770987033844
    },
    {
      "name": "GRASP",
      "score": 0.6902706623077393
    },
    {
      "name": "Cognition",
      "score": 0.6803709268569946
    },
    {
      "name": "Task (project management)",
      "score": 0.6078864336013794
    },
    {
      "name": "Computer science",
      "score": 0.5934369564056396
    },
    {
      "name": "Analogical reasoning",
      "score": 0.5886353254318237
    },
    {
      "name": "Cognitive science",
      "score": 0.5409397482872009
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4710022509098053
    },
    {
      "name": "Raising (metalworking)",
      "score": 0.4282871186733246
    },
    {
      "name": "Cognitive psychology",
      "score": 0.39916831254959106
    },
    {
      "name": "Natural language processing",
      "score": 0.3906461000442505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3774566650390625
    },
    {
      "name": "Psychology",
      "score": 0.3177170753479004
    },
    {
      "name": "Epistemology",
      "score": 0.21591141819953918
    },
    {
      "name": "Engineering",
      "score": 0.09598168730735779
    },
    {
      "name": "Programming language",
      "score": 0.0847366452217102
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ]
}