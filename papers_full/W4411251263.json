{
  "title": "Large Language Models and the Analyses of Adherence to Reporting Guidelines in Systematic Reviews and Overviews of Reviews (PRISMA 2020 and PRIOR)",
  "url": "https://openalex.org/W4411251263",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5026706303",
      "name": "Diego A. Forero",
      "affiliations": [
        "Fundación Universitaria del Área Andina"
      ]
    },
    {
      "id": "https://openalex.org/A5069854757",
      "name": "Sandra E Abreu",
      "affiliations": [
        "Fundación Universitaria del Área Andina"
      ]
    },
    {
      "id": "https://openalex.org/A5043635672",
      "name": "Blanca Elpidia Tovar Riveros",
      "affiliations": [
        "Fundación Universitaria del Área Andina"
      ]
    },
    {
      "id": "https://openalex.org/A5025298197",
      "name": "Marilyn H. Oermann",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2588302396",
    "https://openalex.org/W2791681873",
    "https://openalex.org/W2944960005",
    "https://openalex.org/W2163834848",
    "https://openalex.org/W2052432047",
    "https://openalex.org/W2129163850",
    "https://openalex.org/W2884633725",
    "https://openalex.org/W4308293487",
    "https://openalex.org/W2341469553",
    "https://openalex.org/W3118615836",
    "https://openalex.org/W4290769249",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4402490473",
    "https://openalex.org/W4392791588",
    "https://openalex.org/W2792955190",
    "https://openalex.org/W4409483468",
    "https://openalex.org/W4387599176",
    "https://openalex.org/W4408685817",
    "https://openalex.org/W4408216127",
    "https://openalex.org/W4407957917",
    "https://openalex.org/W4402905217",
    "https://openalex.org/W4393058649",
    "https://openalex.org/W2756578555",
    "https://openalex.org/W4404224702",
    "https://openalex.org/W4306173816",
    "https://openalex.org/W2093152548",
    "https://openalex.org/W2909055311",
    "https://openalex.org/W1586187046",
    "https://openalex.org/W2079761222",
    "https://openalex.org/W2150839822",
    "https://openalex.org/W621251951",
    "https://openalex.org/W4407712009",
    "https://openalex.org/W4405636647",
    "https://openalex.org/W2912081150",
    "https://openalex.org/W4402891150",
    "https://openalex.org/W4388142096",
    "https://openalex.org/W4408108874",
    "https://openalex.org/W4407189609",
    "https://openalex.org/W4406706905",
    "https://openalex.org/W4389577521",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W4398201968",
    "https://openalex.org/W4405456257",
    "https://openalex.org/W4398256172",
    "https://openalex.org/W4392376454",
    "https://openalex.org/W4404869895",
    "https://openalex.org/W4389306395",
    "https://openalex.org/W4395052272"
  ],
  "abstract": null,
  "full_text": "RESEARCH\nJournal of Medical Systems           (2025) 49:80 \nhttps://doi.org/10.1007/s10916-025-02212-0\nIntroduction\nEvidence-Based Practice (EBP) has changed many aspects \nof the practice, research and teaching in medicine and other \nhealth sciences [ 1]. In this context, Systematic Reviews \n(SRs) and Meta-Analyses (MAs) have become cornerstones \nfor the synthesis of research findings (for different types of \nprimary studies) [ 2–4], being among the most highly cited \narticles [5] and are major inputs for clinical guidelines [6].\nMore recently, overviews of reviews, also called umbrella \nreviews [7], have emerged as novel types of articles, sum -\nmarizing the results from multiple SRs and MAs and being \nuseful for the research synthesis of entire topics and fields \n[8]. Reporting guidelines have been developed for multiple \n \r Diego A. Forero\ndforero41@areandina.edu.co\n1 School of Health and Sport Sciences, Fundación \nUniversitaria del Área Andina, Bogotá, Colombia\n2 Psychology Program, Fundación Universitaria del Área \nAndina, Medellín, Colombia\n3 Nursing Program, School of Health and Sport Sciences, \nFundación Universitaria del Área Andina, Bogotá, Colombia\n4 Duke University School of Nursing, Durham, NC, USA\nAbstract\nIn the context of Evidence-Based Practice (EBP), Systematic Reviews (SRs), Meta-Analyses (MAs) and overview of \nreviews have become cornerstones for the synthesis of research findings. The Preferred Reporting Items for Systematic \nReviews and Meta-Analyses (PRISMA) 2020 and Preferred Reporting Items for Overviews of Reviews (PRIOR) state -\nments have become major reporting guidelines for SRs/MAs and for overviews of reviews, respectively. In recent years, \nadvances in Generative Artificial Intelligence (genAI) have been proposed as a potential major paradigm shift in scientific \nresearch. The main aim of this research was to examine the performance of four LLMs for the analysis of adherence to \nPRISMA 2020 and PRIOR, in a sample of 20 SRs and 20 overviews of reviews. We tested the free versions of four com -\nmonly used LLMs: ChatGPT (GPT-4o), DeepSeek (V3), Gemini (2.0 Flash) and Qwen (2.5 Max). Adherence to PRISMA \n2020 and PRIOR was compared with scores defined previously by human experts, using several statistical tests. In our \nresults, all the four LLMs showed a low performance for the analysis of adherence to PRISMA 2020, overestimating \nthe percentage of adherence (from 23 to 30%). For PRIOR, the LLMs presented lower differences in the estimation of \nadherence (from 6 to 14%) and ChatGPT showed a performance similar to human experts. This is the first report of the \nperformance of four commonly used LLMs for the analysis of adherence to PRISMA 2020 and PRIOR. Future studies of \nadherence to other reporting guidelines will be helpful in health sciences research.\nClinical Trial Number\nNot applicable.\nKeywords Reporting guidelines · Systematic reviews · Overview of reviews · Umbrella reviews · Evidence-based \npractice · Generative artificial intelligence · Meta-research\nReceived: 28 March 2025 / Accepted: 29 May 2025\n© The Author(s) 2025\nLarge Language Models and the Analyses of Adherence to Reporting \nGuidelines in Systematic Reviews and Overviews of Reviews (PRISMA \n2020 and PRIOR)\nDiego A. Forero1 · Sandra E. Abreu2 · Blanca E. Tovar3 · Marilyn H. Oermann4\n1 3\n\nJournal of Medical Systems           (2025) 49:80 \ntypes of studies in the health sciences to ensure complete \nand transparent reporting [ 9] and in recent years, the Pre -\nferred Reporting Items for Systematic Reviews and Meta-\nAnalyses (PRISMA) 2020 statement [10] and the Preferred \nReporting Items for Overviews of Reviews (PRIOR) state -\nment [ 11] are recommended for reporting SRs/MAs and \noverviews of reviews, respectively. Although these state -\nments are robust, they require careful application and criti -\ncal evaluation, to adapt to new challenges and advances in \nthe health sciences.\nIn recent years, advances in Generative Artificial Intel -\nligence (genAI) have been proposed as a potential major \nparadigm shift in scientific research [ 12]. In this context, \nLarge Language Models (LLMs) have been explored for \nuse in several processes employed in epidemiological \nresearch [13, 14], highlighting the need for their analysis of \nadherence to major reporting guidelines in health sciences \nresearch.\nThe automated analysis of adherence of reporting guide-\nlines will be useful for meta-research works [ 15], as it will \ndecrease the time needed to carry out these labor and time-\nintensive activities (given the large number of items to be \nextracted), allowing for much larger sizes of analyzed stud-\nies [13]. Those large studies of adherence might be a major \ninput for efforts aimed at creating or modifying reporting \nguidelines [9], particularly for other type of studies (such as \nthose for -omics sciences). In addition, the automated anal -\nysis of adherence will be helpful for authors of SRs/MAs \nand overviews of reviews to easily analyze, or improve, the \nadherence in their manuscripts before submission [16].\nThe main aim of this work was to examine the per -\nformance of four LLMs for the analysis of adherence to \nPRISMA 2020 and PRIOR, in a sample of SRs and over -\nviews of reviews.\nMethods\nIn the current study, we tested the free versions of four com-\nmonly used LLMs: ChatGPT (GPT-4o) [ 17], DeepSeek \n(V3) [ 18], Gemini (2.0 Flash) [ 19] and Qwen (2.5 Max) \n[20]. Further details of the LLMs are described in Table S1. \nThese four LLMs were selected as they are broadly used, \nfree and allow the uploading of pdf files. As the chatbots \nfor these LLMs were used, there was no option to modify \nparameters, such as those minimizing randomness.\nAn initial pilot phase was carried out to create work -\ning prompts for the LLMs and for both PRISMA 2020 and \nPRIOR. It involved several phases of refinement [ 21, 22], \ntesting the improved versions of the prompts for each of the \nLLMs, and using three articles for both reporting guidelines. \nThe selected prompts were chosen after complete answers \nwere provided by the LLMs, simulating a real-word deploy-\nment [23] by health sciences researchers. The authors of this \nstudy include PhD-level experts in several fields of health \nsciences research.\nThe choice of the PRISMA 2020 and PRIOR statements \nis based on their recognition as highly used reporting guide-\nlines of SRs/MAs and overviews of reviews, respectively. \nThe exploration of the analysis of adherence by LLMs is \nof potential interest for many researchers around the world, \nas it would decrease the time needed to carry out these \nlabor and time-intensive activities (given the large number \nof items to be extracted), allowing for much larger sizes of \nanalyzed studies [24].\nThe PRISMA 2020 statement was published in 2021 [10] \nand is a widely used guideline for the reporting of SRs and \nMAs. PRISMA 2020 contains 27 items (with a total of 42 \nsubitems; compliance with a larger number of items means \na higher adherence), for the different sections of the SRs, \nsuch as Title, Abstract, Introduction, Methods, Results and \nDiscussion). It also includes a commonly used flow dia -\ngram, which indicates the number of identified and included \nprimary studies in the SRs [10].\nThe PRIOR statement was published in 2022 [ 11] and \nprovides a guideline for reporting overviews of reviews, \nparticularly related to health interventions. PRIOR contains \n27 items (with a total of 46 subitems; compliance with a \nlarger number of items means a higher adherence) and, sim-\nilar to PRISMA 2020, it involves multiple aspects related to \nthe different sections of the overviews of reviews and a flow \ndiagram [11].\nIn order to have definitions of adherence to PRISMA \n2020 and PRIOR, carried out by human experts, published \nstudies about these were searched and identified. The sup -\nplementary files of Qin et al. [ 25] (for PRISMA 2020) and \nLu et al. [26] (for PRIOR) were used to select randomly 20 \nSRs/MAs [ 27] and 20 overviews of reviews (Lists of the \nincluded studies are available in Supplementary file 1; the \nsample size of 20 has been previously used in other similar \narticles [22, 27, 28]). Qin et al. [25] and Lu et al. [26] were \nfocused on SRs and overviews of reviews in the field of acu-\npuncture and the adherence to PRISMA 2020 and PRIOR \nwere carried out by the consensus of several experts in \nhealth sciences research. These two datasets were selected \nas they provided the complete information for each one of \nthe included articles. The RAND function of the MS Excel \n365 software (Microsoft Corporation, Redmond, WA) was \nused for the generation of random numbers, to select the \nstudies to be included. The pdf files with the full text for \neach one of the selected SRs and overviews of reviews were \nuploaded (in March 2025) to each one of the LLMs, using \nthe optimized prompts (Supplementary file 2), and their \n1 3\n   80  Page 2 of 9\nJournal of Medical Systems           (2025) 49:80 \nresponses (including the explanations for their assessments) \nwere retrieved and stored.\nFor the statistical analysis, several complementary \napproaches were used [ 21]. Based on previous studies [ 29, \n30], for the determination of overall adherence for both \nPRISMA 2020 and PRIOR, each item was defined by each \nLLM as having adherence (reported), no adherence (not \nreported) or partial adherence (partially reported) and were \ncounted as 1, 0 or 0.5, respectively, as previously done \n[26]. The overall adherence percentage was defined as the \ntotal sum divided by the total number of subitems (42 for \nPRISMA 2020; 46 for PRIOR), multiplied by 100.\nThe Shapiro–Wilk test was used to explore the normality \nof the studied numerical variables [ 31]. ANOV A tests [32] \nwere carried out, followed by Tukey´s tests for adjustment \n[33], to determine statistical differences in the adherence \nscores determined by the LLMs and the human experts. \nA Pearson´s r coefficient [ 34] was calculated to determine \nthe correlation between the responses of the LLMs and the \nhuman experts. The Altman-Bland plots and tables [35] were \ncalculated to analyze in detail the agreement between each \nof the LLMs and the human experts (the mean differences \nand the confidence interval: +/- 1.96 SDs of the differences). \nAdditionally, as previously described [36], the accuracy for \neach of the LLMs was also estimated. A p value < 0.05 was \ndefined as statistically significant.\nIn addition, the calculation of adherence percentages to \nsections of PRISMA 2020 and PRIOR (such as for Meth -\nods and Results, among others) were also carried out and \nsome examples of texts were extracted to visualize details \nof the responses generated by the LLMs. We did not calcu-\nlate parameters such as specificity and sensitivity as they are \nused for topics with dichotomic variables [ 37]. The JASP \nprogram (version 0.18.3.0) [ 38] was employed for the sta -\ntistical analyses. For the reporting of this study, we took into \nconsideration key aspects of the MI-CLEAR-LLM guide -\nlines [39].\nResults\nFor the analysis of the adherence to PRISMA 2020, our \nanalysis identified that there was a higher percentage of \nadherence in the responses defined by all the four LLMs, \nin comparison to human subjects: this is shown in box plots \n(Fig. 1A) and in detailed plots for each one of the included \nSRs (Fig. 1B). A statistical analysis showed that there were \nsignificant differences between each of the LLMs compared \nto the human experts and that there were no large corre -\nlations to the scores defined by human experts (Table 1). \nAn analysis of agreement, using the Bland-Altman plots \nand tables, showed that the differences between the LLMs \nand the human experts were large: on average, from 23.1 \nto 29.7% (Table  1; Fig. 2). The accuracy for each one of \nthe LLMs, for PRISMA 2020, complemented the previ -\nously described analyses (Table 3). Overall, these results for \nPRISMA 2020 indicate a poor performance for all the four \ntested LLMs.\nAn analysis of the adherence to sections of the PRISMA \n2020 also found that the differences were large for the \nmultiple sections of items (Table S2) and examples of text \nshowed the differences in the texts of the responses of the \nLLMs (Table S4).\nRegarding the PRIOR statement, our analysis showed \nthat, in comparison to the results from PRISMA 2020, there \nwas better concordance for the adherence defined by the \nLLMs, in comparison to human experts: this is shown in \nbox plots (Fig. 3A) and in detailed plots for each one of \nthe included overviews of reviews (Fig. 3B). A statistical \nanalysis showed that there were no significant differences \nbetween the responses generated by ChatGPT and the \nscores defined by the human experts and that there was also \na significant correlation for ChatGPT and human experts \n(Table 2). The analysis of agreement, using the Bland-Alt -\nman plots and tables, showed that the differences between \nthe ChatGPT and the human experts were smaller: on aver-\nage, 6.1 (Table 2; Fig. 4). The accuracy for each one of the \nLLMs, for PRIOR, complemented the previously described \nanalyses (Table 3). Overall, these results for PRIOR indicate \na poor performance for three of the four tested LLMs.\nFinally, the analysis of the adherence to sections of the \nPRIOR found that the differences, between ChatGPT and \nthe human experts, were smaller for the Results and Discus-\nsion sections (Table S3) and examples of text also showed \nthe differences in the texts of the responses of the LLMs \n(Table S5).\nDiscussion\nThis is the first report of the performance of four commonly \nused LLMs for the automatic identification of adherence \nto PRISMA 2020 and PRIOR, in a sample of SRs/MAs \nand overviews of reviews. Among these two statements, \nPRISMA 2020 [10] is one of the most used and highly cited \nreporting guidelines in the health sciences, with more than \n86,000 citations to date.\nIn our results, all the four LLMs showed a low perfor -\nmance for the analysis of adherence to PRISMA 2020, in \ncomparison to human experts. In general, the LLMs overes-\ntimated the percentage of adherence (in addition to exhibit-\ning low accuracy), and it was more evident for SRs with \na low percentage. In contrast, for PRIOR, the LLMs pre -\nsented lower differences in the estimation of adherence and \n1 3\nPage 3 of 9    80 \nJournal of Medical Systems           (2025) 49:80 \nmodels in health sciences research [42] is a major challenge \nwhen there is a lack of available information about the arti-\ncles used for the training of LLMs [ 43, 44]. In addition, it \nis possible that some advanced uses of LLMs, such as the \nevaluation of adherence to PRISMA 2020, require a better \nperformance of the LLMs in functional linguistic compe -\ntence [45].\nChatGPT showed a performance similar to human experts. \nOur current findings are consistent with the results of a \nrecent scoping review highlighting that general-purpose \nLLMs are not ready for use in research synthesis [24].\nA current major challenge, from the perspective of health \nsciences research, is the presence of errors, commonly \ndefined as hallucinations or confabulations, in the outputs \nof LLMs [ 40, 41]. Transparence about the use of genAI \nTable 1 Analysis of the performance of four LLMs for the analysis of adherence to PRISMA 2020, in a sample of SRs, in comparison to human \nexperts\nMeasurement Adherence (a) p value (b) Correlation (c) Bland-Altman (d)\nHuman Experts 61.2 (11.2) Taken as reference Taken as reference Taken as reference\nChatGPT 90.0 (7.0) < 0.001 0.60 (0.005) 29.7 (12.2–47.2)\nDeepSeek 90.6 (6.8) < 0.001 0.43 (0.06) 29.4 (9.2–49.6)\nGemini 84.3 (8.5) < 0.001 0.36 (0.12) 23.2 (0.9–45.4)\nQwen 89.2 (5.6) < 0.001 0.39 (0.09) 28.1 (7.7–48.4)\n(a) Presented as mean (SD) of overall adherence percentages. (b) Results from an ANOVA test, with Tukey´s adjustment, in comparison with \nhuman experts. (c) Pearson’s correlation (p value in parenthesis), in comparison with human experts. (d) Results from a Bland-Altman analysis: \nMean differences (range of difference in parenthesis), in comparison with human experts\nFig. 1 Analysis of the perfor-\nmance of four LLMs for the esti-\nmation of adherence to PRISMA \n2020, in a sample of SRs, in \ncomparison to human experts. A. \nA box plot for the overall adher-\nence percentages to PRISMA \n2020 estimated by the four LLMs \nand by human experts. B. A \ndetailed plot of the adherence to \nPRISMA 2020, estimated by each \nof the four LLMs and the human \nexperts, for each one of the SRs \nincluded\n \n1 3\n   80  Page 4 of 9\nJournal of Medical Systems           (2025) 49:80 \nstudies and data [ 24, 51] are needed. Finally, as the major -\nity of research on LLMs and health sciences research has \nbeen carried out in the Global North [52], there is a need for \nfurther studies in these topics done in the Global South [43].\nSome recent articles have explored the results from LLMs \nfor related tools. Woelfle et al. explored five versions of \nthree LLMs (including Claude and ChatGPT) for the anal -\nysis of PRISMA 2009 (an older version of PRISMA; this \nstudy was focused in its current version: PRISMA 2020) in \na sample of SRs; they found that the accuracy for the LLMs \nrange from 63 to 70% [13]. Roberts et al. explored ChatGPT \n3 for the analysis of the CONSORT-A guidelines and found \nsmall differences with scores defined by humans [21]. Other \nrecent studies have focused on LLMs and analysis of risk of \nbias in SRs/MAs and in primary studies [46–48].\nLimitations of the current study include the homogenous \nnature and relatively small sample size of the included stud-\nies [22, 28]. Future studies need the use of more heteroge -\nneous and larger samples of studies and the testing of the \npotential effect of more complex prompts [49].\nFuture studies of adherence to other reporting guidelines, \nincluding additional LLMs, will be helpful in health sciences \nresearch. Additionally, future studies of LLMs designed, or \nfine-tuned [50], for advanced analyses of epidemiological \nFig. 2 Bland-Altman plots for the concordance of determination of adherence to the PRISMA 2020 statement (overall adherence percentages), for \nfour LLMs in comparison to human experts\n \n1 3\nPage 5 of 9    80 \nJournal of Medical Systems           (2025) 49:80 \nMeasurement Adherence (a) p value (b) Correlation (c) Bland-Altman (d)\nHuman Experts 57.1 (10.1) Taken as reference Taken as reference Taken as reference\nChatGPT 63.2 (9.9) 0.27 0.65 (0.002) 6.1 (-10.3-22.5)\nDeepSeek 71.0 (10.3) < 0.001 0.66 (0.001) 14.0 (-2.5-30.4)\nGemini 65.8 (8.7) 0.04 0.41 (0.07) 8.7 (-11.4-28.8)\nQwen 68.0 (9.0) 0.005 0.75 (< 0.001) 10.9 (-2.5-24.3)\n(a) Presented as mean (SD) of overall adherence percentages. (b) Results from an ANOVA test, with \nTukey´s adjustment, in comparison with human experts. (c) Pearson’s correlation ( p value in parenthesis), \nin comparison with human experts. (d) Results from a Bland-Altman analysis: Mean differences (range of \ndifference in parenthesis), in comparison with human experts\nTable 2 Analysis of the perfor-\nmance of four LLMs for the \nanalysis of adherence to PRIOR, \nin a sample of overviews of \nreviews, in comparison to human \nexperts\n \nFig. 3 Analysis of the perfor-\nmance of four LLMs for the \nestimation of adherence to \nPRIOR, in a sample of overviews \nof reviews, in comparison to \nhuman experts. A. A box plot for \nthe overall adherence percentages \nto PRIOR estimated by the four \nLLMs and by human experts. B. \nA detailed plot of the adherence \nto PRISMA 2020, estimated by \neach of the four LLMs and the \nhuman experts, for each one \nof the overviews of reviews \nincluded\n \n1 3\n   80  Page 6 of 9\nJournal of Medical Systems           (2025) 49:80 \nthe public, commercial, or not-for-profit sectors.\nData Availability Data is provided within the manuscript or supple -\nmentary information files.\nDeclarations\nResearch Involving Human Participants and/or Animals  This manu -\nscript describes only computational analyses and did not describe re -\nsearch involving human participants or animal models.\nDeclaration of generative AI in Scientific Writing  No generative AI \ntools were used for scientific writing.\nCompeting Interests The authors declare no competing interests.\nOpen Access   This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nSupplementary Information  The online version contains \nsupplementary material available at  h t t p  s : /  / d o i  . o  r g /  1 0 . 1  0 0 7  / s 1  0 9 1 6 - 0 \n2 5 - 0 2 2 1 2 - 0.\nAcknowledgements DAF has been previously supported by research \ngrants from Minciencias and Areandina.\nAuthor Contributions DAF and SEA extracted the information. DAF \nand BET analyzed the data. DAF wrote a first version of the manu -\nscript. SEA, BET and MHO contributed to the final manuscript. All \nauthors reviewed the manuscript.\nFunding Open Access funding provided by Colombia Consortium\nThis research received no specific grant from any funding agency in \nTable 3 Mean accuracy of four LLMs for the automatic identification \nof adherence to the PRISMA 2020 (upper part) and PRIOR (lower \npart) statements\nPRISMA 2020\nLLM Gemini DeepSeek ChatGPT QWEN\nMean (SD) 70.4 (9.2) 67.3 (9.0) 61.8 (9.3) 60.5 (9.3)\nPRIOR\nLLM DeepSeek Gemini QWEN ChatGPT\nMean (SD) 74.0 (7.2) 72.2 (7.0) 70.8 (7.0) 67.9 (8.2)\nFig. 4 Bland-Altman plots for the concordance of determination of adherence to the PRIOR statement (overall adherence percentages), for four \nLLMs in comparison to human experts\n \n1 3\nPage 7 of 9    80 \nJournal of Medical Systems           (2025) 49:80 \n20. Yang A, Yang B, Zhang B, Hui B, Zheng B, Yu B, et al. Qwen2. 5 \ntechnical report. arXiv (preprint). 2024.\n21. Roberts RH, Ali SR, Hutchings HA, Dobbs TD, Whitaker IS. \nComparative study of ChatGPT and human evaluators on the \nassessment of medical literature according to recognised report -\ning standards. BMJ Health Care Inform. 2023;30(1).\n22. Wrightson JG, Blazey P, Moher D, Khan KM, Ardern CL. GPT \nfor RCTs? Using AI to determine adherence to clinical trial \nreporting guidelines. BMJ Open. 2025;15(3):e088735.\n23. Srinivasan A, Berkowitz J, Kivelson S, Friedrich N, Tatonetti \nN. Evaluating the Reporting Quality of 21,041 Randomized \nControlled Trial Articles. Preprint, medRxiv. 2025:2025.03. \n06.25323528.\n24. Lieberum JL, Tows M, Metzendorf MI, Heilmeyer F, Siemens W, \nHaverkamp C, et al. Large language models for conducting sys -\ntematic reviews: on the rise, but not yet ready for use-a scoping \nreview. J Clin Epidemiol. 2025;181:111746.\n25. Qin C, Ma H, Mandizadza OO, Xu X, Ji C. Reporting quality \nof meta-analyses in acupuncture: Investigating adherence to the \nPRISMA statement. Medicine (Baltimore). 2024;103(39):e39933.\n26. Lu T, Liu B, Lu C, Du Z, Yang K, Ge L. Reporting quality of \nacupuncture overviews: A methodological investigation based on \nthe PRIOR statement. Complement Ther Med. 2024;82:103034.\n27. Shea BJ, Reeves BC, Wells G, Thuku M, Hamel C, Moran J, et \nal. AMSTAR 2: a critical appraisal tool for systematic reviews \nthat include randomised or non-randomised studies of healthcare \ninterventions, or both. BMJ. 2017;358:j4008.\n28. Alharbi F, Asiri S. Automated Assessment of Reporting Com -\npleteness in Orthodontic Research Using LLMs: An Observa -\ntional Study. Applied Sciences. 2024;14(22):10323.\n29. Innocenti T, Feller D, Giagio S, Salvioli S, Minnucci S, \nBrindisino F, et al. Adherence to the PRISMA statement and its \nassociation with risk of bias in systematic reviews published in \nrehabilitation journals: A meta-research study. Braz J Phys Ther. \n2022;26(5):100450.\n30. Panic N, Leoncini E, de Belvis G, Ricciardi W, Boccia S. Evalu-\nation of the endorsement of the preferred reporting items for sys-\ntematic reviews and meta-analysis (PRISMA) statement on the \nquality of published systematic review and meta-analyses. PLoS \nOne. 2013;8(12):e83138.\n31. Mishra P, Pandey CM, Singh U, Gupta A, Sahu C, Keshri A. \nDescriptive statistics and normality tests for statistical data. Ann \nCard Anaesth. 2019;22(1):67–72.\n32. Bewick V , Cheek L, Ball J. Statistics review 9: one-way analysis \nof variance. Crit Care. 2004;8(2):130–6.\n33. McHugh ML. Multiple comparison analysis testing in ANOV A. \nBiochem Med (Zagreb). 2011;21(3):203–9.\n34. Bewick V , Cheek L, Ball J. Statistics review 7: Correlation and \nregression. Crit Care. 2003;7(6):451–9.\n35. Giavarina D. Understanding Bland Altman analysis. Biochem \nMed (Zagreb). 2015;25(2):141–51.\n36. Li Z, Luo X, Yang Z, Zhang H, Wang B, Ge L, et al. RAPID: \nReliable and efficient Automatic generation of submission rePort-\ning checklists with Large language moDels. Preprint, bioRxiv. \n2025:2025.02. 13.638015.\n37. Gartlehner G, Kahwati L, Nussbaumer-Streit B, Crotty K, \nHilscher R, Kugley S, et al. From promise to practice: challenges \nand pitfalls in the evaluation of large language models for data \nextraction in evidence synthesis. BMJ Evid Based Med. 2024.\n38. Love J, Selker R, Marsman M, Jamil T, Dropmann D, Verhagen J, \net al. JASP: Graphical statistical software for common statistical \ndesigns. Journal of Statistical Software. 2019;88:1–17.\n39. Park SH, Suh CH, Lee JH, Kahn CE, Moy L. Minimum Report -\ning Items for Clear Evaluation of Accuracy Reports of Large \nLanguage Models in Healthcare (MI-CLEAR-LLM). Korean J \nRadiol. 2024;25(10):865–8.\nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit  h t t p :   /  / c r e a t i  v e c  o m m o  n  s .  o \nr   g / l i c e n s  e s /  b  y / 4 . 0 /.\nReferences\n1. Djulbegovic B, Guyatt GH. Progress in evidence-based medicine: \na quarter century on. Lancet. 2017;390(10092):415–23.\n2. Gurevitch J, Koricheva J, Nakagawa S, Stewart G. Meta-\nanalysis and the science of research synthesis. Nature. \n2018;555(7695):175–82.\n3. Forero DA, Lopez-Leon S, Gonzalez-Giraldo Y , Bagos PG. Ten \nsimple rules for carrying out and writing meta-analyses. PLoS \nComput Biol. 2019;15(5):e1006922.\n4. Chalmers I, Hedges LV , Cooper H. A brief history of research \nsynthesis. Eval Health Prof. 2002;25(1):12–37.\n5. Patsopoulos NA, Analatos AA, Ioannidis JP. Relative citation \nimpact of various study designs in the health sciences. JAMA. \n2005;293(19):2362–6.\n6. Murad MH, Montori VM, Ioannidis JP, Jaeschke R, Devereaux \nPJ, Prasad K, et al. How to read a systematic review and meta-\nanalysis and apply the results to patient care: users’ guides to the \nmedical literature. JAMA. 2014;312(2):171–9.\n7. Fusar-Poli P, Radua J. Ten simple rules for conducting umbrella \nreviews. Evid Based Ment Health. 2018;21(3):95–100.\n8. Bougioukas KI, Pamporis K, V ounzoulaki E, Karagiannis T, \nHaidich AB. Types and associated methodologies of overviews \nof reviews in health care: a methodological study with published \nexamples. J Clin Epidemiol. 2023;153:13–25.\n9. Altman DG, Simera I. A history of the evolution of guidelines \nfor reporting medical research: the long road to the EQUATOR \nNetwork. J R Soc Med. 2016;109(2):67–77.\n10. Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, \nMulrow CD, et al. The PRISMA 2020 statement: an updated \nguideline for reporting systematic reviews. BMJ. 2021;372:n71.\n11. Gates M, Gates A, Pieper D, Fernandes RM, Tricco AC, Moher \nD, et al. Reporting guideline for overviews of reviews of health -\ncare interventions: development of the PRIOR statement. BMJ. \n2022;378:e070849.\n12. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan \nTF, Ting DSW. Large language models in medicine. Nat Med. \n2023;29(8):1930–40.\n13. Woelfle T, Hirt J, Janiaud P, Kappos L, Ioannidis JPA, Hemkens \nLG. Benchmarking Human-AI collaboration for common evi -\ndence appraisal tools. J Clin Epidemiol. 2024;175:111533.\n14. Khraisha Q, Put S, Kappenberg J, Warraitch A, Hadfield K. Can \nlarge language models replace humans in systematic reviews? \nEvaluating GPT-4’s efficacy in screening and extracting data \nfrom peer-reviewed and grey literature in multiple languages. Res \nSynth Methods. 2024;15(4):616–26.\n15. Ioannidis JPA. Meta-research: Why research on research matters. \nPLoS Biol. 2018;16(3):e2005468.\n16. Collins GS. Innovative solutions are needed to overcome \nimplementation barriers to using reporting guidelines. BMJ. \n2025;389:r718.\n17. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, \net al. Gpt-4 technical report. arXiv (preprint). 2023.\n18. Liu A, Feng B, Xue B, Wang B, Wu B, Lu C, et al. Deepseek-v3 \ntechnical report. arXiv (preprint). 2024.\n19. Gemini Team, Anil R, Borgeaud S, Alayrac J-B, Yu J, Soricut \nR, et al. Gemini: a family of highly capable multimodal models. \narXiv (preprint). 2023.\n1 3\n   80  Page 8 of 9\nJournal of Medical Systems           (2025) 49:80 \n48. Barsby J, Hume S, Lemmey HA, Cutteridge J, Lee R, Bera KD. \nPilot study on large language models for risk-of-bias assessments \nin systematic reviews: A(I) new type of bias? BMJ Evid Based \nMed. 2025;30(1):71–4.\n49. Lin Z. How to write effective prompts for large language models. \nNat Hum Behav. 2024;8(4):611–5.\n50. Anisuzzaman D, Malins JG, Friedman PA, Attia ZI. Fine-Tuning \nLarge Language Models for Specialized Use Cases. Mayo Clin \nProc Digit Health. 2025;3(1):100184.\n51. Cox LA, Jr. An AI assistant to help review and improve causal \nreasoning in epidemiological documents. Glob Epidemiol. \n2024;7:100130.\n52. Meng X, Yan X, Zhang K, Liu D, Cui X, Yang Y , et al. The appli-\ncation of large language models in medicine: A scoping review. \niScience. 2024;27(5):109713.\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional affiliations.\n40. Smith AL, Greaves F, Panch T. Hallucination or Confabulation? \nNeuroanatomy as metaphor in Large Language Models. PLOS \nDigit Health. 2023;2(11):e0000388.\n41. Kim Y , Jeong H, Chen S, Li SS, Lu M, Alhamoud K, et al. Medi-\ncal Hallucination in Foundation Models and Their Impact on \nHealthcare. medRxiv (Preprint). 2025:2025.02. 28.25323115.\n42. Miao BY , Chen IY , Williams CYK, Davidson J, Garcia-Agundez \nA, Sun S, et al. The MI-CLAIM-GEN checklist for generative \nartificial intelligence in health. Nat Med. 2025.\n43. Lobentanzer S, Feng S, Bruderer N, Maier A, BioChatter C, \nWang C, et al. A platform for the biomedical application of large \nlanguage models. Nat Biotechnol. 2025;43(2):166–9.\n44. Nguyen T. ChatGPT in Medical Education: A Precursor for Auto-\nmation Bias? JMIR Med Educ. 2024;10:e50174.\n45. Mahowald K, Ivanova AA, Blank IA, Kanwisher N, Tenenbaum \nJB, Fedorenko E. Dissociating language and thought in large lan-\nguage models. Trends Cogn Sci. 2024;28(6):517–40.\n46. Lai H, Ge L, Sun M, Pan B, Huang J, Hou L, et al. Assessing the \nRisk of Bias in Randomized Clinical Trials With Large Language \nModels. JAMA Netw Open. 2024;7(5):e2412687.\n47. Kuitunen I, Ponkilainen VT, Liukkonen R, Nyrhi L, Pakarinen O, \nVaajala M, et al. Evaluating the Performance of ChatGPT-4o in \nRisk of Bias Assessments. J Evid Based Med. 2024;17(4):700–2.\n1 3\nPage 9 of 9    80 ",
  "topic": "Health informatics",
  "concepts": [
    {
      "name": "Health informatics",
      "score": 0.664601743221283
    },
    {
      "name": "Systematic review",
      "score": 0.6582055687904358
    },
    {
      "name": "Computer science",
      "score": 0.4783608317375183
    },
    {
      "name": "Data science",
      "score": 0.4363616704940796
    },
    {
      "name": "Management science",
      "score": 0.33368003368377686
    },
    {
      "name": "MEDLINE",
      "score": 0.30831778049468994
    },
    {
      "name": "Medicine",
      "score": 0.26772981882095337
    },
    {
      "name": "Public health",
      "score": 0.11812883615493774
    },
    {
      "name": "Engineering",
      "score": 0.1073424220085144
    },
    {
      "name": "Nursing",
      "score": 0.09152674674987793
    },
    {
      "name": "Political science",
      "score": 0.07877910137176514
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103977",
      "name": "Fundación Universitaria del Área Andina",
      "country": "CO"
    },
    {
      "id": "https://openalex.org/I170897317",
      "name": "Duke University",
      "country": "US"
    }
  ],
  "cited_by": 1
}