{
    "title": "LessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models",
    "url": "https://openalex.org/W4403333866",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2126210742",
            "name": "Hao-Xiang Fan",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A4284491729",
            "name": "Guanzheng Chen",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        },
        {
            "id": "https://openalex.org/A2074490930",
            "name": "Xingbo Wang",
            "affiliations": [
                "Cornell University"
            ]
        },
        {
            "id": "https://openalex.org/A2139126265",
            "name": "Zhenhui Peng",
            "affiliations": [
                "Sun Yat-sen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2025741896",
        "https://openalex.org/W4381886504",
        "https://openalex.org/W2805896868",
        "https://openalex.org/W4281611690",
        "https://openalex.org/W3006837908",
        "https://openalex.org/W4319746602",
        "https://openalex.org/W4284670538",
        "https://openalex.org/W1996683840",
        "https://openalex.org/W2735212770",
        "https://openalex.org/W4398886010",
        "https://openalex.org/W4393970695",
        "https://openalex.org/W4390268440",
        "https://openalex.org/W3126063179",
        "https://openalex.org/W4312842259",
        "https://openalex.org/W4392210665",
        "https://openalex.org/W4319167005",
        "https://openalex.org/W2044140042",
        "https://openalex.org/W4385682544",
        "https://openalex.org/W4313451803",
        "https://openalex.org/W4220747294",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4365601419",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4366547384",
        "https://openalex.org/W4393178543",
        "https://openalex.org/W4388817859",
        "https://openalex.org/W4360793310",
        "https://openalex.org/W2399499702",
        "https://openalex.org/W4366729084",
        "https://openalex.org/W4381982883",
        "https://openalex.org/W3204805294",
        "https://openalex.org/W2149115301",
        "https://openalex.org/W2780209091",
        "https://openalex.org/W2791132710",
        "https://openalex.org/W2979380221",
        "https://openalex.org/W4366549767",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W27311267",
        "https://openalex.org/W4309125528",
        "https://openalex.org/W4221055872",
        "https://openalex.org/W4221164056",
        "https://openalex.org/W4382398800",
        "https://openalex.org/W4393970811",
        "https://openalex.org/W211642610",
        "https://openalex.org/W1881549354"
    ],
    "abstract": "Preparing a lesson plan, e.g., a detailed road map with strategies and\\nmaterials for instructing a 90-minute class, is beneficial yet challenging for\\nnovice teachers. Large language models (LLMs) can ease this process by\\ngenerating adaptive content for lesson plans, which would otherwise require\\nteachers to create from scratch or search existing resources. In this work, we\\nfirst conduct a formative study with six novice teachers to understand their\\nneeds for support of preparing lesson plans with LLMs. Then, we develop\\nLessonPlanner that assists users to interactively construct lesson plans with\\nadaptive LLM-generated content based on Gagne's nine events. Our\\nwithin-subjects study (N=12) shows that compared to the baseline ChatGPT\\ninterface, LessonPlanner can significantly improve the quality of outcome\\nlesson plans and ease users' workload in the preparation process. Our expert\\ninterviews (N=6) further demonstrate LessonPlanner's usefulness in suggesting\\neffective teaching strategies and meaningful educational resources. We discuss\\nconcerns on and design considerations for supporting teaching activities with\\nLLMs.\\n",
    "full_text": "LessonPlanner: Assisting Novice Teachers to Prepare\nPedagogy-Driven Lesson Plans with Large Language Models\nHaoxiang Fan\nfanhx6@mail2.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nGuanzheng Chen\nchengzh59@mail2.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nXingbo Wang\nxiw4011@med.cornell.edu\nCornell University\nNew York, NY, United States\nZhenhui Peng‚àó\npengzhh29@mail.sysu.edu.cn\nSun Yat-sen University\nGuangzhou, China\nABSTRACT\nPreparing a lesson plan, e.g., a detailed road map with strategies\nand materials for instructing a 90-minute class, is beneficial yet\nchallenging for novice teachers. Large language models (LLMs) can\nease this process by generating adaptive content for lesson plans,\nwhich would otherwise require teachers to create from scratch or\nsearch existing resources. In this work, we first conduct a forma-\ntive study with six novice teachers to understand their needs for\nsupport of preparing lesson plans with LLMs. Then, we develop\nLessonPlanner that assists users to interactively construct lesson\nplans with adaptive LLM-generated content based on Gagne‚Äôs nine\nevents. Our within-subjects study (ùëÅ = 12) shows that compared\nto the baseline ChatGPT interface, LessonPlanner can significantly\nimprove the quality of outcome lesson plans and ease users‚Äô work-\nload in the preparation process. Our expert interviews ( ùëÅ = 6)\nfurther demonstrate LessonPlanner‚Äôs usefulness in suggesting ef-\nfective teaching strategies and meaningful educational resources.\nWe discuss concerns on and design considerations for supporting\nteaching activities with LLMs.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíUser interface toolkits ; ‚Ä¢\nComputing methodologies ‚ÜíNatural language generation ;\n‚Ä¢ Applied computing ‚ÜíComputer-assisted instruction .\nKEYWORDS\nLarge language models, lesson plan preparation, pedagogy-driven\nsystem\n‚àóCorresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0628-8/24/10. . . $15.00\nhttps://doi.org/10.1145/3654777.3676390\nFigure 1: An example lesson plan preparation process. We\nbuild an interactive system LessonPlanner to assist teachers\nwith generated content to prepare a documented lesson plan,\nwhich can be optionally transformed into slides by users and\ndelivered in their courses.\nACM Reference Format:\nHaoxiang Fan, Guanzheng Chen, Xingbo Wang, and Zhenhui Peng. 2024.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Les-\nson Plans with Large Language Models. In The 37th Annual ACM Sym-\nposium on User Interface Software and Technology (UIST ‚Äô24), October 13‚Äì\n16, 2024, Pittsburgh, PA, USA. ACM, New York, NY, USA, 20 pages. https:\n//doi.org/10.1145/3654777.3676390\n1 INTRODUCTION\nEffective teaching often stems from a well-prepared and organized\nlesson plan [67], which serves as a teacher‚Äôs detailed description\nof the course of instruction or learning [36]. In this paper, we fo-\ncus on the process of preparing a documented plan that normally\ndescribes the teaching flow and materials [49] for each lesson (e.g.,\n45-minute or 90-minute). Such a documented plan can serve as the\nbasis for users to optionally create slides in PowerPoint or Keynote\nto deliver the lesson, or to be directly used as speaking notes in\nthe class (Figure 1). However, constructing an effective lesson plan\ncould be challenging for teachers, especially those who are novices\nor teaching a course for the first time [56]. For one thing, novice\nteachers lack experience in delivering a course using effective strate-\ngies [6, 23], e.g., Gagne‚Äôs Nine Events of Instruction [ 28], which\narXiv:2408.01102v1  [cs.HC]  2 Aug 2024\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nhelps teachers to prepare and deliver instructional content while\nconsidering and addressing the students‚Äô conditions, like gaining\nattention and presenting a stimulus at the beginning of a class. For\nanother, even when novice teachers have a mind of planning an\nevent or activity (e.g., in-class exercises) at a certain time of a course,\nthey could face difficulties in curating relevant materials to execute\nthe activity [ 23, 59, 61]. As a common practice, novice teachers\noften refer to others‚Äô lesson plans and search related materials in\ntextbooks or online [59]. Nevertheless, these traditional approaches\ncould be limited by the diversity of referred materials and the ability\nto support teachers in adapting the content in the lesson plan based\non their personal thoughts.\nRecent advances in large language models (LLMs) show great\npotential to address these challenges in composing a lesson plan\nby suggesting in-situ-generated content that adapts to teachers‚Äô\nthoughts. In fact, many teachers have explored the usage of LLMs to\nprepare for their lectures [47, 48]. For example, Mollick and Mollick\n[48] suggested the prompts to LLMs to implement effective teach-\ning strategies in the classroom, such as providing diverse examples\nto help students comprehend abstract concepts and collecting test\nquestions to help students assess their knowledge. However, query-\ning the LLMs in the wild, e.g., via the web app of ChatGPT, could\nlead to a fragmented lesson plan because of the difficulties in or-\nganizing all of the LLM outputs. More importantly, due to the lack\nof experience in teaching and guidance for prompting, the gener-\nated content may not be comprehensive and readily usable, thereby\nrequiring iterative refinement [3, 9, 38]. There is a need to build\nan interactive system with customized features to ease the process\nof adopting LLMs to prepare a comprehensive lesson plan. Never-\ntheless, little is known about what are the design requirements for\nsuch a lesson planning system, how LLMs can help to satisfy these\nrequirements, and how would teachers perceive and collaborate\nwith the system.\nIn this paper, we introduceLessonPlanner1, an interactive system\nthat offers pedagogy-driven generated content to assist users in\nconstructing lesson plans. We first conduct interviews with six\nparticipants, including three novice teachers and three teaching\nassistants in the universities, to understand their challenges and\nneeds for support when using LLMs to prepare lesson plans. The\nfindings reveal users‚Äô demands for the generated content that aligns\nwith effective teaching strategies and flexible interactions with\nLLMs. Based on the derived design goals from the interviews, we\nbuild LessonPlanner as a web app powered by the LLM GPT-4.0. In\nLessonPlanner, users can first input basic course information (e.g.,\ncourse name, topic of one lecture) to generate an outline of the\nlesson plan with teaching strategies suggested by Gagne‚Äôs nine\nevents [28]. Then, for each section of the outline, users can extend\nit with user-specified LLM-generated activities belonging to each\ninstructional event to get suggested teaching materials, and they\nhave the option to customize the materials via prompts. At any time\nof the lesson plan preparation process with LessonPlanner, users\ncan select any content to ask the LLM to explain it and freely query\nthe LLM about anything, including knowledge delivery strategies\nand presentation suggestions for creating slides.\n1https://github.com/fanhaoxiang1/LessonPlanner\nWe first evaluated LessonPlanner via a within-subjects study in\nwhich twelve graduate students or senior undergraduates act as\nteachers and prepare lesson plans for their familiar course topics.\nThe results showed that LessonPlanner can significantly improve\nthe quality of outcome lesson plans and ease users‚Äô workload in the\npreparation process. We further conducted expert interviews with\nfive novice teachers (who have less than three years of teaching\nexperience) and one experienced teacher across various educational\nstages and subjects. Experts further demonstrated that LessonPlan-\nner is useful in lesson planning because it offers a well-organized\noutline and inspiring content. We discuss the concerns and impli-\ncations of our study on facilitating users in teaching activities with\nLLMs.\nIn summary, this paper has three main contributions. First, we\npresent LessonPlanner, an interactive system that leverages gener-\nated content to assist teachers in preparing lesson plans. Second,\nvia a within-subjects study and an interview study, we offer em-\npirical evidence on LessonPlanner‚Äôs effectiveness and usefulness in\nhelping novice teachers prepare lesson plans. Third, based on our\nfindings, we offer design implications for future systems that use\nlarge language models to assist teachers.\n2 RELATED WORK\n2.1 Preparation of Lesson Plans\nA lesson plan is the instructor‚Äôs road map of what students need\nto learn and how it will be done effectively during class time [49].\nA carefully constructed lesson plan allows teachers to enter the\nclassroom with more confidence and achieve effective teaching out-\ncomes of their courses [31, 32]. As suggested by the CIPP (Context,\nInput, Process, and Product) evaluation model [ 63], an effective\nlesson should be needs-oriented, resource-adequate, systematically\nexecuted, and outcome-focused. A carefully constructed lesson\nplan can reveal the first two aspects of the CIPP model [5], that is,\ngoals aligned with subject and societal demands as Context and a\nwell-organized content plan as Input.\nHowever, teachers, especially those who are novices or teach a\ncourse for the first time, could find it difficult to construct a well-\ndesigned lesson plan. For example, they may need to spend a lot of\ntime writing a lesson plan from scratch [23], finding high-quality\nresources related to the lesson [6, 59], and thinking of the strategies\nto teach each knowledge concept [23]. Researchers have explored\nvarious methods that can assist in the preparation of lesson plans.\nThey have tried using knowledge graphs [ 16, 56, 58] and recom-\nmendation systems [17] to integrate existing web resources, which\ncould make it convenient for teachers to find needed teaching ma-\nterials. For example, CollectiveTeach [ 56] retrieves and collects\ndocuments related to a specific topic based on a particular optimiza-\ntion objective, and then rearranges the documents into a coherent\nlesson plan. They have also proposed a variety of lesson planning\nsystems [4, 15, 17, 50, 62, 72]. For instance, Pender et al. [50] have\ndeveloped the CLEVER digital web platform, which includes Di-\ndactic Guidance, Content Management, Platform Services, and the\nData section. It assists users in selecting existing content from the\nplatform‚Äôs library or in creating new content. Zain [72] also pro-\nposed CIDS (Collaborative Instructional Design System), a lesson\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nplan design system aimed at assisting teachers in designing and im-\nplementing the requirements of 21st-century learning, integrating\nfeatures of the ASIE (Analyze, Strategize, Implement, and Eval-\nuate) Instructional Design Model and the Professional Learning\nCommunity. Nevertheless, these previous approaches and systems\nlargely rely on existing high-quality teaching resources related to\nthe lessons, while such resources may not always be available for\nany lesson that the teachers want to deliver.\nOur work is motivated by the benefits of preparing a carefully\nconstructed lesson plan and is aligned with previous work that\naims at easing this preparation process. Different from previous\nwork, we explore the usage of machine-generated content to assist\nlesson plan preparation.\n2.2 Large Language Models in Education\nResearchers have started to explore the usage of large language\nmodels (LLMs) in educational settings due to their ability to provide\npersonalized, efficient, and engaging learning and teaching experi-\nences [46]. For learners, depending on a student‚Äôs needs and learn-\ning style, LLMs are able to create self-study and self-assessment\nmaterials [18, 55], such as knowledge flashcards or course-specific\nquestions [8, 10, 24, 27]. For example, since ChatGPT demonstrated\nstrong insights in explaining medical questions without any ad-\nditional training [39], Divito et al. [24] have utilized it to support\nmedical learners in problem-based learning. Furthermore, LLMs\nhave the capability to encourage students to think critically [1]e.g.,\nby creating chatting chances during learning and adding suitable\nproblems if needed [22].\nFor teachers, LLMs can help to automate the knowledge assess-\nment process and provide personalized feedback to students [7, 69].\nAdditionally, teachers can get support from generated teaching\nmaterials including varied examples, instruction notes [11], expla-\nnations [48], post-class exercises [ 60], a list of frequently asked\nquestions [45], and so on. For instance, Mitra et al. [45] proposed\na system named RetLLM-E that assists educators in acquiring fre-\nquently asked questions from students related to their courses and\nin generating responses. The system initially retrieves context from\nstudent questions previously answered by teachers on a forum\nand from related course materials. It then uses this information\nto prompt LLMs to generate specific, high-quality, and precise an-\nswers to students‚Äô questions, which generally have a better quality\nthan other available answers on the forum.\nDespite the promising potential, few works have explored using\nLLMs to help teachers prepare their lesson plans. In this paper,\nwe will first work with six novice teachers to understand their\nopinions about using LLMs for preparing lesson plans and identify\ntheir needs for support.\n2.3 Interactive Systems that Supports Users\nwith Large Language Models\nHuman-Computer Interaction (HCI) researchers have proposed\na bunch of interactive systems that leverage LLMs to improve\nthe efficiency and user experience in various tasks, such as group\ndecision making [19, 29], software engineering [ 42, 51], human-\nUI interaction design [65], and so on. Specifically, many HCI re-\nsearchers have embedded LLMs in their systems to support co-\nwriting tasks [33, 35, 40, 44, 52], which is similar to our setting of\nco-writing a lesson plan with LLMs. For example, Lee et al . [40]\ndeveloped CoAuthor and conducted a user study, which highlighted\nthat co-writing with LLMs can aid users by enhancing fluency, pool-\ning ideas, and improving writing quality. Wordcraft [71] consists\nof an editor and some buttons used to call an LLM to generate\nvarious kinds of content. Compared with using a chatting inter-\nface, users find out this tool is more helpful and collaborative [71].\nThis finding inspires our design by replacing the chat box with\nsimple pre-set buttons. Furthermore, Jamplate [70] is an idea re-\nflection system that integrates LLM responses into the templates\noriginated from the traditional reflection theory. It is shown that\nthe well-organized generated content helps users a lot in expanding\nand refining their ideas [70]. Just like Jamplate does, we integrate\neducation theory into our system to structure LLM responses in a\nlesson plan template.\nWe contribute an interactive systemLessonPlanner that is in line\nwith these previous interactive systems and offer insights into how\nLLMs can facilitate lesson plan preparation.\n3 FORMATIVE STUDY\nThis study aims to help novice teachers design lesson plans in an\nefficient way and get high-quality outcomes. To achieve this, we\nconduct a formative study with six novice teachers or teaching\nassistants. The insights from the study will inform the design goals\nfor LessonPlanner. The participants, with a mean age of 29.83 (ùëÜùê∑ =\n5.15), include three females and three males, and we note as P1\nto P6. Half of them (P2, P5, P6) are novice teachers with less than\nthree years of teaching experience, and the other half are teaching\nassistants (P1, P2, P4) who have taught undergraduate students\nduring one to three academic terms. Four of the participants teach\ncourses related to computer science, while P1 teaches Architecture-\nrelated courses and P5 teaches Korean Intensive Reading.\n3.1 Pedagogies for Lesson Planning\nBefore the interviews, we prepared three educational theories\nusually used to guide teachers in preparing structured lesson plans,\nwhich have proven to be effective [2, 53, 57]. They are easy to im-\nplement in the system to assist teachers in designing lesson plans.\nBloom‚Äôs Taxonomy is a hierarchical classification of cognitive\nskills that educators use to structure curriculum learning objectives,\nassessments, and activities, ranging from lower-order thinking skills\nlike remembering, understanding, and applying, to higher-order\nskills like analyzing, evaluating, and creating.Kolb‚Äôs Learning Cy-\ncle is a four-stage model of experiential learning that emphasizes\nthe process of learning through experience, consisting of concrete\nexperience, reflective observation, abstract conceptualization, and\nactive experimentation. Gagne‚Äôs Nine Events of Instruction in-\nvolve the actions of both teachers and learners throughout the\nteaching process [37]. The nine events include: Gaining Atten-\ntion: Present introductory activities that engage learners; Inform-\ning Learners of Objectives : Clearly state the learning goals and\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\noutcomes; Stimulating Recall of Prior Learning : Encourage\nlearners to remember and connect previous knowledge; Present-\ning Stimulus: Introduce new content and information; Providing\nLearner Guidance: Offer instructions and strategies to help learn-\ners understand and process the new content; Eliciting Perfor-\nmance: Have learners practice what they have learned; Providing\nFeedback: Give constructive feedback on learners‚Äô performance;\nAssessing Performance: Evaluate learners‚Äô understanding and\nskills; Enhancing Retention and Transfer : Use activities that\nhelp learners retain information and apply it to new situations.\nThe nine events and corresponding activities prepared for the dis-\ncussion in the formative study are summarized in Table 1. Each\nevent may occur sequentially in a class as described above, but they\ncan also be repeated to better organize the instructional content.\n3.2 Procedure\nTo assist novice teachers in solving the difficulties in lesson plan-\nning and offer a better user experience, we conduct a two-phase\nformative study to comprehensively understand the users‚Äô needs.\nInitially, the participants are involved in semi-structured interviews.\nA set of questions is presented, and they are encouraged to express\ntheir viewpoints. The discussions are intended to investigate the\nprocesses involved in creating their ways of routine lesson plan\ncreation (e.g., ‚ÄúDo you typically design instructional notes or slides\nfor a 90-minute class?‚Äù, ‚ÄúWhere do you like to access related mate-\nrials when developing lesson plans?‚Äù), the potential of integrating\nthe three educational theories (introduced in subsection 3.1) into\na lesson plan ( e.g., ‚ÄúDo you agree that Gagne‚Äôs Nine Events can\neffectively guide instruction?‚Äù), and the challenges encountered\nduring the design process (e.g., ‚ÄúWhat challenges do you face dur-\ning this process?‚Äù). Furthermore, we specifically investigate their\nperspectives regarding LLMs such as ChatGPT in the design of\nlesson plans. Each interview has a duration of around 45 minutes\nin this phase.\nSubsequently, they are invited to participate in a co-design ses-\nsion aimed at exploring and evaluating various interface designs.\nThe objective is to identify the user interface requirements for a\nsystem designed for lesson plan design. Four slide decks (shown in\nAppendix A) are presented to participants, featuring example de-\nsigns. These include a page for input meta-data of class, an outline\noverview page, and two editing pages with LLMs in different forms.\nParticipants are provided with the ability to resize and crop screen-\nshots, draw shapes, and use text boxes for the purpose of designing.\nOne author assists with sketching by making edits based on partic-\nipants‚Äô responses. We also ask questions to provide more detailed\nexplanations of their ideas or explore different design options. Each\nparticipant spends about 15 minutes in this session.\n3.3 Findings\nWe use the reflexive thematic analysis method [12] to analyze\nthe transcribed recordings of each participant‚Äôs semi-structured\ninterviews and co-design sessions. All mentioned that they primar-\nily prepared a lesson plan for a lecture in the form of PowerPoint\nslides. P2 said that he sometimes chose to annotate some text in\nthe slides or textbook to remind themselves of extra information,\nsuch as proof of formulas or supplemental examples. Except for\nP2, no participants reported that they left notes under the slides\nor in a separate paper as a reminder of critical information during\nthe lecture, such as the logic of proof and supplemental examples.\nNevertheless, they all agreed that having a word-like document to\nlist the main flow and content of a lecture was helpful, as they could\n‚Äúeasily build up slides based on the documented lesson plan‚Äù (P6). We\nsummarize the participants‚Äô challenges in lesson plan preparation\nand using large language models (LLMs) to assist the preparation\nas below.\n3.3.1 Challenges in Lesson Plan Preparation. C1. Lack of adap-\ntive support in planning effective teaching strategies in the\nlessons. All participants said that they mostly relied on their teach-\ning experience and others‚Äô plans to design their teaching strategies\nin a lesson. Nevertheless, after we introduced Gagne‚Äôs Nine Events\nof Instruction (the list of the events is shown in Table 1), all partici-\npants agreed that they actually adopted some of the events in their\nlectures. ‚ÄúI did not get official training on educational or teaching the-\nories, but I found that I had enacted several strategies in the suggested\nevents of instruction. These nine events will be generally helpful in my\ncourse‚Äù (P2). Compared to the other two theories mentioned in sub-\nsection 3.1, two participants (i.e., P1, P2) confirmed that Gagne‚Äôs\ntheory was more feasible to be embedded in the system because ‚Äúit\nis relatively more specific and provides teachers with comprehensive\nguidelines‚Äù (P2). Furthermore, four participants ( i.e., P1, P3, P4,\nand P6) with insufficient teaching experience further mentioned\nthat they would like to exercise all the nine events in their lectures\nbut they lack adaptive support in incorporating these events into a\nspecific lesson. ‚ÄúI would like to try the suggested ‚Äògaining attention‚Äô\nand ‚Äòproviding learner guidance‚Äô, but the example usages of these\nstrategies could not be directly adopted in my Experiments of Data\nstructures course‚Äù (P1).\nC2. Time-consuming process in searching for adequate\nteaching materials. All participants indicated that preparing for\na lesson, especially the one they taught for the first time, is a time-\nconsuming process, where they spent most of the time preparing the\nteaching materials. ‚ÄúI usually need to spend a considerable amount of\neffort finding and determining the proper materials that support the\nteaching activities in my course, e.g., finding a good case to illustrate\nthe real-world application of the taught concepts‚Äù (P5). Even though\nthey can get started from textbooks or others‚Äô lesson plans, all\nparticipants expressed their desire to customize the teaching flow\nand materials based on their thoughts. As such, they spent time\nlooking for related high-quality materials online or creating needed\nmaterials by themselves. ‚ÄúI was used to prepare an in-class exercise\nafter explaining a key concept. I normally search for suitable exercises\nonline but the returned results were often disorganized or unrelated. I\nhad to pay a lot of attention to identify the ones I need‚Äù (P1).\n3.3.2 Challenges on Using LLMs in Lesson Plan Preparation. C3.\nHigh mental demand in manipulating prompts to get related\ncontent. Three participants (P3, P4, and P6) had experiences using\nLLMs (Specifically, ChatGPT) to prepare for their lessons. For ex-\nample, P3 reported that she had used ChatGPT to generate precise\nanswers to the questions that students may ask when delivering\nknowledge points. The others also see the potential of using LLMs\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nto generate suggestions on the teaching flow and materials based on\ntheir needs. However, P5 and P6 commented that it was or would be\nmentally demanding to think of the prompts for LLMs to get needed\ncontent. ‚ÄúSearching for information directly seems more efficient than\nquerying ChatGPT with specific questions. ‚Äù (P6). ‚Äú If LLMs cannot\nprovide satisfying materials for me instantly, I prefer not to engage in\nback-and-forth dialogue to refine the results‚Äù (P5).\nC4. Distrust of the usefulness of generated content . Except\nfor P3, the remaining five participants conveyed a sense of distrust\ntoward the outputs of the LLM. On one hand, some (P2, P5, P6)\ndoubted the accuracy of the content.‚ÄúI don‚Äôt trust the detailed content\ngenerated by LLMs, and I would definitely double-check it before using\nit. Therefore, I generally only use LLMs to create a rough outline, and I\nsearch for the specific content myself‚Äù (P1). On the other hand. some\n(P1, P2, P6) have raised concerns about the expertise of LLMs in\nspecific subjects, suggesting that they may struggle to generate\nuseful content.‚ÄúChatGPT can generate many correct questions, but\nthat doesn‚Äôt mean they are good questions in this course (Computer\nGraphics). Because it may lack a deep understanding of the subject‚Äù\n(P6).\n3.4 Users‚Äô Preference on the Interface\nDuring the co-design phase, all participants reached a consensus\nthat the layout of our outline overview page, which organizes the\noutline into distinct blocks based on the generated subtitles, was a\ngood design. P4 advised that ‚Äúthe instructional events are important\ncues for me, and they should be more eye-catching. ‚Äù . For the course\nmetadata collection page, P2 questioned that ‚Äúthe available data\ninput was excessively restricted. I believe it is necessary to enter the\ncourse designed for graduate or undergraduate students. Without this\ninformation, how can the system accurately determine the specific\ntype of content to generate?‚Äù Moreover, participants were requested\nto compare the LLM assistant interface integrated within the editor\nas opposed to a sidebar interface. P1, P2, P3, P5, and P6 expressed a\npreference for the LLM as a sidebar. ‚ÄúIt resembled common design\npractices‚Äù (P3).\n3.5 Teachers‚Äô Needs for Generated Content in\nLesson Plans\nTable 1 summarizes the particular instructional activities our partic-\nipants commonly incorporate during each of Gagne‚Äôs Nine Events\nin the classroom. Additionally, we ask their perspectives on the\ncapacity of LLMs to contribute to the development of materials for\nthe activities they have mentioned in each interview. The results are\ndisplayed in the third column of Table 1, with ‚ÄòY‚Äô if they think the\nLLMs can facilitate the activity and ‚ÄòN‚Äô if they can not. The results\nindicate that, with the exception of subject-specific activities (e.g.,\nproviding source code, providing example sentences), most teachers\norganize comparable activities for a particular instructional event.\nFor the majority of classrooms, LLM can help teachers in creating\nrelevant instructional materials. This highlights the potential of\nLLMs to serve as assistants in lesson planning.\n3.6 Design Goals of LessonPlanner\nOur participants actively offered suggestions on the design of\nLessonPlanner to address their challenges. Based on their sugges-\ntions and related work, we derive the following four design goals\n(DGs) of LessonPlanner.\nDG1: LessonPlanner should encourage and facilitate teach-\ners to apply effective teaching strategies in the planned lessons.\nDue to the insufficient adaptive support provided by teachers in\nlesson planning for the development of effective teaching strate-\ngies (C1), LessonPlanner to motivate and assist teachers in applying\nefficient teaching strategies, especially those who lack experience\nin teaching.\nDG2: LessonPlanner should generate high-quality mate-\nrials adapted to the planned teaching activities and provide\nguidance on how to deliver these materials. The process of\nsearching for suitable teaching materials is time-consuming (C2).\nHence, LessonPlanner is expected to produce high-quality materials\nthat are in line with the teaching activities and offer suggestions\non how to effectively deliver these materials in the classroom.\nDG3: LessonPlanner should offer pre-set prompts about\nwhat users often want to ask the large language models for.\nThe utilization of LLMs for lesson planning requires a high cognitive\nload (C3). Therefore, LessonPlanner offers pre-set prompts to assist\nusers in getting materials more efficiently from LLMs.\nDG4: LessonPlanner should provide flexible user control\nfor interacting with the large language models and editing\nthe lesson plans. Granting users the autonomy to select their\npreferred method of managing the content generated by LLMs, and\nto discard, modify, or regenerate it as needed, can mitigate users‚Äô\ndistrust towards LLMs (C4).\n4 LESSONPLANNER\nIn this subsection, we present the design and evaluation of Lesson-\nPlanner, an interactive system that supports novice teachers to cre-\nate lesson plans with large language models (LLMs). LessonPlanner\nis designed as a web application. The front end is implemented us-\ning Vue 3 and JavaScript, while the back end is created with Python\nFastAPI. Furthermore, we opt to utilize the OpenAI gpt-4-1106-\npreview as the LLM to provide generated content in LessonPlanner.\nThe front end is in Chinese, so the Edge translation plugin was\nutilized to translate the website in order to present the illustrations\nin this chapter.\nWe carefully designed LessonPlanner‚Äôs interface and interaction\nbased on the design goals obtained from the formative study. To\nplan a lesson with LessonPlanner, users can engage with the LLM\nto set teaching objectives to adhere to Bloom‚Äôs Taxonomy [ 25]\nand generate an initial outline of the lesson plan conditioned on\nGagne‚Äôs Nine Events (DG1, Figure 2). For each section in the out-\nline, users can see suggested teaching materials and strategies to\nenact this event (DG2, Figure 3: C1, D2). Based on the identified\nteachers‚Äô needs for generated content in lesson plans in formative\nstudy, LessonPlanner presets some activities for each event about\nwhat users often want to ask the LLM for (DG3, D1, D2). Besides,\nLessonPlanner also offers users flexible options to regenerate the\ncontent of an event and copy it to the editor (D), freely edit the\ncontent (Figure 4: E, F), select wanted events (Figure 5), check the\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nGoal-Setting Page\nA.   Input the meta-data of the lesson\n(Lesson Goals)\n(Generate Lesson Goals)\nB.   Refine lesson goals with LLMs\nRemove item 6.\nAdd requirements.\nClick \nFigure 2: Goal-setting page of LessonPlanner, with the illustration of the process of refining lesson goals with LLMs. The text in\nparentheses serves as a correction to the mistranslated output.\nLesson-Planning Page\nA.Generate or Re-generate the Lesson Plan Outline\nB. Advanced Markdown Editor\nC. A Block Containing a Subsection. \nC.1 The Content Area\nD. Assistant (Click on E.1)\nD.1 Core Action Set\nD.2 Contextual Action Set\nD.3Output Text Area\n(History)\n(I need)\nD.4\nC.2\n(Set Instructional Events)\nC.3\nC.5\nD.5\nC.4\nFigure 3: The lesson-planning interface for LessonPlanner. The text in parentheses serves as a correction to the mistranslated\noutput.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nTable 1: Nine events of Gagne‚Äôs instructional theory and the activities that our participants in the formative study suggest to\nuse in each event. Participants also share their opinions on whether (Yes or No) LLMs are able to help them in each activity. In\nLessonPlanner, we have preset template prompts to LLMs for most of the suggested activities.\nGagne‚Äôs Event Common activities or resources in this event LLMs facilitated Implemented\nGain attention Pose open-ended questions or case studies Y Y\nInform learners of objectives Create ordered lists of knowledge points Y Y\nDisplay table of contents in slide Y Y\nStimulate recall of prior learning Compile prerequisite knowledge list Y Y\nProvide prerequisite knowledge examples Y Y\nPresent stimulus\nProvide the definition Y Y\nProvide algorithms Y Y\nProvide source code Y Y\nProvide equations Y Y\nProvide a example sentence Y N\nProvide learner guidance\nExplain examples in detailed Y Y\nDesign animations in PowerPoint N -\nPlay videos N -\nElicit performance\nConstruct multiple choice or fill-in-the-blank questions Y Y\nPropose open-ended questions Y Y\nConstruct group discussion topics Y Y\nProvide feedback Offer problem solutions Y Y\nAssess performance Assign homework Y Y\nEnhance Retention and Transfer Assign Projects as homework. Y Y\nSelect topics for writing papers Y N\nhistory of generated content (Figure 4: G), and so on (DG4). In the\nfollowing subsections, we will detail the design and implementation\nof the key features in LessonPlanner.\n4.1 Goal-Setting Page\nAs shown in Figure 2A, on the goal-setting page, users need to input\nthe course name (e.g., Data Structures), the topic of the planned\nlesson (e.g., Quick Sort), and the specific stage of the lesson tailored\nfor (e.g., Sophomore (2nd-year undergraduate)). This meta infor-\nmation is utilized by LessonPlanner in all predefined prompts to\nfacilitate content generation.\nUsers can input their lesson goals, or, in a more convenient way,\nclick the ‚ÄúGenerate Lesson Goals‚Äù button to first check the generated\ngoals conditioned on Bloom‚Äôs Taxonomy. They can then modify\nthe generated goals, e.g., remove item 6 and add requirements like\n‚Äúprogramming using python‚Äù (Figure 2: B), and click the ‚ÄúGenerate\nLesson Goals‚Äù again to iterate the design goals with LLM. Once\nusers are satisfied with the lesson goals, they can click on the\n‚ÄúGenerate outline‚Äù button, which will initialize a lesson plan outline\nconditioned on Gagne‚Äôs Nine Events. Users now can proceed to the\nLesson-Planning page (Figure 3), as described in the next subsection.\n4.2 Lesson-Planning Page\n4.2.1 Interactive block-based lesson plan outline. The generated les-\nson plan outline is displayed in the form of block-based interactive\nmarkdown text (Figure 3: C). In each block, the editable h1 heading,\ne.g., ‚ÄúOpen question: special requirements in sorting‚Äù, is a summary\nof the content in one section in the planned lesson. Each section\ncontains one or more suggested events (C4) that teachers can plan\nin this section. The events are sourced from Gagne‚Äôs Nine Events\nand separated by editable h2 headings (e.g., ‚Äúattract attention‚Äù) with\nan h3 heading (e.g., 5 minutes) indicating the planned time for this\nevent. In each section, there are related teaching materials and\nstrategies, e.g., Related application cases ‚ÄúDo you want to know\nin advance where a specific element will be ranked in the entire\nsequence before the sorting is completed?‚Äù. As a lesson may not\nnecessarily cover all of the suggested sections and may include\nother sections the users need, users have the options to ignore or\nspecify needed sections, delete the section (C3), insert a new block,\nand specify the title of a new section at any location (C5). Users\ncan also regenerate the lesson plan outline (A), e.g., if they are not\nsatisfied with the current one.\nUsers can click the content area (Figure 3: C1) of each block\nto invoke the editing mode of this section (Figure 4: E) or click\nthe ‚ÄúEdit using Markdown editor‚Äù button to invoke an advanced\nMarkdown editor of all content in the lesson plan (Figure 4: H).\nThe editor provides features like ‚Äúbold‚Äù, ‚Äúheadings‚Äù, and ‚Äúlists‚Äù that\ncommon editors have. Upon finishing the edition of the lesson plan,\nusers can click the ‚ÄúDownload Lesson Plan‚Äù button (not shown in\nFigure 3) at the bottom of the Lesson Planning page to download\nthe lesson plan as a markdown file.\n4.2.2 LLM assistant. In the markdown editor of each blocked sec-\ntion (Figure 4: E), users can click the ‚ÄúOpen the lesson plan assistant‚Äù\nbutton (Figure 4: E1) to invoke the LLM assistant in the sidebar\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nE. Editing Mode of Content  (Click on C.1)\n(Set as context)\nG. History Drawer (Click on D.5)\nF. Advanced Markdown Editor (Click on B)\nE.1\nFigure 4: The subsequent changes or pop-up windows trig-\ngered by clicking a button on Figure 3.\nD.2 Changed after Click On Confirm\nH. Setting Instructional Events\nD.2‚Äô\nD.2\nClick on C.2 (Set Instructional Events)\nFigure 5: The process of setting instructional events by users\nin the lesson-planning interface.\n(Figure 3: D) that helps users refine the content in this section. The\nassistant includes a Core Action set (D1), Contextual Action set (D2),\nand Output Area (D3). The Core Action set contains a collection of\npreset buttons designed to call functions that regenerate the content\nin this section, evaluate the content give instructional suggestions,\nand advice on presenting it, and give suggestions on making a slide\nbased on it. The Contextual Action set consists of buttons that list\nall the preset activities of events (C4) under this section. Table 1\ndisplays the implemented activities for each event in our system,\nderived from the results of the User Study, excluding activities that\nwere difficult to implement because of the needed various prompts\nin different subjects. If clicking on ‚ÄúSet Instructional Events‚Äù (Fig-\nure 3: C2) to change events in the current section (Figure 5: F), the\nactivities in the Contextual Action set will also change (Figure 5:\nD2 change into D2‚Äô). If users prefer to freely inquire about LLM,\nthey can enable the ‚ÄúI need‚Äù button and question the LLM. The\nOutput Area contains a Markdown editor (Figure 3: D3) that dis-\nplays the LLM outputs after users click any button or freely query\nLLM in the Action sets. It also has a text area under D3 for users to\nconduct multiple rounds of interaction with LLM on the content in\nthe markdown editor. When interacting with LLM via either preset\nbuttons or text area, users can select any text in the editor and\nclick the ‚Äú(set as context)‚Äù button (Figure 4: E) to use the selected\ntext as context to prompt LLM. For example, if there are four key\nknowledge points in the current section, clicking \"Generate Mul-\ntiple Choice Questions\" is highly likely to result in the generated\nquestions that include a range of knowledge points. If users select\nthe text of one knowledge point as context, the generated questions\nwill be likely to be specific to that knowledge point. Users can select\nany content in the LLM outputs 2 and click the \"Copy\" button at\nthe bottom of the assistant panel to copy to the pasteboard and\nthen paste it in any position of the editor of this current section.\nSelecting the Trash icon (Figure 3: D4) will delete all content in\nthe Output Area while clicking the \"Stop Generating\" button can\nhalt the LLM outputs. Clicking the \"Close Assistant\" button will\nhide the assistant. Additionally, clicking the \"history record\" button\n(Figure 3: D5) allows users to access previously generated content\n(Figure 4: G).\n4.3 Implementation Workflow\nLessonPlanner incorporates a workflow that chains all prompts\nand system functions with flexible user control.\n4.3.1 Metadata and generated objectives as slots in prompt tem-\nplates. In the Goal-setting page Figure 2, teachers are able to enter\ncourses‚Äô metadata (e.g., course name, course topic), which are used\nas slots in the prompt templates, as shown below.\n‚Ä¢{course name}: e.g., Data Structures and Algorithms\n‚Ä¢{lesson topic}: e.g., Quick sort\n‚Ä¢{students stage}: e.g., Sophomore\n‚Ä¢{lesson goals}: the content input by the user or LLM in the\nLesson Goals area in the Goal-Setting Page\nWhen designing Prompts for Core or Context Actions set, we always\nplace this paragraph at the beginning of them:\nI will instruct the course of {lesson name} - {lesson\ntopic} for students in {students stage}. Here are my\nlesson goals: {lesson goals}.\nThis setting helps impose strict restrictions on all generated content.\nSometimes, due to the lack of key information, LLMs are unable\nto infer accurate information about the course from the content of\nthe current section or selected context, resulting in output that is\nunrelated to the course. This also applies to the prompts designed for\n‚ÄúI need‚Äù feature or for having continuous conversations with LLM.\n2If nothing is selected, clicking the \"Copy\" button will copy all the content in the\noutput text area to the pasteboard.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nBefore engaging in a conversation, we always place this prompt at\nthe beginning of the user‚Äôs first request.\n4.3.2 Leveraging the nine events to ensure control over generated con-\ntent. As described in DG1, LessonPlanner should provide effective\nteaching strategies at each stage (i.e., outline generation, interacting\nwith LLMs) rather than merely offering educational resources. Ad-\nditionally, displaying the outline in a block-based format for each\nsection and ensuring that LLMs are aware of the corresponding\nteaching strategies requires a precisely defined workflow.\nCombine all the instructional events and metadata when\ngenerating outlines. As seen in Figure 3 C4, recommended instruc-\ntional events are set once a section of the outline is generated. The\npreset prompt for generating the outline is provided in supplemen-\ntary material 1. Developers have confirmed that GPT-4 possesses\nbackground knowledge about Gagne‚Äôs Nine Events in their trials on\nthe preset prompt. Therefore, only the Nine Events and correspond-\ning activities in Table 1 are included in the prompts of each action.\nLLMs have demonstrated superior reading comprehension and the\nability to analyze lengthy texts [64]. We have fully leveraged this\ncapability to create outlines that are aligned with the course content\nand incorporate the Nine Events effectively.\nTo ensure that the format of the contents generated by the LLMs\nmeets the requirement and minimizes errors, the formatting instruc-\ntions are set as simple as possible. We defined that a new section\nshould start with a single ‚Äú#‚Äù, and instructional events in every\nsection should be specified with ‚Äú##‚Äù. After generation, a Formatter\nthen splits and maps the content into several blocks in the user\ninterface. Also, the whole templates and instructional events in\nthe section can be defined as slots, which will be used in some\ntemplates.\n‚Ä¢{outline}: generated outline of the lesson plan.\n‚Ä¢{current section}[events]: the events in current section.\nEmphasize solely the instructional events involved in the\ncurrent section. As for interacting with LLMs within a certain\nsection, if the same context or even section content is assigned\nto different teaching events, we hope to click on the activities in\nthe Core Actions set or use ‚ÄúI need‚Äù feature to bring results that\nmatch the current teaching events. For example, when clicking\non ‚Äúevaluate the content and give instructional suggestions‚Äù in\nthe section with the event ‚ÄúStimulate recall of prior learning‚Äù, we\nexpect a brief explanation. In contrast, in the section with ‚ÄúProvision\nof Learner Guidance‚Äù, we anticipate detailed information to be\nprovided about the knowledge. We dynamically insert the current\nsection‚Äôs instructional events and their corresponding definitions\ninto the prompts through programming. It helps to remind the\nLLM to generate more relevant content based on these events. An\nexample is shown below.\nThe educational theories involved in this section are\nas follows. Be sure to construct the lesson plan around\nthe following events. Events that are not mentioned\ncannot be covered in this section.\n({current section}[events] and their definitions, respec-\ntively.)\nBased on our trials, it is not recommended to include all instruc-\ntional events and their corresponding definitions and add the cur-\nrent events after that, which will make the generated content ex-\ntensively cover each event and cause information redundancy.\n4.3.3 Chaining the prompts with examples of input and output.\nDuring the process of iteratively designing prompts, we found that\nrelying on descriptive statements frequently results in unstable out-\nput. To improve the robustness of the output, we design an example\nat the end of every prompt to ensure that the system generates\nresults that align with expectations every time. For example, for\nthe activity ‚ÄúGenerate definition‚Äù, the example\nInput:\nContext: Quick Sort\nOutput: **Quick Sort * *:\n-Definition: Quick Sort is a divide-and-conquer algo-\nrithm. It works by selecting a ‚Äôpivot‚Äô element from the\narray and partitioning the other elements into two\nsub-arrays, according to whether they are less than\nor greater than the pivot. 3\nwas added to the end of the prompt, forcing LLMs not to generate\nmore detailed explanations to further explain the definition.\n5 EVALUATION\nTo explore the effectiveness and user experience of LessonPlanner\nfor assisting novice teachers in lesson planning, we conducted two\nuser studies. First, we conduct a within-subjects study with 12\nuniversity students who have little or no teaching experience to\nuse LessonPlanner and a baseline tool to prepare lesson plans for a\ncourse that they have learned before. Second, we conduct an expert\ninterview with 6 teachers from different educational levels and\nsubjects. Our research questions (RQs) are:\nRQ1. How would LessonPlanner affect the lesson planning out-\ncome?\nRQ2. How would LessonPlanner affect the lesson planning pro-\ncess?\nRQ3. How would users perceive LessonPlanner for preparing\nlesson plans?\n5.1 Within-subjects study\n5.1.1 Participants. We recruit 12 students (P1-P12, two females,\nten males; age: ùëÄùëíùëéùëõ = 23.42, ùëÜùê∑ = 1.98) via word-of-mouth\nfrom a university in mainland China (Table 2). All participants,\nincluding two 4th-year undergraduate students and ten graduate\nstudents, major in artificial intelligence and are familiar with the\ncourse Data Structures and Algorithms. The experimental setup\ncontrols the lessons that participants need to prepare for, which\nleads to unbalances of majors and genders of our participants. We\nwill discuss the limitations of this setting in the Discussion section.\nNine of the participants have experience working as a teaching\nassistant (TA) in the university or as a home tutor for high-school\nstudents. All participants are quite interested in having a trial on\nutilizing artificial intelligence tools or online resources for lesson\n3https://en.wikipedia.org/wiki/Quicksort\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nInstructions of the task and \nassign system\nDesign a lesson plan for assigned\ncourse topic in 40 minutes\nQuestionnaire; interview\nTask 1\nFinal interview; \npayment\nReview learned course topics\nand\nlearn about needed knowledge\nA digital textbook about\ntwo course topics\nA mind map introducing\nGagne‚Äôs Nine Events\nA document introducing \nMarkdown\nTwo example \nlesson plans\nLesson planning day (Within-subjects)\nCounterbalance\nRest for \n10 minutes\nOutcome lesson plan\nOne day before \nconducting tasks\nSystem\nLessonPlanner\nBaseline\nCourse topic\nHash Table\nBinary Tree\nX\nInstructions of the task and \nthe other system\nDesign a lesson plan for the\nother course topic in 40 minutes\nQuestionnaire; interview\nTask 2\nOutcome lesson plan\nFigure 6: The procedure of the within-subject study.\nTable 2: Participants involved in the within-subjects study.\nID Gender Age Year Experiences Freq. of LLMs Usage\nP1 M 22 Graduate TA & Home Tutor Daily\nP2 M 23 Graduate TA & Home Tutor Weekly\nP3 M 29 Graduate N/A Have Tried\nP4 M 22 Graduate N/A Weekly\nP5 M 25 Graduate TA & Home Tutor Weekly\nP6 M 24 Graduate TA Infrequently\nP7 M 22 Undergraduate N/A Weekly\nP8 M 24 Graduate TA Daily\nP9 F 23 Graduate TA Infrequently\nP10 M 23 Graduate TA Weekly\nP11 F 21 Undergraduate TA Daily\nP12 M 23 Graduate TA Daily\nplanning (ùëÄùëíùëéùëõ = 4.33, ùëÜùê∑ = 0.47; 1 - no interest at all, 5 - a large\namount of interest). Nine participants are daily or weekly users of\nlarge language models (LLMs, e.g., ChatGPT and Claude), and the\nother three participants are infrequent users.\n5.1.2 Experiment Setup. The goal of the within-subjects study is to\nquantitatively evaluate the LessonPlanner‚Äôs effectiveness and user\nexperience compared to a baseline system.\nBaseline condition. The baseline system is the ChatGPT web\napp that uses the GPT-4 model as used in LessonPlanner. In the\nbaseline condition, participants can also use the Bing search engine\nin their web browsers and a markdown editor 4 that is identical to\nthe markdown component embedded in LessonPlanner.\nLessonPlanner condition. In the LessonPlanner condition, par-\nticipants are permitted to use Bing and LessonPlanner, but are not\nallowed to visit the ChatGPT web app or another markdown editor.\nIn either condition, participants could freely choose whether, when,\nand how to use the provided tools.\nTask-system assignment. In the recruitment survey, we ask\nparticipants to choose the course topics of Data Structures they\n4https://github.com/code-farmer-i/vue-markdown-editor\nare familiar with. We select the two topics that all participants\nindicate their familiarity with, i.e., ‚ÄúHash Table‚Äù and ‚ÄúIntroduction\nand Traversal of Binary Trees‚Äù. We counter-balance the order of\nlesson planning tasks and use a system using Latin Square, with\nthree participants in each of the four assignments:\n‚Ä¢LessonPlanner - Hash Table, Baseline - Introduction and Tra-\nversal of Binary Trees\n‚Ä¢Baseline - Introduction and Traversal of Binary Trees,Lesson-\nPlanner - Hash Table\n‚Ä¢LessonPlanner - Introduction and Traversal of Binary Trees,\nBaseline - Hash Table\n‚Ä¢Baseline - Hash Table, LessonPlanner - Introduction and Tra-\nversal of Binary Trees\n5.1.3 Tasks and Procedure. Each participants have two lesson-\nplanning tasks. The prompt for each task is:\nThe lesson plan is a teacher‚Äôs detailed description of\nthe course of instruction. Now, you are instructed\nto act as a teacher who will deliver a lesson about\nData Structures - [Topic Name in this task] to second-\nyear university students who are new to this subject.\nYour class is composed of 70 students whose academic\nabilities follow a normal distribution. The lesson is\nscheduled to last for 90 minutes, and you have 40\nminutes available to develop your lesson plan.\nThe whole procedure is illustrated in Figure 6. Before the day of\nconducting lesson planning tasks, we sent participants the following\nmaterials, which they should spend 15-30 minutes reviewing before\nthe tasks.\n‚Ä¢Sections on Hash Tables and Introduction and Traversal of\nBinary Trees in a digital textbook of Data Structures and\nAlgorithms\n‚Ä¢A mind map that introduces Gagne‚Äôs Nine Events.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\n‚Ä¢A document that introduces the basics of Markdown opera-\ntions.\n‚Ä¢Two example lesson plans of course topics different from\nthose in Data Structure and Algorithm.\nOn the day of conducting lesson planning tasks, each three par-\nticipants that are in the same task-system assigned group come to\nour lab. We briefly introduce Gagne‚Äôs Nine Events and the consid-\nerations for a good lesson plan using the materials sent yesterday.\nTo motivate them to put forth their best effort in the task, we\ninform them that an additional 80RMB reward will be given to the\nbest lesson plan among others rated by a seasonal lecturer of Data\nStructures and Algorithms.\nIn each task, we first introduce the task and demonstrate the\nassigned system to participants. We then help participants to set up\nthe environments of study in our provided computers or their lap-\ntops, i.e., opening webpages of the baseline system orLessonPlanner,\nBing, and digital textbook of the assigned course topic. Each par-\nticipant then independently works on the lesson plan preparation\ntask. We allocate 40 minutes for each task and inform them that\nthey can finish earlier or have a few more minutes to complete\nthe task. After each task, we ask participants to fill out a ques-\ntionnaire about their experience in this lesson planning process\nand perceptions of the used system. Subsequently, we interview\nthem for 5 minutes. There is a 10-minute break between two tasks\nfor participants to take a rest. Upon completion of two tasks, we\nconduct a final semi-structured interview with the participants that\nfocuses on their preferences on the used systems, perceptions on\nthe generated content, opinions on the features of LessonPlanner,\nand suggestions for improving it. In total, each participant spends\naround 120 minutes in our experiment and receives 120RMB for\ncompensation.\n5.1.4 Measurements. We employ a standard 7-point Likert scale\n(1 - strongly disagree, 7 - strongly agree) to measure the quality of\noutcome lesson plans, experience in the lesson planning process,\nand perceptions towards the used system.\nRQ1: Lesson planning outcome. To assess the quality of the\noutcomes, we invite a teacher with three years of experience teach-\ning Data Structures and algorithms to score all outcome lesson plans\nin a shuffle order. The aspects of evaluation are adapted from Aziz\net al. [5], based on the CIPP model, and the lesson plans are as-\nsessed on a 7-point Likert scale to indicate the degree to which\neach criterion is met. For each lesson plan, the teacher assesses\nit from five key aspects adapted from the CIPP lecture evaluation\nmodel [5] include the alignment of teaching content with learning\nobjectives, facilitation of students‚Äô skill acquisition, integration of\neffective teaching materials, a balance between theoretical and prac-\ntical activities, and the employment of effective teaching strategies.\nSufficient detail has also been added as an aspect to further evaluate\nthe lesson plans.\nRQ2: Lesson Planning Process. Drawing from the NASA-\nTLX survey [30], we pose six questions to measure the workload\nduring the lesson planning process, including the aspects of Mental\nDemand, Physical Demand, Temporal Demand, Performance, Effort,\nand Frustration. In the interviews, we encourage users to share how\nthe system has increased or decreased their workload.\nRQ3: Perception of LessonPlanner. First, the ten questions\nfrom the System Usability Scale (SUS) [13] are set in our ques-\ntionnaire. We divide SUS into and analyze it from three levels:\nEffectiveness & Learnability, Use Efficiency, and Satisfaction. Then\nwe adapted five questions from Jian‚Äôs Trust Scale [34] and investi-\ngated how users trust the system by inquiring about the system‚Äôs\nvigilance, potential negative impact on teaching, their trust in the\nsystem‚Äôs ethical standards, commitment to users, and the reliability\nof outputs. In the interviews, we encourage them to share their\nexperiences with and preferences for each tool (e.g., search engine;\nChatGPT web app; LessonPlanner) and each component (e.g., differ-\nent pre-set activities and the editor in LessonPlanner). We further\ninquire about their reasons for trusting or distrusting the system.\n5.2 Expert interviews\nFollowing the within-subjects study, to gather additional feedback\nto enhance the generalization of our findings, we conducted think-\naloud studies and semi-structured interviews with six teachers.\n5.2.1 Participants. The six teachers (female=2, male=4) include\nfive novice teachers with teaching experience ranging from 1 to\n3 years ( ùëÄ = 2), and one experienced teacher with 15 years of\nteaching experience. Participants E1-E3, who have been involved\nin the formative study, teach at the university level. E4 and E5 are\nprimary school teachers, and E6 teaches at a senior high school.\nDetailed information about their teaching subjects in the current\nyear (the same as what topics they choose to test on our system)\nare presented in Table 3.\n5.2.2 Method. We conduct a 60-minute interview for every ex-\npert. Initially, we introduce the participants to the background and\ngoal of our research. For E4-E6, we present Gagne‚Äôs Nine Events\nof Instruction and confirm their full understanding of the concept.\nSubsequently, we give them a 10-minute tutorial on how to use\nLessonPlanner. They were given 30 minutes to complete the fol-\nlowing tasks, being asked to think aloud. The examples of expert‚Äôs\ninteractions with LessonPlanner is provided in supplementary ma-\nterial 3.\n‚Ä¢Small Task 1. Generate lesson goals and the outline and set\nup instructional events.\n‚Ä¢Small Task 2. Refine a section. Add an example of a question\ngenerated by LLMs.\n‚Ä¢Exploration. Explore and refine any areas that they are\ninterested in.\nFinally, we conduct a 15-minute semi-structured interview, the\nfixed questions are shown below.\nRQ1: Lesson planning outcome.\n‚Ä¢Please rate the quality of lesson plans created by Lesson-\nPlanner, compared to those you prepare during your usual\nplanning routine.\n‚Ä¢Can LessonPlanner help you manage the teaching process\neffectively?\nRQ2: Lesson Planning Process.\n‚Ä¢Does our system impose additional cognitive load on you,\nsuch as memory or thought burden, compared to preparing\nlesson plans without it?\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nTable 3: Experts involved in user study.\nID Gender Educational Career length Educational Level Subject(s)\nE1 F 15 years Senior High School English\nE2 M 3 years Elementary School Mathematics\nE3 M 1 year Elementary School Science\nE4 F 3 years University Korean Intensive Reading\nE5 M 2 years University Natural Language Processing\nE6 M 2 years University Convex Optimization & Computer Network\nRQ3: Perception of LessonPlanner.\n‚Ä¢Is LessonPlanner easy to handle? Can you use LessonPlanner\nsmoothly without any technical support?\n‚Ä¢Which feature (including goal-setting, outline generating,\npre-set activities, and output refinement) provided byLesson-\nPlanner help you most?\n‚Ä¢Please provide some suggestions for LessonPlanner.\n6 ANALYSES AND RESULTS\nIn this section, we present the quantitative and qualitative results\nfor each research question (RQ). As for the items measured on\na 7-point Likert scale about the lesson planning outcome (RQ1),\nlesson planning process (RQ2), and perceptions with the system\n(RQ3), we employ the Wilcoxon signed-rank tests [68] compare the\ndifferences between the LessonPlanner and baseline condition in\nthe within-subjects study. For the qualitative data in the within-\nsubjects study and expert interviews, two authors transcribe the\naudio into text scripts and conduct a thematic analysis on these\nscripts. they first familiarize themselves by reviewing all the text\nscripts independently. After several rounds of coding with compari-\nson and discussion, they finalize the codes of all the interview data.\nWe count the occurrences of codes and incorporate these qualitative\nfindings in the following presentation of our results.\n6.1 Lesson Planning Outcome (RQ1)\nFigure 7 shows the quality of outcome lesson plans from the within-\nsubjects study, and the the lesson plan designed by P5 is pro-\nvided in supplementary material 2. Overall, the lesson plans cre-\nated using LessonPlanner (Mùëíùëéùëõ = 32.25,ùëÜùê∑ = 5.43) are rated\nsignificantly better than those developed with the baseline sys-\ntem (ùëÄ = 26.33,ùëÜùê∑ = 5.588) in terms of the total scores; ùëä =\n10.5,ùëù = 0.02. Regarding each aspect of the outcome lesson plans,\nLessonPlanner (ùëÄ = 5.42,ùëÜùê∑ = 1.037) does not show a signifi-\ncant advantage over the baseline system ( ùëÄ = 4.92,ùëÜùê∑ = 1.256)\nregarding aligning the teaching content to the objectives; ùëä =\n15.5,ùëù = 0.40. However, compared with the baseline condition,\nthe lesson plans from the LessonPlanner conditions have a ten-\ndency to perform better in facilitating students‚Äô skill acquisition\n(LessonPlanner: ùëÄ = 5.42,ùëÜùê∑ = 1.04; baseline: ùëÄ = 4.58,ùëÜùê∑ = 0.86;\nùëä = 7,ùëù = 0.058) and incorporating effective teaching materials\n(LessonPlanner: ùëÄ = 5.33,ùëÜùê∑ = 1.03; baseline: ùëÄ = 4.50,ùëÜùê∑ = 1.26;\nùëä = 13.50,ùëù = 0.070). Lesson plans constructed with LessonPlan-\nner are significantly better in terms of balancing the theoretical\nand practical content (LessonPlanner: ùëÄ = 5.08,ùëÜùê∑ = 0.86; base-\nline: ùëÄ = 4.08,ùëÜùê∑ = 0.86; ùëä = 8,ùëù = 0.039), using effective\nteaching strategies (LessonPlanner:ùëÄ = 5.58,ùëÜùê∑ = 0.86; baseline:\nùëÄ = 4.42,ùëÜùê∑ = 1.04 ;ùëä = 7,ùëù = 0.034), and providing sufficientde-\ntails on the teaching materials (LessonPlanner: ùëÄ = 5.42,ùëÜùê∑ = 1.19;\nbaseline: ùëÄ = 3.83,ùëÜùê∑ = 1.14; ùëä = 3,ùëù = 0.007). Our partici-\npants in the within-subjects study and teachers in the expert inter-\nview share how LessonPlanner affect their lesson planning outcome,\nwhich is summarized below.\n6.1.1 The Impact of Structured Outlines on Overall Lesson Plan Qual-\nity. Four participants (P1, P2, P6, P12) mentioned that the structured\noutlines helped improve the quality of their lesson plans. As the gen-\nerated outlines take into account various lesson goals and Gagne‚Äôs\nNine Events,‚ÄúLessonPlanner made my lesson plan more standardized‚Äù\n(P6), and \"it has a more reasonable instructional structure, compared\nto most resources I found on the Internet\" (P12). Furthermore, the\nblock-based structure strengthens users‚Äô grasp of the overall lesson\nplans. ‚ÄúIt (LessonPlanner) makes me more clear about what should I\ndo in the different stages of this lesson‚Äù (E5). However, E3 offered a\ndifferent perspective on the impact of the structured outline.‚ÄúWhen\nI prepare lessons, I already have a clear framework of this lesson (ele-\nmentary science) in mind. It is synthesized from many lesson plans\nI‚Äôve read. Compared to my own framework, I personally find the gener-\nated one less effective‚Äù (E3). It reveals that the generated lesson plan\noutline in LessonPlanner, which is based on Gagne‚Äôs Nine Events,\nmay be less effective in specialized subject domains.\n6.1.2 Experts‚Äô Opinions on the Impact of Event Hints on the Teaching\nContent. The teachers in the expert interviews confirmed that the\ntags that give hints of Gagne‚Äôs Nine Events in LessonPlanner can\nguide them to prepare a lesson plan with more effective teaching\ncontent. E6 mentioned, ‚Äúwithout the system, I‚Äôd always forget to\nput some important content in the lesson plans. The hints act like\nreminders‚Äù. E1 also affirmed this point, ‚Äú(Without LessonPlanner,) I\nwouldn‚Äôt keep so many tags (instructional events)in my mind. These\nconstantly visible tags help me think and explore the possibilities of\nconducting my class from various perspectives‚Äù . However, not all\nexperts think the Events Hints generated in the outlines are useful.\nE2 and E3 complained that some hints were useless for them. E2\nsaid, ‚ÄúElementary students have limited comprehension, and a single\nmath lesson is impossible to encompass all nine events. I have to adjust\nthe events in each part to fit my need‚Äù .\nIn all, we find that LessonPlanner significantly improves partic-\nipants‚Äô quality of lesson plans in the within-subjects study. Par-\nticipants and teachers generally value the structured lesson plan\noutline and hints of Gagne‚Äôs Nine Events. Nevertheless, these events\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nSUM\n0\n5\n10\n15\n20\n25\n30\n35\n*\nObjectives \nAlignment\nSkill \nAcquisition\nT eaching Materials \nEffectiveness\nTheory / \nPractice \nBalance\nT eaching \nStrategies \nEffectiveness\nSufficient \nDetails\n0\n1\n2\n3\n4\n5\n6\n+ *\n*\n+ **\nBaseline LessonPlanner\nFigure 7: RQ1 results regarding the lesson plan outcome evaluated by experienced teachers in six different aspects and total\nscores. ***: p< 0.001, **: p< 0.01, *: p< 0.05, +: p<0.1.\nMental \nDemand\nPhysical \nDemand\nT emporal \nDemand\nOwn \nPerformance\nEffort Frustration \nLevel\n0\n1\n2\n3\n4\n5\n6 *\n* * +\n*\nFigure 8: RQ2 results regarding the test of task load. ***: p<\n0.001, **: p< 0.01, *: p< 0.05, +: p<0.1.\nmay not be always necessary if teachers have their own lesson plan\nstructures and if they want to focus on specific events in a lesson.\n6.2 Lesson Planning Process (RQ2)\nFigure 8 shows the statistical results of the participants‚Äô per-\nceived task workload in the lesson planning process with Lesson-\nPlanner and the baseline system. On average, participants perceived\nsignificantly less mental demand (LessonPlanner: ùëÄ = 2.75,ùëÜùê∑ =\n1.09; baseline: ùëÄ = 4.58,ùëÜùê∑ = 1.75;ùëä = 7.00,ùëù = 0.035), physi-\ncal demand (LessonPlanner: ùëÄ = 2.42,ùëÜùê∑ = 1.11; baseline: ùëÄ =\n3.75,ùëÜùê∑ = 1.64;ùëä = 2,ùëù = 0.024), temporal demand (LessonPlan-\nner: ùëÄ = 1.83,ùëÜùê∑ = 0.55; baseline: ùëÄ = 3.58,ùëÜùê∑ = 2.02;ùëä =\n1,ùëù = 0.027), and frustration ( LessonPlanner: ùëÄ = 2,ùëÜùê∑ = 1.35;\nbaseline: ùëÄ = 3.08,ùëÜùê∑ = 2.10;ùëä = 6,ùëù = 0.046) when they\nprepare lesson plans with LessonPlanner than with the baseline\nsystem. Besides, participants tend to perceive less effort spent\nin the lesson planning process when they are with LessonPlan-\nner (ùëÄ = 2.42,ùëÜùê∑ = 1.19) than with the baseline system ( ùëÄ =\n3.83,ùëÜùê∑ = 1.72); ùëä = 12.5,ùëù = 0.066. There is no significant differ-\nence regarding their perceived task performance in lesson planning\nbetween the LessonPlanner (ùëÄ = 6.08,ùëÜùê∑ = 0.86) and baseline\nconditions (ùëÄ = 5.17,ùëÜùê∑ = 1.57); ùëä = 14.5,ùëù = 0.175. Nine par-\nticipants believe that utilizing our system requires less workload\ncompared to the baseline in the interview after two tasks. For exam-\nple, P12 said ‚ÄúUsing ChatGPT can be quite skill-intensive, and often\nthe first response you receive may not align with your expectations.\nLessonPlanner‚Äôs first response is of high quality, which significantly\neases the mental and physical demand on me. ‚Äù .\n6.2.1 Experts‚Äô Opinions on the Workload of Preparing Lesson Plans\nwith LessonPlanner. Our within-subjects user study showcases that\nLessonPlanner can reduce task workload compared to the baseline\ninterface. In the expert interviews, we are more concerned about\nwhat brings workload to teachers when planning lessons with\nLessonPlanner. We found that the workload mainly arises from\nthe selection, refinement, and verification of LLMs‚Äô outputs. First,\nE4 and E6 indicated that the process of selecting output imposed\nan additional burden. Interestingly, they both reported that this\nburden was beneficial, as reported by E6, \"The system offers varied\ncontent, allowing me to select anything I am satisfied with. This process\nnaturally demands more effort. Before, I could only stick to one basic\nidea\". Second, E3 and E4 noted that they invested the majority of\ntheir time in ensuring that the system accurately produced a small\nsegment of the content, such as formulas, a suitable example, and\nso on. E3 complained that \"When I realized that the results provided\nby this generative model were not what I was looking for, I kept\nengaging in conversing with it and iterating the generated results.\nClearly, it takes up more time\" . Third, the experts emphasized that\nchecking the accuracy of the content (E4, E5) and making sure it\nwas appropriate for the student‚Äôs current learning stage (E2, E3)\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nalso contributed to their workload. For example, E2 stated, ‚Äú\"In\nthis example, it (LessonPlanner) uses the area calculation formula\nfor triangles as an introduction, asking students to think about the\nformula for the area of parallelograms. Yet, the students don‚Äôt know\nhow to calculate the area of triangles because it is the content in the\nnext chapter... Having to check every output makes me drained. ‚Äù .\n6.3 Perception of LessonPlanner (RQ3)\nFigure 9 shows the participants‚Äô ratings on the usability of and\ntheir trust on LessonPlanner and the baseline system in the within-\nsubjects user study. Overall, participants gave a higher score for\nLessonPlanner than that for the baseline system regarding the sys-\ntem‚Äôs Effectiveness & Learnability (LessonPlanner: ùëÄ = 72.70,ùëÜùê∑ =\n15.97; baseline: ùëÄ = 63.90,ùëÜùê∑ = 19.90;ùëä = 21,ùëù = 0.286), Effi-\nciency in Use ( LessonPlanner: ùëÄ = 73.30,ùëÜùê∑ = 18.50; baseline:\nùëÄ = 66.70,ùëÜùê∑ = 19.78;ùëä = 27,ùëù = 0.380), and Satisfaction\n(LessonPlanner: ùëÄ = 82.90,ùëÜùê∑ = 10.45; baseline: ùëÄ = 74.10,ùëÜùê∑ =\n21.62;ùëä = 20,ùëù = 0.246), but the differences are not significant.\nAs for the trust in the system, compared to the baseline system,\nparticipants generally have significantly fewer concerns regarding\nthe LessonPlanner‚Äôs vigilance (LessonPlanner: ùëÄ = 2.83,ùëÜùê∑ = 1.72;\nbaseline: ùëÄ = 4.58,ùëÜùê∑ = 1.50;ùëä = 4,ùëù = 0.028) and potential\nnegative impact on teaching (LessonPlanner: ùëÄ = 3.00,ùëÜùê∑ = 1.68;\nbaseline: ùëÄ = 4.42,ùëÜùê∑ = 1.90;ùëä = 4,ùëù = 0.048). Participants\nalso have a significantly higher level of trust on LessonPlanner‚Äôs\nethical standards (LessonPlanner: ùëÄ = 6.50,ùëÜùê∑ = 0.76; baseline:\nùëÄ = 5.67,ùëÜùê∑ = 0.85;ùëä = 3.5,ùëù = 0.020), commitment to user\n(LessonPlanner: ùëÄ = 5.92,ùëÜùê∑ = 0.76; baseline: ùëÄ = 4.67,ùëÜùê∑ =\n1.49;ùëä = 3.5,ùëù = 0.012), and reliability of outputs (LessonPlanner:\nùëÄ = 5.91,ùëÜùê∑ = 0.95; baseline: ùëÄ = 4.75,ùëÜùê∑ = 1.42;ùëä = 2,ùëù =\n0.024). Participants and teachers actively comment on the usability\nof LessonPlanner and their trust or concerns about its generated\ncontent in the interviews, which we summarize below.\n6.3.1 Experts‚Äô Gained Insights from LessonPlanner. Our teachers\nin the expert interviews appreciated that LessonPlanner can pro-\nvide inspiring content for constructing lesson plans. Specifically, all\nteachers agreed that the \"Generate an Example in Detailed\" feature\nis well-designed and useful, as it provides them with new perspec-\ntives for lesson design. For instance, E2 mentioned, ‚ÄúSometimes I\ncome up with an example but I am uncertain if it‚Äôs appropriate. I am\nalso often not sure from which angle I should present the example to\nthe students. I can get other examples from the system as a reference to\nmy example and get guidance on how to present it‚Äù . Moreover, when\nusing \"Construct multiple choice or fill-in-the-blank questions\" or\n\"Construct group discussion topics\" in their small tasks of lesson\nplanning, four experts expressed that the system output was \"very\ninspiring\". For example, E6 stated, \"I had never considered incorpo-\nrating discussion questions into the (Computer Networks) course, but\nnow it has generated a great topic I think. Discussing this topic is\nbeneficial for students, actually. \". These qualitative findings once\nagain confirm that LLMs are competent tools for promoting users‚Äô\ncreativity [21]. Novice teachers greatly benefit from the insights\ngenerated by LLMs and use them to broaden their thinking.\n6.3.2 Diverse Attitudes to the Reliability of LessonPlanner. Eight\nparticipants in the user study indicated that they were more confi-\ndent with the reliability of the generated content in LessonPlanner\ncompared to that in the baseline. For example, P9 and P12 accounted\nfor their increased confidence in the preset prompts and structured\noutput, which make LessonPlanner seem professional and reliable.\nIn contrast, one participant, P3, doubted the preset prompts,‚ÄúI would\nonly trust the system if I can access all the details of the prompts. ‚Äù .\nTeachers in the expert interviews have different opinions on the\nreliability of the LLM‚Äôs output. For example, the most frequently\ndebated issue was the credibility of the ‚ÄúGenerate definition‚Äù activ-\nity. E5, who teaches Computer Network and Convex Optimization\nat the university level, believed that the generated definitions were\naccurate and could be directly utilized in his lessons. Conversely, E2\nand E3, who are instructors in elementary school, contended that\nthe definitions did not correspond with the student‚Äôs current stage\nof learning, making them nearly impracticable for the classroom.\nThis conflicting view could be due to that our LessonPlanner was\nprimarily designed for supporting teachers in university in our\nstudy, but it reveals the need to tailor the generated content to the\nknowledge level of targeted students, which we will discuss in the\nDiscussion section.\nAnother interesting finding is that users‚Äô trust in LessonPlanner\ncould be affected by the level of their familiarity with LLMs. Par-\nticipants who interact with LLMs daily or weekly (ùëÄ = 78.0,ùëÜùê∑ =\n17.20) generally show a higher degree of trust in the system than\nthose who use it less frequently ( ùëÄ = 68.3,ùëÜùê∑ = 14.55). Addi-\ntionally, in the expert interviews, teachers (E2, E4) who do not\nknow how LLMs work and rarely used LLMs before, expressed\nless satisfaction with LessonPlanner, which could be due to their\ninappropriate trust [54]. For instance, E4 stated,\"The system needs to\ngive clear sources for its generated examples, such as from newspaper.\nI would not count on an example without credible sources, because it\nmight be misleading for students. \"\n6.3.3 Room for Improving LessonPlanner‚Äôs Usability. Eight partic-\nipants in the user study suggested that our system had room for\nimproved usability. First, the embedded editor was considered \"not\nso intelligent\" by four participants (P2, P4, P8, and P10). For ex-\nample, P2 mentioned, ‚ÄúI usually use shortcuts, but here, I have to\nuse the mouse to click. It is inconvenient. ‚Äù Second, LessonPlanner\ninteraction design may not match the routine way to use LLM. P1\nand P9, who are familiar with the ChatGPT web app, struggled with\nour system‚Äôs conversation design. ‚ÄúI am used to scrolling up to see\nwhat it (LLM) responded to before, but it does not work in this system.\nI miss that the way I do all the time with ChatGPT‚Äù (P1).\nIn the expert interviews, we further inquired about the learn-\nability of LessonPlanner. All teachers believed that the system was\neasy to use, but they reminded that it should have a clear tutorial\nto walk through the system before using it. Without a tutorial, they\nmight be confused about the system‚Äôs functions. For example, E2\nand E5 claimed that they were confused by the labels on buttons\nwithout detailed descriptions of their functions, input, and output.\nE5 suggested that ‚Äúwhen a user enters this system for the first time,\nproviding a new user tutorial or a product tour could be helpful, as\nmany websites do ‚Äù.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nEffectiveness \n& Learnability\nEfficiency \nin Use\nSatisfaction\n0\n20\n40\n60\n80\nVigilance Negative \nImpact\nEthics Commitment Output \nReliability\n0\n1\n2\n3\n4\n5\n6\n7\n* *\n*\n* *\nBaseline LessonPlanner\nFigure 9: RQ3 results regarding the test of system usability and participants‚Äô trust in LessonPlanner. ***: p< 0.001, **: p< 0.01, *:\np< 0.05, +: p<0.1.\n7 DISCUSSION\nIn this work, we design and develop LessonPlanner, an interactive\nsystem that facilitates novice teachers to prepare lesson plans with\nlarge language models (LLMs). Our within-subjects study with 12\nparticipants compared to a ChatGPT interface and our expert inter-\nviews with six teachers demonstrate the usefulness ofLessonPlanner\nin improving the outcome and reducing the task load of preparing\nlesson plans. Our user study findings also highlight the imitations,\nand opportunities of LessonPlanner, which we summarize as design\nconsiderations for future interactive systems that support teachers\nwith LLMs.\n7.1 Design Considerations\n7.1.1 Base the generated content on reliable knowledge databases.\nBoth participants and experts acknowledged the remarkable gen-\nerative capabilities of LLMs for supporting lesson-planning tasks.\nLessonPlanner can assist teachers in producing various types of\ninsightful and creative teaching materials and suggesting strategies\nto deliver these materials. Nevertheless, the reliability of the LLM\nused in LessonPlanner was still questioned by some participants and\nexperts (E2-E5), primarily because of the hallucination of LLMs [74]\nand the limitation of not being able to automatically access web re-\nsources. Furthermore, P12 and E3 expressed a continued preference\nfor utilizing search engines to search for materials, though LLMs\nare equipped in LessonPlanner. On the one hand, the generated\ncontent may not match different educational levels, for example,\nincluding knowledge that elementary students have not yet grasped\nin previous lessons (E2). This problem stems from teachers provid-\ning only a brief description of prior knowledge in a single line on\nthe Goal-setting page (Figure 2), which is insufficient. To improve\nthe alignment of the generated content with the students‚Äô educa-\ntional levels, future work could construct a detailed knowledge\ngraph of courses at different levels and recommend prior knowl-\nedge of current course topics to guide the LessonPlanner. On the\nother hand, the preference stemmed from their belief that despite\nthe unstructured results returned from search engines, the online\nmaterials published by other teachers are more professional, com-\npared to those generated by LLMs. Hence, content generated by\nLLMs should not constitute the entire lesson plan but rather serve\nas a supplementary resource to complement the lesson plan. This\nprinciple is fundamental in our motivation, design, and implemen-\ntation of LessonPlanner. To maximize the potential of traditional\nresources, web resources, and generated content within the system,\nwe suggest that future lesson planning systems should integrate re-\nliable knowledge databases, either prepared by teachers or curated\nfrom web resources, into the prompts to LLMs. There are already\nmany ways that help us do this, such as LangChain 5, Microsoft\nCopilot, and the recent advances in Retrieval-Augmented Genera-\ntion techniques [14, 41]. With the assistance of external resources,\nteachers can enhance their confidence in the system and decrease\nthe workload they spend on verifying the generated content.\n7.1.2 Allow flexible customization of prompts to LLMs. When de-\nsigning and developing LessonPlanner, we spend a lot of effort in\nproviding flexible user control (DG4) to the generated content, e.g.,\nusers are able to alter or cooperate with LLMs to refine the les-\nson goals in the Goal-Setting page, and users can freely modify all\ngenerated content of the lesson plan in the Lesson-Planning page.\nHowever, users hold elevated expectations regarding the customiza-\ntion of the interactive features with LLMs. For example, participants\nexpressed a desire to see and adjust our built-in prompts of the\ndefault buttons, which would make the generated content align\nmore closely with their expectations. Teachers in the expert inter-\nviews further highlighted that Instructional Events used in the LLM\n5https://github.com/langchain-ai/langchain\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nprompt for initializing the lesson plan may need to be adjustable by\nusers as well. Potential solutions to address these expectations are\nincorporating additional predefined teaching scenarios (e.g., a les-\nson after midterm exams) offered by developers and enabling users\nto customize the system‚Äôs predefined prompts. Another approach\nis to automate the prompting process [43] by utilizing course meta-\ndata to provide different predefined functionalities tailored to users.\n7.1.3 Integrate multi-modal generated content. The findings of our\nstudy demonstrate that planning with LessonPlanner leads to a no-\ntable enhancement in the quality of lesson plans and a reduction in\nteachers‚Äô workload, which can be attributed to the abundant textual\ncontent produced and structured by the LLM. An observation iden-\ntified in the expert interviews is thatLessonPlanner demonstrates its\ninterest in using multi-modal materials to teach particular knowl-\nedge points. For instance, it attempts to draw a binary tree through\na combination of text and symbols or directs the teacher to present\na video on specific topics or display images in the classroom. The\naforementioned phenomenon reminds us that integrating other\nmulti-modal models, such as text-to-image generation models [73]\nand text-to-video generation models [20], with LLMs might make\nthe content of lesson plans richer and save time by reducing the\nneed to gather supplementary materials from the web. These two\npoints were further confirmed by the experts in the interviews,\nwhere they hoped to insert generated images into the lesson plans.\n7.2 Generality\nIn the within-subjects study, LessonPlanner demonstrated strong\nperformance, as no participants raised concerns about the suitabil-\nity of the content delivered by LLM for students at their current\nstage. The teachers of different educational levels in the expert\ninterviews have expressed overall satisfaction with the system‚Äôs\nperformance, despite that the examples in the prompt templates of\nLessonPlanner are about the subjects of computer science for uni-\nversity students. This suggests that our system has the potential to\nbe adapted to various educational levels and subjects. However, our\nteachers reported that they occasionally got generated definitions\nor examples unaligned with the students‚Äô knowledge background.\nTo mitigate this issue, two feasible solutions can be considered.\nFirst, the simplest and most practical approach could be encour-\naging users to customize examples or definitions as templates and\nembed them into the original prompts. This will guide LLMs to gen-\nerate content that is aligned with the provided templates. Second,\nthe Chain-of-Thought (CoT) [66] technique could be employed in\nprompt engineering, instead of providing fixed examples (e.g., data\nstructures) which might lead to content that is inappropriate for\nthe educational level or subject. For example, the prompt shown\nin subsubsection 4.3.3 could be edited as below.\n‚Ä¢Q1: I will instruct the course of {lesson name} for students\nin {students‚Äô educational level}. Please provide the names of\nthree key concepts that students may need to learn for this\ncourse.\n‚Ä¢Q2 (after receiving the response to Q1 from the LLMs): Pro-\nvide the specific definition of the first concept that is suitable\nfor the {students‚Äô educational level}. The response must con-\nform to the following format.\nInput:\nContext: the name of the concept\nOutput:**the name of the concept ** -Definition: The defini-\ntion of the concept.\nMoreover, LessonPlanner serves not only as an assistant in prepar-\ning lesson plans for courses but also as a platform helping novice\nteachers promote their critical thinking and delve deeper into the\nsubjects. It assists educators in gaining the knowledge of develop-\ning a lesson based on educational theory in a systematic way, thus\nimproving their pedagogical abilities.\n7.3 Limitations and Future Work\nOur work has several limitations that call for future work. First, all\nparticipants in our within-subjects study majored in subjects related\nto computer science, which helps to control the topics of planned\nlessons in the study. While we qualitatively evaluate LessonPlanner\nwith six experts teaching different subjects at various educational\nlevels, we lack quantitative findings on LessonPlanner‚Äôs effective-\nness in facilitating users with diverse backgrounds in planning\nlessons on other subjects. Second, LessonPlanner, which includes\ndetailed instructional events and delivery methods, is presently\noriented towards assisting novice teachers. In the future, we will\nevaluate and customize LessonPlanner with teachers with diverse\nbackgrounds and teaching experience. Third, while the output mark-\ndown file of the lesson plans from LessonPlanner can serve as the\nintermediate materials to deliver a course, teachers usually need to\nprepare other digital materials (e.g., slides) upon the lesson plans in\ntheir teaching practices. Future researchers can explore the applica-\ntion of deep learning techniques to generate slides [26, 75] based on\nthe output lesson plans from LessonPlanner or support users to pre-\npare a lesson in Powerpoint or Google slides embedded with LLMs.\nFourth, we invite a teacher who is experienced with the course top-\nics to evaluate the outcome of lesson plans in the within-subjects\nstudy. However, a more ideal way to assess the effectiveness of a\nlesson plan would be to enact the plan in a real-world lesson and\ncollect feedback from students and teachers. In the future, we plan\nto invite teachers to use LessonPlanner to prepare for the lessons\nthey are going to teach and evaluate LessonPlanner‚Äôs effectiveness\nafter the outcome lesson plans are enacted in a course. Recordings\nof the lessons and post-lesson interviews with the teachers will\nalso be collected and analyzed to ensure LessonPlanner‚Äôs practical\nusability and benefits for educators. Last but not least, to generate\nan initial lesson plan, we included all nine events in the prompt.\nWhile nine events are generally helpful, not all events are applica-\nble to one specific lesson. Future iteration of LessonPlanner should\nallow the users to specify the events they would like to incorporate\ninto their course before prompting the LLMs to generate a lesson\nplan.\n8 CONCLUSION\nIn this paper, we designed and developed an interactive system,\nLessonPlanner, to support novice teachers in interactively construct-\ning lesson plans using LLM-generated content grounded on Gagne‚Äôs\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nnine events. LessonPlanner is capable of initializing lesson goals and\noutlines, generating activities and materials that adapt to planned\nteaching events, and offering flexible user control to customize the\nlesson plan and query the LLM. The within-subjects experiment\nwith 12 participants demonstrates thatLessonPlanner leads to an en-\nhancement in the quality of the lesson plan outcome and eases their\nworkload during the preparation process. We further conducted\nexpert interviews with six teachers who highlighted the potential\nof utilizing it to support novice teachers at various educational\nlevels and in diverse subjects. We discuss design considerations\nand insights derived from the user study for leveraging generative\nmodels to support lesson-planning tasks.\n9 ACKNOWLEDGEMENT\nThis work is supported by the Young Scientists Fund of the Na-\ntional Natural Science Foundation of China (NSFC) with Grant No.:\n62202509, NSFC Grant No.: U22B2060, and the General Projects\nFund of the Natural Science Foundation of Guangdong Province in\nChina with Grant No. 2024A1515012226.\nREFERENCES\n[1] R Abdelghani, YH Wang, X Yuan, T Wang, H Sauz√©on, and PY Oudeyer. 2022.\nGPT-3-driven pedagogical agents for training children‚Äôs curious question-asking\nskills. ArXiv. preprint arXiv 2211 (2022).\n[2] Mahmoud Abdulwahed and Zoltan K Nagy. 2009. Applying Kolb‚Äôs experiential\nlearning cycle for laboratory education. Journal of engineering education 98, 3\n(2009), 283‚Äì294.\n[3] Ashraf Alam. 2023. Intelligence unleashed: An argument for AI-enabled learning\necologies with real world examples of today and a peek into the future. In AIP\nConference Proceedings , Vol. 2717. AIP Publishing.\n[4] Alages Andre. 2011. The iLessonPlan: a lesson planning tool for the 21 st century.\nIn Proceedings of the Annual Conference of the Australasian Society for Computers\nin Learning in Tertiary Education, ASCILITE 2011 . 93‚Äì105.\n[5] Shamsa Aziz, Munazza Mahmood, and Zahra Rehman. 2018. Implementation\nof CIPP Model for Quality Evaluation at School Level: A Case Study. Journal of\nEducation and Educational Development 5, 1 (2018), 189‚Äì206.\n[6] Anna L Ball, Neil A Knobloch, and Sue Hoop. 2007. The instructional planning\nexperiences of beginning teachers. Journal of Agricultural Education 48, 2 (2007),\n56‚Äì65.\n[7] Jan Philip Bernius, Stephan Krusche, and Bernd Bruegge. 2022. Machine learning\nbased feedback on textual student answers in large courses. Computers and\nEducation: Artificial Intelligence 3 (2022), 100081.\n[8] Shravya Bhat, Huy A Nguyen, Steven Moore, John Stamper, Majd Sakr, and\nEric Nyberg. 2022. Towards automated generation and evaluation of questions\nin educational domains. In Proceedings of the 15th international conference on\neducational data mining , Vol. 701.\n[9] Mary Kalantzis Bill Cope and Duane Searsmith. 2021. Artificial intelligence\nfor education: Knowledge and its assessment in AI-enabled learning ecologies.\nEducational Philosophy and Theory 53, 12 (2021), 1229‚Äì1245. https://doi.org/10.\n1080/00131857.2020.1728732\n[10] Sairavi Kiran Biri, Subir Kumar, Muralidhar Panigrahi, Shaikat Mondal, Joshil Ku-\nmar Behera, Himel Mondal, and Joshil K Behera IV. 2023. Assessing the Utilization\nof Large Language Models in Medical Education: Insights From Undergraduate\nMedical Students. Cureus 15, 10 (2023).\n[11] Euan Bonner, Ryan Lege, and Erin Frazier. 2023. Large Language Model-Based\nArtificial Intelligence in the Language Classroom: Practical Ideas for Teaching.\nTeaching English with Technology 23, 1 (2023), 23‚Äì41.\n[12] Virginia Braun and Victoria Clarke. 2012. Thematic analysis . American Psycho-\nlogical Association.\n[13] John Brooke. 2013. SUS: a retrospective. Journal of usability studies 8, 2 (2013),\n29‚Äì40.\n[14] Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. 2022. Recent advances in\nretrieval-augmented text generation. In Proceedings of the 45th international ACM\nSIGIR conference on research and development in information retrieval . 3417‚Äì3419.\n[15] Brendan Calandra, Laurie Brantley-Dias, and Kezia McNeal. 2007. An electronic\nsystem to support novice teachers‚Äô reflective lesson design. Multicultural Educa-\ntion & Technology Journal 1, 2 (2007), 100‚Äì111.\n[16] Jerry CK Chan, Yaowei Wang, Qing Li, George Baciu, Jiannong Cao, Xiao Huang,\nRichard Chen Li, and Peter HF Ng. 2022. Intelligent instructional design via\ninteractive knowledge graph editing. In International Conference on Web-Based\nLearning. Springer, 41‚Äì52.\n[17] Hung Chau, Jordan Barria-Pineda, and Peter Brusilovsky. 2017. Content wizard:\nconcept-based recommender system for instructors of programming courses.\nIn Adjunct Publication of the 25th Conference on User Modeling, Adaptation and\nPersonalization. 135‚Äì140.\n[18] Qiaoyi Chen, Siyu Liu, Kaihui Huang, Xingbo Wang, Xiaojuan Ma, Junkai Zhu,\nand Zhenhui Peng. 2024. RetAssist: Facilitating Vocabulary Learners with Genera-\ntive Images in Story Retelling Practices. InProceedings of the 2024 ACM Designing\nInteractive Systems Conference . 2019‚Äì2036.\n[19] Chun-Wei Chiang, Zhuoran Lu, Zhuoyan Li, and Ming Yin. 2024. Enhancing\nAI-Assisted Group Decision Making through LLM-Powered Devil‚Äôs Advocate.\n(2024).\n[20] Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng, Lik-Hang\nLee, Tae-Ho Kim, Choong Seon Hong, and Chaoning Zhang. 2024. Sora as an AGI\nWorld Model? A Complete Survey on Text-to-Video Generation. arXiv preprint\narXiv:2403.05131 (2024).\n[21] Geoff Davis, Mick Grierson, et al. 2022. Investigating attitudes of professional\nwriters to GPT text generation AI based creative support tools. (2022).\n[22] Paul Denny, Sumit Gulwani, Neil T Heffernan, Tanja K√§ser, Steven Moore, Anna N\nRafferty, and Adish Singla. 2024. Generative AI for Education (GAIED): Advances,\nOpportunities, and Challenges. arXiv preprint arXiv:2402.01580 (2024).\n[23] Samantha L Dias-Lacy and Ruth V Guirguis. 2017. Challenges for New Teachers\nand Ways of Coping with Them. Journal of Education and Learning 6, 3 (2017),\n265‚Äì272.\n[24] Christopher B Divito, Bryan M Katchikian, Jenna E Gruenwald, and Jennifer M\nBurgoon. 2024. The tools of the future are the challenges of today: The use of\nChatGPT in problem-based learning medical education. Medical Teacher 46, 3\n(2024), 320‚Äì322.\n[25] Mary Forehand. 2010. Bloom‚Äôs taxonomy. Emerging perspectives on learning,\nteaching, and technology 41, 4 (2010), 47‚Äì56.\n[26] Tsu-Jui Fu, William Yang Wang, Daniel McDuff, and Yale Song. 2022. Doc2ppt: Au-\ntomatic presentation slides generation from scientific documents. In Proceedings\nof the AAAI Conference on Artificial Intelligence , Vol. 36. 634‚Äì642.\n[27] Ebrahim Gabajiwala, Priyav Mehta, Ritik Singh, and Reeta Koshy. 2022. Quiz\nmaker: Automatic quiz generation from text using NLP. In Futuristic Trends in\nNetworks and Computing Technologies: Select Proceedings of Fourth International\nConference on FTNCT 2021 . Springer, 523‚Äì533.\n[28] Robert Gagne. 1985. The conditions of learning and theory of instruction Robert\nGagn√©. New York, NY: Holt, Rinehart ja Winston (1985).\n[29] Qiushi Han, Haitong Chen, Haoxiang Fan, and Zhenhui Peng. 2023. ReDBot:\nExploring Conversational Recommendation for Decision-Making Support in\nGroup Chats. In Proceedings of the Eleventh International Symposium of Chinese\nCHI. 73‚Äì80.\n[30] Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task\nLoad Index): Results of empirical and theoretical research. In Advances in psy-\nchology. Vol. 52. Elsevier, 139‚Äì183.\n[31] Kenneth H Hoover and Paul M Hollingsworth. 1970. Learning and teaching in\nthe elementary school. (No Title) (1970).\n[32] Christina L Jacobs, Sonya N Martin, and Tracey C Otieno. 2008. A science lesson\nplan analysis instrument for formative and summative program evaluation of a\nteacher education program. Science education 92, 6 (2008), 1096‚Äì1126.\n[33] Maurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalmanson, and Mor Naaman.\n2023. Co-writing with opinionated language models affects users‚Äô views. In\nProceedings of the 2023 CHI conference on human factors in computing systems .\n1‚Äì15.\n[34] Jiun-Yin Jian, Ann M Bisantz, and Colin G Drury. 2000. Foundations for an\nempirically determined scale of trust in automated systems. International journal\nof cognitive ergonomics 4, 1 (2000), 53‚Äì71.\n[35] Peiling Jiang, Jude Rayan, Steven P Dow, and Haijun Xia. 2023. Graphologue: Ex-\nploring large language model responses with interactive diagrams. InProceedings\nof the 36th Annual ACM Symposium on User Interface Software and Technology .\n1‚Äì20.\n[36] Sushma N Jogan. 2019. An Effective 5 E Lesson Plan in Teaching Prose: A Model.\nOnline Submission 6, 50 (2019), 11999‚Äì12009.\n[37] Kayvan Khadjooi, Kamran Rostami, and Sauid Ishaq. 2011. How to use Gagne‚Äôs\nmodel of instructional design in teaching psychomotor skills. Gastroenterology\nand hepatology from bed to bench 4, 3 (2011), 116.\n[38] Osama Koraishi. 2023. Teaching English in the age of AI: Embracing ChatGPT to\noptimize EFL materials and assessment. Language Education and Technology 3, 1\n(2023).\n[39] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie\nDe Leon, Camille Elepa√±o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido,\nJames Maningo, et al. 2023. Performance of ChatGPT on USMLE: potential for\nAI-assisted medical education using large language models. PLoS digital health 2,\n2 (2023), e0000198.\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\n[40] Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing a human-\nai collaborative writing dataset for exploring language model capabilities. In\nProceedings of the 2022 CHI conference on human factors in computing systems .\n1‚Äì19.\n[41] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459‚Äì9474.\n[42] Michael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack\nWilliams, Neil Toronto, and Andrew D Gordon. 2023. ‚ÄúWhat it wants me to\nsay‚Äù: Bridging the abstraction gap between end-user programmers and code-\ngenerating large language models. In Proceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems . 1‚Äì31.\n[43] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\n1‚Äì35.\n[44] Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, and Richard Evans. 2023.\nCo-writing screenplays and theatre scripts with language models: Evaluation\nby industry professionals. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems . 1‚Äì34.\n[45] Chancharik Mitra, Mihran Miroyan, Rishi Jain, Vedant Kumud, Gireeja Ranade,\nand Narges Norouzi. 2024. RetLLM-E: Retrieval-Prompt Strategy for Question-\nAnswering on Student Discussion Forums. In Proceedings of the AAAI Conference\non Artificial Intelligence , Vol. 38. 23215‚Äì23223.\n[46] Reza Hadi Mogavi, Chao Deng, Justin Juho Kim, Pengyuan Zhou, Young D\nKwon, Ahmed Hosny Saleh Metwally, Ahmed Tlili, Simone Bassanelli, Antonio\nBucchiarone, Sujit Gujar, et al. 2023. Exploring user perspectives on chatgpt:\nApplications, perceptions, and implications for ai-integrated education. arXiv\npreprint arXiv:2305.13114 (2023).\n[47] Reza Hadi Mogavi, Chao Deng, Justin Juho Kim, Pengyuan Zhou, Young D\nKwon, Ahmed Hosny Saleh Metwally, Ahmed Tlili, Simone Bassanelli, Antonio\nBucchiarone, Sujit Gujar, et al . 2024. ChatGPT in education: A blessing or a\ncurse? A qualitative study exploring early adopters‚Äô utilization and perceptions.\nComputers in Human Behavior: Artificial Humans 2, 1 (2024), 100027.\n[48] Ethan R Mollick and Lilach Mollick. 2023. Using AI to implement effective\nteaching strategies in classrooms: Five strategies, including prompts. Including\nPrompts (March 17, 2023) (2023).\n[49] Ali Jamali Nesari and Mina Heidari. 2014. The important role of lesson plan on\neducational achievement of Iranian EFL teachers‚Äô attitudes. International Journal\nof Foreign Language Teaching & Research 3, 5 (2014), 25‚Äì31.\n[50] Hanna-Liisa Pender, Lennart Bohl, Marius Sch√∂nberger, and Julia Knopf. 2022.\nAn AI-based lesson planning software to support competence-based learning. In\n8th International Conference on Higher Education Advances (HEAd‚Äô22) . Editorial\nUniversitat Polit√®cnica de Val√®ncia, 1033‚Äì1041.\n[51] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The impact\nof ai on developer productivity: Evidence from github copilot. arXiv preprint\narXiv:2302.06590 (2023).\n[52] Savvas Petridis, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren\nHenderson, Stan Jastrzebski, Jeffrey V Nickerson, and Lydia B Chilton. 2023.\nAnglekindling: Supporting journalistic angle ideation with large language models.\nIn Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems .\n1‚Äì16.\n[53] I Gede Widiartana Putra, I Wayan Sukra Warpala, I Gde Wawan Sudatha, and\nNi Putu Rara Stianing Utami. 2022. Developing a Gagne Theory-Based Learning\nVideo for Thematic Subject in Elementary School. International Research Journal\nof Management, IT and Social Sciences 9, 4 (2022), 666‚Äì675.\n[54] Crystal Qian and James Wexler. 2024. Take It, Leave It, or Fix It: Measuring Pro-\nductivity and Trust in Human-AI Collaboration. arXiv preprint arXiv:2402.18498\n(2024).\n[55] Md Mostafizer Rahman and Yutaka Watanobe. 2023. ChatGPT for education\nand research: Opportunities, threats, and strategies. Applied Sciences 13, 9 (2023),\n5783.\n[56] Rishabh Ranawat, Ashwin Venkataraman, and Lakshminarayanan Subramanian.\n2021. Collectiveteach: a system to generate and sequence web-annotated lesson\nplans. In ACM SIGCAS Conference on Computing and Sustainable Societies . 1‚Äì13.\n[57] Nabil Y Razzouk, Jay N Razzouk, et al. 2008. Analysis In Teaching With Cases: A\nRevisit To Blooms Taxonomy Of Learning Objectives. College Teaching Methods\n& Styles Journal (CTMS) 4, 1 (2008), 49‚Äì56.\n[58] Aslina Saad and Christian Dawson. 2018. Requirement elicitation techniques\nfor an improved case based lesson planning system. Journal of Systems and\nInformation Technology 20, 1 (2018), 19‚Äì32.\n[59] Amanda G Sawyer and Joy Myers. 2018. Seeking comfort: How and why pre-\nservice teachers use internet resources for lesson planning. Journal of Early\nChildhood Teacher Education 39, 1 (2018), 16‚Äì31.\n[60] Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun\nLiu. 2021. Generate & rank: A multi-task framework for math word problems.\narXiv preprint arXiv:2109.03034 (2021).\n[61] Hana Stein, Irina Gurevich, and Dvora Gorev. 2020. Integration of technology by\nnovice mathematics teachers‚Äìwhat facilitates such integration and what makes\nit difficult? Education and Information Technologies 25, 1 (2020), 141‚Äì161.\n[62] Sven Strickroth. 2019. PLATON: Developing a graphical lesson planning system\nfor prospective teachers. Education Sciences 9, 4 (2019), 254.\n[63] Daniel L Stufflebeam. 2000. The CIPP model for evaluation. In Evaluation models:\nViewpoints on educational and human services evaluation . Springer, 279‚Äì317.\n[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[65] Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with\nmobile ui using large language models. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems . 1‚Äì17.\n[66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824‚Äì24837.\n[67] Judy W Wood and Jennifer W Miederhoff. 1988. Adapting lesson plans for the\nmainstreamed student. The Clearing House 61, 6 (1988), 269‚Äì276.\n[68] Robert F Woolson. 2007. Wilcoxon signed-rank test.Wiley encyclopedia of clinical\ntrials (2007), 1‚Äì3.\n[69] Meng Xia, Qian Zhu, Xingbo Wang, Fei Nie, Huamin Qu, and Xiaojuan Ma. 2022.\nPersua: A Visual Interactive System to Enhance the Persuasiveness of Arguments\nin Online Discussion. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 319\n(nov 2022), 30 pages. https://doi.org/10.1145/3555210\n[70] JIAYU YIN, CATHERINE GU, JENNY MAR, SYDNEY ZHANG, and STEVEN P\nDOW. 2024. Jamplate: Exploring LLM-Enhanced Templates for Idea Reflection.\n(2024).\n[71] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft:\nstory writing with large language models. In 27th International Conference on\nIntelligent User Interfaces . 841‚Äì852.\n[72] Ismail Md Zain. 2017. The Collaborative Instructional Design System (CIDS):\nVisualizing the 21st Century Learning. Universal Journal of Educational Research\n5, 12 (2017), 2259‚Äì2266.\n[73] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon.\n2023. Text-to-image diffusion model in generative ai: A survey. arXiv preprint\narXiv:2303.07909 (2023).\n[74] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting\nHuang, Enbo Zhao, Yu Zhang, Yulong Chen, et al . 2023. Siren‚Äôs song in the\nAI ocean: a survey on hallucination in large language models. arXiv preprint\narXiv:2309.01219 (2023).\n[75] Chengbo Zheng, Dakuo Wang, April Yi Wang, and Xiaojuan Ma. 2022. Telling\nstories from computational notebooks: Ai-assisted presentation slides creation\nfor presenting data science work. In Proceedings of the 2022 CHI Conference on\nHuman Factors in Computing Systems . 1‚Äì20.\nLessonPlanner: Assisting Novice Teachers to Prepare Pedagogy-Driven Lesson Plans with Large Language Models UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nA INTERFACES OF LESSONPLANNER\nHIGH-FIDELITY PROTOTYPE IN\nFORMATIVE STUDY\nThe original version used for the actual formative study is in Chi-\nnese. We translate all the text in the interface into English com-\npletely, as shown in Figure 10- Figure 13.\nFigure 10: The page of meta-data collection.\nFigure 11: The page of outline overview.\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Haoxiang Fan and Guanzheng Chen et al.\nFigure 12: The editing page with LLM sidebar.\nFigure 13: The editing page with embedded LLM assistant."
}