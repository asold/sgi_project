{
  "title": "Probing Contextual Language Models for Common Ground with Visual Representations",
  "url": "https://openalex.org/W3094503863",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5068360032",
      "name": "Gabriel Ilharco",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5030637764",
      "name": "Rowan Zellers",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5101576595",
      "name": "Ali Farhadi",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A5082305994",
      "name": "Hannaneh Hajishirzi",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2994803089",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3096826274",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2963871344",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W2963777148",
    "https://openalex.org/W2789366140",
    "https://openalex.org/W2978491132",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W3096655658",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2252238675",
    "https://openalex.org/W2009942101",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3016923549",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963854535",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3020712669",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W2250742840",
    "https://openalex.org/W38804426",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2568389463",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2899335602",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3010937474",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3188447078",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2963583512",
    "https://openalex.org/W2950761309",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W2049705550",
    "https://openalex.org/W2124033848",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964121744"
  ],
  "abstract": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5367–5377\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5367\nProbing Contextual Language Models for Common Ground\nwith Visual Representations\nGabriel Ilharco Rowan Zellers Ali Farhadi Hannaneh Hajishirzi\nPaul G. Allen School of Computer Science & Engineering\nUniversity of Washington\n{gamaga,rowanz,ali,hannaneh}@cs.washington.edu\nAbstract\nThe success of large-scale contextual language\nmodels has attracted great interest in probing\nwhat is encoded in their representations. In\nthis work, we consider a new question: to what\nextent contextual representations of concrete\nnouns are aligned with corresponding visual\nrepresentations? We design a probing model\nthat evaluates how effective are text-only rep-\nresentations in distinguishing between match-\ning and non-matching visual representations.\nOur ﬁndings show that language representa-\ntions alone provide a strong signal for retriev-\ning image patches from the correct object cat-\negories. Moreover, they are effective in re-\ntrieving speciﬁc instances of image patches;\ntextual context plays an important role in this\nprocess. Visually grounded language models\nslightly outperform text-only language models\nin instance retrieval, but greatly under-perform\nhumans. We hope our analyses inspire future\nresearch in understanding and improving the\nvisual capabilities of language models.\n1 Introduction\nContextual language models trained on text-only\ncorpora are prevalent in recent natural language\nprocessing (NLP) literature (Devlin et al., 2019;\nLiu et al., 2019b; Lan et al., 2019; Raffel et al.,\n2019). Understanding what their representations\nencode has been the goal of a number of recent\nstudies (Belinkov and Glass, 2019; Rogers et al.,\n2020). Yet, much is left to be understood about\nwhether—or to what extent—these models can en-\ncode visual information.\nWe study this problem in the context of lan-\nguage grounding (Searle et al., 1984; Harnad, 1990;\nMcClelland et al., 2019; Bisk et al., 2020; Ben-\nder and Koller, 2020), empirically investigating\nwhether text-only representations can naturally be\nconnected to the visual domain, without explicit\nvisual supervision in pre-training.\nFigure 1: We introduce a probing mechanism that\nlearns a mapping from contextual language represen-\ntations to visual features. For a number of contextual\nlanguage models, we evaluate how useful their repre-\nsentations are for retrieving matching image patches.\nWe argue that context plays a signiﬁcant role in\nthis investigation. In language, the ability to form\ncontext-dependent representations has shown to be\ncrucial in designing pre-trained language models\n(Peters et al., 2018; Devlin et al., 2019). This is\neven more important for studying grounding since\nmany visual properties depend strongly on con-\ntext (Sadeghi and Farhadi, 2011). For instance,\na “flying bat” shares very few visual similarities\nwith a “baseball bat”; likewise, a “dog sleeping”\nlooks different from a “dog running”. While align-\nments between language representations and visual\nattributes have attracted past interest (Leong and\nMihalcea, 2011; Lazaridou et al., 2014, 2015; Lucy\nand Gauthier, 2017; Collell Talleda et al., 2017),\nthe role of context has been previously overlooked,\nleaving many open questions about what visual\ninformation contextual language representations\nencode.\nIn this work, we introduce a method for empiri-\ncally probing contextual language representations\nand their relation to the visual domain. In general,\nprobing examines properties for which the models\nare not designed to predict, but can be encoded in\ntheir representations (Shi et al., 2016; Rogers et al.,\n5368\nFigure 2: Examples of retrieved image patches from text-only representations using our probe. All shown images\nare retrieved from MS-COCO (Lin et al., 2014), using representations from BERT base. Importantly, these object\ncategories (e.g. kite) are previously unseen by our probe. On the bottom rows, we show examples of the inﬂuence\nof context in retrieval: while all retrieved image patches belong to the correct object category,cat, more descriptive\ncontexts allow more accurate retrieval at the instance level.\n2020). Here, our probe is a lightweight model\ntrained to map language representations of con-\ncrete objects to corresponding visual representa-\ntions. The probe (illustrated in Figure 1) measures\nwhether language representations can be used to\ngive higher scores to matching visual representa-\ntions compared to mismatched ones.\nTextual and visual representations are collected\nfrom image captioning data, where we ﬁnd pairs\nof concrete words (e.g. cat or kite) and their cor-\nresponding image patches. The probe is trained\nusing a contrastive loss (Oord et al., 2018) that\ngauges the mutual information between the lan-\nguage and visual representations. Given text-only\nrepresentations of an unseen object category, the\ntrained probe is evaluated by retrieving correspond-\ning image patches for categories it has never seen\nduring training. Qualitative examples can be found\nin Figure 2.\nWe examine representations from a number\nof contextual language models including BERT,\nRoBERTa, ALBERT and T5 (Devlin et al., 2019;\nLiu et al., 2019b; Lan et al., 2019; Raffel et al.,\n2019). For all of them, we ﬁnd that interesting map-\npings can be learned from language to visual repre-\nsentations, as illustrated in Figure 2. In particular,\nusing its top-5 predictions, BERT representations\nretrieve the correctly paired visual instance 36%\nof the time, strongly outperforming non-contextual\nlanguage models (e.g., GloVe (Pennington et al.,\n2014)). Moreover, for all examined models, im-\nage patches of the correct object category are re-\ntrieved with a recall of 84-90%. Our experiments\nare backed by a control task where visual repre-\nsentations are intentionally mismatched with their\ntextual counterparts. Retrieval performance drops\nsubstantially in these settings, attesting the selec-\ntivity of our probe.\nMoreover, we measure the impact of context\non retrieval at the instance level. Contextual mod-\nels substantially outperform non-contextual embed-\ndings, but this difference disappears as context is\ngradually hidden from contextual models. When\nthe context includes adjectives directly associated\nwith the noun being inspected, we ﬁnd signiﬁcantly\nbetter instance retrieval performance.\nFinally, we investigate a number of grounded lan-\nguage models—such as LXMERT and VILBERT\n(Tan and Bansal, 2019; Lu et al., 2019, 2020)—\nthat see visual data in training, ﬁnding them to\nslightly outperform text-only models. Contrasting\nthe learned mappings with human judgment, the\nexamined visually grounded language models sig-\nniﬁcantly underperform human subjects, exposing\nmuch room for future improvement.\n2 Related Work\nWhat is encoded in language representations?\nUnderstanding what information NLP models en-\ncode has attracted great interest in recent years\n5369\n(Rogers et al., 2020). From factual (Petroni et al.,\n2019; Jawahar et al., 2019; Roberts et al., 2020) to\nlinguistic (Conneau et al., 2018; Liu et al., 2019a;\nTalmor et al., 2019) and commonsense (Forbes\net al., 2019) knowledge, a wide set of properties\nhave been previously analysed. We refer to Be-\nlinkov and Glass (2019) and Rogers et al. (2020)\nfor a more comprehensive literature review. A com-\nmon approach, often used for inspecting contextual\nmodels, is probing (Shi et al., 2016; Adi et al., 2016;\nConneau et al., 2018; Hewitt and Liang, 2019). In\nshort, it consists of using supervised models to pre-\ndict properties not directly inferred by the models.\nProbing is typically used in settings were discrete,\nlinguistic annotations such as parts of speech are\navailable. Our approach differs from previous work\nin both scope and methodology, using a probe to\nmeasure similarities with continuous, visual repre-\nsentations. Closer to our goal of better understand-\ning grounding is the work of Cao et al. (2020), that\ndesign probes for examining multi-modal models.\nIn contrast, our work examines text-only models\nand does not rely on their ability to process images.\nLanguage grounding. A widely investigated re-\nsearch direction aims to connect natural language\nto the physical world (Bisk et al., 2020; McClel-\nland et al., 2019; Tan and Bansal, 2019; Lu et al.,\n2019, 2020; Chen et al., 2020; Li et al., 2020; Tan\nand Bansal, 2020). This is typically done through\ntraining and evaluating models in tasks and datasets\nwhere both images and text are used, such as visual\nquestion answering (Antol et al., 2015; Hudson\nand Manning, 2019). A number of previous work\nhave investigated mappings between language and\nvisual representations or mappings from both to a\nshared space. Leong and Mihalcea (2011) investi-\ngate semantic similarities between words and im-\nages through a joint latent space, ﬁnding a positive\ncorrelation with human rated similarities. Similarly,\nSilberer and Lapata (2014) builds multi-modal rep-\nresentations by using stacked autoencoders. Socher\net al. (2013) and Lazaridou et al. (2014) show that\na shared latent space allows for zero-shot learning,\ndemonstrating some generalization to previously\nunseen objects. Lazaridou et al. (2015) construct\ngrounded word representations by exposing them\nto aligned visual features at training time. Lucy\nand Gauthier (2017) investigate how well word rep-\nresentations can predict perceptual and conceptual\nfeatures, showing that a number of such features\nare not adequately predicted. Collell Talleda et al.\n(2017) uses word embeddings to create a mapping\nfrom language to visual features, using its outputs\nto build multimodal representations. While our\nconclusions are generally aligned, our work dif-\nfers from these in two important ways. Firstly,\nprevious work studies context-independent word\nrepresentations, while our method allows analysing\nlanguage representations that depend on the context\nthey are used in. We use this to examine a number\nof trained contextual language models. Secondly,\nwhile most previous work uses these mappings for\nbuilding better grounded representations—often\ntraining the language models in the process—our\nwork focuses on using them as a tool for inspecting\nalready trained models, without modifying them.\nZero-shot detection. Recent work attempts to\nbuild object detectors that generalize to unseen ob-\nject categories, by conditioning the predictions on\nword embeddings of the class (Rahman et al., 2018;\nDemirel et al., 2018), visual attributes (Demirel\net al., 2018; Zhu et al., 2019; Mao et al., 2020) or\ntext descriptions (Li et al., 2019). In our work, we\nuse language representations of words in context\n(captions) as inputs. More fundamentally, although\nour experiments on unseen object categories can be\nused for zero-shot detection, we differ from previ-\nous work in motivation, which translates to further\nexperimental differences. Given our goal to anal-\nyse already trained models (as opposed to learning\na generalizable object detector), we train nothing\napart from a lightweight probe in our analyses.\n3 Probing contextual representations\nOur main goal is to characterize the relation be-\ntween contextual language representations and the\nvisual domain. We ﬁrst describe how language and\nvisual representations of concrete concepts can be\ncollected from image captioning datasets ( §3.1).\nNext, we design a probe that examines the relation\nbetween these representations, learning a mapping\nfrom language to visual representations (§3.2). An\noverview is illustrated in Figure 3.\n3.1 Collecting data\nAt the center of our analysis are contextual repre-\nsentations of visually observable nouns, which we\nrefer to asobject categories. Here, we describe how\npairs of matching language and visual representa-\ntions (ℓ,v) are collected from image captioning\ndatasets.\n5370\nFigure 3: An overview of the proposed probing procedure. Frozen language and vision models ( Λ and Θ) extract\nrepresentations from matching pairs of words in text and objects in images. A probe Ψθ is trained to map repre-\nsentations from text (green) to visual (blue) domains while maximally preserving mutual information. For a given\nlanguage representation ℓi, the loss (Equation 1) drives the probe’s outputs ˆvi=Ψθ(ℓi) to be maximally useful for\nﬁnding the aligned visual representation vi given all other visual representations in the batch ( VNEG\ni = vj,i ̸= j).\nFor such, only the pair-wise dot products ⟨ˆvi, vj⟩are required (red).\nLanguage representations (ℓ) are extracted\nfrom image captions. To accommodate recent\nlanguage models and tokenizers, we allow such\nrepresentations to be contextual and have variable\nlength,1 where each element in ℓhas a ﬁxed dimen-\nsion dL. The length of the representations ℓ for\neach object category is determined by the tokenizer.\nWe treat a model that extracts representations from\ntext as a function Λ that maps a string o(here, ob-\nject categories) in a larger textual context c(here,\ncaptions) to the representation ℓ= Λ(o|c). This\nformalism also encompasses non-contextual em-\nbeddings, with Λ(o|c) = Λ(o).\nVisual representations (v) are extracted from\nobjects in images using a trained object detection\nmodel Θ. For simplicity, we use v = Θ( o |i)\nto refer to the extracted features corresponding to\nthe detected object from image i that is both 1)\nclassiﬁed as a member of object category o and\n2) assigned the highest conﬁdence by the model\namong those. Visual representations Θ(o|i) have\nﬁxed dimensions dV.\nPaired data (ℓ,v) with aligned representations\nis collected from an image captioning dataset with\npaired captions cand images i. For each image i,\nand each object odetected by the object detector\nΘ, if oappears in some associated caption c, we\ninclude the pair (ℓ= Λ(o|c),v = Θ(o|i)). To\navoid having multiple pairs (ℓ,v) associated with\n1Conforming with sub-word tokenizers or multi-word ex-\npressions such as fire extinguisher.\nthe same visual instance, we ensure that at most\none pair (ℓ,v) per object category in each image\nis included. In this work, we use the 1600 object\ncategories from Faster R-CNN (Ren et al., 2015)\ntrained on Visual Genome (Krishna et al., 2017).\n3.2 Probing representations\nAt a high level, language representations are in-\nspected via a shallow neural probing model (Figure\n3). In training, the probe learns a mapping from lan-\nguage to visual representations (§3.2.1). We then\nevaluate the quality of these mappings by measur-\ning how well they can be used to retrieve matching\nimage patches (§3.2.2).\n3.2.1 Training the probe\nThe probe is optimized to maximally preserve the\nmutual information between the distributions of\nlanguage and visual representations. This is done\nvia InfoNCE (Oord et al., 2018) (Equation 1), a\nloss function commonly used for retrieval and\ncontrastive learning (Le-Khac et al., 2020). We\nnote the mutual information is a bottleneck on\nhow well two random variables can be mapped\nto one another, given its relation to conditional en-\ntropy. In training, the probe Ψθ with parameters θ\ntakes inputs ℓand estimates visual representations\nˆv = Ψ θ(ℓ) with the same dimensionality dV as\nthe corresponding visual representations v. For\neach pair (ℓ,v), this loss relies on a set of dis-\ntractors VNEG\nℓ , containing visual representations\nwhich are not aligned with the language represen-\ntations ℓ. The representations in VNEG\nℓ are used\n5371\nfor contrastive learning and are drawn from the\nsame visual model, using different objects or im-\nages. Minimizing this loss drives the dot product\n⟨Ψθ(ℓ) ,u⟩to be maximal for u = v and small\nfor all u∈VNEG\nℓ . In other words, training pushes\nthe estimates ˆv = Ψθ(ℓ) to be maximally useful\nin discerning between positive and negative visual\npairings.\nLInfoNCE = −Eℓ\n\nlog e⟨Ψθ(ℓ) ,vℓ⟩\n∑\nv′∈{v}⋃VNEG\nℓ\ne⟨Ψθ(ℓ) ,v′⟩\n\n\n(1)\nIn practice, the expectation in Equation 1 is\nestimated over a batch of size B with samples\nof aligned language and visual representations\n((ℓ1,v1),..., (ℓB,vB)). For efﬁciency, we use\nother visual representations in the batch as distrac-\ntors for a given representation (VNEG\ni = {vj,j ̸=\ni}). Thus, only the dot products ⟨ˆvi = Ψθ(ℓi), vj⟩\nare needed to calculate the loss, as illustrated in\nFigure 3. Importantly, we note that the models\nused to extract representations are not trained or\nchanged in any way during the probing procedure.\n3.2.2 Evaluation procedure\nFor evaluation, we compute recall in retrieving im-\nage patches given objects in text, using new pairs\nof language and visual representations from unseen\nimages and captions. Consider the set of all col-\nlected visual representations for evaluation, V. For\neach language representation ℓ, we use the trained\nprobe to generate our estimate ˆv= Ψθ(ℓ), and ﬁnd\nthe instances v′∈V that maximize the dot product\n⟨ˆv,v ′⟩. Given an integer k, we consider recall at k\nat both instance and category levels. Formally:\nInstance Recall (IR@k) measures how fre-\nquently the correct visual instance is retrieved.\nMore precisely, it is the fraction of pairs (ℓ,v)\nwhere the instance vis in the top-kvisual represen-\ntations retrieved from ˆv= Ψθ(ℓ).\nCategory Recall (CR@k) measures how fre-\nquently instances of the correct object category are\nretrieved. More precisely, it is the fraction of pairs\n(ℓ,v = Θ(o|i)) where any of the top-kretrieved\nvisual representations v′ = Θ(o′ |i′) belongs to\nthe same object category as v(i.e. o′= o).\nHigher IR and CR scores indicate better perfor-\nmance and, by deﬁnition, CR@k cannot be smaller\nthan IR@k. These metrics form the basis of our\nevaluation, and we take multiple steps to promote\nexperimental integrity. Learned mappings are eval-\nuated in two scenarios, where pairs (ℓ,v) are col-\nlected using object categories either seen or unseen\nby the probe during training. The later is the fo-\ncus of the majority of our experiments. For both\nscenarios, images and captions have no intersec-\ntion with those used in training. Further, we create\nmultiple seen/unseen splits from our data, training\nand testing on each split. We then report average\nand standard deviation of the recall scores across 5\nsplits.\n4 Experimental settings\n4.1 Language models\nThe majority of examined models are contextual\nrepresentation models based on the transformer\narchitecture (Vaswani et al., 2017) trained on text-\nonly data. We examine the base (dL = 768) and\nlarge (dL = 1024 ) versions of BERT uncased,\nRoBERTa, ALBERT and T5 (Devlin et al., 2019;\nLiu et al., 2019b; Lan et al., 2019; Raffel et al.,\n2019). For T5, we also examine the small version,\nwith dL = 512. For all these models, we use pre-\ntrained weights from the HuggingFace Transform-\ners library (Wolf et al., 2020)2, and use representa-\ntions from the last layer. Additionally, we inspect\nnon-contextual representations using GloVe embed-\ndings (Pennington et al., 2014), using embeddings\ntrained on 840 billion tokens of web data, with\ndL = 300 and a vocabulary size of 2.2 million.3\n4.2 Vision models\nAs is common practice in natural language ground-\ning literature (Anderson et al., 2018; Tan and\nBansal, 2019; Su et al., 2020; Lu et al., 2020),\nwe use a Faster R-CNN model (Ren et al., 2015)\ntrained on Visual Genome (Krishna et al., 2017) to\nextract visual features with dV = 2048. We use\nthe trained network provided by Anderson et al.\n(2018)4, and do not ﬁne-tune during probe training.\n4.3 Data\nWe collect representations from two image cap-\ntioning datasets, Flickr30k (Young et al., 2014),\nwith over 150 thousand captions and 30 thousand\nimages, and MS-COCO (Lin et al., 2014), with\n600 thousand captions and 120 thousand images\n2https://github.com/huggingface/transformers\n3https://nlp.stanford.edu/projects/glove/\n4https://github.com/peteanderson80/bottom-up-attention\n5372\nin English. The larger MS-COCO is the focus of\nthe majority of our experiments. We build disjoint\ntraining, validation and test sets from the aggre-\ngated training and validation image captions. To\nexamine generalization to new objects, we test on\nrepresentations from both seen or unseen object cat-\negories, built from images and captions not present\nin the training data. From the 1600 object cate-\ngories of our object detector, we use 1400 chosen\nat random for training and seen evaluation. The\nremaining 200 are reserved for unseen evaluation.\nFurthermore, we train and test our probe 5 times,\neach with a different 1400/200 split of the object\ncategories. For each object category split, we build\nvalidation and test sets with sizes proportional to\nthe number of object categories present: seen test\nsets contain 7000 representation pairs and unseen\ntest sets contain 1000 pairs. The validation sets\nused for development consists of seen object cate-\ngories, with the same size as the seen test sets. All\nremaining data is used for training.\n4.4 Control task\nContrasting the probe performance with a control\ntask is central to probing (Hewitt and Liang, 2019).\nWe follow this practice by learning in a control\ntask where representations are mapped topermuted\nvisual representations. More precisely, we replace\neach visual representation v = Θ(o |i) with an-\nother v′ = Θ(o′ |i′) chosen at random from an\nobject category o′= f(o) that depends on the orig-\ninal object category o. Here, f dictates a random\npermutation of the object categories. For instance,\nvisual representations of the original category cat\nare replaced with representations from a second\ncategory dog; representations from the category dog\nare replaced by those from tree, and so on.\n4.5 Implementation and hyper-parameters\nOur probe consists of a shallow neural model. To\nprocess the naturally sequential language repre-\nsentations ℓ, we use a single-layered model with\nLSTM cells (Hochreiter and Schmidhuber, 1997)\nwith 256 hidden units and only unidirectional con-\nnections. The outputs are then projected by a linear\nlayer to the visual space. The probe is trained us-\ning Adam optimizer (Kingma and Ba, 2015) with a\nlearning rate of 0.0005, weight decay of 0.0005 and\ndefault remaining coefﬁcients (β1=0.9 β2=0.999\nand ϵ=10−8). We train with a batch size of 3072,\nfor a total of 5 epochs on one GPU.\n# Experiment IR@1 IR@5 CR@1\n0 Random 0.1 ±0.1 0.5 ±0.2 6.0 ±2.0\n1 Control 0.0 ±0.0 0.4 ±0.2 3.0 ±1.4\n2 GloVe 5.1 ±0.5 18.5 ±1.4 87.3 ±3.5\n3 BERT base 12.0 ±1.0 36.0 ±0.9 88.1 ±2.4\n4 BERT large 11.6 ±0.7 34.9 ±2.6 89.3 ±2.4\n5 RoBERTa base 11.6 ±0.3 34.4 ±2.2 90.4 ±0.6\n6 RoBERTa large 10.9 ±1.1 32.8 ±2.5 88.7 ±3.2\n7 ALBERT base 8.7 ±0.2 28.8 ±1.6 84.4 ±2.1\n8 ALBERT large 9.4 ±1.0 28.8 ±2.3 84.2 ±4.2\n9 T5 small 10.1 ±0.7 32.9 ±1.5 87.2 ±4.1\n10 T5 base 10.8 ±0.8 33.3 ±2.3 85.3 ±2.8\n11 T5 large 11.8 ±0.5 34.7 ±2.1 87.2 ±2.4\nTable 1: Average instance recall (IR@k) and category\nrecall (CR@k) for test sets with unseen object cate-\ngories. For each model, we train and evaluate 5 times,\nusing different sets of object categories seen in training.\nUnlike the control task with permuted representations,\nmappings learned from sensible representations gener-\nalize well to unseen object categories.\n5 Results and discussion\nAt a high level, our experiments show that i)\nlanguage representations are strong signals for\nchoosing between different visual features both\nat the instance and category levels; ii) context is\nlargely helpful for instance retrieval; iii) InfoNCE\nworks better than other studied losses, and some\nconsistency is found across datasets; iv) visually\ngrounded models outperform text-only models; v)\nall models lag greatly behind human performance.\nWe provide further details in §5.1-5.3.\n5.1 Retrieval results\nTable 1 summarizes instance and category retrieval\nperformance for different language models and con-\ntrol experiments, using test data with unseen object\ncategories. Our results indicate that language rep-\nresentations alone are strong signals for predicting\nvisual features: for all examined language models,\nrecall scores are signiﬁcantly better than random\nand control. Qualitative results can be found in\nFigure 2. We note that category recall scores are\nsigniﬁcantly higher than instance recall. This is rea-\nsonable since there are many more positive align-\nments at the category level. Compared to other\ninspected models, BERT base shows the best re-\nsults for instance retrieval, and will be the focus of\nfurther analyses.\nContrasting the performance of non-contextual\nrepresentations from GloVe with that of contex-\ntual models shows that context considerably affects\n5373\n# Experiment IR@1 IR@5 CR@1\n0 Random 0.1 ±0.1 0.1 ±0.1 1.2 ±0.1\n1 Control 1.6 ±0.1 7.8 ±0.6 41.3 ±5.6\n2 BERT base 14.9 ±0.3 43.4 ±0.8 90.4 ±0.4\nTable 2: Average instance recall (IR@k) and category\nrecall (CR@k) for test sets with seen object categories.\nLoss function IR@1 IR@5 CR@1\nMSE 3.0 ±0.3 12.1 ±1.3 57.5 ±8.7\nNeg. cosine sim. 6.9 ±0.7 23.4 ±1.3 75.1 ±6.3\nTriplet loss 8.4 ±0.6 28.8 ±0.9 81.7 ±3.6\nInfoNCE 12.0 ±1.0 36.0 ±0.9 88.1 ±2.4\nTable 3: Comparison in retrieval performance on un-\nseen object categories for different training losses, us-\ning representations from BERT base. InfoNCE yields\nbetter results than other loss functions.\ninstance recall. For instance, GloVe and BERT\nbase yield 5.1% to 12.0% IR@1, respectively. This\ngap is sensible, since a non-contextual representa-\ntion should not be able to discern between distinct\nimage patches depicting the same object category.\nWhile still lagging behind a number of contextual\nrepresentations, we observe strong category recall\nfor GloVe, which we hypothesize is due to the ease\nin predicting the correct output category since input\nrepresentations are ﬁxed, independently of context.\nWe further explore the role of context in §5.3.\nMoreover, Table 2 shows performance on test\nsets with seen object categories. Comparing with\nTable 1, BERT representations show good general-\nization to unseen object categories. This general-\nization is consistent with previous observations on\nzero-shot experiments, using non-contextual word\nembeddings (Lazaridou et al., 2014).\nFinally, our results attest to the selectivity of the\nprobe: for the control task with permuted represen-\ntations (Tables 1 and 2, Row 1), substantially lower\nperformance is found. This gap is particularly high\nfor unseen object categories, where only sensibly\npaired representations perform better than chance.\n5.2 Ablations\nLoss ablations. In addition to InfoNCE, we ab-\nlate on 3 other loss functions: mean squared error\n(MSE), negative cosine similarity, and triplet loss5.\nThe results for unseen object categories are summa-\nrized in Table 3: while all losses yield better than\nrandom results, InfoNCE performs the best. This\n5Ltrip = Eℓ[max(δℓ,v′\nℓ\n−δℓ,vℓ +α,0)], where the margin\nαis set to 1.0, v′ ∈VNEG and δℓ,v = cos(Ψθ(ℓ),vℓ).\nDataset # Images / # Captions IR@1 CR@1\nMS-COCO 120k / 600k 12.0 ±1.0 88.1 ±2.4\nFlickr30k 30k / 150k 9.8 ±0.9 85.6 ±3.4\nTable 4: Comparison for different datasets in retrieval\nperformance of unseen object categories with represen-\ntations from BERT base. Despite large differences in\nsize, results indicate consistency across datasets.\nvalidates the theoretical intuition that InfoNCE\nwould be advantageous, as it allows for directly\noptimizing the probe to maximally preserve the\nmutual information between the representations, a\nbottleneck on the remaining entropy after the map-\nping.\nData ablations. In addition to MS-COCO,\nwhich is the used for the majority of our experi-\nments, we show results with data collected from\nthe smaller Flickr30k. We report the test retrieval\nperformance for unseen object categories using rep-\nresentations from BERT base in Table 4. These\nresults indicate consistency across the datasets, de-\nspite their considerable difference in size.\n5.3 Analyses\nInﬂuence of context. We study whether the gap\nin instance retrieval performance from GloVe and\nBERT comes from the use of context or intrinsic\ndifferences of these models. This is explored by\nmeasuring how instance recall varies as we prob-\nabilistically mask out context tokens in the cap-\ntions at different rates. As shown in Figure 4, per-\nformance drops substantially as more tokens are\nmasked; in the limit where only the object tokens\nremain (i.e. the fraction of context masked is 1.0),\nBERT’s representations perform marginally worse\nthan the non-contextual GloVe embeddings.\nFigure 5 compares instance-level retrieval accu-\nracy for representations when objects have none or\nat least one adjective associated with them, as pro-\ncessed by the dependency parser from AllenNLP\nlibrary (Gardner et al., 2018). These adjectives\ncommonly include colors (e.g. white, black) and\nsizes (e.g. big, small), indicating contextual infor-\nmation. The results show clear gains in instance\nrecall when objects are accompanied by adjectives,\nconﬁrming that context enables more accurate re-\ntrieval. We refer back to Figure 2 for qualitative\nresults on the inﬂuence of context.\nGrounded language models. We further inspect\nrepresentations from several grounded language\n5374\n0.0 0.2 0.4 0.6 0.8 1.0\nfraction of context masked\n4\n6\n8\n10\n12\n14IR@1 (%)\nModel\nBERT base\nGloVe\nTest split\nSeen\nUnseen\nFigure 4: Instance recall as context tokens are progres-\nsively masked out. Retrieval performance for BERT\nquickly degrades as higher proportions of the context\nare masked.\nIR@1 (%)\nSeen\nUnseen\n0 5 10 15 20\nno adjectives\n1+ adjectives\nFigure 5: More descriptive contexts enable more accu-\nrate retrieval. In the plot, we show instance recall at 1\nwhen object categories are or are not accompanied by\nadjectives, using representations from BERT base.\nmodels, namely LXMERT, VL-BERT (base and\nlarge) and VILBERT-MT (Tan and Bansal, 2019;\nSu et al., 2020; Lu et al., 2019, 2020)). While\nthese models typically process visual and textual\ninputs jointly, we adapt them to include only the\nlanguage branches, restricting attention to the text\ninputs. For all these models, we use the code and\nweights made public by the authors.6 The results,\nsummarized in Table 5, show that grounded models\nslightly outperform the ungrounded BERT base. At\nthe category level, we see small relative differences\nin performance between grounded and ungrounded\nmodels. At the instance level, the relative improve-\nment is higher, especially for VILBERT-MT, while\nstill much lower than human performance as shown\nin the next experiment.\nHuman performance. Finally, we contrast the\nexamined models with human performance in re-\ntrieving visual patches given words in sentences.\nSuch a comparison helps disentangling the quality\nof the learned mappings with possible incidental\nmatches, i.e., language representations with more\n6github.com/airsplay/lxmert; github.com/jackroos/VL-\nBERT; github.com/facebookresearch/vilbert-multi-task\nModel IR@1 IR@5 CR@1\nBERT base 12.0 ±1.0 36.0 ±0.9 88.1 ±2.4\nLXMERT 13.7 ±1.0 39.2 ±2.5 90.3 ±1.2\nVL-BERT base 12.5 ±1.0 37.6 ±1.1 88.7 ±1.4\nVL-BERT large 12.6 ±1.1 37.5 ±2.4 88.7 ±2.3\nVILBERT-MT 15.4 ±1.2 42.4 ±2.7 90.8 ±1.9\nTable 5: Retrieval performance for unseen object cate-\ngories, using representations from BERT and a number\nof grounded language models.\nChance BERT base VILBERT-MT Human\n1% 43% 53% 76%\nTable 6: A sizable gap in instance recall (IR@1) is seen\nby comparing the performance of humans and the ex-\namined models in a reduced test set with 100 samples.\nthan one positive visual match. As they are also\naffected by these artifacts, human subjects offer\na sensible point of comparison. In virtue of the\nlimited human attention, we evaluate on a reduced\ntest set with unseen object categories, randomly\nsampling 100 data points from it. For each object\nin a sentence, subjects are presented with 100 im-\nage patches and asked to choose the closest match.\nWe collect over 1000 annotations from 17 in-house\nannotators, with at least 30 annotations each. Our\nresults are shown in Table 6. On the same test set,\nwe ﬁnd a large gap from learned mappings for both\ngrounded and ungrounded models to human perfor-\nmance, exposing much room for improvement.\n6 Conclusion\nUnderstanding the similarities between language\nand visual representations has important implica-\ntions on the models, training paradigms and bench-\nmarks we design. We introduced a method for em-\npirically measuring the relation between contextual\nlanguage representations and corresponding visual\nfeatures. We found contextual language models\nto be useful—while far from human subjects—in\ndiscerning between different visual representations.\nMoreover, we explored how these results are in-\nﬂuenced by context, loss functions, datasets and\nexplicit grounding during training. Altogether, we\nhope that our new methodological and practical\ninsights foster further research in both understand-\ning the natural connections between language and\nvisual representations and designing more effective\nmodels at the intersection the two modalities.\n5375\nAcknowledgements\nThis research was supported by the grants from\nONR N00014-18-1-2826, DARPA N66001-19-2-\n4031, 67102239, NSF III-1703166, IIS-1652052,\nIIS-17303166, and an Allen Distinguished Inves-\ntigator Award and a Sloan Fellowship. Authors\nwould also like to thank Raymond J. Mooney and\nmembers of the UW-NLP, H2Lab and RAIVN Lab\nat the University of Washington for their valuable\nfeedback and comments.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual question an-\nswering. In Proceedings of the IEEE International\nConference on Computer Vision.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards nlu: On meaning, form, and understand-\ning in the age of data. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-\nChun Chen, and Jingjing Liu. 2020. Behind the\nscene: Revealing the secrets of pre-trained vision-\nand-language models. In Proceedings of the 2020\nEuropean Conference on Computer Vision.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Learning universal\nimage-text representations. In Proceedings of the\n2020 European Conference on Computer Vision.\nGuillem Collell Talleda, Teddy Zhang, and Marie-\nFrancine Moens. 2017. Imagined visual representa-\ntions as multimodal embeddings. In Proceedings of\nthe 2017 AAAI Conference on Artiﬁcial Intelligence.\nAAAI.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics . Association\nfor Computational Linguistics.\nBerkan Demirel, Ramazan Gokberk Cinbis, and Na-\nzli Ikizler-Cinbis. 2018. Zero-shot object detec-\ntion by hybrid region embedding. arXiv preprint\narXiv:1805.06157.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo neural language representations learn physical\ncommonsense? Proceedings of the 41st Annual\nConference of the Cognitive Science Society.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS). Association\nfor Computational Linguistics.\nStevan Harnad. 1990. The symbol grounding problem.\nPhysica D: Nonlinear Phenomena.\nJ. Hewitt and P. Liang. 2019. Designing and interpret-\ning probes with control tasks. In Proceedings of Em-\npirical Methods in Natural Language Processing.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation.\nDrew A Hudson and Christopher D Manning. 2019.\nGQA: A new dataset for real-world visual reason-\ning and compositional question answering. In Pro-\nceedings of the IEEE Conference on Computer Vi-\nsion and Pattern Recognition.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics.\nAssociation for Computational Linguistics.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Proceedings of\nthe 2015 International Conference for Learning Rep-\nresentations.\n5376\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational Journal of Computer Vision.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. In Proceedings of\nthe 2020 International Conference on Learning Rep-\nresentations.\nAngeliki Lazaridou, Elia Bruni, and Marco Baroni.\n2014. Is this a wampimuk? cross-modal map-\nping between distributional semantics and the visual\nworld. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics.\nAssociation for Computational Linguistics.\nAngeliki Lazaridou, Nghia The Pham, and Marco Ba-\nroni. 2015. Combining language and vision with a\nmultimodal skip-gram model. In Proceedings of the\n2015 Conference of the North American Chapter of\nthe Association for Computational Linguistics . As-\nsociation for Computational Linguistics.\nP. H. Le-Khac, G. Healy, and A. F. Smeaton. 2020.\nContrastive representation learning: A framework\nand review. IEEE Access.\nChee Wee Leong and Rada Mihalcea. 2011. Measur-\ning the semantic relatedness between words and im-\nages. In Proceedings of the Ninth International Con-\nference on Computational Semantics (IWCS 2011).\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, Yejin Choi,\nand Jianfeng Gao. 2020. Oscar: Object-semantics\naligned pre-training for vision-language tasks. In\nProceedings of the 2020 European Conference on\nComputer Vision.\nZhihui Li, Lina Yao, Xiaoqin Zhang, Xianzhi Wang,\nSalil Kanhere, and Huaxiang Zhang. 2019. Zero-\nshot object detection with textual descriptions. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European Confer-\nence on Computer Vision. Springer.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics . Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Proceedings of the 33rd Conference on Ad-\nvances in Neural Information Processing Systems.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 2020. 12-in-1: Multi-task\nvision and language representation learning. In Pro-\nceedings of the 2020 IEEE Conference on Computer\nVision and Pattern Recognition.\nLi Lucy and Jon Gauthier. 2017. Are distributional\nrepresentations ready for the real world? evaluat-\ning word vectors for grounded perceptual meaning.\nIn Proceedings of the First Workshop on Language\nGrounding for Robotics , pages 76–85, Vancouver,\nCanada. Association for Computational Linguistics.\nQiaomei Mao, Chong Wang, Shenghao Yu, Ye Zheng,\nand Yuqi Li. 2020. Zero-shot object detection with\nattributes based category similarity. IEEE Transac-\ntions on Circuits and Systems II: Express Briefs.\nJames L McClelland, Felix Hill, Maja Rudolph, Ja-\nson Baldridge, and Hinrich Sch ¨utze. 2019. Ex-\ntending machine language models toward human-\nlevel language understanding. arXiv preprint\narXiv:1912.05877.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the Empirical\nMethods in Natural Language Processing (EMNLP).\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics. Association for Com-\nputational Linguistics.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv e-prints.\n5377\nShaﬁn Rahman, Salman Khan, and Fatih Porikli. 2018.\nZero-shot object detection: Learning to simultane-\nously recognize and localize novel concepts. In Pro-\nceedings of the Asian Conference on Computer Vi-\nsion. Springer.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster R-CNN: Towards real-time ob-\nject detection with region proposal networks. In Ad-\nvances in Neural Information Processing Systems.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we\nknow about how BERT works. arXiv preprint\narXiv:2002.12327.\nMohammad Amin Sadeghi and Ali Farhadi. 2011.\nRecognition using visual phrases. In CVPR 2011 ,\npages 1745–1752. IEEE.\nJohn R Searle, S Willis, et al. 1984. Minds, brains, and\nscience. Harvard University Press.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In\nProceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing . Associa-\ntion for Computational Linguistics.\nCarina Silberer and Mirella Lapata. 2014. Learn-\ning grounded meaning representations with autoen-\ncoders. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguistics.\nAssociation for Computational Linguistics.\nRichard Socher, Milind Ganjoo, Christopher D Man-\nning, and Andrew Ng. 2013. Zero-shot learning\nthrough cross-modal transfer. In Advances in Neu-\nral Information Processing Systems.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn Proceedings of the 2020 International Conference\non Learning Representations.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019. olmpics–on what lan-\nguage model pre-training captures. arXiv preprint\narXiv:1912.13283.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning.\nHao Tan and Mohit Bansal. 2020. V okenization:\nImproving language understanding with contextual-\nized, visual-grounded supervision. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing. Association for Com-\nputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations . Association for Computa-\ntional Linguistics.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics.\nPengkai Zhu, Hanxiao Wang, and Venkatesh Saligrama.\n2019. Zero shot detection. IEEE Transactions on\nCircuits and Systems for Video Technology.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7537859678268433
    },
    {
      "name": "Natural language processing",
      "score": 0.6562115550041199
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5670188069343567
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5598545670509338
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5538811683654785
    },
    {
      "name": "Language model",
      "score": 0.49265748262405396
    },
    {
      "name": "Object (grammar)",
      "score": 0.46185266971588135
    },
    {
      "name": "Process (computing)",
      "score": 0.46184059977531433
    },
    {
      "name": "Visual language",
      "score": 0.4556082487106323
    },
    {
      "name": "Noun",
      "score": 0.4460674524307251
    },
    {
      "name": "Linguistics",
      "score": 0.29769402742385864
    },
    {
      "name": "Mathematics",
      "score": 0.07356852293014526
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}