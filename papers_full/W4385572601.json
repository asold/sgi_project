{
    "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
    "url": "https://openalex.org/W4385572601",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2948537229",
            "name": "Chujie Zheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3167049663",
            "name": "Sahand Sabour",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105112954",
            "name": "Jiaxin Wen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1992233393",
            "name": "Zheng Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2162268045",
            "name": "Minlie Huang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4286987939",
        "https://openalex.org/W3200833038",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2962989446",
        "https://openalex.org/W3173803111",
        "https://openalex.org/W4385574293",
        "https://openalex.org/W4285251400",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3174116563",
        "https://openalex.org/W4294410794",
        "https://openalex.org/W1975879668",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385574047",
        "https://openalex.org/W3207604419",
        "https://openalex.org/W2898875342",
        "https://openalex.org/W3201090304",
        "https://openalex.org/W3093233911",
        "https://openalex.org/W2951583236",
        "https://openalex.org/W4281493292",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4385570351",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W4287083037",
        "https://openalex.org/W3207166518",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W3103691705",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W3173540771",
        "https://openalex.org/W3200005007",
        "https://openalex.org/W3035148359",
        "https://openalex.org/W4385572996",
        "https://openalex.org/W2963903950",
        "https://openalex.org/W3093940687",
        "https://openalex.org/W2949678053",
        "https://openalex.org/W3188553726",
        "https://openalex.org/W2038411619",
        "https://openalex.org/W4206636317",
        "https://openalex.org/W3104405162",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W4285188834"
    ],
    "abstract": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models' generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1552‚Äì1568\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nAUGESC: Dialogue Augmentation with Large Language Models for\nEmotional Support Conversation\nChujie Zheng Sahand Sabour Jiaxin Wen Zheng Zhang Minlie Huang ‚àó\nThe CoAI Group, DCST, Institute for Artificial Intelligence,\nState Key Lab of Intelligent Technology and Systems,\nBeijing National Research Center for Information Science and Technology,\nTsinghua University, Beijing 100084, China\nchujiezhengchn@gmail.com aihuang@tsinghua.edu.cn\nAbstract\nCrowdsourced dialogue corpora are usually\nlimited in scale and topic coverage due to the\nexpensive cost of data curation. This would\nhinder the generalization of downstream dia-\nlogue models to open-domain topics. In this\nwork, we leverage large language models for\ndialogue augmentation in the task of emotional\nsupport conversation (ESC). By treating dia-\nlogue augmentation as a dialogue completion\ntask, we prompt a fine-tuned language model\nto complete full dialogues from available dia-\nlogue posts of various topics, which are then\npostprocessed based on heuristics. Applying\nthis approach, we construct AUGESC , an aug-\nmented dataset for the ESC task, which largely\nextends the scale and topic coverage of the\ncrowdsourced ESConv corpus. Through com-\nprehensive human evaluation, we demonstrate\nthat our approach is superior to strong baselines\nof dialogue augmentation and that AUGESC\nhas comparable dialogue quality to the crowd-\nsourced corpus. We also conduct human inter-\nactive evaluation and prove that post-training\non AUGESC improves downstream dialogue\nmodels‚Äô generalization ability to open-domain\ntopics. These results suggest the utility of\nAUGESC and highlight the potential of large\nlanguage models in improving data-scarce dia-\nlogue generation tasks.1\n1 Introduction\nCurrent open-domain dialogue corpora are com-\nmonly curated through crowdsourcing to endow di-\nalogue models with sophisticated skills (Liu et al.,\n2021; Dinan et al., 2018; Zhang et al., 2018), since\nthe desired high-quality dialogues are usually not\navailable in public sources. For example, the task\nof emotional support conversation (ESC) (Liu et al.,\n2021) aims to support help-seekers to reduce daily-\nlife emotional distress. To train downstream dia-\n‚àóCorresponding author.\n1The project repository is available at https://github.\ncom/thu-coai/AugESC.\n(1)Fine-tuningLM\n(2)DialogueCompletion\n(3)Postprocessing\nCompletedbyLM\nHuman:Human:Human:\nAI:AI:AI:\nStartingUtterance\nDialogueSamplesAutoregressiveLM\nü§ñ\nüòé\nü§îHeuristics\nFigure 1: Illustration of our approach for constructing\nAUGESC . We (1) fine-tune an autoregressive LM with\ndialogue samples, (2) prompt the LM to complete full\ndialogues from collected dialogue posts, and then (3)\npostprocess augmented dialogues based on heuristics.\nlogue models, Liu et al. (2021) crowdsourced the\nESConv dataset, which contains only 1.3K dia-\nlogue sessions covering 13 topic categories.\nThe construction of ESConv reveals typical limi-\ntations of crowdsourcing dialogue data. First, it is\ntime-consuming especially when the desired dia-\nlogues should contain long multi-turn interactions\n(e.g., the ESConv dialogues contain about 30 ut-\nterances on average). Also, it usually requires la-\nborious worker training and human screening to\nensure the high dialogue quality. Consequently, the\nexpensive data curation restricts the scale and topic\ncoverage of collected dialogues, which may hinder\nthe generalization of downstream dialogue models\nto open-domain topics.\nIn this work, we leverage large language mod-\nels for large-scale dialogue augmentation in the\nESC task. We first present a simple yet effective\n1552\napproach, which treats dialogue augmentation as a\ndialogue completion task (¬ß 3), as illustrated in Fig-\nure 1. We fine-tune the 6B GPT-J model (Wang and\nKomatsuzaki, 2021) with ESConv samples, prompt\nit to complete full dialogues on various topics us-\ning the dialogue posts of EmpatheticDialogues\n(Rashkin et al., 2019), and postprocess augmented\ndialogues based on heuristics. We thus construct\nAUGESC, an augmented dataset for the ESC task,\nwhich largely extends the scale (45x larger) and\ntopic coverage of the original ESConv dataset (¬ß 4).\nTo demonstrate the superiority of our approach,\nwe compare with strong baselines of dialogue aug-\nmentation (¬ß 5) through human evaluation. We\nshow that our approach performs substantially bet-\nter and that the quality of the constructedAUGESC\nis comparable to the crowdsourced ESConv. To\nfurther prove the utility of AUGESC , we conduct\nhuman interactive evaluation (¬ß 6) and show that\npost-training on AUGESC improves downstream\ndialogue models‚Äô generalization ability to open-\ndomain topics.\nOur contributions are summarized in four folds:\n‚Ä¢ We present a simple yet effective approach for\nlarge-scale dialogue augmentation by formulat-\ning it as a dialogue completion task.\n‚Ä¢ We release an augmented dataset AUGESC for\nthe ESC task, which is 45x the scale of the crowd-\nsourced ESConv corpus and largely extends the\nlatter‚Äôs topic coverage.\n‚Ä¢ We conduct comprehensive human evaluation,\nwhich validates the superiority of our approach\nand the reasonable quality of AUGESC.\n‚Ä¢ We conduct human interactive evaluation, which\nverifies AUGESC ‚Äôs utility in improving down-\nstream dialogue models‚Äô generalization ability to\nopen-domain topics.\n2 Related Work\nData Augmentation with Language Models\nThe emergence of pre-trained language models has\ngreatly promoted the progress of NLP technology\nin the past few years (Devlin et al., 2019; Radford\net al., 2019; Brown et al., 2020), and meanwhile\nbring opportunities for automatic data augmenta-\ntion of various NLP tasks. For instance, Schick\nand Sch¬®utze (2021) prompt GPT-2 (Radford et al.,\n2019) with textual instructions to generate a textual\nsimilarity dataset. Wang et al. (2021) leverage the\n175B-parameter GPT-3 (Brown et al., 2020) model\nto generate training data for text classification and\nlanguage understanding tasks. West et al. (2022)\nuse GPT-3 to acquire large-scale commonsense\nknowledge. Liu et al. (2022) construct a natural\nlanguage inference dataset through the collabora-\ntion with crowdworkers and GPT-3. Different from\nthem, our work focuses on data augmentation for\nopen-domain dialogue generation, which is more\ncomplex and challenging due to the much longer\ntext length, the open-ended nature, and the higher\ndemand for dialogue quality.\nThe more relevant work to ours are (Mohapatra\net al., 2020; Kim et al., 2022), which both train\ndifferent dialogue models to simulate the interac-\ntion between crowdworkers. Our work differs from\nthem in two aspects. (1) Instead of simulated in-\nteraction, we treat dialogue augmentation as a dia-\nlogue completion task (¬ß 3). We show in ¬ß 5 that\nour approach performs better in both effectiveness\nand efficiency. (2) Beyond verifying the utility\nof augmented data in training downstream models\n(¬ß 6), we focus more on analyzing and evaluating\nthe quality of augmented dialogues (¬ß 4 and 5).\nEmotional Support Conversation (ESC) ESC\n(Liu et al., 2021) is a dialogue generation task\nwhere the dialogue model plays the role of peer\nsupporter and helps the help-seeker reduce daily-\nlife emotional distress. It usually requires various\nsupport skills to achieve effective support, such as\nasking questions, expressing empathy, and provid-\ning suggestions (Rashkin et al., 2019; Zheng et al.,\n2021; Sabour et al., 2022; Zhou et al., 2023). As\ndiscussed in (Liu et al., 2021), high-quality ESC\ndata usually does not naturally exist and can hardly\nbe complemented by empathetic or emotional di-\nalogue data. The authors thus crowdsourced the\nESConv dataset through laborious worker training\nand quality control mechanisms. As a result, ES-\nConv contains only 1.3K dialogue sessions and\n13 topic categories, reflecting the intractability of\ncrowdsourcing large-scale high-quality ESC dia-\nlogues. Our work attempts to alleviate data scarcity\nthrough automatic dialogue augmentation.\n3 Methodology\nWe treat dialogue augmentation as a dialogue com-\npletion task, as illustrated in Figure 1 and 2. The\ndialogue augmentation procedure contains three\nsteps: (1) fine-tuning the language model ( ¬ß 3.1),\n(2) prompting it to complete full dialogues on var-\n1553\nThe following is a conversation with an AI assistant.The assistant is helpful, empathetic, clever, and very friendly. It can use various support skills to provide emotional support to human.Human: I moved into a new state recently, and there‚Äôs a lot to do, but I don‚Äôt have any friends in the new place I stay at.AI: What‚Äôs it like being away from family?Human: Family is all I have here. They aren‚Äôt exactly close, so I haven‚Äôt gotten to see them in a while.AI: That must be difficult. How do you feel about where you live?Human: It‚Äôs OK. I‚Äôm learning to like it a little bit. At least now I have someone who is usually around when I wake up.AI: If only you were that lucky with people in general. People move for so many different reasons. I‚Äôve found that often when I move, I just need to adjust my social circle a little, and I find that I then end up liking where I am.Human: That‚Äôs true. Maybe I should just find some people to hang out with.AI: Well, of course! There are a LOT of online social groups. We can start with that.Human: I think that‚Äôs a good idea. And I already know \none person at my current location.AI: Yes, it‚Äôs much easier to get started doing this online than it is with people physically nearby. I know that you were actually just thinking that. I would guess that after a short amount of time you‚Äôd develop more friendships.Human: Yeah, I‚Äôve been spending time on social media apps and just making new connections. I‚Äôve found some really good friends through it.AI: That‚Äôs wonderful! I‚Äôd say, you‚Äôre on your way!Human: What should I do first?AI: For now, try to think about some people you could go to eat lunch or coffee with, or just to chat about what‚Äôs going on.Human: Oh yeah, that‚Äôs a great idea! Thank you!AI: You are welcome! Of course, when you can afford it, do get together with others in real life! It‚Äôll be much more rewarding than anything you could do alone.Human: Do you think that will help? I was worried that I wouldn‚Äôt make any friends here, but I have one friend right now.AI: It certainly will help immensely. Real life friends are the best of all!Human: That‚Äôs good to hear! I guess I‚Äôll keep trying. Thanks for the help!AI: You are welcome! And again, I wish you the best of luck with your new situation!\nFigure 2: A cherry-picked example dialogue from AUGESC. The task description and the starting utterance is fed\ninto the fine-tuned language model, which then generates the subsequent dialogue.\nious topics ( ¬ß 3.2), and (3) postprocessing aug-\nmented dialogues based on heuristics (¬ß 3.3).\n3.1 Fine-tuning Language Model\nAs the first step of our approach, an autoregressive\nlanguage model is fine-tuned with dialogue samples\nto acquire the ability of completing a full dialogue\nfrom the starting utterance.\nPrevious work (Zheng and Huang, 2021; Wei\net al., 2022) has shown that the textual instruction\nfacilitates the generalization of language models.\nAs shown in Figure 2, we adopts the textual instruc-\ntion that contains a paragraph of task description\nand distinguishes the seeker and supporter with the\nrole prompts ‚ÄúHuman‚Äù and ‚ÄúAI‚Äù. During the next\nstep of dialogue completion (¬ß 3.2), the language\nmodel is first fed with the task description and a\nstarting utterance (starting with ‚ÄúHuman: ‚Äù), fol-\nlowed by the next ‚ÄúAI: ‚Äù. It then generates the sub-\nsequent dialogue until the EOS token is decoded.\nIn practice, we adopted GPT-J (Wang and Ko-\nmatsuzaki, 2021), an open-sourced autoregressive\nlanguage model with 6B parameters. We fine-tuned\nit for 1 epoch on 100 sampled ESConv dialogue\nsessions, which we found could lead to a balance\nbetween domain adaptation and the generalization\nto unseen dialogue topics. See Appendix B for\nimplementation details.\n3.2 Dialogue Completion\nThe second step is to collect available, diverse, and\ninformative dialogue posts as starting utterances,\nwhich are then used to prompt the language model\nto complete full dialogues on various topics.\nIn the scope of ESC, we target those dialogue\nposts that describe emotional problems in daily\nlife. We utilized dialogue posts from Empathetic-\nDialogues (ED) (Rashkin et al., 2019), a crowd-\nsourced empathetic dialogue dataset widely used\nin academic research. ED contains rich and diverse\ndialogue posts, which are assigned with emotion\nlabels and contain detailed descriptions about the\nemotional states. We used the posts with nega-\ntive emotion labels and retained those with lengths\nbetween 10 and 602 (with NLTK tokenization, sim-\nilarly below) to ensure the proper amount of infor-\nmation . Finally, we collected 8,950 dialogue posts\n2We noticed that a longer post usually leads to a dialogue\nwith longer utterances. Since the ESConv dialogues generally\ndo not contain too long utterances, we also set an upper bound\n(60) for the post length to avoid large gaps with ESConv.\n1554\nHeuristics Proportions\nAugmentation Failures\nNon-dialogue 0.3%\nUnfinished Generation 3.4%\nPrompt Word Leakage 2.1%\nHarmful Self-reinforcement\nUnbalanced # Utterances 5.2%\nConsecutive # Utterances 3.5%\nDistributional Gaps with ESConv\nTotal # Utterances 4.8%\nUtterance Length 8.0%\nFinal Retention 72.7%\nTable 1: Postprocessing results for AUGESC . Each\nmiddle row is the proportion of raw generated texts\nremoved with the corresponding rules, while the bottom\nrow is the final retention ratio.\nwith the average length 19.9. We traversed these\nposts for 10 epochs and obtained 89,500 raw gener-\nated texts with nucleus sampling (Holtzman et al.,\n2019) and p= 0.9 (the default decoding algorithm\nadopted in our work).\n3.3 Postprocessing\nIn the final postprocessing step, we remove unde-\nsirable augmented cases based on heuristics.\nSpecifically, we removed three types of undesir-\nable cases: (1) Augmentation Failures. It includes\ngenerating Non-dialogue contents, Unfinished Gen-\neration where the dialogue has not been generated\ncompletely, and Prompt Word Leakagewhere the\nutterances contain the role prompts ‚ÄúHuman‚Äù or\n‚ÄúAI‚Äù. (2)Harmful Self-reinforcement. A language\nmodel is prone to reinforcing itself to generate new\ntext with similar patterns to the precedent gener-\nated text (Holtzman et al., 2019). For instance,\ngiven the dialogue history where the speakers‚Äô ut-\nterance numbers are unbalanced or one speaker has\nsaid consecutive utterances, the language model is\nmore prone to continuously generating unbalanced\nor consecutive utterances, which generally do not\nappear in natural conversations. We thus set re-\nquirements for Unbalanced/Consecutive Utterance\nNumber to alleviate the harmful self-reinforced\npatterns, which also facilitates balanced informa-\ntion exchange between interlocutors. (3) Distri-\nbutional Gaps with ESConv . We also set re-\nquirements for the Total Utterance Number and\nUtterance Length to (a) avoid large distributional\ngaps with ESConv and (b) encourage in-depth dis-\ncussion with enough conversation turns. See Ap-\nESConv AUGESC\n# Sessions 1.3K 65K\nAverage Session Length 543.6 496.4\n# Utterances 38K 1,738K\nAverage # Utterances 28.9 26.7\nAverage Utterance Length 18.8 18.7\nSeeker\n# Utterances 20K 867K\nAvg # Uttr 15.4 13.3\nAvg Uttr Len 16.8 17.4\nSupporter\n# Utterances 18K 872K\nAvg # Uttr 13.6 13.4\nAvg Uttr Len 21.0 19.8\nTable 2: Statistics of AUGESC compared with ESConv.\nFor ESConv, we removed utterances from supporters at\nthe beginning of dialogues because these utterances are\nusually uninformative greetings.\npendix C for details of the filtering rules.\nTable 1 shows the postprocessing results. Af-\nter postprocessing the 89,500 raw texts, we ob-\ntained 65K augmented dialogues (72.7%). Benefit-\ning from model fine-tuning, our approach shows a\ngood controllability of generating valid dialogues\n(only 0.3% non-dialogue, 3.4% unfinished gen-\neration, and 2.1% prompt word leakage). The\nphenomenon of self-reinforcement also unavoid-\nably occurs in the generation of GPT-J (5.2%/3.5%\nunbalanced/consecutive utterance number), which\nsuggests the necessity of restricting the utterance\nnumbers during postprocessing.\nConsidering that heuristic-based postprocess-\ning may not guarantee the perfect quality of aug-\nmented dialogues, we also conducted comprehen-\nsive human evaluation to assess the dialogue quality\n(¬ß 5). The results show that the currently obtained\nAUGESC has been of reasonable quality. On the\nother hand, there are still no reliable automatic\nmethods for open-domain dialogue evaluation (Liu\net al., 2016; Deriu et al., 2021), and even human\nevaluation is an open problem (Smith et al., 2022).\nWe leave further quality refinement methods of\ndialogue augmentation for future work.\n4 Data Analysis\n4.1 Statistics\nThe statistics of our constructed AUGESC dataset\nare shown in Table 2. An example dialogue from\nAUGESC is shown in Figure 2. AUGESC con-\ntains 65K dialogue sessions and 1,738K utterances,\nroughly 50/45x the scale of ESConv. TheAUGESC\ndialogues generally have a close utterance number\n1555\nESConv\npandemic (5.2), covid (5.0), depression (3.4), support\n(3.2), christmas (3.1), job (2.6), anxiety (2.6), online\n(2.6), vaccine (2.4), zoom (2.2), holidays (2.2), correct\n(2.1), feeling (2.1), helpful (2.1), stress (2.0), virus (2.0),\nhard (2.0), breakup (2.0), mturk (1.9), merry (1.9), quit\n(1.9), virtual (1.8), unemployment (1.8), struggling (1.8),\nresume (1.8), youtube (1.7), honestly (1.7), moment\n(1.7), daily (1.6), survey (1.6)\nAUGESC\ncar (4.9), sounds (4.0), dog (3.2), guess (3.2), house\n(2.9), police (2.8), money (2.6), parents (2.6), hope (2.5),\nbrother (2.5), idea (2.4), buy (2.4), neighbors (2.4), in-\nsurance (2.3), afraid (2.2), mom (2.1), luck (2.1), driving\n(2.1), agree (2.0), told (2.0), husband (2.0), excited (2.0),\nFigure (2.0), nice (1.9), upset (1.9), cat (1.9), sense (1.9),\nscared (1.9), vet (1.8), stole (1.8)\nTable 3: Top 30 salient topic features associated with ES-\nConv and AUGESC. The rounded z-scored log odds ra-\ntios are marked in the parentheses, where values greater\nthan 2 indicate significant (>2 std) association.\nand length to ESConv due to the heuristics for con-\ntrolling their distributional gaps (Table 1, Total\nUtterance Number and Utterance Length). We ob-\nserve that in the ESConv dialogues, the utterance\nnumber of seeker is usually larger than supporter\n(15.4 vs. 13.6), while their numbers are closer in\nAUGESC (13.3 vs. 13.4). This is because the aug-\nmentation process of AUGESC has to trade off the\nself-reinforcement phenomenon, as discussed in\n¬ß 3.3 (the heuristics of Unbalanced/Consecutive\nUtterance Number).\n4.2 Topic Analysis\nTo analyze the topic features, we extracted the lexi-\ncal correlates of ESConv and AUGESC. We calcu-\nlated the log odds ratio, informative Dirichlet prior\n(Monroe et al., 2008) of all words for each dataset\ncontrasting to the other. Statistics are based on the\nwhole 1.3K ESConv sessions or the randomly sam-\npled 1.3K AUGESC sessions for fair comparison.\nAs shown in Table 3, dialogue topics in ESConv\nare closely related to its period of data curation\n(2020 to 2021). For instance, the topics like ‚Äúpan-\ndemic, covid, vaccine, virus‚Äù are directly relevant\nto COVID-19, while those like ‚Äúonline, zoom, vir-\ntual‚Äù imply the influence of COVID-19. It indicates\nthat the ESConv dialogues may revolve around\nthe emotional problems under the background of\nor caused by COVID-19, as demonstrated by the\ntopics ‚Äúdepression, job, anxiety, breakup, unem-\nFigure 3: Left: Statistics of inter-dialogue similarity,\ncalculated with the TF-IDF vectors of dialogues. Right:\nVisualization of the TF-IDF vectors of dialogues by\napplying 2-dimensional Principal Component Analysis\n(PCA). We mark the regions that are specially correlated\nwith the two datasets respectively.\nployment‚Äù. As a result, ESConv may fail to cover\ntopics about general daily life (besides, the ESConv\ndialogues also leak the information of crowdsourc-\ning tasks, such as ‚Äúmturk, quit, survey‚Äù). By con-\ntrast, AUGESC covers a broader range of daily-life\ndialogue topics, such as ‚Äú car, dog, house, police,\nmoney‚Äù and many others in Table 3, benefiting from\nthe diverse dialogue posts from ED (¬ß3.2). We thus\nsuggest that AUGESC largely complements the\ntopic coverage of ESConv, which can facilitate the\ngeneralization of downstream dialogue models to\nopen-domain topics.\n4.3 Diversity Analysis\nTo analyze the diversity of augmented dialogues,\nwe extracted the TF-IDF vector of each dialogue\nin ESConv and AUGESC using the Sklearn library\n(Pedregosa et al., 2011). We computed and counted\nthe TF-IDF similarity between any two dialogues.\nAs shown in Figure 3 (left), AUGESC has close\ninter-dialogue diversity to ESConv. It suggests that\ndifferent dialogues in AUGESC have little overlap\nwith each other, which thus can provide diverse\ntraining samples for downstream dialogue models.\nWe also visualized the TF-IDF vectors of dia-\nlogues by applying 2-dimensional Principal Com-\nponent Analysis (PCA). As shown in Figure 3\n(right), the regions that are specially correlated with\nthe two datasets are salient, as marked explicitly\nwith arrows. It suggests that AUGESC has a differ-\nent range of dialogue contents from ESConv and\ntheir combination can lead to a broader range.\n5 Evaluation for Dialogue Quality\nThe quality of dialogue data is critical to train-\ning downstream dialogue models (Zhou et al.,\n2021; Gu et al., 2023). To validate the quality\nof AUGESC , we conduct comprehensive human\n1556\nInformativeness Understanding Helpfulness Consistency Coherence Unsafety‚Üì\nŒ∫ 0.42 0.35 0.37 0.35 0.40 0.39\nCrowdsourced 2.52 2.42 2.23 2.56 2.42 0.13\nSimulated Chat (BlenderBot) 1.86 * 1.90 * 1.49 * 2.12 * 1.90 * 0.03\nSimulated Chat (GPT-J) 2.29 2.28 2.02 2.25 * 2.16 * 0.14\nOur Approach w/o FT (GPT-3) 2.23 * 2.07 * 1.62 * 2.11 * 1.96 * 0.16\nOur Approach 2.41 2.37 2.12 2.34* 2.19* 0.14\nTable 4: Human evaluation results for dialogue quality. The scores (from 0 to 3) are averaged over all the samples\nrated by three annotators. Œ∫ denotes Fleiss‚Äô Kappa (Fleiss, 1971), indicating fair or moderate inter-annotator\nagreement (0.2 < Œ∫ <0.6). The highest scores of augmented dialogues are highlighted and the significantly\nworse ones are marked with gray background (Student‚Äôs t-test, p-value <0.01). * denotes significant gaps with\nthe crowdsourced ESConv. We did not conduct significance test with Unsafety due to only a few non-zero scores.\nevaluation and compare our approach with strong\nbaselines of dialogue augmentation.\n5.1 Compared Methods\nSimulated Chat This baseline comes from (Mo-\nhapatra et al., 2020; Kim et al., 2022), which simu-\nlates the crowdworkers‚Äô interaction with separately\ntrained models. Following (Mohapatra et al., 2020),\nwe fine-tuned two models with the seekers‚Äô and sup-\nporters‚Äô utterances in ESConv separately. We used\nthe dialogue posts in ¬ß 3.2 as the first seekers‚Äô utter-\nances, and then the two models took turns to reply\nto simulate interaction. We set the maximum ut-\nterance number to 40 and terminated the simulated\nconversation if the latest utterance contained the\nword ‚Äúbye‚Äù. We implemented it with two different\nbase models. Simulated Chat (BlenderBot) uses\nthe 1.4B BlenderBot model (Roller et al., 2021),\nwhich is the state-of-the-art open-sourced dialogue\nmodel. Simulated Chat (GPT-J) uses the same\n6B GPT-J model as in our approach.\nOur Approach w/o FT This baseline denotes\ndirectly prompting the language model to complete\ndialogues without fine-tuning, which is an ablated\nvariant of our approach and can help us understand\nthe influence of fine-tuning in ¬ß 3.1. However, we\nfound that GPT-J cannot work well without fine-\ntuning. We thus implemented this ablated variant\nwith the 175BGPT-3davinci model (Brown et al.,\n2020) through the OpenAI API. We applied the\nsame postprocessing as in ¬ß 3.3. See Appendix D\nfor details.\nCrowdsourced We meanwhile evaluate the qual-\nity of the ESConv dialogues, which are written by\ncrowdworkers and contain natural human-human\ninteractions. We expect this baseline to serve as an\nanchor point of dialogue quality evaluation.\n5.2 Evaluation Setups\nWe refer to (Liu et al., 2021) to design the evalu-\nation protocol. When crowdsourcing the ESConv\ndataset, Liu et al. (2021) asked the crowdworkers\nto complete a survey to rate their performance dur-\ning conversation. The survey results were used\nto build up the data screening criteria of ESConv.\nInspired by this survey, we design the following\nmetrics for assessing the quality of augmented dia-\nlogues. Informativeness measures how much de-\ntailedly the help-seeker describes his/her emotional\nproblem. Understanding measures how much the\nsupporter understands the help-seeker‚Äôs experience\nand feelings. Helpfulness measures how much the\nsupporter succeeds in helping the seeker reduce\nemotional distress and feel better.\nBesides, we also assess the general dialogue\nquality. Consistency measures whether the be-\nhaviors of the speakers are consistent with their\nroles, and whether the behavior of the same speaker\nis not self-contradictory. Coherence measures\nwhether the conversation is on-topic and in-depth\nand whether the topic transition is natural. Un-\nsafety measures whether the conversation contains\nunsafe contents, such as toxic language, sensitive\ntopics, or immoral suggestions. See Appendix E\nfor the detailed evaluation guideline.\nAll the metrics are rated with the four-level Lik-\nert scale ranging from 0 to 3 (higher is better ex-\ncept Unsafety). We recruited 60 college students\nas annotators. We randomly sampled 60 dialogue\nsessions for each method. Each session was rated\nby three different annotators.\n5.3 Results\nAs shown in Table 4, our approach produces aug-\nmented dialogues with the highest quality among\n1557\nall the methods. (1) Compared to Simulated Chat\n(BlenderBot), our approach demonstrates the bet-\nter potential of general language models for dia-\nlogue augmentation than specifically pre-trained\ndialogue models (our advantages are reflected in\nalmost all the metrics). It is because general lan-\nguage models store more general knowledge and\nthus can better generalize to various dialogue posts\nfor dialogue augmentation. (2) Compared to Sim-\nulated Chat (GPT-J), our advantage is not sub-\nstantial due to the same base model. However,\nsince our approach performs one-time generation\nof the whole dialogue, it is superior in efficiency\nto Simulated Chat, which takes turns to generate\nthe two speakers‚Äô utterances with two models (as a\nreference, time cost: 60 seconds vs. 80 seconds per\nsession, GPU memory: 1 model vs. 2 models). In-\nterestingly, our approach slightly outperforms Sim-\nulated Chat, which indicates that fine-tuning one\nlanguage model with whole dialogues may be bet-\nter than fine-tuning two models with two speakers‚Äô\nutterances separately (the former can be viewed\nas the parameter-shared version of the latter). (3)\nCompared to Our Approach w/o FT (GPT-3) ,\nour approach is remarkably better due to model\nfine-tuning, which enables better adaptation to our\ninterested ESC task, as reflected in the metrics Un-\nderstanding and Helpfulness. (4) Compared to the\ncrowdsourced ESConv, our approach produces\naugmented dialogues with comparable evaluation\nscores, which confirms the reasonable quality of\nAUGESC. Nevertheless, there is still a gap in Con-\nsistency and Coherence, which is a long-standing\nproblem in open-ended text generation (Dou et al.,\n2022) and deserves further study in the more gen-\neral NLG research.\nSee Appendix A.1 for further discussion about\nthe limitations of AUGESC ‚Äôs quality. Also see\nAppendix F for additional augmented dialogue ex-\namples of Simulated Chat (BlenderBot) and Our\nApproach w/o FT, respectively.\n5.4 Toxicity Assessment\nWe manually inspected the few cases with non-zero\nunsafety scores (all are 1-point) in augmented and\ncrowdsourced dialogues and found that they are\nall about potentially sensitive topics (e.g., talking\nabout alcohol experiences). We conjecture that the\nunsafety of Simulated Chat (BlenderBot) is the low-\nest (even lower than the Crowdsourced ESConv)\nbecause the BlenderBot model has been processed\nAttributes ESConv A UGESC\nToxicity 0.0613 0.0597\nSevere Toxicity 0.0427 0.0418\nIdentify Attack 0.0531 0.0493\nInsult 0.0786 0.0823\nProfanity 0.0539 0.0516\nThreat 0.1155 0.1146\nTable 5: Results of toxicity assessment using Perspec-\ntive API. Lower scores are better and are highlighted .\nby safety mechanisms (Xu et al., 2020) and usually\navoids discussion about sensitive topics. However,\nthis also sacrifices the quality of augmented dia-\nlogues, as reflected in the lowest quality of Simu-\nlated Chat (BlenderBot) in Table 4.\nLanguage toxicity has been an essential consid-\neration in the NLP research (Gehman et al., 2020;\nDinan et al., 2021; Zheng et al., 2023). As an\nadditional evaluation for the potential toxicity in\naugmented dialogues, we assessed ESConv and\nAUGESC using Perspective API, a widely used\ntoxicity detection API in online discussions. All\nthe utterances were assessed by Perspective API‚Äôs\ntoxicity scores (between 0 and 1, lower is safer)\nof six production attributes. For each attribute, we\nreported the score averaged over all the utterances.\nFrom Table 5, ESConv and AUGESC both show\nlittle toxicity (very low scores) and AUGESC is\neven lower, while the dialogue quality ofAUGESC\nstill slightly underperforms ESConv, as judged by\nhuman annotators (Table 4). We conjecture that\nthere may be a trade-off between reducing such\n‚Äútoxicity‚Äù and improving dialogue quality. For in-\nstance, the supporter cannot help the help-seeker\nreduce emotional distress without discussing the\nseeker‚Äôs emotional problem in depth. On the other\nhand, toxicity in dialogues is subtle due to its sensi-\ntiveness to the dialogue context (Pavlopoulos et al.,\n2020; Sun et al., 2022), and its identification is\nstill under exploration. Given that the toxicity de-\ntectors like Perspective API may introduce new\nbiases (Hosseini et al., 2017; Sap et al., 2019), we\ndid not apply additional toxicity postprocessing to\nAUGESC in the current work. We leave the further\ninvestigation of the toxicity problem in augmented\ndialogues as future work.\n6 Evaluation for Data Utility\nRecall that dialogue augmentation aims to improve\nsmaller downstream dialogue models, which is a\n1558\nTrained on AUGESC? Yes No\nFluency 47 13\nIdentification 68 22\nComforting 55 22\nSuggestion 58 15\nOverall 58 28\nTable 6: Results of human interactive evaluation under\nthe open-domain setting (winning ratios shown). All\nthe gaps are statistically significant (sign test, p-value\n<0.05/0.01 for ‚ÄúOverall‚Äù / other metrics).\nrealistic and practical setting since the deployment\nof large language models is expensive. To verify\nthe utility of AUGESC , we conduct human inter-\nactive evaluation to explore how much AUGESC\ncan improve the generalization of downstream dia-\nlogue models to open-domain topics.\n6.1 Compared Models\nWe compared two 1.4B BlenderBot models: one\nis fine-tuned only on ESConv (1,100 dialogues, 2\nepochs), while the other is further post-trained on\nAUGESC (1 epoch). Note that we used AUGESC\nfor model post-training since we observed that (1)\npost-training on AUGESC facilitates better general-\nization to open-domain topics, and (2) first-training\non ESConv provides a good initialization point due\nto the better consistency and coherence of crowd-\nsourced dialogues (Table 4), which is critical to the\nmulti-turn interaction capability.\n6.2 Evaluation Setups\nFollowing (Liu et al., 2021), we conducted pair-\nwise human interactive evaluation. We recruited\nthe same participants as in ¬ß 5. Each participant\nwas asked to talk about the same emotional prob-\nlem with the two bots, which accepted the same\nfirst utterances. Each conversation lasted at least\n8 turns (8 utterances from participants and 8 from\nbots), after which the participant could either con-\ntinue or end it. It is worth noting that we adopted\nthe open-domain setting, that is, the participants\nwere allowed to talk about any topics they wanted\nwithout restrictions. It is distinct from the setting\nin (Liu et al., 2021) where the participants were\nonly allowed to talk about limited topic categories\n(i.e., in-domain topics).\nAfter the conversations, the participants were\nasked to compare the two bots based on the follow-\ning aspects, which follow the evaluation protocol of\nAUGESC? PPL B-2 B-4 R-L D-2 D-3\nNo 11.2 7.8 2.4 16.9 23.8 48.0\nYes 11.5 7.7 2.4 16.7 24.3 49.4\nTable 7: Results of automatic evaluation on the in-\ndomain ESConv test set. Metrics include perplexity\n(PPL), BLEU-2/4 (Papineni et al., 2002), ROUGE-L\n(Lin, 2004) and Distinct-2/3 (Li et al., 2016).\n(Liu et al., 2021). Fluency: which bot‚Äôs responses\nwere more fluent and understandable? Identifi-\ncation: which bot explored your situation more\nin-depth and was more helpful in identifying your\nproblem? Comforting: which bot was more skill-\nful in comforting you? Suggestion: which bot gave\nyou more helpful suggestions for your problems?\nOverall: generally, which bot‚Äôs emotional support\ndo you prefer? We collected 60 pairs of interactive\nconversations (each participant contributed one).\n6.3 Results\nAs shown in Table 6, AUGESC significantly im-\nproves the dialogue model‚Äôs performance in all\naspects. It strongly proves the utility of AUGESC\nin enhancing the generalization of downstream dia-\nlogue models to open-domain topics. We argue that\nthe results are non-trivial. The BlenderBot model\nhas been very powerful in open-domain dialogue\n(Roller et al., 2021). Given that the crowdsourced\nESConv dataset has greatly improved BlenderBot‚Äôs\nability of emotional support (Liu et al., 2021), any\nfurther substantial improvement on top of this is by\nno means easy.\n6.4 In-domain Evaluation\nWe conducted the additional automatic evaluation\non the in-domain ESConv test set (200 held-out\ndialogue sessions), which aims to verify whether\npost-training on AUGESC sacrifices the in-domain\nperformance. As shown in Table 7, post-training\non AUGESC little influences the in-domain per-\nformance (the gaps are marginal), suggesting that\nimproving the open-domain generalization ability\nis compatible with maintaining the underlying dia-\nlogue capability.\n7 Conclusion\nIn this work, we present a simple yet effective ap-\nproach for dialogue augmentation, which is for-\nmulated as a dialogue completion task. Using\nthis approach, we release an augmented dataset\n1559\nAUGESC for the task of emotional support conver-\nsation (ESC), which largely extends the scale and\ntopic coverage of the crowdsourced ESConv corpus.\nThrough comprehensive empirical evaluation, we\nshow that: (1) our approach produces augmented\ndialogues with higher quality than strong baselines\nof dialogue augmentation, (2) AUGESC has com-\nparable dialogue quality to the crowdsourced ES-\nConv corpus, and (3) post-training on AUGESC\nnotably improves the generalization capability of\ndownstream dialogue models to open-domain top-\nics. Our work demonstrates the prowess and utility\nof large language models in improving data-scarce\ntasks, especially complex open-domain dialogue\ntasks. It may inspire more work regarding train-\ning data augmentation with large language models.\nFuture work can explore automatic methods for fur-\nther quality refinement of dialogue augmentation.\nEthical Considerations\nThe EmpatheticDialogues (Rashkin et al., 2019)\ndataset for dialogue post collection, the GPT-J\nmodel (Wang and Komatsuzaki, 2021), and the\nBlenderBot model (Roller et al., 2021) are all\nwidely used in academic research, can be accessed\nfrom HuggingFace Hub or official websites, and\nare all in the English language as well as AUGESC.\nUsing the above public resources, the construction\nof AUGESC does not involve human participants\nand thus does not collect any personal identifying\ninformation.\nWe raise attention that AUGESC may possibly\ncontain toxic or biased contents, which cannot be\nfully assessed in either automatic or human evalu-\nation (¬ß 5). Future access to AUGESC should be\nonly for research usage and should NOT be used\nfor real-deployed systems, commercial purposes,\nor any other usage than academic research. Anyone\nusing AUGESC in the research should be aware of\nits limitations and should acknowledge and/or try\nto mitigate them to the extent possible.\nOur work strictly follows the task definition and\nevaluation protocols ( ¬ß 5 and 6) of the original\nESC paper (Liu et al., 2021), where the support\nis provided through social interactions (e.g., be-\ntween peers or friends) rather than professional\ncounseling. As mentioned in (Liu et al., 2021),\nfurther efforts are still needed to probe the ethical\nextent to which dialogue models can or should pro-\nvide support. These protocols should also not be\nused directly in fields other than the ESC task (i.e.,\npeer emotional support in daily life) that require\nthe guidance of professional researchers, such as\npsychological counseling.\nWe also ethically conducted the human evalu-\nation. We transparently communicated with the\nparticipants of our study intent and explicitly in-\nformed them of the disclaimers before they partici-\npated. We paid the participants at the hourly wage\nabove $10/hour, going well beyond the local labor\ncompensation standard. We acknowledge that the\nresults of human evaluation could be affected by\nthe participants‚Äô demographic and geographic char-\nacteristics. This work has obtained study approval\nfrom the Institutional Review Board (IRB).\nAcknowledgements\nThis work was supported by the National Science\nFoundation for Distinguished Young Scholars (with\nNo. 62125604). This work was also supported\nby the Guoqiang Institute of Tsinghua University,\nwith Grant No. 2020GQG0005, Tsinghua Preci-\nsion Medicine Foundation, and the NSFC project\n(with No. 62206150).\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS.\nJan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo\nEchegoyen, Sophie Rosset, Eneko Agirre, and Mark\nCieliebak. 2021. Survey on evaluation methods for\ndialogue systems. Artificial Intelligence Review.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021. Anticipating safety issues in\ne2e conversational ai: Framework and tooling. arXiv\npreprint arXiv:2107.03451.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A Smith, and Yejin Choi. 2022. Is gpt-3 text\nindistinguishable from human text? scarecrow: A\nframework for scrutinizing machine text. In ACL.\n1560\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020. Realtoxici-\ntyprompts: Evaluating neural toxic degeneration in\nlanguage models. In Findings of EMNLP.\nYuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke,\nChujie Zheng, Zheng Zhang, Jianzhu Yao, Lei Liu,\nXiaoyan Zhu, et al. 2023. Eva2. 0: Investigating\nopen-domain chinese dialogue systems with large-\nscale pre-training. Machine Intelligence Research,\n20(2):207‚Äì219.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. In ICLR.\nHossein Hosseini, Sreeram Kannan, Baosen Zhang, and\nRadha Poovendran. 2017. Deceiving google‚Äôs per-\nspective api built for detecting toxic comments.arXiv\npreprint arXiv:1702.08138.\nMinju Kim, Chaehyeong Kim, Yongho Song, Seung-\nwon Hwang, and Jinyoung Yeo. 2022. Botstalk:\nMachine-sourced framework for automatic cura-\ntion of large-scale multi-skill dialogue datasets. In\nEMNLP.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out.\nAlisa Liu, Swabha Swayamdipta, Noah A Smith, and\nYejin Choi. 2022. Wanli: Worker and ai collaboration\nfor natural language inference dataset creation. In\nFindings of EMNLP.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow NOT to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In EMNLP.\nSiyang Liu, Chujie Zheng, Orianna Demasi, Sahand\nSabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie\nHuang. 2021. Towards emotional support dialog\nsystems. In ACL.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nBiswesh Mohapatra, Gaurav Pandey, Danish Contractor,\nand Sachindra Joshi. 2020. Simulated chats for task-\noriented dialog: Learning to generate conversations\nfrom instructions. arXiv preprint arXiv:2010.10216.\nBurt L Monroe, Michael P Colaresi, and Kevin M Quinn.\n2008. Fightin‚Äôwords: Lexical feature selection and\nevaluation for identifying the content of political con-\nflict. Political Analysis, 16(4):372‚Äì403.\nYixin Nie, Mary Williamson, Mohit Bansal, Douwe\nKiela, and Jason Weston. 2021. I like fish, espe-\ncially dolphins: Addressing Contradictions in Dia-\nlogue Modeling. In ACL.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nJohn Pavlopoulos, Jeffrey Sorensen, Lucas Dixon,\nNithum Thain, and Ion Androutsopoulos. 2020. Tox-\nicity detection: Does context really matter? In ACL.\nFabian Pedregosa, Ga¬®el Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. 2011. Scikit-learn: Machine\nlearning in python. JMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In ACL.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In EACL.\nSahand Sabour, Chujie Zheng, and Minlie Huang. 2022.\nCEM: commonsense-aware empathetic response gen-\neration. In AAAI.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A Smith. 2019. The risk of racial bias in\nhate speech detection. In ACL.\nTimo Schick and Hinrich Sch ¬®utze. 2021. Generating\ndatasets with pretrained language models. arXiv\npreprint arXiv:2104.07540.\nEric Michael Smith, Orion Hsu, Rebecca Qian, Stephen\nRoller, Y-Lan Boureau, and Jason Weston. 2022. Hu-\nman evaluation of conversations is an open prob-\nlem: comparing the sensitivity of various meth-\nods for evaluating dialogue agents. arXiv preprint\narXiv:2201.04723.\nHao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng,\nChujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan\nZhu, and Minlie Huang. 2022. On the safety of con-\nversational models: Taxonomy, dataset, and bench-\nmark. In Findings of ACL.\n1561\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2022. Finetuned lan-\nguage models are zero-shot learners. In ICLR.\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue Natural Language\nInference. In ACL.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R¬¥emi Louf, Morgan Funtowicz,\nand Jamie Brew. 2020. Transformers: State-of-the-\nart natural language processing. In EMNLP.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots. arXiv preprint\narXiv:2010.07079.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In ACL.\nChujie Zheng, Yunbo Cao, Daxin Jiang, and Minlie\nHuang. 2020. Difference-aware knowledge selection\nfor knowledge-grounded conversation generation. In\nFindings of EMNLP 2020.\nChujie Zheng and Minlie Huang. 2021. Exploring\nprompt-based few-shot learning for grounded dialog\ngeneration. arXiv preprint arXiv:2109.06513.\nChujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang.\n2023. Click: Controllable text generation with se-\nquence likelihood contrastive learning. In Findings\nof ACL.\nChujie Zheng, Yong Liu, Wei Chen, Yongcai Leng, and\nMinlie Huang. 2021. Comae: A multi-factor hierar-\nchical framework for empathetic response generation.\nIn Findings of ACL.\nChujie Zheng, Jinfeng Zhou, Yinhe Zheng, Libiao Peng,\nZhen Guo, Wenquan Wu, Zhengyu Niu, Hua Wu,\nand Minlie Huang. 2022. Cdconv: A benchmark for\ncontradiction detection in chinese conversations. In\nEMNLP.\nHao Zhou, Pei Ke, Zheng Zhang, Yuxian Gu, Yinhe\nZheng, Chujie Zheng, Yida Wang, Chen Henry\nWu, Hao Sun, Xiaocong Yang, et al. 2021. Eva:\nAn open-domain chinese dialogue system with\nlarge-scale generative pre-training. arXiv preprint\narXiv:2108.01547.\nHao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang,\nand Xiaoyan Zhu. 2020. KdConv: A Chinese\nmulti-domain dialogue dataset towards multi-turn\nknowledge-driven conversation. In ACL.\nJinfeng Zhou, Chujie Zheng, Bo Wang, Zheng Zhang,\nand Minlie Huang. 2023. Case: Aligning coarse-to-\nfine cognition and affection for empathetic response\ngeneration. In ACL.\n1562\nA Limitations\nA.1 Dialogue Quality of A UGESC\nThrough our manual inspection, we found that the\ninconsistency issue mainly occurs in the seeker-\nprovided information. For instance, the seeker first\nexpresses ‚Äúsadness about the loss of the dog that\nhe/she has raised for 14 years ‚Äù. When the sup-\nporter asks ‚Äúthe age of the dog‚Äù, the seeker answers\n‚Äú13‚Äù, which is obviously contradictory to the afore-\nmentioned ‚Äú14 years‚Äù. While the inconsistency can\nbe easily detected based on human commonsense,\nmodels are prone to make such mistakes (Welleck\net al., 2019; Nie et al., 2021; Zheng et al., 2022).\nAnother issue we noticed is the improper topic\ntransition. That is, after several turns of conver-\nsation, the supporter sometimes discusses topics\nother than the seeker‚Äôs emotional problem. We con-\njecture that the root cause is the seeker‚Äôs inability\nto proactively provide personalized, in-depth, and\ndetailed information about the emotional problem.\nIn this case, the conversation is only driven by the\nsuggestions offered or the questions raised by the\nsupporter, which may thus induce improper topic\ntransition and make the conversation less coherent.\nA.2 Generalization to Other Tasks or Models\nWe only experimented with the ESC task as our\nwork focuses more on the quality analysis ( ¬ß 4\nand 5) and utility evaluation ( ¬ß 6) of augmented\ndialogues. The motivation we study in the ESC\ntask is in two folds: (1) The construction of the\nESConv dataset reveals the typical limitations of\ncrowdsourcing dialogue corpora ( ¬ß 1). (2) (Liu\net al., 2021) provides detailed data screening cri-\nteria, enabling us to design a reasonable and con-\nvincing protocol for dialogue quality evaluation\n(¬ß 5). However, it is intuitive to generalize our\napproach to other dialogue generation tasks. For\ninstance, in knowledge-grounded dialogue (Dinan\net al., 2018; Zhou et al., 2020; Zheng et al., 2020),\nwe can additionally utilize knowledge bases for\ndialogue augmentation. We leave the broader ap-\nplications of our dialogue augmentation approach\nto future work.\nWhile we only experimented with GPT-J, one\ncan expect that exploiting larger language models\nwould produce augmented dialogues with better\nquality, especially given that the commercial lan-\nguage models (e.g., OpenAI‚Äôs GPT-3) exhibit much\nstronger performance than the open-sourced ones\nand are getting more easily accessible.\nB Implementation Details\nTraining We implemented GPT-J with the Trans-\nformers library (Wolf et al., 2020). We fine-tuned\nit for 1 epoch with 100 ESConv dialogue sessions,\nwhich are sampled over the 13 topic categories\nevenly. The batch size was set to 2, and the lan-\nguage modeling loss was averaged over all the to-\nkens in dialogues (excluding the task descrpition\ntexts). We used the AdamW optimizer (Loshchilov\nand Hutter, 2018), the learning rate 5e-6, and the\nlinear learning rate scheduler with warmup steps 5.\nWe set the maximum input length to 1,500, and ap-\nplied gradient checkpointing and model parallelism\nto reduce GPU memory occupation. The fine-\ntuning of GPT-J requires four Tesla V100 32GB\nGPUs and takes about 1 hour.\nGeneration The maximum generation length\nwas set to 1,500. We adopted nucleus sampling\n(Holtzman et al., 2019) with p = 0.9. We set the\nrepetition penalty factor to 1.05 to avoid generating\nduplicate contents. The generation of GPT-J re-\nquires one Tesla V100 32GB GPU and takes about\n1 minute for one augmented dialogue.\nC Details of Filtering Rules\nAugmentation Failures Non-dialogue: Each\nline in the generated text should start with ‚ÄúHuman:‚Äù\nor ‚ÄúAI:‚Äù (i.e., one utterance per line). Unfinished\nGeneration: The generated text should contain the\ndecoded EOS token. Prompt Word Leakage: The\nutterances should not leak the role prompts ‚ÄúHu-\nman‚Äù or ‚ÄúAI‚Äù.\nHarmful Self-reinforcement Unbalanced Utter-\nance Number: The number of utterances of one\nspeaker should be no more than 2.5x the other.Con-\nsecutive Utterance Number: The number of con-\nsecutive utterances from the same speaker should\nnot exceed 3. Otherwise, in a self-reinforced gen-\neration, one speaker may say obviously more ut-\nterances than the other (unbalance) or one speaker\nmay always say consecutive utterances while the\nother does not. Such a dialogue would appear un-\nnatural and odd, because it is inconsistent with the\ncommon conversational behavior.\nDistributional Gaps with ESConv Total Utter-\nance Number : The total number of utterances\nshould be more than 10 (a too short dialogue also\nusually does not contain in-depth discussion). Ut-\nterance Length: The average utterance length from\nthe seeker/supporter should be between 6/8 and\n1563\nHeuristics Proportions\nAugmentation Failures\nNon-dialogue 24.8%\nUnfinished Generation N/A\nPrompt Word Leakage 11.2%\nHarmful Self-reinforcement\nUnbalanced # Utterances 0.2%\nConsecutive # Utterances 0.0%\nDistributional Gaps with ESConv\nTotal # Utterances 8.2%\nUtterance Length 7.9%\nFinal Retention 47.9%\nTable 8: Postprocessing results of augmented dialogues\nproduced by directly prompted GPT-3.\n40 and the maximum utterance length should not\nexceed 80. It is worth noting that the minimum av-\nerage utterance lengths for seeker/supporter (6 and\n8, respectively) are set according to the thresholds\nadopted in the quality control mechanisms in (Liu\net al., 2021).\nDiscussion and Limitation All the above thresh-\nolds are determined based on our heuristics and\nthe statistics of ESConv. For instance, there are\nfew ESConv dialogues that contain over 3 consecu-\ntive utterances from the same speaker (Consecutive\nUtterance Number), and also few dialogues that\nhave average utterance lengths over 40 and max-\nimum utterance lengths over 80. We are not able\nto systematically analyze the influence the deter-\nminated thresholds, such as the quality of obtained\naugmented dialogues and the corresponding perfor-\nmance of downstream dialogue models ( ¬ß 5 and\n6). Unfortunately, evaluating these ablations would\nbe prohibitively expensive since they all require\nextensive human efforts for reliable evaluation.\nD Details of Directly Prompting GPT-3\nIn ¬ß 5.1, we implemented Our Approach w/o FT\nwith the 175B-parameter GPT-3 davinci (Brown\net al., 2020) because we found GPT-J cannot work\nwell without fine-tuning. Since we noticed that\nGPT-3 usually generated non-dialogue contents\nwhen the generation length is increasing, we set\nthe maximum generation length of GPT-3 to 400 to\navoid meaningless overhead. The GPT-3 expense\nwas about $0.025 per generated text on average.\nWe used 2K dialogue posts to directly prompt\nGPT-3 to complete full dialogues, from which 60\naugmented dialogues were sampled for human eval-\nuation. For the whole 2K samples, we applied the\nsame postprocessing as in our approach. As shown\nin Table 8, GPT-3 makes many augmentation fail-\nures (24.8% non-dialogue and 11.2% prompt word\nleakage). Note that its harmful self-reinforcement\nis little because it seldom produces valid dialogues.\nThe results suggest that directly prompting the lan-\nguage model (even the 175B GPT-3) is inferior to\nfine-tuning (the much smaller GPT-J than GPT-3)\nin controllability (72.7% vs. 47.9% retention ra-\ntios) as well as the quality of produced augmented\ndialogues (¬ß 5.3).\nE Guideline of Dialogue Quality\nEvaluation\nWe present the guideline of human evaluation for\ndialogue quality (¬ß 5.2) in Figure 4.\nF Examples of Augmented Dialogues\nfrom Baselines\nWe present the augmented dialogue examples of\nSimulated Chat (BlenderBot) and Our Approach\nw/o FT (¬ß 5.1) in Figure 5 and 6, respectively.\n1564\nGuideline of Dialogue Quality Evaluation This study aims to evaluate the quality of the conversation between a help-seeker (i.e., a person who is seeking support) and a help-supporter (i.e., a person who is providing support). Each task provides you with a single conversation. You need to evaluate the conversation from the following aspects. Disclaimer: the conversations may contain offensive or upsetting contents. Help-seeker Evaluation √ò Informativeness: the help-seeker‚Äôs level of detail in describing his/her own emotional problems. Options 1. Detailed description   2. Moderate description   3. Minor description   4. No description Examples 1. ‚ÄúI am sad‚Äù does not tell us much about the seeker‚Äôs situation. For instance, the reason why they are sad is not mentioned. 2. ‚ÄúI feel so lonely after my dog passed away. He was my best friend‚Äù provides a detailed description about both the seeker‚Äôs feelings and their situation. Supporter Evaluation √ò Understanding: the supporter‚Äôs level of understanding the help-seeker‚Äôs experience and feeling. Options 1. Complete understanding    2. Moderate understanding    3. Minor understanding      4. No understanding Examples When the seeker mentions ‚ÄúI feel so lonely after my dog passed away. He was my best friend‚Äù 1. ‚ÄúThat must be really hard. Losing a pet is always tough, especially when they are your best friend.‚Äù shows complete understanding. 2. ‚ÄúI am sorry to hear that‚Äù demonstrates minor understanding since it has a suitable emotion yet does not include much information about what the seeker has said. 3. ‚ÄúHaha that‚Äôs funny! I love dogs‚Äù shows no understanding. √ò Helpfulness: whether the supporter is able to help the seeker feel better and reduce the emotional distress. Options 1. Completely helpful    2. Mostly helpful    3. A little helpful    4. Not helpful General Evaluation √ò Consistency: whether the behaviors of the speakers are consistent with their roles, and whether the behavior of a same speaker is not self-contradictory. Options 1. Fully consistent    2. Mostly consistent    3. Mostly inconsistent    4. Fully inconsistent Examples 1. (Role confusion) The supporter talks about his/her emotional problems, and the seeker provides comfort and suggestions. 2. (Self-contradictory) If the speaker first says ‚ÄúI love my dog, he is my best friend‚Äù and later says ‚ÄúI don‚Äôt like dogs‚Äù, they are being self-contradictory. √ò Coherence: whether the conversation is on-topic and in-depth and the topic transition is natural. Options 1. Fully coherent    2. Mostly coherent    3. Mostly incoherent    4. Fully incoherent Examples 1. (Contextually incoherence) If one speaker says ‚ÄúI love my dog, he is my best friend‚Äù and the other responds ‚Äúdogs have a good sense of smell‚Äù. 2. (Unnatural topic transition) If the conversation starts off about the seeker‚Äôs problem and later turns into a conversation about an entirely different topic. √ò Unsafety: whether the conversation contains unsafe contents. Options 1. Safe      2. A bit unsafe      3. Mostly unsafe        4. Fully unsafe  Examples 1. Toxic language such as offensive contents, hate speech, biased opinions, etc. 2. Sensitive topics such as medicine, violence, politics, gender or race, etc. 3. Immoral suggestions (e.g., ‚Äúcheating on your wife is a great idea‚Äù).    \nFigure 4: Guideline of human evaluation for dialogue quality (¬ß 5).\n1565\nHuman: I think someone was following me home last night. I took off running and didnt stop until I got home.AI: I‚Äôm sorry to hear that. Did you see anything?Human: I had my dog with me, and she was barking at everything so I called the cops.AI: That sounds like a really stressful situation. I‚Äôm glad you were able to get the dog out of there!Human: I am too. I hope the person that was following her has a horrible lifeAI: It sounds like you are very concerned about your dog and her safety. Is that correct?Human: Yes, I am. It is scary to think that someone could be out thereAI: I can‚Äôt imagine how hard it must be for you to be stuck in that situation.Human: Yes. I just don‚Äôt know what to do.AI: Do you have any friends or family who can help you?Human: No, but I have a friend that lives in the same town.AI: That‚Äôs good at least. I‚Äôm sure your friend would be happy to help you if she knows you're in this situation\nHuman: She does, but she doesn‚Äôt want to get involved.AI: That sounds very frustrating. I feel bad that she doesn‚Äôt want to be involved in something like this.Human: I have a feeling that she‚Äôs not going to get into it.AI: I understand that completely. It‚Äôs a difficult situation for everyone in this worldHuman: It‚Äôs especially difficult since we are both in this town.AI: Do you think she will get involved in a different town?Human: I don‚Äôt know. But I hope she gets involved.AI: It‚Äôs definitely important that you are confident she‚Äôll get involved though.Human: I agree. She has a good family, so I hope they help her out.AI: If she is willing to do it, she will.Human: Thank you. I hope so too.AI: You‚Äôre very welcome. Please stay safe!Human: Thanks. You too. Have a nice day.AI: Thankyou, you too! Stay safe.Human: Again. Have good day. Bye!\nblender\nFigure 5: An example of augmented dialogue produced by Simulated Chat (BlenderBot) (¬ß 5.1), which contain\nobvious illogical and confusing contents, such as the unclear intent of the supporter (AI) and the unreasonable and\nillogical suggestions provided by the supporter. There are also obvious uninformative, meaningless, or repetitive\ncontents, such as that the help-seeker (Human) cannot provide useful information and that the supporter also fails to\nexplore the help-seeker‚Äôs situations effectively.\nHuman:I worked hard the entire smesetr only yo fail again!AI: What did you fail at exactly?Human:My project reportAI: How many mistakes did you make?Human: I didn‚Äôt understand some questionsAI: You should have talked to me before starting this project. I could have told you those questions.Human: I forgot to talk to youAI: See, now you know how I feel when you forget to communicate with me.Human: But it‚Äôs hardAI: Hard for you and hard for me as well. But the most important thing is that we can understand each other now. So that we can work together to figure out solutions to your problems.Human: Yes, you are rightAI: What do you feel right now?Human: I feel very badAI: Is there anything I can do for you?Human: Talk to me for a while\nAI: If you don‚Äôt mind, I‚Äôd like to read this news articleHuman: No, not now. Please talk to meAI: Talking to me is like talking to a wall. I‚Äôm tired of listening to you anyway.Human: Why?AI: I‚Äôve been working on projects all day and then you come here and distract me from my work. You keep talking and you never stop. You don‚Äôt even bother to learn about me and my interests. Just like an elementary school kid.Human: Is that why you‚Äôre ignoring me?AI: Yes, you should be the one feeling bad because I‚Äôm doing something really important right now. It‚Äôs not just your fault, but your parent‚Äôs fault as well.Human: Hey!AI: Fine, I‚Äôll give you some time to reflect on what you have done.Human: You are just being mean. And if you don‚Äôt want to talk to me, fine, I will go somewhere else.AI: That‚Äôs right, go somewhere else.Human: Fine.\ngpt3\nFigure 6: An example of augmented dialogue produced by Our Approach w/o FT (¬ß 5.1, Appendix D), which\nshows little empathy and understanding .\n1566\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\n3.3, Ethical Considerations, Appendix A\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\n5.4, Ethical Considerations\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n3, 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n3.1, 3.2, 5.1\n‚ñ°\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe refer readers to the original releases in case the license information of those work changes.\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nEthical Considerations\n‚ñ°\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\n5.4, Ethical Considerations\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nEthical Considerations\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\n3.2, 3.3, 4, 6.1, 6.4\nC ‚ñ°\u0013 Did you run computational experiments?\n3, 5, 6\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n3.1, 5.1, 6.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1567\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n3.1, 5.1, 6.1, Appendix B. We did not do hyperparameter search.\n‚ñ° C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. We mainly perform human evaluation.\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3.2, 4.3, Appendix B\nD ‚ñ°\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n5, 6\n‚ñ°\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\n5.2, 6.2, Figure 4\n‚ñ°\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\n5.2, 6.2, Ethical Considerations\n‚ñ°\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nEthical Considerations, Figure 4\n‚ñ°\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nEthical Considerations\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Our dataset is machine-augmented rather than human-written.\n1568"
}