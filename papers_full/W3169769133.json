{
  "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens",
  "url": "https://openalex.org/W3169769133",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5063536578",
      "name": "Jiemin Fang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5075290241",
      "name": "Lingxi Xie",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5037191476",
      "name": "Xinggang Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100614065",
      "name": "Xiaopeng Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100665053",
      "name": "Wenyu Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100393506",
      "name": "Qi Tian",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2904699287",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3035223629",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3139445856",
    "https://openalex.org/W2950141105",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3122484828",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2972010972",
    "https://openalex.org/W2964081807",
    "https://openalex.org/W3214395992",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2964259004",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W3212131936",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2952865063",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3202406646"
  ],
  "abstract": "Transformers have offered a new methodology of designing neural networks for visual recognition. Compared to convolutional networks, Transformers enjoy the ability of referring to global features at each stage, yet the attention module brings higher computational overhead that obstructs the application of Transformers to process high-resolution visual data. This paper aims to alleviate the conflict between efficiency and flexibility, for which we propose a specialized token for each region that serves as a messenger (MSG). Hence, by manipulating these MSG tokens, one can flexibly exchange visual information across regions and the computational complexity is reduced. We then integrate the MSG token into a multi-scale architecture named MSG-Transformer. In standard image classification and object detection, MSG-Transformer achieves competitive performance and the inference on both GPU and CPU is accelerated. Code is available at https://github.com/hustvl/MSG-Transformer.",
  "full_text": "MSG-Transformer: Exchanging Local Spatial Information by\nManipulating Messenger Tokens\nJiemin Fang1,2 , Lingxi Xie3 , Xinggang Wang2†, Xiaopeng Zhang3 , Wenyu Liu2 , Qi Tian3\n1Institute of Artiﬁcial Intelligence, Huazhong University of Science & Technology\n2School of EIC, Huazhong University of Science & Technology 3Huawei Inc.\n{jaminfong, xgwang, liuwy}@hust.edu.cn\n{198808xc, zxphistory}@gmail.com tian.qi1@huawei.com\nAbstract\nTransformers have offered a new methodology of de-\nsigning neural networks for visual recognition. Compared\nto convolutional networks, Transformers enjoy the ability\nof referring to global features at each stage, yet the at-\ntention module brings higher computational overhead that\nobstructs the application of Transformers to process high-\nresolution visual data. This paper aims to alleviate the\nconﬂict between efﬁciency and ﬂexibility, for which we pro-\npose a specialized token for each region that serves as a\nmessenger ( MSG). Hence, by manipulating these MSG to-\nkens, one can ﬂexibly exchange visual information across\nregions and the computational complexity is reduced. We\nthen integrate the MSG token into a multi-scale architec-\nture named MSG-Transformer. In standard image classi-\nﬁcation and object detection, MSG-Transformer achieves\ncompetitive performance and the inference on both GPU\nand CPU is accelerated. Code is available at https:\n//github.com/hustvl/MSG-Transformer.\n1. Introduction\nThe past decade has witnessed the convolutional neural\nnetworks (CNNs) dominating the computer vision commu-\nnity. As one of the most popular models in deep learning,\nCNNs construct a hierarchical structure to learn visual fea-\ntures, and in each layer, local features are aggregated using\nconvolutions to produce features of the next layer. Though\nsimple and efﬁcient, this mechanism obstructs the commu-\nnication between features that are relatively distant from\neach other. To offer such an ability, researchers propose to\nreplace convolutions by the Transformer, a module which\nis ﬁrst introduced in the ﬁeld of natural language process-\ning [51]. It is shown that Transformers have the potential to\n†Corresponding author.\nThe work was done during Jiemin Fang’s internship at Huawei Inc.\nlearn visual representations and achieve remarkable success\nin a wide range of visual recognition problems including\nimage classiﬁcation [14, 40], object detection [4], semantic\nsegmentation [62], etc.\nThe Transformer module works by using a token to for-\nmulate the feature at each spatial position. The features\nare then fed into self-attention computation and each to-\nken, according to the vanilla design, can exchange infor-\nmation with all the others at every single layer. This de-\nsign facilitates the visual information to exchange faster but\nalso increases the computational complexity, as the compu-\ntational complexity grows quadratically with the number of\ntokens – in comparison, the complexity of a regular convo-\nlution grows linearly. To reduce the computational costs,\nresearchers propose to compute attention in local windows\nof the 2D visual features. However constructing local at-\ntention within overlapped regions enables communications\nbetween different locations but causes inevitable memory\nwaste and computation cost; computing attention within\nnon-overlapped regions impedes information communica-\ntions. As two typical local-attention vision Transformer\nmethods, HaloNet [50] partitions query features without\noverlapping but overlaps key and value features by slightly\nincreasing the window boundary; Swin Transformer [32]\nbuilds implicit connections between windows by alterna-\ntively changing the partition style in different layers, i.e.,\nshifting the split windows. These methods achieve com-\npetitive performance compared to vanilla Transformers, but\nHaloNet still wastes memories and introduces additional\ncost in the key and value; Swin Transformer relies on fre-\nquent 1D-2D feature transitions, which increase the imple-\nmentation difﬁculty and additional latency.\nTo alleviate the burden, this paper presents a new\nmethodology towards more efﬁcient exchange of informa-\ntion. This is done by constructing a messenger (MSG) to-\nken in each local window. Each MSG token takes charge\nof summarizing information in the corresponding window\n1\narXiv:2105.15168v3  [cs.CV]  25 Mar 2022\nand exchange it with other MSG tokens. In other words, all\nregular tokens are not explicitly connected to other regions,\nand MSG tokens serve as the hub of information exchange.\nThis brings two-fold beneﬁts. First, our design is friendly\nto implementation since it does not create redundant copies\nof data like [40, 50, 61]. Second and more importantly, the\nﬂexibility of design is largely improved. By simply ma-\nnipulating the MSG tokens (e.g., adjusting the coverage of\neach messenger token or programming how they exchange\ninformation), one can easily construct many different archi-\ntectures for various purposes. Integrating the Transformer\nwith MSG tokens into a multi-scale design, we derive a pow-\nerful architecture named MSG-Transformer that takes ad-\nvantages of both multi-level feature extraction and compu-\ntational efﬁciency.\nWe instantiate MSG-Transformer as a straightforward\ncase that the features of MSG tokens are shufﬂed and recon-\nstructed with splits from different locations. This can effec-\ntively exchange information from local regions and deliv-\nered to each other in the next attention computation, while\nthe implementation is easy yet efﬁcient. We evaluate the\nmodels on both image classiﬁcation and object detection,\nwhich achieve promising performance. We expect our ef-\nforts can further ease the research and application of multi-\nscale/local-attention Transformers for visual recognition.\nWe summarize our contributions as follows.\n• We propose a new local-attention based vision Trans-\nformer with hierarchical resolutions, which computes\nattention in non-overlapped windows. Communica-\ntions between windows are achieved via the proposed\nMSG tokens, which avoid frequent feature dimension\ntransitions and maintain high concision and efﬁciency.\nThe proposed shufﬂe operation effectively exchanges\ninformation from different MSG tokens with negligible\ncost.\n• In experiments, MSG-Transformers show promising\nresults on both ImageNet [11] classiﬁcation, i.e.,\n84.0% Top-1 accuracy, and MS-COCO [29] object de-\ntection, i.e., 52.8 mAP, which consistently outperforms\nrecent state-of-the-art Swin Transformer [32]. Mean-\nwhile, due to the concision for feature process, MSG-\nTransformer shows speed advantages over Swin Trans-\nformer, especially on the CPU device.\n• Not directly operating on the enormous patch tokens,\nwe propose to use the lightweight MSG tokens to ex-\nchange information. The proposed MSG tokens effec-\ntively extract features from local regions and may have\npotential to take effects for other scenarios. We be-\nlieve our work will be heuristic for future explorations\non vision Transformers.\n2. Related Works\nConvolutional Neural Networks CNNs have been a pop-\nular and successful algorithm in a wide range of computer\nvision problems. As AlexNet [27] shows strong perfor-\nmance on ImageNet [11] classiﬁcation, starting the bloom-\ning development of CNNs. A series of subsequent meth-\nods [18,22,43,45,46] emerge and persist in promoting CNN\nperformance on vision tasks. Beneﬁting from the evolv-\ning of backbone networks, CNNs have largely improved\nthe performance of various vision recognition scenarios in-\ncluding object detection [3,30,31,41,42], semantic/instance\nsegmentation [6, 7, 17], etc. As real-life scenarios usually\ninvolve resource-constrained hardware platforms ( e.g., for\nmobile and edge devices), CNNs are designed to take less\ncomputation cost [20, 35, 47]. Especially, with NAS ap-\nproaches [2, 15, 54, 63] applied, CNNs achieved high per-\nformance with extremely low cost, e.g., parameter number,\nFLOPs and hardware latency. A clear drawback of CNNs\nis that it may take a number of layers for distant features to\ncommunicate with each other, hence limiting the ability of\nvisual representation. Transformers aim to solve this issue.\nVision Transformer Networks Transformers, ﬁrst pro-\nposed by [51], have been widely used in natural language\nprocessing (NLP). The variants of Transformers, together\nwith improved frameworks and modules [1,12], have occu-\npied most state-of-the-art (SOTA) performance in NLP. The\ncore idea of Transformers lies in the self-attention mecha-\nnism, which aims at building relations between local fea-\ntures. Some preliminary works [21, 24, 40, 53, 61] explore\nto apply self-attention to networks for vision tasks and have\nachieved promising effects. Recently, ViT [14] proposes\nto apply a pure Transformer to image patch sequences,\nwhich matches or even outperforms the concurrent CNN\nmodels on image classiﬁcation. Inspired by ViT, a series\nof subsequent works [10, 16, 48, 49, 59] explore better de-\nsigns of vision Transformers and achieve promising promo-\ntion. Some works [28, 44, 55, 57] integrated modules from\nCNNs into vision Transformer networks and also achieve\ngreat results. In order to achieve strong results on image\nclassiﬁcation, many of the above ViT-based methods pro-\ncess features under a constant resolution and compute at-\ntentions within a global region. This makes it intractable\nto apply vision Transformers to downstream tasks, e.g., ob-\nject detection and semantic segmentation, as multi-scale ob-\njects are hard to be represented under a constant resolution,\nand increased input resolutions cause overloaded computa-\ntion/memory cost for attention computation.\nDownstream-friendly Vision Transformers To apply\nvision Transformers to downstream tasks, two key issues\nneed to be solved, i.e., involving hierarchical resolutions to\ncapture elaborate multi-scale features and decreasing cost\n2\nLocal-MSA\nMSG Token\nShuffling\nMSG Token Patch Token\nLayer Norm\nMLP\nLayer Norm\nFigure 1. Structure of the MSG-Transformer block. The 2D features are split into local windows (by green lines), and several windows\ncompose a shufﬂe region (the red one). Each local window is attached with oneMSG token. MSG tokens are shufﬂed to exchange information\nin each Transformer block and deliver the obtained information to patch tokens in the next self-attention.\nbrought by global attention computation. PVT [52] pro-\nposed to process features under multi-resolution stages and\ndown-samples key and value features to decrease the com-\nputation cost. HaloNet [50] and Swin Transformer [32]\npropose to compute attention in a local window. To over-\ncome the contradiction that non-overlapped windows lack\ncommunication while overlapped windows introduce ad-\nditional memory/computation cost, HaloNet proposes to\nslightly overlap features in the key and value tokens but\nleave the query non-overlapped; Swin Transformer alterna-\ntively changes the window partition style to implicitly build\nconnections between non-overlapped windows. A series of\nsubsequent works [9, 13, 23, 58] explore new methods for\nbuilding local-global relations or connecting local regions.\nWe newly propose MSG tokens to extract information from\nlocal windows, and use a lightweight method, i.e., shufﬂe,\nto exchange information between MSG tokens. This con-\ncise manner avoids direct operation on cumbersome patch\ntokens and shows high ﬂexibility.\n3. The MSG-Transformer\nThis section elaborates the proposed approach, MSG-\nTransformer. The core part is Sec. 3.1 where we introduce\nthe MSG token and explain how it works to simplify infor-\nmation exchange. Then, we construct the overall architec-\nture (i.e., the MSG-Transformer) in Sec. 3.2 and analyze the\ncomplexity in Sec. 3.3.\n3.1. Adding MSG Tokens to a Transformer Block\nThe MSG-Transformer architecture is constructed by\nstacking a series of MSG-Transformer blocks, through var-\nious spatial resolutions. As shown in Fig. 1, a MSG-\nTransformer block mainly composes of several modules,\ni.e., layer normalization (layer norm), local multi-head self-\nattention (local-MSA), MSG token shufﬂing and MLP.\nFig. 1 presents how features from a local spatial region\nare processed. First, the 2D features X ∈RH×W×C are\ndivided into non-overlapped windows (by green lines in\nFig. 1) as Xw ∈ R\nH\nw ×W\nw ×w2×C, where (H,W ) denotes\nthe 2D resolution of the features, Cdenotes the channel di-\nmension, and wdenotes the window size. Then R×Rwin-\ndows compose a shufﬂe region (boxed in red lines in Fig. 1),\nnamely features are split as Xr ∈ R\nH\nRw ×W\nRw ×R2×w2×C,\nwhere R denotes the shufﬂe region. In vision Transform-\ners [14, 48], image features are commonly projected into\npatch tokens by the input layer. Besides the patch tokens,\nwhich represent the intrinsic information of the images, we\nintroduce an additional token, named messenger ( MSG) to-\nken, to abstract information from patch tokens in a local\nwindow. Each local window is attached with one MSG to-\nken as X′\nw ∈R\nH\nRw ×W\nRw ×R2×(w2+1)×C. Then a layer nor-\nmalization is applied on all the tokens. The multi-head self-\nattention is performed within each local window between\nboth patch and MSG tokens. MSG tokens can capture in-\nformation from the corresponding windows with attention.\nAfterwards, all the MSG tokens TMSG ∈R\nH\nRw ×W\nRw ×R2×C\nfrom a same local regionR×Rare shufﬂed to exchange in-\nformation from different local windows. We name a region\nwith MSG tokens shufﬂed as the shufﬂe region. Finally, to-\nkens are processed by a layer normalization and a two-layer\nMLP.\nThe whole computing procedure of a MSG-Transformer\nblock can be summarized as follows.\nX′\nw = [TMSG; Xw] (1)\nX′\nw = Local-MSA(LN(X′\nw)) +X′\nw (2)\nTMSG = shufﬂe(TMSG) (3)\nX′\nw = MLP(LN(X′\nw)) +X′\nw (4)\nLocal Multi-head Self-Attention Different from previ-\nous vision Transformers [14, 48] which performer atten-\ntion computation along the global region, we compute self-\nattention within each local window. Taking a window of\nw ×w for example, the attention is computed on the to-\nken sequence of X = [tMSG; x1; ...; xw2 ], where tMSG\ndenotes the MSG token associated with this window and\nxi(1 ≤i ≤w2) denotes each patch token within the win-\ndow.\nAttention(Q,K,V ) =softmax(Q·KT /\n√\nd+ B) ·V, (5)\nwhere Q,K,V ∈R(w2+1)×d denotes the query, key and\nvalue matrices projected from sequence X respectively, d\n3\nLinear projection\n& window partition\nMSG-Transformer\nBlock × N1\nMSG-Transformer\nBlock × N2\n...\nToken Merging\nMSG-Transformer\nBlock × N3\nToken Merging\nMSG-Transformer\nBlock × N4\nToken Merging\nAttachMSG tokens\nStage 1 Stage 2 Stage 3 Stage 4\n×     ×(w2+1)×C_H\n4w\n_W\n4w ×     ×(w2+1)×2C_H\n8w\n_W\n8w ×      ×(w2+1)×4C_H\n16w\n_W\n16w ×      ×(w2+1)×8C_H\n32w\n_W\n32wH×W×3\nFigure 2. Overall architecture of MSG-Transformer. Patches from the input image are projected into tokens, and token features are\npartitioned into windows. Then each window is attached with one MSG token, which will participate in subsequent attention computation\nwith all the other patch tokens within the local window in every layer.\ngrouping\nshuffle\nFigure 3. Shufﬂing MSG tokens, where we inherit the example in\nFig. 1 for illustration.\ndenotes the channel dimension, and B denotes the relative\nposition biases. Following previous Transformer works [21,\n32, 39], the relative position biases between patch tokens in\nBare taken from the bias parameterbrel ∈R(2w−1)×(2w−1)\naccording to the relative token distances. The position bi-\nases between patch tokens and theMSG token tMSG is all set\nas equal, which is the same as the manner dealing with the\n[CLS] token in [25]. Speciﬁcally, matrix Bare computed\nas\nB =\n\n\n\nbrel\ni′,j′ i̸= 0,j ̸= 0\nθ1 i= 0\nθ2 i̸= 0,j = 0\n, (6)\nwhere i′= imod w−jmod w+w−1,j′= i//w−j//w+\nw−1, θ1,θ2 are two learnable parameters.\nExchanging Information by Shufﬂing MSG Tokens The\nMSG tokens allow us to exchange visual information ﬂex-\nibly. Here we instantiate an example using the shufﬂing\noperation, while we emphasize that the framework easily\napplies to other operations (see the next paragraph). In each\nMSG-Transformer block, MSG tokens in a same shufﬂe re-\ngion are shufﬂed to exchange information from different\nlocal windows. Assuming the shufﬂe region has a size of\nR×R, it means there exist R×RMSG tokens in this re-\ngion and each MSG token is associated with a w×wlocal\nwindow. As shown in Fig 3, channels of each MSG token\nare ﬁrst split into R ×R groups. Then the groups from\nR×RMSG tokens are recombined. With shufﬂe ﬁnished,\neach MSG token obtains information from all the other ones.\nWith the next attention computing performed, spatial infor-\nmation from the other local windows is delivered to patch\ntokens in the current window via the MSG token. Denoting\nMSG tokens in a R×Rshufﬂe region as TMSG ∈RR2×d,\nthe shufﬂe process can be formulated as\nT′\nMSG = reshape(TMSG), T′\nMSG ∈RR2×R2×d\nR2\nT′\nMSG = transpose(T′\nMSG,dim0 = 0,dim1 = 1)\nTMSG = reshape(T′\nMSG), TMSG ∈RR2×d\n, (7)\nwhere ddenotes the channel dimension of the MSG token,\nwhich is guaranteed to be divisible by the group number,\nR2.\nThough the shufﬂe operation has the similar manner with\nthat in convolutional network ShufﬂeNet [35,60], the effect\nis entirely different. ShufﬂeNet performs the shufﬂe oper-\nation to fuse separated channel information caused by the\ngrouped 1 ×1 convolution, while our MSG-Transformer\nshufﬂes the proposed MSG tokens to exchange spatial infor-\nmation from different local windows.\nExtensions There exist other ways of constructing and\nmanipulating MSG tokens. For example, one can extend the\nframework so that neighboring MSG tokens can overlap, or\nprogram the propagation rule so that the MSG tokens are\nnot fully-connected to each other. Besides, one can freely\ninject complex operators, rather than shufﬂe-based identity\nmapping, when the features of MSG tokens are exchanged.\nNote that some of these functions are difﬁcult to implement\nwithout taking MSG tokens as the explicit hub. We will in-\nvestigate these extensions in the future.\n3.2. Overall Architecture\nFig. 2 shows the overall architecture of MSG-\nTransformer. The input image is ﬁrst projected into patch\ntokens Tp ∈R\nH\n4 ×W\n4 ×C by a 7 ×7 convolution with stride\n4, where Cdenotes the channel dimension. The overlapped\nprojection is used for building better relations between\npatch tokens. Similar manners have also been adopted in\n4\nTable 1. Detailed settings for MSG-Transformer architecture variants. ‘cls’ denotes image classiﬁcation on ImageNet. ‘det’ denotes object\ndetection on MS-COCO. ‘dim’ denotes the embedding channel dimension. ‘#head’ and ‘#blocks’ denote the number of self-attention heads\nand MSG-Transformer blocks in each stage.\nStage Patch Token Shufﬂe Size MSG-Transformer-T MSG-Transformer-S MSG-Transformer-B\nResolution cls det dim #heads #blocks dim #heads #blocks dim #heads #blocks\n1 H\n4 ×W\n4 4 4 64 2 2 96 3 2 96 3 2\n2 H\n8 ×W\n8 4 4 128 4 4 192 6 4 192 6 4\n3 H\n16 ×W\n16 2 8 256 8 12 384 12 12 384 12 28\n4 H\n32 ×W\n32 1 4 512 16 4 768 24 4 768 24 4\nprevious methods [9,55]. Then the tokens are split into win-\ndows with the shape ofw×w, and each window is attached\nwith one MSG token, which has an equal channel number\nwith the patch token. The rest part of the architecture is con-\nstructed by stacking a series of MSG-Transformer blocks as\ndeﬁned in Sec. 3.1. To obtain features under various spa-\ntial resolutions, we downsample features by merging both\npatch and MSG tokens. Blocks under the same resolution\nform a stage. For both patch and MSG tokens, we use an\noverlapped 3 ×3 convolution with stride 2 to perform to-\nken merging and double the channel dimension in the next\nstage1. For image classiﬁcation, the ﬁnally merged MSG to-\nkens are projected to produce classiﬁcation scores. And for\ndownstream tasks like object detection, only patch tokens\nare delivered into the head structure while MSG tokens only\nserve for exchanging information in the backbone.\nIn our implementation, we build three architecture vari-\nants with different scales. As shown in Tab. 1, MSG-\nTransformer-T, -S and -B represent tiny, small, and base ar-\nchitectures with different channel numbers, attention head\nnumbers and layer numbers. The window size is set as\n7 for all architectures. The shufﬂe region size is set as\n4,4,2,1 in four stages respectively for image classiﬁcation\nand 4,4,8,4 for object detection. As demonstrated in sub-\nsequent studies (Sec. 4.3), our MSG-Transformer prefers\ndeeper and narrower architecture scales than Swin Trans-\nformer [32].\n3.3. Complexity Analysis\nThough introduced one MSG token in each local window,\nthe increased computational complexity is negligible. The\nlocal attention-based Transformer block includes two main\npart, i.e., local-MSA and two-layer MLP. Denoting the in-\nput patch token features as Tp ∈ R\nH\nw ×W\nw ×w2×C, where\nH,W denote the 2D spatial resolution, w denotes the lo-\ncal window size, and C denotes the channel number, the\n1The convolution parameters for merging tokens are shared between\npatch and MSG tokens.\ntotal FLOPs are computed as\nFLOPs = FLOPsMSA + FLOPsMLP\n= HW\nw2 ×(4w2C2 + 2w4C) + 2HW\nw2 w2 ·4C2.\n(8)\nWith the MSG tokens applied, the total FLOPs change to\nFLOPs′=HW\nw2 (4(w2 + 1)C2 + 2(w2 + 1)2C)\n+ 2HW\nw2 (w2 + 1)·4C2.\n(9)\nThe FLOPs increase proportion is computed as\nFLOPs′−FLOPs\nFLOPs\n=\nHW\nw2 (4C2 + 2(w2 + 1)C) + 2HW\nw2 ·4C2\nHW\nw2 ×(4w2C2 + 2w4C) + 2HW\nw2 ×w2 ×4C2\n= 6C+ w2 + 1\n6w2C+ w4 .\n(10)\nAs the window sizewis set as 7 in our implementations, the\nFLOPs increase proportion becomes 6C+50\n294C+74 . Taking the\nchannel number as 384 for example, the increased FLOPs\nonly account for ∼2.04% which are negligible to the total\ncomplexity.\nFor the number of parameters, all the linear projection\nparameters are shared between patch andMSG tokens. Only\nthe input MSG tokens introduce additional parameters, but\nthey are shared between shufﬂe regions, only taking 42C =\n16C, i.e., ∼0.0015M for the 96 input channel dimension.\nIn experiments, we prove even with the input MSG tokens\nnot learned, MSG-Transformers can still achieve as high\nperformance. From this, parameters from input MSG tokens\ncan be abandoned.\nIt is worth noting that due to local region communica-\ntion is achieved by shufﬂing MSG tokens, the huge feature\nmatrix of patch tokens only needs to be window-partitioned\nonce in a stage if the input images have a regular size. With\nMSG tokens assisting, cost from frequent 2D-to-1D matrix\ntransitions of patch tokens can be saved, which cause addi-\ntional latencies especially on computation-limited devices,\n5\nTable 2. Image classiﬁcation performance comparisons on\nImageNet-1K [11].\nMethod Input\nsize Params FLOPs Imgs/s CPU\nlatency\nTop-1\n(%)\nConvolutional Networks\nRegY-4G [38] 2242 21M 4.0G 930.1 138ms 80.0\nRegY-8G [38] 2242 39M 8.0G 545.5 250ms 81.7\nRegY-16G [38] 2242 84M 16.0G 324.6 424ms 82.9\nEffNet-B4 [47] 3802 19M 4.2G 345 315ms 82.9\nEffNet-B5 [47] 4562 30M 9.9G 168.5 768ms 83.6\nEffNet-B6 [47] 5282 43M 19.0G 96.4 1317ms 84.0\nTransformer Networks\nDeiT-S [48] 2242 22M 4.6G 898.3 118ms 79.8\nT2T-ViTt-14 [59] 2242 22M 5.2G 559.3 225ms 80.7\nPVT-Small [52] 2242 25M 3.8G 749.0 146ms 79.8\nTNT-S [16] 2242 24M 5.2G 387.1 215ms 81.3\nCoaT-Lite-S [57] 2242 20M 4.0G - - 81.9\nSwin-T [32] 2242 28M 4.5G 692.1 189ms 81.3\nMSG-T 2242 25M 3.8G 726.5 157ms 82.4\nDeiT-B [48] 2242 87M 17.5G 278.9 393ms 81.8\nT2T-ViTt-19 [59] 2242 39M 8.4G 377.3 314ms 81.4\nT2T-ViTt-24 [59] 2242 64M 13.2G 268.2 436ms 82.2\nPVT-Large [52] 2242 61M 9.8G 337.1 338ms 81.7\nTNT-B [16] 2242 66M 14.1G 231.1 414ms 82.8\nSwin-S [32] 2242 50M 8.7G 396.6 346ms 83.0\nMSG-S 2242 56M 8.4G 422.5 272ms 83.4\nViT-B/16 [14] 3842 87M 55.4G 81.1 1218ms 77.9\nViT-L/16 [14] 3842 307M 190.7G 26.3 4420ms 76.5\nDeiT-B [48] 3842 87M 55.4G 81.1 1213ms 83.1\nSwin-B [32] 2242 88M 15.4G 257.6 547ms 83.3\nMSG-B 2242 84M 14.2G 267.6 424ms 84.0\n* “Imgs/s” denotes the GPU throughput which is measured on one\n32G-V100 with a batch size of 64. Noting that throughput on 32G-\nV100 used in our experiments is sightly lower than 16G-V100 used\nin some other papers.\n* The CPU latency is measured with one core of Intel(R) Xeon(R)\nGold 6151 CPU @ 3.00GHz.\ne.g., CPU and mobile devices, but are unavoidable in most\nprevious local attention-based [32,40] or CNN-attention hy-\nbrid Transformers [10, 28, 55].\n4. Experiments\nIn experiments, we ﬁrst evaluate our MSG-Transformer\nmodels on ImageNet [11] classiﬁcation in Sec. 4.1. Then\nin Sec. 4.2, we evaluate MSG-Transformers on MS-\nCOCO [29] object detection and instance segmentation. Fi-\nnally, we perform a series of ablation studies and analysis\nin Sec. 4.3. Besides, we provide a MindSpore [36] imple-\nmentation of MSG-Transformer.\n4.1. Image Classiﬁcation\nWe evaluate our MSG-Transformer networks on the\ncommonly used image classiﬁcation dataset ImageNet-\n1K [11] and report the accuracies on the validation set\nin Tab. 2. Most training settings follow DeiT [48]. The\nTable 3. Object detection and instance segmentation performance\ncomparisons on MS-COCO [29] with Cascade Mask R-CNN [3,\n17]. “X101-32” and “X101-64” denote ResNeXt101-32×4d [56]\nand -64×4d respectively.\nMethod APboxAPbox\n50 APbox\n75 APmaskAPmask\n50 APmask\n75 Params FLOPs FPS\nDeiT-S 48.0 67.2 51.7 41.4 64.2 44.3 80M 889G -\nResNet-5046.3 64.3 50.5 40.1 61.7 43.4 82M 739G 10.5\nSwin-T 50.5 69.3 54.9 43.7 66.6 47.1 86M 745G 9.4\nMSG-T 51.4 70.1 56.0 44.6 67.4 48.1 83M 731G 9.1\nX101-32 48.1 66.5 52.4 41.6 63.9 45.2 101M 819G 7.5\nSwin-S 51.8 70.4 56.3 44.7 67.9 48.5 107M 838G 7.5\nMSG-S 52.5 71.1 57.2 45.5 68.4 49.5 113M 831G 7.5\nX101-64 48.3 66.4 52.3 41.7 64.0 45.1 140M 972G 6.0\nSwin-B 51.9 70.9 56.5 45.0 68.4 48.7 145M 982G 6.3\nMSG-B 52.8 71.3 57.3 45.7 68.9 49.9 142M 956G 6.1\n* FPS is measured on one 32G-V100 with a batch size of1.\nAdamW [26] optimizer is used with0.05 weight decay. The\ntraining process takes 300 epochs in total with a cosine an-\nnealing decay learning rate schedule [34] and 20-epoch lin-\near warmup. The total batch size is set as 1024 and the ini-\ntial learning rate is 0.001. The repeated augmentation [19]\nand EMA [37] are not used as in Swin Transformer [32].\nWe provide the ImageNet classiﬁcation results in Tab. 2\nand compare with other convolutional and Transformer net-\nworks. Compared with DeiT [33], MSG-Transformers\nachieve signiﬁcantly better trade-offs between accuracy and\ncomputation budget. MSG-Transformer-T achieves 2.6\nTop-1 accuracy promotion over DeiT-S with 0.8G smaller\nFLOPs; MSG-Transformer-S promotes the accuracy by 1.6\nwith only 48.0% FLOPs; MSG-Transformer-B achieves\nan 84.0% Top-1 accuracy, beating larger-resolution DeiT-\nB by 0.9 with only 25.6% FLOPs. Compared with the\nrecent state-of-the-art method Swin Transformer [32], our\nMSG-Transformers achieve competitive accuracies with\nsimilar Params and FLOPs. It is worth noting, as fre-\nquent 1D-2D feature transitions and partition are avoided,\nMSG-Transformers show promising speed advantages over\nSwin Transformers. Especially on the CPU device, the la-\ntency improvement is more evident. MSG-Transformer-T\nis 16.9% faster than Swin-T; MSG-Transformer-S is21.4%\nfaster than Swin-S; MSG-Transformer-B is 22.5% faster\nthan Swin-B.\n4.2. Object Detection\nWe evaluate our MSG-Transformer networks on MS-\nCOCO [29] object detection with the Cascade Mask R-\nCNN [3, 17] framework. The training and evaluation are\nperformed based on the MMDetection [5] toolkit. For train-\ning, we use the AdamW [26] optimizer with 0.05 weight\ndecay, 1 ×10−4 initial learning rate and a total batch size\nof 16. The learning rate is decayed by 0.1 at the 27 and 33\nepoch. The training takes the 3×schedule, i.e., 36 epochs\nin total. Multi-scale training with the shorter side of the im-\n6\nTable 4. Ablation studies about MSG tokens and shufﬂe operations\non ImageNet classiﬁcation.\nRow MSG Token Shufﬂe Op. Images / s Top1 (%)\nMSG-Transformer-T (depth=12)\n1 \u0017 \u0017 720.3 80.2\n2 ✓ \u0017 702.2 80.5↑0.3\n3 ✓ ✓ 696.7 81.1↑0.9\nMSG-Transformer-S (depth=24)\n4 \u0017 \u0017 412.9 81.2\n5 ✓ \u0017 403.9 81.9↑0.7\n6 ✓ ✓ 401.0 83.0↑1.8\nage resized between 480 and 800 and the longer side not\nexceeding 1333 is also used. As the input image size is not\nﬁxed for object detection, the patch tokens are padded with\n0 to guarantee they can be partitioned by the given window\nsize for attention computation. And the shufﬂe region is\nalternatively changed at the left-top and right-bottom loca-\ntions between layers to cover more windows.\nAs shown in Tab. 3, MSG-Transformers achieve signif-\nicantly better performance than CNN-based models, i.e.,\n5.1 APbox better than ResNet-50 [18], 4.4 APbox better\nthan ResNeXt101-32 ×4d [56], and 4.5 APbox better than\nResNeXt101-64×4d. Even though Swin Transformers have\nachieved extremely high performance on object detection,\nour MSG-Transformers still achieve signiﬁcant promotion\nby 0.9, 0.7, 0.9 APbox and 0.9, 0.8, 0.7 APmask for T, S, B\nscales respectively.\n4.3. Ablation Study\nIn this section, we perform a series of ablation studies\non ImageNet-1K about the shufﬂing operation, MSG tokens,\nnetwork scales, and shufﬂe region sizes2. We further visual-\nize the attention map ofMSG tokens for better understanding\nthe working mechanism.\nEffects of MSG Tokens & Shufﬂe Operations We study\nthe effects of MSG tokens and shufﬂe operations, providing\nthe results in Tab. 4. As shown in Row 1, with both MSG\ntokens and shufﬂe operations removed, the performance de-\ngrades by 0.9. With MSG tokens applied in Row 2, the per-\nformance is promoted by0.3 compared to that without both.\nThough without shufﬂe operations, MSG tokens can still ex-\nchange information in each token merging (downsampling)\nlayer, which leads to slight promotion. However, exchang-\ning information only in token merging layers is too limited\nto expanding receptive ﬁelds. With the same ablation ap-\nplied on a deeper network MSG-Transformer-S, the perfor-\nmance gap becomes signiﬁcant. The Top-1 accuracy drops\n2Without speciﬁed, experiments for ablation study remove the\noverlapped downsampling and follow the network scales in Swin-\nTransformer [32] for clear and fair comparisons.\nTable 5. Effects of input MSG/CLS token parameters on ImageNet\nclassiﬁcation.\nRow Training Evaluation Top1 (%)\nMSG-Transformer-T (MSG Token)\n1 learnable learned 80.9\n2 learnable random 80.8↓0.1\n3 random random 80.8↓0.1\nDeit-S (CLS Token)\n4 learnable learned 79.9\n5 learnable random 77.7↓2.2\nby 1.8 with both modules removed, and drops by 1.1 with\nshufﬂe removed. It is worth noting that both MSG tokens\nand shufﬂe operations are light enough and cause no evi-\ndent throughput decay.\nInput Parameters of MSG Tokens To further understand\nthe role MSG tokens play in Transformers, we study impacts\ncaused by parameters of input MSG tokens. As shown in\nRow 2 of Tab. 5, we randomly re-initialize parameters of\ninput MSG tokens for evaluation, which are learnable dur-\ning training, interestingly the accuracy only drops by 0.1%.\nThen in Row 3, we randomly initialize input MSG tokens\nand keep them ﬁxed during training. It still induces neg-\nligible accuracy drop when input MSG token parameters\nare also randomly re-initialized for evaluation. This im-\nplies input MSG token parameters are not so necessary to be\nlearned. We speculate that if input parameters of CLS to-\nkens in conventional Transformers need to be learned, and\nperform the same experiment on Deit-S [48]. Then we ﬁnd\nrandomly re-parameterizing input CLS tokens for evalua-\ntion leads to severe degradation to the accuracy, i.e., 2.2%\nin Row 5.\nThe above experiments show that the proposed MSG to-\nkens play a different role from conventional CLS tokens,\nwhich serve as messengers to carry information from dif-\nferent local windows and exchange information with each\nother. The input parameters of their own matter little in lat-\nter information delivering as they absorb local features layer\nby layer via attention computing. In other words, with unin-\nterrupted self-attention and information exchanging, patch\ntokens make what MSG tokens are and MSG tokens are just\nresponsible for summarizing local patch tokens and deliver\nthe message to other locations. Therefore, input parameters\nof MSG tokens do not affect the ﬁnal performance.\nNetwork Scales Considering different types of architec-\ntures ﬁt different network scales, we study the scales of\nboth Swin- and MSG-Transformer as follows. As shown\nin Tab. 6, two scales are evaluated where one is shallow and\nwide with 96 input dimension and [2,2,6,2] blocks in each\nstage, while another one is deep and narrow with64 dimen-\nsion and [2,4,12,4] blocks. We observe MSG-Transformer\nachieves a far better trade-off between computation cost and\n7\nInput image\n Block-2\n Block-4\n Block-7\n Block-10\n Block-12\nFigure 4. Visualization of attention maps computed between each MSG token and patch tokens within the local window in different blocks.\nTable 6. Network scale study on Swin- and MSG-Transformer.\nModel Dim #Blocks Params FLOPs Top1 (%)\nSwin 96 [2, 2, 6, 2] 28M 4.5G 81.3\n64 [2, 4, 12, 4] 24M 3.6G 81.3\nMSG 96 [2, 2, 6, 2] 28M 4.6G 81.1\n64 [2, 4, 12, 4] 24M 3.7G 82.1\nTable 7. Ablation studies about the shufﬂe region sizes on Ima-\ngeNet classiﬁcation.\nShufﬂe Region Sizes Images / s Top1(%)\n2, 2, 2, 1 695.1 80.6\n4, 2, 2, 1 696.1 80.8\n4, 4, 2, 1 696.7 80.9\naccuracy with the deeper-narrower scale. We analyze the\nreason as follows. In Swin Transformer, each patch to-\nken is involved in two different windows between layers,\nwhich requires a wider channel dimension with more atten-\ntion heads to support the variety. On the contrary, MSG-\nTransformer uses MSG tokens to extract window-level in-\nformation and transmit to patch tokens. This reduces the\ndifﬁculty for patch tokens to extract information from other\nwindows. Thus MSG-Transformer requires a smaller chan-\nnel capacity to support variety in one window. A deeper\nand narrower architecture brings a better trade-off for MSG-\nTransformer.\nShufﬂe Region Sizes We study the impacts of shufﬂe re-\ngion sizes on the ﬁnal performance. As shown in Tab. 7,\nwith the shufﬂe region enlarged, the ﬁnal accuracy in-\ncreases. It is reasonable that larger shufﬂe region sizes\nlead to larger receptive ﬁelds and are beneﬁcial for tokens\ncapturing substantial spatial information. Moreover, the\nthroughput/latency is not affected by the shufﬂe size chang-\ning.\nAttention Map Visualization of MSG Tokens To under-\nstand the working mechanism of MSG tokens, we visualize\nattention maps computed between each MSG token and its\nassociated patch tokens within the local window in different\nblocks. As shown in Fig. 4, local windows in attention maps\nare split into grids. Though the local window size to the to-\nken features is constant, i.e. 7 in our settings, with tokens\nmerged, the real receptive ﬁeld is enlarged when reﬂected\nonto the original image. In shallower blocks, attention of\nMSG tokens is dispersive which tends to capture contour in-\nformation; in deeper layers, though attention is computed\nwithin each local window, MSG tokens can still focus on lo-\ncations closely related to the object.\n5. Discussion and Conclusion\nThis paper proposes MSG-Transformer, a novel Trans-\nformer architecture that enables efﬁcient and ﬂexible infor-\nmation exchange. The core innovation is to introduce the\nMSG token which serves as the hub of collecting and prop-\nagating information. We instantiate MSG-Transformer by\nshufﬂing MSG tokens, yet the framework is freely extended\nby simply altering the way of manipulating MSG tokens.\nOur approach achieves competitive performance on stan-\ndard image classiﬁcation and object detection tasks with re-\nduced implementation difﬁculty and faster inference speed.\nLimitations We would analyze limitations from the per-\nspective of the manipulation type for MSG tokens. Though\nshufﬂing is an efﬁcient communication operation, the speci-\nﬁcity of shufﬂed tokens is not so well as shufﬂing integrates\ntoken segments from different local windows equally on the\nchannel dimension. On the other hand, it is valuable to\nexplore other manipulation types with a better efﬁciency-\nspeciﬁcity trade-off which may further motivate the poten-\ntial of MSG-Transformer.\nFuture work Our design puts forward an open problem:\nsince information exchange is the common requirement of\ndeep networks, how to satisfy all of capacity, ﬂexibility, and\nefﬁciency in the architecture design? The MSG token offers\na preliminary solution, yet we look forward to validating its\nperformance and further improving it in visual recognition\ntasks and beyond.\nAcknowledgement\nWe thank Yuxin Fang, Bencheng Liao, Liangchen Song,\nYuzhu Sun and Yingqing Rao for constructive discussions\nand assistance. This work was in part supported by NSFC\n(No. 61876212 and No. 61733007) and CAAI-Huawei\nMindSpore Open Fund.\n8\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. In NeurIPS, 2020. 2\n[2] Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct\nneural architecture search on target task and hardware. In\nICLR, 2019. 2\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In CVPR, 2018. 2, 6\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 1\n[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue\nWu, and Dahua Lin. Mmdetection: Open mmlab detection\ntoolbox and benchmark. arXiv:1906.07155, 2019. 6\n[6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv:1706.05587, 2017. 2\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. TPAMI, 2017. 2\n[8] Franc ¸ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017. 11\n[9] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vision\ntransformers. In NeurIPS 2021, 2021. 3, 5\n[10] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv e-prints, pages arXiv–2102,\n2021. 2, 6\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Fei-Fei Li. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 6\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv:1810.04805,\n2018. 2\n[13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo.\nCswin transformer: A general vision transformer backbone\nwith cross-shaped windows. arXiv:2107.00652, 2021. 3, 11\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2021. 1, 2, 3,\n6\n[15] Jiemin Fang, Yuzhu Sun, Qian Zhang, Yuan Li, Wenyu Liu,\nand Xinggang Wang. Densely connected search space for\nmore ﬂexible neural architecture search. In CVPR, 2020. 2\n[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. In NeurIPS\n2021, 2021. 2, 6\n[17] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 2, 6\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2, 7\n[19] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoeﬂer, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In CVPR, 2020.\n6\n[20] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efﬁcient con-\nvolutional neural networks for mobile vision applications.\narXiv:1704.04861, 2017. 2\n[21] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 2,\n4\n[22] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-\nian Q. Weinberger. Densely connected convolutional net-\nworks. In CVPR, 2017. 2\n[23] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,\nGang Yu, and Bin Fu. Shufﬂe transformer: Rethinking spa-\ntial shufﬂe for vision transformer. arXiv:2106.03650, 2021.\n3, 11\n[24] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 2\n[25] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional\nencoding in language pre-training. In ICLR, 2021. 4\n[26] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 6\n[27] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. In NeurIPS, 2012. 2\n[28] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng,\nBing Wang, Xiaodan Liang, and Xiaojun Chang. Bossnas:\nExploring hybrid cnn-transformers with block-wisely self-\nsupervised neural architecture search. arXiv:2103.12424,\n2021. 2, 6\n[29] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in\ncontext. In ECCV, 2014. 2, 6\n[30] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In ICCV,\n2017. 2\n[31] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng-Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In ECCV, 2016. 2\n9\n[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 11\n[33] Zili Liu, Tu Zheng, Guodong Xu, Zheng Yang, Haifeng Liu,\nand Deng Cai. Training-time-friendly network for real-time\nobject detection. arXiv:1909.00700, 2019. 6\n[34] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient\ndescent with warm restarts. In ICLR, 2017. 6\n[35] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufﬂenet V2: practical guidelines for efﬁcient CNN archi-\ntecture design. In ECCV, 2018. 2, 4\n[36] MindSpore. https://github.com/mindspore-\nai/mindspore. 6\n[37] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 1992. 6\n[38] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll ´ar. Designing network design\nspaces. In CVPR, 2020. 6\n[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. Journal of Machine Learn-\ning Research, 2020. 4\n[40] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In NeurIPS, 2019. 1, 2, 6\n[41] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Uniﬁed, real-time object de-\ntection. In CVPR, 2016. 2\n[42] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015. 2\n[43] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. InICLR,\n2014. 2\n[44] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck\ntransformers for visual recognition. arXiv:2101.11605,\n2021. 2\n[45] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR, 2015. 2\n[46] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. In CVPR, 2016.\n2\n[47] Mingxing Tan and Quoc V Le. Efﬁcientnet: Re-\nthinking model scaling for convolutional neural networks.\narXiv:1905.11946, 2019. 2, 6\n[48] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In ICML, 2021. 2, 3, 6, 7\n[49] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herv´e J´egou. Going deeper with im-\nage transformers. In ICCV, 2021. 2\n[50] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,\nNiki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling\nlocal self-attention for parameter efﬁcient visual backbones.\narXiv:2103.12731, 2021. 1, 2, 3\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 1,\n2\n[52] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. arXiv:2102.12122, 2021. 3,\n6\n[53] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 2\n[54] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang,\nFei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing\nJia, and Kurt Keutzer. Fbnet: Hardware-aware efﬁcient\nconvnet design via differentiable neural architecture search.\narXiv:1812.03443, 2018. 2\n[55] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers. arXiv:2103.15808, 2021.\n2, 5, 6\n[56] Saining Xie, Ross B. Girshick, Piotr Doll ´ar, Zhuowen Tu,\nand Kaiming He. Aggregated residual transformations for\ndeep neural networks. In CVPR, 2017. 6, 7\n[57] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen\nTu. Co-scale conv-attentional image transformers.\narXiv:2104.06399, 2021. 2, 6\n[58] Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan Yuille,\nand Wei Shen. Glance-and-gaze vision transformer. In\nNeurIPS, 2021. 3, 11\n[59] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv:2101.11986, 2021. 2, 6\n[60] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\nShufﬂenet: An extremely efﬁcient convolutional neural net-\nwork for mobile devices. arXiv:1707.01083, 2017. 4\n[61] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 2\n[62] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. In CVPR, 2021. 1\n[63] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\nLe. Learning transferable architectures for scalable image\nrecognition. In CVPR, 2018. 2\n10\nTable 8. Image classiﬁcation accuracy on ImageNet-1K compar-\ning with concurrent hierarchical Transformers.\nw/ dw-conv? Method Params FLOPs Top-1 (%)\n\u0017 Swin-T [32] 28M 4.5G 81.3\nMSG-T 25M 3.8G 82.4\n✓\nGG-T [58] 28M 4.5G 82.0\nShufﬂe-T [23] 29M 4.6G 82.5\nCSWin-T [13] 23M 4.3G 82.7\nMSG-Tdwc 25M 3.9G 83.0\nA. Appendix\nA.1. Comparisons with Concurrent Hierarchical\nTransformers\nSome related concurrent works [13, 23, 58] also focus\non improving attention computing patterns with different\nmanners based on a hierarchical architecture and achieve\nremarkable performance. These works introduce additional\ndepth-wise convolutions [8] into Transformer blocks, which\nimprove recognition accuracy with low FLOPs increase.\nOur MSG-Transformers in the main text do not include\ndepth-wise convolutions to make the designed model a\npurer Transformer. We further equip MSG-T with depth-\nwise convolutions, resulting in a variant named MSG-Tdwc.\nAs in Tab. 8, MSG-T dwc shows promising performance\nwith low FLOPs. We believe these newly proposed atten-\ntion computing patterns will facilitate future vision Trans-\nformer research in various manners and scenarios.\nA.2. Analysis about Advantages of MSG Tokens\nWe take Swin- [32] and MSG-Transformerfor compar-\nison, and analyze their behaviors from two aspects as fol-\nlows.\nReceptive ﬁelds. Let the window size be W. For Swin,\nthe window is shifted by W\n2 in every two Transformer\nblocks and the receptive ﬁeld is\n(3W\n2\n)2\nafter two atten-\ntion computations. For MSG, assuming the shufﬂe size is\nS ≥2, a larger receptive ﬁeld of (SW)2 is obtained with\ntwo attention computations.\nInformation exchange. In Swin, each patch token ob-\ntains information from other patch tokens in different win-\ndows, where valuable information is extracted by interact-\ning with many other patch tokens. In MSG, information\nfrom one window is summarized by a MSG token and di-\nrectly delivered to patch tokens in other windows. This\nmanner eases the difﬁculty of patch tokens obtaining infor-\nmation from other locations and promotes the efﬁciency.\nTable 9. Ablation study about MSG token manipulation.\nManip. Op. Shufﬂe Average Shift\nImageNet Top-1 (%) 81.1 80.8 80.6\nA.3. Study about Manipulations on MSG Tokens\nAs claimed in the main text, how to manipulate MSG\ntokens is not limited to the adopted shufﬂe operation. We\nstudy two additional manipulations, namely, the ‘average’\n(MSG tokens are averaged for the next-round attention) and\n‘shift’ (MSG tokens are spatially shifted). As in Tab. 9,\n‘shufﬂe’ works the best, and we conjecture that ‘average’\nlacks discrimination for different windows, and ‘shift’ re-\nquires more stages to deliver information to all the other\nwindows. We believe explorations on manipulation types\ncarry great potential and will continue this as an important\nfuture work.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7529846429824829
    },
    {
      "name": "Transformer",
      "score": 0.6797932386398315
    },
    {
      "name": "Security token",
      "score": 0.5220561027526855
    },
    {
      "name": "Convolutional neural network",
      "score": 0.49185991287231445
    },
    {
      "name": "Inference",
      "score": 0.4707760512828827
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4340583086013794
    },
    {
      "name": "Engineering",
      "score": 0.13651829957962036
    },
    {
      "name": "Voltage",
      "score": 0.12982553243637085
    },
    {
      "name": "Computer network",
      "score": 0.09786075353622437
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 10
}