{
  "title": "Semi-supervised vision transformer with adaptive token sampling for breast cancer classification",
  "url": "https://openalex.org/W4286470556",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Hubei Cancer Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2106085448",
      "name": "Ran Jiang",
      "affiliations": [
        "Hubei Provincial Women and Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2065860115",
      "name": "Ning Cui",
      "affiliations": [
        "Hubei Cancer Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1964961396",
      "name": "Qian Li",
      "affiliations": [
        "Hubei Cancer Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1916474955",
      "name": "Feng Yuan",
      "affiliations": [
        "Hubei Cancer Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1991537926",
      "name": "Zhifeng Xiao",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Hubei Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2106085448",
      "name": "Ran Jiang",
      "affiliations": [
        "Hubei Provincial Women and Children's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2065860115",
      "name": "Ning Cui",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Hubei Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1964961396",
      "name": "Qian Li",
      "affiliations": [
        "Hubei Cancer Hospital",
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1916474955",
      "name": "Feng Yuan",
      "affiliations": [
        "Hubei Cancer Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1991537926",
      "name": "Zhifeng Xiao",
      "affiliations": [
        "Pennsylvania State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3127426043",
    "https://openalex.org/W2991372685",
    "https://openalex.org/W2900257566",
    "https://openalex.org/W2789894922",
    "https://openalex.org/W2977153525",
    "https://openalex.org/W3084893494",
    "https://openalex.org/W2889646458",
    "https://openalex.org/W2012844989",
    "https://openalex.org/W6728184133",
    "https://openalex.org/W6640212811",
    "https://openalex.org/W3003949976",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3155361587",
    "https://openalex.org/W6804788224",
    "https://openalex.org/W6803042982",
    "https://openalex.org/W6775358223",
    "https://openalex.org/W2716665989",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6737664043",
    "https://openalex.org/W3037765194",
    "https://openalex.org/W6725739302",
    "https://openalex.org/W6763935843",
    "https://openalex.org/W2951250226",
    "https://openalex.org/W6703077136",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W3196993835",
    "https://openalex.org/W6839943048",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W2289683153",
    "https://openalex.org/W2962697252",
    "https://openalex.org/W2910570496",
    "https://openalex.org/W6634217980",
    "https://openalex.org/W3091883770",
    "https://openalex.org/W3081366712",
    "https://openalex.org/W3162357183",
    "https://openalex.org/W3002592716",
    "https://openalex.org/W2088252378",
    "https://openalex.org/W4214916274",
    "https://openalex.org/W6628973269",
    "https://openalex.org/W2143668817",
    "https://openalex.org/W6749781174",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W6730742563",
    "https://openalex.org/W2151801481",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2972214381",
    "https://openalex.org/W2344480160",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W2984353870",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4281631055",
    "https://openalex.org/W6765939562",
    "https://openalex.org/W3208123223",
    "https://openalex.org/W2619649363",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2335999708",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3215545016",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W4232743627",
    "https://openalex.org/W3013538309",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W4297775537",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4286892097",
    "https://openalex.org/W4294975187",
    "https://openalex.org/W2561184891",
    "https://openalex.org/W4285257024",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W1571052358",
    "https://openalex.org/W4288315162"
  ],
  "abstract": "Various imaging techniques combined with machine learning (ML) models have been used to build computer-aided diagnosis (CAD) systems for breast cancer (BC) detection and classification. The rise of deep learning models in recent years, represented by convolutional neural network (CNN) models, has pushed the accuracy of ML-based CAD systems to a new level that is comparable to human experts. Existing studies have explored the usage of a wide spectrum of CNN models for BC detection, and supervised learning has been the mainstream. In this study, we propose a semi-supervised learning framework based on the Vision Transformer (ViT). The ViT is a model that has been validated to outperform CNN models on numerous classification benchmarks but its application in BC detection has been rare. The proposed method offers a custom semi-supervised learning procedure that unifies both supervised and consistency training to enhance the robustness of the model. In addition, the method uses an adaptive token sampling technique that can strategically sample the most significant tokens from the input image, leading to an effective performance gain. We validate our method on two datasets with ultrasound and histopathology images. Results demonstrate that our method can consistently outperform the CNN baselines for both learning tasks. The code repository of the project is available at https://github.com/FeiYee/Breast-area-TWO .",
  "full_text": "Semi-supervised vision\ntransformer with adaptive token\nsampling for breast cancer\nclassiﬁcation\nWei Wang1† , Ran Jiang2† , Ning Cui3, Qian Li3, Feng Yuan1* and\nZhifeng Xiao4*\n1Department of Breast Surgery, Hubei Provincial Clinical Research Center for Breast Cancer, Hubei\nCancer Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan,\nHubei, China,2Department of Thyroid and Breast Surgery, Maternal and Child Health Hospital of Hubei\nProvince, Wuhan, Hubei, China,3Department of Ultrasound, Hubei Cancer Hospital, Tongji Medical\nCollege, Huazhong University of Science and Technology, Wuhan, Hubei, China,4School of\nEngineering,Penn State Erie, The Behrend College, Erie, PA, United States\nVarious imaging techniques combined with machine learning (ML) models have been\nused to build computer-aided diagnosis (CAD) systems for breast cancer (BC)\ndetection and classiﬁcation. The rise of deep learning models in recent years,\nrepresented by convolutional neural network (CNN) models, has pushed the\naccuracy of ML-based CAD systems to a new level that is comparable to human\nexperts. Existing studies have exploredt h eu s a g eo faw i d es p e c t r u mo fC N Nm o d e l s\nfor BC detection, and supervised learning has been the mainstream. In this study, we\npropose a semi-supervised learning framework based on the Vision Transformer\n(ViT). The ViT is a model that has been validated to outperform CNN models on\nnumerous classiﬁcation benchmarks but its application in BC detection has been\nrare. The proposed method offers a custom semi-supervised learning procedure that\nuniﬁes both supervised and consistency training to enhance the robustness of the\nmodel. In addition, the method uses an adaptive token sampling technique that can\nstrategically sample the most signiﬁcant tokens from the input image, leading to an\neffective performance gain. We validate our method on two datasets with ultrasound\nand histopathology images. Results demonstrate that our method can consistently\noutperform the CNN baselines for both learning tasks. The code repository of the\nproject is available athttps://github.com/FeiYee/Breast-area-TWO.\nKEYWORDS\nsemi-supervised learning, breast cancer detection, vision transformer, adaptive token\nsampling, data enhancement\nOPEN ACCESS\nEDITED BY\nYuanpeng Zhang,\nNantong University, China\nREVIEWED BY\nTongguang Ni,\nChangzhou University, China\nXiongtao Zhang,\nHuzhou University, China\n*CORRESPONDENCE\nFeng Yuan,\nwqdyu_yf@163.com\nZhifeng Xiao,\nzux2@psu.edu\n†These authors have contributed equally\nto this work and shareﬁrst authorship\nSPECIALTY SECTION\nThis article was submitted to\nExperimental Pharmacology and Drug\nDiscovery,\na section of the journal\nFrontiers in Pharmacology\nRECEIVED 27 April 2022\nACCEPTED 29 June 2022\nPUBLISHED 22 July 2022\nCITATION\nWang W, Jiang R, Cui N, Li Q, Yuan F and\nXiao Z (2022), Semi-supervised vision\ntransformer with adaptive token\nsampling for breast\ncancer classiﬁcation.\nFront. Pharmacol. 13:929755.\ndoi: 10.3389/fphar.2022.929755\nCOPYRIGHT\n© 2022 Wang, Jiang, Cui, Li, Yuan and\nXiao. This is an open-access article\ndistributed under the terms of the\nCreative Commons Attribution License\n(CC BY). The use, distribution or\nreproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nFrontiers inPharmacology frontiersin.org01\nTYPE Technology and Code\nPUBLISHED 22 July 2022\nDOI 10.3389/fphar.2022.929755\n1 Introduction\nBreast cancer (BC) has been the most common cancer type\nfor women. The 2020 report of the World Cancer Research Fund\nshows that there were more than 2 million newly diagnosed BC\ncases in 2018 ( Bray et al., 2018 ). Such worrying numbers\nhighlight the signi ﬁcance of properly using present\ntechnological advancements to undertake ef ﬁcient BC\ndetection in its early stage. In particular, a recent development\nin artiﬁcial intelligence (AI) that explores the usage of deep\nlearning models in a wide spectrum of health care applications\npresents a promising direction toward building a more effective\ncomputer-aided diagnosis (CAD) system for BC detection (Hu\net al., 2020; Mewada et al., 2020; Moon et al., 2020; Boumaraf\net al., 2021; Eroğlu et al., 2021; Mishra et al., 2021).\nA variety of imaging techniques can be used for BC detection\nand diagnosis, including X-rays (mammograms) (Abdelrahman\net al., 2021), ultrasound (sonography) (Moon et al., 2020; Mishra\net al., 2021), thermography (Singh and Singh, 2020), magnetic\nresonance imaging (MRI) ( Mann et al., 2019 ), and\nhistopathology imaging (Benhammou et al., 2020). Ultrasound\nhas been a widely adopted, low-cost, non-invasive, and non-\nradioactive imaging modality in the procedure of BC diagnosis\nand is usually followed by histopathological analysis. The latter\napplies biopsy techniques to collect cell/tissue samples that are\nplaced on a microscope slide and then stained for microscopic\nexamination. With a high degree of conﬁdence, histopathological\ndiagnosis has become the gold standard for almost all cancer\ntypes (Das et al., 2020). However, in spite of the usage of various\nimaging modalities, it requires radiologists or pathologists to\nperform a visual inspection, which is time-consuming and in\nneed of a high degree of radiological/pathological expertise. In\naddition, it has been shown by several studies that a high\npercentage of inter-observer variability exists when the same\nset of images are read by different experts (Kaushal et al., 2019).\nAn AI-powered system has the potential to eliminate this\nassessment discrepancy caused by different experiences,\nanalytical methodology, and knowledge between human\nbeings, providing a more accurate diagnostic result to support\nclinical decision-making (Hamed et al., 2020).\nRecent advances in AI, especially in deep learning, have been\nextensively investigated in the health care industry (Beam and\nKohane, 2018; Li and Xiao, 2022; Qu and Xiao, 2022). The\nnumber of use cases of deep learning in BC detection has also\nbeen increasing (Hamed et al., 2020). Our literature investigation\nshows that prior efforts in breast cancer image classiﬁcation share\ntwo common characteristics. First, the learning models are\nmostly based on the convolutional neural network (CNN),\nincluding existing deep CNN architectures, custom CNNs,\nand hybrid models with a CNN as a component. Despite the\neffectiveness of CNN-based classi ﬁcation models, recent\nadvances have witnessed the rise of a novel vision model,\nnamely, the Vision Transformer (ViT) ( Dosovitskiy et al.,\n2020), which has been shown to be more accurate in multiple\npublic benchmarks. Few studies have investigated the usage of\nthe ViT in BC detection (Gheﬂati and Rivaz, 2021), and the\npotential of the ViT has not been fully explored in this area.\nSecond, most existing studies are based on supervised learning,\nwhich requires a full annotation for all image samples in the\ndataset. The procedure of annotation is time-consuming and\nrequires domain expertise. Semi-supervised learning (SSL) (Van\nEngelen and Hoos, 2020), on the other hand, only requires\nannotation on a small subset of training data and combines a\nlarger subset of unlabeled data during training. SSL can\neffectively reduce the efforts of annotation. However, SSL has\nnot been extensively used in present studies of BC detection.\nOur study aims to address these methodological gaps.\nSpeciﬁcally, we propose a ViT-based BC classiﬁcation learning\npipeline that combines both supervised learning and SSL. We use\nan adaptive token sampling (ATS) technique (Fayyaz et al., 2021)\nthat allows the original ViT model to dynamically choose the\nmost critical image tokens. Moreover, we present a custom\nconsistency training (CT) strategy (Xie et al., 2020) to unify\nsupervised and unsupervised learning with image augmentation.\nThe CT-based SSL, when combined with an ATS-ViT (namely,\nViT with ATS), can effectively boost the model performance. The\nproposed method has been validated on two datasets, including\nthe dataset of breast ultrasound images (BUSI) (Al-Dhabyani\net al., 2020) and the Breast Cancer Histopathological Image\nClassiﬁcation (BreakHis) dataset (Spanhol et al., 2016). The\nresults of our method have been promising and superior\ncompared to the CNN models. The project is released under\nthe MIT License and is available athttps://github.com/FeiYee/\nBreast-area-TWO.\nThe rest of this study is organized as follows. We provide a\nliterature review for relevant studies in Section 2. Section 3\ndescribes the datasets used in this study and the details of the\nproposed model. InSection 4, several experiments are conducted\nto evaluate the effectiveness of the proposed model. Finally, in\nSection 5, we conclude the study and provide future work.\n2 Related work\nThis section reviews the prior studies in two aspects,\nincluding DNN-based BC detection methods and SSL applied\nin biomedical image classiﬁcation.\n2.1 Deep neural network-based breast\ncancer detection\nNumerous existing and custom deep CNN models have been\nused on both ultrasound and histopathology images for breast\ntumor classiﬁcation. Compared to feature-based learning models\nthat require hand-crafted features (Mishra et al., 2021), deep\nFrontiers inPharmacology frontiersin.org02\nWang et al. 10.3389/fphar.2022.929755\nneural network (DNN) models such as CNNs can learn\ndiscriminative patterns with automatically extracted features\nto represent an image sample (Li et al., 2021). For ultrasound\nimaging, Masud et al. (2020)proposed a custom CNN model\ncompared with several existing CNN models, including AlexNet\n(Kri zhevsky et al., 2012), Darknet19 (Redmon et al., 2016),\nGoogleNet (Szegedy et al., 2015), MobileNet (Howard et al.,\n2017), ResNet18 (He et al., 2016), ResNet50, VGG16 (Simonyan\nand Zisserman, 2014), and Xception (Chollet, 2017). In addition\nto single models, ensemble learning has also been used.Moon\net al. (2020) aggregated three CNN models, including VGGNet,\nResNet, and DenseNet (Huang et al., 2017) by fusing the image\nrepresentations. Similarly, Eroğlu et al. (2021 ) adopted a\nconcatenation of features generated by Alexnet, MobilenetV2\n(Sa ndler et al., 2018), and Resnet50, followed by a Minimum\nRedundancy Maximum Relevance-based feature selection\nstrategy to choose a set of the most valuable features that\nwere used to train a feature-based classi ﬁer [e.g., support\nvector machine (SVM) (Pisner and Schnyer, 2020), k-nearest\nneighbors (KNNs) ( Peterson, 2009)]. As for histopathology\nimaging, prior studies have adopted CNN models with\nimprovements in several aspects.Alom et al. (2019)proposed\nan Inception Recurrent Residual Convolutional Neural Network\n(IRRCNN) to combine the predictive power of the recurrent\nCNN, ResNet, and the Inception network. Wang et al. developed\nFE-BkCapsNet that integrates the CNN and CapsNet (Sabour\net al., 2017) with deep feature fusion and enhanced routing.\nMewada et al. (2020)proposed the use of both the spatial features\nof a CNN and the spectral features of a wavelet transform to\naddress the convergence issue during training. In addition to the\nimprovements in models, novel training strategies have also been\ndeveloped. Boumaraf et al. (2021)used a block-wiseﬁne-tuning\nmethod, allowing the last few residual blocks in the CNN to be\nmore domain-speciﬁc. Despite the extensive studies of DNN-\nbased models for BC detection, other model types have not been\nfully explored. The ViT, as a recently developed and highlighted\nvision model, has received signiﬁcant attention in a wide range of\ntasks. It is desirable to validate the effect of the ViT in imaging-\nbased BC detection. Our study is such an attempt.\n2.2 Semi-supervised learning-based\nbiomedical image classiﬁcation\nSSL has been an effective training technique to reduce the\nnumber of training examples required for a fully supervised\nlearning procedure. Obtaining a data point in the biomedical\ndomain could be time-consuming, especially in theﬁeld of cancer\nresearch, where it could take months or even years to determine a\npatient’s ﬁnal status (Zemmal et al., 2016). Thus, prior studies\nhave adopted SSL to use the unlabeled data.Zemmal et al. (2016)\nadopted a Semi-Supervised Support Vector Machine (S3VM)\nwith hand-crafted features for BC detection.Jaiswal et al. (2019)\nused pseudo labels on the PatchCamelyon-level to detect\nmetastasized cancer cells in histopathology diagnosis.Shi and\nZhang (2011) used low-density separation, an SSL method, to\nconduct gene expression-based outcome prediction for cancer\nrecurrence. Ma and Zhang (2018)developed an SSL model that\ncombines afﬁnity network fusion and a neural network to\nimplement few-shot learning, signi ﬁcantly improving the\nmodel’s learning ability with fewer training data. Other\napplications of SSL include cancer survival analysis ( Liang\net al., 2016 ), skin cancer diagnosis ( Masood et al., 2015 ),\nbladder cancer grading (Wenger et al., 2022), and colorectal\ncancer detection (Yu et al., 2021). To our best knowledge, prior\nstudies have not explored CT for BC detection, and our research\naims to ﬁll this gap.\n3 Materials and methods\n3.1 Dataset\nTwo datasets are used to validate the proposed method,\nincluding the dataset of breast ultrasound images (BUSI) (Al-\nDhabyani et al., 2020) and the Breast Cancer Histopathological\nImage Classiﬁcation (BreakHis) dataset (Spanhol et al., 2016)\nthat represent non-invasive and invasive BC detection methods,\nrespectively. Also, the choice of these two datasets allows our\nmodel to be trained and validated using images from diverse\nsources, which can be used to evaluate a model’s robustness.\n3.1.1 Breast ultrasound images dataset\nTable 1shows the three classes of BUSI and the number of\nimage samples for each class. Typically, ultrasound images are in\ngrayscale. The images were gathered at the Baheya hospital, saved\nin DICOM format, and converted to PNG format afterward.\nData collection and annotation took around 1 year to complete.\nThe total number of images acquired at the start of the project\nwas 1,100, which decreased to 780 after preprocessing to\neliminate images with unimportant information. The LOGIQ\nE9 and the LOGIQ E9 Agile ultrasound systems were used in the\nscanning procedure, producing images with a resolution of\n1280 × 1024. Figure 1 shows two example samples per class,\ntotaling six samples, in which (a) and (d) are benign, (b) and (e)\nTABLE 1 Three classes in the DBUI dataset.\nClass # Images per class\nBenign 487\nMalignant 210\nNormal 133\nTotal 780\nFrontiers inPharmacology frontiersin.org03\nWang et al. 10.3389/fphar.2022.929755\nare malignant, and (c) and (f) are normal. An experienced\nradiologist reads an ultrasound image based on a set of\nstandard criteria that involve mass size, echo nodule, tumor\nborders and morphology, calciﬁcation, blood ﬂow, and so on.\nThese criteria can be regarded as discriminative features allowing\na trained human being to determine the class of an image.\nTraditional feature-based models encode these criteria into\nhand-crafted features to represent an image, while DNN-\nbased models can automatically extract discriminative patterns\nand yield a higher accuracy (Shaheen et al., 2016; Han et al.,\n2017).\n3.1.2 BreakHis dataset\nThe BreakHis dataset contains 7,909 microscopic images of\nbreast tumor tissue, including 2,480 benign and 5,429 malignant\nsamples, collected from 82 patients by the P&D\nLaboratory–Pathological Anatomy and Cytopathology, Parana,\nBrazil. These images are with four magnifying factors,\nnamely, ×40, ×100, ×200, and ×400. All of the samples are of\n700 × 460 pixels with 3-channel RGB and 8-bit depth in each\nchannel, stored in PNG format. A histologically benign sample\ndoes not meet any malignancy criteria such as mitosis, basement\nmembranes disruption, metastasize, etc. In other words, benign\ntumors grow slowly and stay localized. On the contrary, the\nmalignant ones have locally invasive lesions that can disrupt\nadjacent structures and lead to metastasis to distant sites of the\nhuman body. Table 2 shows a stats summary of the BreakHis\ndataset.\nThe breast tissue slides are imaged digitally using an\nOlympus BX-50 system microscope equipped with a 3.3x\nrelay lens and a Samsung SCC-131AN digital color camera.\nThe collected slides are then stained with hematoxylin and\neosin (HE). The samples are obtained through surgical (open)\nbiopsy (SOB), which is then processed for histological\nexamination and labeled by pathologists from the P&D\nLaboratory. The standard paraf ﬁn method, which is widely\nused in clinical routine, was used in the preparation of the\nsamples in this study. The primary purpose is to keep the\noriginal tissue structure and molecular composition, which\nallows it to be observed under a light microscope in its\nnatural state. After staining, the anatomopathologists visually\nexamine the tissue samples with a microscope to determine\nwhether or not there are any cancerous lesions present in\neach slide. Experienced pathologists make the ﬁnal diagnosis\nFIGURE 1\nBUSI samples:(A,D) are benign tumor samples,(B,E) are malignant, and(C,F) are normal.\nTABLE 2 Stats of the BreakHis dataset.\nMagniﬁcation Benign Malignant Total\nx40 625 1,370 1,995\nx100 644 1,437 2,081\nx200 623 1,390 2,013\nx400 588 1,232 1,820\nTotal 2,480 5,429 7,909\n# Patients 24 58 82\nFrontiers inPharmacology frontiersin.org04\nWang et al. 10.3389/fphar.2022.929755\nin each case, which is then conﬁrmed by additional tests such as\nimmunohistochemistry (IHC) analysis.Figure 2 shows a set of\nsamples from the BreakHis dataset, in which the subﬁgures (a),\n(e), and (h) are benign samples, and the rest are all malignant.\n3.2 Overview of the learning framework\nFigure 3shows the overall workﬂow of the proposed method.\nThe core model to be trained is the ATS-ViT. The training\nprocedure comprises two parts, namely, supervised and\nconsistency training. The former aims to improve the model’s\npredictive ability, and the latter improves its generalization. Both\nparts are uniﬁed via an end-to-end training procedure (described\nin Algorithm 1). It should be noted that the parameters of the\nATS-ViT are shared across both parts of training. Also, three\ntypes of losses are combined to guide the optimization of the\nneural network via gradient descent. The training details are\ncovered in Subsection 3.6.\n3.3 Transformer\nA transformer (Vaswani et al., 2017) is a neural architecture that\nuses an attention mechanism tomine and capture the semantic\nmeanings and relations among the input tokens for sequential\nmodeling problems. One of the beneﬁts of the transformer is\nthat it allows parallelization since tokens passing through its\narchitecture can be processed independently rather than\nsequentially, presenting a un ique advantage over recurrent\nmodels such as long short term memory (LSTM) (Kim et al.,\n2016) and recurrent gated unit (GRU) (Chung et al., 2014). The\ntransformer was originally designed for machine translation in\nnatural language processing (NLP) and show ed superior\nperformance. Moreover, recent advances have explored\napplications of the transformer in a wide spectrum of NLP tasks\nand developed a rich set of pre-training techniques, making it one of\nthe most inﬂuential works in AI in the past 5 years.\nA transformer adopts an encoder-decoder structure. The\nencoder module comprises a stack of transformer encoders;\nsimilarly, the decoder module is a stack of transformer\ndecoders. Each transformer encoder includes a self-attention\nlayer with multiple attention heads to capture the semantic\ninteraction among the input tokens. Speci ﬁcally, each\nattention head calculates a tensor of scores to express how\neach token is affected (attended) by every other token. The\noutputs of these attention heads are aggregated, normalized,\nand passed to a feed-forward layer to generate a set of\nembeddings, which are the output of the present encoder. The\nsubsequent encoder takes as input the embeddings generated\nfrom its previous encoder and repeats the process. A transformer\ndecoder, on the other hand, comprises three layers, including a\nmulti-head self-attention layer, an encoder-decoder attention\nlayer, and a feed-forward layer. At each time step, a\nFIGURE 2\nBreakHis samples:(A,E,H) are benign, and(B– D,F,G) are malignant.\nFrontiers inPharmacology frontiersin.org05\nWang et al. 10.3389/fphar.2022.929755\ntransformer decoder takes as input two intermediate tensors\ngenerated by the last encoder layer, the embeddings from its\nprevious decoder (it would be the output of the decoder module\nat the previous time step for theﬁrst decoder); these data are fed\nthrough a stack of decoders, followed by a linear and a softmax\nlayer to produce the prediction result.\nFIGURE 3\nOverview of the proposed learning framework. The framework comprises supervised and consistency training uniﬁed via an end-to-end\ntraining procedure. For simplicity, theﬁgure only uses image samples from the BUSI dataset. The method has been validated on both datasets used in\nthis study.\nFIGURE 4\nArchitecture of the ATS-ViT. The ATS module can be integrated into each transformer block to perform two steps, including token score\nassignment and inverse transform sampling. The ATS can identify the most informative tokens that are passed to the subsequent layers, effectively\nreducing the computational cost and improving the classiﬁcation accuracy.\nFrontiers inPharmacology frontiersin.org06\nWang et al. 10.3389/fphar.2022.929755\n3.4 Vision transformer\nThe wide success of a transformer in NLP tasks inspired\nresearchers to explore its potential in computer vision. The ViT\nhas been one of theﬁrst efforts. The ViT adopts the same structure as\nthe original transformer with the following changes to the input. An\nimage is chunked into a set of image patches to meet the input\nrequirement of the transformer. The so-called image patch\nembedding operation is essentially a linear transformation, that\nis, a fully connected layer. Speciﬁc a l l y ,i fa ni n p u ti m a g eo fs i z eH ×\nW × C is split intoN patches (i.e., tokens), each of sizeP × P × C,\nthen we can determine thatN /equals\nHW\nP2 . Then, each patch is spread out\ninto a vector of sizeD. Thus, the input is transformed into a 2D\ntensor of sizeN × D.I na d d i t i o n ,as p e c i a l[ C L S ]t o k e ni si n s e r t e d\ninto the ﬁrst position of the token sequence to encode the\ninformation used for classi ﬁcation. This strategy has been\ncommonly seen in other pre-training strategies such as the\nBidirectional Encoder Repres entations from Transformers\n(BERT) (Devlin et al., 2018). Furthermore, to maintain the\nrelative position relationship between different patches, a position\nencoding vector is added to each patch embedding, generating a\ntoken embedding used by theﬁrst layer of the transformer encoder.\n3.5 Adaptive token sampler\nThe ViT is computationally expensive since the computing\ncost rises quadratically with the number of tokens. CNNs reduce\nthe resolution inside the network with different pooling\noperations. However, because the tokens are permutation\ninvariant, using pooling in the ViT is not feasible. Thus, we\nadopt an adaptive token sampler (ATS), a technique that allows\nthe model to dynamically choose signiﬁcant tokens from the\ninput tokens to reduce computational cost.Figure 4 shows the\nnetwork structure of ViT with ATS.\nAn ATS works by assigning a score to each of theN input\ntokens to determine which ones to keep. The score indicates a\ntoken’s contribution to the ﬁnal prediction. Let K be the\nmaximum number of retained tokens, and a sampling strategy\nis adopted as follows. LetK, Q, andV be the query, key, and value\nvectors, respectively, in the standard self-attention layer of the\ntransformer. The attention matrixA can be computedvia Eq. 1.\nA /equals Softmax\nQK⊤\n/radicaltpext/radicaltpext\nd\n√() .( 1 )\nThus, A is (N +1 )×( N + 1) (with the [CLS] token counted)\nand sums up to 1 after the softmax operation. The output tokens,\nbefore sampling, are given byEq. 2.\nO /equals AV.( 2 )\nLet Ai,j denote the element at rowi and columnj in A, the\nsigniﬁcance score of tokenj can be calculated byEq. 3.\nSj /equals A1,j∥Vj∥\n∑i/equals 2A1,i∥Vi∥.( 3 )\nOnly theﬁrst row of the attention matrixA is used since each\nelement A1,j represents the importance of tokenj to token 1,\nnamely, the [cls] token. With a signiﬁcance score calculated for\neach input token, the inverse transform sampling strategy is used\nfor token sampling. First, the cumulative distribution function of\nS can be calculatedvia Eq. (4).\nCDF\ni /equals ∑\nj/equals i\nj/equals 2\nSj.( 4 )\nIt is noted that theﬁrst token is excluded since it is used to\nencode the classiﬁcation information, and thus, is not needed for\nthe calculation of the CDF. The sampling function, denoted by\nϒ(k), can now be obtained via the inverse function of the CDF,\nwhich is given byEq. 5.\nϒ k() /equals CDF\n−1 k() .( 5 )\nTo obtain K′ samples (K′ ≤ K), ϒ(·)i sr u nK′ times from\nuniform distributionU[0, 1], which generatesK′ real numbers that\nare rounded to the nearest integers and used as the sampling indices.\nThe selected K′ output tokens should carry more informative\npatterns and are passed to the next transformer block.\n3.6 Semi-supervised learning\nSSL is a training paradigm that explores both labeled and\nunlabeled data to enhance the robustness of a model. Also, SSL is\na popular strategy when the number of training samples is\nlimited because of high annotation costs. In this study, we\nassume that similar images should belong to the same class,\nwhich is referred to as the smoothness assumption and has been\nadopted by many SSL training systems (Chen and Wang, 2010).\nCT is a typical SSL method used in prior studies (Xie et al., 2020;\nLee and Cho, 2021). CT allows a model to be trained to yield\nconsistent results for an image and its augmented versions with\nvarious perturbations such as crop, contrast,ﬂip, jittering, etc.\nThe proposed CT method is described in detail as follows.\nFirst, we divide the original training setX into two setsX\nl andXu,\ntreated as labeled and unlabeled datasets during CT, respectively.\nSecond, a set of image augmentation algorithms{hi}m\ni/equals 1 are deﬁned.\nAn unlabeled samplexu is fed into algorithmhi to generate an\naugmented image denoted byzu,i.L e tF denote the ViT model. The\ntraining objective of our SSL algorithm is three-fold.\n First, the supervised loss should be minimized to improve\nthe predictive ability of modelF. For our study, the binary\ncross-entropy loss is used, denoted byLCE. For a batch ofm\nlabeled samples {(xl,y l)}m\nl/equals 1, we can calculate LCE based\non Eq. 6\nFrontiers inPharmacology frontiersin.org07\nWang et al. 10.3389/fphar.2022.929755\nLCE /equals− 1\nm ∑\nm\nl/equals 1\nyl logF xl() .( 6 )\n Second, the pseudo-label loss should be minimized to\nencourage the model to produce consistent results for an\nimage and its augmented versions with perturbations. For\neach image x\nu in a batch ofm unlabeled data, a random\naugmentation algorithm is selected from{hi}m\ni/equals 1 and applied\nto the imagexu to generate an augmented imagezu.L e tF(xu)\nbe a pseudo-label, and we can then calculate pseudo-label loss\nusing the mean squared error based onEq. 7.\nLMSE /equals 1\nm ∑\nm\nu/equals 1\nF xu() − F zu()() 2.( 7 )\n Last, to ensure the consistency of the whole process, we also\nneed to measure the intermediate result of unlabeled data and\nits augmented version, and since the intermediate result of the\nViT is a one-dimensional sequence, we use Earth Mover’s\ndistance (Rubner et al., 2000), noted asL\nEM,w h i c hi su s e dt o\ndescribe the degree of similarity of two distributions. Given\ntwo sets of distributionsp1, p2... .pm and q1, q2... .qm,w en e e d\nto ﬁnd a way to arrangeq in such a way that the EML loss is\nminimized. The loss can be given byEq. 8.\nLEM p, q() /equals min\nq∈Q\n∑\nm\ni\nlq i,p i() , (8)\nwhere Q is the set of all possible permutations ofq and l stands for\nthe measurement, here, we choose it as L2 loss.\nAggregating the three aforementioned individual losses\nyields the following overall loss function, which is our ﬁnal\noptimization objective.\nL /equals LCE + LMSE + LEM.( 9 )\nWhen we ask the model to obtain similar features for data\nbefore and after adding multiple join perturbations, we can force\nthe model to learn what does not change with perturbation, and\nthe information that remains constant before and after\nperturbation is more relevant to the classiﬁcation result, and\nsuch a strategy will lead to stronger generalization ability.\nTherefore, we can conﬁrm that combining data augmentation\nstrategies with semi-supervised learning can give better results.\nAlgorithm 1. SSL algorithm.\n4 Results\nCodes in this study have been written in Python 3.6.10 and\nusing PyTorch 1.8.0 as the deep learning framework. All\nexperiments were run on a workstation with a Windows\n10 operating system, an i7-10875h CPU, and an Nvidia\nGTX2080TI 12G graphic card.\n4.1 Evaluation metrics\nSince the classes for both datasets are imbalanced, accuracy\n(Acc) is not sufﬁcient to reﬂect the true performance of a model.\nTherefore, in addition to ACC, we also use precision (Pre), recall\n(Rec), and F1 scores for performance evaluation. These\nindicators are deﬁned in Eqs 10–13.\nAcc /equals TP + TN\nTP + TN + FP + FN, (10)\nPre /equals TP\nTP + FP ×, (11)\nRec /equals TP\nTP + FN ×, (12)\nF1 /equals 2× Pre × Rec\nPre + Rec, (13)\nwhere TP, TN, FP, and FN refer to the number of true positives,\ntrue negatives, false positives, and false negatives, respectively.\nPre reﬂects the ratio of false alarms. The higher the pre, the fewer\nfalse alarms the model has. Meanwhile, Rec reﬂects the quantity\nof missed positive samples. In other words, the higher the Rec,\nthe fewer positive samples that have been missed. F1 represents\nthe harmonic mean of Pre and Rec, presenting a more suitable\nmetric than Acc for a classiﬁcation task with an imbalanced\ndataset.\n4.2 Baselines\nFour models have been chosen as the baselines in this study,\nnamely, the VGG16, ResNet101, DenseNet201, and ViT. All four\nmodels have been extensively used in a variety of image\nclassiﬁcation tasks and served as solid baselines.\n The VGG16 network comprises a sequence ofﬁve blocks,\neach with two or three convolutional layers for feature\nextraction, followed by a pooling layer for downscale\nsampling. The last block is further followed by three fully\nconnected layers and a softmax layer to generate a normalized\nvector as the prediction result. The VGG neural architecture\nextensively uses small (3 × 3) convolutionalﬁlters, which is the\nbasis for building a deep and accurate network.\nFrontiers inPharmacology frontiersin.org08\nWang et al. 10.3389/fphar.2022.929755\n The ResNet neural architecture stacks a sequence of\nresidual blocks, each of which facilitates the learning of\nan identity functionvia a shortcut connection by feeding\nthe input of a block directly into the output. This way, an\nidentify function can be easily learned, allowing a network\nwith more layers to be trained more effectively without\ndiminishing returns. ResNet101 contains a series of\nrepeated residual blocks followed by a dense and a\nsoftmax layer, with a total of 101 layers.\n DenseNet is a variant of ResNet with two differences. First,\nDenseNet uses a concatenation instead of a summation\n(used in ResNet) to aggregate the layer output and the\nshortcut data within each block. Second, DenseNet\nintroduces a transition layer placed between two dense\nblocks. Each transition layer comprises a 1 × 1\nconvolutional layer and an average pooling layer with a\nstride of two to control the model complexity.\n The ViT has been covered inSection 3.4.\n4.3 Training setting\nThe main hyperparameters used for training are shown in\nTable 3. We adopted Adam as the optimizer with a learning rate\nof 2e-5. We set eps = 1e-08 to prevent the denominator from\nbeing 0. A batch size of 64 was chosen. The loss function was the\nbinary cross entropy with logits. All evaluated models were\ntrained with 300 epochs. For the ViT, each input image was\nre-scaled to aﬁxed size of 256 × 256 and split into 16 patches. The\nViT model used in the study comprises six encoders. In the ATS\nprocedure, the numbers of tokens kept in each layer were 256,\n128, 64, 32, 16, and 8, which was the default setting from the\noriginal paper of the ATS. These parameters were obtained based\non empirical results. It is noted that we tried a variety of token\nsample numbers in addition to the default setting and did not\nobserve a signiﬁcant difference in results, which was because of\nthe fact that the sampling strategy of the ATS ensures that the\nmodel focuses on key regions, but does not completely discard\nthe information of some outlier data, so it can adjust the pattern\nextraction ability of the model for different types of data\naccording to the input.\nBoth datasets are split into training, validation, and test sets\nin the ratio of 7:1:2. In addition, the training set is further split in\nthe ratio of 8:2; 80% of the data in the training set participate in\nthe supervised training to learn an ATS-ViT model, and the rest\n20% are treated as unlabeled data used for CT.\n4.4 Results\nTable 4 presents a performance comparison between the\nproposed method and the chosen baselines. Also, an ablation\nstudy has been conducted to evaluate the efﬁcacy of the ATS and\nCT. Speciﬁcally, we used the ViT as a base model and added the\nATS and CT to form the ViT + ATS model and the CT + ViT +\nATS model. For each evaluated model, four metrics deﬁned in\nSection 3.1have been reported, including Acc, Pre, Rec, and F1.\nWe provide the result interpretation as follows.\n It is observed that the CNN models, namely, VGG19,\nResNet101, and DenseNet201, can achieve similar\nperformance compared with the ViT base model. In\nparticular, ResNet101 presents the highest Acc (95.59%)\nand F1 (94.76%) among the four baselines.\n The ViT base model does not perform better in our\nexperiments than the CNN models. In the original study\non the ViT, it has been validated to outperform the CNN\nmodels on several image classi ﬁcation tasks such as\nImageNet (Deng et al., 2009). In our experiment, the\nViT achieves an Acc of 93.38% and an F1 of 93.43%,\nranked the third and second places among the four\nbaselines. The reason why the ViT does not outperform\nall CNN models may be because of the training\nconﬁguration or the hyperparameter setting that has not\nbeen sufﬁciently optimized.\n The addition of the ATS to the ViT has improved the Acc\nand F1 by 1.07 and 1.04%, respectively. However, the ViT +\nATS is still not as good as ResNet101. The performance\ngain is mainly due to the sampling strategy that can\neffectively select a subset of tokens that contribute the\nmost to the classiﬁcation task.\nTABLE 3 Training setting.\nHyperparameter Value\nLearning rate 2e-5\nEps 1e-8\nBatch size 64\nEpochs 300\nInput image size 256 × 256\nATS # tokens [256, 128, 64, 32, 16, 8]\nTABLE 4 Results on BUSI.\nModel Acc Pre Rec F1\nVGG19 93.02 92.3 92.07 92.19\nResNet101 94.95 94.29 95.23 94.76\nDenseNet201 93.62 92.88 93.71 93.29\nViT 93.38 93.02 93.37 93.43\nViT + ATS 94.45 94.29 94.78 94.47\nCT + ViT + ATS (ours) 95.29 96.29 96.01 96.15\nThe highest scores of each metric are in bold.\nFrontiers inPharmacology frontiersin.org09\nWang et al. 10.3389/fphar.2022.929755\n Our best model, namely, CT + ViT + ATS, achieves the best\nresults on all four metrics with 95.29% Acc, 96.29% Pre,\n96.01% Rec, and 95.15% F1, outperforming the second-best\nscores by 0.34, 2, 0.78, and 1.39%, respectively. Compared with\nthe Vit + ATS model, the four scores have improved by 0.84, 2,\n1.23, and 1.86%. The performance gains are mainly due to the\ntraining procedure that combines both supervised and\nunsupervised training so that the model can experience\nmore diversiﬁed samples via data augmentation during\nconsistency training.\nTable 5s h o w st h er e s u l t so ft h ev a l i d a t e dm o d e l so nB r e a k H i s .\nT h es a m es e to fm o d e l sh a sb e e ne v a l u a t e d ,a n dt h er e s u l t sa r es i m i l a r\nto the ones on BUSI. We highlight the observations as follows.\n Among the four baseline models, DenseNet201 shows the\nhighest Acc of 97.42%, while VGG19 presents the highest\nF1 of 96.16%. The ViT base model posts an Acc of 95.68%\nand an F1 of 95.69%, ranked the third and second places,\nrespectively. Again, the ViT does not stand out on this\nclassiﬁcation task.\n The addition of the ATS improves the Acc and F1 by 1.3 and\n0.57%, respectively, lifting the model to the top place in F1\n(96.26), with CT + ViT + ATS excluded. This improvement\nshows that the ATS can effectively locate the image tokens with\nthe most informative parts, allowing the model to learn more\ndistinguishable patterns to boost accuracy. The result shows\nthat the ATS presents the desired effect and has been consistent\nacross both classiﬁcation tasks.\n CT + ViT + ATS, on the other hand, achieves the best\nperformance for all four metrics with an Acc of 98.12%, a\nPre of 98.17%, a Rec of 98.65%, and an F1 of 98.41%. This\nresult shows that CT can bring consistent performance\nboost on both datasets and is a promising strategy to\nimprove a model’s generalization ability.\nFigure 5shows the effect of the ATS on the four samples, with\ntwo from each dataset. In this, Figures 5A,B are ultrasound\nimages; and Figures 5C,D are histopathology samples.\nMeanwhile, Figures 5E–H are the same images as Figures\n5A–D with the eight most signiﬁcant tokens (image patches)\nkept for each image. These eight tokens are obtained from the last\ntransformer block, which is closer to the detection head, and\nFIGURE 5\nVisualized effect of the ATS. Subﬁgures (A,B) are ultrasound images; and(C,D) are histopathology samples. Meanwhile,(E–H) are the same\nimages as(A–D) with the eight most signiﬁcant tokens (image patches) kept for each image.\nTABLE 5 Results on BreakHis.\nModel Acc Pre Rec F1\nVGG19 96.41 96.45 95.88 96.16\nResNet101 95.53 95.54 94.38 94.96\nDenseNet201 97.42 93.98 97.89 95.6\nViT 95.68 95.67 95.7 95.69\nViT + ATS 96.98 96.85 95.68 96.26\nCT + ViT + ATS (ours) 98.12 98.17 98.65 98.41\nThe highest scores of each metric are in bold.\nFrontiers inPharmacology frontiersin.org10\nWang et al. 10.3389/fphar.2022.929755\nthus, is more expressive for the classiﬁcation result. It is observed\nthat these tokens can accurately identify the regions of interest\nthat are more indicative of the actual classes. Instead of looking at\nthe whole image, an ATS-enabled model can reduce the amount\nof global information and pinpoint the most critical areas that\ncontribute the most to the prediction results, which explains the\neffectiveness of the ATS.\n5 Discussion\nThis study presents CT + ViT + ATS, a ViT model trainedvia\nCT and boosted via ATS. The proposed model has been validated\non two BC imaging datasets and shown superior performance\ncompared to three representative CNN baseline models. The\nresults have demonstrated the efﬁcacy of both the ATS and CT.\nThe former allows the learning algorithm to identify the regions\nof interest that provide signiﬁcant patterns for the classiﬁcation\ntask, and the latter uniﬁes both supervised and unsupervised\ntraining to improve the generalization ability of the model. The\nproposed model, with the validated results, can serve as a credible\nbenchmark for future research.\nThere are several notable ﬁndings from this study. Our\nexperimental results show tha t the original ViT model does\nnot present superior perfor mance compared to its CNN\ncompetitors. On the BUSI dataset, the ViT is on a par with\nthe CNN models, whereas on the BreakHis dataset, the ViT is\nslightly worse but still comparable. This could be because of\nthe BC detection task, in which the images may contain subtle\npatterns hard to capture even with the self-attention\nmechanism used by the ViT. To discover these subtle\npatterns and improve detection accuracy, we adopt the ATS\nand CT as two boosting modules, which turn out to be\neffective. The gains, in Acc and F1, brought by the ATS\nand CT, have been notable and consistent on both datasets.\nAlthough the ATS was originally developed to reduce\ncomputational costs, we demonstrate that it also improves\nthe detection accuracy since the model is encouraged to focus\nmore on the critical image tokens and learn more subtle\npatterns. CT, on the other hand, exploits the existing\ntraining resources via a weakly-supervised training\nparadigm that effectively improves the robustness of the\nmodel. The two boosting modules reﬁne the original ViT in\nthree aspects: model, data, and training procedure. These joint\nefforts have been consistent for our task and have the potential\nto be used for other biomedical computer vision tasks.\nThe proposed CT + ViT + ATS method can be a core\nfunctional module of a CAD system for BC detection. It offers\ntwo merits. First, the ATS component allows the system to\nhighlight the most informative image patches, which can help\nphysicians quickly pinpoint the critical areas for precise and\npersonalized diagnosis. Second, the backend of the CAD system\ncan be easily modiﬁed to be a continuous learning system once\nnew images are available. Since CT is semi-supervised, only a\nportion of the newly added data needs to be labeled, signiﬁcantly\nreducing the labor cost for annotation.\nThe proposed method can be extended in the following\ndirections. First, we mainly compared CNN models and the\nViT, while an ensemble of the two or feature-level aggregation\ncan be another model design option that may bring together\nthe strengths of both neural architectures. Given that the\nunderlying designs of the CNN and the ViT are\nfundamentally different, the former adopts multiple ﬁlters\nto capture multi-scale features, while the latter explores\nsemantic relations between each pair of tokens; a\ncombination of the two could present superior performance\ncompared to any single model. Second, a generative model\nsuch as a generative adversarial network (GAN) can be used to\nperform data augmentation in CT. Since a GAN captures the\ndistribution of images belong ing to a class, a well-trained\nGAN can generate synthetic images that look similar to real\nones. These generated images can enhance the quantity and\ndiversity of the training samples during CT, potentially\nleading to a more robust mo del. Lastly, the proposed\nmethod can be applied to a wider range of BC imaging\ndatasets with additional image modalities such as X-ray,\nMRI, and thermography that are not considered in this\nstudy. It would be interesting to evaluate the proposed\nmethod on a multi-modal BC imaging dataset that offers\nmulti-dimensional feat ure representations.\nData availability statement\nThe datasets presented in this study can be found in online\nrepositories. The names of the repository/repositories and\naccession number(s) can be found at:https://www.kaggle.com/\ndatasets/aryashah2k/breast-ultrasound-images-dataset (accessed\non 20 November 2021) and http://web.inf.ufpr.br/vri/breast-\ncancer-database (accessed on 25 November 2021).\nAuthor contributions\nConceptualization and methodology, WW, RJ, ZX, NC, QL,\nand FY; data analysis, software, validation, and original draft\npreparation, WW and RJ; review and editing, and supervision,\nZX, QL, and FY. All authors have read and agreed to the\npublished version of the manuscript.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nFrontiers inPharmacology frontiersin.org11\nWang et al. 10.3389/fphar.2022.929755\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors, and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbdelrahman, L., Al Ghamdi, M., Collado-Mesa, F., and Abdel-Mottaleb, M.\n(2021). Convolutional neural networks for breast cancer detection in\nmammography: A survey. Comput. Biol. Med. 131, 104248. doi:10.1016/j.\ncompbiomed.2021.104248\nAl-Dhabyani, W., Gomaa, M., Khaled, H., and Fahmy, A. (2020). Dataset of\nbreast ultrasound images.Data Brief. 28, 104863. doi:10.1016/j.dib.2019.104863\nAlom, M. Z., Yakopcic, C., Nasrin, M. S., Taha, T. M., and Asari, V. K. (2019).\nBreast cancer classiﬁcation from histopathological images with inception recurrent\nresidual convolutional neural network.J. Digit. Imaging32, 605–617. doi:10.1007/\ns10278-019-00182-7\nBeam, A. L., and Kohane, I. S. (2018). Big data and machine learning in health\ncare. Jama 319, 1317–1318. doi:10.1001/jama.2017.18391\nBenhammou, Y., Achchab, B., Herrera, F., and Tabik, S. (2020). BreakHis based\nbreast cancer automatic diagnosis using deep learning: Taxonomy, survey and\ninsights. Neurocomputing 375, 9–24. doi:10.1016/j.neucom.2019.09.044\nBoumaraf, S., Liu, X., Zheng, Z., Ma, X., and Ferkous, C. (2021). A new transfer\nlearning based approach to magniﬁcation dependent and independent classiﬁcation\nof breast cancer in histopathological images.Biomed. Signal Process. Control63,\n102192. doi:10.1016/j.bspc.2020.102192\nBray, F., Ferlay, J., Soerjomataram, I., Siegel, R. L., Torre, L. A., Jemal, A., et al.\n(2018). Global cancer statistics 2018: GLOBOCAN estimates of incidence and\nmortality worldwide for 36 cancers in 185 countries.Ca. Cancer J. Clin.68, 394–424.\ndoi:10.3322/caac.21492\nChen, K., and Wang, S. (2010). Semi-supervised learning via regularized boosting\nworking on multiple semi-supervised assumptions. IEEE Trans. Pattern Anal.\nMach. Intell. 33, 129–143. doi:10.1109/TPAMI.2010.92\nChollet, F. (2017). “Xception: Deep learning with depthwise separable\nconvolutions,” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 1251–1258.\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv Prepr. arXiv:\n1412.3555.\nDas, A., Nair, M. S., and Peter, S. D. (2020). Computer-aided\nhistopathological image analysis tech niques for automated nuclear atypia\nscoring of breast cancer: A review. J. Digit. Imaging 33, 1091–1121. doi:10.\n1007/s10278-019-00295-z\nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., and Fei-Fei, L. (2009).“Imagenet: A\nlarge-scale hierarchical image database,” in 2009 IEEE conference on computer\nvision and pattern recognition (Miami, FL, USA: IEEE), 248–255.\nDevlin, J., Chang, M. W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of\ndeep bidirectional transformers for language understanding.arXiv Prepr. arXiv:\n1810.04805.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: Transformers for image\nrecognition at scale.arXiv Prepr. arXiv:2010.11929.\nEroğ\nlu, Y., Yildirim, M., and Çinar, A. (2021). Convolutional Neural Networks\nbased classiﬁcation of breast ultrasonography images by hybrid method with\nrespect to benign, malignant, and normal using mRMR. Comput. Biol. Med.\n133, 104407. doi:10.1016/j.compbiomed.2021.104407\nFayyaz, M., Kouhpayegani, S. A., Jafari, F. R., Sommerlade, E., Joze, H. R. V.,\nPirsiavash, H., et al. (2021). Ats: Adaptive token sampling for efﬁcient vision\ntransformers. arXiv:2111.15667 [cs].\nGheﬂati, B., and Rivaz, H. (2021). Vision transformer for classiﬁcation of breast\nultrasound images. arXiv Prepr. arXiv:2110.14731.\nHamed, G., Marey, M. A. E. R., Amin, S. E. S., and Tolba, M. F. (2020).“Deep\nlearning in breast cancer detection and classi ﬁcation,” in The International\nConference on Artiﬁcial Intelligence and Computer Vision (Berlin, Germany:\nSpringer), 322–333.\nHan, Z., Wei, B., Zheng, Y., Yin, Y., Li, K., Li, S., et al. (2017). Breast cancer multi-\nclassiﬁcation from histopathological images with structured deep learning model.\nSci. Rep. 7, 4172. doi:10.1038/s41598-017-04075-z\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).“Deep residual learning for image\nrecognition,” in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 770–778.\nHoward, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., et al.\n(2017). Mobilenets: Efﬁcient convolutional neural networks for mobile vision\napplications. arXiv Prepr. arXiv:1704.04861.\nHu, Q., Whitney, H. M., and Giger, M. L. (2020). A deep learning methodology\nfor improved breast cancer diagnosis using multiparametric MRI.Sci. Rep. 10,\n10536. doi:10.1038/s41598-020-67441-4\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. (2017).“Densely\nconnected convolutional networks,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 4700–4708.\nJaiswal, A. K., Panshin, I., Shulkin, D., Aneja, N., and Abramov, S. (2019). Semi-\nsupervised learning for cancer detection of lymph node metastases.arXiv Prepr.\narXiv:1906.09587.\nKaushal, C., Bhat, S., Koundal, D., and Singla, A. (2019). Recent trends in\ncomputer assisted diagnosis (CAD) system for breast cancer diagnosis using\nhistopathological images. Irbm 40, 211–227. doi:10.1016/j.irbm.2019.06.001\nK i m ,J . ,K i m ,J . ,T h u ,H .L .T . ,a n dK i m ,H .( 2 0 1 6 ) .“Long short term memory recurrent\nneural network classiﬁer for intrusion detection,” in 2016 international conference on\nplatform technology and service (PlatCon) (Jeju, South Korea: IEEE), 1–5.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation\nwith deep convolutional neural networks.Adv. neural Inf. Process. Syst.25.\nLee, J., and Cho, S. (2021). Semi-supervised image classiﬁcation with grad-CAM\nconsistency. arXiv:2108.13673. doi:10.48550/arXiv.2108.13673\nLi, G., and Xiao, Z. (2022). Transfer learning-based neuronal cell instance\nsegmentation with pointwise attentive path fusion.IEEE Access.\nLi, Z., Liu, F., Yang, W., Peng, S., and Zhou, J. (2021). A survey of convolutional\nneural networks: Analysis, applications, and prospects.IEEE Trans. Neural Netw.\nLearn. Syst. 2021, 1–21. doi:10.1109/TNNLS.2021.3084827\nL i a n g ,Y . ,C h a i ,H . ,L i u ,X .Y . ,X u ,Z .B . ,Z h a n g ,H . ,L e u n g ,K .S . ,e ta l .( 2 0 1 6 ) .C a n c e r\nsurvival analysis using semi-supervised learning method based on Cox and AFT models\nwith L1/2 regularization.BMC Med. Genomics9, 11. doi:10.1186/s12920-016-0169-6\nMa, T., and Zhang, A. (2018). Afﬁnity network fusion and semi-supervised\nlearning for cancer patient clustering.Methods 145, 16–24. doi:10.1016/j.ymeth.\n2018.05.020\nM a n n ,R .M . ,K u h l ,C .K . ,a n dM o y ,L .( 2 0 1 9 ) .C o n t r a s t - e n h a n c e dM R If o r\nbreast cancer screening. J .M a g n .R e s o n .I m a g i n g50, 377–390. doi:10.1002/\njmri.26654\nMasood, A., Al- Jumaily, A., and Anam, K. (2015).“Self-supervised learning\nmodel for skin cancer diagnosis, ” in 2015 7th International IEEE/EMBS\nConference on Neural Engineering (NER), 1012 –1015. doi:10.1109/NER.\n2015.7146798\nMasud, M., Eldin Rashed, A. E., and Hossain, M. S. (2020). Convolutional neural\nnetwork-based models for diagnosis of breast cancer.Neural Comput. Appl.doi:10.\n1007/s00521-020-05394-5\nMewada, H. K., Patel, A. V., Hassaballah, M., Alkinani, M. H., and Mahant, K.\n(2020). Spectral–spatial features integrated convolution neural network for breast\ncancer classiﬁcation. Sensors 20, 4747. doi:10.3390/s20174747\nMishra, A. K., Roy, P., Bandyopadhyay, S., and Das, S. K. (2021). Breast\nultrasound tumour classi ﬁcation: A machine learning — radiomics based\napproach. Expert Syst. 38, e12713. doi:10.1111/exsy.12713\nMoon, W. K., Lee, Y. W., Ke, H. H., Lee, S. H., Huang, C. S., Chang, R. F., et al.\n(2020). Computer-aided diagnosis of breast ultrasound images using ensemble\nlearning from convolutional neural networks.Comput. Methods Programs Biomed.\n190, 105361. doi:10.1016/j.cmpb.2020.105361\nPeterson, L. E. (2009). K-nearest neighbor.Scholarpedia 4, 1883. doi:10.4249/\nscholarpedia.1883\nPisner, D. A., and Schnyer, D. M. (2020).Support vector machine. Machine\nLearning. Amsterdam, Netherlands: Elsevier, 101–121.\nFrontiers inPharmacology frontiersin.org12\nWang et al. 10.3389/fphar.2022.929755\nQu, R., and Xiao, Z. (2022). An attentive multi-modal cnn for brain tumor\nradiogenomic classiﬁcation. Information 13, 124. doi:10.3390/info13030124\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016).“You only look once:\nUniﬁed, real-time object detection,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 779–788.\nRubner, Y., Tomasi, C., and Guibas, L. J. (2000). The Earth mover’s distance as a metric\nfor image retrieval.Int. J. Comput. Vis.40, 99–121. doi:10.1023/a:1026543900054\nSandler, M., Howard, A., Zhu, M., ZhmogiNov, A., and Chen, L. C. (2018).\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 4510–4520.\nSabour, S., Frosst, N., and Hinton, G. E. (2017). Dynamic routing between\ncapsules. Adv. neural Inf. Process. Syst.30.\nShaheen, F., Verma, B., and Asafuddoula, M. (2016).“Impact of automatic feature\nextraction in deep learning architecture,” in 2016 International conference on\ndigital image computing: techniques and applications (DICTA) (Gold Coast, QLD,\nAustralia: IEEE), 1–8.\nShi, M., and Zhang, B. (2011). Semi-supervised learning improves gene\nexpression-based prediction of cancer recurrence.Bioinformatics 27, 3017–3023.\ndoi:10.1093/bioinformatics/btr502\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutional networks for\nlarge-scale image recognition.arXiv Prepr. arXiv:1409.1556.\nSingh, D., and Singh, A. K. (2020). Role of image thermography in early breast\ncancer detection-Past, present and future.Comput. Methods Programs Biomed.183,\n105074. doi:10.1016/j.cmpb.2019.105074\nSpanhol, F. A., Oliveira, L. S., Petitjean, C., and Heutte, L. (2016). A dataset for\nbreast cancer histopathological image classiﬁcation. IEEE Trans. Biomed. Eng.63,\n1455–1462. doi:10.1109/TBME.2015.2496264\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015).\n“Going deeper with convolutions,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 1–9.\nVan Engelen, J. E., and Hoos, H. H. (2020). A survey on semi-supervised learning.\nMach. Learn. 109, 373–440. doi:10.1007/s10994-019-05855-6\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need.Adv. neural Inf. Process. Syst.30.\nWenger, K., Tirdad, K., Dela Cruz, A., Mari, A., Basheer, M., Kuk, C., et al. (2022).\nA semi-supervised learning approach for bladder cancer grading.Mach. Learn.\nAppl. 9, 100347. doi:10.1016/j.mlwa.2022.100347\nXie, Q., Dai, Z., Hovy, E., Luong, T., and Le, Q. (2020). Unsupervised data\naugmentation for consistency training. Adv. Neural Inf. Process. Syst. 33,\n6256–6268.\nY u ,G . ,S u n ,K . ,X u ,C . ,S h i ,X .H . ,W u ,C . ,X i e ,T . ,e ta l .( 2 0 2 1 ) .A c c u r a t e\nrecognition of colorectal cancer with semi-supervised deep learning on\npathological images. Nat. Commun. 12, 6311. doi:10.1038/s41467-021-\n26643-8\nZemmal, N., Azizi, N., Dey, N., and Sellami, M. (2016). Adaptive semi supervised\nsupport vector machine semi supervised learning with features cooperation for\nbreast cancer classiﬁcation. J. Med. Imaging Health Inf.6, 53–62. doi:10.1166/jmihi.\n2016.1591\nFrontiers inPharmacology frontiersin.org13\nWang et al. 10.3389/fphar.2022.929755",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8335819244384766
    },
    {
      "name": "Artificial intelligence",
      "score": 0.72508704662323
    },
    {
      "name": "Machine learning",
      "score": 0.6627695560455322
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5817549228668213
    },
    {
      "name": "Security token",
      "score": 0.5690604448318481
    },
    {
      "name": "Deep learning",
      "score": 0.5587152242660522
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5294662714004517
    },
    {
      "name": "CAD",
      "score": 0.501124382019043
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43379172682762146
    },
    {
      "name": "Artificial neural network",
      "score": 0.4288978576660156
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Engineering drawing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210160841",
      "name": "Hubei Cancer Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210124300",
      "name": "Hubei Provincial Women and Children's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    }
  ],
  "cited_by": 56
}