{
  "title": "PolyTransform: Deep Polygon Transformer for Instance Segmentation",
  "url": "https://openalex.org/W2994028575",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287574757",
      "name": "Liang, Justin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287574756",
      "name": "Homayounfar, Namdar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225190243",
      "name": "Ma, Wei-Chiu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353665690",
      "name": "Xiong Yu-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120954864",
      "name": "Hu Rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749412874",
      "name": "Urtasun, Raquel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2897894710",
    "https://openalex.org/W2935807868",
    "https://openalex.org/W2608858501",
    "https://openalex.org/W2558156561",
    "https://openalex.org/W2979913009",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2086921140",
    "https://openalex.org/W2971475609",
    "https://openalex.org/W2921231843",
    "https://openalex.org/W2953100873",
    "https://openalex.org/W1923115158",
    "https://openalex.org/W2950477723",
    "https://openalex.org/W2216125271",
    "https://openalex.org/W2965289249",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W2557889580",
    "https://openalex.org/W2945490722",
    "https://openalex.org/W2798725844",
    "https://openalex.org/W1507506748",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2949842255",
    "https://openalex.org/W2982723417",
    "https://openalex.org/W2991405684",
    "https://openalex.org/W2912662889",
    "https://openalex.org/W2964236837",
    "https://openalex.org/W2984281342",
    "https://openalex.org/W2161236525",
    "https://openalex.org/W2168804568",
    "https://openalex.org/W2104095591",
    "https://openalex.org/W2555751471",
    "https://openalex.org/W2912997088",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2895065325",
    "https://openalex.org/W2317851288",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1948751323",
    "https://openalex.org/W2322480645",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W2963623257",
    "https://openalex.org/W2928684767",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2895355932",
    "https://openalex.org/W2974371958",
    "https://openalex.org/W2470139095",
    "https://openalex.org/W2950646696",
    "https://openalex.org/W260801291",
    "https://openalex.org/W2955671438",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2787998955",
    "https://openalex.org/W2949533892",
    "https://openalex.org/W2777795072",
    "https://openalex.org/W2938992057",
    "https://openalex.org/W2065429801",
    "https://openalex.org/W2963073398",
    "https://openalex.org/W2963406768",
    "https://openalex.org/W2795276939",
    "https://openalex.org/W2793693263",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W2956134899",
    "https://openalex.org/W2990361805",
    "https://openalex.org/W809122546"
  ],
  "abstract": "In this paper, we propose PolyTransform, a novel instance segmentation algorithm that produces precise, geometry-preserving masks by combining the strengths of prevailing segmentation approaches and modern polygon-based methods. In particular, we first exploit a segmentation network to generate instance masks. We then convert the masks into a set of polygons that are then fed to a deforming network that transforms the polygons such that they better fit the object boundaries. Our experiments on the challenging Cityscapes dataset show that our PolyTransform significantly improves the performance of the backbone instance segmentation network and ranks 1st on the Cityscapes test-set leaderboard. We also show impressive gains in the interactive annotation setting. We release the code at https://github.com/uber-research/PolyTransform.",
  "full_text": "PolyTransform: Deep Polygon Transformer for Instance Segmentation\nJustin Liang1 Namdar Homayounfar1,2\nWei-Chiu Ma1,3 Yuwen Xiong1,2 Rui Hu1 Raquel Urtasun1,2\n1Uber Advanced Technologies Group 2University of Toronto 3 MIT\n{justin.liang,namdar,weichiu,yuwen,rui.hu,urtasun}@uber.com\nAbstract\nIn this paper, we propose PolyTransform, a novel\ninstance segmentation algorithm that produces precise,\ngeometry-preserving masks by combining the strengths of\nprevailing segmentation approaches and modern polygon-\nbased methods. In particular, we ﬁrst exploit a segmenta-\ntion network to generate instance masks. We then convert\nthe masks into a set of polygons that are then fed to a de-\nforming network that transforms the polygons such that they\nbetter ﬁt the object boundaries. Our experiments on the\nchallenging Cityscapes dataset show that our PolyTrans-\nform signiﬁcantly improves the performance of the back-\nbone instance segmentation network and ranks 1st on the\nCityscapes test-set leaderboard. We also show impressive\ngains in the interactive annotation setting.\n1. Introduction\nThe goal of instance segmentation methods is to identify\nall countable objects in the scene, and produce a mask for\neach of them. With the help of instance segmentation, we\ncan have a better understanding of the scene [68], design\nrobotics systems that are capable of complex manipulation\ntasks [17], and improve perception systems of self-driving\ncars [44]. The task is, however, extremely challenging. In\ncomparison to the traditional semantic segmentation task\nthat infers the category of each pixel in the image, instance\nsegmentation also requires the system to have the extra no-\ntion of individual objects in order to associate each pixel\nwith one of them. Dealing with the wide variability in the\nscale and appearance of objects as well as occlusions and\nmotion blur make this problem extremely difﬁcult.\nTo address these issues, most modern instance segmen-\ntation methods employ a two-stage process [21, 63, 42],\nwhere object proposals are ﬁrst created and then foreground\nbackground segmentation within each bounding box is per-\nformed. With the help of the box, they can better handle\nsituations (e.g., occlusions) where other methods often fail\n[4]. While these approaches have achieved state-of-the-\nart performance on multiple benchmarks (e.g., COCO [38],\nCityscapes [11]) their output is often over-smoothed failing\nto capture ﬁne-grained details.\nAn alternative line of work tackles the problem of inter-\nactive annotation [5, 2, 62, 39]. These techniques have been\ndeveloped in the context of having an annotator in the loop,\nwhere a ground truth bounding box is provided. The goal\nof these works is to speed up annotation work by provid-\ning an initial polygon for annotators to correct as annotat-\ning from scratch is a very expensive process. In this line of\nwork, methods exploit polygons to better capture the geom-\netry of the object [5, 2, 39], instead of treating the problem\nas a pixel-wise labeling task. This results in more precise\nmasks and potentially faster annotation speed as annotators\nare able to simply correct the polygons by moving the ver-\ntices. However, these approaches suffer in the presence of\nlarge occlusions or when the object is split into multiple dis-\nconnected components.\nWith these problems in mind, in this paper we develop a\nnovel model, which we call PolyTransform, and tackle both\nthe instance segmentation and interactive annotation prob-\nlems. The idea behind our approach is that the segmentation\nmasks generated by common segmentation approaches can\nbe viewed as a starting point to compute a set of polygons,\nwhich can then be reﬁned. We performed this reﬁnement\nvia a deforming network that predicts for each polygon the\ndisplacement of each vertex, taking into account the loca-\ntion of all vertices. By deforming each polygon, our model\nis able to better capture the local geometry of the object.\nUnlike [5, 2, 39], our model has no restriction on the num-\nber of polygons utilized to describe each object. This allows\nus to naturally handle cases where the objects are split into\nparts due to occlusion.\nWe ﬁrst demonstrate the effectiveness of our approach on\nthe Cityscapes dataset [11]. On the task of instance segmen-\ntation, our model improves the initialization by 3.0 AP and\n10.3 in the boundary metric on the validation set. Impor-\ntantly, we achieve1st place on the test set leaderboard, beat-\ning the current state of the art by 3.7 AP. We further eval-\nuate our model on a new self-driving dataset. Our model\nimproves the initialization by 2.1 AP and 5.6 in the bound-\narXiv:1912.02801v4  [cs.CV]  16 Jan 2021\nFigure 1. Overview of our PolyTransform model.\nary metric. In the context of interactive annotation, we out-\nperform the previous state of the art [62] by 2.0% in the\nboundary metric. Finally, we conduct an experiment where\nthe crowd-sourced labelers annotate the object instances us-\ning the polygon output from our model. We show that this\ncan speed up the annotation time by 35%!\n2. Related Work\nIn this section, we brieﬂy review the relevant literature\non instance segmentation and annotation in the loop.\nProposal-based Instance Segmentation: Most modern\ninstance segmentation models adopt a two-stage pipeline .\nFirst, an over-complete set of segment proposals is iden-\ntiﬁed, and then a voting process is exploited to determine\nwhich one to keep [8, 14] As the explicit feature extraction\nprocess [53] is time-consuming [19, 20], Dai et al. [13, 12]\nintegrated feature pooling into the neural network to im-\nprove efﬁciency. While the speed is drastically boosted\ncomparing to previous approaches, it is still relatively slow\nas these approach is limited by the traditional detection\nbased pipeline. With this problem in mind, researchers have\nlooked into directly generating instance masks in the net-\nwork and treat them as proposals [51, 52]. Based on this\nidea, Mask R-CNN [21] introduced a joint approach to do\nboth mask prediction and recognition. It builds upon Faster\nR-CNN [54] by adding an extra parallel header to predict\nthe object’s mask, in addition to the existing branch for\nbounding box recognition. Liu et al. [42] propose a path ag-\ngregation network to improve the information ﬂow in Mask\nR-CNN and further improve performance. More recently,\nChen et al. [6] interleaves bounding box regression, mask\nregression and semantic segmentation together to boost in-\nstance segmentation performance. Xu et al. [64] ﬁt Cheby-\nshev polynomials to instances by having a network learn\nthe coefﬁcients, this allows for real time instance segmen-\ntation. Huang et al. [25] optimize the scoring of the bound-\ning boxes by predicting IoU for each mask rather than only\na classiﬁcation score. Kuo et al. [34] start with bounding\nboxes and reﬁne them using shape priors. Xiong et al. [63]\nand Kirillov et al. [31] extended Mask R-CNN to the task\nof panoptic segmentation. Yang et al. [65] extended Mask\nR-CNN to the task of video instance segmentation.\nProposal-free Instance Segmentation: This line of re-\nsearch aims at segmenting the instances in the scene without\nan explicit object proposal. Zhang et al. [67, 66] ﬁrst pre-\ndicts instance labels within the extracted multi-scale patches\nand then exploits dense Conditional Random Field [33] to\nobtain a consistent labeling of the full image. While achiev-\ning impressive results, their approach is computationally in-\ntensive. Bai and Urtasun [4] exploited a deep network to\npredict the energy of the watershed transform such that each\nbasin corresponds to an object instance. With one simple\ncut, they can obtain the instance masks of the whole im-\nage without any post-processing. Similarly, [32] exploits\nboundary prediction to separate the instances within the\nsame semantic category. Despite being much faster, they\nsuffer when dealing with far or small objects whose bound-\naries are ambiguous. To address this issue, Liu et al. [41]\npresent a sequential grouping approach that employs neural\nnetworks to gradually compose objects from simpler ele-\nments. It can robustly handle situations where a single in-\nstance is split into multiple parts. Newell and Deng [49] im-\nplicitly encode the grouping concept into neural networks\nby having the model to predict both semantic class and a\ntag for each pixel. The tags are one dimensional embed-\ndings which associate each pixel with one another. Kendall\net al. [28] propose a method to assign pixels to objects hav-\ning each pixel point to its object’s center so that it can be\ngrouped. Soﬁiuk et al. [58] use a point proposal network\nto generate points where the instances can be, this is then\nprocessed by a CNN to outputs instance masks for each lo-\ncation. Neven et al. [48] propose a new clustering loss that\npulls the spatial embedding of pixels belonging to the same\ninstance together to achieve real time instance segmenta-\ntion while having high accuracy. Gao et al. [18] propose\na single shot instance segmentation network that outputs a\npixel pair afﬁnity pyramid to compute whether two pixels\nbelong to the same instance, they then combine this with a\npredicted semantic segmentation to output a single instance\nsegmentation map.\nFigure 2. Our feature extraction network.\nInteractive Annotation: The task of interactive annota-\ntion can also be posed as ﬁnding the polygons or curves\nthat best ﬁt the object boundaries. In fact, the concept of de-\nforming a curve to ﬁt the object contour can be dated back to\nthe 80s where the active contour model was ﬁrst introduced\n[27]. Since then, variants of ACM [10, 47, 9] have been\nproposed to better capture the shape. Recently, the idea\nof exploiting polygons to represent an instance is explored\nin the context of human in the loop segmentation [5, 2].\nCastrej´on et al. [5] adopted an RNN to sequentially predict\nthe vertices of the polygon. Acuna et al. [2] extended [5]\nby incorporating graph neural networks and increasing im-\nage resolution. While these methods demonstrated promis-\ning results on public benchmarks [11], they require ground\ntruth bounding box as input. Ling et al. [39] and Dong et\nal. [16] exploited splines as an alternative parameterization.\nInstead of drawing the whole polygon/curves from scratch,\nthey start with a circle and deform it. Wang et al . tack-\nled this problem with implicit curves using level sets [62],\nhowever, because the outputs are not polygons, an anno-\ntator cannot easily corrected them. In [46], Maninis et al.\nuse extreme boundary as inputs rather than bounding boxes\nand Majumder et al. [45] uses user clicks to generate con-\ntent aware guidance maps; all of these help the networks\nlearn stronger cues to generate more accurate segmenta-\ntions. However, because they are pixel-wise masks, they\nare not easily amenable by an annotator. Acuna et al. [1]\ndevelop an approach that can be used to reﬁne noisy an-\nnotations by jointly reasoning about the object boundaries\nwith a CNN and a level set formulation. In the domain of\nofﬂine mapping, several papers from Homayounfar et al .\nand Liang et al. [23, 35, 24, 36] have tackled the problem of\nautomatically annotating crosswalks, road boundaries and\nlanes by predicting structured outputs such as a polyline.\n3. PolyTransform\nOur aim is to design a robust segmentation model that is\ncapable of producing precise, geometry-preserving masks\nfor each individual object. Towards this goal, we develop\nPolyTransform, a novel deep architecture that combines\nprevailing segmentation approaches [21, 63] with modern\npolygon-based methods [5, 2]. By exploiting the best of\nboth worlds, we are able to generate high quality segmenta-\ntion masks under various challenging scenarios.\nIn this section, we start by describing the backbone ar-\nchitecture for feature extraction and polygon initialization.\nNext, we present a novel deforming network that warps the\ninitial polygon to better capture the local geometry of the\nobject. An overview of our approach is shown in Figure 1.\n3.1. Instance Initialization\nThe goal of our instance initialization module is to pro-\nvide a good polygon initialization for each individual ob-\nject. To this end, we ﬁrst exploit a model to generate a mask\nfor each instance in the scene. Our experiments show that\nour approach can signiﬁcantly improve performance for a\nwide variety of segmentation models. If the segmentation\nmodel outputs proposal boxes, we use them to crop the im-\nage, otherwise, we ﬁt a bounding box to the mask. The\ncropped image is later resized to a square and fed into a\nfeature network (described in Sec. 3.2) to obtain a set of\nreliable deep features. In practice, we resize the cropped\nimage to (Hc, Wc) = (512, 512). To initialize the polygon,\nwe use the border following algorithm of [60] to extract the\ncontours from the predicted mask. We get the initial set\nof vertices by placing a vertex at every 10 px distance in\nthe contour. Empirically, we ﬁnd such dense vertex inter-\npolation provides a good balance between performance and\nmemory consumption.\n3.2. Feature Extraction Network\nThe goal of our feature extraction network is to learn\nstrong object boundary features. This is essential as we\nwant our polygons to capture high curvature and complex\nshapes. As such, we employ a feature pyramid network\n(FPN) [37] to learn and make use of multi-scale features.\nThis network takes as input the (Hc, Wc) crop obtained\nfrom the instance initialization stage and outputs a set of\nfeatures at different pyramid levels. Our backbone can be\nseen in Figure 2.\n3.3. Deforming Network\nWe have computed a polygon initialization and deep fea-\ntures of the FPN from the image crop. Next we build a\nfeature embedding for all N vertices and learn a deforming\nmodel that can effectively predict the offset for each vertex\nso that the polygon snaps better to the object boundaries.\nVertex embedding: We build our vertex representation\nupon the multi-scale feature extracted from the backbone\nFPN network of the previous section. In particular, we take\nthe P2, P3, P4, P5 and P6 feature maps and apply two lat-\neral convolutional layers to each of them in order to reduce\nthe number of feature channels from 256 to 64 each. Since\nthe feature maps are 1/4, 1/8, 1/16, 1/32 and 1/64 of\nthe original scale, we bilinearly upsample them back to the\ntraining data APval AP AP 50 person rider car truck bus train mcycle bcycle\nDWT [4] fine 21.2 19.4 35 .3 15.5 14 .1 31 .5 22 .5 27 .0 22 .9 13 .9 8 .0\nKendall et al. [28] fine − 21.6 39 .0 19.2 21 .4 36 .6 18 .8 26 .8 15 .9 19 .4 14 .5\nArnab et al. [3] fine − 23.4 45 .2 21.0 18 .4 31 .7 22 .8 31 .1 31 .0 19 .6 11 .7\nSGN [41] fine+coarse 29.2 25.0 44 .9 21.8 20 .1 39 .4 24 .8 33 .2 30 .8 17 .7 12 .4\nPolygonRNN++ [2] fine − 25.5 45 .5 29.4 21 .8 48 .3 21 .2 32 .3 23 .7 13 .6 13 .6\nMask R-CNN [21] fine 31.5 26.2 49 .9 30.5 23 .7 46 .9 22 .8 32 .2 18 .6 19 .1 16 .0\nBShapeNet+ [29] fine − 27.3 50 .4 29.7 23 .4 46 .7 26 .1 33 .3 24 .8 20 .3 14 .1\nGMIS [43] fine+coarse − 27.3 45 .6 31.5 25 .2 42 .3 21 .8 37 .2 28 .9 18 .8 12 .8\nNeven et al. [48] fine − 27.6 50 .9 34.5 26 .1 52 .4 21 .7 31 .2 16 .4 20 .1 18 .9\nPANet [42] fine 36.5 31.8 57 .1 36.8 30 .4 54 .8 27 .0 36 .3 25 .5 22 .6 20 .8\nMask R-CNN [21] fine+COCO 36.4 32.0 58 .1 34.8 27 .0 49 .1 30 .1 40 .9 30 .9 24 .1 18 .7\nAdaptIS [58] fine 36.3 32.5 52 .5 31.4 29.1 50.0 31.6 41.7 39.4 24.7 12.1\nSSAP [18] fine 37.3 32.7 51 .8 35.4 25 .5 55 .9 33 .2 43 .9 31 .9 19 .5 16 .2\nBShapeNet+ [29] fine+COCO − 32.9 58 .8 36.6 24 .8 50 .4 33 .7 41 .0 33 .7 25 .4 17 .8\nUPSNet [63] fine+COCO 37.8 33.0 59 .7 35.9 27 .4 51 .9 31 .8 43 .1 31 .4 23 .8 19 .1\nPANet [42] fine+COCO 41.4 36.4 63 .1 41.5 33 .6 58 .2 31 .8 45 .3 28 .7 28 .2 24.1\nOurs fine+COCO 44.6 40.1 65.9 42.4 34 .8 58.5 39.8 50.0 41.3 30 .9 23.4\nTable 1. Instance segmentation on Cityscapes val and test set: This table shows our instance segmentation results on Cityscape test. We\nreport models trained on fine and fine+COCO. We report AP and AP50.\nfine COCO AP AP 50 car truck bus train person rider bcycle+r bcycle mcycle+r mcycle\nMask RCNN [21] ✓ - 26.6 53 .5 47.0 41 .1 42 .8 10 .7 32 .8 27 .5 18 .6 10 .2 14 .8 20 .2\nPANet [42] ✓ - 26.6 53 .5 46.6 41 .8 44 .2 2 .7 32 .8 27 .4 18 .7 11 .3 15 .1 25 .8\nUPSNet [63] ✓ - 29.0 56 .0 47.1 41 .8 47 .8 12 .7 33 .5 27 .3 18 .6 10 .4 20 .4 30 .2\nPANet [42] ✓ ✓ 29.1 55 .2 47.4 43 .7 47 .6 10 .7 34 .4 30 .1 20 .5 11 .8 17 .3 27 .4\nUPSNet [63] ✓ ✓ 31.5 58 .4 46.9 44 .0 49 .8 21 .6 34 .1 30 .3 21 .7 12 .8 19 .3 34.5\nOurs ✓ ✓ 35.3 60.8 50.5 47.3 52.5 23.4 40 .4 37 .0 25 .1 16 .0 28 .7 32.6\nTable 2. Instance segmentation on test set of our new self-driving dataset: This table shows our instance segmentation results our new\ndataset’s test set. We report models trained onfine and fine+COCO. We report AP and AP50. +r is short for with rider.\noriginal size and concatenate them to form aHc ×Wc ×320\nfeature tensor. To provide the network a notion of where\neach vertex is, we further append a 2 channel CoordConv\nlayer [40]. The channels represent x and y coordinates with\nrespect to the frame of the crop. Finally, we exploit the bi-\nlinear interpolation operation of the spatial transformer net-\nwork [26] to sample features at the vertex coordinates of\nthe initial polygon from the feature tensor. We denote such\nN ×(320 + 2)embedding as zzz.\nDeforming network: When moving a vertex in a poly-\ngon, the two attached edges are subsequently moved as\nwell. The movement of these edges depends on the po-\nsition of the neighboring vertices. Each vertex thus must\nbe aware of its neighbors and needs a way to communicate\nwith one another in order to reduce unstable and overlap-\nping behavior. In this work, we exploit the self-attending\nTransformer network [61] to model such intricate depen-\ndencies. We leverage the attention mechanism to propagate\nthe information across vertices and improve the predicted\noffsets. More formally, given the vertex embeddings zzz, we\nﬁrst employ three feed-forward neural networks to trans-\nform it into Q(zzz), K(zzz), V (zzz), where Q, K, V stands for\nQuery, Key and Value. We then compute the weightings\nbetween vertices by taking a softmax over the dot product\nQ(zzz)K(zzz)T . Finally, the weightings are multiplied with the\nkeys V (zzz) to propagate these dependencies across all ver-\ntices. Such attention mechanism can be written as:\nAtten(Q(zzz), K(zzz), V(zzz)) = softmax(Q(zzz)K(zzz)T\n√dk\n)V (zzz),\nwhere dk is the dimension of the queries and keys, serving\nas a scaling factor to prevent extremely small gradients. We\nrepeat the same operation a ﬁxed number of times, 6 in our\nexperiments. After the last Transformer layer, we feed the\noutput to another feed-forward network which predictsN ×\n2 offsets for the vertices. We add the offsets to the polygon\ninitialization to transform the shape of the polygon.\n3.4. Learning\nWe train the deforming network and the feature extrac-\ntion network in an end-to-end manner. Speciﬁcally, we min-\nimize the weighted sum of two losses. The ﬁrst penalizes\nthe model for when the vertices deviate from the ground\ntruth. The second regularizes the edges of the polygon to\nprevent overlap and unstable movement of the vertices.\nInit Backbone COCO AP AP gain AF AF gain\nDWT Res101 - 18.7 +2 .2 44.2 +5 .8\nUPSNet Res50 - 33.3 +3 .0 41.4 +10 .3\nUPSNet Res50 ✓ 37.8 +2 .4 45.7 +7 .8\nUPSNet WRes38+PANet ✓ 41.4 +1 .6 51.1 +4 .9\nUPSNet WRes38+PANet+DCN ✓ 43.0 +1 .6 51.5 +4 .2\nTable 3.Improvement on Cityscapes val instance segmentation\ninitializations: We report the AP, AF of the initialization and gain\nin AP, AF from the initialization instances when running our Poly-\nTransform model for Cityscapes val.\nInit Backbone COCO AP AP gain AF AF gain\nM-RCNN Res50 - 28.8 +2 .2 44.2 +5 .6\nUPSNet Res101 - 31.7 +1 .6 45.7 +3 .2\nUPSNet Res101 ✓ 34.2 +1 .9 45.8 +3 .4\nUPSNet WRes38+PANet+DCN ✓ 36.1 +1 .4 50.1 +3 .4\nTable 4. Improvement over instance segmentation initializa-\ntions on the validation of our new self-driving dataset: We re-\nport the AP, AF of the initialization and gain in AP, AF from the\ninitialization instances when running our PolyTransform model for\nthe validation of our new self-driving dataset.\nPolygon Transforming Loss: We make use of the Cham-\nfer Distance loss similar to [23] to move the vertices of our\npredicted polygon P closer to the ground truth polygon Q.\nThe Chamfer Distance loss is deﬁned as:\nLc(P, Q) = 1\n|P|\n∑\ni\nmin\nq∈Q\n∥pi −q∥2+ 1\n|Q|\n∑\nj\nmin\np∈P\n∥p −qj∥2\nwhere p and q are the rasterized edge pixels of the polygons\nP and Q. To prevent unstable movement of the vertices, we\nadd a deviation loss on the lengths of the edges eee between\nvertices. Empirically, we found that without this term the\nvertices can suddenly shift a large distance, incurring a large\nloss and causing the gradients to blow up. We deﬁne the\nstandard deviation loss as: Ls(P) =\n√∑(eee−¯eee)2\nn , where ¯eee\ndenotes the mean length of the edges.\n4. Experiments\nWe evaluate our model in the context of both instance\nsegmentation and interactive annotation settings.\nExperimental Setup: We train our model on 8 Titan 1080\nTi GPUs using the distributed training framework Horovod\n[56] for 1 day. We use a batch size of 1, ADAM [30], 1e-4\nlearning rate and a 1e-4 weight decay. We augment our data\nby randomly ﬂipping the images horizontally. During train-\ning, we only train with instances whose proposed box has an\nIntersection over Union (IoU) overlap of over 0.5 with the\nground truth (GT) boxes. We train with both instances pro-\nduced using proposed boxes and GT boxes to further aug-\nment the data. For our instance segmentation experiments,\nwe augment the box sizes by −3% to +3% during train-\ning and test with a 2% box expansion. For our interactive\nannotation experiments, we train and test on boxes with an\nexpansion of 5px on each side; we only compute a chamfer\nloss if the predicted vertex is at least 2px from the ground\nCityscapes (fine+COCO) AP AF\nUPSNet 43.0 51 .5\nBaseline 1 43.8 52 .6\nBaseline 2 43.5 52 .4\nOurs 44.6 55.7\nTable 5. Comparison with naive reﬁners on Cityscapes val set.\ntruth polygon. When placing weights on the losses, we\nfound ensuring the loss values were approximately balanced\nproduced the best result. For our PolyTransform FPN, we\nuse ResNet50 [22] as the backbone and use the same pre-\ntrained weights from UPSNet [63] on Cityscapes. For our\ndeforming network we do not use pretrained weights.\n4.1. Instance Segmentation\nDatasets: We use Cityscapes [11] which has high quality\npixel-level instance segmentation annotations. The 1024 ×\n2048 images were collected in 27 cities, and they are split\ninto 2975, 500 and 1525 images for train/val/test. There are\n8 instance classes: bicycle, bus, person, train, truck, mo-\ntorcycle, car and rider. We also report results on a new\ndataset we collected. It consists of 10235/1139/1186 im-\nages for train/val/test split annotated with 10 classes: car,\ntruck, bus, train , person, rider, bicycle with rider, bicycle,\nmotorcycle with rider and motorcycle. Each image is of size\n1200 ×1920.\nMetrics: For our instance segmentation results, we report\nthe average precision (AP and AP50) for the predicted mask.\nHere, the AP is computed at 10 IoU overlap thresholds rang-\ning from 0.5 to 0.95 in steps of 0.05 following [11]. AP 50\nis the AP at an overlap of 50%. We also introduce a new\nmetric that focusses on boundaries. In particular, we use a\nmetric similar to [62, 50] where a precision, recall and F1\nscore is computed for each mask, where the prediction is\ncorrect if it is within a certain distance threshold from the\nground truth. We use a threshold of 1px, and only compute\nthe metric for true positives. We use the same 10 IoU over-\nlap thresholds ranging from 0.5 to 0.95 in steps of 0.05 to\ndetermine the true positives. Once we compute the F1 score\nfor all classes and thresholds, we take the average over all\nexamples to get AF.\nInstance Initialization: We want to use a strong instance\ninitialization to show that we can still improve the results.\nWe take the publicly available UPSNet [63], and replace its\nbackbone with WideResNet38 [55] and add all the elements\nof PANet [42] except for the synchronized batch normaliza-\ntion (we use group normalization instead). We then pretrain\non COCO and use deformable convolution (DCN) [15] in\nthe backbone.\nComparison to SOTA: As shown in 1 we outperform\nall baselines in every metric on the val and test sets of\nCityscapes. We achieve a new state-of-the-art test result\nInput Image Our Instance Segmentation GT Instance Segmentation\nFigure 3. We showcase qualitative instance segmentation results of our model on the Cityscapes validation set.\nof 40.1AP. This outperforms PANet by 3.7 and 2.8 points\nin AP and AP 50m respectively. It also ranks number 1 on\nthe ofﬁcial Cityscapes leaderboard. We report the results on\nour new dataset in Table 2. We achieve the strongest test\nAP result in this leaderboard. We see that we improve over\nPANet by 6.2 points in AP and UPSNet by 3.8 points in AP.\nRobustness to Initialization: We report the improvement\nover different instance segmentation networks used as ini-\ntialization in Table 3 on Cityscapes, showing signiﬁcant and\nconsistent improvements in val AP across all models. When\nwe train our model on top of the DWT [4] instances we see\nan improvement of +2.2, +5.8 points in AP and AF. We\nalso train on top of the UPSNet results from the original\npaper along with UPSNet with WRes38+PANet as a way\nto reproduce the current SOTA val AP of PANet. We show\nan improvement of +1.6, +4.9 points in AP and AF. Fi-\nnally we improve on our best initialization by +1.6, +4.2\nAP points in AP and AF. As we can see, our boundary met-\nric sees a very consistent 4% −10% gain in AF across all\nmodels. This suggests that our approach signiﬁcantly im-\nprovs the instances at the boundary. We notice that a large\ngain in AP ( WRes38+PANet to WRes38+PANet+DCN)\ndoes not necessarily translate to a large gain in AF, how-\never, our model will always provide a signiﬁcant increase in\nthis metric. We also report the validation AP improvement\nover different instance segmentation networks in Table 4 for\nour new dataset. We see that we can improve on Mask R-\nCNN [21] by +2.2, +5.6 points in AP, AF. For the differ-\nent UPSNet models, we improve upon it between 1.4-2.2\nAP points. Once again, our model shows a consistent and\nstrong improvement over all initializations. We also see a\nvery consistent 3% −6% gain in AF across all the models.\nAnnotation Efﬁciency: We conduct an experiment where\nwe ask crowd-sourced labelers to annotate 150 images from\nour new dataset with instances larger than 24x24px for ve-\nhicles and 12x14px for pedestrians/riders. We performed a\ncontrol experiment where the instances are annotated com-\npletely from scratch (without our method) and a parallel ex-\nperiment where we use our model to output the instances\nfor them to ﬁx to produce the ﬁnal annotations. In the\nfully manual experiment, it took on average 60.3 minutes\nto annotate each image. When the annotators were given\nthe PolyTransform output to annotate on top of, it took on\naverage 39.4 minutes to annotate each image. Thus reduc-\ning 35% of the time required to annotate the images. This\nresulted in signiﬁcant cost savings.\nInput Image\nOur Instance Seg\nGT Instance Seg\nFigure 4. We showcase the qualitative instance segmentation results of our model on the validation set of our new self-driving dataset\nMean bicycle bus person train truck mcycle car rider F1px F2px\nDEXTR* [46] 79.11 71.92 87 .42 78 .36 78 .11 84 .88 72 .41 84 .62 75 .18 54.00 68 .60\nDeep Level Sets [62] 80.86 74.32 88.85 80.14 80.35 86.05 74 .10 86 .35 76 .74 60.29 74 .40\nOurs 80.90 74.22 88 .78 80.73 77.91 86.45 74.42 86.82 77.85 62.33 76.55\nTable 6. Interactive Annotation (Cityscapes Stretch): This table shows our IoU % performance in the setting of annotation where we\nare given the ground truth boxes. DEXTR* represents DEXTR without extreme points.\nMean bicycle bus person train truck mcycle car rider F1px F2px\nPolygon-RNN [5] 61.40 52.13 69 .53 63 .94 53 .74 68 .03 52 .07 71 .17 60 .58 − −\nPolygon-RNN++ [2] 71.38 63.06 81 .38 72 .41 64 .28 78 .90 62 .01 79 .08 69 .95 46.57 62 .26\nCurve GCN [39] 73.70 67.36 85 .43 73 .72 64 .40 80 .22 64 .86 81 .88 71 .73 47.72 63 .64\nDeep Level Sets [62] 73.84 67.15 83 .38 73 .07 69 .10 80 .74 65 .29 81 .08 70 .86 48.59 64 .45\nOurs 78.76 72.97 87.53 78.58 72.25 85.08 72.50 85.36 75.83 56.89 71.60\nTable 7. Interactive Annotation (Cityscapes Hard): This table shows our IoU % performance in the setting of annotation where we are\ngiven the ground truth boxes.\nNaive reﬁner: We implemented two baselines that apply\na semantic segmentation network on top of the initial mask.\n1) We replace PolyTransform with a reﬁnement network in-\nspired by DeepLabV3 [7] and PWC-Net [59] . It takes as\ninput the same initialization mask, the cropped RGB image\nand the cropped feature, and exploits a series of convolu-\ntions to reﬁne the binary mask. 2) We add an extra head to\nUPSNet, with the initialization mask and the cropped fea-\nture as input to reﬁne the binary mask. The head’s archi-\ntecture is similar to that of the semantic head (i.e., uses the\nfeatures from UPSNet’s FPN). For fairness, the number of\nparameters of both baselines are similar to PolyTransform.\nAs shown in Tab. 5, our approach performs the best.\nTiming: Our model takes 575 ms to process each image\non Cityscapes. This can easily be improved with more GPU\nmemory, as this will allow to batch all the instances. Fur-\nthermore, the hidden dimension of the FPN can be tuned to\nspeed up the model.\nQualitative Results: We show qualitative results of our\nmodel on the validation set in Figure 3. In our instance\nsegmentation outputs we see that in many cases our model\nis able to handle occlusion. For example, in row 3, we see\nthat our model is able to capture the feet of the purple and\nblue pedestrians despite their feet being occluded from the\nbody. We also show qualitative results on our new dataset\nin Figure 4. We see that our model is able to capture precise\nboundaries, allowing it to capture difﬁcult shapes such as\ncar mirrors and pedestrians.\nFailure Modes: Our model can fail when the initializa-\ntion is poor (left image in Figure 5). Despite being able to\nhandle occlusion, our model can still fail when the occlu-\nsion is complex or ambiguous as seen in the right of Figure\n5. Here there is a semi-transparent fence blocking the car.\nBBone COCO mIOU mIOUgain F1 F1,gain F2 F2,gain\nFCN R50 - 79.93 +0 .15 59.43 +1.53 73.64 +1.30\nFCN R101 - 80.94 +0 .11 60.64 +1.14 74.78 +1.06\nFCN R101 ✓ 80.65 +0 .08 59.21 +1.39 73.47 +1.10\nDeepLabV3 R50 - 80.41 +0 .17 59.70 +1.51 73.81 +1.48\nDeepLabV3 R101 - 80.93 +0 .09 60.50 +1.18 74.44 +1.33\nDeepLabV3+ R101 ✓ 80.90 +0 .08 61.10 +1.23 75.25 +1.30\nTable 8. Improvement on Cityscapes Stretch segmentation\ninitializations: We report the metric improvements when run-\nning our PolyTransform model on different models. We re-\nport our model results trained on FCN [57] and DeepLabV3 [7].\nDeepLabV3+ uses the class balancing loss from [46]. We report\non models with various backbones (Res50 vs Res101) and also\nwith and without pretraining on COCO [38].\n4.2. Interactive Annotation\nThe goal is to annotate an object with a polygon given its\nground truth bounding box. The idea is that the annotator\nprovides a ground truth box and our model works on top of\nit to output a polygon representation of the object instance.\nDataset: We follow [5] and split the Cityscapes dataset\nsuch that the original val set is the test set and two cities\nfrom the training (Weimar and Zurich) form the val set. [62]\nfurther splits this dataset into two settings: 1) Cityscapes\nHard, where the ground truth bounding box is enlarged to\nform a square and then the image is cropped. 2) Cityscapes\nStretch, where the ground truth bounding box along with\nthe image is stretched to a square and then cropped.\nMetric: To evaluate our model for this task, we report the\nintersection over union (IoU) on a per-instance basis and\naverage for each class. Then, following [5] this is averaged\nacross all classes. We also report the boundary metric re-\nported in [62, 50], which computes the F measure along the\ncontour for a given threshold. The thresholds used are 1 and\n2 pixels as Cityscapes contains a lot of small instances.\nInstance Initialization: For our best model we use a vari-\nation of DeepLabV3 [7], which we call DeepLabV3+ as the\ninstance initialization network. The difference is that we\ntrain DeepLabV3 with the class balancing loss used in [46].\nComparison to SOTA: Tables 6 and 7 show results on the\ntest set in both Cityscapes Stretch and Hard. For Cityscapes\nStretch, we see that our model signiﬁcantly outperforms the\nSOTA in the boundary metric, improving it by up to 2%.\nUnlike the Deep Level Sets [62] method which outputs a\npixel wise mask, our method outputs a polygon which al-\nlows for it to be amenable to modiﬁcation by an annotator\nby simply moving the vertices. For Cityscapes Hard, our\nmodel outperforms the SOTA by 4.9%, 8.3% and 7.2% in\nmean IOU, F at 1px and F at 2px respectively.\nFigure 5. Failure modes: (Left) Our model fails because the ini-\ntialization is poor. (Right) The model fails because of complex\nocclusion. (Yellow: Initialization; Cyan: Ours)\nRobustness to Initalization: We also report improve-\nments over different segmentation initializations in Table\n8, the results are on the test set. Our models are trained\non various backbone initialization models (FCN [57] and\nDeepLabV3 [7] with and without pretraining on COCO\n[38]). Our model is able to consistently and signiﬁcantly\nimprove the boundary metrics at 1 and 2 pixels by up to\n1.5% and we improve the IOU between 0.1-0.2%. We\nalso note that the difference in mean IOU between FCN\nand DeepLabV3 is very small (at most 0.5%) despite\nDeepLabV3 being a much stronger segmentation model.\nWe argue that the margin for mean IOU improvement is\nvery small for this dataset.\nTiming: Our model runs on average 21 ms per object in-\nstance. This is 14x faster than Polygon-RNN++ [2] and 1.4x\nfaster than Curve GCN [39] which are the state of the arts.\n5. Conclusion\nIn this paper, we present PolyTransform, a novel deep\narchitecture that combines the strengths of both prevailing\nsegmentation approaches and modern polygon-based meth-\nods. We ﬁrst exploit a segmentation network to generate a\nmask for each individual object. The instance mask is then\nconverted into a set of polygons and serve as our initializa-\ntion. Finally, a deforming network is applied to warp the\npolygons to better ﬁt the object boundaries. We evaluate\nthe effectiveness of our model on the Cityscapes dataset as\nwell as a novel dataset that we collected. Experiments show\nthat our approach is able to produce precise, geometry-\npreserving instance segmentation that signiﬁcantly outper-\nforms the backbone model. Comparing to the instance seg-\nmentation initialization, we increase the validation AP and\nboundary metric by up to 3.0 and 10.3 points, allowing us\nto achieve 1st place on the Cityscapes leaderboard. We also\nshow that our model speeds up annotation by 35%. Com-\nparing to previous work on annotation-in-the-loop [2], we\noutperform the boundary metric by 2.0%. Importantly, our\nPolyTransform generalizes across various instance segmen-\ntation network.\nReferences\n[1] David Acuna, Amlan Kar, and Sanja Fidler. Devil is in\nthe edges: Learning semantic boundaries from noisy annota-\ntions. In CVPR, 2019.\n[2] David Acuna, Huan Ling, Amlan Kar, and Sanja Fidler. Ef-\nﬁcient interactive annotation of segmentation datasets with\npolygon-rnn++. 2018.\n[3] Anurag Arnab and Philip H. S. Torr. Pixelwise instance\nsegmentation with a dynamically instantiated network. In\nCVPR, 2017.\n[4] Min Bai and Raquel Urtasun. Deep watershed transform for\ninstance segmentation. In CVPR, 2017.\n[5] Lluıs Castrej ´on, Kaustav Kundu, Raquel Urtasun, and Sanja\nFidler. Annotating object instances with a polygon-rnn. In\nCVPR, 2017.\n[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao\nLi, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,\nWanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid\ntask cascade for instance segmentation. In CVPR, 2019.\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. CoRR, 2017.\n[8] Liang-Chieh Chen, Alexander Hermans, George Papan-\ndreou, Florian Schroff, Peng Wang, and Hartwig Adam.\nMasklab: Instance segmentation by reﬁning object detection\nwith semantic and direction features. In CVPR, 2018.\n[9] Dominic Cheng, Renjie Liao, Sanja Fidler, and Raquel Ur-\ntasun. DARNet: Deep active ray network for building seg-\nmentation. In CVPR, 2019.\n[10] Laurent D Cohen. On active contour models and balloons.\nCVGIP, 1991.\n[11] Marius Cordts, Mohamed Omran Sebastian Ramos, Markus\nEnzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,\nBernt Schiele, Daimler Ag R, Tu Darmstadt, Mpi Informat-\nics, and Tu Dresden. The cityscapes dataset.\n[12] Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun.\nInstance-sensitive fully convolutional networks. In ECCV,\n2016.\n[13] Jifeng Dai, Kaiming He, and Jian Sun. Convolutional feature\nmasking for joint object and stuff segmentation. In CVPR,\n2015.\n[14] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-\nmantic segmentation via multi-task network cascades. In\nCVPR, 2016.\n[15] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. ICCV, 2017.\n[16] Zihao Dong, Ruixun Zhang, and Xiuli Shao. Automatic an-\nnotation and segmentation of object instances with deep ac-\ntive curve network. IEEE Access, 2019.\n[17] Nima Fazeli, Miquel Oller, Jiajun Wu, Zheng Wu, Joshua B.\nTenenbaum, and Alberto Rodriguez. See, feel, act: Hierar-\nchical learning for complex manipulation skills with multi-\nsensory fusion. Science Robotics, 2019.\n[18] Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu,\nMing Yang, and Kaiqi Huang. Ssap: Single-shot instance\nsegmentation with afﬁnity pyramid. In ICCV, 2019.\n[19] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nECCV, 2014.\n[20] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Hypercolumns for object segmentation and\nﬁne-grained localization. In CVPR, 2015.\n[21] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\nGirshick. Mask R-CNN. CoRR, 2017.\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. CoRR, 2015.\n[23] Namdar Homayounfar, Wei-Chiu Ma, Shrinidhi Kowshika\nLakshmikanth, and Raquel Urtasun. Hierarchical recurrent\nattention networks for structured online maps. In CVPR,\n2018.\n[24] Namdar Homayounfar, Wei-Chiu Ma, Justin Liang, Xinyu\nWu, Jack Fan, and Raquel Urtasun. Dagmapper: Learning to\nmap by discovering lane topology. In ICCV, 2019.\n[25] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang\nHuang, and Xinggang Wang. Mask Scoring R-CNN. In\nCVPR, 2019.\n[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In NIPS, 2015.\n[27] Michael Kass, Andrew Witkin, and Demetri Terzopoulos.\nSnakes: Active contour models. IJCV, 1988.\n[28] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\nlearning using uncertainty to weigh losses for scene geome-\ntry and semantics. In CVPR, 2018.\n[29] Ha Young Kim and Ba Rom Kang. Instance segmentation\nand object detection with bounding shape masks. CoRR,\n2018.\n[30] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, 2014.\n[31] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Dollar. Panoptic segmentation. In CVPR,\n2019.\n[32] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bog-\ndan Savchynskyy, and Carsten Rother. Instancecut: from\nedges to instances with multicut. In CVPR, 2017.\n[33] Philipp Kr ¨ahenb¨uhl and Vladlen Koltun. Efﬁcient inference\nin fully connected crfs with gaussian edge potentials. In\nNIPS, 2011.\n[34] Weicheng Kuo, Anelia Angelova, Jitendra Malik, and\nTsung-Yi Lin. Shapemask: Learning to segment novel ob-\njects by reﬁning shape priors. In ICCV, 2019.\n[35] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Shen-\nlong Wang, and Raquel Urtasun. Convolutional recurrent\nnetwork for road boundary extraction. In CVPR, 2019.\n[36] Justin Liang and Raquel Urtasun. End-to-end deep structured\nmodels for drawing crosswalks. In ECCV, 2018.\n[37] Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. CoRR, 2016.\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014.\n[39] Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, and Sanja\nFidler. Fast interactive object annotation with curve-gcn. In\nCVPR, 2019.\n[40] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski\nSuch, Eric Frank, Alex Sergeev, and Jason Yosinski. An\nintriguing failing of convolutional neural networks and the\ncoordconv solution. CoRR, 2018.\n[41] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn:\nSequential grouping networks for instance segmentation. In\nICCV, 2017.\n[42] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\nPath aggregation network for instance segmentation. In\nCVPR, 2018.\n[43] Yiding Liu, Siyu Yang, Bin Li, Wengang Zhou, Jizheng Xu,\nHouqiang Li, and Yan Lu. Afﬁnity derivation and graph\nmerge for instance segmentation. In ECCV, 2018.\n[44] Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and\nRaquel Urtasun. Deep rigid instance scene ﬂow. In CVPR,\n2019.\n[45] Soumajit Majumder and Angela Yao. Content-aware multi-\nlevel guidance for interactive instance segmentation. In\nCVPR, 2019.\n[46] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and\nLuc Van Gool. Deep extreme cut: From extreme points to\nobject segmentation. CVPR, 2017.\n[47] Diego Marcos, Devis Tuia, Benjamin Kellenberger, Lisa\nZhang, Min Bai, Renjie Liao, and Raquel Urtasun. Learn-\ning deep structured active contours end-to-end. In CVPR,\n2018.\n[48] Davy Neven, Bert De Brabandere, Marc Proesmans, and\nLuc Van Gool. Instance segmentation by jointly optimiz-\ning spatial embeddings and clustering bandwidth. In CVPR,\n2019.\n[49] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-\ntive embedding: End-to-end learning for joint detection and\ngrouping. In NIPS, 2017.\n[50] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams,\nLuc Van Gool, Markus H. Gross, and Alexander Sorkine-\nHornung. A benchmark dataset and evaluation methodology\nfor video object segmentation. CVPR, 2016.\n[51] Pedro O Pinheiro, Ronan Collobert, and Piotr Doll ´ar. Learn-\ning to segment object candidates. In NIPS, 2015.\n[52] Pedro O Pinheiro, Tsung-Yi Lin, Ronan Collobert, and Piotr\nDoll´ar. Learning to reﬁne object segments. In ECCV, 2016.\n[53] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Fer-\nran Marques, and Jitendra Malik. Multiscale combinatorial\ngrouping for image segmentation and object proposal gener-\nation. PAMI, 2017.\n[54] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NIPS, 2015.\n[55] Samuel Rota Bul `o, Lorenzo Porzi, and Peter Kontschieder.\nIn-place activated batchnorm for memory-optimized training\nof dnns. In CVPR, 2018.\n[56] Alexander Sergeev and Mike Del Balso. Horovod: fast and\neasy distributed deep learning in tensorﬂow. CoRR, 2018.\n[57] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. IEEE\nTrans. Pattern Anal. Mach. Intell., 2017.\n[58] Konstantin Soﬁiuk, Olga Barinova, and Anton Konushin.\nAdaptis: Adaptive instance selection network. In ICCV,\n2019.\n[59] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPWC-Net: CNNs for optical ﬂow using pyramid, warping,\nand cost volume. 2018.\n[60] Satoshi Suzuki and Keiichi Abe. Topological structural anal-\nysis of digitized binary images by border following. Com-\nputer Vision, Graphics, and Image Processing, 1985.\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. CoRR, 2017.\n[62] Zian Wang, Huan Ling, David Acuna, Amlan Kar, and Sanja\nFidler. Object instance annotation with deep extreme level\nset evolution. In CVPR, 2019.\n[63] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min\nBai, Ersin Yumer, and Raquel Urtasun. Upsnet: A uniﬁed\npanoptic segmentation network. CoRR, 2019.\n[64] Wenqiang Xu, Haiyang Wang, Fubo Qi, and Cewu Lu. Ex-\nplicit shape encoding for real-time instance segmentation. In\nICCV, 2019.\n[65] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-\nmentation. In ICCV, 2019.\n[66] Ziyu Zhang, Sanja Fidler, and Raquel Urtasun. Instance-\nlevel segmentation for autonomous driving with deep\ndensely connected mrfs. In CVPR, 2016.\n[67] Ziyu Zhang, Alexander G Schwing, Sanja Fidler, and Raquel\nUrtasun. Monocular object instance segmentation and depth\nordering with cnns. In ICCV, 2015.\n[68] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In CVPR, 2017.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.844735860824585
    },
    {
      "name": "Polygon (computer graphics)",
      "score": 0.7828079462051392
    },
    {
      "name": "Computer science",
      "score": 0.7030137181282043
    },
    {
      "name": "Exploit",
      "score": 0.6647086143493652
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5888912081718445
    },
    {
      "name": "Image segmentation",
      "score": 0.4692297875881195
    },
    {
      "name": "Code (set theory)",
      "score": 0.4493286907672882
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44369199872016907
    },
    {
      "name": "Computer vision",
      "score": 0.4408620595932007
    },
    {
      "name": "Source code",
      "score": 0.44055694341659546
    },
    {
      "name": "Annotation",
      "score": 0.43294796347618103
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34764569997787476
    },
    {
      "name": "Programming language",
      "score": 0.07503268122673035
    },
    {
      "name": "Computer network",
      "score": 0.06531843543052673
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Frame (networking)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210123843",
      "name": "Advanced Technologies Group (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I185261750",
      "name": "University of Toronto",
      "country": "CA"
    }
  ]
}