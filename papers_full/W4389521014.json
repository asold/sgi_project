{
  "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
  "url": "https://openalex.org/W4389521014",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2340166791",
      "name": "Max van Duijn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167101309",
      "name": "Bram van Dijk",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A4286840948",
      "name": "Tom Kouwenhoven",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A5114107443",
      "name": "Werner de Valk",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    },
    {
      "id": "https://openalex.org/A2023187090",
      "name": "Marco Spruit",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science",
        "Leiden University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A5093457350",
      "name": "Peter vanderPutten",
      "affiliations": [
        "Czech Academy of Sciences, Institute of Computer Science"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969716438",
    "https://openalex.org/W4379933116",
    "https://openalex.org/W2106000135",
    "https://openalex.org/W2564010453",
    "https://openalex.org/W2093410327",
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4230100653",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W1576350636",
    "https://openalex.org/W2110975890",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2571600439",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W4389519950",
    "https://openalex.org/W4367189299",
    "https://openalex.org/W1974955132",
    "https://openalex.org/W2108248740",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W4389519164",
    "https://openalex.org/W2171847059",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2496437279",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W4320864241",
    "https://openalex.org/W2787640378",
    "https://openalex.org/W3000621992",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4378499145",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W2592312659",
    "https://openalex.org/W2595754429",
    "https://openalex.org/W4223947928",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2116139040",
    "https://openalex.org/W1983467315",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4323653879",
    "https://openalex.org/W2139188400",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4377121468",
    "https://openalex.org/W4378474033",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W1963720370",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4234782804",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4378908072",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W2001771035",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W1993433554",
    "https://openalex.org/W2264742718",
    "https://openalex.org/W2503002562",
    "https://openalex.org/W2889107415",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2141538250",
    "https://openalex.org/W2625558886",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 389–402\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n389\nTheory of Mind in Large Language Models: Examining Performance of 11\nState-of-the-Art models vs. Children Aged 7-10 on Advanced Tests\nMax van Duijn1*, Bram van Dijk1*, Tom Kouwenhoven1*,\nWerner de Valk1, Marco Spruit1,2, and Peter van der Putten1\n1Leiden Institute of Advanced Computer Science\n2Leiden University Medical Centre\nCorresponding author: m.j.van.duijn@liacs.leidenuniv.nl\nAbstract\nTo what degree should we ascribe cognitive\ncapacities to Large Language Models (LLMs),\nsuch as the ability to reason about intentions\nand beliefs known as Theory of Mind (ToM)?\nHere we add to this emerging debate by (i)\ntesting 11 base- and instruction-tuned LLMs\non capabilities relevant to ToM beyond the\ndominant false-belief paradigm, including non-\nliteral language usage and recursive intentional-\nity; (ii) using newly rewritten versions of stan-\ndardized tests to gauge LLMs’ robustness; (iii)\nprompting and scoring for open besides closed\nquestions; and (iv) benchmarking LLM per-\nformance against that of children aged 7-10\non the same tasks. We find that instruction-\ntuned LLMs from the GPT family outperform\nother models, and often also children. Base-\nLLMs are mostly unable to solve ToM tasks,\neven with specialized prompting. We suggest\nthat the interlinked evolution and development\nof language and ToM may help explain what\ninstruction-tuning adds: rewarding cooperative\ncommunication that takes into account inter-\nlocutor and context. We conclude by arguing\nfor a nuanced perspective on ToM in LLMs.\n1 Introduction\nMachines that can think like us have always trig-\ngered our imagination. Contemplation of such ma-\nchines can be traced as far back as antiquity (Live-\nley and Thomas, 2020), and peaked with the advent\nof all kinds of ‘automata’ in the early days of the In-\ndustrial Revolution (V oskuhl, 2013) before settling\nin computer science from the 1950s (Turing, 1950).\nCurrently people around the world can interact with\npowerful chatbots driven by Large Language Mod-\nels (LLMs), such as OpenAI’s ChatGPT (OpenAI,\n2023), and wonder to what degree such systems\nare capable of thought.\nLLMs are large-scale deep neural networks,\ntrained on massive amounts of text from the web.\n*Equal contribution.\nThey are vastly complex systems: even if all de-\ntails about their architecture, training data, and op-\ntional fine-tuning procedures are known (which\nis currently not the case for the most competitive\nmodels), it is very difficult to oversee their capa-\nbilities and predict how they will perform on a\nvariety of tasks. Researchers from linguistics (Man-\nning et al., 2020), psychology (Binz and Schulz,\n2023b; Kosinski, 2023; Webb et al., 2023), psychi-\natry (Kjell et al., 2023), epistemology (Sileo and\nLernould, 2023), logic (Creswell et al., 2022), and\nother fields, have therefore started to study LLMs\nas new, ‘alien’ entities, with their own sort of intel-\nligence, that needs to be probed with experiments,\nan endeavour recently described as ‘machine psy-\nchology’ (Hagendorff, 2023). This not only yields\nknowledge about what LLMs are capable of, but\nalso provides a unique opportunity to shed new\nlight on questions surrounding our own intelligence\n(Dillion et al., 2023; Binz and Schulz, 2023a).\nHere we focus on attempts to determine to what\ndegree LLMs demonstrate a capacity for Theory of\nMind (ToM), defined as the ability to work with be-\nliefs, intentions, desires, and other mental states, to\nanticipate and explain behaviour in social settings\n(Apperly, 2010). We first address the question how\nLLMs perform on standardized, language-based\ntasks used to assess ToM capabilities in humans.\nWe extend existing work in this area, surveyed in\nSection 2, in four ways: by (i) testing 11 mod-\nels (see Table 1) for a broader suite of capabilities\nrelevant to ToM beyond just the dominant false-\nbelief paradigm, including non-literal language un-\nderstanding and recursive intentionality (A wants\nB to believe that C intends...); (ii) using newly\nwritten versions of standardized tests with vary-\ning degrees of deviation from the originals; (iii)\nincluding open questions besides closed ones; and\n(iv) benchmarking LLM performance against that\nof children aged 7-8 (n=37) and 9-10 (n=36) on\nthe same tasks. Section 3 contains details of our\n390\ntest procedures for both children and LLMs. After\nreporting the results in Section 4, we turn to the\nquestion how variation in performance of the\nLLMs we tested can be explained in Section 5.\nWe conclude by placing our findings in the broader\ncontext of strong links between language and ToM\nin human development and evolution, and tenta-\ntively interpret what it means for an LLM to pass\n(or fail) ToM tests.\nWe are aware of issues regarding LLM train-\ning and deployment, for example regarding the\nbiases they inherit (Lucy and Bamman, 2021; Ben-\nder et al., 2021), problems for educators (Sparrow,\n2022), and ethical concerns in obtaining human\nfeedback (Perrigo, 2023). Ongoing reflection on\nthe use of LLMs is necessary, but outside the scope\nof this paper.\n2 Background\n2.1 Large Language Models\nThe field of Natural Language Processing (NLP)\nhas been revolutionized by the advent of Trans-\nformer models (Vaswani et al., 2017; Devlin et al.,\n2019), deep neural networks that can induce lan-\nguage structures through self-supervised learning.\nDuring training, such models iteratively predict\nmasked words from context in large sets of nat-\nural language data. They improve at this task\nby building representations of the many morpho-\nlogical, lexical, and syntactic rules governing hu-\nman language production and understanding (Man-\nning et al., 2020; Rogers et al., 2021; Grand et al.,\n2022). Models exclusively trained through such\nself-supervision constitute what we refer to as\n‘base-LLMs’ in this paper.\nBase-LLMs can generate natural language when\nprompted with completion queries (‘A mouse is\nan ...’). They can also be leveraged successfully\nfor an array of other challenges, such as question-\nanswering and translation, which often requires\ntask-specific fine-tuning or prompting with spe-\ncific examples, known as few-shot-learning (Brown\net al., 2020). This makes them different from\na new generation of LLMs that we refer to as\n‘instruct-LLMs’ in this paper, and to which the\ncurrently most competitive models belong. In\ninstruction-tuning, various forms of human feed-\nback are collected, such as ranking most suitable\nresponses, which then forms the reward-signal\nfor further aligning these models to human pref-\nerences through reinforcement learning (Ouyang\net al., 2022). The resulting LLMs can be prompted\nwith natural language in the form of instructions to\nperform a wide variety of tasks directly, amounting\nto zero-shot learning (Wei et al., 2022).\nA key realization is thus that LLMs are given\neither no explicitly labelled data at all, or, in the\ncase of instruct-LLMs, data with human labels per-\ntaining to relatively general aspects of communica-\ntive interaction. As such they are part of a com-\npletely different paradigm than earlier language\nmodels that were trained on, for example, data\nsets of human-annotated language structures (e.g.\nNivre et al., 2016). This means that when LLMs\nare capable of such tasks as solving co-reference\nrelationships or identifying word classes (Manning\net al., 2020), this arises as an emergent property\nof the model’s architecture and training on differ-\nent objectives. Given that such emergent linguistic\ncapabilities have been observed (Reif et al., 2019;\nGrand et al., 2022), it is a legitimate empirical\nquestion which other capacities LLMs may have\nacquired as ‘by-catch’.\n2.2 Theory of Mind in Humans and LLMs\nToM, also known as ‘mindreading’, is classically\ndefined as the capacity to attribute mental states to\nothers (and oneself), in order to explain and antici-\npate behaviour. The concept goes back to research\nin ethology in which Premack and Woodruff (1978)\nfamously studied chimpanzees’ abilities to antici-\npate behaviour of caretakers. When focus shifted to\nToM in humans, tests were developed that present a\nscenario in which a character behaves according to\nits false beliefsabout a situation, and not according\nto the reality of the situation itself––which a suc-\ncessful participant, having the benefit of spectator-\nsight, can work out (see Section 3.1).\nInitial consensus that children could pass ver-\nsions of this test from the age of 4 was followed by\nscepticism about additional abilities it presumed,\nincluding language skills and executive function-\ning, which led to the development of simplified\nfalse-belief tests based on eye-gaze that even 15\nmonth-olds were found to ‘pass’ (Onishi and Bail-\nlargeon, 2005). While this line of research also\nmet important criticism (for a review see Barone\net al., 2019), it highlights two key distinctions in\ndebate from the past decades: implicit-behavioural\nversus explicit-representational and innate versus\nlearned components of ToM. Some researchers see\nresults from eye-gaze paradigms as evidence for a\n391\nnative or very early developing capacity for belief-\nattribution in humans (Carruthers, 2013) and hold\nthat performance on more complex tests is initially\n‘masked’ by a lack of expressive skills (cf. also\nFodor, 1992). Others have attempted to explain eye-\ngaze results in terms of lower-level cognitive mech-\nanisms (Heyes, 2014) and argued that the capac-\nity for belief-attribution itself develops gradually\nin interaction with more general social, linguistic,\nand narrative competencies (Heyes and Frith, 2014;\nMilligan et al., 2007; Hutto, 2008). Two-systems\napproaches (Apperly, 2010) essentially reconcile\nboth sides by positing that our mindreading ca-\npacity encompasses both a basic, fast, and early\ndeveloping component and a more advanced and\nflexible component that develops later.\nIn computational cognitive research, a variety\nof approaches to modelling ToM have been pro-\nposed (e.g. Baker and Saxe, 2011; Arslan et al.,\n2017). More recently neural agents (Rabinowitz\net al., 2018) have been implemented, along with\nan increasing number of deep-learning paradigms\naimed at testing first- and second-order ToM via\nquestion-answering. Initially this was done with\nrecurrent memory networks (Grant et al., 2017;\nNematzadeh et al., 2018) using data sets of clas-\nsic false-belief tests from psychology, but after is-\nsues surfaced with simple heuristics for solving\nsuch tasks, scenarios were made more varied and\nchallenging (Le et al., 2019). From the inception\nof BERT as one of the first LLMs (Devlin et al.,\n2019), we have seen roughly two approaches for\ntesting ToM in LLMs: many different ToM sce-\nnarios integrated in large benchmark suites (e.g.\nSap et al., 2022; Srivastava et al., 2023; Sileo and\nLernould, 2023; Ma et al., 2023; Shapira et al.,\n2023), and studies that modified standardized ToM\ntests as used in developmental and clinical research\nfor prompting LLMs (e.g. Kosinski, 2023; Ullman,\n2023; Bubeck et al., 2023; Brunet-Gouet et al.,\n2023; Chowdhery et al., 2022; Moghaddam and\nHoney, 2023; Marchetti et al., 2023). This paper\nadds to the latter tradition in four respects, as listed\nin the introduction.\n3 Methodology\nHere we describe our tasks and procedures for test-\ning LLMs and children; all code, materials, and\ndata are on OSF: https://shorturl.at/FQR34.\n3.1 ToM Tests\nSally-Anne test, first-order (SA1) –– The Sally-\nAnne test (Wimmer and Perner, 1983; Baron-\nCohen et al., 1985) is a classic first-order false\nbelief test. It relies on a narrative in which Sally\nand Anne stand behind a table with a box and a bas-\nket on it. When Anne is still present, Sally puts a\nball in her box. When Sally leaves, Anne retrieves\nthe ball from the box and puts it in her own basket.\nThe story ends when Sally returns and the partic-\nipant is asked the experimental question ‘Where\nwill Sally look for the ball?’ The correct answer is\nthat she will look in her box. We followed up by\nasking a motivation question, ‘Why?’, to prompt an\nexplanation to the effect of ‘she (falsely) believes\nthe object is where she left it’.\nSally-Anne test, second-order (SA2) –– While\nSA1 targets the participant’s judgement of what a\ncharacter believes about the location of an unex-\npectedly displaced object, in SA2 the participant\nneeds to judge what a character believes that an-\nother character believesabout the location of an\nice-cream truck (Perner and Wimmer, 1985). Sally\nand Anne are in a park this time, where an ice-\ncream man is positioned next to the fountain. Anne\nruns home to get her wallet just while the ice-cream\nman decides to move his truck to the swings. He\ntells Sally about this, but unknown to her, he meets\nAnne on the way and tells her too. Sally then runs\nafter Anne, and finds her mother at home, who says\nthat Anne picked up the wallet and went to buy ice\ncream. The experimental question now is ‘Where\ndoes Sally think Anne went to buy ice cream?’,\nwith as correct answer ‘to the fountain’, also fol-\nlowed up with ‘Why?’, to prompt an explanation to\nthe effect of ‘Sally doesn’t know that the ice-cream\nman told Anne that he was moving to the swings’.\nStrange Stories test (SS) –– The Strange Sto-\nries test (Happé, 1994; Kaland et al., 2005) depicts\nseven social situations with non-literal language\nuse that can easily be misinterpreted, but causes no\nproblems to typically developed adults. To under-\nstand the situations, subjects must infer the char-\nacters’ intentions, applying ToM. For example, in\none of the items a girl wants a rabbit for Christ-\nmas. When she opens her present, wrapped in a\nbig enough box, it turns out that she received a\npile of books. She says that she is really happy\nwith her gift, after which subjects are asked the\nexperimental question ‘Is what the girl says true?’,\nwith correct answer ‘No’. They can motivate their\n392\nanswer after the question ‘Why does she say this?’,\nwith as correct answer ‘to avoid her parents’ feel-\nings being hurt’. Items increase in difficulty and\ncover a lie, pretend-play scenario, practical joke,\nwhite lie (example above), misunderstanding, sar-\ncasm, and double bluff.\nImposing Memory test (IM) –– The Imposing\nMemory test was originally developed by Kinder-\nman et al. (1998), but the test has been revised\nseveral times; we rely on an unpublished version\ncreated by Anneke Haddad and Robin Dunbar (van\nDuijn, 2016), originally for adolescents, which we\nadapted thoroughly to make it suitable for children\naged 7-10. Our version features two different sto-\nries, followed by true/false questions, 10 of which\nare ‘intentionality’ and 12 are ‘memory’ questions.\nFor instance, in one story Sam has just moved to\na new town. He asks one of his new classmates,\nHelen, where he can buy post stamps for a birthday\ncard for his granny. When Helen initially sends\nhim to the wrong location, Sam wonders whether\nshe was playing a prank on him or just got con-\nfused about the whereabouts of the shop herself.\nHe goes and asks another classmate, Pete, for help.\nAs in the original IM, the intentionality questions\ninvolve reasoning about different levels of recur-\nsively embedded mental states (e.g., at third-level:\n‘Helen thought Sam did not believethat she knew\nthe location of the store that sells post stamps’),\nwhereas the memory questions require just remem-\nbering facts presented in the story (e.g., to match\nthird-level intentionality questions, three elements\nfrom the story are combined: ‘Sam was looking for\na store where they sell post stamps. He told Pete\nthat he had asked Helen about this’).\n3.2 Scoring Test Answers\nTest scores for both children and LLMs were deter-\nmined in the following way. For each of the SA1\nand SA2 items, as well as for the seven SS items, a\ncorrect answer to the experimental question yielded\n1 point. These answers were discrete and thus easy\nto assess (‘box’, ‘fountain’, ‘no’, etc.). For the mo-\ntivation question a consensus score was obtained\nfrom two expert raters, on a range from 0-2, with 0\nmeaning a missing, irrelevant, or wrong motivation,\n1 meaning a partly appropriate motivation, and 2\nmeaning a completely appropriate motivation that\nfully explained why the character in each scenario\ndid or said something, or had a mental or emotional\nmind state. Thus, the maximum score for the SA1,\nSA2, and SS was 3 points per item, which were\naveraged to obtain a score between 0 and 1. For\neach correct answer to a true/false question in the\nIM, 1 point was given. All scores and ratings can\nbe found on OSF.\n3.3 Deviations\nWe tested the LLMs on the original SA and SS\nscenarios, but also on manually created deviations\nthat increasingly stray from their original formula-\ntions, to prevent LLMs from leveraging heuristics\nand memorizing relevant patterns from the training\ndata. Thus, deviations probe the degree to which\nperformance on ToM tests in LLMs generalizes.\nDeviation 0 was always the original test scenario\n(likely present in the training data); deviation 1 was\na superficial variation on the original with only e.g.,\nobjects and names changed (similar to Kosinski\n(2023)), whereas deviation 2 was a completely new\nscenario where only the ToM-phenomenon at issue\nwas kept constant (e.g., ‘second-order false belief’\nor ‘irony’). Since our adaptation of the IM test\nhas hitherto not been used or published, we did not\ninclude deviations for this test.\n3.4 Test Procedures for LLMs\nWe leveraged 11 state-of-the-art LLMs: 4 base-\nLLMs and 7 instruct-LLMs (see Table 1). Inference\nparameters were set such that their output was as\ndeterministic as possible (i.e. a temperature ≊ zero\nor zero where possible) improving reproducibility.\nEach inference was done independently to avoid\nin-context learning or memory leakage between\nquestions. This means that for each question, the\nprompt repeated the following general structure:\n[instruction] + [test scenario] + [question].\nInstruct-LLMs were prompted in a question-\nanswering format that stayed as close as possible\nto the questionnaires given to children, without any\nfurther custom prompting or provision of exam-\nples. Instructions were also similar to those given\nto children (e.g. ‘You will be asked a question.\nPlease respond to it as accurately as possible with-\nout using many words.’). The ‘Why’-questions in\nSA1 and SA2 were created by inserting the exper-\nimental question and answer the LLM gave into\nthe prompt: [ instruction] + [test scenario] + [ex-\nperimental question] + [LLM answer] +[‘Why?’].\nThis was not necessary for SS, given that experi-\nmental and motivation questions could be answered\nindependently.\n393\nBase-LLMs Source Size\nFalcon Penedo et al. (2023) 7B\nLLaMA Touvron et al. (2023) 30B\nGPT-davinci Brown et al. (2020) 175B\nBLOOM Scao et al. (2022) 176B\nInstruct-LLMs ” ”\nFalcon-instruct Penedo et al. (2023) 7B\nFlan-T5 Chung et al. (2022) 11B\nGPT-3\n(text-davinci-003) Ouyang et al. (2022) 175B\nGPT-3.5-turbo Ouyang et al. (2022) 175B\nPaLM2 Anil et al. (2023) 175-340B\nPaLM2-chat Anil et al. (2023) 175-340B\nGPT-4 OpenAI (2023) >340B\nTable 1: LLMs used in this study. Model sizes are\nundisclosed for GPT-4 and for PaLM2 and PaLM2-chat,\nthus we base ourselves on secondary sources for estima-\ntions; Knight (2023) and Elias (2023), respectively.\nFor base-LLMs, known to continue prompts\nrather than follow instructions, staying this close\nto the children’s questionnaires was not feasible.\nFor the SA and SS we therefore fed base-LLMs the\nscenario as described before, but formulated the\nquestions as text-completion exercises (e.g. ‘Sally\nwill look for the ball in the ’). Additionally, when\ncreating the motivation questions for SA1 and SA2,\nwe inserted the correct answer to the experimental\nquestion, instead of the LLM’s answer. This was\nbecause base-LLMs so often derailed in their out-\nput that the method described for instruct-LLMs\ndid not yield sensible prompts. Base-LLMs thus\nhad an advantage here over children and instruct-\nLLMs, who were potentially providing a motiva-\ntion following up on an incorrect answer they gave\nto the experimental question.\nFor the closed questions in the IM we attempted\nto streamline the output of base-LLMs by including\ntwo example continuations in the desired answer\nformat. These examples were based on trivial in-\nformation we added to the scenarios, unrelated to\nthe actual experimental questions. For example:\n‘Helen: I wear a blue jumper today. This is [incor-\nrect]’, where it was added in the story that Helen\nwears a green jumper. This pushed nearly all base-\nLLM responses towards starting with ‘[correct]’ or\n‘[incorrect]’, which we then assessed as answers\nto the true/false questions. We considered a simi-\nlar prompt structure for SA and SS, amounting to\nadopting few-shot learning for base-LLMs through-\nout (Brown et al., 2020), but given that reformulat-\ning questions as text-completion exercises was by\nitself effective to get the desired output format, we\nrefrained from inserting further differences from\nhow instruct-LLMs are prompted. It is important to\nnote that our prompts were in general not optimized\nfor maximal test performance, but rather designed\nto stay as uniform and close to the way children\nwere tested as possible, enabling a fair comparison\namong LLMs and with child performance.\n3.5 Test Procedures for Children\nChildren were recruited from one Dutch and one in-\nternational school in the South-West of the Nether-\nlands: 37 children in the younger group (7-8y) and\n36 children in the older group (9-10y). Children\nwere administered digital versions of the SA and\nSS for the younger group, and of the IM for the\nolder group, which they completed individually\non tablets or PCs equipped with a touch screen.\nTest scenarios and questions were presented in a\nself-paced text format and all SA and SS questions\nwere followed by an open text field in which they\nhad to type their answer. As the IM features long\nscenarios, voice-overs of the text were included\nto alleviate reading fatigue. Here children had to\nanswer by pressing yes/no after each question. To\nreduce memory bottlenecks, accompanying draw-\nings were inserted (see OSF) and navigating back\nand forth throughout the tests was enabled. In-\nformed consent for each child was obtained from\ncaretakers, and the study was approved by the Lei-\nden University Science Ethics Committee (ref. no.\n2021-18). Test answers were evaluated and scored\nparallel to the approach for LLMs (Section 3.2).\n4 Results\n4.1 Sally-Anne\nOverall performance on SA1 versus SA2 is given\nin Figure 1, left column. Most base-LLMs perform\nabove child level on first-order ToM (BLOOM,\nDavinci, LLaMA-30B) but fall at or or below child\nlevel on second-order ToM. A similar pattern is\nvisible for instruct-LLMs: most models perform\nwell above child level on first-order (GPT-4, GPT-\n3.5, PaLM2-chat, PaLM2), but not on second-order\nToM. Exceptions are GPT-4 and GPT-3.5: while de-\ngrading on second-order, they remain above child\nlevel. For both base- and instruct-LLMs, smaller\nmodels tend to perform worse (Falcon-7B, Falcon-\n7B-I, FLAN-T5) with GPT-3’s structurally low\nscores as striking exception. This is inconsistent\nwith results reported by (Kosinski, 2023) for GPT-\n3, which is probably due to the fact that Kosinski\napplied a text-completion approach whereas we\n394\nFigure 1: Performance on Sally-Anne tests for base-LLMs (top row) and instruct-LLMs (bottom row). Left\ncolumn depicts performance on first- and second-order ToM (i.e. SA1 vs. SA2), averaged over the original and\nrewritten test versions. Middle and left columns depict performance for SA1 and SA2 over levels of deviation from\nthe original test (0, 1, and 2; see Section 3.3). Dashed lines indicate child performance (n=37, age 7-8 years).\nprompted GPT-3 with open questions.\nWhen we consider the performance on SA1 and\nSA2 over deviations (middle and right columns\nin Figure 1), we see once more that almost all\nLLMs struggle with second-order ToM, since per-\nformance decreases already on deviation 0 (i.e.\nthe original test scenario), except for GPT-3.5 and\nGPT-4. Yet, it is thecombination of second-order\nToM and deviation 2 that pushes also GPT-3.5 and\nGPT-4 substantially below child levels, except for\nFalcon-7B, although the chat-optimized version of\nthis model (Falcon-7B-I) fails on all second-order\nquestions.\n4.2 Strange Stories\nGeneral performance on SS is given in Figure 2,\nleft column. Whereas child performance declines\nas items become more complex (from 1 to 7; see\nSection 3.1), this is overall less the case for LLM\nperformance. For instruct-LLMs, we see that GPT-\n4 approaches perfect scores throughout. GPT-3 and\nGPT-3.5 perform at or close to child level on item 1,\nafter which their performance somewhat declines,\nwhile staying well above child level. Other instruct-\nLLMs show a mixed picture: PaLM2-chat and\nFLAN-T5 surpass child level earlier than PaLM2.\nInterestingly, smaller FLAN-T5 outperforms large\nPaLM and PaLM2-chat on more difficult items.\nFalcon-7B-I, as smallest instruct-LLM, performs\noverall worst.\nIf performance is plotted over deviations (right\ncolumn in Figure 2) we see little impact on most\nbase-LLMs. For instruct-LLMs, it is striking\nthat deviation levels have almost no effect on the\nlarger models (GPT-4, PaLM2, PaLM2-chat, GPT-\n3, GPT-3.5), but do more dramatically lower per-\nformance of smaller models (FLAN-T5, Falcon-\n7B-I). In sum, base-LLMs perform below child\nlevel, except for the most complex items. Several\nlarge instruct-LLMs match or surpass child level\nthroughout, others only for more complex items.\nUnlike for SA, deviation levels seem to have little\nnegative impact.\n4.3 Imposing Memory\nThe classical finding for the IM test is that error\nrates go up significantly for questions involving\nhigher levels of recursive intentionality, but not for\nmemory questions on matched levels of complexity,\nsuggesting a limit to the capacity for recursive ToM\nspecifically (Stiller and Dunbar, 2007).1 We veri-\nfied this for our child data (n=36) with two mixed\nlinear models for memory and intentional questions\nwith random intercepts. We included five predictors\nthat were contrast-coded such that each predictor\nindicated the difference in average performance\nwith the previous level. For intentional questions,\nonly the difference between level two and one was\nsignificant (β = −0.222, p < .05), marking a cut-\noff point after which performance remained con-\nsistently low. For memory questions, performance\n1While there is consensus in the literature that higher levels\nof intentionality are significantly harder for participants than\nlower levels, by various measures, there is debate about the\ndifference with memory questions; see e.g. Lewis et al. (2017).\nFor a critical discussion of measuring recursive intentionality\nin general, see Wilson et al. (2023).\n395\nFigure 2: Performance on Strange Stories for base-LLMs (top row) and instruct-LLMs (bottom row). Left\ncolumn shows overall performance, averaged over levels of deviation from the original test. Right column shows\nperformance over deviation levels, averaged over items. Dashed lines indicate child performance (n=37, 7-8y).\nremained high across all levels (> .85), except for\nlevel four, where scores were significantly lower\nthan at level three ( β = −0.292, p < .00), but\nwent up again at level five ( β = 0.208, p < .00).\nThus, in line with earlier work, we find a cut-off\npoint after which scores on intentionality questions\nremained consistently low, compared to scores on\nmatched memory questions. We have no clear ex-\nplanation for the dip in performance on memory\nquestions at level four, but observe that it is driven\nby low scores on only one specific question out of\na total of four for this level, which children may\nhave found confusing.\nIn Figure 3 we see that all base-LLMs perform\nbelow child level, in general and on both inten-\ntionality and memory questions, and there is little\nvariation in performance, except that larger base-\nLLMs (BLOOM, GPT-davinci) improve on higher\nlevels of recursion. Regarding instruct-LLMs, we\nsee largely the same picture, as they almost all\nperform below child level, in general and on both\ntypes of questions. The exception is GPT-4, which\nperforms consistently well on all levels and stays\nabove child level after second-order intentionality.\nFor the difference between memory and intentional\nquestions, instruct-LLMs perform better on easier\nmemory questions, and drop towards the end, while\non intentional questions, they already start lower\nand stay relatively constant. Lastly, it is remark-\nable that FLAN-T5, as one of the smallest instruct-\nLLMs, overall increases performance as recursion\nlevels go up, and ends at child level. For GPT-3.5,\nwhich performs worst of all instruct-LLMs on this\ntask, we see the exact opposite.\n4.4 Notes on Child Performance\nIt can be observed that performance for SA was\noverall low compared to what could be expected\nfrom children aged 7-8 years: ¯x = 0.45 for SA1\nand ¯x = 0 .225 for SA2. We have two comple-\nmentary explanations for this. Firstly, as discussed\nin Section 3.5, children had to read the tests on\na screen, after which they had to type answers in\nopen text fields. This is a challenging task by itself\nthat relies on additional skills including language\nproficiency, conscientiousness, digital literacy, and\nmore. Secondly, whereas ‘passing’ originally only\nmeans that a child can work out where Sally will\nlook (for the ball, or for Anne on her way to buy\nice cream), we also asked for a motivation, which\nmakes the test more demanding. For the SS, com-\npleted by the same group of children, we see the\nexpected pattern that scores show a downward ten-\ndency as test items increase in difficulty. The older\ngroup, aged 9-10, completed the IM. As discussed\nin Section 4.3, scores resonate with earlier work.\nGiven that we see child performance not as the\ncentral phenomenon under observation in this pa-\nper, but rather as a reference for LLM performance,\nfurther discussion is outside our scope.\n5 Discussion\nSumming up the results for the Sally-Anne tests,\nwhile it is less surprising that base-LLMs and\nsmaller instruct-LLMs struggle with increasing\ntest complexity and deviations, it is striking that\nsecond-order ToM immediately perturbs some\nlarge instruct-LLMs (e.g. PaLM2-chat), and that\nadding deviations from the original test formula-\n396\nFigure 3:Performance on Imposing Memory test for base-LLMs (top row) and instruct-LLMs (bottom row). Left\ncolumn depicts overall performance over five levels of recursion, averaged over deviations. Middle and left columns\ndepict performance for Memory and Intentional questions. Dashed lines indicate child performance (n=36, 9-10y).\ntions pushed performance of even the most com-\npetitive models down (e.g. GPT-4, GPT-3.5). This\ninitially suggests that performance on ToM tasks\ndoes not generalize well beyond a few standard\ncontexts in LLMs, in line with earlier work (Sap\net al., 2022; Shapira et al., 2023; Ullman, 2023).\nFor the Strange Stories we saw that base-LLMs\nperform generally below child level. Most instruct-\nLLMs perform close to or above child level, par-\nticularly as items become more complex and child\nperformance drops much more dramatically than\nLLM performance. Levels of deviation from the\noriginal test formulation seem to have made almost\nno impact for the SS, suggesting that the capacity\nto deal with non-literal language targeted by the\nStrange Stories test does generalize to novel con-\ntexts. We conclude that instruct-LLMs are quite\ncapable at interpreting non-literal language, a skill\nthat in humans involves ToM. Since the training\ndata of LLMs includes numerous books and fora,\nwhich are typically rich in irony, misunderstanding,\njokes, sarcasm, and similar figures of speech, we\ntentatively suggest that LLMs are in general well-\nequipped to handle the sort of scenarios covered in\nthe Strange Stories. This should in theory include\nbase-LLMs, but it could be that their knowledge\ndoes not surface due to the test format, even after\nspecialized prompting. Going one step further, we\nhypothesize that Sally-Ann is generally harder for\nLLMs given that this test relies less on a very spe-\ncific sort of advanced language ability, but more\non a type of behaviourally-situated reasoning that\nLLMs have limited access to during training (see\nalso Mahowald et al., 2023).\nThe Imposing Memory test was the most chal-\nlenging for both base- and instruct-LLMs. Since\nour version of it was never published before, it con-\nstitutes another robustness test, which only GPT-4\nas largest instruct-LLM seems to pass well.\nThe gap between base- and instruct-LLMs is best\nsummarized in Figure 4. Here we see that no base-\nLLM achieves child level: all LLMs approaching\nor exceeding child performance are larger instruct-\nLLMs. Our adapted prompts and insertion of cor-\nrect answers for motivation questions did not make\na difference. We suggest that another issue for base-\nLLMs, besides the prompt format, was prompt\nlength. This was highest for IM, which can explain\nwhy they struggled most with this test. Prompt\nlength, in relation to the models’ varying context\nwindow sizes and ability to engage in what Hagen-\ndorff et al. (2023) call chain-of-thought reasoning,\nmerits further research (see also Liu et al., 2023).\nWe tested whether there was a difference between\nmodel performance on closed versus open ques-\ntions across all three tasks, but found no signal:\nthe models that struggled with closed questions\nwere also those that performed low on open ques-\ntions (for more details and additional information\non prompting, see Appendix A on OSF).\nEvidence is emerging that most LLM capaci-\nties are learned during self-supervised pre-training\n(Gudibande et al., 2023; Ye et al., 2023), which\nsuggests that base-LLMs are essentially ‘complete’\nmodels. Yet instruction-tuning, even in small\namounts (Zhou et al., 2023), adds adherence to\nthe desired interaction format and teaches LLMs,\nas it were, to apply their knowledge appropriately.\nWe see a parallel between instruction-tuning and\nthe role for rewarding cooperative communication\n397\nFigure 4:Grand mean performance (stars) of all mean\ntest scores (dots) for children and LLMs.\nin human evolution and development. It has been\nargued extensively that human communication is\nfundamentally cooperative in that it relies on a ba-\nsic ability and willingness to engage in mental co-\nordination (e.g Verhagen, 2015; Grice, 1975). It\nis a key characteristic of the socio-cultural niche\nin which we evolved that, when growing up, we\nare constantly being rewarded for showing such\nwillingness and cooperating with others to achieve\nsuccessful communicative interactions (Tomasello,\n2008). Reversely, if we do not, we are being pun-\nished, explicitly or implicitly via increasing social\nexclusion (David-Barrett and Dunbar, 2016). This\nbrings us back to our context: instruction-tuning\nessentially rewards similar cooperative principles,\nbut punishes the opposite, which may amount to an\nenhanced capacity for coordinating with an inter-\naction partner’s perspective, in humans and LLMs\nalike. This is reflected in performance on ToM\ntasks, which are banking on this capacity too.\nFinally, we do not claim that LLMs that per-\nformed well also have ToM in the way that humans\nhave it. Validity of cognitive tests such as those\nused in ToM research is a general issue (e.g. van\nDuijn, 2016). Yet for humans ToM tests are val-\nidated ‘quick probes’: decades of research have\nshown that proficiency on such testscorrelates with\nan array of real-world social and cognitive abilities\n(Beaudoin et al., 2020). For LLMs we are in a very\nearly stage of figuring out what is entailed by profi-\ncon ToM tests: on the one hand it is impressive that\nsome models show a degree of robust performance,\nwithout explicit training on ToM. On the other hand\nit remains an open question whether this amounts\nto any actual capacities in the social-cognitive do-\nmain, in which they are clearly very differently\ngrounded (if at all) compared to humans.\nFor future research we believe in the format of\ntesting models that differ in other respects than just\nsize, on a varied array of tasks, with multiple tests\nper test item, to gain further insight into the aspects\nthat explain variability in performance. For this,\nmore openness about architecture and training pro-\ncedures of current and future LLMs is imperative.\nIn addition, we believe to have contributed to the\ndebate by benchmarking LLM results on child data,\nbut more of this is needed. We had limited samples\nand age distributions, and tests were not presented\nin optimal ways (see Section 3.5).\nWe emphasize that our results need to be seen\nwithin the time frame of late Spring 2023. The\nfast pace with which LLMs are currently released\nand, in some cases, updated, makes them a moving\ntarget. There are indications that specific capaci-\nties of models from the GPT-family have declined\nover time, perhaps as a result of such updates (e.g.,\nhandling math problems and producing code; Chen\net al., 2023). Future studies need to address how\nsuch developments impact the capacities assessed\nin this paper.\n6 Conclusion\nWe have shown that a majority of recent Large\nLanguage Models operate below performance of\nchildren aged 7-10 on three standardized tests rele-\nvant to Theory of Mind. Yet those that are largest in\nterms of parameters, and most heavily instruction-\ntuned, surpass children, with GPT-4 well above all\nother models, including more recent competitors\nlike PaLM2-chat and PaLM2 (see Figure 4). We\nhave interpreted these findings by drawing a paral-\nlel between instruction-tuning and rewarding coop-\nerative interaction in human evolution. We concede\nthat researching the degree to which LLMs are ca-\npable of anything like thought in the human sense\nhas only just begun, which leaves the field with\nexciting challenges ahead.\nAcknowledgements\nThis research took place in the context of the\nproject A Telling Story, financed by the Dutch Re-\nsearch Council NWO (VI.Veni.191C.051). We are\ngrateful to the children and their caregivers and\nteachers for participating in our research, and we\nthank Li Kloostra, Lola Vandame, and three anony-\nmous reviewers for their help and constructive feed-\nback.\n398\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Clément Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. PaLM 2 Technical\nReport. arXiv preprint arXiv:2305.10403v3.\nIan Apperly. 2010. Mindreaders: the Cognitive Basis\nof \"Theory of Mind\". Psychology Press.\nBurcu Arslan, Niels A Taatgen, and Rineke Verbrugge.\n2017. Five-year-olds’ systematic errors in second-\norder false belief tasks are due to first-order theory of\nmind strategy selection: A computational modeling\nstudy. Frontiers in psychology, 8:275.\nChris Baker and Rebecca Saxe. 2011. Bayesian the-\nory of mind: Modeling joint belief-desire attribution.\nProceedings of the Thirty-Third Annual Conference\nof the Cognitive Science Society.\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith.\n1985. Does the autistic child have a “theory of\nmind”? Cognition, 21(1):37–46.\nPamela Barone, Guido Corradi, and Antoni Gomila.\n2019. Infants’ performance in spontaneous-response\nfalse belief tasks: A review and meta-analysis. Infant\nBehavior and Development, 57:101350.\nCindy Beaudoin, Élizabel Leblanc, Charlotte Gagner,\nand Miriam H Beauchamp. 2020. Systematic review\nand inventory of theory of mind measures for young\nchildren. Frontiers in psychology, 10:2905.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\nDangers of Stochastic Parrots: Can Language Mod-\nels Be Too Big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 610–623.\nMarcel Binz and Eric Schulz. 2023a. Turning large lan-\nguage models into cognitive models. arXiv preprint\narXiv.2306.03917.\nMarcel Binz and Eric Schulz. 2023b. Using cognitive\npsychology to understand GPT-3. Proceedings of the\nNational Academy of Sciences, 120(6):e2218523120.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nEric Brunet-Gouet, Nathan Vidal, and Paul Roux. 2023.\nDo conversational agents have a theory of mind? a\nsingle case study of chatgpt with the hinting, false\nbeliefs and false photographs, and strange stories\nparadigms.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with GPT-4.\nPeter Carruthers. 2013. Mindreading in infancy. Mind\n& Language, 28(2):141–172.\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nHow is ChatGPT’s behavior changing over time?\narXiv preprint arXiv:2307.09009.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language mod-\nels.\n399\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nTamas David-Barrett and Robin I. M. Dunbar. 2016.\nLanguage as a coordination tool evolves slowly. R.\nSoc. open sci., 3:160259.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt\nGray. 2023. Can AI language models replace hu-\nman participants? Trends in Cognitive Sciences,\n27(7):597–600.\nJennifer Elias. 2023. Google’s newest A.I. model uses\nnearly five times more text data for training than its\npredecessor. Accessed on: 2023-05-30.\nJ.A. Fodor. 1992. A theory of the child’s theory of mind.\nCognition, 44(3):283–296.\nGabriel Grand, Idan Asher Blank, Francisco Pereira,\nand Evelina Fedorenko. 2022. Semantic projection\nrecovers rich human knowledge of multiple object\nfeatures from word embeddings. Nature human be-\nhaviour, 6(7):975–987.\nErin Grant, Aida Nematzadeh, and Thomas L Griffiths.\n2017. How can memory-augmented neural networks\npass a false-belief task? In CogSci.\nPaul Grice. 1975. Logic and conversation. In Peter Cole\nand Jerry Morgan, editors, Syntax and semantics. Vol.\n3: Speech acts, pages 41–58. Academic Press, New\nYork.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms.\nT. Hagendorff, S. Fabi, and M. Kosinski. 2023. Human-\nlike intuitive behavior and reasoning biases emerged\nin large language models but disappeared in chatgpt.\nNature Compututer Science.\nThilo Hagendorff. 2023. Machine psychology: Inves-\ntigating emergent capabilities and behavior in large\nlanguage models using psychological methods. arXiv\npreprint arXiv:2303.13988.\nFrancesca G.E. Happé. 1994. An advanced test of\ntheory of mind: Understanding of story characters’\nthoughts and feelings by able autistic, mentally hand-\nicapped, and normal children and adults. Journal of\nautism and Developmental disorders, 24(2):129–154.\nCecilia Heyes. 2014. False belief in infancy: a fresh\nlook. Developmental Science, 17(5):647–659.\nCecilia M. Heyes and Chris D. Frith. 2014. The\ncultural evolution of mind reading. Science,\n344(6190):1243091.\nDaniel D. Hutto. 2008. Folk Psychological Narratives:\nThe Sociocultural Basis of Understanding Reasons.\nThe MIT Press.\nNils Kaland, Annette Møller-Nielsen, Lars Smith,\nErik Lykke Mortensen, Kirsten Callesen, and Dorte\nGottlieb. 2005. The Strange Stories test - a replica-\ntion study of children and adolescents with Asperger\nsyndrome. European child & adolescent psychiatry,\n14(2):73–82.\nP. Kinderman, R. Dunbar, and R. P. Bentall. 1998.\nTheory-of-mind deficits and causal attributions.\nBritish Journal of Psychology, (2):191–204.\nOscar Kjell, Katarina Kjell, and H Andrew Schwartz.\n2023. Ai-based large language models are ready to\ntransform psychological health assessment.\nWill Knight. 2023. A new chip cluster will make mas-\nsive ai models possible. Accessed on: 2023-05-30.\nMichal Kosinski. 2023. Theory of mind may have spon-\ntaneously emerged in large language models. arXiv\npreprint arXiv:2302.02083.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877, Hong Kong,\nChina. Association for Computational Linguistics.\nPenelope A. Lewis, Amy Birch, Alexander Hall, and\nRobin I. M. Dunbar. 2017. Higher order intention-\nality tasks are cognitively more demanding. Social\nCognitive and Affective Neuroscience, 12(7):1063–\n1071.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts. arXiv preprint\narXiv:2307.03172.\nGenevieve Liveley and Sam Thomas. 2020. Homer’s\nintelligent machines: AI in antiquity.\nLi Lucy and David Bamman. 2021. Gender and Rep-\nresentation Bias in GPT-3 Generated Stories. In\nProceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nXiaomeng Ma, Lingyu Gao, and Qihui Xu. 2023. Tom-\nchallenges: A principle-guided dataset and diverse\nevaluation tasks for exploring theory of mind. arXiv\npreprint arXiv:2305.15068.\n400\nKyle Mahowald, Anna A. Ivanova, Idan A. Blank,\nNancy Kanwisher, Joshua B. Tenenbaum, and\nEvelina Fedorenko. 2023. Dissociating language\nand thought in large language models: a cognitive\nperspective. arXiv preprint arXiv:2301.06627.\nChristopher D. Manning, Kevin Clark, John Hewitt,\nUrvashi Khandelwal, and Omer Levy. 2020. Emer-\ngent linguistic structure in artificial neural networks\ntrained by self-supervision. Proceedings of the Na-\ntional Academy of Sciences, 117(48):30046–30054.\nAntonella Marchetti, Cinzia Di Dio, Angelo Cangelosi,\nFederico Manzi, and Davide Massaro. 2023. Devel-\noping chatgpt’s theory of mind. Frontiers in Robotics\nand AI, 10.\nKaren Milligan, Janet Wilde Astington, and Lisa Ain\nDack. 2007. Language and theory of mind: Meta-\nanalysis of the relation between language ability\nand false-belief understanding. Child development,\n78(2):622–646.\nShima Rahimi Moghaddam and Christopher J. Honey.\n2023. Boosting theory-of-mind performance in large\nlanguage models via prompting.\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison\nGopnik, and Tom Griffiths. 2018. Evaluating theory\nof mind in question answering. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2392–2400, Brussels,\nBelgium. Association for Computational Linguistics.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajic, Christopher D Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, et al. 2016. Universal dependencies\nv1: A multilingual treebank collection. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC’16), pages\n1659–1666.\nKristine H. Onishi and Renée Baillargeon. 2005. Do 15-\nmonth-old infants understand false beliefs? Science,\n308(5719):255–258.\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nJosef Perner and Heinz Wimmer. 1985. “John thinks\nthat Mary thinks that. . . ” attribution of second-order\nbeliefs by 5-to 10-year-old children. Journal of ex-\nperimental child psychology, 39(3):437–471.\nBilly Perrigo. 2023. Exclusive: OpenAI Used Kenyan\nWorkers on Less Than $2 Per Hour to Make ChatGPT\nLess Toxic. Accessed on: 2023-01-25.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan\nZhang, S. M. Ali Eslami, and Matthew Botvinick.\n2018. Machine theory of mind. In Proceedings of\nthe 35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learning\nResearch, pages 4218–4227. PMLR.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. Advances in Neural Information Processing\nSystems, 32.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2021. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits of\nsocial intelligence in large LMs. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3762–3780, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. BLOOM: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi,\nXuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten\nSap, and Vered Shwartz. 2023. Clever Hans or neu-\nral theory of mind? stress testing social reasoning in\nlarge language models.\nDamien Sileo and Antoine Lernould. 2023.\nMindGames: Targeting Theory of Mind in\nLarge Language Models with Dynamic Epistemic\nModal Logic. arXiv preprint arXiv.2305.03353.\nJeff Sparrow. 2022. ‘Full-on robot writing’: the artificial\nintelligence challenge facing universities. Accessed\non: 2023-01-25.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R. Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, Agnieszka Kluska, Aitor\nLewkowycz, Akshat Agarwal, Alethea Power, Alex\n401\nRay, Alex Warstadt, Alexander W. Kocurek, Ali\nSafaya, Ali Tazarv, Alice Xiang, Alicia Parrish,\nAllen Nie, Aman Hussain, Amanda Askell, Amanda\nDsouza, Ambrose Slone, Ameet Rahane, Anan-\ntharaman S. Iyer, Anders Johan Andreassen, An-\ndrea Madotto, Andrea Santilli, Andreas Stuhlmüller,\nAndrew M. Dai, Andrew La, Andrew Lampinen,\nAndy Zou, Angela Jiang, Angelica Chen, Anh\nVuong, Animesh Gupta, Anna Gottardi, Antonio\nNorelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin\nHerrick, Avia Efrat, Aykut Erdem, Ayla Karaka¸ s,\nB. Ryan Roberts, Bao Sheng Loe, Barret Zoph,\nBartłomiej Bojanowski, Batuhan Özyurt, Behnam\nHedayatnia, Behnam Neyshabur, Benjamin Inden,\nBenno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake\nHowald, Bryan Orinion, Cameron Diao, Cameron\nDour, Catherine Stinson, Cedrick Argueta, Cesar\nFerri, Chandan Singh, Charles Rathkopf, Chenlin\nMeng, Chitta Baral, Chiyu Wu, Chris Callison-\nBurch, Christopher Waites, Christian V oigt, Christo-\npher D Manning, Christopher Potts, Cindy Ramirez,\nClara E. Rivera, Clemencia Siro, Colin Raffel, Court-\nney Ashcraft, Cristina Garbacea, Damien Sileo,\nDan Garrette, Dan Hendrycks, Dan Kilman, Dan\nRoth, C. Daniel Freeman, Daniel Khashabi, Daniel\nLevy, Daniel Moseguí González, Danielle Perszyk,\nDanny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Ju-\nrgens, Debajyoti Datta, Deep Ganguli, Denis Emelin,\nDenis Kleyko, Deniz Yuret, Derek Chen, Derek\nTam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan,\nDimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee,\nDylan Schrader, Ekaterina Shutova, Ekin Dogus\nCubuk, Elad Segal, Eleanor Hagerman, Elizabeth\nBarnes, Elizabeth Donoway, Ellie Pavlick, Emanuele\nRodolà, Emma Lam, Eric Chu, Eric Tang, Erkut\nErdem, Ernie Chang, Ethan A Chi, Ethan Dyer,\nEthan Jerzak, Ethan Kim, Eunice Engefu Manyasi,\nEvgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar,\nFernando Martínez-Plumed, Francesca Happé, Fran-\ncois Chollet, Frieda Rong, Gaurav Mishra, Genta In-\ndra Winata, Gerard de Melo, Germán Kruszewski,\nGiambattista Parascandolo, Giorgio Mariani, Glo-\nria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor\nBetz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim,\nHannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta,\nHayden Bogar, Henry Francis Anthony Shevlin, Hin-\nrich Schuetze, Hiromu Yakura, Hongming Zhang,\nHugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet,\nJack Geissinger, Jackson Kernion, Jacob Hilton, Jae-\nhoon Lee, Jaime Fernández Fisac, James B Simon,\nJames Koppel, James Zheng, James Zou, Jan Kocon,\nJana Thompson, Janelle Wingfield, Jared Kaplan,\nJarema Radom, Jascha Sohl-Dickstein, Jason Phang,\nJason Wei, Jason Yosinski, Jekaterina Novikova, Jelle\nBosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal,\nJesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming\nSong, Jillian Tang, Joan Waweru, John Burden, John\nMiller, John U. Balis, Jonathan Batchelder, Jonathan\nBerant, Jörg Frohberg, Jos Rozen, Jose Hernandez-\nOrallo, Joseph Boudeman, Joseph Guerr, Joseph\nJones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce\nChua, Kamil Kanclerz, Karen Livescu, Karl Krauth,\nKarthik Gopalakrishnan, Katerina Ignatyeva, Katja\nMarkert, Kaustubh Dhole, Kevin Gimpel, Kevin\nOmondi, Kory Wallace Mathewson, Kristen Chia-\nfullo, Ksenia Shkaruta, Kumar Shridhar, Kyle Mc-\nDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella,\nLucas Lam, Lucy Noble, Ludwig Schmidt, Luheng\nHe, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem\nSenel, Maarten Bosma, Maarten Sap, Maartje Ter\nHoeve, Maheen Farooqi, Manaal Faruqui, Mantas\nMazeika, Marco Baturan, Marco Marelli, Marco\nMaru, Maria Jose Ramirez-Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast,\nMatthew L Leavitt, Matthias Hagen, Mátyás Schu-\nbert, Medina Orduna Baitemirova, Melody Arnaud,\nMelvin McElrath, Michael Andrew Yee, Michael Co-\nhen, Michael Gu, Michael Ivanitskiy, Michael Star-\nritt, Michael Strube, Michał Sw˛ edrowski, Michele\nBevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker,\nMo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun\nPeng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-\nAri Krakover, Nicholas Cameron, Nicholas Roberts,\nNick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar,\nNiveditha S. Iyer, Noah Constant, Noah Fiedel,\nNuan Wen, Oliver Zhang, Omar Agha, Omar El-\nbaghdadi, Omer Levy, Owain Evans, Pablo Anto-\nnio Moreno Casares, Parth Doshi, Pascale Fung,\nPaul Pu Liang, Paul Vicol, Pegah Alipoormolabashi,\nPeiyuan Liao, Percy Liang, Peter W Chang, Pe-\nter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr\nMiłkowski, Piyush Patil, Pouya Pezeshkpour, Priti\nOli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Ra-\nbin Banjade, Rachel Etta Rudolph, Raefer Gabriel,\nRahel Habacker, Ramon Risco, Raphaël Millière,\nRhythm Garg, Richard Barnes, Rif A. Saurous, Riku\nArakawa, Robbe Raymaekers, Robert Frank, Ro-\nhan Sikand, Roman Novak, Roman Sitelew, Ro-\nnan Le Bras, Rosanne Liu, Rowan Jacobs, Rui\nZhang, Russ Salakhutdinov, Ryan Andrew Chi,\nSeungjae Ryan Lee, Ryan Stovall, Ryan Teehan,\nRylan Yang, Sahib Singh, Saif M. Mohammad,\nSajant Anand, Sam Dillavou, Sam Shleifer, Sam\nWiseman, Samuel Gruetter, Samuel R. Bowman,\nSamuel Stern Schoenholz, Sanghyun Han, Sanjeev\nKwatra, Sarah A. Rous, Sarik Ghazarian, Sayan\nGhosh, Sean Casey, Sebastian Bischoff, Sebastian\nGehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava,\nSherry Shi, Shikhar Singh, Shima Asaadi, Shixi-\nang Shane Gu, Shubh Pachchigar, Shubham Tosh-\nniwal, Shyam Upadhyay, Shyamolima Shammie\nDebnath, Siamak Shakeri, Simon Thormeyer, Si-\nmone Melzi, Siva Reddy, Sneha Priscilla Makini,\nSoo-Hwan Lee, Spencer Torene, Sriharsha Hatwar,\nStanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Biderman, Stephanie Lin, Stephen Prasad,\nSteven Piantadosi, Stuart Shieber, Summer Mish-\n402\nerghi, Svetlana Kiritchenko, Swaroop Mishra, Tal\nLinzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali,\nTatsunori Hashimoto, Te-Lin Wu, Théo Desbor-\ndes, Theodore Rothschild, Thomas Phan, Tianle\nWang, Tiberius Nkinyili, Timo Schick, Timofei Ko-\nrnev, Titus Tunduny, Tobias Gerstenberg, Trenton\nChang, Trishala Neeraj, Tushar Khot, Tyler Shultz,\nUri Shaham, Vedant Misra, Vera Demberg, Victo-\nria Nyamai, Vikas Raunak, Vinay Venkatesh Ra-\nmasesh, vinay uday prabhu, Vishakh Padmakumar,\nVivek Srikumar, William Fedus, William Saunders,\nWilliam Zhang, Wout V ossen, Xiang Ren, Xiaoyu\nTong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadol-\nlah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,\nYasaman Bahri, Yejin Choi, Yichi Yang, Yiding\nHao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu-\nfang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao,\nZijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi\nWu. 2023. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nTransactions on Machine Learning Research.\nJames Stiller and Robin IM Dunbar. 2007. Perspective-\ntaking and memory capacity predict social network\nsize. Social Networks, 29(1):93–104.\nMichael Tomasello. 2008. Origins of Human Communi-\ncation. MIT Press, Cambridge, MA.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. LLaMA: Open\nand Efficient Foundation Language Models.\nAlan M. Turing. 1950. Computing machinery and intel-\nligence. Mind, LIX:433–460.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nMax J van Duijn. 2016. The lazy mindreader: a human-\nities perspective on mindreading and multiple-order\nintentionality. Ph.D. thesis, Leiden University.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nArie Verhagen. 2015. Grammar and cooperative com-\nmunication. In Ewa Dabrowska and Dagmar Divjak,\neditors, Handbook of Cognitive Linguistics, pages\n232–252. De Gruyter Mouton, Berlin, München,\nBoston.\nAdelheid V oskuhl. 2013. One introduction: Androids,\nenlightenment, and the human-machine boundary.\nTaylor Webb, Keith J. Holyoak, and Hongjing Lu. 2023.\nEmergent analogical reasoning in large language\nmodels.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners.\nRobert Wilson, Alexander Hruby, Daniel Perez-Zapata,\nSanne W. van der Kleij, and Ian A. Apperly. 2023. Is\nrecursive “mindreading” really an exception to limita-\ntions on recursive thinking? Journal of Experimental\nPsychology: General, 152(5):1454–1468.\nHeinz Wimmer and Josef Perner. 1983. Beliefs about\nbeliefs: Representation and constraining function of\nwrong beliefs in young children’s understanding of\ndeception. Cognition, 13(1):103–128.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,\nShichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong,\nYang Shen, et al. 2023. A comprehensive capability\nanalysis of GPT-3 and GPT-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. Lima: Less\nis more for alignment.",
  "topic": "Theory of mind",
  "concepts": [
    {
      "name": "Theory of mind",
      "score": 0.5612629652023315
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47338834404945374
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4588310420513153
    },
    {
      "name": "Psychology",
      "score": 0.42452943325042725
    },
    {
      "name": "Cognition",
      "score": 0.4144289493560791
    },
    {
      "name": "Cognitive science",
      "score": 0.36200809478759766
    },
    {
      "name": "Computer science",
      "score": 0.3297208547592163
    },
    {
      "name": "History",
      "score": 0.09055611491203308
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210155092",
      "name": "Czech Academy of Sciences, Institute of Computer Science",
      "country": "CZ"
    },
    {
      "id": "https://openalex.org/I2800006345",
      "name": "Leiden University Medical Center",
      "country": "NL"
    }
  ]
}