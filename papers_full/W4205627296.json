{
    "title": "Predicting Stock Closing Prices in Emerging Markets with Transformer Neural Networks: The Saudi Stock Exchange Case",
    "url": "https://openalex.org/W4205627296",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4208031205",
            "name": "Nadeem Malibari",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2275076356",
            "name": "Iyad Katib",
            "affiliations": [
                "Computing Center"
            ]
        },
        {
            "id": "https://openalex.org/A1889280345",
            "name": "Rashid Mehmood",
            "affiliations": [
                "King Abdulaziz University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3112597358",
        "https://openalex.org/W3117955110",
        "https://openalex.org/W3007443784",
        "https://openalex.org/W3159427678",
        "https://openalex.org/W2954067306",
        "https://openalex.org/W2588547626",
        "https://openalex.org/W3013421259",
        "https://openalex.org/W4206103388",
        "https://openalex.org/W2966984844",
        "https://openalex.org/W3136491841",
        "https://openalex.org/W2790822776",
        "https://openalex.org/W2059852492",
        "https://openalex.org/W2993491036",
        "https://openalex.org/W2971724044",
        "https://openalex.org/W2954731415",
        "https://openalex.org/W2798702047",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6762496457",
        "https://openalex.org/W2774513877",
        "https://openalex.org/W2946030813",
        "https://openalex.org/W2911218645",
        "https://openalex.org/W2890096158",
        "https://openalex.org/W2604847698",
        "https://openalex.org/W2785770483",
        "https://openalex.org/W2602977295",
        "https://openalex.org/W6751183075",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W3126721948",
        "https://openalex.org/W2999411920",
        "https://openalex.org/W2031034601",
        "https://openalex.org/W2891702854",
        "https://openalex.org/W2209610041",
        "https://openalex.org/W3048630347",
        "https://openalex.org/W2271840356",
        "https://openalex.org/W3035669514",
        "https://openalex.org/W2802023636",
        "https://openalex.org/W3027907822",
        "https://openalex.org/W2955024262",
        "https://openalex.org/W2945680505",
        "https://openalex.org/W2963751193",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2734986640",
        "https://openalex.org/W2970658101"
    ],
    "abstract": "Deep learning has transformed many fields includ-ing computer vision, self-driving cars, product recommendations, behaviour analysis, natural language processing (NLP), and medicine, to name a few. The financial sector is no surprise where the use of deep learning has produced one of the most lucrative applications. This research proposes a novel fintech machine learning method that uses Transformer neural networks for stock price predictions. Transformers are relatively new and while have been applied for NLP and computer vision, they have not been explored much with time-series data. In our method, self-attention mechanisms are utilized to learn nonlinear patterns and dynamics from time-series data with high volatility and nonlinearity. The model makes predictions about closing prices for the next trading day by taking into account various stock price inputs. We used pricing data from the Saudi Stock Exchange (Tadawul) to develop this model. We validated our model using four error evaluation metrics. The applicability and usefulness of our model to fintech are demonstrated by its ability to predict closing prices with a probability above 90%. To the best of our knowledge, this is the first work where transformer networks are used for stock price prediction. Our work is expected to make significant advancements in fintech and other fields depending on time-series forecasting.",
    "full_text": "(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nPredicting Stock Closing Prices in Emerging\nMarkets with Transformer Neural Networks: The\nSaudi Stock Exchange Case\nNadeem Malibari1, Iyad Katib 2, Rashid Mehmood 3\nDepartment of Computer Science, Faculty of Computing and Information Technology 1,2\nHigh Performance Computing Center 3\nKing Abdulaziz University, Jeddah 21589, Saudi Arabia\nAbstract—Deep learning has transformed many fields includ-\ning computer vision, self-driving cars, product recommendations,\nbehaviour analysis, natural language processing (NLP), and\nmedicine, to name a few. The financial sector is no surprise\nwhere the use of deep learning has produced one of the most\nlucrative applications. This research proposes a novel fintech\nmachine learning method that uses Transformer neural networks\nfor stock price predictions. Transformers are relatively new and\nwhile have been applied for NLP and computer vision, they have\nnot been explored much with time-series data. In our method,\nself-attention mechanisms are utilized to learn nonlinear patterns\nand dynamics from time-series data with high volatility and\nnonlinearity. The model makes predictions about closing prices\nfor the next trading day by taking into account various stock price\ninputs. We used pricing data from the Saudi Stock Exchange\n(Tadawul) to develop this model. We validated our model using\nfour error evaluation metrics. The applicability and usefulness of\nour model to fintech are demonstrated by its ability to predict\nclosing prices with a probability above 90%. To the best of our\nknowledge, this is the first work where transformer networks are\nused for stock price prediction. Our work is expected to make\nsignificant advancements in fintech and other fields depending on\ntime-series forecasting.\nKeywords—Stock price prediction; time-series forecasting;\ntransformer deep neural networks; Saudi Stock Exchange\n(Tadawul); financial markets\nI. I NTRODUCTION\nWe have come a long way in developing our societies,\nimproving and optimising every task and thing we do, and ar-\ntificial intelligence (AI) is at the heart of these endeavours [1],\n[2]. Machine and deep learning-based AI has revolutionised\nmany aspects of our daily activities, be it healthcare [3], [4],\ntransportations [5], [6], big data [7], distance learning [8], dis-\naster management [9], risk prediction in aviation systems [10],\nDNA profiling [11], smart cities [12], [13], and more. The use\nof machine and deep learning in the financial sector is one\nof the most lucrative tasks. Forecasting time-series data is an\nimportant topic that plays a key role in analysis, decision-\nmaking, and resource management in many industrial sectors.\nFor example, in the financial sector, forecasting based on\nhistorical data can be helpful for investors in maximizing\nreturn and reducing risk on investments [14], [15]. Many works\nhave been reported on the use of AI for the financial sector,\nsuch as the use of multilayer perceptrons (MLP) for NSADA\nstock index [16], the use of stacked autoencoders for US\nstock forecasting [17], and the use of Long short-term memory\nnetwork (LSTM) to predict the closing prices of iShares MSCI\nUnited Kingdom index [18] (for further motivation on the\nsubject, see Section II).\nA time-series forecast is a way of determining future values\nbased on historical experience. Correlational data is used for\nthis process, either time-based correlations (years, months,\nweeks, etc.) or sequential correlations, for gaining insights\nthat inform decisions. A range of methods has been developed\nto predict, ranging from traditional to machine-learning ap-\nproaches. Despite their wide usage, traditional time-series pre-\ndiction methods such as auto-regression (AR), Seasonal Na¨ıve,\nETS, and integrated moving average ARIMA are designed to\nfit each time-series separately [19]. Moreover, practitioners\nshould learn how to select specific trends, seasonal compo-\nnents, and other data components manually, especially for\nfinancial data series with highly nonlinear and fluctuating data.\nThese drawbacks have limited their applications in advanced\nlarge-scale time-series prediction tasks.\nThe challenges mentioned above can be overcome by\nalgorithms that can capture the patterns in the data and\nthe dynamics underlying them. In deep neural networks,\ncontinuous developments have led to breakthroughs that are\nproposed as another alternative. An array of deep neural\nnetwork architectures has been applied to time-series models to\nunderstand trends and patterns by learning from ground truth\ndata. However, many challenges remain. For example, while\na Recurrent Neural Network (RNN) can model and process\nsequential and time-series data, its gradient vanishing and\nexploding properties prevent them from detecting long-term\ndependencies (relationship between entities that are several\nsteps apart). In real-world forecasting, there are long-term and\nshort-term repeating patterns [20], which means that complex\nRNN models are required to analyze long-term time series and\nstudy long-term effects. Therefore, long short-term memory\n(LSTM) models have been proposed to improve the standard\nRNN model for time series analysis. Theoretically, they are\nexplicitly geared towards minimizing long-term dependency\nproblems. However, according to [21], LSTM has an adequate\ncontext size of 200 tokens on average, but they are only able to\ndistinguish 50 tokens within a context, suggesting that even it\nis incapable of capturing long-term trends. Furthermore, RNNs\nand all their variants use mostly sequential operations, thus\ncannot benefit from the performance advantages offered by\nmodern GPUs.\nwww.ijacsa.thesai.org 876 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nRather than RNNs, the next big step was a completely new\narchitecture – Transformer [22] utilizes attention mechanisms\nthat leverage self-attention mechanisms to process the entire\nsequence of data. The transformer architecture is the most\nprevalent model for natural language modelling and has proven\nquite successful in several other applications. The complex-\nity of the space corresponding to self-attention can grow\nquadratically as sequence length increases; for this reason, self-\nattention cannot be extended to extremely long sequences [20].\nThe quadratic complexity of computing poses a significant\nchallenge when forecasting time series with long-term solid\ndependence and fine granularity. Researchers had the same\nchallenges adapting transformers from language to computer\nvision applications due to pictures containing more significant\namounts of information than sentences. However, they are\nable to replace this quadratic computational complexity with\na linear computational complexity to image size.\nIn this work, we specifically delve into adapting the com-\nputer vision transformer model [23] to time series forecasting.\nWe propose a novel fintech machine learning method that uses\nTransformer neural networks for stock price predictions. In\nour method, self-attention mechanisms are utilized to learn\nnonlinear patterns and dynamics from time-series data with\nhigh volatility and nonlinearity. Our Contributions follow.\n• We propose a novel predictive Transformer based\nmodel with divided time series data into patches for\npredicating future value. Regardless of how complex\na situation is, our proposed method can discover the\nbroad conditional probability distribution of the future\nvalues.\n• The model makes predictions about closing prices\nfor the next trading day by taking into account var-\nious inputs, Open, High, Low, V olume, and Closing\nPrices. We used pricing data from the Saudi Stock\nExchange (Tadawul) to develop this model. We val-\nidated our model using a range of metrics; Mean\nAbsolute (MAE), Square (MSE), Root MSE (RMSE),\nand Percentage (MAPE) Error.\n• The applicability and usefulness of our model to fin-\ntech are demonstrated by its ability to predict closing\nprices with a probability above 90%.\nNovelty: As mentioned earlier, transformers are relatively\nnew and while these have been applied for NLP and computer\nvision, they have not been explored much with time-series data.\nOur work is expected to make significant advancements in\nfintech and other fields depending on time-series forecasting.\nTo the best of our knowledge, this is the first work where\ntransformer networks are used for stock price prediction.\nThe structure of this paper is as follows. We discuss related\nresearch in Section II as well as past innovations using deep\nlearning for stock forecasts. The methodology for this study,\nthe dataset, and the transformer model with divided space are\ndescribed in Section III. This section also provides details of\ndata prepossessing, and hyperparameters selection. Section IV\nprovides the results and analysis. Section V concludes and\nprovides directions for the future work.\nII. R ELATED WORK\nIn the not-so-distant past, Neural Networks (NN) were\ncriticized by many forecasting practitioners as not suitable and\nnot being competitive in forecasting fields [24]. Consequently,\npractitioners have usually selected statistical methods that were\nconsidered more straight forward to apply [19]. However, with\nthe ever-increasing availability of data, neural networks (NNs)\nand deep learning have revolutionized and achieved remarkable\nsuccess in many research fields and practical scenarios, includ-\ning medical predictions, NLP, image recognition, etc. Because\nof their capabilities to identify complex nonlinear patterns and\nexplore unstructured relationships without hypothesizing them\na priori. These technological breakthroughs have attracted sig-\nnificant attention from the enthusiasts’ researcher community\npresenting many complex novel NN architectures on time\nseries forecasting. Over recent decades, plenty of works and\nresearch exist where deep learning is used for forecasting.\nThere is a possibility to predict stock price changes and foreign\nexchange rates according to [14]. As a result, AI applications\nare becoming increasingly popular among investors to increase\nreturns and reduce the risk [15].\nSelvin et al. [25] illustrated how deep neural network\narchitectures can capture hidden dynamics and can be used\nto forecast. Guresen, Kayakutlu, and Daim [16] predicts the\nNASDA stock index by using multilayer perceptrons (MLP),\ndynamic, and hybrid artificial neural networks. Using a stacked\nautoencoder and deep neural network, Takeuchi and Lee [17]\nobtains an accuracy of 53.36 % when predicting the US stock\ndirection.\nSince their inception in 2014 by Hochreiter and Schmid-\nhuber [26], the Long short-term memory network (LSTM)\nintroduced is a variation of the Recurrent neural network model\n(RNN), which is the most commonly used architecture for\nsequence prediction problems [8]. In contrast to RNN, LSTM\nnetworks are capable of detecting long-term dependency and\ncan prevent gradient vanishing. It utilizes historical information\nvia the input, forget and output gates. In their study, Nikou,\nMansourfar, and Bagherzadeh [18] predict the closing prices of\niShares MSCI United Kingdom index using an LSTM model.\nThe model performed significantly better than the ANN, Sup-\nport Vector Regression (SVR), and RF models. LSTMs are\nutilized in another study by [27] in order to forecast future\nstock returns. Also, an Autoregressive Integrated Moving Av-\nerage (ARIMA) and an LSTM model were utilized to improve\nforecast accuracy [28]. According to Nelson, Pereira, and De\nOliveira [29], the average accuracy for predicting the direction\nof some stocks traded on the Brazilian stock exchange could\nreach up to 55.9% with the LSTM model.\nBecause of its powerful pattern recognition ability, the\nconvolutional neural network (CNN) is a variation of the\nmultilayer perceptron (MLP). Its use has extended increasingly\nfor time-series forecasting. The work by [30], [31], and [32]\nused CNN to predict stock trends. Ugur Gudelek, Arda Boluk,\nand Murat Ozbayoglu [32] have also experimented with 2D\nCNN for trend detection. The model performance evaluation\nhas 72% accuracy values and looks promising.\nA comparison study of differences between Multi-layer\nPerceptron (MLP), Convolutional Neural Network, and Long\nShort-Term Memory (LSTM) was performed by [33]. They\nwww.ijacsa.thesai.org 877 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nTABLE I. S UMMARY OF RELATED WORKS\nModel Architecture\nResearch ANN MLP RF SVR RNN CNN LSTM Transformer Dataset Accuracy\nSelvin et al. [25] ✓ ✓ ✓ 1721 NSE 2.36\nGuresen et al. [16] ✓ ✓ NASDAQ MSE: 1472\nNikou et al. [18] ✓ ✓ ✓ ✓ iShares MSCI MSE: 0.09396\nNaik et al. [27] ✓ ✓ Indian NSE RMSE: 23.78\nNelson et al. [29] ✓ BMF Bovespa 55.9%\nGudelek et al. [32] ✓ ETF 72%\nPersio et al. [33] ✓ ✓ ✓ SP500 MSE: 0.2491\nOur Study ✓ Tadawul 90%\nadopted a sliding window approach, using 30 previous days\nto predict the value of the 31 st day using historical data of\nthe S&P500 index from 1950 to 2016. CNN’s results are\nexceptional even without the use of additional features such\nas technical analysis.\nRecently, the well-known self-attention-based Transformer\n[22] was proposed for sequence modeling is the most prevalent\nmodel for natural language modeling and has proven quite\nsuccessful in several other applications such as as translation,\nspeech, image generation, and music [22], [34], [35]. The\nextension of self-attention to extremely long sequences would,\nhowever, be computationally prohibitive since space com-\nplexity increases quadratically with the sequence length [20].\nHowever, Vision Transformer (ViT) [23] and TimeSformer\n(Time-Space Transformer) [36] offer entirely new architec-\ntures for image classification, and video understanding based\nsolely on Transformers eliminating the problems associated\nwith long sequences. In particular, ViT divides an image into\npatches (also called tokens) with fixed length; then following\nthe practice of using transformers to model language, ViT\nthen uses transformer layers to model the relationship among\ntokens for classification. The TimeSformer, on the other hand,\ntranslates the input video into a sequence of image patches\nderived from the individual frames. The model then captures\nthe semantic information about each patch through comparison\nwith those of the other patches. This allows TimeSformer to\ncapture the space-time dependency based on the whole video.\nTransformers’ recent success in natural language processing\n(NLP) has motivated researchers to implement this model in\ncomputer vision applications and tasks.\nTable I summarises the related works discussed in this\nsection. It lists the various ML models that the researchers\nhave used for stock price prediction along with the respec-\ntive datasets used and the model accuracies reported in the\nrespective works. The most commonly used architecture for\nproblems involving stock price prediction is the LSTM. It can\ndetect long-term dependency and prevent gradient vanishing\nto some extent. However, LSTM accuracy is much less than\nthe convolutional neural network (CNN) because of CNN’s\npowerful pattern recognition ability. The accuracy metrics are\nreported in the table if these are provided by the researchers,\notherwise, we reported the numeric value from the article\nwithout the accuracy metric name. As shown in the table, the\nbest result achieved is 72% accuracy. Our transformer model\nwith its attention features has provided 90% or higher accuracy.\nWe have kept the content in the table to the minimum due to\nthe space issue, please refer to the listed works for details.\nIII. M ETHODOLOGY , DATASETS AND MODEL DESIGN\nThe objective of our study is to predict the subsequent\nand future closing of the trades in the Saudi Stock Exchange\n(Tadawul). We use a transformer-based temporal model archi-\ntecture. In this section, we describe our methodology, Trans-\nformer neural network model design, datasets, preprocessing,\nand validation metrics.\nWe first present an overview of our methodology in Sec-\ntion III-A. The transformer-based temporal model architecture\nis described in Section III-B. The the Saudi Stock Exchange\n(Tadawul) datasets are explored in Section III-C. The data\nmodelling methodology using transformer neural networks is\nsummarised in Section III-D. In Section III-E, we describe\nthe preparation of the dataset, including data splitting, normal-\nization, and feature selection. We discuss the hyperparameter\nconfiguration for the model. Section III-F describes the concept\nof sliding window for framing the dataset. Section III discusses\nhyperparameter configuration of our model.\nA. Methodology Overview\nThe overall methodology we have adopted is depicted in\nFig. 1. It consists of seven main phases as highlighted in\nthe figure. The first process involved extracting Saudi Stock\nExchange (Tadawul) data, followed by data cleaning and\nnormalization. As a result of this procedure, we only get data\nthat is appropriate for machine learning algorithms. We then\nselect the four features (open, low, high, previous closing)\nthat the model will use. Thereafter, the data are sorted into\nnon-overlapping batches, which are then fed into the model\nuntil performance measures are optimized. Ultimately, the\noptimized model is used to forecast future closing prices for\nunseen stock data.\nB. Transformer Neural Network Architecture\nA significant influence on our architecture is a vision trans-\nformer (ViT) [36] using divided space. The vision transformer\n(ViT) is among the first attempts to apply the outstanding\nperformance of Transformers [22] to image classification tasks\nrather than natural language processing. The vision transformer\n(ViT) model, which comprises three main elements: a linear\nlayer for patch embedding, a stack of transformer blocks with\nmulti-head self-attention and feed-forward layers, and a linear\nlayer classification score prediction.\nwww.ijacsa.thesai.org 878 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nFig. 1. An Overview of Our Methodology.\nFig. 2. Proposed Transformer Encoder Model Overview (Illustration Inspired by [22]).\nAn overview of our suggested model is depicted in Fig. 2.\nThe Vision Transformer (ViT) model serves as the basis for\nour predictive model. Our suggested model added one more\ncomponent to ViT architecture. Its primary purpose is to\ncreate sliding windows from historical data. Since daily trading\nvolumes on the stock market are substantial, historical data on\nthe market can be challenging to manipulate, and manipulating\nit can cause a computational burden. Furthermore, the effect\nof more recent data on a training model is greater than that\nof older data [37]. Braverman et al. [38] developed a sliding-\nwindow method that utilizes recent data while disregarding\nolder observations to solve this problem.\nThe range of data of interest is selected using a window.\nThe sliding window represents a period that stretches backward\nin time from the present to the past. The sliding window is\nheld steady (the number of data stays constant), and only\nthe window is moved. Resulting, the training data volume is\nreduced while maintaining the model’s efficiency and general\nusability [37].\nIn summary, Fig. 2 depicts our proposed model as follows.\nThe historical data is split into windows and then those\nwindows are divided into fixed-size patches. Linear embed-\ndings are then applied to the patches, followed by position\nembeddings. Then we feed the resulting sequence of vectors\nto the Transformer encoder. As a standard approach, we add\nan extra token to the sequence of learnable tokens to perform\nprediction. The Transformer encoder diagram in Fig. 2 was\ninspired by [22].\nC. Datasets\nThe Saudi Stock Exchange (Tadawul) database contains\nstock trading information for more than 200 Saudi Arabian\nlisted companies. The companies are grouped into sectors with\ndifferent indices for each industry. The data we downloaded\nspans the period from 1993-01-02 through 2021-06-17 and\nconsists of 772,189 trading days. Listed companies’ and in-\ndices trading information includes their Open, High, Low,\nV olume, and Closing Price for each trading day. From the\ndataset, we extracted four indices to illustrate model capa-\nbilities and performance. These are Tadawul All Share Index\n(TASI), the Banks Index (TBNI), Materials Index (TMTI), and\nTelecommunication Services Index (TTSI).\nTable II lists a small selection of the dataset. Specifically, it\nshows the trading information in the dataset for the TASI index\nfor the period 1994-01-26 to 2021-07-01, which corresponds\nto 7311 trading days. The rows correspond to one trading\nday and contain the following features: the index column,\nthe transaction date, the ticker code, High, Low, V olume, and\nClosing Price. Fig. 3 depicts the histograms of the closing price\nfeature of the four datasets. There are four panels in the figure,\nwww.ijacsa.thesai.org 879 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nTABLE II. TASI S AMPLE DATA\ndate ticker open low high vol close\n0 1994-01-26 TASI 1751.71 1751.71 1751.71 312907 1751.71\n1 1994-01-29 TASI 1751.71 1750.91 1751.71 204831 1750.91\n—- —- — —- —- —- —— ——\n7310 2021-06-30 TASI 11002.74 10940.44 11009.70 374658538 10984.15\n7311 2021-07-01 TASI 10987.13 10968.11 11006.66 352200486 10979.05\nFig. 3. The Histograms of the Closing Price for the Four Datasets.\neach showing the histogram of its respective index. A display\nof the closing price is shown on the x-axis for each panel,\ngrouped into 25 bins of equal width. Each bin is plotted as a\nbar whose height (the y-axis) indicates the number of closing\nprices (frequencies) occurrences in that bin.\nFig. 4. TASI Boxplot.\nFig. 5. TTSI Boxplot.\nFig. 4 to 7 depict the four features (open, low, high, close)\nof the four indices in our dataset as a boxplot. A boxplot is\nan in-depth statistical data analysis tool for gaining a broad\nperspective on the center and spread of the data distribution,\nwhich can assist with checking for errors and protecting other\nanalyses. The median, interquartile range box, and whiskers\nare the primary elements of the boxplot to help understand the\ncenter and spread of the sample data. You’ll see the green line\nrepresenting the median in each box, which is the center of\neach feature. The interquartile range (the range between the\nthird quartile and the first quartile) box, on the other hand,\nrepresents the middle 50% of the data and reflects how the\ndata is distributed. The whiskers extend from both sides of\nthe box (the bottom line is called lower whiskers, whereas the\nupper one is called higher whiskers). The whiskers denote the\nranges for the bottom 25% and the top 25% of the data values,\nexcluding outliers. Graphs that are skewed have the majority\nof data on the high or low side. Skewed graphs indicate that\nthe data isn’t normally distributed.\nFig. 6. TMTI Boxplot.\nFig. 7. TBNI Boxplot.\nThe data distribution for the TTSI, TMTI, and TBNI in\nthe figures (Fig. 5 to 7) is almost normally distributed while\nit is positively skewed for the TASI index (Fig. 4). Moreover,\nany value greater than higher whiskers and less than lower\nwhiskers values is an outlier and is represented in the figure\nas circles beyond the minimum and maximum values. Fig. 4,\nshows reasonable outliers points for TASI, which is expected\nas the closing of TASI is directly impacted by each and every\nlisted company.\nFig. 8 highlights the correlation between the features (High,\nLow, V olume, and Closing Price), which is considered an\nessential step in the feature selection phase of data pre-\nwww.ijacsa.thesai.org 880 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nFig. 8. TASI Correlation Matrix.\nprocessing, especially if the data types of the features are\ncontinuous. As you can see in the figure, there is a high\ncorrelation between volume and the other features.\nFig. 9. TASI Closing Price and V olume.\nTadawul All Share Index (TASI) volume and closing fig-\nures are shown in Fig. 9.\nD. Data Modelling Methodology\nAt first, the historical stock data of earlier days X ∈ RM×F\nconsisting of M periods with F features (previous closing,\nopening, high ,low and volume) is split into a sequence of flat-\ntened 2D Windows xw ∈ RL×F of size M−L, where L is look\nback time interverls. Then the input window is divided into\nnon-overlapping temporal patches of size xp ∈ W ×(F ×2).\nFinally, following the protocol in ViT ,the patches xp ∈\nRW×(F×2) are flattened forming a sequence of embeddings.\nUsing learnable 1D position embeddings, we embed positional\ninformation into the patch embeddings so that all patches\nwithin a given window w are given the same temporal position.\nThis allows the model to determine the temporal positions of\npatches.\nE. Data Prepossessing\nIt is imperative to preprocess data in order to achieve\ngood predictions. The indexes data were checked to determine\nwhether the Tadawul Dataset contained inconsistencies. All\nthe numerical data were normalized, and the missing values\nwere removed. The open, high, low, volume and close prices\nwere used to calculate the features, but information such as\nthe stock code and stock name was omitted since they do not\nmake sense. The following sections describe how the various\npreprocessing steps are implemented.\n1) Splitting the Dataset:The training and test datasets are\nseparated, similar to the ideas presented by [39]. We reserve\napart from the end the training for validation from each time\nseries. This approach is illustrated in the Fig. 10.\nFig. 10. Training, Validation and Testing Dataset Allocation.\n2) Data Normalization:Normalization refers to the process\nof changing the range of values in a set of data. As we use\nprices and volume data, all the stock data must be within a\ntypical value range. In general, machine learning algorithms\nconverge faster or perform better when they are close to\nnormally distributed and/or on a similar scale. Also, in a\nmachine learning algorithm, the activation function, such as\na sigmoid function, has a saturation point after which the\noutputs are constant [40]. As a result, when using model\ncells, the inputs should be normalized before being used. This\nprocess was done using MinMaxScaler methods of the scikit-\nlearn library. When MinMaxScaler is applied to a feature, it\nsubtracts the minimum value from each value in the feature\nand divides the range by the result. Thus, the range of a\nfeature is the difference between the maximum and minimum\nvalues. In this way, MinMaxScaler preserves the shape of the\noriginal distribution. MinMaxScaler normalizes input values to\nbe between [0,1].\n3) Feature Selection: The downloaded data contains sev-\neral features, including stock code, stock name, opening price,\nhigh price, low price, volume and closing price. Aside from\nsome features that may not make any sense, these initial data\nhave a lot of noise. For this reason, the data should be neglected\nwhen it is being trained. Based on [41] using open price, high\nprice, low price, volume and close price, the input features will\nwww.ijacsa.thesai.org 881 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nyield a satisfactory result. Therefore, we have selected the first\nfive features as our input and have neglected irrelevant data like\nstock names and stock codes.\nF . Divided Space\nAt this stage, we apply the concept of the sliding window\nfor framing the dataset. With a window size of 2, we use the\ndata before two days to predict the subsequent day closing.\nThe process is repeated until all data are segmented. Then, the\nframing dataset is further split into patches.\nG. Hyperparameter Selection\nA number of parameters, called hyperparameters, are usu-\nally included in all deep learning models (apart from Na ¨ıve\nBayes) that need to be adjusted to optimize results [42]. The\nvarious hyperparameters used during training are summarized\nin Table III. The AdamW optimizer is used during training\nwith a learning rate of 0.001 and a weight decay of 0.0001.\nWe train the model for 500 epochs with early stopping and\ndropout to prevent overfitting using TensorFlow [43] library.\nTABLE III. V ARIOUS HYPERPARAMETERS USED IN THIS MODEL WITH\nTHEIR VALUES\nHyperparameter Value\nLearning Rate 0.001\nOptimizer AdamW\nBatch size 256\nEpochs 500\nEarly stopping Patience = 70 epochs Monitoring parameter =\nvalidation loss\nLoss Function MSE\nH. Evaluation Metrics\nDeep learning evaluation is categorized into accuracy\nindex, financial index, and error-index [44]. Accuracy and\nfinancial index are widely used for prediction by classifying\ndata (e.g., price direction prediction) and stock trading and\nportfolio management. On the other hand, error terms are\nfrequently used in the evaluation for predicting numeric de-\npendent variables (for instance, exchange rates or stock market\npredictions). The error terms evaluation rules compare the Real\nData Yt+1 and the prediction data Ft+1 using performance\nmetrics: MAE, MSE, MAPE, and RMSE. Detailed information\nabout the measures is provided below:\nMSE is used to assess model performance based on the\naverage error of forecasting. The formula of the MSE is given\nbelow: mX\ni=1\n(Yt+i − Ft+i)2\nm (1)\nRMSE is one of the most commonly used error metrics in\nregression. It is equal to the square root of the MSE. RMSE\nis a measure of how spread out the residuals are. Based on\nthe RMSE formula, it is possible to determine how well the\ndata was focused around the optimal line. The optimal RMSE\nvalue is close to zero.vuut\nmX\ni=1\n(Yt+i − Ft+i)2\nm (2)\nMAPE shows how much error was in the forecast. It\nmeasures how accurate the forecast is. A value of accuracy is\ncalculated by subtracting the actual values from the average\nvalues of the previous period. The concept of MAPE is\nseparated from the measurement level by data conversion.\nMAPE has minimal deviation in practice and cannot tell which\ndirection the error is coming from. Ideally, MAPE should be\nclose to zero. MAPE can be calculated using the following\nequation:\n100\nm\nmX\ni=1\n|Yt+i − Ft+i|\nYt+i\n(3)\nIV. R ESULT AND DISCUSSION\nWe now discuss the results beginning with results on\nmodel optimisation in Section IV-A, model validation in\nSection IV-B, and future stock closing price prediction in\nSection IV-C.\nA. Model Optimisation\nFor this study, we experimented with different batch sizes\nand kept all the other hyper-parameters unchanged. Our pre-\ndictive transformer model is implemented using TensorFlow\nwritten in Python. The study found that training smaller\nbatches yielded better estimates but had a long training process.\nOur findings indicate that models perform better for all the four\nindices until the batch sizes reach around 4, with other batches\nnot delivering significant performance improvements worth the\ntime and effort devoted to estimating them. Fig. 11 depicts the\nfour prediction performance measures MAE, MAPE, MSE,\nand RMSE results, respectively, for the Tadawul All Share\nIndex (TASI), the Banks Index (TBNI), the Materials Index\n(TMTI), and the Telecommunication Services Index (TTSI).\nFig. 11a Mean Absolute Error (MAE) measure. It shows\nthe batch size of the Tadawul All Share Index(TASI) is\nincreased, and we find that forecast measures enhance until it\nreaches its best results at a batch size of 8 at a value of 0.0001,\nwhile it gets fluctuated for the other indices. Taking the MSE\nfor Banks (TBNI) Index as an example, the optimal value for\nthe TBNI index at batch size 2 is 0.0013, and it increases to\n0.1114 at batch size 32. Thereafter, the index decreases with\nbatch size. Similarly, when batch size over batch size exceeds\neight, the Materials Index (TMTI) and Telecommunication\nServices Index (TTSI) also apply. Fig. 11b illustrates the mean\nsquare error (MSE) measure. It shows the batch size of the\nTadawul All Share Index(TASI) is increased, and we find that\nforecast measures enhance until it reaches its best results at a\nbatch size of 8 at a value of 0.0001, while it gets fluctuated\nfor the other indices. Taking the MSE for Banks (TBNI) Index\nas an example, the optimal value for the TBNI index at batch\nsize 2 is 0.0013, and it increases to 0.1114 at batch size 32.\nThereafter, the index decreases with batch size. Similarly, when\nbatch size over batch size exceeds eight, the Materials Index\n(TMTI) and Telecommunication Services Index (TTSI) also\napply.\nFig. 11c, however, presents the root means square error\n(RMSE) of each batch size determined by the indices. For each\nbatch size, each experiment was repeated 500 times(number of\nEpochs). As indicated in the figure, the RMSE is substantially\nhigher for the Banks Index (TBNI), the Materials Index\nwww.ijacsa.thesai.org 882 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\n(a) MAE\n(b) MSE\n(c) RMSE\n(d) MAPE\nFig. 11. Model Performance Versus Varying Batch Sizes.\n(TMTI), and the Telecommunication Services Index (TTSI)\nmainly because of the number of trading days for these indices\ncompared to the Tadawul All Share Index(TASI). Results show\nan evident dependence on the number of trading days. The\nRMSE ranges from 0.1697 to .5409 obtained for the Banks\nIndex (TBNI), from 0.1885 to 0.7638 for Telecommunication\nServices (TTSI), from 0.2361 to 0.5323 for Materials Index\n(TMTI). Fig. 11d shows the Mean Absolute Percentage Error\n(MAPE) for each batch size for the four indices. Despite\noutperforming practically all other indices with a MAPE value\nof 1.681 for batch size 8, there is another batch size where\nTASI does just as well on this accuracy measure. Considering\nthe effects of sampling (trading days) on results, it makes sense\nthat the result would differ.\nB. Model Validation\nFig. 12 to 15 depict the predicted versus actual closing\nprice for the four datasets: Tadawul All Share Index (TASI),\nBanks (TBNI), Telecommunication Services (TTSI), and Mate-\nrials Index (TMTI) indices. These graphs show the best results\nbased on a comparison between the actual and forecasted stock\nprices (close prices). On each chart, orange and blue lines\ndepict the actual values and predicted values, respectively.\nThe plots provide the timelines of the whole dataset. Fig. 12\nplots the closing prices for the TASI dataset for the period\nfrom early 1990s to 2021. Note in the figure that there is a\nrelatively bigger difference between the actual and predicted\nvalues of the stock closing prices in the earlier period of the\ndata. However, the differences get smaller for the later time\nperiods. Overall, all the four figures show a reasonably small\ndifferences between the actual and predicted values, indicating\na good model performance. The results of our study indicate\nthat the proposed model is very effective in analyzing and\ncapturing trends, as well as forecasting them accurately.\nFig. 12. TASI-Predicted vs Actual Closing Price for the whole dataset.\nFig. 13. TBNI-Predicted vs Actual Closing Price.\nC. Predicting Future Stock Closing Prices\nThe next day’s closing price of the selected stock is derived\nfrom the model prediction. Fig. 16 depicts the predicted and\nwww.ijacsa.thesai.org 883 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nFig. 14. TTSI-Predicted vs Actual Closing Price.\nFig. 15. TMTI-Predicted vs Actual Closing Price.\nactual closing for eight trading days starting from 6/17/2021\ntill 6/28/2021 by using the model for the four indices TASI,\nTBNI, TTSI, and TMTI. The figure illustrates that the range\nof relative error fluctuation within the eight working days is\nbetween 0.19 and 0.58 for Tadawul All Share Index (TASI) and\nbetween 4.43 and 6.15 for the Banks index. As a result, the\nmodel accurately predicted the closing price of TASI and Bank\nwith more than 99 and 94 percent, respectively. According to\nthe model, TASI’s closing price, for example, on 2021/06/17,\nwill be 10807.94, while it was actually 10853.12 at the time.\n45.18 points is a relatively small difference. In contrast, the\nrelative error of Telecommunication Services (TTSI) fluctuated\nbetween 0.2, and 2.12, while Materials Index (TMTI) fluctu-\nated between 5.25, and 7.33. Consequently, TMTI and TTSI\nclosing prices were correctly predicted with more than 92,\nand 97 percent, respectively. The proposed model predicts the\nmarket closing price with a better than 90% accuracy, making\nit an exceptionally effective and practical model.\nV. C ONCLUSION AND FUTURE WORK\nWe propose a transformer-based formalization model for\nstock price prediction. A significant influence on our architec-\nture is a vision transformer (ViT) [36] using divided space.\nThe vision transformer (ViT) is among the first attempts to\napply the outstanding performance of Transformers. Using\ntransformer network architectures with split time series into\npatches shows that hidden dynamics can be captured and\npredictions made reasonably. The model was trained using data\nfrom the Saudi Stock Exchange (Tadawul). As a result, we\nwere able to predict the stock price of the TadawulAll Share In-\ndex (TASI), Telecommunication services Index (TTSI), Banks\nIndex (TBNI), and Materials Index (TMTI) with accuracy that\nexceeds 90%.\nWe evaluated the proposed transformer model using four\naccuracy metrics, MAE, MSE, MAPE, and RMSE. We de-\nscribed the experimental results related to model optimisation\n(a) TASI\n(b) TBNI\n(c) TMTI\n(d) TTSI\nFig. 16. Prediction of Unseen Future Stock Closing Price.\nand model validation for all the four datasets. Subsequently,\nwe presented results for the prediction of future stock closing\nprices. We were able to achieve over 90% accuracy compared\nto the best 72% reported in the literature (see Table I).\nFurthermore, the experiments showed that the proposed model\narchitectures that split time series into patches were able to\nidentify the dynamics and complex patterns from irregularities\nin financial time series. Transformer architecture has also been\nshown to identify sudden changes in stock markets, as reflected\nin the results. However, the changes occurring may not always\nwww.ijacsa.thesai.org 884 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\nappear regularly or follow the same cycles each time.\nACKNOWLEDGMENT\nThe experiments reported in this paper were performed on\nthe Aziz supercomputer at King Abdulaziz University.\nREFERENCES\n[1] T. Yigitcanlar, L. Butler, E. Windle, K. C. Desouza, R. Mehmood,\nand J. M. Corchado, “Can Building “Artificially Intelligent Cities”\nSafeguard Humanity from Natural Disasters, Pandemics, and Other\nCatastrophes? An Urban Scholar’s Perspective,”Sensors, vol. 20, no. 10,\np. 2988, may 2020. [Online]. Available: https://www.mdpi.com/1424-\n8220/20/10/2988\n[2] T. Yigitcanlar, N. Kankanamge, M. Regona, A. Maldonado, B. Rowan,\nA. Ryu, K. C. Desouza, J. M. Corchado, R. Mehmood, and R. Y . M.\nLi, “Artificial Intelligence Technologies and Related Urban Planning\nand Development Concepts: How Are They Perceived and Utilized\nin Australia?” Journal of Open Innovation: Technology, Market, and\nComplexity, vol. 6, no. 4, p. 187, dec 2020. [Online]. Available:\nhttps://www.mdpi.com/2199-8531/6/4/187\n[3] E. Alomari, I. Katib, A. Albeshri, and R. Mehmood, “COVID-19:\nDetecting Government Pandemic Measures and Public Concerns from\nTwitter Arabic Data Using Distributed Machine Learning,”International\nJournal of Environmental Research and Public Health, vol. 18, no. 1,\np. 282, jan 2021. [Online]. Available: https://www.mdpi.com/1660-\n4601/18/1/282\n[4] S. Alotaibi, R. Mehmood, I. Katib, O. Rana, and A. Albeshri, “Sehaa:\nA Big Data Analytics Tool for Healthcare Symptoms and Diseases\nDetection Using Twitter, Apache Spark, and Machine Learning,”\nApplied Sciences , vol. 10, no. 4, p. 1398, feb 2020. [Online].\nAvailable: https://www.mdpi.com/2076-3417/10/4/1398\n[5] E. Alomari, I. Katib, A. Albeshri, T. Yigitcanlar, R. Mehmood, and\nA. A. Sa, “Iktishaf+: A Big Data Tool with Automatic Labeling for\nRoad Traffic Social Sensing and Event Detection Using Distributed\nMachine Learning,” Sensors, vol. 21, no. 9, p. 2993, apr 2021.\n[Online]. Available: https://www.mdpi.com/1424-8220/21/9/2993\n[6] M. Aqib, R. Mehmood, A. Alzahrani, I. Katib, and A. Albeshri, “A\nDeep Learning Model to Predict Vehicles Occupancy on Freeways\nfor Traffic Management,” IJCSNS - International Journal of Computer\nScience and Network Security, vol. 18, no. 12, pp. 246–254, 2018.\n[7] S. Usman, R. Mehmood, and I. Katib, “Big data and hpc convergence\nfor smart infrastructures: A review and proposed architecture,” in Smart\nInfrastructure and Applications Foundations for Smarter Cities and\nSocieties. Springer Cham, 2020, pp. 561–586.\n[8] R. Mehmood, F. Alam, N. N. Albogami, I. Katib, A. Albeshri, and\nS. M. Altowaijri, “UTiLearn: A Personalised Ubiquitous Teaching and\nLearning System for Smart Societies,” IEEE Access, vol. 5, pp. 2615–\n2635, 2017.\n[9] M. Aqib, R. Mehmood, A. Alzahrani, and I. Katib, A smart disaster\nmanagement system for future cities using deep learning, gpus, and\nin-memory computing, 2020.\n[10] A. Omar Alkhamisi and R. Mehmood, “An Ensemble Machine and\nDeep Learning Model for Risk Prediction in Aviation Systems,”\nin 2020 6th Conference on Data Science and Machine Learning\nApplications (CDMA). Riyadh, Saudi Arabia: Institute of Electrical\nand Electronics Engineers (IEEE), mar 2020, pp. 54–59. [Online].\nAvailable: https://ieeexplore.ieee.org/abstract/document/9044233\n[11] H. Alotaibi, F. Alsolami, and R. Mehmood, “DNA Profiling: An\nInvestigation of Six Machine Learning Algorithms for Estimating the\nNumber of Contributors in DNA Mixtures,” International Journal of\nAdvanced Computer Science and Applications (IJACSA), vol. 12, pp.\n130–137, 2021.\n[12] R. Mehmood, S. See, I. Katib, and I. Chlamtac, Smart Infrastructure and\nApplications: foundations for smarter cities and societies, R. Mehmood,\nS. See, I. Katib, and I. Chlamtac, Eds. Springer International\nPublishing, Springer Nature Switzerland AG, 2020.\n[13] S. Alotaibi, R. Mehmood, and I. Katib, “Sentiment Analysis of Arabic\nTweets in Smart Cities: A Review of Saudi Dialect,” in 2019 Fourth\nInternational Conference on Fog and Mobile Edge Computing (FMEC).\nIEEE, 2019, pp. 330–335.\n[14] Z. Hu, Y . Zhao, and M. Khushi, “A Survey of Forex and Stock Price\nPrediction Using Deep Learning,” Appl. Syst. Innov., vol. 4, no. 1, p. 9,\nfeb 2021. [Online]. Available: https://www.mdpi.com/2571-5577/4/1/9\n[15] J. Sirignano and R. Cont, “Universal features of price formation\nin financial markets: perspectives from deep learning,” Quant.\nFinanc., vol. 19, no. 9, pp. 1449–1459, 2019. [Online]. Available:\nhttps://www.tandfonline.com/doi/abs/10.1080/14697688.2019.1622295\n[16] E. Guresen, G. Kayakutlu, and T. U. Daim, “Using artificial neural\nnetwork models in stock market index prediction,” Expert Syst.\nAppl., vol. 38, no. 8, pp. 10 389–10 397, 2011. [Online]. Available:\nhttps://www.researchgate.net/publication/220219343\n[17] L. Takeuchi and Y . Lee, “Applying Deep Learning\nto Enhance Momentum Trading Strategies in\nStocks,” Tech. Rep. December 1989, 2013. [Online].\nAvailable: http://cs229.stanford.edu/proj2013/TakeuchiLee-\nApplyingDeepLearningToEnhanceMomentumTradingStrategiesInStocks.pdf\n[18] M. Nikou, G. Mansourfar, and J. Bagherzadeh, “Stock price\nprediction using DEEP learning algorithm and its comparison\nwith machine learning algorithms,” Intell. Syst. Accounting, Financ.\nManag., vol. 26, no. 4, pp. 164–174, 2019. [Online]. Available:\nhttps://www.researchgate.net/publication/337735594\n[19] H. Hewamalage, C. Bergmeir, and K. Bandara, “Recurrent neural net-\nworks for time series forecasting: Current status and future directions,”\nInt. J. Forecast., vol. 37, no. 1, pp. 388–427, 2021.\n[20] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang,\nand X. Yan, “Enhancing the Locality and Breaking the Memory\nBottleneck of Transformer on Time Series Forecasting,” Adv.\nNeural Inf. Process. Syst. , vol. 32, jun 2019. [Online]. Available:\nhttp://arxiv.org/abs/1907.00235\n[21] U. Khandelwal, H. He, P. Qi, and D. Jurafsky, “Sharp Nearby, Fuzzy\nFar Away: How Neural Language Models Use Context,” in Proc. 56th\nAnnu. Meet. Assoc. Comput. Linguist. (Volume 1 Long Pap., vol. 1.\nStroudsburg, PA, USA: Association for Computational Linguistics, may\n2018, pp. 284–294. [Online]. Available: http://arxiv.org/abs/1805.04623\nhttp://aclweb.org/anthology/P18-1027\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdv. Neural Inf. Process. Syst., vol. 2017-Decem. Neural information\nprocessing systems foundation, jun 2017, pp. 5999–6009. [Online].\nAvailable: https://arxiv.org/abs/1706.03762v5\n[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” oct 2020. [Online].\nAvailable: http://arxiv.org/abs/2010.11929\n[24] R. J. Hyndman, “A brief history of forecasting\ncompetitions,” Tech. Rep. 1, 2020. [Online]. Available:\nhttp://monash.edu/business/ebs/research/publications\n[25] S. Selvin, R. Vinayakumar, E. A. Gopalakrishnan, V . K. Menon,\nand K. P. Soman, “Stock price prediction using LSTM, RNN and\nCNN-sliding window model,” 2017 Int. Conf. Adv. Comput. Commun.\nInformatics, ICACCI 2017, vol. 2017-Janua, pp. 1643–1647, 2017.\n[26] S. Hochreiter and J. Schmidhuber, “Long Short-Term\nMemory,” Neural Comput. , vol. 9, no. 8, pp. 1735–1780,\nnov 1997. [Online]. Available: http://direct.mit.edu/neco/article-\npdf/9/8/1735/813796/neco.1997.9.8.1735.pdf\n[27] N. Naik and B. R. Mohan, “Study of stock return predictions using\nrecurrent neural networks with LSTM,” in Commun. Comput. Inf.\nSci., vol. 1000. Springer Verlag, may 2019, pp. 453–459. [Online].\nAvailable: https://link.springer.com/chapter/10.1007/978-3-030-20257-\n6 39\n[28] T. Skehin, M. Crane, and M. Bezbradica, “Day ahead forecasting of\nFAANG stocks using ARIMA, LSTM networks and wavelets,” inCEUR\nWorkshop Proc., vol. 2259, 2018, pp. 186–197.\n[29] D. M. Nelson, A. C. Pereira, and R. A. De Oliveira,\n“Stock market’s price movement prediction with LSTM\nneural networks,” in Proc. Int. Jt. Conf. Neural Networks ,\nvol. 2017-May, 2017, pp. 1419–1426. [Online]. Available:\nhttps://www.researchgate.net/publication/318329563\nwww.ijacsa.thesai.org 885 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 12, No. 12, 2021\n[30] S. Y . Shih, F. K. Sun, and H. yi Lee, “Temporal pattern\nattention for multivariate time series forecasting,” Mach. Learn. ,\nvol. 108, no. 8-9, pp. 1421–1441, sep 2019. [Online]. Available:\nhttps://doi.org/10.1007/s10994-019-05815-0\n[31] G. Lai, W. C. Chang, Y . Yang, and H. Liu, “Modeling long- and\nshort-term temporal patterns with deep neural networks,” Tech. Rep.,\n2018. [Online]. Available: https://doi.org/10.475/123 4\n[32] M. U. Gudelek, S. A. Boluk, and A. M. Ozbayoglu, “A deep learning\nbased stock trading model with 2-D CNN trend detection,” in 2017\nIEEE Symp. Ser. Comput. Intell. IEEE, nov 2017, pp. 1–8. [Online].\nAvailable: http://ieeexplore.ieee.org/document/8285188/\n[33] L. Di Persio and O. Honchar, “Artificial neural networks architectures\nfor stock price prediction: Comparisons and applications,” Tech. Rep.,\n2016.\n[34] D. Povey, H. Hadian, P. Ghahremani, K. Li, and S. Khudanpur, “A\ntime-restricted self-attention layer for ASR,” in ICASSP , IEEE Int. Conf.\nAcoust. Speech Signal Process. - Proc., vol. 2018-April. Institute of\nElectrical and Electronics Engineers Inc., sep 2018, pp. 5874–5878.\n[35] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku,\nand D. Tran, “Image transformer,” Tech. Rep., jul 2018. [Online].\nAvailable: http://proceedings.mlr.press/v80/parmar18a.html\n[36] G. Bertasius, H. Wang, and L. Torresani, “Is Space-Time Attention All\nYou Need for Video Understanding?” feb 2021. [Online]. Available:\nhttp://arxiv.org/abs/2102.05095\n[37] J.-S. Chou, D.-N. Truong, and T.-L. Le, “Interval Forecasting\nof Financial Time Series by Accelerated Particle Swarm-\nOptimized Multi-Output Machine Learning System,” IEEE\nAccess, vol. 8, pp. 14 798–14 808, 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/8955860/\n[38] V . Braverman, R. Ostrovsky, and C. Zaniolo, “Optimal sampling from\nsliding windows,” J. Comput. Syst. Sci., vol. 78, no. 1, pp. 260–272, jan\n2012. [Online]. Available: http://dx.doi.org/10.1016/j.jcss.2011.04.004\nhttps://linkinghub.elsevier.com/retrieve/pii/S0022000011000493\n[39] K. Bandara, C. Bergmeir, and S. Smyl, “Forecasting across time series\ndatabases using recurrent neural networks on groups of similar series:\nA clustering approach,” Expert Syst. Appl., vol. 140, p. 112896, feb\n2020.\n[40] S. Smyl and K. Kuber, “Data Preprocessing and Augmentation\nfor Multiple Short Time Series Forecasting with Recurrent\nNeural Networks,” Tech. Rep., 2016. [Online]. Available:\nhttps://www.researchgate.net/publication/309385800\n[41] K. Chen, Y . Zhou, and F. Dai, “A LSTM-based method for stock\nreturns prediction: A case study of China stock market,” in 2015 IEEE\nInt. Conf. Big Data (Big Data). IEEE, oct 2015, pp. 2823–2824. [On-\nline]. Available: https://ieeexplore.ieee.org/abstract/document/7364089/\nhttp://ieeexplore.ieee.org/document/7364089/\n[42] M. Nabipour, P. Nayyeri, H. Jabani, S. S., and A. Mosavi, “Predicting\nStock Market Trends Using Machine Learning and Deep Learning\nAlgorithms Via Continuous and Binary Data; a Comparative Analysis,”\nIEEE Access, vol. 8, pp. 150 199–150 212, 2020. [Online]. Available:\nhttps://ieeexplore.ieee.org/document/9165760/\n[43] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\nCorrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow,\nA. Harp, G. Irving, M. Isard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur,\nJ. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah,\nM. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,\nV . Vanhoucke, V . Vasudevan, F. Viegas, O. Vinyals, P. Warden,\nM. Wattenberg, M. Wicke, Y . Yu, and X. Zheng, “TensorFlow: Large-\nScale Machine Learning on Heterogeneous Distributed Systems,” mar\n2016. [Online]. Available: http://arxiv.org/abs/1603.04467\n[44] J. Huang, J. Chai, and S. Cho, “Deep learning in finance and\nbanking: A literature review and classification,” Front. Bus. Res.\nChina, vol. 14, no. 1, p. 13, dec 2020. [Online]. Available:\nhttps://fbr.springeropen.com/articles/10.1186/s11782-020-00082-6\nwww.ijacsa.thesai.org 886 | P a g e"
}