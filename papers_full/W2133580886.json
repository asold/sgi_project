{
  "title": "Gradable adjectives, vagueness, and optimal language use: A speaker-oriented model",
  "url": "https://openalex.org/W2133580886",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A5017209475",
      "name": "Ciyang Qing",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076654614",
      "name": "Michael Franke",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1993979041",
    "https://openalex.org/W2094536313",
    "https://openalex.org/W4242017001",
    "https://openalex.org/W2028189805",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2312609093",
    "https://openalex.org/W2397767849",
    "https://openalex.org/W2136944077",
    "https://openalex.org/W1566379068",
    "https://openalex.org/W3015812362",
    "https://openalex.org/W1528435119",
    "https://openalex.org/W1606648952",
    "https://openalex.org/W1607732647",
    "https://openalex.org/W2138046402",
    "https://openalex.org/W4214717370",
    "https://openalex.org/W171980342"
  ],
  "abstract": "&lt;p&gt;This paper addresses two issues that arise in a degree-based approach to the semantics of positive forms of gradable adjectives such as tall in the sentence “John is tall” (e.g., &lt;span&gt;Kennedy &amp;amp; McNally 2005&lt;/span&gt;; &lt;span&gt;Kennedy 2007&lt;/span&gt;): First, how the standard of comparison is contextually determined; Second, why gradable adjectives exhibit the relative-absolute distinction. Combining ideas of previous evolutionary and probabilistic approaches (e.g., &lt;span&gt;Potts 2008&lt;/span&gt;; &lt;span&gt;Franke 2012&lt;/span&gt;; &lt;span&gt;Lassiter 2011&lt;/span&gt;; &lt;span&gt;Lassiter &amp;amp; Goodman 2013&lt;/span&gt;), we propose a new model that makes exact and empirically testable probabilistic predictions about speakers’ use of gradable adjectives and that derives the relative-absolute distinction from considerations of optimal language use. Along the way, we distinguish between vagueness and loose use, and argue that, within our approach, vagueness can be understood as the result of uncertainty about the exact degree distribution within the comparison class.&lt;/p&gt;",
  "full_text": "Proceedings of SALT 24: 23–41, 2014\nGradable adjectives, vagueness, and optimal language use:\nA speaker-oriented model∗\nCiyang Qing\nILLC, University of Amsterdam\nMichael Franke\nSfS, University of Tübingen\nAbstract This paper addresses two issues that arise in a degree-based approach to\nthe semantics of positive forms of gradable adjectives such as tall in the sentence\nJohn is tall (e.g., Kennedy & McNally 2005; Kennedy 2007): First, how the standard\nof comparison is contextually determined; Second, why gradable adjectives exhibit\nthe relative-absolute distinction. Combining ideas of previous evolutionary and\nprobabilistic approaches (e.g., Potts 2008; Franke 2012; Lassiter 2011; Lassiter &\nGoodman 2013), we propose a new model that makes exact and empirically testable\nprobabilistic predictions about speakers’ use of gradable adjectives and that derives\nthe relative-absolute distinction from considerations of optimal language use. Along\nthe way, we distinguish between vagueness and loose use, and argue that, within our\napproach, vagueness can be understood as the result of uncertainty about the exact\ndegree distribution within the comparison class.\nKeywords: gradable adjectives, vagueness, absolute and relative adjectives, evolutionary\nlinguistics, probabilistic models, language production\n1 Introduction\nAccording to the degree-based approach to the semantics of gradable adjectives\n(e.g., Kennedy & McNally 2005; Kennedy 2007), the denotation of a gradable\nadjective such as tall is a function that maps individuals to degrees on an abstract\nscale structure, e.g., JtallK = λx.height(x). The meaning of the positive form of\na gradable adjective, such as tall in the sentence John is tall , is taken to be the\ncomposition of the gradable adjective with a silent morpheme pos.\n(1) Jpos tallK = λx.height(x) ≥θ\nθ is the contextual standard of comparison (also referred to as threshold).\n∗Many thanks to Daniel Lassiter and Noah Goodman for patiently answering our questions about\ntheir work. We are grateful also to Leon Bergen, Itamar Francez, Chris Kennedy, Malte Willer and\nthe other participants of SALT and the Chicago semantics colloquium for insightful discussion.\nThanks also to three anonymous reviewers for very helpful feedback. Michael Franke gratefully\nacknowledges support by NWO-VENI grant 275-80-004.\n©2014 Qing & Franke\nQing & Franke\nIn order to fully understand the meaning and use of gradable adjectives, we need\nto understand how the threshold θ is determined. Clearly, θ is context-dependent\nin the sense that different thresholds are in place for tall when we talk about men\nor trees, i.e., for different so-called comparison classes. But the picture is even\nmore subtle. On the one hand, for many gradable adjectives, θ appears to be\nvague in the sense that there can be uncertainty about θ despite perfect knowledge\nof the comparison class. Such adjectives are called relative adjectives, and their\nvagueness manifests itself in borderline cases. For example, in the expression “a\ntall basketball player,” even though the comparison class, i.e., the set of basketball\nplayers, is explicit, one may still hesitate when deciding whether it is true for a player\nwho is 2m tall. On the other hand, as Kennedy (2007) observes, some gradable\nadjectives, such as full and dry, have positive forms which are arguably not vague.\nFor instance, a glass of water is full only when it is totally ﬁlled with water.1 These\ngradable adjectives are called absolute adjectives. In sum, a complete theory of the\nmeaning and use of gradable adjectives must spell out the contextual resolution of\nthe threshold and in particular correctly predict the difference between absolute and\nrelative adjectives.\nKennedy (2007) argues that conventional lexical semantic properties of gradable\nadjectives and contextual factors play a role in determining standards of comparison.\nHe distinguishes between open and closed scale structures underlying relative and\nabsolute gradable adjectives respectively and proposes the Interpretive Economy\nprinciple to spell out how lexical semantic properties determine the meaning of\nabsolute adjectives. Subsequent evolutionary approaches (Potts 2008; Franke 2012)\nhave tried to derive Interpretive Economy from more basic assumptions about goal-\noriented language use, but these approaches make no concrete predictions about\nactual language use, in particular, the resolution of θ for relative adjectives. This\nis partially compensated by Lassiter & Goodman (2013) who propose a Rational\nSpeech-Act (RSA) model to give precise quantitative predictions about the contextual\ninterpretation of gradable adjectives that are derived from statistical properties of\nthe contextual comparison class. However, since the RSA model is listener-oriented,\na predictive speaker model is missing, as we will argue here. We will also argue that\nthe RSA model might not satisfactorily explain the relative-absolute distinction.\nWe therefore propose a speaker-oriented probabilistic model that is inspired\nby the RSA model but also adopts the idea of an optimal linguistic convention\nfrom evolutionary approaches. The model makes concrete and empirically testable\nprobabilistic predictions about the use of gradable adjectives and, by doing so, ex-\nplains the robust dichotomy between absolute and relative adjectives. The following\n1 In reality we often use these positive forms loosely, e.g., one may use full to describe a glass of water\nthat is not absolutely full. We will discuss the relation between such imprecision and vagueness in\nlater sections.\n24\nSpeaker model for gradable adjectives\nsection 2 introduces our speaker-oriented model (SOM) in more detail. Section 3\nillustrates the predictions of SOM for different scale structures and contextual priors\nand exposes our explanation of the absolute-relative distinction. Section 4 compares\nSOM with the closely related RSA model (Lassiter & Goodman 2013), before we\nconclude by discussing the implications of the new model and some open issues.\n2 Optimal descriptive use of gradable adjectives\nOne common theme in both evolutionary and probabilistic pragmatics is to address\nlanguage use in the broader context of social interactions between goal-oriented lan-\nguage users. We will adopt this functional view as well and focus on the descriptive\nuse of positive forms, whose purpose is to convey information about the degree of\nsome designated individual.2 As a working example, in the following we assume\nthat the possibly implicit question under discussion (QUD) is how tall John is.\nThe semantic problem that we are facing is whether the positive form can be\napplied. Thus we assume that the speaker can only choose between using the\npositive form (u1) and saying nothing (u0). This is certainly a radical simpliﬁcation\nof real life communication. Alternatives include taking into account the antonym,\ncompositional expressions (e.g., neither tall nor short) or explicit measure terms,\nif applicable. Nevertheless, as a ﬁrst step we will adopt this minimalist setting to\nillustrate the main idea. We will discuss some alternatives in later sections.\nAs introduced before, the meaning of a positive form is relative to a contextual\ncomparison class. Another shared feature of recent evolutionary and probabilistic\naccounts is to exploit the statistical information of a comparison class, in the form of\na probability distribution over degrees on the scale, rather than to treat a comparison\nclass simply as a set of individuals. This view provides a speciﬁc explanation\nof how a comparison class inﬂuences the meaning of positive forms, capturing\nthe interaction between our background world knowledge and language use. For\ninstance, when we talk about John’s height, we not only know that we are comparing\nhim against the set of male individuals, but also have some prior world knowledge\nabout the distibution of adult male heights, φ(h). Of course, such prior knowledge\nis usually imprecise, since the comparison class can be implicit, and we usually do\nnot have perfect world knowledge. Nevertheless, in this section we will consider\nthe ideal case in which speaker and listener have an exact prior distribution φ(h)\nand it is common knowledge between them. In the next section we will argue that\nvagueness is closely related to the violation of this assumption in reality.\nLet us summarize the setting so far, we have assumed that the goal is to convey\nthe height of John. There is a commonly known prior distribution of male heights,\n2 Of course, this is not the only purpose of using positive forms. For instance, positive forms can be\nused referentially to help the listener pick up the intended referent in the context (e.g., Franke 2012).\n25\nQing & Franke\nφ(h). In addition, the speaker knows John’s heighth0 but the listener does not. The\nspeaker can either use the positive form tall (u1), or say nothing (u0). Our task is\nto predict how likely the speaker will use the positive form to describe John, i.e.,\nσ(u1 |h0;φ).\nIf the speaker’s probabilistic knowledge of the threshold, Pr(θ), is already\nknown, then a natural production rule, proposed by Lassiter (2011), predicts that the\nprobability that the speaker would use the positive form is the probability that the\nthreshold θ is no greater than h0.\n(2) σ(u1 |h0,Pr) =p(θ ≤h0) =\n∫ h0\n−∞\nPr(θ)dθ\nThis rule can be intuitively understood as that the speaker randomly samples a\nthreshold from the distribution Pr(θ), compares h0 with this θ, and uses the seman-\ntics to decide whether the positive form should be used. The remaining question is,\nof course, how such probabilistic knowledge of the threshold, Pr(θ), is derived from\nthe prior degree distribution φ(h).\nHere we adopt the evolutionary perspective that Pr(θ) is the conventional 3\nlinguistic knowledge formed under evolutionary pressure to efﬁciently communicate\nan individual’s degrees in the comparison classφ(h). Speciﬁcally, for positive forms,\nthe optimization problem that a linguistic community faces is to choose a threshold\nfor the comparison class so that on average the positive form can be used to most\nsuccessfully convey the degree of an individual from that comparison class.\nWe will use a hypothetical literal listener to evalute thecommunicative efﬁency of\neach semantic convention of the threshold θ. Recall that according to the semantics\nJohn is tall is true iff h0 ≥θ. We have two cases to consider. On the one hand, if\nh0 < θ, the speaker can say nothing, since tall is not true in this case. As a result,\nthe literal listener can only use the prior information to infer John’s height, so his\nbelief about John’s height is the same as the prior distribution.\n(3) φ(h |u0,θ) =φ(h)\nIn particular, the probability of him believing in the correct height is φ(h0). On the\nother hand, if h0 ≥θ, the speaker can use tall truthfully to describe John and the\nliteral listener can do an update by conditioning on its truth, which yields a new\ndistribution.\n(4) φ(h |u1,θ) =φ(h |h ≥θ) =\n{ φ(h)∫∞\nθ φ(h)dh if h ≥θ ,\n0 otherwise\n3 Note that such a convention need not be explicit. Many conventions are formed implicitly and\ninductively, via reasonable generalizations from past experiences.\n26\nSpeaker model for gradable adjectives\nIn particular, the probability of the literal listener believing in the correct height\nis φ(h0)\n1−Φ(θ), where Φ(θ) =\n∫θ\n−∞ φ(h)dh is the cumulative probability of the prior\ndistribution φ(h) at θ.\nRecall that we want to measure the communicative success of a threshold in the\nlong run. Here John is taken to be a random individual from the comparison class,\nso the probability of John’s height beingh0 is φ(h0). Hence, on average we have the\nexpected success rate of θ.\n(5) ES(θ) =\n∫ θ\n−∞\nφ(h0)φ(h0|u0,θ)dh0 +\n∫ ∞\nθ\nφ(h0)φ(h0|u1,θ)dh0\nThe left summand corresponds to situations where the speaker has to stay silent\nbecause h0 < θ and the literal listener can only use the prior knowledge, and the\nright summand corresponds to heights to which tall is applicable to induce a more\naccurate belief. Since h0 is a bound variable in the above formula, we will simply\nrewrite it as h.\n(6) ES(θ) =\n∫ θ\n−∞\nφ(h)φ(h|u0,θ)dh +\n∫ ∞\nθ\nφ(h)φ(h|u1,θ)dh\nIf communicative success is all that we care about, then ES(θ) already provides\nus with a measure. In reality, however, language users have other goals such as\nreducing the speaking effort. Following Lassiter & Goodman (2013), also for better\ncomparison (see section 4), we introduce a cost parameter of the positive form, c,\nto capture these other factors, and deﬁne the utility of a threshold as its expected\nsuccess with the cost subtracted.\n(7) U(θ) =ES(θ)−\n∫ ∞\nθ\nφ(h)·cdh\nNote that the integral of cost starts from θ because the positive form is used only\nwhen h ≥θ.\nNow we are ﬁnally able to address where the linguistic knowledge Pr(θ) comes\nfrom. From evolutionary considerations, we predict that the greater the utility of a\nthreshold, U(θ), the more likely that people are going to use it as the convention.\n(8) Pr (θ) ∝ exp(λ ·U(θ))\nHere we use a standard soft-max function to select the threshold sub-optimally\n(Luce 1959; Sutton & Barto 1998). The intuition is that people are more likely\nto select thresholds with higher utilities, but since they are not perfectly rational,\nthey might make mistakes occasionally and end up with less optimal ones. The\nparameter λ ≥0 is used to quantify the degree of rationality, i.e., the extent to which\n27\nQing & Franke\n−3 −2 −1 0 1 2 3\n0.0 0.2 0.4 0.6 0.8\nDegree (θ or h)\nDensity\nPr(θ) , c=0\nρ(h | u1) , c=0\nφ(h)\nPr(θ) , c=2\nρ(h | u1) , c=2\n(a) Pr(θ) and ρ(h |u1)\n−3 −2 −1 0 1 2 3\n0.0 0.2 0.4 0.6 0.8 1.0\nh\nσ(\"tall\" | h)\nc = 2\nc = 0 (b) σ(u1 |h)\nFigure 1 SOM predictions for Gaussian prior N(0,1), with λ = 4.\npeople stick to the strictly optimal threshold. If λ = 0, then Pr(θ) reduces to a\nuniform distribution, meaning that there is no optimality consideration at all. When\nλ →∞, the soft-max function strictly maximizes the utility. We should assume that\nλ takes some value in between for actual language use. For instance, if we have\nthree thresholds whose utilities are 1, 2, and 3, with λ = 4 we will have Pr(θ1) =\nexp(4×1)\nexp(4×1)+exp(4×2)+exp(4×3) = .0003 and similarly Pr(θ2) =.018, Pr(θ3) =.9817. We\ncan see that the optimal threshold θ3 has the greatest probability, and even the least\noptimal threshold θ1 has a small probability to be selected.\nCombining (2), (7) and (8), we have a full production model at our disposal,\nand the corresponding interpretation model for listeners can be derived by applying\nBayes’ rule.\n(9) ρ(h |u1) ∝ φ′(h)·σ(u1 |h,Pr′)\nNote that Pr′(θ) and φ′(h) are correlated the same way as before, but in general\nthe listener’s prior world knowledge φ′(h) need not be the same as the speaker’s.\nNevertheless, as mentioned earlier, in the simplest case, we assume that prior world\nknowledge is in the common ground, i.e., φ′(h) =φ(h).\nFig. 1 shows predictions by the SOM for the Gaussian distribution φ(h) ∼\nN(0,1), with all parameters the same for both the speaker and the listener. We can\nsee from Fig. 1(a) that the SOM predicts that the distribution of the threshold, Pr(θ),\npeaks slightly to the right of the average height, and that the posterior of height after\n28\nSpeaker model for gradable adjectives\nhearing tall, ρ(h |u1), is shifted from the height prior to the right. This corresponds\nwell to our intuition that someone needs to be sufﬁciently taller than average to be\ndescribed as tall. Also, we can see from Fig. 1(b) that the production rule of the\nSOM gives sensible predictions. The probability of describing someone of height\nh as tall, σ(u1 |h), roughly has an S-shaped curve. 4 Note that our model gives\nreasonable predictions even when the cost c = 0.5\nSince Pr(θ) is the core component of the SOM, in the next section we will focus\non Pr(θ) to better illustrate the SOM’s predictions for different prior distributions.\nWe will further show how the SOM accounts for the difference between absolute\nand relative adjectives observed by Kennedy (2007).\n3 The absolute-relative distinction\nAccording to degree semantics (Kennedy & McNally 2005; Kennedy 2007), the\ncrucial difference between absolute and relative adjectives is whether their underlying\nscale structures have accessible endpoints (i.e., lower or upper bounds). Previous\naccounts (e.g., Franke 2012; Lassiter & Goodman 2013) interpret this difference as\na constraint on the type of probability distribution of the degrees. More speciﬁcally,\nprobability distributions on open and closed scales differ in whether there can be\nsigniﬁcant probability mass on the endpoint. For instance, a relative adjective such\nas tall corresponds to a scale that has no maximal element because degrees of height\nare in principle unbounded (and this would be reﬂected via world knowledge in\nany prior even for a contextually ﬁxed comparison class) and thus the probability\nmust asymptotically fall to 0. In contrast, an absolute adjective such as open is\nassociated with a scale that has a maximal element, and the occurrence probability\nof maximally open objects is usually non-negligible.\nWe adopt this view and apply the SOM to various distributions within the beta\ndistribution family, which not only has a wide range of distributions that help us\nexplore the exact boundary between absolute and relative adjectives, but also has\nnice closure properties that facilitate analytic derivations. A beta distribution is\ndeﬁned on [0,1] and has two positive shape parameters α,β. Its densitiy function is\ndeﬁned as follows.\n(10) φ(d;α,β) =Kdα−1(1 −d)β−1\n4 For c = 2 the curve is shifted far to the right, so its shape is not as obvious. It will be clear later that\nthis value is too large, but we use it mainly to illustrate the realm of possible predictions.\n5 Given the prevalence of gradable adjectives, we do not expect it to be very costly to utter them. In\nparticular, the cost should not be the main factor that drives model prediction. Thus we take c = 0 as\nan approximation of the relatively small cost of the positive form in the next section when we discuss\nthe absolute-relative distinction.\n29\nQing & Franke\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4 5 6\nh\nDensity\nTotally open: Beta(3,7)\nUpper closed: Beta(5,0.9)\nLower closed: Beta(1,5)\nTotally closed: Beta(1,1)\nTotally closed: Beta(0.5,0.5)\n(a) Beta(α,β)\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4\nh\nDensity\nTotally Closed: Beta(0.7,1)\nTotally Closed: Beta(0.4,1)\nTotally Closed: Beta(0.3,1)\nTotally Closed: Beta(0.1,1) (b) Beta(α,1)\nFigure 2 Correspondence between beta distributions and scale structures.\nK = 1/B(α,β) is a normalization constant. Indeed, there is a tight correspondence\nbetween parameters of the beta distribution and scale types (Fig. 2). If α,β > 1,\nboth endpoints have zero probability mass, which corresponds to open scales. If\nα > 1,β ≤1, the lower endpoint has zero probability mass and the upper endpoint\nhas nonzero probability mass, which corresponds to upper closed scales. Similarly,\nα ≤1,β > 1 corresponds to lower closed scales. Finally, if α,β ≤1, both endpoints\nhave nonzero probability mass, which corresponds to totally closed scales.\nThe remainder of this section is dedicated to demonstrating that the SOM predicts\nthe following correspondence between endpoint probability mass and the optimal\nthreshold:6\n(11) (i) If there is a sufﬁcient amount of probability mass at the upper end-\npoint, then the maximal threshold is always optimal and so we obtain\na maximum-standard reading, as in The line is straight, which is true,\nstrictly speaking, only when the line is completely straight.\n(ii) If (i) is not the case and the probability mass at the lower endpoint is\nsufﬁciently larger than elsewhere, then the non-minimal threshold7 is\noptimal and so we obtain a minimum-standard reading, as in The line is\n6 For now we always assume c = 0, and in the end we will show that this assumption is not crucial.\n7 By non-minimal threshold we mean the one that corresponds to the non-minimal reading. In discrete\ncases this simply means the second minimal degree. In continuous scales, it means the utility function\nis decreasing on non-minimal degrees.\n30\nSpeaker model for gradable adjectives\nbent, which is true, strictly speaking, as soon as the line is not perfectly\nstraight.\n(iii) Otherwise the optimal threshold is highly sensitive to φ(h) and we\nobtain a relative reading.\nLet us ﬁrst brieﬂy explain how this correspondence correctly predicts the differ-\nence between absolute and relative adjectives observed by Kennedy (2007). Recall\nthat when we formulate the model in the previous section, we assume that the prior\ndistribution φ(h) is the speaker’s exact knowledge about the comparison class. As\nalready pointed out there, in reality, besides the fact that comparison classes are\noften implicit, the speaker almost always has uncertainty about the exact distribution\nφ(h) due to imperfect world knowledge. Typically, speakers only know the type of\nprobability distribution for each adjective; they do not know the exact distribution.\nEven if the speaker directly observes the set of entities that forms the comparison\nclass, due to noisy perception and the unavoidable blend from past experiences,\nhis knowledge of φ(h) will be imprecise. Nevertheless, in the case of absolute\nadjectives, the speaker does not need to know the exact φ(h) in order to know where\nthe optimal threshold is. According to (i) and (ii), as long as there is sufﬁcient prob-\nability mass at either endpoint, the optimal threshold will be there.8 This stability\nof optimal threshold explains why absolute adjectives are semantically not vague.\nIn contrast, for open-scale adjectives, the optimal threshold is highly sensitive to\nφ(h) according to (iii) and the speaker cannot be sure where the optimal threshold\nis. Thus, the vagueness of relative adjectives is the result of such sensitivity of the\noptimal threshold when there is uncertainty about the exact prior.\nIn the following we try to show that (11) holds through representative examples\nin the beta distribution family. We start from the relatively simple part, (iii). For open\nscales, the corresponding beta distribution has parameters α,β > 1. For example,\nBeta(3,7) is a distribution on an open scale that roughly corresponds to cheap and\nexpensive.9 Fig. 3(a) shows the SOM’s prediction ofPr(θ). Indeed, we can see that\nas the prior probability mass shifts to the left in Fig. 3(a), the optimal threshold also\nshifts to the left. (Compare the red and blue lines with those in Fig. 1(a).) We can\nalso see that the optimal threshold is sensitive to the cost. Higher cost will drive the\noptimal threshold to a greater degree (recall that the ordering for cheap is reversed).\nLet’s look at closed scales next. We will ﬁrst focus on cases where β = 1,\n8 The speaker actually chooses the threshold sub-optimally via soft-max, reﬂecting the loose use of\nlanguage, but if they are forced to, they can conﬁrm that semantically the threshold is not vague\nbecause it is always at either endpoint.\n9 Technically, we use Beta(3,7) fordegrees of expense and Beta(7,3) for degrees of cheapness, as they\nhave inverse orderings on the degrees, and put the predictions of both models in the same plot. The\npredictions do not change much for a prior distribution that has small non-zero probability mass at 0\nused by Lassiter & Goodman (2013).\n31\nQing & Franke\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4 5 6\nDegree (θ or h)\nDensity\nPr(θcheap | c=2)\nPr(θcheap | c=0)\nφ(h)\nPr(θexpensive | c=2)\nPr(θexpensive | c=0)\n(a) Pr(θ) predicted by the SOM\n0.0 0.2 0.4 0.6 0.8 1.0\n0.5 1.0 1.5 2.0\nθ\nDensity\nBeta(0.7,1)\nBeta(0.4,1)\nBeta(0.3,1)\nBeta(0.1,1) (b) Pr(θ) predicted by the SOM, with\nc = 0\nFigure 3 Predictions by the SOM for Beta(α,β), with λ = 4.\nas other cases will be straightforward thereafter. From (10) it can be proved that\nBeta(α,1) has density function φ(h;α,1) =αhα−1, and speciﬁcally the probability\nmass at the end point h = 1 is α. Fig. 3(b) shows the SOM’s prediction ofPr(θ). We\ncan see that when α is high (0.4 or 0.7), Pr(θ) is always increasing, which means the\nupper endpoint is always the optimal standard. This corresponds to (i). Meanwhile,\nwhen α is low (0.3 or 0.1), Pr(θ) is always decreasing on (0,1], which means that a\nnon-minimal standard is always optimal.10 This corresponds to (ii). In fact, it can be\nshown that α = 1\n3 is when a “phase transition” takes place, i.e.,Pr(θ) is increasing\nwhen α > 1\n3 , uniform when α = 1\n3 and decreasing when α < 1\n3 . Hence we know that\noptimal thresholds for absolute adjectives are stable under a wide range of priors,\nwhich means slight uncertainty in φ(h) will not affect the speaker’s knowledge about\nthe optimal standard.\nFig. 4 further shows the robustness of the SOM’s predictions with respect to\ncosts. We already know that a higher cost will drive θ to the right (greater degree),\nwhich means higher costs will not affect maximal readings. Hence we only need to\nfocus on thresholds for non-minimal readings such as Beta(0.3,1) and Beta(0.1,1).\nWe can see that when the cost is relatively low for the prior, the prediction is almost\nunaffected, as shown in Fig. 4(a) for Beta(0.1,1). Meanwhile, when the cost becomes\n10 The plot does not show the result of θ = 0. Note that θ = 0 means the positive form is always true,\nwhich is effectively the same as staying silent all the time. Thus θ = 0 has very low utility and can\nnever be optimal if c > 0.\n32\nSpeaker model for gradable adjectives\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.5 1.0 1.5 2.0 2.5\nθ\nDensity\nBeta(0.7,1)\nBeta(0.4,1)\nBeta(0.3,1)\nBeta(0.1,1)\n(a) c = 0.2\n0.0 0.2 0.4 0.6 0.8 1.0\n0 2 4 6\nθ\nDensity\nBeta(0.7,1)\nBeta(0.4,1)\nBeta(0.3,1)\nBeta(0.1,1) (b) c = 2\nFigure 4 Predictions by the SOM for Beta(α,1), with λ = 4.\nrelatively high, the upper endpoint also becomes a local optimum, as reﬂected by\nthe v-shaped curves in Fig. 4(a) for Beta(0.3,1) and in Fig. 4(b) for Beta(0.1,1).11\nNevertheless, the maximal threshold is only a local optimum. In fact, it can be proved\nthat for α < 1\n3 , U(θ) always goes to inﬁnity when θ approaches 0 (remember that\nθ = 0 is excluded), regardless of the cost, so the non-minimal threshold is always\nglobally optimal. Finally, when β < 1, the density φ(h) goes to inﬁnity at the upper\nendpoint, which means Pr(θ) will be driven even faster to the upper endpoint. So,\nagain we obtain the maximal threshold as predicted by (i) and this prediction is also\nrobust with respect to the cost parameter.\nIn sum, we have shown that the SOM correctly predicts the difference between\nrelative and absolute adjectives. We interpreted vagueness as the stability of the\noptimal threshold under uncertainty about the exact prior distribution of degrees in\nthe comparison class. We also illustrated how degree scales, by constraining the type\nof priors, inﬂuence speaker’s knowledge about the optimal threshold. Open scales\nby deﬁnition do not have probability mass on endpoints, thus optimal thresholds are\nsensitive to the exact prior. As a result, the speaker cannot be sure about where they\nare when she is uncertain about the exact prior. This accounts for the vagueness of\nrelative adjectives. In contrast, closed scales can constrain the priors such that there\nis sufﬁcient probability at either end point, which is enough for the speaker to be\ncertain about the optimal threshold, even if she is not sure about the exact prior. This\n11 In fact, it is also true for Beta(0.3,1) in Fig. 4(b), but the turning point is too close to 0 to be observable.\n33\nQing & Franke\nexplains why absolute adjectives are semantically not vague.\nIt should be noted here that prior distributions are not only constrained by scale\ntypes, but also by general world knowledge and immediate contextual informa-\ntion. Scale types might provide default priors (abstracted away from general world\nknowledge) that can be seen as part of the lexical properties of gradable adjec-\ntives. Nevertheless, when the speaker has more speciﬁc world knowledge about the\ncomparison class or even directly observes it from the immediate context, she will\nknow more about the degree distribution and adjust the prior accordingly. Different\ngradable adjectives can have constraints with different strengths. Open scale adjec-\ntives have no endpoints in principle so the prior distribution is more contextually\nvariable. Closed scale adjectives, on the other hand, will usually be associated\nwith the knowledge that there is in principle a lower or an upper bound, but still\nthe corresponding priors that a speaker entertains at any given moment can still\nbe inﬂuenced by world knowledge ( full for glasses of wine) or direct contextual\ninformation (see experimental evidence by Solt & Gotzner 2012; Qing & Franke\n2014).\n4 Comparison to the Rational Speech-Act (RSA) model\nIn previous sections we introduced our model and illustrated how it predicts the\nrelative-absolute distinction. In this section we will compare it with the closely\nrelated Rational Speech-Act (RSA) model proposed by Lassiter & Goodman (2013).\nThe RSA model provides a probabilistic account of the semantics of gradable\nadjectives, based on a series of work in Bayesian pragmatics (e.g., Frank & Goodman\n2012; Goodman & Stuhlmüller 2013). The RSA model improves on previous\nprobabilistic approaches in that it provides precise quantitative predictions about\n(the listener’s beliefs about) the probability distribution of the threshold.\nOur speaker-oriented model shares similar assumptions and formalisms with the\nRSA model, but is also different from it in both conceptual and technical aspects.\nBelow, we will explain the RSA model12 in detail and focus on the main differences.\nThe RSA model is based on the same scenario of descriptive language use and has\nthe same literal listener component, i.e., (3) and (4).\nThe difference mainly resides in the speaker component. In the RSA model, the\nspeaker is assumed to have an exact threshold θ saturated. In addition, when the\npositive form is applicable, i.e., h0 ≥θ, the speaker needs to choose between the\npositive form and staying silent, according to the informativity and cost of either\n12 To make comparison easier, the version of the RSA model presented here is slightly different from the\noriginal formulation in that we do not consider antonyms. This allows us to do analytic derivations\nthat simplify the computation of the posteriors. The obtained predictions are not crucially different\nfrom the original version and we will further discuss this modiﬁcation in the end.\n34\nSpeaker model for gradable adjectives\nchoice. Technically, the speaker applies the soft-max function according to the\nutilities deﬁned below.\n(12) σ(u |h0,θ) ∝ exp(λU(u,h0,θ)) =exp(λ(Info(u,h0,θ)−Cost(u)))\nInformativity is measured as the negative surprisal of the literal listener’s updated\nbelief about h0.\n(13) Info (u,h0,θ) =−log(1/φ(h0 |u,θ)) =logφ(h0 |u,θ)\nAssuming the costs for silence and tall are 0 and c, respectively, from (3), (4), (12),\nand (13), we can write down the probability of uttering tall explicitly.\nσ(u1 |h0,θ) = exp(λU(u1,h0,θ))\nexp(λU(u1,h0,θ))+ exp(λU(u0,h0,θ))(14)\n=\nexp(λ(log φ(h0)∫∞\nθ φ(h)dh −c))\nexp(λ(log φ(h0)∫∞\nθ φ(h)dh −c))+ exp(λ(logφ(h0)−0))\n= 1\n1 +eλc ·(\n∫∞\nθ φ(h)dh)λ if h0 ≥θ, otherwise 0 .\nThe predictions of the rule are shown in Fig. 5(a). We can see that σ(u1 |h0,θ)\nincreases as θ increases, as long as θ ≤h0 holds (so that the positive form is\nsemantically true). Another important feature is that once θ is ﬁxed, σ(u1 |h0,θ) is\nthe same for all h0 ≥θ. This shows that the RSA model does not have a convincing\nsay on actual production of positive forms. First, it assumes that the speaker knows\nthe exact threshold for the positive form, which is arguably not the case in reality.\nSecond, even if the speaker does know (perhaps implicitly) the exact threshold θ,\nthe RSA model would predict that the speaker uses the positive form with equal\nlikelihood no matter what the actual degree h0 is (as long as h0 ≥θ). This does\nnot capture the actual production of positive forms, i.e., the greater the degree, the\nmore likely that the positive form will be used (e.g., Schmidt, Goodman, Barner &\nTenenbaum 2009; Solt & Gotzner 2012; Qing & Franke 2014).\nIn contrast, the SOM gives precise predictions about the speaker’s uncertainty of\nthe threshold and directly predicts the production probability of the positive form for\neach degree, which captures our intuition and can be further empirically tested.\nNext we consider the derivation of the probability distribution of the threshold,\nwhich according to the RSA model is on the actual/pragmatic listener’s level. Upon\nhearing the positive form, the pragmatic listener tries to make a joint inference about\nthe threshold as well as the true value by using Bayes’ rule.\n(15) ρ(h,θ |u1) ∝ φ(h)·Pr(θ)·σ(u1 |h,θ) = φ(h)·Pr(θ)\n1 +eλc(\n∫∞\nθ φ(h)dh)λ\n35\nQing & Franke\n−3 −2 −1 0 1 2 3\n0.0 0.2 0.4 0.6 0.8 1.0\nθ\nσ(\"tall\" | h0 , θ)\nh0 = 2\nh0 = 1\nh0 = 0.5\n(a) σ(u1 |h0,θ)\n−3 −2 −1 0 1 2 3\n0.0 0.2 0.4 0.6 0.8\nDegree (θ or h)\nDensity\nφ(h)\nPr(θ)\nρ(θ |\"tall\")\nρ(h |\"tall\") (b) Posteriors of θ and h\nFigure 5 RSA predictions for Gaussian distribution N(0,1), with λ = 4,c = 2.\nPr(θ) is the prior linguistic knowledge about the threshold θ.\nIn order to obtain the posterior distribution of θ, we can marginalize over h.\n(16) ρ(θ |u1) ∝\n∫ ∞\n−∞\nφ(h)·Pr(θ)·σ(u1 |h,θ)dh = Pr(θ)·\n∫∞\nθ φ(h)dh\n1 +eλc ·(\n∫∞\nθ φ(h)dh)λ\nSimilarly, we can derive the posterior distribution ofh.\nρ(h |u1) ∝\n∫ ∞\n−∞\nφ(h)Pr(θ)σ(u1 |h,θ)dθ(17)\n= φ(h)\n∫ h\n−∞\nPr(θ)\n1 +eλc ·(\n∫∞\nθ φ(h)dh)λ dθ\nIf we take the prior of height φ(h) to be the normal distribution N(0,1) and use\nuniform threshold prior Pr(θ), we can see from Fig. 5(b) that the RSA model’s\nprediction of the threshold and the height posterior with λ = 4, c = 2. While the\nRSA model’s posteriors of threshold and degree look very similar to the SOM’s\nprediction in Fig. 1(a), we will argue below that the RSA model has certain features\nthat can be problematic, especially for its account of the absolute-relative distinction.\nSensitivity to the cost parameterFig. 6 shows the predictions of the RSA model\nfor Gaussian and uniform distributions. We can see that when the costc = 2, the RSA\n36\nSpeaker model for gradable adjectives\n−3 −2 −1 0 1 2 3\n0.0 0.2 0.4 0.6 0.8\nDegree (θ or h)\nDensity\nρ(θ |\"tall\", c=2)\nρ(θ |\"tall\", c=1)\nρ(θ |\"tall\", c=0.5)\nρ(θ |\"tall\", c=0)\nφ(h)\n(a) “Tall”: Normal prior N(0,1)\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4 5\nDegree (θ or h)\nDensity\nρ(θ |\"full\", c=2)\nρ(θ |\"full\", c=1)\nρ(θ |\"full\", c=0.5)\nρ(θ |\"full\", c=0)\nφ(h) (b) “Full”: Uniform prior Beta(1,1)\nFigure 6 RSA predictions with λ = 4 and various costs.\nmodel gives reasonable threshold posteriors (blue lines); Lassiter & Goodman (2013)\nargue that this captures the relative-absolute distinction. When c is small, however,\nthreshold posteriors both shift to the left. In particular the threshold posterior of the\nabsolute adjective (Fig. 6(b)) soon moves away from the endpoint. This means that\npositive forms have to require enough production effort in order for the RSA model\nto predict correctly, especially for its account of the absolute adjectives. Whether\nthis assumption is warranted is, of course, an empirical question and depends on a\nprecise theory about production effort. Nonetheless, the above feature also implies\nthat if it took only little effort to utter positive forms, even absolute adjectives would\nreceive weak meanings. This seems rather counter-intuitive. In contrast, the SOM\ndoes not crucially rely on the cost parameter for its predictions. In particular, the\nabsolute-relative distinction does not rely on speciﬁc values of production effort.\nSensitivity to the degree priorFig. 7 shows the RSA model’s predictions for\nvarious closed scale priors. We can see that the maximum of the threshold posterior\nalways shifts as the degree prior changes. This calls for an explanation of the\ncontextual invariance of absolute adjectives. Speciﬁcally, given that the maximum\nof its threshold posterior is sensitive to the prior and is never actually the maximal\nor minimal degree, what in the RSA model’s prediction corresponds to the stable\nmaximal/minimal reading of absolute adjectives?\n37\nQing & Franke\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4\nh\nDensity\nTotally Closed: Beta(0.7,1)\nTotally Closed: Beta(0.4,1)\nTotally Closed: Beta(0.3,1)\nTotally Closed: Beta(0.1,1)\n(a) Beta(α,1)\n0.0 0.2 0.4 0.6 0.8 1.0\n0 1 2 3 4\nθ\nDensity\nBeta(0.7,1)\nBeta(0.4,1)\nBeta(0.3,1)\nBeta(0.1,1) (b) RSA model’s prediction\nFigure 7 Predictions by the RSA for Beta(α,1), with λ = 4,c = 2.\nHere we want to emphasize the difference between vagueness and loose talk.\nIndeed, people often use absolute adjectives in ways that do not conform to maximal\nor minimal readings, e.g., people would use full for a glass of water even when there\nis still some room left. It may seem that the absolute-relative distinction is blurry\nafter all. While this may be true for actual language use, there remain rather distinct\nsemantic intuitions about absolute and relative adjectives, i.e., for absolute adjectives\nwe can give a precise threshold with conﬁdence if forced to, which is impossible for\nrelative adjectives. This semantic aspect of vagueness also needs explanation, even\nif the probabilistic use of positive forms has been accounted for. It is unclear how\nthe RSA model will address this issue.\nIn contrast, the SOM predicts stable optimal endpoint thresholds for absolute ad-\njectives under reasonably small changes in priors. Only when the prior signiﬁcantly\ndeviates from typical cases will the optimal threshold change. Essentially, the SOM\npredicts two sources of the uncertainty about positive forms. First, the threshold\nis usually not optimized perfectly due to bounded rationality. This predicts the\nprobabilistic use of both types of adjectives in reality. Second, vagueness is also the\nresult of uncertainty about the prior degree distribution from imperfect knowledge\nabout the comparison class. In this respect, absolute and relative adjectives exhibit\ndifferent properties in terms of the stability of the optimal threshold. This accounts\nfor their difference in context-variability.\n38\nSpeaker model for gradable adjectives\nCounter-intuitive metalinguistic effects Finally, the RSA model predicts rather\ncounter-intuitive metalinguistic effects in a scenario that only involves purely de-\nscriptive language use. Suppose people are talking about John and somebody says\nJohn is tall. You have never met John, but you have some world knowledge about\nthe distribution of adult male height, φ(h). According to the RSA model, prior to\nthe utterance you know nothing about what tall means for an adult male (even with\nthe world knowledge you have), but afterwards you both know what tall means and\ngain some information about John’s height.\nHowever, it seems more plausible to say that you are able to gain some informa-\ntion about John’s height because you already know whattall roughly means for men\neven before the utterance, and you apply this knowledge afterwards to infer John’s\nheight.13 Also, if this utterance is all you get and you receive no extra information\nabout John’s height, then intuitively it seems that after the utterance you know\nnothing more about what tall means beyond what you have anticipated from the\nprior world knowledge. The SOM adopts this perspective in its predictions.\nSemantic vs pragmatic optimality So far we have seen how the SOM’s predic-\ntions differ from the RSA model in several respects. Here we will further discuss\nthe conceptual relation between the two models. We will argue that the SOM,\nwhich stems from previous evolutionary approaches, emphasizes considerations of\nsemantic optimality, while the RSA model focuses on pragmatic optimality.\nTo be precise, we distinguish between three senses of pragmatics: (1) language\nuse in general, (2) how contexts affect meaning, and (3) how language can be used\nbeyond the conventional/literal meaning. The following discussion is in the sense of\n(3), as both models deal with (1) and (2).\nThe SOM is a semantic approach with respect to (3) in that it treats the prob-\nability distribution of the threshold as conventional semantic knowledge within a\nlinguistic community. The probability of each threshold depends on the utility of\nthe corresponding semantic system. Thus for each threshold, in order to evaluate its\ncommunicative success, we introduce speakers and listeners who use the positive\nform according to the semantics and measure the expected success in conveying\nthe true degree. The optimization problem is for the whole community to select\nthresholds that form good semantic systems.\nThe RSA model, on the other hand, is pragmatic in that its speaker faces a choice\nbetween several alternatives. As a result, the use of one expression depends on not\nonly its semantics, but also its relation to the alternatives. Such a model has been\nhighly successful for typical pragmatic phenomena such as scalar implicature.\n13 If such knowledge does not exist beforehand, then if you ask the question Is John tall? before anyone\nelse uses tall, it will have an implausibly weak meaning.\n39\nQing & Franke\nWe believe that the two approaches should be complementary rather than con-\ntradictory, and ideally they should be integrated to fully capture the complexity of\nmeaning and use. The challenge is that it is often unclear which aspect weighs\nheaviest for any particular phenomenon we are interested in. For instance, we have\nonly considered two possible utterances in the current setting for simplicity, and we\nhave seen that the semantic approach of the SOM provides reasonable predictions.\nHowever, when we are to extend the model to allow for more alternatives, such as\nantonyms and compositional expressions, we can either hold on to a fully semantic\napproach, i.e., optimizing over all possible semantic conventions, or use the seman-\ntic approach to derive the base semantics of each alternative and put them into a\npragmatic model to compete with each other. We leave more elaborate discussions\nfor future work.\n5 Conclusion\nIn this paper, we combined previous evolutionary and probabilistic approaches to\nthe meaning of gradable adjectives and proposed a new speaker-oriented model. The\ncontributions are the following. First, we addressed how the probability distribution\nof conventional thresholds is contextually determined. Second, we added a fully\npredictive speaker model and illustrated that it makes plausible predictions under\nvarious parameters. Third, we distinguished between vagueness and loose talk to\naddress the difference between relative and absolute adjectives’ interpretation. In\nparticular, we argued that an important source of vagueness is the uncertainty about\nthe exact degree distribution within the comparison class.\nAlthough we have been critical to the RSA model, it is clear that our approach\nowes a lot to it. We believe that SOM and RSA focus on different aspects of positive\nforms of gradable adjectives and that more work is necessary to further integrate the\ntwo approaches. This becomes especially relevant when trying to test the predictions\nof probabilistic models of this kind experimentally (for a ﬁrst attempt, see Qing &\nFranke 2014).\nReferences\nFrank, Michael C. & Noah D. Goodman. 2012. Predicting pragmatic reasoning in\nlanguage games. Science 336(6084). 998. doi:10.1126/science.1218633.\nFranke, Michael. 2012. On scales, salience & referential language use. In Maria\nAloni, Floris Roelofsen & Katrin Schulz (eds.), Amsterdam Colloquium 2011\nLecture Notes in Computer Science, 311–320. Springer.\nGoodman, Noah D. & Andreas Stuhlmüller. 2013. Knowledge and implicature:\n40\nSpeaker model for gradable adjectives\nModeling lanuage understanding as social cognition. Topics in Cognitive Science\n5. 173–184.\nKennedy, Christopher. 2007. Vagueness and grammar: The semantics of relative and\nabsolute gradable adjectives. Linguistics and Philosophy 30. 1–45.\nKennedy, Christopher & Louise McNally. 2005. Scale structure, degree modiﬁcation,\nand the semantics of gradable predicates. Language 81(2). 345–381.\nLassiter, Daniel. 2011. Vagueness as probabilistic linguistic knowledge. In Rick\nNouwen, Robert van Rooij, Uli Sauerland & Hans-Christian Schmitz (eds.),\nVagueness in Communication, 127–150. Springer.\nLassiter, Daniel & Noah D. Goodman. 2013. Context, scale structure, and statistics\nin the interpretation of positive-form adjectives. In Todd Snider (ed.), Semantics\nand Linguistic Theory (SALT) 23, 587–610. CLC Publications.\nLuce, Duncan R. 1959. Individual Choice Behavior: A Theoretical Analysis. New\nYork: Wiley.\nPotts, Christopher. 2008. Interpretive Economy, Schelling points, and evolutionary\nstability. Manuscript, UMass Amherst.\nQing, Ciyang & Michael Franke. 2014. Meaning and use of gradable adjectives:\nFormal modeling meets empirical data. In Paul Bello, Marcello Guarini, Marjorie\nMcShane & Brian Scassellati (eds.), Cognitive Science Society (CogSci 2014),\nvol. 36, 1204–1209. Curran Associates, Inc.\nSchmidt, Lauren A., Noah D. Goodman, David Barner & Joshua B. Tenenbaum.\n2009. How tall is tall? compositionality, statistics, and gradable adjectives.\nIn Niels Taatgen, Hedderik van Rijn, Lambert Schomaker & John Nerbonne\n(eds.), Cognitive Science Society (CogSci 2009) , vol. 31, 3151–3156. Curran\nAssociates, Inc.\nSolt, Stephanie & Nicole Gotzner. 2012. Experimenting with degree. In Anca\nChereches (ed.), Semantics and Linguistic Theory (SALT) 22, 166–187. CLC\nPublications.\nSutton, Richard S. & Andrew G. Barto. 1998. Reinforcement learning: An introduc-\ntion. Cambridge: MIT Press.\nCiyang Qing\nInstitute for Logic, Language and Computation\nUniversiteit van Amsterdam\nP.O. Box 94242\n1090 GE Amsterdam\nThe Netherlands\nqciyang@gmail.com\nMichael Franke\nSeminar für Sprachwissenschaft\nEberhard Karls Universität Tübingen\nWilhelmstraße 19\n72072 Tübingen\nGermany\nmchfranke@gmail.com\n41",
  "topic": "Vagueness",
  "concepts": [
    {
      "name": "Vagueness",
      "score": 0.8247807621955872
    },
    {
      "name": "Span (engineering)",
      "score": 0.6696416735649109
    },
    {
      "name": "Mathematics",
      "score": 0.5151420831680298
    },
    {
      "name": "Probabilistic logic",
      "score": 0.4603734612464905
    },
    {
      "name": "Class (philosophy)",
      "score": 0.43776506185531616
    },
    {
      "name": "Sentence",
      "score": 0.4190966784954071
    },
    {
      "name": "Linguistics",
      "score": 0.3581719398498535
    },
    {
      "name": "Psychology",
      "score": 0.33385777473449707
    },
    {
      "name": "Statistics",
      "score": 0.22712194919586182
    },
    {
      "name": "Computer science",
      "score": 0.22347202897071838
    },
    {
      "name": "Philosophy",
      "score": 0.16644877195358276
    },
    {
      "name": "Artificial intelligence",
      "score": 0.14262831211090088
    },
    {
      "name": "Fuzzy logic",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 110
}