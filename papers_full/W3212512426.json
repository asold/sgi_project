{
  "title": "ArabicTransformer: Efficient Large Arabic Language Model with Funnel Transformer and ELECTRA Objective",
  "url": "https://openalex.org/W3212512426",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3166846399",
      "name": "Sultan Alrowili",
      "affiliations": [
        "University of Delaware"
      ]
    },
    {
      "id": "https://openalex.org/A2109851914",
      "name": "Vijay Shanker",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2784823820",
    "https://openalex.org/W2917085252",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3155561744",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W4297823766",
    "https://openalex.org/W3156703103",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W2963574252",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3153540814",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W4297663785",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Pre-training Transformer-based models such as BERT and ELECTRA on a collection of Arabic corpora, demonstrated by both AraBERT and AraELECTRA, shows an impressive result on downstream tasks. However, pre-training Transformer-based language models is computationally expensive, especially for large-scale models. Recently, Funnel Transformer has addressed the sequential redundancy inside Transformer architecture by compressing the sequence of hidden states, leading to a significant reduction in the pre-training cost. This paper empirically studies the performance and efficiency of building an Arabic language model with Funnel Transformer and ELECTRA objective. We find that our model achieves state-of-the-art results on several Arabic downstream tasks despite using less computational resources compared to other BERT-based models.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1255–1261\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n1255\nArabicTransformer: Efﬁcient Large Arabic Language Model with Funnel\nTransformer and ELECTRA Objective\nSultan Alrowili\nUniversity of Delaware\nNewark, Delaware, USA\nalrowili@udel.edu\nK. Vijay-Shanker\nUniversity of Delaware\nNewark, Delaware, USA\nvijay@udel.edu\nAbstract\nPre-training Transformer-based models such\nas BERT and ELECTRA on a collection\nof Arabic corpora, demonstrated by both\nAraBERT and AraELECTRA, shows an im-\npressive result on downstream tasks. How-\never, pre-training Transformer-based language\nmodels is computationally expensive, espe-\ncially for large-scale models. Recently, Fun-\nnel Transformer has addressed the sequential\nredundancy inside Transformer architecture by\ncompressing the sequence of hidden states,\nleading to a signiﬁcant reduction in the pre-\ntraining cost. This paper empirically stud-\nies the performance and efﬁciency of building\nan Arabic language model with Funnel Trans-\nformer and ELECTRA objective. We ﬁnd that\nour model achieves state-of-the-art results on\nseveral Arabic downstream tasks despite us-\ning less computational resources compared to\nother BERT-based models.\n1 Introduction\nThe introduction of Transformer and attention\nmechanism (Vaswani et al., 2017) have achieved\nsigniﬁcant success by exploiting transfer learning.\nBidirectional Encoder Representations from Trans-\nformers BERT (Devlin et al., 2019), builds upon\nthe idea of pre-training a Transformer with self-\nattention on large amounts of unlabeled text. Then,\nleverage the idea of transfer learning to ﬁne-tune\nthe pre-trained language model on downstream\ntasks. BERT has achieved impressive performance\ngains against its predecessor Bi-LSTM (Huang\net al., 2015) on many downstream tasks. In the\nArabic domain, both AraBERT (Antoun et al.,\n2020) and AraELECTRA (Antoun et al., 2021)\nhave adapted BERT and ELECTRA (Clark et al.,\n2020b) models to the Arabic language and show\nimpressive results on downstream tasks.\nHowever, pre-training Transformer-based mod-\nels, especially at a large scale, requires enormous\ncomputational resources. This issue motivates us\nto investigate a solution to reduce the cost of pre-\ntraining Transformer-based models. Reducing the\ncost to train Arabic language models will help ac-\ncelerate research advancement in Arabic language\nprocessing. Additionally, this will help researchers\nwith limited resources to ﬁne-tune large models.\nSeveral techniques in the literature have sug-\ngested solutions to reduce the cost of pre-training\nand ﬁne-tuning, including cross-layer parameter\nsharing with ALBERT (Lan et al., 2020) and dis-\ntillation (Sanh et al., 2020). Distillation and sim-\nilar techniques have a detrimental effect on per-\nformance since they aim to reduce the parameter\nsize. On the other hand, the ﬁne-tuning and infer-\nence time for the ALBERT model, especially for\nALBERTxlarge and ALBERTxxlarge scale is signiﬁ-\ncantly higher than BERTLarge and ELECTRALarge\nas a result of having more hidden layer size. Thus,\nwe seek alternative architectures that could increase\nthe scale of the model without adding additional\ncost to the pre-training.\nFunnel Transformer (Dai et al., 2020) introduces\na novel solution to address the cost of pre-training\nby reconstructing the Transformer architecture us-\ning pooling and up-sampling techniques. Addi-\ntionally, ELECTRA speeds up the pre-training by\nintroducing a new objective function, employing\na small generator model trained with maximum\nlikelihood. This study investigates the effect of pre-\ntraining Funnel Transformer with ELECTRA ob-\njective on the performance of Arabic downstream\ntasks. Our results show that we achieve state-of-\nthe-art results with less computational resources\nthan existing Arabic language models described in\nthe literature. Thus, our contributions in this paper\ninclude :\n• We pretrain ArabicTransformer on a large col-\nlection of unlabeled Arabic corpora with Fun-\nnel Transformer and ELECTRA objective that\nrequires signiﬁcantly less time and resources\nthan state-of-the-art models.\n1256\nPool Pool\nEncoder Decoder\nBlock 1\nUp-sample\nBlock 2 Block 3\nFigure 1: Overview of Funnel Transformer Architecture. Figure adapted from (Dai et al., 2020).\n• We ﬁne-tune and evaluate our model on a suite\nof Arabic downstream tasks, including ques-\ntion answering and sentiment analysis tasks\nshowing that we achieve state-of-the-art per-\nformance on several downstream tasks.\n• We released our models to the research com-\nmunity along with our GitHub repository 1.\n2 Related Work\n2.1 ELECTRA\nThe loss function inside the BERT model consists\nof semi-supervised learning objectives that aim\nto capture the contextual representation of an un-\nstructured unlabeled dataset. This loss function\nin BERT has two objectives: Masked Language\nModel MLM and Next Sentence Prediction NSP.\nSeveral studies have investigated the effect of those\ntwo objectives on language model perplexity (Liu\net al., 2019), (Lan et al., 2020) . ELECTRA (Clark\net al., 2020b), reconstructed the BERT model’s\nloss function based on game theory concepts , par-\nticularly the GAN (Goodfellow et al., 2014) and\nMaskGAN (Fedus et al., 2018) models. In ELEC-\nTRA, the loss function is formed as a zero-sum\ngame where the goal of the discriminator and gen-\nerator is to reach the Nash equilibrium point. This\npoint represents the convergence of the language\nmodel to the optimal solution. As a result of hav-\ning a binary loss function, the ELECTRA model’s\nlearning curve is higher than the MLM objective.\n2.2 Funnel Transformer\nThe ELECTRA paper only introduces novelty to\nthe loss function without signiﬁcant changes to\nthe Transformer architecture. The major prob-\nlem with Transformer architecture is the sequen-\ntial redundancy within its structure. This redun-\n1We released our code and our models at https://\ngithub.com/salrowili/ArabicTransformer .\ndancy adds additional pre-training cost to the lan-\nguage model. Funnel Transformer reconstructed\nthe Transformer’s architecture to address the redun-\ndancy issue.\nThe key idea is to use a pooling technique to\ncompress the full sequence of hidden states in the\nencoder part through a series of blocks. Then re-\ncover the full sequence representation in the de-\ncoder part using an up-sampling technique. A com-\nmon conﬁguration for the block layout, as shown\nby (Dai et al., 2020) consists of 3 blocks and a hid-\nden layer size of 768 for base-scale models. For\nexample, an architecture with B6-6-6 design has\nthree blocks where each has 6 layers of a hidden\nsize of 768. A model with a B4-4-4 design consists\nof three blocks where each has 4 layers of hidden\nsize of 768. Figure 1 shows a high-level illustration\nof Funnel Transformer architecture.\nFunnel Transformer with this novel design man-\naged to save more FLOPs. The saved FLOPs can\nbe used either to increase the model parameters\nor to speed up the pre-training process (Dai et al.,\n2020). Results of Funnel Transformer on English\ndomain show signiﬁcant performance leap, espe-\ncially at base scale. These results motivate us to\ninvestigate the cost and efﬁciency of pre-training\nFunnel Transformer in the Arabic domain.\n3 Pre-Training the Language Model\n3.1 Dataset\nWe pretrain our models using a collection of large\nArabic corpora (45GB) including :\n• Arabic Wikipedia dump 1.3GB .\n• Abu El-Khair corpus 14GB (El-khair, 2016).\n• Unshufﬂed Arabic Oscar dataset 30GB (Or-\ntiz Suárez et al., 2020).\n1257\nSettings AraELECTRA ArabicTransformer AraBERT L\nModel-Scale Base B4-4-4 B6-6-6 Large\nHidden Layer Size 768 768 768 1024\nV ocabulary Size 64K 50K 50K 64K\nCorpora Size 77GB 45GB 45GB 77GB\nPre-Segmentation No No No Yes (v2) - No (v02)\nLearning Rate 2e-4 1e-4 4e-4 -\nMax Sequence Length 512 512 512 128-512\nBatch Size 256 256 1024 13440-2056\nSteps 2M 1M 250k 550K\nComputational Ratio 1.0x 0.5x 0.5x 7.8x\nPre-Training Hardware TPUv3-8 TPUv3-8 TPUv3-32 TPUv3-128\nTable 1: The structure and hyperparameters of ArabicTransformer models compared to AraELECTRA and\nAraBERTL. Computational Ratio (C ratio) represents the training steps multiplied by the batch size where the\nAraELECTRA model is the baseline. AraBERTL follows a similar approach to (Devlin et al., 2019) by pretraining\nAraBERTL initially for 250K steps with a maximum sequence length of 128 and batch size of 13440. Then, they\ncontinue the pre-training for addtional 300K steps with a maximum sequence length of 512 and batch size of 2056.\nBoth AraBERTv2L and AraBERTv02L have similar hyperparameters except the use of pre-segmentation.\n3.2 Environmental Setup\nWe pretrain our models using the google cloud\ncompute engine and TensorFlow units (TPUs). We\nuse TensorFlow 1.15 (Abadi et al., 2015) and the\nopen-source code of Funnel Transformer .\n3.3 Pre-Training Hyperparameters\nTable 1 provides our choice of pre-training hy-\nperparameters for our models against both Ara-\nELECTRA (Antoun et al., 2021) and AraBERT\n(Antoun et al., 2020). We build our base model\nwith a structure that consists of a 6-6-6 block lay-\nout and 768 hidden layer size. This block layout\nincreases the model parameters up to 1.39x com-\npared to BERTBase and ELECTRABase (Dai et al.,\n2020). Additionally, we pretrain a smaller model\nwith a 4-4-4 block layout. This model has a similar\nparameter size to ELECTRAbase.\nInstead of using a batch size of 256 as proposed\nin the original paper of ELECTRA and AraELEC-\nTRA, we increase the batch size to 1024 and the\nlearning rate to 4e-4 for our B6-6-6 model. Sev-\neral studies in the literature support the idea of\nusing large batch size since it improves the lan-\nguage model’s perplexity (Liu et al., 2019), (You\net al., 2020). On the other hand, we use similar\npre-training hyperparameters to (Dai et al., 2020)\nfor our B4-4-4 model. We build our vocabulary ﬁle\nwith a size of 50K without using Farasa segmenter\n(Abdelali et al., 2016). Farasa segmenter is a tool\nthat breaks words into stems, sufﬁxes, and preﬁxes\n(Antoun et al., 2020).\n4 Fine-tuning on Downstream Tasks\n4.1 Question Answering\nTo compare our model with existing models in the\nliterature, we use ARCD (Mozannar et al., 2019)\nand the Arabic portion of TyDi QA (Clark et al.,\n2020a). Both ARCD and TyDi QA are in format of\nSQuADv1.1 dataset (Rajpurkar et al., 2016). Simi-\nlar to the AraELECTRA and AraBERT approach,\nwe ﬁne-tune our model on both ArabicSQuAD and\nARCD training datasets. Then, we evaluate our\nmodel on the test portion of the ARCD dataset.\nMoreover, as is a common practice, we use a pre-\nprocessing script developed by the AUB MIND lab,\nwhich ﬁxes the position of text spans and handles\nspecial characters in the ARCD dataset.\nOur baseline models for QA tasks including\nAraBERTv02large, AraBERTv2large (Antoun et al.,\n2020), Arabic-ALBERTxlarge (Safaya, 2020) and\nAraELECTRA. We follow the same split of train-\ning and development dataset used by AraELEC-\nTRA, summarized in Table 2. We only include\nmodels that have reported results in the literature\nfor ARCD and TyDi QA in our baseline models.\nTask Train Test\nARCD Mozannar et al. (2019) 49,037 702\nTyDiQA Clark et al. (2020a) 14,805 921\nTable 2: Summary of Question Answering datasets.\n1258\n4.2 Sentiment Analysis\nSentiment analysis (SA) task is a text classiﬁca-\ntion task where we classify each sentence (se-\nquence) with a (sentiment) label. Those labels\ncan be either binary or categorical. Our choice\nfor sentiment analysis task including Hotel Arabic-\nReviews Dataset (HARD) (Elnagar et al., 2018),\nArabic Jordanian General Tweets (AJGT) (Da-\nhou et al., 2019) and ArScarcasmv2 (sentiment\nshared task) (Abu Farha et al., 2021). Our base-\nline models for sentiment analysis tasks including\nXLM-RBase;XLM-RLarge (Conneau et al., 2020),\nAraBERTv2Large;AraBERTv02Large (Antoun et al.,\n2020), AraELECTRA (Antoun et al., 2021), AR-\nBERT and MARBERT (Abdul-Mageed et al.,\n2021). Table 3 summarize the details of the dataset\nwe use for SA tasks.\nTask Labels Train Test\nHARD [neg, pos] 84.5k 21.1k\nArSarcasm [neg, neut, pos] 12.5k 3K\nAJGT [neg, pos] 1.4k 360\nTable 3: Summary of sentiment analysis (SA) datasets.\n(neg: negative , pos:positive , neut: neutral)\n4.3 Fine-tuning Hyperparameters\nWe extensively conduct a grid search to ﬁnd\nthe best hyperparameters for each task using the\nTPUv3-8 unit and Tensorﬂow 1.15. Our grid search\nspace range is : learning rate (2e-5, 3e-5, 4e-5, 5e-5,\n6e-5) , batch size (16, 24, 32, 40, 48, 64), layer-\nwise decay (0.75, 0.8, 1.0), max sequence length\n(384, 512) and epochs number (2-12). For senti-\nment analysis tasks, we use 256 as the maximum\nsequence length. We report our result as the best re-\nsult out of ﬁve different runs for each task, which is\na similar approach used by both ELECTRA (Clark\net al., 2020b) and BERT (Devlin et al., 2019). We\nuse the following seeds: 123, 1234, 12345, 666,\n42 for each run. We deﬁne our choices of seeds to\nimprove the reproducibility of results.\n5 Results and Discussion\n5.1 Pre-Training\nTable 4 shows the pre-training time of our models\nagainst AraELECTRA. The reduction in cost for\nboth B6-6-6 and B4-4-4 models is a result of using\na 0.5x C ratio (batch x steps) compared to Ara-\nELECTRA. Additionally, Funnel-transformer ar-\nchitecture contributes to additional reduction from\nModel Hardware Time Cost\nAraELECTRA TPUv3-8 24d 1.00x\nB6-6-6 (Ours) TPUv3-32 2d 10h 0.40x\nB4-4-4 (Ours) TPUv3-8 7d 11h 0.31x\nTable 4: Pretraining cost of our models compared to\nAraELECTRA.\n0.5x to 0.4x (B6-6-6) and from 0.5x to 0.31x for\n(B4-4-4) model. We have also evaluated our pre-\ntrained models on a random Arabic sample (2.5M\nwords with a size of 25MB) from CCNet dataset\n(Wenzek et al., 2020). Our evaluation shows that\nthe B4-4-4 model has a loss score of 11.58%\nagainst 11.12% for the B6-6-6 model.\n5.2 Question Answering\nTable 5 shows the performance of our models on\nQA tasks compared to state-of-the-art models re-\nported by (Antoun et al., 2021).\nModel TyDiQA ARCD\nEM F1 EM F1\nAraBERT02L 73.72 86.03 36.89 71.32\nAraBERT2L 64.49 82.15 34.19 68.12\nArabicALBERTxl 71.12 84.59 37.75 68.03\nAraELECTRAB 74.91 86.68 37.03 71.22\nOurs B4-4-4 74.70 85.89 31.48 67.70\nOurs B6-6-6 75.35 87.21 36.89 72.70\nTable 5: Evaluation results of ArabicTransformer com-\npared to SOTA models on QA tasks. We use F1 and\nexact match (EM) score for both tasks which is a com-\nmon practice to evaluate task in format of SQuAD1.1.\nWe use reported number by (Antoun et al., 2021) for\nour baseline models results.\nOur base-scale model (B6-6-6) outperforms Ara-\nELECTRA on both TyDi QA and ARCD tasks.\nThis performance improvement is due to the fact\nthat B6-6-6 has larger parameter size (1.39x) than\nELECTRABase architecture. Furthermore, our\nsmall model (B4-4-4) has a competitive perfor-\nmance against AraELECTRA and AraBERTL on\nthe TyDi QA task, especially on the exact match\n(EM) metric. The discrepancy in performance be-\ntween ARCD and TyDi QA tasks is due to the poor\nquality of the training dataset that we use for the\nARCD task. This training dataset uses the Arabic\nTranslation of SQuAD1.1 dataset (Antoun et al.,\n2021).\n1259\n5.3 Sentiment Analysis\nTable 6 summarizes the performance of Arabic-\nTransformer against SOTA models on sentiment\nanalysis tasks. In both HARD and ArScarcasm\nTask HARD AJGT Scarcasm\nMetric Acc. Acc. Acc. F1 PN\nXLM-RB 95.7 89.4 64.3 66.1\nXLM-RL 96.0 91.9 67.8 69.9\nAraBERT02L 96.4 94.5 69.5 71.8\nAraBERT2L 96.5 96.4 70.0 72.4\nARBERTB 96.1 94.4 67.3 69.5\nMARBERTB 96.2 96.1 69.3 72.4\nAraELECTB 96.4 95.0 69.6 72.3\nOurs B4-4-4 96.5 95.0 70.4 72.8\nOurs B6-6-6 96.6 95.0 70.8 74.0\nTable 6: Evaluation results of our models compared\nto SOTA models. F1 PN score takes only positive and\nnegative classes in calculation excluding neutral class.\nFor HARD and AJGT tasks, we use reported numbers\nof XLM-R, ARBERT and MARBERT (Abdul-Mageed\net al., 2021). For ArScarcasm task we use the reported\nnumbers by (Farha and Magdy, 2021). We reproduced\nAraELECTRA results on all tasks and AraBERTL mod-\nels on HARD and AJGT tasks.\ntasks, our models perform better than other state-\nof-the-art models, including larger models such as\nXLM-RL and AraBERTv2L. However, our mod-\nels perform worse on the AJGT task. We attribute\nthis performance to the fact that the AJGT task\nhas a relatively smaller dataset than HARD and\nArScarcasm. Therefore, it is more sensitive to hy-\nperparameter tuning, leading to a signiﬁcant perfor-\nmance ﬂuctuation.\n5.4 Pre-Segmentation\nAraBERTv2L, in contrast to other models in Table\n5 and Table 6, uses Farasa segmenter. Although\nAraBERTv2L outperforms AraELECTRA on the\nArScarcasm task, AraBERTv2L performs worse on\nQA tasks despite having a 7.5x computational ra-\ntio compared to AraELECTRA. The performance\nof AraELECTRA, AraBERTv02 L and our mod-\nels against AraBERTv2L on the QA task suggests\nthat pre-segmentation do not always lead to better\nperformance on span-based QA tasks. In contrast,\npre-segmentation contributes to the performance\nimprovement of AraBERTv2 on sentiment analy-\nsis tasks against AraBERTv02, especially on the\nArScarcasm task.\n5.5 Efﬁciency of Fine-Tuning\nTable 7 shows the ﬁne-tuning time of our models\ncompared to AraELECTRAbase. In addition to im-\nprovment in ﬁne-tuning speed, we also observe\nthat B4-4-4 uses less memory consumption than\nAraELECTRA.\nModel Time / Ratio #Params\nAraELECTRAB 25:31 (1.00x) 1.00x\nOurs (B4-4-4) 18:27 (0.72x) 1.00x\nOurs (B6-6-6) 27:24 (1.07x) 1.39x\nTable 7: Fine-Tuning time of our models compared\nto SOTA models. We ﬁnetune all models on HARD\ndataset for 3 epochs and with a batch size of 32 using\nV100 16GB Tesla GPU with PyTorch ( FP16 - O2 ).\nParameters ratio does not include embedding matrix.\n6 Conclusion\nWe introduce Arabic Transformer, a pretrained Ara-\nbic language representation model based on Funnel\nTransformer and ELECTRA objective. We show\nthat we achieve state-of-the-art results on several\nArabic downstream tasks, including question an-\nswering and sentiment analysis tasks. Addition-\nally, we show that our models are computationally\nefﬁcient and pretrained using signiﬁcantly less re-\nsources than state-of-the-art models. For future\nwork, we plan to investigate different designs of\nthe Funnel Transformer, including larger models\nsuch as (B8-8-8).\n7 Acknowledgement\nWe would like to acknowledge the support we have\nfrom Tensorﬂow Research Cloud (TFRC) team to\ngrant us access to TPUv3 units. The authors also\nwould like to thank anonymous reviewers from\nEMNLP21 for their constructive feedback on our\ninitial manuscript.\nReferences\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay\nGhemawat, Ian Goodfellow, Andrew Harp, Geoffrey\nIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-\nicz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-\nberg, Dandelion Mané, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon\nShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\nPaul Tucker, Vincent Vanhoucke, Vijay Vasudevan,\n1260\nFernanda Viégas, Oriol Vinyals, Pete Warden, Mar-\ntin Wattenberg, Martin Wicke, Yuan Yu, and Xiao-\nqiang Zheng. 2015. TensorFlow: Large-scale ma-\nchine learning on heterogeneous systems. Software\navailable from tensorﬂow.org.\nAhmed Abdelali, Kareem Darwish, Nadir Durrani, and\nHamdy Mubarak. 2016. Farasa: A fast and furious\nsegmenter for Arabic. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Demon-\nstrations, pages 11–16, San Diego, California. Asso-\nciation for Computational Linguistics.\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT\n& MARBERT: Deep bidirectional transformers for\nArabic. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 7088–7105, Online. Association for Computa-\ntional Linguistics.\nIbrahim Abu Farha, Wajdi Zaghouani, and Walid\nMagdy. 2021. Overview of the wanlp 2021 shared\ntask on sarcasm and sentiment detection in arabic.\nIn Proceedings of the Sixth Arabic Natural Lan-\nguage Processing Workshop.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15, Marseille, France. Eu-\nropean Language Resource Association.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021.\nAraELECTRA: Pre-training text discriminators for\nArabic language understanding. In Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop, pages 191–195, Kyiv, Ukraine (Virtual). Asso-\nciation for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020a. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics, 8:454–\n470.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020b. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAbdelghani Dahou, Mohamed Abd Elaziz, Junwei\nZhou, Shengwu Xiong, and Rodolfo Zunino. 2019.\nArabic sentiment classiﬁcation using convolutional\nneural network and differential evolution algorithm.\nIntell. Neuroscience, 2019.\nZihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.\n2020. Funnel-transformer: Filtering out sequential\nredundancy for efﬁcient language processing. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 4271–4282. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nIbrahim Abu El-khair. 2016. 1.5 billion words arabic\ncorpus.\nAshraf Elnagar, Yasmin S. Khalifa, and Anas Einea.\n2018. Hotel Arabic-Reviews Dataset Construction\nfor Sentiment Analysis Applications, pages 35–52.\nSpringer International Publishing, Cham.\nAbu Farha and Walid Magdy. 2021. Benchmarking\ntransformer-based language models for arabic sen-\ntiment and sarcasm detection. In Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop.\nWilliam Fedus, Ian Goodfellow, and Andrew M. Dai.\n2018. MaskGAN: Better text generation via ﬁlling\nin the . In International Conference on Learning\nRepresentations.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in Neural Information\nProcessing Systems, volume 27, pages 2672–2680.\nCurran Associates, Inc.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\n1261\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nHussein Mozannar, Elie Maamary, Karl El Hajal, and\nHazem Hajj. 2019. Neural Arabic question answer-\ning. In Proceedings of the Fourth Arabic Natu-\nral Language Processing Workshop, pages 108–118,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nPedro Javier Ortiz Suárez, Laurent Romary, and Benoît\nSagot. 2020. A monolingual approach to contextual-\nized word embeddings for mid-resource languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n1703–1714, Online. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAli Safaya. 2020. Arabic-albert.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Con-\nference on Learning Representations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7899025082588196
    },
    {
      "name": "Computer science",
      "score": 0.682823896408081
    },
    {
      "name": "Language model",
      "score": 0.6020241379737854
    },
    {
      "name": "Arabic",
      "score": 0.5925065875053406
    },
    {
      "name": "Architecture",
      "score": 0.5071306228637695
    },
    {
      "name": "Natural language processing",
      "score": 0.4937649667263031
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47436317801475525
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.47064584493637085
    },
    {
      "name": "Engineering",
      "score": 0.20045405626296997
    },
    {
      "name": "Electrical engineering",
      "score": 0.16171440482139587
    },
    {
      "name": "Linguistics",
      "score": 0.11807698011398315
    },
    {
      "name": "Voltage",
      "score": 0.09166756272315979
    },
    {
      "name": "Art",
      "score": 0.0848778784275055
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86501945",
      "name": "University of Delaware",
      "country": "US"
    }
  ],
  "cited_by": 6
}