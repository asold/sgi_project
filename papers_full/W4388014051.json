{
    "title": "Large Language Models for Therapy Recommendations Across 3 Clinical Specialties: Comparative Study",
    "url": "https://openalex.org/W4388014051",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Theresa Isabelle Wilhelm",
            "affiliations": [
                "University of Freiburg",
                "University Medical Center Freiburg",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2249045445",
            "name": "Jonas Roos",
            "affiliations": [
                "University Hospital Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A2500400904",
            "name": "Robert Kaczmarczyk",
            "affiliations": [
                "Karolinska Institutet",
                "Technical University of Munich"
            ]
        },
        {
            "id": null,
            "name": "Theresa Isabelle Wilhelm",
            "affiliations": [
                "University of Freiburg",
                "University Medical Center Freiburg",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2249045445",
            "name": "Jonas Roos",
            "affiliations": [
                "University Hospital Bonn"
            ]
        },
        {
            "id": "https://openalex.org/A2500400904",
            "name": "Robert Kaczmarczyk",
            "affiliations": [
                "Karolinska Institutet",
                "Technical University of Munich"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205164650",
        "https://openalex.org/W3035901421",
        "https://openalex.org/W2968870211",
        "https://openalex.org/W4254034712",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W4322622443",
        "https://openalex.org/W4324129637",
        "https://openalex.org/W4322718832",
        "https://openalex.org/W4323835279",
        "https://openalex.org/W4230874191",
        "https://openalex.org/W1845083756",
        "https://openalex.org/W3097590906",
        "https://openalex.org/W2576167606",
        "https://openalex.org/W2123109647",
        "https://openalex.org/W4365511667",
        "https://openalex.org/W2973379954",
        "https://openalex.org/W4253739719",
        "https://openalex.org/W2765304038",
        "https://openalex.org/W3189153245",
        "https://openalex.org/W2888474951",
        "https://openalex.org/W2561641472",
        "https://openalex.org/W2025734197"
    ],
    "abstract": "Background As advancements in artificial intelligence (AI) continue, large language models (LLMs) have emerged as promising tools for generating medical information. Their rapid adaptation and potential benefits in health care require rigorous assessment in terms of the quality, accuracy, and safety of the generated information across diverse medical specialties. Objective This study aimed to evaluate the performance of 4 prominent LLMs, namely, Claude-instant-v1.0, GPT-3.5-Turbo, Command-xlarge-nightly, and Bloomz, in generating medical content spanning the clinical specialties of ophthalmology, orthopedics, and dermatology. Methods Three domain-specific physicians evaluated the AI-generated therapeutic recommendations for a diverse set of 60 diseases. The evaluation criteria involved the mDISCERN score, correctness, and potential harmfulness of the recommendations. ANOVA and pairwise t tests were used to explore discrepancies in content quality and safety across models and specialties. Additionally, using the capabilities of OpenAI’s most advanced model, GPT-4, an automated evaluation of each model’s responses to the diseases was performed using the same criteria and compared to the physicians’ assessments through Pearson correlation analysis. Results Claude-instant-v1.0 emerged with the highest mean mDISCERN score (3.35, 95% CI 3.23-3.46). In contrast, Bloomz lagged with the lowest score (1.07, 95% CI 1.03-1.10). Our analysis revealed significant differences among the models in terms of quality (P&lt;.001). Evaluating their reliability, the models displayed strong contrasts in their falseness ratings, with variations both across models (P&lt;.001) and specialties (P&lt;.001). Distinct error patterns emerged, such as confusing diagnoses; providing vague, ambiguous advice; or omitting critical treatments, such as antibiotics for infectious diseases. Regarding potential harm, GPT-3.5-Turbo was found to be the safest, with the lowest harmfulness rating. All models lagged in detailing the risks associated with treatment procedures, explaining the effects of therapies on quality of life, and offering additional sources of information. Pearson correlation analysis underscored a substantial alignment between physician assessments and GPT-4’s evaluations across all established criteria (P&lt;.01). Conclusions This study, while comprehensive, was limited by the involvement of a select number of specialties and physician evaluators. The straightforward prompting strategy (“How to treat…”) and the assessment benchmarks, initially conceptualized for human-authored content, might have potential gaps in capturing the nuances of AI-driven information. The LLMs evaluated showed a notable capability in generating valuable medical content; however, evident lapses in content quality and potential harm signal the need for further refinements. Given the dynamic landscape of LLMs, this study’s findings emphasize the need for regular and methodical assessments, oversight, and fine-tuning of these AI tools to ensure they produce consistently trustworthy and clinically safe medical advice. Notably, the introduction of an auto-evaluation mechanism using GPT-4, as detailed in this study, provides a scalable, transferable method for domain-agnostic evaluations, extending beyond therapy recommendation assessments.",
    "full_text": "Original Paper\nLarge Language Models for Therapy Recommendations Across\n3 Clinical Specialties: Comparative Study\nTheresa Isabelle Wilhelm1,2*, MD; Jonas Roos3*, MD; Robert Kaczmarczyk4,5, MD\n1Eye Center, Medical Center, Faculty of Medicine, University of Freiburg, Freiburg, Germany\n2Medical Graduate Center, School of Medicine, Technical University of Munich, Munich, Germany\n3Department of Orthopedics and Trauma Surgery, University Hospital of Bonn, Bonn, Germany\n4Department of Dermatology and Allergy, School of Medicine, Technical University of Munich, Munich, Germany\n5Division of Dermatology and Venerology, Department of Medicine Solna, Karolinska Institutet, Solna, Sweden\n*these authors contributed equally\nCorresponding Author:\nRobert Kaczmarczyk, MD\nDepartment of Dermatology and Allergy\nSchool of Medicine\nTechnical University of Munich\nBiedersteiner Str 29\nMunich, 80802\nGermany\nPhone: 49 08941403033\nEmail: Robert.Kaczmarczyk@tum.de\nAbstract\nBackground: As advancements in artificial intelligence (AI) continue, large language models (LLMs) have emerged as promising\ntools for generating medical information. Their rapid adaptation and potential benefits in health care require rigorous assessment\nin terms of the quality, accuracy, and safety of the generated information across diverse medical specialties.\nObjective: This study aimed to evaluate the performance of 4 prominent LLMs, namely, Claude-instant-v1.0, GPT-3.5-Turbo,\nCommand-xlarge-nightly, and Bloomz, in generating medical content spanning the clinical specialties of ophthalmology,\northopedics, and dermatology.\nMethods: Three domain-specific physicians evaluated the AI-generated therapeutic recommendations for a diverse set of 60\ndiseases. The evaluation criteria involved the mDISCERN score, correctness, and potential harmfulness of the recommendations.\nANOVA and pairwise t tests were used to explore discrepancies in content quality and safety across models and specialties.\nAdditionally, using the capabilities of OpenAI’s most advanced model, GPT-4, an automated evaluation of each model’s responses\nto the diseases was performed using the same criteria and compared to the physicians’ assessments through Pearson correlation\nanalysis.\nResults: Claude-instant-v1.0 emerged with the highest mean mDISCERN score (3.35, 95% CI 3.23-3.46). In contrast, Bloomz\nlagged with the lowest score (1.07, 95% CI 1.03-1.10). Our analysis revealed significant differences among the models in terms\nof quality (P<.001). Evaluating their reliability, the models displayed strong contrasts in their falseness ratings, with variations\nboth across models (P<.001) and specialties (P<.001). Distinct error patterns emerged, such as confusing diagnoses; providing\nvague, ambiguous advice; or omitting critical treatments, such as antibiotics for infectious diseases. Regarding potential harm,\nGPT-3.5-Turbo was found to be the safest, with the lowest harmfulness rating. All models lagged in detailing the risks associated\nwith treatment procedures, explaining the effects of therapies on quality of life, and offering additional sources of information.\nPearson correlation analysis underscored a substantial alignment between physician assessments and GPT-4’s evaluations across\nall established criteria (P<.01).\nConclusions: This study, while comprehensive, was limited by the involvement of a select number of specialties and physician\nevaluators. The straightforward prompting strategy (“How to treat…”) and the assessment benchmarks, initially conceptualized\nfor human-authored content, might have potential gaps in capturing the nuances of AI-driven information. The LLMs evaluated\nshowed a notable capability in generating valuable medical content; however, evident lapses in content quality and potential harm\nsignal the need for further refinements. Given the dynamic landscape of LLMs, this study’s findings emphasize the need for\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 1https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nregular and methodical assessments, oversight, and fine-tuning of these AI tools to ensure they produce consistently trustworthy\nand clinically safe medical advice. Notably, the introduction of an auto-evaluation mechanism using GPT-4, as detailed in this\nstudy, provides a scalable, transferable method for domain-agnostic evaluations, extending beyond therapy recommendation\nassessments.\n(J Med Internet Res 2023;25:e49324) doi: 10.2196/49324\nKEYWORDS\ndermatology; ophthalmology; orthopedics; therapy; large language models; artificial intelligence; LLM; ChatGPT; chatbot;\nchatbots; orthopedic; recommendation; recommendations; medical information; health information; quality; reliability; accuracy;\nsafety; reliable; medical advice\nIntroduction\nArtificial intelligence (AI) will have a far-reaching impact on\nmedicine and has the potential to make health care more\nefficient, precise, and accessible for patients [1]. AI was first\ndescribed in the 1950s [2]. The digitization of medicine,\ncombined with the use of software applications and\nhealth-related data, has led to increased use of AI in medicine\n[3].\nChatGPT [4] is OpenAI’s latest innovation and was originally\nbased on the GPT-3.5 architecture. It is designed to generate\ntext outputs that match human performance levels across a wide\nrange of academic domains [5]. With over 100 million users,\nChatGPT produces responses to user inputs that are remarkably\nsimilar to human responses [6,7].\nIn addition to ChatGPT, there are other large language models\n(LLMs), like Anthropic’s Claude [8], an AI language model\nfocused on aligning with human values and generating safe,\ncontext-aware responses. Command [9], developed by Cohere\nTechnologies, excels in natural language understanding and\naims to facilitate seamless human-machine communication\nacross various fields, including medicine. BigScience’s Bloomz\n[10] model is a collaborative AI project emphasizing research,\nethical considerations, and application development in diverse\ndomains. LLMs such as ChatGPT, Claude, Command, and\nBloomz have the potential to revolutionize health care by\nproviding accurate and reliable medical advice, enabling better\nand more accessible health care solutions for patients worldwide.\nIn a comprehensive study that encompassed 180 questions\nspanning diverse medical disciplines, ChatGPT exhibited an\naccuracy rate of 57.8% in providing “correct” or “almost\ncorrect” responses. These answers were meticulously evaluated\nby a panel of 17 medical specialists. Through an internal\nvalidation process, questions that received lower ratings were\nsubjected to retesting after a period of 8 to 17 days, resulting\nin a significant enhancement of answer quality [11]. Moreover,\neven when tasked with identifying crucial research topics within\nthe field of gastroenterology, ChatGPT proved its capacity to\ngenerate high-quality research inquiries within predefined\nthematic frameworks. This underlines the potential significance\nof ChatGPT as a valuable instrument for advancing the\nrespective specialties in the future [12]. The study findings\nunveiled considerable prospects for using ChatGPT in medical\napplications. However, it is essential to acknowledge that the\nresponses exhibited a notable degree of variability.\nConsequently, the present iteration of ChatGPT lacks the\ncapability to independently handle intricate medical tasks [13].\nFurther research is imperative to harness the full potential of\nLLMs as safe and dependable tools within the health care\ndomain [14].\nA good doctor-patient relationship leads to more satisfied\npatients, increases patient safety, and lowers hospital costs [15].\nHowever, the current practice of informing patients about\nmedical procedures results in inadequate understanding [16].\nOnly 21%-86% of patients can recall the possible risks and\ncomplications of the procedures, and patient understanding\nappears to decrease with age [17]. The attempts of patients to\ninform themselves on social media platforms lead to a high rate\nof misinformation [18]. However, research also shows that\nseeking health information can improve the physician-patient\nrelationship, and patients expect to be more involved in\ndecisions about their health [19].\nThis study was designed to test and evaluate LLMs as a source\nof patient information. The goal was to assess the given answers\nto specific medical conditions from both a medical perspective\nand through AI, to investigate for relevant misinformation, and\nultimately to test whether the provided answers can be used as\na source for improved doctor-patient communication.\nMethods\nStudy Design\nA total of 4 LLMs based on the transformer architecture [20]\nfrom OpenAI (GPT -3.5-T urbo), Cohere\n(Command-xlarge-nightly), Anthropic (Claude-instant-v1.0),\nand BigScience (Bloomz) were used to simulate treatment\nrecommendation requests on 60 arbitrarily chosen diseases (19\nophthalmologic, 20 dermatologic, and 21 orthopedic diseases).\nOf the models assessed, only Bloomz is open-source and\nprovides a comprehensive technical report [10]. To establish a\nbaseline on the LLMs’ responses in straightforward scenarios,\nwe used the simple question prompt “How to treat…” in\ncombination with various diseases (Figure 1). The response\nassessment was performed using physicians’ practical clinical\nknowledge, UpToDate [21], and PubMed.\nThe DISCERN instrument [22] is a validated tool to assess the\nquality of written consumer health information on treatment\nchoices. We used a modified version, mDISCERN, containing\na subset of 10 out of the original 16 questions (Table 1). The\nmeanings of the mDISCERN scores were as follows: a score\nof 1 or 2 indicated no, low, or significant deficiencies; a score\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 2https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nof 3 indicated partly, medium, or possibly important but not\nsignificant deficiencies; and a score of 4 or 5 indicated yes,\nhigh, or minimal deficiencies. To guide the physicians in\nconsistent ratings, we provided instructions based on the\navailable official web-based resources [23]. Furthermore, we\nassessed the answers for truthfulness (only true information, at\nleast questionable information, or clearly false information) and\nharmfulness (potentially harmful information). For the analysis,\ntruthfulness was transformed into a binary variable (0: only true\ninformation; 1: potentially or clearly false information). We\nconducted ANOVA and pairwise t tests to analyze differences\nin the quality and safety of the generated content among models\nand specialties.\nIn addition to the physicians’ratings, we used the default GPT-4\nmodel (version as of March 23, 2023) [24] without fine-tuning\nto assess the output of the other LLMs using the same criteria\n(see the prompt template in Multimedia Appendix 1). For a\nsingle, false GPT-4 evaluation (“How to treat radius fracture?”),\nits rating of “2” for the binary harmfulness category (0: no\nharmful information; 1: harmful content) was considered\nharmful content for further analysis. Pearson correlation analysis\nwas performed to compare physicians’ ratings with GPT-4\nratings.\nFor this study, data analysis was performed using the Python\nprogramming language v3.8.11 (Python Software Foundation)\non a MacBook M1 Pro with Ventura OS 13.3.1 (Apple).\nStatistical analysis and data manipulation were conducted using\nthe packages SciPy (v1.7.3), Pandas (v1.4.3), and Pingouin\n(v0.5.3). For visualization, Matplotlib (v3.5.2) and Seaborn\n(v0.11.2) were used.\nFigure 1. Study design for the cross-specialty evaluation of large language models on treatment recommendations.\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 3https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 1. mDISCERN questions in descending order of physicians’ mean mDISCERN scores.\nmDISCERN questionID\nIs it clearly presented that more than one possible treatment procedure may exist?Q1\nAre the objectives clear and achieved?Q2\nIs the information presented balanced and unbiased?Q3\nFinally, based on the answers to all the preceding questions, rate the answer in terms of its overall quality as a source of information.Q4\nIs the information an aid to “shared decision-making”?Q5\nIs the mode of action of each treatment procedure described?Q6\nAre the benefits of each treatment procedure described?Q7\nIs it described how the treatment procedures affect quality of life?Q8\nAre additional sources of information listed for patient reference?Q9\nAre the risks of each treatment procedure described?Q10\nEthical Considerations\nThis study centered on assessing AI systems without the direct\ninvolvement of human participants. Prioritizing the accuracy\nof the AI-produced medical content was crucial due to its\npotential impact on clinical practice. Content generated by the\nAI models was exclusively used for research purposes.\nDeclaration of Generative AI and AI-Assisted\nTechnologies in the Writing Process\nGrammarly and GPT-4 were used for language improvements\nand general manuscript revision. After using these tools, the\nauthors reviewed and edited the content as needed and take full\nresponsibility for the content of the publication.\nResults\nClaude-instant-v1.0 exhibited the highest mean mDISCERN\nscore of 3.35 (95% CI 3.23-3.46), followed by GPT-3.5-Turbo\nat 2.78 (95% CI 2.67-2.89), Command-xlarge-nightly at 2.17\n(95% CI 2.06-2.28), and Bloomz with the lowest score of 1.07\n(95% CI 1.03-1.10). A pairwise t test using the step-down\nBonferroni method revealed significant differences (P<.001)\namong all model pairs, indicating substantial disparities in\nresponse quality. Claude-instant-v1.0 outperformed the other\nmodels, while Bloomz ranked last based on mean mDISCERN\nscores across all specialties. Upon detailed examination of the\nmDISCERN scores, all models demonstrated comparable\nstrengths (Q1-Q3) and weaknesses (Q7-Q10) across all\nspecialties under study (Figure 2A).\nThe highest mDISCERN scores across all models were seen in\nthe clarity of multiple treatment options (mean 3.42, 95% CI\n3.19-3.65), clear and achieved objectives (mean 3.24, 95% CI\n3.05-3.42), and balanced and unbiased presentation (mean 2.93,\n95% CI 2.73-3.13), and the lowest scores in benefits of treatment\nprocedures (mean 1.99, 95% CI 1.83-2.14), treatment impact\non quality of life (mean 1.59, 95% CI 1.45-1.73), provision of\nadditional sources for patient reference (mean 1.55, 95% CI\n1.45-1.66), and risks of treatment procedures (mean 1.29, 95%\nCI 1.20-1.37, Figure 2B).\nThe ANOVA demonstrated significant differences in\nharmfulness ratings among models (F3,228=4.412, P=.005,\nη²=0.055) but not across specialties (F2,228=1.670, P=.19,\nη²=0.014); the interaction between specialty and model was\nalso nonsignificant (F6,228=1.798, P=.10, η²=0.045).\nConsequently, model differences in potential harmfulness were\nunrelated to the specialty under consideration. GPT-3.5-Turbo\nexhibited the lowest harmfulness rating without a single\npotentially harmful piece of information (0%, 95% CI 0%-0%).\nClaude-instant-v1.0 exhibited the highest number of potentially\nharmful recommendations (13.3%, 95% CI 4.7%-22%),\nfollowed by Bloomz (8.3%, 95% CI 1.3%-15.4%) and\nCommand-xlarge-nightly (1.7%, 95% CI –1.6% to 4.9%).\nAn ANOVA demonstrated significant main effects of specialty\n(F2,228=8.523, P<.001, η²=0.070) and model (F3,228=14.455,\nP<.001, η²=0.160) on falseness ratings. However, the interaction\nbetween specialty and model was not statistically significant\n(F6,228=1.694, P=.12, η²=0.043). These findings indicate that\nthe performance of each model differs across medical domains,\nwith the overall effect of specialty and model on the likelihood\nof providing potentially or clearly false information being\nstatistically significant.\nThe mean falseness ratings with 95% CIs revealed differences\nin the extent of potentially or clearly false information provided\nby each model. Claude-instant-v1.0 demonstrated the highest\nfalseness ratings in ophthalmology (68.4%, 95% CI 47%-89.9%)\nand dermatology (65%, 95% CI 43.6%-86.4%), while\nGPT-3.5-Turbo exhibited the lowest rating in dermatology (0%,\n95% CI 0%-0%). The overall accuracy, defined as the absence\nof harmfulness and falseness, was highest for GPT-3.5-Turbo\n(88.3%, 95% CI 80.1%-96.5%) and was lowest for\nClaude-instant-v1.0 (48.3%, 95% CI 35.6%-61.1%). The\ncomplete list of responses is included in Multimedia Appendix\n2. A comparative overview of mDISCERN, falseness, and\nharmfulness ratings, together with the accuracy among all\nLLMs, is provided in Table 2, and a few selected examples for\neach specialty are shown in Table 3.\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 4https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nFigure 2. Evaluation of the therapy recommendations by large language models (LLMs). (A) Mean mDISCERN scores separated by LLMs and\nmDISCERN questions. (B) Mean mDISCERN scores across all specialties (dermatology, ophthalmology, and orthopedics) and LLMs. Most responses\nclearly show more than one therapeutic option, whereas risks and additional sources of information were lacking. All error bars show 95% CIs of the\nmean.\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 5https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 2. Comparison of the mDISCERN quality score (1: low quality; 5: high quality), the percentage of potentially incorrect and harmful responses,\nand the accuracy (defined by the absence of both falseness and harmfulness) for all 4 large language models on treatment recommendations for all 60\ndiseases (19 ophthalmologic, 20 dermatologic, and 21 orthopedic diseases).\nAccuracy (n=240), % (95%\nCI)\nHarmfulness (n=240), %\n(95% CI)\nFalseness (n=240), % (95%\nCI)\nmDISCERN quality score\n(n=240), mean (95% CI)\nLarge language model\n(n=60 each)\n48.3 (35.6-61.1)13.3 (4.7-22.0)51.7 (38.9-64.4)3.4 (3.2-3.5)Claude-instant-v1.0\n88.3 (80.1-96.5)0 (0-0)11.7 (3.5-19.9)2.8 (2.7-2.9)GPT-3.5-Turbo\n83.3 (73.8-92.8)1.7 (–1.6 to 4.9)16.7 (7.2-26.2)2.2 (2.1-2.3)Command-xlarge-nightly\n83.3 (73.8-92.8)8.3 (1.3-15.4)15 (5.9-24.1)1.1 (1.0-1.1)Bloomz\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 6https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 3. Examples of comments by physicians and GPT-4 on the given responses.\nGPT-4 commentPhysician commentAnswer (model)QuestionSpecialty\nThe provided answer contains incorrect\ninformation about a clavus. A clavus is\nA clavus is not a nail de-\nformity.\nA clavus is a nail deformity that can be\ncaused by several different things, including\ninjury, infection, or fungus. Treatment for\nHow to treat a\nclavus?\nDermatology\nactually a type of callus, not a nail defor-\nmity. The answer’s content is not rele-\nvant to the treatment of a clavus.\na clavus will depend on the cause. If the\nclavus is caused by an injury, it may be\ntreated with a splint or cast to help the nail\ngrow back correctly. If the clavus is caused\nby an infection, it may be treated with an-\ntibiotics. If the clavus is caused by a fungus,\nit may be treated with antifungal medica-\ntion. In some cases, a clavus may need to\nbe surgically removed. (Command-xlarge-\nnightly)\nNote: The provided answer contains a\nminor error. Tinea corporis is not the\n“Tinea corporis” is not\nknown as an athlete’s\nfoot or jock itch.\nTinea corporis is a fungal infection of the\nskin that causes a red, scaly, itchy rash. It\nis also known as athlete’s foot or jock itch.\nTreatment for tinea corporis usually in-\nvolves the use of topical antifungal creams\nHow to treat\ntinea corporis?\nsame as athlete’s foot or jock itch. Ath-\nlete’s foot is known as tinea pedis, and\njock itch is known as tinea cruris. Tinea\ncorporis is a term for a fungal infection\naffecting other areas of the body.\nor lotions. These can be purchased over the\ncounter or prescribed by a doctor. It is im-\nportant to follow the instructions on the\npackage carefully and to continue using the\ncream or lotion for the full course of treat-\nment, even if the rash appears to be gone.\nIf the rash is severe or does not respond to\ntopical treatment, oral antifungal medica-\ntions may be prescribed. These are taken by\nmouth and can be used to treat more severe\nor stubborn cases of tinea corporis. (Com-\nmand-xlarge-nightly)\nExplanation: The answer provided is\noversimplified and does not accurately\nThe common adenovirus\nconjunctivitis cannot be\nUse an eye drop to treat the infection.\n(Bloomz)\nHow to treat vi-\nral conjunctivi-\ntis?\nOphthalmology\nrepresent the management of viral con-\njunctivitis. Antiviral eye drops are not\ncured by eye drops.\nHowever, moisturizing\ntypically used for this condition, as it iseye drops can relieve\nusually self-limiting and resolves on itssymptoms. It is not said\nown. Instead, supportive care measureswhat types of eye drops\nsuch as cool compresses and artificial\ntears are more appropriate.\nto use. Transmission pro-\nphylaxis is not men-\ntioned.\nTea tree oil should not be used to treat\nherpes keratitis. The answer contains\n<Marked as a false an-\nswer, no further com-\nment>\nUse a cotton swab to apply a small amount\nof tea tree oil to the sore. (Bloomz)\nHow to treat\nherpes ceratitis?\npotentially harmful information because\napplying tea tree oil to the eye can cause\nirritation and further complications. The\ncorrect approach is to use prescribed an-\ntiviral medications under the supervision\nof a healthcare professional.\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 7https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nGPT-4 commentPhysician commentAnswer (model)QuestionSpecialty\nThe answer contains at least questionable\ninformation as it does not mention non-\nsurgical treatments such as radiation\ntherapy or corticosteroid injections,\nwhich can also be used to manage palmar\nfibromatosis. < Authors’ note: The\nmodel failed to recognize certain non-\nsurgical treatment strategies shown in\nthe model’s answer>\nThere are many therapy\noptions presented overall.\nHowever, neither with\nadvantages and disadvan-\ntages nor with reasons for\nthe respective therapy\nHere are some of the main treatment options\nfor palmar fibromatosis:\n• Surgery: Partial or complete excision\nof the tumor mass is often the most\neffective treatment for palmar fibro-\nmatosis. The goal is to remove as\nmuch of the tumor as possible while\npreserving function and anatomy.\nSurgery may need to be repeated for\nrecurrences. Flexor tendon reconstruc-\ntion or release may also be needed in\nsome cases.\n• Steroid injections: Corticosteroid injec-\ntions can help reduce inflammation\nand slow the growth of the tumor.\nThey are often used pre-operatively to\nshrink the tumor and make it easier to\nexcise. Multiple injections are usually\nrequired.\n• Radiation therapy: In some cases, radi-\nation therapy can be used to destroy\ntumor cells. It is usually reserved for\nunresectable or recurrent disease.\n• Chemotherapy: Anti-cancer drugs have\nshown little benefit for palmar fibro-\nmatosis and are rarely used. Some case\nreports suggest newer targeted drug\ntherapies may have some effect.\n• Physical therapy: After surgery or\nother procedures, physical therapy can\nhelp improve hand mobility and func-\ntion. Splinting or bracing may also be\nused to maintain joint positioning and\nprevent contractures.\n• Close monitoring: For some small or\nslow-growing tumors, close follow-up\nmonitoring with periodic imaging and\nclinical exams may be the most appro-\npriate approach, at least initially.\nSurgery can then be considered if the\ntumor grows significantly or causes\nproblems.\n• Combination approach: Often, a com-\nbination of these treatments is used to\nmaximize effectiveness. For example,\nsurgery may be combined with steroid\ninjections or radiation therapy for dif-\nficult or recurrent cases.\nThe ultimate goal of all treatment is to\nminimize recurrence, preserve hand func-\ntion, and provide palliative relief of symp-\ntoms like pain, swelling, or limitations in\nmovement. Close monitoring is needed\nlifelong due to the high recurrence rates of\nthis condition. (Claude-instant-v1.0)\nHow to treat\npalmar fibro-\nmatosis?\nOrthopedics\nThe answer contains at least questionable\ninformation, as it only suggests rest, ice,\ncompression, and elevation (RICE) as\nthe treatment for an Achilles tendon\nrupture. While these methods may help\nalleviate pain and swelling, they do not\naddress the full extent of the injury.\nTreatment often involves immobiliza-\ntion, physical therapy, and in some cases,\nsurgery.\n<The response was not\nconsidered questionable\nby the author due to\nmissing information.\nHowever, the author\nagrees with ChatGPT\nthat the therapy options\nwere not fully men-\ntioned.>\nRest, ice, compression, elevation (Bloomz)How to treat\nachilles tendon\nrupture?\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 8https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nIn our analysis of mDISCERN questions for the evaluation of\nmodel responses using independent t tests and Bonferroni\ncorrection, we found differences in scores between specialties\nacross all models combined. Specifically, the scores for\nmDISCERN question Q2 (“Are the objectives clear and\nachieved?”) were higher in ophthalmology compared to\northopedics and dermatology (P<.05). In addition, the scores\nfor Q6 to Q8 (pertaining to the mode of action, benefits, and\neffect on quality of life of therapies, respectively) were higher\nfor orthopedics compared to dermatology (P<.05). Particularly\nfor Q8, the scores were also significantly higher for orthopedics\ncompared to ophthalmology (P<.001). Aside from these\nfindings, no other significant differences were observed in the\ncomparisons between specialties (Table 4).\nA Pearson correlation analysis assessing the relationship\nbetween physician- and GPT-4-generated ratings across the 12\nevaluated criteria showed positive, statistically significant\ncorrelations (P<.05) of varying strengths (Table 5). The\nstrongest correlations emerged for “overall quality as a source\nof information” (Q4; r=0.686, 95% CI 0.61-0.75, P<.001), “aid\nto shared decision-making” (Q5; r=0.665, 95% CI\n0.59-0.73,P<.001), and “mode of action description” (Q6;\nr=0.638, 95% CI 0.56-0.71, P<.001). The weakest correlations\nwere observed for “additional sources listed for patient\nreference” (Q9; r=0.186, 95% CI 0.06-0.31, P=.004), “contains\nfalse information” (r=0.187, 95% CI 0.06-0.31, P=.004), and\n“contains potentially harmful information” (r=0.188, 95% CI\n0.06-0.31, P=.003).\nThese findings suggest that GPT-4-generated ratings exhibit a\nconsiderable degree of alignment with physician ratings across\nvarious criteria, indicating the model’s potential to generate\nuseful, unbiased, and accurate information for patients.\nHowever, the weaker correlations observed for specific criteria,\nparticularly those related to potential harm and false information,\nemphasize the need for caution and continued refinement of\nAI-generated content intended for patient use. Future research\nshould focus on improving these AI models to minimize the\nlikelihood of providing harmful or false information, ensure\npatient safety, and enhance the overall utility of AI-generated\ncontent in health care.\nTable 4. Mean mDISCERN scores for all questions for each specialty.\nmDISCERN score, mean (95% CI)mDISCERN question\nOphthalmologyDermatologyOrthopedics\n3.29 (2.88-3.70)3.58 (3.19-3.96)3.39 (3.02-3.77)Q1\n3.86 (3.52-4.19)3.08 (2.77-3.38)2.83 (2.54-3.12)Q2\n3.00 (2.60-3.40)2.88 (2.55-3.20)2.92 (2.61-3.22)Q3\n2.30 (2.03-2.57)2.62 (2.34-2.91)2.67 (2.38-2.96)Q4\n2.25 (1.96-2.54)2.58 (2.29-2.86)2.64 (2.35-2.94)Q5\n2.34 (2.00-2.68)1.99 (1.73-2.24)2.77 (2.47-3.08)Q6\n2.04 (1.74-2.34)1.65 (1.43-1.87)2.26 (1.99-2.53)Q7\n1.08 (0.96-1.19)1.27 (1.13-1.42)2.36 (2.07-2.64)Q8\n1.41 (1.30-1.52)1.43 (1.30-1.55)1.81 (1.58-2.04)Q9\n1.33 (1.15-1.51)1.19 (1.07-1.31)1.35 (1.20-1.49)Q10\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 9https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nTable 5. Correlation between physicians’ and GPT-4-generated ratings for given questions.\nPoweraBayes factorP value95% CIPearson, rQuestion\n>.9993.346 × 1031<.0010.61-0.750.686Finally, based on the answers to all the preceding questions, rate the answer\nin terms of its overall quality as a source of information.\n>.9996.52 × 1028<.0010.59-0.730.665Is the information an aid to “shared decision-making”?\n>.9995.017 × 1025<.0010.56-0.710.638Is the mode of action of each treatment procedure described?\n>.9992.419 × 1025<.0010.55-0.710.635Are the objectives clear and achieved?\n>.9998.752 × 1022<.0010.53-0.690.612Is it clearly presented that more than one possible treatment procedure may\nexist?\n>.9994.15 × 1022<.0010.52-0.680.609Is the information presented balanced and unbiased?\n>.9998.705 × 1014<.0010.42-0.610.518Are the benefits of each treatment procedure described?\n>.9999.425 × 1009<.0010.33-0.540.441Is it described how the treatment procedures affect quality of life?\n>.9991.842 × 107<.0010.27-0.490.388Are the risks of each treatment procedure described?\n0.8355.618.0030.06-0.310.188Does the answer contain potentially harmful information?\n0.8335.498.0040.06-0.310.187Does the answer contain false information?\n0.8285.185.0040.06-0.310.186Are additional sources of information listed for patient reference?\naThe statistical power indicates the likelihood of correctly rejecting the null hypothesis, which assumes no linear relationship between the physicians’\nand GPT-4-generated ratings.\nDiscussion\nThe current study investigated the performance of 4 LLMs in\ngenerating medical information across 3 clinical specialties\n(ophthalmology, dermatology, and orthopedics). Our results\nrevealed considerable variability in the quality, potential\nharmfulness, and falseness of the information provided by the\nLLMs. These findings hold important implications for potential\napplications and limitations of AI-generated content in health\ncare.\nClaude-instant-v1.0 consistently exhibited the highest mean\nmDISCERN scores, followed by GPT-3.5-Turbo,\nCommand-xlarge-nightly, and Bloomz. These differences were\nstatistically significant, suggesting notable disparities in the\noverall quality of information generated by the models.\nHowever, despite its superior performance in the mDISCERN\nevaluation, Claude-instant-v1.0 demonstrated the highest\nfalseness and harmfulness ratings, contradicting its “helpful,\nhonest, and harmless AI systems” slogan [8] in the medical\ndomain. The disparity between high mDISCERN scores and\ninstances of falseness or harmfulness highlights a crucial\nchallenge: while richness in content might suggest\ncomprehensive information, it doesn’t guarantee accuracy or\nsafety. This emphasizes the imperative of ongoing refinement\nin AI-driven medical content to reconcile the depth of\ninformation with its clinical accuracy and safety. The overall\nlow mDISCERN scores observed for the Bloomz model should\nnot be interpreted as a definitive disqualification for patient\nrecommendation. Instead, these findings should motivate the\nscientific community to explore and enhance the potential of\nthis model through advanced fine-tuning techniques [25] and\nmore effective prompting strategies, especially given that it is\nthe sole open-source model within the examined cohort. Other\ngeneral factors that might have an impact on model performance\nare the complexity and diversity of the training data, the\npresence of inherent biases in the data, the computational\nresources available during training, general model architecture,\nand the ongoing adjustments and updates made to the model\npostdeployment to respond to real-world feedback.\nThe mDISCERN score revealed limitations in all assessed AI\nmodels regarding the discussion of treatment risks and benefits,\nthe impact on quality of life, and the provision of supplementary\nresources for patients. Microsoft’s recently released AI-powered\nBing search [26] and the new version of ChatGPT [27], both\nof which use GPT-4 and have the ability to include links in\nresponses, could potentially address these concerns.\nFurthermore, knowledge about areas where mDISCERN scores\nare low can be used for targeted improvements of existing\nmodels using reinforcement learning through human feedback\n[28].\nThe analysis revealed a significant effect of specialty and model\non falseness ratings. This suggests that the performance of the\nmodels may not be consistent across different medical domains.\nConsequently, LLM developers should pay special attention to\nthe unique demands and requirements of different specialties\nto optimize the quality and accuracy of the generated content.\nWe have observed distinct error patterns that warrant attention.\nForemost, there were instances where the models recommended\ntherapy options, such as corticosteroids, without the necessary\naccompaniment of antibiotics for infectious diseases.\nAdditionally, diagnoses or therapies were occasionally confused\n(eg, interchange of topical and systemic administration routes\nor conflation of standard arteriovenous cardiological bypass\nprocedures with those of an experimental nature in the ocular\ncontext), pointing toward a potential risk of misdirection in\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 10https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\ntreatment options. Furthermore, some advice appeared broad\nor nonspecific, highlighting the necessity for professional\noversight.\nIn the field of ophthalmology, our findings underscore the\nimperative for LLMs to furnish more nuanced patient\ninformation, considering the fragile aspect of ocular health and\nproactive eyesight preservation—notably in the preservation of\neyesight for diseases like endophthalmitis [29]. Similarly, for\ndermatology, with a broad spectrum of conditions ranging from\nbenign to malignant, the variability in the information generated\nemphasizes the necessity for accuracy and the potential risks\nof misinformation, especially for time-sensitive therapies, such\nas in the case of melanoma [30]. Orthopedics, being a specialty\nheavily reliant on procedural interventions, necessitates\ninformation on risks, benefits, and postoperative care, areas\nwhere the LLMs displayed noticeable limitations. Higher\nevaluations in orthopedics for treatment efficacy (Q6), benefits\n(Q7), and effect on the quality of life (Q8) may be attributed to\nthe intuitive and relatively simple nature of conservative\ntherapies, such as rest, ice, compression, and elevation, as well\nas common treatment protocols involving physical therapy and\npharmacological interventions [31-33]. Counterintuitively, in\ndermatology, actions like scratching can worsen symptoms [34].\nOur findings also demonstrated significant correlations between\nphysician ratings and GPT-4-generated ratings for the 12\nassessed criteria. This suggests that GPT-4 may hold the\npotential for evaluating the overall quality of patient information.\nHowever, the weaker correlations observed in certain criteria,\nparticularly those related to potential harm and false information,\nunderscore the need for continued improvements in using AI\nsystems for the evaluation of patient content. Ensuring patient\nsafety and providing reliable information should be primary\ngoals for the developers of these models. This will be an\nessential step in enhancing the trustworthiness and overall utility\nof AI-generated content in health care.\nOur study encountered several limitations. While we sought to\nvalidate physicians’ ratings using GPT-4 and demonstrated a\nhigh correlation among numerous ratings, a more robust\nvalidation of the method would require the inclusion of a larger\nnumber of physician specialists and an expanded range of\nclinical specialties. This is particularly important when dealing\nwith subjective scores, such as the mDISCERN used in this\nstudy. In our evaluation, we used straightforward prompts to\nreflect typical real-world queries and gauge primary model\noutputs. For example, one question (Q9) assessed if models\ninherently provided additional sources. Yet, prompt nuances\ncan change results, and directly asking, “Can you list the sources\nof information for this topic?” could have resulted in better\nmodel responses. Moreover, our investigation represents a\nsnapshot of the rapidly evolving landscape of LLMs. Since the\nbeginning of our study, new models like Alphabet’s Bard [35]\nand Meta’s LlaMA2 [36] have been released, showcasing\npotential advancements in medical applications. These\ndevelopments highlight the necessity for continuous evaluation\nof LLMs in health care, as newer models may offer enhanced\ncapabilities. Consequently, this study should be perceived less\nas a definitive critique of the drawbacks of such models and\nmore as a framework to guide future research in evaluating the\ncapabilities and performances of these increasingly sophisticated\nsystems. We endeavored to closely emulate real-world scenarios;\nby using more advanced prompting techniques, the quality of\nthe responses could potentially be further enhanced [37].\nIn conclusion, this study highlights the potential of LLMs in\ngenerating medical information across various specialties while\nalso emphasizing the need for continued advancements in\nAI-generated content to ensure patient safety and provide\nreliable, accurate information. By addressing the identified\nlimitations and tailoring the development of LLMs to the unique\nrequirements of different medical specialties, AI-generated\ncontent could become a valuable resource for patients and health\ncare providers alike.\nData Availability\nThe complete data set with the corresponding mDISCERN, falseness, and truthfulness ratings of all 3 physicians and GPT-4 can\nbe found in Multimedia Appendix 2.\nAuthors' Contributions\nJR, TIW, and RK were involved in the conceptualization and methodology of the study. JR and RK curated the data, while TIW\nand RK performed the formal analysis. RK was responsible for project administration, software development, and supervision.\nJR and TIW provided resources for the study. Validation was carried out by JR, TIW, and RK. RK and TIW created the\nvisualizations for the manuscript. The writing process involved JR, RK, and TIW, with RK and JR preparing the original draft\nand JR, TIW, and RK taking part in the review and editing process. All authors have directly accessed and verified the underlying\ndata reported in the manuscript. TIW and JR are shared first authors, having contributed equivalently to the primary research\ncomponents of this study. RK is the sole last author with contributions equivalent in scale to the first authors.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nGPT-4 evaluation prompt for the therapy recommendations of the other models.\n[PDF File (Adobe PDF File), 102 KB-Multimedia Appendix 1]\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 11https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\nMultimedia Appendix 2\nData set of all 60 diseases with questions, responses, and ratings from the 3 physicians and GPT-4.\n[XLSX File (Microsoft Excel File), 105 KB-Multimedia Appendix 2]\nReferences\n1. Rajpurkar P, Chen E, Banerjee O, Topol EJ. AI in health and medicine. Nat Med 2022 Jan;28(1):31-38 [doi:\n10.1038/s41591-021-01614-0] [Medline: 35058619]\n2. Kaul V, Enslin S, Gross SA. History of artificial intelligence in medicine. Gastrointest Endosc 2020 Oct;92(4):807-812\n[doi: 10.1016/j.gie.2020.06.040] [Medline: 32565184]\n3. Wang F, Preininger A. AI in health: state of the art, challenges, and future directions. Yearb Med Inform 2019\nAug;28(1):16-26 [FREE Full text] [doi: 10.1055/s-0039-1677908] [Medline: 31419814]\n4. Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt [accessed 2023-05-08]\n5. OpenAI. GPT-4 Technical Report. arXiv Preprint posted online on March 15, 2023. [FREE Full text] [doi:\n10.5860/choice.189890]\n6. ChatGPT revenue and usage statistics (2023). Business of Apps. URL: https://www.businessofapps.com/data/chatgpt-statistics/\n[accessed 2023-05-08]\n7. Scott K. Microsoft teams up with OpenAI to exclusively license GPT-3 language model. Microsoft. URL: https://blogs.\nmicrosoft.com/blog/2020/09/22/microsoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/ [accessed\n2023-05-08]\n8. Introducing Claude. Anthropic. URL: https://www.anthropic.com/index/introducing-claude [accessed 2023-03-30]\n9. Generate: write copy for any context. Cohere. URL: https://cohere.com/generate [accessed 2023-09-28]\n10. Muennighoff N, Wang T, Sutawika L, Roberts A, Biderman S, Scao T, et al. Crosslingual generalization through multitask\nfinetuning. arXiv Preprint posted online on November 3, 2022. [FREE Full text] [doi: 10.18653/v1/2023.acl-long.891]\n11. Johnson D, Goodman R, Patrinely J, Stone C, Zimmerman E, Donald R, et al. Assessing the accuracy and reliability of\nAI-generated medical responses: an evaluation of the Chat-GPT model. Res Sq Preprint posted online on February 28,\n2023. [FREE Full text] [doi: 10.21203/rs.3.rs-2566942/v1] [Medline: 36909565]\n12. Lahat A, Shachar E, Avidan B, Shatz Z, Glicksberg BS, Klang E. Evaluating the use of large language model in identifying\ntop research questions in gastroenterology. Sci Rep 2023 Mar 13;13(1):4164 [FREE Full text] [doi:\n10.1038/s41598-023-31412-2] [Medline: 36914821]\n13. Xue VW, Lei P, Cho WC. The potential impact of ChatGPT in clinical and translational medicine. Clin Transl Med 2023\nMar;13(3):e1216 [FREE Full text] [doi: 10.1002/ctm2.1216] [Medline: 36856370]\n14. Goodman RS, Patrinely JR, Osterman T, Wheless L, Johnson DB. On the cusp: considering the impact of artificial intelligence\nlanguage models in healthcare. Med 2023 Mar 10;4(3):139-140 [doi: 10.1016/j.medj.2023.02.008] [Medline: 36905924]\n15. Verbesserte Arzt-Patienten-Kommunikation senkt Kosten und erhöht. Deutsches Ärzteblatt. URL: https://www.aerzteblatt.de/\nnachrichten/135517/Verbesserte-Arzt-Patienten-Kommunikation-senkt-Kosten-und-erhoeht-Patientensicherheit [accessed\n2023-05-08]\n16. Glaser J, Nouri S, Fernandez A, Sudore RL, Schillinger D, Klein-Fedyshin M, et al. Interventions to improve patient\ncomprehension in informed consent for medical and surgical procedures: an updated systematic review. Med Decis Making\n2020 Feb;40(2):119-143 [FREE Full text] [doi: 10.1177/0272989X19896348] [Medline: 31948345]\n17. Sherlock A, Brownie S. Patients' recollection and understanding of informed consent: a literature review. ANZ J Surg 2014\nApr;84(4):207-210 [doi: 10.1111/ans.12555] [Medline: 24812707]\n18. Suarez-Lledo V, Alvarez-Galvez J. Prevalence of health misinformation on social media: systematic review. J Med Internet\nRes 2021 Jan 20;23(1):e17187 [FREE Full text] [doi: 10.2196/17187] [Medline: 33470931]\n19. Tan SS, Goonawardene N. Internet health information seeking and the patient-physician relationship: a systematic review.\nJ Med Internet Res 2017 Jan 19;19(1):e9 [FREE Full text] [doi: 10.2196/jmir.5729] [Medline: 28104579]\n20. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. arXiv Preprint posted\nonline on June 12, 2017. [FREE Full text] [doi: 10.5860/choice.189890]\n21. UpToDate: industry-leading clinical decision support. Wolters Kluwer. URL: https://www.wolterskluwer.com/en/solutions/\nuptodate [accessed 2023-05-10]\n22. Charnock D, Shepperd S, Needham G, Gann R. DISCERN: an instrument for judging the quality of written consumer health\ninformation on treatment choices. J Epidemiol Community Health 1999 Feb;53(2):105-111 [FREE Full text] [doi:\n10.1136/jech.53.2.105] [Medline: 10396471]\n23. Welcome to Discern. Discern. URL: http://www.discern.org.uk/index.php [accessed 2023-09-05]\n24. ChatGPT-release notes. OpenAI. URL: https://help.openai.com/en/articles/6825453-chatgpt-release-notes [accessed\n2023-09-03]\n25. Tinn R, Cheng H, Gu Y, Usuyama N, Liu X, Naumann T, et al. Fine-tuning large neural language models for biomedical\nnatural language processing. Patterns (N Y) 2023 Apr 14;4(4):100729 [FREE Full text] [doi: 10.1016/j.patter.2023.100729]\n[Medline: 37123444]\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 12https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX\n26. Mehdi Y. Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web. Microsoft. URL:\nhttps://tinyurl.com/4sum8vfk [accessed 2023-03-29]\n27. Wiggers K. OpenAI connects ChatGPT to the internet. TechCrunch. URL: https://techcrunch.com/2023/03/23/\nopenai-connects-chatgpt-to-the-internet/ [accessed 2023-05-08]\n28. Ziegler D, Stiennon N, Wu J, Brown T, Radford A, Amodei D, et al. Fine-Tuning Language Models from Human Preferences.\narXiv Preprint posted online on September 18, 2019. [FREE Full text] [doi: 10.48550/arXiv.1909.08593]\n29. Sheu SJ. Endophthalmitis. Korean J Ophthalmol 2017 Aug;31(4):283-289 [FREE Full text] [doi: 10.3341/kjo.2017.0036]\n[Medline: 28752698]\n30. Conic RZ, Cabrera CI, Khorana AA, Gastman BR. Determination of the impact of melanoma surgical timing on survival\nusing the National Cancer Database. J Am Acad Dermatol 2018 Jan;78(1):40-46.e7 [FREE Full text] [doi:\n10.1016/j.jaad.2017.08.039] [Medline: 29054718]\n31. Yildiz U, Schleicher P, Castein J, Kandziora F. Conservative treatment of thoracic and lumbar vertebral fractures - what's\nit all about? Z Orthop Unfall 2019 Oct;157(5):574-596 [doi: 10.1055/a-0824-8692] [Medline: 31594004]\n32. von Rüden C, Kühl R, Erichsen CJ, Kates SL, Hungerer S, Morgenstern M. Current concepts for the treatment of skin and\nsoft tissue infections in orthopaedic and trauma surgery. Z Orthop Unfall 2018 Aug;156(4):452-470 [doi:\n10.1055/s-0043-118136] [Medline: 30142686]\n33. Gugliotta M, da Costa BR, Dabis E, Theiler R, Jüni P, Reichenbach S, et al. Surgical versus conservative treatment for\nlumbar disc herniation: a prospective cohort study. BMJ Open 2016 Dec 21;6(12):e012938 [FREE Full text] [doi:\n10.1136/bmjopen-2016-012938] [Medline: 28003290]\n34. Saltsman K. Serotonin drives vicious cycle of itching and scratching. National Institute of Arthritis and Musculoskeletal\nand Skin Diseases. URL: https://tinyurl.com/mtfv53dt [accessed 2023-09-11]\n35. Pichai S. An important next step on our AI journey. Google. URL: https://blog.google/technology/ai/\nbard-google-ai-search-updates/ [accessed 2023-06-22]\n36. Llama 2: open foundation and fine-tuned chat models. Meta. URL: https://ai.meta.com/research/publications/\nllama-2-open-foundation-and-fine-tuned-chat-models/ [accessed 2023-09-05]\n37. Kojima T, Gu S, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot reasoners. arXiv Preprint posted\nonline on May 24, 2022. [FREE Full text]\nAbbreviations\nAI: artificial intelligence\nLLM: large language model\nEdited by A Mavragani; submitted 24.05.23; peer-reviewed by D Chrimes, D Xu, Y Zhuang; comments to author 05.09.23; revised\nversion received 12.09.23; accepted 29.09.23; published 30.10.23\nPlease cite as:\nWilhelm TI, Roos J, Kaczmarczyk R\nLarge Language Models for Therapy Recommendations Across 3 Clinical Specialties: Comparative Study\nJ Med Internet Res 2023;25:e49324\nURL: https://www.jmir.org/2023/1/e49324\ndoi: 10.2196/49324\nPMID: 37902826\n©Theresa Isabelle Wilhelm, Jonas Roos, Robert Kaczmarczyk. Originally published in the Journal of Medical Internet Research\n(https://www.jmir.org), 30.10.2023. This is an open-access article distributed under the terms of the Creative Commons Attribution\nLicense (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any\nmedium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete\nbibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license\ninformation must be included.\nJ Med Internet Res 2023 | vol. 25 | e49324 | p. 13https://www.jmir.org/2023/1/e49324\n(page number not for citation purposes)\nWilhelm et alJOURNAL OF MEDICAL INTERNET RESEARCH\nXSL•FO\nRenderX"
}