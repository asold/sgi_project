{
  "title": "ChatGPT-3.5 Versus Google Bard: Which Large Language Model Responds Best to Commonly Asked Pregnancy Questions?",
  "url": "https://openalex.org/W4401044302",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4298600323",
      "name": "Keren Khromchenko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5060727569",
      "name": "Sameeha Shaikh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2608175666",
      "name": "Meghana Singh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2804975951",
      "name": "Gregory Vurture",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Rima A Rana",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148087928",
      "name": "Jonathan D. Baum",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4386200227",
    "https://openalex.org/W4379467554",
    "https://openalex.org/W4387393234",
    "https://openalex.org/W4324370593",
    "https://openalex.org/W4386746721",
    "https://openalex.org/W4386644975",
    "https://openalex.org/W3030333530",
    "https://openalex.org/W2085227760",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4206939956",
    "https://openalex.org/W4236194499",
    "https://openalex.org/W4252773824",
    "https://openalex.org/W2939817495",
    "https://openalex.org/W4248020592",
    "https://openalex.org/W4234991052",
    "https://openalex.org/W4206603089",
    "https://openalex.org/W2320842834",
    "https://openalex.org/W2014730499",
    "https://openalex.org/W4297659097",
    "https://openalex.org/W2972905028"
  ],
  "abstract": "Large language models (LLM) have been widely used to provide information in many fields, including obstetrics and gynecology. Which model performs best in providing answers to commonly asked pregnancy questions is unknown. A qualitative analysis of Chat Generative Pre-Training Transformer Version 3.5 (ChatGPT-3.5) (OpenAI, Inc., San Francisco, California, United States) and Bard, recently renamed Google Gemini (Google LLC, Mountain View, California, United States), was performed in August of 2023. Each LLM was queried on 12 commonly asked pregnancy questions and asked for their references. Review and grading of the responses and references for both LLMs were performed by the co-authors individually and then as a group to formulate a consensus. Query responses were graded as \"acceptable\" or \"not acceptable\" based on correctness and completeness in comparison to American College of Obstetricians and Gynecologists (ACOG) publications, PubMed-indexed evidence, and clinical experience. References were classified as \"verified,\" \"broken,\" \"irrelevant,\" \"non-existent,\" and \"no references.\" Grades of \"acceptable\" were given to 58% of ChatGPT-3.5 responses (seven out of 12) and 83% of Bard responses (10 out of 12). In regard to references, ChatGPT-3.5 had reference issues in 100% of its references, and Bard had discrepancies in 8% of its references (one out of 12). When comparing ChatGPT-3.5 responses between May 2023 and August 2023, a change in \"acceptable\" responses was noted: 50% versus 58%, respectively. Bard answered more questions correctly than ChatGPT-3.5 when queried on a small sample of commonly asked pregnancy questions. ChatGPT-3.5 performed poorly in terms of reference verification. The overall performance of ChatGPT-3.5 remained stable over time, with approximately one-half of responses being \"acceptable\" in both May and August of 2023. Both LLMs need further evaluation and vetting before being accepted as accurate and reliable sources of information for pregnant women.",
  "full_text": null,
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.7435128092765808
    },
    {
      "name": "Grading (engineering)",
      "score": 0.6934406757354736
    },
    {
      "name": "Pregnancy",
      "score": 0.433893620967865
    },
    {
      "name": "Correctness",
      "score": 0.422702431678772
    },
    {
      "name": "Family medicine",
      "score": 0.382644385099411
    },
    {
      "name": "Medical education",
      "score": 0.3367612063884735
    },
    {
      "name": "Computer science",
      "score": 0.1323818862438202
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}