{
  "title": "Meta-learning for transformer-based prediction of potent compounds",
  "url": "https://openalex.org/W4387037895",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2775508793",
      "name": "Hengwei Chen",
      "affiliations": [
        "Lamarr Institute for Machine Learning and Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A192274276",
      "name": "Jürgen Bajorath",
      "affiliations": [
        "Lamarr Institute for Machine Learning and Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2775508793",
      "name": "Hengwei Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A192274276",
      "name": "Jürgen Bajorath",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937307539",
    "https://openalex.org/W3113447514",
    "https://openalex.org/W2801991413",
    "https://openalex.org/W3099252273",
    "https://openalex.org/W4224232334",
    "https://openalex.org/W1908851533",
    "https://openalex.org/W3023042104",
    "https://openalex.org/W2952635193",
    "https://openalex.org/W2738737407",
    "https://openalex.org/W2970175280",
    "https://openalex.org/W4311436943",
    "https://openalex.org/W4312810284",
    "https://openalex.org/W4375856114",
    "https://openalex.org/W3166272013",
    "https://openalex.org/W4321614295",
    "https://openalex.org/W4362520745",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4378909195",
    "https://openalex.org/W4380225176",
    "https://openalex.org/W3028589594",
    "https://openalex.org/W3018980093",
    "https://openalex.org/W4384296638",
    "https://openalex.org/W2165618135",
    "https://openalex.org/W1991238353",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3042826782",
    "https://openalex.org/W2145680191",
    "https://openalex.org/W4309656728",
    "https://openalex.org/W1975875968",
    "https://openalex.org/W2911155903",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W4220802400",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4323338513"
  ],
  "abstract": "Abstract For many machine learning applications in drug discovery, only limited amounts of training data are available. This typically applies to compound design and activity prediction and often restricts machine learning, especially deep learning. For low-data applications, specialized learning strategies can be considered to limit required training data. Among these is meta-learning that attempts to enable learning in low-data regimes by combining outputs of different models and utilizing meta-data from these predictions. However, in drug discovery settings, meta-learning is still in its infancy. In this study, we have explored meta-learning for the prediction of potent compounds via generative design using transformer models. For different activity classes, meta-learning models were derived to predict highly potent compounds from weakly potent templates in the presence of varying amounts of fine-tuning data and compared to other transformers developed for this task. Meta-learning consistently led to statistically significant improvements in model performance, in particular, when fine-tuning data were limited. Moreover, meta-learning models generated target compounds with higher potency and larger potency differences between templates and targets than other transformers, indicating their potential for low-data compound design.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports\nMeta‑learning \nfor transformer‑based prediction \nof potent compounds\nHengwei Chen  & Jürgen Bajorath *\nFor many machine learning applications in drug discovery, only limited amounts of training data \nare available. This typically applies to compound design and activity prediction and often restricts \nmachine learning, especially deep learning. For low‑data applications, specialized learning strategies \ncan be considered to limit required training data. Among these is meta‑learning that attempts to \nenable learning in low‑data regimes by combining outputs of different models and utilizing meta‑data \nfrom these predictions. However, in drug discovery settings, meta‑learning is still in its infancy. In this \nstudy, we have explored meta‑learning for the prediction of potent compounds via generative design \nusing transformer models. For different activity classes, meta‑learning models were derived to predict \nhighly potent compounds from weakly potent templates in the presence of varying amounts of fine‑\ntuning data and compared to other transformers developed for this task. Meta‑learning consistently \nled to statistically significant improvements in model performance, in particular, when fine‑tuning \ndata were limited. Moreover, meta‑learning models generated target compounds with higher potency \nand larger potency differences between templates and targets than other transformers, indicating \ntheir potential for low‑data compound design.\nPredicting new active compounds is one of the major tasks in computer-aided drug discovery, for which machine \nlearning approaches have been widely applied over the past two  decades1,2. In recent years, deep learning has \nalso been increasingly applied for compound activity and property  predictions1,2. The prediction of compounds \nexhibiting a desired biological activity (that is, activity against a target of interest) is mostly attempted using \nmachine learning models for binary classification (that is, a compound is predicted to have or not to have \na specific activity) 3–5. For this purpose, models for class label prediction (active versus inactive compounds) \nare typically derived based on training sets of known specifically active compounds and randomly selected \ncompounds assumed to be inactive. These qualitative activity predictions mostly involve virtual screening of \ncompound databases to identify new hits. In addition to qualitative predictions of biological activity, predicting \ncompounds that are highly potent against a given target also is of interest. Compound potency prediction can \nbe quantitative or semi-quantitative in nature. Quantitative predictions aim to specify numerical potency values \nusing, for example, quantitative structure–activity relationship (QSAR) 6,7 or free energy  methods8,9. Different \nfrom qualitative predictions and virtual screening, quantitative potency predictions are usually carried out for \nsmall compound sets or structural analogues from lead series. Furthermore, semi-quantitative approaches aim \nto predict new potent compounds, that is, compounds having higher potency than known actives. For example, \nsuch predictions might focus on activity  cliffs10, which are defined as pairs of structurally similar compounds \nor structural analogues with large potency  differences10. Prediction of activity cliffs fall outside the applicability \ndomain of standard QSAR  methods4.\nWhile quantitative potency predictions are widely carried out, they are difficult to evaluate in benchmark \nsettings. It has been observed that benchmark predictions of different machine learning models and randomized \npredictions are typically only separated by small error  margins11, which makes it difficult to non-ambiguously \nassess relative method  performance 11. Therefore, we currently prefer semi-quantitative approaches focusing \non the prediction of potent compounds (rather than trying to predict compound potency values across wide \npotency ranges). Semi-quantitative predictions can be attempted by deep generative  modeling2. For example, \ntransformer models have been derived based on pairs of active structural analogues with varying potency to \npredict activity cliffs and design potent  compounds12,13. Therefore, the transformer models were conditioned on \nobserved potency differences. This generative design approach successfully reproduced highly potent compounds \nOPEN\nDepartment of Life Science Informatics and Data Science, B-IT, Lamarr Institute for Machine Learning and Artificial \nIntelligence, LIMES Program Unit Chemical Biology and Medicinal Chemistry, Rheinische Friedrich-Wilhelms-\nUniversität, Friedrich-Hirzebruch-Allee 5/6, 53115 Bonn, Germany. *email: bajorath@bit.uni-bonn.de\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nfor different activity classes based on weakly potent input  compounds13. Transformer models have also been \nderived for other compound property  predictions14–16 and generative compound design  applications17–19 as well \nas for the prediction of drug-target  interactions20–22.\nNotably, all compound activity and potency predictions depend on available data for learning. Like many \nother data in early-phase drug discovery, high-quality compound potency measurements for given targets are \ngenerally sparse, which limits generative design. Therefore, we are considering machine learning approaches for \nlow-data regimes to enable predictions of potent compounds for targets, for which only little compound data is \navailable. Among learning strategies for sparsely distributed data, active  learning23,24 and transfer  learning25,26 \nhave been investigated for machine learning in drug discovery in various  studies24,26. Transfer learning attempts \nto use information obtained from related prediction tasks to streamline model derivation for such tasks, while \nactive learning focuses on the selection of most informative training instances for iterative model building. Meta-\nlearning including few-shot learning represents another low-data approach that is relevant for drug  discovery27–30. \nIn artificial intelligence, meta-learning is a sub-discipline of machine  learning27. It aims to combine the output \nof different machine learning models and/or meta-data from these models such as parameters derived from \ntraining instances to generate models for other prediction  tasks27. Alternatively, the same algorithm might be \napplied to generate models for individual prediction tasks whose outputs are then used to iteratively update a \nmeta-learning model. Hence, meta-learning can also be regarded as a form of ensemble learning. The general aim \nof meta-learning is achieving transferability of models to related prediction tasks, including the application of \nprior model knowledge to limit the number of training instances required for new tasks. Given the use of meta-\ndata for learning, the approach is well-suited for parameter-rich deep learning  architectures28 and -compared to \ntransfer learning- principally applicable to a wider spectrum of predictions tasks. However, in compound design \nand property prediction, the exploration of meta-learning is still in its early stages. Therefore, we have explored \nmeta-learning in semi-quantitative potency predictions. To this end, we have adapted a transformer architec-\nture designed for the prediction of potent  compounds13 as a base model for deriving meta-learning models and \nassessed the potential of meta-learning for predicting highly potent compounds for different activity classes and \nvarying amounts of training data.\nMethods\nCompounds, activity data, and analogue series\nBioactive compounds with high-confidence activity data were collected from ChEMBL (release 29) 31. Only \ncompounds with direct interactions (assay relationship type: \"D\") with human targets at the highest assay con -\nfidence level (assay confidence score 9) were considered. In addition, potency measurements were restricted to \nnumerically specified equilibrium constants  (Ki values), which were recorded as (negative decadic logarithmic) \n pKi values. When multiple measurements were available for the same compound, the geometric mean was cal-\nculated as the final potency annotation, provided all values fell within the same order of magnitude. If not, the \ncompound was disregarded. Qualifying compounds were organized into target-based activity classes.\nIn activity classes, analogue series (AS) with one to five substitution sites were identified using the compound-\ncore relationship (CCR)  algorithm32. The core structure of an AS was required to consist of at least twice the \nnumber of non-hydrogen atoms as the combined substituents. For each AS, all possible pairs of analogues were \ngenerated, termed All_CCR pairs. For each activity class, ALL_CCR pairs from all AS were pooled. All_CCR \npairs were then divided into CCR pairs with a potency difference of less than 100-fold and activity cliff (AC)-\nCCR pairs with a potency difference of at least 100-fold.\nOn the basis of the specified data curation criteria and AS distributions, 10 activity classes were assembled \nthat consisted of at least ~ 500 qualifying compounds and ~ 50 AS, as summarized in Table  1. These activity \nclasses included ligands of various G protein coupled receptors and inhibitors of different enzymes. Figure  1 \nshows exemplary AC_CCR pairs for each class.\nTable 1.  Activity classes. The composition of activity classes is summarized. For each class, the ChEMBL \ntarget ID and target name are provided.\nChEMBL ID Target name Compounds AS CCR pairs AC-CCR pairs\n226 Adenosine A1 receptor 1924 318 18,623 1207\n234 Dopamine D3 receptor 1529 213 21,008 755\n237 Kappa opioid receptor 940 129 19,277 2897\n244 Coagulation factor X 702 92 9718 1288\n251 Adenosine A2a receptor 1825 312 16,084 870\n259 Melanocortin receptor 4 543 145 25,126 3086\n264 Histamine H3 receptor 1235 173 10,812 532\n1862 Tyrosine-protein kinase ABL 499 64 15,573 1873\n2014 Nociceptin receptor 512 52 11,472 1058\n4792 Orexin receptor 2 1133 131 12,368 1271\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nMeta‑learning approach\nThe basic premise of meta-learning, as investigated herein, is parameterizing a model on a series of training tasks \nby combining and updating parameter settings across individual tasks. This process aims to improve the ability \nof the model to adapt to new prediction tasks through the use of meta-data.\nFor designing the meta-learning module of Meta-CLM, we adopted the model-agnostic meta-learning \n(MAML)  framework28 for an activity class-specific prediction task distribution p(T). Given its model-agnostic \nnature, the only assumption underlying the MAML approach is that a given model is parameterized using a \nparameter vector θ. Accordingly, a meta-learning model is considered as a function fθ with parameter vector θ. \nThe model aims to learn parameter settings θmeta that are derived for individual training tasks and updated across \ndifferent tasks such that they can be effectively adjusted to new prediction tasks. Therefore, for each of a series \nof prediction tasks, training data are randomly divided into a support set and a query set Accordingly, when the \nmeta-learning module is applied to a new prediction task Ti such as an activity class the current parameter vector \nθmeta is updated for task Ti with activity class-specific parameters θ i obtained by gradient descent optimization \nminimizing training errors.\nDuring meta-training, as summarized in Fig. 2, the model fθ is first updated to a task-specific model fθ′ using \nits support set. Then, the corresponding query set is used to determine the prediction loss of model f θ′ for this \ntask. The procedure is repeated for all prediction tasks (activity classes). Finally, model parameters are further \nadjusted for testing by minimizing the sum of the prediction loss over all activity classes. Model derivation based \non the support sets and evaluation based on query sets are implemented as inner and outer loops, respectively. \nFor meta-testing, the trained meta-learning module is fine-tuned on a specific activity class, for which parameters \nare adjusted, as also illustrated in Fig. 2. For each class, an individual fine-tuned model is generated.\nThe meta-learning process aims to capture prior training information through initial parameter vector \nadjustments, followed by updates through monitoring of the joint loss across all training  tasks29. Capturing \nprior training knowledge should enable the model to more effectively adapt to new prediction tasks based on \nFigure 1.  Analogue pairs representing activity cliffs. For each activity class, exemplary AC_CCR pairs are \nshown and their potency differences are reported. Numbers on arrows identify activity classes according to \nTable 1. Core structures and substituents are colored blue and red, respectively.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nadvanced parameter settings available for initialization and shorter optimization paths with reduced training \ndata  requirments33,34.\nThis algorithmic approach differs from conventional multi-task learning where a single model is trained on \nmultiple tasks, aiming to share representations and knowledge between these tasks to collectively improve the \nbasis for learning. Hence, the primary goal of multi-task learning is to improve predictive performance for all \ntasks by leveraging commonalities between them. Accordingly, model weights are updated based on a combina-\ntion of the losses from all tasks in a single optimization step. Shared representations for multiple tasks support \nthe model’s ability to simultaneously learn features common to these tasks.\nTransformer models\nBase model\nFor meta-learning, the transformer architecture derived previously for the prediction of highly potent compounds \nbased on weakly potent templates was  adopted13. Figure  3 illustrates the architecture of the base CLM. The \ntransformer consisted of multiple encoder-decoder modules with attention  mechanism35 and was designed for \ntranslating string-based representations of chemical structure. Accordingly, the transformer can be perceived as \na chemical language model (CLM). The base model (referred to as CLM in the following) was devised to predict \ncompounds with higher potency for given input  compounds13. An encoder module consisted of encoding sub-\nlayers including a multi-head self-attention sub-layer and a fully connected feed-forward network sub-layer. The \nencoder compressed an input sequence into a context vector in its final hidden state, providing the input for the \ndecoder module composed of a feed-forward sub-layer and two multi-head attention sub-layers. The decoder \ntransformed the context vector into a sequence of tokens. Both the encoder and decoder modules utilized the \nattention mechanism during training to effectively learn from the underlying feature space.\nFigure 2.  Meta-learning. The illustration summarizes training, fine-tuning, and testing of the meta-learning \nmodule of Meta-CLM using exemplary AC-CCR pairs. For each activity class, the support set is used for the \ninitial parameterization of the model (θ). The support loss Lsupport is calculated for updating model parameters \n(θ′). Then, the query set is used to calculate the prediction loss L′\nquery for this task. The process is repeated for \nall training classes, followed by summation of L′\nquery over all tasks to further adjust the parameter settings. The \ntrained module then enters the fine-tuning and testing phase. Solid and dashed lines indicate inner and outer \nloops, respectively, for meta-training and -testing including fine-tuning.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nDuring training, the CLM was challenged to learn mappings of template/source compounds (SCs) to target \ncompounds (TCs) conditioned on potency differences (ΔPot) resulting from replacements of substituent(s):\nHence, training focused on structural analogues with specific potency differences. Then, given a new (SC,  \nΔPot) test instance, the model generated a set of structurally related TCs with putatively higher potency than SCs.\nFor transformer modeling, compounds and potency differences must be tokenized. Accordingly, compounds \nwere represented as molecular-input line-entry system (SMILES)  strings36 generated using  RDKit37. Tokenization \nwas facilitated by representing atoms with single-character tokens (e.g., \"C\"or \"N\"), two-character tokens (e.g., \n\"Cl\" or \"Br\"), or tokens enclosed in brackets (e.g. \"[nH]\" or \"[O-]\"). Potency differences were subjected to binning \n tokenization12,13,38,39 by dividing the global range of potency differences (-6.62 to 6.52  pKi units) into 1314 bins \nwith a constant width of 0.01. Each bin was encoded by a single token and each potency difference was assigned \nto the corresponding  token12,13. In addition, two special \"start\" and \"end\" tokens were defined as the start and \nend points of a sequence, respectively.\nThe model was pre-trained using a large set of 881,990 All_CCR pairs originating from 496 public activity \n classes13. For pre-training, All_CCR triples (CpdA, CpdB, PotB-PotA) were generated in in which CpdA and CpdB \nrepresented the SC and TC, respectively, and (PotB-PotA) their potency difference.\nCLM was implemented using  Pytorch40. Default hyperparameter settings were used for the transformer archi-\ntecture together with a batch size of 64, learning rate of 0.001, and encoding dimension of 256. During training, \nthe transformer model minimized the cross-entropy loss between the ground-truth and output sequence. The \nAdam optimizer was  used41. The model was trained for a maximum of 1000 epochs. At each epoch, a checkpoint \nwas saved, and the final model was selected based on the minimal loss.\nThe base model achieved a reproducibility of 0.857 for the entire test set (corresponding to 10% of pre-\ntraining set). Hence, the base CLM model regenerated ~ 86% of the target compounds from CCR-triples not \nused for training.\nModel for meta-learning\nThe CLM variant for meta-learning was also implemented using Pytorch following the protocol described above. \nThe meta-learning model, designated Meta-CLM, consisted of two modules including the base model for gener-\nating mappings of SCs to TCs conditioned on potency differences and the meta-learning module (the design of \nwhich is detailed below). For derivation of the metal-learning module, a subset of 176 of the 496 activity classes \nwas selected for which at least 300 All-CCR pairs per class were available, amounting to a total of 491,688 qualify-\ning All_CCR triples. For meta-learning, each activity class was considered a separate training task (see below). \nTherefore, All_CCR triples from each class were randomly split into support set (80%) and query set (20%). The \nAdam optimizer was used for gradient descent optimization during meta-learning.\n(SC , �Pot) → (TC ).\nFigure 3.  Base CLM. The architecture of the base CLM for designing potent compounds is schematically \nillustrated (the representation was adapted from ref. 13).\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nModel fine-tuning\nFor fine-tuning and comparative evaluation of CLM and Meta-CLM, the 10 activity classes in Table 1 were used. \nFine-tuning was separately carried out using AC-CCR pairs from each class. The AC-CCR pairs from each class \nwere randomly divided into fine-tuning (80%) and test instances (20%). In each case, it was confirmed that \nthe fine-tuning and test pairs had no core structure overlap (otherwise, a new partition was generated). For \nfine-tuning, AC_CCR pairs were exclusively used. AC_CCR triples were ordered such that TC was the highly \npotent compound. To assess the ability of CLM and Meta-CLM to learn in low-data regimes, model variants \nwere derived based on 10%, 25%, 50% and 100% of the training data. To adapt to differently sized training sets, \nthe pre-trained model was fine-tuned with a smaller learning rate of 0.0001. With a maximum of 200 training \nepochs, the final fine-tuned model was selected based on minimal cross-entropy loss.\nModel evaluation\nFor each activity class, CCR pairs sharing core structures with the fine-tuning set were excluded, then the final \ntest set was generated by adding the remaining CCR pairs to test AC-CCR pairs. Test set CCR and AC-CCR pairs \nyielded class-dependent numbers of unique CCR and AC-CCR test compounds. To evaluate the performance \nof each fine-tuned CLM and corresponding Meta-CLM, test compounds were divided into two categories: SCs \nwith a maximum potency of 1 μM (corresponding to a  pKi value of 6) and TCs with a potency greater than 1 \nμMol  (pKi > 6). These test TCs were termed known target compounds (KTCs), which represented highly potent \ntest compounds. Table 2 reports the test composition for each activity class. Depending on the activity class, 139 \nto 3838 KTCs were available.\nFor each test set SC, 50 hypothetical TCs were sampled and compared to available KTCs. The ability of a \nmodel to reproduce KTCs was considered as the key criterion for model validation.\nResults\nReproducibility of known target compounds\nWe first analyzed the ability of Meta-CLM to reproduce KTCs in comparison to CLM. The results are reported \nin Table 3. For all activity classes, Meta-CLM and CLM correctly reproduced multiple KTCs over all fine-tuning \nconditions, thus providing non-ambiguous proof for the models’ ability to predict potent compounds. From \ncorrectly predicted SC-KTC pairs, unique KTCs were extracted (a given KTC can occur in multiple pairs). The \nnumber of correctly predicted SC-KTC pairs and unique KTCs varied depending on the activity class. Impor -\ntantly, Meta-CLM consistently predicted more SC-KTC pairs and unique KTCs than CLM across all activity \nclasses, without an exception. For Meta-CLM, the number of SC-KTC pairs varied from 71 to 5102 pairs when \nutilizing 100% of the training samples and the number of unique KTCs varied from 27 to 287, corresponding \nto a reproducibility ratio of ~ 7% to ~ 45% of available KTCs per class. For comparison, CLM, the base model, \ngenerated from 53 to 4385 SC-KTC pairs, with 23 to 241 unique KTCs and a corresponding reproducibility ratio \nof ~ 5% to ~ 36% per class. Moreover, for decreasing numbers of fine-tuning samples, Meta-CLM consistently \nreproduced more KTCs than CLM. For complete fine-tuning sets, Meta-CLM and CLM reached mean repro-\nducibility rates of ~ 21% and ~ 14%, respectively. For only 10% of the fine-tuning samples, Meta-CLM reached a \nmean reproducibility rate of ~ 15% compared to only ~ 7% for CLM. Thus, Meta-CLM learned more effectively \nfrom sparse data than CLM, consistent with the aims of meta-learning.\nFigure 4 illustrates the differences in KTC reproducibility rates between Meta-CLM and CLM. Independent-\nsamples t-tests were carried out to assess the statistical significance of the observed differences. For complete \nfine-tuning sets, increases in reproducibility detected for Meta-CLM were statistically significant for three of 10 \nactivity classes. However, for fine-tuning sets of deceasing size, 25 of 30 increases across all activity classes were \nstatistically significant, thus providing further evidence for the ability of Meta-CLM to more effectively learn \nfrom sparse data. For most classes, there was a sharp decline in CLM reproducibility rates when 25% or 10% of \nthe fine-tuning samples were used.\nTable 2.  Test sets. For each activity class (ChEMBL IDs are used according to Table 1), the composition of the \ntest set is reported. CPD stands for compound.\nChEMBL ID CCR Pairs\nUnique CCR \nCPDs AC-CCR Pairs\nUnique \nAC-CCR CPDs\nOverlapping \nCPDs SCs  (pki <  = 6) KTCs  (pki > 6)\n226 5950 1174 144 84 80 359 819\n234 7790 913 50 53 53 89 824\n237 1032 477 31 24 20 115 366\n244 1949 308 287 118 88 90 248\n251 4706 5210 85 57 38 1391 3838\n259 702 169 59 69 33 66 139\n264 4756 840 72 81 58 33 830\n1862 4554 175 82 51 51 27 148\n2014 1388 256 80 62 29 23 266\n4792 1941 615 49 50 48 146 471\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nWe also note that both models produced large numbers of novel candidate compounds for SCs. For complete \nfine-tuning sets, Meta-CLM and CLM generated on average 2375 and 2818 new candidate compounds per activ-\nity class (ranging from 119 to 9952 and 234 to 10,779 candidates, respectively). While these new compounds \ncannot be considered for model validation, they provide large pools of candidates for practical applications in \nthe search for potent compounds.\nTable 3.  Reproducibility of compound pairs and known target compounds.\nChEMBL ID Ratio\nSC-KTC Pairs Unique KTCs Reproducibility (%)\nMeta-CLM CLM Meta-CLM CLM Meta-CLM CLM\n226\n10 799 379 223 118 27.2 14.4\n25 965 510 263 167 32.1 20.4\n50 1041 614 268 183 32.7 22.3\n100 1193 735 287 216 35.0 26.4\n234\n10 174 75 50 19 6.1 2.3\n25 268 130 68 36 8.3 4.4\n50 343 197 87 58 10.6 7.0\n100 398 239 101 71 12.3 8.6\n237\n10 397 325 90 52 24.6 14.2\n25 449 366 101 66 27.6 18.0\n50 433 362 103 81 28.1 22.1\n100 480 429 118 102 32.2 27.9\n244\n10 109 62 26 11 10.5 4.4\n25 111 66 31 17 12.5 6.9\n50 160 98 39 28 15.7 11.3\n100 193 129 45 36 18.2 14.5\n251\n10 3930 3288 233 138 6.1 3.6\n25 4685 3959 249 172 6.5 4.5\n50 4856 4153 245 201 6.4 5.2\n100 5102 4385 264 241 6.9 6.3\n259\n10 51 40 14 5 10.1 3.6\n25 73 60 24 13 17.3 9.4\n50 98 88 30 22 21.6 15.8\n100 129 116 33 30 23.7 21.6\n264\n10 16 11 14 6 1.7 0.7\n25 33 19 28 17 3.3 2.1\n50 54 33 42 31 5.1 3.7\n100 71 53 57 40 6.9 4.8\n1862\n10 65 29 25 6 16.9 4.0\n25 96 48 28 14 18.9 9.5\n50 93 56 32 23 21.6 15.5\n100 147 94 33 30 22.3 20.3\n2014\n10 85 53 20 9 7.5 3.4\n25 102 71 25 12 9.4 4.5\n50 113 84 22 16 8.3 6.0\n100 131 99 27 23 10.2 8.7\n4792\n10 849 622 176 106 37.4 22.5\n25 976 746 179 129 38.0 27.4\n50 1085 828 199 151 42.3 32.1\n100 1262 969 212 170 45.0 36.1\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nFigure 4.  Reproducibility of known target compounds. For each activity class, the proportion of correctly \nreproduced KTCs is reported for Meta-CLM and CLM over varying percentages of fine-tuning samples. Mean \nand standard deviations (error bars) are provided. To assess the statistical significance of observed differences \nbetween reproducibility rates, independent-samples t tests were conducted: 0.05 < p ≤ 1.00 (ns), 0.01 <  p ≤  0.05 \n(*), 0.001 <  p  ≤ 0.01 (**), 0.0001 <  p  ≤ 0.001 (***),  p ≤ 0.0001 (****). Stars denote increasing levels of statistical \nsignificance and “ns” stands for “not significant” .\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nFigure 5.  Potency value distribution of reproduced known target compounds. For all activity classes, boxplots \nreport the distributions of logarithmic potency values of KTCs correctly reproduced by Met-CLM and CLM \nover varying numbers of fine-tuning samples. To assess the statistical significance of differences between \npotency value distributions, independent-samples t tests were conducted: 0.05 <  p  ≤ 1.00 (ns), 0.01 <  p ≤ 0.05 (*), \n0.001 <  p ≤ 0.01 (**), 0.0001 <  p ≤ 0.001 (***),  p  ≤ 0.0001 (****).\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nCompound potency\nIn addition to reproducing KTCs, the actual potency level of correctly predicted KTCs and potency differences \nbetween SCs and corresponding KTCs represented other highly relevant criteria for model assessment. Accord-\ning to our semi-quantitative design approach, ideally, the models should predict highly potent compounds from \ngiven SCs. Therefore, we next analyzed the potency of correctly predicted KTCs and potency differences between \nMeta-CLM and CLM.\nKnown target compounds\nFigure 5 shows the distributions of logarithmic potency values of KTCs reproduced by Meta-CLM and CLM. \nImportantly, KTCs generated by Meta-CLM were overall consistently more potent than those generated by CLM \nacross all activity classes and fine-tuning conditions. Thirty-eight of the total of 40 observed differences between \nthe respective potency value distributions were statistically significant. Especially for 25% and 10% of the fine-\ntuning samples, Meta-CLM generated multiple KTCs with low-nanomolar or even sub-nanomolar potency \nfor each activity class, whereas CLM only generated a few KTCs with potency higher than 10 nM (pKi  > 8) for \nthree classes.\nPotency differences between source and target compounds\nFurthermore, we analyzed potency differences captured by SC-KTC pairs. Following our design strategy, increas-\ningly large potency differences between corresponding SCs and correctly reproduced KTCs were favored. Figure 6 \nshows the distribution of potency differences between corresponding SCs and KTCs for Meta-CLM and CLM \npredictions. In the case of Meta-CLM (CLM), four (six) activity classes displayed median potency differences \nbetween SCs and corresponding KTCs between one and two orders of magnitude (10- to100-fold) and the \nremaining six (four) classes displayed median potency differences exceeding two orders of magnitude (> 100-\nfold) for complete fine-tuning sets. Hence, significant potency differences were generally observed. For half of \nthe activity classes, median potency differences were comparable for all fine-tuning conditions when separately \nviewed for Meta-CLM and CLM, respectively. However, when Meta-CLM and CLM were compared, potency \ndifferences of SC-KTC pairs were consistently larger for Meta-CLM. Again, 38 of 40 observed differences were \nstatistically significant. Overall, many more KTCs with at least 1000-fold higher potency than the correspond -\ning SCs were generated by Meta-CLM compared to CLM. Thus, Meta-CLM predicted KTCs with overall higher \npotency than CLM and much larger potency differences between SCs and KTCs.\nConclusion\nIn this work, we have explored meta-learning for the prediction of potent compounds using conditional trans-\nformer models. Compound potency predictions are of high interest in drug discovery but high-quality activity \ndata available for machine learning are typically sparse. For these predictions, meta-learning was of particular \ninterest to us because the approach is well-suited for models that are rich in meta-data, yet currently only lit-\ntle explored for drug discovery applications. Therefore, we have adapted a previously investigated transformer \narchitecture to construct a meta-learning model by adding a special meta-learning module to a pre-trained \ntransformer. Then, meta-learning model variants were derived for different activity classes and their performance \nin the design of potent compounds was compared to reference transformers. For model validation, the ability \nto reproduce potent KTCs served as the major criterion. All models successfully reproduced KTCs. However, \ncompared to reference models, meta-learning significantly increased the number of correctly predicted KTCs \nacross all activity classes, especially for decreasing numbers of fine-tuning samples. This was an encouraging find-\ning, consistent with expectations for successful meta-learning. Moreover, meta-learning models also produced \ntarget compounds with overall higher potency than other transformers and larger potency differences between \ntemplates and targets. These improvements were not anticipated but are highly attractive for practical applica -\ntions. The generative models designed for predicting potent compounds produced large numbers of candidate \ncompounds with novel structures. New candidate compounds predicted by the meta-learning models should \nrepresent an attractive resource for prospective applications in searching for potent compounds for targets of \ninterest. Taken together, the results reported herein, provide proof-of-concept for the potential of meta-learning \nin generative design of potent compounds. Moreover, in light of our findings, we anticipate that meta-learning \nwill also be a promising approach for other compound design applications in low-data regimes.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nFigure 6.  Distribution of potency differences between source and known target compounds. For all activity \nclasses, boxplots report the distributions of logarithmic potency differences for SC-KTC pairs predicted by \nMeta-CLM and CLM over varying numbers of fine-tuning samples. To assess the statistical significance of \ndifferences between the distributions, independent-samples t tests were conducted: 0.05 <  p ≤ 1.00 (ns), 0.01 <  p  \n≤ 0.05 (*), 0.001 <  p ≤ 0.01 (**), 0.0001 <  p ≤ 0.001 (***),  p ≤ 0.0001 (****).\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nData availability\nCalculations were carried out using publicly available programs and compound data. Python scripts generated \nfor the study and the activity classes used are available via the following link: https:// uni- bonn. sciebo. de/s/ \nkfAQZ 0mbCG Htr0m.\nReceived: 22 June 2023; Accepted: 18 September 2023\nReferences\n 1. Vamathevan, J. et al. Applications of machine learning in drug discovery and development. Nat. Rev. Drug. Discov.  18, 463–477 \n(2019).\n 2. Walters, W . P . & Barzilay, R. Applications of deep learning in molecule generation and molecular property prediction. Acc. Chem. \nRes. 54, 263–270 (2020).\n 3. Lo, Y . C., Rensi, S. E., Torng, W . & Altman, R. B. Machine learning in chemoinformatics and drug discovery. Drug Discov. Today \n23, 1538–1546 (2018).\n 4. Patel, L., Shukla, T., Huang, X., Ussery, D. W . & Wang, S. Machine learning methods in drug discovery. Molecules 25, 5277 (2020).\n 5. Rodríguez-Pérez, R., Miljković, F . & Bajorath, J. Machine learning in chemoinformatics and medicinal chemistry. Annu. Rev. \nBiomed. Data Sci. 5, 43–65 (2022).\n 6. Lewis, R. A. & Wood, D. Modern 2D QSAR for drug discovery. WIREs Comput. Mol. Sci. 4, 505–522 (2014).\n 7. Muratov, E. N. et al. QSAR without borders. Chem. Soc. Rev. 49, 3525–3564 (2020).\n 8. Mobley, D. L. & Gilson, M. K. Predicting binding free energies: Frontiers and benchmarks. Annu. Rev. Biophys. 46, 531–558 (2017).\n 9. Williams-Noonan, B. J., Yuriev, E. & Chalmers, D. K. Free energy methods in drug design: Prospects of “ Alchemical perturbation” \nin medicinal chemistry. J. Med. Chem. 61, 638–649 (2018).\n 10. Stumpfe, D., Hu, H. & Bajorath, J. Evolving concept of activity cliffs. ACS Omega 4, 14360–14368 (2019).\n 11. Janela, T. & Bajorath, J. Simple nearest-neighbour analysis meets the accuracy of compound potency predictions using complex \nmachine learning models. Nat. Mach. Intell. 4, 1246–1255 (2022).\n 12. Chen, H., Vogt, M. & Bajorath, J. DeepAC–conditional transformer-based chemical language model for the prediction of activity \ncliffs formed by bioactive compounds. Dig. Discov. 1, 898–909 (2022).\n 13. Chen, H. & Bajorath, J. Designing highly potent compounds using a chemical language model. Sci. Rep. 13, 7412 (2023).\n 14. Chen, D. et al. Algebraic graph-assisted bidirectional transformers for molecular property prediction. Nat. Commun.  12, 3521 \n(2021).\n 15. Song, Y ., Chen, J., Wang, W ., Chen, G. & Ma, Z. Double-head transformer neural network for molecular property prediction. J. \nCheminform. 15, 27 (2023).\n 16. Jiang, Y . et al. Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction. Commun. \nChem. 6, 60 (2023).\n 17. Bagal, V ., Aggarwal, R., Vinod, P . K. & Priyakumar, U. D. MolGPT: Molecular generation using a transformer-decoder model. J. \nChem. Inf. Model. 62, 2064–2076 (2022).\n 18. Mazuz, E., Shtar, G., Shapira, B. & Rokach, L. Molecule generation using transformers and policy gradient reinforcement learning. \nSci. Rep. 13, 8799 (2023).\n 19. Wang, Y ., Zhao, H., Sciabola, S. & Wang, W . cMolGPT: a conditional generative pre-trained transformer for target-specific de novo \nmolecular generation. Molecules 28, 4430 (2023).\n 20. Chen, L. et al. TransformerCPI: Improving compound–protein interaction prediction by sequence-based deep learning with self-\nattention mechanism and label reversal experiments. Bioinformatics 36, 4406–4414 (2020).\n 21. Huang, K., Xiao, C., Glass, L. M. & Sun, J. MolTrans: molecular interaction transformer for drug–target interaction prediction. \nBioinformatics 37, 830–836 (2021).\n 22. Chen, L. et al. Sequence-based drug design as a concept in computational drug design. Nat. Commun. 14, 4217 (2023).\n 23. Warmuth, M. K. et al. Active learning with support vector machines in the drug discovery process. J. Chem. Inf. Comput. Sci. 43, \n667–673 (2003).\n 24. Reker, D. & Schneider, G. Active-learning strategies in computer-assisted drug discovery. Drug Discov. Today 20, 458–465 (2015).\n 25. Weiss, K., Khoshgoftaar, T. M. & Wang, D. A survey of Transfer Learning. J. Big Data 3, 9 (2016).\n 26. Cai, C. et al. Transfer learning for drug discovery. J. Med. Chem. 63, 8683–8694 (2020).\n 27. Vilalta, R. & Drissi, Y . A Perspective View and Survey of Meta-Learning. Artif. Intell. Rev. 18, 77–95 (2002).\n 28. Finn, C., Abbeel, P . & Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of 34th Inter-\nnational Conference on Machine Learning (Eds. Precup, D. & Teh, Y . W .) 1126–1135 (JMLR.org, 2017).\n 29. Wang, Y ., Y ao, Q., Kwok, J. T. & Ni, L. M. Generalizing from a few examples: A survey on few-shot learning. ACM Comput. Surv. \n53, 1–34 (2020).\n 30. Vella, D. & Ebejer, J. P . Few-shot learning for low-data drug discovery. J. Chem. Inf. Model. 63, 27–42 (2023).\n 31. Bento, A. P . et al. The CHEMBL bioactivity database: An update. Nucleic Acids Res. 42, D1083–D1090 (2014).\n 32. Naveja, J. J., Vogt, M., Stumpfe, D., Medina-Franco, J. L. & Bajorath, J. Systematic extraction of analogue series from large compound \ncollections using a new computational compound–core relationship method. ACS Omega 4, 1027–1032 (2019).\n 33. Raghu, A., Raghu, M., Bengio, S. & Vinyals, O. Rapid learning or feature reuse? towards understanding the effectiveness of MAML. \nIn 8th International Conference on Learning Representations (OpenReview.net, 2020).\n 34. Lv, Q., Chen, G., Y ang, Z., Zhong, W . & Chen, C. Y . C. Meta learning with graph attention networks for low-data drug discovery. \nIEEE Trans. Neural Netw. Learn. Syst. 6, 1–13 (2023).\n 35. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 5, 6000–6010 (2017).\n 36. Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. \nInf. Comput. Sci. 28, 31–36 (1988).\n 37. RDKit: Cheminformatics and Machine Learning Software. http:// www. rdkit. org (Accessed on 1 July 2021).\n 38. He, J. et al. Molecular optimization by capturing chemist’s intuition using Deep Neural Networks. J. Cheminform. 13, 26 (2021).\n 39. He, J. et al. Transformer-based molecular optimization beyond matched Molecular Pairs. J. Cheminform. 14, 18 (2022).\n 40. Paszke, A. et al. PyTorch: An imperative style, high-performance deep learning library. Adv. Neural Inf. Process. Syst. 32, 8026–8037 \n(2019).\n 41. Kingma, D.P . & Ba, J. Adam: a method for stochastic optimization. In 3th International Conference on Learning Representations \n(OpenReview.net, 2015).\nAcknowledgements\nThe authors thank Martin Vogt for many helpful suggestions. H.C. is supported by the China Scholarship Council \n(CSC).\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:16145  | https://doi.org/10.1038/s41598-023-43046-5\nwww.nature.com/scientificreports/\nAuthor contributions\nAll authors contributed to designing and conducting the study, analyzing the results, and preparing the \nmanuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to J.B.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7248466610908508
    },
    {
      "name": "Machine learning",
      "score": 0.7153533101081848
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6345422267913818
    },
    {
      "name": "Transformer",
      "score": 0.6159285306930542
    },
    {
      "name": "Template",
      "score": 0.5495815873146057
    },
    {
      "name": "Training set",
      "score": 0.4805217683315277
    },
    {
      "name": "Drug discovery",
      "score": 0.47908979654312134
    },
    {
      "name": "Deep learning",
      "score": 0.4269437789916992
    },
    {
      "name": "Meta learning (computer science)",
      "score": 0.4159669876098633
    },
    {
      "name": "Multi-task learning",
      "score": 0.410235196352005
    },
    {
      "name": "Bioinformatics",
      "score": 0.24973857402801514
    },
    {
      "name": "Task (project management)",
      "score": 0.22060370445251465
    },
    {
      "name": "Biology",
      "score": 0.11016020178794861
    },
    {
      "name": "Engineering",
      "score": 0.0889115035533905
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}