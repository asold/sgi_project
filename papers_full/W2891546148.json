{
    "title": "Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling",
    "url": "https://openalex.org/W2891546148",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2139640379",
            "name": "Zhiqing Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2245903860",
            "name": "Zhi-Hong Deng",
            "affiliations": [
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2592647456",
        "https://openalex.org/W2395662515",
        "https://openalex.org/W25062297",
        "https://openalex.org/W2964260331",
        "https://openalex.org/W3149336535",
        "https://openalex.org/W2251479516",
        "https://openalex.org/W2963667932",
        "https://openalex.org/W2140991203",
        "https://openalex.org/W2250330186",
        "https://openalex.org/W2092977545",
        "https://openalex.org/W2252225757",
        "https://openalex.org/W2404744763",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2100163972",
        "https://openalex.org/W4297801177",
        "https://openalex.org/W2250739653",
        "https://openalex.org/W1533861849",
        "https://openalex.org/W2251362855",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2949193663",
        "https://openalex.org/W1512811194",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2138136853",
        "https://openalex.org/W2126377586",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963077280",
        "https://openalex.org/W2964194354",
        "https://openalex.org/W2964121744"
    ],
    "abstract": "Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.",
    "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4915â€“4920\nBrussels, Belgium, October 31 - November 4, 2018.câƒ2018 Association for Computational Linguistics\n4915\nUnsupervised Neural Word Segmentation for Chinese\nvia Segmental Language Modeling\nZhiqing Sun\nPeking University\n1500012783@pku.edu.cn\nZhi-Hong Deng\nPeking University\nzhdeng@pku.edu.cn\nAbstract\nPrevious traditional approaches to unsuper-\nvised Chinese word segmentation (CWS) can\nbe roughly classiï¬ed into discriminative and\ngenerative models. The former uses the care-\nfully designed goodness measures for candi-\ndate segmentation, while the latter focuses on\nï¬nding the optimal segmentation of the high-\nest generative probability. However, while\nthere exists a trivial way to extend the dis-\ncriminative models into neural version by us-\ning neural language models, those of genera-\ntive ones are non-trivial. In this paper, we pro-\npose the segmental language models (SLMs)\nfor CWS. Our approach explicitly focuses on\nthe segmental nature of Chinese, as well as\npreserves several properties of language mod-\nels. In SLMs, a context encoder encodes the\nprevious context and a segment decoder gen-\nerates each segment incrementally. As far as\nwe know, we are the ï¬rst to propose a neu-\nral model for unsupervised CWS and achieve\ncompetitive performance to the state-of-the-\nart statistical models on four different datasets\nfrom SIGHAN 2005 bakeoff.\n1 Introduction\nUnlike English and many other languages, Chi-\nnese sentences have no explicit word boundaries.\nTherefore, Chinese Word Segmentation (CWS) is\na crucial step for many Chinese Natural Language\nProcessing (NLP) tasks such as syntactic pars-\ning, information retrieval and word representation\nlearning (Grave et al., 2018).\nRecently, neural approaches for supervised\nCWS are attracting huge interest. A great quan-\ntities of neural models, e.g., tensor neural network\n(Pei et al., 2014), recursive neural network (Chen\net al., 2015a), long-short-term-memory (RNN-\nLSTM) (Chen et al., 2015b) and convolutional\nneural network (CNN) (Wang and Xu, 2017), have\nğ’š\nğŸ\nâŒ©\nğ’†ğ’ğ’”\nâŒª\nğ’š\nğŸ\nğ’š\nğŸ\nğ’š\nğŸ‘\nğ’š\nğŸ\nâŒ©\nğ’†ğ’ğ’”\nâŒª\nğ’š\n3\nğ’š\nğŸ’\nâŒ©\nğ’†ğ’ğ’”\nâŒª\nğ’š\nğŸ’\nğ’š\nğŸ\nğ’š\nğŸ\nğ’š\nğŸ‘\nğ’š\nğŸ’\nğ’š\nğŸ\nContext \nEncoder\nSegment Decoder\nFigure 1: A Segmental Language Model (SLM) works\non y = y1y2y3y4 with the candidate segmentation\ny1, y2:3 and y4, where y0 is an additional start sym-\nbol which is kept same for all sentences.\nbeen proposed and given competitive results to the\nbest statistical models (Sun, 2010). However, the\nneural approaches for unsupervised CWS have not\nbeen investigated.\nPrevious unsupervised approaches to CWS can\nbe roughly classiï¬ed into discriminative and gen-\nerative models. The former uses carefully de-\nsigned goodness measures for candidate segmen-\ntation, while the latter focuses on designing sta-\ntistical models for Chinese and ï¬nds the optimal\nsegmentation of the highest generative probability.\nPopular goodness measures for discriminative\nmodels include Mutual Information (MI) (Chang\nand Lin, 2003), normalized Variation of Branch-\ning Entropy (nVBE) (Magistry and Sagot, 2012)\nand Minimum Description Length (MDL) (Mag-\nistry and Sagot, 2013). There is a trivial way to\nextend these statistical discriminative approaches,\nbecause we can simply replace the n-gram lan-\nguage models in these approaches by neural lan-\nguage models (Bengio et al., 2003). There may\n4916\nexists other more sophisticated neural discrimina-\ntive approaches, but it is not the focus of this paper.\nFor generative approaches, typical statistical\nmodels includes Hidden Markov Model (HMM)\n(Chen et al., 2014), Hierarchical Dirichlet Pro-\ncess (HDP) (Goldwater et al., 2009) and Nested\nPitman-Yor Process (NPY) (Mochihashi et al.,\n2009). However, none of them can be easily ex-\ntended into a neural model. Therefore, neural gen-\nerative models for word segmentation are remain-\ning to be investigated.\nIn this paper, we proposed the Segmental Lan-\nguage Models (SLMs), a neural generative model\nthat explicitly focuses on the segmental nature of\nChinese: SLMs can directly generate segmented\nsentences and give the corresponding generative\nprobability. We evaluate our methods on four dif-\nferent benchmark datasets from SIGHAN 2005\nbakeoff (Emerson, 2005), namely PKU, MSR, AS\nand CityU. To our knowledge, we are the ï¬rst to\npropose a neural model for unsupervised Chinese\nword segmentation and achieve competitive per-\nformance to the state-of-the-art statistical models\non four different datasets.1\n2 Segmental Language Models\nIn this section, we present our segmental language\nmodels (SLMs). Notice that in Chinese NLP, char-\nacters are the atom elements. Thus in the context\nof CWS, we use â€œcharacterâ€ instead of â€œwordâ€ for\nlanguage modeling.\n2.1 Language Models\nThe goal of language modeling is to learn the joint\nprobability function of sequences of characters in\na language. However, This is intrinsically difï¬-\ncult because of the curse of dimensionality. Tradi-\ntional approaches obtain generalization based on\nn-grams, while neural approaches introduce a dis-\ntributed representation for characters to ï¬ght the\ncurse of dimensionality.\nA neural Language Model (LM) can give the\nconditional probability of the next character given\nthe previous ones, and is usually implemented by\na Recurrent Neural Network (RNN):\nht = f(ytâˆ’1,htâˆ’1) (1)\np(yt|y1:tâˆ’1) = g(ht,yt) (2)\n1Our implementation can be found at https://\ngithub.com/Edward-Sun/SLM\nwhere yt is the distributed representation for the\ntth character and ht represents the information of\nthe previous characters.\n2.2 Segmental Language Models\nSimilar to neural language modeling, the goal of\nsegmental language modeling is to learn the joint\nprobability function of the segmented sequences\nof characters. Thus, for each segment, we have:\nË†p(y(i)\nt |y(i)\n1:tâˆ’1,y(1:iâˆ’1)) = g(h(i)\nt ,y(i)\nt ) (3)\nwhere y(i)\nt is the distributed representation for\nthe tth character in the ith segment and y(1:iâˆ’1)\nis the previous segments. And the concatenation\nof all segments y(i)\n1:Ti is exactly the whole sentence\ny1:T , where Ti is the length of theith segment y(i),\nT is the length of the sentence y.\nMoreover, we introduce a context encoder RNN\nto process the character sequence y(1:iâˆ’1) in order\nto make y(i)\nt conditional on y(1:iâˆ’1). Speciï¬cally,\nwe initialize h(i)\n0 with the context encoderâ€™s output\nof y(1:iâˆ’1).\nNotice that although we have an encoder and\nthe segment decoder g, SLM is not an encoder-\ndecoder model. Because the content that the de-\ncoder generates is not the same as what the en-\ncoder provides.\nFigure 1 illustrates how SLMs work with a can-\ndidate segmentation.\n2.3 Properties of SLMs\nHowever, in unsupervised scheme, the given sen-\ntences are not segmented. Therefore, the probabil-\nity for SLMs to generate a given sentence is the\njoint probability of all possible segmentation:\np(y1:T ) =\nâˆ‘\nT1,T2,...\nâˆ\ni\nË†p(y(i)\n1:Ti )\n=\nâˆ‘\nT1,T2,...\nâˆ\ni\nTi+1âˆ\nt=1\nË†p(y(i)\nt |y(i)\n0:tâˆ’1) (4)\nwhere y(i)\nTi+1 = âŸ¨eosâŸ©is the end of segment\nsymbol at the end of each segment, and y(i)\n0 is the\ncontext representation of y(1:iâˆ’1).\nMoreover, for sentence generation, SLMs are\nable to generate arbitrary sentences by generating\n4917\nsegments one by one and stopping when gener-\nating end of sentencesymbol âŸ¨EOSâŸ©. In addi-\ntion, the time complexity is linear to the length of\nthe generated sentence, as we can keep the hid-\nden state of the context encoder RNN and update\nit when generating new words.\nLast but not least, it is easy to verify that SLMs\npreserve the probabilistic property of language\nmodels: âˆ‘\ni\nP(si) = 1 (5)\nwhere si enumerates all possible sentences.\nIn summary, the segmental language models\ncan perfectly substitute vanilla language models.\n2.4 Training and Decoding\nSimilar to language model, the training is achieved\nby maximizing the training corpus log-likelihood:\nL= âˆ’log p(y1:T ) (6)\nLuckily, we can compute the loss objective\nfunction in linear time complexity using dynamic\nprogramming, given the initial condition that\np(y1:0) = 1:\np(y1:n) =\nKâˆ‘\nk=1\np(y1:nâˆ’k)Ë†p(ynâˆ’k+1:n) (7)\nwhere p(Â·) is the joint probability of all possible\nsegmentation, Ë†p(Â·) is the probability of one seg-\nment and Kis the maximal length of the segments.\nWe can also ï¬nd the segmentation with maxi-\nmal probability (namely, decoding) in linear time\nusing dynamic programming in the similarly way\nwith Â¯p(y1:0) = 1:\nÂ¯p(y1:n) =\nK\nmax\nk=1\nÂ¯p(y1:nâˆ’k)Ë†p(ynâˆ’k+1:n) (8)\nÎ´(y1:n) = arg\nK\nmax\nk=1\nÂ¯p(y1:nâˆ’k)Ë†p(ynâˆ’k+1:n) (9)\nwhere Â¯pis the probability of the best segmenta-\ntion and Î´is used to trace back the decoding.\n3 Experiments\n3.1 Experimental Settings and Detail\nWe evaluate our models on SIGHAN 2005 bake-\noff (Emerson, 2005) datasets and replace all the\npunctuation marks with âŸ¨puncâŸ©, English charac-\nters with âŸ¨engâŸ©and Arabic numbers with âŸ¨numâŸ©\nF1 score PKU MSR AS CityU\nHDP 68.7 69.9 - -\nHDP + HMM 75.3 76.3 - -\nESA 77.8 80.1 78.5 76.0\nNPY-3 - 80.7 - 81.7\nNPY-2 - 80.2 - 82.4\nnVBE 80.0 81.3 76.6 76.7\nJoint 81.1 81.7 - -\nSLM-2 80.2 78.5 79.4 78.2\nSLM-3 79.8 79.4 80.3 80.5\nSLM-4 79.2 79.0 79.8 79.7\nTable 1: Main results on SIGHAN 2005 bakeoff\ndatasets with previous state-of-the-art models (Chen\net al., 2014; Wang et al., 2011; Mochihashi et al., 2009;\nMagistry and Sagot, 2012)\nfor all text and only consider segment the text be-\ntween punctuations. Following Chen et al. (2014)\n, we use both training data and test data for train-\ning and only test data are used for evaluation. In\norder to make a fair comparison with the previous\nworks, we do not consider using other larger raw\ncorpus.\nWe apply word2vec (Mikolov et al., 2013) on\nChinese Gigaword corpus (LDC2011T13) to get\npretrained embedding of characters.\nA 2-layer LSTM (Hochreiter and Schmidhuber,\n1997) is used as the segment decoder and a 1-layer\nLSTM is used as the context encoder.\nWe use stochastic gradient decent with a mini-\nbatch size of 256 and a learning rate of 16.0 to op-\ntimize the model parameters in the ï¬rst 400 steps,\nthen we use Adam (Kingma and Ba, 2014) with\na learning rate of 0.005 to further optimize the\nmodels. Model parameters are initialized by nor-\nmal distributions as Glorot and Bengio (2010) sug-\ngested. We use a gradient clip = 0.1 and apply a\ndropout with dropout rate = 0 .1 to the character\nembedding and RNNs to prevent over-ï¬t.\nThe standard word precision, recall and F1 mea-\nsures (Emerson, 2005) are used to evaluate seg-\nmentation performance.\n3.2 Results and Analysis\nOur ï¬nal results are shown in Table 1, which\nlists the results of several previous state-of-the-\nart methods2, where we mark the best results in\n2Magistry and Sagot (2012) evaluated their nVBE on the\ntraining data, and the joint model of Chen et al. (2014) com-\nbine HDP+HMM and is initialized with nVBE, so in princi-\nple these results can not be compared directly.\n4918\nF1 score PKU MSR AS CityU\nSLM-4 79.2 79.0 79.8 79.7\nSLM-4* 81.9 83.0 81.0 81.4\nSLM-4â€  87.5 84.3 84.2 86.0\nSLM-4â€ * 87.3 84.8 83.9 85.8\nTable 2: Results of SLM-4 incorporating ad hoc guide-\nlines, where â€ represents using additional 1024 seg-\nmented setences for training data and * represents using\na rule-based post-processing\nboldface. We test the proposed SLMs with differ-\nent maximal segment length K = 2,3,4 and use\nâ€œSLM-Kâ€ to denote the corresponding model. We\ndo not try K >4 because there are rare words that\nconsist more than 4 characters.\nAs can be seen, it is hard to predict what choice\nof K will give the best performance. This is be-\ncause the exact deï¬nition of what a word remains\nhard to reach and different datasets follow differ-\nent guidelines. Zhao and Kit (2008) use cross-\ntraining of a supervised segmentation system in\norder to have an estimation of the consistency be-\ntween different segmentation guidelines and the\naverage consistency is found to be as low as 85\n(f-score). Therefore, this can be regarded as a top\nline for unsupervised CWS.\nTable 1 shows that SLMs outperform previous\nbest discriminative and generative models on PKU\nand AS datasets. This might be due to that the\nsegmentation guideline of our models are closer\nto these two datasets.\nMoreover, in the experiments, we observe that\nChinese particles often attach other words, for ex-\nample, â€œçš„â€ following adjectives and â€œäº†â€ follow-\ning verbs. It is hard for our generative models to\nsplit them apart. Therefore, we propose a rule-\nbased post-processing module to deal with this\nproblem, where we explicitly split the attached\nparticles from other words. 3 The post-processing\nis applied on the results of â€œSLM-4â€. In addi-\ntion, we also evaluate â€œSLM-4â€ using the ï¬rst\n1024 sentences of the segmented training datasets\n(about 5.4% of PKU, 1.2% of MSR, 0.1% of AS\nand 1.9% of CityU) for training, in order to teach\nâ€œSLM-4â€ the corresponding ad hoc segmentation\nguidelines. Table 2 shows the results.\nWe can ï¬nd from the table that only 1024\nguideline sentences can improve the performance\nof â€œSLM-4â€ signiï¬cantly. While rule-based\n3The rules we use are listed in the appendix at https:\n//github.com/Edward-Sun/SLM.\nError SLM-2 SLM-3 SLM-4\nInsertion 7866 4803 3519\nDeletion 3855 7518 8851\nTable 3: Statistics of insertion errors and deletion errors\nthat SLM-Kproduces on PKU dataset\npost-processing is very effective, â€œSLM-4 â€ â€ can\noutperform â€œSLM-4*â€ on all the four datasets.\nMoreover, performance drops when applying the\nrule-based post-processing to â€œSLM-4 â€ â€ on three\ndatasets. These indicate that SLMs can learn the\nempirical rules for word segmentation given only\na small amount of training data. And these guide-\nline data can improve the performance of SLMs\nnaturally, superior to using explicit rules.\n3.3 The Effect of the Maximal Segment\nLength\nThe maximal segment length K represents the\nprior knowledge we have for Chinese word seg-\nmentation. For example K = 3 represents that\nthere are only unigrams, bigrams and trigrams in\nthe text. While there do exist words that con-\ntain more than four characters, most of the Chi-\nnese words are unigram or bigram. Therefore, K\ndenotes a trade-off between the accuracy of short\nwords and long words.\nSpeciï¬cally, we investigate two major segmen-\ntation problems that might affect the accuracy of\nword segmentation performance, namely, inser-\ntion errors and deletion errors. An insertion error\ninsert a segment in a word, which split a correct\nword. And an deletion error delete the segment\nbetween two words, which results in a composi-\ntion error (Li and Yuan, 1998). Table 3 shows the\nstatistics of different errors on PKU of our model\nwith different K. We can observe that insertion er-\nror rate decrease with the increase of K, while the\ndeletion error rate increase with the increase ofK.\nWe also provide some examples in Table 4,\nwhich are taken from the results of our models. It\nclearly illustrates that different K could result in\ndifferent errors. For example, there is an insertion\nerror on â€œåè¿‡æ¥â€ by SML-2, and a deletion error\non â€œä¿ƒè¿›â€ and â€œäº†â€ by SLM-4.\n4 Related Work\nGenerative Models for CWS Goldwater et al.\n(2009) are the ï¬rst to proposed a generative\nmodel for unsupervised word segmentation. They\n4919\nModel Example\nSLM-2 è€Œ è¿™äº›åˆ¶åº¦çš„ å®Œå–„å è¿‡æ¥åˆä¿ƒè¿› äº†æ£€å¯Ÿäººå‘˜æ‰§æ³• æ°´å¹³ çš„è¿›ä¸€ æ­¥æé«˜\nSLM-3 è€Œè¿™äº› åˆ¶åº¦çš„ å®Œå–„åè¿‡æ¥ åˆä¿ƒè¿›äº†æ£€å¯Ÿ äººå‘˜æ‰§æ³• æ°´å¹³çš„è¿›ä¸€æ­¥æé«˜\nSLM-4 è€Œè¿™äº› åˆ¶åº¦çš„ å®Œå–„åè¿‡æ¥ åˆä¿ƒè¿›äº† æ£€å¯Ÿäººå‘˜æ‰§æ³• æ°´å¹³çš„è¿›ä¸€æ­¥æé«˜\nGold è€Œ è¿™äº›åˆ¶åº¦ çš„å®Œå–„åè¿‡æ¥åˆä¿ƒè¿› äº†æ£€å¯Ÿäººå‘˜æ‰§æ³• æ°´å¹³ çš„è¿›ä¸€æ­¥ æé«˜\nTable 4: Examples of segmentation with different maximal segment length K\nbuilt a nonparametric Bayesian bigram language\nmodel based on HDP (Teh et al., 2005). Mochi-\nhashi et al. (2009) proposed a Bayesian hier-\narchical language model using Pitman-Yor (PY)\nprocess, which can generate sentences hierarchi-\ncally. Chen et al. (2014) proposed a Bayesian\nHMM model for unsupervised CWS inspired by\nthe character-based scheme in supervised CWS\ntask, where the hidden state of charaters are set to\n{Single,Begin,End,Middle}to represents their\ncorresponding positions in the words. The seg-\nmental language model is not a neural extension\nof the above statistical models, as we model the\nsegments directly.\nSegmental Sequence Models Sequence model-\ning via segmentations has been well investigated\nby Wang et al. (2017), where they proposed the\nSleep-AWake Network (SW AN) for speech recog-\nnition. SW AN is similar to SLM. However, SLMs\ndo not have sleep-awake states. And SLMs pre-\ndict the following segment given the previous con-\ntext while SW AN tries to recover the information\nin the encoded state. Therefore, the key differ-\nence is that SLMs are unsupervised language mod-\nels while SW ANs are supervised seq2seq models.\nThereafter, Huang et al. (2017) successfully apply\nSW AN in their phrase-based machine translation.\nAnother related work in machine translation is the\nonline segment to segment neural transduction (Yu\net al., 2016), where the model is able to capture un-\nbounded dependencies in both the input and output\nsequences. Kong (2017) also proposed a Segmen-\ntal Recurrent Neural Network (SRNN) with CTC\nto solve segmental labeling problems.\n5 Conclusion\nIn this paper, we proposed a neural generative\nmodel for fully unsupervised Chinese word seg-\nmentation (CWS). To the best of knowledge, this\nis the ï¬rst neural model for CWS. Our segmen-\ntal language model is an intuitive generalization\nof vanilla neural language models that directly\nmodeling the segmental nature of Chinese. Ex-\nperimental results show that our models achieve\ncompetitive performance to the previous state-of-\nthe-art statistical models on four datasets from\nSIGHAN 2005. We also show the improvement of\nincorporating ad hoc guidelines into our segmen-\ntal language models. Our future work may include\nthe following two directions.\nâ€¢In this work, we only consider the sequential\nsegmental language modeling. In the future,\nwe are interested in build a hierarchical neu-\nral language model like the Pitman-Yor pro-\ncess.\nâ€¢Like vanilla language models, the segmental\nlanguage models can also provide useful in-\nformation for semi-supervised learning tasks.\nIt would also be interesting to explore our\nmodels in the semi-supervised schemes.\nAcknowledgements\nThis work is supported by the National Train-\ning Program of Innovation for Undergraduates\n(URTP2017PKU001). We would also like to\nthank the anonymous reviewers for their helpful\ncomments.\nReferences\nYoshua Bengio, RÂ´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137â€“1155.\nJason S Chang and Tracy Lin. 2003. Unsupervised\nword segmentation without dictionary. ROCLING\n2003 Poster Papers, pages 355â€“359.\nMiaohong Chen, Baobao Chang, and Wenzhe Pei.\n2014. A joint model for unsupervised chinese word\nsegmentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 854â€“863, Doha, Qatar.\nAssociation for Computational Linguistics.\nXinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing\nHuang. 2015a. Gated recursive neural network for\nchinese word segmentation. In Proceedings of the\n4920\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) , pages 1744â€“1753, Beijing,\nChina. Association for Computational Linguistics.\nXinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu,\nand Xuanjing Huang. 2015b. Long short-term mem-\nory neural networks for chinese word segmentation.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1197â€“1206, Lisbon, Portugal. Association for Com-\nputational Linguistics.\nThomas Emerson. 2005. The second international chi-\nnese word segmentation bakeoff. In Proceedings of\nthe fourth SIGHAN workshop on Chinese language\nProcessing, volume 133, pages 123â€“133.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difï¬culty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth In-\nternational Conference on Artiï¬cial Intelligence and\nStatistics, pages 249â€“256.\nSharon Goldwater, Thomas L Grifï¬ths, and Mark John-\nson. 2009. A bayesian framework for word segmen-\ntation: Exploring the effects of context. Cognition,\n112(1):21â€“54.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nSepp Hochreiter and J Â¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735â€“1780.\nPo-Sen Huang, Chong Wang, Sitao Huang, Dengyong\nZhou, and Li Deng. 2017. Computer science Â¿ com-\nputation and language towards neural phrase-based\nmachine translation. arxiv.org/abs/1706.05565.\nDiederik Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nLingpeng Kong. 2017. Neural Representation Learn-\ning in Linguistic Structured Prediction. Ph.D. thesis,\nGoogle Research.\nHaizhou Li and Baosheng Yuan. 1998. Chinese word\nsegmentation. In Proceedings of the 12th Paciï¬c\nAsia Conference on Language, Information and\nComputation, pages 212â€“217.\nPierre Magistry and BenoË†Ä±t Sagot. 2012. Unsupervized\nword segmentation: the case for mandarin chinese.\nIn Proceedings of the 50th Annual Meeting of the\nAssociation for Computational Linguistics: Short\nPapers-Volume 2, pages 383â€“387. Association for\nComputational Linguistics.\nPierre Magistry and Beno Ë†Ä±t Sagot. 2013. Can mdl\nimprove unsupervised chinese word segmentation?\nIn Sixth International Joint Conference on Natural\nLanguage Processing: Sighan workshop, page 2.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111â€“3119.\nDaichi Mochihashi, Takeshi Yamada, and Naonori\nUeda. 2009. Bayesian unsupervised word segmen-\ntation with nested pitman-yor language modeling.\nIn Proceedings of the Joint Conference of the 47th\nAnnual Meeting of the ACL and the 4th International\nJoint Conference on Natural Language Processing\nof the AFNLP: Volume 1-Volume 1, pages 100â€“108.\nAssociation for Computational Linguistics.\nWenzhe Pei, Tao Ge, and Baobao Chang. 2014. Max-\nmargin tensor neural network for chinese word seg-\nmentation. In Proceedings of the 52nd Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 293â€“303, Bal-\ntimore, Maryland. Association for Computational\nLinguistics.\nWeiwei Sun. 2010. Word-based and character-based\nword segmentation models: Comparison and combi-\nnation. In Coling 2010: Posters, pages 1211â€“1219,\nBeijing, China. Coling 2010 Organizing Committee.\nYee W Teh, Michael I Jordan, Matthew J Beal, and\nDavid M Blei. 2005. Sharing clusters among re-\nlated groups: Hierarchical dirichlet processes. In\nAdvances in neural information processing systems,\npages 1385â€“1392.\nChong Wang, Yining Wang, Po-Sen Huang, Abdel-\nrahman Mohamed, Dengyong Zhou, and Li Deng.\n2017. Sequence modeling via segmentations. arXiv\npreprint arXiv:1702.07463.\nChunqi Wang and Bo Xu. 2017. Convolutional Neural\nNetwork with Word Embeddings for Chinese Word\nSegmentation. In Proceedings of the 8th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing.\nHanshi Wang, Jian Zhu, Shiping Tang, and Xiaozhong\nFan. 2011. A new unsupervised approach to\nword segmentation. Computational Linguistics ,\n37(3):421â€“454.\nLei Yu, Jan Buys, and Phil Blunsom. 2016. Online seg-\nment to segment neural transduction. arXiv preprint\narXiv:1609.08194.\nHai Zhao and Chunyu Kit. 2008. An empirical com-\nparison of goodness measures for unsupervised chi-\nnese word segmentation with a uniï¬ed framework.\nIn Proceedings of the Third International Joint Con-\nference on Natural Language Processing: Volume-I."
}