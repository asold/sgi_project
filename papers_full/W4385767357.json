{
    "title": "CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo",
    "url": "https://openalex.org/W4385767357",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100650401",
            "name": "Weitao Chen",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5100774685",
            "name": "Hongbin Xu",
            "affiliations": [
                "Alibaba Group (China)",
                "South China University of Technology",
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5083237977",
            "name": "Zhipeng Zhou",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5100355692",
            "name": "Yang Liu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5087131650",
            "name": "Baigui Sun",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5089840086",
            "name": "Wenxiong Kang",
            "affiliations": [
                null,
                "South China University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5101763148",
            "name": "Xuansong Xie",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6863994431",
        "https://openalex.org/W2968109109",
        "https://openalex.org/W2989661779",
        "https://openalex.org/W3210771406",
        "https://openalex.org/W3216453138",
        "https://openalex.org/W2285268364",
        "https://openalex.org/W2064381770",
        "https://openalex.org/W2205172244",
        "https://openalex.org/W2996441973",
        "https://openalex.org/W1968799614",
        "https://openalex.org/W3193766563",
        "https://openalex.org/W6803771590",
        "https://openalex.org/W3096678291",
        "https://openalex.org/W4281748179",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2992718396",
        "https://openalex.org/W3204354488",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W4221160813",
        "https://openalex.org/W4234282793",
        "https://openalex.org/W2741885505",
        "https://openalex.org/W6759363029",
        "https://openalex.org/W3140551255",
        "https://openalex.org/W2128052895",
        "https://openalex.org/W6864014924",
        "https://openalex.org/W3109999916",
        "https://openalex.org/W6863071542",
        "https://openalex.org/W3191548539",
        "https://openalex.org/W6780948282",
        "https://openalex.org/W3045241461",
        "https://openalex.org/W2796404584",
        "https://openalex.org/W2926429807",
        "https://openalex.org/W2990386223",
        "https://openalex.org/W2993866577",
        "https://openalex.org/W6775539776",
        "https://openalex.org/W6791858558",
        "https://openalex.org/W3069256834",
        "https://openalex.org/W4312725793",
        "https://openalex.org/W3034524082",
        "https://openalex.org/W3166285241",
        "https://openalex.org/W3102132650",
        "https://openalex.org/W4287605221",
        "https://openalex.org/W3214762792",
        "https://openalex.org/W4312725665",
        "https://openalex.org/W4312253793",
        "https://openalex.org/W3170262190",
        "https://openalex.org/W3035257660",
        "https://openalex.org/W2471962767",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2129404737",
        "https://openalex.org/W3034564916",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W3042904022",
        "https://openalex.org/W4313143504",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4206424847",
        "https://openalex.org/W3103455452",
        "https://openalex.org/W3204267695",
        "https://openalex.org/W3163849933",
        "https://openalex.org/W2982169158",
        "https://openalex.org/W3034530552",
        "https://openalex.org/W2338968644",
        "https://openalex.org/W2962793285",
        "https://openalex.org/W3097305369",
        "https://openalex.org/W3211650806"
    ],
    "abstract": "The core of Multi-view Stereo(MVS) is the matching process among reference and source pixels. Cost aggregation plays a significant role in this process, while previous methods focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost aggregation network, namely CostFormer. The Residual Depth-Aware Cost Transformer(RDACT) is proposed to aggregate long-range features on cost volume via self-attention mechanisms along the depth and spatial dimensions. Furthermore, Residual Regression Transformer(RRT) is proposed to enhance spatial attention. The proposed method is a universal plug-in to improve learning-based MVS methods.",
    "full_text": "CostFormer: Cost Transformer for Cost Aggregation in Multi-view Stereo\nWeitao Chen1 , Hongbin Xu1,2 , Zhipeng Zhou1 , Yang Liu1 , Baigui Sun†1 , Wenxiong\nKang†2,3 and Xuansong Xie1\n1Alibaba Group\n2South China University of Technology\n3Guangdong Artificial Intelligence and Digital Enconomy Laboratory, Pazhou Lab\n{hillskyxm, hongbinxu1013}@gmail.com, auwxkang@scut.edu.cn\nAbstract\nThe core of Multi-view Stereo(MVS) is the match-\ning process among reference and source pixels.\nCost aggregation plays a significant role in this pro-\ncess, while previous methods focus on handling it\nvia CNNs. This may inherit the natural limitation\nof CNNs that fail to discriminate repetitive or incor-\nrect matches due to limited local receptive fields.\nTo handle the issue, we aim to involve Transformer\ninto cost aggregation. However, another problem\nmay occur due to the quadratically growing com-\nputational complexity caused by Transformer, re-\nsulting in memory overflow and inference latency.\nIn this paper, we overcome these limits with an\nefficient Transformer-based cost aggregation net-\nwork, namely CostFormer. The Residual Depth-\nAware Cost Transformer(RDACT) is proposed to\naggregate long-range features on cost volume via\nself-attention mechanisms along the depth and spa-\ntial dimensions. Furthermore, Residual Regression\nTransformer(RRT) is proposed to enhance spatial\nattention. The proposed method is a universal plug-\nin to improve learning-based MVS methods.\n1 Introduction\nGiven a series of calibrated images from different views in\none scene, Multi-view Stereo (MVS) aims to recover the\n3D information of the observed scene. It is a fundamen-\ntal problem in computer vision and widely applied to robot\nnavigation, autonomous driving, augmented reality, and etc.\nRecent learning-based MVS networks [Yao et al., 2018;\nGu et al., 2020; Wang et al., 2021] have achieved inspiring\nsuccess both in the quality and the efficiency of 3D recon-\nstruction. Generally, deep MVS approaches consist of the fol-\nlowing five steps: feature extraction from multi-view images\nvia CNN network with shared weights, differentiable warping\nto align all source features to the reference view, matching\ncost computation from reference features and aligned source\nfeatures, matching cost aggregation or regularization, depth\nor disparity regression.\n†Corresponding authors.\n‡Appendix is presented in: https://arxiv.org/abs/2305.10320.\nFigure 1: Comparison with state-of-the-art MVS methods on DTU.\nRelationship between error, GPU memory and run-time with image\nsize 1152×864.\nCurrent progresses in learning-based MVS primarily con-\ncentrate on the limitation of reconstruction quality[Wei et al.,\n2021; Yang et al., 2020a], memory consumption [Yan et al.,\n2020; Wei et al., 2021], and efficiency [Wang et al., 2021;\nWang et al., 2022a]. The basic network architecture of these\nworks is based on the pioneering backbone network called\nMVSNet [Yao et al., 2018], which provides an elegant and\nstable baseline. However, instead of taking the inheritance\nof network design principle in MVSNet [Yao et al., 2018]\nfor granted, we can rethink the task of MVS problem as a\ndense correspondence problem [Hosni et al., 2012] alterna-\ntively. The core of MVS is a dense pixelwise correspondence\nestimation problem that searches the corresponding pixel of\na specific pixel in the reference image along the epipolar\nline in all warped source images. No matter which task this\ncorrespondence estimation problem is applied to, the match-\ning task can be boiled down to a classical matching pipeline\n[Scharstein and Szeliski, 2002]: (1) feature extraction, and (2)\ncost aggregation. In learning-based MVS methods, the transi-\ntion from traditional hand-crafted features to CNN-based fea-\ntures inherently solves the former step of the classical match-\ning pipeline via providing powerful feature representation\nlearned from large-scale data. However, handling the cost\naggregation step by matching similarities between features\nwithout any prior usually suffers from the challenges due to\nambiguities generated by repetitive patterns or background\nclutters [Cho et al., 2021]. Consequently, a typical solution\nin MVSNet and its variants [Yao et al., 2018; Guet al., 2020;\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n599\nWang et al., 2021] is to apply a 3D CNN or an RNN to reg-\nularize the cost volume among reference and source views,\nrather than directly rely on the quality of the initial corre-\nlation clues in cost volume. Although formulated variously\nin previous methods, these methods either use hand-crafted\ntechniques that are agnostic to severe deformations or inherit\nthe limitation of CNNs, e.g. limited receptive fields, unable\nto discriminate incorrect matches that are locally consistent.\nIn this work, we focus on the cost aggregation stepof cost\nvolume and propose a novel cost aggregation Transformer\n(CostFormer) to tackle the issues above. Our CostFormer\nis based on Transformer [Vaswani et al., 2017 ], which is\nrenowned for its global receptive field and long-range de-\npendent representation. By aggregating the matching cost in\nthe cost volume, our aggregation network can explore global\ncorrespondences and refine the ambiguous matching points\neffectively with the help of the self-attention (SA) mecha-\nnism in Transformer. Though the promising performances\nof Vision Transformers have been proven in many applica-\ntions [Dosovitskiy et al., 2020; Sun et al., 2021], the time\nand memory complexity of the key-query dot product inter-\naction in conventional SA grow quadratically with the spatial\nresolution of inputs. Hence, replacing 3D CNN with Trans-\nformer may result in unexpected extra occupancy in memory\nand latency in inference. Inspired by [Wang et al., 2021],\nwe further introduce the Transformer architecture into an it-\nerative multi-scale learnable PatchMatch pipeline. It inherits\nthe advantages of the long-range receptive field in Transform-\ners, improving the reconstruction performance substantially.\nMeantime, it also maintains a balanced trade-off between ef-\nficiency and performance, which is competitive in the infer-\nence speed and parameters magnitude compared with other\nmethods.\nOur main contributions are as follows:\n(1) In this paper, we propose a novel Transformer-based\ncost aggregation network called CostFormer, which can be\nplugged into learning-based MVS methods to improve cost\nvolume effectively. (2) CostFormer applies an efficient Resid-\nual Depth-Aware Cost Transformer to cost volume, extending\n2D spatial attention to 3D depth and spatial attention. (3)\nCostFormer applies an efficient Residual Regression Trans-\nformer between cost aggregation and depth regression, keep-\ning spatial attention. (4) The proposed CostFormer brings\nbenefits to learning-based MVS methods when evaluating\nDTU [Aanæs et al., 2016], Tanks&Temples [Knapitsch et al.,\n2017] ETH3D [Sch¨ops et al., 2017], BlendedMVS [Yao et\nal., 2020] and YFCC[Thomee et al., 2016] datasets.\n2 Related Work\n2.1 Learning-based MVS Methods\nPowered by the great success of deep learning-based tech-\nniques, many learning-based methods have been proposed to\nboost the performance of Multi-view Stereo. MVSNet [Yao\net al., 2018] is a landmark for the end-to-end network that in-\nfers the depth map on each reference view for the MVS task.\nFeature maps extracted by a 2D CNN on each view are repro-\njected to the same reference view to build a variance-based\ncost volume. A 3D CNN is further used to regress the depth\nmap. Following this pioneering work, lots of efforts have\nbeen devoted to boosting speed and reducing memory occu-\npation. To relieve the burden of huge memory cost, recurrent\nneural networks are utilized to regularize the cost volume in\nAA-RMVSNet [Wei et al., 2021]. Following a coarse-to-fine\nmanner to develop a computationally efficient network, a re-\ncent strand of works divide the single cost volume into several\ncost volumes at multiple stages, like CasMVSNet [Gu et al.,\n2020], CVP-MVSNet [Yang et al., 2020a], UCSNet [Cheng\net al., 2020], and etc. Inspired by the traditional PatchMatch\nstereo algorithm, PatchMatchNet [Wang et al., 2021] inherits\nthe pipeline in PatchMatch stereo in an iterative manner and\nextend it into a learning-based end-to-end network.\n2.2 Vision Transformer\nThe success of Transformer [Vaswani et al., 2017] and its\nvariants [Dosovitskiy et al., 2020; Liu et al., 2021 ] have\nmotivated the development of Neural Language Processing\nin recent years. Borrowing inspiration from these works,\nTransformer has been successfully extended to vision tasks\nand proven to boost the performance of image classification\n[Dosovitskiy et al., 2020]. Following the pioneering work,\nmany efforts are devoted to boosting the development of var-\nious vision tasks with the powerful representation ability of\nTransformer.\nIn [Li et al., 2021 ], the application of Transformer in\nthe classic stereo disparity estimation task is investigated\nthoughtfully. Swin Transformer [Liu et al., 2021] involves\nthe hierarchical structure into Vision Transformers and com-\nputes the representation with shifted windows. Consider-\ning Transformer’s superiority in extracting global content in-\nformation via attention mechanism, many works attempt to\nutilize it in the task of feature matching. Given a pair of\nimages, CATs [Cho et al., 2021] explore global consensus\namong correlation maps extracted from a Transformer, which\ncan fully leverage the self-attention mechanism and model\nlong-range dependencies among pixels. LoFTR [Sun et al.,\n2021] also leverages Transformers with a coarse-to-fine man-\nner to model dense correspondence. STTR [Li et al., 2021]\nextends the feature matching Transformer architecture to the\ntask of stereo depth estimation task in a sequence-to-sequence\nmatching perspective. TransMVSNet [Ding et al., 2021] is\nthe most relevant concurrent work compared with ours, which\nutilizes a Feature Matching Transformer (FMT) to lever-\nage self-attention and cross-attention to aggregate long-range\ncontext information within and across images. Specifically,\nthe focus of TransMVSNet is on the enhancement of feature\nextraction before cost aggregation, while our proposed Cost-\nFormer aims to improve the cost aggregation process on cost\nvolume.\n3 Methodology\nIn this section, we introduce the detailed architecture of the\nproposed CostFormer which focuses on the cost aggrega-\ntion step of cost volume. CostFormer contains two spe-\ncially designed modules called Residual-Depth Aware Cost\nTransformer (RDACT) and Residual Regression Transformer\n(RRT), which are utilized to explore the relation between pix-\nels within a long range and the relation between different\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n600\nW Warping\nC Cost Computation\n3D 3D CNN\nCA Cost Aggregation\nD Depth Regression\nWarped Features\nCost V olume\nRegularized Cost V olume\nTransformed Cost V olume\nAggregated Cost V olume\nTransformed Aggregated Cost V olume\nINIT&PROP Initialization and Propagation\nRDACT Residual Depth-Aware Cost Transformer\nRR T Residual Regression Transformer\nW C 3D CA D \nINIT&PROP RDACT RR T \nW C 3D CA D \nINIT&PROP RDACT RR T \nW C 3D CA D \nINIT&PROP RDACT RR T \nST AGE3 \nST AGE2 \nST AGE1 \nST AGE0 \nRefinement \nDASTL DA TL DASTL DA TL REC \nRDACT \nRSTL R TL RER \nRR T \nReference\nImage\nSource\nImages\nReference Image\nFigure 2: Structure of CostFormer based on PatchMatchNet.\ndepth hypotheses during the evaluation process. In Section\nPreliminary, we give a brief preliminary on the pipeline of\nour method. Then we show the construction of RDACT and\nRRT respectively. Finally, we show experiments.\n3.1 Preliminary\nIn general, the proposed RDACT and RRT can be integrated\nwith arbitrary cost volume of learning-based MVS networks.\nBased on the patch match architecture[Wanget al., 2021], we\nfurther explore the issue of cost aggregation on cost volume.\nAs shown in Figure 2, CostFormer based on PatchMatchNet\n[Wang et al., 2021] extracts feature maps from multi-view\nimages and performs initialization and propagation to warp\nthe features maps in source views to reference view. Given a\npixel p at the reference view and its corresponding pixel pi,j\nat the i-th source view under the j-th depth hypothesis dj is\ndefined as:\npi,j = Ki · (R0,i · (K−1\n0 · p · dj) + t0,i) (1)\nwhere R0,i and t0,i denote the rotation and translation be-\ntween the reference view andi-th source view.K0 and Ki are\nthe intrinsic matrices of the reference and i-th source view.\nThe warped feature maps at the i-th source view Fi(pi,j)\nare bilinearly interpolated to remain the original resolution.\nThen, a cost volume is constructed from the similarity of fea-\nture maps, and 3D CNNs are applied to regularize the cost\nvolume. Warped features from all source views are integrated\ninto a single cost for each pixel p and depth hypothesis dj by\ncomputing the cost per hypothesis Si(p, j)g via group-wise\ncorrection as follows:\nSi(p, j)g = G\nC < F0(p)g, Fi(pi,j)g >∈ RG (2)\nwhere G is the group number, C is the channel number, <\n·, · > is the inner product, F0(p)g and Fi(pi,j)g are grouped\nreference feature map and grouped source feature map at the\ni-th view respectively. Then they aggregate over the views\nwith a pixel-wise view weight wi(p) to get\nS(p, j).\nTaking no account of Transformer at the cost aggregation\n(CA) step, a CA module firstly utilizes a small network with\n3D convolution with1×1×1 kernels to obtain a single cost,C\n∈ RH×W×D. For a spatial window ofKe pixels {pk}Ke\nk=1 can\nbe organized as a grid, per pixel additional offsets{∆pk}Ke\nk=1\ncan be learned for spatial adaptation. The aggregated spatial\ncost eC(p, j) is defined as:\neC(p, j) = 1\nPKe\nk=1 wkdk\nKeX\nk=1\nwkdkC(p + pk + ∆pk, j) (3)\nwhere wk and dk weight the costC based on feature and depth\nsimilarity. Given the sampling positions(p + pk + ∆pk)Ke\nk=1,\ncorresponding features from F0 are extracted via bilinear in-\nterpolation. Then group-wise correlation is applied between\nthe features at each sampling location and p. The results are\nconcatenated into a volume on which 3D convolution layers\nwith 1×1×1 kernels and sigmoid non-linearities are applied to\noutput normalized weights {wk}Ke\nk=1. The absolute difference\nin inverse depth between each sampling point and pixel p with\ntheir j-th hypotheses are collected. Then a sigmoid function\non the inverted differences is applied to obtain {dk}Ke\nk=1.\nThe remarkable thing is that such cost aggregation in-\nevitably suffers from challenges due to ambiguities gener-\nated by repetitive patterns or background clutters. The local\nmechanisms in ambiguities exist in many operations, such as\nlocal propagation and spatial adaptation by small learnable\nslight offset. CostFormer significantly alleviates these prob-\nlems through RDACT and RRT. The original CA module is\nalso repositioned between RDACT and RRT.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n601\nGround TruthOursPatchmatchNet\nUCSNet\nFigure 3: Comparison of different methods on the DTU evaluation set. The backbone of CostFormer is PatchMatchNet here.\nAfter RRT, soft argmin is applied to get the regressed\ndepth. Finally, a depth refinement module is designed to re-\nfine the depth regression.\nFor CascadeMVS and other cascade architectures, Cost-\nFormer can be plugged into similarly.\n3.2 Residual Depth-Aware Cost Transformer\nIn this section, we explore the details of the Residual Depth-\nAware Cost Transformer (RDACT). Each RDACT consists\nof two parts. The first part is a stack of Depth-Aware Trans-\nformer layer (DATL) and Depth-Aware Shifted Transformer\nlayer (DASTL), which deal with the cost volumes to ex-\nplore the relations sufficiently. The second part is the Re-\nEmbedding Cost layer (REC) which recovers the cost volume\nfrom the first part.\nGiven a cost volume C0 ∈ RH×W×D×G, temporary inter-\nmediate cost volumes C1,C2,...,CL ∈ RH×W×D×E are firstly\nextracted by DATL and DASTL alternatively:\nCk = DASTLk(DATLk(Ck−1)), k= 1, 2, ..., L (4)\nwhere DATLk is the k-th Depth-Aware Transformer layer\nwith regular windows, DASTL k is the k-th Depth-Aware\nTransformer layer with shifted windows, E is the embedding\ndimension number of DATLk and DASTLk.\nThen a Re-Embedding Cost layer is applied to the last Ck,\nnamely CL, to recover G from E. The output of RDACT is\nformulated as:\nCout = REC(CL) + C0 (5)\nwhere REC is the Re-Embedding Cost layer, and it can be a\n3D convolution with G output channels. If E = G, Cout can\nbe simply formulated as:\nCout = CL + C0 (6)\nThis residual connection allows the aggregation of different\nlevels of cost volumes; Cout instead of C0 is then aggregated\nby the original aggregation network described in section 3.1.\nThe whole RDACT is shown in the red window in Figure 2.\nBefore introducing the construction of DATL and DASTL,\nwe dive into the details of core constitutions called Depth-\nAware Multi-Head Self-Attention (DA-MSA) and Depth-\nAware Shifted Multi-Head Self-Attention (DAS-MSA). Both\nDA-MSA and DAS-MSA are based on Depth-Aware Self-\nAttention Mechanism. In order to explain Depth-Aware\nSelf-Attention Mechanism, we supply the knowledge about\nDepth-Aware Patch Embedding and Depth-Aware Windows\nas preliminary.\nDepth-Aware Patch Embedding\nObviously, directly applying the attention mechanism for fea-\nture maps at pixel-wise level is quite costly in terms of GPU\nmemory usage. In order to tackle this issue, we propose a\nDepth-Aware Patch Embedding to reduce the high memory\ncost and get an additional regularization. Specifically, given a\ngrouped cost volume before aggregationC ∈RH×W×D×G, a\ndepth-aware patch embedding is firstly applied to C to get to-\nkens. It consists of a 3D convolution with kernel sizeh×w×d\nand a layer normalization.To downsample the spatial sizes of\ncost volume and keep the depth hypotheses, we set h and w\nto more than 1 and d as 1. So the sample ratio is adaptive for\nmemory cost and run time. Before convolution, cost volume\nwill be padded to fit the spatial sizes and downsampling ratio.\nAfter layer normalization(LN), these embedded patches are\nfurther partitioned by depth-aware windows.\nDepth-Aware Windows\nBeyond the nonlinear and linear global self-attention, local\nself-attention within a window has been proven to be more\neffective and efficient. As an example of 2D windows, Swin\nTransformer [Liu et al., 2021 ] directly applies multi-head\nself-attention mechanisms on non-overlapping 2D windows\nto avoid the big computation complexity of global tokens.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n602\nCOLMAP UCSNet PatchmatchNet Ours \nFigure 4: Comparison of different methods on Tanks&Temples. The Recall reported by official benchmark is presented.\nExtended from the 2D spatial window, an embedded cost vol-\nume patch ∈ RH∗×W∗×D∗×G with depth information is par-\ntitioned into non-overlapping 3D windows. These local win-\ndows are then transposed and reshaped to local cost tokens.\nAssuming the sizes of these windows are hs × ws × ds, the\ntotal number of tokens is ⌈H∗\nhs\n⌉ × ⌈W∗\nws\n⌉ × ⌈D∗\nds\n⌉. These local\ntokens are further processed by the multi-head self-attention\nmechanism.\nDepth-Aware Self-Attention Mechanism\nFor a cost window token X ∈ Rhs×ws×ds×G, the query, key,\nand value matrices Q, K and V ∈ Rhs×ws×ds×G are com-\nputed as:\nQ = XPQ, K= XPK, V= XPV (7)\nwhere PQ, PK, and PV ∈ RG×G are projection ma-\ntrices shared across different windows. By introduc-\ning depth and spatial aware relative position bias B1 ∈\nR(hs×hs)×(ws×ws)×(ds×ds) for each head, the depth-aware\nself-attention(DA-SA1) matrix within a 3D local window is\nthus computed as:\nDA-SA1 = Attention1(Q1, K1, V1) =SoftMax ( Q1K1T\n√\nG\n+ B1)V 1\n(8)\nWhere Q1, K1 and V 1 ∈ Rhswsds×G are reshaped from\nQ, K and V ∈ Rhs×ws×ds×G. The process of DATL with\nLayerNorm(LN) and multi-head DA-SA1 at the current level\nis formulated as:\nbXl = DA-MSA1((LN(Xl−1)) + Xl−1 (9)\nBy introducing depth-aware relative position bias B2 ∈\nRds×ds for each head, the depth-aware self-attention(DA-\nSA2) matrix along the depth dimension is an alternative mod-\nule to DATL and thus computed as:\nDA-SA2 = Attention2(Q2, K2, V2) =SoftMax ( Q2K2T\n√\nG\n+ B2)V 2\n(10)\nWhere Q2, K2 and V 2 ∈ Rhsws×ds×G are reshaped from\nQ, K and V ∈ Rhs×ws×ds×G. B1 and B2 will be along the\ndepth dimension and lie in the range of [−ds + 1, ds − 1].\nAlong the height and width dimension, B1 lies in the range\nof [−hs + 1, hs − 1] and [−ws + 1, ws − 1]. In prac-\ntice, we parameterize a smaller-sized bias matrix B1 ∈\nR(2hs−1)×(2ws−1)×(2ds−1) from B1 and perform the atten-\ntion functionfor f times in parallel, and then concatenate\nthe depth-aware multi-head self-attention (DA-MSA) out-\nputs. The process of DATL with LayerNorm(LN), multi-head\nDA-SA1, and DA-SA2 at the current level is formulated as:\nbXl = DA-MSA1(LN(DA-MSA2(LN(Xl−1)))) + Xl−1\n(11)\nThen, an MLP module that has two fully-connected layers\nwith GELU non-linearity between them is used for further\nfeature transformations:\nXl = MLP(LN( bXl))) + bXl (12)\nCompared with global attention, local attention makes it pos-\nsible for computation in high resolution.\nHowever, there is no connection across local windows with\nfixed partitions. Therefore, regular and shifted window par-\ntitions are used alternately to enable cross-window connec-\ntions. So at the next level, the window partition configu-\nration is shifted along the height, width, and depth axes by\n(hs\n2 , ws\n2 , ds\n2 ). Depth-aware self-attention will be computed\nin these shifted windows(DAS-MSA); the whole process of\nDASTL can be formulated as:\nbXl+1 = DAS-MSA1(LN(DAS-MSA2(LN(Xl)))) + Xl\n(13)\nXl+1 = MLP(LN( bXl+1)) + bXl+1 (14)\nDAS-MSA1 and DAS-MSA2 correspond to multi-head\nAttention1 and Attention2 within a shifted window, respec-\ntively. Assuming the number of stages is n, there are n\nRDACT blocks in CostFormer.\n3.3 Residual Regression Transformer\nAfter aggregation, the cost eC ∈RHXWXD will be used for\ndepth regression. To further explore the spatial relation under\nsome depth, a Transformer block is applied to eC before soft-\nmax. Inspired by the RDACT, the whole process of Residual\nRegression Transformer(RRT) can be formulated as:\neCk = RSTk(RTk( eCk−1)), k= 1, 2, ..., L (15)\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n603\nMethods Intermediate Group (F-score ↑)\nMean Fam. Fra. Hor. Lig. M60 Pan. Pla. Tra.\nMVSNet 43.48 55.99 28.55 25.07 50.79 53.96 50.86 47.90 34.69\nCasMVSNet 56.84 76.37 58.45 46.26 55.81 56.11 54.06 58.18 49.51\nUCS-Net 54.83 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89\nCVP-MVSNet 54.03 76.50 47.74 36.34 55.12 57.28 54.28 57.43 47.54\nPV A-MVSNet 54.46 69.36 46.80 46.01 55.74 57.23 54.75 56.70 49.06\nAA-RMVSNet 61.51 77.77 59.53 51.53 64.02 64.05 59.47 60.85 54.90\nPatchmatchNet 53.15 66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81\nUniMVSNet 64.36 81.20 66.34 53.11 63.46 66.09 64.84 62.23 57.53\nMVSTR 56.93 76.92 59.82 50.16 56.73 56.53 51.22 56.58 47.48\nTransMVS 63.52 80.92 65.83 56.94 62.54 63.06 60.00 60.20 58.67\nCostFormer-PM 56.27↑3.12 72.46 52.59 54.27 55.83 56.80 50.88 55.05 52.32\nCostFormer-PM* 57.10↑3.95 74.22 56.27 54.41 56.65 54.46 51.45 57.65 51.70\nCostFormer-Uni− 64.40↑0.04 81.45 66.22 53.88 62.94 66.12 65.35 61.31 57.90\nCostFormer-Uni* 64.51↑0.15 81.31 65.51 55.57 63.46 66.24 65.39 61.27 57.30\nTable 1: Quantitative results on the Intermediate group of Tanks&Temples benchmark (higher is better). * is pretrained on DTU and fine-\ntuned on BlendedMVS. - is not pretrained on DTU and trained from scratch on BlendedMVS\nMethods Advanced Group (F-score ↑)\nMean Aud. Bal. Cou. Mus. Pal. Tem.\nCasMVSNet 31.12 19.81 38.46 29.10 43.87 27.36 28.11\nAA-RMVSNet 33.53 20.96 40.15 32.05 46.01 29.28 32.71\nPatchmatchNet 32.31 23.69 37.73 30.04 41.80 28.31 32.29\nUniMVSNet 38.96 28.33 44.36 39.74 52.89 33.80 34.63\nMVSTR 32.85 22.83 39.04 33.87 45.46 27.95 27.97\nTransMVS 37.00 24.84 44.59 34.77 46.49 34.69 36.62\nMVSTER 37.53 26.68 42.14 35.65 49.37 32.16 39.19\nCostFormer-PM 34.07↑1.76 24.05 39.20 32.17 43.95 28.62 36.46\nCostFormer-PM* 34.31↑2.00 26.77 39.13 31.58 44.55 28.79 35.03\nCostFormer-Uni− 39.55↑0.59 28.61 45.63 40.21 52.81 34.40 35.62\nCostFormer-Uni* 39.43↑0.47 29.18 45.21 39.88 53.38 34.07 34.87\nTable 2: Quantitative results on the Advanced group of Tanks&Temples benchmark (higher is better). * is pretrained on DTU and fine-tuned\non BlendedMVS. - is not pretrained on DTU and trained from scratch on BlendedMVS\neCout = RER( eCL) + eC0 (16)\nwhere RTk is the k-th Regression Transformer layer with reg-\nular windows, RSTk is the k-th Regression Transformer layer\nwith shifted windows, RER is the re-embedding layer to re-\ncover the depth dimension from eCL, and it can be a 2D con-\nvolution with D output channels.\nRRT also computes self-attention in a local window. Com-\npared with RDACT, RRT focuses more on spatial relations.\nCompared with regular Swin [Liu et al., 2021] Transformer\nblock, RRT treats the depth as a channel, the number of chan-\nnels is actually 1 and this channel is squeezed before the\nTransformer. The embedding parameters are set to fit the cost\naggregation of different iterations. If the embedding dimen-\nsion number equals D, eCout can be simply formulated as:\neCout = eCL + eC0 (17)\nAs a stage may iterate many times with different depth hy-\npotheses, the number of RRT blocks should be set the same\nas the number of iterations. The whole RRT is shown in the\nyellow window in Figure 2.\n3.4 Training\nLoss Function\nFinal loss combines with the losses of all iterations at all\nstages and the loss from the final refinement module:\nLoss =\nsX\nk=1\nnX\ni=1\nLk\ni + Lref (18)\nwhere Lk\ni is the regression or unification loss of the i-th iter-\nation at k-th stage. Lref is the regression or unification loss\nfrom refinement module. If refinement module does not exist,\nthe Lref loss is set to zero.\nCommon Training Settings\nCostFormer is implemented by Pytorch [Paszke et al., 2019].\nFor RDACT, we set the depth number at stages 3, 2, 1 as 4,\n2, 2; patch size at height, width and depth axes as 4, 4, 1;\nwindow size at height, width and depth axes as 7, 7, 2. If\nthe backbone is set as PatchMatchNet, embedding dimension\nnumber at stages 3, 2, 1 are set as 8, 8, 4. For RRT, we set\nthe depth number as 2 at all stages, patch size as 1 at all axes;\nwindow size as 8 at all axes. If the backbone is set as Patch-\nMatchNet, embedding dimension number at iteration 2, 2, 1\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n604\nat stages 3, 2, 1 as 32, 64, 16, 16, 8. All models are trained\non Nvidia GTX V100 GPUs. After depth estimation, we re-\nconstruct point clouds similar to MVSNet [Yao et al., 2018].\n4 Experiments\n4.1 Compared Methods\nIn the experiments, we compare our proposed method with\nfollowing state-of-the-art MVS methods, including: 1) Tradi-\ntional MVS methods, like Furu [Furukawa and Ponce, 2010],\nTola [Tola et al., 2012 ], Gipuma [Galliani et al., 2015 ],\nColmap [Sch¨onberger and Frahm, 2016], MVE [Fuhrmann et\nal., 2014], PMVS [Furukawa and Ponce, 2010]; 2) Learning-\nbased MVS methods, like SurfaceNet [Ji et al., 2017], MVS-\nNet [Yao et al., 2018 ], R-MVSNet [Yao et al., 2019 ], P-\nMVSNet [Luo et al., 2019 ], Point-MVSNet [Chen et al.,\n2019], Fast-MVSNet [Yu and Gao, 2020], CasMVSNet [Gu\net al., 2020], UCS-Net [Cheng et al., 2020], CVP-MVSNet\n[Yang et al., 2020b], PV A-MVSNet[Yi et al., 2020], Patch-\nmatchNet [Wang et al., 2021], AA-RMVSNet [Wei et al.,\n2021], UniMVSNet [Peng et al., 2022], PVSNet [Xu and Tao,\n2020], IterMVS [Wang et al., 2022a], PatchMatch-RL [Lee\net al., 2021], MVSNet-s [Darmon et al., 2021] , VisMVSNet\n[Zhang et al., 2020], and EPPMVSNet [Ma et al., 2021]. Fur-\nthermore, we also conduct the comparison with Transformer\nbased MVS networks: MVSTR [Zhu et al., 2021], Trans-\nMVSNet [Ding et al., 2022], MVSTER [Wang et al., 2022b],\nand WT-MVSNet [Liao et al., 2022].\n4.2 Main Settings\nOur proposed CostFormer can utilize arbitrary MVS network\nas backbone. We name the framework as CostFormer-PM,\nCostFormer-Cas, and CostFormer-Uni, when PatchMatch-\nNet, CasMVSNet, and UniMVSNet are respectively used as\nbackbone.\n4.3 Datasets\nThe datasets used in the evaluation are DTU [Aanæs et al.,\n2016], BlendedMVS [Yao et al., 2020 ], ETH3D [Sch¨ops\net al., 2017], Tanks&Temples [Knapitsch et al., 2017], and\nYFCC-100M [Thomee et al., 2016]. The DTU dataset is an\nindoor multi-view stereo dataset with 124 different scenes,\nthere are 49 views under seven different lighting conditions\nin one scene. Tanks& Temples is collected in a more com-\nplex and realistic environment, and it’s divided into the in-\ntermediate and advanced set. ETH3D benchmark consists of\ncalibrated high-resolution images of scenes with strong view-\npoint variations. It is divided into training and test datasets.\nWhile the training dataset contains 13 scenes, the test dataset\ncontains 12 scenes. BlendedMVS dataset is a large-scale syn-\nthetic dataset, consisting of 113 indoor and outdoor scenes\nand split into 106 training scenes and 7 validation scenes.\n4.4 Results on Tanks&Temples Benchmark\nFor the evaluation on Tanks& Temples, we use the DTU\ndataset and the Blended MVS dataset. We compare our\nmethod to those recent learning-based MVS methods, in-\ncluding PatchMatchNet and UniMVSNet which are also set\nas backbones of CostFormer. The quantitative results on\nMethods Acc. Comp. Overall\nFuru 0.613 0.941 0.777\nTola 0.342 1.190 0.766\nGipuma 0.283 0.873 0.578\nColmap 0.400 0.644 0.532\nSurfaceNet 0.450 1.040 0.745\nMVSNet 0.396 0.527 0.462\nR-MVSNet 0.383 0.452 0.417\nP-MVSNet 0.406 0.434 0.420\nPoint-MVSNet 0.342 0.411 0.376\nFast-MVSNet 0.336 0.403 0.370\nCasMVSNet 0.325 0.385 0.355\nUCS-Net 0.338 0.349 0.344\nCVP-MVSNet 0.296 0.406 0.351\nPV A-MVSNet 0.379 0.336 0.357\nPatchMatchNet 0.427 0.277 0.352\nAA-RMVSNet 0.376 0.339 0.357\nUniMVSNet 0.352 0.278 0.315\nCostFormer-PM 0.424 0.262 0.343\nCostFormer-Cas 0.378 0.313 0.345\nCostFormer-Uni 0.301 0.322 0.312\nTable 3: Quantitative results of different methods on DTU.\nMethod GPU Mem. Run-time Overall\nCasMVSNet 262.47% 221.24% 0.355\nUCSNet 195.31% 202.84% 0.344\nCVP-MVSNet 273.97% 440.53% 0.351\nOurs 100.00% 100.00% 0.343\nTable 4: Comparison with other SOTA learning-based MVS meth-\nods on DTU. Relationship between overall performance, GPU mem-\nory and run-time.\nthe Tanks&Temples set are summarized in Table 1 and 2,\nwhich indicate the robustness of CostFormer. Note that the\nUniMVSNet− in the table only uses BlendedMVS for train-\ning which uses less data (no DTU) than the UniMVSNet\nbaseline. The qualitative results are shown in Figure 4.\n4.5 Results on DTU Benchmark\nComparisons with Other Methods\nFor the evaluation on the DTU evaluation set, we only use\nthe DTU training set. During the training phase, we set the\nimage resolution to 640 × 512. We compare our method to\nrecent learning-based MVS methods, including CasMVSNet\nand PatchMatchNet which are also set as backbones of Cost-\nFormer. We follow the evaluation metrics provided by the\nDTU dataset. The quantitative results on the DTU evaluation\nset are summarized in Table 3, which indicates that the plug-\nand-play CostFormer improves the cost aggregation. Partial\nvisualization results of Table 3 are shown in Figure 3.\nComplexity Analysis\nFor the complexity analysis of CostFormer, we plug it into\nPatchMatchNet and first compare the memory consumption\nand run-time with this backbone. For a fair comparison, a\nfixed input size of 1152 × 864 is used to evaluate the computa-\ntional cost on a single GPU of NVIDIA Telsa V100. Memory\nconsumption and run-time of PatchMatchNet are 2323MB\nand 0.169s. They are only increased to 2693MB and 0.231s\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n605\nMethod Trans Improv.(mm) Time(s) Time(%)\nMVSTR +0.0140 +0.359s +78.21%\nTransMVSNet +0.0160 +0.367s +135.42%\nWT-MVSNet +0.0130 +0.265s -\nMVSTER +0.0040 +0.016s +13.34%\nOurs +0.0097 +0.062s +36.69%\nTable 5: Quantitative improvement of performance and incremental\ncost of run time of different Transformers on DTU evaluation set.\nMethods Training Testing\nF1 ↑ Time(s) ↓ F1 ↑ Time(s) ↓\nMVE 20.47 1.32 × 104 30.37 1.06 × 104\nGipuma 36.38 5.87 × 102 45.18 6.90 × 102\nPMVS 46.06 8.37 × 102 44.16 9.57 × 102\nCOLMAP 67.66 2.69 × 103 73.01 1.66 × 103\nPVSNet 67.48 - 72.08 8.29 × 102\nIterMVS 66.36 - 74.29 -\nPatchMatchNet 64.21 4.52 × 102 73.12 4.92 × 102\nPatchMatch-RL 67.78 - 72.38 -\nCostFormer-PM 68.92 5.66 × 102 75.24 5.48 × 102\nTable 6: Quantitative results of different methods on ETH3D.\nby the plug-in.\nBased on the reports of PatchMatchNet, we then get the\ncomparison results of other state-of-the-art learning-based\nmethods. Memory consumption and run-time are reduced\nby 61.9% and 54.8% compared to CasMVSNet, by 48.8%\nand 50.7% compared to UCSNet and by 63.5% and 77.3%\ncompared to CVP-MVSNet. Combining the results(lower is\nbetter) are shown in Table 4 and Figure 1, GPU memory and\nrun-time of CostFormer are set as 100%.\nComparison with Other Transformer-based Methods\nWe also compare CostFormer with other Transformers-based\nnetworks which are used in MVS methods and not plug-and-\nplay. For a fair comparison, only direct improvements(higer\nis better) and incremental cost of run time(low is better) from\npure Transformers under similar depth hypotheses are sum-\nmarized in Table 5.\n4.6 Results on ETH3D Benchmark\nWe use the PatchMatchNet as backbone and adopt the trained\nmodel used in the Tanks&Temples dataset to evaluate the\nETH3D dataset. As shown in Table 6, our method outper-\nforms others on both the training and particularly challenging\ntest datasets(higher is better).\n4.7 Results on BlendedMVS Benchmark\nWe use the model used in ETH3D. On BlendedMVS evalu-\nation set, we set N = 5 and image resolution as 576 × 768.\nEnd point error (EPE), 1 pixel error (e1), and 3 pexels er-\nror (e3) are used as the evaluation metrics. Quantitative re-\nsults(lower is better) of different methods are shown in Table\n7.\n4.8 Results on YFCC Dataset\nFollowing the depth evaluation protocal in previous work, we\nutilize the YFCC-100M [Thomee et al., 2016] to test the per-\nMethod EPE e1(%) e3(%)\nMVSNet 1.49 21.98 8.32\nMVSNet-s 1.35 25.91 8.55\nCVP-MVSNet 1.90 19.73 10.24\nVisMVSNet 1.47 18.47 7.59\nCasMVSNet 1.98 15.25 7.60\nEPPMVSNet 1.17 12.66 6.20\nTransMVSNet 0.73 8.32 3.62\nCostFormer-PM 0.84 12.37 4.59\nCostFormer-Uni 0.43 7.05 2.70\nTable 7: Quantitative results of different methods on BlendedMVS.\nMethod EPE e1(%) e3(%)\nMVSNet 21.56 67.93 49.75\nMVSNet-s 20.98 69.57 49.86\nCVP-MVSNet 40.07 85.88 76.25\nVisMVSNet 19.60 64.98 46.38\nCasMVSNet 13.86 63.83 47.27\nPatchMatchNet 9.97 62.51 43.62\nCostFormer-PM 7.82 54.10 34.71\nTable 8: Quantitative results of different methods on the YFCC test\nset.\nformance. YFCC100M is the largest public multimedia col-\nlection that has ever been released. The dataset contains a\ntotal of 100 million media objects, of which approximately\n99.2 million are photos and 0.8 million are videos. We set\nbackbone as PatchMatchNet, N = 5, and image resolution\nas 512x512. End point error (EPE), 1 pixel error (e1), and\n3 pixels error (e3) are also used as the evaluation metrics.\nQuantitative results of different methods are shown in Tabisle\n8.\n5 Conclusion\nIn this work, we explore whether cost Transformer can im-\nprove the cost aggregation and propose a novel CostFormer\nwith the cascade RDACT and RRT modules. The experimen-\ntal results on DTU , Tanks&Temples, ETH3D, BlendedMVS,\nand YFCC show that our method is competitive, efficient, and\nplug-and-play. Cost Transformer can be your need for better\ncost aggregation in multi-view stereo.\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China (No.61976095).\nContribution Statement\nWeitao Chen and Hongbin Xu contribute equally to this work.\nReferences\n[Aanæs et al., 2016] Henrik Aanæs, Rasmus Ramsbøl\nJensen, George V ogiatzis, Engin Tola, and An-\nders Bjorholm Dahl. Large-scale data for multiple-view\nstereopsis. Int. J. Comput. Vis., 120(2):153–168, 2016.\n[Chen et al., 2019] Rui Chen, Songfang Han, Jing Xu, and\nHao Su. Point-based multi-view stereo network. In ICCV,\npages 1538–1547. IEEE, 2019.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n606\n[Cheng et al., 2020] Shuo Cheng, Zexiang Xu, Shilin Zhu,\nZhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao\nSu. Deep stereo using adaptive thin volume representation\nwith uncertainty awareness. In CVPR, pages 2521–2531.\nIEEE, 2020.\n[Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul\nJeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong\nKim. Cats: Cost aggregation transformers for visual cor-\nrespondence. Advances in Neural Information Processing\nSystems, 34, 2021.\n[Darmon et al., 2021] Franc ¸ois Darmon, B´en´edicte Bascle,\nJean-Cl´ement Devaux, Pascal Monasse, and Mathieu\nAubry. Deep multi-view stereo gone wild. CoRR,\nabs/2104.15119, 2021.\n[Ding et al., 2021] Yikang Ding, Wentao Yuan, Qingtian\nZhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and\nXiao Liu. Transmvsnet: Global context-aware multi-\nview stereo network with transformers. arXiv preprint\narXiv:2111.14600, 2021.\n[Ding et al., 2022] Yikang Ding, Wentao Yuan, Qingtian\nZhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and\nXiao Liu. Transmvsnet: Global context-aware multi-view\nstereo network with transformers. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8585–8594, 2022.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Fuhrmann et al., 2014] Simon Fuhrmann, Fabian Langguth,\nand Michael Goesele. Mve - a multi-view reconstruction\nenvironment. In GCH, pages 11–18. Eurographics Asso-\nciation, 2014.\n[Furukawa and Ponce, 2010] Yasutaka Furukawa and Jean\nPonce. Accurate, dense, and robust multiview stereopsis.\nIEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362–1376,\n2010.\n[Galliani et al., 2015] Silvano Galliani, Katrin Lasinger, and\nKonrad Schindler. Massively parallel multiview stereop-\nsis by surface normal diffusion. In ICCV, pages 873–881.\nIEEE Computer Society, 2015.\n[Gu et al., 2020] Xiaodong Gu, Zhiwen Fan, Siyu Zhu,\nZuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost\nvolume for high-resolution multi-view stereo and stereo\nmatching. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2495–\n2504, 2020.\n[Hosni et al., 2012] Asmaa Hosni, Christoph Rhemann,\nMichael Bleyer, Carsten Rother, and Margrit Gelautz. Fast\ncost-volume filtering for visual correspondence and be-\nyond. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 35(2):504–511, 2012.\n[Ji et al., 2017] Mengqi Ji, Juergen Gall, Haitian Zheng,\nYebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d\nneural network for multiview stereopsis. In ICCV, pages\n2326–2334. IEEE Computer Society, 2017.\n[Knapitsch et al., 2017] Arno Knapitsch, Jaesik Park, Qian-\nYi Zhou, and Vladlen Koltun. Tanks and temples: bench-\nmarking large-scale scene reconstruction. ACM Trans.\nGraph., 36(4):78:1–78:13, 2017.\n[Lee et al., 2021] Jae Yong Lee, Joseph DeGol, Chuhang\nZou, and Derek Hoiem. Patchmatch-rl: Deep mvs with\npixelwise depth, normal, and visibility. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, October 2021.\n[Li et al., 2021] Zhaoshuo Li, Xingtong Liu, Nathan\nDrenkow, Andy Ding, Francis X Creighton, Russell H\nTaylor, and Mathias Unberath. Revisiting stereo depth\nestimation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 6197–6206,\n2021.\n[Liao et al., 2022] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe\nHuang, Shihao Ren, Jia Guo, Wensen Feng, and Kai\nZhang. Wt-mvsnet: Window-based transformers for\nmulti-view stereo. In S. Koyejo, S. Mohamed, A. Agar-\nwal, D. Belgrave, K. Cho, and A. Oh, editors,Advances in\nNeural Information Processing Systems, volume 35, pages\n8564–8576. Curran Associates, Inc., 2022.\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 10012–\n10022, 2021.\n[Luo et al., 2019] Keyang Luo, Tao Guan, Lili Ju, Haipeng\nHuang, and Yawei Luo. P-mvsnet: Learning patch-wise\nmatching confidence aggregation for multi-view stereo. In\nICCV, pages 10451–10460. IEEE, 2019.\n[Ma et al., 2021] Xinjun Ma, Yue Gong, Qirui Wang, Jing-\nwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-\nassembling based depth prediction for multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5732–5740, 2021.\n[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco\nMassa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward Yang,\nZachary DeVito, Martin Raison, Alykhan Tejani, Sasank\nChilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox,\nand R. Garnett, editors, Advances in Neural Information\nProcessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc., 2019.\n[Peng et al., 2022] Rui Peng, Rongjie Wang, Zhenyu Wang,\nYawen Lai, and Ronggang Wang. Rethinking depth esti-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n607\nmation for multi-view stereo: A unified representation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022.\n[Scharstein and Szeliski, 2002] Daniel Scharstein and\nRichard Szeliski. A taxonomy and evaluation of dense\ntwo-frame stereo correspondence algorithms. Interna-\ntional journal of computer vision, 47(1):7–42, 2002.\n[Sch¨onberger and Frahm, 2016] Johannes L. Sch ¨onberger\nand Jan-Michael Frahm. Structure-from-motion revis-\nited. In CVPR, pages 4104–4113. IEEE Computer Society,\n2016.\n[Sch¨ops et al., 2017] Thomas Sch ¨ops, Johannes L.\nSch¨onberger, Silvano Galliani, Torsten Sattler, Kon-\nrad Schindler, Marc Pollefeys, and Andreas Geiger. A\nmulti-view stereo benchmark with high-resolution images\nand multi-camera videos. In CVPR, pages 2538–2547.\nIEEE Computer Society, 2017.\n[Sun et al., 2021] Jiaming Sun, Zehong Shen, Yuang Wang,\nHujun Bao, and Xiaowei Zhou. Loftr: Detector-free lo-\ncal feature matching with transformers. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8922–8931, 2021.\n[Thomee et al., 2016] Bart Thomee, David A. Shamma, Ger-\nald Friedland, Benjamin Elizalde, Karl Ni, Douglas\nPoland, Damian Borth, and Li-Jia Li. Yfcc100m: the new\ndata in multimedia research.Commun. ACM, 59(2):64–73,\n2016.\n[Tola et al., 2012] Engin Tola, Christoph Strecha, and Pascal\nFua. Efficient large-scale multi-view stereo for ultra high-\nresolution image sets. Mach. Vis. Appl., 23(5):903–920,\n2012.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing systems,\n30, 2017.\n[Wang et al., 2021] Fangjinhua Wang, Silvano Galliani,\nChristoph V ogel, Pablo Speciale, and Marc Pollefeys.\nPatchmatchnet: Learned multi-view patchmatch stereo. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 14194–14203,\n2021.\n[Wang et al., 2022a] Fangjinhua Wang, Silvano Galliani,\nChristoph V ogel, and Marc Pollefeys. Itermvs: iterative\nprobability estimation for efficient multi-view stereo. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8606–8615, 2022.\n[Wang et al., 2022b] Xiaofeng Wang, Zheng Zhu, Guan\nHuang, Fangbo Qin, Yun Ye, Yijia He, Xu Chi, and Xin-\ngang Wang. Mvster: epipolar transformer for efficient\nmulti-view stereo. In Computer Vision–ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23–27,\n2022, Proceedings, Part XXXI, pages 573–591. Springer,\n2022.\n[Wei et al., 2021] Zizhuang Wei, Qingtian Zhu, Chen Min,\nYisong Chen, and Guoping Wang. Aa-rmvsnet: Adap-\ntive aggregation recurrent multi-view stereo network. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6187–6196, 2021.\n[Xu and Tao, 2020] Qingshan Xu and Wenbing Tao. Pvs-\nnet: Pixelwise visibility-aware multi-view stereo network.\nCoRR, abs/2007.07714, 2020.\n[Yan et al., 2020] Jianfeng Yan, Zizhuang Wei, Hongwei\nYi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping\nWang, and Yu-Wing Tai. Dense hybrid recurrent multi-\nview stereo net with dynamic consistency checking. InEu-\nropean Conference on Computer Vision, pages 674–689.\nSpringer, 2020.\n[Yang et al., 2020a] Jiayu Yang, Wei Mao, Jose M Alvarez,\nand Miaomiao Liu. Cost volume pyramid based depth\ninference for multi-view stereo. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4877–4886, 2020.\n[Yang et al., 2020b] Jiayu Yang, Wei Mao, Jose M. Alvarez,\nand Miaomiao Liu. Cost volume pyramid based depth in-\nference for multi-view stereo. InCVPR, pages 4876–4885.\nIEEE, 2020.\n[Yao et al., 2018] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang,\nand Long Quan. Mvsnet: Depth inference for unstructured\nmulti-view stereo. InProceedings of the European Confer-\nence on Computer Vision (ECCV), pages 767–783, 2018.\n[Yao et al., 2019] Yao Yao, Zixin Luo, Shiwei Li, Tianwei\nShen, Tian Fang, and Long Quan. Recurrent mvsnet\nfor high-resolution multi-view stereo depth inference. In\nCVPR, pages 5525–5534. Computer Vision Foundation /\nIEEE, 2019.\n[Yao et al., 2020] Yao Yao, Zixin Luo, Shiwei Li, Jingyang\nZhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.\nBlendedmvs: A large-scale dataset for generalized multi-\nview stereo networks. Computer Vision and Pattern\nRecognition (CVPR), 2020.\n[Yi et al., 2020] Hongwei Yi, Zizhuang Wei, Mingyu Ding,\nRunze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing\nTai. Pyramid multi-view stereo net with self-adaptive view\naggregation. In Andrea Vedaldi, Horst Bischof, Thomas\nBrox, and Jan-Michael Frahm, editors, ECCV (9), volume\n12354 of Lecture Notes in Computer Science, pages 766–\n782. Springer, 2020.\n[Yu and Gao, 2020] Zehao Yu and Shenghua Gao. Fast-\nmvsnet: Sparse-to-dense multi-view stereo with learned\npropagation and gauss-newton refinement. In CVPR,\npages 1946–1955. IEEE, 2020.\n[Zhang et al., 2020] Jingyang Zhang, Yao Yao, Shiwei Li,\nZixin Luo, and Tian Fang. Visibility-aware multi-view\nstereo network. British Machine Vision Conference\n(BMVC), 2020.\n[Zhu et al., 2021] Jie Zhu, Bo Peng, Wanqing Li, Haifeng\nShen, Zhe Zhang, and Jianjun Lei. Multi-view stereo with\ntransformer. ArXiv, abs/2112.00336, 2021.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n608"
}