{
  "title": "Richer Interpolative Smoothing Based on Modified Kneser-Ney Language Modeling",
  "url": "https://openalex.org/W2567188764",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A108414181",
      "name": "Ehsan Shareghi",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2188741563",
      "name": "Trevor Cohn",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1432492132",
      "name": "Gholamreza Haffari",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2189576523",
    "https://openalex.org/W2005902041",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2154099718",
    "https://openalex.org/W3103362336",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W126222424",
    "https://openalex.org/W2250988012",
    "https://openalex.org/W2075201173"
  ],
  "abstract": "In this work we present a generalisation of the Modified Kneser-Ney interpolative smoothing for richer smoothing via additional discount parameters.We provide mathematical underpinning for the estimator of the new discount parameters, and showcase the utility of our rich MKN language models on several European languages.We further explore the interdependency among the training data size, language model order, and number of discount parameters.Our empirical results illustrate that larger number of discount parameters, i) allows for better allocation of mass in the smoothing process, particularly on small data regime where statistical sparsity is severe, and ii) leads to significant reduction in perplexity, particularly for out-of-domain test sets which introduce higher ratio of out-ofvocabulary words. 1",
  "full_text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 944–949,\nAustin, Texas, November 1-5, 2016.c⃝2016 Association for Computational Linguistics\nRicher Interpolative Smoothing Based on Modiﬁed Kneser-Ney\nLanguage Modeling\nEhsan Shareghi,♣Trevor Cohn♠and Gholamreza Haffari♣\n♣Faculty of Information Technology, Monash University\n♠Computing and Information Systems, The University of Melbourne\nfirst.last@{monash.edu,unimelb.edu.au}\nAbstract\nIn this work we present a generalisation of the\nModiﬁed Kneser-Ney interpolative smoothing\nfor richer smoothing via additional discount\nparameters. We provide mathematical under-\npinning for the estimator of the new discount\nparameters, and showcase the utility of our\nrich MKN language models on several Euro-\npean languages. We further explore the in-\nterdependency among the training data size,\nlanguage model order, and number of dis-\ncount parameters. Our empirical results illus-\ntrate that larger number of discount parame-\nters, i) allows for better allocation of mass in\nthe smoothing process, particularly on small\ndata regime where statistical sparsity is se-\nvere, and ii) leads to signiﬁcant reduction in\nperplexity, particularly for out-of-domain test\nsets which introduce higher ratio of out-of-\nvocabulary words.1\n1 Introduction\nProbabilistic language models (LMs) are the core\nof many natural language processing tasks, such as\nmachine translation and automatic speech recogni-\ntion. m-gram models, the corner stone of language\nmodeling, decompose the probability of an utter-\nance into conditional probabilities of words given a\nﬁxed-length context. Due to sparsity of the events\nin natural language, smoothing techniques are criti-\ncal for generalisation beyond the training text when\nestimating the parameters of m-gram LMs. This\nis particularly important when the training text is\n1For the implementation see: https://github.com/\neehsan/cstlm\nsmall, e.g. building language models for translation\nor speech recognition in low-resource languages.\nA widely used and successful smoothing method\nis interpolated Modiﬁed Kneser-Ney (MKN) (Chen\nand Goodman, 1999). This method uses a linear in-\nterpolation of higher and lower order m-gram prob-\nabilities by preserving probability mass via absolute\ndiscounting. In this paper, we extend MKN by in-\ntroducing additional discount parameters, leading to\na richer smoothing scheme. This is particularly im-\nportant when statistical sparsity is more severe, i.e.,\nin building high-order LMs on small data, or when\nout-of-domain test sets are used.\nPrevious research in MKN language modeling,\nand more generally m-gram models, has mainly\ndedicated efforts to make them faster and more com-\npact (Stolcke et al., 2011; Heaﬁeld, 2011; Shareghi\net al., 2015) using advanced data structures such as\nsuccinct sufﬁx trees. An exception is Hierarchical\nPitman-Yor Process LMs (Teh, 2006a; Teh, 2006b)\nproviding a rich Bayesian smoothing scheme, for\nwhich Kneser-Ney smoothing corresponds to an ap-\nproximate inference method. Inspired by this work,\nwe directly enrich MKN smoothing realising some\nof the reductions while remaining more efﬁcient in\nlearning and inference.\nWe provide estimators for our additional discount\nparameters by extending the discount bounds in\nMKN. We empirically analyze our enriched MKN\nLMs on several European languages in in- and out-\nof-domain settings. The results show that our dis-\ncounting mechanism signiﬁcantly improves the per-\nplexity compared to MKN and offers a more elegant\n944\nway of dealing with out-of-vocabulary (OOV) words\nand domain mismatch.\n2 Enriched Modiﬁed Kneser-Ney\nInterpolative Modiﬁed Kneser-Ney (MKN) (Chen\nand Goodman, 1999) smoothing is widely accepted\nas a state-of-the-art technique and is implemented in\nleading LM toolkits, e.g., SRILM (Stolcke, 2002)\nand KenLM (Heaﬁeld, 2011).\nMKN uses lower order k-gram probabilities to\nsmooth higher order probabilities. P(w|u) is de-\nﬁned as,\nc(uw) −Dm(c(uw))\nc(u) + γ(u)\nc(u) ×¯P(w|π(u))\nwhere c(u) is the frequency of the pattern u, γ(.) is\na constant ensuring the distribution sums to one, and\n¯P(w|π(u)) is the smoothed probability computed\nrecursively based on a similar formula2 conditioned\non the sufﬁx of the pattern u denoted by π(u). Of\nparticular interest are the discount parametersDm(.)\nwhich remove some probability mass from the max-\nimum likelihood estimate for each event which is\nredistributed over the smoothing distribution. The\ndiscounts are estimated as\nDm(i) =\n\n\n\n0, if i = 0\n1 −2 n2[m]\nn1[m]\nn1[m]\nn1[m]+2n2[m] , if i = 1\n2 −3 n3[m]\nn2[m]\nn1[m]\nn1[m]+2n2[m] , if i = 2\n3 −4 n4[m]\nn3[m] . n1[m]\nn1[m]+2n2[m] , if i ≥3\nwhere ni(m) is the number of unique m-grams3 of\nfrequency i. This effectively leads to three discount\nparameters {Dm(1),Dm(2),Dm(3+)}for the distri-\nbutions on a particular context length, m.\n2.1 Generalised MKN\nNey et al. (1994) characterized the data sparsity us-\ning the following empirical inequalities,\n3n3[m] <2n2[m] <n1[m] for m≤3\nIt can be shown (see Appendix A) that these em-\npirical inequalities can be extended to higher fre-\n2Note that in all but the top layer of the hierarchy, con-\ntinuation counts, which count the number of unique contexts,\nare used in place of the frequency counts (Chen and Goodman,\n1999).\n3Continuation counts are used for the lower layers.\nquencies and larger contexts m> 3,\n(N −m)nN−m[m] <...< 2n2[m]\n<n1[m] <\n∑\ni>0\nni[m] ≪n0[m] <σm\nwhere σm is the possible number of m-grams over\na vocabulary of size σ, n0[m] is the number of m-\ngrams that never occurred, and ∑\ni>0 ni[m] is the\nnumber of m-grams observed in the training data.\nWe use these inequalities to extend the discount\ndepth of MKN, resulting in new discount parame-\nters. The additional discount parameters increase the\nﬂexibility of the model in altering a wider range of\nraw counts, resulting in a more elegant way of as-\nsigning the mass in the smoothing process. In our\nexperiments, we set the number of discounts to 10\nfor all the levels of the hierarchy, (compare this to\nthese in MKN). 4 This results in the following esti-\nmators for the discounts,\nDm(i) =\n\n\n\n0, if i = 0\ni −(i + 1)\nni+1[m]\nni[m]\nn1[m]\nn1[m]+2n2[m] , if i <10\n10 −11 n11[m]\nn10[m] . n1[m]\nn1[m]+2n2[m] , if i ≥10\nIt can be shown that the above estimators for our dis-\ncount parameters are derived by maximizing a lower\nbound on the leave-one-out likelihood of the training\nset, following (Ney et al., 1994; Chen and Goodman,\n1999) (see Appendix B for the proof sketch).\n3 Experiments\nWe compare the effect of using different numbers of\ndiscount parameters on perplexity using the Finnish\n(FI), Spanish (ES), German (DE), English (EN) por-\ntions of the Europarl v7 (Koehn, 2005) corpus. For\neach language we excluded the ﬁrst 10K sentences\nand used it as the in-domain test set (denoted as EU),\nskipped the second 10K sentences, and used the rest\nas the training set. The data was tokenized, sentence\nsplit, and the XML markup discarded. We tested\nthe effect of domain mismatch, under two settings\nfor out-of-domain test sets: i) mild using the Span-\nish section of news-test 2013, the German, English\nsections of news-test 2014, and the Finnish section\n4We have selected the value of 10 arbitrarily; however our\napproach can be used with larger number of discount parame-\nters, with the caveat that we would need to handle sparse counts\nin the higher orders.\n945\nPerplexity\nsize (M) size (K) MKN ( D1...3) MKN ( D[1...4]) MKN ( D[1...10])\nTraining tokens sents Test tokens sents OOV% m= 2 m= 5 m= 10m= 2 m= 5 m= 10m= 2 m= 5 m= 10\nNT 19.8 3 9 .2 6536.6 5900.3 5897.3 6451.3 5827.6 5824.6 6154.4 5575.0 5572.5\nFI 46.5 2 .2 EU 197.3 10 6 .1 390 .7 287 .4 286 .8 390 .7 287 .3 286 .6 390 .4 287 .3 286 .8\nTW 10.9 1 .3 52 .1 57 825.1 51 744.1 51 740.1 55 550.2 49 884.2 49 881.3 47 696.2 43 277.3 43 275.5\nNT 70.7 3 9 .1 565 .6 431 .5 429 .4 560 .0 425 .5 423 .5 541 .5 409 .0 407 .3\nES 68.0 2 .2 EU 281.5 10 2 .4 92 .7 51 .5 51 .1 92 .8 51 .5 51 .1 92 .8 51 .4 51 .0\nTW 3141.3 293 78 .5 17 804.2 14 062.7 14 027.1 17 121.4 13 487.4 13 454.1 14 915.7 11 832.1 11 807.2\nNT 64.5 3 18 .7 2190.7 1784.6 1781.8 2158.9 1755.8 1753.2 2065.3 1680.6 1678.3\nDE 61.2 2 .3 EU 244.0 10 4 .6 156 .9 91 .7 91 .2 156 .9 91 .6 91 .2 156 .4 91 .7 91 .2\nMED 317.7 10 59 .8 5135.7 4232.4 4226.7 5007.5 4123.0 4117.5 4636.0 3831.2 3826.6\nNT 69.5 3 5 .5 1089.2 875 .0 872 .2 1071.1 857 .2 854 .4 1011.5 806 .7 804 .4\nEN 67.5 2 .2 EU 274.9 10 1 .7 90 .1 48 .4 48 .1 90 .1 48 .3 48 .0 90 .5 48 .3 48 .0\nMED 405.9 10 44 .1 2319.7 1947.9 1942.5 2261.6 1893.3 1888.2 2071.9 1734.9 1730.8\nTable 1: Perplexity for various m-gram orders m ∈2,3,10 and training languages from Europarl, using different\nnumbers of discount parameters for MKN. MKN (D[1...3]), MKN (D[1...4]), MKN (D[1...10]) represent vanilla MKN,\nMKN with 1 more discounts, and MKN with 7 more discount parameters, respectively. Test sets sources EU, NT,\nTW, MED are Europarl, news-test, Twitter, and medical patent descriptions, respectively. OOV is reported as the ratio\n|{OOV ∈test-set}|\n|{w∈test-set}| .\n0\n1\n2\n4\n8\n14\nCZ FI ES DE EN FR CZ FI ES DE EN FR\n[Corpus]\n[%Perplexity Reduction]\npplD [1...10]\npplD [1...4]\nFigure 1: Percentage of perplexity reduction for\npplxD[1...4]\nand pplxD[1...10]\ncompared with pplxD[1..3]\non\ndifferent training corpora (Europarl CZ, FI, ES, DE, EN,\nFR) and on news-test sets (NT) for m = 2 (left), and\nm= 10 (right).\nof news-test 2015 (all denoted as NT) 5, and ii) ex-\ntreme using a 24 hour period of streamed Finnish,\nand Spanish tweets6 (denoted as TW), and the Ger-\nman and English sections of the patent description\nof medical translation task7 (denoted as MED). See\nTable 1 for statistics of the training and test sets.\n3.1 Perplexity\nTable 1 shows substantial reduction in perplexity on\nall languages for out-of-domain test sets when ex-\npanding the number of discount parameters from 3\nin vanilla MKN to 4 and 10. Consider the English\n5http://www.statmt.org/{wmt13,14,15}/test.tgz\n6Streamed via Twitter API on 17/05/2016.\n7http://www.statmt.org/wmt14/medical-task/\nnews-test (NT), in which even for a2-gram language\nmodel a single extra discount parameter ( m = 2 ,\nD[1...4]) improves the perplexity by 18 points and\nthis improvement quadruples to 77 points when us-\ning 10 discounts ( m = 2 , D[1...10]). This effect\nis consistent across the Europarl corpora, and for\nall LM orders. We observe a substantial improve-\nments even for m= 10-gram models (see Figure 1).\nOn the medical test set which has 9 times higher\nOOV ratio, the perplexity reduction shows a simi-\nlar trend. However, these reductions vanish when an\nin-domain test set is used. Note that we use the same\ntreatment of OOV words for computing the perplex-\nities which is used in KenLM (Heaﬁeld, 2013).\n3.2 Analysis\nOut-of-domain and Out-of-vocabulary We se-\nlected the Finnish language for which the number\nand ratio of OOVs are close on its out-of-domain\nand in-domain test sets (NT and EU), while show-\ning substantial reduction in perplexity on out-of-\ndomain test set, see FI bars on Figure 1. Figure 2\n(left), shows the full perplexity results for Finnish\nfor vanilla MKN, and our extensions when tested on\nin-domain (EU) and out-of-domain (NT) test sets.\nThe discount plot, Figure 2 (middle) illustrates the\nbehaviour of the various discount parameters. We\nalso measured the average hit length for queries by\nvarying mon in-domain and out-of-domain test sets.\nAs illustrated in Figure 2 (right) the in-domain test\nset allows for longer matches to the training data as\n946\n2 3 4 5 6 7 8 9 10 ∞\n285\n390\n5500\n5800\n5900\n6550\nm\nPerplexity\n●\n● ● ● ● ● ● ● ● ●\n●\n●\n● ● ● ● ● ● ● ●\n●\nD [1...3]\nD [1...4]\nD [1...10]\nNT\nEU\n1 2 3 4 5 6 7 8 9 10\n1\n1.5\n2.5\n3.5\n4.5\ni\nDiscount\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n● ● ●\n● ●\n● ●\n●\n●\n● ● ●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n● ● ● ●\n●\n●\n●\n●\n●●\n●\n●\n●\n●\n1−gram\n2−gram\n3−gram\n4−gram\n5−gram\n6−gram\n7−gram\n8−gram\n9−gram\n10−gram\n2 3 4 5 6 7 8 9 10\n1\n1.5\n2\n2.5\n3\nm\nAverage hit length\nNT\nEU\nFigure 2: Statistics for the Finnish section of Europarl. The left plot illustrates the perplexity when tested on an out-\nof-domain (NT) and in-domain (EU) test sets varying LM order, m. The middle plot shows the discount parameters\nDi∈[1...10] for different m-gram orders. The right plot correspond to average hit length on EU and NT test sets.\nm Discount\nPerplexity2\n3\n4\n5\n6\n7\n8\n9\n10\n3\n4\n10\n1678\n2190\nm Discount\nPerplexity2\n3\n4\n5\n6\n7\n8\n9\n10\n3\n4\n10\n180\n426\nFigure 3: Perplexity (z-axis) vs. m∈[2...10] (x-axis) vs.\nnumber of discounts Di∈3,4,10 (y-axis) for German lan-\nguage trained on Europarl (left), and CommonCrawl2014\n(right) and tested on news-test. Arrows show the direc-\ntion of the increase.\nmgrows. This indicates that having more discount\nparameters is not only useful for test sets with ex-\ntremely high number of OOV , but also allows for a\nmore elegant way of assigning mass in the smooth-\ning process when there is a domain mismatch.\nInterdependency of m, data size, and discounts\nTo explore the correlation between these factors we\nselected the German and investigated this correla-\ntion on two different training data sizes: Europarl\n(61M words), and CommonCrawl 2014 (984M\nwords). Figure 3 illustrates the correlation between\nthese factors using the same test set but with small\nand large training sets. Considering the slopes of the\nsurfaces indicates that the small training data regime\n(left) which has higher sparsity, and more OOV in\nthe test time beneﬁts substantially from the more ac-\ncurate discounting compared to the large training set\n(right) in which the gain from discounting is slight.8\n8Nonetheless, the improvement in perplexity consistently\ngrows with introducing more discount parameters even under\n4 Conclusions\nIn this work we proposed a generalisation of Modi-\nﬁed Kneser-Ney interpolative language modeling by\nintroducing new discount parameters. We provide\nthe mathematical proof for the discount bounds used\nin Modiﬁed Kneser-Ney and extend it further and il-\nlustrate the impact of our extension empirically on\ndifferent Europarl languages using in-domain and\nout-of-domain test sets.\nThe empirical results on various training and test\nsets show that our proposed approach allows for a\nmore elegant way of treating OOVs and mass assign-\nments in interpolative smoothing. In future work,\nwe will integrate our language model into the Moses\nmachine translation pipeline to intrinsically measure\nits impact on translation qualities, which is of partic-\nular use for out-of-domain scenario.\nAcknowledgements\nThis research was supported by the National ICT\nAustralia (NICTA) and Australian Research Council\nFuture Fellowship (project number FT130101105).\nThis work was done when Ehsan Shareghi was an\nintern at IBM Research Australia.\nA. Inequalities\nWe prove that these inequalities hold in expectation\nby making the reasonable assumption that events in\nthe large training data regime, which suggests that more dis-\ncount parameters, e.g., up to D30, may be required for larger\ntraining corpus to reﬂect the fact that even an event with fre-\nquency of 30 might be considered rare in a corpus of nearly 1\nbillion words.\n947\nthe natural language follow the power law (Clauset\net al., 2009), p\n(\nC(u) = f\n)\n∝f−1− 1\nsm , where sm\nis the parameter of the distribution, and C(u) is the\nrandom variable denoting the frequency of the m-\ngrams pattern u. We now compute the expected\nnumber of unique patterns having a speciﬁc fre-\nquency E[ni[m]]. Corresponding to each m-grams\npattern u, let us deﬁne a random variable Xu which\nis 1 if the frequency of u is iand zero otherwise. It\nis not hard to see that ni[m] = ∑\nu Xu, and\nE\n[\nni[m]\n]\n= E\n[∑\nu\nXu\n]\n=\n∑\nu\nE[Xu] = σmE[Xu]\n= σm\n(\np\n(\nC(u) = i\n)\n×1 + p\n(\nC(u) ̸= i\n)\n×0\n)\n∝σmi−1− 1\nsm .\nWe can verify that\n(i+ 1)E\n[\nni+1[m]\n]\n<iE\n[\nni[m]\n]\n⇔\n(i+ 1)σm(i+ 1)−1− 1\nsm <iσmi−1− 1\nsm ⇔\ni\n1\nsm <(i+ 1)\n1\nsm .\nwhich completes the proof of the inequalities.\nB. Discount bounds proof sketch\nThe leave-one-out (leaving those m-grams which\noccurred only once) log-likelihood function of the\ninterpolative smoothing is lower bounded by back-\noff model’s (Ney et al., 1994), hence the estimated\ndiscounts for later can be considered as an approx-\nimation for the discounts of the former. Consider a\nbackoff model with absolute discounting parameter\nD, were P(wi|wi−1\ni−m+1) is deﬁned as:\n\n\n\nc(wi\ni−m+1)−D\nc(wi−1\ni−m+1) if c(wi\ni−m+1) >0\nDn1+(wi−1\ni−m+1 ·)\nc(wi−1\ni−m+1)\n¯P(wi|wi−1\ni−m+2) if c(wi\ni−m+1) = 0\nwhere n1+(wi−1\ni−m+1 ·) is the number of unique\nright contexts for the wi−1\ni−m+1 pattern. Assume that\nfor any choice of 0 < D <1 we can deﬁne ¯P such\nthat P(wi|wi−1\nim+1) sums to 1. For readability we\nuse the λ(wi−1\ni−m+1) =\nn1+(wi−1\ni−m+1 ·)\nc(wi−1\ni−m+1)−1 replacement.\nFollowing (Chen and Goodman, 1999), rewriting\nthe leave-one-out log-likelihood for KN (Ney et al.,\n1994) to include more discounts (in this proof up to\nD4), results in:\n∑\nwi\ni−m+1\nc(wi\ni−m+1)>4\nc(wi\ni−m+1) log c(wi\ni−m+1) −1 −D4\nc(wi−1\ni−m+1) −1 +\n4∑\nj=2\n(∑\nwi\ni−m+1\nc(wi\ni−m+1)=j\nc(wi\ni−m+1) log c(wi\ni−m+1) −1 −Dj−1\nc(wi−1\ni−m+1) −1\n)\n+\n∑\nwi\ni−m+1\nc(wi\ni−m+1)=1\n(\nc(wi\ni−m+1) log(\n4∑\nj=1\nnj[m]Dj)λ(wi−1\ni−m+1) ¯P\n)\nwhich can be simpliﬁed to,\n∑\nwi\ni−m+1\nc(wi\ni−m+1)>4\nc(wi\ni−m+1) log(c(wi\ni−m+1) −1 −D4)+\n4∑\nj=2\n(\njnj[m] log(j−1 −Dj−1)\n)\n+\nn1[m] log(\n4∑\nj=1\nnj[m]Dj) + const\nTo ﬁnd the optimal D1,D2,D3,D4 we set the par-\ntial derivatives to zero. For D3,\n∂\n∂D3\n= n1[m] n3[m]∑4\nj=1 nj[m]Dj\n−4n4[m]\n3 −D3\n= 0 ⇒\nn1[m]n3[m](3 −D3) = 4n4[m]\n4∑\nj=1\nnj[m]Dj ⇒\n3n1[m]n3[m] −D3n1[m]n3[m] −4n4[m]n1[m]D1 >0\n⇒3 −4n4[m]\nn3[m]D1 >D3 ■\nAnd after taking c(wi\ni−m+1) = 5 out of the summa-\ntion, for D4:\n∂\n∂D4\n=\n∑\nc(wi\ni−m+1)>5\n−c(wi\ni−m+1)\nc(wi\ni−m+1) −1 −D −5n5[m]\n4 −D4\n+ n1[m] n4[m]∑4\nj=1 nj[m]Dj\n= 0 ⇒−5n5[m]\n4 −D4\n+ n1[m] n4[m]∑4\nj=1 nj[m]Dj\n>0 ⇒n1[m]n4[m](4 −D4)\n>5n5[m]\n4∑\nj=1\nnj[m]Dj ⇒4 −5n5[m]\nn4[m]D1 >D4 ■\n948\nReferences\nStanley F. Chen and Joshua Goodman. 1999. An empir-\nical study of smoothing techniques for language mod-\neling. Computer Speech & Language, 13(4):359–393.\nAaron Clauset, Cosma Rohilla Shalizi, and Mark EJ\nNewman. 2009. Power-law distributions in empirical\ndata. SIAM review, 51(4):661–703.\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller lan-\nguage model queries. In Proceedings of the Workshop\non Statistical Machine Translation.\nKenneth Heaﬁeld. 2013. Efﬁcient Language Modeling\nAlgorithms with Applications to Statistical Machine\nTranslation. Ph.D. thesis, Carnegie Mellon University.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\nMachine Translation summit.\nHermann Ney, Ute Essen, and Reinhard Kneser. 1994.\nOn structuring probabilistic dependences in stochastic\nlanguage modelling. Computer Speech & Language ,\n8(1):1–38.\nEhsan Shareghi, Matthias Petri, Gholamreza Haffari, and\nTrevor Cohn. 2015. Compact, efﬁcient and unlimited\ncapacity: Language modeling with compressed sufﬁx\ntrees. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing.\nAndreas Stolcke, Jing Zheng, Wen Wang, and Victor\nAbrash. 2011. SRILM at sixteen: Update and\noutlook. In Proceedings of IEEE Automatic Speech\nRecognition and Understanding Workshop.\nAndreas Stolcke. 2002. SRILM–an extensible language\nmodeling toolkit. In Proceedings of the International\nConference of Spoken Language Processing.\nYee Whye Teh. 2006a. A Bayesian interpretation of in-\nterpolated Kneser-Ney. Technical report, NUS School\nof Computing.\nYee Whye Teh. 2006b. A hierarchical Bayesian language\nmodel based on Pitman-Yor processes. InProceedings\nof the Annual Meeting of the Association for Compu-\ntational Linguistics.\n949",
  "topic": "Smoothing",
  "concepts": [
    {
      "name": "Smoothing",
      "score": 0.8301069736480713
    },
    {
      "name": "Perplexity",
      "score": 0.7373491525650024
    },
    {
      "name": "Estimator",
      "score": 0.7020145654678345
    },
    {
      "name": "Computer science",
      "score": 0.7012423276901245
    },
    {
      "name": "Language model",
      "score": 0.55680912733078
    },
    {
      "name": "Vocabulary",
      "score": 0.42975491285324097
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.41388818621635437
    },
    {
      "name": "Machine learning",
      "score": 0.38714170455932617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37071293592453003
    },
    {
      "name": "Econometrics",
      "score": 0.32936814427375793
    },
    {
      "name": "Mathematics",
      "score": 0.22999310493469238
    },
    {
      "name": "Statistics",
      "score": 0.19660615921020508
    },
    {
      "name": "Linguistics",
      "score": 0.11400574445724487
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    }
  ]
}