{
  "title": "Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning",
  "url": "https://openalex.org/W3166143997",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2186161757",
      "name": "Wei, Colin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202173773",
      "name": "Xie, Sang Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163134908",
      "name": "Ma Tengyu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1511986666",
    "https://openalex.org/W3154560120",
    "https://openalex.org/W2995856824",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W3131755153",
    "https://openalex.org/W3169890186",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W1498269992",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3090048242",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3149173402",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2951585248",
    "https://openalex.org/W3046882683",
    "https://openalex.org/W3009571263",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W3120044914"
  ],
  "abstract": "Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.",
  "full_text": "Why Do Pretrained Language Models Help in Downstream\nTasks? An Analysis of Head and Prompt Tuning\nColin Wei Sang Michael Xie Tengyu Ma\nStanford University\nDepartment of Computer Science\n{colinwei,xie,tengyuma}@cs.stanford.edu\nApril 22, 2022\nAbstract\nPretrained language models have achieved state-of-the-art performance when adapted to a downstream\nNLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining\nand downstream tasks can be very diﬀerent. We propose an analysis framework that links the pretraining\nand downstream tasks with an underlying latent variable generative model of text — the downstream\nclassiﬁer must recover a function of the posterior distribution over the latent variables. We analyze head\ntuning (learning a classiﬁer on top of the frozen pretrained model) and prompt tuning in this setting.\nThe generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented\nwith a latent memory component, motivated by long-term dependencies in natural language. We show\nthat 1) under certain non-degeneracy conditions on the HMM, simple classiﬁcation heads can solve\nthe downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy\nconditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for\nthe vanilla HMM because task-relevant information is easier to recover from the long-term memory.\nExperiments on synthetically generated data from HMMs back our theoretical ﬁndings.\n1 Introduction\nNatural language processing (NLP) has been revolutionized by large-scale pretrained language models such\nas BERT [4] and GPT [27], which are adapted to a variety of downstream NLP tasks. Although a large body\nof empirical work seeks to understand the eﬀectiveness of pretrained models [7, 5, 12, 37, 36, 11, 29, 15],\ntheoretical understanding is scarce. Theoretically analyzing the relationship between the pretraining and\ndownstream tasks is challenging because pretraining and downstream settings can greatly diﬀer.\nThe key starting point for our analysis is to link the pretraining and downstream settings through an\nunderlying generative model of the data. We model the data distribution as a latent variable model and the\ndownstream task as a function of the latent variables. Assuming that pretraining on a large corpus allows us\nto learn the generative model, the conditional token probabilities predicted by the pretrained model carry\ninformation about the hidden variables. In downstream adaptation, we aim to recover this information to\nsolve the downstream task.\nThough full ﬁnetuning is the de facto empirical standard, analyzing it is challenging because it requires\ncharacterizing the weights of the pretrained model. In this paper, we focus onhead tuningand prompt tuning,\nwhich both freeze all pretrained parameters and allow us to treat the pretrained model as a black box. Head\ntuning [24] trains task-speciﬁc heads on top of the pretrained model outputs. Prompt tuning [33, 20, 9, 22]\n1\narXiv:2106.09226v2  [cs.LG]  20 Apr 2022\noptimizes a task-speciﬁc “prompt” that is concatenated to the model input. Studying prompt tuning\nis particularly interesting since it can match the performance of full ﬁnetuning with less computation\ntime [20, 9, 22].\nOur work contrasts with prior theoretical work [30], whichassumes that downstream labels are recoverable\nvia a linear head applied to the conditional token probabilities, and analyze how errors in pretraining or\nmodel misspeciﬁcation propagate downstream. We consider speciﬁc generative distributions for which we can\nprove these assumptions, showing that head and prompt tuning can recover the downstream labels.\nOur analysis considers two data-generating distributions with increasing realism. First, we consider data\ngenerated from a Hidden Markov Model (HMM), where the downstream task is to learn a linear classiﬁer on\nthe posterior distribution over the hidden states (Section 3). We prove that, under strong non-degeneracy\nconditions on token emission probabilities, a linear head applied to a pretrained modelGwhich outputs exact\nconditional token probabilities (Gipxq“ PrXi|x´is) can recover the downstream label (Theorem 3.3). Fur-\nthermore, we can prove better recovery guarantees with relaxed non-degeneracy assumptions (Assumption 3.1)\nby using continuous prompt tuning (Theorem 3.6), reﬂecting the strong empirical performance of prompt\ntuning [20, 9, 22]. Intuitively, prompt tuning conditions the latent variables so that nonessential information\nfor the downstream task can be ignored during the tuning phase, making task-essential information easier to\nrecover.\nSecond, we also strengthen our analysis by leveraging additional structure in the data. Motivated by long-\nrange dependences in natural language, we analyze HMM variants with additional latent “memory” variables\nthat can store long-term information more easily than vanilla HMMs (Section 4). Here, the downstream task\nis to learn a linear classiﬁer on the posterior distribution of the memory variables. We show that, under\nweaker non-degeneracy conditions than the ﬁrst setting, an attention-based classiﬁcation head can recover\nground-truth downstream labels from pretrained model outputs (Theorem 4.3). Intuitively, our recovery\nguarantees improve because the classiﬁcation head can focus on the persistent, task-essential information in\nthe memory while ignoring other transient and nonessential aspects of the latent variables. As with the vanilla\nHMM, we analyze prompt tuning for relaxing the non-degeneracy conditions even further (Theorem 4.6).\nIn summary, we relate the pretraining and downstream tasks by assuming that the downstream task is to\nlearn a classiﬁer on the posterior distributions of the latent variables deﬁned by an underlying generative\nmodel of text. Our theoretical contributions are: 1) in this setting we analyze an HMM generative model\nshow that simple classiﬁcation heads can recover the true downstream labels under certain non-degeneracy\nassumptions, 2) we prove that soft prompt tuning can relax the non-degeneracy assumptions needed for\ndownstream recovery making it easier to extract task-speciﬁc information, and 3) our recovery guarantees are\nstronger for memory-augmented HMMs in comparison to the vanilla HMM when tuning an attention-based\nclassﬁcation head.\nWe empirically evaluate our theoretical results with language models pretrained on synthetically generated data\nfrom HMMs. We ﬁnd that prompt tuning obtains good downstream performance when our non-degeneracy\nconditions are relaxed, whereas head tuning performs poorly. Furthermore, we show that head tuning obtains\nbetter downstream performance when data is generated from a memory-augmented HMM, compared to a\nvanilla HMM, as is predicted by our theory.1\n1.1 Related works\nThe black box nature of BERT and related models has inspired a variety of empirical works which seek to\nunderstand them. Probing papers study whether a pretrained model computes various types of structured\ninformation (e.g., syntactic [37, 11]) by evaluating the performance of simple classiﬁers, or probes, on the\nrepresentations [7, 12, 36, 29, 15]. Other papers ablate various aspects of pretraining, such as changing the\nmasking scheme [14, 21, 42] or permuting the word order [34].\n1Code is available athttps://github.com/sangmichaelxie/pretraining_analysis.\n2\nIn comparison, theoretical analysis of pretrained language models is limited. Besides [30], which we discussed\nin Section 1, Zhang and Hashimoto [42] analyze using a linear classiﬁer to approximately recover the latent\nvariable in a Gaussian graphical model with sparse dependencies between observed variables. However, their\nanalysis and setting are focused towards understanding syntactic dependencies between tokens, whereas we\ndirectly model and analyze downstream performance.\nPrompt-based tuning [33, 20, 9, 22, 13, 6, 43, 2, 25], which has improved empirical downstream performance for\nlightweight adaptation methods beyond head tuning to approach full ﬁnetuning, is an important focus of our\ntheoretical analysis. Shin et al.[33] employ task-speciﬁc prompts that are optimized over the discrete token\nspace. Schick and Schütze[31, 32] reformulate natural language tasks as cloze-style phrases to enable few-shot\nlearning. Subsequent methods [20, 9, 22] optimize “soft” prompts, or continuous embedding vectors. Lester\net al.[20] employ soft prompts on pretrained large-scale T5 [28] models and show that as the model size\nincreases, prompt tuning performance can eventually match ﬁnetuning. Hambardzumyan et al.[9] applies a\nvariant of soft prompt tuning to MLM models. Li and Liang[22] propose preﬁx tuning, which prepends a\ntrainable preﬁx embedding sequence to all layers of the transformer.\nMore broadly, Lee et al.[19] analyze reconstruction-based self-supervised learning methods in a general setting\nand show that under certain conditional independence assumptions, predicting one observed variable from\nanother allows recovery of the latent with a linear head. Other theoretical works analyzing self-supervised\nor constrastive learning include [1, 10, 38, 40, 39, 23], but they are not directly relevant for our particular\nsetting.\n2 Formulations and notations\nWe analyze models pretrained on masked language modeling (MLM) objectives. LetX denote a ﬁnite\nvocabulary of input tokens,X˚ the set of variable-length sequences of tokens, andX “pX1,...,X TqP X˚ a\nrandom sequence ofT tokens. Let ∆|X| denote the space of probability distributions over tokens.\nPretraining and downstream task.Let Gpxq“p G1pxq,G2pxq,... qdenote the masked language model\nwhich predicts a probability vector for each timestep in the inputx. Our theoretical abstraction is that\nGi perfectly computes the distribution ofXi, the i-th token, conditioned on all other tokens:Gipxq “\nPrXi|X´i “x´is. Here PrXi|X´i “x´is P∆|X| is a probability vector. In particular,Gipxq does not\ndepend onxi. The downstream task involves labeled examplespx,F‹pxqqP X˚ˆY, whereF‹ : X˚ ÑY\nprovides ground-truth downstream labels andY is a discrete set of labels for classiﬁcation.\nHead and prompt tuning. Head tuning trains a classiﬁcation headf on top of ﬁxed model outputs,\nresulting in the classiﬁerFpxq“ 1 pfpGpxqqě 0q. We expectf to be a simple function such as a linear or\none layer attention model. We also analyze variants wheref also takes the tokensx or embeddings ofx as\ninput, which provides additional information. Soft prompt tuning requires viewing the pretrained modelG\nas a function of the token embeddings; we refer to this model byG. Letting epxq“ epx1q,...,e pxtqdenote\nthe token embeddings, we haveGpepxqq“ Gpxq. Soft prompt tuning concatenates a trainable promptu so\nthat the model output isGppu,epxqq. We consider simultaneously training the prompt parameteru and a\nclassiﬁcation head to ﬁt the downstream task.\nNotations. Let ∆d denote the space ofd-dimensional probability vectors. We work with discrete random\nvariables V taking values in a ﬁnite setV. We use PrVs P∆|V| to denote the distribution of V and\nPrU|V “ vs PR|U| the conditional distribution of U given V “ v. PrpV “ vq P r0,1s will denote\nthe probability that V takes values v. We also let PrU “ u|Vs PR|V| denote the vector with entries\nPrpU “u|V “vq. PrU|VsP R|U|ˆ|V| will describe the matrix with entriesPrU|Vsu,v “PrpU “u|V “vq.\nFor a sequencev “pv1,...,v tq, we use the notationvi:j for iďj to denotepvi,...,v jq, andv´i to denote\npv1:i´1,vi`1:tq. We let1 denote the indicator function. For setV, we letV˚ “V1 YV2 Y¨¨¨ denote variable-\nlength sequences of elements ofV. Let ddenote elementwise product. Let1d,0d denote thed-dimensional\n3\nFigure 1: Left: Illustration of HMM graphical model.Right: Overview of the formulation and analysis\nsetting for prompt (and head) tuning. To abstractify soft prompt tuning, we note that every token has a\nnatural embedding, the corresponding row of the emission probability matrix. We view prompt tuning as\nadding a fake tokenrz to the vocabulary, assigning it a rowuin the emission matrix, and prepending it to the\ninput embedding sequence. More details are provided in Section 3.1.\nall-1’s and all-0’s vector. We omit the subscript if the dimension is clear from context. For two vectors\na,b PRd, we leta{b denote their element-wise division. We usesupppaqto denote the set of indices where\nvector a is non-zero.\n3 Analysis for Hidden Markov Models\nDeﬁning a relation between pretraining and downstream tasks is the foremost challenge for analysis. We\npropose to link the two via latent variable generative assumptions on the input distribution. We model the\ndownstream task as a function of the posterior distribution of the latent variables. Towards a ﬁrst result,\nthis section studies the case where inputs are generated by HMMs (see Figure 1 (left)), which have been\nwell-studied in the context of language and speech processing (see e.g. [26, 18, 3]).\nData distribution.Let H denote the hidden state space of the HMM. We useH “pH0,H1,...,H TqP H˚\nto denote the sequence of hidden states. For all timestepsią0, the transition probabilities are time-invariant,\ni.e. PrHi|Hi´1s “A for A P R|H|ˆ|H|. For each timestepi ě 1, tokens Xi are emitted following some\ntime-invariant probability:PrXi|His“ W for W PR|X|ˆ|H|. The joint probability ofX,H is\nPrpX,H “x,h |T “tq“ PrpH0 “h0q\ntź\ni“1\nPrpHi “hi|Hi´1 “hi´1qPrpXi “xi|Hi “hiq.\nDownstream tasks.We assume thatH0 has the meaningful information for the downstream task, which is\na binary classiﬁcation task where the ground-truth labelingF‹ is assumed to be a linear classiﬁer on the\nposterior PrH0 |X1:T “xs:\nF‹pxq“ 1 pµJPrH0 |X1:T “xsě 0q (3.1)\nfor µPR|H|. Our results are easily extended to the multiclass setting. We consider tuning a linear head for the\ndownstream classiﬁer, which formally computes1 pbJG1pxqě 0qfor bPR|X|. The following non-degeneracy\ncondition is crucial for our recovery result in this setting.\nAssumption 3.1(Non-degeneracy, vanilla HMM). The token emission probability matrixW has linearly\nindependent columns.\n4\nWe also require the following regularity conditions onH0 and the state transitions.\nAssumption 3.2(Regularity). The Markov chainH0,H1,... is ergodic, andPrH0shas full support.\nWe show that ifW has linearly independent columns, a linear head ﬁts downstream labels.\nTheorem 3.3. Assume that non-degeneracy (Assumption 3.1) and regularity (Assumption 3.2) hold. Then\nany downstream taskF‹pxq of the form(3.1) can be computed by a linear head onG applied to a shifted\nsequence. That is, there exists linear head weightsbPR|X| such that for allxPsupppPrXsq,\nF‹pxq“ 1 pbJG1px1qě 0q\nwhere x1 “p∅,x1:tqis the concatenation of a special token∅ with x.2\nThe key for the proof is to leverage the following general statement about random variablesU,V,Z such that\nU KV |Z, which decomposes the expression forPrU|Vs.\nProposition 3.4. Let U,V,Z be random variables such that U K V |Z. Then for any v, PrU|V “\nvs “PrU|Zs¨ PrZ|V “ vs. Thus, if PrU|Zs has a left inverse pPrU|Zsq:, then PrZ|V “ vs “\npPrU|Zsq:PrU|V “vs.\nBy the conditional independence structure of the HMM, Proposition 3.4 immediately implies\nG1px1q“ WPrH1|X2:T`1 “xs ùñPrH1|X2:T`1 “xs“ W:G1px1q\nwhere W:is the left inverse forW, guaranteed to exist by Assumption 3.1. This lets us recoverPrH1|X2:T`1 “\nxsbyapplyingalinearfunctionto G1px1q. Additionallinearfunctionswillbesuﬃcienttoobtain µJPrH0|X1:T “\nxsfrom PrH1|X2:T`1 “xs. We provide the full proof in Section A.\nProposition 3.4 is reminiscent of the arguments of [19], which leverages the independence structure in the\nsame way. Subsequent sections will require more complicated analyses and recovery procedures.\nA drawback of Theorem 3.3 is that it relies heavily on assumingW has full column rank, which implies the\nnecessary condition that|H|ď| X|. Without this assumption, it is unclear how to recoverPrH0 |X1:T “xs\nfrom Gpxq alone. However, in realistic settings we would expect|H| ą |X|, as increasing the size of the\nhidden state space improves language modeling capabilities of HMMs [3].\n3.1 Relaxed non-degeneracy assumptions via prompt tuning\nIn this section, we study applying soft, or continuous, prompt tuning [20, 9] to the setting above. We\nshow that by using soft prompt tuning, we can recoverF‹ using a linear head onG for HMMs where the\nnon-degeneracy assumptions onW are relaxed. Our analysis provides insight into the empirical successes of\nprompt-tuning: intuitively, prompt tuning enables better recovery of the downstream task by conditioning\nthe output ofG to only contain task-speciﬁc information.\nSoft prompt tuning trains task-speciﬁc embedding vectors, but analyzing how the model processes embedding\nvectors is challenging because it requires opening up the black box of the pretrained model. Thus, we require\nadditional abstractions about how the pretrained model processes the embedding vectors. We will extend\nthe mask language modelG to a modelG that maps a sequence of embeddingse1,...,e t to conditional\nprobabilities G1pxq,...,G tpxq as follows. We observe that each tokenz in the vocabulary X naturally\ncorresponds to a|H|-dimensional vector: thez-th row of the emission probability matrixW, or equivalently,\nPrXi “z|His. We denote this embedding byepzqand call the family of embeddingstepzq: zPXuproper\nembeddings. A fundamental property of HMMs is that the conditional probabilityPrXi|X´i “x´isonly\ndepends on x1,...,x t through their embeddingsepxq“p epx1q,...,e pxtqq. In other words, there exists a\nfunction Gi such that\nGipx1,...,x tq“ Gipepx1q,...,e pxtqq\n2We note thatG1px1q does not depend onx1\n1 and thereforex1\n1 can be any token.\n5\nIn particular, we letGi compute the standard message passing algorithm [16] that computes the conditional\nprobability of HMMs. This ensures thatGi is well deﬁned on all sequences of nonnegative vectors inr0,1s|H|,\nbeyond sequences of proper embeddings.We assume that pretraining produces thisGi, which we treat as a\nblackbox for prompt tuning.\nIn particular, for prompt tuning we can consider the case where we pass an arbitrary nonnegative vector\nuPr0,1s|H| to G in the ﬁrst argument and proper embeddings at positionsią1. We can interpretu as the\nembedding of a fake tokenrz. Concretely, consider adding a new tokenrzto the vocabularyX, and changing the\nemission probability at position 1 to satisfyPrX1 “rz|H1s“ uand for allz‰rz, PrX1 “z|H1s9p1´uqdepzq.\nThen Gipu,epx1q,...,e pxtqqprecisely computes the conditional probabilityPrXi|X´i “prz,x1,...,x tq´is\nunder the modiﬁed HMM. We refer the readers to Section B for the formal deﬁnition ofGi and formal proofs\nof the interpretation above.\nWe consider a downstream training algorithm which trains the prompt tuning parameteru described above\nand a linear classiﬁcation head. Lettingu denote the trainable prompt parameter andbPR|X| the trainable\nlinear head weights, the model uses the embedding sequence\npepxqﬁ pu,ep∅q,epx1q,...,e pxtqq (3.2)\nand outputs the predictionFpxq“ 1 pbJG2ppepxqqě 0q. We can provide recovery guarantees for this model if\nthe ground-truth classiﬁer weightsµ (deﬁned in(3.1)) and columns of the HMM transition matrixA satisfy\nthe following relaxation of the requirement in Theorem 3.3 thatW is nondegenerate.\nAssumption 3.5(Relaxed non-degeneracy condition). There exists a set of essential hidden statesH‹ ĎH,\nso that the columns ofW corresponding toH‹, tW:,huhPH‹ , are linearly independent. Furthermore,H‹\ncovers all meaningful information for the downstream tasks:supppµqĎ H‹.\nIn addition, a last technical requirement onH‹ is as follows: there exists a setB Ď H such that H‹ “\nYhPBsupppA:,hq. In other words,H‹ must be the set of all states reachable by starting from some state inB\nand transitioning one step in the hidden Markov chain.\nCompared to Assumption 3.1, which required thatall columns ofW are linearly independent, Assumption 3.5\nonly requires linear independence on a subsetH‹ of essential states. In the setting where|H|ą| X|, the\ncondition for Theorem 3.3 can never hold. On the other hand, Assumption 3.5 could still hold, for example, if\n|supppµq|ă| X|and the set of columns ofW corresponding to hidden states insupppµqis linearly independent.\nThe last technical requirement in Assumption 3.5 is also required, which could be satisﬁed if columns ofAare\nsparse. The following theorem shows that when Assumption 3.5 holds, we can recoverF‹ using soft prompt\ntuning with a linear head.\nTheorem 3.6. In the above setting, assume that Assumptions 3.2 and 3.5 hold. ThenF‹ can be computed\nusing soft prompt tuning with a linear head onG. Concretely, there is a continuous prompt parameteruPR|H|\nand weight vectorbPR|X|, such that for allxPsupppPrXsq,\nF‹pxq“ 1 pbJG2ppepxqqě 0q\nwhere pe prependsu to the input embedding sequence, as deﬁned in(3.2).\nTheorem 3.6 provides a stronger recovery result than Theorem 3.3, which only used a linear head. This is\nalso reﬂected in our synthetic experiments (Section 5), and prior work which shows that variants of prompt\ntuning can perform much better than only training the last few layers of the model [22]. Our theory suggests\nthat prompt tuning could help by conditioning the hidden variables to remove nonessential information for\nthe task from the output ofG. This makes task-essential information easier to recover.\nThe key proof intuition is that although recoveringPrH0 |X1:T “ xs is impossible without strong non-\ndegeneracy conditions (Assumption 3.1), we can aim to recoverPrH0 |X1:T “xson the subset of essential\nstates H‹ deﬁned in Assumption 3.5, which suﬃces for computingµJPrH0 |X1:T “xs, sinceH‹ Ěsupppµq.\n6\nFigure 2: Left: Memory-augmented HMM with a single memory cell. The memoryM and hidden stateHi\ndetermine the emission probabilities for each stateXi. Right: Memory-augmented HMM with multiple\nmemories M1,...,M N. The hidden stateHi consists of a cell indexJi and syntax stateSi. To sampleXi,\nwe ﬁrst look up theJi-th memory cellMJi. The token emission probability is then determined by the tuple\npMJi,Ji,Siq.\nTo recoverPrH0 |X1:T “xson H‹, we observe in Lemma B.2 that prepending the promptu is equivalent to\nintroducing a modiﬁed random sequencepX and fake tokenrz which inﬂuences the posterior ofH2 as follows:\nG2ppepxqq“ rxWDpPrH2 | pX1 “rzsd PrH0 |X1:T “xsq (3.3)\nfor invertible diagonal matrixD and positive scalarrx. We choose u such that the vectorPrH2 | pX1 “\nrzsdPrH0 |X1:T “xsis supported only onH‹. Because corresponding columns ofW are linearly independent\nby Assumption 3.5, we can then recoverPrpH0 “h|X1:T “xqfor hPH‹ by applying a linear function to\nG2ppepxqq. This suﬃces for computingµJPrH0 |X1:T “xs. More details are in Section B.\n4 Analysis for memory-augmented Hidden Markov Models\nWe study a memory-augmented HMM which explicitly disentangles the evolution of hidden states from a\npersistent “memory” variable. Inspired by natural sentences, this model is intended to better capture the\ndistinction between syntax, which constantly evolves, and semantics, which changes less. This additional\nstructure in the generative model allows us to strengthen our results by relaxing the non-degeneracy conditions\non W, the token emission probabilities. Thus, both head and prompt tuning are more powerful in this setting\ncompared to Section 3 and can recover the downstream label with weaker non-degeneracy assumptions onW.\nIn Section 4.2, we show that soft prompt tuning also provides an advantage over head tuning alone.\nData distribution. The memory-augmented HMM, depicted in Figure 2, can be viewed as a generative\nvariant of memory networks [41, 35] and is closely related to Hidden Topic Markov Models [8]. There are two\nsets of latent variables in the memory-augmented HMM: a Markov chain on hidden statesH0,H1,... , meant\nto model the evolution of syntax, and a persistent “memory”M “pM1,...,M Nqwith N total cells, where\neach Mi takes values in a ﬁnite setM. The full joint probability is as follows:\nPrpX,H,M “x,h,m |T “tq“\nPrpM “mqPrpH0 “h0q\ntź\ni“1\nPrpHi “hi|Hi´1 “hi´1qPrpXi “xi|M “m,Hi “hiq\nThe hidden state is modiﬁed to explicitly consist of a disentangled cell indexJ P rNs and syntax state\nS PS, such thatHi “pJi,Siqand H “rNsˆ S. To sample the token at timestepi given the hidden state\n7\nHi “pJi,Siq, we ﬁrst useJi to index the memoryM, obtaining the random variableMJi. Xi is then sampled\naccording to some time-invariant probability depending onMJi,Ji,Si:\nPrXi|M “m,Hi “pj,sqs“ PrXi|MJi “mj,Hi “pj,sqs“ W:,pmj,j,sq\nHere W PR|X|ˆ|M||H| stores the emission probabilities for each choice of memory cell value and hidden state.\nNote that in particular, the conditional probabilities forXi only depend on a single memory cell for each\ntimestep. We also note that memory-augmented HMMs can be viewed as vanilla HMMs with structured\ntransitions becausepH0,Mq,pH1,Mq,... can be viewed as a Markov chain where the memory component\ndoes not change.\nExample 4.1(Generating natural sentence with memory-augmented HMM). We consider how this model\nmay generate the sentence “The cow in the pasture rolled on the grass’ happily.”M1 could store the subject\n(“cow”),M2 the location (“pasture”),M3 the sentiment (“happily”), andSi could determine part-of-speech.\nFor timesteps where “cow” and “rolled” are emittedJi “1 because we emit information related to the sentence\nsubject. Timesteps for “pasture” and “grass” would haveJi “2.\nDownstream tasks. We consider downstream tasks where ground-truth labels are obtained via a linear\nclassiﬁer on the posterior distribution of a particular memory cellj‹ PrNs: F‹pxq“ 1 pµJPrMj‹|X1:T “\nxsě 0q, whereµPR|M|. Intuitively, this formulation models downstream tasks which depend on a particular\naspect of the semantics but not on syntax (e.g. in the setting of Example 4.1, ifj‹ “3, the task is sentiment\nanalysis).\n4.1 Tuning attention head for recovering ground-truth downstream labels\nTo recover the downstream labeling, we require an attention-based classiﬁcation head, which is a function\nof both the input embeddings and outputs ofG. Formally, letq PR|H|`1 denote a query parameter and\nβ1,...,β t P R|H|`1 denote trainable position embeddings. Given pretrained model outputs Gipxq and\ntrainable token embeddingsepxiq, the attention headAttnp¨qapplies key and value functionsK,V to compute\nthe output as follows:\nI ﬁ arg max\ni\ntqJpKpGipxqq` βiqu (4.1)\nAttnppGipxq,epxiqqt\ni“1qﬁ 1\n|I|\nÿ\niPI\nVpGipxq,epxiqq (4.2)\nwhere arg max refers to the set of indices achieving the maximum in(4.1). We note that standard attention\nheads in practice rely on the softmax function, but the expression based onarg max above captures the\nlimiting behavior as}q}2 Ñ 8. We consider linear key functions given byKpGipxqq “ΘpKqGipxq. The\nvalue functionV : R|X|ˆR|M||H| ÑR uses parametersΘpVq PR|M||H|ˆ|X| and bPR|M||H| and computes\nVpGipxq,epxiqq“ bJppΘpVqGipxqqd epxiqq.\nBecause our generative model disentanglesH and M, we can relax the non-degeneracy assumption on\nthe token emission probabilitiesW, compared to Theorem 3.3. The relaxed assumption only requires the\ncolumns tW:,pm,hqumPM,hPH‹ to be linearly independent in a subsetH‹ of “recoverable” hidden states, whereas\nAssumption 3.1 required all columns to be linearly independent.\nAssumption 4.2(Existence of “recoverable” hidden states). There exists a set of recoverable hidden states\nH‹ “tj‹uˆ S‹, such that the collection of token emission probabilities fromM ˆH‹, tW:,pm,hqumPM,hPH‹,\nis a linearly independent set of vectors.\nFurthermore, the span of these vectors must be disjoint from the span of token emission probabilities from\nM ˆpHzH‹q: spanptW:,pm,hqumPM,hPH‹qX spanptW:,pm,h1qumPM,hPHzH‹q“t 0|X|u.\nNote that the non-degeneracy condition of Theorem 3.3 would requiretW:,pm,hqumPM,hPH to be linearly\nindependent, whereas Assumption 4.2 only requires linear independence forhPH‹. The second condition\nstates thatH‹ and HzH‹ are distinguishable by the token emission probabilities.\n8\nWe explain Assumption 4.2 in the setting of Example 4.1. For natural language, there might be choices of\nh“pji,siqfor which the settW:,pm,hqumPM of token emission probabilities is fundamentally not very diverse,\nand therefore not linearly independent. For example, if the syntaxsi indicates “article”, i.e. words such as\n“a”, “an”, and “the”, the token emission probabilities would carry little information aboutMji because the\nchoice of article does not depend much on semantics, so columns corresponding tosi ““article”would not be\nlinearly independent, violating Assumption 3.1. However, Assumption 4.2 allows us to avoid this issue by\nplacing suchh in HzH‹, a set of hidden states which we can ignore, and only including hidden states which\ncarry a lot of information aboutM in H‹. In Example 4.1, whenJi “2 (location), Si ““noun”, the position\ni should convey a lot about the location (in this case, “pasture”), so it is more reasonable to assume that\ntW:,m,humPM is linearly independent for this hidden state.\nThus, our aim is to focus on recovering information for the downstream task from positionsi where Hi PH‹.\nFormally, we deﬁne the following set of input sequences containing positionsiwhere the posterior ofHi given\nx´i concentrates onH‹:\nR ﬁ tpx1,...,x tqP supppPrXsq: Di with supppPrHi|X´i “x´isqĎ H‹u (4.3)\nThe following theorem shows that under Assumption 4.2, we can recoverF‹ using the attention head described\nabove, ifxPR is nonempty. Note thatR is nonempty if the posterior ofHi concentrates onH‹ for some\ni. For natural language, it is realistic to assume this can occur because syntactic aspects of a sentence are\ntypically low-entropy when the full sentence is observed.\nTheorem 4.3. Assume that non-degeneracy (Assumption 4.2) and regularity (Assumption 3.2) hold. Deﬁne\nR as in(4.3). Then there exist an attention head onGpxqand token embeddingsepxiqsuch that the following\nholds for anyxPR:\nF‹pxq“ 1 pAttnppGipxq,epxiqqt\ni“1qě 0q\nwhere the functionAttn is in the form described in(4.2).\nThe idea is to use the attention mechanism to attend to positionsi where supppPrHi|X´i “x´isqĎ H‹.\nThe intuition of Assumption 4.2 is that such positions are more informative for recovering the latent\nposteriors; indeed, from the outputsGipxqat suchi, the value function in the attention will be able to recover\nPrMj‹ |X1:T “xs. A full proof is provided in Section C.1.\n4.2 Guarantees for prompt-tuning\nThough the generative modeling assumptions in this section already allowed us to relax the non-degeneracy\nassumptions, applying soft prompt tuning allows us to relax them even further. For simplicity, we consider\nthe setting where there is a single memory cell, soM PM, and the downstream task is a linear classiﬁer on\nthe posterior of the memory:F‹pxq“ 1 pµJPrM|X1:T “xsě 0q. This simpliﬁed setting also doesn’t require\nthe explicit disentanglement betweenJi and Si in Hi. We analyze continuous prompt-tuning in a setting\nwhere the pretrained modelG follows the same abstraction as in Section 3.1. We modify the model to take\n|M||H|-dimensional vectors, so the proper embedding for tokenz is given byepzq“ PrXi “z|M,His“ WJ\nz,:.\nIn Section C.3, we describe the formal construction and interpretation ofG in the more general setting with\nmore memories.\nLetting uPR|M||H| denote the trainable prompt parameter, we deﬁne the input embeddings\npepxqﬁ pu,epx1q,...,e pxtqq (4.4)\nThe downstream model applies an attention head to the output ofG: Fpxq“ 1 pAttnppGippepxqq,peipxqqt`1\ni“1qě\n0q, whereAttn is deﬁned in(4.2). An additional stationarity assumption onPrH0swill simplify the recovery\nprocedure (though it can be removed).\n9\nAssumption 4.4 (Stationarity). Assumption 3.2 holds on the Markov chainH0,H1,... . Furthermore,\nPrH0sis the stationary distribution:PrH0s“ APrH0s, whereA is the transition matrix.\nAs before, we assume sparsity ofµ and some non-degeneracy ofW, though the assumption is more relaxed\nand easier to state compared to the vanilla HMM setting.\nAssumption 4.5 (Relaxed version of Assumption 4.2). Let M‹ ﬁ supppµq denote the set of non-zero\ncoordinates inµ. There exists a set of recoverable hidden statesH‹, such that the collection of token emission\nprobabilities fromM‹ˆH‹, tW:,pm,hqumPM‹,hPH‹, is linearly independent.\nFurthermore, the span of these vectors must be disjoint from the span of token emission probabilities from\nM‹ˆpHzH‹q: spanptW:,pm,hqumPM‹,hPH‹qX spanptW:,pm,h1qumPM‹,hPHzH‹q“t 0|X|u.\nWe note that Assumption 4.5, and Assumption C.5 for multiple memories, are relaxations of Assumption 4.2,\nas they only consider memory values insupppµq, whereas Assumption 4.2 considers allmPM. An additional\nadvantage of the memory-augmented HMM is that Assumption 4.2 is simpler than Assumption 3.1 and does\nnot require any conditions on the transition matrixA. We now state our result for recoveringF‹ with soft\nprompt tuning and an attention head.\nTheorem 4.6. In the setting above, suppose that non-degeneracy Assumption 4.5 and stationarity Assump-\ntion 4.4 hold. Then there exists a promptu and attention head onGppepxqqand the token embeddings which\ncan compute the ground-truthF‹pxqfor anyxPR, deﬁned in(4.3):\nF‹pxq“ 1 pAttnppGippepxqq,peipxqqt`1\ni“1qě 0q\nwhere pe is the embedding in(4.4) and Attn is deﬁned in(4.2).\nThe intuition for this proof is similar to Theorem 3.6: the soft prompt conditions the memoryM to concentrate\non supppµq. As a result, all irrelevant information to the task is removed fromGippepxqq, making it easier to\nrecover the task-speciﬁc information about the posterior ofM. A more general theorem statement for the\nmultiple memories setting, and the full proof, is provided in Section C.3\n5 Simulations\nWe empirically evaluate our theoretical results by pretraining a BERT-like masked language model (MLM) [4]\non synthetic data generated by an HMM. Our goal is to verify key implications of our theory in a more\nrealistic setting where some assumptions, such as thatG outputs exact conditional probabilities, may not\nhold. First, we compare head and prompt tuning and show that prompt tuning improves downstream\nperformance, especially when the recovery problem is degenerate. Second, we compare the eﬀect of changing\nthe data distribution from vanilla HMMs to memory-augmented HMMs on head tuning with an attention\nlayer. We ﬁnd that the downstream performance improves when the data has a long-term memory component.\nThese observations support our theory. Our code is available at the following URL:https://github.com/\nsangmichaelxie/pretraining_analysis.\nPretraining data and downstream task.We generate pretraining data from an HMM with randomly\ngenerated transition matrix, emission probabilities, and start distributions. In all experiments, the HMMs\nhave 10 vocabulary symbols, while the hidden state size varies. The downstream task uses input sequences\nX1:T of length 129, where the ﬁrst tokenX1 “[MASK]. We consider binary classifcation where labels are\ngenerated using linear functions of the analytically-computed posteriors in the HMMs. In all experiments,\nthe ground truth linear weight is sparse with 6 nonzero entries at uniformly random locations with Gaussian\nvalues. More details are in Appendix D.\nHead vs. prompt tuning. We compare head and prompt tuning as the hidden state size of the data-\ngenerating HMM varies. The downstream label is generated by computingµJPrH1 |X´1 “x´1s, whereµ is\na random ground-truth linear weight. Head tuning learns a linear head on top of the softmax probabilities\n10\n8 10 15 20 25 30\n# of hidden states\n60\n70\n80\n90\n100Test Accuracy\nPrompt Tuning\nHead Tuning\nVocab size\n10 15 20 25\n# of hidden states\n60\n70\n80\n90\n100Test Accuracy\nVanilla HMM + linear\nMemory HMM + attention\nVocab size\nFigure 3: Left: Head vs. prompt tuning with a linear head on synthetically-generated HMM data, with\nvarying hidden state sizes. Prompt tuning improves downstream accuracy especially when the problem is\ndegenerate (|H|ą| X|). Right: Downstream accuracy of head tuning on data from vanilla HMM vs. memory-\naugmented HMM, across varying values of|M||H|. Long-term dependencies in the memory-augmented HMM\ndata improve downstream recovery when using attention. Experiments average over 20 trials (left) and 5\ntrials (right) of pretraining and ﬁnetuning, with 95% intervals shown.\npredicted by the pretrained model for ﬁlling in the ﬁrst[MASK] token. Prompt tuning uses the same setup\nbut also optimizes a length 20 continuous embedding and preprends it to the input sequence.\nFigure 3 (left) shows that prompt tuning improves downstream performance substantially across all hidden\nstate sizes ({4,8,10,15,25,30}). Prompt tuning improves especially when the hidden state size increases beyond\nthe vocabulary size, which makes the recovery problem degenerate. Thus, as suggested by Theorem 3.6,\nprompt tuning helps relax the non-degeneracy conditions.\nMemory-augmented HMMs. We investigate the eﬀect of augmenting the data-generating HMM with\na long-term memory. We consider the single memory case with|H|“ 4 and varying memory sizes|M|P\nt2,3,5,7u. The downstream label is generated by computingµJPrM|X´1 “x´1s, where µ denotes the\nground-truth weights. Viewing the memory HMM as a HMM where the component onM never changes, we\ncan compare against the vanilla HMMs from the previous setting. For the memory-augmented HMM, we use\nhead tuning with a single-cell attention layer on the entire sequence of softmax probability outputs. For the\nvanilla HMM in the comparison, we use a linear head on the output at the ﬁrst position, as an attention\nhead would perform worse since the downstream task depends only onH1 and not any other timesteps.\nFigure 3 (right) veriﬁes that head tuning recovers the downstream task better when there is more structure in\nthe data, as predicted by Theorem 4.3. Head tuning achieves near 100% downstream accuracy on all hidden\nstate sizes.\n6 Conclusion\nWe analyze how pretraining on generic language modeling tasks can improve performance on diverse\ndownstream tasks. In our analysis framework, the downstream task requires predicting properties of the\nposterior distribution over latent variables in an underlying generative model. When the generative model is a\nstandard HMM, downstream recovery is possible with a simple classiﬁcation head under strong non-degeneracy\nassumptions. We also show that we can relax the non-degeneracy conditions by changing the generative\nmodel to a memory-augmented HMM or using prompt tuning. The generative distributions studied here are\nmeant to provide a ﬁrst-cut result – we also conjecture similar theorems to hold for other generative models,\nwhich we leave as an interesting direction for future work.\n11\nAnother direction for future work is to analyze ﬁnetuning. Existing work analyzes ﬁnetuning for linear neural\nnetworks and obtains empirically useful insights [17], but analyzing neural networks with nonlinear activations\nis very challenging. Our analysis of head and prompt tuning treats the model as a black box. Analyzing\nﬁnetuning requires understanding how to open up the black box, which is a major open question.\nAcknowledgements\nWe thank Percy Liang, Tianyi Zhang, and Nelson Liu for helpful discussions. CW was supported by a NSF\nGraduate Research Fellowship. SMX was supported by a NDSEG Fellowship. TM acknowledges support of\nGoogle Faculty Award, NSF IIS 2045685, and JD.com.\nReferences\n[1] Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A\ntheoretical analysis of contrastive unsupervised representation learning. InInternational Conference on\nMachine Learning, 2019.\n[2] Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. Adaprompt: Adaptive prompt-based ﬁnetuning for relation extraction.arXiv preprint\narXiv:2104.07650, 2021.\n[3] Justin T Chiu and Alexander M Rush. Scaling hidden markov language models. arXiv preprint\narXiv:2011.04640, 2020.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018.\n[5] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry\nof bert, elmo, and gpt-2 embeddings.arXiv preprint arXiv:1909.00512, 2019.\n[6] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.\narXiv preprint arXiv:2012.15723, 2020.\n[7] Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the\nhood: Using diagnostic classiﬁers to investigate and improve how language models track agreement\ninformation. arXiv preprint arXiv:1808.08079, 2018.\n[8] Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. Hidden topic markov models. InArtiﬁcial intelligence\nand statistics, pages 163–170. PMLR, 2007.\n[9] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. Warp: Word-level adversarial repro-\ngramming. arXiv preprint arXiv:2101.00121, 2021.\n[10] Jeﬀ Z. HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised\ndeep learning with spectral contrastive loss, 2021.\n[11] John Hewitt and Christopher D Manning. A structural probe for ﬁnding syntax in word representations. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, 2019.\n[12] Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. What does bert learn about the structure of\nlanguage? In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019.\n[13] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models\nknow? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.\n12\n[14] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert:\nImproving pre-training by representing and predicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77, 2020.\n[15] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee. Are pre-trained language models aware of\nphrases? simple but strong baselines for grammar induction.arXiv preprint arXiv:2002.00737, 2020.\n[16] Daphne Koller and Nir Friedman.Probabilistic graphical models: principles and techniques. MIT press,\n2009.\n[17] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-tuning can\ndistort pretrained features and underperform out-of-distribution.arXiv preprint arXiv:2202.10054, 2022.\n[18] Julian Kupiec. Robust part-of-speech tagging using a hidden markov model.Computer speech & language,\n6(3):225–242, 1992.\n[19] Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:\nProvable self-supervised learning.arXiv preprint arXiv:2008.01064, 2020.\n[20] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-eﬃcient prompt\ntuning. arXiv preprint arXiv:2104.08691, 2021.\n[21] Yoav Levine, Barak Lenz, Opher Lieber, Omri Abend, Kevin Leyton-Brown, Moshe Tennenholtz, and\nYoav Shoham. Pmi-masking: Principled masking of correlated spans.arXiv preprint arXiv:2010.01825,\n2020.\n[22] Xiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation.arXiv,\n2021.\n[23] Hong Liu, Jeﬀ Z. HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust to\ndataset imbalance, 2021.\n[24] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations.arXiv preprint arXiv:1802.05365, 2018.\n[25] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.\narXiv preprint arXiv:2104.06599, 2021.\n[26] Lawrence Rabiner and Biinghwang Juang. An introduction to hidden markov models.ieee assp magazine,\n3(1):4–16, 1986.\n[27] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding\nby generative pre-training. 2018.\n[28] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\n[29] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how\nbert works. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\n[30] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language\nmodels help solve downstream tasks.arXiv preprint arXiv:2010.03648, 2020.\n[31] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classiﬁcation and natural\nlanguage inference. arXiv preprint arXiv:2001.07676, 2020.\n13\n[32] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also few-shot\nlearners. arXiv preprint arXiv:2009.07118, 2020.\n[33] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. arXiv preprint\narXiv:2010.15980, 2020.\n[34] Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. Masked\nlanguage modeling and the distributional hypothesis: Order word matters pre-training for little.arXiv\npreprint arXiv:2104.06644, 2021.\n[35] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks.\narXiv preprint arXiv:1503.08895, 2015.\n[36] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline.arXiv preprint\narXiv:1905.05950, 2019.\n[37] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context?\nprobing for sentence structure in contextualized word representations.arXiv preprint arXiv:1905.06316,\n2019.\n[38] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic posterior\ninformation to linear models.arXiv:2003.02234, 2020.\n[39] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy,\nand linear models. InAlgorithmic Learning Theory, pages 1179–1206. PMLR, 2021.\n[40] Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with deep\nnetworks on unlabeled data, 2020. URLhttps://openreview.net/forum?id=rC8sJ4i6kaH.\n[41] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks.arXiv preprint arXiv:1410.3916,\n2014.\n[42] Tianyi Zhang and Tatsunori Hashimoto. On the inductive bias of masked language modeling: From\nstatistical to syntactic dependencies.arXiv preprint arXiv:2104.05694, 2021.\n[43] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning to\nrecall. arXiv preprint arXiv:2104.05240, 2021.\n14\nA Proofs for Section 3\nWe provide the formal proof of Theorem 3.3 based on the sketch in Section 3. The following lemma will be\nuseful in our analysis.\nClaim A.1. In the setting of Section 3, suppose that Assumption 3.2 holds. Fix any timestepiě1. Then\nthere exists a diagonal matrixD such that for allxPsupppPrXsq,\nPrHi|Xi`1:T`i “xs“ rxDPrH0 |X1:T “xs\nwhere rx ą0 is a positive scalar.\nProof. First, we note that by Assumption 3.2,PrHishas full support. As a consequence,PrpXi`1:t`i “xqą 0.\nBy Bayes’ rule,\nPrHi|Xi`1:T`i “xs“ PrXi`1:T`i “x|Hisd PrHis\nPrpXi`1:T`i “xq\n“ PrX1:T “x|H0sd PrH0s\nPrpXi`1:T`1 “xq d PrHis\nPrH0s (by Markovian property of HMMs)\n“PrH0 |X1:T “xsd PrHis\nPrH0s¨ PrpX1:T “xq\nPrpXi`1:T`i “xq\nNote that the vectorPrHis\nPrH0s has ﬁnite and positive entries. The same applies to the ratiorx ﬁ PrpX1:T “xq\nPrpXi`1:T`i“xq.\nThus, we get the desired statement.\nThe proof of Theorem 3.3 follows below.\nProof of Theorem 3.3.By deﬁnition, G1px1q “PrX1 |X2:T`1 “ xs. Therefore, our goal is to rewrite\nPrH0 |X1:T “ xs as a linear function ofPrX1|X2:T`1 “ xs (up to a scaling which won’t aﬀect the lin-\near head prediction). Concretely, we will show\nPrH0 |X1:T “xs“ rxBPrX1 |X2:T`1 “xs (A.1)\nfor a scalarrx ě0. With this equation, takingb“µJB will give the desired result.\nFirst, observe thatPrX1 |X2:T`1 “xs“ WPrH1 |X2:T`1 “xsby Proposition 3.4. Next, we apply Claim A.1\nto obtain an invertible matrixDsuch that for allxPsupppPrXsq, PrH1|X2:T`1 “xs“ rxDPrH0|X1:T “xs,\nwhere rx ą0 is a scalar.\nIf W has full row rank, it has a left inverseW: with W:W “I|H|ˆ|H|. Choosing b“µD´1W:, we obtain\n1 pbJG1px1qě 0q“ 1 pµJD´1W:WPrH1 |X2:T`1 “xsě 0q\n“1 pµJPrH0 |X1:T “xsě 0q“ F‹pxq\nNext, we complete the proof of Proposition 3.4.\n15\nProof of Proposition 3.4.We write\nPrU|V “vs“\nÿ\nz\nPrU,Z “z|V “vs\n“\nÿ\nz\nPrU|Z “z,V “vsPrpZ “z|V “vq (by Bayes’ rule)\n“\nÿ\nz\nPrU|Z “zsPrpZ “z|V “vq (since U KV |Z)\n“PrU|ZsPrZ|V “vs\nB Formal abstraction for prompt tuning and proofs for Section 3.1\nWe ﬁrst formalize the deﬁnition of the modelG described in Section 3.1. The modelG takes a sequence of\nembedding vectorsv “pv1,...,v tqas input and implements message passing to compute a sequence oft\noutputs. We ﬁrst deﬁne left and right messagesÐ Ýδi`1Ñipvqand Ý Ñδi´1Ñipvqfor iPrts, as follows:\nÐ Ýδt`1Ñtpeq“ PrHts\nÐ ÝδiÑi´1peq“ PrHi´1 |HispÐ Ýδi`1Ñipvqd viq@1 ăiăt\nÝ Ñδ0Ñ1peq“ PrH1s\nÝ ÑδiÑi`1peq“ PrHi`1 |HispÝ Ñδi´1Ñipvqd viq@1 ăiăt\nNext, we deﬁne the aggregated message at timestepi by\nτipvqﬁ\n$\n’’&\n’’%\nÐ Ýδ2Ñ1pvq if i“1Ð Ýδi`1Ñipvqd\nÝ Ñδi´1Ñipvq\nPrHis if 1 ăiăt\nÝ Ñδt´1Ñtpvq if i“t\n(B.1)\nNote that if Assumption 3.2 holds about the Markov chainH0,H1,... , τipvqis always well-deﬁned because\nPrHis will have full support. Note that for the proper embeddingsepxiq “PrXi “ xi|His, where for\nx“px1,...,x tq, we useepxq“p epx1q,...,e pxtqq, we can check via classical results on message passing [16]\nthat\nτipepxqq“ PrHi,X´i “x´is\nFinally, we let the model modelG compute\nGipvq“ W τipvq\n}τipvq}1\nThere is an edge case where the demoninator is 0, i.e.}τipvq}1 “0. To make the behavior ofG well-deﬁned,\nin this case we setGipvq“ 0|X|. We observe that if the input embedding are obtained byepxq, Gipvqindeed\ncomputes the desired conditional probability vector forxPsupppPrXsq:\nGipepxqq“ PrXi|X´i “x´is\nB.1 Proof of Theorem 3.6\nFirst we formalize the observation that soft prompt tuning is equivalent to adding a fake tokenrz to the\nvocabulary with emission probabilities at timestep 1 given byu, and lettingGcompute conditional probabilities\nfor this new distribution over sequences.\n16\nLemma B.1. In the setting of Theorem 3.6, ﬁx any prompt vectoruPr0,1s|H|. Deﬁne the random variable\npX with the same emission probabilities asX for ią1: PrpXi|His“ PrXi|His. For timestep 1, we deﬁne\nthe emission probabilities ofpX1 as follows:\nPrpX1 “rz|H1s“ u\nPrpX1 “z|H1s“p 1 ´uqd PrX1 “z|H1s@zPX\nIn the above equations,rz is a fake token added to the vocabulary at timestep 1. It follows that for anyi,\ndeﬁning τi as in (B.1)\nτippepxqq“ PrHi, pX´i “prz,∅,xq´is (B.2)\nAs a consequence, it follows that forią1 and anyx such thatprz,∅,xq´i PsupppPrpX´isq,\nGippepxqq“ PrpXi| pX´i “prz,∅,xq´is“ WPrHi| pX´i “prz,∅,xq´is\nFor anyx with prz,∅,xq´i RsupppPrpX´isq, Gippepxqq“ 0.\nNext, the following lemma disentangles the inﬂuences of the fake tokenrz and the input sequence on the\nposterior distribution of the hidden variable.\nLemma B.2. In the setting above, there exists an invertible diagonal matrixD such that for allx such that\nprz,xqP supppPrpX´2sq, the following equation holds:\nPrH2 | pX1 “rz, pX3:T`2 “xs“ rxDpPrpX1 “rz,H2sd PrH0 |X1:T “xsq\nHere rx ą0 is a positive scalar.\nWe now complete the proof of Theorem 3.6.\nProof of Theorem 3.6.Let B be the set deﬁned in Assumption 3.5 and deﬁneusuch thatuh “1 if hPB and\nuh “0 otherwise. First, we restrict our focus toxsuch thatprz,xqP supppPrpX´2sq. For thesex, we can apply\nLemma B.1 and Lemma B.2 in the manner described in the proof sketch. This givesG2ppepxqq“ rxWDv\nfor v ﬁ pApudPrH1sqqd PrH0 |X1:T “ xs. By deﬁnition of B, we havesupppApudPrH1sqq “H‹, so\nsupppDvqĎ H‹. Thus, there is a matrixyW: such that\nyW:G2ppepxqq“ rxyW:WDv “rxWDv\nThe existence of yW: is due to the fact that tW:,huhPH‹ is a linearly independent set of vectors, and\nsupppDvqĎ H‹ whenever x satisﬁes prz,xqP supppPrpX´2sq. Next, we note that a matrixB exists such that\npBDvqh “PrpH0 “h|X1:T “xqfor hPH‹ and pBDvqh “0 otherwise. This is becauseD is invertible, and\nsupppApudPrH1sqq“ H‹, so we can recoverPrH0 |X1:T “xson coordinates inH‹ by applying another\ncoordinate-wise scaling. It follows that we can setb“µJByW:. With this choice ofb, we compute\nbJG2ppepxqq“ rxµJBDv “rx\nÿ\nhPH‹\nµhPrpH0 “h|X1:T “xq“ rxµJPrH0 |X1:T “xs\nwhere the last equality follows becausesupppµqĎ H‹. This completes the case whereprz,xqP supppPrpX´2sq.\nOtherwise, forprz,xqR supppPrpX´2sq, by the behavior ofG in Lemma B.1,G2ppepxqq“ 0, so any linear head\nmust outputbJG2ppepxqq“ 0. Furthermore, by the conditional independence structure inpX, we must also have\nsupppPrH2, pX1 “rzsqX supppPrH2, pX3:T`2 “xsq“H . As supppµqĎ supppPrH2, pX1 “rzsq, this must also\nmean supppµqXsupppPrH2, pX3:T`2 “xsq“H . However, we also havePrH2, pX3:T`2 “xs“ PrH2,X3:T`2 “\nxsby the deﬁnition ofpX, and this must have the same support asPrH0 |X1:T “xsby applying Claim A.1\nand the fact thatxPsupppPrXsq. It follows that for this choice ofx, µJPrH0 |X1:T “xs“ 0, so the desired\nstatement still stands.\n17\nWe ﬁll in the proofs of the lemmas below.\nProof of Lemma B.1.First, we note that(B.2) follows directly from the derivation ofτ, and well-known\nresults about message passing [16]. Next, it suﬃces to consider the case whereprz,∅,xq´i RsupppPrpX´isq,\nas the other case follows directly from the deﬁnition ofG in terms of τ. In this case, we observe that\nτippepxqq“ PrHi, pX´i “prz,∅,xq´is“ 0. It follows that}τippepxqq}1 “0. Thus, from our deﬁnition ofG, we\nmust haveGippepxqq“ 0.\nProof of Lemma B.2.By the conditional independence relations in a HMM,pX1 K pX3:T`2 |H2. Using Bayes’\nrule, we obtain\nPrH2 | pX1 “rz, pX3:T`2 “xs“ PrpX1 “rz, pX3:T`2 “x|H2sd PrH2s\nPrppX1 “rz, pX3:T`2 “xq\n“ PrpX1 “rz|H2sd PrpX3:T`2 “x|H2sd PrH2s\nPrppX1 “rz, pX3:T`2 “xq\n(by conditional independence)\n“ PrpX1 “rz|H2sd PrX1:T “x|H0sd PrH2s\nPrppX1 “rz, pX3:T`2 “xq\n(by deﬁnition ofpX and the Markovian property)\n“rxPrpX1 “rz,H2sd PrH0 |X1:T “xsd 1\nPrH0s\nWhere we deﬁnerx ﬁ PrpX1:T “xq\nPrpxX1“rz,xX3:T`2“xq. We note thatrx is positive and well-deﬁned by the conditions of\nthe lemma and Theorem 3.6. We can setD to be the matrixdiagp 1\nPrH0sq, which has ﬁnite positive entries on\nthe diagonal by Assumption 3.2.\nC Proofs for Section 4\nFirst, we introduce a proposition which is generally useful for proving the theorems in Section 4.\nProposition C.1. In the setting of Section 4, it holds that\nPrXi|X´i “x´is“ PrXi|MJi,Ji,SisPrMJi,Ji,Si,X´i “x´is\nEquivalently, we have the expansion\nPrXi|X´i “x´is“\nÿ\nh“pj,sq\nÿ\nm\nW:,pm,j,sqPrpMj “m,Hi “h|X´i “x´iq (C.1)\nProof. An alternative interpretation of this statement is thatXi is conditionally independent from everything\nelse givenMJi,Ji,Si. However, we will prove this statement algebraically. We compute\nPrXi |X´i “x´is“\nÿ\nh“pj,sq\nÿ\nmj\nÿ\nm´j\nPrXi |M´j “m´j, Mj “mj, Hi “hsPrpM´j “m´j, Mj “mj, Hi “h |X´i “x´iq\n“\nÿ\nh“pj,sq\nÿ\nmj\nÿ\nm´j\nW:,pmj,j,sqPrpM´j “m´j, Mj “mj, Hi “h |X´i “x´iq\n“\nÿ\nh“pj,sq\nÿ\nmj\nW:,pmj,j,sqPrpMj “mj, Hi “h |X´i “x´iq\n18\nC.1 Proof of Theorem 4.3\nThroughout this section, we useMJi to denote the random variable obtained by indexingM by Ji, both of\nwhich are themselves random variables. LetpI denote the set of indicesiwhere supppPrJi|X´i “x´isq“t j‹u\nand supppPrSi|X´i “x´isqĎ S‹. We will ﬁrst construct the key functionK and queryq such that the set\nof I of attended-to positions(4.2) is preciselypI. This construction does not require the position embeddings\nβ1,...,β t, so we set them to0.\nThe following lemma demonstrates the existence ofK and q such thatI “pI.\nLemma C.2.In the setting of Theorem 4.3, deﬁnepI ﬁ ti: supppPrJi|X´i “x´isq“t j‹uand supppPrSi|X´i “\nx´isqĎ S‹u. Then there exist queryqPR|H| and keyK parameterized byΘpKq PR|H|ˆ|X|, such that when\nxPsupppPrXsqand pI is nonempty, the setI of attended-to positions satisﬁesI “pI.\nThe proof of Lemma C.2 requires the following claim.\nClaim C.3.In the setting of Theorem 4.3, there is a matrixΘp1q PR|H|ˆ|X| such that for allxPsupppPrXsq\nand sPS‹, pΘp1qGipxqqpj‹,sq “PrHi “pj‹,sq|X´i “x´is. Furthermore,}Θp1qGipxq}1 “1. In addition, for\nsPS‹, there existsΘp2,sq PR|M|ˆ|X| such that for allxPsupppPrXsq,\nΘp2,sqGipxq“ PrMj‹,Hi “pj‹,sq|X´i “x´is\nProof. We have, by Proposition C.1,\nGipxq“ PrXi|X´i “x´is\n“\nÿ\nh“pj,sq\n˜ÿ\nm\nW:,pm,j,sqPrpMj “m,Hi “h|X´i “x´iq\n¸\n“\nÿ\nh“pj,sq\nνphq\nIn the last equality, we deﬁnedνphq to be the expression in the parentheses. Note thatνphq P Vphq ﬁ\nspanptW:,pm,hqumPMq. Furthermore, for h R H‹, νphq P sV ﬁ spanptW:,pm,hqumPM,hPHzH‹q. As the spans\npVphqqhPH‹ and sV are all pairwise disjoint, by Assumption 4.2, for eachhPH‹, we can recover\nνphq “BphqPrXi|X´i “x´is\nLikewise, we can obtain\nÿ\nhRH‹\nνphq “ sBPrXi|X´i “x´is\nNow we have, forhPH‹,\n1Jνphq “\nÿ\nm\n1JW:,pm,hqPrpMj “m,Hi “h|X´i “x´iq\n“\nÿ\nm\nPrpMj “m,Hi “h|X´i “x´iq (because 1JW:,pm,hq “1)\n“PrpHi “h|X´i “x´iq\nLikewise, the same reasoning gives1Jř\nhRH‹ νphq “ř\nhRH‹ PrpHi “h|X´i “x´iq. Thus, we can choose\nΘp1q to be the matrix with rowsΘp1q\nh,: “1JBphq when hPH‹, and for some arbitraryshRH‹, Θp1q\nsh,: “1JsB.\nWe set all other rows to0, and we can check that this satisﬁes the lemma requirements.\n19\nWe now constructΘp2,hq. We can expressνphq in a vectorized manner by writing\nνphq “W:,pM,hqPrMj,Hi “h|X´i “x´is\nwhere W:,pM,hq PR|X|ˆ|M| has columnstW:,pm,hqumPM. Note that forj “j‹, sPS‹, the non-degeneracy\nassumptions imply thatW:,pM,j‹,sq has left inverseW:\n:,pM,j‹,sq. Thus, we setΘp2,sq “W:\n:,pM,j‹,sqBpj‹,sq to\nobtain forsPS‹,\nΘp2,sqGipxq“ W:\n:,pM,j‹,sqBpj‹,sqPrXi|X´i “x´is\n“W:\n:,pM,j‹,sqW:,pM,j‹,sqPrMj‹,Hi “pj‹,sq|X´i “x´is\n“PrMj‹,Hi “pj‹,sq|X´i “x´is\nThis gives the desired result.\nProof of Lemma C.2.We choose the ﬁrst|H|entries ofqsuch thatqh “1 if h“pj‹,sqfor sPS‹, andqh “0\notherwise. The last entry is 0. Next, we chooseΘpKq so that the ﬁrst|H|rows areΘp1q, and the last row is\nall zeros. whereΘp1q is deﬁned in Claim C.3. With this choice ofΘpKq, KpGipxqqh “PrpHi “h|X´i “x´iq\nfor hPH‹. Furthermore,}KpGipxqq}1 “1, by Claim C.3.\nNow we note that for alli, 1 “}KpGipxqq}1 ěqJKpGipxqq, and fori P pI, qJKpGipxqq“ ř\nsPS‹ PrpHi “\npj‹,sq|X´i “x´iq“ 1 by deﬁnition ofq and pI. This implies that positionsi P pI do indeed achieve the\nmaximum attention scores.\nNext, we also require a construction of the value function such that it computes the correct prediction for all\niPpI.\nLemma C.4.In the setting of Theorem 4.3, letpI be deﬁned as in Lemma C.2. We can choose the parameters\nof the value functionV, ΘpVq PR|M||H|ˆ|X|, bPR|M||H|, such that whenxPsupppPrXsqand pI is nonempty,\nfor alliPpI,\nVpGipxq,epxiqq“ rx,iµJPrMj‹ |X1:T “xs\nwhere rx,i ą0 is a positive scalar.\nProof. We ﬁrst chooseΘpVq such that the rows satisfyΘpVq\npm,j‹,sq,: “Θp2,sq\nm,: when sPS‹ for Θp2,sq constructed\nin Claim C.3, andΘpVq\npm,j,sq,: “0|X| otherwise forj ‰j‹ or sRS‹.\nWe claim that foriPpI,\nΘpVqGipxq“ PrMJi,Ji,Si|X´i “x´is (C.2)\nThis is because forsPS‹, Θp2,sqGipxq“ PrMj‹,Hi “pj‹,sq|X´i “x´isby Claim C.3, and forh“pj,sq\nfor j ‰j‹ or sRS‹,\nPrMj,Hi “h|X´i “x´is“ PrMj|Hi “h,X´i “x´isPrpHi “h|X´i “x´iq“ 0|M|\nNote that this last equality followed becausePrpHi “h|X´i “x´iq“ 0 for the choice ofh and iPpI. By\nconstruction ofΘpVq, these computations imply that(C.2) does indeed hold. The embedding can be chosen\nsuch thatepxiq“ PrXi “xi|MJi,Ji,Sis. Thus, we have foriPpI:\npΘpVqGipxqqd epxiq“ PrMJi,Ji,Si|X´i “x´isd PrXi “xi|MJi,Ji,Sis\n“PrXi “xi,MJi,Ji,Si|X´i “x´is\n20\nThe last equality followed from applying the same reasoning as in Proposition C.1.\nNow we letB PR|M|ˆ|M||H| be the matrix such that\npBPrXi “xi,MJi,pJi,Hiq|X´i “x´isqm “\nÿ\ns\nPrpXi “xi,Mj‹ “m,Ji “j‹,Si “s|X´i “x´iq\nNow we pick the last linear weight in the value function byb“BJµ. It follows that foriPpI,\nVpGipxq,epxiqq“ bJppΘpVqGipxqqd epxiqq\n“µJBppΘpVqGipxqqd epxiqq\n“µJBPrXi “xi,MJi,Ji,Si|X´i “x´is\n“µJÿ\ns\nPrXi “xi,Mj‹,Ji “j‹,Si “s|X´i “x´is\n“µJPrMj‹,Xi “xi|X´i “x´is\nWe obtained the last equality by observing thatř\nsPrXi “ xi,Mj‹,Ji “ j‹,Si “ s|X´i “ x´is “\nPrMj‹,Xi “xi|X´i “x´isfor i P pI, as the distribution ofHi must concentrate whereJi “j‹. Finally,\nwe observe thatµJPrMj‹,Xi “xi|X´i “x´is“ µJPrMj‹ |X1:T “xsPrpXi “xi|X´i “x´iq, so setting\nrx,i “PrpXi “xi|X´i “x´iqcompletes the proof.\nNow we can complete the proof of Theorem 4.3.\nProof of Theorem 4.3.By applying Lemmas C.2 and C.4, we constructed key, query, and value functions\nfor the attention head such that for allx P supppPrXsq with pI (deﬁned in Lemma C.2) nonempty, the\nattended-to positionsI satisfy I “pI, andVpGipxq,epxiqq“ rx,iµJPrMj‹ |X1:T “xs. As the attention head\ncomputes the average ofVpGipxq,epxiqqover attended-to positions, andrx,i is positive for alliPpI, we obtain\nthe desired result.\nWe note that this proof also works for the case where there is a single memory cell, as that is a special case\nwhere Ji “j‹ always, and we only need to consider the evolution ofSi.\nC.2 Formal abstraction for prompt tuning in Section 4.2\nWe will work directly in the case with multiple memories, as the single memory case is captured in this setting.\nWe follow the construction in Section B. our message passing formulation requires the augmented Markov\nchain rH0 ﬁ pM1,...,M N,H0q, rH1 ﬁ pM1,...,M N,H1q,..., which uses the following transition probabilities:\nPrprHi`1 “pm1,h1q| rHi “pm,hqq“ Ah1,h1 pm1 “mq\nLet rH denote the set of possible values forrH. For vectorvPR|M||H| we deﬁne a lifting functionη: R|M||H| Ñ\nR|rH| by\nηpvqpm1:N,j,sq “vpmj,j,sq\nWe observe thatηpPrXi “xi|MJi,pJi,Siqsq“ PrXi “xi| rHis.\n21\nNow we formalize the modelG. G will take embedding vectorsv“pv1,...,v tqwith vi PR|rH| as follows. We\ndeﬁne left and right messagesÐ Ýδi`1Ñipvqand Ý Ñδi´1Ñipvqfor iPrtsvia:\nÐ Ýδt`1Ñtpvq“ PrrHts\nÐ ÝδiÑi´1pvq“ PrrHi´1 | rHispÐ Ýδi`1Ñipvqd viq@1 ăiăt\nÝ Ñδ0Ñ1pvq“ PrrH1s\nÝ ÑδiÑi`1pvq“ PrrHi`1 | rHispÝ Ñδi´1Ñipvqd viq@1 ăiăt\nWe observe that this deﬁnition almost matches Section B, except it replacesH with rH. Next, we deﬁne the\naggregated message at timestepi by\nτipvq“\n$\n’’&\n’’%\nÐ Ýδ2Ñ1pvq if i“1Ð Ýδi`1Ñipvqd\nÝ Ñδi´1Ñipvq\nPrĂHis if 1 ăiăt\nÝ Ñδt´1Ñtpvq if i“t\n(C.3)\nIn the edge case wherePrMs does not have full support, the coordinate-wise division in the deﬁnition\nabove would sometimes divide by 0. However, for all these cases both of the corresponding terms in the\nnumerator must also be 0, so we can simply set the value ofτi in this coordinate to 0. We will see that this\npreserves the meaning of the messageτi, which for the proper embeddingsepxiq“ PrXi “xi| rHis, with\nepxq“p epx1q,...,e pxtqq, computes\nτipepxqq“ PrrHi,X´i “x´is\nWe can now deﬁne the reverse lifting functionφ: R|rH|Ñ|M||H| as follows:\npφpvqqmj,j,s “ 1\n|M|N´1\nÿ\nm´j\nvm1:N,j,s (C.4)\nWe observe thatφpτipepxqqq“\nPrMJi,Ji,Si,X´i“x´is\n|M|N´1 . We now compute the model output as follows:\nGipvq“ W φpτipvqq\n}φpτipvqq}1\nIn the edge case where}φpτipvqq}1 “ 0, we again deﬁneGpvq “0|X|. We can observe thatGipepxqq “\nPrXi|X´i “x´is.\nThe downstream classiﬁer uses the embeddingpepxqdeﬁned as follows:\npepxq“p u,epx1qq,...,e pxtqqq\nwith a tunable prompt embeddinguPR|rH|. We also require a slightly modiﬁed attention head. The value\nfunction V in the attention head is slightly modiﬁed to accomodate the new embedding dimension. Letting\nV : R|X|ˆR|rH| ÑR,\nVpa,vq“ bJppΘpVqaqd φpvqq\nThe dimensions of the parametersb,ΘpVq remain unchanged. Note that when there is just a single memory,\nthis reduces to the case in Section 4.\n22\nC.3 Analysis for prompt tuning in the multiple memory setting\nWe will state and prove our result for the prompt tuning setting with multiple memories. For the multiple\nmemory setting, the downstream classiﬁer uses the following embedding functionpe:\npepxq“p u,ηpepx1qq,...,η pepxtqqq\nwith a tunable prompt embeddinguPR|rH|. The attention head is changed so that the value function takes a\nlarger dimensional embedding:\nVpa,vq“ bJppΘpVqaqd φpvqq\nwhere φ is deﬁned in(C.4). The following assumption extends Assumption 4.5 to the multiple memory case.\nAssumption C.5 (Multiple memories version of Assumption 4.5). Let M‹ ﬁ supppµq denote the set of\nnon-zero coordinates inµ. There exists a set of recoverable hidden statesH‹, such that the collection of token\nemission probabilities fromM‹ˆH‹, tW:,pm,hqumPM‹,hPH‹, is a linearly independent set of vectors.\nFurthermore, deﬁne the following span of vectors:\nsV ﬁ spanptW:,pm,j‹,squmPM‹,sPSzS‹ YtW:,pm,j,squmPM,j‰j‹,sPSq\nThen sV must be disjoint from the span of token emission probabilities fromM‹ˆH‹:\nspanptW:,pm,hqumPM‹,hPH‹qX sV “t0|X|u\nNote that Assumption C.5 reduces to Assumption 4.5 the case whereN, the number of memory cells, is 1. In\nany case, it is a relaxation of Assumption 4.2.\nWe now state and prove the result for multiple memories.\nTheorem C.6. In the setting above, suppose that non-degeneracy Assumption C.5 and holds. In addition,\nsuppose that Assumption 4.4 (stationarity) holds. Then there exists a promptuand attention head onGppepxqq\nand the token embeddings which can compute the ground-truthF‹pxqfor anyxPR, deﬁned in(4.3):\nF‹pxq“ 1 pAttnppGippepxqq,peipxqqt`1\ni“1qě 0q\nHere pe is the embedding in(4.4) and Attn is deﬁned in(4.2).\nWe begin by rigorously stating the observation that soft prompt tuning is equivalent to adding a fake tokenrz\nto the vocabulary and modifying the token emission probabilities at timestep 1, analogous to Lemma B.1.\nLemma C.7. In the setting of Theorem C.6, deﬁnerH as in Section C.2. Fix any prompt vectoruPr0,1s|rH|.\nDeﬁne the random variablepX with the same emission probabilities asX for ią1: PrpXi| rHis“ PrXi| rHis.\nFor timestep 1, we deﬁne the emission probabilities ofpX1 as follows:\nPrpX1 “rz| rH1s“ u\nPrpX1 “z| rH1s“p 1 ´uqd PrX1 “z| rH1s@zPX\nIn the above equations,rz is a fake token added to the vocabulary at timestep 1. It follows that for anyi,\ndeﬁning τi as in (C.3)\nτippepxqq“ PrrHi, pX´i “prz,xq´is (C.5)\nAs a consequence, it follows that forią1 and anyx such thatprz,xq´i PsupppPrpX´isq,\nGippepxqq“ PrpXi| pX´i “prz,xq´is“ WPrMJi,Ji,Si| pX´i “prz,xq´is\nFor anyi and x with prz,xq´i RsupppPrpX´isq, Gippepxqq“ 0.\n23\nThe proof of Lemma C.7 mirrors the proof of Lemma B.1, so we omit it here.\nIn particular, throughout the proof we will use the following promptu:\num1:N,j,s “\n#\n1 if mj‹ Psupppµq\n0 otherwise (C.6)\nWe will also use the notationpxﬁ prz,x1,...,x tq. The following lemma considers behaviors in edge cases with\nthis choice ofu.\nTowards our proofs, the following result is useful.\nProposition C.8. In the setting of Theorem C.6, wherePrH0s is the stationary distributions satisfying\nPrH0s“ APrH0s, it holds that\nPrM,Hi,Xi`1:i`ts“ PrM,H0,X1:ts\nfor anytě1, iě1.\nProof. Because PrH0sis stationary, we observe thatPrM,His“ PrM,H0sfor alli. We write\nPrXi`1:i`t,M “m,Hi “hs“ PrXi`1:i`t|M “m,Hi “hsPrpM “m,Hi “hq\n“PrX1:t|M “m,H0 “hsPrpM “m,Hi “hq\n(by time-invariance of HMMs)\n“PrX1:t|M “m,H0 “hsPrpM “m,H0 “hq\nWe will now restrict our focus to the set of inputs\nZ ﬁ tx: PrppX´i “prz,xq´iqą 0 @iPrtsu (C.7)\nWe also deﬁne the set\npI ﬁ ti`1 : supppPrSi|X´i “x´isqĎ S‹,supppPrJi|X´i “x´isqĎt j‹u,i Prtsu (C.8)\nHere S‹ is deﬁned in the non-degeneracy assumption. We will ﬁrst construct key and query parameters such\nthat the set of attended-to positions is preciselypI, following the proof of Theorem 4.3.\nLemma C.9 (Analogue to Lemma C.2). In the setting of Theorem C.6 and above, deﬁneu as in (C.6).\nThere are parametersΘpKq PRp|H|`1qˆ|X|, qPR|H|`1, andβ1,β2,... PR|H|`1 such that for anyxPZ where\npI is nonempty, the set of attended-to positionsI (deﬁned in(4.1)) satisﬁesI “pI.\nTowards proving Lemma C.9, the following construction will be useful.\nClaim C.10 (Analogue of Claim C.3). In the setting of Theorem C.6, deﬁneH‹ as in Assumption C.5.\nThere is a matrixΘp1q PR|H|ˆ|X| such that for allx PsupppPrXsq, and i ą1 with PrppX´i “ px´iq ą0,\npΘp1qGippepxqqqh “PrHi “h| pX´i “px´isfor anyhPH‹. Furthermore,}Θp1qGippepxqq}1 “1.\nIn addition, forsPS‹, there existsΘp2,sq PR|M|ˆ|X| such that for allią1 and x with PrppX´i “px´iqą 0,\nΘp2,sqGippepxqq“ PrMj‹,Hi “pj‹,sq| pX´i “px´is\nOur proof will require the following result which shows that the distribution ofMj‹ has limited support.\nProposition C.11. In the setting of Theorem C.6 and Lemma C.7, letu be deﬁned as in(C.6). Then for\nall ią1, supppPrMj‹ | pX´i “px´isqĎ supppµqif PrppX´i “px´iqą 0.\n24\nProof. We have\nPrMj‹|xX´i “px´is“\nÿ\nm´j‹,h\nPrMj‹,M´j‹ “m´j‹,H1 “h|xX´i “px´is\n“\nÿ\nm´j‹,h\nPrxX1 “rz|Mj‹,M´j‹ “m´j‹,H1 “hsd PrMj‹,M´j‹ “m´j‹,H1 “h|xX´p1,iq “px´p1,iqs\nPrpxX1 “rz|xX´p1,iq “px´p1,iqq\nIn this equation we used´p1,iq to index all but the ﬁrst andi-th element of the sequence. We note that\nsupppPrpX1 “rz|Mj‹,M´j‹ “m´j‹,H1 “hsq“ supppµqfor allm´j‹,h, so the desired statement follows.\nNow we complete the proof of Claim C.10.\nProof of Claim C.10.The proof of this statement will be analogous to Claim C.3. As before, we have\nGippepxqq“\nÿ\nh“pj,sq\n˜ÿ\nm\nW:,pm,j,sqPrpMj “m,Hi “h| pX´i “px´iq\n¸\n“\nÿ\nh“pj,sq\nνphq\nIn the last equality, we deﬁnedνphq to be the expression in the parentheses. We consider several cases. First,\nwhen h “ pj‹,sq for s P S, we must have that wheni ą 1, PrMj‹ | pX´i “ px´is is supported onM‹ by\nProposition C.11. Thus, νphq PVphq ﬁ spanptW:,pm,hqumPM‹q. As a result, forh RH‹, νphq P sV, which is\nthe span of vectors deﬁned in Assumption C.5. As the spanspVphqqhPH‹ and sV are all pairwise disjoint, by\nAssumption 4.2, for eachhPH‹, we can recover\nνphq “BphqPrXi|X´i “x´is\nLikewise, we can obtain\nÿ\nhRH‹\nνphq “ sBPrXi|X´i “x´is\nThe remainder of this proof for the construction ofΘp1q follows the same steps as Claim C.3.\nFor the second part about constructingΘp2,sq, we modify Claim C.3 in a few ways. First, eachνpj‹,sqis\nrecoverable as a linear function ofGippepxqqwhen s PS‹. Now usingM‹ ĎM as shorthand forsupppµq,\nwe deﬁne the matrixW:\n:,pM‹,j‹,sq PR|M‹|ˆ|X| to be the left inverse ofW:,pM‹,j‹,sq, the matrix with columns\ntW:,pm,j‹,squmPM‹. This left inverse exists by the non-degeneracy assumptions. Now we construct the matrix\n{W:\n:,pM‹,j‹,sq PR|M|ˆ|X|, where them-th row of {W:\n:,pM‹,j‹,sq matches the corresponding row ofW:\n:,pM‹,j‹,sq if\nmPM‹ and is0 otherwise.\nWe observe that becausesupppPrMj‹,Hi “pj‹,sq| pX´i “px´isqĎ M‹ by Proposition C.11, we can ﬁnish\nthe proof by repeating the argument of Claim C.3.\nThe following claim relating the support ofHi conditioned on pX to the support ofHi conditioned onX will\nalso be useful.\nClaim C.12. In the setting of Theorem C.6 and Lemma C.7, suppose thatu is deﬁned as in(C.6). For\nią1 with PrppX´i “px´iqą 0, we have\nsupppPrHi| pX´i “px´isqĎ supppPrHi´1 |X´pi´1q “x´pi´1qsq\n25\nProof. We have\nPrHi | pX´i “px´is“\nÿ\nm,h\nPrM “m, H1 “h, Hi | pX´i “px´is“\nř\nm,h PrppX1 “rz |M “m, H1 “hqPrM “m, H1 “h, Hi | pX2:i´1 “px2:i´1, pXi`1:T`1 “pxi`1:t`1s\nPrpX1 “rz | pX2:i´1 “px2:i´1, pXi`1:T`1 “pxi`1:t`1q\n“\nř\nm,h PrppX1 “rz |M “m, H1 “hqPrM “m, H0 “h, Hi´1 |X´pi´1q “x´pi´1qs\nPrpX1 “rz | pX2:i´1 “px2:i´1, pXi`1:T`1 “pxi`1:t`1q\n(C.9)\nThe last line used the time-invariance property of the HMM (Proposition C.8), the deﬁnition ofpx, and the\nfact thatPrpXi|Hi,Msis distributed the same asPrXi|Hi,Msfor i ą1. On the other hand, note that\nPrHi´1 |X´pi´1q “x´pi´1qs“ ř\nm,hPrM “m,H0 “h,Hi´1 |X´pi´1q “x´pi´1qs. This involves a sum over\nthe same terms in the numerator in(C.9). Thus, as all the terms in the sum of(C.9) are nonnegative, the\ndesired statement follows.\nThis lets us complete the proof of Lemma C.9.\nProof of Lemma C.9.By settingΘpKq “\n„\nΘp1q\n0\n\n, whereΘp1q is deﬁned in Claim C.10, we obtainK such that\nfor alli > 1,pKpGippepxqqqqh “PrpHi “h|pX´i “px´iqfor h PH‹. Furthermore, pKpGippepxqqqq|H|`1 “0,\nand }KpGippepxqqq}1 “1. We chooseβ1 “\n„\n0|H|\n´2\n\nand βi “0|H|`1 for ią1. We also constructq so that the\nﬁrst |H|dimensions are the indicator on the settj‹uˆ S‹. We setq|H|`1 “1. Note that this construction\nensures that forią1, 1 “}KpGippepxqqq}1 ěqJpKpGippepxqqq` βiqě 0. Note that foriPpI, by Claim C.12\nwe havesupppPrHi| pX´i “px´isqĎ supppPrHi´1 |X´pi´1q “x´pi´1qsqĎt j‹uˆ S‹. Thus, for suchi P pI,\nwe haveqJpKpGippepxqqq` βiq“ 1, achieving the maximum over all positions. Finally, we note that1 RI\nbecause the position embeddingβ1 ensures thatqJpKpG1ppepxqqq` β1qď´ 1. Thus, I “pI, as desired.\nNext, the following lemma constructs the value function, analogously to Lemma C.4.\nLemma C.13 (Analogue to Lemma C.4). In the setting of Theorem C.6 and Lemma C.7, deﬁneu as\nin (C.6), and pI as in (C.8). We can choose the parameters of the value functionV, ΘpVq PR|M||H|ˆ|X|,\nbPR|M||H|, such that forxPsupppPrXsqwhere pI is nonempty, for alliPpI with PrppX´i “px´iqą 0,\nVpGippepxqq,peipxqq“ µJPrpXi “pxi,Mj‹ | pX´i “px´is\nAs a consequence, for alliPpI,\nVpGippepxqq,peipxqq“ rx,iµJPrMj‹ |X “xs\nwhere rx,i ą 0 is a positive scalar. In particular, this holds regardless of whetherPrppX´i “ px´iq ą0.\nFurthermore, whenpxRsupppPrpXsq, for allią1, we must have\nVpGippepxqq,peipxqq“ 0\nWe rely on the following claim.\nClaim C.14. In the setting of Theorem C.6 and Lemma B.1 whereu takes the value in in(C.6), for allx\nwhere pxﬁ prz,xqP supppPrpXsq, we have\nµJPrMj‹ | pX “pxs“ µJPrMj‹ |X1:T “xs\nPrppX1 “rz|pX2:T`1 “px2:t`1q\n26\nProof. We observe that\nµJPrM|xX “pxs (C.10)\n“µJÿ\nh\nÿ\nm´j‹\nPrMj‹,M´j‹ “m´j‹,H1 “h|xX “pxs\n“µJ\nř\nh\nř\nm´j‹ PrxX1 “rz|Mj‹,M´j‹ “m´j‹,H1 “hsd PrMj‹,M´j‹ “m´j‹,H1 “h|xX2:T`1 “px2:t`1s\nPrpxX1 “rz|xX2:T`1 “px2:t`1q\n“µJ\nř\nh\nř\nm´j‹ PrxX1 “rz|Mj‹,M´j‹ “m´j‹H1 “hsd PrMj‹,M´j‹ “m´j‹,H0 “h|X1:T “xs\nPrpxX1 “rz|xX2:T`1 “px2:t`1q\n(by Proposition C.8 and the deﬁnition ofxX)\nNow we haveµJdiagpPrpX1 “ rz|Mj‹,M´j‹ “ m´j‹,H1 “ hsq “µJ because by construction,PrpX1 “\nrz|Mj‹,M´j‹ “m´j‹,H1 “hsis only supported on supppµqand equals 1 on the support. Thus, we obtain\nµJPrMj‹ | pX “pxs“\nř\nhµJPrMj‹,H0 “h|X1:T “xs\nPrppX1 “rz| pX2:T`1 “px2:t`1q\n“ µJPrMj‹|X1:T “xs\nPrppX1 “rz| pX2:T`1 “px2:t`1q\nWe also require the following result to handle edge cases where probability values are 0.\nClaim C.15. In the setting of Theorem C.6 and Lemma C.7, deﬁneu as in (C.6). Consider an input\nx P supppPrXsq such that px ﬁ prz,x1,...,x tq satisﬁes PrppX “ pxq “0. Then µJPrMj‹|X1:T “ xs “0.\nFurthermore, for anyx where PrppX´i “px´iq“ 0 for somei, we must haveGippepxqq“ 0|X|.\nProof. First, we observe that\n0 “PrppX “pxq\n“PrpX1 “rz|M,H1sJPrM,H1, pX´1 “px´1s\n“uJPrM,H0,X “xs (by Proposition C.8 and Lemma C.7)\nIn particular, assupppuqX supppPrM,H0,X1:T “xsq“H , it follows thatPrpMj‹ “m,H0 “h,X1:T “\nxq “0 for all m P supppµq and any h, by the construction ofu. Since x P supppPrXsq, it follows that\nPrpMj‹ “m|X1:T “xq“ 0 for allmPsupppµq, soµJPrMj‹|X1:T “xs“ 0.\nWe note that the statement aboutGippepxqqfollows because of Lemma C.7.\nProof of Lemma C.13.To construct the value function, we deﬁneΘpVq in the same manner as Lemma C.4,\nsuch thatΘpVq contains Θp2,sq constructed in Claim C.10 as a submatrix:ΘpVq\npm,j‹,sq,: “Θp2,sq\nm,: for sPS‹. All\nother rows ofΘpVq are 0. It now follows that foriPpI and x where PrppX´i “px´iqą 0, by deﬁnition ofpI,\npΘpVqGippepxqqqd φpepxiqq“ PrpXi “pxi,MJi,pJi,Siq|pX´i “px´is\nThe proof that this claim is correct follows the same reasoning as Lemma C.4, where we argue that\nPrHi| pX´i “px´ismust concentrate ontj‹uˆ S‹ for alliPpI. Thus, we can deﬁneb“BJµ, whereB is\ndeﬁned in Lemma C.4. We observe that foriPpI, the same reasoning as before gives\nVpGippepxqq,peipxqq“ µJPrpXi “pxi,Mj‹ | pX´i “px´is\n27\nFirst, ifprz,xqR supppPrpXsq, by Claim C.15, we haveµJPrMj‹ |X1:T “xs“ 0. The expression above must\nalso equal0, asprz,xqR supppPrpXsq. Otherwise, we have\nVpGippepxqq,peipxqq“ µJPrMj‹ | pX “pxsPrppXi “pxi| pX´i “px´iq\nNow we apply Claim C.14 to get the desired result in this case. A additional case is whenPrppX´i “px´iq“ 0.\nIn this case, Claim C.15 shows thatGippepxqq“ 0, so it follows that the value function also computes 0 in\nthis case.\nFinally, we need to check the case wherepxRsupppPrpXsq, and we want to showVpGippepxqq,peipxqq“ 0 for all\nią1. The case wherePrppX´i “px´iq“ 0 is already handled above. In the case wherePrppX´i “px´iqą 0,\nwe can apply Claim C.10 to our construction forΘpVq to get\npΘpVqGippepxqqqm,h “\n#\nPrMj‹ “m,Hi “pj‹,sq| pX´i “px´is if h“pj‹,sqfor sPS‹\n0 otherwise\nThus, taking the element-wise product withφpepxiqq“ PrpXi “pxi|MJi,Ji,Sis, we must have, by Proposi-\ntion C.1,\nppΘpVqGippepxqqqd φpepxiqqqm,h “#\nPrpXi “pxi,Mj‹ “m,Hi “pj‹,sq| pX´i “px´is if h“pj‹,sqfor sPS‹\n0 otherwise\nBoth of these terms must be 0 sincepxRsupppPrpXsq, giving the desired result.\nNow we are ready to prove Theorem C.6.\nProof of Theorem C.6.The ﬁrst case we consider is whenxPZ, deﬁned in(C.7). By applying Lemmas C.9\nand C.13, we constructed key, query, and value functions for the attention head such that whenpI (C.8) is\nnonempty, the attended-to positionsI satisfy I “pI. In addition, by applying Lemma C.13, we also obtain\nthat for x P supppPrXsq, VpGippepxqq,peipxqq “rx,iµJPrMj‹ |X1:T “ xs. As the attention head averages\nVpGippepxqq,peipxqqover the attended-to positions, andrx,i is positive for alli P pI, we obtain the desired\nresult.\nIn the second case,xRZ, soprz,xqR supppPrpXsq. By Lemma C.13, for allią1, the value function outputs\n0. However, by the construction in Lemma C.9, the attention will only attend toią1. Thus, the output of\nthe attention head is0. However, Claim C.15 also implies thatµJPrMj‹ |X1:T “xs“ 0, giving the desired\nresult.\nD Experimental details\nGenerating HMM parameters. For all experiments, we randomly generated the parameters of an HMM\nwith 10 output symbols in its vocabulary. We generate a random transition matrix by taking a random convex\ncombination of random permutation matrices. We mix as many permutation matrices as there are hidden\nstates; i.e. if there are 4 hidden states, then we mix 4 random permutation matrices. The mixing weights\nare generated by sampling logits IID from a uniform distribution onr0,1sand then taking a softmax with\ntemperature 0.01. Although this is a small temperature, the transition probabilities can still be around 0.7\nfor some transitions. The start distribution is also sampled in the same way, but with softmax temperature\n10.0. The rows of the emission probability matrix is also sampled the same way with temperature 0.01.\n28\nPretrain model. The pretrained model follows the BERT-base architecture, except with 6 layers and a\nmuch smaller vocab size.\nPretrain data and task. The pretraining data consists of 5000 sequences (documents) generated from\nthe HMM, each with length 10240. We pretrain on this data by doing 5% masked LM on chunks of length\n512. Pretraining runs for 3 epochs and takes about 5 hours on a single NVIDIA Tesla K80 GPU on 16-bit\nprecision. We use an internal cluster for all experiments. Pretraining uses batch size 8 and learning rate 1e-5\nwith a linear warmup of 500 steps and linear decay schedule after 500 steps. We generated 20 pretraining\n(and downstream) datasets for each problem instance and average over the 20 runs in the vanilla HMM\ncomparison, while the memory-based distributions are run for 5 trials of pretraining and ﬁnetuning.\nDownstream. The downstream task samples a sparse ground truth linear weightµwith 6 nonzero elements.\nPositions for nonzero entries are sampled uniformly at random and values are sampled i.i.d. from a standard\nnormal distribution. Although we do binary classiﬁcation, we sampleµ with 2 rows and take the label to be\nthe argmax of the two scores, instead of having 1 row and taking the sign. We ﬁnd that this results in less\ndegenerate datasets (datasets where all labels are the same).\nWe generate 5000 training, 500 validation and 1000 test examples for the downstream tasks. Downstream\ntraining uses learning rate 0.01 for both prompt tuning and head tuning, with a linear warmup/decay schedule,\nfor 5 epochs over the downstream data. We take the model returned at the last checkpoint as the result\n(no early stopping). We found that it was important to train prompt tuning with full precision, since the\ngradients are relatively small and become zero with discretization.\nWe used message passing in the HMM to compute the posterior distributions of the latent variables analytically.\nPrompt tuning. We prepended a length 20 continuous prompt to each sequence of input word embed-\ndings. We initialize elements of the prompt vectors IID from the uniform distribution onr´0.5,0.5s. Our\nimplementation for prompt tuning used the code of [20], available athttps://github.com/kipgparker/\nsoft-prompt-tuning.\n29",
  "topic": "Hidden Markov model",
  "concepts": [
    {
      "name": "Hidden Markov model",
      "score": 0.8054415583610535
    },
    {
      "name": "Computer science",
      "score": 0.7735810279846191
    },
    {
      "name": "Generative model",
      "score": 0.6350657343864441
    },
    {
      "name": "Classifier (UML)",
      "score": 0.622653603553772
    },
    {
      "name": "Generative grammar",
      "score": 0.5692524909973145
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5614968538284302
    },
    {
      "name": "Language model",
      "score": 0.5093683004379272
    },
    {
      "name": "Task (project management)",
      "score": 0.5015654563903809
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.44069892168045044
    },
    {
      "name": "Machine learning",
      "score": 0.4317089021205902
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3508765399456024
    },
    {
      "name": "Speech recognition",
      "score": 0.33803147077560425
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}