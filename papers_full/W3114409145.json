{
    "title": "Cloze Distillation: Improving Neural Language Models with Human Next-Word Prediction",
    "url": "https://openalex.org/W3114409145",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2941804323",
            "name": "Tiwalayo Eisape",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2232124804",
            "name": "Noga Zaslavsky",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences",
                "Massachusetts Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1982032445",
            "name": "Roger Lévy",
            "affiliations": [
                "Institute of Cognitive and Brain Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2022878472",
        "https://openalex.org/W3034510440",
        "https://openalex.org/W1979900603",
        "https://openalex.org/W4255690937",
        "https://openalex.org/W2134797427",
        "https://openalex.org/W2921890305",
        "https://openalex.org/W2134273450",
        "https://openalex.org/W3117738520",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2119728020",
        "https://openalex.org/W3034578688",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2616278914",
        "https://openalex.org/W2108010971",
        "https://openalex.org/W1996359725",
        "https://openalex.org/W2942054564",
        "https://openalex.org/W2030330674",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2962941914",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2013112874",
        "https://openalex.org/W2020944885",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2949629417",
        "https://openalex.org/W2153791616",
        "https://openalex.org/W3033254023",
        "https://openalex.org/W2118276816",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2041221140",
        "https://openalex.org/W2139450036",
        "https://openalex.org/W3037115370",
        "https://openalex.org/W2470606929",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3159258320",
        "https://openalex.org/W2795342569",
        "https://openalex.org/W2140475149",
        "https://openalex.org/W3027353876",
        "https://openalex.org/W2034041981",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2986940265",
        "https://openalex.org/W3045555480",
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2549835527",
        "https://openalex.org/W2151073408",
        "https://openalex.org/W2168274916",
        "https://openalex.org/W2130987742",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3042795397",
        "https://openalex.org/W2083758911",
        "https://openalex.org/W2000387713",
        "https://openalex.org/W2054125330",
        "https://openalex.org/W2578785875",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W3100748148",
        "https://openalex.org/W2294370754"
    ],
    "abstract": "Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.",
    "full_text": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 609–619\nOnline, November 19-20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n609\nCloze Distillation: Improving Neural Language Models with Human\nNext-Word Predictions\nTiwalayo N. Eisape1 Noga Zaslavsky1,2\n1Department of Brain and Cognitive Sciences, 2Center for Brains Minds and Machines\nMassachusetts Institute of Technology\n{eisape, nogazs, rplevy}@mit.edu\nRoger P. Levy1\nAbstract\nContemporary autoregressive language mod-\nels (LMs) trained purely on corpus data have\nbeen shown to capture numerous features of\nhuman incremental processing. However, past\nwork has also suggested dissociations between\ncorpus probabilities and human next-word pre-\ndictions. Here we evaluate several state-of-the-\nart language models for their match to human\nnext-word predictions and to reading time be-\nhavior from eye movements. We then propose\na novel method for distilling the linguistic in-\nformation implicit in human linguistic predic-\ntions into pre-trained LMs: Cloze Distillation.\nWe apply this method to a baseline neural LM\nand show potential improvement in reading\ntime prediction and generalization to held-out\nhuman cloze data.\n1 Introduction\nModern language models (LMs) demonstrate out-\nstanding general-purpose command over language.\nThe majority of these models acquire language by\nmaximizing the in-context probability of each word\nin their training corpus (Figure 1), typically with\na self-supervised objective. This simple corpus\nprobability matching has resulted in models that\nlearn impressive powers of both psychometric pre-\ndiction (Frank and Bod, 2011; Fossum and Levy,\n2012; Frank et al., 2015; Goodkind and Bicknell,\n2018; Hale et al., 2018; van Schijndel and Linzen,\n2018; Warstadt and Bowman, 2020; Wilcox et al.,\n2020) and language more generally (Devlin et al.,\n2019; Radford et al., 2019).\nIn humans, prediction may underlie both learn-\ning (Kuhl, 2004; Huang and Snedeker, 2013) and\nprocessing (Ryskin et al., 2020; Levy, 2008; Clark,\n2013). Human linguistic prediction can be under-\nstood as not only lexical but also as taking place\nboth above and below the word level (Federmeier\nand Kutas, 1999; Federmeier et al., 2002); parallel,\ni.e., predictive commitments are maintained over\nseveral linguistic units at once (Levy, 2008); and\ngraded, i.e., commitment is licensed to varying de-\ngrees based on features of the linguistic unit being\npredicted. Rather than placing bets (Jackendoff,\n1987) on which single word will come next, hu-\nmans make many diffuse bets at multiple linguistic\nlevels (e.g., syntactic, orthographic, lexical, etc.).\nSurprisal theory (Hale, 2001; Levy, 2008) de-\nscribes the utility of the approach taken by the\nhuman language processor, as lexical prediction\nis often an ill-constrained classiﬁcation problem\n— for agents with very large vocabularies (LMs,\nhumans), context is often not sufﬁciently constrain-\ning for high accuracy multiple, thousand-way clas-\nsiﬁcation decisions, but is typically constraining\nenough to accurately infer next-word features (such\nas part of speech, and semantic category). A large\nbody of evidence demonstrates that these graded\nnext-word predictions are reﬂected in human pro-\ncessing times (Ehrlich and Rayner, 1981; Demberg\nand Keller, 2008; Smith and Levy, 2013; Luke and\nChristianson, 2016) as well as neural responses\n(Kutas and Hillyard, 1980; Frank et al., 2015).\nCorpus data are (imperfect) samples from the\nlinguistic environment of a native speaker, and psy-\ncholinguistic data indicate that accurate prediction\nis important to efﬁcient language comprehension.\nUnder the principle of rational analysis (Ander-\nson, 1990), it is thus to be expected that artiﬁcial\nlanguage models trained on corpus data would cor-\nrelate with human linguistic predictions and thus\nhave good psychometric predictive accuracy. Nev-\nertheless, past work (Smith and Levy, 2011) has\nsuggested dissociations between corpus probabil-\nities and human next-word estimates. Here, we\nfurther investigate this relationship using artiﬁcial\nlanguage models and the most extensive corpus\nof sequential cloze completions that we are aware\nof: the Provo Corpus (Provo henceforth; Luke and\n610\nChristianson, 2018).\nFirst, we use Provo to test the psychometric\nperformance of three state-of-the-art Transformer-\nbased (Vaswani et al., 2017) LMs — XLNet (Yang\net al., 2019), Transformer-XL (Dai et al., 2019),\nand GPT-2 (Radford et al., 2019) — alongside a\nsmaller 2-layer LSTM (Hochreiter and Schmidhu-\nber, 1997) trained on wikitext-103 (Merity et al.,\n2016), and a 5-gram LM baseline (Stolcke, 2002).\nWe ﬁnd that, while the Transformer models achieve\nthe lowest perplexity on Provo and the best ﬁt to the\ncloze data, the LSTM model provides the best ac-\ncount of reading times in terms of raw correlation.\nThese ﬁndings show a dissociation between recapit-\nulating corpus statistics and mimicking human lan-\nguage processing, operationalized here with read-\ning times. That is, models that minimize perplexity\non next-word prediction do not necessarily pro-\nvide the best account of reading times. Second,\nbased on these ﬁndings, we propose Cloze Dis-\ntillation: a novel method for distilling linguistic\ninformation implicit in human cloze completions\ninto pre-trained LMs. We apply this method to the\nLSTM model and show substantial improvement\nin reading time prediction and word frequency es-\ntimation, in addition to generalization to held-out\nhuman cloze data.\n2 Human Cloze Predictions\nThe objective for most modern LMs is to compute\na probability distribution over the model’s vocabu-\nlary V for the likely next-word x∈V at position i\ngiven the context x<i consisting of the sequence\nof preceding words in the document. Similarly, as\nhumans process language, they make constant and\nimplicit linguistic predictions.\nOne commonly used measure of these predic-\ntions in humans is the Cloze task. In its original\nform (Taylor, 1953), the task involved masking a\nword or words in a source text passage and asking\nparticipants to provide words for the masked ele-\nments that would make the passage “whole again”,\na task structure adopted by contemporary masked\nlanguage models (Devlin et al., 2019). In experi-\nmental psycholinguistics, however, the most com-\nmon version of the Cloze task has involved pre-\nsenting the beginning, or preﬁx, of a passage and\nhaving participants either complete it or provide\nthe word that they think comes next (Figure 1), a\ntask more closely matching that of autoregressive\nlanguage models (Radford et al., 2019). In this\npaper, we focus on this latter type of Cloze task,\nwhich elicits samples from comprehenders’ subjec-\ntive next-word probability distributions (DeLong\net al., 2005; Staub et al., 2015). For any given\npreﬁx, we can estimate the cloze distribution of a\ntypical native speaker from pooled cloze responses\nacross a large number of participants (Luke and\nChristianson, 2018), similar to how the fundamen-\ntal output of an autoregressive language model is a\nvector of next-word probabilities.\n2.1 The Provo Corpus\nWe use the Provo Corpus (Luke and Christianson,\n2018) as our source of paired cloze completion\nand reading time data. The Provo Corpus derives\nfrom 55 paragraphs of text taken from sources in-\ncluding online news articles, popular science, and\nﬁction. For each paragraph p, next-word cloze\ncompletions were elicited for each preﬁx x<i for\ni = 2,... |p|(2,689 sentence preﬁxes total). Pre-\nﬁxes were presented to participants (N = 470) as a\ncontinuous multi-line text (Figure 1). This resulted\nin an average of 40 cloze responses with 15 unique\ncontinuations per preﬁx.\nAdditionally, Luke and Christianson (2018) col-\nlected eye movement data from eighty-four native\nspeakers of American English as they read these 55\ntext passages, using a high-resolution SR Research\nEyeLink 1000 eye tracker.\nThe Provo cloze data, eye movement data, and\nthe relationship between them are analyzed in de-\ntail in (Luke and Christianson, 2016). Luke and\nChristianson (2016) point out that while context\nis rarely constraining enough to facilitate exact\nnext-word prediction, modal cloze responses of-\nten constitute partial matches to the target words.\nFor example, given the preﬁx With schools still\nclosed, cars still buried and streets still ... , the\ntrue continuation, blocked, has a cloze probability\nof only 0.07. But the overwhelming majority of\ncloze responses are partial ﬁts to the correct word:\n79% of the responses are verbs, and 72% are in-\nﬂectional matches (ended with -ed), with the two\nmost frequent responses being closed and covered\n(example from Luke and Christianson, 2018). In\naddition, they showed that cloze probabilities are\nhighly predictive of reading times, adding to prior\nwork showing a word’s reading time is a function of\nits predictability in context (e.g., Smith and Levy,\n2013).\n611\n…\nLSTM\nLSTM\nmodelscurrentbestscience’s\n0.1\n0.2\n0.3\n0.4\nof\npredictsuggest\nwould\ncan\nexplain\nshowhaveare\n0.025\n0.05\n0.075\n0.1\nof\npredictsuggest\nwould\ncan\nexplain\nshowhaveare\n0.25\n0.5\n0.75\n1\nof\npredictsuggest\nwould\ncan\nexplain\nshowhaveare\nPlease type the word  \nyou think will come \nnext.\nCloze task\n…\nLSTM\nLSTM\n…\nLSTM\nLSTM\n…\nLSTM\nLSTM\nPre-trained LM\nCorpus\nLM Predictions\nHuman Predictions\nGround Truth\nα· Di\n(1-α)· Si\n+Language Modeling\nCloze Prediction\nFigure 1: Illustration of the Cloze task and the Cloze Distillation objective. Given one of Provo’s preﬁxes — in\nthis example, one that ends in . . . science’s best current models, where the true next word (ground truth) is predict\n— human subjects were prompted, as shown in the Cloze task box, to predict the word they thought was likely\nto follow. The Cloze Distillation loss is constructed by combining (1) the KL divergence Di between the human\ncloze distribution and the LM’s next-word distribution, and (2) the LM’s predicted surprisal Si of the true next\nword given the preﬁx.\n3 Testing Language Models on Provo\nThe ﬁndings of Luke and Christianson (2016) high-\nlight cloze as a useful test-bed for LMs. Specif-\nically, a LM that employs predictions similar to\nthose that underlie human language processing is\nexpected to be a good model of human cloze re-\nsponses. Therefore, we evaluate here a suite of\nLMs on their ability to match human cloze distri-\nbutions. Additionally, we use the LMs’ ability to\npredict reading times as a second measure of ﬁt\nto human expectations, extending past work us-\ning LMs to predict reading times (Frank and Bod,\n2011; Wilcox et al., 2020).\n3.1 Models\nWe consider in our analysis the following LMs:\n1. 5-gram: N-gram model using a window size\nof 5 with Kneser-Ney smoothing, obtained via\nthe SRILM language modeling toolkit (Stol-\ncke, 2002).\n2. LSTM: A standard 2-layer LSTM RNN im-\nplemented in PyTorch (Paszke et al., 2017),\nused here with 256 hidden units and word\nembedding size of 256, and trained on the\nwikitext-103 corpus (Merity et al., 2016) via\na next-word prediction task (40 epochs, batch\nsize = 40, learning rate = 20).\n3. GPT-2: A Transformer-based LM trained on\nthe WebText corpus (Radford et al., 2019).\n4. Transformer-XL(TXL; Dai et al., 2019): A\nTransformer-based LM with a segment level\nrecurrence mechanism and relative positional\nembeddings trained on wikitext-103.\n5. XLNet (Yang et al., 2019): A Transformer-\nbased LM trained with a permutation lan-\nguage modeling objective as well as a segment\nlevel recurrence mechanism and relative posi-\ntional embeddings. Training data consists of\n∼30 billion tokens across 6 different copora.\nWe use the LMzoo python package (Gauthier\net al., 2020) to access the 5-gram model, and\nthe HuggingFace transformers python pack-\nage (Wolf et al., 2019) for accessing Transformer\nmodels (gpt2-large, transfo-xl-wt103,\nand xlnet-large-cased respectively). These\nTransformer models use subword tokens (Sennrich\net al., 2016); we deﬁned word probabilities for\nthese models as the joint probability of the subword\ntokens comprising the word given the context.\n612\nModel ⟨Di⟩ ⟨ τi⟩ ⟨ Si⟩ Fintr Fbase ρgaze ρfreq\nCloze NA NA 3.99 ±2.60 198 .10 30 .90 0 .36 −0.43\nGPT-2 2.30 ±1.57 −0.57 ±0.004 6 .11 ±5.00 252 .70 46 .11 0 .40 −0.46\nXLNet 2.39 ±1.68 −0.58 ±0.005 6 .39 ±5.70 260 .50 46 .08 0 .41 −0.48\nTXL 3.27 ±1.92 −0.47 ±0.005 8 .09 ±5.50 238 .30 30 .54 0 .39 −0.50\nLSTM 3.74 ±1.86 −0.39 ±0.006 8 .58 ±4.90 361 .20 41 .47 0 .47 −0.63\n5-gram 3.89 ±1.84 −0.20 ±0.007 12 .48 ±7.00 161 .00 16 .72 0 .31 −0.41\nTable 1: Evaluation of LMs on Provo reveals a dissociation between performance on next-word prediction and\npsychometric measures that reﬂect human language processing. Fintr and Fbase show the F-test statistics (Section\n3.2.2) against various baseline predictors. ρgaze and ρfreq show correlation with gaze and frequency respectively\n(Pearson’sρ). ⟨Di⟩is average KL-divergence between the empirical cloze distribution and the LM’s distributions;\n⟨τi⟩is rank correlation between down-sampled model surprisals and surprisal values based on the empirical cloze\nprobabilities; ⟨Si⟩is average surprisal over the text in Provo; all standard deviations are computed by paragraph.\n3.2 Metrics\nWe use several metrics to evaluate the ﬁt of our\nmodels to human reading times and cloze re-\nsponses. We discuss and motivate them in the\nfollowing section.\n3.2.1 Cloze Responses\nWe use two measures to evaluate the performance\nof each model on human cloze data. First, we\nmeasure the deviation between the empirically es-\ntimated cloze distribution, Pcloze(x|x<i), where\nx is a potential next-word at position i in a doc-\nument1 and the model’s next-word distribution,\nPmodel(x|x<i), using the Kullback-Leibler (KL) di-\nvergence:\nDi ≡D[Pcloze(x|x<i)∥Pmodel(x|x<i)] (1)\n=\n∑\nx∈V\nPcloze(x|x<i) logPcloze(x|x<i)\nPmodel(x|x<i) .\nWhile the KL divergence is a natural measure for\ncomparing distributions, it is potentially limited for\nour purposes due to the sparsity of the cloze data.\nTo address this, we also consider Kendall’s Tau\ncorrelation coefﬁcient, which may be more robust\nto estimation errors resulting from small sample\neffects. Speciﬁcally, we consider Kendall’s Tau\ncorrelation between LM surprisals and surprisals\nestimated form human cloze data, denoted here by\nτi ≡τ[Pcloze(x|x<i),Pmodel(x|x<i)].\nTo further evaluate the models’ ability to mimic\ncloze responses and to control for the sparsity of\nthe human cloze data, we simulated a cloze task\n1As participants in Luke and Christianson (2018) were\ngiven only within-paragraph context when prompted for each\ncloze response, each paragraph constitutes a unique document\nin our analysis.\nexperiment with our LMs. For each LM, we gener-\nated 40 cloze responses2 per preﬁx x<i in Provo by\nsampling from Pmodel(x|x<i). We repeated this ex-\nperiment 50 times for each model. The results were\nsimilar in both the down-sampling and without-\ndown-sampling conditions, and we report only the\ndown-sampling condition in Table 1.\n3.2.2 Reading Times\nWe use gaze duration during ﬁrst-pass reading as\nour measure of reading times, which is the amount\nof time a reader’s eyes spend on a word the ﬁrst\ntime they ﬁxate it (Rayner, 1998; if a reader ﬁxates\na word to the right before ﬁxating the word in ques-\ntion, the word has been “skipped” and there is no\nvalid gaze duration). It is well established that gaze\nduration captures a wide variety of cognitive pro-\ncesses during real-time language-comprehension,\nincluding the relationship between a word and the\ncontext in which it appears (Staub, 2011).\nWe evaluate the ability of a LM to account for\nhuman reading times based on their predicted sur-\nprisal values,\nSi ≡−log2 Pmodel(xi|x<i) , (2)\nas it has been previously shown to capture sev-\neral characteristics of human language compre-\nhension and pattern with reading times (Smith\nand Levy, 2013; Wilcox et al., 2020). Similarly,\nwe deﬁne cloze surprisals by taking the nega-\ntive log of the empirical cloze probabilities3, i.e.,\n2We generated 40 responses because most preﬁxes in\nProvo had at least 40 responses provided by participants.\n3We use the cloze probability estimates from Luke and\nChristianson (2018)’s ‘Orthographic Match Model’ – a logit\nmixed-effects model including only random by-word inter-\ncepts. These estimates are nearly perfectly correlated with the\nrelative frequency estimate of cloze (ρ= .999), but crucially\n613\n−log2 Pcloze(xi|x<i). We then measure Pearson’s\ncorrelation ρbetween reading times and surprisal\nvalues. In addition, we use ANOV A tests to mea-\nsure the models’ predictive capacities beyond stan-\ndard baseline predictors of reading time (Howes\nand Solomon, 1951; Kliegl et al., 2006; Leyland\net al., 2013) — log word frequency and word length.\nThat is, for each model (either an LM or the cloze\ndistribution), we enter its surprisal values into a\nlinear mixed-effects model (LME) along with the\nbaseline predictors, and measure their contribution\nby computing the F-test statistic between the full\nLME and an LME where model surprisals are ab-\nlated out. In the case of Fbase the baseline predic-\ntors were frequency, length, and their interaction.\nIn the case of Fintr the baseline predictors were\nsimply random by-word intercepts. We use both\nword frequencies estimated from the Corpus of\nContemporary American English (COCA; Davies,\n2010) and from wikitext-103 (Merity et al., 2016)\nin our analysis. As the results of our analyses were\nqualitatively the same in both conditions we report\nonly results from COCA in the analyses to follow.\n3.3 Results\nThe main results of evaluating the LMs on Provo\nare summarized in Table 1. First, averaging the\nKL divergence and suprisals values over word po-\nsitions i in Provo (that is, ⟨Di⟩and ⟨Si⟩respec-\ntively), shows that the ability of LMs to predict\nhuman cloze responses tracks with their language\nmodeling performance. This pattern is also re-\nﬂected in Kendall’s τ correlation between model\nsurprisals and surprisals constructed from the hu-\nman cloze distribution. At the same time, Table 1\nreveals a dissociation between next-word predic-\ntion, reﬂected by ⟨Si⟩, and human language pro-\ncessing, as reﬂected in reading times. Speciﬁcally,\nthe LSTM model, which does not perform as well\nas the Transformer-based LMs in next-word pre-\ndiction on Provo, as reﬂected in its higher ⟨Si⟩, ex-\nhibits superior ability in predicting reading times,\nas measured in ρgaze and Fintr. This result is simi-\nlar to that of Merkx and Frank (2020), who found\nthat Gated Recurrent Unit networks outperformed\nTransformer models with lower perplexity in pre-\ndicting gaze duration.\nWe note that when predicting reading times not\nonly from the model’s surprisal values, but also\nusing the baseline predictors (word frequency and\ndo not include cloze probabilities of zero (which would yield\ninﬁnite surprisal).\nlength), the LSTM model no longer outperforms\nthe Transformer-based models (Table 1, Fbase).\nNonetheless, it is striking that the LSTM model,\nwhich is much smaller than the Transformer-based\nmodels and was trained on much less data, achieves\nthe best performance in predicting reading times\nwithout the baseline predictors.\n3.4 Intermediate Conclusions\nPast work shows that human predictions systemati-\ncally diverge from corpus probabilities (Smith and\nLevy, 2011). Our analysis extends these ﬁndings\nby testing current state-of-the-art LMs trained on\nmuch larger datasets, and showing that, while better\nestimates of corpus probabilities may yield better\nmodels of human next-word predictions, there does\nnot seem to be a strict positive correlation between\nthe ability to approximate corpus probabilities and\nthe ability to predict human reading times, as ev-\nidenced by models with higher ⟨Si⟩being on-par\nand even better at predicting reading times com-\npared to models with lower ⟨Si⟩.\nRecent studies (Ettinger, 2020; Hao et al., 2020;\nJacobs and McCarthy, 2020) have found similar\ntrends when comparing LMs to cloze data. Hu\net al. (2020) also found only a loose relationship\nbetween perplexity (a monotonic function of ⟨Si⟩)\nand syntactic generalization, adding to a grow-\ning body of evidence suggesting that while opti-\nmizing for corpus probabilities can create some-\nwhat psycholinguistically-enabled language mod-\nels (Linzen et al., 2016; Futrell et al., 2019; Hu\net al., 2020), there may be a dissociation between\ncorpus probabilities and human expectations.\n4 Cloze Distillation\nHere, we show how to leverage these ﬁndings to\nimprove the ability of LMs to match human expec-\ntations, providing more appealing neural language\nmodels for human language processing. To this end,\nwe propose Cloze Distillation: a method for using\nhuman next-word predictions as learning targets\ntogether with corpus statistics within a knowledge\ndistillation framework.\n4.1 Knowledge Distillation\nKnowledge distillation (Buciluundeﬁned et al.,\n2006; Ba and Caruana, 2014; Hinton et al., 2015) is\na technique of imbuing knowledge from a teacher\nmodel into a student model by training the student\nto make the same predictions as the teacher. Typ-\n614\nically deployed as a form of model compression,\nknowledge distillation is useful for those looking to\ndeploy insights from one or more complicated mod-\nels into a single smaller model. Recently, knowl-\nedge distillation has also proven useful to cogni-\ntive scientists in creating low-dimensional neural\nnetwork cognitive models (Schaeffer et al., 2020).\nWhen humans are used as the ‘teacher’ this can be\nseen as a speciﬁc case of a more general cognitive\nmodeling strategy, task-based modeling.\n4.2 The Cloze Distillation Objective\nKnowledge distillation has proven its usefulness\nin NLP where researchers have distilled knowl-\nedge from very large and/or syntactically aware\nlanguage models into naive models showing it is\npossible to transfer even subtle linguistic prefer-\nences from teacher to student (Kim and Rush, 2016;\nKuncoro et al., 2019; Sanh et al., 2020; Kuncoro\net al., 2020).\nWe take inspiration from this work and lever-\nage the general framework both as a method for\ndistilling knowledge from a ‘teacher’ with desir-\nable linguistic biases (humans in our case) and as\na tool for cognitive modeling by using empirical\ncloze distributions Pcloze as target distributions in a\nknowledge distillation framework.\nWe follow this approach to arrive at the follow-\ning loss function for Cloze Distillation (CD):\nLi = αDi −(1 −α)Si . (3)\nThat is, for each context x<i we compute the CD\nloss by linearly interpolatingDi, the KL divergence\nbetween the distributions of the human teacher and\nthe student model as deﬁned in equation (1), with\nan autoregressive language modeling objective that\nplaces unit probability mass on the true next-word,\nformally deﬁned by Si in equation (2). Thus, CD\nﬁne-tunes LMs to predict the next word in the docu-\nment while simultaneously producing a distribution\nover next-words that mirrors the empirical human\ncloze distribution for that context. This process is\nillustrated in Figure 1.\nTo evaluate the utility of the human cloze data,\nwe vary the values of αfrom α = 0, which cor-\nresponds to pure next-word prediction driven ﬁne-\ntuning, to α= 1, which corresponds to pure cloze-\nprediction based ﬁne-tuning.\n4.3 Cloze-Distilled LSTM\nTo begin to evaluate the CD paradigm, we apply\nit to the LSTM from Section 3 by ﬁne-tuning this\nmodel using the CD objective over Provo. To test\ngeneralization and utilize the full corpus, we use\na k-fold cross-validation scheme with k= 55, the\nnumber of paragraphs in Provo where humans are\nprovided the full preceding paragraph as context.\nThat is, each fold consists of data from one para-\ngraph in the Provo dataset. We use 100 epochs for\ntraining. We provide our LM with the same con-\ntext as humans, up to the beginning of the current\nparagraph.\nAdditionally, we vary αto test the utility of our\ncloze data and cross-validated separately for each\nvalue of αin the range [0,1], sampled at intervals\nof 0.05. This resulted in 1,155 unique models for\ntesting. We wish to emphasize that even utilizing\nthe entire Provo corpus via cross-validation, we\nare left with only 2685 training samples, which is\nminuscule with respect to the model’s pre-training\ndata (roughly 100 million samples). We refer to\nthe resultant model as cloze-distilled LSTM (CD-\nLSTM).\n4.4 Results\nAfter ﬁne-tuning on the CD objective, we note sev-\neral interesting adaptions in model behavior. These\nmainly include signiﬁcant improvement over the\nstandard LSTM baseline in predicting human read-\ning times and cloze distributions (Figure 2). We\nalso discuss improvements in next-word prediction\nperformance over Provo (Figure 3).\n4.4.1 Reading times\nPsychometric predictive capacity is starkly im-\nproved with Cloze Distillation, and the strength\nof the effect scales with α. This can be seen in\nFigure 2, which shows the statistical comparison\nof the CD-LSTM for varying levels of α. We add\nanother model comparison designed to isolate the\nability of CD-LSTM to predict reading times above\nthe standard LSTM (Figure 2a). Speciﬁcally, we\nenter CD-LSTM’s surprisals into an LME along\nwith baseline predictors and surprisals from the\nstandard LSTM and compute the F-test statistic\nagainst a LME with CD-LSTM surprisal ablated\nout.\nCD-LSTM exhibits a signiﬁcant improvement\nwith αin its ability to predict reading times above\nthe non-ﬁne-tuned model (Figure 2a), as well as\n615\n(a) CD-LSTM vs. LSTM\n0.0 0.25 0.5 0.75 1\n0\n5\n10\n15\n20\nF\nns (b) CD-LSTM vs. average RT\n0.0 0.25 0.5 0.75 1\n300\n320\n340\n360\n380\nF (c) CD-LSTM+baseline vs. baseline\n0.0 0.25 0.5 0.75 1\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\nF\n(d) Reading time\n0.0 0.25 0.5 0.75 1\n0.44\n0.45\n0.46\n0.47\n0.48\n (e) Word frequency\n0.0 0.25 0.5 0.75 1\n0.66\n0.64\n0.62\n0.60\n (f) Cloze correlation\n0.0 0.25 0.5 0.75 1\n0.410\n0.415\n0.420\n0.425\nFigure 2: Results for CD-LSTM and the LSTM model (without any ﬁne tuning) show that Cloze Distillation yields\nsubstantial improvement across several psychometric measures. Panels (a)-(c) show changes in F statistics as a\nfunction of αfor three LME comparisons, and panels (d)-(f) show changes in three correlational measures. Dashed\nlines in panels (b)-(f) show the performance of the LSTM model. (a) LME based on CD-LSTM’s surprisals outper-\nforms the LME based on the LSTM’s surprisals for most values ofα(not signiﬁcant for α< 0.65). (b) LME based\non CD-LSTM’s surprisals outperforms the null (intercept only) model, and this performance generally improves\nwith α. (c) LME based on CD-LSTM’s surprisals with the baseline factors (word frequency and length) outper-\nforms the baseline-only LME for several values of α. (d) Pearson’s correlation between CD-LSTM’s surprisals\nand reading times. (e) Pearson’s correlation between CD-LSTM’s surprisals and word frequencies. (f) Kendall’sτ\ncorrelation between CD-LSTM’s surprisals and human cloze surprisals.\nimprovements over an intercept-only model (Fig-\nure 2b) and baseline-only (Figure 2c). Correlation\nwith reading time and CD-LSTM’s surprisal also\nsteadily increases with α(Figure 2d). These ﬁnd-\nings suggest that, as we postulate, Cloze Distilla-\ntion is a useful paradigm for extracting the infor-\nmation about human linguistic expectations that is\nimplicit in human cloze predictions and incorporat-\ning it into LMs.\n4.4.2 Cloze\nWe report improvements in predicting held out\ncloze data, where ⟨Di⟩is decreased from 3.8 (at\nα = 0) to 3.6 (at α = 0.65) (Figure 3). τ corre-\nlation also exceeds that of the baseline model for\nseveral values of α(though there does not seem to\nbe a consistent trend across α-s).\nThis result is intriguing as it implies that the req-\nuisite information for computing cloze distributions\nis learned over ﬁne-tuning. Furthermore, we see a\npeak at α= 0.65 and not at α= 1, which suggests\nthat in training LMs to predict cloze data, some\nsignal from next-word prediction remains vital.\n4.4.3 Language modeling\nIn addition to improved performance on our hu-\nman language processing benchmarks, we see a\nrobust increase in language modeling performance\nfor most values of α, as evidenced by average sur-\nprisal over Provo (Figure 3). We note, the standard\ndeviation in ⟨Si⟩for our LSTM over Provo was\n1.86 bits (Table 1). The improvements we see are\nless than this deviation, and are thusly below the\nlevel of signiﬁcance, though we do see a consistent\ntrend in α. This effect is most substantial for inter-\nmediate values of α, suggesting that a combination\nof human knowledge and next-word prediction im-\nproves relative to either one of these factors on\nits own. This indicates that both parts of the loss\nfunction (ground truth next-words, human cloze)\nprovide useful information for predicting text that\nis not entirely overlapping.\nThis is interesting given the low ⟨Si⟩of human\ncloze data. The fact that humans can contend with\nlarge language models trained explicitly on next-\nword prediction even on subsets of text, together\nwith our Cloze Distillation results suggests there\nis linguistic information in human cloze that can\nbe harnessed by LMs to subserve general language\n616\n0.0 0.25 0.5 0.75 1\n8.45\n8.50\n8.55\n8.60\n8.65Surpisal\n3.70\n3.75\n3.80\nKL Divergence\nFigure 3: Average Surprisal (left) and KL divergence\n(right) over Provo as a function of the distillation inter-\npolation coefﬁcient α. Dashed lines show LSTM per-\nformance before ﬁne-tuning.\nmodeling and is disjoint from the information ac-\ncessible in corpus probability (Smith and Levy,\n2011).\n4.4.4 Frequency\nWe also note that as αincreases, the CD-LSTM\nnext-word predictions exhibit increased correlation\nwith frequency (Figure 2e), suggesting that cloze\ndistilled LMs may learn to better predict frequent\nwords. This is interesting as a proof of concept\nthat Cloze Distillation distills information implicit\nin cloze into language models as previous work\n(Smith and Levy, 2011) has shown human cloze\nis skewed toward more frequent words, relative to\ncorpus probability.\n5 Conclusion\nOur analyses provide further evidence of a mis-\nalignment between language model estimates and\nhuman expectations. The method we provide:\nCloze Distillation, demonstrates that shifting train-\ning incentives away from corpus probability toward\npsycholinguistic task-based modeling can result in\nbetter cognitive models and better language models.\nStill, given several of our models predict reading\ntimes beyond the cloze data collected in Provo (Ta-\nble 1) there are several possible explanations for\nthe effect Cloze Distillation has on language model\nperformance.\nOne is that the Cloze task produces data that\nare a more faithful reﬂection of the expectations\ndeployed in human reading and are thus able to\nguide the models toward a fundamentally more\nhuman-like set of expectations – despite being\nunder-sampled. If this is true and human subjec-\ntive next-word estimates also provide signal about\nnext-word probabilities across corpora (reﬂecting\nthe implicit knowledge speakers have learned about\nthe statistics of their language), this would explain\nwhy Cloze Distillation improves next-word predic-\ntion accuracy on a new corpus (Provo).\nAnother possibility is that the models we sur-\nvey are fundamentally better than the cloze data\nat capturing the human expectations deployed in\nreading. Though this would not explain the boost\nin performance we see in reading time prediction\nwith Cloze Distillation, because several of our mod-\nels predict reading times better than the cloze data\nitself, this can not yet be ruled out. We leave the\nfurther exploration of this to future work as larger-\nscale collection of human cloze data allows.\nThat said, the fact that we were able to induce ap-\npreciable adaptions in model behavior with such lit-\ntle data highlights the richly orienting information\navailable in even noisy human predictions. Though\nit is unclear how language users learn to make such\nsophisticated predictions (we provided this infor-\nmation to our model with direct supervision), our\nmodel’s ability to learn from such small scale data\nhighlights the potential utility of such predictions\nin a language acquisition setting — it seems that hu-\nman predictions are strong enough to signiﬁcantly\nbolster the signal in raw linguistic input abetting\nextensive adaption from relatively little data.\nAs of now, the current dataset’s scale restricts\nCloze Distillation to use as a ﬁne-tuning method.\nFurthermore, we use simple LSTMs to perform a\ndetailed analysis of Cloze Distillation with dense\nsampling in αand thorough cross-validation. It is\npossible that deploying Cloze Distillation during\npre-training in large models (e.g., Transformers)\ncould result in models better able to learn the word\nfeatures humans demonstrate knowledge of in their\ncloze responses and we leave the exploration of\nthis to future work as well.\nMethods such as Cloze Distillation provide an\navenue forward for psycholinguists interested in\ntaking LMs seriously as candidate models of hu-\nman language processing and to natural language\nprocessing researchers interested in reverse engi-\nneering and deploying insights from human sen-\ntence processing. Cloze Distillation highlights\nthese goals as potentially mutually-reinforcing.\n617\n6 Acknowledgements\nTNE was supported by the GEM consortium and\nthe MIT Dean of Sciences Fellowship. NZ was sup-\nported by an MIT BCS Fellowship in Computation.\nRPL was supported by NSF grant IIS1815529, a\nGoogle Faculty Research Award, and a Newton\nBrain Science Award. We thank Robert Chen for\nhelping collect model surprisals, as well as Peng\nQian and Jon Gauthier for helpful discussions.\nReferences\nJohn R. Anderson. 1990. The Adaptive Character of\nHuman Thought. Hillsdale, NJ: Lawrence Erlbaum.\nJimmy Ba and Rich Caruana. 2014. Do Deep Nets Re-\nally Need to be Deep? In Advances in Neural Infor-\nmation Processing Systems 27, pages 2654–2662.\nCristian Buciluundeﬁned, Rich Caruana, and Alexan-\ndru Niculescu-Mizil. 2006. Model Compression.\nIn Proceedings of the 12th ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and\nData Mining, KDD, page 535–541, New York, NY ,\nUSA. Association for Computing Machinery.\nAndy Clark. 2013. Whatever Next? Predictive Brains,\nSituated Agents, and the Future of Cognitive Sci-\nence. Brain and Behavioral Sciences , 36(3):181–\n204.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nMark Davies. 2010. The Corpus of Contemporary\nAmerican English as the First Reliable Monitor Cor-\npus of English. Literary and Linguistic Computing,\n25(4):447–464.\nKatherine A DeLong, Thomas P Urbach, and Marta\nKutas. 2005. Probabilistic Word Pre-Activation\nDuring Language Comprehension Inferred from\nElectrical Brain Activity. Nature Neuroscience ,\n8(8):1117–1121.\nVera Demberg and Frank Keller. 2008. Data from\nEye-tracking Corpora as Evidence for Theories\nof Syntactic Processing Complexity. Cognition,\n109(2):193–210.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSusan F. Ehrlich and Keith Rayner. 1981. Contextual\nEffects on Word Perception and Eye Movements\nDuring Reading. Journal of Verbal Learning and\nVerbal Behavior, 20(6):641 – 655.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a New Suite of Psycholinguistic Diagnostics\nfor Language Models. Transactions of the Associa-\ntion for Computational Linguistics, 8:34–48.\nKara D Federmeier and Marta Kutas. 1999. A Rose\nby Any Other Name: Long-Term Memory Structure\nand Sentence Processing. Journal of Memory and\nLanguage, 41(4):469–495.\nKara D Federmeier, Devon B McLennan, Esmeralda\nDe Ochoa, and Marta Kutas. 2002. The Impact of\nSemantic Memory Organization and Sentence Con-\ntext Information on Spoken Language Processing by\nYounger and Older Adults: an ERP Study. Psy-\nchophysiology, 39(2):133–146.\nVictoria Fossum and Roger Levy. 2012. Sequential\nvs. Hierarchical Syntactic Models of Human Incre-\nmental Sentence Processing. In Proceedings of\nthe 3rd Workshop on Cognitive Modeling and Com-\nputational Linguistics (CMCL 2012) , pages 61–69,\nMontr´eal, Canada. Association for Computational\nLinguistics.\nStefan L Frank and Rens Bod. 2011. Insensitivity of\nthe Human Sentence-processing System to Hierar-\nchical Structure. Psychological Science, 22(6):829–\n834.\nStefan L Frank, Leun J Otten, Giulia Galli, and\nGabriella Vigliocco. 2015. The ERP Response to\nthe Amount of Information Conveyed by Words in\nSentences. Brain and Language, 140:1–11.\nRichard Futrell, Ethan Wilcox, Takashi Morita, Peng\nQian, Miguel Ballesteros, and Roger Levy. 2019.\nNeural Language Models as Psycholinguistic Sub-\njects: Representations of Syntactic State. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers) , pages 32–42, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An Online Plat-\nform for Targeted Evaluation of Language Models.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System\nDemonstrations, pages 70–76, Online. Association\nfor Computational Linguistics.\nAdam Goodkind and Klinton Bicknell. 2018. Predic-\ntive Power of Word Surprisal for Reading Times is\na Linear Function of Language Model Quality. In\n618\nProceedings of the 8th Workshop on Cognitive Mod-\neling and Computational Linguistics (CMCL 2018) ,\npages 10–18, Salt Lake City, Utah. Association for\nComputational Linguistics.\nJohn Hale. 2001. A Probabilistic Earley Parser as a\nPsycholinguistic Model. In Second Meeting of the\nNorth American Chapter of the Association for Com-\nputational Linguistics.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan Brennan. 2018. Finding Syntax in Human\nEncephalography with Beam Search. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2727–2736, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nYiding Hao, Simon Mendelsohn, Rachel Sterneck,\nRandi Martinez, and Robert Frank. 2020. Probabilis-\ntic Predictions of People Perusing: Evaluating Met-\nrics of Language Model Performance for Psycholin-\nguistic Modeling. arXiv preprint arXiv:2009.03954.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the Knowledge in a Keural Net-\nwork. In Deep Learning and Representation Learn-\ning Workshop at NuerIPS.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735–1780.\nD.H. Howes and R.L. Solomon. 1951. Visual duration\nthreshold as a function of word-probability. Journal\nof Experimental Psychology, 41(6):401—410.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A Systematic Assessment of\nSyntactic Generalization in Neural Language Mod-\nels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 1725–1744, Online. Association for Compu-\ntational Linguistics.\nYi Ting Huang and Jesse Snedeker. 2013. The Use of\nLexical and Referential Cues in Children’s Online\nInterpretation of Adjectives. Developmental Psy-\nchology, 49(6):1090–1102.\nRay Jackendoff. 1987. Consciousness and the Compu-\ntational Mind, volume 356. The MIT Press, Cam-\nbridge, MA.\nCassandra L. Jacobs and Arya D. McCarthy. 2020.\nThe Human Unlikeness of Neural Language Mod-\nels in Next-word Prediction. In Proceedings of the\nThe Fourth Widening Natural Language Processing\nWorkshop, page 115, Seattle, USA. Association for\nComputational Linguistics.\nYoon Kim and Alexander M. Rush. 2016. Sequence-\nLevel Knowledge Distillation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1317–1327, Austin,\nTexas. Association for Computational Linguistics.\nReinhold Kliegl, Antje Nuthmann, and Ralf Engbert.\n2006. Tracking the Mind During Reading: The In-\nﬂuence of Past, Present, and Future Words on Fix-\nation Durations. Journal of Experimental Psychol-\nogy, 135:12–35.\nPatricia K Kuhl. 2004. Early Language Acquisition:\nCracking the Speech Code. Nature Reviews Neuro-\nscience, 5(11):831–843.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable Syntax-\nAware Language Models Using Knowledge Distil-\nlation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3472–3484, Florence, Italy. Association\nfor Computational Linguistics.\nAdhiguna Kuncoro, Lingpeng Kong, Daniel Fried,\nDani Yogatama, Laura Rimell, Chris Dyer, and Phil\nBlunsom. 2020. Syntactic Structure Distillation Pre-\ntraining for Bidirectional Encoders. arXiv preprint\narXiv:2005.13482.\nMarta Kutas and Steven A. Hillyard. 1980. Reading\nsenseless sentences: Brain potentials reﬂect seman-\ntic incongruity. Science, 207(4427):203–205.\nRoger Levy. 2008. Expectation-based syntactic com-\nprehension. Cognition, 106(3):1126–1177.\nLouise-Ann Leyland, Julie A. Kirkby, Barbara J.\nJuhasz, Alexander Pollatsek, and Simon P. Liv-\nersedge. 2013. The Inﬂuence of Word Shading\nand Word Length on Eye Movements During Read-\ning. Quarterly Journal of Experimental Psychology,\n66(3):471–486. PMID: 21988376.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to Learn\nSyntax-Sensitive Dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nSteven G. Luke and Kiel Christianson. 2016. Limits\non Lexical Prediction During Reading. Cognitive\nPsychology, 88:22 – 60.\nSteven G. Luke and Kiel Christianson. 2018. The\nProvo Corpus: A Large Eye-Tracking Corpus with\nPredictability Norms. Behavior Research Methods,\n50(2):826–833.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer Sentinel Mixture\nModels. arXiv preprint arXiv:1609.07843.\nDanny Merkx and Stefan L Frank. 2020. Com-\nparing Transformers and RNNs on Predicting Hu-\nman Sentence Processing Data. arXiv preprint\narXiv:2005.09471.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming\nLin, Alban Desmaison, Luca Antiga, and Adam\n619\nLerer. 2017. Automatic Differentiation in Py-\nTorch. Neural Information Processing Systems Au-\ntodiff Workshop.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Ope-\nnAI Blog, 1(8):9.\nKeith Rayner. 1998. Eye Movements in Reading and\nInformation Processing: 20 Years of Research. Psy-\nchological Bulletin, 124(3):372–422.\nRachel Ryskin, Roger P Levy, and Evelina Fedorenko.\n2020. Do Domain-General Executive Resources\nPlay a Role in Linguistic Prediction? Re-evaluation\nof the Evidence and a Path Forward.Neuropsycholo-\ngia, 136:107258.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. DistilBert, a Distilled Ver-\nsion of BERT:Smaller, Faster, Cheaper and Lighter.\narXiv preprint arXiv:1910.01108.\nRylan Schaeffer, Mikail Khona, Leenoy Meshulam,\nand Ila Rani Fiete. 2020. Reverse-engineering Re-\ncurrent Neural Network Solutions to a Hierarchical\nInference Task for Mice. bioRxiv.\nMarten van Schijndel and Tal Linzen. 2018. Modeling\nGarden Path Effects without Explicit Hierarchical\nSyntax. In Proceedings of the 40th Annual Meeting\nof the Cognitive Science Society , pages 2600–2605,\nAustin, Texas. Cognitive Science.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nNathaniel Smith and Roger Levy. 2011. Cloze but no\nCigar: The Complex Relationship between Cloze,\nCorpus, and Subjective Probabilities in Language\nProcessing. Proceedings of the Annual Meeting of\nthe Cognitive Science Society, 33(33).\nNathaniel J Smith and Roger Levy. 2013. The Effect\nof Word Predictability on Reading Time is Logarith-\nmic. Cognition, 128(3):302–319.\nAdrian Staub. 2011. Word Recognition and Syntac-\ntic Attachment in Reading: Evidence for a Staged\nArchitecture. Journal of Experimental Psychology.\nGeneral, 140(3):407.\nAdrian Staub, Margaret Grant, Lori Astheimer, and An-\ndrew Cohen. 2015. The Inﬂuence of Cloze Proba-\nbility and Item Constraint on Cloze Task Response\nTime. Journal of Memory and Language, 82:1 – 17.\nAndreas Stolcke. 2002. SRILM - an Extensible Lan-\nguage Modeling Toolkit. In Seventh international\nconference on spoken language processing.\nW. L. Taylor. 1953. “Cloze Procedure”: A New tool\nfor Measuring Readability. Journalism Quarterly,\n30:415.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Warstadt and Samuel R Bowman. 2020. Can Neu-\nral Networks Acquire a Structural Bias from Raw\nLinguistic Data? In Proceedings of the 2020 Confer-\nence of the Cognitive Science Society , pages 1737–\n1743.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng\nQian, and Roger P. Levy. 2020. On the Predictive\nPower of Neural Language Models for Human Real-\nTime Comprehension Behavior. In Proceedings of\nthe 2020 Conference of the Cognitive Science Soci-\nety, pages 1707–1713.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, Joe Davison, Sam Shleifer, Patrick von\nPlaten, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M Rush.\n2019. HuggingFace’s Transformers: State-of-the-\nArt Natural Language Processing. arXiv e-prints ,\npage arXiv:1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized Autoregressive Pretraining for\nLanguage Understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763."
}