{
  "title": "Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",
  "url": "https://openalex.org/W4287889735",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2434258845",
      "name": "Joseph McDonald",
      "affiliations": [
        "MIT Lincoln Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2142715184",
      "name": "Baolin Li",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2560468558",
      "name": "Nathan Frey",
      "affiliations": [
        "MIT Lincoln Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2136352167",
      "name": "Devesh Tiwari",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2056870915",
      "name": "Vijay Gadepally",
      "affiliations": [
        "MIT Lincoln Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2139018926",
      "name": "Siddharth Samsi",
      "affiliations": [
        "MIT Lincoln Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970283086",
    "https://openalex.org/W2883672905",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4308090237",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4200178892",
    "https://openalex.org/W4288796004",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4310492983",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2792050151",
    "https://openalex.org/W2419597278",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4205140476",
    "https://openalex.org/W4287204036"
  ],
  "abstract": "The energy requirements of current natural language processing models continue to grow at a rapid, unsustainable pace. Recent works highlighting this problem conclude there is an urgent need for methods that reduce the energy needs of NLP and machine learning more broadly. In this article, we investigate techniques that can be used to reduce the energy consumption of common NLP applications. In particular, we focus on techniques to measure energy usage and different hardware and datacenter-oriented settings that can be tuned to reduce energy consumption for training and inference for language models. We characterize the impact of these settings on metrics such as computational performance and energy consumption through experiments conducted on a high performance computing system as well as popular cloud computing platforms. These techniques can lead to significant reduction in energy consumption when training language models or their use for inference. For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1962 - 1970\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nGreat Power, Great Responsibility: Recommendations for Reducing\nEnergy for Training Language Models\nJoseph McDonald1, Baolin Li2, Nathan Frey1, Devesh Tiwari2, Vijay Gadepally1,\nSiddharth Samsi1\n1MIT Lincoln Laboratory\n2Northeastern University\n{jpmcd,ncfrey}@mit.edu\n{li.baol,d.tiwari}@northeastern.edu\n{vijayg,sid}@ll.mit.edu\nAbstract\nThe energy requirements of current natural lan-\nguage processing models continue to grow at\na rapid, unsustainable pace. Recent works\nhighlighting this problem conclude there is\nan urgent need for methods that reduce the\nenergy needs of NLP and machine learning\nmore broadly. In this article, we investigate\ntechniques that can be used to reduce the en-\nergy consumption of common NLP applica-\ntions. In particular, we focus on techniques\nto measure energy usage and different hard-\nware and datacenter-oriented settings that can\nbe tuned to reduce energy consumption for\ntraining and inference for language models.\nWe characterize the impact of these settings\non metrics such as computational performance\nand energy consumption through experiments\nconducted on a high performance computing\nsystem as well as popular cloud computing\nplatforms. These techniques can lead to signif-\nicant reduction in energy consumption when\ntraining language models or their use for in-\nference. For example, power-capping, which\nlimits the maximum power a GPU can con-\nsume, can enable a 15% decrease in energy us-\nage with marginal increase in overall compu-\ntation time when training a transformer-based\nlanguage model.1\n1 Introduction\nArtiﬁcial intelligence and machine learning (ML)\nare increasingly used in diverse areas ranging from\nNLP to autonomous driving. Broadly, larger and\n1This material is based upon work supported by the As-\nsistant Secretary of Defense for Research and Engineering\nunder Air Force Contract No. FA8702-15-D-0001, and United\nStates Air Force Research Laboratory Cooperative Agreement\nNumber FA8750-19-2-1000. Any opinions, ﬁndings, conclu-\nsions or recommendations expressed in this material are those\nof the author(s) and do not necessarily reﬂect the views of the\nAssistant Secretary of Defense for Research and Engineering,\nor the United States Air Force. The U.S. Government is au-\nthorized to reproduce and distribute reprints for Government\npurposes notwithstanding any copyright notation herein.\ndeeper models are found to be more accurate. How-\never, as models and datasets increase in size, the\ncomputational demands of AI/ML have increased\ncorrespondingly (Amodei et al., 2018; Thompson\net al., 2020). In particular Thompson et al. (2020)\nestimates that achieving a 10-fold improvement in\nmodel performance comes at a cost of at least a\n10,000-fold increase in computation and a corre-\nsponding increase in the energy required to perform\nthese computations. The growth in computational\nand energy requirements is particularly glaring in\nNLP with the introduction of transformer-based lan-\nguage models (Vaswani et al., 2017; Devlin et al.,\n2019; Radford et al., 2018). For example, training\nGPT-3 is estimated to consume almost 1300MWh\n(Patterson et al., 2021). Given the considerable\ncompute requirements and the associated carbon\nfootprint of training models with increasing accu-\nracy, there is growing interest and research into the\nenergy demands and carbon footprint of AI (Patter-\nson et al., 2021; Toews, 2020).\nHowever, estimating energy usage for a particu-\nlar AI application depends on a number of parame-\nters such as model architecture, hardware details,\nenvironmental parameters, and implementation de-\ntails. For this reason, important works such as\nPatterson et al. (2021) and Strubell et al. (2019)\nrely on estimates extrapolated from industry aver-\nages for some language models in some of their\nanalysis of large neural net training, including to-\ntal ﬂoating point operations per second (FLOPS)\nand hardware estimates, rather than values that oth-\nerwise might have been measured. Further, these\narticles call for new research that identiﬁes mitiga-\ntion techniques that can reduce the energy usage of\nNLP applications.\nThis paper proposes and characterizes potential\nways to reduce the energy impact of NLP applica-\ntions. To our knowledge this is the ﬁrst presentation\nof power-capping as a useful tool for reducing GPU\nenergy consumption. Particularly in the context of\n1962\ndeep learning and NLP, this work provides an ap-\nproach alongside estimates for possible energy sav-\nings for training large, energy-intensive language\nmodels. Moreover this method does not affect the\npredictions of trained models or consequently their\nperformance accuracy on tasks. That is, if two net-\nworks with the same structure, initial values and\nbatched data are trained for the same number of\nbatches under different power-caps, their resulting\nparameters will be identical and only the energy\nrequired to produce them may differ. Section 2\npresents related work on tracking energy usage of\nNLP applications and their environmental impact.\nSection 3 introduces different techniques, including\npower-capping and energy-aware scheduling, that\ncan be used to reduce the energy usage, including\nexperiments and other relevant data to characterize\ntheir effectiveness. In Section 4, we discuss these\napproaches with broader recommendations before\nconcluding with future avenues of research.\n2 Prior work\nEnergy efﬁciency considerations for deep learning\nhave trailed model developments targeted at im-\nproving accuracy among other metrics with new,\noften growing architectures. Highlighting this fo-\ncus, the growth of neural network architecture sizes\nis considered in Canziani et al. (2016). That study\noffers a comparison of state-of-the-art image recog-\nnition models where their computational perfor-\nmance is analyzed including inference time and\npower utilization. Techniques for model compres-\nsion have been widely studied including knowledge\ndistillation and pruning (Hinton et al., 2015; Fran-\nkle and Carbin, 2019). In NLP, distillation has been\nused to reduce the size of large language models\n(Sanh et al., 2019), and other methods of compres-\nsion have been effective at shrinking model param-\neters such as embedding layers (Mu and Viswanath,\n2018; McDonald et al., 2021).\nRecent attention has focused on the size of NLP\nmodels alongside their extensive training times and\nenvironmental impact (Strubell et al., 2019; Pat-\nterson et al., 2021; Schwartz et al., 2020). These\nworks illustrate efforts to place greater considera-\ntion on the efﬁciency or inefﬁciencies of large neu-\nral network architectures. For instance, Schwartz\net al. (2020) weighs the advantages of different met-\nrics to evaluate efﬁciency while advocating for the\nuse of ﬂoating point operations as a way to objec-\ntively compare models. Another area of focus has\nbeen on the dependence of model test accuracies on\nthe amount of computation expended on hyperpa-\nrameter tuning (Dodge et al., 2019). Some of these\nworks propose considering efﬁciency alongside ac-\ncuracy as a metric for evaluating ML models, and\nat the very least to require reporting energy con-\nsumption and carbon impact used in research for\nconference and journal submissions.\nWhile calls to prioritize more efﬁcient methods\nof training NLP models are made in the previously\ncited papers among other works, to the best of our\nknowledge this is not reﬂected in publicly avail-\nable academic or industry research. In fact, in\nHenderson et al. (2020) a random sampling of 100\nNeurIPS papers showed that few papers tracked\nand reported these statistics – and none reported\ncarbon impact. This and the previous works point\nout that tracking energy usage is not yet a standard\npractice, in part because of the difﬁculty in imple-\nmenting a framework for collecting these statistics\nfrom hardware. An implementation for accurately\ncapturing this data on common hardware (specif-\nically Intel and NVIDIA hardware) is presented\nin Henderson et al. (2020) which relies on query-\ning device software tools. We describe another,\nsimilar approach for gathering power expenditure\nand energy usage in this work, in order to present\na straightforward process for obtaining accurate\nmeasurements of energy consumption.\nCompared to these works, this paper presents\nsteps that can be taken to reduce the energy re-\nquired for training and inference with language\nmodels. There is limited prior research investi-\ngating power-capping as a method for reducing\nenergy consumption (Haidar et al., 2019), and it\nhas focused on CPUs for scientiﬁc computing ap-\nplications. Our focus is speciﬁcally on widely-used\nAI/ML frameworks used with available commodity\nhardware. This approach is described with experi-\nments showcasing its effectiveness for a range of\nsettings. Additionally our ﬁndings for GPU energy\nreduction when training neural networks are com-\nparable and consistent with the outcomes for CPU\nconsumption presented in Haidar et al. (2019).\nSimilar recent work investigates distributed\nDNN training ﬁtting power law models that de-\nscribe how training time scales with available com-\npute resources and energy constraints (Frey et al.,\n2022). Additionally we address other approaches\ntowards reducing energy footprints by considering\nshifting habits in training. Utilizing datacenters\n1963\nand climate-aware workload scheduling can pro-\nvide considerable savings, and we share statistics\nfrom our institutional datacenter to support this\n(Reuther et al., 2018; Samsi et al., 2021).\n3 Reducing the Energy Impact of NLP\nThis section outlines various approaches that can\nbe used to reduce the energy consumption of NLP\nworkloads. We focus primarily on a simple yet\neffective method – power-capping – that yields sig-\nniﬁcant beneﬁt with minimal cost and translates\nacross different computing platforms. Experiments\nmeasuring the effect of power-caps on energy con-\nsumption are presented. For completeness, we\ndiscuss other potential avenues for reducing the\ncarbon impact of NLP applications. Data is pre-\nsented for the monthly and daily variation in energy\nefﬁciency of our institution’s datacenter. This il-\nlustrates in detail how much energy usage can be\nreduced by simple approaches like timing work-\nloads to certain hours or seasons if possible. While\nfactors like efﬁciency and daily variation depend\nheavily on characteristics unique to each organi-\nzation’s datacenter, we share general insights that\nwill hold true for most cases.\nMeasuring Energy Usage: Currently, there are\ntwo vendor-provided utilities to monitor resource\nconsumption on NVIDIA GPUs. The NVIDIA\nData Center GPU Manager (DCGM) is a suite of\ntools for managing and monitoring NVIDIA GPUs\nin cluster environments (NVIDIA, 2021a) and the\nNVIDIA System Management Interface (NVIDIA,\n2021b) (or nvidia-smi) utility, which can also\nperform similar monitoring. Broadly, these tools\nenable monitoring of GPU usage on a node and the\ncollection of metrics on Streaming Multi-processor\n(SM) utilization, GPU memory footprint, power\ndraw, GPU temperatures, PCI Express (PCIe) band-\nwidth, and several other hardware settings. On\nour system, this data is collected on every node\nand every GPU assigned to a job. The data is col-\nlected every 100ms and data collection is started\nand stopped automatically using the scheduler that\nmanages resources on the system.\n3.1 Limiting Hardware Power\nMost modern computing platforms allow users to\nadjust hardware settings for processors and GPUs.\nThis can be done via command line tools that are\ngenerally not visible to users of a shared comput-\ning system. Over the duration of an NLP task,\nthe power consumed by hardware components can\nvary signiﬁcantly based on the operation being per-\nformed, environmental conditions, and hardware\nlimits. Power-capping allows users to limit the max-\nimum power available to hardware devices through\nthese tools. On our cluster, this is implemented\nusing the nvidia-smi command line utility.\nA series of experiments is presented here that\nuse energy tracking tools to measure the reduc-\ntion in energy consumption provided by power-\ncapping GPUs. Power-capping requires no changes\nto user code and is done at a hardware level. Be-\nlow, we validate these savings for various scenarios\nsuch as how these savings translate across different\nmodels for masked language modeling (MLM) or\nhow these savings work across different sets of re-\nsources and hardware platforms. From our observa-\ntions, this method provides a noticeable difference\nin all scenarios with very little incurred effect on\ncomputation time. For these experiments we use\na popular PyTorch implementation for MLM from\nHugging Face2.\nPower-capping works across different models :\nWe train different transformer-based networks –\nBERT, DistilBERT, and Big Bird (Devlin et al.,\n2019; Sanh et al., 2019; Zaheer et al., 2020) – with\nMLM and observe that power-capping is beneﬁcial\nto energy usage regardless of architecture. Each\nmodel was trained on 16 V100 GPUs using four\ndifferent power caps: 100 watts (W), 150W, 200W\nand 250W (the default power limit for an NVIDIA\nV100 GPU on our system). Models were trained\n2https://github.com/huggingface/trans\nformers/blob/master/examples/pytorch/lan\nguage-modeling/run_mlm.py\nFigure 1: Time and energy usage comparison of\ntraining three language modeling network architectures\nwith different maximum power limits. Values given are\npercentage relative to performance of default 250W set-\nting (100% indicated by black line). For example, train-\ning BERT with a 150W limit required 108.5% of the\ntime and only 87.7% the energy needed to train with\ndefault settings.\n1964\nFigure 2: Time and energy required for training with varying number of GPUs at different power thresholds.\nValues are the percent relative to time or energy required for the default setting of 250W. Average relative time\nfor 150W is indicated by blue line, and average relative energy consumption for 150W is indicated by orange line.\nFor 32, 64, 128, 256, 384 and 424 GPUs, training was performed for 6, 10, 15, 25, 40, and 40 epochs respectively\nto ensure similar job durations. In most cases, power-capping required additional time to complete training but\nresulted in less overall energy consumption.\nwith the WikiText-103 (Merity et al., 2017) dataset\nfor 4 epochs and batches of 8 samples per GPU.\nNetwork parameters were trained from scratch with\nrandomly initialized values, and random number\nseeds ﬁxed for consistency across runs with differ-\nent power thresholds.\nFigure 1 depicts training performance with\npower-capping at 100W, 150W and 200W. Results\nare plotted as a percent relative to the default limit\nof 250W. Our experiments indicate that implement-\ning power caps can signiﬁcantly reduce energy us-\nage at the cost of training time.\nEnergy savings at larger scales: We performed\na similar test training BERT with MLM on dis-\ntributed conﬁgurations of varying numbers of\nGPUs. Energy measurements were gathered for\neach training run on different node conﬁgurations\nequipped with between 2 and 400+ GPUs and the\nsame choices for power limits as before. Models\nwere trained on WikiText-103 with a batch size of\n8 samples per GPU.\nThe time and energy required for training at dif-\nferent power thresholds is given in Figure 2, where\nvalues are the percent relative to time or energy\nrequired for the default setting of 250W. Averag-\ning across each choice of conﬁguration, a 150W\nbound on power utilization led to an average 13.7%\ndecrease in energy usage and 6.8% increase in train-\ning time compared to the default maximum. Note\nfrom Figure 2 that the 100W setting has signiﬁ-\ncantly longer training times (31.4% longer on av-\nerage). A 200W limit corresponds with almost the\nsame training time as a 250W limit but more mod-\nest energy savings than a 150W limit. These out-\ncomes support the use of power-capping at 150W\nfor this GPU architectures and this application. We\nexpect that different applications may require dif-\nferent settings for optimal efﬁciency which could\nbe identiﬁed empirically.\nEnergy savings translate across hardware plat-\nforms: We performed additional experiments\nacross several different GPUs used widely in ML\nresearch to check this method’s effectiveness. This\nwas tested on NVIDIA’s K80, T4 and A100 GPUs,\navailable through our institution’s HPC resources\nas well as Amazon Web Services. Figure 3 presents\nthese results. While there is not a single obvious\nchoice for optimal settings, we conﬁrm that the\neffect of power-capping is not limited to one type\nof hardware platform. In each of the platforms,\nmodifying the maximum power limit affected the\nefﬁciency of the device. For A100s the effect is\nsimilar to the V100s discussed previously, if more\npronounced with greater energy savings for both\nthe 150W and 200W settings. However for T4\nprocessors the default 70W settings perform opti-\nmally, and the effect for K80s is less clear. Many\nfactors affect how much power is needed for ef-\nﬁcient GPU computation, and memory intensive\nbatch training required by language models as well\nas hardware speciﬁc behaviors could lead to poorer\nperformance on these older NVIDIA architectures.\nEnergy savings apply to inference: Different es-\ntimates from NVIDIA and Amazon suggest that\ninference tasks account for 80% or more of AI com-\nputational demand (Barr, 2019; Leopold, 2019)\nwhile training new models is responsible for a\nmuch smaller fraction. Thus, methods for reduc-\ning energy on inference tasks can have a greater\nimpact in reducing AI’s carbon footprint compared\nto training.\nWe measure the effect of power-capping applied\n1965\nFigure 3: Performance impact of power-caps on differ-\nent NVIDIA GPUs relative to default limits, 250W on\nA100, 150W on K80, and 70W on T4. Limiting max-\nimum power has a signiﬁcant effect on each platform.\nFor A100s the effect is similar to the V100s we test\nin other cases, if more pronounced with greater energy\nsavings for both the 150W and 200W settings. How-\never for T4 architectures the default 70W settings per-\nform optimally, and the effect for K80s is less clear.\nto hardware when performing inference with a\ntrained BERT model. This test was limited to a sin-\ngle node with two V100 GPUs as inference is natu-\nrally parallelizable across multiple devices. Mea-\nsurements show that power-capping has a more pro-\nnounced effect for inference tasks on running time\nand energy usage. Compared to 250W, a 100W\nsetting required double the inference time (a 114%\nincrease) and consumed 11.0% less energy, 150W\nrequired 22.7% more time and saved 24.2% the\nenergy, and 200W required 8.2% more time with\n12.0% less energy. For language modeling with\nBERT, energy gains through power-capping are no-\nticeably greater when performing inference than\nfor training. If this is consistent for other AI ap-\nplications, this could have signiﬁcant ramiﬁcations\nin terms of energy consumption for large-scale or\ncloud computing platforms serving inference appli-\ncations for research and industry.\n3.2 Energy-aware Scheduling\nAI and NLP researchers often rely on HPC data-\ncenters managed by cloud computing providers or\ntheir institutions if available. The efﬁciency of a\ndatacenter varies through the day as well as through\nthe year. A common metric used across the data-\ncenter community to measure datacenter efﬁciency\nis Power Usage Effectiveness (PUE) deﬁned as\nPUE = FE + IT\nIT (1)\nwhere IT is the information technology energy and\nFE is the facility energy. Facility energy includes\nenergy consumed by the datacenter to perform cli-\nmate control and any additional energy required for\nFigure 4: PUE measurements averaged for each day\nthroughout 2020. Hotter summer temperatures corre-\nspond to more energy required for cooling compute re-\nsources and greater PUE values.\nFigure 5: Average hourly variation in PUE for our dat-\nacenter over one week in July 2020. Measurements\ntend to peak during hot afternoon hours and decrease\nthroughout cooler night temperatures. For example the\nhourly minimum on July 27 is 1.48 from 12–1 a.m.\nwhile the maximum is 1.63 between 2–3 p.m., trans-\nlating to a daily variation of 10.4%.\noperating the computing equipment. IT energy in-\ncludes the energy used by computing hardware. A\nhighly efﬁcient datacenter will have a PUE close to\n1, such that the facility energy overhead is minimal,\nwhile the global average for PUE is 1.59 (Ascierto\nand Lawrence, 2020). A PUE of 1.59 indicates\nthat nearly 40% of a datacenter’s energy usage is\nconsumed by facility energy.\nIn Figure 4 the average daily PUE measurements\nfrom our institutional datacenter are plotted for the\nentirety of 2020, showing how seasonal changes\nin temperature can affect the energy consumption\nof any individual computational workload. For in-\nstance the average PUE in January is 1.05 while\nin July it is 1.49, a 42% difference. Evidently,\nheavy NLP workloads are typically much less ef-\nﬁcient in the summer than those executed during\nwinter. Given the large seasonal variation, if there\nare computationally expensive experiments that can\nbe timed to cooler months this timing can signiﬁ-\ncantly reduce the carbon footprint.\nTo show how resource efﬁciency can vary even\n1966\nMonth PUE Variation (%)\nJanuary 1.30\nFebruary 0.69\nMarch 0.77\nApril 2.15\nMay 11.51\nJune 21.70\nJuly 7.76\nAugust 17.37\nSeptember 12.41\nOctober 8.07\nNovember 2.88\nDecember 1.07\nAnnual 7.30\nTable 1: Average daily variation in PUE for each\nmonth at our institution’s datacenter in 2020. A sin-\ngle day’s PUE variation is the percent difference of the\nhour with the greatest average PUE and the hour with\nthe minimum PUE average. The monthly variation is\nthe average of this value over days in the month. The\nannual variation is the average over all days in the year,\nand not the average among the months.\nover relatively short periods of time, our datacen-\nter’s average hourly PUE across the last week of\nJuly 2020 is plotted in Figure 5. Each point in\nthe curve is the average of the several PUE mea-\nsurements taken each hour, so that the swings in\nefﬁciency between daytime and night hours can be\nreadily observed. Daytime peaks result from extra\nenergy required for cooling while outside temper-\natures are high. For instance, on July 27 the PUE\npeaks at 2 p.m. at an average of 1.63 while ten\nhours later the average measurement is 1.46, a 12%\ndifference.\nWe consider the variation in PUE over the course\nof a day, where the variation is the percent differ-\nence of the day’s maximum hourly average com-\npared to the minimum hourly average. The monthly\nvariation is the average of this percent difference\nover every day of the month and is listed in Table 1.\nThe annual variation is the average over all days in\nthe year, not the average among the months. Daily\nvariation of PUE is 7.3% on average – with larger\ndaily swings in the summer months and smaller\nswings in the winter months.\nSigniﬁcant energy savings can be obtained if\nworkloads can be scheduled at times when a lower\nPUE is expected. For example, moving a short-\nrunning job from daytime to nighttime may provide\na roughly 10% reduction, and moving a longer, ex-\npensive job (e.g. a language model taking weeks to\nFigure 6: Examples of sampled power measurements\nfor four identical jobs at different power-cap thresholds\nare presented, where points in each curve give averages\nover one minute intervals. Note that average power re-\nmains consistent for the duration of each job.\ncomplete) from summer to winter may see a 33%\nreduction. While it is difﬁcult to predict the savings\nthat an individual researcher may achieve, the infor-\nmation presented here highlights the importance of\nenvironmental factors affecting the overall energy\nconsumed by their workloads.\n3.3 Relaxing Training Duration\nIn training different models we tracked energy con-\nsumption throughout each run and observed that\nthe rate of energy consumption (power) is roughly\nconstant after averaging over short intervals (one\nminute in this case). This is depicted in Figure 6\nfor four jobs with identical parameters but different\npower-cap limits. It can be expected that cutting\ntraining time by X percent will correspond to an\nX percent reduction in energy. We highlight this in\nconsideration of common practices of signiﬁcantly\nextending training times for marginal performance\ngains. For instance in (Devlin et al., 2019) doubling\nthe number of training batches provided an addi-\ntional 1% increase in performance on a particular\nbenchmark test set. For certain applications or do-\nmains this additional training may make sense, but\nin cases where evaluation metrics include energy\nconsiderations, longer training for marginal perfor-\nmance improvements would be counterproductive\nand could incur signiﬁcant energy expenditure.\n3.4 Utilizing Efﬁcient Datacenters\nOne last practice we address that can help re-\nsearchers reduce their environmental impact is uti-\nlizing institutional shared datacenters and cloud\ncomputing resources for energy-intensive NLP ap-\nplications. By considering this approach for ap-\nplications as opposed to building and managing\nsmaller, private HPC workstations or clusters, re-\n1967\nsearchers can save money on equipment purchases\nand potentially energy bills depending on where re-\nsources are housed, as well as reducing the carbon\nfootprint of their workloads. While there is conve-\nnience in having private computing resources that\nare accessible, this convenience comes at a cost.\nGenerally speaking energy savings and impact is\nmore easily obtained at larger scales. Datacenters\nand cloud computing providers make signiﬁcant\ninvestments in the efﬁciency of their facilities. For\ninstance, Google publishes data on its PUE, reach-\ning a 12 month average of 1.10 in 20213, and the\nNational Renewable Energy Laboratory sets an an-\nnual goal of running their computing facility at a\nPUE of less than 1.064, recently achieving a record\nof 1.036.\nAdditionally, many cloud providers are mov-\ning their energy supply towards more environmen-\ntally friendly and renewable energy sources in at-\ntempts to reduce their carbon output to zero (Barr,\n2015). These types of improvements would be\ntime-consuming and difﬁcult to make for individual\nresearchers, but by sacriﬁcing some conveniences,\nAI researchers can reap these beneﬁts without addi-\ntional effort beyond moving their projects to these\nplatforms.\n4 Discussion and Recommendations\nWe believe the approaches proposed here offer\neasy-to-implement solutions for reducing the car-\nbon footprint of NLP applications without signiﬁ-\ncant algorithmic or software changes. Though they\ndo not involve new algorithmic methods which are\noutside the scope of this article, these represent\nearly steps towards more efﬁcient NLP. Coupling\nthem with algorithmic changes would further im-\nprove energy consumption. The goal of this arti-\ncle is to initiate a conversation between NLP re-\nsearchers and those in the hardware and datacenter\ndomains. Below, we list additional recommenda-\ntions that may help shape such a conversation.\nUnderstanding your computational environ-\nment’s characteristics: Previous works high-\nlighted the carbon footprint of computationally ex-\npensive NLP applications, and their recommenda-\ntions of tracking and reporting energy usage was\nintended to encourage researchers to be aware of\ntheir individual impact. Similarly we highlight\n3https://www.google.com/about/datacen\nters/efficiency/\n4https://www.nrel.gov/computational-s\ncience/measuring-efficiency-pue.html\nthe importance of datacenter characteristics and\nPUE variation to promote a deeper understanding\nof research energy requirements and the factors\nthat constitute them. We hope this work leads\nNLP researchers to question assumptions about\nthe datacenters where their workloads are running\nand what the relative efﬁciency of those datacen-\nters are. For example, researchers should opt for\nenergy-efﬁcient datacenters and encourage their or-\nganizations to deploy or leverage energy-efﬁcient\ndatacenters. If possible, it would also be helpful\nfor researchers to learn these operating character-\nistics of their datacenters or computing providers.\nFurther we encourage the NLP community to work\nwith their computing facility or datacenter to imple-\nment frameworks for tracking energy consumption\nlike that outlined in Section 3 and other works (e.g.\nHenderson et al. (2020)).\nPromoting better energy usage: In recent years,\ntop conferences in AI and machine learning have\nintroduced the requirement that papers include an\nethics statement addressing the potential impact of\ntheir work on the broader society. However, one\narea that is currently lacking is the impact of AI\non the environment. It may be difﬁcult to account\nfor every trial run or hyperparameter tuning when\ntracking and reporting estimates of energy usage.\nHowever, we hope that this practice promotes better\nawareness of AI energy consumption and fosters a\ngreater focus on optimization pathways to reduce\nenergy usage.\nAlongside reporting energy consumption statis-\ntics, we make the additional recommendation that\nconference papers’ statements discussing ethical\nconsiderations also identify steps undertaken to\nminimize energy consumption. We give an exam-\nple of such an “energy statement\" after conclud-\ning. Additionally, it is critical that energy-efﬁcient\nNLP research be promoted in the research commu-\nnity, perhaps via specialized tracks or workshops\nfocused on these problems.\nReducing the environmental impact: The pur-\npose of this research is to educate NLP researchers\non tools that can be used to reduce their energy\nusage and empower them to leverage those tools\nto minimize their carbon footprint. The methods\ndiscussed ﬁt into a wider research effort to enable\nmore efﬁcient AI. Lastly, we also echo earlier calls\nfor promoting more energy-conscious NLP prac-\ntices and discourage overtraining or extensive hy-\nperparameter searches. Reviews of conference sub-\n1968\nmissions should consider whether new methodolo-\ngies are effective or the result of expensive opti-\nmization.\n5 Conclusions\nThis article presents techniques that can improve\nthe energy efﬁciency of training and inference\nfor NLP applications. Importantly, the methods\ndiscussed can be used jointly with each other to\nachieve a compounding effect of energy savings.\nFuture work relevant to these topics would include\na wider survey of AI hardware and power-capping\ncapabilities. While we focused on NVIDIA GPUs,\nevaluation of AI hardware from other vendors and\ncloud providers could have a potentially large im-\npact for cloud computing as well as large shared\nhigh performance computing centers.\nEnergy Statement\nThe experiments performed in this work consumed\na total of 782 kWh. A majority of the experi-\nments (approximately 760 kWh) were performed\non our institution’s high performance computing\ncluster, powered by largely carbon-free, hydroelec-\ntric power sources. To minimize energy consump-\ntion, much of these experiments were performed\nduring system downtimes (e.g., when the system is\nundergoing scheduled maintenance and less busy)\nand when cooling needs are reduced.\nAcknowledgements\nThe authors acknowledge the MIT Lincoln Lab-\noratory Supercomputing Center (LLSC) for pro-\nviding HPC resources that have contributed to\nthe research results reported in this paper. The\nauthors wish to acknowledge the following indi-\nviduals for their contributions and support: Bob\nBond, Tucker Hamilton, Jeff Gottschalk, Tim\nKraska, Mike Kanaan, CK Prothmann, Charles\nLeiserson, Dave Martinez, John Radovan, Steve\nRejto, Daniela Rus, Marc Zissman, Matthew\nL Weiss, David Bestor, Michael Jones, Albert\nReuther, William Arcand, William Bergeron, Chan-\nsup Byun, Michael Houle, Matthew Hubbell, Hay-\nden Jananthan, Jeremy Kepner, Kurt Keville, Anna\nKlein, Adam Michaleas, Peter Michaleas, Lauren\nMilechin, Julia Mullen, Charles Yee, Andrew Prout,\nand Antonio Rosa. We also acknowledge support\nfrom NSF awards 1920020, 2124897, and 1910601,\nthe Massachusetts Green High Performance Com-\nputing Center, and Northeastern University.\nReferences\nDario Amodei, Danny Hernandez, Girish SastryJack,\nJack Clark, Greg Brockman, and Ilya Sutskever.\n2018. Ai and compute. https://openai.c\nom/blog/ai-and-compute/.\nRhonda Ascierto and Andy Lawrence. 2020. Uptime\ninstitute global data center survey 2020. Technical\nreport, Uptime Institute.\nJeff Barr. 2015. Cloud computing, server utilization, &\nthe environment. https://aws.amazon.com\n/blogs/aws/cloud-computing-server-\nutilization-the-environment/.\nJeff Barr. 2019. Amazon ec2 update – inf1 instances\nwith aws inferentia chips for high performance cost-\neffective inferencing. https://aws.amazon.c\nom/blogs/aws/amazon-ec2-update-inf\n1-instances-with-aws-inferentia-ch\nips-for-high-performance-cost-effe\nctive-inferencing/.\nAlfredo Canziani, Adam Paszke, and Eugenio Cu-\nlurciello. 2016. An analysis of deep neural net-\nwork models for practical applications. arXiv,\nabs/1605.07678.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4171–4186.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2185–\n2194, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJonathan Frankle and Michael Carbin. 2019. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. In International Conference on Learn-\ning Representations.\nNathan C. Frey, Baolin Li, Joseph McDonald, Dan\nZhao, Michael Jones, David Bestor, Devesh Tiwari,\nVijay Gadepally, and Siddharth Samsi. 2022. Bench-\nmarking resource usage for efﬁcient distributed deep\nlearning. arXiv, abs/2201.12423.\nAzzam Haidar, Heike Jagode, Phil Vaccaro, Asim\nYarKhan, Stanimire Tomov, and Jack Dongarra.\n2019. Investigating power capping toward energy-\nefﬁcient scientiﬁc applications. Concurrency and\nComputation: Practice and Experience, 31(6).\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma\nBrunskill, Dan Jurafsky, and Joelle Pineau. 2020.\n1969\nTowards the systematic reporting of the energy and\ncarbon footprints of machine learning. Journal of\nMachine Learning Research, 21(248):1–43.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv,\nabs/1503.02531.\nGeorge Leopold. 2019. Aws to offer nvidia’s t4 gpus\nfor ai inferencing. https://www.hpcwire.co\nm/2019/03/19/aws-upgrades-its-gpu-\nbacked-ai-inference-platform/ .\nJoseph McDonald, Siddharth Samsi, Daniel Edelman,\nChansup Byun, Jeremy Kepner, and Vijay Gade-\npally. 2021. Improved compression for word em-\nbeddings by scaling principal components. In 2021\nIEEE High Performance Extreme Computing Con-\nference (HPEC).\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of the 5th International Confer-\nence on Learning Representations (ICLR).\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In International Conference on\nLearning Representations.\nNVIDIA. 2021a. Nvidia data center GPU manager\n(dcgm). https://developer.nvidia.c\nom/dcgm. Online; accessed November 2021.\nNVIDIA. 2021b. Nvidia-smi. http://develope\nr.download.nvidia.com/compute/DCGM\n/docs/nvidia-smi-367.38.pdf . Online;\naccessed November 2021.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlbert Reuther, Jeremy Kepner, Chansup Byun, Sid-\ndharth Samsi, William Arcand, David Bestor,\nBill Bergeron, Vijay Gadepally, Michael Houle,\nMatthew Hubbell, Michael Jones, Anna Klein, Lau-\nren Milechin, Julia Mullen, Andrew Prout, Antonio\nRosa, Charles Yee, and Peter Michaleas. 2018. Inter-\nactive supercomputing on 40,000 cores for machine\nlearning and data analysis. In 2018 IEEE High Per-\nformance extreme Computing Conference (HPEC),\npages 1–6. IEEE.\nSiddharth Samsi, Matthew L Weiss, David Bestor,\nBaolin Li, Michael Jones, Albert Reuther, Daniel\nEdelman, William Arcand, Chansup Byun, John\nHolodnack, Matthew Hubbell, Jeremy Kepner, Anna\nKlein, Joseph McDonald, Adam Michaleas, Peter\nMichaleas, Lauren Milechin, Julia Mullen, Charles\nYee, Benjamin Price, Andrew Prout, Antonio Rosa,\nAllan Vanterpool, Lindsey McEvoy, Anson Cheng,\nDevesh Tiwari, and Vijay Gadepally. 2021. The\nmit supercloud dataset. In 2021 IEEE High Per-\nformance Extreme Computing Conference (HPEC),\npages 1–8.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv,\nabs/1910.01108.\nRoy Schwartz, Jesse Dodge, Noah Smith, and Oren Et-\nzioni. 2020. Green ai. Communications of the ACM,\n63:54 – 63.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nNeil C. Thompson, Kristjan Greenewald, Keeheon Lee,\nand Gabriel F. Manso. 2020. The computational lim-\nits of deep learning.\nRob Toews. 2020. Deep learning’s carbon emissions\nproblem. https://www.forbes.com/sites\n/robtoews/2020/06/17/deep-learning\ns-climate-change-problem/ .\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big bird: Trans-\nformers for longer sequences. In Advances in\nNeural Information Processing Systems, volume 33,\npages 17283–17297. Curran Associates, Inc.\n1970",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8307162523269653
    },
    {
      "name": "Energy consumption",
      "score": 0.6713173985481262
    },
    {
      "name": "Inference",
      "score": 0.6471107602119446
    },
    {
      "name": "Pace",
      "score": 0.5818783640861511
    },
    {
      "name": "Cloud computing",
      "score": 0.4761243462562561
    },
    {
      "name": "Transformer",
      "score": 0.47238588333129883
    },
    {
      "name": "Efficient energy use",
      "score": 0.45619115233421326
    },
    {
      "name": "Machine learning",
      "score": 0.43580561876296997
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42279714345932007
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.34701329469680786
    },
    {
      "name": "Engineering",
      "score": 0.08142560720443726
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210122954",
      "name": "MIT Lincoln Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    }
  ],
  "cited_by": 49
}