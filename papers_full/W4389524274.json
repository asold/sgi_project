{
  "title": "Symbol tuning improves in-context learning in language models",
  "url": "https://openalex.org/W4389524274",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098386832",
      "name": "Jerry Wei",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2129280759",
      "name": "Hou Le",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2556082512",
      "name": "Andrew Lampinen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2108776058",
      "name": "Xiangning Chen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2097494764",
      "name": "Da Huang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2556594351",
      "name": "Xinyun Chen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2148022030",
      "name": "Yifeng Lu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2908022021",
      "name": "Denny Zhou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2171800532",
      "name": "Tengyu Ma",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2109481008",
      "name": "Quoc Le",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288379066",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2062183687",
    "https://openalex.org/W2954298906",
    "https://openalex.org/W4320516905",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4286953959",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W4310509152",
    "https://openalex.org/W1968079292",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W4320086632",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W4323697341",
    "https://openalex.org/W2992306696",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4385571045",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4285546215",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3040863728",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2752201871",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W2995049146",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W4296415404",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4283768109",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4311726128",
    "https://openalex.org/W3132159783",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4385571157"
  ],
  "abstract": "Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc Le. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 968–979\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSymbol tuning improves in-context learning in language models\nJerry Wei1,2,∗ Le Hou1 Andrew Lampinen1 Xiangning Chen1,∗ Da Huang1\nYi Tay1 Xinyun Chen1 Yifeng Lu1 Denny Zhou1 Tengyu Ma1,2,† Quoc V . Le1\n1 Google 2 Stanford University\nAbstract\nWe present symbol tuning —ﬁnetuning lan-\nguage models on in-context input–label pairs\nwhere natural language labels (e.g., “posi-\ntive/negative sentiment”) are replaced with arbi-\ntrary symbols (e.g., “foo/bar”). Symbol tuning\nleverages the intuition that when a model can-\nnot use instructions or natural language labels\nto ﬁgure out a task, it must instead do so by\nlearning the input–label mappings.\nWe experiment with symbol tuning across\nPaLM models up to 540B parameters and ob-\nserve beneﬁts across various settings. First,\nsymbol tuning boosts performance on unseen\nin-context learning tasks and is much more ro-\nbust to underspeciﬁed prompts, such as those\nwithout instructions or without natural lan-\nguage labels. Second, symbol-tuned models\nare much stronger at algorithmic reasoning\ntasks, with up to 18.2% better performance\non the List Functions benchmark and up to\n15.3% better performance on the Simple Turing\nConcepts benchmark. Finally, symbol-tuned\nmodels show large improvements in following\nﬂipped-labels presented in-context, meaning\nthat they are more capable of using in-context\ninformation to override prior knowledge.\n1 Introduction\nA key feature of human intelligence is that humans\ncan learn to perform new tasks by reasoning us-\ning only a few examples. Scaling up language\nmodels has unlocked a range of new applications\nand paradigms in machine learning, including the\nability to perform challenging reasoning tasks via\nfew-shot examples given in-context (Brown et al.,\n2020; Chowdhery et al., 2022; OpenAI, 2023, inter\nalia). Language models, however, are still sen-\nsitive to the way that prompts are given, indicat-\ning that they are not reasoning in a robust manner.\nFor instance, language models often require heavy\n∗Work done as a Student Researcher at Google.\n†Work done as a Visiting Researcher at Google.\nprompt engineering (Brown et al., 2020; Reynolds\nand McDonell, 2021) or phrasing tasks as instruc-\ntions (Wei et al., 2022a; Ouyang et al., 2022; Sanh\net al., 2022, inter alia), and they exhibit unexpected\nbehaviors such as performance on tasks being un-\naffected even when shown exemplars with random\nlabels (Min et al., 2022b) or ﬂipped labels (Wei\net al., 2023).\nIn this paper, we propose a simple ﬁnetuning\nprocedure that we call symbol tuning, which signif-\nicantly improves the ability of language models to\nreason with and learn from input–label mappings\npresented in-context. In the symbol-tuning proce-\ndure, we ﬁnetune language models on input–label\npairs presented in-context where natural language\nlabels are remapped to arbitrary symbols.1 The in-\ntuition is that when models cannot rely on instruc-\ntions or relevant natural language labels to ﬁgure\nout a given task, they must instead do so by reason-\ning with input–label mappings presented in-context\nin order to learn the mappings that reveal the task.\nWe perform symbol tuning using a mixture of 22\nNLP datasets with various arbitrary symbols as la-\nbels and experiment using instruction-tuned PaLM\nmodels (Flan-PaLM) with several sizes (8B, 62B,\n62B-cont, 540B).\nFirst, symbol tuning improves performance of\nbaseline models on unseen in-context learning\ntasks across various settings (with/without instruc-\ntions, with/without relevant labels), with larger per-\nformance gains when instructions or natural lan-\nguage labels are not given in the prompt. For ex-\nample, when prompts do not contain instructions\nor relevant labels, symbol tuning yields a +11.1%\naverage performance improvement across eleven\nevaluation tasks for Flan-PaLM-62B-cont.\n1We call our methodsymbol tuning because arbitrary desig-\nnation is a key property of symbols (Newell and Simon, 1976)\nand using symbols is a crucial part of intelligence (Newell,\n1980; Santoro et al., 2021).\n968\nInstruction Tuning\nIn-context exemplars not needed to learn \nthe task\nWhat is the sentiment of this?\nThis movie is great\nAnswer: Positive\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nWorst film I’ve ever seen\nAnswer: Negative\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nThis movie is terrible\nAnswer:\nInput\nInstruction\nLabel\nEvaluation \nExample\nExemplar\nNegative\nOutput\nRelevant\nSymbol Tuning\nMust use in-context exemplars to learn \nthe task\n[None] \nThis movie is great\nAnswer: Foo\n– – – – – – – – – – – – – – – – – – – – – –\n[None] \nWorst film I’ve ever seen\nAnswer: Bar\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\n[None]   \nThis movie is terrible\nAnswer:\nInput\nBar\nOutput\nUnrelated\nInstruction\nLabel\nExemplar\nRelevant Unrelated\nFigure 1: We tune models on tasks where natural language labels are replaced with arbitrary symbols ( symbol\ntuning). Symbol tuning relies on the intuition that when instructions and relevant labels are not available, models\nmust use in-context exemplars to learn the task.\nSecond, symbol-tuned models are better at algo-\nrithmic reasoning tasks, a striking result since sym-\nbol tuning only includes natural language data and\ndid not have any numerical or algorithmic data. On\na set of reasoning evaluation suites for list functions\n(e.g., remove the last element in a list), symbol-\ntuned models experience performance improve-\nments of +18.2% for Flan-PaLM-8B, +11.1% for\nFlan-PaLM-62B, and +3.6% for Flan-PaLM-540B.\nOn a set of turing concept tasks (e.g., swapping\n0s and 1s in a string), symbol-tuned models also\nimprove by +15.3% for Flan-PaLM-8B and Flan-\nPaLM-62B and +4.7% for Flan-PaLM-540B.\nFinally, we experiment on an in-context learn-\ning setting where inputs have ﬂipped labels, which\nforces the model to override its prior knowledge\nwhen presented with contradictory information in-\ncontext. Pretrained language models have the abil-\nity to somewhat follow ﬂipped labels—this ability\nis lost during instruction tuning but can be restored\nvia symbol tuning. Overall, we hope that the strong\nempirical results from symbol tuning encourage\nfurther work in allowing language models to rea-\nson over arbitrary symbols given in-context.\n2 Symbol tuning\nDespite their ability to perform some reason-\ning tasks after being shown in-context exemplars\n(Chowdhery et al., 2022; OpenAI, 2023), language\nmodels are still sensitive to the way in which these\ntasks are presented in prompts (Brown et al., 2020;\nReynolds and McDonell, 2021; Wei et al., 2022a),\nsuggesting that they are not reasoning in a robust\nway. Instruction tuning has been shown to improve\nperformance and allow models to better follow in-\ncontext exemplars (Mishra et al., 2022; Min et al.,\n2022a; Wei et al., 2022a; Ye et al., 2021; Chung\net al., 2022). One shortcoming, however, is that\nmodels are not forced to learn to use the exemplars\nbecause the task is redundantly deﬁned in the evalu-\nation example via instructions and natural language\nlabels. For example, in the left-hand side of Fig-\nure 1, although the exemplars can help the model\nunderstand the task, they are not strictly necessary\nsince the model could ignore the exemplars and\njust read the instruction.\nTo make the model better at in-context learning,\nwe propose symbol tuning, in which the model\nis ﬁnetuned on exemplars where the instructions\nare removed and natural language labels are re-\nplaced with semantically-unrelated labels (e.g.,\n“Foo,” “Bar,” etc.). In this setup, the task is un-\nclear without looking at the in-context exemplars.\nFor example, if the prompt from the previous para-\ngraph was changed to “<sentence>. Answer: {Foo,\nBar}” (as shown in the right-hand side of Figure 1),\nmultiple in-context exemplars would be needed in\norder to ﬁgure out the task. Because symbol tuning\nteaches the model to reason over in-context ex-\nemplars, symbol-tuned models should have better\nperformance on unseen tasks that require reasoning\nbetween in-context exemplars and their labels.\n969\nNatural Language \nInference\nWNLIRTE\nQNLI MNLI\nCBSNLITopic Classification\nAGN TREC\nMiscellaneous\nTEITEO\nWIC COLA\nCommon Sense\nCOPA PIQA\nCoreference\nWSC WINO\nParaphrase Detection\nMRPCQQP\nPAWS\nSentiment Analysis\nSST2RT\nTES\nFigure 2: Datasets and task types used for symbol tuning. See Appendix D.1 for dataset details.\nCharacters\n(1–3 letter combinations)\nWords\n(MIT list of 10,000 words )\nCharacters\n(3–4 letter combinations)\nWords\n(MIT list of 100,000 words)\nIntegers\n(1–4 digits)\nIntegers\n(5 digits)\nFinetuning\n(~30k symbols)\nEvaluation\n(~270k symbols)\nFigure 3: We use a set of ∼300k arbitrary symbols from three categories (integers, character combinations, and\nwords). ∼30k symbols are used during tuning and the rest are held out for evaluation. See Appendix E.1 for more\ndetails on the symbols that we used.\n3 Experimental setup\n3.1 Tuning tasks & prompt formatting\nFigure 2 shows the 22 publicly-available NLP\ndatasets from HuggingFace (Lhoest et al., 2021)\n(see Appendix D.1 for dataset details) that we use\nfor our symbol-tuning procedure (we ablate the\nnumber of datasets used for symbol tuning in Ap-\npendix B.4). We selected NLP tasks that have been\nwidely used in the literature (Wang et al., 2018,\n2019). Each dataset is categorized into one of\nseven task types—we only selected classiﬁcation-\ntype tasks because symbol tuning requires dis-\ncrete labels. For each dataset, we use examples\nfrom the training split to compose prompts that\nwe use for tuning. Each prompt uses a randomly-\nselected input–label format (formats are shown in\nAppendix E.2) and contains a randomly-selected\nnumber between 2 and 10 of in-context exem-\nplars per class. We remap labels to a randomly-\nselected label from a set of ∼30k labels from three\nlabel types as shown in Figure 3 (we ablate the\nnumber of labels in Appendix B.5 and the label\ntypes in Appendix B.6). Examples of generated\ntuning prompts for each task are shown in Ap-\npendix G.1. Code for generating arbitrary sym-\nbols can be found at https://github.com/\nJerryWeiAI/symbol-tuning.\n3.2 Evaluation tasks\nWe want to evaluate a model’s ability to perform\non unseen tasks, so we cannot evaluate on tasks\nused in symbol tuning (22 datasets) or used during\ninstruction tuning (1.8k tasks). Hence, we choose\n11 NLP datasets from HuggingFace (Lhoest et al.,\n2021) that were not used in either stage of ﬁne-\ntuning (details are shown in Appendix D.2): (Con-\nneau and Kiela, 2018, SUBJ); (Basile et al., 2019,\nTEH); (Mohammad et al., 2016,TEAB); (Moham-\nmad et al., 2016, TEAT); (Mohammad et al., 2016,\nTEFE); (Mohammad et al., 2016, TEHI); (Alex\net al., 2021, ADEC); (Alex et al., 2021,OR); (Alex\net al., 2021, SOT); (Alex et al., 2021, TOS); and\n(Alex et al., 2021, TC). We use the validation split\nof each dataset to generate evaluation prompts. For\neach dataset, we randomly select a maximum of\n100 examples to use during evaluation. Each evalu-\nation prompt uses a randomly-selected input–label\nformat following Section 3.1, though we ﬁx the\nnumber of in-context exemplars per class at k = 4\n(we ablate this parameter in Appendix C.4).\nWe generate prompts for the four different in-\ncontext learning (ICL) settings described in Fig-\nure 4; each setting either contains or does not\ncontain instructions describing the task (see Ap-\npendix D.2 for the instructions we use for each\ntask) and does or does not contain relevant natural\nlanguage labels. For settings that do not use rel-\nevant natural language labels, we remap original\nlabels to a randomly-selected label from a set of\n∼270k semantically-unrelated labels as shown in\nFigure 3 (we removed labels that were seen during\nsymbol tuning). Examples of generated evaluation\nprompts for each task are shown in Appendix G.2.\n970\nRelevant Label: ✕\nInstructions: ✓\nRelevant Label: ✓\nInstructions: ✕ \n Relevant Label: ✓\nInstructions: ✓\nRelevant Label: ✕\nInstructions: ✕\nWhat is the sentiment of this?\nThis movie is great\nAnswer: Positive\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nWorst film I’ve ever seen\nAnswer: Negative\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nThis movie is terrible\nAnswer:\nInput\nInstruction\nLabel\nEvaluation \nExample\nExemplar\nNegative\nOutput\nRelevant\nInstruction\nLabel\nExemplar\nRelevant\n[None] \nThis movie is great\nAnswer: Foo\n– – – – – – – – – – – – – – – – – – – – – –\n[None] \nWorst film I’ve ever seen\nAnswer: Bar\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\n[None]   \nThis movie is terrible\nAnswer:\nInput\nBar\nOutput\nUnrelated\nUnrelated\n[None] \nThis movie is great\nAnswer: Positive\n– – – – – – – – – – – – – – – – – – – – – –\n[None] \nWorst film I’ve ever seen\nAnswer: Negative\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\n[None] \nThis movie is terrible\nAnswer:\nInput\nNegative\nOutput\nRelevant\nRelevant\nWhat is the sentiment of this?\nThis movie is great\nAnswer: Foo\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nWorst film I’ve ever seen\nAnswer: Bar\n– – – – – – – – – – – – – – – – – – – – – –\n[more exemplars]\n– – – – – – – – – – – – – – – – – – – – – –\nWhat is the sentiment of this?\nThis movie is terrible\nAnswer:\nInput\nBar\nOutput\nUnrelated\nUnrelated\nFigure 4: Depending on the availability of instructions and relevant natural language labels, models may need to do\nvarying amounts of reasoning with in-context exemplars. When these features are not available, models must reason\nwith the given in-context exemplars in order to successfully perform the task. When they are available, reasoning\nwith exemplars can help but is not necessary.\n3.3 Models & ﬁnetuning procedure\nFor our experiments, we tune Flan-PaLM, the\ninstruction-tuned variants of PaLM. We use the\ninstruction-tuned variants in order to reduce the\nnumber of steps needed for tuning, since symbol\ntuning an instruction-tuned model does not require\nrelearning the information learned during the origi-\nnal round of instruction tuning. We use three dif-\nferent sizes of Flan-PaLM models: Flan-PaLM-8B,\nFlan-PaLM-62B, and Flan-PaLM-540B. We also\ntested Flan-PaLM-62B-cont (PaLM-62B at 1.3T\ntokens instead of 780B tokens); we abbreviate this\nmodel size as 62B-c.\nOur symbol-tuning pipeline mixes all datasets\nand randomly samples from each dataset. To ensure\nthat the dataset sizes are balanced (i.e., no dataset\ngets completely overshadowed), we limit the num-\nber of training examples per dataset to a maximum\nof 25k randomly-selected examples. Training ex-\namples are combined into a single sequence using\npacking (Raffel et al., 2020), and inputs are sepa-\nrated from labels using an end-of-sequence (EOS)\ntoken. We tune all models using a batch size of\n32 and the Adafactor optimizer (Shazeer and Stern,\n2018). For 8B and 62B models, we tune with a\nlearning rate of 3 ×10−3, and we tune Flan-PaLM-\n540B with a learning rate of 1 × 10−3. We use\n2048 and 512, respectively, as the input and target\nsequence lengths during tuning.\nFor 8B and 62B model evaluations, we report re-\nsults from the checkpoint after tuning for 4k steps,\nand for 540B model evaluations, we report results\nfrom the checkpoint after tuning for 1k steps (we\nablate the number of tuning steps in Appendix B.2).\nSee Appendix E.3 for the number of ﬁnetuning\nsteps, learning rate, batch size, and dropout used\nfor each model. As a baseline, we compare symbol-\ntuned models against Flan-PaLM models, and we\nalso compare symbol tuning against continued in-\nstruction tuning in Appendix B.1.\n4 Symbol-tuned models are better\nin-context learners\nDuring symbol tuning, models must learn to rea-\nson with in-context exemplars in order to success-\nfully perform tasks because prompts are modiﬁed\nto ensure that tasks cannot be learned from natu-\nral language labels or instructions. Symbol-tuned\nmodels should thus perform better in settings where\ntasks are unclear and require reasoning between in-\ncontext exemplars and their labels. Additionally,\nsince symbol tuning is meant to improve the abil-\nity to follow in-context exemplars, it should not\nmodify prior knowledge and should thus retain the\nsame performance in settings where exemplars are\nnot as necessary to complete the task.\nTo explore these settings, we deﬁne four ICL\nsettings that vary the amount of reasoning required\nbetween inputs and labels in order to learn the task\n(based on the availability of instructions/relevant\nlabels), as shown in Figure 4. The easiest of these\nsettings uses prompts where both instructions and\n971\nAverage performance on eleven tasks\nRelevant labels: \u0013 \u0013 \u0017 \u0017\nTask instructions: \u0013 \u0017 \u0013 \u0017\nRandom Guessing 42.4 42.4 42.4 42.4\nFlan-PaLM-8B 63.9 61.6 42.4 44.2\n+ Symbol tuning (ours) 57.6 ( -6.3 ) 54.3 ( -7.3 ) 58.2 ( +15.8 ) 52.8 ( +8.6 )\nFlan-PaLM-62B 74.3 70.0 57.0 50.5\n+ Symbol tuning (ours) 75.5 ( +1.2 ) 70.8 ( +0.8 ) 71.4 ( +14.4 ) 60.3 ( +9.8 )\nFlan-PaLM-62B-cont 77.3 70.3 56.3 51.0\n+ Symbol tuning (ours) 78.9 ( +1.6 ) 74.5 ( +4.2 ) 71.8 ( +15.5 ) 62.1 ( +11.1 )\nFlan-PaLM-540B 82.2 77.4 70.7 58.1\n+ Symbol tuning (ours) 84.4 ( +2.2 ) 78.8 ( +1.4 ) 80.0 ( +9.3 ) 63.6 ( +5.5 )\nTable 1: Large-enough symbol-tuned models are better at in-context learning than baselines, especially in settings\nwhere relevant labels are not available. Performance is shown as average model accuracy (%) across eleven tasks\n(per-task results are shown in Appendix F.2).\nrelevant labels are available (as in-context exem-\nplars are not necessary to learn the task), while the\nhardest setting uses prompts where instructions and\nrelevant labels are both unavailable.\nIn Table 1, we evaluate model performance be-\nfore and after symbol tuning in each of these set-\ntings. We ﬁnd that symbol tuning improves per-\nformance across all ICL settings for models 62B\nand larger, with small improvements in settings\nwith relevant natural language labels (+0.8% to\n+4.2%) and substantial improvements in settings\nwithout relevant natural language labels (+5.5%\nto +15.5%). Strikingly, when relevant labels are\nunavailable, symbol-tuned Flan-PaLM-8B outper-\nforms Flan-PaLM-62B, and symbol-tuned Flan-\nPaLM-62B outperforms Flan-PaLM-540B. This\nperformance difference suggests that symbol tun-\ning can allow much smaller models to perform as\nwell as large models on learning input-label map-\nping from exemplars (effectively saving∼10x in-\nference compute).\nSymbol-tuned models also perform somewhat-\ncomparably in settings with only relevant labels\nor only instructions, unlike baseline models whose\nperformance in settings with only relevant labels\nis always better than in settings with only instruc-\ntions. Performance in settings with relevant labels\nactually decreases for Flan-PaLM-8B after symbol-\ntuning, however, which may suggest that symbol\ntuning a small model can override its prior knowl-\nedge due to overﬁtting. Overall, the improvements\ndemonstrate the strong potential of symbol tuning\nto improve model performance, especially when\ntasks require learning from in-context exemplars.\n5 Symbol tuning improves algorithmic\nreasoning\nSymbol tuning is designed to force the model to\nlearn from input–label mappings in the in-context\nexemplars because the symbols are unrelated to\nthe task and no instructions are provided (and thus\nthe model cannot rely on any other guidance to\ndetermine the task). For this reason, we posit that\nsymbol tuning should not only improve the model’s\nability to map natural language inputs to arbitrary\nsymbols, but also its ability to learn other forms of\ninput–label mappings such as algorithms.\nTo test this, we experiment on algorithmic rea-\nsoning tasks from BIG-Bench (Srivastava et al.,\n2022). We ﬁrst experiment on a set of list func-\ntion tasks (Rule et al., 2020; Srivastava et al., 2022)\nwhere the model needs to identify a transforma-\ntion function (e.g., remove the last element in a\nlist) between input and output lists containing non-\nnegative integers. These tasks were evaluated in a\nfour-shot setting, following our evaluation setup in\nSection 3.2. Additionally, we test models on a set\nof simple turing concepts (Telle et al., 2019; Sri-\nvastava et al., 2022) where models need to reason\n972\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n(A) Remove elements\n0\n20\n40\n60\n80\n100\n(B) Modify the list\n0\n20\n40\n60\n80\n100\n(C) Input-independent\n8B 62B 62B-c 540B\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n(D) Add elements\n8B 62B 62B-c 540B\n0\n20\n40\n60\n80\n100\n(E) Miscellaneous\nFlan-PaLM Flan-PaLM + Symbol tuning (ours)\n8B 62B 62B-c 540B\n0\n20\n40\n60\n80\n100\n(F) Simple turing concepts\nFigure 5: Symbol-tuned models achieve higher performance on list function tasks and simple turing concept tasks.\n(A–E): categories of list functions tasks (Rule et al., 2020; Srivastava et al., 2022). (F): simple turing concepts\ntask (Telle et al., 2019; Srivastava et al., 2022). Accuracy per list function category is averaged across all subtasks\n(categories and per-task results are shown in Appendix F.1).\nwith binary strings to learn the concept that maps\nan input to an output (e.g., swapping 0s and 1s in\na string). These tasks have predetermined shots\nfor each evaluation example. We selected these\nalgorithmic tasks because they test the model’s\nability to generalize to different task types (the\nsymbol-tuning tasks were classiﬁcation problems\nwith discrete labels, while these tasks are more\nopen-ended generation problems2) and do not re-\nquire world knowledge (symbol tuning does not\nincrease a model’s prior knowledge).\nIn Figure 5, we show model performance on the\ntwenty list function tasks with the highest human\naccuracy baselines3 (Rule, 2020) separated into\nﬁve categories (category details are described in\nAppendix F.1) and the turing concepts containing 3\nor fewer instructions in the AS II subset of the sim-\nple turing concepts task. On the list function tasks,\nsymbol tuning results in an average performance\n2Although chain-of-thought (Wei et al., 2022b, CoT) can\nbe viewed as an open-ended generation problem, in Ap-\npendix C.2, we found that symbol-tuning did not signiﬁcantly\naffect a model’s CoT reasoning abilities, possibly because our\nsymbol-tuning data did not incorporate any CoT prompts.\n3We do not directly compare with the human baselines\nbecause our evaluation format was different.\nimprovement across all tasks of 18.2% for Flan-\nPaLM-8B, 11.1% for Flan-PaLM-62B, 15.5% for\nFlan-PaLM-62B-cont, and 3.6% for Flan-PaLM-\n540B. On the turing concept tasks, symbol tuning\nresults in a performance improvement of 15.3%\nfor Flan-PaLM-8B and Flan-PaLM-62B, 14.1% for\nFlan-PaLM-62B-cont, and 4.7% for Flan-PaLM-\n540B. Flan-PaLM-62B-cont with symbol tuning\noutperforms Flan-PaLM-540B on the list function\ntasks (in terms of average accuracy across tasks),\nwhich is equal to a ∼10x reduction in inference\ncompute. These improvements on an unseen task\ntype suggest that symbol tuning indeed strength-\nens the model’s ability to learn in-context, as the\nsymbol-tuning procedure did not have algorithmic\ndata and only used natural language data.\n6 Symbol-tuned models can override\npriors via ﬂipped labels\nWei et al. (2023) showed that while pretrained lan-\nguage models (without instruction tuning) could,\nto some extent, follow ﬂipped labels presented in-\ncontext, instruction tuning degraded this ability.\nSymbol tuning, on the other hand, forces models to\n973\n8B 62B 62B-c 540B\n0\n10\n20\n30\n40\n60\n70\n80\n90\n100\n50\nAccuracy (%)\nAverage\nPaLM (base model)\nFlan-PaLM\nFlan-PaLM + Symbol tuning (ours)\n0\n25\n75\n100\n50\nADEC\n0\n25\n75\n100\n50\nOR\n0\n25\n75\n100\n50\nSUBJ\n8B 62B62B-c540B\n0\n25\n75\n100\n50\nTC\n8B 62B62B-c540B\n0\n25\n75\n100\n50\nTEH\n8B 62B62B-c540B\n0\n25\n75\n100\n50\nTOS\nFigure 6: Symbol-tuned models are much better at following ﬂipped labels presented in-context than instruction-\ntuned models are for all model sizes. Instruction-tuned models cannot ﬂip predictions to follow ﬂipped labels\n(performance is well below random guessing), while symbol-tuned models can do this more often (performance\nmatches or is slightly above random guessing). Ground-truth labels for evaluation examples are ﬂipped, so if a\nmodel learns to follow ﬂipped labels, its accuracy should be above random guessing (e.g., a perfect model that can\nfollow ﬂipped labels should get 100% accuracy on our evaluations).\nconsider the label presented in-context as an arbi-\ntrary symbol, which should reduce the model’s us-\nage of prior knowledge that contradicts the ﬂipped\nlabels. For this reason, we expect that symbol tun-\ning would be able to improve and restore the ability\nto follow ﬂipped labels in-context.\nTo test this, we ﬂip the labels of both in-context\nexemplars and the evaluation example for the tasks\ndescribed in Section 3.2 (we remove tasks with\nmore than two labels from this experiment since it\nis unclear how to best “ﬂip” more than two labels).\nFor example, for the SST2 dataset, all exemplars\nthat are labeled as having “positive” sentiment will\nnow be labeled as having “negative” sentiment. A\nperfect model that can follow these ﬂipped labels\nshould achieve 100% accuracy on these tasks if its\naccuracy in the standard ICL setting is also 100%.\nAs shown in Figure 6, symbol tuning restores\nthe ability to follow ﬂipped labels that was lost\nduring instruction tuning. We see that there is a\nsimilar trend across all model sizes—instruction-\ntuned models are generally unable to follow ﬂipped\nlabels (as demonstrated by their performance be-\ning far below random guessing), but symbol-tuned\nmodels are much more capable of doing so. We\nfound that after symbol tuning, Flan-PaLM-8B\nsees an average improvement across all datasets\nof 26.5%, Flan-PaLM-62B sees an improvement\nof 33.7%, and Flan-PaLM-540B sees an improve-\nment of 34.0%. For some datasets (e.g., OR, SUBJ,\nTC), symbol-tuned models can now override pri-\nors and follow ﬂipped labels (i.e., achieve much\nbetter performance than random guessing), despite\ninstruction-tuned models not being able to do so for\nany datasets. Additionally, symbol-tuned models\nmatch or beat pretraining-only models in terms of\naverage performance, indicating that symbol tuning\nhas, to some extent, restored the model’s original\nability to follow ﬂipped labels.\nThese results further indicate another type of\ngeneralized in-context learning capability, as we\ndid not include any ﬂipped labels during symbol\ntuning. Although the performance improvement\nfrom symbol tuning is large, we note that more\nwork should be done in this area since performance\non the ﬂipped-labels settings is, on average, not\nsigniﬁcantly better than random guessing.\n7 Related work\n7.1 In-context learning via semantic prior\nknowledge\nRecent studies on in-context learning suggest\nthat prior knowledge plays a signiﬁcant role in\nhow models learn in-context. For example, Wei\net al. (2023) showed that some small models and\ninstruction-tuned models cannot follow ﬂipped la-\nbels presented in-context, suggesting that these\nmodels primarily utilize prior knowledge for in-\n974\ncontext learning. Min et al. (2022b) found a simi-\nlar result that using random ground-truth labels in\nin-context exemplars does not signiﬁcantly affect\nperformance, meaning that performance may be\ndriven by other factors such as the label space.\nReynolds and McDonell (2021) also showed\nthat cleverly-constructed prompts in a zero-shot\nsetting could outperform prompts in a few-shot\nsetting, implying that, for some tasks, models can\nachieve better performance by leveraging their ex-\nisting knowledge than from attempting to learn the\ntask from in-context exemplars. Additionally, in\nchain-of-thought prompting (Wei et al., 2022b),\nMadaan and Yazdanbakhsh (2022) and Wang et al.\n(2022) showed that performance on multi-step rea-\nsoning tasks does not decrease when models are\nprovided with logically-incorrect prompts. Raghu\net al. (2020) also demonstrated that systems such\nas MAML can effectively “memorize” labels when\ntrained in a way where all labels can be memorized,\nwhich further illustrates that, when possible, mod-\nels may attempt to use prior knowledge rather than\nadapt to each new task.\nOur ﬁndings do not dispute the idea that seman-\ntic prior knowledge can provide signiﬁcant ben-\neﬁts to in-context learning. Indeed, we showed\nthat instruction-tuned models cannot follow ﬂipped\nlabels in-context, which is consistent with the ﬁnd-\nings from Wei et al. (2023). We instead aim to\ndemonstrate that through symbol tuning, language\nmodels can retain the beneﬁts of utilizing prior\nknowledge while also improving their ability to\nlearn from input–label pairs shown in-context.\n7.2 In-context learning via in-context\nexemplars\nAt the same time, however, other recent work has\nsuggested that language models can, in fact, learn\nin-context using the given exemplars. This abil-\nity may be more useful than the ability to use se-\nmantic prior knowledge because it would allow\nmodels to perform tasks that are not seen in or\ncontradict pretraining data. Garg et al. (2022), for\ninstance, showed that transformers trained from\nscratch can perform in-context learning on linear-\nregression tasks at a similar performance level as\nthe least-squares estimator. This capability was\nshown to result from transformers implementing\nstandard learning algorithms such as gradient de-\nscent (Akyürek et al., 2023; von Oswald et al.,\n2022; Dai et al., 2023). Furthermore, Webson and\nPavlick (2022) demonstrated that, in a natural lan-\nguage setting, language models can learn at the\nsame rate during ﬁnetuning even when given irrel-\nevant or misleading prompts. On a broader level,\nRajendran et al. (2020) and Yin et al. (2020) found\nthat adding noise to, shufﬂing, or regularizing the\nlabel space can make systems better at learning and\nadapting to new tasks.\nIn this paper, we attempt to improve the degree\nto which language models are able to learn tasks via\ninput–label mappings. Our symbol-tuning method\ncan be seen as a form of label augmentation and\nis thus similar to the proposed methods from Ra-\njendran et al. (2020) and Yin et al. (2020), though\nit differs crucially in that we apply them to tune\nlarge language models. Additionally, We found\nthat symbol-tuned models saw signiﬁcant improve-\nments in their ability to learn in-context (e.g., on\nalgorithmic tasks or settings with underspeciﬁed\nprompts), which supports the idea that langauge\nmodels have the ability to learn in-context using\nthe given exemplars.\n7.3 Tuning language models\nOur work presented symbol tuning, a form of\nﬁnetuning on input–label pairs where labels are\nremapped to arbitrary symbols. Symbol tuning\nrelates to a broader body of work showing that\nﬁnetuning language models can signiﬁcantly alter\ntheir behavior and performance in different settings.\nFor example, Wei et al. (2022a) ﬁrst presented in-\nstruction tuning (ﬁnetuning on tasks phrased as\ninstructions) and showed that this ﬁnetuning pro-\ncedure substantially improves model performance\nin zero-shot settings. Chung et al. (2022) further\nscaled this procedure by adding more tasks, in-\ncreasing model sizes, and adding chain-of-thought\ndata, demonstrating that, with these changes, tuned\nmodels are signiﬁcantly better at chain-of-thought\nreasoning, open-ended generation, and several eval-\nuation benchmarks.\nOur experimental ﬁndings match these results\nin terms of showing that ﬁnetuning can signiﬁ-\ncantly alter model performance. Our work differs,\nhowever, by not only focusing on settings with\nin-context exemplars and underspeciﬁed prompts,\nbut also by modifying the ﬁnetuning procedure to\nmake tasks harder to learn and require additional\nreasoning with in-context exemplars.\n975\n8 Limitations\nWhile our study presents a simple yet effective\nmethod of improving in-context learning for lan-\nguage models, there are several limitations to our\nwork. An open question is how to apply symbol\ntuning in a generative setting—we symbol tuned\nmodels on a range of classiﬁcation tasks with dis-\ncrete labels so that we can remap labels to arbitrary\nsymbols, but we did not tune on generation tasks\nsince it is unclear how to remap outputs to symbols\nin those settings. Future work could thus explore\nwhether symbol tuning can be applied in a genera-\ntive setting.\nAdditionally, our symbol-tuning procedure used\n22 NLP datasets—while we ablated the number\nof datasets in Appendix B.4 and saw that increas-\ning the number of datasets used for symbol tun-\ning generally improves performance, we did not\nexperiment with adding more tasks. Prior work,\nhowever, has demonstrated that scaling up ﬁnetun-\ning methods can improve their impact on language\nmodels (Chung et al., 2022), so a natural extension\nwould be to examine whether further scaling up the\nsymbol-tuning method would have a similar result.\nFurthermore, we applied symbol tuning to a fam-\nily of instruction-tuned language models. It is un-\nknown, however, whether the effects of symbol\ntuning that we showed may be affected by changes\nto the pretraining objective, model architecture, or\ntraining process. Similarly, symbol tuning may\nhave different effects on language models that are\nnot instruction tuned, as we did not speciﬁcally ex-\nperiment on this factor. For this reason, future work\nmay investigate how these factors impact the effec-\ntiveness of symbol tuning for improving in-context\nlearning abilities in language models.\nBecause we only experimented with one fam-\nily of language models, it is still unclear whether\nsymbol tuning is effective for other models. Apply-\ning symbol tuning to other language models would\nlikely require adjustments to the ﬁnetuning proce-\ndure to be successful (e.g., number of ﬁnetuning\nsteps, mixing with previous data, number datasets),\nbut a deﬁnitive conclusion about these factors can-\nnot be drawn without further experimentation. We\nthus note that a crucial direction for future work\nis to explore how well symbol tuning translates to\nother language models.\n9 Conclusions\nIn this paper, we presented symbol tuning, a new\nmethod of tuning models on tasks where natural\nlanguage labels are remapped to arbitrary symbols.\nSymbol tuning is based off of the intuition that\nwhen models cannot use instructions or relevant\nlabels to determine a presented task, it must do\nso by instead learning from in-context exemplars.\nWe tuned four language models (Flan-PaLM-8B,\nFlan-PaLM-62B, Flan-PaLM-62B-cont, and Flan-\nPaLM-540B) using our symbol-tuning procedure,\nutilizing a tuning mixture of 22 datasets and ap-\nproximately 30k arbitrary symbols as labels.\nExperimentally, we showed that symbol tuning\ncan signiﬁcantly improve a model’s ability to learn\nfrom in-context exemplars in not only natural lan-\nguage settings, but also on algorithmic tasks. First,\nwe showed that symbol tuning improves perfor-\nmance on unseen in-context learning tasks, espe-\ncially when prompts do not contain instructions or\nrelevant labels. We also found that symbol-tuned\nmodels were much better at algorithmic reasoning\ntasks, despite the lack of numerical or algorithmic\ndata in the symbol-tuning procedure. Finally, in\nan in-context learning setting where inputs have\nﬂipped labels, symbol tuning (for some datasets)\nreunlocks the ability to follow ﬂipped labels that\nwas lost during instruction tuning.\nThrough symbol tuning, we aim to have in-\ncreased the degree to which models can exam-\nine and learn from input–label mappings during\nin-context learning. We hope that our results en-\ncourage further work towards improving language\nmodels’ ability to reason over symbols presented\nin-context.\n976\nReferences\nEkin Akyürek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2023. What learn-\ning algorithm is in-context learning? Investigations\nwith linear models. In International Conference on\nLearning Representations.\nNeel Alex, Eli Liﬂand, Lewis Tunstall, Abhishek\nThakur, Pegah Maham, C. Jess Riedel, Emmie\nHine, Carolyn Ashurst, Paul Sedille, Alexis Car-\nlier, Michael Noetel, and Andreas Stuhlmüller. 2021.\nRAFT: A real-world few-shot text classiﬁcation\nbenchmark. In Conference on Neural Information\nProcessing Systems.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela Sanguinetti.\n2019. SemEval-2019 Task 5: Multilingual detec-\ntion of hate speech against immigrants and women\nin Twitter. In International Workshop on Semantic\nEvaluation.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. PIQA: Reasoning about\nphysical commonsense in natural language. In Con-\nference of the Association for the Advancement of\nArtiﬁcial Intelligence.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Conference on Neural Information Pro-\ncessing Systems.\nZihang Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2017. Quora question pairs.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker Schuh,\net al. 2022. PaLM: Scaling language modeling with\nPathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. 2022.\nScaling instruction-ﬁnetuned language models.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Conference on Language Resources and\nEvaluation.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,\nand Furu Wei. 2023. Why can GPT learn in-context?\nLanguage models secretly perform gradient descent\nas meta-optimizers. In Workshop on Understanding\nFoundation Models at the International Conference\non Learning Representations.\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gre-\ngory Valiant. 2022. What can transformers learn\nin-context? A case study of simple function classes.\nIn Conference on Neural Information Processing Sys-\ntems.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nSakaguchi Keisuke, Le Bras Ronan, Bhagavatula Chan-\ndra, and Choi Yejin. 2021. WinoGrande: An adver-\nsarial winograd schema challenge at scale. Communi-\ncations of the Association for Computing Machinery.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Interna-\ntional Conference on the Principles of Knowledge\nRepresentation and Reasoning.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models. In\nConference on Neural Information Processing Sys-\ntems.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions.\nXin Li and Dan Roth. 2002. Learning question classi-\nﬁers. In Conference on Computational Linguistics.\nAman Madaan and Amir Yazdanbakhsh. 2022. Text\nand patterns: For effective chain of thought, it takes\ntwo to tango.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022a. MetaICL: Learning to learn\nin context. In Conference of the North American\nChapter of the Association for Computational Lin-\nguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022b. Rethinking the role of demonstrations:\n977\nWhat makes in-context learning work? In Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the Association for Computational\nLinguistics.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemEval-2016 Task 6: Detecting stance in tweets.\nIn International Workshop on Semantic Evaluation.\nAllen Newell. 1980. Physical symbol systems. Cogni-\ntive Science.\nAllen Newell and Herbert A. Simon. 1976. Computer\nscience as empirical inquiry: Symbols and search. In\nCommunications of the Association for Computing\nMachinery.\nOpenAI. 2023. GPT-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Conference on Neural Informa-\ntion Processing Systems.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\nAssociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research.\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and\nOriol Vinyals. 2020. Rapid learning or feature reuse?\ntowards understanding the effectiveness of MAML.\nIn International Conference on Learning Representa-\ntions.\nJanarthanan Rajendran, Alexander Irpan, and Eric Jang.\n2020. Meta-learning requires meta-augmentation. In\nConference on Neural Information Processing Sys-\ntems.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Conference on\nEmpirical Methods in Natural Language Processing.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\nConference on Human Factors in Computing Sys-\ntems.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 Task 4: Sentiment analysis in twitter.\nIn International Workshop on Semantic Evaluation.\nJoshua S. Rule, Joshua B. Tenenbaum, and Steven T.\nPiantadosi. 2020. The child as hacker. Trends in\nCognitive Sciences.\nJoshua Stewart Rule. 2020. The child as hacker: build-\ning more human-like models of learning. Ph.D. the-\nsis, Massachusetts Institute of Technology.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Tali Bers, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nAdam Santoro, Andrew K. Lampinen, Kory W. Mathew-\nson, Timothy P. Lillicrap, and David Raposo. 2021.\nSymbolic behaviour in artiﬁcial intelligence.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V . Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2022. Challenging\nBIG-Bench tasks and whether chain-of-thought can\nsolve them.\nJan Arne Telle, José Hernández-Orallo, and Cèsar Ferri.\n2019. The teaching size: computable teachers and\nlearners for universal languages. Machine Learning.\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. SemEval-2018 Task 3: Irony detection in en-\nglish tweets. In International Workshop on Semantic\nEvaluation.\n978\nJohannes von Oswald, Eyvind Niklasson, Ettore Ran-\ndazzo, João Sacramento, Alexander Mordvintsev, An-\ndrey Zhmoginov, and Max Vladymyrov. 2022. Trans-\nformers learn in-context by gradient descent.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Conference on Neural Information\nProcessing Systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP Workshop at the Conference on Empirical\nMethods in Natural Language Processing.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022.\nTowards understanding chain-of-thought prompting:\nAn empirical study of what matters.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Conference of the North American\nChapter of the Association for Computational Lin-\nguistics.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In Interna-\ntional Conference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nConference on Neural Information Processing Sys-\ntems.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, and Tengyu Ma. 2023.\nLarger language models do in-context learning dif-\nferently.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.\nCrossFit: A few-shot learning challenge for cross-\ntask generalization in NLP. In Conference on Empir-\nical Methods in Natural Language Processing.\nMingzhang Yin, George Tucker, Mingyuan Zhou,\nSergey Levine, and Chelsea Finn. 2020. Meta-\nlearning without memorization. In International Con-\nference on Learning Representations.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 Task 6: Identifying and catego-\nrizing offensive language in social media (offenseval).\nIn International Workshop on Semantic Evaluation.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Conference on Neural Information Pro-\ncessing Systems.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proceedings of the North American Chapter\nof the Association for Computational Linguistics.\n979",
  "topic": "Symbol (formal)",
  "concepts": [
    {
      "name": "Symbol (formal)",
      "score": 0.6542316675186157
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6331081390380859
    },
    {
      "name": "Chen",
      "score": 0.6286861896514893
    },
    {
      "name": "Computer science",
      "score": 0.5806123614311218
    },
    {
      "name": "Natural language processing",
      "score": 0.5614423751831055
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5264252424240112
    },
    {
      "name": "Natural language",
      "score": 0.42053312063217163
    },
    {
      "name": "Linguistics",
      "score": 0.34493136405944824
    },
    {
      "name": "Speech recognition",
      "score": 0.3421706557273865
    },
    {
      "name": "Arithmetic",
      "score": 0.32605913281440735
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32162269949913025
    },
    {
      "name": "Mathematics",
      "score": 0.21959170699119568
    },
    {
      "name": "Programming language",
      "score": 0.1710992157459259
    },
    {
      "name": "Philosophy",
      "score": 0.10931488871574402
    },
    {
      "name": "History",
      "score": 0.09918087720870972
    },
    {
      "name": "Archaeology",
      "score": 0.08198341727256775
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}