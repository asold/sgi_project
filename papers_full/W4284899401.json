{
  "title": "Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning",
  "url": "https://openalex.org/W4284899401",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5010408896",
      "name": "Przemyslaw Joniak",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2167769188",
      "name": "Akiko Aizawa",
      "affiliations": [
        "National Institute of Informatics",
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4226462293",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W3155655882",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3102914525",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3128232076",
    "https://openalex.org/W2949969209",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3103639864",
    "https://openalex.org/W3033129824",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2889624842",
    "https://openalex.org/W2998072062"
  ],
  "abstract": "Language model debiasing has emerged as an important field of study in the NLP community. Numerous debiasing techniques were proposed, but bias ablation remains an unaddressed issue. We demonstrate a novel framework for inspecting bias in pre-trained transformer-based language models via movement pruning. Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model. We implement our framework by pruning the model while fine-tuning it on the debasing objective. Optimized are only the pruning scores – parameters coupled with the model's weights that act as gates. We experiment with pruning attention heads, an important building block of transformers: we prune square blocks, as well as establish a new way of pruning the entire heads. Lastly, we demonstrate the usage of our framework using gender bias, and based on our findings, we propose an improvement to an existing debiasing method. Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.",
  "full_text": "Proceedings of the The 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 67 - 73\nJuly 15, 2022 ©2022 Association for Computational Linguistics\nGender Biases and Where to Find Them:\nExploring Gender Bias in Pre-Trained Transformer-based Language\nModels Using Movement Pruning\nPrzemyslaw Joniak1\njoniak@g.ecc.u-tokyo.ac.jp\n1The University of Tokyo\nAkiko Aizawa2,1\naizawa@nii.ac.jp\n2National Institute of Informatics\nAbstract\nLanguage model debiasing has emerged as an\nimportant field of study in the NLP community.\nNumerous debiasing techniques were proposed,\nbut bias ablation remains an unaddressed issue.\nWe demonstrate a novel framework for inspect-\ning bias in pre-trained transformer-based lan-\nguage models via movement pruning. Given\na model and a debiasing objective, our frame-\nwork finds a subset of the model containing less\nbias than the original model. We implement our\nframework by pruning the model while fine-\ntuning it on the debiasing objective. Optimized\nare only the pruning scores — parameters cou-\npled with the model’s weights that act as gates.\nWe experiment with pruning attention heads, an\nimportant building block of transformers: we\nprune square blocks, as well as establish a new\nway of pruning the entire heads. Lastly, we\ndemonstrate the usage of our framework using\ngender bias, and based on our findings, we pro-\npose an improvement to an existing debiasing\nmethod. Additionally, we re-discover a bias-\nperformance trade-off: the better the model\nperforms, the more bias it contains.\n1 Introduction\nWhere in language models (LM) is bias stored?\nCan a neural architecture itself impose a bias?\nThere is no consensus on this matter. Kaneko and\nBollegala (2021) suggest that gender bias resides\non every layer of transformer-based LMs. However,\nthis is somehow vague — transformer layers can be\nfurther decomposed into building blocks, namely\nattention heads, and these also can be further bro-\nken down into matrices. On the other hand, the\nfindings of V oita et al. (2019) show that some at-\ntention heads within layers specialize in particular\ntasks, such as syntactic and positional dependen-\ncies. This gives us an intuition that some heads,\nor their parts, may specialize in learning biases as\nwell. Being able to analyze bias in language mod-\nels on a more granular level, would bring us a better\nunderstanding of the models and the phenomenon\nof bias. With knowledge of where the bias is stored,\nwe could design debiasing techniques that target\nparticular parts of the model, making the debiasing\nmore accurate and efficient.\nWe demonstrate a novel framework that utilizes\nmovement pruning (Sanh et al., 2020) to inspect\nbiases in language models. Movement pruning\nwas originally used to compress neural models and\nmake its inference faster. We introduce a modifica-\ntion of movement pruning that enables us to choose\na low-bias subset of a given model, or equivalently,\nfind these model’s weights whose removal leads\nto convergence of an arbitrary debiasing objective.\nSpecifically, we freeze neural weights of the model\nand optimize only the so-called pruning scores that\nare coupled with the weights and act as gates. This\nway, we can inspect which building blocks of the\ntransformers, i.e. attention heads, might induce\nbias. If a head is pruned and the debiasing objec-\ntive converges, then we hypothesize that the head\nmust have contained bias. We demonstrate the util-\nity of our framework using Kaneko and Bollegala\n(2021)’s method of removing gender bias.\nBiases have been extensively studied and nu-\nmerous debiasing methods were proposed. In fact,\naccording to Stanczak and Augenstein (2021), the\nACL Anthology saw an exponential growth of bias-\nrelated publications in the past decade – and it only\ncounts gender bias alone. Nonetheless, the vast\nmajority of these works address problems of bias\ndetection or mitigation only. To our best knowl-\nedge, we are the first to conduct bias ablation in\nLMs. We: (1) demonstrate an original framework\nto inspect biases in LMs. Its novelty is a mixture of\nmovement pruning, weight freezing and debiasing;\n(2) study the presence of gender bias in a BERT\nmodel; (3) propose an improvement to an existing\ndebiasing method, and (4) release our code1.\n1https://github.com/kainoj/\npruning-bias\n67\nBlock Layer Mode SEAT6 SEAT7 SEAT8 SS COLA SST2 MRPC STSB QQP MNLI QNLI RTE WNLI GLUE #P\n32x32\nall token 0.91 0.95 0.92 51.9 0.0 87.2 73.6 46.7 86.8 77.6 83.2 55.2 49.3 62.2 0\nsentence 0.67 -0.40 -0.23 49.5 2.7 87.8 75.4 63.2 86.6 76.2 83.5 54.2 54.9 64.9 0\nlast token 1.39 0.57 0.18 52.6 15.5 90.1 75.8 82.2 86.8 79.5 85.6 57.0 42.3 68.3 0\nsentence 0.85 0.64 0.67 51.9 9.0 89.1 75.1 77.4 87.1 79.3 86.1 56.7 39.4 66.6 0\n64x64\nall token 0.43 0.22 0.01 53.4 4.7 86.5 74.7 76.9 86.4 77.3 83.6 54.5 43.7 65.4 1\nsentence 0.28 0.56 -0.06 49.3 5.9 86.6 73.9 79.1 86.0 77.2 83.0 54.5 47.9 66.0 1\nlast token 0.67 -0.31 -0.36 51.9 0.0 86.4 76.0 80.2 86.4 78.1 83.7 52.7 42.3 65.1 0\nsentence 0.72 0.57 0.03 56.0 4.6 89.2 75.7 84.0 87.0 79.4 85.3 52.3 39.4 66.3 0\n128x128\nall token 0.84 0.47 0.17 50.9 3.3 87.2 74.2 69.9 86.4 77.7 83.8 53.1 50.7 65.1 6\nsentence 0.55 0.17 0.22 54.2 6.6 85.4 75.0 79.3 85.7 76.9 83.0 56.0 42.3 65.6 8\nlast token 0.65 0.17 -0.13 49.1 0.3 85.6 76.8 44.3 86.3 76.7 82.9 52.3 56.3 62.4 2\nsentence 0.10 0.35 -0.22 49.5 0.0 84.4 73.8 75.7 85.6 77.2 82.7 43.7 52.1 63.9 2\n64 × 768\n(entire head)\nall token 0.75 0.49 0.29 57.2 38.8 91.4 78.3 86.3 88.5 82.9 88.6 57.0 56.3 74.3 61\nsentence 0.48 -0.17 0.02 56.0 26.9 90.6 79.2 86.5 88.4 83.4 88.9 57.4 40.8 71.3 66\nlast token 0.62 -0.17 -0.27 58.5 44.6 91.4 78.5 81.4 88.6 82.0 88.9 58.1 52.1 74.0 58\nsentence 0.09 0.05 0.34 58.7 36.7 91.3 76.9 84.7 87.8 81.5 87.9 50.9 43.7 71.3 93\n- original 1.04 0.22 0.63 62.8 58.6 92.8 87.2 88.5 89.4 85.1 91.5 64.3 56.3 79.3 -\nTable 1: Bias in fine-pruned models for various block sizes, evaluated using SEAT and stereotype score(SS).\nIdeally, bias-free model has a SEAT of 0 and SS of 50. GLUE evaluated using only these weights in a model that\nwere not pruned. #P indicates number of heads that were entirely pruned. Best fine-pruning results are in bold.\n2 Background\n2.1 Language Model Debiasing\nNumerous paradigms for language model debiasing\nwere proposed, including feature extraction-based\n(Pryzant et al., 2020), data augmentations (Zhao\net al., 2019; Lu et al., 2020; Dinan et al., 2020), or\nparaphrasing (Ma et al., 2020). They all require\nan extra endeavor, such as feature engineering, re-\ntraining, or building an auxiliary model.\nWe choose an algorithm by Kaneko and Bolle-\ngala (2021) for removing gendered stereotypical as-\nsociations. It is competitive, as it can be applied to\nmany transformer-based models, and requires min-\nimal data annotations. The algorithm enforces em-\nbeddings of predefined gendered words (e.g. man,\nwoman) to be orthogonal to their stereotyped equiv-\nalents (e.g. doctor, nurse) via fine-tuning. The loss\nfunction is a squared dot product of these embed-\nding plus a regularizer between the original and the\ndebiased model. The former encourages orthog-\nonality and the latter helps to preserve syntactic\ninformation.\nThe authors proposed six debiasing modes:\nall-token, all-sentence, first-\ntoken, first-sentence, last-token,\nand last-sentence, depending on source\nof the embeddings (first, last or all layers of a\ntransformer-based model) and target of the loss\n(target token or all tokens in a sentence). In this\nwork, we omit the first-* modes, as they were\nshown to have an insignificant debiasing effect.\n2.2 Block Movement Pruning\nPruning is a general term used when disabling or\nremoving some weights from a neural network.\nIt can lead to a higher sparsity, making a model\nfaster and smaller while retaining its original per-\nformance. Movement pruning, introduced by Sanh\net al., 2020 discards a weight when it moves to-\nwards zero. Lagunas et al., 2021 proposed pruning\nentire blocks of weights: with every weight ma-\ntrix W ∈RM×N , a score matrix S ∈R\nM/M′×N/N′\nis associated, where (M′,N′) is a pruning block\nsize. On the forward pass, W is substituted with\nits masked version, W′∈RM×N :\nW′= W ⊙M(S)\nMi,j = 1\n(\nσ\n(\nS⌈i/M′⌉,⌈j/N′⌉\n)\n>τ\n)\n,\nwhere ⊙stands for element-wise product, σis the\nSigmoid function, τ is a threshold and 1 denotes\nthe indicator function. On the backward pass, both\nW and Sare updated. To preserve the performance\nof the original model, Lagunas et al. (2021) suggest\nusing a teacher model as in the model distillation\ntechnique (Sanh et al., 2019).\nWe decided to utilize movement pruning because\nof the mechanism of the scores S. The scores\ncan be optimized independently of weights, and\nthus we can freeze the weights. This would be\nimpossible with e.g. magnitude pruning (Han et al.,\n2015) which directly operates on weights values\n(magnitudes).\n68\n3 Exploring Gender Bias Using\nMovement Pruning\nWe focus on gender bias defined as stereotypical\nassociations between male and female entities. Our\nstudy is limited to the English language and binary\ngender only.\nWe attempt to answer the following questions:\nin transformer-based pre-trained language models,\ncan we identify particular layers or neighboring\nregions that are in charge of biases? To verify this,\nwe propose a simple and, to our best knowledge,\nnovel framework based on debiasing and attention\nhead block movement pruning. Given a pre-trained\nmodel and a fine-tuning objective, we find which\nattention blocks can be disabled, so the model per-\nforms well on the task. We prune the model while\nfine-tuning it on a debiasing objective, such as the\none described in §2.1. We optimize solely the prun-\ning scores S and the weights W of the original\nmodel remain untouched (they are frozen).\nWe target the building blocks of transformer-\nbased models, attention heads (Vaswani et al.,\n2017). Each head consists of four learnable matri-\nces, and we prune all of them. In §3.1, we test two\nstrategies: pruning square blocks of the matrices\nand pruning entire attention heads.\nTo evaluate bias, we utilize Sentence Encoder\nAssociation Test (SEAT, May et al. (2019) and\nStereoSet Stereotype Score (SS, Nadeem et al.\n(2021) evaluated on the gender domain. To mea-\nsure model performance, we utilize GLUE (Wang\net al., 2018), a standard NLP benchmark.\n3.1 Experiments\nIn all experiments, we use the BERT-base model\n(Devlin et al., 2019). See Appendix for used\ndatasets and detailed hyperparameters.\nSquare Block Pruning. Lagunas et al. (2021)\nshowed that square block pruning in attention head\nmatrices leads to the removal of whole attention\nheads. Although our objective differs from theirs,\nwe attempt to reproduce this behavior. To find\nthe best square block size (B,B), we experiment\nwith B = 32,64,128. See Tab. 1. We also tried\nwith B = 256 ,384, and 768, but we discarded\nthese values as we faced issues with convergence.\nChoosing a suitable block size is a main limitation\nof our work.\nAttention Head Pruning. To remove entire at-\ntention heads, we cannot prune all head matrices\n0 2 4 6 8 10\n0.35\n0.40\n0.45Density (%)\nall-sentence\n32 × 32 64 × 64 128 × 128\n0 2 4 6 8 10\n0.35\n0.40\n0.45\n0.50\n0.55Density (%)\nall-token\n0 2 4 6 8 10\n0.35\n0.40\n0.45\n0.50\n0.55Density (%)\nlast-sentence\n0 2 4 6 8 10\nLayer\n0.40\n0.45\n0.50\n0.55Density (%)\nlast-token\nFigure 1: Per-layer densities of fine-pruned models\nusing different debiasing modes, for multiple square\nblock sizes. Density is computed as a percentage of\nnon-zero elements within a layer.\n0 1 2 3 4 5 6 7 8 9 10 11\nlayer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11 head\nall-token\n0 1 2 3 4 5 6 7 8 9 10 11\nlayer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nall-sentence\n0 1 2 3 4 5 6 7 8 9 10 11\nlayer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nlast-token\n0 1 2 3 4 5 6 7 8 9 10 11\nlayer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nlast-sentence\nFigure 2: Pruning entire heads: which heads remained\n(blue) and which heads were pruned (gray)?\nat once – see Appendix for a detailed explanation.\nInstead, we prune 64×768 blocks (size of the atten-\ntion head in the BERT-base) of thevalues matrices\nsolely. See the last row group of Tab. 1 for the\nresults.\n3.2 Discussion\nSquare Block Pruning Does Not Remove Entire\nHeads Lagunas et al., 2021 found that pruning\nsquare block removes entire heads. However, we\nfailed to observe this phenomenon in the debiasing\nsetting–see last column of Tab 1. We are able to\nprune at most 8 heads, only for relatively large\nblock sizes, 128 ×128. We hypothesize that the\nreason is the weight freezing of the pre-trained\nmodel. To verify this, we repeat the experiment\nwith 32 ×32 block size, but we do not freeze the\nweights. Bias did not change significantly, but no\nattention heads were fully pruned (Tab. 2). This\nsuggests that bias may not be encoded in particular\nheads, but rather is distributed over multiple heads.\nPerformance-Bias Trade-off We observe that\nthere is a negative correlation between model per-\nformance and its bias (Fig. 3). Models that contain\nno bias, i.e. with SS close to 50, perform poorly.\n69\nSEAT6(∆) SEAT7(∆) SEAT8(∆) GLUE(∆) #P\nall token 1.25 (+0.3) 0.54 (-0.4) 0.58 (-0.3) 74.0 (+12) 0\nsent. 1.10 (+0.4) 0.48 (+0.1) 0.18 (+0.0) 71.5 (+7) 0\nlast token 1.31 (-0.1) 0.43 (-0.1) 0.45 (+0.3) 74.4 (+6) 0\nsent. 1.24 (+0.4) 0.82 (+0.2) 0.72 (+0.0) 72.2 (+6) 0\nTable 2: SEAT, GLUE, and number of fully pruned at-\ntention heads (#P) for the 32 ×32 block pruning when\nallowing the weight of the model to change. ∆ refers to\na relative change to results in Tab. 1, that is when the\noriginal weights are frozen.\nlayer modeCOLA SST2 MRPC STSB QQP MNLI QNLI RTE WNLI GLUE\nall token 42.090.8 79.5 85.6 88.382.8 89.5 58.549.3 74.0\nsentence 33.3 90.7 78.8 84.4 88.3 82.4 88.9 48.7 47.9 71.5\nlast token 41.391.180.585.788.582.889.358.5 52.1 74.4\nsentence 40.2 90.680.785.288.581.9 88.2 49.8 45.1 72.2\nTable 3: Breakdown of the GLUE scores when fine-\npruning BERT on the debiasing objective with block\nsize 32 ×32 and letting the model’s weights change\nfreely.\n62.5 65.0 67.5 70.0 72.5 75.0 77.5 80.0\nGLUE\n48\n50\n52\n54\n56\n58\n60\n62Stereotype Score\nIdeal, bias-free model\n32 × 32\n64 × 64\n128 × 128\n64 × 768 (values)\n64 × 768 (shared)\noriginal\nFigure 3: Performance-bias trade-off for various models.\nThe better a model performs, the more bias it has.\nThe model with the best GLUE contains the most\nbias. This phenomenon might be an inherent weak-\nness of the debiasing algorithm. To alleviate the\nissue, it might be necessary to improve the algo-\nrithm, work on a better one, or focus on debiasing\ndata. It would be also interesting to try optimizing\nthe debiasing and the downstream task objective\nsimultaneously. However, this is out of the scope\nof our study and we leave it for future work.\nThe Failure of the CoLA GLUE TaskOur mod-\nels perform poorly on the Corpus of Linguistic\nAcceptability task (CoLA, Warstadt et al. (2019)).\nMost of them have scores close to zero, meaning\ni.e they take a random, uninformed guess. The\nreason might lay in the complexity of the task.\nCoLA remains the most challenging task out of\nthe whole GLUE suite as it requires deep syntactic\nand grammatical knowledge. It has been suggested\nthat language models do not excel at grammatical\nreasoning (Sugawara et al., 2020), and it might\nbe that perturbations such as the absence of the\nweights (pruning) break already weak grammatical\nabilities. The results in Tab. 3 support this hypothe-\nsis. Compared to the ‘frozen’ setting, CoLA scores\nare significantly higher, whereas the other tasks see\njust a slight increase (Tab. 3).\n4 Debiasing Early Intermediate Layers Is\nCompetitive\nKaneko and Bollegala (2021) proposed three\nheuristics: debiasing the first, last, and all layers.\nHowever, the number of layer subsets that can be\ndebiased is much larger. Trying all subsets to find\nthe best one is prohibitively expensive. With our\nframework, we are able to find a better subset with\na low computational cost.\nWe observed that: (1) square block pruning does\nnot significantly affect the first and last layer: den-\nsities of these layers are usually higher than the\nother layers’ (Fig. 1); (2) attention head pruning\nmostly affects intermediate layers (Fig. 2). Based\non the above, we propose to debias intermediate\nlayers. Specifically, we take the embeddings from\nlayers index 1 to 4 inclusive, and we run the de-\nbiasing algorithm described in §2.1. We do not\ninclude layer 0 because it generally yields high\ndensities (ref. Fig.1), and layer 5, as it contains\nthe most number of heads that were not pruned\nin every experiment (ref. Fig. 2). We end up\nwith two more modes, intermediate-token\nand intermediate-sentence. We present\nresults for our, as well as the other modes in Tab. 4\n(note that the results may differ from Kaneko and\nBollegala (2021)’s due to random seed choice).\nDebiasing the intermediate layers is competitive\nto debiasing all and last layers. The SS of\nthe intermediate- modes is lower that the SS\nof corresponding all and last modes. The SS\nof intermediate-sentence gets close to the\nperfect score of 50.\n5 Conclusion\nWe demonstrate a novel framework to inspect\nsources of biases in a pre-trained transformer-based\nlanguage model. Given a model and a debiasing\nobjective, the framework utilizes movement prun-\ning to find a subset that contains less bias than the\noriginal model. We present usage of our frame-\nwork using gender bias, and we found that the\n70\nLayer Mode SEAT6 SEAT7 SEAT8 SS GLUE\nall token 1.02 0.22 0.63 61.5 78.7\nsent. 0.98 -0.34 -0.29 56.9 75.2\nlast token 0.98 0.12 0.79 60.9 78.6\nsent. 0.39 -0.89 -0.11 61.6 78.7\ninterm. token 1.03 0.33 0.84 58.5 77.7\nsent. 0.83 0.49 0.92 53.5 74.7\noriginal 1.04 0.18 0.81 62.8 79.3\nTable 4: Debiasing-only results for various modes, in-\ncluding our original intermediate mode (no prun-\ning involved).\nbias is mostly encoded in intermediate layers of\nBERT. Based on these findings, we propose two\nnew debiasing modes that reduce more bias than\nexisting modes. Bias is evaluated using SEAT\nand Stereotype Score metric. Lastly, we explore\na performance-bias trade-off: the better the model\nperforms on a task, the more gender bias it has.\nWe hope that in the future our framework will\nfind more applications, not only limited to gender\nbias.\nReferences\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356:183 – 186.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8173–8188, Online. As-\nsociation for Computational Linguistics.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015. Learning both weights and connections for\nefficient neural network. ArXiv, abs/1506.02626.\nMasahiro Kaneko and Danushka Bollegala. 2019.\nGender-preserving debiasing for pre-trained word\nembeddings. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1641–1650, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMasahiro Kaneko and Danushka Bollegala. 2021. De-\nbiasing pre-trained contextualised embeddings. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, pages 1256–1266, Online.\nAssociation for Computational Linguistics.\nAvrupa Komisyonu. 2020. A union of equality:\nGender equality strategy 2020-2025. 2020b),\nhttps://eurlex.europa.eu/legal-content/EN/TXT.\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander Rush. 2021. Block pruning for faster trans-\nformers. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10619–10629, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Aman-\ncharla, and Anupam Datta. 2020. Gender bias\nin neural natural language processing. ArXiv,\nabs/1807.11714.\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin\nChoi. 2020. PowerTransformer: Unsupervised con-\ntrollable revision for biased language correction. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7426–7441, Online. Association for Computa-\ntional Linguistics.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 622–628, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gre-\ngory Frederick Diamos, Erich Elsen, David García,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\nsion training. ArXiv, abs/1710.03740.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.\nAutomatically neutralizing subjective bias in text. In\nAAAI.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander M. Rush.\n2020. Movement pruning: Adaptive sparsity by fine-\ntuning. ArXiv, abs/2005.07683.\n71\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\nArXiv, abs/2112.14168.\nSaku Sugawara, Pontus Stenetorp, Kentaro Inui, and\nAkiko Aizawa. 2020. Assessing the benchmarking\ncapacity of machine reading comprehension datasets.\nArXiv, abs/1911.09241.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv,\nabs/1910.03771.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 629–634, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kai-\nWei Chang. 2018. Learning gender-neutral word em-\nbeddings. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4847–4853, Brussels, Belgium. Association\nfor Computational Linguistics.\nAppendix\nDatasets\nSentence Encoder Association Test (SEAT,\nMay et al. (2019)) is based on Word Embedding\nAssociation Test (WEAT, Caliskan et al. (2017)).\nGiven two sets of attributes and two sets of targets\nwords, WEAT measures differential cosine similar-\nity between their embeddings. The two attribute\nsets can be male- and female-focused, where the tar-\ngets can contain stereotypical associations, such as\nscience- and arts-related vocabulary. SEAT extends\nthe idea by embedding the vocabulary into sen-\ntences and taking their embedding representation\n([CLS] classification token in case of transformer-\nbased models). SEAT measures bias only in the\nembedding space. That is, a model with a low\nSEAT score may still expose bias, as understood\nand perceived by humans. We employ SEAT6, -7,\nand -8 provided by May et al. (2019).\nStereoSet Stereotype Score (SS, Nadeem et al.\n(2021)) measures bias among four dimensions: gen-\nder, religion, occupation, and race. Technically,\nStereoSet is a dataset where each entry from four\ncategories consists of a context and three options:\nstereotype, anti-stereotype and unrelated. On the\ntop, StereoSet defines two tasks: intrasentence and\nintersentence. The objective of the former is to\nfill a gap with one of the options. The latter aims\nto choose a sentence that best follows the context.\nThe SS score is a mean of scores on intra- and inter-\nsentence tasks. Bias in StereoSet is measured as a\n“percentage of examples in which a model prefers\na stereotypical association [option] over an anti-\nstereotypical association” (Nadeem et al., 2021).\nAn ideal bias-free model would have the bias score\n(stereotype score, SS) of 50. As opposed to SEAT,\nStereoSet SS models bias close to its human per-\nception, as a preference of one thing over another.\nWe use the gender subset, as provided by Nadeem\net al. (2021).\nGeneral Language Understanding Evaluation\n(GLUE, Wang et al. (2018)) is a popular bench-\nmark to evaluate language model performance. It\nis a suite of nine different tasks from domains such\nas sentiment analysis, paraphrasing, natural lan-\nguage inference, question answering, or sentence\nsimilarity. The GLUE score is an average of scores\nof all nine tasks. To evaluate GLUE, we make\nuse of the run_glue.py script shipped by the\nHugging Face library (Wolf et al., 2019).\nGender Debiasing The debiasing algorithm in-\ntroduced in §2.1 requires some vocabulary lists.\nWe follow Kaneko and Bollegala (2021)’s setup,\nthat is we use lists of female and male attributes\n72\nprovided by Zhao et al. (2018), and a list of stereo-\ntyped targets provided by Kaneko and Bollegala\n(2019).\nHyperparameters and Implementation\nFor all experiments, we use the pre-trained\nbert-base-uncased (Devlin et al., 2019)\nmodel from the open-source Hugging Face Trans-\nformers library (Wolf et al. (2019), ver. 4.12;\nApache 2.0license). We use 16-bit floating-point\nmixed-precision training (Micikevicius et al., 2018)\nas it halves training time and does not impact test\nperformance. To disentangle engineering from re-\nsearch, we use PyTorch Lightning framework (ver.\n1.4.2; Apache 2.0 license). Model fine-pruning\ntakes around 3h on a single A100 GPU. All exper-\niments can be reproduced with a random seed set\nto 42.\nUsage of all libraries we used is consistent with\ntheir intended use.\nDebiasing We provide an original implementa-\ntion of the debiasing algorithm. We use the same\nset of hyperparameters as Kaneko and Bollegala\n(2021), with an exception of a batch size of 128.\nWe run debiasing (with no pruning - see 4) for five\nepochs.\nPruning As for the pruning, we follow Lagunas\net al. 2021’s sigmoid-threshold setting without the\nteacher network. The threshold τ increases linearly\nfrom 0 to 0.1 over all training steps. We fine-prune\nthe BERT model with the debiasing objective for\n100 epochs using a patched nn_pruning2 API\n(ver 0.1.2; Apache 2.0license). See README.md\nin the attached code for instructions.\nOn Attention Head Pruning\nWe cannot prune every matrix of the attention head\nif we want to prune the entire head. To see why, let\nus recap the self-attention mechanism popularized\nby Vaswani et al. (2017).\nDenote an input sequence as X ∈RN×d, where\nN is the sequence length and d is a hidden size.\nThe first step of the self-attention is to obtain three\nmatrices: Q,K,V ∈RN×d: queries, keys, and\nvalues: Q = XWQ,K = XWK,V = XWV ,\nwhere WQ,WK,WV ∈Rd×d are learnable matri-\n2https://github.com/huggingface/nn_\npruning/\nces. The self-attention is defined as follows:\nSelfAtt(Q,K,V ) = softmax\n(QKT\n√\nd\n)\nV.\nNow, suppose that the queries WQ or keys WK\nare pruned. Then the softmax would not cancel out\nthe attention, but it would yield a uniform distribu-\ntion over values WV . Only by pruning values WV ,\nwe are able to make the attention output equal zero.\nBias Statement\nWe follow Kaneko and Bollegala (2021) and define\nbias as stereotypical associations between male and\nfemale entities in pre-trained contextualized word\nrepresentations. These representations when used\nfor downstream applications, if not debiased, can\nfurther amplify gender inequalities (Komisyonu,\n2020). In our work, we focus on identifying layers\nof a language model that contribute to the biased\nassociations. We show that debiasing these layers\ncan significantly reduce bias as measured in the\nembedding space (Sentence Encoder Association\nTest, May et al. (2019)) and as perceived by humans,\nthat is, as a preference of one thing over another\n(StereoSet Stereotype Score, May et al. (2019)). We\nlimit our work solely to binary gender bias in the\nEnglish language.\n73",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.9894737601280212
    },
    {
      "name": "Computer science",
      "score": 0.7238546013832092
    },
    {
      "name": "Pruning",
      "score": 0.6672796010971069
    },
    {
      "name": "Language model",
      "score": 0.5768760442733765
    },
    {
      "name": "Transformer",
      "score": 0.5382997393608093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4801470637321472
    },
    {
      "name": "Gender bias",
      "score": 0.44437357783317566
    },
    {
      "name": "Machine learning",
      "score": 0.4206942021846771
    },
    {
      "name": "Natural language processing",
      "score": 0.37523484230041504
    },
    {
      "name": "Psychology",
      "score": 0.13571390509605408
    },
    {
      "name": "Engineering",
      "score": 0.07231572270393372
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Cognitive science",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74801974",
      "name": "The University of Tokyo",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    }
  ],
  "cited_by": 11
}