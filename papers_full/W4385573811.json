{
  "title": "What do Large Language Models Learn beyond Language?",
  "url": "https://openalex.org/W4385573811",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2942719413",
      "name": "Avinash Madasu",
      "affiliations": [
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A2118811920",
      "name": "Shashank Srivastava",
      "affiliations": [
        "University of North Carolina at Chapel Hill",
        "University of North Carolina Health Care"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177361240",
    "https://openalex.org/W2973122905",
    "https://openalex.org/W3017779903",
    "https://openalex.org/W3176893837",
    "https://openalex.org/W2963753324",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3038040931",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2914557243",
    "https://openalex.org/W1746889935",
    "https://openalex.org/W2963723151",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2950645060",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W3094540663",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W3131815757",
    "https://openalex.org/W3098666169",
    "https://openalex.org/W3103671331",
    "https://openalex.org/W2048176942",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2971044268"
  ],
  "abstract": "Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive biases’ for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hithertho unexplored deep connection between pre-training and inductive learning abilities of language models",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6940–6953\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nWhat do Large Language Models Learn beyond Language?\nAvinash Madasu Shashank Srivastava\nUNC Chapel Hill\n{avinashm,ssrivastava}@cs.unc.edu\nAbstract\nLarge language models (LMs) have rapidly be-\ncome a mainstay in Natural Language Process-\ning. These models are known to acquire rich\nlinguistic knowledge from training on large\namounts of text. In this paper, we investi-\ngate if pre-training on text also confers these\nmodels with helpful ‘inductive biases’ for non-\nlinguistic reasoning. On a set of 19 diverse\nnon-linguistic tasks involving quantitative com-\nputations, recognizing regular expressions and\nreasoning over strings. We find that pretrained\nmodels significantly outperform comparable\nnon-pretrained neural models. This remains\ntrue also in experiments with training non-\npretrained models with fewer parameters to ac-\ncount for model regularization effects. We fur-\nther explore the effect of text domain on LMs\nby pretraining models from text from different\ndomains and provenances. Our experiments\nsurprisingly reveal that the positive effects of\npre-training persist even when pretraining on\nmulti-lingual text or computer code, and even\nfor text generated from synthetic languages.\nOur findings suggest a hithertho unexplored\ndeep connection between pre-training and in-\nductive learning abilities of language models1.\n1 Introduction\nPretrained Language Models (LMs) have shown\nsingular succcess on a range of natural language un-\nderstandings tasks, to the extent that they have be-\ncome foundational for contemporary NLP systems.\nSeveral works have investigated why pretraining\nworks so well (Warstadt et al., 2019; Zhao et al.,\n2020). In particular, studies have shown that the\npretrained LMs like BERT capture linguistic knowl-\nedge about syntax (Lin et al., 2019; Wu et al., 2020),\nsemantics (Vuli´c et al., 2020b,a) and morphology\n(Hofmann et al., 2020, 2021). In fact, Tenney et al.\n(2019) demonstrated that learned representations\n1https://github.com/avinashsai/NILM\nFigure 1: We investigate the effect of pretraining of\nlanguages models on learning non-linguistic tasks using\nthree task paradigms involving symbolic reasoning.\nin pretrained LMs even internally reflect the clas-\nsical NLP pipeline. Since most NLP benchmarks\nsuch as SuperGLUE (Wang et al., 2019) naturally\nare focused on tasks such as textual entailment\nand reading comprehension that require linguistic\nknowledge and reasoning, it is unsurprising that\nLMs have achieved strong results on these tasks.\nOn the other hand, little work so far has explored\nthe abilities of pretrained LMs for learning non-\nlinguistic tasks.\nIn this paper, we explore whether pretraining on\ntext is inherently about learning language, or if pre-\ntraining also imbues LMs with skills for symbolic\nmanipulation and non-linguistic reasoning (for ex-\nample, performing quantitative computation such\nas finding the median of a set of numbers, recog-\nnizing regular expressions, or identifying whether\na string is a palindrome, as shown in Figure 1).\nIn other words, we investigate whether and how\npretraining develops helpful inductive biases for\nnon-linguistic reasoning. For this analysis, we cre-\nate a set of 19 tasks from three categories of task\nparadigms: quantitative computation (§3.1), recog-\nnizing regular expressions (§3.2), and string rea-\nsoning (§3.3). Figure 1 shows an example for each\ncategory, and the full list of tasks is described in the\ntable 1. We experiment with transformer and RNN\nbased LMs (§4) for learning these tasks, and per-\n6940\nTask Input Eg. Output Eg. Classes Input range\nOdd classification 4210 0 0 - 1 [1, 20000]\nEven classification 4210 1 0 - 1 [1, 20000]\nOdd even classification 4210 even 1 0 - 1 [1, 20000]\nDecimal operation 872 / 436 2 0 - 9 [1, 10000]\nDecimal & word operation four / 2 2 0 - 9 [1, 10000]\nMean 15,-8,15,-5,-14,-3 ? 0 0 - 9 [-15, 15]\nMedian 3,6,5,15,2,3,-6,-2,9,-3,-9,-5,-14 ? 2 0 - 9 [-15, 15]\nMode 5,9,7,0,2,5,3,3,3,0 ? 3 0 - 9 [0, 9]\nRecognize {0, 1, 2}*02* 0 1 2 0 2 1 0 2 2 2 2 1 0 - 1 [0, 2]\nRecognize AA*BB*CC*DD*EE* a a a a a a a b b b b c c c c c d d d e1 0 - 1 [a, e]\nPalindrome classification a W X X W a 1 0 - 1 [a-z], [A-Z]\nAnagram classification r G r P J h k - k h G r P J r 1 0 - 1 [a-z],[A-Z]\nIsogram classification v F J o S j 1 0 - 1 [a-z], [A-Z]\nTautonym classification s t P v g - t P v g a 1 0 - 1 [a-z], [A-Z]\nLength of a string t e e o 4 0 - 9 [a-z]\nCount of unique characters d e i i e e d i i d 3 0 - 9 [a-j]\nParity check 0 1 1 1 0 1 0 0 1 1 1 0 0 0 - 1 [0, 1]\nV owels classification i i v x c m o o u o 0 0 - 9 [a-z]\nMaximum frequent character j j j c j j 9 (j) 0 - 9 [a-j]\nTable 1: Description of the non-linguistic tasks with input and output examples. Classes are the class labels for each\ntask. Input range denotes the range of the input operands in each task.\nform a comparative analysis with (non-pretrained)\nneural model variants from the perspective of learn-\ning metrics such as accuracy and sample efficiency.\nOur experiments (§5) reveal that pretrained mod-\nels overall perform substantially better and are\nmore sample efficient on most tasks. However,\nthere are significant differences and patterns in per-\nformance between task types, as well as variance\nbetween different LM architectures. Since non-\npretrained models do not have the benefit of reg-\nularization that comes from pretraining, a plausi-\nble reason for the discrepancy between them and\npretrained LMs might be underfitting of the non-\npretrained models when trained on comparatively\nsmall dataset sizes. To account for this, we also\ncomprehensively explore the effect of model size\n(§6) of non-pretrained models for both transformer\nand RNN architectures. We find that the discrep-\nancy in performance remains even for smaller neu-\nral models, indicating that the differences are not\nsimply due to a mismatch in model and data sizes.\nFinally, we investigate the role that pretraining\ndata plays in influencing task performance on non-\nlinguistic tasks (§7). We experiment with pretrain-\ning on different domains of text, pretraining on\nperturbed representations of natural language text\n(such as shuffled word order), pretraining on text of\ncomputer programs (no linguistic properties of nat-\nural languages), pretraining on multi-lingual and\nnon-English text, and pretraining with synthetic\ntext (data sampled from synthetic distributions).\nOur analysis reveals that the advantages of pretrain-\ning surprisingly persist with various degrees across\nthese variations, suggesting hithertho unexplored\nconnections between pretraining and the learning\nabilities of language models. Our contributions are:\n• We compare a range of pretrained LMs and non-\npretrained models on a carefully designed suite of\n19 classifications tasks that require non-linguistic\nreasoning.\n• We comprehensively explore the role of the pre-\ntraining data by experimenting with models pre-\ntrained from texts with different provenances.\n• We establish that the positive effects of pretrain-\ning are not simply due to better model regulariza-\ntion by experimenting with neural models with\ndifferent complexities and architectures.\n2 Related Work\nA body of work has investigated contextual word\nembeddings to determine whether they capture as-\npects of mathematical meaning for numbers (Naik\net al., 2019). Wallace et al. (2019) probed numer-\nical supremacy on token embeddings of contex-\ntual language models such as ELMO and BERT.\n(Thawani et al., 2021) surveyed numerical under-\nstanding in NLP models using 7 sub-tasks such as\nmeasurement estimation and word problems. Our\nwork diverges from these in exploring a richer set of\ntasks including harder tasks such as set operations.\nFurther, previous methods explore mathematical\nreasoning tasks posed as language problems, which\n6941\nconflates the problems of language and mathemati-\ncal learning and also makes the datasets susceptible\nto biases due to data collection. Our analysis cir-\ncumvents both these issues by design.\nSome previous works have explored the ability\nof RNN and Transformer architectures for learning\nregular languages (Weiss et al., 2018; Sennhauser\nand Berwick, 2018; Suzgun et al., 2019b; Bhat-\ntamishra et al., 2020), closing brackets (Skachkova\net al., 2018), and dynamic counting (Suzgun et al.,\n2019a). However, they focus on the learnability of\nthese tasks with specific architectures, and do not\nlook at pretrained LMs, which are our focus here.\nFinally, in our discussion, we conceptually\nstretch the notion of inductive bias. The idea of\ninductive bias is usually associated with specific\nmodel types (McCoy et al., 2020; Kharitonov and\nChaabouni, 2021), architectures (Xu et al., 2021;\nBrutzkus and Globerson, 2021) and regularization\napproaches (Helmbold and Long, 2015). We be-\nlieve that extending this to refer to learning tasks\nwith pretrained LMs is both reasonable and useful.\n3 NILM\nIn this section, we describe the tasks used for our\nanalysis, which we refer to as NILM (measuring\nNon-linguistic Inductive bias in Language Models).\nThe tasks correspond to three task paradigms: (1)\nquantitative computation, (2) regular expressions,\nand (3) string reasoning. Each task inNILM is posed\nas a classification task. The descriptions for all the\ntasks with input and output examples, class labels\nand the input range are shown in Table 1. Each task\nhas a synthetically generated dataset with train/de-\nv/test splits2. To avoid biases in the datasets, rel-\nevant numbers and strings in individual examples\nare uniformly sampled from the appropriate ranges.\n3.1 Quantitative computation\nThis task paradigm focuses on tasks involving arith-\nmetic and set statistics.\nOdd classification. Classify if a number is odd.\nEven classification. Classify if a number is even.\nOdd even classification. For a given number N\nand a string “even” or “odd”, classify if the number\nsatisfies the string condition.\nDecimal operation. Subtract or divide two num-\nbers. Operands are represented in decimal notation.\n2The training set size for all tasks is 10K, dev set size is 1K\nand test set size is 1K, except for tasks on recognizing regular\nexpressions, where the test set size is 2K following previous\nwork (Bhattamishra et al., 2020).\nDecimal & word operation. Subtract or divide\ntwo numbers. Operands are represented in decimal\nor word notation.\nMean. Given a set of numbers, output the mean.\nMedian. Given a set, output the median.\nMode. Given a set of numbers, output the mode.\n3.2 Recognizing regular expressions\nThis task paradigm focuses on recognizing regular\nexpressions. The training data consists of positive\nand negative examples of strings matching a regu-\nlar expression (Bhattamishra et al., 2020).\nRecognize {0,1,2}*02*. Recognize if a pattern\nmatches {0,1,2}*02*. The maximum length of the\npatterns is 20.\nRecognize AA*BB*CC*DD*EE*. Recognize if a\npattern matches AA*BB*CC*DD*EE*. The maxi-\nmum length of the patterns is 30.\n3.3 String reasoning\nThis task paradigm focuses on reasoning tasks over\nindividual strings or pairs of strings.\nPalindrome classification. A string is a palin-\ndrome if it reads the same forward and backward.\nThe task is to classify whether a given string is a\npalindrome. The string length ranges from 1 to 15.\nAnagram classification. Two strings are anagrams\nif one is formed by rearranging letters from the\nother. The task is to classify if a pair of strings are\nanagrams. The string length ranges from 2 to 15.\nIsogram classification. A string is an isogram if it\nhas no repeating characters. The task is to classify\nwhether a given string is an isogram. The string\nlength ranges from 1 to 52.\nTautonym classification. A tautonym is a word\nwhich can be broken down into two identical parts,\nwith the same spelling. The task is to classify\nwhether a given string is a tautonym. The string\nlength ranges from 1 to 10.\nLength of a string. Output the length of a given\nstring. The string length ranges from 1 to 10.\nCount of unique characters. Given a string, count\nthe number of unique characters in it. The string\nlengths ranges from 10 to 30.\nParity check. Given a binary string, output if the\ncounts of ones and zeros are the same. The maxi-\nmum length of the binary string is 20.\nVowels classification. Given a string, classify if the\nstring contains only vowel characters. The string\nlength ranges from 3 to 10.\nMaximum frequent character. Given a string,\noutput the character with the maximum frequency.\n6942\n(a) BERT small\n (b) ELMO\nFigure 2: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four\nquantitative computation tasks (odd classification, even classification, odd even classification and decimal operation).\n(a) BERT small\n (b) ELMO\nFigure 3: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four\nquantitative computation tasks (mean, median, mode and decimal & word operation tasks).\nThe string length ranges from 5 to 30.\n4 Models & variants\nNext, we describe the LMs and their variants used\nin NILM. We experiment with four language models,\nbased on both Transformer and RNN architectures.\nBERT small. This is the bert-base-uncased model\nwith 12 transformer encoder layers and the dimen-\nsion of the representations is 768. BERT tokenizer\nis based on the WordPiece model (Wu et al., 2016).\nBERT large. This is the bert-large-uncased model\nwhich has 24 transformer encoders and representa-\ntions have 1024 dimensions.\nDeBERTa. This is a transformer based language\nmodel and its tokenizer is built using Byte Pair En-\ncoding (Sennrich et al., 2016). We consider the De-\nBERTa base model. It has 12 transformer encoder\nlayers and representations have 768 dimensions.\nELMO. This is an LSTM based language model\n(Peters et al., 2018). It has 3 layers and the output\nrepresentations have 1024 dimensions.\nOur experiments are based on pretrained and\nnon-pretrained variants of these architectures. For\npretrained variants, the weights are initialized with\nthe pretrained weights. The tokenization on the\ntraining data is performed using the pre-built vo-\ncabulary. For the non-pretrained neural models,\nthe weights are initialized randomly and updated\nduring training. The tokenizer used is the same as\nin the pretrained variant.\nAll the models are trained with varying train-\ning data of sizes 10, 20, 40, 80, 160, 320, 640,\n1280, 2560, 5120, 6000, 7000, 8000, 9000 and\n10000. For training set sizes of less than 1000 sam-\nples, we report the average of 10 runs. For training\nset sizes greater than 1000, all reported numbers\nare averages of 5 runs. In the next section, we\npresent a comparative analysis of pretrained and\nnon-pretrained models.\n5 Comparative Evaluation\nNext, we compare the performance of pretrained\nand non-pretrained models on tasks in NILM3.\nQuantitative computation: Figure 2 shows results\non odd classification, even classification, odd even\nclassification and decimal operation tasks. We find\nthat pretrained LMs outperformed non-pretrained\nmodel for all of these tasks. Further, Transformer-\n3Details, including statistical significance results with the\npaired t-value test, are included in Appendix 6\n6943\n(a) BERT small\n (b) ELMO\nFigure 4: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on regular\nexpression tasks (AA*BB*CC*DD*EE* and recognize {0,1,2}*02*).\nbased LMs outperformed the RNN-based ELMO\nmodels in all the tasks4. We note that for the rel-\natively easy tasks such as odd and even classifica-\ntions, the pretrained LMs show more stable training.\nHowever, for harder tasks such as Decimal oper-\nations (where the baseline performance is around\n10%), no models are able to learn the task well\neven with 10K labeled examples.\nFigure 3 shows results on median, mean, mode\nand decimal & word operation tasks. The median\ntask requires complex reasoning (sorting numbers\nand computing the middle element), and shows sig-\nnificantly lower performance than the mean and\nmode tasks for the non-pretrained models even\nwith the maximum training set size. The pre-\ntrained LM models show little eventual difference\nin performance between these three tasks. On the\nother hand, for the easiest of these tasks (mode),\nnon-pretrained models actually show higher perfor-\nmance than pretrained LMs in the low data regime.\nRecognizing regular expressions: Figure 4 shows\nthe comparative performance of pretrained LMs on\nnon-pretrained models on the two tasks involving\nrecognizing regular expressions. For both tasks, we\nnote that the pretrained LMs can perfectly learn the\ntasks with many fewer labeled examples compared\nto the non-pretrained models. In both cases, the\nnon-pretrained Transformer-based models eventu-\nally reach optimal performance as well. However,\ncuriously the ELMO based non-pretrained models\nstruggle with learning both tasks.\nString reasoning: Figures 6 show the results on\nPalindrome, Anagram, Isogram and Tautonym clas-\nsification. These tasks require character compari-\nson within the string or with another string. Again,\n4We will focus on BERT small as representative of trans-\nformer models. Results for BERT large and DeBERTa follow\nsimilar trends, and are included in the supplementary material\nthe pretrained variants consistently outperformed\nnon-pretrained models variants in all of these tasks.\nIn particular, the non-pretrained models completely\nfail to learn the Anagram and Palindrome tasks\neven for the largest training set size. Again, Trans-\nformer based LMs outperform LSTM based LMs.\nFigure 7 shows the results on vowels classifi-\ncation, maximum frequent character, length of a\nstring and parity check tasks. These tasks don’t\nrequire intra-string comparisons. We see that most\nTransformer-based variants eventually achieve opti-\nmal performance. For these simpler tasks, we again\nobserve several instances where the Transformer-\nbased non-pretrained models actually outperform\npretrained LMs in the low data regime.\n6 Effect of model size\nFigure 5: Effect of model size on non-pretrained mod-\nels. NP denotes a non-pretrained model and PT denotes\nthe pretrained model. Mid-sized non-pretrained models\noutperform bigger and smaller variants, but still perform\nsignificantly lower than pretrained LM models. Results\nare the average of six representative tasks: palindrome\nclassification, anagram classification, isogram classifi-\ncation, tautonym classification, mean and median.\nAs previously mentioned, a possible explanation\nfor the underperformance of non-pretrained mod-\nels ise that the large number of parameters of the\n6944\narchitecture relative to the sizes of the training data\nmight be leading to under-fitting. To test this, we\nexperiment with smaller Transformer-based mod-\nels with varying numbers of parameters.\nFigure 5 illustrates the effect of model sizes of\nnon-pretrained model. The original 110 million pa-\nrameter model has 12 encoder layers, 12 attention\nheads, and 768 dimensional representations. The\n42 million parameter model has 8 encoder layers,\n8 attention heads and 512 dimensional represen-\ntations. The 29 million parameter model has 4\nencoder layers, 8 attention heads and 512 dimen-\nsional representations. The 11 million parameter\nmodel has 4 encoder layers, 4 attention heads and\n256 dimensional representations. The smallest 4\nmillion parameter model has 2 encoder layers, 2 at-\ntention heads and 128 dimensional representations.\nAs seen in the figure, reducing the model size\nsignificantly improves the average performance of\nthe non-pretrained models over 6 representative\ntasks. However, the smallest models show a perfor-\nmance drop. Most significantly, even the best per-\nforming intermediate-sized architectures are signif-\nicantly worse than the pretrained LM models. This\nstrongly suggests that the discrepancy between pre-\ntrained and non-pretrained models is not simply\ndue to a mismatch between model and data sizes.\n7 Effects of Pretraining Data\nWe observe that pretrained LMs consistently per-\nformed better than non-pretrained models. This\nleads to the natural question of what role the text\ndata used for pretraining plays in the process. Next,\nwe investigate this in depth by experimenting with\nlanguage models pretrained on different types of\ntext. For this, we pretrain models using the BERT-\nsmall and DeBERTa architectures and an MLM\nobjective on different text datasets, and evaluate\nthe performance of these models on NILM tasks.\n7.1 Variance with text domain\nWe first explore models pretrained on three differ-\nent domains of text.\nSNLI. We pretrained BERT small from scratch on\nSNLI data (Bowman et al., 2015). It has 1000k\nsentences (570k pairs of text and hypothesis).\nAmazon reviews. We selected 500k movies and\ntv reviews from the larger Amazon reviews dataset\n(He and McAuley, 2016) and used for pretraining.\nSince reviews are in a free-text format, and their\ncollection was not tailored with a NLP task in mind,\nthey might be more representative of the complex-\nity of real-world language use than SNLI.\nROC. ROC is a corpora of 100K children stories,\neach made up of five sentences (Mostafazadeh\net al., 2017). The language in ROC is relatively\nsimple in both vocabulary and sentence structure.\nTables 2 and 3 shows the average accuracy of\nsix non-linguistic tasks (palindrome classification,\nisogram classification, tautonym classification, odd\neven classification, decimal operation and median)\nfine-tuned using different BERT and DeBERTA\nrepresentations respectively. We note that the mod-\nels pretrained on all three domains outperformed\nthe non-pretrained model (NP). This suggests that\nthe results of experiments in Section 5 generalize\nto new text corpora for pretraining, and do not rely\non having access to text on specific topics during\npretraining. This is a non-trivial result, since it sug-\ngests for example, that the higher performance of\npretrained models on tasks such as palindrome and\nanagram classification is not due to the pretrained\nmodels having seen information about such con-\ncepts during pretraining. This is especially so since\nthe results even generalize to ROC stories, which\ncontain no information on such technical concepts.\n7.2 Perturbed text\nNext, we experiment with perturbing the text used\nfor pretraining by changing the order of words in\nthe text. We explore the following models:\nSNLI sort. The words in the sentences of SNLI\ndataset are sorted based on alphabetical order.\nSNLI shuffle. We randomly shuffle words in sen-\ntences in the SNLI dataset.\nAmazon reviews sort. Similar to SNLI sort, the\nwords in sentences are alphabetically sorted.\nAmazon reviews shuffle. We randomly shuffle\nwords in sentences in the Amazon reviews dataset.\nWe observe that models pretrained with perturbed\ntext also significantly outperformed non-pretrained\nmodels, and perform comparably to the original\npretrained LMs. For the SNLI dataset, there is 3%\ndrop in best performance when pretrained on SNLI\nsort and 2% drop in performance when pretrained\non SNLI shuffle for BERT (Table 2). In fact, for\nDeBERTa, SNLI shuffle outperformed the standard\nSNLI by 2% (Table 3). Similarly, the Amazon\nsort and Amazon shuffle versions outperformed or\nachieved similar performance as the standard Ama-\nzon data version. A likely explanation for this is\nthat, even though syntactic word order is disturbed\n6945\n(a) BERT small\n (b) ELMO\nFigure 6: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on four\nstring reasoning tasks (palindrome, anagram, isogram and tautonym classification).\n(a) BERT small\n (b) ELMO\nFigure 7: Performance comparison of pretrained and non-pretrained models of BERT small, and ELMO on five\nstring reasoning tasks (length of a string, maximum frequent character, vowels classification, parity check and count\nof unique character).\nSample\nsize SNLI\nSNLI\nsort\nSNLI\nshuffle Amz\nAmz\nsort\nAmz\nshuffle ROC\nX-ling\nBERT\nChinese\nBERT\nCode\nBERT Zipf Unif\nSyn\nVoc NP\n10 37 39 38 36 36 36 36 38 38 37 38 36 36 37\n20 37 37 37 36 38 38 38 37 37 38 37 37 37 37\n40 37 38 36 37 36 36 36 42 42 37 42 36 37 37\n80 38 40 40 37 38 38 38 55 55 47 55 36 36 38\n160 38 40 37 37 40 40 40 56 56 37 56 37 37 39\n320 40 49 41 38 41 41 41 64 64 61 64 39 37 41\n640 44 60 47 43 52 52 52 75 75 69 75 42 39 44\n1280 60 71 63 55 69 69 69 80 80 92 80 52 41 50\n2560 76 84 75 75 79 79 79 81 81 89 81 59 48 50\n5120 82 87 82 83 89 89 89 94 94 97 94 71 58 58\n6000 83 87 83 85 90 90 90 94 94 96 94 73 60 59\n7000 88 89 88 89 91 91 91 94 94 97 94 78 62 64\n8000 89 89 88 90 92 92 92 94 94 97 94 81 63 59\n9000 90 90 89 91 92 92 92 94 94 97 94 84 64 59\n10000 91 88 89 91 92 92 92 94 94 97 94 85 64 64\nTable 2: Average accuracy scores of different pretrained BERT representations on six representative non-linguistic\ntasks: palindrome, anagram, isogram, tautonym, mean, and median. The results are rounded to the nearest percentage\npoint. All models except Synthetic V ocabulary (Syn V oc). show statistically significant improvements (p< 0.05)\nover the non-pretrained models.\nby shuffling, distributional information over sen-\ntence contexts is still preserved in the perturbed\ndata. We describe experiments with text data hav-\ning no distributional information in later sections.\n7.3 Non-English and Computer Languages\nA possible rationale for explaining the beneficial\neffect of pretraining for non-linguistic tasks is\nthat irrespective of whether the tasks require non-\nlinguistic reasoning, their format is in language,\nand hence language models should be able to learn\nthese tasks with fewer examples. To test this hy-\npothesis, we also experiment with models pre-\ntrained on text from languages different from En-\nglish, as well as models pretrained on computer\ncode. These include the following models:\nMultilingual BERT. Multilingual BERT is pre-\ntrained on text from 102 different languages. About\n6946\nSample\nsize SNLI\nSNLI\nsort\nSNLI\nshuffle Amz\nAmz\nsort\nAmz\nshuffle ROC\nX-ling\nDeBERTa Zipf Unif\nSyn\nVoc NP\n10 36 36 37 36 35 36 37 36 37 36 36 37\n20 37 36 36 36 35 35 37 39 36 37 37 37\n40 37 36 36 36 36 35 37 38 37 36 37 37\n80 38 37 39 37 37 36 37 38 37 36 36 37\n160 37 38 37 36 38 37 37 40 38 37 37 38\n320 39 39 39 37 42 39 41 58 40 39 37 38\n640 44 44 45 42 52 46 48 71 47 42 39 47\n1280 54 51 54 50 72 58 52 80 61 52 41 60\n2560 70 70 69 65 81 72 65 90 75 59 48 72\n5120 79 78 80 79 87 83 83 93 83 71 58 73\n6000 79 82 80 81 88 84 82 91 84 73 60 74\n7000 84 86 87 85 89 87 84 93 84 78 62 74\n8000 85 87 87 86 89 88 85 93 87 81 63 76\n9000 86 87 88 86 91 90 85 93 88 84 64 77\n10000 87 87 89 86 91 90 85 93 87 85 64 78\nTable 3: Average accuracy scores of different pretrained DeBERTA representations on six representative non-\nlinguistic tasks: palindrome, anagram isogram, tautonym, mean, and median. The results are rounded to the nearest\npercentage point. All models except Synthetic V ocabulary (Syn V oc). show statistically significant improvements\n(p< 0.05) over the non-pretrained models.\n21% of the pretraining text is English.\nChinese BERT.Chinese BERT is a BERT model\npretrained on Chinese text.\nCode BERT.CodeBERT (Feng et al., 2020) is pre-\ntrained on code from six programming languages.\nIn Table 2, we note that all three non-English\npretrained LMs significantly outperformed non-\npretrained models, with the best performance being\ncomparable or marginally lower than English ver-\nsions. In fact, Code-BERT surprisingly surpasses\nROC by 5%. These findings strongly indicate that\nthe advantages from pretraining have little to do\nwith the format of the tasks, since they persist for\nscenarios with little shared linguistic structure.\n7.4 Synthetic languages\nFinally, to investigate what happens if we weaken\nthe distributional properties that hold even in the\nperturbed text versions from Section 6.2, we ex-\nperiment with pretraining models on synthetic text\nsampled from simple probability distributions:\nZipf distribution. We select 30k words (types)\nfrom the Amazon reviews dataset. Words are\npicked with a unigram probability that follows\nZipf’s word frequency law, which all natural lan-\nguages empirically follow (Piantadosi, 2014). For\nthe Zipf distribution, we chose α=1 and β=2.7, to\nmatch the parameters of most natural languages.\nThe text does not follow any word order.\nUniform distribution. In this dataset, words are\nsampled from the same vocabulary as in ‘Zipf dis-\ntribution’, but with a uniform unigram probability.\nThe text does not follow any word order.\nSynthetic Vocabulary. Words are selected with\nuniform distribution from a vocabulary to form\nsentences. However, instead of a vocabulary of En-\nglish words, the words in the vocabulary are also\nsynthetically generated (3 letter combinations of\nlower-case alphabets). In this text, the words do\nnot possess morphology in addition to no syntax.\nIn Tables 2 and 3, we note that surprisingly,\neven models pretrained on Zipfian and uniform\ndistribution text continue to outperform the non-\npretrained models. In fact, the Zipf version’s best\naccuracy is 3% higher than the standard Amazon\ndata version and 2% compared to perturbed Ama-\nzon shuffled data version in case of BERT. Zipf\noutperforms standard amazon data by 1% and lags\nbehind amazon shuffle by 3% for DeBERTA. The\nUniform distribution version lags behind Zipf by\n9% and 2% for BERT and DeBERTa respectively.\nWe note that the Zipf and Uniform versions still use\nthe prebuilt vocabulary from the Amazon data, and\nhence this text maintains morphological structure.\nHowever, the gains finally disappear for the Syn-\nthetic vocabulary model, which cannot leverage\nmorphological structure in the text, and its perfor-\nmance is similar to the non-pretrained models.\n8 Conclusion\nWe explore the non-linguistic inductive biases of\npretrained LMs. While the general trend (that pre-\ntraining helps) is unsurprising, our analysis with\nmodels pretrained on different text corpora shows\nthat this is not due to the model seeing related top-\nics during pretraining. We find that these gains\npersist even in absence of any shared linguistic\nstructure (in cross-lingual settings). Our observa-\ntion that this behavior is seen even when pretraining\non synthetically generated languages is intriguing\n6947\nand can be explored further by future work.\nAcknowledgements\nThis work was supported in part by NSF grant\nDRL2112635. We are also thankful to the anony-\nmous reviewers for their thoughtful suggestions.\nEthics and Broader Impact\nOur synthetic datasets contain no linguistic or so-\ncial information, and hence cannot introduce any\ntype of social, gender and cultural biases in our\nanalyses. The datasets used in the section 7 are pub-\nlicly available, and should contribute towards the\ngoal of reproducible research. In terms of broader\nimpact, our results suggest that LMs accrue helpful\ninductive biases for non-linguistic reasoning during\npretraining. This suggests that LMs can potentially\nbe explored for a broader range of downstream ap-\nplications rather than language-related tasks, which\nis the current predominant focus of these models.\nIn the long run, making such foundational models\navailable for learning a broad range of tasks from\nlimited data can make predictive AI technologies\nmore accessible than in the current day.\nLimitations\nIn terms of findings, we find strong evidence of\npretraining on text providing advantageous induc-\ntive biases for non-linguistic tasks. Our analysis\nin Section 6 suggests that this is not simply a regu-\nlarization effect. However, it does not definitively\nrule out this possibility since direct comparisons\nbetween pretrained and non-pretrained networks\n(even of different sizes) are difficult. Also, the\nscope of our analysis here is limited to small to\nmid-sized language models (with tens of millions\nof parameters), rather than massive language mod-\nels such as GPT3 (with tens of billions of param-\neters). Finally, we note that all tasks chosen for\nthis analysis are formulated as classification, where\nthe number of classes is not high. Hence, learning\nsome of the tasks might easier than possible more\ngeneral formulations. e.g., quantitative computa-\ntion.\nReferences\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal.\n2020. On the Ability and Limitations of Transform-\ners to Recognize Formal Languages. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n7096–7116, Online. Association for Computational\nLinguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAlon Brutzkus and Amir Globerson. 2021. On the in-\nductive bias of a {cnn} for distributions with orthog-\nonal patterns.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nBERT: A pre-trained model for programming and\nnatural languages. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1536–1547, Online. Association for Computational\nLinguistics.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative filtering. In proceedings of\nthe 25th international conference on world wide web,\npages 507–517.\nDavid P Helmbold and Philip M Long. 2015. On the\ninductive bias of dropout. The Journal of Machine\nLearning Research, 16(1):3403–3454.\nValentin Hofmann, Janet Pierrehumbert, and Hinrich\nSchütze. 2021. Superbizarre is not superb: Deriva-\ntional morphology improves BERT’s interpretation\nof complex words. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 3594–3608, Online. Association for\nComputational Linguistics.\nValentin Hofmann, Janet B Pierrehumbert, and Hinrich\nSchütze. 2020. Dagobert: Generating derivational\nmorphology with a pretrained language model. arXiv\npreprint arXiv:2005.00672.\nEugene Kharitonov and Rahma Chaabouni. 2021. What\nthey do when in doubt: a study of inductive biases\nin seq2seq learners. In International Conference on\nLearning Representations.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside bert’s linguistic knowl-\nedge. arXiv preprint arXiv:1906.01698.\nR. Thomas McCoy, Robert Frank, and Tal Linzen. 2020.\nDoes syntax need to grow on trees? sources of hier-\narchical inductive bias in sequence-to-sequence net-\nworks. Transactions of the Association for Computa-\ntional Linguistics, 8:125–140.\n6948\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. LS-\nDSem 2017 shared task: The story cloze test. In\nProceedings of the 2nd Workshop on Linking Models\nof Lexical, Sentential and Discourse-level Seman-\ntics, pages 46–51, Valencia, Spain. Association for\nComputational Linguistics.\nAakanksha Naik, Abhilasha Ravichander, Carolyn Rose,\nand Eduard Hovy. 2019. Exploring numeracy in\nword embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3374–3380, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nSteven T Piantadosi. 2014. Zipf’s word frequency law\nin natural language: A critical review and future di-\nrections. Psychonomic bulletin & review, 21(5):1112–\n1130.\nLuzi Sennhauser and Robert Berwick. 2018. Evaluat-\ning the ability of LSTMs to learn context-free gram-\nmars. In Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 115–124, Brussels, Bel-\ngium. Association for Computational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nNatalia Skachkova, Thomas Trost, and Dietrich Klakow.\n2018. Closing brackets with recurrent neural net-\nworks. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 232–239, Brussels, Bel-\ngium. Association for Computational Linguistics.\nMirac Suzgun, Yonatan Belinkov, Stuart Shieber, and\nSebastian Gehrmann. 2019a. LSTM networks can\nperform dynamic counting. In Proceedings of the\nWorkshop on Deep Learning and Formal Languages:\nBuilding Bridges, pages 44–54, Florence. Associa-\ntion for Computational Linguistics.\nMirac Suzgun, Yonatan Belinkov, and Stuart M. Shieber.\n2019b. On evaluating the generalization of LSTM\nmodels in formal languages. In Proceedings of the\nSociety for Computation in Linguistics (SCiL) 2019,\npages 277–286.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, Roi Reichart,\nand Anna Korhonen. 2020a. Multi-SimLex: A large-\nscale evaluation of multilingual and crosslingual lexi-\ncal semantic similarity. Computational Linguistics,\n46(4):847–897.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020b. Prob-\ning pretrained language models for lexical semantics.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7222–7240, Online. Association for Computa-\ntional Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. arXiv preprint arXiv:1905.00537.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investigating\nBERT’s knowledge of language: Five analysis meth-\nods with NPIs. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2877–2887, Hong Kong, China. Association\nfor Computational Linguistics.\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On\nthe practical computational power of finite precision\nRNNs for language recognition. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\n6949\npages 740–745, Melbourne, Australia. Association\nfor Computational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. Asso-\nciation for Computational Linguistics.\nRui Xu, Xintao Wang, Kai Chen, Bolei Zhou, and\nChen Change Loy. 2021. Positional encoding as\nspatial inductive bias in gans. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 13569–13578.\nMengjie Zhao, Philipp Dufter, Yadollah Yaghoobzadeh,\nand Hinrich Schütze. 2020. Quantifying the contextu-\nalization of word representations with semantic class\nprobing. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1219–1234,\nOnline. Association for Computational Linguistics.\n6950\nA Appendix\nBaseline p-value\nSNLI 5.45 × 10−5\nSNLI sort 3.33 × 10−4\nSNLI shuffle 5.5 × 10−4\nAmazon 7.48 × 10−5\nAmazon sort 7.2 × 10−5\nAmazon shuffle 4.5 × 10−5\nMultilingual BERT 9.07 × 10−4\nChinese BERT 8.9 × 10−5\nCode BERT 8.1 × 10−5\nROC 2.64 × 10−5\nZipf distribution 7.45 × 10−5\nUniform distribution 4.61 × 10−4\nSynthetic vocabulary 1.2 × 10−1\nTable 4: Statistical significance values (paired t-test)\nbetween non-pretrained model and other baseline BERT\nmodels trained on different datasets.\nBaseline p-value\nSNLI 2.45 × 10−5\nSNLI sort 1.33 × 10−4\nSNLI shuffle 4.3 × 10−5\nAmazon 6.32 × 10−4\nAmazon sort 8.7 × 10−5\nAmazon shuffle 7.3 × 10−5\nMultilingual BERT 9.07 × 10−5\nROC 2.14 × 10−3\nZipf distribution 3.1 × 10−3\nUniform distribution 4.61 × 10−4\nSynthetic vocabulary 1.3 × 10−1\nTable 5: Statistical significance values (paired t-test)\nbetween non-pretrained model and other baseline De-\nBERTA models trained on different datasets.\nA.1 Implementation details\nFor transformer LMs, we add a fully connected\nclassification layer on the top of final encoder\nlayer. The pooled representations from the final\nencoder layer are then passed onto fully connected\nlayer. We train these models in an end-to-end man-\nner. For the RNN LMs, we first pretrain LM onto\nthe task. The final word representations are the\nweighted sum of three layers. Max-pooling op-\neration is applied on the time step dimension for\nthese weighted representations. A final classifica-\ntion layer is trained with the pooled representations.\nA.2 Computational requirements\nAll the models are run using PyTorch framework on\n4 geforce gtx 1080 gpus. Each of the fine-tuning ex-\nperiments takes about 5 gpu hours and pre-training\ntakes about 10 gpu hours.\nA.3 Statistical significance\nWe perform a paired t-test between pretrained and\nnon-pretrained models of the LMs on all the tasks.\nThe statistical significance values are shown in the\ntable 6. We also calculated the paired t-value be-\ntween non-pretrained model and BERT and De-\nBERTA pretrained on different datasets. The paired\nt-values are shown in the table 4 and 5.\n6951\n(a) DeBERTa\n (b) BERT large\nFigure A.1: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on\nfour quantitative computation tasks (odd classification, even classification, odd even classification and decimal\noperation).\n(a) DeBERTa\n (b) BERT large\nFigure A.2: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on four\nquantitative tasks (mean, median, mode, decimal & word operation).\n(a) BERT small\n (b) ELMO\nFigure A.3: Performance comparison of pretrained and non-pretrained models of DeBERTa, and BERT large on\nregular expression tasks (AA*BB*CC*DD*EE* and recognize {0,1,2}*02*).\n(a) DeBERTa\n (b) BERT large\nFigure A.4: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on four\nstring reasoning (palindrome, anagram, isogram and tautonym classification).\n6952\n(a) DeBERTa\n (b) BERT large\nFigure A.5: Performance comparison of pretrained and non-pretrained models of DeBERTa and BERT large on five\nstring reasoning tasks (length of a string, maximum frequent character, vowels classification, parity check and count\nof unique character).\nTask BERT small DeBERTa BERT large ELMO\nOdd classification 10.4 × 10−2 8.8 × 10−1 2.9 × 10−3 7.35 × 10−7\nEven classification 8.1 × 10−2 8.7 × 10−2 5.25 × 10−3 7.35 × 10−7\nOdd even classification 2.2 × 10−1 6.96 × 10−7 6.46 × 10−4 7.35 × 10−7\nDecimal operation 4.1 × 10−4 7.07 × 10−1 1.35 × 10−5 3.49 × 10−7\nDecimal & word operation 6.85 × 10−8 6.43 × 10−7 4.34 × 10−8 5.39 × 10−7\nMean 9.5 × 10−2 7.56 × 10−1 7.8 × 10−6 2.2 × 10−7\nMedian 9.28 × 10−6 8.04 × 10−1 5.68 × 10−7 1.99 × 10−7\nMode 9.2 × 10−2 2.27 × 10−1 9.2 × 10−1 3.35 × 10−7\nRecognize {0,1,2}*02* 1.31 × 10−1 8.4 × 10−1 4.34 × 10−1 5.48 × 10−5\nRecognize AA*BB*CC*DD*EE* 4.06 × 10−1 6.97 × 10−1 4.02 × 10−1 2.39 × 10−6\nPalindrome classification 4.34 × 10−7 2.1 × 10−3 1.85 × 10−7 1.97 × 10−6\nAnagram classification 5.1 × 10−6 1.44 × 10−6 3.45 × 10−7 7.46 × 10−6\nIsogram classification 1.28 × 10−7 4.77 × 10−3 3.47 × 10−4 2.18 × 10−6\nTautonym classification 1.92 × 10−7 1.29 × 10−5 1.69 × 10−8 4.39 × 10−6\nLength of a string 2.7 × 10−1 1.27 × 10−4 3.39 × 10−4 7.07 × 10−4\nCount of unique characters 1.79 × 10−4 2.7 × 10−2 1.23 × 10−7 3.18 × 10−6\nParity check 2.68 × 10−4 4.66 × 10−4 4.34 × 10−7 6.05 × 10−6\nV owels classification 4.26 × 10−1 9.5 × 10−1 7.22 × 10−1 5.11 × 10−2\nMaximum frequent character 5.02 × 10−1 5.65 × 10−1 6.07 × 10−1 6.47 × 10−1\nTable 6: Statistical significance values (paired t-test) between pretrained and non-pretrained model on all the tasks.\n6953",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.772354006767273
    },
    {
      "name": "Natural language processing",
      "score": 0.6304349899291992
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6233087778091431
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5807638168334961
    },
    {
      "name": "Language model",
      "score": 0.5752785801887512
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4933830797672272
    },
    {
      "name": "Inductive bias",
      "score": 0.4597414433956146
    },
    {
      "name": "Natural language",
      "score": 0.43977224826812744
    },
    {
      "name": "Multi-task learning",
      "score": 0.1692662537097931
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1333535994",
      "name": "University of North Carolina Health Care",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I114027177",
      "name": "University of North Carolina at Chapel Hill",
      "country": "US"
    }
  ],
  "cited_by": 4
}