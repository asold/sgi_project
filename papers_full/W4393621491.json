{
  "title": "Retrieval-Augmented Generation Approach: Document Question Answering using Large Language Model",
  "url": "https://openalex.org/W4393621491",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5033573318",
      "name": "Kurnia Muludi",
      "affiliations": [
        "Informatics and Business Institute Darmajaya"
      ]
    },
    {
      "id": "https://openalex.org/A5035899732",
      "name": "Kaira Milani Fitria",
      "affiliations": [
        "Informatics and Business Institute Darmajaya"
      ]
    },
    {
      "id": "https://openalex.org/A5046989332",
      "name": "Joko Triloka",
      "affiliations": [
        "Informatics and Business Institute Darmajaya"
      ]
    },
    {
      "id": "https://openalex.org/A5068896096",
      "name": "Sutedi Sutedi",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2081157820",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W2739716023",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4317898419",
    "https://openalex.org/W4312854033",
    "https://openalex.org/W2293453011",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W4295253143",
    "https://openalex.org/W2938205538",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4378942418",
    "https://openalex.org/W4287891177",
    "https://openalex.org/W3126880001",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4286750988",
    "https://openalex.org/W6939387801",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W2983309655",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2949021450",
    "https://openalex.org/W3019631707",
    "https://openalex.org/W2115268776",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2997323475",
    "https://openalex.org/W3215717093",
    "https://openalex.org/W2963159735",
    "https://openalex.org/W4389766097",
    "https://openalex.org/W2897610589",
    "https://openalex.org/W4307481286",
    "https://openalex.org/W4390905756",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4385570371"
  ],
  "abstract": "This study introduces the Retrieval Augmented Generation (RAG) method to improve Question-Answering (QA) systems by addressing document processing in Natural Language Processing problems. It represents the latest breakthrough in applying RAG to document question and answer applications, overcoming previous QA system obstacles. RAG combines search techniques in vector store and text generation mechanism developed by Large Language Models, offering a time-efficient alternative to manual reading limitations. The research evaluates RAG's that use Generative Pre-trained Transformer 3.5 or GPT-3.5-turbo from the ChatGPT model and its impact on document data processing, comparing it with other applications. This research also provides datasets to test the capabilities of the QA document system. The proposed dataset and Stanford Question Answering Dataset (SQuAD) are used for performance testing. The study contributes theoretically by advancing methodologies and knowledge representation, supporting benchmarking in research communities. Results highlight RAG's superiority: achieving a precision of 0.74 in Recall-Oriented Understudy for Gisting Evaluation (ROUGE) testing, outperforming others at 0.5; obtaining an F1 score of 0.88 in BERTScore, surpassing other QA apps at 0.81; attaining a precision of 0.28 in Bilingual Evaluation Understudy (BLEU) testing, surpassing others with a precision of 0.09; and scoring 0.33 in Jaccard Similarity, outshining others at 0.04. These findings underscore RAG's efficiency and competitiveness, promising a positive impact on various industrial sectors through advanced Artificial Intelligence (AI) technology.",
  "full_text": "(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n776 | P a g e  \nwww.ijacsa.thesai.org \nRetrieval-Augmented Generation Approach: \nDocument Question Answering using Large \nLanguage Model\nKurnia Muludi1, Kaira Milani Fitria2*, Joko Triloka3, Sutedi4 \nInformatics Engineering Graduate Program, Darmajaya Informatics and Business Institute, Bandar Lampung, Indonesia \n \n \nAbstract‚ÄîThis study introduces the Retrieval Augmented \nGeneration (RAG) method to improve Question -Answering (QA) \nsystems by addressing document processing in Natural Language \nProcessing problems. It represents the latest break through in \napplying RAG to document question and answer applications, \novercoming previous QA system obstacles. RAG combines search \ntechniques in vector  store and text generation mechanism  \ndeveloped by Large Language Models, offering a time -efficient \nalternative to manual reading limitations. The research evaluates \nRAG's that use Generative Pre-trained Transformer 3.5 or GPT-\n3.5-turbo from the ChatGPT model and its impact on document \ndata processing, comparing it with other applications. This \nresearch also p rovides datasets to test the capabili ties of the QA \ndocument system. The proposed dataset and Stanford Question \nAnswering Dataset  (SQuAD) are used for performance testing. \nThe study contributes theoretically by advancing methodologies \nand knowledge represe ntation, supporting benchmarking in \nresearch communities. Results highlight RAG's superiority: \nachieving a precision of 0.74 in Recall-Oriented Understudy for \nGisting Evaluation  (ROUGE) testing , outperforming others at \n0.5; obtaining an F1 score of 0.88 in  BERTScore, surpassing \nother QA apps at 0.81; attaining a precision of 0.28 in Bilingual \nEvaluation Understudy (BLEU) testing, surpassing others with a \nprecision of 0.09; and scoring 0.33 in Jaccard Similarity, \noutshining others at 0.04. These findings und erscore RAG's \nefficiency and competitiveness, promising a positive impact on \nvarious industrial sectors through advanced A rtificial \nIntelligence (AI) technology. \nKeywords‚ÄîNatural Language Processing ; Large Language \nModel; Retrieval Augmented Generation ; Question Answering ; \nGPT \nI. INTRODUCTION \nThis research proposes a new approach to the increasing \nreliance on articles and journal documents by introducing a \nQuestion-Answering (QA) document processing system  [1]. \nThe identification of several critical problems  motivates this \nresearch. The problems motivating this research are \nmultifaceted. Firstly, manual reading and processing to \ncomprehend document text are time -consuming, error -prone, \nand inefficient. Secondly, previous methods employed to \nmodify Large Langu age Models (LLM) for document \nprocessing demanded substantial resources and were \nchallenging to implement widely. Lastly, models relying solely \non the capabilities of LLM for QA systems without \nmodifications tend to generate hallucinatory answers, lacking \ncorrectness and precision.  Manual processing for document \nunderstanding leads to time-consuming efforts, susceptibility to \nhuman error, and inefficient analysis processes. Based on \nprevious methods, the use of modified Large Language Models \n(LLM) for docum ent processing requires significant resources \nand poses challenges for widespread implementation. Also, the \nunderutilization of the recently discovered Retrieval \nAugmented Generation (RAG) method, particularly in \ndocument processing within Question -Answering (QA) \nsystems, provides an opportunity for further exploration. The \nmotivation stems from the challenges associated with manual \ndocument processing, resource -intensive Large Language \nModel (LLM) modifications, and the underutilization of the \nRetrieval-Augmented Generation (RAG) method in the \ndocument-based question -answering domain  [2], [3] . In \naddition, there is a tendency to produce hallucinatory responses \nthat lack accuracy and precision in models that rely solely on \nLLM capabilities for QA systems w ithout modifications. \nFinally, the implementation of RAG in QA systems for \ndocument processing offers the untapped potential to improve \nthe ability of the system to produce accurate and non -\nhallucinatory responses. \nBuilding on this line of research, this p aper proposes the \nimplementation of the Retrieval Augmented Generation model \nfor document question  answering tasks, specifically using the \nChatGPT model.  RAG, introduced in 2021  [4], addresses the \nlimitations of previous methods by merging parametric and \nnon-parametric memory. This hybrid model seamlessly \nintegrates generative capabilities with data retrieval \nmechanisms, linking language models to external knowledge \nsources. RAG combines generative capabilities and the ability \nto search for data and in corporate relevant information from \nthe knowledge base in the model.  The distinct advantage s of \nRAG lie in its ability to adapt to dynamic data, its flexibility in \nworking with external data sources, and its ability to mitigate \nhallucinatory responses [5]. These characteristics make RAG \nparticularly suitable for QA tasks on internal organi zational \ndocuments by leveraging external knowledge to reduce \nresponse hallucinations [6]. \nThe current research aims to exploit the innovative \napproach of RAG to const ruct an application capable of \nautomatically processing external text documents. The focus of \nthis research is to develop an application system capable of \nprocessing external document text uploaded by the user . The \nsystem will automatically read the docume nt text, allowing \nusers to input questions related to the document. Subsequently, \nthe system provides answers based on the processed document \n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n777 | P a g e  \nwww.ijacsa.thesai.org \ntext, eliminating the need for manual reading to find answers . \nThis comprehensive solution not only overcomes the \nlimitations of previous methods, but also promises to \nsignificantly speed up research and study exploration in \nvarious domains. \nTesting of the proposed model is performed , like several \nprevious QA-based studies, by calculating the suitability of the \nanswer results provided by the model with the ground truth of \nthe test dataset. Some of the metrics used to calculate the \nperformance of this model include Accuracy, ROUGE, BLEU, \nBERTScore, and Jaccard Similarity. \nII. RELATED WORKS \nThis study examines the applicabil ity of RAG, its impact \non the document processing task, and compares it to the \nprevious methods. This research also investigates the capability \nof the large language model within the ChatGPT systems, gpt-\n3.5-turbo within the framework of RAG. This work als o \nhighlights the development of Artificial Intelligence (AI ) and \nNatural Language Processing (NLP) , so this  research focuses \non the improvement of intelligence and the capabilities of \napplications [7], [8], [9]. Machine Learning and Deep Learning \nalgorithms, which include BERT Base , and Text-to-Text \nTransfer Transformer (T5) models , and  RAG method, have \nmade significant advances in QA tasks  [4], [10], [11], [12] . \nThis research motivated the implementation of RAG for \nprocessing documents, integrat ed in to an interactive QA \nsystem. \nBetween 2015 and the present, the evolution of question -\nanswering (QA) systems shows a trajectory characteri zed by \ndiverse methodologies.  Starting with semantic parsing -based \nsystems in 2015, Wen -tau Yih et al. focused on  transforming \nnatural language queries into structured logical forms, \nachieving a performance of 52.5% in  the F1-score [2]. \nSubsequent knowledge-based paradigms (KB-QA) by Yanchao \nHao et al. in 2017 reformulated questions as predicates, \nachieving a performance of 42.9% [3]. Progress has been made \nin integrating AI technologies. Caiming Xiong's exploration of \ndynamic memory networks (DMN) in 2016 , achieved an \naccuracy of 28.79% [7]. In the same year, Minjoon Seo et al.'s \nBi-Directional Attention Flow ( BiDAF) framework \ndemonstrated significant performance with a 68% exact match \nand 77.3% F1 score, albeit with a computational time of 20 \nhours [8]. Adams Wei Yu et al. introduced the QANet model in \n2018, with a performance of 76.2% exact match and 84.6% F1-\nScore, within a shorter computational time of 3 hours  [9]. As \nQA systems evolve, in 2019 Wei Yang et al. applied fine -\ntuning methods with data augmentation techniques, achieving \nremarkable results wit h a modified BERT -Base model of \n49.2% for exact match and 65.4% for F1 -Score [10]. Colin \nRaffel et al. introduced the Text -to-Text Transfer Transformer \n(T5), with impressive performance of 63.3% for exact match, \n94.1% for F1 score, and a peak accuracy of 93.8%, albeit with \nan increased number of parameters of 11 billion [11]. In 2020, \nthe focus was on fine -tuning pre -trained models, with Adam \nRoberts et al. achieving a recall performance of 34.6% using \nthe T5 model [12]. The Retrieval -Augmented Generation \n(RAG) method, which combines parametric an d non -\nparametric methods, was introduced by Patrick Lewis et al. in \n2021. RAG has demonstrated its capabilities in open domain \nQA tasks, overcoming previous limitations to deliver more \nefficient and comprehensive QA systems [4]. \nLarge language model call ed GPT, or Generative Pre -\nTrained Transformer  was developed by OpenAI. Previous \nresearch that has compared the performance of ChatGPT with \nother large language models like PaLM and LLaMA  in open-\ndomain QA tasks indicates that ChatGPT consistently achieves \nthe highest scores across various open -domain QA datasets  \n[13]. Table I presents performance comparisons among LLMs. \nTABLE I.  LLM PERFORMANCE ON OPEN DOMAIN QA DATASET \nModel TriviaQA WebQuestion NQ-Open \nPaLM-540B (few-shot) 81.4 43.5 39.6 \nPaLM-540B (zero-shot) 76.9 10.6 21.2 \nLLaMA-65B (zero-shot) 68.2 - 23.8 \nChatGPT (zero-shot) 85.9 50.5 48.1 \nThe PolyQuery Synthesis test, which identifies multiple \nqueries within a single -query prompt and extracts the answers \nto all of the questions from the model's latent representation, \nalso shows that ChatGPT outperforms other GPT models from \nOpenAI (ada-001, babbage-001, curie, and davinci) in terms of \naccuracy [13]. According to the evaluations, the gpt -3.5-turbo \nmodel has been selected for implementation in this research. \nIII. RESEARCH METHOD \nThis research undergoes a development phase, starting \nwith designing the application system and integrating the APIs \nof ChatGPT, LangChain and FAISS. Subsequent stages \ninclude extensive system modeling, interface testing and data \npreparation using the proposed dataset and the SQuAD \ndataset. The testing phase, which includes a performance \ncomparison with other applications using ground truth metrics \n(ROUGE, BERTScore, BLEU and Jaccard Similarity), guides \nthe exploration of the capabilit ies of the proposed system, as \nshown in Fig. 1. \nA. RAG Integration \nRetrieval Augmented Generation (RAG) combines retrieval \nand generation models. It uses a Large Language Model \n(LLM) to generate text based on commands and integrates \ninformation from a separate retrieval system to improve output \nquality and contextual relevance [14]. The mechanism involves \nretrieving factual content from a knowledge base via retrieval \nmodels and using generative processes to provide additional \ncontext for more accurate output [15]. External data sources are \nused, and the numerical representation is facilitated by \nembedding methods to ensur e compatibility. Based on Fig.  2, \nuser queries converted into embeddings are compared with \nvectors from the knowledge lib rary. Relevant context is added \nto the queries before they are fed into the base language model. \n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n778 | P a g e  \nwww.ijacsa.thesai.org \n \n \nFig. 1. Research flow diagram. \nOpenAI, the creator of the Large Language Model GPT, \nconducted a comprehensive number of RAG experiments, \nexploring various implementations such as cosine similarity \nretrieval, chunk/embedding experiments, reranking, \nclassification steps, and prompt engineering, as depicted in Fig. \n3. OpenAI's findings, presented in Fig.  3, revealed that RAG \nimplementation with prompt engineering achieved the highest \naccuracy, positioning it as the most effective RAG technique to \ndate [16]. This discovery serves as a catalyst for the integration \nof RAG with prompt engineering using the LangChain module. \n \nFig. 2. RAG mechanism with LLM. \n \nFig. 3. Accuracy of the RAG method by Open AI. \nLangChain provides a robust data processing pipeline that \nutilizes FAISS to perform an efficient retrieval operation in the \nVectorDB. The query phase transforms inputs into vectors for \ndatabase searches, and prompt engineering enhances the \nreusability of retrievals. Output parsers interpret LLM outputs, \nensuring consistency [17]. A highly efficient similarity search \nand vector clustering library, Facebook AI Similarity Search or \nFAISS [18]. It optimizes the trade-off between memory, speed \nand accuracy, allowing developers to effectively navigate \nmultimedia documents. The mechanism involves the \nconstruction of an index for efficient storage, with vector \nsearches retrieving the most similar vect ors using cosine \nsimilarity scores [19]. \nB. Proposed Model \nThis research employs a modified Large Language Model \n(LLM), ChatGPT, augmented with additional libraries to \nfunction as a Question -Answering (QA) system capable of \nprocessing external documents for supplementary information. \nA \nA \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n779 | P a g e  \nwww.ijacsa.thesai.org \nThe chosen methodology for QA system development is the \nRetrieval Augmented Generation (RAG) mechanism. Unlike \nprevious approaches such as semantic parsing -based, \nknowledge-based, and fine -tuning using LSTM or other DL \nalgorithms, RAG addresses shortcomings like difficulty \nexpanding or revising model memory,  an inability to provide \ndirect insight into generated predictions, and a tendency to \nproduce hallucinative answers  [12]. The solution involves the \ncreation of a hybrid mode l, merging generative and retrieval \nmodels, which forms  the basis for the RAG method. RAG \noffers advantages such as adaptive responses to dynamic data, \nflexibility with external data sources, and minimization of \nhallucinative responses [5]. Thus, RAG is chosen to construct a \ntext document-based QA system interacting with users through \na chatbot interface. The system's workflow, implemented using \nRAG and supporting libraries like LangChain and  FAISS, is \nillustrated in Fig. 4. \nThe integration of the LangCha in framework into the QA \ndocument system includes document loading, memory \nmanagement, and prompting to connect to the LLM model. \nThe process starts with document loading, followed by \ndocument splitting into text chunks. These text chunks undergo \nword embe dding, converting them into vectors stored in the \nvector database. Simultaneously, user -inputted text questions \nare embedded and converted into word vectors. The system \nconnects these vectors to the vector database, performing a \nsemantic search and ranking  the relevance between vectors. \nThe semantic search results in relevant context between \nquestions and answers. The system retrieves pertinent answers \nbased on user queries and sends them to the LLM (using the \nChatGPT model). The final outcome involves the system \nreceiving LLM-generated answers and delivering them to the \nuser. The application system interacts with users, requiring an \ninterface connecting the user and the system. Mockups, design \nlayouts, and elements for the web application are created using \nthe Streamlit framework, facilitating rapid development and \nsharing of the AI model web application. The mockup for the \napplication system and user interaction within the system is \ndepicted in Fig. 5. \n \nFig. 4. Integration of langchain framework in RAG for the proposed document QA system. \n \nFig. 5. Mockup of the application system and user interaction for the app. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n780 | P a g e  \nwww.ijacsa.thesai.org \nC. Proposed Dataset DocuQA \nThe proposed dataset, DocuQA, designed for application -\nbased question -answering system s that process document \ninputs, consists of 20 diverse documents, encompassing journal \narticles, news reports, financial documents, and tutorials. Each \ndocument file includes five  questions with corresponding \nground truth answers, enabling a thorough evalu ation of QA \nsystem capabilities, with a total 100 questions in the dataset. \nDocuQA consists of journal documents with calculations and \nformulas, news documents with specific titles, financial reports \nand news documents with numbers and currency data, and \ntutorial documents with step -by-step instructions . Accuracy \ncan be  calculated based on the correct answers out of 100, \nproviding a metric for information extraction accuracy. The \ndataset aims to challenge QA systems in understanding \ncontext, identifying key words, and efficiently extracting \nspecific information, offering a robust evaluation tool for \ndevelopers and researchers across various document and \nquestion types. The dataset can be accessed publicly [20]. \nProper citation of the dataset is encouraged for research or \nprojects using DocuQA to ensure appropriate credit is given . \nThe preview of the DocuQA dataset can be seen in Fig. 6. \n \nFig. 6. Preview of the DocuQA test dataset. \nD. Testing and Evaluation \nThe tests were performed on two types of test datasets, with \nDocuQA [20] and SQuAD 1.1  [21]. DocuQA is a dataset \noriginally created by this research, consisting of 100 questions \nwith the ground truth and a total of 20 test documents for \ndocument-based QA systems. In addition, the SQuAD dataset \nwas used in the fo rm of modified pdf documents that can be \nused to test the QA system's ability to process documents and \nretrieve information based on the questions and related ground \ntruth in the SQuAD dataset. Both types of test datasets will be \ntested on the QA system de veloped in this research, and also \non other commercial QA systems that process pdf documents, \nsuch as typeset.io. The results of these tests will give an idea of \nthe QA system performance built on this research, whether it is \nsuperior to other document-based QA applications. \nThe proposed QA document processing system is evaluated \nthrough rigorous testing using established metrics such as  \nROUGE or Recall-Oriented Understudy for Gisting \nEvaluation, BERTscore, BLEU  or Bilingual Evaluation \nUnderstudy, and Jaccard Similarity. These metrics provide \nreliable benchmarks for assessing the system's performance \nacross various dimensions. The testing process involves two \nkey variables. \"Predictions RAG\" and \"Prediction Others\" \nrepresent the test results from the developed application and \ncomparable commercial applications, respectively. Both sets of \npredictions are compared to the ground truth  data, which is \nencapsulated in the \"references\" variable. Different aspects of \nlanguage models and question answering systems are evaluated \nusing different metrics . ROUGE measures the overlap in \nsummarization [22]. BERTscore assesses semantic similarity \nusing contextual embeddings  [23]. BLEU evaluates n -gram \nprecision [24], and Jaccard Similarity compares text simil arity \nbased on word or n -gram overlap [25]. Precision in question \nanswering systems is commonly assessed through accuracy, F1 \nscore, and precision metrics, providing insights into their \neffectiveness. The metrics are used to quantitatively evaluate \nsystem performance and establish its superiority over existing \ncommercial applications in document processing and \ninformation retrieval tasks. \n1) Accuracy: Accuracy is defined as the  proportion of \ncorrect responses from the total number of responses. \nAccuracy can be calculated by calculating the percentage of \ncorrect predictions over the total number of references  [26]. In \nessence, accuracy represents the ability of the system to \nprovide correct answers, which is expressed as a percentage \nusing the following formula (see Eq.(1)). \nAccuracy=\ncorrect predictions\nall predictions  √ó100%                        (1) \nThis metric serves as a valuable indicator of the overall \ncorrectness of the model in the response it generates. \n2) ROUGE: Recall-Oriented Understudy for Gisting \nEvaluation can be used to evaluate the text generation models, \nwhich are based on the measurement of the overlap between \ncandidate text and reference text  [27]. ROUGE has several \nmeasurement variants, each depending on the number of \noverlapping n -grams. The ROUGE -L variant is the most \nwidely used, because it uses the longest sequence or longest \ncommon subsequence or LCS with the longest word sequence \nthat both sentences have. Precision refers to the proportion of \nn-grams in the candidate that are also in the reference (see Eq. \n2.). Recall, on the other hand , refers  to the  proportion of n -\ngrams that are in the reference text that exactly match in the \npredicted candidate text as shown in Eq. (3) . The F1-score can \nbe calculated from the precision and recall as shown in Eq. \n(4). \nROUGE-Lrecall=\nLCS (candidate, reference)\n#words in reference                  (2) \nROUGE-Lprecision=\nLCS (candidate, reference)\n#words in candidate                (3) \nROUGE-LF1-Score= 2√ó\nrecall ‚àô precision\nrecall + precision                (4) \nwhere, the reference is based on the ground truth in the test \ndataset, and the candidate is from the system predictions. The \nscore generated by the ROUGE measure is between 0 and 1. A \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n781 | P a g e  \nwww.ijacsa.thesai.org \nscore of 1 indicates total agreement between reference and \ncandidate text. \n3) BERTScore: BERTScore is an automatic evaluation \nmetric in text generation tasks that evaluates the similarity of \neach candidate sentence token to each reference sentence \ntoken by means of contextual embeddings  [23]. The \nembeddings in BERTScore are contextual, changing \ndepending on the sentence context. The context awareness \nallows BERTScore to score semantically similar sentences \ndespite their different sentence order.  For the recall \ncalculation, each token in ùë• is matched with the most similar \ntoken in ùë•ÃÇ, as for the precision calculation. Greedy matching is \nused to maximize the similarity score. The  values of precision \n(see Eq. (5)), recall (see Eq. (6)) and F1 score (see Eq. (7)) for \nreference ùë•  and candidate ùë•ÃÇ  can be calculated using the \nfollowing equations. \nRBERT = \n1\n|x| ‚àë max  \nxÃÇj ‚àà xÃÇxi ‚àà x\nxi\n‚ä§xÃÇj                    (5) \nwhere, ùëÖùêµùê∏ùëÖùëá is the Recall BERTScore, ùë• is the reference \ntoken, ùë•ÃÇ is the candidate token, ùë•ùëñ is the sequence vector ùë•, ùë•ùëó \nis the sequence vector ùë•ÃÇ , where ùõ¥ùë•ùëñ‚ààùë•  is the number of ùë•ùëñ \npresent in ùë•, and also ùëöùëéùë• \nùë•ÃÇùëó ‚àà ùë•ÃÇ \n is the maximum value of ùë•ÃÇùëó present \nin ùë•ÃÇ, and ùë•ùëñ\n‚ä§ùë•ÃÇùëó  is the cosine similarity of ùë• and ùë•ÃÇ. \nPBERT = \n1\n|xÃÇ| ‚àë max  \nxi ‚àà xxÃÇj ‚àà xÃÇ\nxi\n‚ä§xÃÇj                       (6) \nGiven ùëÉùêµùê∏ùëÖùëá  as Precision BERTScore, ùë•  as reference \ntoken, ùë•ÃÇ  as candidate token, ùë•ùëñ  as sequence vector ùë• , ùë•ÃÇùëó  as \nsequence vector ùë•ÃÇ, where ùõ¥ùë•ÃÇùëó ‚àà ùë•ÃÇ is the number of ùë•ÃÇùëó present in \nùë•ÃÇ, and also ùëöùëéùë• \nùë•ùëñ ‚àà ùë• \n is the maximum value of ùë•ùëñ present in ùë•, and \nùë•ùëñ\n‚ä§ùë•ÃÇùëó  is the cosine similarity of ùë• and ùë•ÃÇ. \nFBERT = 2√ó\nPBERT   ‚ãÖ  RBERT\nPBERT +  RBERT\n                             (7) \nwhere ùêπùêµùê∏ùëÖùëá is the F1 -score of BERTScore, then ùëÉùêµùê∏ùëÖùëá  is \nthe precision and ùëÖùêµùê∏ùëÖùëá is the recall from BERTScore results. \nAlthough the cosine similarity value is theoretically in the \ninterval [ -1, 1], in practice the value is rescaled so that it is \nbetween 0 and 1 in the result of the BERTScore calculation. \n4) BLEU: Bilingual Evaluation Understudy is a metric \nthat computes a modification of precision for n -grams, \ncombines it with weights, and applies a brevity penalty to \nobtain the final BLEU score [28]. The score range of BLEU is \nfrom 0 to 1. The greater the BLEU score, the better  the \nsystem's performance  is considered to be compared to the \nreferences. The formula for calculating BLEU can be seen in \nEq. (8). \nBLEU=BP‚ãÖexp (‚àë wn log pn\nN\nn=1\n)                  (8) \nùêµùëÉ represents the brevity penalty, adjusting the score to \npenalize translations shorter than the reference . ùëÅ denotes the \nmaximum number of considered n -grams. The precision for n-\ngrams, denoted as  ùëùùëõ  signifies the n -grams ratios by the \ncandidate text that appearing in any reference translation to the \ntotal of n-grams in the candidate text. ùë§ùëõ represents the weight \nassigned to each n-gram precision score. \n5) The Jaccard similarity quantifies the similari ty \npercentage between two sets of data by identifying the \ncommon and the different members [29]. This can be \ncalculated by divid ing the number of observations shared by \nthe sum of the  observations in each of the two sets . Jaccard \nsimilarity can be expressed as the ratio of the intersection  \n(ùê¥ ‚à© ùêµ) to the union (ùê¥ ‚à™ ùêµ) of two sets (see Eq. (9)). \nJ(A,B)= |A‚à©B|\n|A‚à™B|                                 (9) \n|ùê¥ ‚à© ùêµ| indicates the size of the intersection of the sets A \nand B, and |ùê¥ ‚à™ ùêµ| indicates the size of the union of the sets A \nand B. The Jaccard similarity is bounded in the range from 0 to \n1. A Jaccard similarity of 1 indicates complete identity \nbetween the sets, while a similarity of 0 implies that the sets \nhave no common elements. \nIV. RESULT AND DISCUSSION \nA. Result \nThe interface of the proposed QA system can be seen i n \nFig. 7. \n \nFig. 7. Document QA system interface. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n782 | P a g e  \nwww.ijacsa.thesai.org \nThe interface of the proposed document QA system can \naccept multiple PDF format documents. If the user clicks the \nsubmit button, the system will process the PDF document to \nconvert it to vector form with embedding (as describe d in the \nRAG mechanism in Fig.  2). Once the document submission \nprocess is complete, the user can ask questions related to the \nsubmitted document, and the QA system will provide answers \nbased on the source documents provided. The set of questions \nand answers generated from the user's i nteraction with the QA \nsystem will be in the form of a chatbot, so that it st ores the \ncommunication history. \nB. Accuracy \nAccuracy in our system model is expressed as the \npercentage of correct answers within the entire answer key \ndataset. To assess accuracy, w e calculate the ratio of the \nnumber of correct predictions to the total number of predictions \n[26]. The visualization of this accuracy result can be figured in \nFig. 8. \nThe accuracy comparison between the proposed QA \ndocument system and other applications reveals the superiority \nof our method. The proposed system achieved accuracy rates \nof 96% (our dataset) and 95.5% (SQuAD dev dataset), \nsurpassing the other application's rates of 55% (our dataset) \nand 85.7% (SQuAD dataset). This underscores the consistently \nhigher accuracy of our proposed method. \nC. ROUGE \nROUGE-L score evaluation compares the results of our \nproposed QA method outperforming other QA applications in \nterms of precision, recall, and F1 -Score. Specifically, on our \ndataset, our proposed method demo nstrated precision, recall, \nand F1 -Score of 73.7%, 23.9%, and 33.7%, respectively. In \ncomparison, other QA applications achieved lower \nperformance metrics with precision, recall, and F1 -Score of \n50.0%, 10.5%, and 15.2%, respectively. Similarly, on the \nSQuAD dev dataset, our proposed method excelled with \nprecision, recall, and F1 -Score reaching 85.5%, 16.2%, and \n26.1%, while other QA applications reported lower scores of \n77.2%, 10.4%, and 17.1%, respectively. These results \nunderscore the superior performance  of our proposed method \nacross both datasets that can be visualized in Fig. 9. \n \nFig. 8. Accuracy result of proposed method using RAG and other document QA application. \n \nFig. 9. ROUGE-L result of proposed method using RAG and other document QA application. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n783 | P a g e  \nwww.ijacsa.thesai.org \nD. BERTScore \nBERTScore evaluation compares the results of our \nproposed QA method outperforming other QA applications in \nterms of precision, recall, and F1 -Score. Specifically, on our \ndataset, our proposed method demonstrate d precision, recall, \nand F1 -Score of 85.2%, 90.1%, and 87.6%, respectively. In \ncomparison, other QA applications achieved lower \nperformance metrics with precision, recall, and F1 -Score of \n81.6%, 86.3%, and 83.8%, respectively. Similarly, on the \nSQuAD dev d ataset, our proposed method excelled with \nprecision, recall, and F1 -Score reaching 82.8%, 87.0%, and \n84.8%, while other QA applications reported lower scores of \n80.4%, 86.3%, and 83.2%, respectively. These results \nunderscore the superior performance of our  proposed method \nacross both datasets that can be visualized in Fig. 10. \nE. BLEU Accuracy \nThe BLEU metric score taken is the precision value, to \ncapture the ability of each model to extract keyword answers \nthat match the ground truth. Specifically, on our dat aset, our \nproposed method demonstrated precision of 28.2%. In \ncomparison, other QA applications achieved lower \nperformance precision 9.7%. Similarly, on the SQuAD dev \ndataset, our proposed method excelled with precision 17.7%, \nwhile other QA applications reported lower scores of precision \n5.6%. These results underscore the superior performance of our \nproposed method across both datasets that can be visualized in \nFig. 11. \nF. Jaccard Similarity \n The performance of our QA system, as evaluated through \nJaccard Similarity, is outstanding. Our method achieved 33.3% \non our dataset and 11.1% on SQuAD dev using RAG method. \nIn comparison, other QA applications scored lower with 4.1% \non our dataset and 9.1% on SQuAD dev. These results \nhighlight our method's superiority in Jaccard Similarity on both \ndatasets that can be visualized in Fig. 12. \nG. Discussion \nThe accuracy result of 95.5% in the SQuAD dev dataset \noutperforms other research with 61.5%  accuracy that tested in \nSQuAD dev dataset [30] and 71.4% accuracy which also tested \nin SQuAD dev dataset [31]. We also using SQuAD dev dataset \nfor testing the other document QA application platform, and it \nshows accuracy 85.7%. So, the model proposed in this study \nhas a higher accuracy score compared to other applicat ions, \nand previous research on the SQuAD test dataset. \n \nFig. 10. BERTScore result of proposed method using RAG and other document QA application. \n \nFig. 11. BLEU precision result of proposed method using RAG and other document QA application. \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n784 | P a g e  \nwww.ijacsa.thesai.org \n \nFig. 12. Jaccard Similarity result of proposed method using RAG and other document QA application. \nOur system's precision, recall, and F1 -Score are 82.8%, \n87%, and 84.8%, respectively, which surpass the precision of \n62%, recall of 87%, and F1 -Score of 67% reported in other \nresearch [32]. The proposed QA system's effectiveness is \naffirmed by the fact that it surpasses the recall result of other \nresearch with 42.70% [33] and outperforms other research \n[31], [34], [35] in terms of F1-Score, which is 42.6% [31], 49% \n[34], and 70.8% [35]. This positions it as a leading solution for \nautomatic document processing and information retrieval tasks \nacross a wide range of domains. \nBased on the results of testing the proposed model, the \nresults of the present study agree with previous literature \nstudies, namely that the RAG  method, through the \nimplementation of a hybrid model combining parametric and \nnonparametric models, is able to provide good results  [4]. In \nthis case we combine the LangChai n and FAISS frameworks \nfor the RAG technique, and it can provide a good result. This \nmodel also c ombined with the use of the best language model  \nat this current time like GPT-3.5, which provides good results. \nThis is a very interesting performance that should be further \ndeveloped. \nV. CONCLUSION \nOur proposed model for Question -Answering (QA) \ndocument processing integrates the Retrieval -Augmented \nGeneration (RAG) model. The evaluation of our proposed QA \nsystem demonstrates its superiority over existing commercial \napplications in terms of Accuracy, ROUGE -L scores, \nBERTScore metrics, BLEU precision, and Jaccard Similarity. \nThe proposed method achieved high accuracy rates of 96% and \n95.5% on our dataset and the SQuAD dev dataset, respectively, \noutperforming other applications tested on the same datasets. \nOur system's precision, recall, and F1 -Score metrics were \nsuperior to those of other QA applications on both datasets, as \nhighlighted by the ROUGE -L evalu ation. Additionally, the \nBERTScore metrics consistently showed higher precision, \nrecall, and F1 -Score for our proposed method compared to \nother applications. In addition, our QA system has \ndemonstrated superior performance in keyword extraction and \ntext si milarity compared to other applications, as assessed by \nBLEU precision and Jaccard Similarity. \nVI. FUTURE WORKS \nIn the future, studies could be conducted to refine the \narchitecture of the system, explore additional ways of using \nexternal data, and improve the scalability of the model for \nbroader applications. The i ntegration of user feedback \nmechanisms and continuous learning modules could contribute \nto the adaptability of the system and further improve its \naccuracy over time. In addition, exploring ways of pro cessing \ndocuments in real time and extending the system's \ncompatibility with different document formats could open up \nnew opportunities for research and study. \nREFERENCES \n[1] F. Ganier and R. Querrec, ‚ÄúTIP-EXE: A Software Tool for Studying the \nUse and Understa nding of Procedural Documents,‚Äù IEEE Trans Prof \nCommun, vol. 55, no. 2, pp. 106 ‚Äì121, Jun. 2012, doi: \n10.1109/TPC.2012.2194600. \n[2] W. Yih, M.-W. Chang, X. He, and J. Gao, ‚ÄúSemantic Parsing via Staged \nQuery Graph Generation: Question Answering with Knowledge Ba se,‚Äù \nin Proceedings of the 53rd Annual Meeting of the Association for \nComputational Linguistics and the 7th International Joint Conference on \nNatural Language Processing (Volume 1: Long Papers), Stroudsburg, \nPA, USA: Association for Computational Linguisti cs, 2015, pp. 1321 ‚Äì\n1331. doi: doi.org/10.3115/v1/P15-1128. \n[3] Y. Hao et al., ‚ÄúAn End -to-End Model for Question Answering over \nKnowledge Base with Cross -Attention Combining Global Knowledge,‚Äù \nin Proceedings of the 55th Annual M eeting of the Association for \nComputational Linguistics (Volume 1: Long Papers), Stroudsburg, PA, \nUSA: Association for Computational Linguistics, 2017, pp. 221 ‚Äì231. \ndoi: 10.18653/v1/P17-1021. \n[4] P. Lewis et al., ‚ÄúRetrieval -Augmented Generation for Knowledge -\nIntensive NLP Tasks,‚Äù NIPS‚Äô20: Pro ceedings of the 34th International \nConference on Neural Information Processing Systems, vol. \nabs/2005.11401, pp. 9459 ‚Äì9474, May 2020, doi: \n10.48550/arXiv.2005.11401. \n[5] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana, \nand S. Nanayakkara, ‚ÄúIm proving the Domain Adaptation of Retrieval \nAugmented Generation (RAG) Models for Open Domain Question \nAnswering,‚Äù Trans Assoc Comput Linguist, vol. 11, pp. 1‚Äì17, 2023, doi: \n10.1162/tacl_a_00530. \n[6] Y. Ahn, S. -G. Lee, J. Shim, and J. Park, ‚ÄúRetrieval -Augmented \nResponse Generation for Knowledge -Grounded Conversation in the \nWild,‚Äù IEEE Access, vol. 10, pp. 131374 ‚Äì131385, 2022, doi: \n10.1109/ACCESS.2022.3228964. \n[7] Xiong, S. Merity, and R. Socher, ‚ÄúDynamic Memory Networks for \nVisual and Textual Question Answering,‚Äù Pr oceedings of The 33rd \n\n(IJACSA) International Journal of Advanced Computer Science and Applications, \nVol. 15, No. 3, 2024 \n785 | P a g e  \nwww.ijacsa.thesai.org \nInternational Conference on Machine Learning, pp. 2397 ‚Äì2406, Mar. \n2016, doi: 10.48550/arXiv.1603.01417. \n[8] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, ‚ÄúBidirectional \nAttention Flow for Machine Comprehension,‚Äù International Confer ence \non Learning Representations, Nov. 2016, doi: \n10.48550/arXiv.1611.01603. \n[9] A. W. Yu et al., ‚ÄúQANet: Combining Local Convolution with Global \nSelf-Attention for Reading Comprehension,‚Äù International Conference \non Learning Representations, vol. abs/1804.095 41, Apr. 2018, doi: \n10.48550/arXiv.1804.09541. \n[10] W. Yang, Y. Xie, L. Tan, K. Xiong, M. Li, and J. Lin, ‚ÄúData \nAugmentation for BERT Fine -Tuning in Open -Domain Question \nAnswering,‚Äù ArXiv, vol. abs/1904.06652, Apr. 2019, doi: \n10.48550/arXiv.1904.06652. \n[11] C. Raffe l et al., ‚ÄúExploring the Limits of Transfer Learning with a \nUnified Text -to-Text Transformer,‚Äù Journal of Machine Learning \nResearch, vol. 21, pp. 140:1 -140:67, 2019, doi: \n10.48550/arXiv.1910.10683. \n[12] A. Roberts, C. Raffel, and N. Shazeer, ‚ÄúHow Much Knowledge Can You \nPack Into the Parameters of a Language Model?,‚Äù in Proceedings of the \n2020 Conference on Empirical Methods in Natural Language Processing \n(EMNLP), Stroudsburg, PA, USA: Association for Computational \nLinguistics, 2020, pp. 5418 ‚Äì5426. doi: 10.18653/ v1/2020.emnlp-\nmain.437. \n[13] M. T. R. Laskar, M. S. Bari, M. Rahman, M. A. H. Bhuiyan, S. R. Joty, \nand J. Huang, ‚ÄúA Systematic Study and Comprehensive Evaluation of \nChatGPT on Benchmark Datasets,‚Äù in Annual Meeting of the \nAssociation for Computational Linguisti cs, 2023. [Online]. Available: \nhttps://api.semanticscholar.org/CorpusID:258967462 \n[14] W. Yu, ‚ÄúRetrieval -augmented Generation across Heterogeneous \nKnowledge,‚Äù in Proceedings of the 2022 Conference of the North \nAmerican Chapter of the Association for Computation al Linguistics: \nHuman Language Technologies: Student Research Workshop, Seattle, \nWashington: Association for Computational Linguistics, Jul. 2022, pp. \n52‚Äì58. doi: 10.18653/v1/2022.naacl-srw.7. \n[15] D. Thulke, N. Daheim, C. Dugast, and H. Ney, ‚ÄúEfficient Retriev al \nAugmented Generation from Unstructured Knowledge for Task -\nOriented Dialog,‚Äù Conference of Association for the Advancement of \nArtificial Intelligence (AAAI), Feb. 2021, doi: \n10.48550/arXiv.2102.04643. \n[16] OpenAI, ‚ÄúA Survey of Techniques for Maximizing LLM Pe rformance.‚Äù \nNov. 2023. \n[17] Jacob Lee, ‚ÄúBuilding LLM -Powered Web Apps with Client -Side \nTechnology.‚Äù Accessed: Dec. 01, 2023. [Online]. Available: \nhttps://ollama.ai/blog/building-llm-powered-web-apps \n[18] J. Johnson, M. Douze, and H. J√©gou, ‚ÄúBillion -Scale Similarity Search \nwith GPUs,‚Äù IEEE Trans Big Data, vol. 7, no. 3, pp. 535‚Äì547, 2021, doi: \n10.1109/TBDATA.2019.2921572. \n[19] J. Zhu, J. Jang -Jaccard, I. Welch, H. Al -Sahaf, and S. Camtepe, A \nRansomware Triage Approach using a Task Memory based on Meta -\nTransfer Learning Framework. 2022. doi: 10.48550/arXiv.2207.10242. \n[20] K. M. Fitria, ‚ÄúDocuQA: Document Question Answering Dataset.‚Äù Feb. \n2024. doi: 10.6084/m9.figshare.25223990.v1. \n[21] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSQuAD: 100,000+ \nQuestions for Machine Comprehensi on of Text,‚Äù in Conference on \nEmpirical Methods in Natural Language Processing, 2016. [Online]. \nAvailable: https://api.semanticscholar.org/CorpusID:11816014 \n[22] A. Chen, G. Stanovsky, S. Singh, and M. Gardner, ‚ÄúEvaluating Question \nAnswering Evaluation,‚Äù in Pro ceedings of the 2nd Workshop on \nMachine Reading for Question Answering, A. Fisch, A. Talmor, R. Jia, \nM. Seo, E. Choi, and D. Chen, Eds., Hong Kong, China: Association for \nComputational Linguistics, Nov. 2019, pp. 119 ‚Äì124. doi: \n10.18653/v1/D19-5817. \n[23] T. Zhan g, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \n‚ÄúBERTScore: Evaluating Text Generation with BERT,‚Äù International \nConference on Learning Representations, vol. abs/1904.09675, Apr. \n2019, doi: 10.48550/arXiv.1904.09675. \n[24] B. Ojokoh and E. Adebisi, ‚ÄúA Review of Question Answering Systems,‚Äù \nJournal of Web Engineering, vol. 17, no. 8, pp. 717 ‚Äì758, 2019, doi: \n10.13052/jwe1540-9589.1785. \n[25] J. Soni, N. Prabakar, and H. Upadhyay, ‚ÄúBehavioral Analysis of System \nCall Sequences Using LSTM Seq -Seq, Cosine Similarity an d Jaccard \nSimilarity for Real -Time Anomaly Detection,‚Äù in 2019 International \nConference on Computational Science and Computational Intelligence \n(CSCI), IEEE, Dec. 2019, pp. 214 ‚Äì219. doi: \n10.1109/CSCI49370.2019.00043. \n[26] J. F. BELL and A. H. FIELDING, ‚ÄúA revie w of methods for the \nassessment of prediction errors in conservation presence/absence \nmodels,‚Äù Environ Conserv, vol. 24, no. 1, pp. 38 ‚Äì49, 1997, doi: DOI: \n10.1017/S0376892997000088. \n[27] C.-Y. Lin, ‚ÄúROUGE: A Package for Automatic Evaluation of \nSummaries,‚Äù Assoc iation for Computational Linguistics, vol. Text \nSumma, no. 12, pp. 74 ‚Äì81, 2004, [Online]. Available: \nhttps://aclanthology.org/W04-1013/ \n[28] K. Papineni, S. Roukos, T. Ward, and W. -J. Zhu, ‚ÄúBleu: a Method for \nAutomatic Evaluation of Machine Translation,‚Äù in Pro ceedings of the \n40th Annual Meeting of the Association for Computational Linguistics, \nP. Isabelle, E. Charniak, and D. Lin, Eds., Philadelphia, Pennsylvania, \nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311 ‚Äì\n318. doi: 10.3115/1073083.1073135. \n[29] N. C. Chung, B. Miasojedow, M. Startek, and A. Gambin, \n‚ÄúJaccard/Tanimoto similarity test and estimation methods for biological \npresence-absence data,‚Äù BMC Bioinformatics, vol. 20, no. 15, p. 644, \n2019, doi: 10.1186/s12859-019-3118-5. \n[30] A. Stricker, ‚ÄúQuestion answering in Natural Language: the Special Case \nof Temporal Expressions,‚Äù in Proceedings of the Student Research \nWorkshop Associated with RANLP 2021, S. Djabri, D. Gimadi, T. \nMihaylova, and I. Nikolova -Koleva, Eds., Online: INCOMA Ltd., Sep. \n2021, pp.  184‚Äì192. [Online]. Available: \nhttps://aclanthology.org/2021.ranlp-srw.26 \n[31] S. Min, V. Zhong, R. Socher, and C. Xiong, ‚ÄúEfficient and Robust \nQuestion Answering from Minimal Context over Documents,‚Äù in \nProceedings of the 56th Annual Meeting of the Association  for \nComputational Linguistics (Volume 1: Long Papers), I. Gurevych and \nY. Miyao, Eds., Melbourne, Australia: Association for Computational \nLinguistics, Jul. 2018, pp. 1725‚Äì1735. doi: 10.18653/v1/P18-1160. \n[32] H. Bahak, F. Taheri, Z. Zojaji, and A. Kazemi, ‚ÄúEvaluating ChatGPT as \na Question Answering System: A Comprehensive Analysis and \nComparison with Existing Models,‚Äù ArXiv, vol. abs/2312.07592, Dec. \n2023, doi: 10.48550/arXiv.2312.07592. \n[33] T. Cakaloglu, C. Szegedy, and X. Xu, ‚ÄúText Embeddings for Retrieval \nFrom a Large Knowledge Base,‚Äù Research Challenges in Information \nScience, vol. abs/1810.10176, Oct. 2018, doi: \n10.48550/arXiv.1810.10176. \n[34] S. Gholami and M. Noori, ‚ÄúZero -Shot Open -Book Question \nAnswering,‚Äù ArXiv, vol. abs/2111.11520, Nov. 2021, doi: \ndoi.org/10.48550/arXiv.2111.11520. \n[35] G. Nur Ahmad and A. Romadhony, ‚ÄúEnd -to-End Question Answering \nSystem for Indonesian Documents Using TF -IDF and IndoBERT,‚Äù in \n2023 10th International Conference on Advanced Informatics: Concept, \nTheory and Application (ICAICTA), 20 23, pp. 1 ‚Äì6. doi: \n10.1109/ICAICTA59291.2023.10390111. \n ",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.813622236251831
    },
    {
      "name": "Information retrieval",
      "score": 0.6076260805130005
    },
    {
      "name": "Computer science",
      "score": 0.6008085608482361
    },
    {
      "name": "Natural language processing",
      "score": 0.546917736530304
    },
    {
      "name": "Language model",
      "score": 0.44162386655807495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3548527956008911
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210132611",
      "name": "Informatics and Business Institute Darmajaya",
      "country": "ID"
    }
  ],
  "cited_by": 18
}