{
    "title": "Training Vision Transformers in Federated Learning with Limited Edge-Device Resources",
    "url": "https://openalex.org/W4293029286",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1990088120",
            "name": "Jiang Tao",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A2135624771",
            "name": "Zhen Gao",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A259644255",
            "name": "Zhaohui Guo",
            "affiliations": [
                "Tianjin University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W3164573547",
        "https://openalex.org/W3206144059",
        "https://openalex.org/W2541884796",
        "https://openalex.org/W2995022099",
        "https://openalex.org/W2963209930",
        "https://openalex.org/W2903470619",
        "https://openalex.org/W3094163844",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3044211235",
        "https://openalex.org/W2990614164",
        "https://openalex.org/W2535838896",
        "https://openalex.org/W3128339117",
        "https://openalex.org/W2978110818",
        "https://openalex.org/W6773976177"
    ],
    "abstract": "Vision transformers (ViTs) demonstrate exceptional performance in numerous computer vision tasks owing to their self-attention modules. Despite improved network performance, transformers frequently require significant computational resources. The increasing need for data privacy has encouraged the development of federated learning (FL). Traditional FL places a computing burden on edge devices. However, ViTs cannot be directly applied through FL on resource-constrained edge devices. To utilize the powerful ViT structure, we reformulated FL as a federated knowledge distillation training algorithm called FedVKD. FedVKD uses an alternating minimization strategy to train small convolutional neural networks on edge nodes and periodically transfers their knowledge to a large server-side transformer encoder via knowledge distillation. FedVKD affords the benefits of reduced edge-computing load and improved performance for vision tasks, while preserving FedGKT-like asynchronous training. We used four datasets and their non-IID variations to test the proposed FedVKD. When utilizing a larger dataset, FedVKD achieved higher accuracy than FedGKT and FedAvg.",
    "full_text": null
}