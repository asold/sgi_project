{
  "title": "Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification",
  "url": "https://openalex.org/W4226139798",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2581083981",
      "name": "Xinlei Cao",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2111068811",
      "name": "Jinyang Yu",
      "affiliations": [
        "Guangdong Province Environmental Monitoring Center"
      ]
    },
    {
      "id": "https://openalex.org/A1978783002",
      "name": "Yan Zhuang",
      "affiliations": [
        "Chinese PLA General Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156333129",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2250966211",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W3121601851",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W2251292973",
    "https://openalex.org/W2962902802",
    "https://openalex.org/W2970261805",
    "https://openalex.org/W2563010554",
    "https://openalex.org/W2963467630",
    "https://openalex.org/W2987954551",
    "https://openalex.org/W3117344805",
    "https://openalex.org/W2537027648",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2242874043",
    "https://openalex.org/W2788889673",
    "https://openalex.org/W2776249353",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W6732742072",
    "https://openalex.org/W6732388730",
    "https://openalex.org/W2514530580",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6736057607",
    "https://openalex.org/W6765807869",
    "https://openalex.org/W6761910064",
    "https://openalex.org/W6727690538",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6755977528",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2908510526"
  ],
  "abstract": "This paper mainly studies the combination of pre-trained language models and user identity information for document-level sentiment classification. In recent years, pre-trained language models (PLMs) such as BERT have achieved state-of-the-art results on many NLP applications, including document-level sentiment classification. On the other hand, a collection of works introduce additional information such as user identity for better text modeling. However, most of them inject user identity into traditional models, while few studies have been conducted to study the combination of pre-trained language models and user identity for even better performance. To address this issue, in this paper, we propose to unite user identity and PLMs and formulate User-enhanced Pre-trained Language Models (U-PLMs). Specifically, we demonstrate two simple yet effective attempts, i.e. embedding-based and attention-based personalization, which inject user identity into different parts of a pre-trained language model and provide personalization from different perspectives. Experiments in three datasets with two backbone PLMs show that our proposed methods outperform the best state-of-the-art baseline method with an absolute improvement of up to 3&#x0025;, 2.8&#x0025;, and 2.2&#x0025; on accuracy. In addition, our methods encode user identity with plugin modules, which are fully compatible with most auto-encoding pre-trained language models.",
  "full_text": "Received February 9, 2022, accepted March 9, 2022, date of publication March 11, 2022, date of current version March 22, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3158975\nInjecting User Identity Into Pretrained\nLanguage Models for Document-Level\nSentiment Classification\nXINLEI CAO\n1, JINYANG YU2, AND YAN ZHUANG3,4\n1School of Computer Science and Technology, East China Normal University, Shanghai 200062, China\n2Center for ADR Monitoring of Guangdong, Guangzhou 510080, China\n3National Engineering Laboratory for Medical Big Data Application Technology, Chinese PLA General Hospital, Beijing 100853, China\n4Medical Innovation Research Division, Medical Big Data Research Center, Chinese PLA General Hospital, Beijing 100853, China\nCorresponding author: Xinlei Cao (cyril.xlcao@gmail.com)\nThis work was supported by the Medical Big Data and AI Research Development Project of the People’s Liberation Army General\nHospital (PLAGH) (2019MBD-046).\nABSTRACT This paper mainly studies the combination of pre-trained language models and user iden-\ntity information for document-level sentiment classiﬁcation. In recent years, pre-trained language mod-\nels (PLMs) such as BERT have achieved state-of-the-art results on many NLP applications, including\ndocument-level sentiment classiﬁcation. On the other hand, a collection of works introduce additional\ninformation such as user identity for better text modeling. However, most of them inject user identity into\ntraditional models, while few studies have been conducted to study the combination of pre-trained language\nmodels and user identity for even better performance. To address this issue, in this paper, we propose to unite\nuser identity and PLMs and formulate User-enhanced Pre-trained Language Models (U-PLMs). Speciﬁcally,\nwe demonstrate two simple yet effective attempts, i.e. embedding-based and attention-based personalization,\nwhich inject user identity into different parts of a pre-trained language model and provide personalization\nfrom different perspectives. Experiments in three datasets with two backbone PLMs show that our proposed\nmethods outperform the best state-of-the-art baseline method with an absolute improvement of up to 3%,\n2.8%, and 2.2% on accuracy. In addition, our methods encode user identity with plugin modules, which are\nfully compatible with most auto-encoding pre-trained language models.\nINDEX TERMS Representation learning, document-level sentiment classiﬁcation, personalized sentiment\nclassiﬁcation, pre-trained language models, attention mechanism.\nI. INTRODUCTION\nDocument-level sentiment classiﬁcation, as a common sub-\ntask of sentiment analysis and text classiﬁcation, aims to\nextract the sentiment polarity in a piece of text document\n(e.g. reviews and tweets) [1].\nRepresentative works of document-level sentiment classi-\nﬁcation are mainly based on neural networks [2]–[4]. Based\non the bag-of-words assumption, typical models usually con-\nsist of the following steps. Firstly, they learn word embed-\ndings to represent words as vectors. Secondly, they use CNNs\nor RNNs to capture dependencies between words. Then they\napply attention mechanisms or simply take the vector sum or\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Victor S. Sheng.\naverage over these vectors to constitute a representation of\nthe whole document. Finally, they make predictions based on\nthe text representation. However, these methods are based on\nstatic word embeddings such as word2vec [5] and GloVe [6]\nand cannot model words well.\nRecent years have seen the rise of a large number of\nPre-trained Language Models (PLMs) based on Transform-\ners [7], such as BERT [8], RoBERTa [9], ALBERT [10], etc.\nThese methods use self-attention and usually deep architec-\ntures to capture dependencies between words repeatedly and\nproduce word representations that are well aware of the whole\ncontext in the text. After pre-trained on large-scale unlabeled\ntext corpora, one can use a PLM by easily adding some\ntask-speciﬁc layers and then ﬁne-tuning the whole model\nwith labeled data. BERT and its alternatives have achieved\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 30157\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nstate-of-the-art results on many NLP tasks, such as question\nanswering, language inference, and text classiﬁcation.\nHowever, few studies have been conducted to apply PLMs\nin the task of personalized sentiment classiﬁcation where\napart from the text itself, its context information such as\nidentities of the user (who is writing it) and the item (what it\nis describing) is also available and can help generate a better\nrepresentation of the text. Most methods for this task are tra-\nditional CNN/RNN-based methods [11]–[16], whereas CNN\nand RNN both have their weakness. Therefore, it is a natural\nthought to ‘‘upgrade’’ the backbone models from CNN/RNN\nto the stronger PLMs. A recent work [17] tried to incorporate\ncontext information into PLMs by using historical reviews of\nuser/item to help the prediction, but the target review text is\nstill modeled without considering user or item in their work.\nTo this end, we propose User-enhanced Pre-trained\nLanguage Models (U-PLMs) to take advantages of both\nPLMs and user data in the process of text modeling. To be\nspeciﬁc, we propose two schemes of personalization, which\ninject user identities into different modules of a pre-trained\nlanguage model.\nThe embedding-based personalization takes advantage of\nthe design of PLMs [8]–[10] that they can accept the sum of\nmultiple embeddings as input. We associate the user id of the\ntext with an embedding vector, and then add it to all words\nat the input. With this document-level global bias, PLMs can\nencode correlations between words in a personalized way and\noutput representations closer to users’ actual intents.\nThe attention-based personalization is inspired by a com-\nmon pattern of some traditional methods [14]–[16]. These\nmethods are based on hierarchical RNN/CNNs, which con-\nstruct a sentence with words in the lower level and then con-\nstruct a document with sentences in the higher level. To incor-\nporate context information, these methods add embedding\nvectors of users and items in attention functions of both levels\nto help select important words in a sentence and important\nsentences in a document. To transfer this idea into PLMs,\nwe introduce user embeddings in self-attention modules in\nPLMs. For each text, we add the embedding of its user as\na bias of the [CLS] token’s query in self-attention, where\n[CLS] is a special token used to represent the whole text. With\nthese enhanced self-attention modules, PLMs are capable of\nselecting user-speciﬁc important words in each layer and\ngenerating more accurate representations of the text.\nOur main contributions are as follows:\n1) We propose to apply PLMs in the task of personalized\nsentiment classiﬁcation for better text representation,\nmaking use of both PLMs’ modeling ability and user\nidentity information in the process of text modeling.\n2) We demonstrate two different attempts to inject user\nidentities into different modules of a pre-trained lan-\nguage model, both of which can effectively improve the\nperformance of sentiment classiﬁcation.\n3) Both of our attempts serve as plugins and are fully\ncompatible with most auto-encoding PLMs such as\nBERT and RoBERTa.\n4) Experiments on three datasets with two backbone\nPLMs show that our framework obtains remarkable\nimprovements over vanilla PLMs and state-of-the-art\nmodels for personalized sentiment classiﬁcation.\nII. RELATED WORK\nA. DOCUMENT-LEVEL SENTIMENT CLASSIFICATION\nDocument-level sentiment classiﬁcation is a task aiming at\ninferring the overall sentiment polarity of a given document,\nwhich is usually a review that describes how the author thinks\nof a particular item (movie, goods, restaurant, etc.) or a tweet\nthat expresses the author’s mood or opinion at some moment.\nSpeciﬁcally, given a document consisting of multiple sen-\ntences, the problem is to determine whether this document\nconveys a positive/negative opinion or even to what extent the\npositive/negative sentiment is. In the ﬁrst case, the problem\nis deﬁned as a binary classiﬁcation task, where 0 represents\nnegative and 1 stands for positive. In the latter case, for the\ncase of scores from 1 to 5 stars as an example, 1 and 5\ncorrespond to greatly negative and greatly positive opinions\nrespectively, and 3 means a neutral sentiment. Then the prob-\nlem is usually deﬁned as a 5-class classiﬁcation task.\nBased on the bag-of-words assumption, a common struc-\nture within typical methods for document-level sentiment\nclassiﬁcation usually consists of four parts: (1) a word embed-\nding layer to represent words, (2) CNNs or RNNs to capture\nword dependencies, (3) attention or simple pooling functions\nto gather word-level information and represent the document,\nand (4) a classiﬁer for sentiment classiﬁcation.\nQuite a few methods are proposed to make improve-\nments to this structure. Xu et al.proposes CLSTM [18], i.e.\na LSTM [19] with cache mechanism, to capture the overall\nsemantic information in long texts. Reference [3] sees a\ndocument from three levels: word, sentence, and document.\nIt applies CNN/LSTM to word embeddings within a sentence\nto construct the representation of this speciﬁc sentence, and\nthen similarly represents the document with multiple sen-\ntences. Based on this hierarchical structure, reference [14]\nfurther discovers the different importance of words (sen-\ntences) within a sentence (document) and introduces atten-\ntion mechanism into the procedure of sentence (document)\ncomposition with words (sentences).\nHowever, these methods are all based on static word\nembeddings such as word2vec [5] and GloVe [6] which have\nlimited expression ability.\nOne issue of these embedding approaches is that they\nonly learn a single vector for a word but cannot handle the\ncases that one word can have different meanings in differ-\nent contexts (such as the word ‘‘chair’’ with meanings of\n‘‘seat’’ or ‘‘leader’’). On the contrary, pre-trained language\nmodels, which are pre-trained on a large corpus of unlabeled\ntext, can generate context-aware word representations in the\nphase of ﬁne-tuning for speciﬁc tasks. Therefore, there is a\ntrend towards the adoption of pre-trained language models in\nplace of traditional methods for many NLP tasks including\ndocument-level sentiment classiﬁcation.\n30158 VOLUME 10, 2022\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nAnother problem of these static word embeddings is that\nthey learn representations of words according to contexts\nbut ignore their sentiment polarities, which is problematic\nwhen applied to sentiment classiﬁcation. For example, words\n‘‘good’’ and ‘‘bad’’ with similar contexts are mapped into\nclose word vectors in the embedding space, but they have\nopposite sentiment polarities. To solve this problem, there\nare some works [20]–[22] focusing on injecting sentiment\nknowledge into word embeddings for better performance for\nthe task of sentiment classiﬁcation.\nB. PERSONALIZED SENTIMENT CLASSIFICATION\nThe task of personalized sentiment classiﬁcation is a subtask\nof document-level sentiment classiﬁcation which assumes\nthat user and item ids of the text are also available.\nFor better performance of document-level sentiment clas-\nsiﬁcation, some researchers have conducted studies to intro-\nduce user representations and item representations in the\nanalysis of review texts [13]–[16]. The intuition is that these\nrepresentations can provide global information such as rating\nand language preferences of users and overall ratings of\nitems [23]. Taking this information into account can help\nprovide better text representations.\nChen et al.[14] make improvements over the hierarchical\nLSTM structure and introduce user and item embeddings into\nthe word-sentence and sentence-document attention func-\ntions. Wu et al.[15] propose to model the same piece of text\nfrom the user’s and item’s perspective, respectively, and then\ncombine the document representations from these two views\ntogether for prediction. Based on that, Yuan et al.[16] intro-\nduce the memory network [24] for users and items to alleviate\nthe cold-start problem by modeling the inherent correlation\nbetween users or items. Speciﬁcally, they store representa-\ntions of representative users or items in memory slots and then\nuse them to infer representations for cold-start users or items.\nDifferent from all these methods, Amplayo [13] attempt to\nrepresent users and items with their proposed ‘‘chunk-wise’’\nweight matrices instead of bias vectors and inject them into\nfour locations (i.e. embedding, encoding, attention, classiﬁer)\nin a model.\nNote that there are some works for rating prediction [25],\n[26] which also take user, item, and text as input and predict\nthe rating scores. However, they assume that the target review\ntext is unknown when testing, which is different from our\ntask. Therefore, we are not considering them for comparison.\nC. PRE-TRAINED LANGUAGE MODELS\nIn recent years, a large number of Pre-trained Language Mod-\nels (PLMs) based on Transformers [7] have emerged. These\nmodels are ﬁrstly pre-trained on a large-scale text corpus\nwhich is unlabeled and therefore easy to acquire. In this\npre-training step, the models can learn universal language\nrepresentations [27]. After pre-training, a model can then be\neasily applied to a speciﬁc task and ﬁne-tuned together with\nrandomly initialized task-speciﬁc parameters using a small\nlearning rate. Since the model has been pre-trained on lots\nof data before, it can avoid overﬁtting the small amount of\nlabeled data for speciﬁc tasks. Instead, the model converges\nfast and usually outperforms traditional methods which are\nonly trained on the task-speciﬁc labeled data.\nSince the Transformer [7] consists of two parts: an encoder\nand a decoder, there are correspondingly two types of\npre-trained language models.\nThe ﬁrst type are the autoregressive models. These models\npredict a word based on the words which precede or suc-\nceed it, which is similar to the traditional statistical language\nmodels. Therefore, methods of this type are usually used for\ngenerative tasks. Representative works are the GPT series\n[28]–[30]. GPT [28] proposes the stages of generative\npre-training and discriminative ﬁne-tuning and is the ﬁrst\nwork to make use of transformer structure for text modeling.\nGPT-2 [29] formulates the supervised tasks as unsupervised\nproblems and demonstrates their model’s ability to perform\na wide range of tasks in a zero-shot setting. GPT-3 [30]\nremoves the stage of ﬁne-tuning. It takes the idea similar to\nMAML [31], uses two nested structures called ‘‘inner-loop’’\nand ‘‘outer-loop’’ in pre-training, and learns a good initial-\nization point for the model. Starting from this initialization\npoint, when faced with any speciﬁc task, the model quickly\nﬁts in with the task and converges within only a few samples.\nThe second type, the autoencoder models, encode token\ncorrelations in a bi-directional manner. The encoding way\nof ‘‘bi-direction’’ makes it unavailable for generative tasks\nbut greatly suitable for discriminative tasks such as text\nclassiﬁcation, sequence labeling, etc. Representative works\nmainly consist of BERT [8] and its variants [9], [10], [32],\n[33]. BERT introduces two tasks in the pre-training stage, i.e.\nMasked LM (MLM) and Next Sentence Prediction (NSP).\nRoBERTa [9] improves BERT by performing dynamic mask-\ning of tokens. ALBERT [10] uses techniques of factorized\nembedding parameterization and cross-layer parameter shar-\ning to reduce the parameters of BERT. SpanBERT [32] masks\ncontiguous words instead of single tokens and introduces\nthe span boundary objective to extend the BERT’s sense of\ntext spans. Similarly, ERNIE [33] proposed by Baidu inte-\ngrates knowledge into BERT with entity-level and phrase-\nlevel masking strategies.\nSince we mainly focus on methods for the task of sen-\ntiment classiﬁcation, we are not going to mention the\ndecoder-based methods in later sections. Therefore, we refer\nto the encoder-based methods (BERT, RoBERTa, etc.) as\npre-trained language models or PLMs in this paper for\nconvenience.\nIII. METHODOLOGY\nIn this section, we ﬁrst deﬁne the problem of person-\nalized sentiment classiﬁcation. Then we introduce our\nproposed U-PLMs which is divided into three modules.\nFinally, we illustrate the two-stage training procedure of our\nframework.\nFig. 1 shows an overview of our framework. As shown in\nthe ﬁgure, user identity is used in two modules in our work.\nVOLUME 10, 2022 30159\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nFIGURE 1. An overview of U-PLMs with BERT as the backbone model.\nThe embedding-based personalization is introduced in the\nembedding module (section III-C), and the attention-based\npersonalization is introduced in the self-attention module in\nencoder blocks (section III-D). We use BERT as the backbone\nmodel for illustration in this section.\nA. PROBLEM DEFINITION\nGiven a piece of text y written by a user u for an item v,\nthe goal of personalized sentiment classiﬁcation is to predict\nthe sentiment category r (e.g. 1-5 stars) of the text, which\nrepresents the user’s opinion on the item.\nNote that we’re using text y and user u, but not item v, in our\nframework. This is because we found that user information is\nmuch more important and effective for personalization than\nitem information in our experiments, which is consistent with\nthe observation of [11].\nB. MODEL INPUT\nAt the very ﬁrst step, a piece of natural language text is\ntokenized into a sequence of tokens/subwords by a subword\nalgorithm (e.g. WordPiece [34] for BERT). The sequence is\nrepresented as:\n{[CLS], W1, W2, W3, . . . ,Wn} (1)\nNote that a special token [CLS] is padded at the beginning\nof the sequence. This token has no actual meaning itself but\nis designed to provide sentence-level information in BERT.\nIts output is often used for sentence-level tasks, including\ntext classiﬁcation. In our work, we apply BERT to the task\nof document-level sentiment classiﬁcation by regarding a\ndocument as a long sentence.\nC. EMBEDDING MODULE\nIn this module, BERT accepts the sequence in (1) as input and\nconvert it to a sequence of hidden states:\nE ={e [CLS], e1, e2, e3, . . . ,en} (2)\nEach hidden state is computed via taking summation\nof token embeddings, position embeddings, and segment\nembeddings:\ne[CLS] =etoken\n[CLS] +epos\n0 +eseg\n0\ne1 =etoken\nw1 +epos\n1 +eseg\n0\n30160 VOLUME 10, 2022\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\n..\n.\nen =etoken\nwn +epos\nn +eseg\n0 (3)\nSince segment embeddings are used to distinguish tokens\nin different sentences for sentence pair tasks such as question\nanswering. However, they are not important in single sentence\ntasks including ours. Based on this reason, we are not men-\ntioning them in Fig. 1 for lack of space.\n1) USER INJECTION\nTo inject user information into the embedding module,\nwe take advantage of the design of PLMs that the sum of\nmultiple embeddings is used as the representation for a token.\nSpeciﬁcally, we introduce a new parameter matrix of user\nembeddings and add the user embedding vector for each text\nto all tokens as a document-level global bias. That is, equation\nin (3) is updated as follow:\ne[CLS] =etoken\n[CLS] +epos\n0 +eseg\n0 +euser\nu\ne1 =etoken\nw1 +epos\n1 +eseg\n0 +euser\nu\n..\n.\nen =etoken\nwn +epos\nn +eseg\n0 +euser\nu (4)\nwhere euser\nu represents the embedding vector for user u who\nwrote the review text.\nNote that although both the user embedding and the seg-\nment embedding are identically added to all tokens, their\nimportance are different. In our task, the segment embeddings\nare all the same across all tokens in a document and also\nacross all documents over the whole dataset. So they provide\nno useful information in our task. However, user embeddings\ndiffer from document to document, since the documents are\nwritten by different users (there are also ones written by the\nsame user though). This difference helps the PLMs encode\ntoken correlations in a personalized way, and output more\naccurate representations.\nD. ENCODER MODULE\nIn this module, BERT uses the structure of transformer\nencoder, which is composed of L identical layers of encoder\nblocks, as shown in Fig. 1. Each encoder block has two parts:\na multi-head self-attention mechanism and a fully connected\nfeed-forward network. Each part is surrounded by a residual\nconnection [35] and followed by a layer normalization [36]\ncomponent.\nEach block accepts a sequence as input, update representa-\ntions with these two parts, and outputs the sequence with the\nsame shape, which is passed into the next block as input. The\ninput of the ﬁrst block is the output of the embedding module.\nThat is:\nHl =Encoder(l)(Hl−1), l ∈[1, L]\nH0 =E (5)\nwhere Hl ∈ RN×d , N is sequence length and d is the\ndimension of the hidden states.\n1) MULTI-HEAD SELF-ATTENTION\nThe multi-head self-attention module is applied to pass con-\ntext information between tokens and update token represen-\ntations iteratively.\nFormally, considering the self-attention module in layer l,\nthis process can be explained in the following steps:\n(1) Apply three MLP layers to all tokens in Hl−1 and\nreshape them to get query Ql,a, key Kl,a and value Vl,a\nfor each attention head a. All of them are in the shape of\nRN×A×da , where N is sequence length, A is the number of\nheads and da is the dimension per head.\n(2) Calculate similarity between all token pairs by calcu-\nlating the dot product of Q and K:\nSl,a =softmax(\nQl,aKT\nl,a\n√da\n) (6)\nwhere Sl,a ∈RN×N represents the similarity matrix which is\nnormalized along the dimension of key. Each element S(i, j)\nrepresents the importance of the jth token (among all tokens)\nto the ith token.\n(3) Using the similarity matrix as attention weights, calcu-\nlate the weighted sum of the whole value sequence for each\nquery token separately:\nOl,a =Sl,aVl,a (7)\nand outputs Ol,a ∈RN×A×da .\n(4) Finally, for each token x, aggregate the updated repre-\nsentation of all heads together:\nO(x)\nl =W T\nl Concat({O(x)\nl,1, . . . ,O(x)\nl,A}) +bl (8)\nwhere Wl ∈ R(A×da)×d , O(x)\nl ∈ Rd . Output of the whole\nsequence is Ol ∈RN×d .\n2) FEED FORWARD NETWORK\nThe part of feed forward network is designed to further\nincrease the model capacity. It is composed of two linear\ntransformations with a activation function between them:\nFFNl (x) =W T\nl,2f (W T\nl,1x +bl,1) +bl,2 (9)\nwhere x ∈Rd , Wl,1 ∈Rd×4d and Wl,2 ∈R4d×d . f is the\nactivation function, i.e. GeLU [37] in BERT. The function in\n(9) is applied to O(x)\nl for all tokens identically.\n3) USER INJECTION\nWe now propose to inject user information into the encoder\nmodule, or its ‘‘multi-head self-attention’’ part to be more\nspeciﬁc. This design is inspired by the traditional methods\nfor personalized sentiment classiﬁcation. These methods are\nmostly based on hierarchical RNN/CNNs with word-level\nand sentence-level attention functions. To inject additional\ncontext information, They incorporate user embeddings and\nitem embeddings as global biases in these attention functions.\nInterestingly, we found that there is also a similar pattern\nin BERT: since the [CLS] token is designed to represent the\nwhole document, the attention with [CLS] as query and all\nVOLUME 10, 2022 30161\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nTABLE 1. Dataset statistics.\ntokens as key aims at gathering information of important\ntokens in the document.\nBased on this observation, we propose to add a user vector\nas bias to the query vector of [CLS]. Then the similarity\ncalculation in self-attention for head a in layer l, i.e. the\nequation as shown in (6), is now formulated as:\nSl,a =softmax(\n(Ql,a +Ul,a)KT\nl,a√da\n) (10)\nwhere both Ql,a and Ul,a are in the shape of RN×A×da . For\nUl,a ∈RN×A×da , we only initialize the part which is added\nto the query of [CLS] (i.e. Ul,a[0, :, :]) with random values,\nand ﬁll all others with 0.\nE. OUTPUT MODULE\nAfter the stacked encoder blocks, BERT outputs a sequence\nof hidden states HL which is then used for speciﬁc tasks. The\ntask of sentiment classiﬁcation is deﬁned as a sentence-level\nclassiﬁcation problem, which uses the output of the [CLS]\ntoken for prediction.\nSpeciﬁcally, it passes the output of [CLS] through a MLP\nlayer and get h ∈Rd to represent the whole text, and then\nuses a classiﬁer with softmax for classiﬁcation:\np =softmax(W T h +b) (11)\nwhere p ∈R|C|is the vector of class probabilities, in which\n|C|is number of classes. W ∈R|C|×d and b ∈R|C|.\nF. TWO-STAGE TRAINING PROCEDURE\nAfter we load the pre-trained BERT, we train it in two stages\nsuccessively:\n1) In the ﬁrst stage, we pre-train the model with the\nMasked LM (MLM) task in the same way as [8], using\nour train set. This is intended for our framework to learn\ndoma in-speciﬁc knowledge in the dataset.\n2) In this stage, we introduce user-speciﬁc parameters by\nfollowing one of the two personalization strategies and\nﬁne-tune the whole model for sentiment classiﬁcation\nwith cross-entropy loss.\nNote that in our training procedure, user identity is only\nused in the second stage because we expect our framework\nto ﬁrstly learn general knowledge of the words in the domain\nin the ﬁrst stage, and then ‘‘ﬁne-tune’’ their representations\nwith user-speciﬁc parameters for the task together.\nIV. EXPERIMENTS\nWe conduct various experiments to evaluate our proposed\nU-PLMs in this section.\nA. EXPERIMENTS SETTINGS\nFollowing prior works, we conduct experiments on three\nsentiment classiﬁcation datasets 1 which consist of user and\nitem IDs apart from review texts. These datasets are collected\nfrom IMDB and Yelp websites and split into train set, dev\nset, and test set with a ratio of 8:1:1. Statistical details of the\ndatasets are given in Table 1. We use Accuracy on dev set to\nselect the best model, and use Accuracy and RMSE on test set\nfor evaluation.\nFor experiments of vanilla BERT and U-PLMs, We pad or\nclip the text to be with a max length of 500. Following [38],\nwe load the BERT model from BERT-base which contains\nL =12 encoder blocks. We run the in-domain pre-training\nwith a learning rate of 5e-5 for 100,000 steps. As for ﬁne-\ntuning, we set the learning rate to 2e-5 and the batch size to\n30 to fully leverage the GPU memory. We empirically set\nthe max epoch number to 3. We optimize our model with\nAdamW [39] and use slanted triangular learning rates[40]\nwith a warmup ratio of 0.1 for both in-domain pre-training\nand ﬁne-tuning.\nAs for the user-speciﬁc parameters in U-PLMs, the user\nembedding in embedding-based personalization is initialized\nwith a normal distribution N(0, 0.0052) for better perfor-\nmance. The ones in self-attention modules are initialized with\nN(0, 0.022) and we add a layer normalization [36] component\nin each module so that the user bias falls into a similar range\nwith the query representation of [CLS].\nTo show the robustness of our framework, we also use\nRoBERTa (loaded from roberta-base) as our backbone model,\nusing the same hyper-parameters with BERT. We run our\nmodels 10 times and report the average results. All experi-\nments are conducted on an RTX 3090 GPU.\nB. BASELINES\nWe compare our methods with the following baseline\nmethods:\n• NSC [14] uses hierarchical LSTM and attention mecha-\nnism to encode the review text.\n• BERT [8] corresponds to the vanilla BERT model. It is\ntrained with the same setting as our method except that\nno user information is used.\n• RoBERTa [9] denotes the vanilla RoBERTa model.\n• NSC +UPA [14] is the enhanced NSC model that\nincorporates user and item identities into the attention\nmechanism.\n1http://ir.hit.edu.cn/~dytang/paper/acl2015/\ndataset.7z\n30162 VOLUME 10, 2022\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nTABLE 2. Results of our method and baselines on test sets. Acc. (higher is better) and RMSE (lower is better) are used as evaluation metrics. The best\nperformances are inbold and the second best are underlined.\n• HUAPA [15] uses two separate networks with the same\nstructure to model the text from the view of user and item\nrespectively and combine them for ﬁnal prediction.\n• CHIM [13] studies how and where to incorporate\nuser and item information in a sentiment classiﬁcation\nmodel.\n• RRP-UPM [16] is based on HUAPA and considers the\ninherent correlation between users or items for better\nrepresentation.\n• IUPC [17] is the ﬁrst method to apply BERT into person-\nalized sentiment classiﬁcation. It uses historical reviews\nto represent user and item and combines these two rep-\nresentations with the target review for better prediction.\nSince the embedding-based and attention-based person-\nalization are independent of each other, we employ them\nseparately in our model when comparing with baselines to\nstudy their effects respectively. The combination of them is\ndiscussed in section IV-D.\nC. MODEL COMPARISONS\nWe implement and train BERT, RoBERTa, and our frame-\nwork, respectively, while using the results of other baselines\nreported in their papers.\nThe results are listed in Table. 2. The methods are divided\ninto three different groups according to additional informa-\ntion they use apart from the text: (1) models considering no\nuser or item identity; (2) models considering both user and\nitem identities; and (3) models considering user identities\nonly.\nFrom these results, we can make the following\nobservations:\nFirstly, vanilla BERT and RoBERTa perform much bet-\nter than NSC. This proves the effectiveness of PLMs for\ndocument-level sentiment classiﬁcation.\nSecondly, traditional models are improved after using user\nand item identities. For example, NSC +UPA gains great\nimprovements over NSC. RRP-UPM achieves competitive\nresults with BERT on accuracy. These results show that addi-\ntional context information truly brings help to the task.\nThirdly, IUPC outperforms other baselines on most met-\nrics. This shows that it’s feasible and worthwhile to combine\nPLMs and context information for better performance.\nFinally, our methods outperform all baselines including\nIUPC on all metrics. This proves that our ways of injecting\nuser identity help both text modeling and ﬁnal prediction.\nIt’s worth mentioning that we’ve also tried to incorpo-\nrate item identity as additional information. Unfortunately,\nwe didn’t ﬁnd an obvious improvement in performance,\nwhich is consistent with the observation of [11] that user\ninformation is much more effective than item information.\nD. ABLATION STUDY\nWe conduct an ablation study to evaluate the effect of\neach component. Speciﬁcally, we use vanilla BERT and\nRoBERTa as base models and then add different combina-\ntions of three components, i.e. the in-domain pre-training\nstage, embedding-based personalization, and attention-based\npersonalization, to construct different alternatives of our\nframework. Results are shown in Table. 3.\nFrom the results, we can observe the positive effects of\nthe in-domain pre-training and both of our user-related com-\nponents. Firstly, by training the model on our task-speciﬁc\ntraining data with MLM before ﬁne-tuning, the model is\nproved to ﬁt in with the dataset better. Based on this, a remark-\nable improvement is further brought by the injection of user\ninformation. Both the embedding-based and attention-based\npersonalization improve the performance, and among them,\nthe embedding-based one is slightly better. Finally, since\nthe two strategies are independent of each other, we’ve also\ntried to combine them together in a single model. However,\nno further improvement is achieved.\nE. ANALYSIS FOR EMBEDDING-BASED PERSONALIZATION\nAs a part of the summation in the embedding module (as\nshown in (4)), the value range of the user embeddings matters\nVOLUME 10, 2022 30163\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nTABLE 3. Ablation study for three components. PT means the training stage of in-domain pre-training. Emb-U means embedding-based personalization.\nAtt-U means attention-based personalization.\nFIGURE 2. Impact of the std of initialized user embeddings on IMDB and Yelp13 datasets with RoBERTa as backbone.\na lot. If it’s too large compared to other embedding values,\nit might be dominant in the token representation and therefore\neliminate the difference between tokens in a text. On the\nother hand, if it’s too small, the injected user information\nmay fail to affect the token representation. Therefore, it’s\nnecessary to choose an appropriate initialization range for the\nuser embeddings.\n1) IMPACT OF USER EMBEDDING INITIALIZATION\nThe original embeddings (word, pos, segment) in BERT\nand RoBERTa are initialized with a normal distribution\nN(0, 0.022). Accordingly, we initialize the user embeddings\nwith N(0, std2), where the standard deviation std is chosen\nfrom [0.001, 0.005, 0.02, 0.05, 0.1].\nThe results on IMDB and Yelp13 with RoBERTa as back-\nbone are shown in Fig. 2. Although our model outperforms\nthe vanilla PLM consistently under different initializations,\nit is observed that a std which is the same as or slightly smaller\nthan the original embeddings (i.e. 0.02) is appropriate. When\nstd is larger than 0.02, the performance of our method signif-\nicantly drops. We can also ﬁnd a trend of drop when std turns\nfrom 0.005 into 0.001.\nF. ANALYSIS FOR ATTENTION-BASED PERSONALIZATION\nIn the proposed U-PLMs (Att), we inject user vectors\ninto the self-attention modules of all encoder blocks,\nserving as biases to queries of [CLS] tokens. In this\nsection, we make further analysis and explore the differ-\nent choices of (1) encoder layers and (2) tokens to be\nbiased.\n1) IMPACT OF PESONALIZATION LAYERS\nInstead of injecting user vectors into all encoder layers,\nwe only choose some of them for injection in this experiment.\nSpeciﬁcally, we divide all 12 layers into four parts, each\nconsisting of three consecutive layers. Then we inject user\nvectors into only one part to affect this part directly and\nlater parts indirectly. For example, if user vectors are injected\ninto layers 7-9, the text is ﬁrstly modeled the same as in\nvanilla PLMs in layers 1-6. Then the queries of [CLS] in\nself-attention modules of layers 7-9 are biased with the user.\nFinally, user information is passed to latter layers implicitly\nin the biased [CLS] representations.\nResults are shown in Table. 4. Overall, comparisons in\nthree datasets show that injecting user vectors into all encoder\nlayers is better than all the alternatives. This is because the\nintroduced user parameters differ from layer to layer. Injec-\ntion into all layers enables the PLMs to encode text more\nﬂexibly in each layer.\nTABLE 4. Impact of encoder layers to get biased. U-Layers means layers\ninto which user vector are injected.\n30164 VOLUME 10, 2022\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nTABLE 5. Impact of tokens to get biased. U-Tokens means tokens whose\nqueries are biased by user vectors.\nHowever, some of the alternatives such as ‘‘10-12’’ can\nachieve similar results with ‘‘All’’. This means that injec-\ntion into three layers is enough for the exploitation of user\ninformation. There is also a trend that the model tends to per-\nform better when user vectors are injected into latter layers,\nespecially for the IMDB dataset which is more complicated.\nThese observations can help us with the improvement of\nattention-based personalization in future work.\n2) IMPACT OF PESONALIZATION TOKENS\nIn our proposed strategy, we add user vectors to the query\nof only the [CLS] token. The intuition behind this design\nis that the [CLS] token is used to represent the whole doc-\nument in BERT or RoBERTa, and adding bias to its query\nin self-attention has a similar pattern with the personalized\nattention mechanism in traditional methods.\nAs an alternative, we try to explore this strategy further by\nadding user vectors to all tokens, instead of the [CLS] token\nonly. Results, as shown in Table. 5, show that the performance\nof our model drops when we add biases to not only [CLS] but\nalso all other tokens. The reason might be the different roles\nof [CLS] and normal tokens as queries in self-attention. The\n[CLS] token has no actual meaning itself and adding bias to its\nquery is to help select important tokens within the document\nfor the user. However, relations between normal tokens are\nnot quite different between users. Therefore, injection into\nthese tokens is not necessary. However, this alternative still\noutperforms the vanilla model, which proves the importance\nof user information.\nG. ARE ALL USERS FULLY TRAINED?\nTo study whether users with different numbers of training\nsamples are all well-trained, we compare our framework\nincluding U-PLMs (Emb) and U-PLMs (Att) with vanilla\nPLMs for different groups of users.\nWe ﬁrst calculate the metrics for each user based on his/her\ncorresponding test samples. Then we divide all users into sev-\neral groups according to their numbers of training samples.\nThe metric for a group is obtained by averaging over metrics\nof all users in this group, and we conduct the comparison\nbetween three models for each group separately.\nTo save space, we only exhibit the results using RoBERTa\nas the backbone model in this experiment, as shown in\nTable6. We can see that both of our methods obtain improve-\nments consistently for users of all groups. This means that\nour framework, both embedding-based and attention-based,\nworks well for all users with many or a few training samples.\nH. COMPARISON BETWEEN EMB-BASED AND\nATT-BASED PERSONALIZATION\nIn this section, we look back to the experiment results again\nand make further observations on the difference between our\ntwo methods, i.e. U-PLMs (Emb) and U-PLMs (Att).\nIt can be observed from Table 2 that U-PLMs (Emb) per-\nforms slightly better than U-PLMs (Att) for both backbone\nmodels, except for the IMDB dataset.\nWe think the reason might be the complexity of user-\nspeciﬁc parameters, the difﬁculty of the task, the amount of\nintroduced user information, and their relationships. Specif-\nically, the only additional parameter introduced in U-PLMs\n(Emb) is the user embedding matrix in the embedding mod-\nule. On the other hand, U-PLMs (Att) introduces a new\nembedding matrix for each encoder block, which is much\nmore complicated. Since we only use user ID as additional\ncontext information for user privacy, the embedding-based\nstrategy might be enough to model this information.\nHowever, on the IMDB dataset which consists of more\nsentiment classes and much longer sentences than the Yelp\ndatasets, U-PLMs (Att) achieves similar or even better per-\nformance than U-PLMs (Emb). This might be because the\ntask is more difﬁcult, and to model the user information with\nthe complicated version of U-PLMs is more appropriate.\nSimilarly, results in Table 6 also support our judgement.\nOn all three datasets and both metrics, U-PLMs (Att) per-\nforms worse than U-PLMs (Emb) for users with under\n20 training samples. However, with the training samples per\nuser increasing, the gap gradually decreases. U-PLMs (Att)\ncan even perform better in some of the results.\nTherefore, we can safely draw a conclusion that our pro-\nposed embedding-based personalization is suitable for simple\nscenarios while the attention-based one is better for compli-\ncated cases. Furthermore, the attention-based personalization\nis more ﬂexible, with many alternatives such as ones in\nsection IV-F1 and IV-F2 can be explored in future work.\nI. RESEARCH IMPLICATIONS\nWe focus on the study of the combination of pre-trained\nlanguage models and user identity information for document-\nlevel sentiment classiﬁcation, which is shown to be effective\nand worth further discovering in our experiments.\nFirstly, both personalized data and PLMs are available in\npractice, especially for corporations. In addition, our experi-\nment results show that both of these are quite effective for the\ntask. However, few studies have been done to combined their\nadvantages and further improve the performance.\nIn our research, we deeply study the ways of combining\nPLMs and user data. Unlike existing works, we introduce user\ndata into not only the prediction module but also the proce-\ndure of text modeling. Experiment results show that these two\ninformation can indeed be exploited jointly. We believe that\nVOLUME 10, 2022 30165\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\nTABLE 6. User-group-wise results using RoBERTa as the backbone.\nthere are many better ways which deserve discovering in this\nﬁeld.\nV. CONCLUSION\nIn this paper, we propose two attempts to inject user identity\ninto PLMs to build U-PLMs, i.e. user-enhanced pre-trained\nlanguage models, for personalized sentiment analysis. Exper-\nimental results show that both of our two methods outperform\nvanilla pre-trained models and state-of-the-art models for\npersonalized sentiment classiﬁcation greatly. These obser-\nvations further indicates that pre-trained language models\nand personalized data can be exploited jointly for better\nperformance in the task of document-level sentiment clas-\nsiﬁcation. Furthermore, we found that the embedding-based\npersonalization is enough to model the user id which contains\nnot much information, while the attention-based strategy is\nsuitable for more complicated situations. In the future work,\nwe will try to make improvements based on our two strategies\nand explore other better ways of injecting user identity and\nother context information of the text into PLMs.\nVI. ETHIC CONSIDERATIONS\nIn this paper, we use a unique ID, as the only information to\nrepresent a user. The ID is in the form of a meaningless string\n(e.g. ‘‘U0001’’), and contains no real information (gender,\nrace, etc.) about users.\nTo ensure acceptable privacy practice, all the datasets we\nuse in this paper are publicly available, and we use them in a\npurely observational and non-intrusive manner.\nREFERENCES\n[1] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. A. Chenaghlu, and\nJ. Gao, ‘‘Deep learning–based text classiﬁcation,’’ ACM Comput. Surv.,\nvol. 54, no. 3, pp. 1–40, Apr. 2021.\n[2] Y . Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’ in\nProc. EMNLP, Doha, Qatar, 2014, pp. 1746–1751.\n[3] D. Tang, B. Qin, and T. Liu, ‘‘Document modeling with gated recurrent\nneural network for sentiment classiﬁcation,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., Lisbon, Portugal, 2015, pp. 1422–1432.\n[4] Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, ‘‘Hierar-\nchical attention networks for document classiﬁcation,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol.,\nSan Diego, CA, USA, 2016, pp. 1480–1489.\n[5] J. Bhatta, D. Shrestha, S. Nepal, S. Pandey, and S. Koirala, ‘‘Efﬁcient\nestimation of nepali word representations in vector space,’’ J. Innov. Eng.\nEduc., vol. 3, no. 1, pp. 71–77, Mar. 2020.\n[6] J. Pennington, R. Socher, and C. Manning, ‘‘GloVe: Global vec-\ntors for word representation,’’ in Proc. EMNLP, Doha, Qatar, 2014,\npp. 1532–1543.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. NIPS,\nLong Beach, CA, USA, 2017, pp. 5998–6008.\n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ in Proc.\nNAACL, Minneapolis, MN, USA, 2019, pp. 4171–4186.\n[9] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized BERT\npretraining approach,’’ 2019, arXiv:1907.11692.\n[10] P.-H. Chi, P.-H. Chung, T.-H. Wu, C.-C. Hsieh, Y .-H. Chen, S.-W. Li, and\nH.-Y . Lee, ‘‘Audio albert: A lite bert for self-supervised learning of audio\nrepresentation,’’ in Proc. IEEE Spoken Lang. Technol. Workshop (SLT),\nAddis Ababa, Ethiopia, Jan. 2021, pp. 344–350.\n[11] D. Tang, B. Qin, and T. Liu, ‘‘Learning semantic representations of users\nand products for document level sentiment classiﬁcation,’’ in Proc. 53rd\nAnnu. Meeting Assoc. Comput. Linguistics 7th Int. Joint Conf. Natural\nLang. Process., Beijing, China, 2015, pp. 1014–1023.\n[12] R. K. Amplayo, J. Kim, S. Sung, and S.-W. Hwang, ‘‘Cold-start aware\nuser and product attention for sentiment classiﬁcation,’’ in Proc. ACL,\nMelbourne, VIC, Australia, 2018, pp. 2535–2544.\n[13] R. K. Amplayo, ‘‘Rethinking attribute representation and injection for\nsentiment classiﬁcation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. 9th Int. Joint Conf. Natural Lang. Process. (EMNLP-IJCNLP),\nHong Kong, 2019, pp. 5602–5613.\n[14] H. Chen, M. Sun, C. Tu, Y . Lin, and Z. Liu, ‘‘Neural sentiment classiﬁca-\ntion with user and product attention,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., Austin,TX, USA, 2016, pp. 1650–1659.\n[15] Z. Wu, X. Dai, C. Yin, S. Huang, and J. Chen, ‘‘Improving review\nrepresentations with user attention and product attention for sentiment\nclassiﬁcation,’’ in Proc. AAAI Conf. Artif. Intell., New Orleans, LA, USA,\nvol. 2018, pp. 5989–5996.\n[16] Z. Yuan, F. Wu, J. Liu, C. Wu, Y . Huang, and X. Xie, ‘‘Neural review rating\nprediction with user and product memory,’’ in Proc. CIKM, Beijing, China,\n2019, pp. 2341–2344.\n[17] C. Lyu, J. Foster, and Y . Graham, ‘‘Improving document-level sentiment\nanalysis with user and product context,’’ in Proc. COLING, Barcelona,\nSpain, 2020, pp. 6724–6729.\n[18] J. Xu, D. Chen, X. Qiu, and X. Huang, ‘‘Cached long short-term memory\nneural networks for document-level sentiment classiﬁcation,’’ in Proc.\nEMNLP, Austin, TX, USA, 2016, pp. 1660–1669.\n[19] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997.\n[20] D. Tang, F. Wei, B. Qin, N. Yang, T. Liu, and M. Zhou, ‘‘Sentiment\nembeddings with applications to sentiment analysis,’’ IEEE Trans. Knowl.\nData Eng., vol. 28, no. 2, pp. 496–509, Feb. 2016.\n[21] P. Fu, Z. Lin, F. Yuan, W. Wang, and D. Meng, ‘‘Learning sentiment-\nspeciﬁc word embedding via global sentiment representation,’’ in Proc.\nAAAI Conf. Artif. Intell., 2018, pp. 4808–4815.\n30166 VOLUME 10, 2022\nX. Caoet al.: Injecting User Identity Into Pretrained Language Models for Document-Level Sentiment Classification\n[22] L.-C. Yu, J. Wang, K. R. Lai, and X. Zhang, ‘‘Reﬁning word embeddings\nusing intensity scores for sentiment analysis,’’ IEEE/ACM Trans. Audio,\nSpeech, Lang. Process., vol. 26, no. 3, pp. 671–681, Mar. 2018.\n[23] L. Zhang, S. Wang, and B. Liu, ‘‘Deep learning for sentiment analysis : A\nsurvey,’’ 2018,arXiv:1801.07883.\n[24] J. Weston, S. Chopra, and A. Bordes, ‘‘Memory networks,’’ 2014,\narXiv:1410.3916.\n[25] W. Zhang, Q. Yuan, J. Han, and J. Wang, ‘‘Collaborative multi-level\nembedding learning from reviews for rating prediction,’’ in Proc. IJCAI,\nNew York, NY , USA, 2016, pp. 2986–2992.\n[26] W. Zhang and J. Wang, ‘‘Integrating topic and latent factors for scalable\npersonalized review-based rating prediction,’’ IEEE Trans. Knowl. Data\nEng., vol. 28, no. 11, pp. 3013–3027, Nov. 2016.\n[27] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, ‘‘Pre-trained\nmodels for natural language processing: A survey,’’ Sci. China Technol.\nSci., vol. 63, no. 10, pp. 1872–1897, 2020.\n[28] A. Radford and K. Narasimhan. (2018). Improving Language\nUnderstanding by Generative Pre-Training. [Online]. Available: https://s3-\nus-west-2.amazonaws.com/openai-assets/research-covers/langua% ge-\nunsupervised/language_understanding_paper.pdf\n[29] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.\n(2019). Language Models are Unsupervised Multitask Learners. [Online].\nAvailable: https://d4mucfpksywv.cloudfront.net/better-language-\nmodels/language_mod% els_are_unsupervised_multitask_learners.pdf\n[30] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, and S. Agarwal, ‘‘Lan-\nguage models are few-shot learners,’’ 2020, arXiv:2005.14165.\n[31] C. Finn, P. Abbeel, and S. Levine, ‘‘Model-agnostic meta-learning for fast\nadaptation of deep networks,’’ 2017, arXiv:1703.03400.\n[32] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, ‘‘Span-\nBERT: Improving pre-training by representing and predicting spans,’’\n2019, arXiv:1907.10529.\n[33] Y . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu,\nH. Tian, and H. Wu, ‘‘ERNIE: Enhanced representation through knowledge\nintegration,’’ 2019, arXiv:1904.09223.\n[34] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi, W. Macherey,\nM. Krikun, Y . Cao, Q. Gao, K. Macherey, and J. Klingner, ‘‘Google’s\nneural machine translation system: Bridging the gap between human and\nmachine translation,’’ 2016, arXiv:1609.08144.\n[35] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. CVPR, Las Vegas, NV , USA, 2016, pp. 770–778.\n[36] J. Lei Ba, J. Ryan Kiros, and G. E. Hinton, ‘‘Layer normalization,’’ 2016,\narXiv:1607.06450.\n[37] D. Hendrycks and K. Gimpel, ‘‘Gaussian error linear units (GELUs),’’\n2016, arXiv:1606.08415.\n[38] C. Sun, X. Qiu, Y . Xu, and X. Huang, ‘‘How to ﬁne-tune BERT for text\nclassiﬁcation?’’ 2019, arXiv:1905.05583.\n[39] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’\n2017, arXiv:1711.05101.\n[40] J. Howard and S. Ruder, ‘‘Universal language model ﬁne-tuning for\ntext classiﬁcation,’’ in Proc. ACL, Melbourne, VIC, Australia, 2018,\npp. 328–339.\nXINLEI CAOreceived the B.S. degree in software\nengineering from East China Normal University,\nChina, in 2019, where he is currently pursuing the\nmaster’s degree with the Department of Computer\nScience and Technology. His research interests\ninclude natural language processing, pre-trained\nlanguage models, and information retrieval.\nJINYANG YU is a Ph.D. in Medicine. He is\nengaged in post marketing drug monitoring and\nevaluation. He has presided over or been respon-\nsible for a number of post marketing drug research\nprojects. Currently, he is mainly responsible for\nthe medical devices monitoring and information\nconstruction.\nYAN ZHUANG received the Ph.D. degree from\nTsinghua University, China, in 2019. He is a\nSenior Engineer with the Medical Big Data\nResearch Center, Chinese People’s Liberation\nArmy (PLA) General Hospital, engaged in hospi-\ntal informatization construction and research for\nmore than ten years, specializing in medical big\ndata and artiﬁcial intelligence related technolo-\ngies, and doing research in database and data\nwarehouse, human–machine computing, knowl-\nedge graph, and graph neural networks. He has published more than\n90 papers in the ﬁeld of medical big data and won the Best Paper Award\nat the 2017 ACM International Conference on Information and Knowledge\nManagement (CIKM2017).\nVOLUME 10, 2022 30167",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8525055050849915
    },
    {
      "name": "Language model",
      "score": 0.7812753915786743
    },
    {
      "name": "Identity (music)",
      "score": 0.7078616619110107
    },
    {
      "name": "Personalization",
      "score": 0.6994693279266357
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5476570725440979
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5447233319282532
    },
    {
      "name": "User modeling",
      "score": 0.5309808850288391
    },
    {
      "name": "Natural language processing",
      "score": 0.5029072165489197
    },
    {
      "name": "ENCODE",
      "score": 0.45155346393585205
    },
    {
      "name": "Baseline (sea)",
      "score": 0.44794678688049316
    },
    {
      "name": "Plug-in",
      "score": 0.41248393058776855
    },
    {
      "name": "Information retrieval",
      "score": 0.34612053632736206
    },
    {
      "name": "World Wide Web",
      "score": 0.18702027201652527
    },
    {
      "name": "User interface",
      "score": 0.16471055150032043
    },
    {
      "name": "Programming language",
      "score": 0.10026630759239197
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210132195",
      "name": "Guangdong Province Environmental Monitoring Center",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2802939634",
      "name": "Chinese PLA General Hospital",
      "country": "CN"
    }
  ],
  "cited_by": 10
}