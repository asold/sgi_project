{
  "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
  "url": "https://openalex.org/W3154311556",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2949023737",
      "name": "Abteen Ebrahimi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2890718399",
      "name": "Manuel Mager",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2884085343",
      "name": "Arturo Oncevay",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2913883794",
      "name": "Vishrav Chaudhary",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2718195782",
      "name": "Luis Chiruzzo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171998422",
      "name": "Angela Fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102821512",
      "name": "John Ortega",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135013761",
      "name": "Ricardo Ramos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141766749",
      "name": "Annette Rios",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269452175",
      "name": "Iván Vladimir Meza Ruíz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2575838670",
      "name": "Gustavo Giménez Lugo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2809846681",
      "name": "Elisabeth Mager",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A277131583",
      "name": "Graham Neubig",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2303276807",
      "name": "Alexis Palmer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295737656",
      "name": "Rolando Coto‐Solano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269955182",
      "name": "Thang Vu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2413443321",
      "name": "Katharina Kann",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2762484717",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3164471170",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2775632580",
    "https://openalex.org/W2913659301",
    "https://openalex.org/W3031427604",
    "https://openalex.org/W4245234602",
    "https://openalex.org/W643936199",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3214161538",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2765961751",
    "https://openalex.org/W3093721400",
    "https://openalex.org/W3087914239",
    "https://openalex.org/W2971120622",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W592849009",
    "https://openalex.org/W2913077545",
    "https://openalex.org/W634928704",
    "https://openalex.org/W4239309852",
    "https://openalex.org/W2270364989",
    "https://openalex.org/W4298393544",
    "https://openalex.org/W2775848483",
    "https://openalex.org/W2250342921",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W3100501376",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W2949303037",
    "https://openalex.org/W2807710978",
    "https://openalex.org/W2252061078",
    "https://openalex.org/W2330568878",
    "https://openalex.org/W650287694",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2325218811",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W200584326",
    "https://openalex.org/W2094291475",
    "https://openalex.org/W3116178498",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2606271679",
    "https://openalex.org/W2970457158",
    "https://openalex.org/W2971418718",
    "https://openalex.org/W3087094142",
    "https://openalex.org/W2574596763",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2121165003",
    "https://openalex.org/W31698366",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4297801177",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2488216425",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W833962122",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2095560109",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W4299579390",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W2246637189",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W4383551279",
    "https://openalex.org/W2251356834",
    "https://openalex.org/W2963091326",
    "https://openalex.org/W3098466758"
  ],
  "abstract": "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, Vishrav Chaudhary, Luis Chiruzzo, Angela Fan, John Ortega, Ricardo Ramos, Annette Rios, Ivan Vladimir Meza Ruiz, Gustavo Giménez-Lugo, Elisabeth Mager, Graham Neubig, Alexis Palmer, Rolando Coto-Solano, Thang Vu, Katharina Kann. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6279 - 6299\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAmericasNLI: Evaluating Zero-shot Natural Language Understanding\nof Pretrained Multilingual Models in Truly Low-resource Languages\nAbteen Ebrahimi♦ Manuel Mager♠ Arturo Oncevay♥ Vishrav Chaudhary§\nLuis Chiruzzo△ Angela Fan∇ John E. OrtegaΩ Ricardo Ramosη Annette Riosψ\nIvan Meza-Ruiz♯ Gustavo A. Giménez-Lugo♣ Elisabeth Mager♯ Graham Neubig⊿◁\nAlexis Palmer♦ Rolando Coto-Solano℧ Ngoc Thang Vu♠ Katharina Kann♦\n⊿◁Carnegie Mellon University ℧Dartmouth College §Microsoft Turing\n∇Facebook AI Research ΩNew York University △Universidad de la República, Uruguay\nηUniversidad Tecnológica de Tlaxcala ♯Universidad Nacional Autónoma de México\n♣Universidade Tecnológica Federal do Paraná ♦University of Colorado Boulder\n♥University of Edinburgh ♠University of Stuttgart ψUniversity of Zurich\nAbstract\nPretrained multilingual models are able to per-\nform cross-lingual transfer in a zero-shot set-\nting, even for languages unseen during pre-\ntraining. However, prior work evaluating per-\nformance on unseen languages has largely\nbeen limited to low-level, syntactic tasks, and\nit remains unclear if zero-shot learning of\nhigh-level, semantic tasks is possible for un-\nseen languages. To explore this question, we\npresent AmericasNLI, an extension of XNLI\n(Conneau et al., 2018) to 10 Indigenous lan-\nguages of the Americas. We conduct experi-\nments with XLM-R, testing multiple zero-shot\nand translation-based approaches. Addition-\nally, we explore model adaptation via contin-\nued pretraining and provide an analysis of the\ndataset by considering hypothesis-only mod-\nels. We ﬁnd that XLM-R’s zero-shot perfor-\nmance is poor for all 10 languages, with an av-\nerage performance of 38.48%. Continued pre-\ntraining offers improvements, with an average\naccuracy of 43.85%. Surprisingly, training on\npoorly translated data by far outperforms all\nother methods with an accuracy of 49.12%.\n1 Introduction\nPretrained multilingual models such as XLM\n(Lample and Conneau, 2019), multilingual BERT\n(mBERT; Devlin et al., 2019), and XLM-R (Con-\nneau et al., 2020) achieve strong cross-lingual trans-\nfer results for many languages and natural language\nprocessing (NLP) tasks. However, there exists a\ndiscrepancy in terms of zero-shot performance be-\ntween languages present in the pretraining data and\nthose that are not: performance is generally highest\nfor well-represented languages and decreases with\nless representation. Yet, even for unseen languages,\nperformance is generally above chance, and model\nadaptation approaches have been shown to yield\nLanguage ISO Family Dev Test\nAymara aym Aymaran 743 750\nAsháninka cni Arawak 658 750\nBribri bzd Chibchan 743 750\nGuaraní gn Tupi-Guaraní 743 750\nNahuatl nah Uto-Aztecan 376 738\nOtomí oto Oto-Manguean 222 748\nQuechua quy Quechuan 743 750\nRarámuri tar Uto-Aztecan 743 750\nShipibo-Konibo shp Panoan 743 750\nWixarika hch Uto-Aztecan 743 750\nTable 1: The languages in AmericasNLI, along with\ntheir ISO codes, language families, and dataset sizes.\nfurther improvements (Muller et al., 2020; Pfeiffer\net al., 2020a,b; Wang et al., 2020).\nImportantly, however, there are currently no\ndatasets for high-level, semantic tasks which fo-\ncus solely on low-resource languages. As these\nlanguages are most likely to be unseen to com-\nmonly used pretrained models, practically all work\nevaluating unseen language performance and lan-\nguage adaptation methods has been limited to low-\nlevel, syntactic tasks such as part-of-speech tag-\nging, dependency parsing, and named-entity recog-\nnition (Muller et al., 2020; Wang et al., 2020). This\nlargely limits our ability to draw more general con-\nclusions with regards to the zero-shot learning abil-\nities of pretrained multilingual models for unseen\nlanguages.\nIn this work, we introduce AmericasNLI, an ex-\ntension of XNLI (Conneau et al., 2018) – a natural\nlanguage inference (NLI; cf. §2.3) dataset cover-\ning 15 high-resource languages – to 10 Indigenous\nlanguages spoken in the Americas: Asháninka, Ay-\nmara, Bribri, Guaraní, Nahuatl, Otomí, Quechua,\nRarámuri, Shipibo-Konibo, and Wixarika. All of\nthem are truly low-resource languages: they have\nlittle to no digitally available labeled or unlabeled\n6279\ndata, and they are not typically studied by the main-\nstream NLP community. The goal of this work\nis two-fold: First, we hope to increase the visibil-\nity of these languages by providing a portion of\nthe resources necessary for NLP research. Sec-\nond, we aim to allow for a more comprehensive\nstudy of multilingual model performance on un-\nseen languages, where improvements will help ex-\ntend the reach of NLP techniques to a larger set\nof languages. We are speciﬁcally interested in the\nfollowing research questions: (1) Do pretrained\nmultilingual models still perform above random\nchance for a high-level, semantic task in an un-\nseen language? (2) Do methods aimed at adapting\nmodels to unseen languages – previously exclu-\nsively evaluated on low-level, syntactic tasks – also\nincrease performance on NLI? (3) Are translation-\nbased approaches effective for truly low-resource\nlanguages, where translation quality is typically\nvery poor?1\nWe experiment with XLM-R, both with and with-\nout model adaptation via continued pretraining on\nmonolingual corpora in the target language. Our\nresults show that the performance of XLM-R out-\nof-the-box is moderately above chance, and model\nadaptation leads to improvements of up to 5.86\npercentage points. Training on machine-translated\ntraining data, however, results in an even larger per-\nformance gain of 11.13 percentage points over the\ncorresponding XLM-R model without adaptation.\nWe further perform an analysis via experiments\nwith hypothesis-only models, to examine poten-\ntial artifacts which may have been inherited from\nXNLI and ﬁnd that performance is above chance\nfor most models, but still below that for using the\nfull example.\nAmericasNLI is publicly available2 and we hope\nthat it will serve as a benchmark for measuring the\nzero-shot natural language understanding abilities\nof multilingual models for unseen languages. Ad-\nditionally, we hope that our dataset will motivate\nthe development of novel pretraining and model\nadaptation techniques which are suitable for truly\nlow-resource languages.\n2 Background and Related Work\n2.1 Pretrained Multilingual Models\nPrior to the widespread use of pretrained trans-\nformer models, cross-lingual transfer was mainly\n1We provide a sample of sentence pairs in Table D.3.\n2https://github.com/abteen/americasnli\nachieved through word embeddings (Mikolov et al.,\n2013; Pennington et al., 2014; Bojanowski et al.,\n2017), either by aligning monolingual embeddings\ninto the same embedding space (Lample et al.,\n2018b,a; Grave et al., 2018) or by training multilin-\ngual embeddings (Ammar et al., 2016; Artetxe and\nSchwenk, 2019). Pretrained multilingual models\nrepresent the extension of multilingual embeddings\nto pretrained transformer models.\nThese models follow the standard pretraining–\nﬁnetuning paradigm: they are ﬁrst trained on unla-\nbeled monolingual corpora from various languages\n(the pretraining languages) and later ﬁnetuned\non target-task data in a – usually high-resource\n– source language. Having been exposed to a\nvariety of languages through this training setup,\ncross-lingual transfer results for these models are\ncompetitive with the state of the art for many lan-\nguages and tasks. Commonly used models are\nmBERT (Devlin et al., 2019), which is pretrained\non the Wikipedias of 104 languages with masked\nlanguage modeling (MLM) and next sentence pre-\ndiction (NSP), and XLM, which is trained on 15\nlanguages and introduces the translation language\nmodeling objective, which is based on MLM, but\nuses pairs of parallel sentences. XLM-R has im-\nproved performance over XLM, and trains on data\nfrom 100 different languages with only the MLM\nobjective. Common to all models is a large shared\nsubword vocabulary created using either BPE (Sen-\nnrich et al., 2016) or SentencePiece (Kudo and\nRichardson, 2018) tokenization.\n2.2 Evaluating Pretrained Multilingual\nModels\nJust as in the monolingual setting, where bench-\nmarks such as GLUE (Wang et al., 2018) and Su-\nperGLUE (Wang et al., 2019) provide a look into\nthe performance of models across various tasks,\nmultilingual benchmarks (Hu et al., 2020; Liang\net al., 2020) cover a wide variety of tasks involv-\ning sentence structure, classiﬁcation, retrieval, and\nquestion answering.\nAdditional work has been done examining what\nmechanisms allow multilingual models to trans-\nfer across languages (Pires et al., 2019; Wu and\nDredze, 2019). Wu and Dredze (2020) examine\ntransfer performance dependent on a language’s\nrepresentation in the pretraining data. For lan-\nguages with low representation, multiple methods\nhave been proposed to improve performance, in-\n6280\ncluding extending the vocabulary, transliterating\nthe target text, and continuing pretraining before\nﬁnetuning (Lauscher et al., 2020; Chau et al., 2020;\nMuller et al., 2020; Pfeiffer et al., 2020a,b; Wang\net al., 2020). In this work, we focus on continued\npretraining to analyze the performance of model\nadaptation for a high-level, semantic task.\n2.3 Natural Language Inference\nGiven two sentences, the premise and the hypothe-\nsis, the task of NLI consists of determining whether\nthe hypothesis logically entails, contradicts, or is\nneutral to the premise. The most widely used\ndatasets for NLI in English are SNLI (Bowman\net al., 2015) and MNLI (Williams et al., 2018).\nXNLI (Conneau et al., 2018) is the multilingual\nexpansion of MNLI to 15 languages, providing\nmanually translated evaluation sets and machine-\ntranslated training sets. While datasets for NLI or\nthe similar task of recognizing textual entailment\nexist for other languages (Bos et al., 2009; Alab-\nbas, 2013; Eichler et al., 2014; Amirkhani et al.,\n2020), their lack of similarity prevents a general-\nized study of cross-lingual zero-shot performance.\nThis is in contrast to XNLI, where examples for all\n15 languages are parallel. To preserve this property\nof XNLI, when creating AmericasNLI, we choose\nto translate Spanish XNLI as opposed to creating\nexamples directly in the target language.\nHowever, NLI datasets are not without issue:\nGururangan et al. (2018) show that artifacts from\nthe creation of MNLI allow for models to classify\nexamples depending on only the hypothesis, show-\ning that models may not be reasoning as expected.\nMotivated by this, we provide further analysis of\nAmericasNLI in Section 6 by comparing the perfor-\nmance of hypothesis-only models to models trained\non full examples.\n3 AmericasNLI\n3.1 Data Collection Setup\nAmericasNLI is the translation of a subset of XNLI\n(Conneau et al., 2018). As translators between\nSpanish and the target languages are more fre-\nquently available than those for English, we trans-\nlate from the Spanish version. Additionally, some\ntranslators reported that code-switching is often\nused to describe certain topics, and, while many\nwords without an exact equivalence in the target\nlanguage are worked in through translation or inter-\npretation, others are kept in Spanish. To minimize\nthe amount of Spanish vocabulary in the translated\nexamples, we choose sentences from genres that\nwe judged to be relatively easy to translate into\nthe target languages: “face-to-face,” “letters,” and\n“telephone.” We choose up to 750 examples from\neach of the development and test set, with exact\ncounts for each language in Table 1.\n3.2 Languages\nWe now discuss the languages in AmericasNLI. For\nadditional background on previous NLP research\non Indigenous languages of the Americas, we refer\nthe reader to Mager et al. (2018). A summary of\nthis information can be found in Table C.1.\nAymara Aymara is a polysynthetic Amerindian\nlanguage spoken in Bolivia, Chile, and Peru by over\ntwo million people (Homola, 2012). Aymara fol-\nlows an SOV word order and has multiple dialects,\nincluding Northern and Southern Aymara, spoken\non the southern Peruvian shore of Lake Titicaca as\nwell as around La Paz and, respectively, in the east-\nern half of the Iquique province in northern Chile,\nthe Bolivian department of Oruro, in northern Po-\ntosi, and southwest Cochabamba. AmericasNLI\nexamples are translated into the Central Aymara\nvariant, speciﬁcally Aymara La Paz.\nAsháninka Asháninka is an Amazonian lan-\nguage from the Arawak family, spoken by 73,567\npeople3 in Central and Eastern Peru, in a geograph-\nical region located between the eastern foothills of\nthe Andes and the western fringe of the Amazon\nbasin (Mihas, 2017). Asháninka is an agglutinat-\ning and polysynthetic language with a VSO word\norder.\nBribri Bribri is a Chibchan language spoken by\n7,000 people in Southern Costa Rica (INEC, 2011).\nIt has three dialects, and while it is still spoken\nby children, it is currently a vulnerable language\n(Moseley, 2010; Sánchez Avendaño, 2013). Bribri\nis a tonal language with SOV word order. There\nare several orthographies which use different dia-\ncritics for the same phenomena, however even for\nresearchers who use the same orthography, the Uni-\ncode encoding of similar diacritics differs amongst\nauthors. Furthermore, the dialects of Bribri differ in\ntheir exact vocabularies, and there are phonological\nprocesses, like the deletion of unstressed vowels,\nwhich also change the tokens found in texts. As\n3https://bdpi.cultura.gob.pe/pueblos/\nashaninka\n6281\nLanguage Premise Hypothesis\nen And he said, Mama, I’m home. He told his mom he had gotten home.\nes Y él dijo: Mamá, estoy en casa. Le dijo a su madre que había llegado a casa.\naym Jupax sanwa: Mamita, utankastwa. Utar purinxtwa sasaw mamaparux sanxa\nbzd E na ie’ iche: ãm`˜ ı, ye’ tso’ ù a. I ãm `˜ ıa iché irir tö ye’ démine ù a.\ncni Iriori ikantiro: Ina, nosaiki pankotsiki. Ikantiro iriniro yaretaja pankotsiki.\ngn Ha ha’e he’i: Mama, aime ógape. He’íkuri isýpe o ˆguahêhague hógape.\nhch metá mik+ petay+: ne mama kitá nepa yéka. yu mama m+pa+ p+ra h+awe kai kename yu kitá he nuakai.\nnah huan yehhua quiihtoh: Nonantzin, niyetoc nochan quiilih inantzin niehcoquia\noto xi nydi biênâ: maMe dimi an ngû bimâbi o ini maMe guê o ngû\nquy Hinaptinmi pay nirqa: Mamay wasipim kachkani. Wasinman chayasqanmanta mamanta willarqa.\nshp Jara neskata iki: tita, xobonkoriki ea. Jawen tita yoiaia iki moa xobon nokota.\ntar A’lí je aníli échiko: ku bitichí ne atíki Nana Iyéla ku ruyéli, mapu bitichí ku nawáli.\nTable 2: A parallel example in AmericasNLI with the entailment label.\nBribri has only been a written language for about\n40 years, existing materials have a large degree of\nidiosyncratic variation. These variations are stan-\ndardized in AmericasNLI, which is written in the\nAmubri variant.\nGuaraní Guaraní is spoken by between 6 to 10\nmillion people in South America and roughly 3 mil-\nlion people use it as their main language, including\nmore than 10 native nations in Paraguay, Brazil,\nArgentina, and Bolivia, along with Paraguayan, Ar-\ngentinian, and Brazilian peoples. According to\nthe Paraguayan Census, in 2002 there were around\n1.35 million monolingual speakers, which has since\nincreased to around 1.5 million people (Dos Santos,\n2017; Melià, 1992).4 Although the use of Guaraní\nas spoken language is much older, the ﬁrst writ-\nten record dates to 1591 (Catechism) followed by\nthe ﬁrst dictionary in 1639 and linguistic descrip-\ntions in 1640. The ofﬁcial grammar of Guaraní\nwas approved in 2018. Guaraní is an agglutinative\nlanguage, with ample use of preﬁxes and sufﬁxes.\nNahuatl Nahuatl belongs to the Nahuan subdivi-\nsion of the Uto-Aztecan language family. There are\n30 recognized variants of Nahuatl spoken by over\n1.5 million speakers across Mexico, where Nahu-\natl is recognized as an ofﬁcial language (SEGOB,\n2020b). Nahuatl is polysynthetic and agglutina-\ntive, and many sentences have an SVO word order\nor, for contrast and focus, a VSO order, and for\nemphasis, an SOV order (MacSwan, 1998). The\n4https://www.ine.gov.py/news/\n25-de-agosto-dia-del-Idioma-Guarani.php\ntranslations in AmericasNLI belong to the Central\nNahuatl (Náhuatl de la Huasteca) dialect. As there\nis a lack of consensus regarding the orthographic\nstandard, the orthography is normalized to a ver-\nsion similar to Classical Nahuatl.\nOtomí Otomí belongs to the Oto-Pamean lan-\nguage family and has nine linguistic variants with\ndifferent regional self-denominations. Otomí is a\ntonal language following an SVO order, and there\nare around 307,928 speakers spread across 7 Mex-\nican states. In the state of Tlaxcala, the yuhmu or\nñuhmu variant is spoken by fewer than 100 speak-\ners, and we use this variant for the Otomí examples\nin AmericasNLI.\nQuechua Quechua, or Runasimi, is an Indige-\nnous language family spoken primarily in the Pe-\nruvian Andes. It is the most widely spoken pre-\nColumbian language family of the Americas, with\naround 8-10 million speakers. Approximately\n25% (7.7 million) of Peruvians speak a Quechuan\nlanguage, and it is the co-ofﬁcial language in\nmany regions of Peru. There are multiple subdi-\nvisions of Quechua , and AmericasNLI examples\nare translated into the standard version of South-\nern Quechua, Quechua Chanka, also known as\nQuechua Ayacucho, which is spoken in different\nregions of Peru and can be understood in differ-\nent areas of other countries, such as Bolivia or\nArgentina. In AmericasNLI, the apostrophe and\npentavocalism from other regions are not used.\nRarámuri Rarámuri, also known asTarahumara,\nwhich means light foot (INALI, 2017), belongs\n6282\naym bzd cni gn hch nah oto quy shp tar\nChrF es→XX 0.19 0.08 0.10 0.22 0.13 0.18 0.06 0.33 0.14 0.05\nXX→es 0.09 0.06 0.09 0.14 0.07 0.10 0.06 0.14 0.09 0.08\nBLEU es→XX 0.30 0.54 0.03 3.26 3.18 0.33 0.01 1.58 0.34 0.01\nXX→es 0.04 0.01 0.01 0.18 0.01 0.02 0.02 0.05 0.01 0.01\nTable 3: Translation performance for all target languages. es→XX represents translating into the target language,\nwhich is used for translate-train, and XX→es represents translating into Spanish, used for translate-test.\nto the Taracahitan subgroup of the Uto-Aztecan\nlanguage family (Goddard, 1996), and is polysyn-\nthetic and agglutinative. Rarámuri is an ofﬁcial\nlanguage of Mexico, spoken mainly in the Sierra\nMadre Occidental region by a total of 89,503\nspeakers (SEGOB, 2020c). AmericasNLI exam-\nples are translated into the Highlands variant (IN-\nALI, 2009), and translation orthography and word\nboundaries are similar to Caballero (2008).\nShipibo-Konibo Shipibo-Konibo is a Panoan\nlanguage spoken by around 35,000 native speakers\nin the Amazon region of Peru. Shipibo-Konibo\nuses an SOV word order (Faust, 1973) and post-\npositions (Vasquez et al., 2018). The translations\nin AmericasNLI make use of the ofﬁcial alphabet\nand standard writing supported by the Ministry of\nEducation in Peru.\nWixarika The Wixarika, or Huichol, language,\nmeaning the language of the doctors and heal-\ners (Lumholtz, 2011), is a language in the Cora-\nchol subgroup of the Uto-Aztecan language fam-\nily (Campbell, 2000). Wixarika is a national lan-\nguage of Mexico with four variants , spoken by a\ntotal of around 47,625 speakers (SEGOB, 2020a).\nWixarika is a polysynthetic language and follows\nan SOV word order. Translations in Americas-\nNLI are in Northern Wixarika and use an orthogra-\nphy common among native speakers (Mager-Hois,\n2017).\n4 Experiments\nIn this section, we detail the experimental setup\nwe use to evaluate the performance of various ap-\nproaches on AmericasNLI.\n4.1 Zero-Shot Learning\nPretrained Model We use XLM-R (Conneau\net al., 2020) as the pretrained multilingual model\nin our experiments. The architecture of XLM-R\nis based on RoBERTa (Liu et al., 2019), and it is\ntrained using MLM on web-crawled data in 100\nlanguages. It uses a shared vocabulary consisting\nof 250k subwords, created using SentencePiece\n(Kudo and Richardson, 2018) tokenization. We use\nthe Base version of XLM-R for our experiments.\nAdaptation Methods To adapt XLM-R to the\nvarious target languages, we continue training with\nthe MLM objective on monolingual text in the tar-\nget language before ﬁnetuning. To keep a fair com-\nparison with other approaches, we only use target\ndata which was also used to train the translation\nmodels, which we describe in Section 4.2. How-\never, we note that one beneﬁt of continued pretrain-\ning for adaptation is that it does not require parallel\ntext, and could therefore beneﬁt from text which\ncould not be used for a translation-based approach.\nFor continued pretraining, we use a batch size of\n32 and a learning rate of 2e-5. We train for a to-\ntal of 40 epochs. Each adapted model starts from\nthe same version of XLM-R, and is adapted indi-\nvidually to each target language, which leads to a\ndifferent model for each language. We denote mod-\nels adapted with continued pretraining as +MLM.\nFinetuning To ﬁnetune XLM-R, we follow the\napproach of Devlin et al. (2019) and use an ad-\nditional linear layer. We train on either the En-\nglish MNLI data or the machine-translated Spanish\ndata, and we call the ﬁnal models XLM-R (en)\nand XLM-R (es), respectively. Following Hu et al.\n(2020), we use a batch size of 32 and a learning\nrate of 2e-5. We train for a maximum of 5 epochs,\nand evaluate performance every 2500 steps on the\nXNLI development set. We employ early stopping\nwith a patience of 15 evaluation steps and use the\nbest performing checkpoint for the ﬁnal evalua-\ntion. All ﬁnetuning is done using the Huggingface\nTransformers library (Wolf et al., 2020) with up\nto two Nvidia V100 GPUs. Using Lacoste et al.\n(2019), we estimate total carbon emissions to be\n75.6 kgCO2eq.\n6283\naym bzd cni gn hch nah oto quy shp tar Avg.\nMajority baseline 33.33 33.33 33.33 33.33 33.33 33.47 33.42 33.33 33.33 33.33 -\nZero-shot\nXLM-R (en) 36.13±0.88 39.65±0.89 37.91±0.82 39.47±1.14 37.20±1.32 42.59±0.34 37.79±0.78 37.24±1.78 40.45±0.89 36.36±1.07 38.48±1.05\nXLM-R (es) 37.25±2.33 39.38±1.96 37.29±1.12 39.25±1.55 35.82±1.01 38.98±1.38 38.32±1.47 39.51±1.92 38.40±0.87 35.73±0.69 37.99±1.51\nZero-shot w/ adaptation\nXLM-R+MLM(en) 43.51±1.69 38.13±1.75 39.47±1.19 52.44±0.93 37.25±2.60 46.21±0.72 37.03±3.28 61.78±2.42 41.34±0.61 39.82±0.95 43.70±1.83\nXLM-R+MLM(es) 43.87±0.14 40.05±2.20 38.76±0.08 52.27±1.20 37.82±1.59 44.17±1.76 40.55±1.07 62.40±1.44 40.18±0.95 38.45±0.86 43.85±1.30\nTranslate-train\nXLM-R 50.00±1.51 51.42±1.24 42.45±1.63 58.89±2.70 43.20±2.07 55.33±1.12 36.01±0.74 59.91±0.20 52.00±0.27 42.04±1.81 49.12±1.52\nTranslate-test\nXLM-R 39.73±0.27 40.40±0.13 34.71±0.73 46.62±2.29 38.00±0.48 41.37±0.16 35.29±1.15 51.38±1.24 39.51±0.47 35.16±0.97 40.22±1.01\nTable 4: Results for zero-shot, translate-train, and translate-test averaged over 3 runs with different seeds. The\nmajority baseline represents expected performance when predicting only the majority class of the test set. Random\nguessing would result in an accuracy of 33.33%. Standard deviations in the Avg. column are calculated by taking\nthe square root of the average variance of the languages in that row.\n4.2 Translation-based Approaches\nWe also experiment with two translation-based ap-\nproaches, translate-train and translate-test, detailed\nbelow along with the translation model used.\nTranslation Models For our translation-based\napproaches, we train two sets of translation mod-\nels: one to translate from Spanish into the tar-\nget language, and one in the opposite direction.\nWe use transformer sequence-to-sequence models\n(Vaswani et al., 2017) with the hyperparameters\nproposed by Guzmán et al. (2019). Parallel data\nused to train the translation models can be found\nin Table B.1. We employ the same model archi-\ntecture for both translation directions, and we mea-\nsure translation quality in terms of BLEU (Papineni\net al., 2002) and ChrF (Popovi´c, 2015), cf. Table 3.\nWe use fairseq (Ott et al., 2019) to implement all\ntranslation models.5\nTranslate-train For the translate-train approach,\nthe Spanish training data provided by XNLI is\ntranslated into each target language. It is then used\nto ﬁnetune XLM-R for each language individually.\nAlong with the training data, we also translate the\nSpanish development data, which is used for vali-\ndation and early stopping. We discuss the effects of\nusing a translated development set in Section F.1.\nNotably, we ﬁnd that the ﬁnetuning hyperparame-\nters deﬁned above do not reliably allow the model\nto converge for many of the target languages. To\n5The code for translation models can be found at https:\n//github.com/AmericasNLP/americasnlp2021\nﬁnd suitable hyperparameters, we tune the batch\nsize and learning rate by conducting a grid search\nover {5e-6, 2e-5, 1e-4} for the learning rate and\n{32, 64, 128} for the batch size. In order to select\nhyperparameters which work well across all lan-\nguages, we evaluate each run using the average per-\nformance on the machine-translated Aymara and\nGuaraní development sets, as these languages have\nmoderate and high ChrF scores, respectively. We\nﬁnd that decreasing the learning rate to 5e-6 and\nkeeping the batch size at 32 yields the best perfor-\nmance. Other than the learning rate, we use the\nsame approach as for zero-shot ﬁnetuning.\nTranslate-test For the translate-test approach,\nwe translate the test sets of each target language\ninto Spanish. This allows us to apply the model\nﬁnetuned on Spanish, XLM-R (es), to each test\nset. Additionally, a beneﬁt of translate-test over\ntranslate-train and the adapted XLM-R models is\nthat we only need to ﬁnetune once overall, as op-\nposed to once per language. For evaluation, we use\nthe checkpoint with the highest performance on the\nSpanish XNLI development set.\n5 Results and Discussion\nZero-shot Models We present our results in Ta-\nble 4. Results for the development set are presented\nin Table E.1. Zero-shot performance is low for all\n10 languages, with an average accuracy of 38.48%\nand 37.99% for the English and Spanish model,\nrespectively. However, in all cases the performance\nis higher than the majority baseline. As shown in\n6284\nFT aym bzd cni gn hch nah oto quy shp tar Avg. Avg.+P\nMajority baseline - 33.33 33.33 33.33 33.33 33.33 33.47 33.42 33.33 33.33 33.33 - -\nZero-shot\nXLM-R (en) 62.34 33.60 33.47 32.40 33.47 34.13 33.06 32.35 33.33 33.60 34.27 33.37 38.48\nXLM-R (es) 62.26 34.13 34.80 35.33 35.33 34.53 33.60 33.16 33.07 36.80 35.73 34.65 37.99\nZero-shot w/ adaptation\nXLM-R +MLM (en) - 37.07 32.80 33.07 42.40 33.73 34.55 33.96 44.40 35.33 34.80 36.21 43.70\nXLM-R +MLM (es) - 36.27 34.80 33.73 41.73 34.00 35.37 32.89 47.87 35.60 34.67 36.69 43.85\nTranslate-train\nXLM-R - 44.93 43.73 43.47 47.60 43.07 45.80 35.83 52.13 46.27 39.47 44.23 49.12\nTranslate-test\nXLM-R - 36.53 42.67 37.33 43.60 38.53 43.22 34.22 48.13 42.67 34.67 40.16 40.22\nTable 5: Hypothesis-only results. The Avg. column represents the average of the hypothesis-only results, while\nthe Avg.+P column, taken from Table 4, represents the average of the languages when using both the premise and\nhypothesis.\nTable E.3 in the appendix, the same models achieve\nan average of 74.20% and 75.35% accuracy respec-\ntively, when evaluated on the 15 XNLI languages.\nInterestingly, even though code-switching with\nSpanish is encountered in many target languages,\nﬁnetuning on Spanish labeled data on average\nslightly underperforms the model trained on En-\nglish, however performance is better for 3 of the\nlanguages. The English model achieves a highest\naccuracy of 42.59%, when evaluated on Nahuatl,\nwhile the Spanish model achieves a highest accu-\nracy of 39.51%, when evaluated on Quechua. The\nlowest performance is achieved when evaluating on\nAymara and Rarámuri, for the English and Spanish\nmodel, respectively.\nWe ﬁnd that model adaptation via continued pre-\ntraining improves both models, with an average\ngain of 5.22 percentage points for English and 5.86\npercentage points for Spanish. Notably, continued\npretraining increases performance for Quechua by\n24.53 percentage points when ﬁnetuning on En-\nglish, and 22.89 points when ﬁnetuning on Spanish.\nPerformance decreases for Bribri and Otomí when\nﬁnetuning on English, however performance for all\nlanguages improves when using Spanish.\nTranslate-test Performance of the translate-test\nmodel improves over both zero-shot baselines. We\nsee the largest increase in performance for Guaraní\nand Quechua, with gains of 7.16 and, respectively,\n11.87 points over the best performing zero-shot\nmodel without adaptation. Considering the trans-\nlation metrics in Table 3, models for Guaraní and\nQuechua achieve the two highest scores for both\nmetrics. On average, translate-test does worse\nwhen compared to the adapted zero-shot models,\nand in all but two cases, both adapted models per-\nform better than translate-test. We hypothesize that\ntranslate-test is more sensitive to noise in the trans-\nlated data; sentences may lose too much of their\noriginal content, preventing correct classiﬁcation.\nTranslate-train The most surprising result is\nthat of translate-train, which considerably outper-\nforms the performance of translate-test for all lan-\nguages, and outperforms the zero-shot models for\nall but two languages. Compared to the best non-\nadapted zero-shot model, the largest performance\ngain is 20.40 points for Quechua. For the language\nwith the lowest performance, Otomí, translate-train\nperforms 2.32 points worse than zero-shot; how-\never, it still outperforms translate-test. When av-\neraged across all languages, translate-train outper-\nforms the English zero-shot model by 10.64 points,\nand translate-test by 8.9 points. It is important to\nnote that the translation performance from Span-\nish to each target language is not particularly high:\nwhen considering ChrF scores, the highest is 0.33,\nand the highest BLEU score is 3.26. Performance\nof both translation-based models is correlated with\nChrF scores, with a Pearson correlation coefﬁcient\nof 0.82 and 0.83 for translate-train and translate-\ntest. Correlations are not as strong for BLEU, with\ncoefﬁcients of 0.37 and 0.59.\nThe sizable difference in performance between\ntranslate-train and the other methods suggests that\ntranslation-based approaches may be a valuable\nasset for cross-lingual transfer, especially for low-\n6285\nresource languages. While the largest downsides\nto this approach are the requirement for parallel\ndata and the need for multiple models, the poten-\ntial performance gain over other approaches may\nprove worthwhile. Additionally, we believe that the\nperformance of both translation-based approaches\nwould improve given a stronger translation system,\nand future work detailing the necessary level of\ntranslation quality for the best performance would\noffer great practical usefulness for NLP applica-\ntions for low-resource languages.\n6 Analysis\n6.1 Hypothesis-only Models\nAs shown by Gururangan et al. (2018), SNLI and\nMNLI – the datasets AmericasNLI is based on\n– contain artifacts created during the annotation\nprocess which models exploit to artiﬁcially inﬂate\nperformance. To analyze whether similar artifacts\nexist in AmericasNLI and if they can also be ex-\nploited, we train and evaluate models using only\nthe hypothesis, and present results in Table 5. We\ncan see that the average performance across lan-\nguages is better than chance for all models except\nfor XLM-R without adaptation. Translate-train\nobtains the highest result with 44.23% accuracy,\nand as shown in Table E.2, hypothesis-only per-\nformance of translate-test is higher than standard\nperformance for 5 languages. Thus, as with SNLI\nand MNLI, artifacts in the hypotheses can be used\nto predict, to some extent, the correct labels. How-\never all but 1 zero-shot and translate-train models\nperform better in the standard setting, indicating\nthat the models are learning something beyond just\nexploiting artifacts in the hypotheses, even with the\nadditional challenge of unseen languages.\n6.2 Case Study: Human Evaluation\nFollowing Conneau et al. (2018), AmericasNLI\nwas created by translating sentences individually,\nin order to prevent additional context being added\ninto the hypotheses. However, this strategy may\nbreak the original semantic relationship between\nthe premise and the hypothesis. Furthermore, for\nsome examples the logical relationship may be de-\npendent on context or subtext which can be lost\nthrough translation, or simply not make sense in\nthe target language. To verify the validity of the\nlabels of AmericasNLI, we conduct a human evalu-\nation experiment, focusing on examples translated\nto Bribri. We create a balanced, random sample\nof 450 examples taken from the Bribri develop-\nment set. An annotator familiar with the task was\nthen asked to classify the pairs of sentences. For\ncomparison, we also annotate parallel examples\ntaken from the English and Spanish development\nsets. For Bribri, we recover the original XNLI label\nfor 76.44% of examples. For English and Spanish,\nwe achieve 81.78% and 71.56% accuracy, respec-\ntively. Due to the relatively small differences in\nperformance across languages, we conclude that\ntranslation to Bribri has a minimal effect on the\nsemantic relationship between the premise and the\nhypothesis.\n7 Limitations and Future Work\nWhile the case study above provides strong evi-\ndence for the validity of our Bribri examples, we\ncannot currently generalize this claim to the re-\nmaining languages. For future work, we plan on\nextending our human evaluation to more languages\nand provide a more detailed analysis.\nAdditionally, due to the limited availability of\nannotators and the difﬁculties of translation for lan-\nguages that are less frequently studied, the size of\nthe AmericasNLI test set is relatively small. As\nsuch, care must be taken to carefully evaluate con-\nclusions drawn using the dataset; following Card\net al. (2020) we present a power analysis of our\nresults in Section D.1. Future work expanding the\ndataset size will help create a stronger baseline.\nFurthermore, while we do not make any model-\nspeciﬁc assumptions in our experiments, our re-\nsults are based on only one pretrained model and\nadaptation method. Methods using vocabulary ex-\ntension or adapters may offer additional improve-\nments. Similarly, other pretrained models could\nperform differently, depending on, e.g., the model\nsize or the set of languages in their pretraining\ndata. In Table F.3, we present results using XLM-\nR Large, and ﬁnd that, while the relationship be-\ntween the approaches differs from the main experi-\nments, the overall highest average performance is\nstill achieved by the translate-train approach with\nXLM-R Base. We provide a longer discussion in\nSection F.3.\n8 Conclusion\nTo better understand the zero-shot abilities of pre-\ntrained multilingual models for semantic tasks in\nunseen languages, we present AmericasNLI, a par-\nallel NLI dataset covering 10 low-resource lan-\n6286\nguages indigenous to the Americas. We conduct\nexperiments with XLM-R, and ﬁnd that the model’s\nzero-shot performance, while better than a majority\nbaseline, is poor. However, it can be improved by\nmodel adaptation via continued pretraining. Addi-\ntionally, we ﬁnd that translation-based approaches\noutperform a zero-shot approach, which is surpris-\ning given the low quality of the employed trans-\nlation systems. We hope that this work will not\nonly spur further research into improving model\nadaptation to unseen languages, but also motivate\nthe creation of more resources for languages not\nfrequently studied by the NLP community.\nEthics Statement\nIn this work, we present a new dataset created\nthrough the translation of an existing resource,\nXNLI (Conneau et al., 2018). While this allows for\nresults that are directly comparable, it also means\nthat this dataset inherits any biases and ﬂaws which\nare contained in the previous dataset. Furthermore,\nresearch involving languages spoken by Indigenous\ncommunities raises ethical concerns regarding the\nexploitation of these languages and communities:\nit is crucial that members of the community are\nable to directly beneﬁt from the research. Trans-\nlation for AmericasNLI was done by either paper\nauthors or translators who were compensated at a\nrate based on the average rate for translation and\nthe minimum wage in their country of residence.\nAdditionally, many authors are members of, and/or\nhave a record of close work with communities who\nspeak a language contained in AmericasNLI.\nAcknowledgments\nWe thank the following people for their work on\nthe translations: Francisco Morales for Bribri, Fe-\nliciano Torres Ríos for Asháninka, Perla Alvarez\nBritez for Guaraní, Silvino González de la Crúz\nfor Wixarika, Giovany Martínez Sebastián, Pedro\nKapoltitan, and José Antonio for Nahuatl, José Ma-\nteo Lino Cajero Velázquez for Otomí, Liz Chávez\nfor Shipibo-Konibo, and María del Cármen Sotelo\nHolguín for Rarámuri. We would also like to thank\nDallas Card for his help with power analysis. This\nwork would not have been possible without the\nﬁnancial support of Facebook AI Research, Mi-\ncrosoft Research, Google Research, the Institute\nof Computational Linguistics at the University of\nZurich, the NAACL Emerging Regions Fund, Co-\nmunidad Elotl, and Snorkel AI.\nReferences\nŽeljko Agi ´c and Ivan Vuli ´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nMaytham Alabbas. 2013. A dataset for Arabic textual\nentailment. In Proceedings of the Student Research\nWorkshop associated with RANLP 2013 , pages 7–\n13, Hissar, Bulgaria. INCOMA Ltd. Shoumen, BUL-\nGARIA.\nHossein Amirkhani, Mohammad AzariJafari, Azadeh\nAmirak, Zohreh Pourjafari, Soroush Faridan\nJahromi, and Zeinab Kouhkan. 2020. Farstail: A\npersian natural language inference dataset. ArXiv,\nabs/2009.08820.\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov,\nGuillaume Lample, Chris Dyer, and Noah A. Smith.\n2016. Massively multilingual word embeddings.\nArXiv, abs/1602.01925.\nM. Artetxe and Holger Schwenk. 2019. Massively mul-\ntilingual sentence embeddings for zero-shot cross-\nlingual transfer and beyond. Transactions of the As-\nsociation for Computational Linguistics, 7:597–610.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJohan Bos, Fabio Massimo Zanzotto, and M. Pennac-\nchiotti. 2009. Textual entailment at evalita 2009.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nDavid Brambila. 1976. Diccionario raramuri - castel-\nlano: Tarahumar.\nGabriela Caballero. 2008. Choguita rarámuri (tarahu-\nmara) phonology and morphology.\nM. Cajero. 1998. Raíces del Otomí: diccionario. Gob-\nierno del Estado de Tlaxcala.\nMateo Cajero. 2009. Historia de los Otomíes en Ix-\ntenco, volume 1. Instituto Tlaxcalteca de la Cultura,\nTlaxcala, México.\nLyle Campbell. 2000. American Indian languages: the\nhistorical linguistics of Native America . Oxford\nUniversity Press.\nD. Card, Peter Henderson, Urvashi Khandelwal, Robin\nJia, Kyle Mahowald, and Dan Jurafsky. 2020. With\nlittle power comes great responsibility. In EMNLP.\n6287\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1324–1334, Online. Association for Computational\nLinguistics.\nLuis Chiruzzo, Pedro Amarilla, Adolfo Ríos, and Gus-\ntavo Giménez Lugo. 2020. Development of a\nGuarani - Spanish parallel corpus. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2629–2633, Marseille, France. Euro-\npean Language Resources Association.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, E. Grave, Myle Ott, Luke Zettlemoyer, and\nVeselin Stoyanov. 2020. Unsupervised cross-lingual\nrepresentation learning at scale. In ACL.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nAdolfo Constenla, Feliciano Elizondo, and Francisco\nPereira. 2004. Curso Básico de Bribri. Editorial de\nla Universidad de Costa Rica.\nRubén Cushimariano Romano and Richer C. Se-\nbastián Q. 2008. Ñaantsipeta asháninkaki bi-\nrakochaki. diccionario asháninka-castellano. versión\npreliminar. http://www.lengamer.org/\npublicaciones/diccionarios/. Visitado:\n01/03/2013.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRafaela Alves Dos Santos. 2017. DIGLOSSIA\nNO PARAGUAI: A restrição dos monolíngues em\nguarani no acesso à informação . Trabalho de\nConclusão de Curso, Bacharelado em Línguas Es-\ntrangeiras. Universidade de Brasília, Brasilia.\nKathrin Eichler, Aleksandra Gabryszak, and Günter\nNeumann. 2014. An analysis of textual inference\nin German customer emails. In Proceedings of\nthe Third Joint Conference on Lexical and Com-\nputational Semantics (*SEM 2014) , pages 69–74,\nDublin, Ireland. Association for Computational Lin-\nguistics and Dublin City University.\nNorma Faust. 1973. Lecciones para el aprendizaje del\nidioma shipibo-conibo, volume 1 of Documento de\nTrabajo. Instituto Lingüístico de Verano, Yarina-\ncocha.\nIsaac Feldman and Rolando Coto-Solano. 2020. Neu-\nral machine translation models with back-translation\nfor the extremely low-resource indigenous language\nBribri. In Proceedings of the 28th International\nConference on Computational Linguistics , pages\n3965–3976, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nSofía Flores Solórzano. 2017. Corpus oral pandialectal\nde la lengua bribri. http://bribri.net.\nAna-Paula Galarreta, Andrés Melgar, and Arturo On-\ncevay. 2017. Corpus creation and initial SMT ex-\nperiments between Spanish and Shipibo-konibo. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing, RANLP\n2017, pages 238–244, Varna, Bulgaria. INCOMA\nLtd.\nIves Goddard. 1996. Introduccion. In William C.\nSturtevant, editor, Handbook of North American In-\ndians (vol. 17), chapter 1, pages 1–6. University of\nTexas.\nHéctor Erasmo Gómez Montoya, Kervy Dante Rivas\nRojas, and Arturo Oncevay. 2019. A continuous\nimprovement framework of machine translation for\nShipibo-konibo. In Proceedings of the 2nd Work-\nshop on Technologies for MT of Low Resource Lan-\nguages, pages 17–23, Dublin, Ireland. European As-\nsociation for Machine Translation.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nJoseph Harold Greenberg. 1963. Universals of lan-\nguage.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A.\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nXimena Gutierrez-Vasques, Gerardo Sierra, and\nIsaac Hernandez Pompa. 2016. Axolotl: a web\naccessible parallel corpus for Spanish-Nahuatl. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 4210–4214, Portorož, Slovenia. European\nLanguage Resources Association (ELRA).\nFrancisco Guzmán, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. The\nﬂores evaluation datasets for low-resource machine\n6288\ntranslation: Nepali–english and sinhala–english. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6100–\n6113.\nPetr Homola. 2012. Building a formal grammar for a\npolysynthetic language. In Formal Grammar, pages\n228–242, Berlin, Heidelberg. Springer Berlin Hei-\ndelberg.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. CoRR, abs/2003.11080.\nINALI. 2009. Catálogo de las lenguas indígenas na-\ncionales: Variantes lingüísticas de méxico con sus\nautodenominaciones y referencias geoestadísticas.\nINALI.\nINALI. 2014. Norma de escritura de la Lengua\nHñähñu (Otomí), 1st edition. Secretaria de culatura.\nINALI. 2017. Etnografía del pueblo tarahumara (rará-\nmuri).\nINEC. 2011. Población total en territorios indígenas\npor autoidentiﬁcación a la etnia indígena y habla de\nalguna lengua indígena, según pueblo y territorio in-\ndígena. In Instituto Nacional de Estadística y Cen-\nsos, editor, Censo 2011.\nINEGI. 2008. Catálogo de las lenguas indígenas na-\ncionales: Variantes lingüísticas de méxico con sus\nautodenominaciones y referencias geoestadísticas.\nDiario Oﬁcial, pages 31–108.\nJosé L. Iturrioz and Paula Gómez-López. 2008. Gra-\nmatica wixarika i.\nCarla Victoria Jara Murillo. 2018a. Gramática de la\nLengua Bribri. EDigital.\nCarla Victoria Jara Murillo. 2018b. I Ttè Historias\nBribris, second edition. Editorial de la Universidad\nde Costa Rica.\nCarla Victoria Jara Murillo and Alí García Segura.\n2013. Se’ ttö’ bribri ie Hablemos en bribri. EDigi-\ntal.\nKatharina Kann, Kyunghyun Cho, and Samuel R. Bow-\nman. 2019. Towards realistic practices in low-\nresource natural language processing: The develop-\nment set. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3342–3349, Hong Kong, China. Association for\nComputational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlexandre Lacoste, Alexandra Luccioni, Victor\nSchmidt, and Thomas Dandres. 2019. Quantifying\nthe carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. In NeurIPS.\nGuillaume Lample, Alexis Conneau, Ludovic Denoyer,\nand Marc’Aurelio Ranzato. 2018a. Unsupervised\nmachine translation using monolingual corpora only.\nIn International Conference on Learning Represen-\ntations (ICLR).\nGuillaume Lample, Alexis Conneau, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Hervé Jégou. 2018b.\nWord translation without parallel data. In 6th Inter-\nnational Conference on Learning Representations,\nICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings . OpenRe-\nview.net.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the lim-\nitations of zero-shot language transfer with multilin-\ngual Transformers. pages 4483–4499.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie\nWu, Shuguang Liu, Fan Yang, Daniel Campos, Ran-\ngan Majumder, and Ming Zhou. 2020. XGLUE: A\nnew benchmark datasetfor cross-lingual pre-training,\nunderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008–6018,\nOnline. Association for Computational Linguistics.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nJames Loriot, Erwin Lauriout, and Dwight Day. 1993.\nDiccionario Shipibo-Castellano. Instituto Lingüís-\ntico de Verano.\nCarl Lumholtz. 2011. Unknown Mexico: A Record\nof Five Years’ Exploration Among the Tribes of the\nWestern Sierra Madre, volume 2. Cambridge Uni-\nversity Press.\nJeff MacSwan. 1998. The argument status of nps in\nsoutheast puebla nahuatl: Comments on the polysyn-\nthesis parameter. Southwest Journal of Linguistics ,\n17(2):101–114.\n6289\nManuel Mager, Dionico Gonzalez, and Ivan Meza.\n2017. Probabilistic ﬁnite-state morphological seg-\nmenter for wixarika (huichol).\nManuel Mager, Ximena Gutierrez-Vasques, Gerardo\nSierra, and Ivan Meza-Ruiz. 2018. Challenges of\nlanguage technologies for the indigenous languages\nof the Americas. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 55–69, Santa Fe, New Mexico, USA. Associ-\nation for Computational Linguistics.\nJesus Manuel Mager-Hois. 2017. Traductor\nhıbrido wixárika-espanol con escasos recursos\nbilingües. Ph.D. thesis, Master’s thesis, Universidad\nAutónoma Metropolitana.\nEnrique Margery. 2005. Diccionario Fraseológico\nBribri-Español Español-Bribri, second edition. Edi-\ntorial de la Universidad de Costa Rica.\nBartomeu Melià. 1992. La lengua Guaraní del\nParaguay: Historia, sociedad y literatura. Editorial\nMAPFRE, Madrid.\nElena Mihas. 2017. The kampa subgroup of the arawak\nlanguage family. The Cambridge Handbook of Lin-\nguistic Typology, page 782–814.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, volume 26, pages 3111–3119. Curran As-\nsociates, Inc.\nChristopher Moseley. 2010. Atlas of the World’s Lan-\nguages in Danger. Unesco.\nB. Muller, Antonis Anastasopoulos, Benoît Sagot, and\nDjamé Seddah. 2020. When being unseen from\nmbert is just the beginning: Handling new lan-\nguages with multilingual language models. ArXiv,\nabs/2010.12858.\nJohanna Nichols. 1986. Head-marking and dependent-\nmarking grammar. Language, 62(1):56–119.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020a. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020b. Unks everywhere: Adapting mul-\ntilingual language models to new scripts.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nCarlos Sánchez Avendaño. 2013. Lenguas en peligro\nen Costa Rica: vitalidad, documentación y descrip-\nción. Revista Káñina, 37(1):219–250.\nSEGOB. 2020a. Sistema de Información Cultural -\nLenguas indígenas: Huichol\n. https://sic.gob.mx/ficha.php?\ntable=inali_li.\nSEGOB. 2020b. Sistema de Información\nCultural - Lenguas indígenas: Nnahuatl.\nhttps://sic.gob.mx/ficha.php?\ntable=inali_li&amp;table_id=5.\nSEGOB. 2020c. Sistema de Información Cul-\ntural - Lenguas indígenas: Tarahumara.\nhttp://sic.gob.mx/ficha.php?table=\ninali_li&amp;table_id=15.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRémi Siméon. 1977. Diccionario de la lengua náhuatl\no mexicana, volume 1. Siglo XXI.\nThelma D Sullivan and Miguel León-Portilla. 1976.\nCompendio de la gramática náhuatl , volume 18.\nUniversidad nacional autónoma de México, Instituto\nde investigaciones históricas.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\n6290\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nPilar Valenzuela. 2003. Transitivity in Shipibo-Konibo\ngrammar. Ph.D. thesis, University of Oregon.\nAlonso Vasquez, Renzo Ego Aguirre, Candy An-\ngulo, John Miller, Claudia Villanueva, Željko Agi ´c,\nRoberto Zariquiey, and Arturo Oncevay. 2018. To-\nward Universal Dependencies for Shipibo-konibo.\nIn Proceedings of the Second Workshop on Uni-\nversal Dependencies (UDW 2018) , pages 151–161,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran As-\nsociates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending multilingual BERT to\nlow-resource languages. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 2649–2656, Online. Association for Computa-\ntional Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\n6291\nA Geographic Distribution of the\nAmericasNLI Languages\nBribri\nGuar aní\nWixarikaNáhuatl\nRar ámuri\nOt omí\nFigure A.1: Maps of Central and South America presenting an approximate distribution of where each Indigenous\nlanguage contained in AmericasNLI is spoken. Please note that this map is hand-drawn and largely an estimate:\nsome regions may not be included, and borders of included regions may not be completely accurate.\nB Sources of Parallel Data\nLang. Source(s) Sent.\naym Tiedemann (2012) 6,531\nbzd\nFeldman and Coto-Solano (2020); Margery (2005);\n7,508Jara Murillo (2018a); Constenla et al. (2004);\nJara Murillo and García Segura (2013);\nJara Murillo (2018b); Flores Solórzano (2017)\ncni Cushimariano Romano and Sebastián Q. (2008) 3,883\ngn Chiruzzo et al. (2020) 26,032\nhch Mager et al. (2017) 8,966\nnah Gutierrez-Vasques et al. (2016) 16,145\noto https://tsunkua.elotl.mx 4,889\nquy Agi ´c and Vuli´c (2019) 125,008\nshp Galarreta et al. (2017); Loriot et al. (1993); 14,592Gómez Montoya et al. (2019)\ntar Brambila (1976); 14,720github.com/pywirrarika/tar_par\nTable B.1: Parallel data used for our translation models.\n6292\nC Additional Information for\nAmericasNLI Languages\nC.1 Aymara\nA rare linguistic phenomenon found in Aymara is\nvowel elision, a deletion of certain vowel sounds\ntriggered by complex phonological, morphological,\nand syntactic factors.\nC.2 Asháninka\nWhile Asháninka in a strict sense refers to the lin-\nguistic varieties spoken in Ene, Tambo and Bajo\nPerené rivers, the name is also used to talk about\nthe following nearby and closely-related Asheninka\nvarieties: Alto Perené, Pichis, Pajonal, Ucayali-\nYurua, and Apurucayali. Although Asháninka is\nthe most widely spoken Amazonian language in\nPeru, certain varieties, such as Alto Perené, are\nhighly endangered.\nThe verb is the most morphologically complex\nword class, with a rich repertoire of aspectual and\nmodal categories. The language lacks case, except\nfor one locative sufﬁx, so the grammatical rela-\ntions of subject and object are indexed as afﬁxes\non the verb itself. Other notable linguistic features\nof the language include obligatory marking of a\nrealis/irrealis distinction on the verb, a rich system\nof applicative sufﬁxes, serial verb constructions,\nand a pragmatically conditioned split intransitivity.\nC.3 Bribri\nAs previously noted, Bribri is a vulnerable lan-\nguage, and there are few settings where the lan-\nguage is written or used in ofﬁcial functions. The\nlanguage does not have ofﬁcial status and it is not\nthe main medium of instruction of Bribri children,\nbut it is offered as a class in primary and secondary\nschools. Bribri features fusional morphology and\nan ergative-absolutive case system. Bribri grammar\nalso includes phenomena like head-internal relative\nclauses, directional verbs and numerical classiﬁers\n(Jara Murillo, 2018a).\nC.4 Guaraní\nWhile the ﬁrst written record dates to 1591,\nGuaraní usage in text continued until the Paraguay-\nTriple Alliance War (1864-1870) and declined\nthereafter. From the 1920s on, Guaraní has slowly\nre-emerged and received renewed focus. In 1992,\nGuaraní was the ﬁrst American language declared\nan ofﬁcial language of a country, followed by a\nsurge of local, national, and international recogni-\ntion in the early 21st century.6\nC.5 Nahuatl\nNahuatl is spoken in 17 different states of Mexico.\nIn Nahuatl, different roots with or without afﬁxes\nare combined to form new words. The sufﬁxes that\nare added to a word modify the meaning of the orig-\ninal word (Sullivan and León-Portilla, 1976), and\n18 prepositions stand out based on postpositions of\nnames and adjectives (Siméon, 1977).\nC.6 Otomí\nThe various regional self-denominations of Otomí\ninclude ñähñu or ñähño, hñähñu, ñuju, ñoju, yühu,\nhnähño, ñühú, ñanhú, ñöthó, ñható and hñothó\n(INALI, 2014). Many words are homophonous\nto Spanish (Cajero, 1998, 2009). When speaking\nñuhmu, pronunciation is elongated, especially on\nthe last syllable. The alphabet is composed of 19\nconsonants, 12 vowel phonemes.\nC.7 Rarámuri\nRarámuri is mainly spoken in the state of Chi-\nhuahua. There are ﬁve variants of Rarámuri.\nC.8 Shipibo-Konibo\nShipibo-Konibo is a language with agglutinative\nprocesses, a majority of which are sufﬁxes. How-\never, clitics are also used, and are a widespread\nelement in Panoan literature (Valenzuela, 2003).\nC.9 Wixarika\nThe four variants of Wixarika are the Northern,\nSouthern, Eastern, and Western variants (INEGI,\n2008). It is spoken mainly in the three Mexican\nstates of Jalisco, Nayari, and Durango. Features\nof Wixarika include head-marking (Nichols, 1986),\na head-ﬁnal structure (Greenberg, 1963), nominal\nincorporation, argumentative marks, inﬂected adpo-\nsitions, possession marks, as well as instrumental\nand directional afﬁxes (Iturrioz and Gómez-López,\n2008).\n6https://es.wikipedia.org/wiki/Idioma_\nguarani\n6293\nC.10 Summary of Language Information\nLanguage Language Family Countries Spoken Number of Speakers Word Order\naym Aymaran Bolivia, Chile, Peru 2m SOV\nbzd Chibchan Costa Rica 7k SOV\ncni Arawak Peru 73k VSO\ngn Tupi-Guarani Paraguay, Brazil, Argentina, Bolivia 6-10m SVO\nhch Uto-Aztecan Mexico 47k SOV\nnah Uto-Aztecan Mexico 1.5m SVO/VSO/SOV\noto Oto-Manguean Mexico 307k SVO\nquy Quechuan Peru 8-10m SOV\nshp Panoan Peru 35k SOV\ntar Uto-Aztecan Mexico 89k SOV\nTable C.1: Summary of the 10 languages in AmericasNLI.\nD Dataset Information\nD.1 Power Analysis\np1Model p1 p2 Lower Bound Power Upper Bound Powerp2Model\nRandom Baseline 33.33\n38.48 40.33 100 Zero-shot (en)\n37.99 35.80 100 Zero-shot (es)\n43.70 91.38 100 Zero-shot +MLM (en)\n43.85 91.52 100 Zero-shot +MLM (es)\n49.12 99.82 100 Translate-train\n40.22 61.85 100 Translate-test\nZero-shot Baseline 38.48\n43.70 33.66 100 Zero-shot +MLM (en)\n43.85 35.33 100 Zero-shot +MLM (es)\n49.12 87.10 100 Translate-train\n40.22 7.13 99.07 Translate-test\nAdaptation Baseline 43.85 49.12 31.29 100 Translate-train\nTable D.1: Here, we use the simulation approach of Card et al. (2020) to calculate upper and lower bounds for\nthe power of our experiments. We use the average accuracies for each approach, and set n= 750,α = 0.05,r =\n10,000, and bold experiments with well-powered lower bounds.\n6294\nD.2 Dataset Statistics\nLanguage Split Entailment Contradiction Neutral Majority Baseline\naym Test 250 250 250 0.333\nDev 248 248 247 0.334\nbzd Test 250 250 250 0.333\nDev 248 248 247 0.334\ncni Test 250 250 250 0.333\nDev 220 220 218 0.334\ngn Test 250 250 250 0.333\nDev 248 248 247 0.334\nhch Test 250 250 250 0.333\nDev 248 248 247 0.334\nnah Test 246 245 247 0.335\nDev 193 195 197 0.337\noto Test 249 249 250 0.334\nDev 78 75 69 0.351\nquy Test 250 250 250 0.333\nDev 248 248 247 0.334\nshp Test 250 250 250 0.333\nDev 248 248 247 0.334\ntar Test 250 250 250 0.333\nDev 248 248 247 0.334\nTable D.2: Distribution of labels in the test and development sets, per language.\nE Detailed Results\nFT aym bzd cni gn hch nah oto quy shp tar Avg.\nMajority baseline - 33.40 33.40 33.40 33.40 33.40 33.70 35.10 33.40 33.40 33.40 -\nZero-shot\nXLM-R (en) 84.55 38.45 41.59 40.07 40.74 37.82 39.50 43.84 38.67 43.56 36.03 40.03\nXLM-R (es) 80.77 37.73 39.70 37.59 40.06 36.74 37.88 39.94 38.54 38.18 35.89 38.23\nZero-shot w/ adaptation\nXLM-R +MLM (en) - 41.77 39.57 40.93 52.40 41.01 43.25 37.24 62.27 44.86 39.30 44.26\nXLM-R +MLM (es) - 45.26 42.22 40.53 53.52 38.40 42.41 40.24 55.00 40.11 45.89 44.36\nTranslate-train\nXLM-R - 53.61 49.98 45.49 61.28 42.22 53.80 41.44 58.62 53.10 43.01 50.25\nTranslate-test\nXLM-R - 37.73 39.70 37.59 40.06 36.74 37.88 39.94 38.54 38.18 35.89 38.23\nTable E.1: Development set results for zero-shot, translate-train, and translate-test. FT represents the XNLI de-\nvelopment set performance for the ﬁnetuning language and is not included in the average. The majority baseline\nrepresents expected performance when predicting only the majority class of the development set. Random guessing\nwould result in an accuracy of 33.33%.\n6295\nFT aym bzd cni gn hch nah oto quy shp tar Avg.\nZero-shot\nXLM-R (en) -22.21 -2.53 -6.18 -5.51 -6.00 -3.07 -9.53 -5.44 -3.91 -6.85 -2.09 -5.11\nXLM-R (es) -18.51 -3.12 -4.58 -1.96 -3.92 -1.29 -5.38 -5.16 -6.44 -1.60 0.00 -3.35\nZero-shot w/ adaptation\nXLM-R +MLM (en) - -6.44 -5.33 -6.40 -10.04 -3.52 -11.66 -3.07 -17.38 -6.01 -5.02 -7.49\nXLM-R +MLM (es) - -7.60 -5.25 -5.03 -10.54 -3.82 -8.80 -7.66 -14.53 -4.58 -3.78 -7.16\nTranslate-train\nXLM-R - -5.07 -7.69 1.02 -11.29 -0.13 -9.52 -0.18 -7.78 -5.73 -2.57 -4.89\nTranslate-test\nXLM-R - -3.20 2.27 2.62 -3.02 0.53 1.85 -1.07 -3.25 3.16 -0.49 -0.06\nTable E.2: Differences between hypothesis-only and standard results on the test set of AmericasNLI.\nSource ar bg de el en es fr hi ru sw th tr ur vi zh Avg.\nen 71.96 77.65 76.62 75.84 84.55 78.74 78.00 70.02 76.04 64.41 72.04 72.54 66.28 74.38 73.97 74.20\nes 73.49 78.71 77.59 77.05 83.36 80.77 78.83 72.25 77.10 64.60 73.32 73.78 68.44 75.82 75.16 75.35\nTable E.3: Results of zero-shot models on the test set of XNLI. Scores are underlined when the same language\nused for training is used for evaluation as well.\nF Additional Results\nSource Model aym bzd cni gn hch nah oto quy shp tar Avg.\nen\nZero-Shot 36.00 39.20 37.20 40.67 36.80 42.28 36.90 35.73 40.67 36.27 38.17\nZ-S +MLM 41.60 36.53 40.80 51.47 39.87 46.48 37.83 64.53 40.67 40.67 44.05\nZ-S+MLMAUG 45.07 38.67 41.47 52.93 38.53 46.48 33.42 62.00 39.73 40.27 43.86\nes\nZero-Shot 37.87 41.60 37.87 39.47 36.27 39.57 39.04 40.93 38.27 35.33 38.62\nZ-S +MLM 43.87 37.60 38.80 52.27 36.00 45.12 41.58 60.80 41.20 38.80 43.60\nZ-S+MLMAUG 45.20 38.67 39.33 54.27 37.07 44.99 42.65 62.67 37.20 38.67 44.07\n−\nTranslate-Train 49.33 52.00 42.80 55.87 41.07 54.07 36.50 59.87 52.00 43.73 48.72\nT-T +MLM 50.93 51.20 42.27 61.60 44.93 56.10 35.16 63.47 50.00 44.13 49.98\nT-T+MLMAUG 51.07 51.87 44.53 61.07 46.27 53.39 35.96 61.07 52.67 40.67 49.86\nTable F.1: Results from models adapted with augmented data before ﬁnetuning. Zero-shot, zero-shot +MLM, and\ntranslate-train results are taken from the main experiments, however we only take results from the run correspond-\ning to the same random seed as the newly trained models.\n6296\nF.1 Early Stopping\nWhile early stopping is vital for machine learn-\ning, in the case of zero-shot learning hand-labeled\ndevelopment sets in the target language are often as-\nsumed to be unavailable (Kann et al., 2019). Thus,\nin our main experiments we use either a machine-\ntranslated development set or one from a high-\nresource language. In both cases, performance on\nthe development set is an imperfect signal for how\nthe model will ultimately perform. To explore how\nthis affects ﬁnal performance, we present the dif-\nference in results for translate-train models when\nan oracle translation is used for early stopping in\nTable F.2. We ﬁnd that performance is 2.34 points\nhigher on average, with a maximum difference of\n7.28 points for Asháninka, suggesting that creating\nways to better approximate a development set may\nlead to higher performance.\naym bzd cni gn hch nah oto quy shp tarAvg.\n2.13 0.98 7.28 0.58 0.53 2.12 3.03 1.42 0.93 4.362.34\nTable F.2: Difference between translate-train results ob-\ntained using the oracle development set and the trans-\nlated development set for early stopping.\nF.2 Data Augmentation with Translated Data\nDue to the success of translate-train, we also in-\nvestigate if we can improve performance further\nby creating data for language adaptation (+MLM)\nthrough translation. To do so, we create a random\nsample of sentences taken from Spanish Wikipedia,\nand translate them into each target language. The\nsample is sized to contain the same number of sub-\nword tokens as the original pretraining data. We\ncombine the original pretraining data and trans-\nlated data to create a new set of sentences for con-\ntinued pretraining, doubling the size of the origi-\nnal. We also ﬁnetune the original adapted models\nusing translate-train. We present results in Table\nF.1. When ﬁnetuning on English and translate-train\ndata, the average performance is highest when us-\ning the models adapted on the original data. When\nﬁnetuning on Spanish, the models adapted on aug-\nmented data are best on average. While on av-\nerage performance increases are not drastic, for\nsome languages the performance increase is no-\ntable, and these mixed and/or augmented models\nmay be worth looking into when interested in a\nparticular language.\nF.3 XLM-R Large\nIn this section we provide results for XLM-R Large.\nDue to computational restrictions, we slightly mod-\nify the experimental setup from the main experi-\nments: we use mixed precision training and a more\naggressive early stopping patience of 3 evaluation\nsteps. Additionally, we use a learning rate of 5e-6\nfor all ﬁnetuning experiments, as we found that the\noriginal learning rate of 2e-5 failed to converge.\nHowever, even when using the modiﬁed hyperpa-\nrameters, we experience some instability during\ntraining. The zero-shot model trained on Spanish\ndata did not converge with the original random\nseed, but successfully trained after changing the\nseed. For translate-train, the models trained on\nAsháninka and Otomí failed to converge, regard-\nless of the seed used, and further hyperparameter\ntuning will be required, which we leave for future\nwork.\nIn this experiment, we can see that the results\nare more varied in comparison to the main results.\nTranslate-train achieves the highest performance\nfor ﬁve languages, with the adapted models achiev-\ning a combined highest performance for the remain-\ning ﬁve. On average, the adapted model ﬁnetuned\non English labeled data achieved the highest per-\nformance, followed closely by the other adapted\nmodel, and the translate-train model. This indi-\ncates that translate-train may be a viable approach\nwhen faced with limited compute, but might also\nhave a restrictive upper limit on performance; in\ncontrast, adaptation may allow for more potential\nperformance gain, especially when larger models\nand datasets are available. Interestingly, when con-\nsidering average performances across only the lan-\nguages for which all models converged (i.e. remov-\ning Asháninka and Otomí from the calculation),\nwe ﬁnd that translate-train offers an average per-\nformance of 51.91%, while adaptation approaches\nachieve 49.39% and 49.83% accuracy on average.\nComparing XLM-R Large to XLM-R Base in\nTable F.4, we see that for all but one language\nthe Large model outperforms the Base model in\nall adaptation and zero-shot runs. Notably, the\nBase model trained on translated data outperforms\nthe Large model, and retains the highest overall\nperformance across all languages and models.\n6297\nFT aym bzd cni gn hch nah oto quy shp tar Avg.\nZero-shot\nXLM-R Large (en) 89.04 40.67 41.33 43.07 42.93 39.20 45.39 42.25 42.13 48.27 40.53 42.58\nXLM-R Large (es) 89.84 38.67 41.60 41.20 42.00 37.20 41.46 42.38 41.33 43.47 36.00 40.53\nZero-shot w/ adaptation\nXLM-R Large +MLM (en) - 54.80 43.87 46.67 59.87 43.60 43.36 44.79 64.80 43.07 41.73 48.66\nXLM-R Large +MLM (es) - 54.93 40.40 42.93 61.07 44.67 45.53 42.51 68.00 43.60 40.40 48.40\nTranslate-train\nXLM-R Large - 51.47 50.13 33.33 61.20 42.00 55.28 33.42 61.47 49.87 43.87 48.20\nTranslate-test\nXLM-R Large - 38.67 40.93 35.73 50.80 38.93 39.97 32.62 47.87 39.33 35.60 40.05\nTable F.3: Results when using XLM-R Large. Underlined results indicate runs which did not converge on the\ntraining data.\nFT aym bzd cni gn hch nah oto quy shp tar Avg.\nZero-shot\nEnglish 4.49 4.54 1.68 5.16 3.46 2.00 2.80 4.46 4.89 7.82 4.17 4.10\nSpanish 9.07 1.42 2.22 3.91 2.75 1.38 2.48 4.06 1.82 5.07 0.27 2.54\nZero-shot w/ adaptation\n+MLM (en) - 11.29 5.74 7.20 7.43 6.35 -2.85 7.76 3.02 1.73 1.91 4.96\n+MLM (es) - 11.06 0.35 4.17 8.80 6.85 1.36 1.96 5.60 3.42 1.95 4.55\nTranslate-train - 1.47 -1.29 -9.12 2.31 -1.20 -0.05 -2.59 1.56 -2.13 1.83 -0.92\nTranslate-test - -1.06 0.53 1.02 4.18 0.93 -1.40 -2.67 -3.51 -0.18 0.44 -0.17\nTable F.4: Difference in performance between XLM-R Large and Base.\n6298\nLanguage Example\naym\nP: Mä jan walt ’ awinakax utjkaniti?\nH: Iglesia JI JI ukax XIFlo XICI ukax XIIII ukan mä jach ’a pacha.\nP: Aka qillqatax Crownwn Squareareareare ukax iwayi, ‘ Ñalacio ‘ ‘ ‘ ñoquis ukch ’ añataki.\nH: Plaza de Plaza de palacio palacio palacio äwipat uñt ’ayi.\nbzd\nP: Ye’r ye’ alà alà dör ye’ alà tã’ alàshshshshshshöö ?\nH: Káxkkk e’ tã káx batà batà ã káx batà ã .\nP: Káx i’r i’ i’ ã káx i’ ulàshshshshshshshshshshshshshshshshsh .\nH: Kéqéqwöwöwöwöwöwöwö ulà ulà ulà ulà wa .\ncni\nP:APAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAPAP\nAPAPAPAPAPAPAPAPAPAPAPA)))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n))))))))))))))))))))))))))))))))))))))))))) O O O O O O O ObibibibibibibiIIIIIIIIIIIIIIIIIII\nH: Ibibibibiti obibiti obibi. Ababababa\nP: b. Akobiro ayiro ayiro ayiro nija Jebabentirori Anampiki.\nH: Itititititititititititi.\ngn\nP: Peteî paseo corto imbarete caminata norte gotyo ha’e pueblo j? Sus , peteî tupão particularmente siglo XIX .\nH: Tupão tavaguasu Jesús omopu’ã siglo XIX .\nP: Péicha Crown Square oime palacio real , kuimba’e preciado tetãme , joy Escocia .\nH: Plaza de la corona cuenta palacio real .\nhch\nP: xewit+ta m+k+ wa+ka xewit+ x+ka xewit+ x+ka mu’at+a.\nH: ’aix+ ’aix+ ti’at+ x+t+ x+a mu’at+ x+a.\nP: wa+ka m+k+ ’aix+ pureh+k+t+a de oro.\nH: ’ik+ p+h+k+ palacio palacio palacio palacio.\nnah\nP: See tosaasaanil , see tosaasaanil , see tosaasaanil . See tosaasaanil , see tosaasaanil , see tosaasaanil .\nH: Yn ipan ciudad de Jesús la Yglesia de Jesús yn ipan in omoteneuh xihuitl de Jesús .\nP: Auh ynic patlahuac cenpohualmatl ypan in yn oncan tecpan quiyahuac yn oncan tecpan quiyahuac yn tecpan\nquiyahuac yn oncan tecpan quiyahuac .\nH: In tlapoalli ica tlapoalli ica tecpan palacio .\noto\nP: Ra nge’a mi b’et¯’em¯ ’i ha ra thuhu ra thuhu ra thuhu ra thuhu ra ñ’ot¯’et¯’et¯’a ra thuhu ra thuhu ra thuhu ra thuhu ra\nthuhu ra thuhu ra thuhu ra hnini .\nH: Nu’u xki tsoh¯o nuni M’onda .\nP: Ra nge’a ra thuhu ra b’ui¯ ha ra thuhu ra thuhu ra thuhu ra thuhu ra thuhu ra thuhu ra thuhu ra thuhu ra thuhu ra\nthuhu ra thuhu ra thuhu ra thuhu ra thuhu ,\nH: Ra nge’a ra b’em¯ ’em¯ ’em¯ ’i .\nquy\nP: Asiria nacionpa norte lawninpim, Sus X00 watapa norte lawninpi kaq Sus X00 watakunapi religionniyoq punta\napaqkunawan hukllawakurqaku.\nH: Jesuspa tiemponpi iglesia\nP: Crown Squarepa hichpanpim tarikunku palaciopi, chayqa Escocia nacionpa chawpinpim kachkan\nH: Alemania nacionpa Plaza sutiyoq runam qollqepaq apuestaspa palaciopi cuentallikun\nshp\nP: Westiora yoxan yoxanya riki ea, jainxon westiora westiora westiora westiora westioraya iki.\nH: Iririririririririririririririririricancancancancancancan.\nP: Nato yobinbinki jawe ati iki, jainxon min keni raometi iki, jainxon westiora westiora westiora raomeomeai,\njainxon min kenkin.\nH:Chomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomo\nmomomomomomomomomomomomomomomomomomomomomomomomomomomomomomo-\nmomomomomomomomom omomomomomomomomomomomomomomomomomomomomomomomomomomo-\nmomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomomin\ntar\nP: ( 2 ) ¿ chí mu ´re’pá ? ¿ chí mu´re’pá ? ¿ atza be’pá ? ¿ chí mu´re’pá ?\nH: a’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko ba’rí ko’rí ko pe pe\npe pe pe pe pe pe pe pe pe pe pe pe pe pe pe pe pe a’rí mi mi mi mi mi mi mi mi mi mi mi mi mi mi mi mi’rí ko’rí\nko’rí ko´re’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko’rí ko mapu mapu mapu mapu´re’rí ko’rí ko’rí ko’rí ko’rí\nko’rí ko’rí ko´re’rí ko´re’rí ko´re’rí ko’rí ko´re’rí ko´re’rí ko´re’rí ko’rí ko´re’rí ko ba’rí\nP: ( 2 ) a ) pe ´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’rí ko , pe pe pe´re’pá\n´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá\n´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá bo’pá bo’pá´re’pá´re’pá´re’pá´re’pápápá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá´re’pá\n´re’pá´re’pá´re’pá´re’pá´re’rí ko ba’pá´re’pá´re’pá\nH: ( 2 ) ( b ) pe ´re’chí na’chí na’chí\nTable D.3: Two randomly selected translate-train examples.\n6299",
  "topic": "Zero (linguistics)",
  "concepts": [
    {
      "name": "Zero (linguistics)",
      "score": 0.5292553305625916
    },
    {
      "name": "Linguistics",
      "score": 0.4790160059928894
    },
    {
      "name": "Computer science",
      "score": 0.4778282642364502
    },
    {
      "name": "Computational linguistics",
      "score": 0.471521258354187
    },
    {
      "name": "Natural language processing",
      "score": 0.4575135111808777
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41877421736717224
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.41681402921676636
    },
    {
      "name": "History",
      "score": 0.3550010919570923
    },
    {
      "name": "Humanities",
      "score": 0.32711347937583923
    },
    {
      "name": "Library science",
      "score": 0.3218645453453064
    },
    {
      "name": "Art",
      "score": 0.26600444316864014
    },
    {
      "name": "Philosophy",
      "score": 0.25939249992370605
    },
    {
      "name": "Physics",
      "score": 0.18835121393203735
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}