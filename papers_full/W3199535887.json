{
  "title": "What Vision-Language Models `See' when they See Scenes",
  "url": "https://openalex.org/W3199535887",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226382021",
      "name": "Cafagna, Michele",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3113635813",
      "name": "van Deemter, Kees",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3178323690",
      "name": "Gatt, Albert",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2963530300",
    "https://openalex.org/W3034837210",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3110662498",
    "https://openalex.org/W3101860364",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W3110909889",
    "https://openalex.org/W3094051456",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2741631785",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3020712669",
    "https://openalex.org/W3162316477",
    "https://openalex.org/W2130569706",
    "https://openalex.org/W3006320872",
    "https://openalex.org/W2106848651",
    "https://openalex.org/W2036167211",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W3131151401",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3101638716",
    "https://openalex.org/W2538243221",
    "https://openalex.org/W3106477503",
    "https://openalex.org/W3163542683",
    "https://openalex.org/W3034727271",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W3171654275",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3177487519",
    "https://openalex.org/W2138381344",
    "https://openalex.org/W3110570034",
    "https://openalex.org/W3095670406",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3116952214",
    "https://openalex.org/W2159564241",
    "https://openalex.org/W3094503863",
    "https://openalex.org/W2144764737",
    "https://openalex.org/W2613718673"
  ],
  "abstract": "Images can be described in terms of the objects they contain, or in terms of the types of scene or place that they instantiate. In this paper we address to what extent pretrained Vision and Language models can learn to align descriptions of both types with images. We compare 3 state-of-the-art models, VisualBERT, LXMERT and CLIP. We find that (i) V&amp;L models are susceptible to stylistic biases acquired during pretraining; (ii) only CLIP performs consistently well on both object- and scene-level descriptions. A follow-up ablation study shows that CLIP uses object-level information in the visual modality to align with scene-level textual descriptions.",
  "full_text": "What Vision-Language Models ‘See’ when they See Scenes\nMichele Cafagna1 Kees van Deemter2 Albert Gatt1,2\n1University of Malta, Institute of Linguistics and Language Technology\n2Universiteit Utrecht, Information and Computing Sciences\nmichele.cafagna@um.edu.mt\n{c.j.vandeemter,a.gatt}@uu.nl\nAbstract\nImages can be described in terms of the objects\nthey contain, or in terms of the types of scene\nor place that they instantiate. In this paper we\naddress to what extent pretrained Vision and\nLanguage models can learn to align descrip-\ntions of both types with images. We com-\npare 3 state-of-the-art models, VisualBERT,\nLXMERT and CLIP. We ﬁnd that (i) V&L\nmodels are susceptible to stylistic biases ac-\nquired during pretraining; (ii) only CLIP per-\nforms consistently well on both object- and\nscene-level descriptions. A follow-up ablation\nstudy shows that CLIP uses object-level infor-\nmation in the visual modality to align with\nscene-level textual descriptions.\n1 Introduction\nGrounding symbols in perception (Harnad, 1990)\nis a crucial step towards achieving full understand-\ning of natural language (Bender and Koller, 2020;\nBisk et al., 2020). This endeavour has received new\nimpetus through the development of pretrained Vi-\nsion and Language (V&L) models (e.g. Lu et al.,\n2019; Tan and Bansal, 2019; Li et al., 2019; Chen\net al., 2020; Li et al., 2020a; Su et al., 2020; Wang\net al., 2020; Luo et al., 2020; Li et al., 2021; Huang\net al., 2021; Radford et al., 2021). Similarly to\nunimodal language models such as BERT (Devlin\net al., 2019), V&L models are intended to be task-\nagnostic and are extensively pretrained on paired\nimage-text data, achieving good performance on\nseveral tasks after ﬁnetuning (e.g. Lu et al., 2020;\nLi et al., 2020c; Kim et al., 2021). Pretraining usu-\nally includes an image-text alignment task to dis-\ncover implicit cross-modal relationships. Although\nthe importance of this task is widely recognized\nand adopted during model pretraining, it is unclear\nhow the models perform on it, since they are usu-\nally evaluated on downstream tasks.\nThe data used for V&L pretraining usually con-\ntains highly descriptive text which mentions ob-\njects and their spatial relationships. For instance,\nLN: This is the picture of a stadium. In the foreground there\nis a person [. . . ] At the back there are group of people sitting\n[. . . ].\nCOCO: a baseball player getting ready to swing at a baseball\ngame in a stadium packed with people.\nHL1K: the picture is shot in a baseball ﬁeld\nFigure 1:An example of scene with COCO and Localized\nNarrative (LN) object-level captions, versus HL1K scene-level\ndescription (Section 3)\nthe COCO (Chen et al., 2015) and Localized Nar-\nratives (LN; Pont-Tuset et al., 2019) captions for\nFigure 1 are of this type, though they differ stylis-\ntically. By contrast, the third caption in the ﬁgure,\nfrom the novel HL1K dataset introduced in Section\n3 below, is what we refer to as ‘scene-level’, focus-\ning on what type of scene or location is depicted.\nNote that both the object- and scene-level descrip-\ntions in the Figure describe the picture, albeit in\ndifferent ways. Indeed, it would be expected that,\nfor a V&L model to display true grounding capabil-\nities, it should be able to identify both types of de-\nscriptions as true of a scene. For models which do\ndisplay this cabability, a natural follow-up question\nis whether their representations capture interesting\nconnections between scenes on the one hand, and\nthe objects within them on the other.\nResearch on human perception suggests that hu-\nmans do not perceive scenes exclusively in terms of\nthe objects they contain, and that visual salience is\nnot exclusively determined by bottom-up features\nsuch as colour and texture. Rather, visual stimuli\nare considered ‘scenes’ because their elements con-\narXiv:2109.07301v1  [cs.CL]  15 Sep 2021\nstitute a meaningful whole, both in terms of their\ncontents (e.g. one expects an oven in a kitchen, but\nnot in a living room) and in terms of their spatial\narrangement (e.g. ovens do not typically hang from\nthe ceiling) (Malcolm et al., 2016). These observa-\ntions gave impetus to work showing that violations\nof scene ‘semantics’ (content) and ‘syntax’ (spatial\narrangement) exact a cognitive cost during percep-\ntion (e.g. Biederman et al., 1982; Võ and Wolfe,\n2013). A related strand of modeling research in\ncomputer vision has also shown that scene-level\npriors generate expectations about objects and their\nconﬁgurations, impacting the salience of objects in\na way that classical, feature-based models of atten-\ntion (e.g. Itti and Koch, 2001) would not predict\n(Torralba et al., 2006; Oliva and Torralba, 2007).\nIndeed, the problem of linking low-level features\nwith high-level semantic information is an instance\nof the problem referred to as the ‘semantic gap’ in\ncomputer vision (Ma et al., 2010).\nIn this paper we investigate whether V&L mod-\nels are able to handle object-level and scene-level\ndescriptions equally well. A positive answer to\nthis question would suggest that such models are\nlearning useful associations between the elements\nof a scene and the overall scene type, as captured\nin the textual descriptions.\nWe perform an analysis in a zero-shot setting\non three state-of-the-art pretrained Vision and Lan-\nguage models. To our knowledge, this is the ﬁrst\nsystematic comparison of model capabilities on\nobject- versus scene-level grounding. The goal of\nthis study is therefore not to establish new SOTA re-\nsults, but to further our understanding of what V&L\nmodels learn, as a function of the data they are pre-\ntrained on and the model architecture. Therefore\nwe choose three models differing in many settings,\n(including training set size, architecture, number\nof parameters and model size). All of the mod-\nels are however optimized on the image-sentence\nalignment task.\nWe ﬁnd that only one of the models under com-\nparison, CLIP (Radford et al., 2021), performs\nconsistently well on both object- and scene-level\nimage-text matching. We then investigate this\nmodel’s abilities in depth, using an ablation method\nto identify the elements of a text and/or an image\nwhich contribute to these abilities.\nTraining size Model size Pretraining\n(# image-sentence pairs)(# parameters)Objectives\nCLIP 400M 151M ISA\nVisualBERT 330k 112M ISA, MLM\nLXMert 9.18M 228M ISA, MLM\nMOP, VQA\nTable 1:Comparison of training settings for the three models\n(ISA: Image-Sentence Alignment, MLM: Masked Language\nModeling, MOP: Masked Object Prediction, VQA: Visual\nQuestion Answering)\n2 Models\nCurrent V&L models typically combine textual and\nvisual features in a single or a dual-stream archi-\ntecture. Though the two architectures have been\nfound to perform roughly at par Bugliarello et al.\n(2020), in this paper we include representatives of\nboth. We also include a third model which differs\nin structure and is trained on a much larger and\nmore varied dataset. Table 1 gives an overview of\nsome of the properties of the models we consider.\nLXMERT (Tan and Bansal, 2019) is a dual-\nstream model, which encodes text and visual fea-\ntures in parallel, combining them using cross-\nmodal layers. LXMERT is trained on COCO cap-\ntions (Chen et al., 2015) as well as a variety of VQA\ndatasets, with an image-text alignment objective,\namong others.\nVisualBERT (Li et al., 2019) is a single-stream,\nmultimodal version of BERT (Devlin et al., 2019),\nwith a Transformer stack to encode image regions\nand linguistic features and align them via self-\nattention. It is pretrained on COCO captions (Chen\net al., 2015). Image-text alignment is conceived as\nan extension of the next-sentence prediction task\nin unimodal BERT. Thus, VisualBERT expects an\nimage and a correct caption, together with a sec-\nond caption, with the goal of determining whether\nthe second caption matches the ⟨image, correct\ncaption⟩pair.\nCLIP (Radford et al., 2021) combines a trans-\nformer encoder for text with an image encoder\nbased on Visual Transformer (Dosovitskiy et al.,\n2020), jointly trained using contrastive learning to\nmaximise scores for aligned image-text pairs. CLIP\nis trained on around 400m pairs sourced from the\nInternet, a strategy similar to the web-scale training\napproach used for unimodal models such as GPT-3\n(Brown et al., 2020). We note that the visual back-\nbone for this model differs from that of LXMERT\nand VisualBERT, both of which use Faster-RCNN\n(Ren et al., 2015).\n3 Data\nWe use four different datasets for our experiments.\nDataset statistics are provided in the Appendix.\nLocalized Narratives Localized Narratives\n(LN) Pont-Tuset et al. (2019) is a V&L dataset\nharvested by transcribing speech from annotators\nwho were instructed to give object-by-object\ndescriptions as they moved a mouse over image\nregions. LN captions tend to be highly detailed\nand stylistically similar to speech. We use LN as a\nsource of object-level captions. The images in LN\ncome from preexisting datasets; this allows us to\nalign LN captions with images and captions from\ndatasets such as COCO and ADE20K.\nADE20K ADE20K (Zhou et al., 2017) is a com-\nputer vision dataset containing 20k images compre-\nhensively annotated with objects, parts and scene\nlabels. We use ADE20K as a source of scene-level\ncaptions. For our experiments, we ﬁlter out images\nwith scenes which in the dataset are labelled as\nunknown. We produce captions for each image\nusing a simple template-based generation method\n(see Appendix). We align ADE20K images and\ntheir scene-level descriptions, to the corresponding\nobject-level captions in LN.\nCOCO COCO (Lin et al., 2014) consists of im-\nages paired with captions and object annotations.\nLN captions are also available for the same images.\nWe use images and captions from the 2017 COCO\nvalidation split, as well as the corresponding LN\ncaptions.\nHL1K H igh Level Scenes - 1k (HL1K) is a new\ndataset and part of an ongoing data collection effort.\nHL1K is composed of 1k images, each depicting\nat least one person, sampled from the 2014 COCO\ntrain split. We crowd-sourced three annotations\nper image on Amazon Mechanical Turk, showing\ncrowd workers the image and asking them to write\na description in response to the question Where is\nthe picture taken? It was made clear to annotators\nthat the answer to this question should bring to bear\ntheir knowledge of typical, or common, scenes.\nDescriptions were corrected for typos using the\nNeuspell Toolkit (Jayanthi et al., 2020). Finally,\nwe paired our scene-level HL1K captions with the\npreviously available COCO and LN object-level\nLXMERT CLIP V isualBERT\nObject\nADE20k + LN 28.4 96.8 39.0\nCOCO + LN 59.1 98.7 65.2\nCOCO Cap. 79.3 99.1 64.4\nScene ADE20k 58.0 97.6 17.3\nHL1K 45.5 91.5 55.3\nTable 2:Image-sentence alignment accuracies on object-level\nand scene-level captions. Chance performance is at 50%. (LN\n= Localized Narratives)\ncaptions. Figure 1 is an example; further examples\nare provided in the Appendix.\n4 Experiments\nWe ﬁrst test models in the image-text alignment\ntask on both object- and scene-level descriptions.\nWe then delve more deeply into the performance\nof the best model, using an ablation task. Since we\nare interested in the capabilities of the pretrained\nmodels, we do not ﬁnetune them.\n4.1 Image-sentence alignment\nFor the image-text alignment task, we use the mod-\nels’ pretrained alignment head to predict whether\na scene-level or object-level caption correctly de-\nscribes an image, or not. 1 Table 2 shows that\nLXMERT and VisualBERT perform adequately on\nobject-level COCO Captions, though performance\nis lower than expected given that they were pre-\ntrained on this dataset. In the case of LXMERT,\none possible explanation is catastrophic forgetting,\narising from the fact that this model is pretrained\nfor its ﬁnal ten epochs on VQA (similar observa-\ntions are made by Parcabalescu et al., 2021). For\nboth models, performance drops dramatically on\nLN captions. This is likely due to a stylistic differ-\nence: LN captions are longer than COCO, more\ndiscursive and contain disﬂuencies. In contrast,\nCLIP performs close to ceiling on all three datasets,\npossibly reﬂecting the beneﬁts accrued from the\nsize and diversity of its pretraining data.\nOn scene-level captions, performance is some-\nwhat above chance for LXMERT on ADE20k\ntemplate-based descriptions, and for VisualBERT\non HL1K. Otherwise, performance is below 50%\nfor both models. Once again, CLIP performs above\n90%, though there is a drop in performance from\nthe template-based ADE20k descriptions to human-\nauthored HL1K scene-level captions, possibly due\n1Note that this setting is the same used by the models is\ntheir pretraining.\nto the more predictable nature of the former.\n4.2 Ablation experiments on CLIP\nSince CLIP is the only one of the three models\nwhich is successful at matching scene-level and\nobject-level captions to images, we probe its ca-\npabilities further, paying particular attention to\nthe question whether CLIP links scene types (e.g.\nkitchen) to scene contents (e.g. oven, pizza) in\nimage-text matching.\nWhereas a standard image-text alignment setup\ncompares the model’s success at identifying actual\nversus random captions, here we directly compare\nthe preference of the model for scene- versus object-\nlevel descriptions, as a function of (i) the entities\nmentioned in the object-level caption; (ii) the enti-\nties visible in an image. To this end, we use textual\nand visual ablation, described below (see Appendix\nfor further examples and details).\nTextual ablation A textual ablation is performed\nby removing noun phrases (NPs) in the object-level\ncaption. For example, for the COCO caption in\nFigure 1, one ablated version would remove the\nNP a baseball player resulting in getting ready to\nswing at a baseball game . . .. For a given image i\nwith object-level caption o and scene-level caption\ns, we compare how P(o|i) – CLIP’s estimate of\nthe probability that o matches i – changes as NPs\nare removed from o, and to what extent this causes\nCLIP to assign higher probability P(s|i), to s as\nthe match for i. We report two comparisons, one\non LN captions versus ADE20K scene template\ndescriptions; and one on COCO captions against\nHL1K scene-level descriptions.\nTo control for possible loss of grammatical-\nity after ablation, we score ablated captions with\nGRUEN (Zhu and Bhat, 2020), a BERT-based\nmodel which has been shown to yield scores that\ncorrelate highly with human judgments. 2 CLIP\nprobabilities for ablated textual captions yielded\na very low correlation with grammaticality (Pear-\nson’sr = 0.1, p < .01) suggesting that grammati-\ncality did not affect the scores.\nVisual ablation Given an image, we detect the\nbounding regions of entities which are also men-\ntioned in the corresponding object-level caption,\nand occlude them with a greyscale mask. Once\n2GRUEN returns a combined score consisting of a linear\ncombinaton of Grammaticality, Focus and Coherence. Here,\nwe use only the Grammaticality scores.\nADE20k HL1K\nLN Scene COCO Scene\nNo ablation 55.6 44.4 95.7 4.3\nT 22.0 78.0 67.2 32.8\nV 74.9 25.1 71.2 28.8\nV+T 68.4 31.6 63.3 36.7\nTable 3:Preference expressed in percentage for object-level\nversus scene-level (sub-columns), where there is no ablation,\nT(extual) ablation, V(isual) ablation, or both (V+T) for a each\ndataset (columns).\nagain, we are interested in whether CLIP’s esti-\nmate of the alignment probability of object- versus\nscene-level captions, changes as elements of the\nvisual input are masked.\nVision vs text Without ablation, the model as-\nsigns higher probability to object-level descrip-\ntions, suggesting that CLIP assigns higher conﬁ-\ndence to an image-text pair when the text focuses\non objects. This preference is far more marked\nfor COCO/HL1K, in line with the observation (Ta-\nble 2) that HL1K scene descriptions are somewhat\nmore challenging for this model. As entity-level\ninformation is removed from the object-level cap-\ntion (row T), the model’s preference swings to\nthe scene-level caption, suggesting that the model\nleverages the visual information to align with the\nscene description. Visual ablation (V), by contrast,\nresults in the opposite tendency: when entities are\noccluded in the image, the model shows greater\npreference for object-level captions. This suggests\nthat the way CLIP ‘understands’ scene-level de-\nscriptions depends on which entities are visible. If\nboth sources of information are ablated (V+T), the\npreference once again veers towards object-level\ncaptions.\nScenes vs. entities These ﬁndings suggest that\nCLIP reasons about scenes on the basis of salient\nobjects within them. To investigate this further, we\nuse scene labels extracted from the HL1K captions\nand the object detections produced for the visual\nablation. For a scene label s and entity label e,\nwe compute P(s|e). Details of this computation\nare given in the Appendix. For example, Table 4\nshows the most likely entities in park scenes. For\nall images with at least three detected entities, we\nconsider the image-sentence alignment probability\nassigned by CLIP to the scene-level description,\nwhen the top 1, 2 or 3 most likely entities in the\nENTITY P(s|e)\nskateboard 0.28\nbench 0.22\nfrisbee 0.11\nkite 0.11\norange 0.11\numbrella 0.09\nperson 0.07\nTable 4: Top entities in\npark scenes.\nFigure 2: CLIP scene-level\ndescription probabilities after\nmasking top 1-3 entities. Error\nbars represent standard devia-\ntions.\nscene are masked. Figure 2 displays a linear trend,\nwith the probability assigned by CLIP to the scene-\nlevel description dropping as more likely entities\nare removed.3 Thus, when CLIP aligns images\nwith scenes, it is relying on object-level informa-\ntion in the visual modality. This explains why the\nremoval of object mentions in text results in higher\npreference for scene-level descriptions, since the\nobjects are detectable in the image. By the same\ntoken, masking objects in images causes the model\nto rely more on the entity-level information in the\ntext.\nSingle word comparison So far, our analysis\nsuggests that CLIP reasons about scenes based on\nobject-level information. However, the length of\nthe caption might be a possible confounding factor.\nSome of our results might simply be due to the\nmodel assigning a higher alignment probability to\na caption which is longer or more informative. This\nwould be an alternative explanation for the changes\nobserved above in the alignment probabilities after\ntextual ablation.\nTo account for this, we replicate the alignment\nexperiment using single words. We use the scene\nlabels extracted from HL1K scene-level descrip-\ntions and identify the top three most likely entities\nin a given scene, as identiﬁed in the previous ex-\nperiment (see Figure 2). Given an image, we com-\npare image-text alignment probabilities in CLIP\nfor single-word object labels (e.g. motorbike) and\n3A one-way ANOV A comparing the change in log proba-\nbility as 1, 2 or 3 entities are removed showed that the differ-\nence is signiﬁcant (F(2, 156) = 4.25, p <0.05).\nresort: 3% person: 96%\nresort: 99% snowboard: 1%\nFigure 3:Scene vs entity one-to-one comparison. In the top\nimage, there are many people in the foreground and the entity\nperson is preferred over the scene label resort. At the bottom,\npeople are snowboarding in the background and the scene\nlabel is preferred over the the entity (snowboard).\nsingle-word scene labels (e.g. road).\nIn this setting, CLIP displays a moderate prefer-\nence for scene labels (63%), suggesting that such\nlabels are more informative than object-level la-\nbels, for the one-word alignment task. Moreover, a\nqualitative analysis shows that CLIP prefers object\nlabels when the images have strongly foregrounded\nentities with little background visibility, as shown\nin Fig 3.\n5 Related work\nLarge, pretrained models are often analysed via\nprobe tasks or through an investigation of their at-\ntention heads (see Belinkov and Glass, 2019, for\na survey). For example, Li et al. (2020b) consider\nVisualBERT’s attention heads in a manner similar\nto Clark et al. (2019), showing that it is able to\nground entities and syntactic relations (see also Il-\nharco et al., 2020; Dahlgren Lindström et al., 2020).\nHendricks and Nematzadeh (2021) similarly seek\nto obtain an in-depth understanding of the represen-\ntations learned by V&L models, ﬁnding that they\nfail to ground verbs in visual data, compared to\nother morphosyntactic categories.\nThe ability of V&L models to reason with a com-\nbination of linguistic and visual cues is explored\nthrough tasks such as VCR (Zellers et al., 2019),\nSW AG (Zellers et al., 2018) and NLVR (Suhr et al.,\n2017, 2019). Pezzelle et al. (2020), in work com-\nplementary to our own, address the relationship\nbetween visual and textual modalities, exploring a\ntask in which the text does not provide an object-\nlevel description of an image. In work presented\nconcurrently with our own, Frank et al. (2021) pro-\npose an ablation-based methodology to evaluate the\nextent to which multimodal models rely on visual\nand/or textual information to perform grounding.\nAlthough scene recognition is a central task in\ncomputer vision, there has been little work explor-\ning the capabilities of models to link scene- and\nobject-level representations. Some precedent for\nthe concerns addressed in this paper are found in\nthe image captioning literature. For example, an\ninﬂuential proposal by (Anderson et al., 2018) com-\nbines top-down and bottom-up attention to improve\nthe quality of image descriptions, while CapWAP\n(Fisch et al., 2020) conditions image captioning\non questions that determine which information is\nrelevant to current communicative needs, going be-\nyond object-level description. Closer to the scope\nof the work presented here, a recent pretrained\nV&L model, SemVLP (Li et al., 2021), combines\nsingle- and dual-streams for feature-level and high-\nlevel semantic alignment. We plan to investigate\nthis model further in future work.\n6 Conclusions\nGrounding language in vision requires V&L mod-\nels to capture the relation between ‘high-level’ char-\nacterisations of scenes and the entities they contain.\nThe experiments in this paper suggest that when\nmodels do this, they rely on object-level informa-\ntion in the visual modality, to link images to scene\ndescriptions in the textual modality. This is partly\ndependent on the probability of entities occurring\nin scenes.\nHowever, this is not an ability that all models\nhave: LXMERT and VisualBERT perform poorly\non scene-level descriptions and also on object-level\ncaptions which are stylistically different from the\nones they were pretrained on, whereas CLIP han-\ndles both object- and scene-level captions. We\nnote that for LXMERT and VisualBERT, testing on\nADE20k, Localized Narratives and HL1K amounts\nto a zero-shot setting. With the exception of HL1K,\na new dataset, it is an open question whether this\nis true of CLIP. Since this model is trained on web-\nscale data, it is difﬁcult to ascertain that the train-\ning and test data in our experiments are disjoint;\nas Bender et al. (2021) recently argued, with such\npre-training strategies, training data is often unfath-\nomable. On the other hand, our ﬁndings also show\nthat, at least as far as object and scene-level ground-\ning is concerned, model size is not a determining\nfactor. As shown in Table 1, CLIP is smaller in\nterms of number of parameters than LXMERT.\nApart from the sheer volume of pre-training data\nused in CLIP, we believe that two additional fac-\ntors contribute to its success. First, its contrastive\nlearning objective may result in greater sensitivity\nto ﬁne-grained distinctions between captions, when\nthe model computes the likelihood with which a\ntext matches a given image. A second important\nfeature of CLIP is its visual backbone, which (in\nthe version used in this paper) is based on Visual\nTransformer (ViT Dosovitskiy et al., 2020). By\ncontrast, the visual component in LXMERT and\nVisualBERT is CNN-based. Recent work by Tuli\net al. (2021) has shown that the attention-based\nViT architecture may be more consistent with char-\nacteristics of human vision than a convolutional\nnetwork. This could partially underlie the model’s\nability to use object-level information in an im-\nage to align to scene-level captions. The extent to\nwhich the visual backbone of V&L models impacts\ntheir grounding capabilities is a topic that should\nbe further explored.\n7 Ethical considerations\nFor the studies presented here, we used a new\ndataset, HL1K, collected using the Amazon Me-\nchanical Turk crowdsourcing platform. For the\ndata collection, participants were shown images\nand asked to answer questions such as Where is\nthe picture taken? Answers took the form of short\nstatements. Workers were paid at the rate of e0.03\nper item, an amount we consider equitable for the\nwork involved, and in line with rates for similar\ntasks. No sensitive or identifying information was\ncollected. All other data and models used are pub-\nlicly available. The HL1K dataset will be made\navailable upon publication.\nAcknowledgments\nThis project has received funding from the Eu-\nropean Union’s Horizon 2020 research and inno-\nvation programme under the Marie Skłodowska-\nCurie grant agreement No 860621.\nReferences\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 6077–6086.\nYonatan Belinkov and James Glass. 2019. Analysis\nMethods in Neural Language Processing: A Survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\nDangers of Stochastic Parrots: Can Language Mod-\nels Be Too Big? In Proceedings of the fourth ACM\nConference on Fairness, Accountability, and Trans-\nparency (FAccT’21), Online. Association for Com-\nputing Machinery.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nIrving Biederman, Robert J. Mezzanotte, and Jan C.\nRabinowitz. 1982. Scene perception: Detecting\nand judging objects undergoing relational violations.\nCognitive Psychology, 14(2):143–177.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718–8735,\nOnline. Association for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot\nLearners. ArXiv, 2005.14165.\nEmanuele Bugliarello, Ryan Cotterell, Naoaki\nOkazaki, and Desmond Elliott. 2020. Multimodal\npretraining unmasked: Unifying the vision and\nlanguage berts. arXiv preprint arXiv:2011.15124.\nXinlei Chen, Hao Fang, Tsung-yi Lin, Ramakrishna\nVedantam, C Lawrence Zitnick, Saurabh Gupta,\nand Piotr Doll. 2015. Microsoft COCO Captions\n: Data Collection and Evaluation Server. arXiv,\n1504.00325:1–7.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. Uniter: Universal image-text\nrepresentation learning. In ECCV.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? An analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAdam Dahlgren Lindström, Johanna Björklund, Suna\nBensch, and Frank Drewes. 2020. Probing mul-\ntimodal embeddings for linguistic properties: the\nvisual-semantic case. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 730–744, Barcelona, Spain (Online). In-\nternational Committee on Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlexey Dosovitskiy, Lucas Beye, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2020. An image\nis worth 16x16 words: Transformers for image\nrecognitoin at scale. arXiv, 2010.11929.\nAdam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan\nClark, and Regina Barzilay. 2020. CapW AP: Image\ncaptioning with a purpose. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8755–8768,\nOnline. Association for Computational Linguistics.\nStella Frank, Emanuele Bugliarello, and Desmond El-\nliott. 2021. Vision-and-Language or Vision-for-\nLanguage? On Cross-Modal Inﬂuence in Multi-\nmodal Transformers. ArXiv, 2109.04448.\nStevan Harnad. 1990. The symbol grounding problem.\nPhysica, D42(1990):335–346.\nLisa Anne Hendricks and Aida Nematzadeh. 2021.\nProbing Image-Language Transformers for Verb Un-\nderstanding. ArXiv, 2106.09141.\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei\nLiu, Dongmei Fu, and Jianlong Fu. 2021. See-\ning Out of tHe bOx: End-to-End Pre-training for\nVision-Language Representation Learning. ArXiv,\n2104.03135.\nGabriel Ilharco, Rowan Zellers, Ali Farhadi, and Han-\nnaneh Hajishirzi. 2020. Probing Contextual Lan-\nguage Models for Common Ground with Visual Rep-\nresentations. arXiv, 2005.00619.\nL Itti and C Koch. 2001. Computational modelling\nof visual attention. Nature reviews neuroscience ,\n2(3):194–203.\nSai Muralidhar Jayanthi, Danish Pruthi, and Graham\nNeubig. 2020. Neuspell: A neural spelling correc-\ntion toolkit. arXiv preprint arXiv:2010.11085.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021.\nVilt: Vision-and-language transformer without con-\nvolution or region supervision. arXiv preprint\narXiv:2102.03334.\nChenliang Li, Ming Yan, Haiyang Xu, Fuli Luo,\nWei Wang, Bin Bi, and Songfang Huang. 2021.\nSemVLP: Vision-Language Pre-training by Align-\ning Semantics at Multiple Levels. arXiv preprint,\n2103.07829.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal en-\ncoder for vision and language by cross-modal pre-\ntraining. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 11336–11344. AAAI Press.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. In Arxiv.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2020b. What Does\nBERT with Vision Look At? In Proceedings ofthe\n58th Annual Meeting ofthe Association for Computa-\ntional Linguistics (ACL’20), pages 5265–5275, On-\nline. Association for Computational Linguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-\naowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, et al. 2020c. Oscar: Object-\nsemantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision,\npages 121–137. Springer.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems, pages 13–23.\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi\nParikh, and Stefan Lee. 2020. 12-in-1: Multi-task\nvision and language representation learning. In The\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan\nDuan, Tianrui Li, Xilin Chen, and Ming Zhou. 2020.\nUniVL: A uniﬁed video and language pre-training\nmodel for multimodal understanding and generation.\narXiv.\nHao Ma, Jianke Zhu, Michael Rung Tsong Lyu, and Ir-\nwin King. 2010. Bridging the semantic gap between\nimage contents and tags. IEEE Transactions on Mul-\ntimedia, 12(5):462–473.\nGeorge L. Malcolm, Iris I.A. Groen, and Chris I. Baker.\n2016. Making Sense of Real-World Scenes. Trends\nin Cognitive Sciences, 20(11):843–856.\nAude Oliva and Antonio Torralba. 2007. The role of\ncontext in object recognition. Trends in cognitive\nsciences, 11(12):520–527.\nLetitia Parcabalescu, Albert Gatt, Annette Frank, and\nIacer Calixto. 2021. Seeing past words: Testing the\ncross-modal capabilities of pretrained v&l models\non counting tasks. In Proceedings of the Workshop\nBeyond Language: Multimodal Semantic Represen-\ntations (MMSR’21), Groningen, The Netherlands.\nSandro Pezzelle, Claudio Greco, Greta Gandolﬁ,\nEleonora Gualdoni, and Raffaella Bernardi. 2020.\nBe Different to Be Better! A Benchmark to Lever-\nage the Complementarity of Language and Vision.\nIn Findings ofthe Association for Computational\nLinguistics: EMNLP 2020 , pages 2751–2767, On-\nline. Association for Computational Linguistics.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo,\nRadu Soricut, and Vittorio Ferrari. 2019. Connect-\ning Vision and Language with Localized Narratives.\narXiv, 1912.03098.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. arXiv preprint, 2103.00020.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster R-CNN: Towards Real-Time Ob-\nject Detection with Region Proposal Networks. In\nAdvances in Neural Information Processing Systems\n28 (NeurIPS 2015), Montreal, Canada.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. Vl-bert: Pre-\ntraining of generic visual-linguistic representations.\nIn International Conference on Learning Represen-\ntations.\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.\n2017. A Corpus of Natural Language for Visual\nReasoning. In Proceedings ofthe 55th Annual Meet-\ning ofthe Association for Computational Linguistics\n(ACL’17), pages 217–223, Vancouver, BC. Associa-\ntion for Computational Linguistics.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A Corpus for\nReasoning about Natural Language Grounded in\nPhotographs. arXiv, 1811.00491:6418–6428.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for\nComputational Linguistics.\nAntonio Torralba, Aude Oliva, Monica S. Castelhano,\nand John M. Henderson. 2006. Contextual guidance\nof eye movements and attention in real-world scenes:\nThe role of global features in object search. Psycho-\nlogical Review, 113(4):766–786.\nShikhar Tuli, Ishita Dasgupta, Erin Grant, and\nThomas L. Grifﬁths. 2021. Are Convolutional Neu-\nral Networks or Transformers more like human vi-\nsion? arXiv, 2105.07197.\nMelissa L.H. Võ and Jeremy M. Wolfe. 2013. Differen-\ntial Electrophysiological Signatures of Semantic and\nSyntactic Scene Processing. Psychological Science,\n24(9):1816–1823.\nJianfeng Wang, Xiaowei Hu, Pengchuan Zhang, Xiu-\njun Li, Lijuan Wang, Lei Zhang, Jianfeng Gao, and\nZicheng Liu. 2020. MiniVLM: A Smaller and Faster\nVision-Language Model. arXiv, 2012.06946.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Vi-\nsual commonsense reasoning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6720–6731.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A Large-Scale Adversar-\nial Dataset for Grounded Commonsense Inference.\narXiv, 1808.05326.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. 2017. Scene\nparsing through ade20k dataset. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 633–641.\nWanzheng Zhu and Suma Bhat. 2020. GRUEN for\nevaluating linguistic quality of generated text.arXiv,\n2010.02498.\nA Dataset details\nTable 5 gives the sizes of the ADE20k, HL1K and\nCOCO datasets. For ADE20k, the numbers are\nfor images which are not labelled as having an\nunknown scene. For COCO captions, there are\nﬁve captions associated with each image. For the\npurposes of the present study, a single caption is\nrandomly selected for each.\nADE20k HL1K COCO\nimages 19733 1000 5000\nCOCO captions 19733 3000 5000\nLocalized Narratives 19733 1000 5000\nTable 5:Datasets sizes\nTable 6 provides the number of ablations anal-\nysed in the study. For both ADE20k and HL1k\nwe obtain a number of ablated captions which is\ngreater than the respective dataset sizes in Table\n5, because for each example we generate all the\npossible combinations of noun phrases. For the\nVisual and Visual+Textual ablations, the number\nof ablated instances is lower than the dataset size,\nbecause we skip all the images where no object is\ndetected.\nADE20k HL1K\nT 205k 10027\nV 10788 625\nT+V 1078 625\nTable 6: Total number of ablations generated per dataset,\nacross all the ablations experiments\nA.1 Scene description templates\nADE20K scene labels are converted to textual de-\nscriptions using three different templates:\n• it is a SCENE\n• this is a SCENE\n• it is located in SCENE\nA scene label is rendered into a description using\na randomly selected template from the above. For\nexample:\n• bathroom –> it is a bathroom\n• bedroom –> this is a bedroom\n• airport –> it is located in an airport\nA.2 HL1K Examples\nHL1K is collected by asking crowd workers to con-\nsider an image, and answer the question Where\nis the picture taken? Crowd workers are asked\nto respond using full sentences, as far as possi-\nble. Since images are selected from the COCO\n2014 train split, they are also accompanied by\ntheir object-level COCO captions and Localized\nNarratives. An example image with corresponding\nHL1K scene-level descriptions is shown below.\nWhere is the picture taken?\n• in a bedroom\n• the picture is taken in a bedroom\n• this is the bedroom\nFigure 4: COCO image with corresponding scene descrip-\ntions in HL1K\nB Experiment details\nB.1 Image-sentence alignment\nResults for image-sentence alignment experiments\nin the paper are averages over three separate runs\nfor each model.\nFor LXMERT, we use the image-sentence align-\nment head.4 CLIP and LXMERT are tested on the\nstandard alignment task: given an image and either\nthe correct caption, or a random caption, the model\nneeds to determine whether the caption correctly\naligns with the image.\nWe use the publicly available implementation\nof VisualBERT.5 The image-sentence alignment\nsetting for this model is somewhat different, since\nalignment is modelled as an extension of the next-\nsentence prediction task in unimodal BERT. Vi-\nsualBERT takes an image and a correct caption,\n4github.com/huggingface/transformers\n5https://github.com/uclanlp/visualbert\ntogether with a second caption, which may be cor-\nrect or randomly selected. The task is to predict\nwhether the second caption correctly aligns with\nthe image+caption pair.\nFor all experiments, we truncate textual captions\nto a maximum length of 50 tokens, following stan-\ndard practice for such models, including CLIP.\nB.2 Ablation\nAblation is performed on object-level captions, and\non images. Results for ablation are reported based\non a single run of CLIP. Figure 5 shows an original\nimage and its object-level caption, together with\nthe visual and textual ablations. Below, we explain\nthe process of ablation in more detail.\nTextual ablation Given an object-level caption,\nwe identify all the NPs in the caption and create\nnew versions by removing each possible subset of\nthe set of NPs, with the restriction that the result-\ning caption must always contain at least one NP.\nWhen NP removal results in dangling predicates,\nwe remove them to preserve grammaticality. NPs\nare detected using Spacy v.3 pipeline for English\nusing the en_core_web_md pretrained models.\nVisual ablation Given an object-level caption\nand an image, we extract all nouns from the caption\nand extract the embedding vector for each noun us-\ning the pretrained FastText embeddings.6 We pass\nthe image through the Faster-RCNN object detec-\ntor7 to detect entities. We extract embeddings for\neach entity label. Then, we identify regions to be\nmasked by comparing embeddings for entity labels\nle against embeddings for nouns ne in the caption,\nconsidering them a match if cosine(le, ne) ≥0.7.\nBounding box regions corresponding to matched\nentities are occluded with a greyscale mask.\nC Scene-entity probabilities\nEntity probabilities are computed on the basis of\nthe entities detected in the image and object-level\ncaption, using the process described above for vi-\nsual ablation. Let e be an entity detected ne times\nin the dataset, of which ne,s times in images depict-\ning scene s. We compute:\nP(s|e) = ne,s\nne\n6We use the model with 2m word vectors trained\nwith subword information from common Crawl https://\nfasttext.cc/docs/en/english-vectors.html\n7Faster R-CNN ResNet-50 FPN pre-trained on COCO,\navailable from the torchvision module in Pytorch\nOriginal Image\nOccluded Image\nCOCO: A man rides a motorcycle on a road through a grassy,\nhilly area.\nAblated Captions:\n• a grassy, hilly area (A man, a motorcycle, a road)\n• a road (A man, a motorcycle, a grassy, hilly area)\n• a road through a grassy, hilly area(A man, a motorcycle)\n• A man rides a road (a motorcycle, a grassy, hilly area)\n• a motorcycle a grassy, hilly area (A man, a road)\n• A man rides a grassy, hilly area (a motorcycle, a road)\n• A man rides a motorcycle (a road, a grassy, hilly area)\n• A man rides a road through a grassy, hilly area (a mo-\ntorcycle)\n• a motorcycle on a road (A man, a grassy, hilly area)\n• A man rides a motorcycle on a road (a grassy, hilly\narea)\n• A man rides a motorcycle a grassy, hilly area (a road)\n• a motorcycle on a road through a grassy, hilly area. (A\nman)\nFigure 5: The original image at the top, in the middle the\nvisually ablated imaged and at the bottom the original caption\nand all the cases generated by the constituent textual ablation.\nFigure 6 shows visualisations for entities de-\ntected in four different scene types, from the HL1K\n(a) Scene: kitchen\n(b) Scene: road\n(c) Scene: room\n(d) Scene: park\nFigure 6: Visualisations of entities in four different scene\ntypes. For a given entity e in scene s, font size is proportional\nto P(s|e)\ndataset. To compute the mean probabilities as-\nsigned by CLIP to scene-level descriptions after\nentities are visually ablated, we average over those\nimages containing at least three detected entities\n(53/174 total scenes).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4157427251338959
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4113006591796875
    },
    {
      "name": "Computer vision",
      "score": 0.3737689256668091
    },
    {
      "name": "Psychology",
      "score": 0.3498702049255371
    }
  ]
}