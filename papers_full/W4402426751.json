{
  "title": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large Language Models",
  "url": "https://openalex.org/W4402426751",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3174968555",
      "name": "Ehsan Upol",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4225781268",
      "name": "Riedl, Mark O.",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3010300896",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W3013149459",
    "https://openalex.org/W3173721282",
    "https://openalex.org/W2155987308",
    "https://openalex.org/W2611748211",
    "https://openalex.org/W3119394424",
    "https://openalex.org/W3097001800",
    "https://openalex.org/W4319165740",
    "https://openalex.org/W2963395533",
    "https://openalex.org/W3162291149",
    "https://openalex.org/W4225137238",
    "https://openalex.org/W2142448246",
    "https://openalex.org/W3096521488",
    "https://openalex.org/W3016099278",
    "https://openalex.org/W4244978797",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2999765337",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W3197347140",
    "https://openalex.org/W2947686205",
    "https://openalex.org/W3193667172",
    "https://openalex.org/W2296463440",
    "https://openalex.org/W4220962633",
    "https://openalex.org/W3032875465",
    "https://openalex.org/W2999637955",
    "https://openalex.org/W2050896993",
    "https://openalex.org/W3104847483",
    "https://openalex.org/W3162428030",
    "https://openalex.org/W3103751997",
    "https://openalex.org/W1602132975"
  ],
  "abstract": "When the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) \"black-box\" of AI so that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the black-box is increasingly limited especially when it comes to non-AI expert end-users. In this paper, we challenge the assumption of \"opening\" the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an algorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by synthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black box, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.",
  "full_text": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of Large\nLanguage Models\nUPOL EHSAN, Georgia Institute of Technology, USA\nMARK O. RIEDL, Georgia Institute of Technology, USA\nWhen the initial vision of Explainable (XAI) was articulated, the most popular framing was to open the (proverbial) “black-box” of AI\nso that we could understand the inner workings. With the advent of Large Language Models (LLMs), the very ability to open the\nblack-box is increasingly limited especially when it comes to non-AI expert end-users. In this paper, we challenge the assumption of\n“opening” the black-box in the LLM era and argue for a shift in our XAI expectations. Highlighting the epistemic blind spots of an\nalgorithm-centered XAI view, we argue that a human-centered perspective can be a path forward. We operationalize the argument by\nsynthesizing XAI research along three dimensions: explainability outside the black-box, explainability around the edges of the black\nbox, and explainability that leverages infrastructural seams. We conclude with takeaways that reflexively inform XAI as a domain.\nCCS Concepts: • Human-centered computing; • Computing methodologies →Artificial intelligence;\nAdditional Key Words and Phrases: Explainable AI, Large Language Models, Generative AI\n1 PROVOCATION\nWith the advent of Foundation Models & Large Language Models like ChatGPT, is “opening the black-box” still a reasonable\nand achievable goal for Explainable AI (XAI)? Do we need to shift our perspectives?\nIn February of 2023, Nadeem (pseudonym), a relative of the first author of this article, asked if ChatGPT could be\nused to do homework. Nadeem is a high-schooler who shared that he was always under a tight deadline and needed to\nbe more “efficient” with his homework. He heard from his friends that ChatGPT can help summarize papers or books,\nwhich can make Nadeem more “productive” with his homework.\nBefore responding to Nadeem, ChatGPT was taken for a test drive. It was prompted to summarize an academic paper\n(Figure 1 similar to how Nadeem might use it – as someone who was not an AI researcher or experienced prompting\nLarge Language Models (LLMs). Fortunately, ChatGPT generated a coherent response. ChatGPT gave the names of the\nauthors of the paper and details about the paper’s publication at ACM FAccT 2020. Unfortunately, the names of the\nauthors and publication details were made up! The confabulated author names were immediately obvious because we\nwrote the paper that was prompted to be summarized [20]. However, the rest of the details was extremely plausible –\nthe paper very well could have appeared at that conference, but did not. The first author of this paper almost missed\nanother detail in ChatGPT’s summary. The original paper described a framework with two dimensions: social and\ntechnical. The generated summary claimed the framework described three dimensions: social, technical, and cultural,\nwhich, while wrong, was plausible enough that even the very author of the paper almost missed that crucial inaccuracy!\n1.1 Separating Fact from Fiction\nThe true story above demonstrates the effortful process required to disentangle fact from fiction in GPT’s output, even\nfrom someone knowledgeable of the source material. Even more notably, there was no way for our protagonist, an\nexpert in Explainable AI, to “open” the black-box of ChatGPT and understand why it produced what it produced or\nwhere it might be faithful to the facts or prone to confabulation (also called hallucination). On the one hand, he lacked\naccess to the internal details such as the parameters of the model. On the other hand, even if one did have access to the\nAuthors’ addresses: Upol Ehsan, Georgia Institute of Technology, USA; Mark O. Riedl, Georgia Institute of Technology, USA.\nAccepted at ACM HTTF 2024 1\narXiv:2408.05345v2  [cs.HC]  13 Aug 2024\n2 Ehsan & Riedl\nFig. 1. Screenshot of ChatGPT’s summary of a paper with hallucinated or confabulated content.\ninternal parameters of the model, given the scale and complexity of the neural architecture of such a large language\nmodel, interpreting it is unlikely to produce human-understandable and actionable information.\n2 TENSIONS: XAI AND LARGE LANGUAGE MODELS\nThe field of Explainable AI (XAI) is concerned with developing techniques, concepts, and processes that can help\nstakeholders understand the reasons behind the AI system’s decision-making [21, 34].\nFor our purposes, we adopt a design lens in XAI that is sociotechnically-informed [12, 19, 34] and adopt the broad\ndefinition that an explanation is an answer to a why-question [11, 30, 35]. Given AI systems exist in sociotechnical\nsettings [33, 45], it takes more than just algorithmic transparency to make them explainable [23, 35]. Thus, explaining\nwhat is happening “inside the black box” often requires us to also understand things “outside the black box” [12, 16, 32],\nrequiring us to consider the entire AI lifecycle (vs. just the algorithm). For instance, why a facial recognition system\ndisproportionately misclassified women of color [ 8] can be explained by looking at demographic compositions in\nthe training data. A sociotechnically situated view of XAI expands the concept of explainability beyond the bounds\nof the algorithm [16] and positions it as a relational and audience-dependent construct instead of a model-inherent\none [4, 5, 35, 36]. Emerging work [27, 40, 41] showcases how a broader XAI perspective can potentially address criticisms\nof popular algorithm-centered XAI techniques, which can be ineffective [3, 39, 48] and potentially risky [29, 44].\nWhen we consider a service such as ChatGPT, GPT-4, Microsoft Copilot, Google Gemini, Claude, or Meta AI, what\nprospects are there for “opening” the black-box of AI? These models have hundreds of billions of parameters, all acting\nin conjunction to generate a distribution over possible words to choose from to build a response, word by word. If\nwe had access to all the weights, could we interpret and explain the model? If we had access to the parameters of\na model and the activation values for an input could we interpret and explain the model? In the case of the above\nlarge language models the point is moot. All these models run on servers behind APIs that do not allow inspection\nAccepted at ACM HTTF 2024\nXAI Reloaded 3\nof the neuron activations and weights. However, even if we could access this information, the raw values of weights\nand activations are meaningless to most people without synthesizing some visualization or text summarization that\nprovides a lay-understandable analysis of the internal operations of the system and how the results were generated by\nthe system. Consider OpenAI’s work on interpreting the patterns that cause individual neurons to activate [7]. How\nwould knowing what causes neuron #2142 to activate have helped Nadeem, a non-AI expert, know how to better use\nChatGPT to complete his homework? What actionable information from this neural activation pattern can a non-AI\nexpert use meaningfully?\nLLMs are increasingly being incorporated as components in systems that chain multiple processes together. In\nparticular, Retrieval Augmented Generation (RAG) combines LLMs with web search such that a web retrieval module\nfirst retrieves relevant documents, which are then used to inform an LLM [31]. While opening the black-box LLMs may\nnot yield actionable explanations, modular architectures afford the ability to inspect and explain how data is changed\ngoing in and out of black-box modules.\n3 IS EXPLAINABLE AI DOOMED TO FAIL?\nDespite the commendable progress in algorithm-centered approaches in XAI, there are significant deficiencies. Studies\nexamining how people actually interact with AI explanations have found popular XAI techniques to be ineffective [3,\n39, 48], potentially risky [29, 44], and even obsolete in real-world deployed contexts [32]. XAI developers tend to design\nexplanations as if people like them are going to use their systems, earning an infamous reputation of “inmates running\nthe asylum” [35]. In fact, a majority of current deployments serve AI engineers instead of end-users whose needs are\nignored [6]. This creates a gap between design expectations and reality— how developers envision the designed AI\nexplanations to get interpreted and how users actually perceive those explanations in reality.\nAs Large Language Models (LLMs) become prominent, is Explainable AI – a research area in flux and its infancy –\ndoomed to fail? No. There is hope. Before we throw in the towel, there are a few things to consider.\n3.1 AI systems are Human-AI assemblages\nFirst, the techno-centric, algorithm-centered, discourse of XAI fails to appreciate the sociotechnical reality of AI systems.\nWhen we say “AI systems, ” what we very often mean to say is “Human-AI assemblages, ” where the “human” part of the\nHuman-AI assemblage is often implicit [16]. No real-world AI systems work in a vacuum. Black-boxes by themselves\ndo not do the work – humans with black-boxes do the work [19]. Even if the human contribution to the work is to\njust provide an input, this is a significant contribution because AI systems are useful to people as tools. Thus, the\nexplainability of AI systems entails explainability of the Human-AI assemblage, which has at least two components: the\nhuman (or humans) and the AI [16, 20]. Thus, how can we achieve the explainability of the Human-AI assemblage by\njust focusing on the explainability of the AI model? XAI is therefore not just technical, it is sociotechnical . It requires\nmore than just algorithmic transparency – more than being able to open the black box.\nSecond, what we mean by “AI” is evolving. Compared to AI systems even five years ago, the Deep Learning systems\nin the Foundation Model era, such as LLMs, are much more complex, have orders of magnitude more parameters,\nand are running at unprecedented scales. Thus, AI as a design material is tricky and is evolving [ 15, 20, 47]. Our\nunderstanding and expectations of what it means for AI-as-design-material to be explainable should also evolve. Further,\nXAI techniques that focus solely on the algorithm or the model face a new challenge: it is getting increasingly hard to\nopen the black box! As AI systems are increasingly end-user facing, those that need the explanations the most are on\nthe other side of an AI or user interface. This is the case for the most popular Large Language Models and chatbots, and\nAccepted at ACM HTTF 2024\n4 Ehsan & Riedl\nFig. 2. Illustrating how the explainability of the Human-AI assemblage is more than just technical (algorithmic) transparency\nit is also the case for other types of consumer-facing systems. When the initial vision of XAI was articulated, a popular\nframing was to “open” the (proverbial) “black-box” of AI [9, 37], so that we could see inside of it, figure out what it was\ndoing, why it was doing it, and if it was doing it correctly. With the advent of large language models, that ability to\nopen the black-box is increasingly limited due to the sheer complexity of the models and the increased prevalence of\nmodels behind restrictive APIs. And even if we did manage to “open” it, we will not understand what we see.\n4 HUMAN-CENTERED EXPLAINABLE AI: BEYOND ALGORITHMIC TRANSPARENCY\nGiven AI systems are bounded by their training data, by construction, they cannot incorporate the real-world dynamics\n\"outside\" the black-box. Thus, an algorithm-centered view of XAI is–by construction–a limiting view, one that handicaps\nthe XAI system from doing what we want to do– solve real world problems. We need a paradigm that can accommodate\nan expansion of the epistemic canvas– an increase of the aperture of the viewing lens– to include the sociotechnical\ndynamics in which XAI systems are embedded so that we can do what we set out to do – solve real world problems.\nThis is where the domain of Human-Centered Explainable AI (HCXAI) [19] can help. HCXAI is a holistic vision of AI\nexplainability, one that is human-centered and sociotechnical in nature. Situated as a Critical Technical Practice [1, 2], it\ndraws its conceptual DNA from critical AI studies and HCI (e.g., reflective design [13, 14, 42], value-sensitive design [25]).\nHCXAI encourages us to critically reflect and question dominant assumptions and practices of a field, such as algorithm-\ncentered XAI. It also adopts a value-sensitive approach to both users and designers in the development of technology\nthat challenges the status quo of a field. HCXAI encapsulates the philosophy that not everything that is important lies\ninside the black box of AI. Critical insights can lie outside it. Why? Because that’s where the humans are.\nThinking outside the black box of AI can help us meet our goals of helping people understand and calibrate their\ntrust in AI systems. Even if we cannot meaningfully open the black box or interpret its complexities, there are a lot of\nthings we can do to understand and explain the systemaround the black box. Increasing the aperture of XAI can help us\nfocus on the most important part: who the human(s) is (are), what they are trying to achieve in seeking an explanation,\nand how to design XAI techniques that meet those needs. Indeed, explanations of the sociotechnical system can offer us\nan important affordance: actionability [18, 28, 43].\nAt its core, actionability is about what a user can do with the information in an explanation [18]. An actionable XAI\nsystem empowers the user by increasing the space of possible informed actions to achieve their end goals. This could\nbe understanding how to change the inputs, contesting a decision, or learning when and how to use the system more\nappropriately. Actionability also addresses another important question: how do we know if an XAI system is useful?\nThere are an increasing number of reports of XAI systems that are deployed and fail to have any measurable impact on\ntheir users [3, 44]. Many of these systems failed because the XAI systems were not designed with user needs in mind,\nsuch as by providing users with information they could already intuit themselves, by providing information that was\nAccepted at ACM HTTF 2024\nXAI Reloaded 5\nonerous to verify, or by providing information that users could not use. In other words, the explanations generated by\nthe systems were not actionable.\n5 THE WAY FORWARD\nWith the reframing around human-AI assemblages and XAI systems that place the human as the central concern, and\narmed with actionability as the metric for success, we now lay out three possible paths forward. This list is not meant\nto be exhaustive or prescriptive. It is meant to be generative by providing emerging evidence for how Human-Centered\nXAI (HCXAI) can address the growing needs for understanding our increasingly AI-infused world.\n5.1 Explainability outside the black-box: Social Transparency\nMost consequential AI systems are embedded in organizational environments where groups of humans interact with it.\nThese real-world AI systems, as well as the explanations they produce, are socially-situated [22, 32]. Therefore, the\nsocio-organizational context in which these systems are used is key. Why are we not incorporating socio-organizational\ncontexts into how we think about explainability in AI? How can we tackle the explainability of Human-AI assemblages?\nEnter Social Transparency (ST) a sociotechnically-informed perspective that incorporates the socio-organizational\ncontext into explaining AI-mediated decision-making [16]. Social transparency allows us to augment the explainability\nof a human-AI assemblage without necessarily changing anything about the AI model. Social transparency allows one\nto annotate an output or behavior from an AI system with the 4W who did what, when and why. These annotations\nare shared between others using the system. They allow users to see whether and why others have accepted or rejected\nan AI’s output. Social transparency does two important things: first, it challenges the dominant narrative of algorithm-\ncentered notions of XAI; second, it expands our understanding of XAI beyond technical transparency by illustrating\nhow adding social context can help people make better, more actionable decisions with AI systems.\nImagine the following scenario (Figure 3): Aziz is a software seller trying to use a powerful AI-based pricing tool to\ndo something consequential: offer the right price to a client company. The AI suggests a price. Moreover, its suggestion\nhas technical transparency – it explains its recommendation by showing Aziz the top features it considered, such as\nsales quota goals, comparative pricing with other clients, and costs. Confident with the AI’s recommendation, Aziz\nmakes a bid, but the client finds the price too high and walks out.\nDespite an accurate AI model and the presence of technical transparency, why did the bid fail? There could be\nalgorithmic reasons for it. But might also be relevant contextual factors outside the box that can help explain why the\nbid failed. Perhaps the history between Aziz and the client that was not honored? Or maybe there were external events\nthat happened since the model was trained, such as a pandemic-induced budgetary crisis.\nNow imagine that Aziz could see that more than 65% of his peers rejected the AI’s pricing recommendation (Block 2\nin Fig. 3). Or, what if Aziz knew that Jess, a director in the company, sold the product at a loss due to pandemic-related\nbudgetary cuts?(Block 5 in Fig. 3)\nThis peripheral vision of who did what, when and why – called the 4W – are the constitutive design elements of\nSocial Transparency that can encode relevant socio-organizational context. The benefit of taking a holistic approach to\nexplainability is clear: a study of real-world AI users in sales, cybersecurity, and healthcare found that social transparency,\nin the form of the 4W, helped people calibrate their trust in the AI’s performance, provide actionable information for AI\ncontestability and robust decision-making, and the organizational context made visible enabled better collective actions\nin the organization and strengthened the human-AI assemblages [16].\nAccepted at ACM HTTF 2024\n6 Ehsan & Riedl\nFig. 3. Sales scenario with Social Transparency (ST) used in [ 16] (reproduced with permission from authors). The labeled blocks are:\n(1) Decision information and model explanation: Information of the current sales decision, the AI’s recommended price and a “feature\nimportance” explanation justifying the model’s recommendation, inspired by real-world pricing tools; (2) ST summary: Beginning of\nST giving a high-level summary of how many teammates in the past had received the recommendation and how many sold at the\nrecommended price; (3-5): ST blocks with \"4W\" features containing the historical decision trajectory of three other users.\nBy incorporating the socio-organizational context, Social Transparency makes our understanding of XAI more\nholistic, representing the Human-AI assemblage more realistically than a purely algorithm-centered XAI view. We\nshould note that Social Transparency is agnostic to whether an AI system is black-boxed or not. As long as there is an\nAI-based recommendation or decision, we can attach 4W – the socio-organizational context – to it. In a completely\nblack-boxed AI system, there will not be any technical transparency. However, the 4W can add transparency to the\nsocial side of the Human-AI assemblage.\n5.2 Explainability around the Edges of the Black Box: Rationale Generation & Scrutability\nIf the black box cannot be cracked open in any meaningful sense, there is another possibility: incorporate explainability\naround the edges of the black box to foster a better functional understanding in the user [ 38] such that it fosters\nactionability. One of the original formulations of rationale generation [21] postulated that there was no need to know\nhow a black box worked as long as we could learn how to give actionable advice about the black box by looking at\nits inputs and outputs. It was philosophically grounded in Fodor’s work on Language of Thought [24]: how is it that,\ndespite not having a 1-1 neural correlate of thought, humans can effectively communicate by translating their thoughts\ninto words? For Human-AI interaction, even if the exact mechanism of the (artificial) neural correlate of AI’s thought\nwas not known to the human, as long as actionable information is present in the explanation from an AI agent, the\nHuman-AI interaction can proceed. In short, explanations that do not directly access the model can still generate\nactionable information.\nIn the case of large language models, the actionable information is whether any particular input is likely to produce\na reliable response that can be trusted. Large language models might be generally capable at many tasks such as\nAccepted at ACM HTTF 2024\nXAI Reloaded 7\nquestion-answering, they are not infallible, and it is always possible for a user to ask a question that results in a\nconfabulation (also called a “hallucination”) that the user is unable to vet. In this case, we can directly use the API to\nprobe how it responds to particular stimuli [46]. It is proposed that an XAI system can decompose the original, human\nauthored question into a series of more fine-grained, related questions that provide more opportunities for the model to\nconfabulate responses if it is not competent at the original question. These sub-questions can be selected to be easier for\nthe user to vet. Generating questions to challenge an LLM has been demonstrated to increase users’ ability to determine\nwhether the answer should be trusted or not.\n5.3 Explainability by Leveraging Infrastructural Seams: Seamful XAI\nNo AI system is perfect. Mistakes are inevitable. Breakdowns in AI systems often occur when the assumptions we\nmake in design and development do not hold true when they are deployed in the real-world. For example, an AI system\ncan fail when it is trained on data from North America but deployed in South Asia, especially when the end user is\nunaware of this infrastructural mismatch. These mismatches between design assumptions and real-world usage are\ncalled seams [17]. Handling the mistakes from AI systems is hard, especially when the AI’s decision-making is hidden\nor black-boxed. Although black-boxing AI systems can make the user experience seamless and easy to use, concealing\nthe seams can lead to downstream harms for end-users, such as uncritical AI acceptance. What can we do differently?\nHow do we move beyond seamless AI? And what can we gain by doing so?\nSeamful XAI is a design lens that incorporates the principles of seamful design [10] to augment explainability and\nuser agency. A classic example of seamful design is a \"seamful map\" of WiFi coverage in your home. If you know the\nWiFi’s dead zones in your home, you will be able to best use it because you can then avoid.Without revealing the seams,\nusers can have reasonable expectations of perfect WiFi. The map makes the seams in the WiFi’s infrastructure visible to\nusers, which allows them to recalibrate their expectations and behavior. A seamful design principle asks us to leverage\nthe weakness in opportunistic ways [26].\nUnlike seamlessness, seamful design does not aim to hide the infrastructure . Rather, it puts the infrastructure and all\nits imperfections front and center. Seamful design helps us recognize and grapple with the complex infrastructures\nsystems reside in. Conversely, seamless design ideals risks making the labor it takes to make the system work invisible\n(e.g., datawork, ghostwork, maintenance work). And, as invisible work is invariably unaccounted for and unappreciated,\nworkers who conduct this work will feel undervalued or invisible. Seamfulness embraces the imperfect reality of spaces\nwe inhabit and makes the most out of it.\nIn the context of AI, seams can be conceptualized as mismatches, gaps, or cracks in assumptions between the world of\nhow AI systems are designed and the world of how AI systems are used in practice . Seamful XAI seeks to empower users\nwith information that augments their agency by identifying gaps between ideal design assumptions and reality.\nAt the heart of Seamful XAI are four observations:\n(1) Seams are inevitable, arising from the integration of heterogeneous sociotechnical components during technology\ndeployments.\n(2) Seams are revealed through system breakdowns.\n(3) Instead of treating seams as problematic negatives to be erased, they can be used strategically to calibrate users’\nreliance and understanding of an AI system.\n(4) The goal of this strategic revelation (and concealment) is to support user agency (actionability, contestability,\nand appropriation).\nSeamful XAI Design Process: Let’s review the design process proposed by [17].\nAccepted at ACM HTTF 2024\n8 Ehsan & Riedl\nFig. 5. The virtual whiteboard used for the seamful XAI design activity showing key features in [ 17] (reproduced with permission).\nArea 1: Envisioning breakdown (Step 1). Participants were provided sample breakdowns (A), which participants could either use\ndirectly or get inspiration for their own envisioning.Area 2: Anticipating & crafting seams (Step 2). Fuiding prompts were provided (B)\nfor effectively crafting the seams. Exemplary seams were shared (C) for each stage of the AI lifecycle framework. Area 3: Designing\nwith seams (Step 3). Participants were asked to articulate their reasoning for choosing a seam and tag which user goals the selected\nseam (E) can support for augmenting user agency.\nThe first step of the process begins with generating \"breakdowns. \" Breakdowns are answers to the question, \"what\ncould go wrong when this technology gets deployed?\" Answers could include technology failures, unfair treatment of\ngroups, inducing over-reliance, or deskilling.\nThe second step is around anticipating and crafting seams, which is done in three parts. First (2A in the diagram),\nwe ask: \"what might we (as developers, designers, researchers, etc.) do to make the breakdown happen?” While this\nquestion might seem counter-intuitive, it allows us to systematically prevent breakdowns by understanding their causes.\nThis step inverts the problem and makes it a goal directed task, which is important to generate concrete outcomes\ninstead of open-ended problems. Next (2B), we try to anticipate the reasons for the breakdown (the seams) in the\nappropriate stage in the AI’s lifecycle (the colored boxes numbered 1-6 in Fig. 5). Finally (2C), we craft the seam by\nthinking about the gap between the ideal expectation and the reality of use.\nThe final step involves using the seams generated in step 2 in a way to empower user agency and explainability.\nHere (3A), we ask: given our end goal, which seams do we show and which do we hide (e.g. strategic revelation and\nconcealment)? The revealed seams (3B) should empower users through better explainability. This step of the Seamful\nXAI process is a major differentiator from other Responsible AI processes: unlike most processes that stop at identifying\ngaps, this one goes beyond. It not only uncovers the gaps but also utilizes them as avenues to support users (for more\ndetails, refer to [17]).\nAccepted at ACM HTTF 2024\nXAI Reloaded 9\nA co-designing study [17] with 43 real-world AI users found three beneficial elements of Seamful XAI:\n•It enhances explainability by helping stakeholders reveal the AI’s blind spots, highlight its fallibility, and\nshowcase the strengths and weaknesses of the system, which can calibrate reliance in AI systems.\n•It augments user agency by providing peripheral vision of the AI’s blind spots. Seamful information expands\nthe action space of what users can do. Information in seams can convert “unknown unknowns” to “known\nunknowns, ” which can empower users to know “where” to start an investigation.\n•It is a resourceful way to not just reveal seams but also anticipate and mitigate harms from AI systems .\n6 TAKEAWAYS\nWe began with the provocation: With the advent of Foundation Models & Large Language Models like ChatGPT, is “opening\nthe black-box” still a reasonable and achievable goal for XAI? Do we need to shift our perspectives?\nYes. The proverbial “black-box” of AI has evolved, and so should our expectations on how to make it explainable. As\nthe box becomes more opaque and harder to “open, ” the human side of the Human-AI assemblage remains as a fruitful\nspace to explore. In the most extreme case, the human side may be all there is left to explore . Even if we can open the\nblack box it is unclear what actionable outcomes would become available.\nThere are four important lessons from Human-centered XAI that can inform the shift in our XAI expectations.\n(1) First, the human-centered XAI perspective takes a pragmatic and resourceful view of explainability, especially if\nblack boxes are expected to persist. By considering the actions afforded to the user by the explanations, HCXAI\ncenters the focus on the human, ensuring AI augments human abilities rather than replace them.\n(2) Second, explainability is not only achieved by looking inside the black box through mechanistic descriptions of\nhow an algorithm works. Actionability can be achieved by exploring explainability outside and around the edges\nof the black box because human-centered XAI takes a more expansive view of what it means to provide insights\ninto a black box that can afford a wider range of actions.\n(3) Third, explicitly treating AI systems as human-AI assemblages means focusing on explainability of the assemblage,\nnot just the AI. This widened perspective opens up avenues for not just factoring in who is interacting with the\nblack box, but also how human teams can work together — directly or indirectly — to contextualize a dynamically\nchanging real-world AI behavior.\n(4) Fourth, seamful XAI turns the disadvantages and weaknesses of an AI system into advantages. The gaps between\nuser expectations and AI capabilities are exactly the gaps that explanations address. Instead of hiding those\ngaps to create seamless experiences, seamful XAI leverages these gaps in an opportunistic manner to augment\nexplainability and user agency.\nAs we reload our expectations on XAI, we invite you to do what HCXAI asks us to do: centering the design and\nevaluation around the human. This positioning can reveal unmet needs that must be addressed while avoiding the\ncostly mistake of building XAI systems that do not make a difference. While there have been many examples of XAI\nsystems that have failed to have the intended impact of users, it is often the case that these tenets of HCXAI were\noverlooked. XAI is a relatively young field of research that has yet to find its footing, even as the landscape of black\nbox AI systems is rapidly evolving. It is not yet time to give up hope on XAI. Instead, we invite you to adopt critical\nreflection and value-sensitivity into XAI research and evaluation, making it human-centered.\nWill Human-centered XAI solve all our problems? No, but it will help us ask the right questions.\nAccepted at ACM HTTF 2024\n10 Ehsan & Riedl\nACKNOWLEDGMENTS\nWith our deepest gratitude, we acknowledge the time of all participants of all the studies reported here. Without\ntheir input, these projects would not have been possible. We thank reviewers for their valuable input. We also want\nto thank the organizations, the sites for the case studies, for their cooperation. We are grateful to members of the\nHuman-Centered AI Lab at Georgia Tech whose continued input refined the conceptualizations presented here. We\nare indebted to Justin Weisz for his editorial feedback that helped scope the project appropriately. This project was\npartially supported by the National Science Foundation under Grant No. 1928586.\nREFERENCES\n[1] P Agre. 1997. Toward a critical technical practice: Lessons learned in trying to reform AI in Bowker. Social science, technical systems, and cooperative\nwork: Beyond the Great Divide (1997).\n[2] Philip E Agre. 1997. Computation and human experience . Cambridge University Press.\n[3] Ahmed Alqaraawi, Martin Schuessler, Philipp Weiß, Enrico Costanza, and Nadia Berthouze. 2020. Evaluating saliency map explanations for\nconvolutional neural networks: a user study. In Proceedings of the 25th International Conference on Intelligent User Interfaces . 275–285.\n[4] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López,\nDaniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward\nresponsible AI. Information Fusion 58 (2020), 82–115.\n[5] Vijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C Hoffman, Stephanie Houde, Q Vera Liao, Ronny Luss,\nAleksandra Mojsilović, et al . 2019. One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques. arXiv preprint\narXiv:1909.03012 abs/1909.03012 (2019). arXiv:1909.03012 http://arxiv.org/abs/1909.03012\n[6] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José MF Moura, and Peter\nEckersley. 2020. Explainable machine learning in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency .\n648–657.\n[7] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023.\nLanguage models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.\n[8] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on\nfairness, accountability and transparency . PMLR, 77–91.\n[9] Davide Castelvecchi. 2016. Can we open the black box of AI? Nature News 538, 7623 (2016), 20.\n[10] Matthew Chalmers and Ian MacColl. 2003. Seamful and seamless design in ubiquitous computing. In Workshop at the crossroads: The interaction of\nHCI and systems issues in UbiComp , Vol. 8.\n[11] Daniel Clement Dennett. 1989. The intentional stance . MIT press.\n[12] Shipi Dhanorkar, Christine T Wolf, Kun Qian, Anbang Xu, Lucian Popa, and Yunyao Li. 2021. Who needs to know what, when?: Broadening the\nExplainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle. In Designing Interactive Systems Conference 2021 .\n[13] Paul Dourish. 2004. Where the action is: the foundations of embodied interaction . MIT press.\n[14] Paul Dourish, Janet Finlay, Phoebe Sengers, and Peter Wright. 2004. Reflective HCI: Towards a critical technical practice. InCHI’04 extended abstracts\non Human factors in computing systems . 1727–1728.\n[15] Graham Dove, Kim Halskov, Jodi Forlizzi, and John Zimmerman. 2017. UX Design Innovation: Challenges for Working with Machine Learning as a\nDesign Material. Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems - CHI ’17 (2017), 278–288. https://doi.org/10.1145/\n3025453.3025739\n[16] Upol Ehsan, Q Vera Liao, Michael Muller, Mark O Riedl, and Justin D Weisz. 2021. Expanding explainability: Towards social transparency in ai\nsystems. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1–19.\n[17] Upol Ehsan, Q Vera Liao, Samir Passi, Mark O Riedl, and Hal Daume III. 2022. Seamful XAI: Operationalizing Seamful Design in Explainable AI.\narXiv preprint arXiv:2211.06753 (2022).\n[18] Upol Ehsan, Samir Passi, Q Vera Liao, Larry Chan, I Lee, Michael Muller, Mark O Riedl, et al. 2021. The who in explainable ai: How ai background\nshapes perceptions of ai explanations. arXiv preprint arXiv:2107.13509 (2021).\n[19] Upol Ehsan and Mark O Riedl. 2020. Human-centered Explainable AI: Towards a Reflective Sociotechnical Approach, In International Conference\non Human-Computer Interaction. arXiv preprint arXiv:2002.01092 , 449–466.\n[20] Upol Ehsan, Koustuv Saha, Munmun De Choudhury, and Mark O Riedl. 2023. Charting the Sociotechnical Gap in Explainable AI: A Framework to\nAddress the Gap in XAI. Proceedings of the ACM on Human-Computer Interaction 7, CSCW1 (2023), 1–32.\n[21] Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, and Mark O Riedl. 2019. Automated rationale generation: a technique for\nexplainable AI and its effects on human perceptions. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray,\nCalifornia) (IUI ’19) . Association for Computing Machinery, New York, NY, USA, 263–274. https://doi.org/10.1145/3301275.3302316\nAccepted at ACM HTTF 2024\nXAI Reloaded 11\n[22] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Martina Mara, Marc Streit, Sandra Wachter, Andreas Riener, and Mark O Riedl. 2021. Operationalizing\nhuman-centered perspectives in explainable AI. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems . 1–6.\n[23] Upol Ehsan, Philipp Wintersberger, Q Vera Liao, Elizabeth Anne Watkins, Carina Manger, Hal Daumé III, Andreas Riener, and Mark O Riedl. 2022.\nHuman-Centered Explainable AI (HCXAI): beyond opening the black-box of AI. In CHI Conference on Human Factors in Computing Systems Extended\nAbstracts. 1–7.\n[24] Jerry A Fodor. 1975. The language of thought . Vol. 5. Harvard university press.\n[25] Batya Friedman, Peter H Kahn, and Alan Borning. 2008. Value sensitive design and information systems. The handbook of information and computer\nethics (2008), 69–101.\n[26] William W Gaver, Jacob Beaver, and Steve Benford. 2003. Ambiguity as a resource for design. In Proceedings of the SIGCHI conference on Human\nfactors in computing systems . 233–240.\n[27] MD Romael Haque, Katherine Weathington, Joseph Chudzik, and Shion Guha. 2020. Understanding Law Enforcement and Common Peoples’\nPerspectives on Designing Explainable Crime Mapping Algorithms. In Conference Companion Publication of the 2020 on Computer Supported\nCooperative Work and Social Computing . 269–273.\n[28] Shalmali Joshi, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. Towards realistic individual recourse and actionable\nexplanations in black-box decision making systems. arXiv preprint arXiv:1907.09615 (2019).\n[29] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. 2020. Interpreting Interpretability:\nUnderstanding Data Scientists’ Use of Interpretability Tools for Machine Learning. In Proceedings of the 2020 CHI Conference on Human Factors\nin Computing Systems (Honolulu, HI, USA) (CHI ’20) . Association for Computing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/\n3313831.3376219\n[30] David K Lewis. 1986. Causal explanation. (1986).\n[31] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim\nRocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33\n(2020), 9459–9474.\n[32] Q Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. In\nProceedings of the SIGCHI Conference on Human Factors in Computing Systems . ACM, 1–15.\n[33] Q Vera Liao, Milena Pribić, Jaesik Han, Sarah Miller, and Daby Sow. 2021. Question-driven design process for explainable ai user experiences. arXiv\npreprint arXiv:2104.03483 (2021).\n[34] Q Vera Liao and Kush R Varshney. 2021. Human-centered explainable ai (xai): From algorithms to user experiences. arXiv preprint arXiv:2110.10790\n(2021).\n[35] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence 267 (2019), 1–38.\n[36] Sina Mohseni, Niloofar Zarei, and Eric D Ragan. 2018. A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI\nSystems. arXiv (2018), arXiv–1811. https://doi.org/10.1145/3387166 arXiv:1811.11839\n[37] George Nott. 2017. Explainable artificial intelligence: Cracking open the black box of AI. Computer world 4 (2017).\n[38] Andrés Páez. 2019. The pragmatic turn in explainable artificial intelligence (XAI). Minds and Machines 29, 3 (2019), 441–459.\n[39] Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. 2018. Manipulating and\nmeasuring model interpretability. arXiv preprint arXiv:1802.07810 (2018).\n[40] Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. 2022. Data Cards: Purposeful and Transparent Dataset Documentation for Responsible\nAI. arXiv preprint arXiv:2204.01075 (2022).\n[41] Jakob Schoeffer and Niklas Kuehl. 2021. Appropriate fairness perceptions? On the effectiveness of explanations in enabling people to assess the\nfairness of automated decision systems. In Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social\nComputing. 153–157.\n[42] Phoebe Sengers, Kirsten Boehner, Shay David, and Joseph’Jofish’ Kaye. 2005. Reflective design. In Proceedings of the 4th decennial conference on\nCritical computing: between sense and sensibility . 49–58.\n[43] Ronal Singh, Tim Miller, Henrietta Lyons, Liz Sonenberg, Eduardo Velloso, Frank Vetere, Piers Howe, and Paul Dourish. 2023. Directive explanations\nfor actionable explainability in machine learning applications. ACM Transactions on Interactive Intelligent Systems (2023).\n[44] Simone Stumpf, Adrian Bussone, and Dympna O’sullivan. 2016. Explanations considered harmful? user interactions with machine learning systems.\nIn ACM SIGCHI Workshop on Human-Centered Machine Learning .\n[45] Jiao Sun, Q Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde, Kartik Talamadupula, and Justin D Weisz. 2022. Investigating Explainability\nof Generative AI for Code through Scenario-based Design. In 27th International Conference on Intelligent User Interfaces . 212–228.\n[46] Kaige Xie, Sarah Wiegreffe, and Mark Riedl. 2022. Calibrating trust of multi-hop question answering systems with decompositional probes. arXiv\npreprint arXiv:2204.07693 (2022).\n[47] Qian Yang, Aaron Steinfeld, Carolyn Rosé, and John Zimmerman. 2020. Re-examining whether, why, and how human-AI interaction is uniquely\ndifficult to design. In Proc. CHI . 1–13.\n[48] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of Confidence and Explanation on Accuracy and Trust Calibration in AI-Assisted\nDecision Making. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* ’20). ACM, Association for\nComputing Machinery, New York, NY, USA, 295–305. https://doi.org/10.1145/3351095.3372852\nAccepted at ACM HTTF 2024",
  "topic": "Status quo",
  "concepts": [
    {
      "name": "Status quo",
      "score": 0.8710024356842041
    },
    {
      "name": "Computer science",
      "score": 0.6262804269790649
    },
    {
      "name": "Data science",
      "score": 0.3491009473800659
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34122270345687866
    },
    {
      "name": "Natural language processing",
      "score": 0.3275689482688904
    },
    {
      "name": "Political science",
      "score": 0.22648155689239502
    },
    {
      "name": "Law",
      "score": 0.11068856716156006
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}