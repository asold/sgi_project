{
  "title": "Polar Transformer Networks",
  "url": "https://openalex.org/W2751473119",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227974231",
      "name": "Esteves, Carlos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4293362980",
      "name": "Allen-Blanchette, Christine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973944412",
      "name": "Zhou Xiaowei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742797110",
      "name": "Daniilidis, Kostas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2107790757",
    "https://openalex.org/W2151103935",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2113325037",
    "https://openalex.org/W2621199038",
    "https://openalex.org/W2952054889",
    "https://openalex.org/W2042243448",
    "https://openalex.org/W1972963882",
    "https://openalex.org/W2336098239",
    "https://openalex.org/W2167383966",
    "https://openalex.org/W2563709626",
    "https://openalex.org/W2726423948",
    "https://openalex.org/W1514759812",
    "https://openalex.org/W2061543840",
    "https://openalex.org/W2951320766",
    "https://openalex.org/W2136026194",
    "https://openalex.org/W2160317232",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2069767826",
    "https://openalex.org/W2518995263",
    "https://openalex.org/W3037148157",
    "https://openalex.org/W2134464438",
    "https://openalex.org/W2952422028",
    "https://openalex.org/W1912570122",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2056653204"
  ],
  "abstract": "Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.",
  "full_text": "Published as a conference paper at ICLR 2018\nPOLAR TRANSFORMER NETWORKS\nCarlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, Kostas Daniilidis\nGRASP Laboratory, University of Pennsylvania\n{machc, allec, xiaowz, kostas}@seas.upenn.edu\nABSTRACT\nConvolutional neural networks (CNNs) are inherently equivariant to translation.\nEfforts to embed other forms of equivariance have concentrated solely on rota-\ntion. We expand the notion of equivariance in CNNs through the Polar Trans-\nformer Network (PTN). PTN combines ideas from the Spatial Transformer Net-\nwork (STN) and canonical coordinate representations. The result is a network\ninvariant to translation and equivariant to both rotation and scale. PTN is trained\nend-to-end and composed of three distinct stages: a polar origin predictor, the\nnewly introduced polar transformer module and a classiﬁer. PTN achieves state-\nof-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an\nMNIST variation obtained by adding clutter and perturbing digits with translation,\nrotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate\nthrough the Cylindrical Transformer Network.\n1 I NTRODUCTION\nWhether at the global pattern or local feature level (Granlund, 1978), the quest for (in/equi)variant\nrepresentations is as old as the ﬁeld of computer vision and pattern recognition itself. State-of-the-art\nin “hand-crafted” approaches is typiﬁed by SIFT (Lowe, 2004). These detector/descriptors identify\nthe intrinsic scale or rotation of a region (Lindeberg, 1994; Chomat et al., 2000) and produce an\nequivariant descriptor which is normalized for scale and/or rotation invariance. The burden of these\nmethods is in the computation of the orbit (i.e. a sampling the transformation space) which is neces-\nsary to achieve equivariance. This motivated steerable ﬁltering which guarantees transformed ﬁlter\nresponses can be interpolated from a ﬁnite number of ﬁlter responses. Steerability was proved for\nrotations of Gaussian derivatives (Freeman et al., 1991) and extended to scale and translations in the\nshiftable pyramid (Simoncelli et al., 1992). Use of the orbit and SVD to create a ﬁlter basis was\nproposed by Perona (1995)and in parallel, Segman et al. (1992) proved for certain classes of trans-\nformations there exists canonical coordinateswhere deformation of the input presents as translation\nof the output. Following this work, Nordberg & Granlund (1996) and Hel-Or & Teo (1996); Teo\n& Hel-Or (1998) proposed a methodology for computing the bases of equivariant spaces given the\nLie generators of a transformation. and most recently, Sifre & Mallat (2013) proposed the scattering\ntransform which offers representations invariant to translation, scaling, and rotations.\nThe current consensus is representations should be learned not designed. Equivariance to trans-\nlations by convolution and invariance to local deformations by pooling are now textbook (LeCun\net al. (2015), p.335) but approaches to equivariance of more general deformations are still maturing.\nThe main veins are: Spatial Transformer Network (STN) (Jaderberg et al., 2015) which similarly to\nSIFT learn a canonical pose and produce an invariant representation through warping, work which\nconstrains the structure of convolutional ﬁlters (Worrall et al., 2016) and work which uses the ﬁlter\norbit (Cohen & Welling, 2016b) to enforce an equivariance to a speciﬁc transformation group.\nIn this paper, we propose the Polar Transformer Network (PTN), which combines the ideas of STN\nand canonical coordinate representations to achieve equivariance to translations, rotations, and dila-\ntions. The three stage network learns to identify the object center then transforms the input into log-\npolar coordinates. In this coordinate system, planar convolutions correspond to group-convolutions\nin rotation and scale. PTN produces a representation equivariant to rotations and dilations without\nhttp://github.com/daniilidis-group//polar-transformer-networks\n1\narXiv:1709.01889v3  [cs.CV]  1 Feb 2018\nPublished as a conference paper at ICLR 2018\nFigure 1: In the log-polar representation, rotations around the origin become vertical shifts, and\ndilations around the origin become horizontal shifts. The distance between the yellow and green\nlines is proportional to the rotation angle/scale factor. Top rows: sequence of rotations, and the cor-\nresponding polar images. Bottom rows: sequence of dilations, and the corresponding polar images.\nthe challenging parameter regression of STN. We enlarge the notion of equivariance in CNNs be-\nyond Harmonic Networks (Worrall et al., 2016) and Group Convolutions (Cohen & Welling, 2016b)\nby capturing both rotations and dilations of arbitrary precision. Similar to STN; however, PTN\naccommodates only global deformations.\nWe present state-of-the-art performance on rotated MNIST and SIM2MNIST, which we introduce.\nTo summarize our contributions:\n• We develop a CNN architecture capable of learning an image representation invariant to\ntranslation and equivariant to rotation and dilation.\n• We propose the polar transformer module, which performs a differentiable log-polar trans-\nform, amenable to backpropagation training. The transform origin is a latent variable.\n• We show how the polar transform origin can be learned effectively as the centroid of a\nsingle channel heatmap predicted by a fully convolutional network.\n2 R ELATED WORK\nOne of the ﬁrst equivariant feature extraction schemes was proposed by Nordberg & Granlund\n(1996) who suggested the discrete sampling of 2D-rotations of a complex angle modulated ﬁlter.\nAbout the same time, the image and optical processing community discovered the Mellin transform\nas a modiﬁcation of the Fourier transform (Zwicke & Kiss, 1983; Casasent & Psaltis, 1976). The\nFourier-Mellin transform is equivariant to rotation and scale while its modulus is invariant.\nDuring the 80’s and 90’s invariances of integral transforms were developed through methods based\nin the Lie generators of the respective transforms starting from one-parameter transforms (Ferraro\n& Caelli, 1988) and generalizing to Abelian subgroups of the afﬁne group (Segman et al., 1992).\nClosely related to the (in/equi)variance work is work in steerability, the interpolation of responses\nto any group action using the response of a ﬁnite ﬁlter basis. An exact steerability framework\nbegan in Freeman et al. (1991), where rotational steerability for Gaussian derivatives was explicitly\ncomputed. It was extended to the shiftable pyramid (Simoncelli et al., 1992), which handle rotation\nand scale. A method of approximating steerability by learning a lower dimensional representation of\nthe image deformation from the transformation orbit and the SVD was proposed by Perona (1995).\nA uniﬁcation of Lie generator and steerability approaches was introduced by Teo & Hel-Or (1998)\nwho used SVD to reduce the number of basis functions for a given transformation group. Teo and\nHel-Or developed the most extensive framework for steerability (Teo & Hel-Or, 1998; Hel-Or &\nTeo, 1996), and proposed the ﬁrst approach for non-Abelian groups starting with exact steerability\n2\nPublished as a conference paper at ICLR 2018\nfor the largest Abelian subgroup and incrementally steering for the remaining subgroups. Cohen &\nWelling (2016a); Jacobsen et al. (2017) recently combined steerability and learnable ﬁlters.\nThe most recent “hand-crafted” approach to equivariant representations is the scattering transform\n(Sifre & Mallat, 2013) which composes rotated and dilated wavelets. Similar to SIFT (Lowe, 2004)\nthis approach relies on the equivariance of anchor points (e.g. the maxima of ﬁltered responses\nin (translation) space). Translation invariance is obtained through the modulus operation which is\ncomputed after each convolution. The ﬁnal scattering coefﬁcient is invariant to translations and\nequivariant to local rotations and scalings.\nLaptev et al. (2016) achieve transformation invariance by pooling feature maps computed over the\ninput orbit, which scales poorly as it requires forward and backward passes for each orbit element.\nWithin the context of CNNs, methods of enforcing equivariance fall to two main veins. In the ﬁrst,\nequivariance is obtained by constraining ﬁlter structure similarly to Lie generator based approaches\n(Segman et al., 1992; Hel-Or & Teo, 1996). Harmonic Networks (Worrall et al., 2016) use ﬁlters\nderived from the complex harmonics achieving both rotational and translational equivariance. The\nsecond requires the use of a ﬁlter orbit which is itself equivariant to obtain group equivariance.\nCohen & Welling (2016b) convolve with the orbit of a learned ﬁlter and prove the equivariance of\ngroup-convolutions and preservation of rotational equivariance in the presence of rectiﬁcation and\npooling. Dieleman et al. (2015) process elements of the image orbit individually and use the set of\noutputs for classiﬁcation. Gens & Domingos (2014) produce maps of ﬁnite-multiparameter groups,\nZhou et al. (2017) and Marcos et al. (2016) use a rotational ﬁlter orbit to produce oriented feature\nmaps and rotationally invariant features, and Lenc & Vedaldi (2015) propose a transformation layer\nwhich acts as a group-convolution by ﬁrst permuting then transforming by a linear ﬁlter.\nOur approach, PTN, is akin to the second vein. We achieve global rotational equivariance and expand\nthe notion of CNN equivariance to include scaling. PTN employs log-polar coordinates (canonical\ncoordinates in Segman et al. (1992)) to achieve rotation-dilation group-convolution through trans-\nlational convolution subject to the assumption of an image center estimated similarly to the STN.\nMost related to our method is Henriques & Vedaldi (2016), which achieves equivariance by warping\nthe inputs to a ﬁxed grid, with no learned parameters.\nWhen learning features from 3D objects, invariance to transformations is usually achieved through\naugmenting the training data with transformed versions of the inputs (Wu et al., 2015), or pooling\nover transformed versions during training and/or test (Maturana & Scherer, 2015; Qi et al., 2016).\nSedaghat et al. (2016) show that a multi-task approach, i.e. prediction of both the orientation and\nclass, improves classiﬁcation performance. In our extension to 3D object classiﬁcation, we explicitly\nlearn representations equivariant to rotations around a family of parallel axes by transforming the\ninput to cylindrical coordinates about a predicted axis.\n3 T HEORETICAL BACKGROUND\nThis section is divided into two parts, the ﬁrst offers a review of equivariance and group-\nconvolutions. The second offers an explicit example of the equivariance of group-convolutions\nthrough the 2D similarity transformations group, SIM(2), comprised of translations, dilations and\nrotations. Reparameterization of SIM(2) to canonical coordinates allows for the application of the\nSIM(2) group-convolution using translational convolution.\n3.1 G ROUP EQUIVARIANCE\nEquivariant representations are highly sought after as they encode both class and deformation infor-\nmation in a predictable way. Let Gbe a transformation group and LgI be the group action applied\nto an image I. A mapping Φ : E →F is said to be equivariant to the group action Lg, g∈Gif\nΦ(LgI) = L′\ng(Φ(I)) (1)\nwhere Lg and L′\ng correspond to application of gto E and F respectively and satisfy Lgh = LgLh.\nInvariance is the special case of equivariance where L′\ng is the identity. In the context of image\nclassiﬁcation and CNNs, g ∈Gcan be thought of as an image deformation and Φ a mapping from\nthe image to a feature map.\n3\nPublished as a conference paper at ICLR 2018\nThe inherent translational equivariance of CNNs is independent of the convolutional kernel and\nevident in the corresponding translation of the output in response to translation of the input. Equiv-\nariance to other types of deformations can be achieved through application of thegroup-convolution,\na generalization of translational convolution. Letting f(g) and φ(g) be real valued functions on G\nwith Lhf(g) = f(h−1g), the group-convolution is deﬁned Kyatkin & Chirikjian (2000)\n(f ⋆G φ)(g) =\n∫\nh∈G\nf(h)φ(h−1g) dh. (2)\nA slight modiﬁcation to the deﬁnition is necessary in the ﬁrst CNN layer since the group is acting\non the image. The group-convolution reduces to translational convolution when Gis translation in\nRn with addition as the group operator,\n(f ⋆φ)(x) =\n∫\nh\nf(h)φ(h−1x) dh\n=\n∫\nh\nf(h)φ(x−h) dh.\n(3)\nGroup-convolution requires integrability over a group and identiﬁcation of the appropriate measure\ndg. It can be proved that given the measure dg, group-convolution is always group equivariant:\n(Laf ⋆G φ)(g) =\n∫\nh∈G\nf(a−1h)φ(h−1g) dh\n=\n∫\nb∈G\nf(b)φ((ab)−1g) db\n=\n∫\nb∈G\nf(b)φ(b−1a−1g) db\n= (f ⋆G φ)(a−1g)\n= La((f ⋆G φ))(g).\n(4)\nThis is depicted in response of an equivariant representation to input deformation (Figure 2 (left)).\n3.2 E QUIVARIANCE IN SIM(2)\nA similarity transformation, ρ∈SIM(2), acts on a point in x∈R2 by\nρx→sRx + t s ∈R+, R∈SO(2), t∈R2, (5)\nwhere SO(2) is the rotation group. To take advantage of the standard planar convolution in classical\nCNNs we decompose aρ∈SIM(2) into a translation,tin R2 and a dilated-rotation rin SO(2)×R+.\nEquivariance to SIM(2) is achieved by learning the center of the dilated rotation, shifting the original\nimage accordingly then transforming the image to canonical coordinates. In this reparameterization\nthe standard translational convolution is equivalent to the dilated-rotation group-convolution.\nThe origin predictor is an application of STN to global translation prediction (Jaderberg et al., 2015),\nthe centroid of the output is taken as the origin of the input.\nTransformation of the image LtI = I(t−t0) (canonization in Soatto (2013)) reduces the SIM(2)\ndeformation to a dilated-rotation if to is the true translation. After centering, we perform SO(2) ×\nR+convolutions on the new image Io = I(x−to):\nf(r) =\n∫\nx∈R2\nIo(x)φ(r−1x) dx (6)\nand the feature maps f in subsequent layers\nh(r) =\n∫\ns∈SO(2)×R+\nf(s)φ(s−1r) ds (7)\nwhere r,s ∈SO(2) ×R+. We compute this convolution through use of canonical coordinates for\nAbelian Lie-groups (Segman et al., 1992). The centered image Io(x,y)1 is transformed to log-\npolar coordinates, I(eξcos(θ),eξsin(θ)) hereafter written λ(ξ,θ) with (ξ,θ) ∈SO(2) ×R+for\n4\nPublished as a conference paper at ICLR 2018\nFigure 2: Left: Group-convolutions in SO(2). The images in the left most column differ by 90◦\nrotation, the ﬁlters are shown in the top row. Application of the rotational group-convolution with\nan arbitrary ﬁlter results is shown to produce an equivariant representation. The inner-product each\nof ﬁlter orbit (rotated from 0 −360◦) and the image is plotted in blue for the top image and red for\nthe bottom image. Observe how the ﬁlter response is shifted by 90◦. Right: Group-convolutions\nin SO(2) ×R+. Images in the left most column differ by a rotation of π/4 and scaling of 1.2.\nCareful consideration of the resulting heatmaps (shown in canonical coordinates) reveals a shift\ncorresponding to the deformation of the input image.\nnotational convenience. The shift of the dilated-rotation equivariant representation in response to\ninput deformation is shown in Figure 2 (right) using canonical coordinates.\nIn canonical coordinates s−1r = ξr −ξ,θr −θ and the SO(2) ×R+group-convolution2 can be\nexpressed and efﬁciently implemented as a planar convolution\n∫\ns\nf(s)φ(s−1r) ds=\n∫\ns\nλ(ξ,θ)φ(ξr −ξ,θr −θ) dξdθ. (8)\nTo summarize, we (1) construct a network of translational convolutions, (2) take the centroid of the\nlast layer, (3) shift the original image to accordingly, (4) convert to log-polar coordinates, and (5)\napply a second network 3 of translational convolutions. The result is a feature map equivariant to\ndilated-rotations around the origin.\n4 A RCHITECTURE\nPTN is comprised of two main components connected by the polar transformer module. The ﬁrst\npart is the polar origin predictor and the second is the classiﬁer (a conventional fully convolutional\nnetwork). The building block of the network is a 3 ×3 ×K convolutional layer followed by batch\nnormalization, an ReLU and occasional subsampling through strided convolution. We will refer to\nthis building block simply as block. Figure 3 shows the architecture.\n4.1 P OLAR ORIGIN PREDICTOR\nThe polar origin predictor operates on the original image and comprises a sequence of blocks fol-\nlowed by a 1 ×1 convolution. The output is a single channel feature map, the centroid of which is\ntaken as the origin of the polar transform.\nThere are some difﬁculties in training a neural network to predict coordinates in images. Some\napproaches (Toshev & Szegedy, 2014) attempt to use fully connected layers to directly regress the\ncoordinates with limited success. A better option is to predict heatmaps (Tompson et al., 2014;\nNewell et al., 2016), and take their argmax. However, this can be problematic since backpropogation\ngradients are zero in all but one point, which impedes learning.\n1we abuse the notation here and momentarily we use xas the x-coordinate instead of x∈R2.\n2abuse of the term, SO(2) ×R+is not a group because the dilation ξis not compact.\n3the network employs rectiﬁer and pooling which have been shown to preserve equivariance (Cohen &\nWelling, 2016b).\n5\nPublished as a conference paper at ICLR 2018\npolar origin predictor classiﬁer\nPolar \nTransformer\nFigure 3: Network architecture. The input image passes through a fully convolutional network, the\npolar origin predictor, which outputs a heatmap. The centroid of the heatmap (two coordinates),\ntogether with the input image, goes into the polar transformer module, which performs a polar\ntransform with origin at the input coordinates. The obtained polar representation is invariant with\nrespect to the original object location; and rotations and dilations are now shifts, which are handled\nequivariantly by a conventional classiﬁer CNN.\nThe usual approach to heatmap prediction is evaluation of a loss against some ground truth. In this\napproach the argmax gradient problem is circumvented by supervision. In PTN the the gradient of\nthe output coordinates must be taken with respect to the heatmap since the polar origin is unknown\nand must be learned. Use of argmax is avoided by using the centroid of the heatmap as the polar\norigin. The gradient of the centroid with respect to the heatmap is constant and nonzero for all\npoints, making learning possible.\n4.2 P OLAR TRANSFORMER MODULE\nThe polar transformer module takes the origin prediction and image as inputs and outputs the log-\npolar representation of the input. The module uses the same differentiable image sampling technique\nas STN (Jaderberg et al., 2015), which allows output coordinates Vi to be expressed in terms of the\ninput U and the source sample point coordinates (xs\ni,ys\ni). The log-polar transform in terms of the\nsource sample points and target regular grid (xt\ni,yt\ni) is:\nxs\ni = x0 + rxt\ni/W cos 2πyt\ni\nH (9)\nys\ni = y0 + rxt\ni/W sin 2πyt\ni\nH (10)\nwhere (x0,y0) is the origin, W,H are the output width and height, and ris the maximum distance\nfrom the origin, set to 0.5\n√\nH2 + W2 in our experiments.\n4.3 W RAP -AROUND PADDING\nTo maintain feature map resolution, most CNN implementations use zero-padding. This is not ideal\nfor the polar representation, as it is periodic about the angular axis. A rotation of the input result\nin a vertical shift of the output, wrapping at the boundary; hence, identiﬁcation of the top and\nbottom most rows is most appropriate. This is achieved with wrap-around padding on the vertical\ndimension.The top most row of the feature map is padded using the bottom rows and vice versa.\nZero-padding is used in the horizontal dimension. Table 5 shows a performance evaluation.\n4.4 P OLAR ORIGIN AUGMENTATION\nTo improve robustness of our method, we augment the polar origin during training time by adding a\nrandom shift to the regressed polar origin coordinates. Note that this comes for little computational\ncost compared to conventional augmentation methods such as rotating the input image. Table 5\nquantiﬁes the performance gains of this kind of augmentation.\n6\nPublished as a conference paper at ICLR 2018\n5 E XPERIMENTS\n5.1 A RCHITECTURES\nWe brieﬂy deﬁne the architectures in this section, see A for details. CCNN is a conventional fully\nconvolutional network; PCNN is the same, but applied to polar images with central origin. STN is\nour implementation of the spatial transformer networks (Jaderberg et al., 2015). PTN is our polar\ntransformer networks, and PTN-CNN is a combination of PTN and CCNN. The sufﬁxes S and B\nindicate small and big networks, according to the number of parameters. The sufﬁxes + and ++\nindicate training and training+test rotation augmentation.\nWe perform rotation augmentation for polar-based methods. In theory, the effect of input rotation is\njust a shift in the corresponding polar image, which should not affect the classiﬁer CNN. In practice,\ninterpolation and angle discretization effects result in slightly different polar images for rotated\ninputs, so even the polar-based methods beneﬁt from this kind of augmentation.\n5.2 R OTATED MNIST (L AROCHELLE ET AL ., 2007)\nTable 1 shows the results. We divide the analysis in two parts; on the left, we show approaches with\nsmaller networks and no rotation augmentation, on the right there are no restrictions.\nBetween the restricted approaches, the Harmonic Network (Worrall et al., 2016) outperforms the\nPTN by a small margin, but with almost 4x more training time, because the convolutions on complex\nvariables are more costly. Also worth mentioning is the poor performance of the STN with no\naugmentation, which shows that learning the transformation parameters is much harder than learning\nthe polar origin coordinates.\nBetween the unrestricted approaches, most variants of PTN-B outperform the current state of the\nart, with signiﬁcant improvements when combined with CCNN and/or test time augmentation.\nFinally, we note that the PCNN achieves a relatively high accuracy in this dataset because the digits\nare mostly centered, so using the polar transform origin as the image center is reasonable. Our\nmethod, however, outperforms it by a high margin, showing that even in this case, it is possible to\nﬁnd an origin away from the image center that results in a more distinctive representation.\nTable 1: Performance on rotated MNIST. Errors are averages of several runs, with\nstandard deviations within parenthesis. Times are average training time per epoch.\nModel error [%] params time [s] Model error [%] params time [s]\nPTN-S 1.83 (0.04) 27k 3.64 (0.04) PTN-B+ 1.14 (0.08) 129k 4.38 (0.02)\nPCNN-S 2.6 (0.08) 22k 2.61 (0.04) PTN-B++ 0.95 (0.09) 129k 4.38 6\nCCNN-S 5.76 (0.35) 22k 2.43 (0.02) PTN-CNN-B+ 1.01 (0.06) 254k 7.36\nSTN-S 7.87 (0.18) 43k 3.90 (0.05) PTN-CNN-B++ 0.89 (0.06) 254k 7.36 6\nHNet 1 1.69 33k 13.29 (0.19) PCNN-B+ 1.37 (0.00) 124k 3.30 (0.04)\nP4CNN 2 2.28 22k - CCNN-B+ 1.53 (0.07) 124k 2.98 (0.02)\nSTN-B+ 1.31 (0.05) 146k 4.57 (0.04)\nOR-TIPooling 3 1.54 ≈1M -\nTI-Pooling 4 1.2 ≈1M 42.90\nRotEqNet 5 1.01 100k -\n1, 2, 3, 4, 5 Worrall et al. (2016); Cohen & Welling (2016b); Zhou et al. (2017); Laptev et al. (2016); Marcos et al. (2016)\n6 Test time performance is 8x slower when using test time augmentation\n5.3 O THER MNIST VARIANTS\nWe also perform experiments in other MNIST variants. MNIST R, RTS are replicated from Jader-\nberg et al. (2015). We introduce SIM2MNIST, with a more challenging set of transformations from\nSIM(2). See B for more details about the datasets.\nTable 2 shows the results. We can see that the PTN performance mostly matches the STN on\nboth MNIST R and RTS. The deformations on these datasets are mild and data is plenty, so the\nperformance may be saturated.\nOn SIM2MNIST, however, the deformations are more challenging and the training set 5x smaller.\nThe PCNN performance is signiﬁcantly lower, which reiterates the importance of predicting the best\n7\nPublished as a conference paper at ICLR 2018\nFigure 4: Left: The rows alternate between samples from SIM2MNIST, where the predicted origin\nis shown in green, and their learned polar representation. Note how rotations and dilations of the\nobject become shifts. Right: Each row shows a different input and correspondent feature maps\non the last convolutional layer. The ﬁrst and second rows show that the 180◦rotation results in a\nhalf-height vertical shift of the feature maps. The third and fourth rows show that the 2.4×dilation\nresults in a shift right of the feature maps. The ﬁrst and third rows show invariance to translation.\npolar origin. The HNet outperforms the other methods (except the PTN), thanks to its translation\nand rotation equivariance properties. Our method is more efﬁcient both in number of parameters and\ntraining time, and is also equivariant to dilations, achieving the best performance by a large margin.\nTable 2: Performance on MNIST variants.\nMNIST R MNIST RTS SIM2MNIST 1\nerror [%] pars time error [%] pars time error [%] pars time\nPTN-S+ 0.88 (0.04) 29k 19.72 0.78 (0.05) 32k 24.48 5.44 (0.03) 35k 11.92\nPTN-B+ 0.62 (0.04) 129k 20.37 0.57 (0.03) 134k 28.74 5.03 (0.11) 134k 12.02\nPCNN-B+ 0.81 (0.04) 124k 13.97 0.70 (0.01) 129k 17.19 15.46 (0.22) 129k 5.33\nCCNN-B+ 0.74 (0.01) 124k 12.79 0.62 (0.07) 129k 15.97 11.73 (0.57) 129k 5.28\nSTN-B+ 0.61 (0.02) 146k 23.12 0.54 (0.02) 150k 27.90 12.35 (1.61) 150k 10.41\nSTN (Jaderberg et al., 2015) 0.7 400k - 0.5 400k - - - -\nHNet 2(Worrall et al., 2016) - - - - - - 9.28 (0.05) 44k 31.42\nTI-Pooling (Laptev et al., 2016) 0.8 ≈1M - - - - - - -\n1 No augmentation is used with SIM2MNIST, despite the + sufﬁxes\n2 Our modiﬁed version, with two extra layers with subsampling to account for larger input\n5.4 V ISUALIZATION\nWe visualize network activations to conﬁrm our claims about invariance to translation and equivari-\nance to rotations and dilations.\nFigure 4 (left) shows some of the predicted polar origins and the results of the polar transform. We\ncan see that the network learns to reject clutter and to ﬁnd a suitable origin for the polar transform,\nand that the representation after the polar transformer module does present the properties claimed.\nWe proceed to visualize if the properties are preserved in deeper layers. Figure 4 (right) shows the\nactivations of selected channels from the last convolutional layer, for different rotations, dilations,\nand translations of the input. The reader can verify that the equivariance to rotations and dilations,\nand the invariance to translations are indeed preserved during the sequence of convolutional layers.\n5.5 E XTENSION TO 3D OBJECT CLASSIFICATION\nWe extend our model to perform 3D object classiﬁcation from voxel occupancy grids. We assume\nthat the inputs are transformed by random rotations around an axis from a family of parallel axes.\nThen, a rotation around that axis corresponds to a translation in cylindrical coordinates.\nIn order to achieve equivariance to rotations, we predict an axis and use it as the origin to transform to\ncylindrical coordinates. If the axis is parallel to one of the input grid axes, the cylindrical transform\n8\nPublished as a conference paper at ICLR 2018\nFigure 5: Top: rotated voxel occupancy grids. Bottom: corresponding cylindrical representations.\nNote how rotations around a vertical axis correspond to translations over a horizontal axis.\namounts to channel-wise polar transforms, where the origin is the same for all channels and each\nchannel is a 2D slice of the 3D voxel grid. In this setting, we can just apply the polar transformer\nlayer to each slice.\nWe use a technique similar to the anisotropic probing of Qi et al. (2016) to predict the axis. Let\nz denote the input grid axis parallel to the rotation axis. We treat the dimension indexed by z as\nchannels, and run regular 2D convolutional layers, reducing the number of channels on each layer,\neventually collapsing to a single 2D heatmap. The heatmap centroid gives one point of the axis,\nand the direction is parallel to z. In other words, the centroid is the origin of all channel-wise\npolar transforms. We then proceed with a regular 3D CNN classiﬁer, acting on the cylindrical\nrepresentation. The 3D convolutions are equivariant to translations; since they act on cylindrical\ncoordinates, the learned representation is equivariant to input rotations around axes parallel to z.\nWe run experiments on ModelNet40 (Wu et al., 2015), which contains objects rotated around the\ngravity direction (z). Figure 5 shows examples of input voxel grids and their cylindrical coordinates\nrepresentation, while table 3 shows the classiﬁcation performance. To the best of our knowledge,\nour method outperforms all published voxel-based methods, even with no test time augmentation.\nHowever, the multi-view based methods generally outperform the voxel-based. (Qi et al., 2016).\nNote that we could also achieve equivariance to scale by using log-cylindrical or log-spherical co-\nordinates, but none of these change of coordinates would result in equivariance to arbitrary 3D\nrotations.\nTable 3: ModelNet40 classiﬁcation performance. We compare only\nwith voxel-based methods.\nModel Avg. class accuracy [%] Avg. instance accuracy [%]\nCylindrical Transformer (Ours) 86.5 89.9\n3D ShapeNets (Wu et al., 2015) 77.3 -\nV oxNet (Maturana & Scherer, 2015) 83 -\nMO-SubvolumeSup (Qi et al., 2016) 86.0 89.2\nMO-Aniprobing (Qi et al., 2016) 85.6 89.9\n6 C ONCLUSION\nWe have proposed a novel network whose output is invariant to translations and equivariant to the\ngroup of dilations/rotations. We have combined the idea of learning the translation (similar to the\nspatial transformer) but providing equivariance for the scaling and rotation, avoiding, thus, fully\nconnected layers required for the pose regression in the spatial transformer. Equivariance with re-\nspect to dilated rotations is achieved by convolution in this group. Such a convolution would require\nthe production of multiple group copies, however, we avoid this by transforming into canonical co-\nordinates. We improve the state of the art performance on rotated MNIST by a large margin, and\noutperform all other tested methods on a new dataset we call SIM2MNIST. We expect our approach\nto be applicable to other problems, where the presence of different orientations and scales hinder the\nperformance of conventional CNNs.\n9\nPublished as a conference paper at ICLR 2018\nREFERENCES\nDavid Casasent and Demetri Psaltis. Scale invariant optical transform. Optical Engineering, 15(3):153258–\n153258, 1976.\nOlivier Chomat, Vincent Colin de Verdi `ere, Daniela Hall, and James L Crowley. Local scale selection for\ngaussian based description techniques. In European Conference on Computer Vision, pp. 117–134. Springer,\n2000.\nTaco S. Cohen and Max Welling. Steerable cnns. 2016a. URL http://arxiv.org/abs/1612.\n08498v1.\nTaco S Cohen and Max Welling. Group equivariant convolutional networks.arXiv preprint arXiv:1602.07576,\n2016b.\nSander Dieleman, Kyle W Willett, and Joni Dambre. Rotation-invariant convolutional neural networks for\ngalaxy morphology prediction. Monthly notices of the royal astronomical society, 450(2):1441–1459, 2015.\nMario Ferraro and Terry M Caelli. Relationship between integral transform invariances and lie group theory.\nJOSA A, 5(5):738–742, 1988.\nWilliam T Freeman, Edward H Adelson, et al. The design and use of steerable ﬁlters. IEEE Transactions on\nPattern analysis and machine intelligence, 13(9):891–906, 1991.\nRobert Gens and Pedro M Domingos. Deep symmetry networks. InAdvances in neural information processing\nsystems, pp. 2537–2545, 2014.\nGoesta H Granlund. In search of a general picture processing operator. Computer Graphics and Image Pro-\ncessing, 8(2):155–173, 1978.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\nYacov Hel-Or and Patrick C Teo. Canonical decomposition of steerable functions. In Computer Vision and\nPattern Recognition, 1996. Proceedings CVPR’96, 1996 IEEE Computer Society Conference on, pp. 809–\n816. IEEE, 1996.\nJo˜ao F Henriques and Andrea Vedaldi. Warped convolutions: Efﬁcient invariance to spatial transformations.\narXiv preprint arXiv:1609.04382, 2016.\nJ¨orn-Henrik Jacobsen, Bert de Brabandere, and Arnold W. M. Smeulders. Dynamic steerable blocks in deep\nresidual networks. CoRR, 2017. URL http://arxiv.org/abs/1706.00598v2.\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in\nNeural Information Processing Systems, pp. 2017–2025, 2015.\nAlexander B Kyatkin and Gregory S Chirikjian. Algorithms for fast convolutions on motion groups. Applied\nand Computational Harmonic Analysis, 9(2):220–241, 2000.\nDmitry Laptev, Nikolay Savinov, Joachim M. Buhmann, and Marc Pollefeys. Ti-pooling: Transformation-\ninvariant pooling for feature learning in convolutional neural networks. In The IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), June 2016.\nHugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evalu-\nation of deep architectures on problems with many factors of variation. In Proceedings of the 24th interna-\ntional conference on Machine learning, pp. 473–480. ACM, 2007.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444, 2015.\nKarel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and\nequivalence. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 991–\n999, 2015.\nTony Lindeberg. Scale-space theory: A basic tool for analyzing structures at different scales.Journal of applied\nstatistics, 21(1-2):225–270, 1994.\nDavid G Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer\nvision, 60(2):91–110, 2004.\n10\nPublished as a conference paper at ICLR 2018\nDiego Marcos, Michele V olpi, Nikos Komodakis, and Devis Tuia. Rotation equivariant vector ﬁeld networks.\nCoRR, 2016.\nDaniel Maturana and Sebastian Scherer. V oxnet: A 3d convolutional neural network for real-time object recog-\nnition. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pp. 922–928.\nIEEE, 2015.\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in\nnatural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised\nfeature learning, volume 2011, pp. 5, 2011.\nAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hourglass networks for human pose estimation. 2016.\nKlas Nordberg and Gosta Granlund. Equivariance and invariance-an approach based on lie groups. In Image\nProcessing, 1996. Proceedings., International Conference on, volume 3, pp. 181–184. IEEE, 1996.\nPietro Perona. Deformable kernels for early vision. IEEE Transactions on pattern analysis and machine\nintelligence, 17(5):488–499, 1995.\nCharles R. Qi, Hao Su, Matthias Niessner, Angela Dai, Mengyuan Yan, and Leonidas J. Guibas. V olumetric\nand multi-view cnns for object classiﬁcation on 3d data. 2016.\nNima Sedaghat, Mohammadreza Zolfaghari, and Thomas Brox. Orientation-boosted voxel nets for 3d object\nrecognition. CoRR, 2016.\nJoseph Segman, Jacob Rubinstein, and Yehoshua Y Zeevi. The canonical coordinates method for pattern\ndeformation: Theoretical and computational considerations. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 14(12):1171–1183, 1992.\nLaurent Sifre and St´ephane Mallat. Rotation, scaling and deformation invariant scattering for texture discrimi-\nnation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1233–1240,\n2013.\nEero P Simoncelli, William T Freeman, Edward H Adelson, and David J Heeger. Shiftable multiscale trans-\nforms. IEEE transactions on Information Theory, 38(2):587–607, 1992.\nStefano Soatto. Actionable information in vision. InMachine learning for computer vision, pp. 17–48. Springer,\n2013.\nPatrick C Teo and Yacov Hel-Or. Design of multi-parameter steerable functions using cascade basis reduction.\nIn Computer Vision, 1998. Sixth International Conference on, pp. 187–192. IEEE, 1998.\nJonathan J Tompson, Arjun Jain, Yann LeCun, and Christoph Bregler. Joint training of a convolutional network\nand a graphical model for human pose estimation. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,\nand K. Q. Weinberger (eds.),Advances in Neural Information Processing Systems 27, pp. 1799–1807. Curran\nAssociates, Inc., 2014.\nAlexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. InThe\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.\nDaniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic networks:\nDeep translation and rotation equivariance. arXiv preprint arXiv:1612.04642, 2016.\nZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 1912–1920, 2015.\nYanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao. Oriented response networks. InThe IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July 2017.\nPhilip E Zwicke and Imre Kiss. A new implementation of the mellin transform and its application to radar\nclassiﬁcation of ships. IEEE Transactions on pattern analysis and machine intelligence, 4(2):191–199,\n1983.\n11\nPublished as a conference paper at ICLR 2018\nAPPENDICES\nA A RCHITECTURES DETAILS\nWe implement the following architectures for comparison,\n• Conventional CNN (CCNN), a fully convolutional network, composed of a sequence of\nconvolutional layers and some rounds of subsampling .\n• Polar CNN (PCNN), same architecture as CCNN, operating on polar images. The log-\npolar transform is pre-computed at the image center before training, as in Henriques &\nVedaldi (2016). The fundamental difference between our method and this is that we learn\nthe polar origin implicitly, instead of ﬁxing it.\n• Spatial Transformer Network (STN), our implementation of Jaderberg et al. (2015), re-\nplacing the localization network by four blocks of 20 ﬁlters and stride 2, followed by a 20\nunit fully connected layer, which we found to perform better. The transformation regressed\nis in SIM(2), and a CCNN comes after the transform.\n• Polar Transformer Network (PTN), our proposed method. The polar origin predictor\ncomprises three blocks of 20 ﬁlters each, with stride 2 on the ﬁrst block (or the ﬁrst two\nblocks, when input is 96 ×96). The classiﬁcation network is the CCNN.\n• PTN-CNN, we classify based on the sum of the per class scores of instances of PTN and\nCCNN trained independently.\nThe following sufﬁxes qualify the architectures described above:\n• S, “small” network, with seven blocks of 20 ﬁlters and one round of subsampling (equiva-\nlent to the Z2CNN in Cohen & Welling (2016b)).\n• B, “big” network, with 8 blocks with the following number of ﬁlters: 16, 16, 32, 32, 32,\n64, 64, 64. Subsampling by strided convolution is used whenever the number of ﬁlters\nincrease. We add up to two 2 extra blocks of 16 ﬁlters with stride 2 at the beginning to\nhandle larger input resolutions (one for 42 ×42 and two for 96 ×96).\n• +, training time rotation augmentation by continuous angles.\n• ++, training and test time rotation augmentation. We input 8 rotated versions the the query\nimage and classify using the sum of the per class scores.\nCylindrical transformer network:The axis prediction part of the cylindrical transformer network\nis composed of four 2D blocks, with 5 ×5 kernels and 32, 16, 8, and 4 channels, no subsampling.\nThe classiﬁer is composed of eight 3D convolutional blocks, with 3 ×3 ×3 kernels, the following\nnumber of ﬁlters: 32, 32, 32, 64, 64, 64, 128, 128, and subsampling whenever the number of ﬁlters\nincrease. Total number of params is approximately 1M.\nB D ATASET DETAILS\n• Rotated MNISTThe rotated MNIST dataset (Larochelle et al., 2007) is composed of28×\n28, 360◦rotated images of handwritten digits. The training, validation and test sets are of\nsizes 10k, 2k, and 50k, respectively.\n• MNIST R, we replicate it from Jaderberg et al. (2015). It has 60k training and 10k testing\nsamples, where the digits of the original MNIST are rotated between[−90◦,90◦]. It is also\nknow as half-rotated MNIST (Laptev et al., 2016).\n• MNIST RTS, we replicate it from Jaderberg et al. (2015). It has 60k training and 10k\ntesting samples, where the digits of the original MNIST are rotated between [−45◦,45◦],\nscaled between 0.7 and 1.2, and shifted within a 42 ×42 black canvas.\n• SIM2MNIST, we introduce a more challenging dataset, based on MNIST, perturbed by\nrandom transformations from SIM(2). The images are 96 ×96, with 360◦rotations; the\nscale factors range from 1 to 2.4, and the digits can appear anywhere in the image. The\ntraining, validation and test set have size 10k, 5k, and 50k, respectively.\n12\nPublished as a conference paper at ICLR 2018\nFigure 6: ROTSVHN samples. Since the digits are cropped from larger images, no artifacts are in-\ntroduced when rotating. The 6s and 9s are indistinguishable when rotated. Note that there are usually\nvisible digits on the sides, which pose a challenge for classiﬁcation and PTN origin prediction.\nTable 4: SVHN classiﬁcation performance. The minus sufﬁx indicate removal of 6s and 9s. PTN\nshows slightly worse performance on the unperturbed dataset, but is clearly superior when rotations\nare present.\nSVHN ROTSVHN SVHN- ROTSVHN-\nPTN-ResNet32 (Ours) 2.82 (0.07) 7.90 (0.14) 2.85 (0.07) 3.96 (0.04)\nResNet32 2.25 (0.15) 9.83 (0.29) 2.09 (0.06) 5.39 (0.09)\nC SVHN E XPERIMENTS\nIn order to demonstrate the efﬁcacy of PTN on real-world RGB images, we run experiments on\nthe Street View House Numbers (SVHN) dataset Netzer et al. (2011), and a rotated version that we\nintroduce (ROTSVHN) . The dataset contains cropped images of single digits, as well as the slightly\nlarger images from where the digits are cropped. Using the latter, we can extract the rotated digits\nwithout introducing artifacts. Figure 6 shows some examples from the ROTSVHN.\nWe use a 32 layer Residual Network (He et al., 2016) as a baseline (ResNet32). The PTN-ResNet32\nhas 8 residual convolutional layers as the origin predictor, followed by a ResNet32.\nIn contrast with handwritten digits, the 6s and 9s in house numbers are usually indistinguishable.\nTo remove this effect from our analysis, we also run experiments removing those classes from the\ndatasets (which is denoted by appending a minus to the dataset name). Table 4 shows the results.\nThe reader will note that rotations cause a signiﬁcant performance loss on the conventional ResNet;\nthe error increases from 2.09% to 5.39%, even when removing 6s and 9s from the dataset. With\nPTN, on the other hand, the error goes from 2.85% to 3.96%, which shows our method is more\nrobust to the perturbations, although the performance on the unperturbed datasets is slightly worse.\nWe expect the PTN to be even more advantageous when large scale variations are also present.\nD A BLATION STUDY\nWe quantify the performance boost obtained with wrap around padding, polar origin augmentation,\nand training time rotation augmentation. Results are based on the PTN-B variant trained on Rotated\nMNIST. We remove one operation at a time and verify that the performance consistently drops,\nwhich indicates that all operations are indeed helpful. Table 5 shows the results.\n13\nPublished as a conference paper at ICLR 2018\nTable 5: Ablation study. Rotation and polar origin augmentation during training time, and wrap\naround padding all contribute to reduce the error. Results are from PTN-B on the rotated MNIST.\nOrigin aug. Rotation aug. Wrap padding Error [%]\nYes Yes Yes 1.12 (0.03)\nNo Yes Yes 1.33 (0.12)\nYes No Yes 1.46 (0.11)\nYes Yes No 1.31 (0.06)\n14",
  "topic": "MNIST database",
  "concepts": [
    {
      "name": "MNIST database",
      "score": 0.809280514717102
    },
    {
      "name": "Transformer",
      "score": 0.7276771664619446
    },
    {
      "name": "Polar",
      "score": 0.571615993976593
    },
    {
      "name": "Equivariant map",
      "score": 0.5566909313201904
    },
    {
      "name": "Computer science",
      "score": 0.555551290512085
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4704640805721283
    },
    {
      "name": "Scaling",
      "score": 0.44761404395103455
    },
    {
      "name": "Invariant (physics)",
      "score": 0.4295969307422638
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3364008665084839
    },
    {
      "name": "Algorithm",
      "score": 0.33185940980911255
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.3311358094215393
    },
    {
      "name": "Physics",
      "score": 0.2960767149925232
    },
    {
      "name": "Artificial neural network",
      "score": 0.289167582988739
    },
    {
      "name": "Mathematics",
      "score": 0.27770155668258667
    },
    {
      "name": "Geometry",
      "score": 0.22165492177009583
    },
    {
      "name": "Engineering",
      "score": 0.192030131816864
    },
    {
      "name": "Pure mathematics",
      "score": 0.1636248230934143
    },
    {
      "name": "Electrical engineering",
      "score": 0.1554126739501953
    },
    {
      "name": "Voltage",
      "score": 0.15526926517486572
    },
    {
      "name": "Combinatorics",
      "score": 0.10955733060836792
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ]
}