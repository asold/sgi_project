{
  "title": "Transformer based Natural Language Generation for Question-Answering",
  "url": "https://openalex.org/W4389009369",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A222454823",
      "name": "Imen Akermi",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2099064637",
      "name": "Johannes Heinecke",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A10888176",
      "name": "Frédéric Herledan",
      "affiliations": [
        "Orange (France)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2492724717",
    "https://openalex.org/W2159169455",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2979535790",
    "https://openalex.org/W2264105282",
    "https://openalex.org/W2516930406",
    "https://openalex.org/W2043200638",
    "https://openalex.org/W1538412973",
    "https://openalex.org/W2740840489",
    "https://openalex.org/W2526471240",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W2739827909",
    "https://openalex.org/W2904097689",
    "https://openalex.org/W2788302424",
    "https://openalex.org/W2002461265",
    "https://openalex.org/W2890152674",
    "https://openalex.org/W2142321231",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2325957229",
    "https://openalex.org/W2962977247",
    "https://openalex.org/W3037420858",
    "https://openalex.org/W2604792945",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2517782820",
    "https://openalex.org/W2963419548",
    "https://openalex.org/W2575884139",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2622069672",
    "https://openalex.org/W1574447377",
    "https://openalex.org/W2467173223",
    "https://openalex.org/W2470710153",
    "https://openalex.org/W1964162497",
    "https://openalex.org/W2988256164",
    "https://openalex.org/W2135571184",
    "https://openalex.org/W2529383646",
    "https://openalex.org/W2727218090",
    "https://openalex.org/W1506013151",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2115758952",
    "https://openalex.org/W2971885209"
  ],
  "abstract": "This paper explores Natural Language Generation within the context of Question-Answering task. The several works addressing this task only focused on generating a short answer or a long text span that contains the answer, while reasoning over a Web page or processing structured data. Such answers’ length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant. In this work, we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data. Tested over English and French datasets, the proposed approach shows very promising results.",
  "full_text": "Proceedings of The 13th International Conference on Natural Language Generation, pages 349–359,\nDublin, Ireland, 15-18 December, 2020.c⃝2020 Association for Computational Linguistics\n349\nT ransformer based Natural Language Generation for Question-\nAnswering\nImen Akermi Johannes Heinecke Fr ´ed´eric Herledan\nOrange / DA T A AI\n22307 Lannion cedex, France\nimen.elakermi@orange.com\njohannes.heinecke@orange.com\nfrederic.herledan@orange.com\nAbstract\nThis paper explores Natural Language Genera-\ntion within the context of Question-Answering\ntask. The several works addressing this task\nonly focused on generating a short answer or a\nlong text span that contains the answer, while\nreasoning over a W eb page or processing struc-\ntured data. Such answers’ length are usually\nnot appropriate as the answer tend to be per-\nceived as too brief or too long to be read\nout loud by an intelligent assistant. In this\nwork, we aim at generating a concise answer\nfor a given question using an unsupervised ap-\nproach that does not require annotated data.\nT ested over English and French datasets, the\nproposed approach shows very promising re-\nsults.\n1 Introduction\nQuestion-Answering systems (QAS) aim at analyz-\ning and processing user questions in order to pro-\nvide relevant answers ( Hirschman and Gaizauskas ,\n2001). The recent popularity of intelligent assis-\ntants has increased the interest in QAS which have\nbecome a key component of “Human-Machine” ex-\nchanges since they allow users to have instant an-\nswers to their questions in natural language us-\ning their own terminology without having to go\nthrough a long list of documents to ﬁnd the appro-\npriate answers.\nMost of the existing research work focuses on\nthe major complexity of these systems residing in\nthe processing and interpretation of the question\nthat expresses the user’s need for information, with-\nout considering the representation of the answer\nitself. Usually , the answer is either represented by\na short set of terms answering exactly the question\n(case of QAS which extract answers from struc-\ntured data), or by a text span extracted from a docu-\nment which, besides the exact answer, can integrate\nother unnecessary information that are not relevant\nto the context of the question asked. The following\npresents two answers for Who is the thesis supervi-\nsor of Albert Einstein? possibly generated by two\nsystems :\nAlfred Kleiner\nAlbert Einstein is a German-born theo-\nretical physicist who developed the the-\nory of relativity, one of the two pillars of\nmodern physics.\nGiven the speciﬁcity of QAS which extract an-\nswers from structured data, users generally receive\nonly a short and limited answer to their questions\nas illustrated by the example above. This type of\nanswer representation might not meet the user ex-\npectations. Indeed, the type of answer given by\nthe ﬁrst system can be perceived as too brief not\nrecalling the context of the question. The second\nsystem returns a passage which contains informa-\ntion that are out of the question’s scope and might\nbe deemed by the user as irrelevant.\nIt is within this framework that we propose in\nthis article an approach which allows to generate\na concise answer in natural language (e.g. The\nthesis superviser of Albert Einstein was Alfred\nKleiner) that shows very promising results tested\nover French and English questions. This approach\nis a component of a QAS that we proposed in Ro-\njas Barahona et al. (2019) and that we will brieﬂy\npresent in this article.\nIn what follows, we detail in section 3 the ap-\nproach we propose for answer generation in Natural\nLanguage and we brieﬂy discuss the QAS devel-\noped. W e present in section 4 the experiments that\nwe have conducted to evaluate this approach.\n2 Related W ork\nThe huge amount of information available nowa-\ndays makes the task of retrieving relevant informa-\n350\ntion complex and time consuming. This complexity\nhas prompted the development of QAS which help\nspare the user the search and the information ﬁlter-\ning tasks, as it is often the case with search engines,\nand directly return the exact answer to a question\nasked in natural language.\nThe QAS cover mainly three tasks: question\nanalysis, information retrieval and answer extrac-\ntion (\nLopez et al. , 2011). These tasks have been\ntackled in different ways, considering the knowl-\nedge bases used, the types of questions addressed\n(Iida et al. , 2019; Zayaraz et al. , 2015; Dwivedi\nand Singh , 2013; Lopez et al. , 2011) and the way\nin which the answer is presented. In this article,\nwe particularly focus on the answer generation pro-\ncess.\nW e generally notice two forms of representation\naddressed in literature. The answer can take the\nform of a paragraph selected from a set of text\npassages retrieved from the web ( Asai et al. , 2018;\nDu and Cardie , 2018; W ang and Jiang , 2016; W ang\net al. , 2017; Oh et al. , 2016), as it can also be\nthe exact answer to the question extracted from a\nknowledge base (\nWu et al. , 2003; Bhaskar et al. ,\n2013; Le et al. , 2016).\nDespite the abundance of work in the ﬁeld of\nQAS, the answers generation issue has received lit-\ntle attention. A ﬁrst approach indirectly addressing\nthis task has been proposed in Brill et al. (2001,\n2002). Indeed, the authors aimed at diversifying\nthe possible answer patterns by permuting the ques-\ntion’s words in order to maximise the number of\nretrieved documents that may contain the answer\nto the given question. Another answer representa-\ntion approach based on rephrasing rules has also\nbeen proposed in\nAgichtein and Gravano (2000);\nLawrence and Giles (1998) within the context of\nquery expansion task for document retrieval and\nnot purposely for the question-answering task.\nThe few works that have considered this task\nwithin the QAS framework have approached it from\na text summary generation perspective ( Ishida et al. ,\n2018; Iida et al. , 2019; Rush et al. , 2015; Chopra\net al. , 2016; Nallapati et al. , 2016; Miao and Blun-\nsom, 2016; See et al. , 2017; Oh et al. , 2016; Sharp\net al. , 2016; T an et al. , 2016; dos Santos et al. ,\n2016). These works consist in generating a sum-\nmary of a single or various text spans that contain\nthe answer to a question. Most of these works have\nonly considered causality questions like the ones\nstarting with “ why” and whose answers are para-\ngraphs. T o make these answers more concise, the\nextracted paragraphs are summed up.\nOther approaches ( Kruengkrai et al. , 2017; Girju,\n2003; V erberne et al. , 2011; Oh et al. , 2013) have\nexplored this task as a classiﬁcation problem that\nconsists in predicting whether a text passage can\nbe considered as an answer to a given question.\nIt should be noted that these approaches only\nintend to diversify as much as possible the answer\nrepresentation patterns to a given question in order\nto increase the probability of extracting the correct\nanswer from the W eb and do not focus on the an-\nswer’s representation itself. It should also be noted\nthat these approaches are only applicable for QAS\nwhich extract answers as a text snippet and can-\nnot be applied to short answers usually extracted\nfrom knowledge bases. The work presented in Pal\net al. (2019) tried to tackle this issue by propos-\ning a supervised approach that was trained on a\nsmall dataset whose questions/answers pairs were\nextracted from machine comprehension datasets\nand augmented manually which make generaliza-\ntion and capturing variation very limited.\nOur answer generation approach differs from\nthese works as it is unsupervised, can be adapted to\nany type of factual question (except for why) and\nis based only on easily accessible and unannotated\ndata. Indeed, we build upon the intuitive hypoth-\nesis that a concise answer and easily pronounced\nby an intelligent assistant can in fact consist of a\nreformulation of the question asked. This approach\nis a part of a QAS that we have developed in Ro-\njas Barahona et al. (2019) that extracts the answer\nto a question from structured data.\nIn what follows, we detail in section 3 the ap-\nproach we propose for answer generation in Natural\nLanguage and we brieﬂy discuss the QAS devel-\noped. W e present in section 4 the experiments that\nwe have conducted to evaluate this approach. and\nwe conclude in section 5 with the limitations noted\nand the perspectives considered.\n3 NLG Approach for Answer Generation\nThe answer generation approach proposed is a com-\nponent of a system which was developed in Ro-\njas Barahona et al. (2019) and which consists in a\nspoken conversational question-answering system\nwhich analyses and translates a question in natural\nlanguage (French or English) in a formal repre-\nsentation that is transformed into a Sparql query 1.\n1 https://www .w3.org/TR/sparql11-overview/\n351\nThe Sparql query helps extracting the answer to the\ngiven question from an RDF knowledge base, in\nour case Wikidata 2. The extracted answer takes the\nform of a list of URIs or values.\nAlthough the QAS that we have developed ( Ro-\njas Barahona et al. , 2019) is able to ﬁnd the cor-\nrect answer to a question, we have noticed that\nits short representation is not user-friendly . There-\nfore, we propose an unsupervised approach which\nintegrates the use of Transformer models such as\nBE RT (Devlin et al. , 2019) and GPT ( Radford et al. ,\n2018). The choice of an unsupervised approach\narises from the fact that there is no available train-\ning dataset associating a question with an exhaus-\ntive and concise answer at the same time. such\ndataset could have helped use an End-to-End learn-\ning neural architecture that can generate an elabo-\nrated answer to a question.\nThis approach builds upon the fact that we have\nalready extracted the short answer to a given ques-\ntion and assumes that a user-friendly answer can\nconsist in rephrasing the question words along with\nthe short answer. This approach is composed of\ntwo fundamental phases: The dependency analysis\nof the input question and the answer generation\nusing Transformer models.\n3.1 Dependency parsing\nFor the dependency analysis, we use an extended\nversion of UDPipeFuture ( Straka, 2018) which\nshowed its state of the art performance by becom-\ning ﬁrst in terms of the Morphology-aware Labeled\nAttachment Score (MLAS) 3 metric at the CoNLL\nShared T ask of dependency parsing in 2018 ( Ze-\nman et al. , 2018). UDPipeFuture is a POS tagger\nand graph parser based dependency parser using a\nBiLSTM, inspired by\nDozat et al. (2017).\nOur modiﬁcation consisted in adding several con-\ntextual word embeddings (with respect to the lan-\nguage). In order to ﬁnd the best conﬁguration we\nexperimented with models like multilingual B E RT\n(Devlin et al. , 2019), XLM-R ( Conneau et al. , 2019)\n(for both, English and French), RoB E RTA ( Liu\net al. , 2019) (for English), FlauB E RT (Le et al. ,\n2020) and CamemB E RT (Martin et al. , 2019) (for\nFrench) during the training of the treebanks French-\n2 https://www .wikidata.org/\n3 MLAS is a metric which takes into account POS tags and\nmorphological features. It is inspired by the Content-W ord\nLabeled Attachment Score (CLAS,\nNivre and Fang (2017)\nwhich differentiates between content word and function words.\nBoth are derived from the standard Labeled Attachment Score\n(LAS) metric.\nGSD and English-EWT 4 , of the Universal Depen-\ndencies project (UD) ( Nivre et al. , 2016)5. Adding\ncontextual word embedding increases signiﬁcantly\nthe results for all metrics, LAS, CLAS and MLAS\n(cf. table 1). This is the case for all languages\n(of the CoNLL shared task), where language spe-\nciﬁc contextual embeddings or multingual ones\n(as B E RT or XLM-R) improved parsing ( Heinecke,\n2020)\nFrench (Fr-GSD)\nembeddings MLAS CLAS LAS\nStraka (2018) 77.29 82.49 85.74\nFlauB E RT 79.53 84.16 87.98\nBE RT 81.64 86.21 89.68\nCamemB E RT 82.17 86.45 89.67\nXLM-R 82.62 86.94 89.82\nEnglish (En-EWT)\nembeddings MLAS CLAS LAS\nStraka (2018) 74.71 79.14 82.51\nBE RT 81.16 85.89 88.63\nRoB E RTA 82.38 86.89 89.40\nXLM-R 82.91 87.24 89.54\nT able 1: Dependency Analysis for English and French\n(UD v2.2) using different contextual word embeddings,\nbest results in bold\nIn order to parse simple, quiz-like questions, the\ntraining corpora of the two UD treebanks are not\nappropriate (enough), since both treebanks do not\ncontain many questions, if at all 6.\nAn explanation for bad performance on ques-\ntions of parser models trained on standard UD is the\nfact, that in both languages, the syntax of questions\ndiffers from the syntax of declarative sentences:\napart from wh question words, in English the to do\nperiphrasis is nearly always used in questions. In\nFrench, subject and direct objects can be inversed\nand the est-ce que construction appears frequently .\nBoth, the English to do periphrasis and the French\nest-ce que construction are absent in declarative\nsentences. T able 2 shows the (much lower) results\nwhen parsing questions using models trained only\non the standard UD treebanks.\nIn order to get a better analysis, we decided to\n4 As for the Shared T ask CoNLL 2018, we use version 2.2\nto be able to compare with the ofﬁcial results\n5 https://universaldependencies.org/\n6 At least for French a question treebank exists within the\nUD project (French-FQB, Seddah and Candito (2016)). How-\never its questions are rather long and literary , not like thoses\nused in quizzes.\n352\nFrench (Fr-GSD)\nembeddings MLAS CLAS LAS\nBE RT 60.52 73.04 79.27\nCamemB E RT 61.32 75.26 80.49\nFlauB E RT 58.09 70.96 78.40\nW ord2V ec 59.83 74.43 80.14\nXLM-R 59.23 73.52 79.27\nEnglish (En-EWT)\nembeddings MLAS CLAS LAS\nBE RT 80.45 88.02 90.58\nRoB E RTa 80.68 89.17 91.49\nXLM-R 80.68 89.42 91.88\nT able 2: Dependency Analysis of questions using mod-\nels trained on the standard UD treebanks\nannotate additional sentences (quiz-like questions)\nand add this data to the basic treebanks.\nFor English we annotated 309 questions (plus 91\nquestions for validation) from the QALD7 ( Usbeck\net al. , 2017) and QALD8 corpora 7. For French we\ntranslated the QALD7 questions into French and\nformulated others ourselves (276 train, 66 valida-\ntion). For the annotations we followed the general\nUD guidelines 8 as well as the treebank speciﬁc\nguidelines of En-EWT and Fr-GSD.\nAs table 3 shows, the quality of the dependency\nanalysis improves considerably . The contextual\nword embeddings CamemB E RT (for French) and\nBE RT (English) have the biggest impact.\nFrench (Fr-GSD)\nembeddings MLAS CLAS LAS\nBE RT 91.20 96.10 97.55\nCamemB E RT 92.12 97.37 98.26\nFlauB E RT 90.53 94.74 96.86\nW ord2V ec 90.88 95.79 97.21\nXLM-R 91.23 96.14 97.56\nEnglish (En-EWT)\nembeddings MLAS CLAS LAS\nBE RT 84.85 91.92 94.24\nRoB E RTa 83.08 91.67 93.85\nXLM-R 83.08 90.66 93.59\nT able 3: Dependency analysis of questions using mod-\nels trained on enriched UD treebanks\nW e rely on the UdpipeFuture version\nwhich we have improved with B E RT (for\nEnglish)/CamemB E RT (for French) and which\n7 https://github.com/ag-sc/QALD\n8 https://universaldependencies.org/guidelines.html\ngives the best results in terms of dependency\nanalysis, in order to proceed with the partitioning\nof the question into textual fragments (also called\nchunks): Q = {c1,c2 ,...,cn }.\nIf we take the example of the question What is\nthe political party of the mayor of P aris? , the set\nof textual fragments would be Q = {What, is, the\npolitical party of the mayor of P aris }.\n3.2 Answer generation\nDuring this phase, we ﬁrst carry out a ﬁrst test of\nthe set Q to check whether the text fragment which\ncontains a question marker (exp: what, when, who\netc.) represents the subject nsubj in the analysed\nquestion. If so, we simply replace that text frag-\nment with the answer we identiﬁed earlier. Let\nus take the previous example What is the political\nparty of the mayor of P aris? , the system automati-\ncally detects that the text fragment containing the\nquestion marker What represents the subject and\nwill therefore be replaced directly by the exact an-\nswer The Socialist P arty . Therefore, the concise\nanswer generated will be The Socialist P arty is the\npolitical party of the mayor of P aris .\nOtherwise, we remove the text fragment con-\ntaining the question marker that we detected\nand we add the short answer\nR to Q: Q =\n{c1 ,c2 ,...,cn− 1 ,R}\nUsing the text fragments set Q, we proceed with\na permutation based generation of all possible an-\nswer structures that can form the sentence answer-\ning the question asked:\nS = {s1(R,c1,c2 ,...,cn− 1),\ns2 (c1 ,R,c2 ,...,cn− 1),\n...,\nsm(c1,c2 ,...,cn− 1,R)}\nThese structures will be evaluated by a Language\nModel (LM) based on Transformer models which\nwill extract the most probable sequence of text\nfragments that can account for the answer to be\nsent to the user:\nstructure∗ = s ∈ S; p(s) =argmaxsi ∈ S p(si)\nOnce the best structure is identiﬁed, we initiate\nthe generation process of possible missing words.\nIndeed, we suppose that there could be some terms\nwhich do not necessarily appear in the question\nor in the short answer but which are, on the other\nhand, necessary to the generation of a correct gram-\nmatical structure of the ﬁnal answer.\nThis process requires that we set two parameters,\nthe number of possible missing words and their\n353\npositions within the selected structure. In this paper,\nwe experiment the assumption that one word could\nbe missing and that it is located before the short\nanswer within the identiﬁed structure, as it could\nbe the case for a missing article ( the, a , etc.) or a\npreposition ( in, at , etc.) for example.\nTherefore, to predict this missing word, we use\nBE RT as the generation model (GM) for its ability\nto capture bidirectionally the context of a given\nword within a sentence. In case when B E RT returns\na non-alphabetic character sequence, we assume\nthat the optimal structure, as predicted by the LM,\ndoes not need to be completed by an additional\nword. The following example illustrates the differ-\nent steps of the proposed approach:\nQuestion: When did princess Diana die?\n1. Question parsing and answer extraction using\nthe system proposed in Rojas Barahona et al.\n(2019):\nshort answer = {August 31, 1997 }\n2. Chunking the question into text fragments us-\ning the UDPipe based dependency analysis:\nQ={When, did die , princess Diana }\n3. Removing question marker fragment ( when)\nand updating the verb tense and form using a\nrule-based approach that we have deﬁned:\nQ={died, princess Diana }\n4. Adding the short answer:\nQ={died; princess Diana ; August 31, 1997 }\n5. Generating the set of possible answer struc-\ntures S:\nS={died princess Diana August 31, 1997 ;\n. August 31, 1997 died princess Diana ;\n. princess Diana died August 31, 1997 ;\n. . . . }\n6. Evaluating the different answer structures us-\ning a LM: p(structure*) = argmaxsi ∈ S p(si):\nstructure∗ = princess Diana died August 31,\n1997\n7. Generating possible missing word for struc-\nture∗ with B E RT:\nPrincess Diana died [missing word] August\n31, 1997\n(missing word = on)\nAnswer: Princess Diana died on August 31, 1997.\n4 Experiments and Evaluation\nThe existing QAS test sets are more tailored to sys-\ntems which generate the exact short answer to a\nquestion or more focused on the Machine Read-\ning Comprehension task where the answer consists\nof a text passage from a document containing the\nshort answer. Therefore, we have created a dataset\nwhich maps questions extracted from the QALD-7\nchallenge dataset ( Usbeck et al. , 2017) with nat-\nural language answers which were deﬁned by a\nlinguist and which we individually reviewed. This\ndataset called Q U E R E O consists of 150 questions\nwith the short answers extracted by the QAS that\nwe described above. W e denote an average of three\npossible gold sanswers in natural language for each\nquestion. French and English versions were created\nfor this dataset.\n{ possible\nanswer\nstructures } \nLM structure*\nmissing\nword\ngeneration\nanswer\n{structures}\nA1\nLM Generation\nB ERT en , fr\nCamemB ERT fr\nFlauB ERT fr\nXLM en , fr\nXLM- Ro B ERT a fr\nXLNet en\nGPT en\nGPT2 en\nB ERT\nCamemB ERT\nArch.\nA1\nA2\nMetrics\nB LEU\nM ETEOR\nR OUGE\nB ERT Score\n{ possible\nanswer\nstructures } \nA2\nmissing\nword\ngeneration\nLM\nstructure* \n=  answer\nFigure 1: Experiment framework\nAs illustrated in ﬁgure 1, two possible architec-\ntures of the approach proposed for answer genera-\ntion have been evaluated. The ﬁrst architecture A1\nconsists in generating all possible answer structures\nin order to have them evaluated afterwards by a LM\nwhich will identify the optimal answer structure to\nwhich we generate possible missing words. Archi-\ntecture A2 starts with generating missing words for\neach structure in S which will then be evaluated by\nthe LM. In this paper, we assume that there is only\none missing word per structure.\nT o evaluate the proposed approach, we have re-\nferred to standard metrics deﬁned for NLG tasks\nsuch as Automatic Translation and Summarization,\nas they allow to assess to what extent a generated\nsentence is similar to the gold sentence. W e con-\n354\nsider three N-gram metrics ( B L E U, M E T E O R and\nRO U G E) and the B E RT score metric which exploits\nthe pre-trained embeddings of B E RT to calculate\nthe similarity between the answer generated and the\ngold answer. T o be able to compare the different\nconﬁgurations of the approach, we refer to Fried-\nman’s test (\nMilton, 1939) which allows to detect\nthe performance variation of different conﬁgura-\ntions of a model evaluated by several metrics based\non the average ranks.\n25\n 50\nrank\nA1/Bert/Cmbert-base\nA2/Bert/flaubert-small\nA1/Bert/xlm-roberta-base\nA1/Bert/flaubert-base-unc\nA1/Bert/xlm-mlm-enfr-1024\nA1/Bert/xlm-roberta-large\nA2/Bert/gpt2\nA1/Bert/bert-base-mlg-unc\nA1/Bert/bert-base-mlg\nA2/Bert/xlm-clm-enfr-1024\nA1/Bert/xlm-clm-enfr-1024\nA2/Bert/xlm-mlm-enfr-1024\nA2/Bert/flaubert-large\nA2/Bert/xlm-roberta-base\nA1/Bert/flaubert-large\nA1/Bert/flaubert-small\nA1/Bert/openai-gpt\nA2/Bert/Cmbert-base\nA2/Bert/flaubert-base\nA2/Bert/flaubert-base-unc\nA2/Bert/xlm-roberta-large\nA1/Bert/gpt2\nA2/Bert/bert-base-mlg-unc\nA2/Bert/gpt2-medium\nA1/Bert/gpt2-large\nA2/Bert/bert-base-mlg\nA1/Bert/gpt2-medium\nA1/Bert/flaubert-base\nA2/Cmbert/xlm-roberta-base\nA1/Cmbert/xlm-roberta-base\nA1/Cmbert/Cmbert-base\nA2/Cmbert/flaubert-small\nA2/Bert/openai-gpt\nA1/Cmbert/xlm-roberta-large\nA1/Cmbert/xlm-mlm-enfr-1024\nA1/Cmbert/bert-base-mlg-unc\nA2/Cmbert/bert-base-mlg-unc\nA1/Cmbert/flaubert-base-unc\nA2/Cmbert/Cmbert-base\nA2/Cmbert/bert-base-mlg\nA2/Cmbert/flaubert-large\nA2/Cmbert/xlm-roberta-large\nA1/Cmbert/bert-base-mlg\nA1/Cmbert/gpt2-large\nA1/Cmbert/xlm-clm-enfr-1024\nA2/Cmbert/gpt2\nA1/Cmbert/flaubert-small\nA2/Cmbert/gpt2-medium\nA2/Cmbert/xlm-mlm-enfr-1024\nA2/Cmbert/flaubert-base-unc\nA1/Cmbert/flaubert-base\nA1/Cmbert/flaubert-large\nA1/Cmbert/gpt2-medium\nA1/Cmbert/gpt2\nA1/Cmbert/openai-gpt\nA2/Cmbert/openai-gpt\nA2/Cmbert/flaubert-base\nA2/Cmbert/xlm-clm-enfr-1024\nconfigurations\nBleu\nRouge\nMeteor\nBertSc.\nHuman\nacc.\nFigure 2: Correlation assessment between human eval-\nuation and the Bleu, Meteor, Rouge and Bert scores -\nFrench Q/A (“CmBert” stands for CamemB E RT)\nFigure 3: Screenshot of the evaluation tool\nW e also conducted a human evaluation study for\nthe French and the English versions of the dataset,\nin which we asked 20 native speakers participants\nto evaluate the relevance of a generated answer\n(correct or not correct ) regarding a given question\nwhile indicating the type of errors depicted ( gram-\nmar, wrong preposition , word order , extra word(s) ,\netc). Figure 3 presents the evaluation framework\nthat we have implemented and provided to the par-\nticipants. The results of each participant are saved\nin a json-ﬁle (ﬁgure 4). The inter-agreement rate\nbetween participants reached 70% which indicates\na substantial agreement. Through the human eval-\nuation study , we wanted to explore to what extent\nthe standard metrics are reliable to assess NLG ap-\nproaches within the context of question-answering\nsystems.\nT able 4 (French dataset) represents the obtained\nresults for the ﬁrst three best models according to\nthe human evaluation ranking and the Friedman\ntest ranking. W e indicate between brackets each\nmodel’s rank according to the metric used.\nW e note that the highest human accuracy score\nfor French of about 85% was scored with the ﬁrst\narchitecture coupled with B E RT as the generation\nmodel (GM) and CamemB E RT as the language\nmodel (LM). W e also notice that the architecture\nA1, which considers the LM assessment of the struc-\nture before generating missing words, performs bet-\nter. Surprisingly , as a generative model, the multi-\n355\nHum. Frm. Arch. GM LM H. Acc BL E U ME T E O R RO U G E BE RTS\nrank rank score score rank score rank score rank score rank\n1 1 A1 BT CmBt 84.85 86.28 [1] 96.76 [1] 93.69 [6] 97.89 [2]\n2 2 A2 B T FB T-s-c 84.09 85.87 [7] 96.75 [2] 94.22 [1] 97.96 [1]\n2 3 A1 B T XRob 84.09 85.93 [4] 96.63 [6] 93.79 [5] 97.88 [3]\n2 9 A1 B T BT-ml-c 84.09 85.01 [19] 96.52 [22] 93.81 [4] 97.79 [7]\n5 4 A1 B T FB T-b-uc 83.33 86.17 [2] 96.72 [3] 93.56 [14] 97.81 [6]\n5 5 A1 B T mlm-1024 83.33 85.39 [10] 96.60 [8] 93.61 [10] 97.83 [4]\n5 6 A2 B T G P T 2 83.33 85.46 [9] 96.67 [4] 93.48 [17] 97.76 [10]\n5 10 A2 B T clm-1024 83.33 85.89 [6] 96.55 [18] 93.57 [13] 97.71 [19]\n5 11 A1 B T clm-1024 83.33 84.99 [20] 96.52 [23] 93.87 [3] 97.76 [12]\n5 12 A2 B T FB T-l-c 83.33 86.15 [3] 96.57 [13] 93.14 [37] 97.79 [8]\n5 13 A2 B T mlm-1024 83.33 85.90 [5] 96.54 [20] 93.30 [27] 97.76 [11]\n5 14 A2 B T XRob 83.33 85.32 [13] 96.46 [28] 93.63 [9] 97.71 [17]\nT able 4: Model ranking for French dataset according to the human evaluation study (best in bold) and the Friedman\ntest (best in yellow). “B T” in Column GM stands for B E RT-base-multilingual-cased. In column LM we use\n“CmB T” for CamemB E RT-base, “B T-ml-c” for B E RT-base-multilingual-cased, “XRob” for XLM-RoB E RTa-base,\n“FB T-s-c” for FlauB E RT-small-cased, “FB T-b-uc” for FlauB E RT-base-uncased and “clm-1024” for XLM-clm-\nenfr-1024\nlingual B E RT model predicts missing words better\nthan CamemB E RT for French sentences. These\nﬁndings are also conﬁrmed by the Friedman test\nwhere we can clearly see that the ﬁrst ranked con-\nﬁguration maps the best conﬁguration selected ac-\ncording to the human accuracy , with a very slight\ndifference for the other four conﬁgurations. Let us\nsee if that means that the four metrics are corre-\nlated with the human accuracy . According to table\n6 which presents the Pearson correlation ( Benesty\net al. , 2009) of the human accuracy with the four\nmetrics and to ﬁgure 2 which illustrates the rank-\ning given by each evaluation metric along with the\nhuman judgement for each conﬁguration (i.e. con-\nﬁguration = GM × architecture × LM) tested, we\ncan clearly see that the human evaluation results are\npositively and strongly correlated with the B L E U,\nthe M E T E O R and the B E RT scores. These metrics\nare practically matching the human ranking and\nthus are obviously able to identify which conﬁgura-\ntion gives better results. The rouge metric, used for\nFrench question/answer evaluation, is moderately\ncorrelated with the human evaluation which means\nthat we should not only rely on this metric when\nassessing such task. On the other hand, when the\nRO U G E metric is considered with the other metrics,\nit helps to get closer to the human judgement.\nT able 5 presents the results for the English\ndataset and shows that the best accuracy scored\nis about 72% with A1, B E RT as the generative\nmodel and the Generative Pretrained Transformer\n(GPT) as the language model. According to the\nﬁrst three conﬁgurations, architecture A2 prevails\nand the GPT transformer takes over the other lan-\nguage models. These results are also conﬁrmed by\nthe Friedman test with a very slight difference on\nthe ranking and also upheld with the correlation\nscores between the human assessment and each of\nthe four metrics as shown by ﬁgure 5 and table 6.\nThese ﬁndings mean that we actually can rely\non the use of these standard metrics to evaluate the\nanswer generation task for question-answering.\nW e also tried to analyse the errors indicated by\nthe participants. As we can note from ﬁgure 6, the\nmost common error reported for both English and\nFrench datasets is the word order which sheds the\nlight on a problem related to the language model\nassessment phase. The second most reported er-\nror addresses the generation process, whether to\nindicate that there are one or more missing words\nwithin the answer (French) or the presence of some\nodd words (English).\nWhen trying to get an insight on the answers\ngenerated by the current intelligent systems such as\nGoogle assistant and Alexa, we noted that these sys-\ntems are very accurate when extracting the correct\nanswer to a question and can sometimes generate\nuser-friendly answers that help recall the question\ncontext, specially with Alexa. However, we no-\nticed that most of the answers generated by these\nsystems are more verbose than necessary , we also\nfound out that when addressing yes/no questions,\nthese systems generally settle for just a yes or no\nwithout elaborating, or, on the other hand, present\na text span extracted from a W eb page and let the\nuser guess the answer. Let us take for example\nthe following question W as US president Jackson\ninvolved in a war?\n356\nHum. Frm. Arch. GM LM H. Acc BL E U ME T E O R RO U G E BE RTS\nrank rank score score rank score rank score rank score rank\n1 1 A2 BT-ml G P T 72.36 78.25 [2] 94.63 [1] 92.83 [2] 97.21 [3]\n1 2 A1 B T-ml G P T 72.36 78.25 [1] 94.51 [2] 92.53 [10] 97.23 [2]\n3 2 A1 B T G P T 71.55 77.12 [6] 94.45 [3] 92.80 [5] 97.32 [1]\n3 4 A2 B T G P T 2 71.55 76.98 [7] 94.40 [7] 92.86 [1] 97.17 [5]\n3 5 A2 B T-ml G P T2 71.55 77.53 [4] 94.39 [8] 92.82 [4] 97.14 [6]\n3 6 A2 B T-ml G P T2-l 71.55 77.85 [3] 94.41 [5] 92.65 [7] 97.07 [10]\n3 7 A2 B T-ml G P T2-m 71.55 77.42 [5] 94.40 [6] 92.58 [9] 97.10 [9]\n3 7 A2 B T G P T 2-m 71.55 75.96 [13] 94.41 [4] 92.60 [8] 97.18 [4]\n3 10 A2 B T G P T 71.55 76.28 [11] 94.30 [9] 92.76 [6] 97.14 [7]\n10 7 A2 B T G P T 2-l 70.73 76.74 [8] 94.26 [10] 92.83 [3] 97.14 [8]\n10 30 A1 B T-ml B T-b-uc 70.73 74.85 [30] 93.94 [31] 90.86 [26] 96.61 [28]\nT able 5: Model ranking for English dataset according to the human evaluation study (best in bold) and the Friedman\nranking (best in yellow). In Column GM we use “B T-ml” for B E RT-base-multilingual-cased and “B T” for B E RT-\nlarge-cased. In column LM “ G P T” stands for for OpenAI-GPT , “ G P T2-l” for GPT2-large, “ G P T2-m” for GPT2-\nmedium, “ G P T2” for GPT2, “B T-b-uc” for B E RT-base-uncased, “mlm-2048” for XLM-mlm-en-2048 and “B T-l-c”\nfor B E RT-large-cased.\n[\n{\n\"ID\":\n\"quereo_5.4\",\n\"QUESTION\":\n\"Quelles sont les companies\nd’´electronique fond ´ees\n`a Beijing ?\",\n\"SHORT_ANSWER\":\n[ \"Xiaomi\", \"Lenovo\" ],\n\"GENERATED_ANSWER\":\n\"Les companies d’ ´electronique\nfond´ees `a beijing sont xiao\nxiaomi et lenovo\",\n\"MISSING_WORD\":\n\"Xiao\",\n\"EVALUATION\":\n\"correcte\",\n\"ERROR\":\n[ \"aucun\" ],\n\"COMMENT\": \"\"\n},\n{\n\"ID\":\n\"quereo_8.8\",\n\"QUESTION\":\n\"Combien de films\na r ´ealis´e Park Chan-wook ?\",\n\"SHORT_ANSWER\":\n[ \"quatorze\" ],\n\"GENERATED_ANSWER\":\n\"Quatorze films a r ´ealis´e\npark chan-wook\",\n\"MISSING_WORD\":\n\".\",\n\"EVALUATION\":\n\"incorrecte\",\n\"ERROR\":\n[ \"ordre\", \"accord\" ],\n\"COMMENT\": \"\"\n},\n...\n]\nFigure 4: Extract of a human evaluation result\n20\n 40\nrank\nA2/Bert-mlg/openai-gpt\nA1/Bert-mlg/openai-gpt\nA1/Bert/openai-gpt\nA2/Bert-mlg/gpt2\nA2/Bert/gpt2\nA2/Bert-mlg/gpt2-large\nA2/Bert/gpt2-large\nA2/Bert-mlg/gpt2-medium\nA2/Bert/gpt2-medium\nA2/Bert/openai-gpt\nA2/Bert/xlm-mlm-en-2048\nA1/Bert/gpt2\nA1/Bert/xlm-clm-enfr-1024\nA1/Bert/gpt2-large\nA2/Bert/xlm-mlm-enfr-1024\nA1/Bert/xlm-mlm-en-2048\nA1/Bert/gpt2-medium\nA1/Bert-mlg/bert-large\nA1/Bert-mlg/xlm-mlm-en-2048\nA2/Bert-mlg/bert-base-unc\nA1/Bert/xlm-mlm-enfr-1024\nA2/Bert/xlm-clm-enfr-1024\nA1/Bert-mlg/gpt2-large\nA1/Bert-mlg/gpt2-medium\nA2/Bert/xlnet-large\nA2/Bert-mlg/bert-base\nA1/Bert-mlg/gpt2\nA1/Bert-mlg/xlm-mlm-enfr-1024\nA1/Bert-mlg/xlm-clm-enfr-1024\nA1/Bert-mlg/bert-base-unc\nA1/Bert/xlnet-large\nA2/Bert-mlg/xlm-mlm-enfr-1024\nA1/Bert/bert-base-unc\nA1/Bert-mlg/bert-base\nA1/Bert-mlg/bert-base-mlg\nA2/Bert-mlg/xlm-mlm-en-2048\nA1/Bert/bert-base-mlg\nA1/Bert-mlg/xlnet-large\nA1/Bert/bert-large\nA2/Bert/bert-base\nA2/Bert/bert-base-mlg\nA2/Bert/bert-large\nA2/Bert-mlg/bert-large\nA1/Bert/bert-base\nA2/Bert/bert-base-unc\nA2/Bert-mlg/bert-base-mlg\nA2/Bert-mlg/xlm-clm-enfr-1024\nA2/Bert-mlg/xlnet-large\nconfigurations\nBleu\nRouge\nMeteor\nBertSc.\nHuman\nacc.\nFigure 5: Correlation assessment between human eval-\nuation and the Bleu, Meteor, rouge and Bert scores -\nEnglish Q/A\n357\nMetrics Pearson Correlation\nQU E R E O-fr QU E R E O-en\nBL E U 98% 85%\nME T E O R 99% 80%\nRO U G E 46% 83%\nBE RT-score 97% 88%\nT able 6: Pearson Correlation of the four metrics with\nthe human evaluation/judgement\nextra\nwords\ngrammar\n missing\nwords\nprepo-\nsitions\nword\norder\nerror categories\n0\n20\n40\n60\n% of all errors\nEnglish\nFrench\nFigure 6: Distribution of generation errors\nAndrew Jackson, who served as a major\ngeneral in the W ar of 1812, commanded\nU.S. forces in a ﬁve-month campaign\nagainst the Creek Indians, allies of the\nBritish.\nHere’s something I found on the W eb.\nAccording to constitutioncenter .org: Af-\nter the W ar of 1812, Jackson led military\nforces against the Indians and was in-\nvolved in treaties that led to the reloca-\ntion of Indians.\nThe user has to focus on the returned text frag-\nment in order to guess that the answer to his ques-\ntion is actually yes. This issue was particularly\nnoted when addressing French questions. If we\nalso take the example How many grandchildren\ndid Jacques Cousteau have ? the two systems an-\nswer as follows:\nF abien Cousteau, Alexandra Cousteau,\nPhilippe Cousteau Jr ., C ´eline Cousteau.\nJacques Cousteau’s grandchildren\nwere Philippe Cousteau Jr ., Alexandra\nousteau, C\n´eline Cousteau, and F abien\nCousteau\nHowever, the user is not asking about the names\nof Cousteau’s grand-children and has to guess by\nhimself that the answer for this question is four.\nA more accurate answer should indicate the exact\nanswer to the question and then elaborate Jacques\nCousteau had four grand-children. But these sys-\ntems perform better in case when the terms em-\nployed in the question are not necessarily relevant\nto the answer. If we take the example of the ques-\ntion who is the wife of Lance Bass , the approach\nthat we propose will generate The wife of Lance\nBass is Michael Turchin . As we can note the an-\nswer generated was not adapted to the actual an-\nswer, while the other systems are able to detect\nsuch nuance:\nLance Bass is married to Michael\nTurchin. They have been married since\n2014.\nThis issue has still to be addressed.\n5 Conclusion and perspectives\nW e have put forward, in this paper, an approach\nfor Natural Language Generation within the frame-\nwork of the question-answering task that considers\ndependency analysis and probability distribution\nof words sequences. This approach takes part of a\nquestion/answering system in order to help gener-\nate a user-friendly answer rather than a short one.\nThe results obtained through a human evaluation\nand standard metrics tested over French and En-\nglish questions are very promising and shows a\ngood correlation with human judgement. However,\nwe intend to put more emphasis on the Language\nModel choice as reported by the human study and\nconsider the generation of more than one missing\nword within the answer.\nReferences\nEugene Agichtein and Luis Gravano. 2000. Snowball:\nExtracting relations from large plain-text collections.\nIn Proceedings of the ﬁfth ACM conference on Digi-\ntal libraries , pages 85–94.\nAkari Asai, Akiko Eriguchi, Kazuma Hashimoto, and\nY oshimasa Tsuruoka. 2018. Multilingual extractive\nreading comprehension by runtime machine transla-\ntion.\nhttps://arxiv .org/abs/1809.03275.\nJacob Benesty , Jingdong Chen, Yiteng Huang, and Is-\nrael Cohen. 2009. P earson Correlation Coefﬁcient ,\npages 1–4. Springer Berlin Heidelberg, Berlin, Hei-\ndelberg.\nPinaki Bhaskar, Somnath Banerjee, Partha Pakray ,\nSamadrita Banerjee, Sivaji Bandyopadhyay , and\nAlexander Gelbukh. 2013. A hybrid question\nanswering system for Multiple Choice Question\n358\n(MCQ). In Question Answering for Machine Read-\ning Evaluation (QA4MRE) at CLEF 2013 .\nEric Brill, Susan Dumais, and Michele Banko. 2002.\nAn analysis of the AskMSR question-answering sys-\ntem. In EMNLP 2002 , pages 257–264. ACL.\nEric Brill, Jimmy Lin, Michele Banko, Susan Dumais,\nand Andrew Ng. 2001. Data-intensive question an-\nswering. In TREC 2001 , pages 393–400.\nSumit Chopra, Michael Auli, and Alexander M. Rush.\n2016. Abstractive sentence summarization with at-\ntentive recurrent neural networks. In NAACL 2016 ,\npages 93–98.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nV ishrav Chaudhary , Guillaume W enzek, Francisco\nGuzm´an, Edouard Grace, Myle Ott, Luke Zettle-\nmoyer, and V eselin Stoyanov . 2019. Unsupervised\nCross-lingual Representation Learning at Scale.\nhttps://arxiv .org/abs/1911.02116.\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova. 2019. BER T: Pre-training of\ndeep bidirectional transformers for language under-\nstanding\n. In NAACL, pages 4171–4186, Minneapo-\nlis. ACL.\nTimothy Dozat, Peng Qi, and Christopher D. Manning.\n2017. Stanford’s Graph-based Neural Dependency\nParser at the CoNLL 2017 Shared T ask . In CoNLL\n2017 Shared T ask. Multilingual P arsing from Raw\nT ext to Universal Dependencies , pages 20–30, V an-\ncouver, Canada. ACL.\nXinya Du and Claire Cardie. 2018. Harvest-\ning paragraph-level question-answer pairs from\nWikipedia. In ACL 2018 , pages 1907–1917, Mel-\nbourne, Australia. ACL.\nSanjay K. Dwivedi and V aishali Singh. 2013. Research\nand reviews in question answering system. Procedia\nT echnology, 10:417–424.\nRoxana Girju. 2003. Automatic detection of causal re-\nlations for question answering. In ACL 2003 , pages\n76–83. ACL.\nJohannes Heinecke. 2020. Hybrid enhanced Universal\nDependencies parsing . In International Conference\non P arsing T echnologies and the IWPT 2020 Shared\nT ask on P arsing into Enhanced Universal Dependen-\ncies, pages 174–180, Online. ACL.\nLynette Hirschman and Robert Gaizauskas. 2001. Nat-\nural language question answering: the view from\nhere. natural language engineering , 7(4):275–300.\nRyu Iida, Canasai Kruengkrai, Ryo Ishida, Kentaro\nT orisawa, Jong-Hoon Oh, and Julien Kloetzer. 2019.\nExploiting background knowledge in compact an-\nswer generation for why-questions. In AAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n142–151.\nRyo Ishida, Kentaro T orisawa, Jong-Hoon Oh, Ryu\nIida, Canasai Kruengkrai, and Julien Kloetzer. 2018.\nSemi-distantly supervised neural model for generat-\ning compact answers to open-domain why questions.\nIn 32nd AAAI Conference on Artiﬁcial Intelligence .\nCanasai Kruengkrai, Kentaro T orisawa, Chikara\nHashimoto, Julien Kloetzer, Jong-Hoon Oh, and\nMasahiro T anaka. 2017. Improving event causal-\nity recognition with multiple background knowl-\nedge sources using multi-column convolutional neu-\nral networks. In 31st AAAI Conference on Artiﬁcial\nIntelligence.\nSteve Lawrence and C. Lee Giles. 1998. Context and\npage analysis for improved web search. IEEE Inter-\nnet computing , 2(4):38–46.\nHang Le, Lo ¨ıc V ial, Jibril Frej, V incent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Beno ˆıt Crabb ´e, Laurent Besacier, and Didier\nSchwab. 2020. FlauBER T: Unsupervised Language\nModel Pre-training for French. In LREC 2020 .\nJuan Le, Chunxia Zhang, and Zhendong Niu. 2016.\nAnswer extraction based on merging score strat-\negy of hot terms. Chinese Journal of Electronics ,\n25(4):614–620.\nYinhan Liu, Myle Ott, Naman Goyal, Mandar Du,\nJingfei adn Joshi, Danqi Chen, Mike Lewis,\nLuke Zettlemoyer, and V eselin Stoyanov . 2019.\nRoBER T a: A Robustly Optimized BER T Pretrain-\ning Approach.\nhttps://arxiv .org/abs/1907.11692.\nV anessa Lopez, V ictoria Uren, Marta Sabou, and En-\nrico Motta. 2011. Is question answering ﬁt for the\nsemantic web?: a survey . Semantic W eb , 2(2):125–\n155.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSu´arez, Y oann Dupont, Laurent Romary , ´Eric V ille-\nmonte de la Clergerie, Djam ´e Seddah, and Beno ˆıt\nSagot. 2019. CamemBER T: a T asty French Lan-\nguage Model.\nhttps://arxiv .org/abs/1911.03894.\nYishu Miao and Phil Blunsom. 2016. Language as a\nlatent variable: Discrete generative models for sen-\ntence compression. EMNLP 2016 .\nFriedman Milton. 1939. A correction: The use of ranks\nto avoid the assumption of normality implicit in the\nanalysis of variance. Journal of the American Statis-\ntical Association , 34(205):109.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nC ¸ a˘glar G ˙ulc ¸ehre, Bing Xiang, et al. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In CoNLL 2016 , pages 280–290.\nJoakim Nivre and Chiao-Ting Fang. 2017.\nUniver-\nsal Dependency Evaluation . In NoDaLiDa 2017\nW orkshop on Universal Dependencies , pages 86–95,\nG ¨oteborg.\n359\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Y oav Goldberg, Y oav Goldberg, Jan Haji ˇc, Man-\nning Christopher D., Ryan McDonald, Slav Petrov ,\nSampo Pyysalo, Natalia Silveira, Reut Tsarfaty , and\nDaniel Zeman. 2016. Universal Dependencies v1:\nA Multilingual Treebank Collection. In 10th LREC ,\npages 23–38, Portoro ˇz, Slovenia. ELRA.\nJong-Hoon Oh, Kentaro T orisawa, Chikara Hashimoto,\nRyu Iida, Masahiro T anaka, and Julien Kloetzer.\n2016. A semi-supervised learning approach to why-\nquestion answering. In Thirtieth AAAI Conference\non Artiﬁcial Intelligence .\nJong-Hoon Oh, Kentaro T orisawa, Chikara Hashimoto,\nMotoki Sano, Stijn De Saeger, and Kiyonori Ohtake.\n2013. Why-question answering using intra-and\ninter-sentential causal relations. In ACL 2013 , pages\n1733–1743.\nV aishali Pal, Manish Shrivastava, and Irshad Bhat.\n2019.\nAnswering naturally: Factoid to full length\nanswer generation . In 2nd W orkshop on New Fron-\ntiers in Summarization , pages 1–9, Hong Kong,\nChina. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Sal-\nimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by gen-\nerative pre-training.\nhttps://cdn.openai.\ncom/research-covers/language-unsupervised/\nlanguage\nunderstanding paper.pdf.\nLina M. Rojas Barahona, Pascal Bellec, Beno ˆıt Bes-\nset, Martinho Dos Santos, Johannes Heinecke, Mun-\nshi Asadullah, Olivier Leblouch, Jean-Yves Lancien,\nG´eraldine Damnati, Emmanuel Mory , and Fr ´ed´eric\nHerl´edan. 2019. Spoken Conversational Search for\nGeneral Knowledge. In SIGdial Meeting on Dis-\ncourse and Dialogue , pages 110–113, Stockholm.\nACL.\nAlexander M Rush, Sumit Chopra, and Jason W eston.\n2015. A neural attention model for abstractive\nsentence summarization.\nhttps://arxiv .org/abs/1509.\n00685.\nCicero dos Santos, Ming T an, Bing Xiang, and Bowen\nZhou. 2016. Attentive pooling networks. https://\narxiv .org/abs/1602.03609.\nDjam´e Seddah and Marie Candito. 2016. Hard Time\nParsing Questions: Building a QuestionBank for\nFrench\n. In 10th LREC , Portoro ˇz, Slovenia. ELRA.\nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get to the point: Summarization with\npointer-generator networks. https://arxiv .org/abs/\n1704.04368.\nRebecca Sharp, Mihai Surdeanu, Peter Jansen, Pe-\nter Clark, and Michael Hammond. 2016. Creating\ncausal embeddings for question answering with min-\nimal supervision. EMNLP 2016 .\nMilan Straka. 2018. UDPipe 2.0 Prototype at CoNLL\n2018 UD Shared T ask. In CoNLL 2018 Shared T ask:\nMultilingual P arsing from Raw T ext to Universal De-\npendencies, pages 197–207, Brussels. ACL.\nMing T an, Cicero dos Santos, Bing Xiang, and Bowen\nZhou. 2016. Improved representation learning for\nquestion answer matching. In ACL 2016 , pages 464–\n473.\nRicardo Usbeck, Axel-Cyrille Ngonga Ngomo, Bastian\nHaarmann, Anastasia Krithara, Michael R ¨oder, and\nGiulio Napolitano. 2017. 7th Open Challenge on\nQuestion Answering over Linked Data (QALD-7).\nIn Semantic W eb Challenges , pages 59–69, Cham.\nSpringer International Publishing.\nSuzan V erberne, Hans van Halteren, Daphne Theijssen,\nStephan Raaijmakers, and Lou Boves. 2011. Learn-\ning to rank for why-question answering. Informa-\ntion Retrieval , 14(2):107–132.\nShuohang W ang and Jing Jiang. 2016. Machine com-\nprehension using match-lstm and answer pointer.\nhttps://arxiv .org/abs/1608.07905.\nT ong W ang, Xingdi Y uan, and Adam Trischler. 2017.\nA joint model for question answering and question\ngeneration. https://arxiv .org/abs/1706.01450.\nMin Wu, Xiaoyu Zheng, Michelle Duan, Ting Liu,\nT omek Strzalkowski, and S Albany . 2003. Ques-\ntion answering by pattern matching, web-prooﬁng,\nsemantic form prooﬁng. In TREC 2003 , pages 500–\n255.\nGodandapani Zayaraz et al. 2015. Concept relation\nextraction using na ¨ıve bayes classiﬁer for ontology-\nbased question answering systems. Journal of\nKing Saud University-Computer and Information\nSciences, 27(1):13–24.\nDaniel Zeman, Jan Haji ˇc, Martin Popel, Martin Pot-\nthast, Milan Straka, Filip Ginter, Joakim Nivre, and\nSlav Petrov . 2018.\nCoNLL 2018 Shared T ask: Mul-\ntilingual Parsing from Raw T ext to Universal Depen-\ndencies\n. In CoNLL 2018 Shared T ask: Multilingual\nP arsing from Raw T ext to Universal Dependencies ,\npages 1–21, Brussels. ACL.",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8981865644454956
    },
    {
      "name": "Computer science",
      "score": 0.8246093988418579
    },
    {
      "name": "Transformer",
      "score": 0.6449534893035889
    },
    {
      "name": "Natural language processing",
      "score": 0.6373021602630615
    },
    {
      "name": "Natural language",
      "score": 0.5859479904174805
    },
    {
      "name": "Task (project management)",
      "score": 0.5477896332740784
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5389280319213867
    },
    {
      "name": "Questions and answers",
      "score": 0.45786526799201965
    },
    {
      "name": "Information retrieval",
      "score": 0.4364956021308899
    },
    {
      "name": "Context (archaeology)",
      "score": 0.43444231152534485
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19370010",
      "name": "Orange (France)",
      "country": "FR"
    }
  ],
  "cited_by": 11
}