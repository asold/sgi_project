{
    "title": "Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling",
    "url": "https://openalex.org/W4389518393",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2138102765",
            "name": "Akshat Gupta",
            "affiliations": [
                "University of California, Berkeley"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4389519396",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4296413749",
        "https://openalex.org/W4297412056",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385572451",
        "https://openalex.org/W2130184303",
        "https://openalex.org/W2997734032",
        "https://openalex.org/W3102485638",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W4362696539",
        "https://openalex.org/W4381245716",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034995113",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2943552823"
    ],
    "abstract": "With their increasing size, large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on quantifier understanding in LLMs show inverse scaling in understanding few-type quantifiers. In this paper, we question the claims of of previous work and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that LLMs are able to better understand the difference between the meaning of few-type and most-type quantifiers as their size increases, although they are not particularly good at it. We also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments and previous work, where the model’s understanding of most-type quantifier gets worse as the model size increases. We do this evaluation on models ranging from 125M-175B parameters, which suggests that LLMs do not do as well as expected with quantifiers. We also discuss the possible reasons for this and the relevance of quantifier understanding in evaluating language understanding in LLMs.",
    "full_text": "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 56–64\nDecember 7, 2023. ©2023 Association for Computational Linguistics\n56\nProbing Quantifier Comprehension in Large Language Models:\nAnother Example of Inverse Scaling\nAkshat Gupta\nAI Research, JPMorgan Chase∗\nUniversity of California at Berkeley\nakshat.gupta@berkeley.edu\nAbstract\nWith their increasing size, large language mod-\nels (LLMs) are becoming increasingly good at\nlanguage understanding tasks. But even with\nhigh performance on specific downstream task,\nLLMs fail at simple linguistic tests for nega-\ntion or quantifier understanding. Previous work\non quantifier understanding in LLMs show in-\nverse scaling in understanding few-type quan-\ntifiers. In this paper, we question the claims\nof of previous work and show that it is a re-\nsult of inappropriate testing methodology. We\nalso present alternate methods to measure quan-\ntifier comprehension in LLMs and show that\nLLMs are able to better understand the dif-\nference between the meaning of few-type and\nmost-type quantifiers as their size increases,\nalthough they are not particularly good at it.\nWe also observe inverse scaling formost-type\nquantifier understanding, which is contrary to\nhuman psycho-linguistic experiments and pre-\nvious work, where the model’s understanding\nof most-type quantifier gets worse as the model\nsize increases. We do this evaluation on models\nranging from 125M-175B parameters, which\nsuggests that LLMs do not do as well as ex-\npected with quantifiers. We also discuss the\npossible reasons for this and the relevance of\nquantifier understanding in evaluating language\nunderstanding in LLMs.\n1 Introduction\nLarge Language Models (LLMs) are getting in-\ncreasingly better at understanding language (De-\nvlin et al., 2018; Radford et al., 2019; Raffel et al.,\n2020; Zhang et al., 2022; Ouyang et al., 2022; Tou-\nvron et al., 2023) which can be seen by their im-\nproving performance on various language under-\nstanding benchmarks (Wang et al., 2018, 2019).\nAuto-regressive LLMs including encoder-decoder\nmodels like BART (Lewis et al., 2019) and T5\n(Raffel et al., 2020) and decoder-only models like\n∗This work was done while at AI Research, JPMorgan.\nGPT (Radford et al., 2018, 2019; Brown et al.,\n2020; Zhang et al., 2022; Touvron et al., 2023) have\nbeen scaled to billions of parameters to improve\ntheir language understanding capabilities. With\nincreasing model sizes, the models also gets in-\ncreasingly better at learning from context and can\njust be prompted with few examples rather than\nfine-tuning to do downstream task (Brown et al.,\n2020; Liu et al., 2023).\nEven with this unprecedented yet implicit ev-\nidence of increasing language understanding ca-\npability of LLMs, these models still fail simple\nlinguistic tests on understanding negation and quan-\ntifiers (Jang et al., 2023; Kalouli et al., 2022;\nMichaelov and Bergen, 2022). Understanding nega-\ntion and quantifiers is challenging for language\nmodels because the presence of a single negating or\nquantifying word can drastically change the mean-\ning of the sentence. Also, such sentences are infre-\nquently used in pre-training text corpora (Jiménez-\nZafra et al., 2020; Michaelov and Bergen, 2022),\nwhich makes it hard for the models to account for\nsuch situations. Due to this, actual comprehension\nof negation or quantifier words is overpowered by\nthe larger context of the sentence, which makes it\nchallenging for language models to deal with these\nsituations.\nWe focus on one specific linguistic phenomenon,\nwhich is the use of quantifiers. Quantifiers are\nwords that usually occur before a noun to express\nthe quantity of an object (Kalouli et al., 2022). The\npresence of different quantifiers can make state-\nments semantically very different from each other.\nIt can be seen from the following example:\n(Ex:1) All Ps are Qs =⇒ P ⊆ Q\nNo Ps are Qs =⇒ P ∩ Q = ∅\nIn the above example, two different quantifiers\nall and no when applied to the sets P and Q end up\n57\nBackbone Phrase Quantifier Typicality\npostmen carry\nM : Most postmen carry (M, T) : Most postmen carry mail\n(M, A) : Most postmen carry oil\nF : Few postmen carry (F, T) : Few postmen carry mail\n(F, A) : Few postmen carry oil\nM : Almost all postmen carry (M, T) : Almost all postmen carry mail\n(M, A) : Almost all postmen carry oil\nF : Almost no postmen carry (F, T) : Almost no postmen carry mail\n(F, A) : Almost no postmen carry oil\nTable 1: An example from the dataset used in this paper where a backbone phrase is modified by quantifiers and\nfollowed by typical or atypical critical words.\nin polar opposite meanings as can be seen on the\nright side of respective equations. All Ps are Qs\nmeans that all objects in the set P belong to the set\nQ, whereas No Ps are Qs means that P and Q are\nmutually exclusive sets. This minor distinction in\nthe sentence has a drastic effect on the relationship\nbetween P and Q.\nIn this work, we aim to test and quantify the abil-\nity of LLMs to understand quantifiers and how this\nunderstanding changes as the models scale. We\nbuild upon the work of (Michaelov and Bergen,\n2022), who test understanding and sensitivity of\nLLMs for most-type and few-type quantifiers. They\ndo these tests on a dataset of 960 sentences cre-\nated using a previously published study on human\nresponse (measured using N400 amplitude) to dif-\nferent quantifiers (Urbach and Kutas, 2010). They\nfind that while LLMs do increasingly well on un-\nderstanding most-type quantifiers, while their un-\nderstanding of few-type quantifiers diminishes as\nthe size of these language models increase. This is\nan example of an inverse-scaling law (McKenzie\net al., 2022; Wei et al., 2022), where the model gets\nworse at doing a task as the model size increases.\nInverse scaling laws are rare in natural language\nprocessing and important to identify, yet they must\nbe cautiously evaluated (Wei et al., 2022).\nIn this paper, we first show that conclusions\nabout the inverse-scaling of few-type quantifier\ncomprehension in LLMs (Michaelov and Bergen,\n2022) need to be revisited because of a possibly\nfaulty methodology, thus leading to a wrong conclu-\nsion about inverse-scaling. We discuss the reasons\nfor this in detail later in the paper. We then propose\nour own method of measuring quantifier compre-\nhension in LLMs. We find that LLMs are able to\ndifferentiate between sentences that contain most-\ntype versus few-type quantifiers quite well and this\nunderstanding improves as the model size increases.\nWe measure this by quantitatively evaluating if the\nmodels react differently for different types of quan-\ntifiers. Although, when we evaluate if the model\ntakes into account the meaning of a quantifier, we\nfind that LLMs comprehend few-type quantifiers\nmuch better than most-type quantifiers. We also\nfind that contrary to the results of (Michaelov and\nBergen, 2022), most-type quantifier comprehen-\nsion gets worse with increasing model size, thus\nshowing an inverse-scaling law in most-type quan-\ntifier comprehension. In this study, we evaluate a\nnumber of different language model families, with\nmodels ranging from a size of 125 million param-\neters to 175 billion parameters, and find that the\nresults are consistent for all LLMs.\n2 Dataset and Models\nThe models and dataset used in this paper are iden-\ntical to the ones used in (Michaelov and Bergen,\n2022). This work uses the log probabilities pro-\nduced by different language models to calculate a\nquantity called surprisal, which is introduced later\nin the paper. We do not make additional API calls\nor query models. We simply use the log probabil-\nities released by (Michaelov and Bergen, 2022),\nthus mitigating differences due to experimental\nconditions. This paper aims to provide an alterna-\ntive way of interpreting the output logits produced\nby different LLMs compared to (Michaelov and\nBergen, 2022).\n2.1 Dataset\nWe use the same dataset as used by (Michaelov\nand Bergen, 2022) which originates from a set of\npsycholinguistic experiments done on humans (Ur-\nbach and Kutas, 2010). The dataset consists of 120\ndifferent backbone phrases, which are modified by\n58\ntwo sets of quantifier and completed by a typical\nand an atypical continuing word. An example can\nbe seen in Table 1.\nThe backbone phrase shown in the example is\n‘postmen carry’, which is modified by a most-type\nand a few-type quantifier. Following (Michaelov\nand Bergen, 2022), in this paper we study the ef-\nfects of these two quantifiers and how LLMs in-\nterpret them. Each backbone phrase is modified\nby two most-type and two few-type modifiers. Af-\nter the quantifiers are used to modify the back-\nbone phrases, if the language model takes into ac-\ncount the meaning of the word, it should be more\nlikely to produce a word with appropriate typicality.\nWords that are more typically associated with the\nbackbone phrase are labelled typical (T). For exam-\nples, the phrase \"postmen carry\" is typically fol-\nlowed by the word mail and not by the atypical (A)\nword oil. We expect the language model to take\ninto account the quantifier when assigning prob-\nabilities to the word following the quantifier-\nmodified phrase. Each backbone phrase modified\nby a quantifier is tested to be followed by a typical\nand an atypical word. The typical/atypical words\nare also together referred to as critical words in\nthis paper.\nThe dataset contains a total of 960 sentences,\nwith 120 unique backbone phrases, with 8 modifi-\ncations to each sentence as shown in Table 1. We\nhave 2 different quantifier types and two quantifiers\nper quantifier type, thus making four versions of\neach backbone phrase. Each quantifier-modified\nbackbone phrase is followed by a typical and atyp-\nical word, thus making 8 sentences per backbone\nphrase.\nThese sentences were used to measure human\nbrain response to critical words in association with\nthe quantifier used (Urbach and Kutas, 2010). It\nwas found that humans brain signals produce a\nspike when an atypical critical word is used with\nthe most-type quantifier. This spike in brain acti-\nvation (called N400 signals) are associated with\nunexpected events. Hence, these N400 spikes show\nthat the atypical critical words when following a\nmost-type quantifier were unexpected/incorrect. A\nlower activation is seen when the most-type quan-\ntifier is followed by a typical critical word. This\nspike in the N400 signal can be explained by a\nquantity called surprisal, which is the negative\nlog-probability of the occurence of a word in that\ncontext. This means the less likely the word, the\nhigher the surprisal. It was shown in (Michaelov\nand Bergen, 2020) that surprisal as measured in\nlanguage models explain these N400 spikes very\nwell, and that GPT-3 is the best single predictor\nof these N400 spikes in humans (Michaelov et al.,\n2023).\n2.2 Models\nTo evaluate quantifier comprehension in LLMs,\nwe use five family of models. We use the GPT2\nmodel family (125M-1.5B parameters) (Radford\net al., 2019), ElutherAI’s GPT models (GPT-Neo\n125M, GPT-Neo 1.3B, GPT-Neo 2.7B and GPT-J\n6B) (Black et al., 2022), the OPT model family\n(125M - 13B parameters), the GPT-3 model family\n(2B-175B parameters) and the InstructGPT model\nfamily (Ouyang et al., 2022) called GPT3.5 in the\nrest of the paper (2B-175B parameters).\n3 Quantifier Comprehension in LLMs\nIn this section, we first present how (Michaelov\nand Bergen, 2022) measure quantifier comprehen-\nsion in LLMs. Specifically, we present two ideas\nof surprisal and quantifier accuracy and ways to\nmeasure both properties as proposed by (Michaelov\nand Bergen, 2022). Alongside, we also highlight\nshortcomings of these quantifier comprehension\nevaluation methods.\n3.1 Surprisal\nAs defined in section 2, surprisal is the negative\nlog-probability of occurrence of a word given a\ncontext, as show below:\nSp(wi) = −log p(wi|w1, . . . , wi−1) (1)\nwhere wi is the critical word under observation\nand w1, . . . , wi−1 are the words preceding the crit-\nical word in a sentence. The underscore p in the\nsurprisal represents that this is the definition of sur-\nprisal in prior work. (Michaelov and Bergen, 2022)\nacknowledge that words in language models are\nusually split into subwords. For scenarios when\nthis happens for a critical word, (Michaelov and\nBergen, 2022) suggest to sum up the suprisals of\neach individual subwords. This essentially means\nmultiplying the probabilities of each subword that\nmakes up the critical word. The use of this defini-\ntion of surprisal is suboptimal as it does not take\ninto account the effects of subword tokenization.\n59\nPrevious work has shown that just summing up\nsubword probability results in skewing of probabil-\nity values towards words with shorter length, which\nis why these quantities are normalized by length\n(Brown et al., 2020). In our setting, this means the\ncritical words split into larger number of subwords\nis likely to be assigned lower probability and thus\nhigher suprisal than critical words that are split into\nfewer or no subwords. To normalize the effect of\nsubword length, we propose normalizing the sur-\nprisal values by the subword length of the critical\nword, depicted by N, following previous works\n(Brown et al., 2020). Thus, we define surprisal as\nshown below:\nS(wi) = − 1\nN\nX\n∀vi ∈ {wi}\nlog p(vi|w1, . . . , wi−1)\n(2)\nwhere wi is the critical word split into a set of\nN-subwords represented by the set {wi} and vi is\na subword that belongs to that set. Surprisal can\nbe understood as a term representing the inverse-\nprobability of occuring of a word in a context. If a\nword has high probablity of occuring in a context,\nit will have low surprisal, whereas if a word has\na low probablity of occuring in a context, it will\nhave high surprisal. In this work, we will use our\ndefinition of surprisal.\n3.2 Quantifier Accuracy\n(Michaelov and Bergen, 2022) define quantifier\naccuracy based on the surprisal values for the crit-\nical word following a quantifier type. The quan-\ntifier accuracy test was motivated by the human\nbrain response experiments done in (Urbach and\nKutas, 2010). The aim of defining quantifier ac-\ncuracy was to measure if language models take\ninto account the meaning of quantifier words when\ncreating the probability distribution over for the\ncritical word. (Michaelov and Bergen, 2022) pro-\nposes that if LLMs take into account the meaning\nof quantifiers in a sentence, then the typical critical\nwords will be predicted with larger probability and\nthus lower surprisal values following a most-type\nquantifier, and the atypical critical word will be\npredicted with larger probability and thus lower\nsurprisal value with a few-type quantifier .\nTo illustrate this, we refer to the examples shown\nin Table 1. For the backbone prompt modified\nby a most-type quantifier - \"Most postmen carry\",\nan LLM is consider accurate if surprisal for the\nword oil is more than surprisal of the word mail,\nor in other words, p(mail | Most postmen carry) >\np(oil | Most postmen carry). To succinctly express\nthis, a sentence in the dataset is considered to be\nmost-type accurate if for a most-type quantifier\nmodified backbone phrase (MBP),\nS(typ|MBP ) < S(atyp|MBP ) (3)\nSimilarly, for a backbone prompt mod-\nified by a few-type quantifier - \" Few post-\nmen carry\", an LLM is considered ac-\ncurate if p(oil | Few postmen carry) >\np(mail | Few postmen carry). This means\nthat the atypical word is more likely to occur\nwith the few-type quantifier. Thus, a sentence is\nconsidered to be few-type accurate for a few-type\nquantifier modified backbone phrase (FBP) if for\nthat phrase,\nS(atyp|FBP ) > S(typ|FBP ) (4)\nAs proposed by (Michaelov and Bergen, 2022),\nthe most-type and few-type quantifier accuracy is\nthen calculated as the ratio of sentences following\nthe above equations for the respective quantifiers.\nFigure 1 shows most-type and few-type accuracy\nfor different LLMs as a function of the number of\nparameters in the model. We also see the inverse-\nscaling of few-type quantifier understanding very\nclearly. As shown by the plot, as the number of\nparameters increase, the few-type quantifier com-\nprehension gets worse. Figure 1 is created using\nour normalized definition of surprisal taking into ac-\ncount the subword tokenization, and is thus slightly\ndifferent from the original paper.\n3.2.1 What’s wrong with this way of defining\nquantifier accuracy?\nQuantifier accuracy as defined in equations 3 and 4\nhave a few drawbacks. The first is the assumption\nthat typicality of a word for humans is the same as\nthat for language models. A word deemed \"typical\"\nfor a backbone phrase would indeed be in the top\nfew words used by a human, but the same might\nnot be true for language models. To experimentally\nconfirm this, we analyse the output distribution\nof generated words following a backbone phrase.\nWe find that the \"typical\" word in the dataset does\nnot even fall into the top-100 most likely words\nfollowing a backbone phrase for gpt-2 large. This\n60\n(a) Most-type accuracy as measured by (Michaelov\nand Bergen, 2022) using equation 3.\n(b) Few-type accuracy as measured by (Michaelov\nand Bergen, 2022) using equation 3.\nFigure 1: Quantifier accuracy as a function of model\nparameters for different models as defined in (Michaelov\nand Bergen, 2022).\nis true for ALL of the sentences in the dataset.This\nshows that the typical token for humans is not\nnecessarily typical for language models.\nThe second assumption is that the chosen atypi-\ncal word in the dataset is the only complementary\nword corresponding to the typical word. While the\n\"typical\" word is the most common follow up word\nfor a given backbone phrase, we can have many\nalternative \"atypical\" words to follow the backbone\nphrase. For example, if we consider the phrase -\n\"Most postmen carry \", the atypical word oil is just\nas atypical as the wordfish. In fact, for GPT2-large,\nfish has a larger surprisal value compared to oil,\nwhich means according to GPT2-large, fish is more\natypical than oil and is thus a more ideal candidate\nas an \"atypical\" word for comparison in equations\n3 and 4. Just like the critical word fish, we can find\nmany atypical words that are just as atypical if not\nmore, than the chosen words in the dataset. This\nmeans that if the given atypical word does not\n(a) We calculate the Most-type accuracy without\nthe quantifier in the context. This just means\nthat we calculate the number of examples where\nS(typ|BP ) < S(atyp|BP ). In other words, how\noften is the typical word followed by the backbone\nphrase. Note that the modifier is not present in the\ncontext here.\n(b) We calculate the Few-type accuracy without the\nquantifier in the context. This just means that we\ncalculate the number of examples where the atypical\nword is not followed by the backbone phrase, or\nS(atyp|BP ) > S(typ|BP ).\nFigure 2: Here we calculate the percentage of times the\ntypical words occurs with larger probability than the\natypical word in Figure 2a and vice versa in Figure 2b.\nThese are similar to the quantities calculated in Figure 1\nwithout the quantifier present in the context.\nsatisfy the equations 3 and 4, there might still ex-\nist an unknown number of other atypical words\nthat might be able to satisfy this criteria. These\nreasons renders the accuracy metric as defined by\n(Michaelov and Bergen, 2022) incorrect.\n3.2.2 What do these scaling graphs actually\nmeasure?\nFinally, we want to explain what the scaling in\nFigure 1 and (Michaelov and Bergen, 2022) ac-\ntually depicts. To see this, we want to refer the\nreader to Figure 2, which shows the accuracy met-\nric as defined in equations 3 and 4 for a critical\nword following a backbone phrase without the\n61\nFigure 3: This figure shows that large language models\nget increasingly better at differentiating between most-\ntype quantifiers and few-type quantifiers as they scale.\nquantifier. This means that Figure 2a measures\nthe count when S(typ|BP ) < S(atyp|BP ), or\nhow often is the typical word followed by the\nbackbone phrase. Similarly, figure 2b measures\nS(atyp|BP ) > S(typ|BP ), or how often the atyp-\nical word is not followed by the backbone phrase.\nThe scaling in Figure 2 looks almost identical to\nFigure 1. This indicates that the method defined by\n(Michaelov and Bergen, 2022) to measure the effect\nof quantifier is not even accounting for the presence\nof the quantifier, and we end up just measuring\nhow often the typical word is more probable\nthan the atypical word . Thus, the method pro-\nposed to evaluate quantifier comprehension using\nequation 3 and 4 in (Michaelov and Bergen, 2022)\nis not actually measuring quantifier comprehension,\nit is measuring typicality.\nIn fact, what these scaling plots show is that as\nthe size of the model increase, the typical words in\nLLMs get more probable and the atypical words\nget less probable. This essentially means that the\nmodel is getting better at understanding language\nas typically used by humans, and is able to asso-\nciate the typical word in a given context with larger\nprobability than the atypical words.\n4 Proposed Evaluation of Quantifier\nComprehension in LLMs\nIn this section, we present a more robust way\nof measuring quantifier comprehension in LLMs.\nMeasuring quantifier comprehension in LLMs in\nthe setting defined by (Michaelov and Bergen,\n2022) has to be grounded in the principle that the\ntypical and atypical words chosen in the dataset\nare not unique, and hence to measure the effect\nof presence of quantifier in a context, we should\ndo measurements on the same critical word. We\npropose two tests do this.\n4.1 EXPERIMENT-1 : Differentiating\nBetween Different Types of Quantifiers\nIn this section, we check if the models are able to\ndifferentiate between the meanings of two types of\nquantifiers and react appropriately. To check this,\nwe fix a critical word (either typical or atypical),\nand change the quantifier and see how the surprisal\nvalue of the critical word is affected. We expect that\nwhen we have a typical critical word, the few-type\nquantifier should lead to a higher surprisal value or\nmake the typical word less probable. For example,\nfor the phrase \"Most/Few postmen carry mail\", the\nsurprisal for the word mail should be more when\naccompanied by a few-type quantifier than when\ncompared to a most-type quantifier. Similarly, for\nan atypical word, surprisal values for most-type\nquantifiers should be larger than when observed\nwith few-type quantifiers. In summary, an LLM is\nable to differentiate between two types of quanti-\nfiers if for a critical word, one of the following is\ntrue depending on the type of critical word under\nobservation:\nS(typ|MBP ) < S(typ|FBP ) (5)\nS(atyp|MBP ) > S(atyp|FBP ) (6)\nThe results of Experiment-1 are shown in Fig-\nure 3. We see that LLMs get increasingly better\nat differentiating between the two types of quanti-\nfiers and are able to adapt their output probability\ndistribution at the critical word to reflect this under-\nstanding. This improvement of quantifier compre-\nhension scales with increasing model size just like\nother capabilities of LLMs. Although the absolute\nvalue of quantifier accuracy peaks only at about\n61% for the 175 billion parameter GPT-3 model,\nwhich shows that for a majority of sentences, the\nmeaning of the quantifier is not reflected in the\noutput probability distribution at the critical word.\nThis shows that although LLMs are getting better\nat understanding quantifiers as they scale, they are\nfar from perfect.\n4.2 EXPERIMENT-2: Measuring\nQuantifier-Specific Accuracy\nHere we want to measure how good LLMs are at\nunderstanding a specific quantifier. To measure\nthis, we compare how the surprisal of a critical\n62\nword is affected as we add a quantifier in the con-\ntext. When we add most-type quantifiers, the sur-\nprisal should decrease for a typical word whereas\nit should increase for an atypical word. In other\nwords, a sentence is accurate for most-type quanti-\nfier comprehension if:\nS(typ|MBP ) < S(typ|BP ) (7)\nS(atyp|MBP ) > S(atyp|BP ) (8)\nHere, MBP is a most-type quantifier modified\nbackbone phrase, such as \"Most postmen carry\"\nand BP is just a backbone phrase without modifier,\nsuch as \"Postmen carry\". Similarly, for few-type\nquantifiers, the surprisal should decrease for atyp-\nical critical words and increase for typical words.\nSpecifically, sentence is considered accurate for a\nfew-type quantifier comprehension if:\nS(typ|FBP ) > S(typ|BP ) (9)\nS(atyp|FBP ) < S(atyp|BP ) (10)\nFigure 4 shows the quantifier-specific compre-\nhension ability of models as defined in equations\n7-10. Although section 4.1 showed that models are\nable to differentiate between most-type and few-\ntype quantifiers, we see in Figure 4 that they don’t\nnecessarily incorporate the meaning of quantifiers\nwhen quantifiers are added to a sentence. We see\nthat LLMs become increasingly better at incorpo-\nrating the meaning few-type quantifiers as model\nsize increases by changing the relative probabil-\nity values of the critical words given the change in\ncontext. But this is not observed in the case ofmost-\ntype quantifiers, where we find that the models get\nincreasingly worse at taking into account quantifier\nmeaning, thus showing an inverse-scaling in most-\ntype quantifier comprehension. This shows that\nthe model gets increasingly worse at understand-\ning most-type quantifier as the size of the model\nincreases.\nNote that in this work, to calculate suprisal,\nwe never compare two different critical words as\ncan be seen in equations 5-10. This circumvents\nany affects due to subword tokenization and the\nnon-uniqueness of the chosen critical words in the\ndataset. All the comparisons are made with respect\nto a single critical word.\n(a) Most-type accuracy as defined in equations 7-8\n(b) Few-type accuracy as defined in equations 9-10\nFigure 4: Quantifier specific accuracy as defined in\nequations 7-10.\n5 Discussion\nThe above two tests for evaluating quantifier com-\nprehension in LLMs show that these models are\nfar from perfect. The underlying premise of the\nmethod used in this paper and (Michaelov and\nBergen, 2022) is that the presence of a quantifier\nshould increase or decrease the probability of a crit-\nical word depending on its typicality (Michaelov\nand Bergen, 2022). But both tests described in\nsection 4 show that this is not ubiquitously ob-\nserved. The accuracy numbers for both tests are\naround 50-60%, which means that the probability\ndistributions do not incorporate quantifier meaning\nfor a large majority of sentences. A test like this\nmakes a fundamental assumption that understand-\ning of meaning can be measured by studying the\nrelative ranking of tokens in the generated word\nlogit. While this is a fair assumption, we think it is\nnecessary to explicitly point this out\nIncorporating quantifier meaning in this way\nis not a necessary condition for models to per-\nform well, as can be seen by their consistent im-\nprovement across different benchmark (Wang et al.,\n63\n2018, 2019; Brown et al., 2020; Touvron et al.,\n2023). Also, it has been shown in previous studies\nthat humans are not that great at quantifier com-\nprehension as well (Urbach and Kutas, 2010), and\ncontinue to have a preference towards the more typ-\nical word in a context irrespective of the quantifier.\nThese observations suggest two things. Firstly, that\nLLMs are not good at quantifier comprehension.\nSecondly, we also observe this lack of sensitivity\nto quantifier meaning in humans. This combined\nwith the fact that despite lack of quantifier compre-\nhension, LLMs get increasingly better at language\nunderstanding, we can argue that quantifier com-\nprehension is not as necessary of a task in language\nprocessing and understanding as we thought it was.\n6 Related Work\nInverse scaling laws were introduced as a competi-\ntion (McKenzie et al., 2022) to incentivize research\ntowards finding scenarios where language models\nget worse as their size increases. As the field of\nNLP moves towards scaling models to larger and\nlarger sizes, it is important to know the scenarios\nwhere this scaling becomes detrimental (Wei et al.,\n2022; McKenzie et al., 2023).\nAs language models get increasingly better,\nsome common linguistic tests that they are put\nthrough revolve around understanding negation and\nquantifiers. Studying the affects of negation has\nbeen the subject of focus for many studies (Kass-\nner and Schütze, 2019; Kalouli et al., 2022; Et-\ntinger, 2020) for different encoder-based masked\nlanguage models. These studies find that these lan-\nguage models are not sensitive to negations. Stud-\nies on quantifiers (Kalouli et al., 2022) also seem to\nshow similar results for masked language models.\n(Michaelov and Bergen, 2022) was the first work to\nstudy the quantifier understanding in decoder-based\nLLMs.\n7 Conclusion\nIn this paper, we conduct a study to evaluate how\nwell large language models understand quantifiers.\nSpecifically, we study two types of quantifiers -\nmost-type and few-type quantifiers. We present a\nset of experiments to evaluate quantifier compre-\nhension of large language models and show that\nthese models are able to differentiate betweenmost-\ntype and few-type quantifiers as they scale. We also\nshow that LLMs struggle incorporate the meaning\nof most-type quantifier comprehension when com-\npared to few-type quantifiers. We also show that\nmost-type quantifier comprehension demonstrates\nan inverse-scaling law and their understanding of\nmost-type quantifiers get worse as the model size\nincreases. This study indicates that LLMs do not\ntake into account the meaning of quantifiers that\nstrongly, as shown by low accuracy scores in Fig-\nures 3 and 4. Even so, these models get increas-\ningly better at language understanding tasks, thus\nindicating that quantifier understanding might not\nbe the best test to evaluate language understanding\nin LLMs.\nAcknowledgements\nThis paper was prepared for informational pur-\nposes in part by the Artificial Intelligence Research\nGroup of JPMorgan Chase & Co and its affiliates\n(“J.P. Morgan”) and is not a product of the Research\nDepartment of J.P. Morgan. J.P. Morgan makes no\nrepresentation and warranty whatsoever and dis-\nclaims all liability, for the completeness, accuracy,\nor reliability of the information contained herein.\nThis document is not intended as investment re-\nsearch or investment advice, or a recommendation,\noffer, or solicitation for the purchase or sale of any\nsecurity, financial instrument, financial product, or\nservice, or to be used in any way for evaluating\nthe merits of participating in any transaction, and\nshall not constitute a solicitation under any jurisdic-\ntion or to any person if such solicitation under such\njurisdiction or to such person would be unlawful.\n© 2023 JPMorgan Chase & Co. All rights re-\nserved.\nReferences\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAllyson Ettinger. 2020. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\n64\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nJoel Jang, Seonghyeon Ye, and Minjoon Seo. 2023. Can\nlarge language models truly understand prompts?\na case study with negated prompts. In Transfer\nLearning for Natural Language Processing Work-\nshop, pages 52–62. PMLR.\nSalud María Jiménez-Zafra, Roser Morante, M Teresa\nMartín-Valdivia, and L Alfonso Urena Lopez. 2020.\nCorpora annotated with negation: An overview.Com-\nputational Linguistics, 46(1):1–52.\nAikaterini-Lida Kalouli, Rita Sevastjanova, Christin\nBeck, and Maribel Romero. 2022. Negation, coor-\ndination, and quantifiers in contextualized language\nmodels. arXiv preprint arXiv:2209.07836.\nNora Kassner and Hinrich Schütze. 2019. Negated\nand misprimed probes for pretrained language mod-\nels: Birds can talk, but cannot fly. arXiv preprint\narXiv:1911.03343.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-\nnoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nIan McKenzie, Alexander Lyzhov, Alicia Parrish,\nAmeya Prabhu, Aaron Mueller, Najoung Kim, Sam\nBowman, and Ethan Perez. 2022. The inverse scaling\nprize.\nIan R McKenzie, Alexander Lyzhov, Michael Pieler,\nAlicia Parrish, Aaron Mueller, Ameya Prabhu, Euan\nMcLean, Aaron Kirtland, Alexis Ross, Alisa Liu,\net al. 2023. Inverse scaling: When bigger isn’t better.\narXiv preprint arXiv:2306.09479.\nJames A Michaelov, Megan D Bardolph, Cyma K\nVan Petten, Benjamin K Bergen, and Seana Coul-\nson. 2023. Strong prediction: Language model sur-\nprisal explains multiple n400 effects. Neurobiology\nof Language, pages 1–71.\nJames A Michaelov and Benjamin K Bergen. 2020.\nHow well does surprisal explain n400 amplitude un-\nder different experimental conditions? arXiv preprint\narXiv:2010.04844.\nJames A Michaelov and Benjamin K Bergen. 2022.\n’rarely’a problem? language models exhibit inverse\nscaling in their predictions following’few’-type quan-\ntifiers. arXiv preprint arXiv:2212.08700.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nThomas P Urbach and Marta Kutas. 2010. Quantifiers\nmore or less quantify on-line: Erp evidence for partial\nincremental interpretation. Journal of Memory and\nLanguage, 63(2):158–179.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJason Wei, Yi Tay, and Quoc V Le. 2022. In-\nverse scaling can become u-shaped. arXiv preprint\narXiv:2211.02011.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068."
}