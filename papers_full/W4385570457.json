{
  "title": "Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion",
  "url": "https://openalex.org/W4385570457",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101827018",
      "name": "Wenjie Xu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2106211251",
      "name": "Ben Liu",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2097240164",
      "name": "Miao Peng",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2104485464",
      "name": "Xu Jia",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2131687624",
      "name": "Min Peng",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174368915",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W2099752825",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W3182741322",
    "https://openalex.org/W3097986917",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3188263062",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4294558607",
    "https://openalex.org/W4398265979",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2950393809",
    "https://openalex.org/W4285261975",
    "https://openalex.org/W3117339789",
    "https://openalex.org/W3100254008",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W3196669501",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W2565330852",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W3099845049",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2996371683",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2890410208",
    "https://openalex.org/W2573189311",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2889782235"
  ],
  "abstract": "Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that our model has great competitiveness compared to other models with four metrics. Our model can effectively incorporate information from temporal knowledge graphs into the language models.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7790–7803\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nPre-trained Language Model with Prompts for Temporal Knowledge\nGraph Completion\nWenjie Xu1, Ben Liu1, Miao Peng1, Xu Jia1, Min Peng1∗\n1School of Computer Science, Wuhan University, China\n{vingerxu,liuben123,pengmiao,jia_xu,pengm}@whu.edu.cn\nAbstract\nTemporal Knowledge graph completion\n(TKGC) is a crucial task that involves\nreasoning at known timestamps to complete\nthe missing part of facts and has attracted\nmore and more attention in recent years.\nMost existing methods focus on learning\nrepresentations based on graph neural networks\nwhile inaccurately extracting information from\ntimestamps and insufficiently utilizing the\nimplied information in relations. To address\nthese problems, we propose a novel TKGC\nmodel, namely Pre-trained Language Model\nwith Prompts for TKGC (PPT). We convert a\nseries of sampled quadruples into pre-trained\nlanguage model inputs and convert intervals\nbetween timestamps into different prompts\nto make coherent sentences with implicit\nsemantic information. We train our model\nwith a masking strategy to convert TKGC task\ninto a masked token prediction task, which\ncan leverage the semantic information in\npre-trained language models. Experiments\non three benchmark datasets and extensive\nanalysis demonstrate that our model has\ngreat competitiveness compared to other\nmodels with four metrics. Our model can\neffectively incorporate information from\ntemporal knowledge graphs into the language\nmodels. The code of PPT is available at\nhttps://github.com/JaySaligia/PPT.\n1 Introduction\nIn recent years, temporal knowledge graphs(TKGs)\nhave attracted much attention. TKGs describe\neach fact in quadruple ( subject, relation, object,\ntimestamp). Compared to static knowledge graphs,\nTKGs need to consider the impact of timestamps on\nevents. For example, (Donald Trump, PresidentOf,\nAmerica, 2018) holds while (Donald Trump, Presi-\ndentOf, America, 2022) does not. There are miss-\ning entities or relations in the TKGs, therefore,\n∗*Corresponding author\nBarack Obama Franç ois Hollande\nUSA\nDiscuss by telephone\nConsult\nSergey Viktorovich Lavrov\nOn the next day\nJohn Kerry Benjamin Netanyahu\nDiscuss by telephone\nConsult\nOn the next day\nDiscuss by telephone\nConsult\nOn the next day\nFigure 1: An example of the time-related semantic in-\nformation between relations in three pairs of entities.\ntemporal knowledge graph completion (TKGC) is\none of the most important tasks of temporal knowl-\nedge graphs. TKGC task can be divided into two\ncategories: interpolation setting and extrapolation\nsetting(Jin et al., 2020). Interpolation setting aims\nto predict missing facts in the known timestamps\nwhile extrapolation setting attempts to infer future\nfacts in the unknown ones. The latter is much\nmore challenging, and in this work, we focus on\nthe extrapolation setting. Some TKGC methods\nare developed from static knowledge graph com-\npletion (KGC). Such as adding time-aware score\nfunctions to KGC models(Jiang et al., 2016; Das-\ngupta et al., 2018), adding time-aware relational\nencoders to graph neural networks (Jin et al., 2020;\nHe et al., 2021), adding a new time dimension to\nthe tensor decomposition(Lacroix et al., 2020; Shao\net al., 2022), etc. In addition to those KGC-based\nmodels, reinforcement learning(Sun et al., 2021),\ntime-aware neural network modeling(Zhu et al.,\n2021), and other methods are also applied to TKGC.\nHowever, the methods mentioned above have some\ndrawbacks, as follows: (1) Insufficient temporal\ninformation extraction from timestamps. Most\n7790\nexisting TKGC methods model timestamps explic-\nitly or implicitly. Explicit modeling utilizes low-\ndimensional vectors to represent timestamps. How-\never, real-life timestamps are infinite, and explicit\nmodeling cannot learn all timestamp representa-\ntions and predict events with unseen timestamps.\nImplicit modeling does not represent timestamps\ndirectly but takes timestamps to connect multi-\nple knowledge graphs by determining the sequen-\ntial relationship of these knowledge graphs. This\napproach often requires modeling the knowledge\ngraph one by one, requires a lot of computation,\nand timestamps are used only to determine before\nand after things happen. All the above methods do\nnot give full play to the temporal information of\ntimestamps. (2) Insufficient information mining\nof associations in relations in TKGC . Existing\nmethods often focus on the structural information\nof the triples or quadruples when modeling KGs\nwithout enough consideration of the implied infor-\nmation in relations. This problem is particularly\nevident in TKGs because some relations contain in-\nformation with potential temporal hints. As shown\nin Figure 1, between three different pairs of subject\nand object entities, after establishing relation Dis-\ncuss by telephone, one day apart, they all establish\nrelation Consult. If relation Discuss by telephone\nis established between the same pair of entities,\nthere is a high probability that they will establish\nrelation Consult within a short period. Among the\nentity pairs in ICEWS14, there are 10,887 types of\nrelation pairs, out of which 2,652 exhibit obvious\ntemporal correlations, where one relation in the\npair high probably occurred before the other, and\nthey have a stable time interval between them.\nTo address these problems, we propose a novel\ntemporal knowledge graph completion method\nbased on pre-trained language models (PLMs) and\nprompts. TKGs contain timestamps, and events\noccurring at different occurrence times have se-\nquential relationships with each other, which are\nwell-suited as inputs to sequence models. Inspired\nby the successful application of pre-trained lan-\nguage models in static knowledge graph represen-\ntation(Yao et al., 2019; Kim et al., 2020; Petroni\net al., 2019; Lv et al., 2022), we apply PLMs to tem-\nporal knowledge graph completion to get implicit\nsemantic information. However, simply splicing\nentities and relations in the input of PLMs gener-\nates incoherent sentences, resulting in the inabil-\nity to use PLMs(Lv et al., 2022) fully. Therefore,\nWe sample the quadruples in TKGs and construct\nprompts for each type of timestamps, which we call\ntime-prompts. Then we train PLMs with a mask-\ning strategy. In this way, TKGC can be converted\ninto a masked token prediction task.\nThe contributions of our work can be summa-\nrized as follows:\n• To the best of our knowledge, we are the\nfirst to convert the temporal knowledge graph\ncompletion task into the pre-trained language\nmodel masked token prediction task.\n• We construct prompts for each type of interval\nbetween timestamps to better extract semantic\ninformation from timestamps.\n• We apply our experiments on a series of\ndatasets of ICEWS and achieve satisfactory re-\nsults compared to graph neural network learn-\ning methods.\n2 Related Work\n2.1 Static KG representation\nStatic KG representation learning can roughly be di-\nvided into distance-based models, semantic match-\ning models, graph neural network models, and\nPLM-based models.\nDistance-based models represent the relation\nof two entities into a translation vector, such\nas TransE(Bordes et al., 2013), RotatE(Sun\net al., 2019), TransH(Wang et al., 2014). Se-\nmantic matching models measure the plausibil-\nity of facts using a triangular norm, such as\nRESCAL(Nickel et al., 2012), Distmult(Yang\net al., 2015), ConvE(Dettmers et al., 2018), Com-\nplEx(Trouillon et al., 2016). Graph neural network\nmodels use feed-forward or convolutional layers or\nextend Laplacian matrix to learn the representation\nof entities and relations, such as GCN(Kipf and\nWelling, 2017), GAT(Velickovic et al., 2018), R-\nGCN(Schlichtkrull et al., 2018), SAGE(Hamilton\net al., 2017).\nPLM-based models have also been considered\nfor static KG representation in recent years due\nto the ability to capture context information. KG-\nBERT(Yao et al., 2019) first introduces PLMs into\nstatic KG representation. Among PLM-based mod-\nels, prompt-learning has attracted much attention\nin recent years and has been shown to be effective\non many NLP tasks. LAMA(Petroni et al., 2019)\nfirst introduces prompt-based knowledge to PLM.\n7791\nOther prompt-based models based on LAMA are\ndedicated to improving the presentation of KGs\nby automatic prompt generation or by adding soft\nprompts(Shin et al., 2020; Zhong et al., 2021; Liu\net al., 2021). PKGC(Lv et al., 2022) proposes a\nnew prompt-learning method to accommodate the\nopen-world assumption based on KG-BERT.\n2.2 Temporal KG representation\nTemporal KG representation requires considera-\ntion of how the facts are modeled in time se-\nries. Some temporal KG representation models\nare extended from static models. TTransE(Jiang\net al., 2016) incorporates temporal information\ninto the scoring function based on TransE(Bordes\net al., 2013), and HyTE(Dasgupta et al., 2018) ex-\ntends TransH(Wang et al., 2014) similarly. TNT-\nComplEx(Lacroix et al., 2020) extends Com-\nplEx(Trouillon et al., 2016) inspired by the CP\ndecomposition of order-4 tensor.\nThese expanded approaches consider times-\ntamps as an additional dimension but lack consid-\neration from a temporal perspective. Some models\nattempt to combine message-passing and tempo-\nral information to solve the problem. RE-NET(Jin\net al., 2020) applies R-GCN(Schlichtkrull et al.,\n2018) for message passing for each snapshot and\nthen uses temporal aggregation across multiple\nsnapshots. HIP Network(He et al., 2021) utilizes\nstructural information passing and temporal infor-\nmation passing to model snapshots. RE-GCN(Li\net al., 2021) uniformly encodes the evolutional rep-\nresentations representation of entities and relations\ncorresponding to different timestamps to apply to\nthe extrapolational TKGC task.\nBesides, some models use other strategies to\nmodel TKG. CyGNet(Zhu et al., 2021) is divided\ninto a copy mode and a generative mode to predict\nmissing entities using neural networks with a time\ndictionary. TITer(Sun et al., 2021) introduces rein-\nforcement learning in TKG representation learning.\n3 Preliminary\nTemporal Knowledge Graph Gis a set of net-\nworks of entities and relations that contain times-\ntamps. It can be defined as G = {E,R,T,Q},\nwhere Eis the set of entities, Ris the set of re-\nlations and T is the set of timestamps. Q =\n{(s,r,o,t )}⊆E×R×E×T is the quadruple\nset, where sand oare the subject entity (head en-\ntity) and object entity (tail entity), ris the relation\nbetween them at timestamp t. Gt = {(s,r,o) ⊆\nE×R×E} is called the TKG snapshot at t, and it\ncan be taken as a static KG filtering the triple set\nfrom Gat t.\nTemporal Knowledge Graph completion\n(TKGC) is the task of predicting the evolution of\nfuture KGs given KGs of a known period. Given\na quadruple (s,r,?,tn) or (?,r,o,tn), we have a\nset of known facts from TKG snapshots G(ti<tn)\nto predict the missing object entity or subject entity\nin the quadruple. The probability of prediction of\nmissing the entity oin quadruple (s,r,?,tn) can be\nformalized as follows:\np(o|G<tn ,s,r,tn). (1)\n4 Methodology\nIn this paper, we propose PPT, a novel PLM-based\nmodel with prompts to solve TKGC task. The\nframework of our model is illustrated in Figure 2.\nWe sample quadruples and convert them into pre-\ntrained language model inputs. The prediction of\n[MASK] token is the completed result.\n4.1 Prompts\nWe design different prompts for entities (ent-\nprompts), relations (rel-prompts), and timestamps\n(time-prompts) to convert quadruples into a form\nsuitable for input to PLMs. We add a soft prompt\n[EVE] before the beginning of each fact tuple\ndue to introducing soft prompts in the input sen-\ntences can improve the expressiveness of the sen-\ntences(Han et al., 2021).\nEnt-prompts. We convert each entity into a spe-\ncial token [ENT-i] according to its index. We use\na special token instead of the name of an entity\nbecause, in the prediction task, we need to predict\nthe whole entity but not a part of it. To maintain the\nsemantic information from entities, we do average\npooling of embedding for all words in each entity\nas the initial embedding of its token.\nRel-prompts. For each relation, we convert it into\nits original phrase. It is worth noting that to main-\ntain the coherence of sentences, we supplemented\neach relation with the preposition it was missing.\nFor example, we supplement the relation Make a\nvisit to Make a visit to.\nTime-prompts. We convert the time interval be-\ntween two timestamps into a phrase that can de-\nscribe the period. We construct a dictionary called\ninterval-dictionary, which maps each period to a\nprompt. As shown in Figure 3, we convert each\n7792\nt0 t1 tn-1\nt\ntn\n?\nΔ0 =t1-t0 Δ1 =t2-t1 Δn-1 =tn-tn-1\n…………\n[CLS] At the beginning,[EVE] Consult [ENT-140][ENT-49] ;\n[SHT] Express … with [ENT-12][ENT-49] ;\n[MID] [ENT-49] Threaten [ENT-18] ;\n①\n②\n③\n[SHT] After three weeks, [ENT-49] Host … for [MASK] . [SEP]④\n[EVE]\n[EVE]\n[EVE]\n① ② ③ ④\nPre-trained Language Model\nTSG\nTIG\nInput\nPrediction\nMASK=? tn\n✔\nOn the next day,\nAfter four months,\n[SHT]\nFigure 2: Illusion of PPT for TKGC. Quadruples are sampled and normalized to convert into PLM inputs with\nprompts. We calculate the time interval of adjacent quadruples in TSG to get TIG. We use the prompts to convert\nTIG into the input of PLM and then make the prediction for the mask. This way, The TKGC task is converted into a\npre-trained language model masked token prediction task.\n[SHT] on the same day\n[SHT] on the next day\n0\n1\n7\n14\n30\n180\n365\n[SHT] during the week\n[SHT] after a week\n[SHT] after two weeks\n[MID] after half a year\n[LNG] after one year\nInterval (days) Time-prompts\n60\n730\n[SHT]\n[MID]\n[LNG]\nFigure 3: Illusion of interval-dictionary. The left side of\nthe vertical axis indicates the interval between two times-\ntamps, and the right side indicates the time-prompts\ncorresponding to the timestamp interval. [SHT] for\nshort intervals ( ∆t ≤ 60), [MID] for medium inter-\nvals ( 60 < ∆t ≤ 365), [LNG] for long intervals\n(∆t >365).\ntimestamp interval into a prompt. Each prompt\ncontains two parts. The front part is a soft prompt\nindicating the length of time, such as [SHT] for a\nshort time (less than 60 days),[MID] for a medium\ntime (from 60 days to 365 days), and [LNG] for a\nlong time (above 365 days); the back part is a state-\nment describing the interval. During our analysis,\nwe observed that news reports frequently use dis-\ntinctive time descriptors to indicate time intervals,\nwhich inspired us to develop these prompts.\n4.2 Construction for Graphs\nUnlike sampling one fact tuple as input to a pre-\ntrained language model in some static knowledge\ngraph models(Yao et al., 2019; Lv et al., 2022),\nwe sample multiple fact tuples simultaneously be-\ncause we need to model the temporal relationship\nbetween facts. We take the head/tail entity for\neach quadruple in the training dataset and ran-\ndomly sample each quadruple from the entire train-\ning dataset while fixing the head/tail entity. The\nsampled quadruples are then arranged in chrono-\nlogical order. We demonstrate different sampling\nstrategies in A.1. The sampled list is called Tem-\nporal Specialization Graph (TSG). TSG can be\ndescribed as a time-ordered sequence TSG =\n7793\n[q0,q1 ...,q n],qi = (si,ri,oi,ti) ∈Q,ti ≤ti+1.\nWe have a total of three types of TSG, which are\nTSGs\nobj, TSGr\nsub and TSGo\nrel:\nTSGs\nobj(n) =[qs\n0,qs\n1 ...,q s\nn],\nqs\ni =(obj,ri,oi,ti) ∈Q,ti ≤ti+1,\nTSGr\nrel(n) =[qr\n0,qr\n1 ...,q r\nn],\nqr\ni =(si,rel,o i,ti) ∈Q,ti ≤ti+1,\nTSGo\nsub(n) =[qo\n0,qo\n1 ...,q o\nn],\nqo\ni =(si,ri,sub,t i) ∈Q,ti ≤ti+1,\n(2)\nwhere we fix object entity objto sample TSGs\nobj,\nfix subject entity subto get sample TSGr\nsub, and\nfix relation relto sample TSGo\nrel. We set a mini-\nmum sampling quadruple number Kmin and a max-\nimum sampling quadruple number Kmax.\nThe timestamps in TSGs are independent and\ncannot reflect the time relationship between events.\nWe convert each TSG to a Time Interval Graph\n(TIG) by calculating the time interval of adjacent\nquadruples. We take the earliest time in TSG as the\ninitial time τ0 and calculate the time interval be-\ntween the timestamp in (si,ri,oi,ti) and the times-\ntamp in (si−1,ri−1,oi−1,ti−1) as the new times-\ntamp τi:\nTIG∗,∗={s,r,o}= [p∗\n0,p∗\n1,...,p ∗\nn],\np∗\ni = (q∗\ni (s,r,o),τi),{τo = 0\nτi = ti −ti−1\n,\n(3)\nwhere q∗\ni (s,r,o) means keeping the fact triple\n(si,ri,oi) of q∗\ni .\n4.3 Training\nThe algorithm of our training strategy can be sum-\nmarized in Algorithm 1. We do not train each\nquadruple separately in the training set for each\nepoch because we believe that independent quadru-\nples cannot provide temporal information in TKGs.\nWe sample each entity multiple times by fixing it\nat the object entity position and the subject entity\nposition, thus generating TSGs of entities. Simi-\nlarly, we fix the relations in the quadruples and, for\neach relation generate the TSGs of the relations.\nThen we convert all the TSGs to TIGs. For each\nquadruple in a TIG, we convert the entities, relation,\nand time interval into PLM inputs with prompts\ndescribed in Section.4.1. We use a pre-trained\nlanguage model with the masking strategy (also\nknown as a masked language model, MLM)(Devlin\net al., 2019) to train our model. Masked language\nmodels aim to predict masked parts based on their\nsurrounding context. When training, we mask 30%\nof tokens in an input sequence.\nAlgorithm 1: Training for PPT\nInput: TKG Gwith training data, maximum number of epochs\nmax_epochs, maximum number of sampling TSG of one\nentity or one relation B, minimum sampling sequence length\nKmin, maximum sampling sequence length Kmax.\nrepeat\nepoch ←1;\nS= {};\nfor b ←1 to B do\nforeach ent ∈E do\n// sample TSG for entities\nk = random(Kmin, Kmax);\nSample a TSG ent with length = k;\nConvert TSG ent into TIG ent;\nadd TIG ent to S;\nend\nforeach rel ∈R do\n// sample TIG for relations\nk = random(Kmin, Kmax);\nSample a TSG rel with length = k;\nConvert TSG rel into TIG rel;\nadd TIG rel to S;\nend\nend\nforeach TIG ∈S do\n// convert TIG into input with prompts\nseq = Prompt(TIG );\n// train in PLM with masking strategy\nMASK_TRAIN(PLM (seq));\nend\nepoch ←epoch + 1;\nuntil epoch = max_epochs;\n;\n4.4 Objective optimization discussion\nThe distribution of all facts in Eq 1 can be consid-\nered as the joint distribution of facts on all times-\ntamps:\np(G<tn ) =p(Gt0 ,Gt1 ,··· ,Gtn−1 )\n=\n∏\nt\n∏\n(st,rt,ot)∈Gt\np(st,rt,ot |G<tn ) .\n(4)\nIt is not realistic to focus on all quadruples in\nthe TKG. When predicting the missing subject en-\ntities, we fix the object entities because relations\nin the neighborhood are of most interest to entities.\nFurther, we simulate the original quadruple distri-\nbution by sampling, thus Eq 4 can be approximated\n7794\nas:\np(G<tn ) ≈\n∏\nt\n∏\n(s,rt,ot)∈Gt\np(s,rt,ot |G<tn )\n≈\nK∏\nk=1\np(s,rk,ok |G<tn )\n≈\nK∏\nk=1\np(TSGs\ns[k] |G<tn )\n≈\nK∏\nk=1\np(TIGs\ns[k] |G<tn ) ,\n(5)\nwhere Kis the number of sampling.\nWe calculate the generation probability of the\nquadruples by the pre-trained language model’s\nability to predict unknown words. We use seqk\nto present the converted inputs with prompts of\nTIGs\ns[k]:\nseq= Prompt(TIGs\ns[k]). (6)\nFor example, as illustrated in Figure 2, here\nare two quadruples in TSG:(49,62,12,2) in times-\ntamp t1 and (49,38,18,130) in timestamp tn−1,\nthe time interval between them is 128 days, ∆1 =\ntn−1 −t1. Then the quadruple (49,38,18,128) in\nTIG can be converted into an input sentence with\nprompts: [EVE] [MID] After four months, [ENT-\n49] Threaten [ENT-18].\nThe formalization of prediction can be defined\nas follows:\nK∏\nk=1\np(TIGs\ns[k] |G<tn )\n=\nK∏\nk=1\np(PLM(seqk)),\n(7)\nwhere PLM(·) means inputting a sequence into\nthe pre-trained language model.\nCombining Eq 1 and Eq 7, we convert the TKGC\ntask into an MLM prediction task:\np(o|G<tn ,s,r,tn)\n≈\nK∏\nk=1\np(PLM(seqk))\n·p(PLM(Prompt(s,r,tn))),\n(8)\nwhere Prompt(·) means converting entities, re-\nlations, and timestamps into input sequences for\nPLM.\nBy Eq 8, the original knowledge-completion task\ncan be equated to the pre-trained language model\nmasked token prediction task.\n5 Experiments\n5.1 Experimental Setup\nDatasets. Intergrated Crisis Early Warning Sys-\ntem (ICEWS)(Boschee et al., 2015) is a reposi-\ntory that contains coded interactions between socio-\npolitical actors with timestamps. We utilize three\nTKG datasets based on ICEWS named ICEWS05-\n15((García-Durán et al., 2018); from 2005 to\n2015), ICEWS14((García-Durán et al., 2018); from\n1/1/2014 to 12/31/2014) and ICEWS18((Boschee\net al., 2015); from 1/1/2018 to 10/31/2018) to per-\nform evaluation. Statistics of these datasets are\nlisted in Table 1.\nEvaluation Protocals. Following prior work(Li\net al., 2021), we split each dataset into a training\nset, validation set, and testing set in chronological\norder following extrapolation setting. Thus, we\nguarantee that timestamps of train < timestamps of\nvalid < timestamps of test. Some methods(Jin et al.,\n2020; Zhu et al., 2021; Wu et al., 2020) apply filter\nschema to evaluate the results by removing all the\nvalid facts that appear in the training, validation,\nor test sets from the ranking list. Since TKGs are\nevolving in time, the same event can occur at differ-\nent times(Li et al., 2021). Therefore, we apply raw\nschema to evaluate our experiments by removing\nnothing. We report the result of Mean Reciprocal\nRanks(MRR) and Hits@1/3/10 (the proportion of\ncorrect test cases that are ranked within the top\n1/3/10) of our approach and baselines following\nraw schema.\nBaselines. We compare our model with two\ncategories of models: static KGC models and\nTKGC models. We select DistMult(Yang et al.,\n2015), ComplEx(Trouillon et al., 2016), R-\nGCN(Schlichtkrull et al., 2018), ConvE(Dettmers\net al., 2018), ConvTransE(Shang et al., 2019), Ro-\ntatE(Sun et al., 2019) as static models. We select\nHyTE(Dasgupta et al., 2018), TTransE(Jiang et al.,\n2016), TA-DistMult(García-Durán et al., 2018),\nRGCRN(Seo et al., 2018), CyGNet(Zhu et al.,\n2021), RE-NET(Jin et al., 2020), RE-GCN(Li et al.,\n2021) as baselines of TKGC.\nHyperparameters. We use bert-base-cased1 as\nour pre-trained model. Bert-base-cased has been\npre-trained on a large corpus of English data in\na self-supervised fashion. Bert-base-cased has a\nparameter size of 110M with 12 layers and 16 at-\ntention heads, and its hidden embedding size is\n1https://huggingface.co/bert-base-cased\n7795\nDataset E R #Granularity #Train #Valid #Test\nICEWS05-15 10094 251 24 (hours) 368868 46302 46159\nICEWS14 6869 230 24 (hours) 74845 8514 7371\nICEWS18 23033 256 24 (hours) 373018 45995 49545\nTable 1: Statistics of the datasets we use.\ndataset seq_len min_sample max_sample\nICEWS05-15 256 2 16\nICEWS14 256 2 12\nICEWS18 256 2 16\nTable 2: Parameters for datasets.\n768. Without loss of generality, we also list other\npre-trained models in A.3. The input sequence\nlength, min sampling number, and max sampling\nnumber of each dataset are listed in Table 2. When\ntraining, we mask 30% tokens randomly, and we\nchoose AdamW as our optimizer. The learning rate\nis set as 5e-5. We make a detailed analysis of the\nparameters in A.2.\n5.2 Results\nWe report the results of PPT and baselines in Table\n3.\nIt can be observed that PPT outperforms all static\nmodels much better. Compared with ConvTransE,\nwhich has the best results among static models, we\nachieve 28.3%, 21.97%, and 14.69% improvement\nwith MRR metric in the three datasets, respectively.\nWe believe temporal information matters in TKGC\ntasks, while static models do not utilize temporal\ninformation.\nAs can be seen that PPT performs better than\nHyTE, TTransE, and TA-DistMult. These models\nare under the interpolation setting. For instance,\nwe achieve 41.22%, 46.53%, and 62.18% improve-\nments with MRR metric in the three datasets com-\npared to TA-DistMult. We believe that HyTE and\nTA-DistMult only focus on independent graphs and\ndo not establish the temporal correlation between\ngraphs. TTransE embeds timestamps into the scor-\ning function while not taking full advantage of\nthem.\nWith MRR, Hits@1, and Hits@3 metrics on\nICEWS05-15 and ICEWS14, PPT achieves the best\nresults compared to other TKGC models. For in-\nstance, PPT improves 6.5% over the second-best\nresult with Hit@1 metric. On ICEWS18, PPT has\na slight gap with the best model RE-GCN. We\nbelieve this is because ICEWS18 has more enti-\nties than other datasets. GNN-based models using\nthe message-passing mechanism have better learn-\ning ability for such graphs with many nodes. Fur-\nthermore, RE-GCN adds additional edges to assist\nlearning for the static parts of the graph.\nBesides the masking strategy for our model, we\nalso attempt other forms of application for pre-\ntrained language models, which are illustrated in\nA.3.\n5.3 Ablation study\nTo investigate the contribution of time-prompts in\nour model, we conduct ablation studies for our\nmodel by testing all datasets under the same param-\neter settings of different variants. The experiment\nresults are shown in Table 4.\nPPT w/o prompts denotes PPT without time-\nprompts. In this variant, we set all timestamps\nas 0. To ensure that the sequence length does not\naffect the experiments, we replaced all the time-\nprompts with on the same day. PPT w/o prompts\ngets worse results than raw PPT with all metrics on\nthree datasets except with Hits@10 on ICEWS14.\nICEWS14 has a smaller number of entities and\ndata size than the other two datasets, so it is possi-\nble to achieve better results in some metrics after\nremoving the timestamps.\nPPT rand prompts denotes PPT with random\ntimestamps set. We replace raw timestamps in\nquadruples with other timestamps randomly. Ran-\ndom timestamps should not affect the results if our\nmodel does not learn the timestamp information\ncorrectly. As shown in Table 4, the raw model\nshows better results than this variant on all metrics.\nThese experiments demonstrate that applying\ntime-prompts in our model can benefit the learning\nof temporal information between events.\n7796\nICEWS05-15 ICEWS14 ICEWS18\nMethod MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10\nDistMult 19.91 5.63 27.22 47.33 20.32 6.13 27.59 46.61 13.86 5.61 15.22 31.26\nComplEx 20.26 6.66 26.43 47.31 22.61 9.88 28.93 47.57 15.45 8.04 17.19 30.73\nR-GCN 27.13 18.83 30.41 43.16 28.03 19.42 31.95 44.83 15.05 8.13 16.49 29.00\nConvE 31.40 21.56 35.70 50.96 30.30 21.30 34.42 47.89 22.81 13.63 25.83 41.43\nConvTransE 30.28 20.79 33.80 49.95 31.50 22.46 34.98 50.03 23.22 14.26 26.13 41.34\nRotatE 19.01 10.42 21.35 36.92 25.71 16.41 29.01 45.16 14.53 6.47 15.78 31.86\nHyTE 16.05 6.53 20.20 34.72 16.78 2.13 24.84 43.94 7.41 3.10 7.33 16.01\nTTransE 16.53 5.51 20.77 39.26 12.86 3.14 15.72 33.65 8.44 1.85 8.95 22.38\nTA-DistMult 27.51 17.57 31.46 47.32 26.22 16.83 29.72 45.23 16.42 8.60 18.13 32.51\nRGCRN 35.93 26.23 40.02 54.63 33.31 24.08 36.55 51.54 23.46 14.24 26.62 41.96\nCyGNet 35.46 25.44 40.20 54.47 35.45 26.05 39.91 53.20 26.46 16.62 30.57 45.58\nRE-NET 36.86 26.24 41.85 57.60 35.77 25.99 40.10 54.87 26.17 16.43 29.89 44.37\nRE-GCN 38.27 27.43 43.06 59.93 37.78 27.17 42.50 58.84 27.51 17.82 31.17 46.55\nPPT 38.85 28.57 43.35 58.63 38.42 28.94 42.5 57.01 26.63 16.94 30.64 45.43\nTable 3: Results on three datasets. The best results are boldfaced, and the second best ones are underlined. The\nresults of baselines are from RE-GCN(Li et al., 2021).\nMethod ICEWS05-15 ICEWS14 ICEWS18\nMRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10\nPPT 38.85 28.57 43.35 58.63 38.42 28.94 42.5 57.01 26.63 16.94 30.64 45.43\nPPT w/o prompts38.4428.09 43.09 58.46 38.2428.52 42.4 57.31 25.4415.68 29.26 44.88\nPPT rand prompts 37.43 27.05 42.16 57.49 36.84 26.89 41.41 55.73 24.22 14.31 28.09 44.32\nTable 4: Ablation experiments results of PPT. The best results are boldfaced and the second best ones are underlined.\n[CLS] [EVE] [SHT] at the beginning\nENT-263 express intent to meet or\nnegotiate with ENT-543 [EVE] [MID] after\nseven months ENT-263physically assault to\nENT-262 [EVE] [SHT] on the next\nday ENT-263physically assault to [MASK]\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nFigure 4: Illustrations of attention patterns of\nPPT. The quadruple that needs to be completed is\n(263,104,?,7536), we sample 2 quadruples with ear-\nlier timestamps than the test example and fixed object\nentities. Transparencies of colors reflect the attention\nscores of other tokens to [MASK].\n5.4 Analysis\n5.4.1 Attention analysis\nTo visually show that our model can learn from\ntemporal knowledge graphs, as shown in Figure 4,\nwe visualize attention patterns of PPT. We need to\ncomplete the missing tail entity in a test quadru-\nple (263,104,?,7536). As mentioned, we sample\ndata from earlier than timestamp 7536 to form the\ninput sequence and obtain the attention weights\nfrom the pre-trained model. In this example, the\nground truth is [ENT-262]. We observe that in our\nmodel, the prediction of [MASK] is made by con-\nsidering all the previous sampling samples together.\nPPT notes that the same relation physical assault\nto occurred a day earlier and captures the temporal\ninformation from token the, next, and day. There-\nfore, PPT can make correct predictions based on\nhistorical events and chronological relationships.\n5.4.2 Time-sensitive relation analysis\nUsing ICEWS05-15 as an example, we analyze\nthe time-sensitive relations present in the dataset.\nFor different relations between the same pairs of\nentities, there is a clear order of occurrence among\nsome of them. For example, the relation Obstruct\npassage, block is always followed by ones related\nto assistance, such as Appeal for aid, Appeal for\nhumanitarian aid, and Provide humanitarian aid.\nSimilarly, the relation Acknowledge or claim re-\nsponsibility is always followed by those related to\nnegotiation, such as Express intent to cooperate\nmilitarily, Meet at a ’third’ location, and Demand\nmaterial cooperation. We provide more examples\nin A.5.\nTo verify the superiority of PPT in handling\n7797\ntime-sensitive relations, a new test dataset named\nICEWS05-filter is constructed from ICEWS05-15.\nSpecifically, we select relations that have a clear\nchronological order within a predefined time win-\ndow, resulting in a total of 139 relations. Only the\nquadruples containing these selected relations are\nretained to construct the new dataset. As demon-\nstrated in Table 5, PPT achieves better performance\nwhen evaluated on the constructed test dataset, in-\ndicating its advantage in handling time-sensitive\nrelations.\nDataset MRR Hits@1 Hits@3 Hits@10\nICEWS05-15 38.85 28.57 43.35 58.63\nICEWS05-filter 39.4 29.02 43.91 59.31\nTable 5: Results of PPT on ICEWS05-15 and ICEWS05-\nfilter.\n6 Conclusions\nThis paper proposes a novel temporal knowledge\ngraph completion model named pre-trained lan-\nguage model with prompts for TKGC (PPT). We\nuse prompts to convert entities, relations, and times-\ntamps into pre-trained model inputs and turn TKGC\nproblem into a masked token prediction problem.\nThis way, we can extract temporal information\nfrom timestamps accurately and sufficiently uti-\nlize implied information in relations. Our proposed\nmethod achieves promising results compared to\nother temporal graph representation learning meth-\nods on three benchmark TKG datasets. For future\nwork, we plan to improve the sampling method\nin temporal knowledge graphs to get more time-\nspecific inputs. We are also interested in combining\nGNNs and pre-trained language models in temporal\nknowledge graph representation learning.\nLimitations\nThis paper proposes a pre-trained language model\nwith prompts for temporal knowledge graph com-\npletion. However, there are some limitations in our\nmethod: 1) Our prompts in the temporal knowl-\nedge graphs, especially the time-prompts, are built\nmanually. It needs to be reconstructed manually\nfor different knowledge graphs. We are explor-\ning a way to build prompts in temporal knowledge\ngraphs automatically. 2) Our model uses a random\nsampling method, which suffers from the problem\nof few high-quality training samples and high sam-\nple noise. For future work, a more effective way to\nsample is worth exploring.\nAcknowledgements\nWe would like to thank all the anonymous\nreviewers for their valuable and insightful com-\nments. This work was supported by National\nKey Research and Development Program of\nChina (No.2021ZD0113304), General Pro-\ngram of Natural Science Foundation of China\n(NSFC) (Grant No.62072346), Key R&D Project\nof Hubei Province (Grant NO.2021BBA099,\nNO.2021BBA029) and Application Foun-\ndation Frontier Project of Wuhan (Grant\nNO.2020010601012168). Our work was founded\nby Joint&Laboratory on Credit Technology.\nEthics Statement\nAll steps and data described in our paper follow the\nACL Ethics Policy2.\nReferences\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In NIPS.\nElizabeth Boschee, Jennifer Lautenschlager, Sean\nO’Brien, Steve Shellman, James Starz, and Michael\nWard. 2015. ICEWS Coded Event Data.\nShib Sankar Dasgupta, Swayambhu Nath Ray, and\nPartha P. Talukdar. 2018. Hyte: Hyperplane-based\ntemporally aware knowledge graph embedding. In\nEMNLP.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In AAAI.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nAlberto García-Durán, Sebastijan Dumancic, and Math-\nias Niepert. 2018. Learning sequence encoders for\ntemporal knowledge graph completion. In EMNLP.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017. Inductive representation learning on large\ngraphs. In NIPS.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. PTR: prompt tuning with rules\nfor text classification. CoRR, abs/2105.11259.\n2https://www.aclweb.org/portal/content/\nacl-code-ethics\n7798\nYongquan He, Peng Zhang, Luchen Liu, Qi Liang,\nWenyuan Zhang, and Chuang Zhang. 2021. HIP net-\nwork: Historical information passing network for ex-\ntrapolation reasoning on temporal knowledge graph.\nIn IJCAI.\nTingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Baobao\nChang, Sujian Li, and Zhifang Sui. 2016. Towards\ntime-aware knowledge graph completion. In COL-\nING.\nWoojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren.\n2020. Recurrent event network: Autoregressive struc-\nture inferenceover temporal knowledge graphs. In\nEMNLP.\nBosung Kim, Taesuk Hong, Youngjoong Ko, and\nJungyun Seo. 2020. Multi-task learning for knowl-\nedge graph completion with pre-trained language\nmodels. In COLING.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In ICLR.\nTimothée Lacroix, Guillaume Obozinski, and Nicolas\nUsunier. 2020. Tensor decompositions for temporal\nknowledge base completion. In ICLR.\nZixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng\nGuo, Huawei Shen, Yuanzhuo Wang, and Xueqi\nCheng. 2021. Temporal knowledge graph reason-\ning based on evolutional representation learning. In\nSIGIR.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. CoRR.\nXin Lv, Yankai Lin, Yixin Cao, Lei Hou, Juanzi Li,\nZhiyuan Liu, Peng Li, and Jie Zhou. 2022. Do pre-\ntrained models benefit knowledge graph completion?\nA reliable evaluation and a reasonable approach. In\nACL(Findings).\nMaximilian Nickel, V olker Tresp, and Hans-Peter\nKriegel. 2012. Factorizing Y AGO: scalable machine\nlearning for linked data. In WWW.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models as\nknowledge bases? In EMNLP-IJCNLP.\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter\nBloem, Rianne van den Berg, Ivan Titov, and Max\nWelling. 2018. Modeling relational data with graph\nconvolutional networks. In ESWC.\nYoungjoo Seo, Michaël Defferrard, Pierre Van-\ndergheynst, and Xavier Bresson. 2018. Structured\nsequence modeling with graph convolutional recur-\nrent networks. In ICONIP.\nChao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong\nHe, and Bowen Zhou. 2019. End-to-end structure-\naware convolutional networks for knowledge base\ncompletion. In AAAI.\nPengpeng Shao, Dawei Zhang, Guohua Yang, Jian-\nhua Tao, Feihu Che, and Tong Liu. 2022. Tucker\ndecomposition-based temporal knowledge graph\ncompletion. Knowl. Based Syst.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP.\nHaohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and\nKun He. 2021. Timetraveler: Reinforcement learn-\ning for temporal knowledge graph forecasting. In\nEMNLP.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. In ICLR.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In ICML.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Liò, and Yoshua Bengio.\n2018. Graph attention networks. In ICLR.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In AAAI.\nJiapeng Wu, Meng Cao, Jackie Chi Kit Cheung, and\nWilliam L. Hamilton. 2020. Temp: Temporal mes-\nsage passing for temporal knowledge graph comple-\ntion. In EMNLP.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2015. Embedding entities and relations\nfor learning and inference in knowledge bases. In\nICLR.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion.\nCoRR, abs/1909.03193.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: learning vs. learning to\nrecall. In NAACL-HLT.\nCunchao Zhu, Muhao Chen, Changjun Fan, Guangquan\nCheng, and Yan Zhang. 2021. Learning from history:\nModeling temporal knowledge graphs with sequen-\ntial copy-generation networks. In AAAI.\n7799\nA Appendix\nA.1 Sampling Analysis\nWe design two sampling strategies, one is the\nuniform sampling strategy, and the other is the\nfrequency-based sampling strategy. The uniform\nsampling strategy assigns equal sampling weights\nto each entity. The frequency-based sampling strat-\negy assigns different weights to each entity based\non the different frequencies of each entity appear-\ning in the dataset, where entities with higher occur-\nrences have a higher probability of being sampled.\nAs shown in Table 6, the frequency-based sampling\nstrategy has better results on ICEWS14. We believe\nthis is because if an entity appears frequently, it is\nmore likely to have relations with other entities and\nshould get more attention.\nStrategy MRR Hits@1 Hits@3 Hits@10\nuniform 34.87 25.37 38.77 53.33\nfrequency-based 38.42 28.94 42.5 57.01\nTable 6: Results of different sampling strategies of PPT\non ICEWS14.\nA.2 Hyperparameter Analysis\nTo test the effect of different sequence lengths and\nthe maximum number of samples on the effect\nof the model, we analyze these hyperparameters\non ICEWS14. Due to GPU performance limita-\ntions, we do not perform experiments on longer\nsequences.\nAs shown in Table 7, we get the best results\nwith setting seq_len = 256,max_sample = 12.\nWe believe that the effect of sequence length is\nsmall while the number of samples matters. A\nlarger number of samples can provide more seman-\ntic contextual information for the prediction but\noverly lengthy sampling can cause a decline in ef-\nfectiveness by not focusing on the most effective\ninformation in learning.\nseq_len max_sample MRR Hits@1 Hits@3 Hits@10\n128 2 35.33 25.71 39.56 53.83\n128 4 37.21 27.59 41.08 56.3\n128 8 37.67 28.16 41.73 56.22\n256 8 37.67 27.78 42.31 56.72\n256 12 38.42 28.94 42.5 57.01\n256 16 37.72 27.74 42.1 56.91\nTable 7: Results of different hyperparameters of PPT\non ICEWS14. The best results are boldfaced and the\nsecond best ones are underlined.\nA.3 Variants\nIn addition to the model we propose in the pa-\nper, we also try some variants, all experiments\nare done with seq_len = 256 ,max_sample =\n12 on ICEWS14. As demonstrated in Table 8,\nPPT_CLS does not use the mask training strategy\nbut takes [CLS] to do classification with a fully\nconnected layer as the decoder; PPT_LSTM uses\na bi-directional LSTM to encode all tokens, max-\npool the out embeddings, and use a fully-connected\nlayer as a decoder. These models do not get satis-\nfactory results compared to our raw model.\nPPT_CLS only uses sequence embedding to pre-\ndict the result is not enough because the sequence\nembedding is suitable for classification task which\nneeds to be focused on the whole input sequence.\nHowever, in our task, we need to consider the im-\npact of each token. For PPT_LSTM, we believe\nthat the representation learned by the pre-trained\nlanguage model is high-level semantic knowledge,\nespecially when additional tokens (entities and rela-\ntions) are added. Simple neural network models are\nunable to capture this high-level semantic knowl-\nedge and instead cause a decrease in effectiveness.\nVariants MRR Hits@1 Hits@3 Hits@10\nPPT_CLS 32.81 23.62 36.81 51.12\nPPT_LSTM 32.6 23.61 36.54 50.06\nPPT 38.42 28.94 42.5 57.01\nTable 8: Variants of PPT.\nA.4 Different PLMs\nBesides bert-base-cased, we also attempt other\npre-trained language models: bert-base-uncased3\nand bert-large-cased4. As shown in Table 9. All\nexperiments are done with setting seq_len =\n128,min_sample = 2 ,max_sample = 8 on\nICEWS14. We find that the experimental results\nwith different PLMs are similar, indicating that our\napproach does not rely on a specific pre-trained\nlanguage model and has the ability to generalize.\nPLMs MRR Hits@1 Hits@3 Hits@10\nbert-base-cased 37.67 28.16 41.73 56.22\nbert-base-uncased 37.75 28.06 41.74 56.84\nbert-large-cased 37.36 27.39 41.39 57.59\nTable 9: Experiments on different PLMs.\n3https://huggingface.co/bert-base-uncased\n4https://huggingface.co/bert-large-uncased\n7800\nPre-relation Post-relation\nDemonstrate for policy change fight with small arms and light weapons\nDemonstrate for policy change Make optimistic comment\nDemonstrate for policy change Conduct suicide, car, or other non-military bombing\nObstruct passage, block Appeal for aid\nObstruct passage, block Appeal for humanitarian aid\nObstruct passage, block Provide humanitarian aid\nAcknowledge or claim responsibility Express intent to cooperate militarily\nAcknowledge or claim responsibility Meet at a ’third’ location\nAcknowledge or claim responsibility Demand material cooperation\nReceive inspectors Expel or deport individuals\nReceive inspectors Express intent to provide material aid\nReceive inspectors Return, release person(s)\nDemand release of persons or property Use unconventional violence\nDemand release of persons or property Demonstrate or rally\nDemand release of persons or property Appeal for military aid\nReject judicial cooperation Appeal to others to settle dispute\nReject judicial cooperation Accuse of espionage, treason\nReject judicial cooperation Retreat or surrender militarily\nTable 10: Examples of pre-relations and post-relations\nA.5 Pre-relations and post-relations\nFor one pair of entities, if relation rel-A always\noccurs before relation rel-B, rel-A is called a pre-\nrelation and rel-B is called a post-relation. Table\n10 shows some of these relations.\n7801\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSee Limitations.\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur experiments are reproducible.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSee Abstract and Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSee 5.1 Experimental Setup\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSee 5.1 Experimental Setup\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSee 5.1 Experimental Setup\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nThe tools we use are consensuses in the ﬁeld like many other papers do.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data we use are consensuses in the ﬁeld like many other papers do.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSee 5.1 Experimental Setup\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSee Table 1\nC □\u0013 Did you run computational experiments?\nSee 5.1 Experimental Setup\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSee 5.1 Experimental Setup\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7802\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSee 5.1 Experimental Setup and Appendix A.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSee 5.2 Results\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSee 5.1 Experimental Setup\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n□\u0017\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7803",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8493024110794067
    },
    {
      "name": "Timestamp",
      "score": 0.7630313634872437
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6769278049468994
    },
    {
      "name": "Language model",
      "score": 0.6531221270561218
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6021001935005188
    },
    {
      "name": "Natural language processing",
      "score": 0.5302436351776123
    },
    {
      "name": "Graph",
      "score": 0.5129132270812988
    },
    {
      "name": "Task (project management)",
      "score": 0.45201972126960754
    },
    {
      "name": "Security token",
      "score": 0.43598970770835876
    },
    {
      "name": "Machine learning",
      "score": 0.40405818819999695
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1719851791858673
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I37461747",
      "name": "Wuhan University",
      "country": "CN"
    }
  ],
  "cited_by": 28
}