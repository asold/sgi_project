{
  "title": "From task structures to world models: what do LLMs know?",
  "url": "https://openalex.org/W4392384010",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1970869511",
      "name": "Ilker Yildirim",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2157859309",
      "name": "L. A. Paul",
      "affiliations": [
        "Yale University",
        "Munich School of Philosophy",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A1970869511",
      "name": "Ilker Yildirim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157859309",
      "name": "L. A. Paul",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6850625674",
    "https://openalex.org/W2765123421",
    "https://openalex.org/W2474590732",
    "https://openalex.org/W2963305465",
    "https://openalex.org/W4238379306",
    "https://openalex.org/W6633799636",
    "https://openalex.org/W3163675517",
    "https://openalex.org/W2151516755",
    "https://openalex.org/W4225299009",
    "https://openalex.org/W6798786038",
    "https://openalex.org/W6603074287",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W6721994641",
    "https://openalex.org/W3089472875",
    "https://openalex.org/W6600961393",
    "https://openalex.org/W6779153708",
    "https://openalex.org/W2096462008",
    "https://openalex.org/W2060623117",
    "https://openalex.org/W6722797378",
    "https://openalex.org/W6797716411",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6678525382",
    "https://openalex.org/W6850936240",
    "https://openalex.org/W6852845789",
    "https://openalex.org/W6846011931",
    "https://openalex.org/W6856761955",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W4317797582",
    "https://openalex.org/W3013065462",
    "https://openalex.org/W2118555141",
    "https://openalex.org/W3212922802",
    "https://openalex.org/W4280491924",
    "https://openalex.org/W4367176309",
    "https://openalex.org/W6988114706",
    "https://openalex.org/W2962802170",
    "https://openalex.org/W6855930669",
    "https://openalex.org/W2059100041",
    "https://openalex.org/W6767148148",
    "https://openalex.org/W2996704210",
    "https://openalex.org/W4281686635",
    "https://openalex.org/W2998484454",
    "https://openalex.org/W4301179145",
    "https://openalex.org/W6839027721",
    "https://openalex.org/W2914823880",
    "https://openalex.org/W2136445846",
    "https://openalex.org/W2854445389",
    "https://openalex.org/W3110486140",
    "https://openalex.org/W4206198375",
    "https://openalex.org/W2000214310",
    "https://openalex.org/W2623589305",
    "https://openalex.org/W2042408133",
    "https://openalex.org/W6741500254",
    "https://openalex.org/W4286218470",
    "https://openalex.org/W2951699680",
    "https://openalex.org/W6851841283",
    "https://openalex.org/W6853786485",
    "https://openalex.org/W4220894114",
    "https://openalex.org/W6853942956",
    "https://openalex.org/W4380359760",
    "https://openalex.org/W579310555",
    "https://openalex.org/W2408290011",
    "https://openalex.org/W2970184844",
    "https://openalex.org/W4389518286",
    "https://openalex.org/W4226288657",
    "https://openalex.org/W2553687679",
    "https://openalex.org/W1892018222",
    "https://openalex.org/W4230789743",
    "https://openalex.org/W2322777383",
    "https://openalex.org/W3147653952",
    "https://openalex.org/W7572799",
    "https://openalex.org/W2598654328",
    "https://openalex.org/W4283692059",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4362597819",
    "https://openalex.org/W2976241900",
    "https://openalex.org/W792483063",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2491143741",
    "https://openalex.org/W2963284196",
    "https://openalex.org/W2736021085",
    "https://openalex.org/W1591428544",
    "https://openalex.org/W3199748991",
    "https://openalex.org/W4244149481",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W576850723",
    "https://openalex.org/W2737660050",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4380558591",
    "https://openalex.org/W4316128987",
    "https://openalex.org/W3184354642",
    "https://openalex.org/W1506196175",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W172370420",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4232589280",
    "https://openalex.org/W2023260195",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W1573613494",
    "https://openalex.org/W2604846069",
    "https://openalex.org/W2594035753",
    "https://openalex.org/W2331227228",
    "https://openalex.org/W4377164434",
    "https://openalex.org/W4312374491",
    "https://openalex.org/W1585280831",
    "https://openalex.org/W23077562",
    "https://openalex.org/W2558230882",
    "https://openalex.org/W2911589237",
    "https://openalex.org/W2912512851",
    "https://openalex.org/W4291991132",
    "https://openalex.org/W2074229945",
    "https://openalex.org/W4381827075"
  ],
  "abstract": null,
  "full_text": "Fromtaskstructurestoworldmodels:WhatdoLLMsknow?\nIlkerYildirim\n1,3,4,5\nandL.A.Paul\n2,4,5\n{ilker.yildirim,la.paul}@yale.edu1\nDepartmentofPsychology,YaleUniversity2\nDepartmentofPhilosophy,YaleUniversity3\nDepartmentofStatistics&DataScience,YaleUniversity4\nWu-TsaiInstitute,YaleUniversity5\nFoundationsoftheDataScienceInstitute,YaleUniversity\nSep42023\nAbstract\nInwhatsensedoesalargelanguagemodelhaveknowledge?TheanswertothisquestionextendsbeyondthecapabilitiesofaparticularAIsystem,andchallengesourassumptionsaboutthenatureofknowledgeandintelligence.WeanswerbygrantingLLMs“instrumentalknowledge”;knowledgedefinedbyacertainsetofabilities.Wethenaskhowsuchknowledgeisrelatedtothemoreordinary,“worldly”knowledgeexhibitedbyhumanagents,andexplorethisintermsofthedegreetowhichinstrumentalknowledgecanbesaidtoincorporatethestructuredworldmodelsofcognitivescience.WediscusswaysLLMscouldrecoverdegreesofworldlyknowledge,andsuggestsuchrecoverywillbegovernedbyanimplicit,resource-rationaltradeoffbetweenworldmodelsandtaskdemands.\nIntroduction\nOpenAI’sGPT-4\n1\n,andsimilarlargelanguagemodelssuchasMeta’sLLaMA\n2\n,showimpressiveconversationalcapabilities.Thesesystemscangeneratecoherent,novel,andoftensurprisinglysophisticatedresponsestoquestionsorpromptsposeddirectlyinnaturallanguage.Reflectingthisperspective,oftenimplicitly,artificialintelligence(AI)researchersthatdevelopthesesystems\n3\n,alongwithauthorsofarticlesinnewsmediaandscientificworks,typicallyfocusdiscussiononwhetherlargelanguagemodelshaveknowledge.\nThegoalofthisopinionarticleistoexplorethispossibility.Wesuggestweareina“Kuhnianmoment”–aconceptualrevolutioninwhatwetakeknowledgetoinvolve,withimplicationsforhowwethinkintelligencecouldarise.Accordingly,weask:inwhatsensecanGPT-4(andsimilarmodels)besaidtohaveknowledge?TheanswertothisquestionextendsfarbeyondthecapabilitiesofaparticularAIchatbot,withimplicationsforthefieldsofcognitivescience,neuroscience,philosophy,andcertainlyAI.\nWegroundouranswerusingacoreconceptfromcognitivescience–worldmodels\n4\n:causalabstractionsoftheentitiesandprocessesintherealworldthatpreservetheirstructure,includingobjectswiththree-dimensionalshapesandphysicalproperties\n5\n,sceneswithspatialstructureandnavigablesurfaces\n6\n,andagentswithbeliefsanddesires\n7\n.Humanthoughtrelies\nonthesetypesofworldmodels\n8\ntoinferhowphysicalprocesseswork\n5\n,and,importantly,toeffectivelyreason\n9\n,plan\n10\n,andtalkabouttheworld\n11\n.\nInparticular,forawiderangeofordinarycontexts,aknowledgeablehumanagentdrawsontheirworldmodel,exploitingastructuralmatchbetweentheirmentalrepresentationsandthestateoftheworld,toreliablygeneratecontentfulanswerstopromptsthatareapproximatelytruth-preservingandrelevant.Wewilldescribesuchworld-model-basedknowledgeas“worldly”knowledge,andthecontentofthematchedrepresentationsthatsupportitas“worldlycontent”.Whenasubjecthasworldlyknowledgeofaproposition,thisis,atleastinpart,invirtueofthesubject’susingtheirworldmodeltograsptheworldlycontentindicatedintheproposition.Forexample,whenthesubjectknowsthatbalancingaball onaboxiseasierthanbalancingaboxonaball,theyusetheirworldmodelofhowobjectsmoveandreacttoexternalforcestograspthatbalancingaballonaboxiseasierthanbalancingaboxonaball.Wetakesuchknowledgetobethetargetofphilosophicalanalysesofhowanindividualknowsapropositionp,wheretoknowthatp,“anagentmustnotonlyhavethementalstateofbelievingthatp,butvariousfurtherindependentconditionsmustalsobemet:pmustbetrue,theagent’sbeliefinpmustbejustifiedorwell-founded,andsoforth.”[p.281,Nagel2013:“KnowledgeasaMentalState”,inOxfordStudiesinEpistemology4(2013),275-310]Wealsotakesuchknowledgetobewell-studiedinscientificcontexts\n12–17\n,andtoincludemuchofourordinaryfactualandrelationalknowledgeabouttheworld.\nRecentdevelopmentsinArtificialIntelligence(AI)seemtohavecreatedadifferentkindofknowledge.Suchsystemsarebasedondeepneuralnetworkspre-trainedonInternet-scaledatatoautocompletethenextword(ortoken,moreaccurately)givenprecedingcontext,whicharethenfurtherfine-tunedwithreinforcementandsupervisedlearningtechniquesforhuman-alignedandhumanlikeresponses(e.g.,Ouyangetal.2022\n18\n).When,usingthesetools,LLMscangeneratesufficientlysuccessfulresponsestoanappropriatelywiderangeofprompts,givinganswersthatareoftenapproximatelytruth-preservingandrelevant,wewilldescribesuchanswersasdemonstrating“instrumentalknowledge.”\nThisextendsourquestion.WecangrantthatLLMsexhibitknowledge,fortheycanbesaidtohaveinstrumentalknowledge.Buthowissuchinstrumentalknowledgerelatedtoordinaryhumanknowledge?Aswewillask:towhatdegree,ifany,cananLLM’sinstrumentalknowledgeincorporateworldlyknowledge?\nInstrumentalknowledge\nWecanunderstandtheinstrumentalknowledgeofasystemintermsofitsabilitytoperformtasksposedforitacrossrelevantdomains.Indeed,amotivatingperspectiveonlargelanguagemodelsistheideaofunsupervisedmultitasklearning\n19\n.Internet-scalenaturallanguagedatacanbeseenasalargedatasetofamultitudeoftasks,posedinvaryingwaysandforms,consistentwiththemessinessofhowlanguageisusednaturally.Forinstance,theabbreviation“TL;DR”oraparagraphthatstartswiththephrase“Insummary,…”mightsignalasummarization-liketask;nearbyorpairedsentencesorphrasesspanningmultiplelanguages\n1\nmightsuggestthetaskoftranslationbetweenthoselanguages.Radfordet al.(2019)\n19\nspeculatedthatforamodeltoaccuratelypredictthenextwordinasequence,itmaybecriticalforthemodeltospontaneouslyinferthetaskstructurefromtheprecedingcontext,andconditionthenextwordpredictionsonthattaskstructure.AfterthetrainingofanLLMisover,inferringsuchtaskstructurefromnaturallanguage,andconditioningtheactivationswithinthemodelaccordingtothisstructure,isapossiblesourceofinstrumentalknowledge(Fig.1A,B).\nCouldinstrumentalknowledgeoccurwithout(orwithverylittle)worldlyknowledge?Aplausibleexampleofsuchascenarioismachinelanguagetranslation.Insteadoffocusingonbuildingsystemsthattranslatethroughsemanticanalyzersoranyotherformalnotionofmeaning,mostprogressinmachinetranslationreliesonincreasinglysophisticatedstatisticalapproaches.\n20,21\nItisplausiblethatLLMsrepresentanewfrontierinthisprogressionofmodels,oneinwhichthemodelsinferthetaskstructureoflanguagetranslation,intermsofhowwords,phrases,andevenparagraphsareemittedwithinandacrosspairsoflanguages,andusethisstructuretotranslate–withoutnecessarilyprojectingacommonknowledgeofthephysicalworldacrossdifferentlanguages.Suchapossibilityisfurthersuggestedby“relational”theoriesofwordmeaning(e.g.,suchasaconceptualrolesemantics,wherethemeaningofawordorphraseisdefineddirectlybyitsrelationtootherwordsorconcepts,andonlyindirectlythroughreferenceandcausalconnectionstothenonlinguisticworld,withlimitedtransmissionofworldlycontent)\n22\n.\nRelatedly,somehaveaskedwhetherLLM“knowledge”ismerelyanabilitytofollowlinguisticrulesandlanguagepatterns.Inaddressingthisquestion,animportantdistinctionwithreferencetotheperformanceofLLMsisbetween‘form’vs.‘meaning’.Bender&Koller\n23\nrightfullycautionthatLLM’sabilitytogeneratecoherentlanguageshouldnotbetakenasevidenceofnaturallanguageunderstanding–whichinthepresentarticle,weoperationalizesuchunderstandingintermsofhavingknowledgeofthephysicalworld.\nSimilarly,basedontheseparationobservedinthehumanbrainbetweenlanguagevs.non-languageregions,\n24,25\nMahowaldetal.\n26\narguethatLLMsacquireformallinguisticcompetence,orknowledgeoftherules\n27\nandstatisticalregularities\n28\nofalanguage,butnot“functionallinguisticcompetence”,whichincludesknowledgeofandreferencetothingsandprocessesinthesocialandphysicalworlds\n29\n.Indoingso,atleastinpart,Mahowaldetal.drawonadistinctionbetweenpre-trainedLLMs(onnext-wordprediction)vs.LLMsthatarefurtherfine-tunedwithsupervisedandreinforcementlearningobjectivesonhumandialoguedata(e.g.,18\n).\nSomefine-tunedLLMs,includingGPT-4,whenpromptedwithexamplesthatseemtorequireworldlyknowledge,includingsomeoftheexamplesMahowaldetal.considered,generatecompellinganswers.Suchfine-tuningproceduresonlyadaptcertainoutputstagesofthepre-trainedmodels,otherwisekeepingmuchofthepre-trainedweightsfrozen\n30,31\n.Thus,explainingawayaspectsoftheLLMperformancethatseemtogobeyondformalcompetenceasentirelyaconsequenceoffine-tuningisdifficult.Moreover,incertaincases,performancesimilartothatofafine-tunedmodelcanbeobtainedwithpre-trainedLLMsviaso-called\n2\n“in-contextlearning”,aformoflearningwhere,intheabsenceofanyparameterupdatesintheunderlyingmodel,simplyprovidingexampleinput-outputpairsinthepromptleadsLLMstolearnnewtasks\n32\n.“Grounding”pre-trainedLLMs(e.g.,establishingacorrespondencebetweenanon-languagedomainandLLMembeddings)isanexampleofwhatcanbeaccomplishedwiththiskindofin-contextlearning\n33\n.\nThisindicatestousthatpre-trainedLLMsmayabsorbbitsandpiecesofknowledgethatgiveitabilitiesthatgobeyondwhatcanbedescribedasformallinguisticcompetence.Forthisreason,theinstrumentalknowledgethatweascribetoLLMscarvesoutaspacethatisdistinctfromandexceedstherulesandpatternsofalanguage,andincludesinferringadeepertaskstructureandconditioningnextwordpredictiononthatstructure(Fig.1A,1B).\nTheleap:Fromnextwordpredictiontoworldmodels\nInstrumentalknowledge,tocountas“knowledge”intheordinarysense,mustsomehowbeabouttheworld;itmustincludesomedegreeofworldlycontent,inordertoincludesomedegreeofworldlyknowledge.Buthowcouldapurelytext-relatedobjective(i.e.,nextword/tokenprediction)leadtoanydegreeofworldlyknowledge(Fig.1C)?\nFig.1.Exploringthenatureofknowledgeinlargelanguagemodels.(A)LLMs,suchasOpenAI’sGPT-4,showimpressivecapabilitiesonabroadrangeoftasks,andsomefailures,thatcollectivelysuggestquestionstotheextentoftheirworldlyknowledge.(GPT-4wasqueried10timesoneachofthesepromptsandthemostfrequentresponse–shortenedforconciseness–isreported,withfrequencydisplayedingreenifthatresponseisconsistentwiththeauthors’intuitionsandredotherwise.)(B)Wesuggestthatpre-trainedLLMsacquireinstrumentalknowledgethatgoesbeyondformallinguisticscompetence(i.e.,thesetofrulesandstatisticalpatternsthatmakesuplanguage).Wesuggestthatthisfallsfromtheobjectiveofnextwordprediction–accurateauto-completionleadstoinferringandusingtaskstructureduringtheprocessingofinputcontext–andisperhapsfurtherstrengthenedandorganizedduringthefine-tuningofLLMs.Thispairofpromptsillustrateanexample,wherethemodelmightinferthatthetaskistocomparetheplausibilityofdifferentphrasesintheprompt,withrespecttothe(oftenhighlycomplex)co-occurrencepatternsofthesephrases.Suchinstrumentalknowledgemightworkforcomparisonsofphraseswhoseco-occurrencepatternsareapproximatedwellbythemodel,butbecomeincreasinglyinaccurateforlessfrequentphrases.(C)Weexplorehowmuchofthisinstrumentalknowledgemightdrawonanunderlyingaccountoftheentitiesandprocessesinthephysicalworld–\n3\n\nwhichwetermworldlyknowledge.Inthisexample,thiswouldincludetheforce-dynamicrelationsbetweenentities.Weexaminethisperspectiveviathecorecognitiveconceptofstructuredworldmodels,areviewofrecentrelevantwork,andthenotionofboundedrationality.\nOneinterestingpossibilitydrawsontheabilitytorecoverworldlycontentusingcompression.\n34,35\nAsobservedbyChiang\n36\n,nextwordpredictioninLLMsreflectscompressionofthevastamountsoftextdatacrawledontheInternetintothemanybillionsofweightsofadeepneuralnetwork,whichinproportionremaintoosmalltomemorizethetrainingdata.Indeed,compressionandpredictionarecloselyrelatedobjectives.\n34,35\nAlower-dimensionalstate-spacefactorizingtherelevantdimensionsofvariationofagivendomainandthedependenceofthesedimensionsoneachothercansimultaneouslyenablecompressionandprediction.Structuredworldmodelsareexamplesofsuchstate-spaces,whereasmallsetofvariablescapturescausalabstractionsofthestructureoftheircounterpartphysicalprocessesintherealworld(Fig.2A).Thecaveat,however,isthattherearemanywaysadatasetcanbecompressed,leadingtovariousapproximationsofthestatisticalregularitiesinthetrainingtext.\nHowever,wespeculatethatcompressioncouldrecoverthedatageneratingprocessunderlyingthetrainingdata.Therearecertainlymultipledatageneratingprocessesunderlyingnaturallanguagedata,includingtherulesandstatisticalregularitiesinalanguage,howtasksareposedandaddressed,andespeciallyrelevanttothepresentcontext,worldlycontentinvolvingtheentities,physicalprocesses,andsituationsprojectedintotextbythehumansperceivingandparticipatinginthesesituationsandtalkingaboutthem.\nWeacknowledgethat,atpresent,whetherLLMsactuallydoorevencouldrecovercausalabstractionsoftheworld,asinworldmodels,involvesaleapoffaith.However,asitisoftenthispossibilitythatmotivatesattributionsofgeneralintelligencetoLLMs(e.g.,Bubecketal.\n37\n),weturntoavailableliteratureexploringthispossibility.\nMeasuredrecoveryofworldmodelsinLLMsunderdomain-specificsettings\nWethinkthatstructuredworldmodels–i.e.,causalabstractionsofarepresentedsystemwithastructurepreserving(homomorphic)representingsystem\n38\n(Fig.2A)–provideaconcreteframeworktoreasonaboutwhetherandhowLLMscanrecoverdegreesofworldlycontentthatcouldleadtoworldlyknowledge.\nAsmallnumberofrecentstudieshaveexploredwhetheralanguagemodeltrainedtopredictnext-tokenspontaneouslyapproximatestheunderlyingdatageneratingprocess–i.e.,theworldmodel.\n39,40\nTodoso,thesestudiesmovedawayfromthenaturallanguagesettingtospecificdomainsinwhichinputsequencesarestilltext-basedandcanbetokenizedjustlikelanguage,buteachtokenreflectstheunfoldingofanunderlyingdomain-specificworldmodel.\n4\nOneexampleoftherecoveryofworldlyknowledgeispresentedbyLietal.\n39\n(Fig.2B). Lietal.trainedaGPT-4-like(butonamuchsmallerscale)languagemodeltopredictlegalmovesinthegameofOthello–atwo-playerboardgameinwhichplayersgainmorediscsbyoutflankingthediscsoftheotherplayer.Theworldmodelinthisgameconsistsofthestateoftheboard(foreachcellinan8x8grid,whetheritisblack,white,orempty)andthesetofrulesbywhicheachplayerchangesthisstate.Lietal.randomlysimulatedthisworldmodelcreatingadatasetofboardstatetracesviastochastic,non-strategicdecisionsforeachplayer.Whentheytrainedalanguagemodel,calledOthello-GPT,onthisdataset,theyfoundthatnotonlythismodelcouldreliablygeneratelegalmovesgiventhepriorsetofmoves,butalsotheentireboardstatecouldbeaccuratelydecodedfromtheintermediate-layeractivationsinthemodel,withalineardecoder(asestablishedbyafollow-upstudy\n41\n). Crucially,theauthorsalsoshowedthatinterveningontheboardstateviathesedecoderscausallyandappropriatelyimpactedthemodel’slegalmovepredictions.\nFig.2.UsingthecoreconceptofworldmodelsfromcognitivesciencetoexploretheextentofworldlyknowledgeinLLMs.(A)Worldmodelsintheexampledomainsofintuitivephysics,cognitivemaps,andembodiedplanning.Worldmodelsarehomomorphicmappingsthatrepresent,inanabstractformat,objectswithphysicalproperties,placeswithnavigablespatialrelations,andagentswithbeliefsandgoals.(B)Compressionandpredictionarelikethetwosidesofacoin,anditispossiblecompressioncanrecoverthedatageneratingprocess.Inadomain-specificsetting(theboardgameofOthello)withtokenizedtracesofrandomlygeneratedgamestates,Lietal.(2022)trainedanLLMonnext-tokenprediction.Surprisingly,theintermediatelayersofthisLLMyieldedalinearlydecodablefullboardstate.(C)Aswediscussinthetext,generalizationofthisresulttoactualnaturallanguagetrainedLLMsissofarlimited.\nFurthermore,Jin&Rinard(2023)\n40\n,usingatoydomain,tookasimilarapproachtosuggestthatalanguagemodeltrainedfornext-tokenpredictionforprogramsynthesisalsorecoverssomethingaboutthedeepersemanticsofthisdomain-specificprogramminglanguage.Finally,asimilarconclusioncomesfromthedomainofcomputationalbiology:arecentLLMtrainedby\n5\n\nMetaresearchers\n42\ntopredictmaskedentriesinproteinaminoacidsequencesrenderedthecoarsethree-dimensionalstructure(i.e.,contactrelationsbetweenaminoacids)oftheactualfoldedproteinlinearlydecodable.\nTheworkofLietal.\n39\nestablishesthespeculativepossibilitymentionedabove:thatnext-tokenpredictionwithalanguagemodelcanrecovertheunderlyingdatageneratingprocess.Itsuggeststhatthe“leap”mentionedintheprevioussectionisinfactarealisticoutcome.Itisexcitingandofpressingimportanceforfutureresearchtosystematicallyexplorethispossibilityacrossthedimensionsoftrainingobjective(e.g.,next-tokenprediction,maskedtokenprediction),networkarchitectures(e.g.,transformerbasedlanguagemodels\n43\n,RNN-basedsequencemodels\n44,45\n–thekindsofneuralnetworkarchitecturesthatunderliethemodernLLMsandtraditionalneurallanguagemodels,respectively),andthecomplexityofworldmodels.\nThatsaid,weraisethreecaveats.First,thesestudiesdependondensesamplingofrelativelysmalldomains(morethan1milliontrainingexamplesforOthello);datasetrequirementscanquicklygrowfornon-toydomains,andwhetherthelargestInternet-scalelanguagedatasetscansatisfytheseconditionsisnotknown.Second,thesestudiesconsidersettingswherethebasicbuildingblocksoftheworldmodelscanbeenumeratedandassigneduniquetokens–e.g.,thelocationsinthe8x8gridandthepossiblestatesforeachcellintheOthelloenvironment.Wesuspectthiswillnotbeapplicableformanyrelevantworldmodelsandforhowtheyareprojectedintotextbyhumanlanguageusers.Finally,itispossiblethattherecoveredworldmodelinalanguagemodel,basedontheobjectiveofnext-tokenprediction,mightbecomputationallycostly.\nThismotivatestheexplorationoftherecoveryofdomain-specificworldmodelsinLLMstrainedwithnaturallanguagedata(Fig.2C),\n33,46,47\ndrawingonrecentworkinthecontextofperceptualcolorspace.\n33,46\nAbdouetal.\n46\nshowsthatpre-trainedlanguagemodelscanrecoveraspectsoftherelationalstructureoftheperceptualcolorspace.Usingrepresentationalsimilarityanalysis,theyreportstatisticallysignificantcorrelationsbetweenthesimilaritystructureofcolorpairswithrespecttolanguagemodelembeddingsvs.euclideandistancesbetweenthesamepairsofcolorsunderawell-establishedperceptualcolorspace.Patel&Pavlick(2022)\n33\nprovidesfurtherevidence(usinglargerlanguagemodels,includingGPT-3\n32\n)butwithadifferentapproach:TheyreportthatLLMsprovidedwithasmallnumberofin-contextexamplesfromasinglehue(e.g.,red)generalize,moreaccuratelythanchance,totherestofthecolorspace,indicatingthatsomeaspectsoftherelationalstructureoftheperceptualcolorspaceisreadilyavailableintheseLLMs.\nInaway,theseresultssuggestthepossibilityofrecoveringworldlyknowledgeinLLMs,despitetheirpurelytext-basedtraining.Butthisneedstobequalified.ThequantitativenatureofthecorrespondencebetweenthephysicalspacesandLLMinternalsisoftenunderwhelminginthesestudies(e.g.,acorrelationvalueofroughly0.2intheAbdouetal.study).Weanticipatethatthiscorrespondencewillincreaseunderbettertrained,largermodels,butnevertheless,the\n6\nfactthatthisrelationshipisweakinadomainlikecolorsistelling:structure-wise,colorspaceisasimpletopology(distancesin3Dspace)andpresumablythereismuchtextinthetrainingcorporathattalksaboutcolor.Thecomplexityoftypicalworldmodelsprojectedtotextbyhumanlanguageusers(e.g.,spatialstructures,intuitivephysics)isoftenfarmorecomplex.Itisplausiblethatinmostrealisticsettings,LLMswillprimarilyacquireinstrumentalknowledgethatreflectsonlylimitedorpartialworldlyknowledge,muchlikethecaseofmachinetranslation.\nAresourcerationalviewofinstrumentalknowledge\nAswenotedabove,worldlyknowledgecanbecostly–someofthesemappingsillustratedinFig.2A(forexample,aphysicsengine,embodiedplanningandmanipulation)mayrivalLLMsinthecomplexityofthecomputationstheyinvolve,astheycapture,inalgorithmicallyefficientforms,complexcausalprocessesfromthephysicalworld.\nFortunately,formosttasks,partialorcoarserworldlyknowledgeisgoodenough.Toseeanexamplewherecoarserworldlyknowledgewouldbesufficient,considerthedomainofintuitivephysics–ourabilitytopredict,oftenataglance,howobjectswillmoveandreacttoexternalforces.\n48,49\nAninfluentialcognitivecomputationalframeworksuggeststhatsuchpredictionsarisefromstructuredworldmodelsinthemind\n9\n:runnablementalmodelsofphysicalobjectsimplementedusingappropriatehomomorphicrepresentations,e.g.,simulationsinprobabilisticorapproximateformsofphysicsenginesfromcomputergraphics(Fig.2A,top).\nThementalprecisionofsuchasimulationcanvarywithtaskrequirements\n50,51\n:Acoarse-grained,qualitativesimulationmightsufficetopredictwhetheraliquidwillflowtowardrightorleft,whileafiner-grainedsimulationmaybenecessarytoworkoutdetailsofitstrajectory52,53\n.Representationgranularityprovidesadirectmeasureovertheextentofworldlyknowledge,andthetaskdemandsonworldmodelscanprovideausefulresource-rational\n54–56\nperspectiveonthekindofinstrumentalknowledgedemonstratedinLLMs.\nThus,wesuggestthedegreetowhichworldlyknowledgeispartofinstrumentalknowledgeinLLMsmaybedeterminedinaresource-rationalway,stemmingfromatradeoffbetweenaneedfor(costly)worldlyknowledgeandtheneedtoperformthetasksthatoccurfrequentlyduringtraining.Instrumentalknowledgecanbesufficientforaccuratepredictioninscenarioswhereacoarse-grainedmentalsimulationsuffices,butsuchpredictionmaybecomeincreasinglyinaccurateinscenariosthatrequirefiner-grainedsimulationinthemind(alsoseeJara-Ettinger&Rubio-Fernandez\n57\n).AnimportantcaveathereisthatinLLMs,thistradeoffoccursimplicitly,withitsblack-boxandfundamentallydifficulttocontrolcomposition.\n7\nFig.3.Resource-rationalworldmodelsthatreflectgoalsofanagent.(A)Atask-drivenhomomorphicmappingwouldinvestlessresourcesontheaspectsofworldlyknowledgenotbenefitingthetaskoftheagent,butotherwisewouldstillbeacausalabstraction.(B)Hoetal.\n58\nintroducedaframeworkthatallowsconstructionsofsimplifiedtaskrepresentationsbybalancingthecomplexityoftaskrepresentationswiththeexpectedvalueofplans.Forinstance,givenamazenavigationtask,includingastartinglocation(bluecircle),atargetlocation(highlightedcell),andthewalls(restofthenon-whitecells),themodelconstructsasimplifiedrepresentationsinwhichonlyasubsetofthewallsarelikelytobeincluded,whilemaintainingtheexpectedaccuracyandpathlengthoftheplan.FromHoetal.(2022).\nFutureAIsystems,aswellasfutureempiricalexplorationsofLLMs,shouldaspiretodirectlyaddressthistradeoffbyintegratingtaskdemandsduringtheformationofworldmodels(Fig.3A).Indeed,humansmakethistradeoffbetweenknowingabouttheworldandtheirgoalsallthetime,leadingtostructured,butstrikinglysparsepercepts.\n59–61\nSuchtask-drivenmentalrepresentationscarvejustenoughofthecomplexityofthephysicalworldtoenablethe(momentary)goalsofanagent\n61\n.Itisonlyrecentlythatthefieldofcognitivesciencehasstartedexploringcomputationaltheoriesthatintegratestructuredworldmodelswithtaskdemand\n58\n.Forexample,Hoetal.\n58\nprovidedacomputational-levelexplanationofsimplifiedrepresentationsrelativetoplanningobjectives,usingthedomainof2Dmazenavigation(Fig.3B).Thisnewlandscapeofresource-rationalworldmodelscouldinturnhelprefinewhatwetakeknowledgetoconsistin.\n8\n\nWorldmodelsasprogrammable,mid-orhigh-levelinterfacesforsafetyandalignment\nBeyondtheirroleinintelligence,whyelseshouldweembracetheuseofworldmodelsinanAIsystem?Thatis,ifasystemhasinstrumentalknowledgeallowingittoperformadiverserangeoftasksaccurately,whatelseshouldwewant?\nWewantsuchsystemstobesafe.Thatis,wewanttheseAIsystemstobedeployedinawaythatisbothtruthfulandalignedwith“humanvalues.”\n62\nExistingdeepneuralnetworks,includingLLMswiththeirtransformerbasedneuralnetworkarchitectures,areblack-boxsystemswithoutanexplicit,high-levelinterfacefordirectlyprogrammingtheirbehavior.ThelackoftransparencyinhowLLMswork,combinedwiththeirtendencytoproduce“hallucinations”–thefrequentlyobservedfabricationofsituations,events,andpersonsbythesesystemsinresponsetoreasonableprompts–hasraisedconcernsintheindustryandinlargersocietalcontexts.\n63,64\nThepossibilityofunexpectedvaluechangeinthesesystems(forexample,thehigherordervaluechangedetailedinPaul(2014)\n65\n)raisesquestionsabouthowsurewecanbethatevolvingAIvalueswillremainalignedwithhumanvalues.\n62\nIncontrast,worldmodelsofferapathtosafer,truthful,andbetteralignedAIsystems.Becauseworldmodels,andmoregenerallydomain-specifichigh-levelprogramminglanguages,formalizeworldlyknowledgeinstructure-preserving,interpretablerepresentations,theycanreadilyenabletruthfulness,supportinganengineerorauserwhowishestoimparttheir“values”andsafetymeasuresasexplicitconstraintsoverthesystem.ThesefeaturescanbeexploitedviahybridpipelinesofLLMsandworldmodels,\n66–68\nneurosymbolicarchitectures\n69\n,andbycreatingnativelyprogrammableneuralnetworkarchitectures.\n70\nConclusion\nTheimpressiveperformanceofLLMsonasurprisinglybroadrangeoftaskschallengeshowwethinkaboutthenatureofknowledge,andbyextension,howintelligencecouldariseinartificialormachinesystems.Toframethischallenge,weasked:inwhatsensecanLLMs,trainedpurelyontext,primarilytopredictthenextword,besaidtohaveknowledge?Weansweredthechallengebygranting“instrumentalknowledge”toLLMs:knowledgedefinedbyacertainsetofabilities,includingparticularlinguisticandtask-basedabilities,andsuggestedthismaybeanewkindofknowledge.Wethenaskedhowsuchknowledgeisrelatedtothemoreordinary,“worldly”knowledgeexhibitedbypeople,andexploredthisintermsofthedegreetowhichinstrumentalknowledgecanbesaidtoincorporateworldmodels.\nWeclosebynotingthatinstrumentalknowledgeinLLMscouldinvolveanimplicitresource-rationaltradeoffbetweentheotherwisecostlyuseofstructuredworldmodelsandtheneedtoperformcommonlyoccurringtasks.Futureworkshouldconstructgoal-conditionedworldmodels,structuredabstractionsthatneverthelesscanintegratetheagent’splanningobjectives.Beyondtheirimplicationforintelligence,worldmodels,incorporatedmoreexplicitlyinAIsystems,willfacilitateamoredirectpathsafeandaligneddeployment,byexposingan\n9\ninterpretablemid-orhigh-levelinterfaceforcontrolandintervention(seeOutstandingQuestions).\nAcknowledgements\nWethankYutaroYamada,MarioBelledonne,andJonathanSchafferforhelpfuldiscussions.WethankmembersoftheCognitive&NeuralComputationLabatYalefortheirfeedbackonearlierversionsofthismanuscript.WethankAalapShahforhishelpmakingdisplayitems.I.Y.wassupportedbyAFOSRgrant#FA9550-22-1-0041.\n10\nGlossary\nInstrumentalknowledge:knowledgeoftruepropositionshadinvirtueofasystem’ssetofabilities.ForanLLM,thisincludesitsabilitytoperformtasksposedforitacrossawiderangeofrelevantdomains,includingtheabilitytoprovidecoherentandoftensurprisinglysophisticatedresponsestonaturallanguageprompts.WesuggestthatsuchperformancerestsonanLLM’sabilitytospontaneouslyinfertaskstructurefromnaturallanguageinput,andtoconditiontheactivationswithinthemodelaccordingtothisstructurefornextwordprediction.SdemonstratesinstrumentalknowledgeofP:“balancingaballonaboxiseasierthanbalancingaboxonaball”,whenitcanspontaneouslyinferPinresponsetoanaturallanguageprompt(inadditiontohavingthemoregenerallydefinedsetofabilitiesdetailedabove).\nWorldlycontent:contentconstitutedbyentitiesandprocessesintherealworld,includingobjectswiththree-dimensionalshapesandphysicalproperties,sceneswithspatialstructureandnavigablesurfaces,andagents.\nWorldlyknowledge:Shasworldlyknowledgethat PonlyifPhasworldlycontentandSusestheirworldmodeltograspthiscontent.\nOrdinaryknowledge:asubjectSknowspropositionP(intheordinaryway)invirtueofstandingintheknowledgerelationtothe(true)propositionP.\nHomomorphicmapping:Amappingbetweentwosystemsishomomorphicifthestructureofhowthesymbolswithineachsystemrelatetoeachotherispreserved.Abasicexampleofhomomorphismislinearlymappablesystems.\nWorldmodels:EventhoughworldmodelsaretalkedaboutindifferentwaysinAIliterature,werefertoaspecificconceptgroundedinrepresentationtheory:homomorphic(i.e.,structurepreserving)representationsofreal-worldprocesses.Theserepresentationsarecausalinthesensethattheycapture,atanabstractlevel,theircounterpartreal-worldprocesses,inalgorithmicallyefficientforms,suchassimulatingopticswithcomputergraphicsandobjectmotionwithphysicsengines.Worldmodelscanbeembeddedwithinprobabilisticmodelstospecifyinference,prediction,andreasoningqueries,toformalizerelevantprocessesinperceptionandcognition.\nPre-trainedLLMs:DeepneuralnetworkstrainedtoautocompletetextonInternet-scalenaturallanguagedata.AtypicalLLMwillhavehundredsofmillionstohundredsofbillionstunableparameters.ThecurrentlybestperformingLLMsshareacommonnetworkarchitectureknownasthe“transformer”architecture.Transformerarchitectureisuniqueinthewayitallowsthemodelstoprocessalongcontextofwordsatonce,andattendoverthatcontextinwaysthatarehighlyflexible.\nFine-tunedLLM:Pre-trainedLLMsthatarefurthertrained,typicallyonmuchlessdatathanusedduringpre-training,tofollowinstructionsandgeneratealignedandhumanlikeresponses,\n11\noftenusingreinforcementandsupervisedlearningtechniques(althoughothermethodsarepossible).\n1. OpenAI.GPT-4TechnicalReport.arXiv[cs.CL] Preprintathttp://arxiv.org/abs/2303.08774\n(2023).\n2. Touvron,H.et al. LLaMA:OpenandEfficientFoundationLanguageModels.arXiv[cs.CL]\n(2023).\n3. EyeonAI.TheMastermindBehindGPT-4andtheFutureofAI|IlyaSutskever.\nhttps://www.youtube.com/watch?v=SjhIlw3Iffs(2023).\n4. Lake,B.M.,Ullman,T.D.,Tenenbaum,J.B.&Gershman,S.J.Buildingmachinesthat\nlearnandthinklikepeople.Behav. BrainSci. 40,e253(2017).\n5. Yildirim,I.,Siegel,M.&Tenenbaum,J.B.PhysicalObjectRepresentations.inThe\nCognitiveNeurosciences, 6thedition(ed.Poeppel,G.M.)399(MITPress,2020).\n6. Epstein,R.A.,Patai,E.Z.,Julian,J.B.&Spiers,H.J.Thecognitivemapinhumans:\nspatialnavigationandbeyond.Nat. Neurosci. 20,1504–1513(2017).\n7. Jara-Ettinger,J.,Gweon,H.,Schulz,L.E.&Tenenbaum,J.B.TheNaïveUtilityCalculus:\nComputationalPrinciplesUnderlyingCommonsensePsychology.TrendsCogn. Sci. 20,\n589–604(2016).\n8. Spelke,E.S.Coreknowledge.Am. Psychol. 55,1233–1243(2000).\n9. Battaglia,P.W.,Hamrick,J.B.&Tenenbaum,J.B.Simulationasanengineofphysical\nsceneunderstanding.Proceedingsof theNational Academyof Sciences110,18327–18332\n(2013).\n10. Baker,C.L.,Jara-Ettinger,J.,Saxe,R.&Tenenbaum,J.B.Rationalquantitativeattribution\nofbeliefs,desiresandperceptsinhumanmentalizing.Nat. Hum. Behav. 1,1–10(2017).\n11. Jones,C.R.&Bergen,B.TheRoleofPhysicalInferenceinPronounResolution.\nProceedingsof theAnnual Meetingof theCognitiveScienceSociety43,(2021).\n12\n12. Dretske,F.KnowledgeandtheFlowofInformation,Cambridge,MA:Bradford.Preprintat\n(1981).\n13. Chisholm,R.M.Theoryof Knowledge.(Prentice-Hall,1977).\n14. Goldman,A.I.EpistemologyandCognition.(HarvardUniversityPress,1986).\n15. Kornblith,H.KnowledgeandItsPlaceinNature.(ClarendonPress,2002).\n16. Nagel,J.Knowledge: AVeryShort Introduction.(OUPOxford,2014).\n17. Williamson,T.KnowledgeandItsLimits.(OxfordUniversityPress,2002).\n18. Ouyang,L.et al. Traininglanguagemodelstofollowinstructionswithhumanfeedback.Adv.\nNeural Inf. Process. Syst. 35,27730–27744(2022).\n19. Radford,A.et al. Languagemodelsareunsupervisedmultitasklearners.OpenAI blog1,9\n(2019).\n20. Och,F.J.,Gildea,D.,Khudanpur,S.&Sarkar,A.Asmorgasbordoffeaturesforstatistical\nmachinetranslation.Proceedingsof the(2004).\n21. Stahlberg,F.NeuralMachineTranslation:AReview.jair69,343–418(2020).\n22. Piantasodi,S.T.&Hill,F.Meaningwithoutreferenceinlargelanguagemodels.arXiv\npreprint arXiv:2208.02957(2022).\n23. Bender,E.M.&Koller,A.ClimbingtowardsNLU:OnMeaning,Form,andUnderstandingin\ntheAgeofData.inProceedingsof the58thAnnual Meetingof theAssociationfor\nComputational Linguistics5185–5198(AssociationforComputationalLinguistics,2020).\n24. Fedorenko,E.,Behr,M.K.&Kanwisher,N.Functionalspecificityforhigh-levellinguistic\nprocessinginthehumanbrain.Proc. Natl. Acad. Sci. U. S. A. 108,16428–16433(2011).\n25. Fedorenko,E.&Thompson-Schill,S.L.Reworkingthelanguagenetwork.TrendsCogn.\nSci. 18,120–126(2014).\n26. Mahowald,K.et al. Dissociatinglanguageandthoughtinlargelanguagemodels:a\ncognitiveperspective.arXiv[cs.CL] (2023).\n27. Chomsky,N.Aspectsof theTheoryof Syntax, 50thAnniversaryEdition.(MITPress,2014).\n13\n28. Bybee,J.&Hopper,P.Introductiontofrequencyandtheemergenceoflinguisticstructure.\ntorrossa.com.\n29. Clark,H.H.UsingLanguage.(CambridgeUniversityPress,1996).\n30. Hu,E.J.et al. LoRA:Low-RankAdaptationofLargeLanguageModels.arXiv[cs.CL]\n(2021).\n31. Tsimpoukelli,M.et al. Multimodalfew-shotlearningwithfrozenlanguagemodels.Adv.\nNeural Inf. Process. Syst. 34,200–212(2021).\n32. Brown,T.et al. Languagemodelsarefew-shotlearners.Adv. Neural Inf. Process. Syst. 33,\n1877–1901(2020).\n33. Patel,R.&Pavlick,E.MappingLanguageModelstoGroundedConceptualSpaces.(2022).\n34. Grunwald,P.Atutorialintroductiontotheminimumdescriptionlengthprinciple.arXiv\n[math.ST] (2004).\n35. Ratsaby,J.PredictionbyCompression.arXiv[cs.IT] (2010).\n36. Chiang,T.ChatGPTisaBlurryJPEGoftheWeb.NewYorker(2023).\n37. Bubeck,S.et al. SparksofArtificialGeneralIntelligence:EarlyexperimentswithGPT-4.\narXiv[cs.CL] (2023).\n38. Gallistel,C.R.&King,A.P.MemoryandtheComputational Brain: WhyCognitiveScience\nwill TransformNeuroscience.(JohnWiley&Sons,2011).\n39. Li,K.et al. EmergentWorldRepresentations:ExploringaSequenceModelTrainedona\nSyntheticTask.arXiv[cs.LG] (2022).\n40. Jin,C.&Rinard,M.EvidenceofMeaninginLanguageModelsTrainedonPrograms.arXiv\n[cs.LG] (2023).\n41. Nanda,N.Actually,Othello-GPTHasALinearEmergentWorldModel.neelnanda. io\nPreprintat(2023).\n42. Lin,Z.et al. Evolutionary-scalepredictionofatomic-levelproteinstructurewithalanguage\nmodel.Science379,1123–1130(2023).\n14\n43. Vaswani,A.et al. Attentionisallyouneed.Adv. Neural Inf. Process. Syst. 30,(2017).\n44. Elman,J.L.Findingstructureintime.Cogn. Sci. 14,179–211(1990).\n45. Hochreiter,S.&Schmidhuber,J.Longshort-termmemory.Neural Comput. 9,1735–1780\n(1997).\n46. Abdou,M.et al. CanLanguageModelsEncodePerceptualStructureWithoutGrounding?A\nCaseStudyinColor.arXiv[cs.CV] (2021).\n47. Søgaard,A.GroundingtheVectorSpaceofanOctopus:WordMeaningfromRawText.\nMindsMach. 33,33–54(2023).\n48. Kubricht,J.R.,Holyoak,K.J.&Lu,H.IntuitivePhysics:CurrentResearchand\nControversies.TrendsCogn. Sci. 21,749–759(2017).\n49. McCloskey,M.Intuitivephysics.Sci. Am. (1983).\n50. Clark,A.Radicalpredictiveprocessing.South. J. Philos. 53,3–27(2015).\n51. Drugowitsch,J.,Wyart,V.,Devauchelle,A.-D.&Koechlin,E.Computationalprecisionof\nmentalinferenceascriticalsourceofhumanchoicesuboptimality.Neuron92,1398–1411\n(2016).\n52. Bates,C.J.,Yildirim,I.,Tenenbaum,J.B.&Battaglia,P.Modelinghumanintuitionsabout\nliquidflowwithparticle-basedsimulation.PLoSComput. Biol. 15,e1007210(2019).\n53. Zhang,Y.,Belledonne,M.,Yates,T.&Yildirim,I.Wheredoestheflowgo?Humans\nautomaticallypredictliquidpathingwithcoarse-grainedsimulation.Proceedingsof the\nAnnual Meetingof theCognitiveScienceSociety45,(2022).\n54. Gigerenzer,G.&Selten,R.BoundedRationality: TheAdaptiveToolbox.(MITPress,2002).\n55. Lieder,F.&Griffiths,T.L.Resource-rationalanalysis:Understandinghumancognitionas\ntheoptimaluseoflimitedcomputationalresources.Behav. BrainSci. 43,e1(2019).\n56. Gershman,S.J.,Horvitz,E.J.&Tenenbaum,J.B.Computationalrationality:Aconverging\nparadigmforintelligenceinbrains,minds,andmachines.Science349,273–278(2015).\n57. Jara-Ettinger,J.&Rubio-Fernandez,P.Quantitativementalstateattributionsinlanguage\n15\nunderstanding.Sci Adv7,eabj0970(2021).\n58. Ho,M.K.et al. Peopleconstructsimplifiedmentalrepresentationstoplan.Nature606,\n129–136(2022).\n59. Neisser,U.Thecontrolofinformationpickupinselectivelooking.inPerceptionandits\ndevelopment 201–219(PsychologyPress,1979).\n60. Rensink,R.A.,O’Regan,J.K.&Clark,J.J.ToSeeornottoSee:TheNeedforAttentionto\nPerceiveChangesinScenes.Psychol. Sci. 8,368–373(1997).\n61. Niv,Y.Learningtask-staterepresentations.Nat. Neurosci. 22,1544–1553(2019).\n62. Bowman,S.R.EightThingstoKnowaboutLargeLanguageModels.arXiv[cs.CL] (2023).\n63. Critch,A.&Russell,S.TASRA:aTaxonomyandAnalysisofSocietal-ScaleRisksfromAI.\narXiv[cs.AI] (2023).\n64. Russell,S.Provablybeneficialartificialintelligence.27thInternational conferenceon\nintelligent user(2022).\n65. Paul,L.A.TransformativeExperience.(OxfordUniversityPress,2014).\n66. Wong,L.et al. FromWordModelstoWorldModels:TranslatingfromNaturalLanguageto\ntheProbabilisticLanguageofThought.arXiv[cs.CL] (2023).\n67. Wolfram,S.Chatgptgetsits‘wolframsuperpowers’.https://writings\n.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/ dehttps://writings.\nstephenwolfram. com.\n68. Ellis,K.ModelingHuman-likeConceptLearningwithBayesianInferenceoverNatural\nLanguage.arXiv[cs.CL] (2023).\n69. Lu,X.,Welleck,S.,West,P.,Jiang,L.&Kasai,J.Neurologica*esquedecoding:\nConstrainedtextgenerationwithlookaheadheuristics.arXivpreprint arXiv(2021).\n70. Kim,J.Z.&Bassett,D.S.Aneuralmachinecodeandprogrammingframeworkforthe\nreservoircomputer.NatureMachineIntelligence5,622–630(2023).\n16\nOutstandingquestions\nHowcanweformalizeinstrumentalknowledge?Onepossibilityistask-conditionedworldmodels,whichneedsfurtherdevelopment.Futureworkshouldalsoexploretherelationshipofinstrumentalknowledgetoamortizedinferenceordata-drivenproposalsinBayesianinferenceandresource-rationalsolutionstointractableproblemsarisingfromworkingwithexpressiveworldmodels.\nWhatistheimpactoffine-tuningonanLLM’sinstrumentalknowledgeandrecoveryofworldlyknowledge?\nHowcanweincorporatestructuredworldmodelsinAI,suchasLLMsandbeyond,forsaferandbetteralignedsystems?\nEmpiricalworkshoulduseworldmodelsandstate-of-the-artcomputationaltheoriesincognitivesciencetocreatedomain-specific(e.g.,intuitivephysics,cognitivemaps)benchmarksforassessingandmonitoringtheextentofworldlyknowledgeinLLMs.Thesebenchmarksshouldexploredimensionssuchasworldmodelgranularityandthetrainingdatadistribution.\nLLMsareoftenreferredtoas“foundationmodels”,inthesenseoftheiradaptabilitytonewtaskswithlittleadditionaldata.Towhatextentcanthedistinctionofinstrumentalknowledgevs.worldlyknowledgehelptowardbuildingandexploringnewfoundationmodelsinlanguageandotherfields,suchascomputervisionandreinforcementlearning?\nWeassumethatLLMsinternalizeformallinguisticcompetence.Isthisfair?Ifso,whatistheformatofsuchknowledge,specificallyinrelationtolinguistictheories?\nHowdoesourdiscussionrelatetoexistingtreatmentsofLLMsinrelatedcontexts,includingconceptualrolesemanticsandembodiment-basedarguments?\nWeavoidedusing“understanding”inourdiscussionofknowledgeinLLMs,asthetermhasmultipleinterpretationsandisformallylessdeveloped.Doesourdiscussionsuggestanopportunityforanalyzing“understanding”intermsofworldlyknowledge?\n17\nHighlights\nOpenAI’sGPT-4,andsimilarlargelanguagemodels(LLMs),showimpressiveconversationalcapabilities.Thisarticleasks:InwhatsensedoesanLLMhaveknowledge?TheanswertothisquestionextendsbeyondthecapabilitiesofaparticularAIchatbot,andchallengesourassumptionsaboutthenatureofknowledgeandintelligence.\nWeanswerbygrantingLLMs“instrumentalknowledge”–knowledgedefinedbyacertainsetofabilities.\nHowissuchknowledgerelatedtothemoreordinary,“worldly”knowledgeexhibitedbyhumans?Toaddress,weturntoacoreconceptincognitivescience,worldmodels,andexplorethedegreetowhichinstrumentalknowledgemightincorporatesuchstructuredrepresentations.\nWediscusshowLLMscouldrecoverdegreesofworldlyknowledge,andsuggestsuchrecoverywillbegovernedbyanimplicit,resource-rationaltradeoffbetweenworldmodelsandtaskdemands.\n18",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.6517391800880432
    },
    {
      "name": "Task (project management)",
      "score": 0.5999308824539185
    },
    {
      "name": "Cognition",
      "score": 0.5609313249588013
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5067115426063538
    },
    {
      "name": "Cognitive science",
      "score": 0.42892566323280334
    },
    {
      "name": "Cognitive psychology",
      "score": 0.42396777868270874
    },
    {
      "name": "Epistemology",
      "score": 0.3689124584197998
    },
    {
      "name": "Computer science",
      "score": 0.15127289295196533
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I292141241",
      "name": "Munich School of Philosophy",
      "country": "DE"
    }
  ]
}