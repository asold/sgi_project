{
  "title": "Using fMRI Brain Activation to Identify Cognitive States Associated with Perception of Tools and Dwellings",
  "url": "https://openalex.org/W1999755328",
  "year": 2008,
  "authors": [
    {
      "id": "https://openalex.org/A5011671585",
      "name": "Svetlana V. Shinkareva",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5102836913",
      "name": "Robert A. Mason",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5053692525",
      "name": "Vicente L. Malave",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5100391700",
      "name": "Wei Wang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5102921433",
      "name": "Tom M. Mitchell",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A5052643318",
      "name": "Marcel Adam Just",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2073683361",
    "https://openalex.org/W2106664807",
    "https://openalex.org/W2129158941",
    "https://openalex.org/W2007804187",
    "https://openalex.org/W2139906140",
    "https://openalex.org/W1968635050",
    "https://openalex.org/W2089632738",
    "https://openalex.org/W2160448907",
    "https://openalex.org/W2059799772",
    "https://openalex.org/W2058046532",
    "https://openalex.org/W1991785932",
    "https://openalex.org/W2074895886",
    "https://openalex.org/W2112532472",
    "https://openalex.org/W2007770989",
    "https://openalex.org/W29779864",
    "https://openalex.org/W2024545575",
    "https://openalex.org/W1978641546",
    "https://openalex.org/W2077116199",
    "https://openalex.org/W2116455947",
    "https://openalex.org/W2152017481",
    "https://openalex.org/W2125935651",
    "https://openalex.org/W2040768047",
    "https://openalex.org/W1977685900",
    "https://openalex.org/W1991926550",
    "https://openalex.org/W2172168442",
    "https://openalex.org/W2062083532",
    "https://openalex.org/W2086593994",
    "https://openalex.org/W2164239909",
    "https://openalex.org/W1979187968",
    "https://openalex.org/W2009120241",
    "https://openalex.org/W2485403294",
    "https://openalex.org/W643606810",
    "https://openalex.org/W4231623949",
    "https://openalex.org/W2602425071"
  ],
  "abstract": "Previous studies have succeeded in identifying the cognitive state corresponding to the perception of a set of depicted categories, such as tools, by analyzing the accompanying pattern of brain activity, measured with fMRI. The current research focused on identifying the cognitive state associated with a 4s viewing of an individual line drawing (1 of 10 familiar objects, 5 tools and 5 dwellings, such as a hammer or a castle). Here we demonstrate the ability to reliably (1) identify which of the 10 drawings a participant was viewing, based on that participant's characteristic whole-brain neural activation patterns, excluding visual areas; (2) identify the category of the object with even higher accuracy, based on that participant's activation; and (3) identify, for the first time, both individual objects and the category of the object the participant was viewing, based only on other participants' activation patterns. The voxels important for category identification were located similarly across participants, and distributed throughout the cortex, focused in ventral temporal perceptual areas but also including more frontal association areas (and somewhat left-lateralized). These findings indicate the presence of stable, distributed, communal, and identifiable neural states corresponding to object concepts.",
  "full_text": "Using fMRI Brain Activation to Identify Cognitive States\nAssociated with Perception of Tools and Dwellings\nSvetlana V. Shinkareva1,2*, Robert A. Mason1, Vicente L. Malave1, Wei Wang2, Tom M. Mitchell2, Marcel Adam Just1\n1 Department of Psychology, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America,2 Machine Learning Department, School\nof Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, United States of America\nPrevious studies have succeeded in identifying the cognitive state corresponding to the perception of a set of depicted\ncategories, such astools, by analyzing the accompanying pattern of brain activity, measured with fMRI. The current research\nfocused on identifying the cognitive state associated with a 4s viewing of an individual line drawing (1 of 10 familiar objects, 5\ntools and 5dwellings, such as ahammer or acastle). Here we demonstrate the ability to reliably (1) identify which of the 10\ndrawings a participant was viewing, based on that participant’s characteristic whole-brain neural activation patterns, excluding\nvisual areas; (2) identify the category of the object with even higher accuracy, based on that participant’s activation; and (3)\nidentify, for the first time, both individual objects and the category of the object the participant was viewing, based only on\nother participants’ activation patterns. The voxels important for category identification were located similarly across\nparticipants, and distributed throughout the cortex, focused in ventral temporal perceptual areas but also including more\nfrontal association areas (and somewhat left-lateralized). These findings indicate the presence of stable, distributed,\ncommunal, and identifiable neural states corresponding to object concepts.\nCitation: Shinkareva SV, Mason RA, Malave VL, Wang W, Mitchell TM, et al (2008) Using fMRI Brain Activation to Identify Cognitive States Associated\nwith Perception of Tools and Dwellings. PLoS ONE 3(1): e1394. doi:10.1371/journal.pone.0001394\nINTRODUCTION\nIt has been a lasting challenge to establish the correspondence\nbetween a simple cognitive state (such as the thought of ahammer)a n d\nthe underlying brain activity. Moreover, it is unknown whether the\ncorrespondence is the same across individuals. A recent approach to\nstudying brain function uses machine learning techniques to identify\nthe neural pattern of brain activity underlying various thought\nprocesses. Previous studies using a machine learning approach have\nbeen able to identify the cognitive states associated with viewing an\nobject category, such as houses [1,2,3,4,5,6,7,8]. The central char-\nacteristic of this approach (compared to a conventional statistical\nparametric mapping-like approach) is its identification of a multi-\nvariate pattern of voxels and their characteristic activation levels that\ncollectively identify the neural response to a stimulus. These machine\nlearning methods have the potential to be particularly useful in\nuncovering how semantic information about objects is represented in\nthe cerebral cortex because they can determine the topographic\ndistribution of the activation and distinguish the content of the\ninformation in various parts of the cortex. In the study reported\nbelow, the neural patterns associated with individual objects as well as\nwith object categories [9] were identified using a machine learning\nalgorithm applied to activation distributed throughout the cortex.\nThis study also investigated th ed e g r e et ow h i c ho b j e c t sa n d\ncategories are similarly represented neurally across different people.\nWe analyzed the brain activity of participants who were viewing\na line drawing of an object from the categories oftools or dwellings,\nof the type shown in Figure 1. We were able to train classifiers to\nidentify which of ten object exemplars and two object categories a\nparticipant was viewing. We discovered a common neural pattern\nacross participants, and used this to train a classifier to identify the\ncorrect object category and object exemplar from the fMRI data\nof new participants who were not involved in training the classifier.\nMATERIALS AND METHODS\nParticipants\nTwelve right-handed adults (8 female) from the Carnegie Mellon\ncommunity participated and gave written informed consent\napproved by the University of Pittsburgh and Carnegie Mellon\nInstitutional Review Boards. Six additional participants were\nexcluded from the analysis due to head motion greater than 2.5 mm.\nExperimental paradigm\nThe stimuli depicted concrete objects from two semantic\ncategories (tools and dwellings), and took the form of white line\ndrawings on a black background. There were five exemplars per\ncategory; the objects were drill, hammer, screwdriver, pliers, saw,\napartment, castle, house, hut, andigloo. The drawings of the ten objects\nwere presented six times (in six random permutation orders) to\neach participant. Participants were asked to think of the same\nobject properties each time they saw a given object, to encourage\nactivation of multiple attributes of the depicted object, in addition\nto those used for visual recognition. The intention was to foster the\nretrieval and assessment of the most salient properties of an object.\nTo ensure that each participant had a consistent set of properties\nto think about, he or she was asked to generate a set of properties\nfor each exemplar prior to the scanning session (such as cold,\nknights, and stone for castle). However, nothing was done to elicit\nconsistency across participants.\nEach stimulus was presented for 3s, followed by a 7s rest period,\nduring which the participants were instructed to fixate on an X\nAcademic Editor: Olaf Sporns, Indiana University, United States of America\nReceived June 7, 2007;Accepted December 9, 2007;Published January 2, 2008\nCopyright: /C2232008 Shinkareva et al. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License, which permits\nunrestricted use, distribution, and reproduction in any medium, provided the\noriginal author and source are credited.\nFunding: This research was supported by the W. M. Keck Foundation and by the\nNational Science Foundation-Collaborative Research in Computational Neurosci-\nence Grant IIS-0423070.\nCompeting Interests: The authors have declared that no competing interests\nexist.\n* To whom correspondence should be addressed.E-mail: shinkareva@sc.edu\nPLoS ONE | www.plosone.org 1 January 2008 | Issue 1 | e1394\ndisplayed in the center of the screen. There were six additional\npresentations of a fixation X, 21s each, distributed across the\nsession to provide a baseline measure of activation. A schematic\nrepresentation of the presentation timing is shown in Figure 1.\nfMRI procedure\nFunctional images were acquired on a Siemens Allegra 3.0T\nscanner (Siemens, Erlangen, Germany) at the Brain Imaging\nResearch Center of Carnegie Mellon University and the\nUniversity of Pittsburgh using a gradient echo EPI pulse sequence\nwith TR = 1000 ms, TE = 30 ms, and a 60u flip angle. Seventeen\n5-mm thick oblique-axial slices were imaged with a gap of 1 mm\nbetween slices. The acquisition matrix was 64 664 with\n3.12563.12565m m\n3 voxels.\nfMRI data processing and analysis\nData processing and statistical analysis were performed with\nStatistical Parametric Mapping software (SPM99, Wellcome\nDepartment of Imaging Neuroscience, London, UK). The data\nwere corrected for slice timing, motion, linear trend, and were\ntemporally smoothed with a high-pass filter using a 190 s cutoff.\nThe data were normalized to the Montreal Neurological Institute\n(MNI) template brain image using a 12-parameter affine\ntransformation. Group contrast maps were constructed using a\nheight threshold of p,0.001 (uncorrected) and an extent threshold\nof 160 voxels, resulting in the cluster-level threshold of p,0.05,\ncorrected for multiple comparisons.\nAnalyses of a single brain region at a time used region\ndefinitions derived from the Anatomical Automatic Labeling\n(AAL) system [10]. In addition to existing AAL regions, left and\nright intraparietal sulcus (IPS) regions were defined, and superior,\nmiddle, and inferior temporal gyrus regions were separated into\nanterior, middle, and posterior sections based on planes F and D\nfrom the Rademacher scheme [11], for a total of 71 regions.\nThe data were prepared for machine learning methods by\nspatially normalizing the images into MNI space and resampling\nto 36366m m\n3 voxels. Voxels outside the brain or absent from at\nleast one participant were excluded from further analysis. The\npercent signal change (PSC) relative to the fixation condition was\ncomputed at each voxel for each object presentation. The mean\nPSC of the four images acquired within a 4s window, offset 4s\nfrom the stimulus onset (to account for the delay in hemodynamic\nresponse) provided the main input measure for the machine\nlearning classifiers. The PSC data for each object-presentation\nwere further normalized to have mean zero and variance one to\nequalize the between-participants variation in exemplars.\nMachine learning methods\nClassifiers were trained to identify cognitive states associated with\nviewing drawings, using the evoked pattern of functional activity\n(mean PSC). Classifiers were functionsf of the form:f: mean_PSCRY\nj,\nj =1 , . . . ,m, where Yj were either categories (tools, dwellings)o rt e n\nexemplars (hammer, pliers,. . . ,house), where m was either 2 or 10,\naccordingly, and wheremean_PSC was a vector of mean PSC voxel\nactivations. To evaluate classification performance, trials were\ndivided into disjoint training and test sets. Prior to classification,\nrelevant features (voxels) were extracted (as described below) to\nreduce the dimensionality of the data, using only the training set for\nthis selection. A classifier was built from the training set, using these\nselected features. Classification performance was then evaluated on\nonly the left-out test set, to ensure unbiased estimation of the\nclassification error. Our previous exploration indicated that several\nfeature selection methods and classifiers produce comparable results.\nHere we report results from one feature selection method and one\nclassifier, chosen for simplicity.\nFeature selection\nFeature selection first identified the voxels whose responses were\nthe most stable over six presentations of objects within a\nparticipant, and then selected from among the stable voxels those\nthat best discriminated among objects within the training set, using\nonly the data in the training set. The 400 most stable voxels were\nselected, where voxel stability was computed as the average\npairwise correlation between 10-object vectors across six presen-\ntations. In the second step, all of the stable voxels were assessed for\nhow discriminating they were, by training a logistic regression\nclassifier to discriminate among object exemplars or categories on\nvarious subsets of only the training set. Finally, from among the\n400 voxels selected for stability, discriminating subsets of sizes 10,\n25, 50, 75, 100, 200, and 400 voxels were selected based on having\nthe highest (absolute valued) regression weights in the logistic\nregression. Locations of these selected voxels (henceforth,\ndiagnostic voxels) were visualized on a standard brain template\nusing MRIcro [12].\nFigure 1. Schematic depiction of presentation timing.\ndoi:10.1371/journal.pone.0001394.g001\nRepresentation of Objects\nPLoS ONE | www.plosone.org 2 January 2008 | Issue 1 | e1394\nClassification\nThe Gaussian Naı¨ ve Bayes (GNB) pooled variance classifier was\nused [13]. It is a generative classifier that models the joint\ndistribution of a class Y and attributes X, and assumes the\nattributes X\n1, ..., Xn are conditionally independent givenY. The\nclassification rule is:\nY/arg max\nyj\nP(Y~yj)P\ni\nP(XijY~yj):\nIn this experiment classes were equally frequent. Classification results\nwere evaluated usingk-fold cross-validation, where one example per\nclass was left out for each fold. For each participant, a classifier was\ntrained to identify either which of 10 object exemplars or which of\ntwo object categories that participant was viewing, based on only 4 s\nof fMRI data per object presentation. In all analyses, the accuracy of\nidentification was based only on test data that was completely disjoint\nfrom the training data. With a two-class classification problem, the\nchance level is 0.5. With the ten-class classification problem,rank\naccuracy was used [13]. The list of potential classes was rank-ordered\nfrom most to least likely, and the normalized rank of a correct class in\na sorted list was computed. Rank accuracy ranges from 0 to 1, and\nthe chance level is 0.5.\nPeak classification accuracy over the previously defined subsets\nhaving different numbers of voxels, e.g., 10, 25, ..., 400, was\nreported. To evaluate the statistical significance of this observed\nclassification accuracy, the result was compared to a permutation\ndistribution. For each of the 1,000 non-informative permutations\nof labels in the training set, permutation classification accuracies\nfor every set of features were computed, and the best permutation\naccuracy over the subsets with different numbers of voxels was\nrecorded. The observed accuracy was then compared to the\ndistribution of recorded permutation classification accuracies; if\nthe observed accuracy had a p-value of at most 0.001, then the\nresult was considered statistically significant.\nAnalyses of a single brain region at a time\nSingle anatomical brain regions that consistently identified object\nexemplars or categories across participants were selected using cross-\nvalidation, and the significance of those identifications was tested\nacross participants. Within each participant, a cross-validated\naccuracy for each region was computed by a logistic regression\nclassifier using all the voxels from that anatomical region. The mean\nclassification accuracy was computed for each anatomical region\nacross participants, and compared to a binomial distribution. The\nobtained p-values (computed using a normal approximation) were\ncompared to the level of significancea = 0.001, using the Bonferroni\ncorrection to account for the multiple comparisons.\nAnalysis of the confusion patterns\nSingle brain regions were compared in terms of their confusion\npatterns using a generalization of the principal components analysis\nmethod [14,15]. Within each participant, for each of the selected\nregions, a confusion matrix was constructed based on the most likely\nprediction of the classifier. Next, a regions-by-regions dissimilarity\nmatrix was constructed for each participant, where the dissimilarity\nbetween any two anatomical regions was measured as one minus the\ncorrelation coefficient of the off-diagonal elements of the correspond-\ning confusion matrices. Each dissimilarity matrix was transformed to\na cross-product matrix and normalized by the first eigenvalue.\nA compromise matrix, representing the agreement across\nparticipants, was constructed as a weighted average of all the\nparticipants’ regions-by-regions cross-product matrices. Partici-\npants’ weights were computed from the first principal component\nof the participants-by-participants similarity matrix (the first\nprincipal component is proportional to the mean of the participant\nmatrices). Each entry in the participants-by-participants similarity\nmatrix was computed by the RV-coefficient [16], which is a\nmultivariate extension of the Pearson correlation coefficient, and\nindicates the overall similarity of the two matrices:\nRV(X,Y)~\ntr(XY0 YX0 )ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\ntr(XX0 )2tr(YY0 )2\nq :\nThe RV-coefficient has been previously used in the fMRI\nliterature [17,18]. The compromise matrix was further analyzed\nby principal components analysis.\nMultiple participant analysis\nData from all but one participant were used to train a classifier to\nidentify the data from the left-out participant. This process was\nrepeated so that it reiteratively left out each of the participants.\nFeature selection was done by pooling the data of all participants\nbut the one left out. Discriminating voxel subsets of sizes 10, 25,\n50, 75, 100, 200, 400, 1000, and 2000 were selected on the basis of\nlogistic regression weights.\nRESULTS\nIdentifying object exemplars: whole brain\nThe highest rank accuracy achieved for any participant while\nidentifying individual object exemplars was 0.94. (The identifica-\ntion process obtained this rank accuracy by correctly identifying\nthe object on its first-ranked guess in 40 out of 60 presentations, on\nits second-ranked guess in 10 presentations, and on its third- and\nfourth-ranked guesses in 10 other presentations.) Reliable\n(p,0.001) classification accuracy for individual object exemplars\nwas reached for eleven out of twelve participants (as shown by the\nfilled bars in Figure 2). The mean classification rank accuracy over\nall 12 participants was 0.78 (SD = 0.11).\nThe locations of voxels that underpinned this accurate object\nexemplar identification (i.e., the diagnostic voxels), were similar (at a\ngyral level) across participants, and were distributed across the cortex\n(as shown in Figure 3). They were located in the left inferior frontal\ngyrus (LIFG), left inferior parietal lobule (LIPL), and bilateral medial\nfrontal gyrus, precentral gyrus, posterior cingulate, parahippocampal\ngyrus, cuneus, lingual gyrus, fusiform gyrus, superior parietal lobule\n(SPL), superior temporal gyrus, and middle temporal gyrus. The\nnumber of voxels (each 3.12563.12566m m3 or 59 mm3 in volume)\nfor which object exemplar identification accuracy was greatest (as\nplotted in Figure 2) ranged from 25 to 400 voxels, depending on the\nparticipant (Table S1). (Although the results are reported here for\nvoxel set sizes that have been tuned for individual participants, the\nresults are not substantially different when a fixed set size of voxels is\nused for item and category classification, within and between\nparticipants. For example, for the within-participant identification of\nindividual items, the mean accuracy (over participants) decreases by\n2.7% (from 0.78) when a fixed size of 120 voxels is used for all\nparticipants. Thus, the optimization of voxel set size is not critical to\nour main arguments, and a modal fixed value of 120 voxels can\nprovide similar outcomes.)\nIdentifying object exemplars: single brain regions\nPrevious studies have focused on one particular region, the ventral\ntemporal cortex, in an attempt to relate cognitive states to activation\npatterns in a particular region (e.g., [7]; Sayres, Ress, and Grill-\nRepresentation of Objects\nPLoS ONE | www.plosone.org 3 January 2008 | Issue 1 | e1394\nSpector, 2005, in Proceedings of Neural Information Processing\nSystems). To determine whether it was possible to identify cognitive\nstates on the basis of the activation in only a single brain region,\nclassifiers were trained using voxels from only one anatomical region\n(such as LIFG) at a time. The accuracies obtained in this ancillary\nanalysis were surprisingly high. For example, for one participant\nwhose object exemplar identification accuracy based on the whole\ncortex was 0.94, the single-region accuracy was 0.77 for left superior\nextrastriate (SES), 0.77 for LIPL, and 0.82 for left inferior extrastriate\ncortex (IES). The regions that generated reliable accuracies across\nparticipants in this single-region identification were bilateral SES,\nIES, calcarine sulcus, fusiform gyrus, IPS, left IPL, posterior\nsuperior, middle and inferior temporal gyri, postcentral gyrus, and\nhippocampus. Thus, many brain regions contain information about\nthe object exemplars.\nThese analyses provide two important clues about object\nrepresentations in the cortex. First, they indicate that the\ndiscrimination of objects was not just mediated by basic retinotopic\nrepresentations in the visual cortex, or by eye movements. Other\nbrain areas also carry reliable information about individualtools and\ndwellings, demonstrating that the exemplar identification can be\nbased on the neural representations of higher-level facets of the\nobject properties. Second, they indicate that the activation of many\nregions individually can discriminate among exemplars, thus\nproviding an important clue concerning the neural representations\nin different regions, which we explore below.\nFigure 2. High classification rank accuracies for object exemplars.Reliable (p,0.001) accuracies for the classification of object exemplars within\nparticipants (filled bars) were reached for eleven out of twelve participants, and reliable (p,0.001) accuracies for the classification of object exemplars\nwhen training on the union of data from eleven participants (unfilled bars) were reached for eight out of twelve participants. The dashed line\nindicates the highest mean of the permutation distribution across participants under the null hypothesis of no difference, i.e., chance level, among\nobject exemplars for cross-participants object exemplar identification.\ndoi:10.1371/journal.pone.0001394.g002\nFigure 3. Locations of the diagnostic voxels in object exemplar classification for the three participants having the highest accuracies are shown\non the three-dimensional rendering of the T1 MNI single-subject brain.Yellow ellipses indicate the commonality of the voxel locations for object\nidentification in LIPL across participants.\ndoi:10.1371/journal.pone.0001394.g003\nRepresentation of Objects\nPLoS ONE | www.plosone.org 4 January 2008 | Issue 1 | e1394\nIdentifying object categories: whole brain\nA classifier was trained to decode which category that the object a\nparticipant was viewing belonged to, i.e., whether it was atool or a\ndwelling. Accuracies of at least 0.97 (correct category identification\nin at least 58 out of 60 object presentations) were attained for four\nof the participants, including perfect accuracy for one of the\nparticipants (correct category identification in 60 out of 60 object\npresentations) (filled bars in Figure 4). Reliable (p ,0.001)\nclassification accuracies were reached for all participants. The\nmean classification accuracy for category identification across\ntwelve participants was 0.87 (SD = 0.10).\nThe locations of the diagnostic voxels were distributed across\nthe cortex. Similarity across participants in the locations of these\ndiagnostic voxels is illustrated in Figure 5. The cortical locations of\nthese voxels provide some face validity for the approach, because\nthey are in areas previously associated with mental functions that\nbear a good correspondence to the stimuli used here. For example,\nvoxels contributing to the identification oftools were mostly in the\nleft hemisphere, and the largest subsets were located in the ventral\npremotor cortex and posterior parietal cortex. These areas were\npreviously implicated in motor representation associated with tool\nusage [19,20,21]. Some of the voxels contributing to the\nFigure 4. High classification accuracies for object categories.Reliable (p,0.001) accuracies for classification of objects by category (filled bars) were\nreached for all participants and reliable (p,0.001) accuracies for classification of objects by category when training on the union of data from eleven\nparticipants (unfilled bars) were reached for ten out of twelve participants. The dashed line indicates the highest mean of the permutation distribution\nacross participants under the null hypothesis of no difference among the categories (i.e., chance level) for cross-participants category identification.\ndoi:10.1371/journal.pone.0001394.g004\nFigure 5. Commonality in voxel locations across the three participants having the highest category classification accuracies.Voxel locations for\nthe tools category are shown in red, and voxel locations for thedwellings category are shown in blue on the three-dimensional rendering of the T1\nMNI single-subject brain. Yellow ellipses indicate commonality in voxel locations for thetools category in LIPL across participants.\ndoi:10.1371/journal.pone.0001394.g005\nRepresentation of Objects\nPLoS ONE | www.plosone.org 5 January 2008 | Issue 1 | e1394\nidentification ofdwellings were located in the right parahippocampal\ngyrus and were within 9 mm of the previously reported para-\nhippocampal place area (PPA) [22]. The number of voxels for which\nthe object category identification accuracy was greatest ranged from\n10 to 100 voxels, depending on the participant (Table S2). For\ncomparison, SPM contrast maps showing areas of greater activity for\nthe objects compared to fixation, and fortools compared todwellings,\nare shown in Figure 6. Similar to the locations of the diagnostic\nvoxels, the activation fortools relative todwellings was left-lateralized,\nand included posterior parietal cortex. In the machine learning\nanalysis, the spatial distribution of the diagnostic voxels was more\nfine-grained, with some spatial interspersing of voxels between\ncategories, compared to the SPM contrasts.\nIdentifying object categories: single brain regions\nAs was the case for exemplar identification, the accuracies of the\ncategory identification using voxels from only a single anatomical\nregion were high; in some cases, these approached the accuracy\nobtained when the whole cortex was used (0.93 for left IES cortex,\n0.83 for left SES cortex, and 0.82 for LIPL, vs. 0.98 for the whole\ncortex, for one of the participants). The regions that generated\nreliable accuracies across participants in this single-region\nidentification analysis were bilateral SES, calcarine, IES, SPL,\nIPL, IPS, fusiform, posterior superior and middle temporal,\nposterior inferior temporal gyri, cerebellum, and left precentral,\nsuperior frontal, inferior frontal triangularis, insula, and postcen-\ntral gyri (Table 1). Although the semantic category of the objects\ncan be accurately identified on the basis of a single region, it is\neven more accurately identified when the whole cortex is taken\ninto account. Similarly to the case of identifying individual object\nexemplars, reliable information abouttools and dwellings categories\nresides not only in low-level visual brain areas but also in brain\nareas that are typically associated with higher-level properties.\nThe results above, along with previously published results,\nindicate that an object is encoded by a pattern of brain activation\nthat is broadly distributed across the brain. The fact that it is\npossible to accurately identify the stimuli based on several different\nsingle regions alone raises a question of whether multiple brain\nregions redundantly encode the same information about the\nobject, or whether each part of the brain encodes somewhat\ndifferent information, reflecting its specialization. One way to\ncompare the content of the neural representations in different\nregions is to compare the object confusion errors (incorrect first\nFigure 6. Brain activation showing areas of greater activity for (A) objects compared to fixation, and (B)tools compared todwellings. Activation\nis projected onto a surface rendering.\ndoi:10.1371/journal.pone.0001394.g006\nTable 1. Anatomical regions (out of 71) that singly produced\nreliable average classification accuracies across the twelve\nparticipants for category identification.......................................................................\nLabel Region\nLPRECENT\nL Precentral gyrus\nLSUPFRONT\nL Superior frontal gyrus\nLTRIA\nL Inferior frontal gyrus, triangular part\nLINSULA\nL Insula, rolandic operculum\nLCALC, RCALC\nL/R Calcarine fissure\nLSES, RSES L/R Cuneus, superior occipital, middle occipital gyri\nLIES, RIES\nL/R Inferior occipital, lingual gyri\nLFUSIFORM,\nRFUSIFORM\nL/R Fusiform gyrus\nLPOSTCENT\nL Postcentral gyrus\nLSPL, RSPL L/R Superior parietal gyrus, precuneus, paracentral lobule\nLIPL, RIPL L/R Inferior parietal, supramarginal, angular gyri\nLIPS, RIPS\nL/R Intraparietal sulcus\nLSTPOS, RSTPOS L/R Posterior superior temporal, posterior middle\ntemporal gyri\nLITPOS, RITPOS\nL/R Posterior inferior temporal gyrus\nLCBEL, RCBEL L/R Cerebellum\nL indicates left, and R indicates right hemisphere.\ndoi:10.1371/journal.pone.0001394.t001............................................................................................................................\nRepresentation of Objects\nPLoS ONE | www.plosone.org 6 January 2008 | Issue 1 | e1394\nguesses) that the classifier makes when it uses input from various\nsingle regions, such as misidentifying ahammer as a drill based on\nonly the left calcarine sulcus.\nSuggestive evidence that the regions systematically differ from\neach other in terms of the confusion errors they generate was\nobtained from a principal components analysis (PCA) of the single\nregions’ dissimilarity matrix. This matrix was constructed as a\nweighted average across participants (and captured 51% of the\nvariability in the data, despite considerable variation among\nparticipants in their region-specific confusion matrices). When the\nconfusion matrices generated by various single-region classifica-\ntions were compared, a number of systematicities emerged,\nindicating that in fact the different regions were encoding different\ninformation. For example, a set of visual regions (CALC,\nFUSIFORM, IES, SES) were similar to each other with respect\nto the confusion errors that they generated, and they differed from\na set of frontal regions (SUPFRONT, TRIA, PRECENT) in terms\nof their confusion errors. Figure 7 shows that a PCA of the\ndissimilarity of the regions’ individual confusion errors produces\nseparation of the regions, interpretable in terms of their\nanatomical locations, indicating that the brain activation that is\nused in identification differs qualitatively and systematically across\nregions, such as the posterior visual regions differing from frontal\nregions.\nAnother observation arising from this principal components\nanalysis was that bilaterally homologous regions were similar to\neach other with respect to confusion errors, despite being\nphysically distant from each other, suggesting that they represent\nand process rather similar information. This observation applies to\nmost regions except for the frontal cortex, where the activation in\nthe two hemispheres was more distinct and more left-lateralized.\nThe PCA indicates that there are regularities to be explored, and\nother methods, such as repetition priming [23,24,25] may\nadditionally be useful to further illuminate which object properties\nare represented in various regions.\nCommonality of neural representations across\nparticipants\nClassifiers were trained on data from 11 of the 12 participants to\ndetermine if it was possible to identify object exemplars and\ncategories in the held-out 12th participant’s data; this procedure\nwas repeated for all participants. For object exemplars, reliable\n(p,0.001) identification accuracies were reached for eight out of\ntwelve participants (unfilled bars in Figure 2). The highest\nexemplar identification rank accuracy obtained in this leave-one-\nparticipant-out method was 0.81 for one of the participants\n(compared to an accuracy of 0.53 from random predictions). The\nnumber of voxels for which the cross-participant object exemplar\nidentification accuracy was greatest ranged from 50 to 2000\nvoxels, depending on the participant (Table S1).\nFor cross-participant identification of the object category, the\nhighest rank accuracy obtained for one of the participants was 0.97\n(the category was correctly identified on the first guess in 58 out of\n60 object presentations) (unfilled bars in Figure 4). The classifier\nachieved reliable (p ,0.001) accuracy in ten out of twelve\nparticipants. The mean accuracy across participants was 0.82\n(SD = 0.09). The number of voxels for which the cross-participant\ncategory identification accuracy was greatest ranged from 10 to\n2000 voxels, depending on the participant (Table S2). Voxel-by-\nvoxel synchronization between individuals has been previously\nshown during movie watching [26]. The new result demonstrates\nthe ability to identify the category of the object (and to some\nextent, the specific object) that a participant was viewing based on\nthe neural signature derived from a set of other participants’\nactivations. This finding indicates that much of the activation\npattern that enables the identification of a cognitive state has a\nhigh degree of commonality across participants.\nDISCUSSION\nThe two main conceptual advances offered by these findings are\nthat there is an identifiable neural pattern associated with\nperception and contemplation of individual objects, and that part\nof the pattern is shared across participants. This neural pattern is\ncharacterized by a distribution of activation across many cortical\nregions, involving locations that encode diverse object properties.\nThe results uncover the biological organization of information\nabout visually depicted objects.\nDistributed representation\nThe fact that individual objects, and the categories they belong to,\ncan be accurately decoded from fMRI activity in any of several\nregions indicates that there are multiple brain regions besides\nclassical object-selective cortex that contain information about the\nobjects and categories. These new findings raise the future\nresearch challenge of determining whether these multiple regions\nall contain similar information about the object (i.e., inter-region\nrepresentational redundancy), or alternatively, whether each of the\nregions contains somewhat region-specific information about the\nobject. The distributed patterns of activation evoked by objects\nwhich are being visualized include many of the parietal and\nprefrontal regions that contained diagnostic voxels in our study\n[7,27,28,29]. The distributed activation pattern may reflect the\ndistribution across cortical areas that are specialized for various\ntypes of object properties [7,30,31]. For example, the diagnostic\nvoxels from the motor cortex that helped identify the handtools\nmay have represented the motor actions involved in the use of the\ntools. Similarly, parahippocampal voxels that were useful for\nidentifying dwellings may have represented contextual information\n[32] about some aspect of dwellinghood that has earned this\nFigure 7. Brain regions in the space of the first two principal\ncomponents of the compromise matrix based on the regions’\nconfusion errors. The first principal component separates anterior\nand posterior regions, and accounts for 8% of the variance. The second\nprincipal component separates parietal and temporal regions, and\naccounts for 6% of the variance in the data. Region labels are color-\ncoded by lobe, and are described in Table 1. The arrows are used to\nseparate labels that are close to each other.\ndoi:10.1371/journal.pone.0001394.g007\nRepresentation of Objects\nPLoS ONE | www.plosone.org 7 January 2008 | Issue 1 | e1394\nregion the label of ‘‘parahippocampal place area’’ [22]. Similarly,\nother diagnostic regions presumably represented other types of\nvisual and functional properties of the objects. An alternative\ncharacterization that is equally compatible with the empirical\nfindings is that there are many ways in which we can think about,\nperceive, visualize, and interact with objects, for which different\nbrain areas are differentially specialized. In this view, it is not just\nthe isolated, intrinsic properties of the objects that are being\nrepresented, but also the different ways that we mentally and\nphysically interact with the objects.\nThe information content within a number of individual\nanatomical regions is sufficient for exemplar and category\nidentification, but the content of the representation appears to\nbe somewhat different across regions. Comparison of the\nconfusions in different regions suggests that despite the similarities\nin the identification accuracies provided by the various regions,\nanterior and posterior regions may represent different aspects of\nthe objects, and that different brain regions provide the classifier\nwith different kinds of information, likely corresponding to the\ndifferent types of perceptual, motor, and conceptual processing\nthat is performed in various brain regions.\nCommonality of the neural representation of object\ncategories and exemplars across participants\nThe ability to identify object categories across participants reveals\nthe striking commonality of the neural basis of this type of\nsemantic knowledge. The neural invariances, in terms of the\nlocations and activation amplitudes of common diagnostic voxels,\nemerged despite the methodological difficulty of normalizing the\nmorphological differences among participants. The challenge of\ncomparing the thoughts of different people has been met here in a\nvery limited sense, although there always remains uncertainty\nabout whether the information content corresponding to a\ndiagnostic voxel’s activity was the same across participants. Still,\nthe new findings indicate that there is cross-participant common-\nality in the neural signature at the level of semantic property\nrepresentations (and not just visual features).\nThe category and exemplar classification accuracies when\ntraining across participants were on average lower than when\ntraining within participants, indicating that a critical diagnostic\nportion of the neural representation of the categories and\nexemplars is still idiosyncratic to individual participants. There is\napparently systematic activation within an individual (permitting\nbetter identification of that individual’s cognitive state) that lends\nindividuality to object representations.\nEven though the classification accuracy was generally higher\nwithin as opposed to across participants, for a small number of\nparticipants (all of whom had low within-participant identification\naccuracies), identification based on training data from other\nparticipants actually resulted in higher accuracy than when\ntraining based on that participant’s own data. In these few cases,\nthe individual’s idiosyncratic activation pattern may have been too\nvariable over presentations to outperform the communal neural\nsignature. These cases provide a demonstration of the remarkable\npower of the shared activation pattern to identify the thoughts of\nothers.\nSUPPORTING INFORMATION\nTable S1 Identification accuracies of object exemplars based on\nthe patterns of functional activity of that or other participants.\nObserved accuracies, number of voxels, and the p-value based on\npermutation distribution with 1,000 permutations are reported.\nFound at: doi:10.1371/journal.pone.0001394.s001 (0.04 MB\nDOC)\nTable S2 Identification accuracies of object categories based on\nthe patterns of functional activity of that or other participants.\nObserved accuracies, number of voxels, and the p-value based on\npermutation distribution with 1,000 permutations are reported.\nFound at: doi:10.1371/journal.pone.0001394.s002 (0.04 MB\nDOC)\nACKNOWLEDGMENTS\nWe would like to thank Vladimir Cherkassky and Sandesh Aryal for\ntechnical assistance, reviewers for helpful comments on the earlier version\nof the manuscript and Stacey Becker and Rachel Krishnaswami for help in\nthe preparation of the manuscript.\nAuthor Contributions\nConceived and designed the experiments: MJ SS TM. Performed the\nexperiments: SS. Analyzed the data: SS VM. Contributed reagents/\nmaterials/analysis tools: SS RM VM WW. Wrote the paper: MJ SS TM.\nREFERENCES\n1. Hanson SJ, Matsuka T, Haxby JV (2004) Combinatorial codes in ventral\ntemporal lobe for object recognition: Haxby (2001) revisited: is there a ‘‘face’’\narea? NeuroImage 23: 156–166.\n2. Cox DD, Savoy RL (2003) Functional magnetic resonance imaging (fMRI)\n‘‘brain reading’’: detecting and classifying distributed patterns of fMRI activity in\nhuman visual cortex. NeuroImage 19: 261–270.\n3. Carlson TA, Schrater P, He S (2003) Patterns of activity in the categorical\nrepresentations of objects. Journal of Cognitive Neuroscience 15: 704–717.\n4. Mourao-Miranda J, Bokde AL, Born C, Hampel H, Stetter M (2005) Classifying\nbrain states and determining the discriminating activation patterns: support\nvector machine on functional MRI data. NeuroImage 28: 980–995.\n5. LaConte S, Strother S, Cherkassky V, Anderson J, Hu X (2005) Support vector\nmachines for temporal classification of block design fMRI data. NeuroImage 26:\n317–329.\n6. Polyn SM, Natu VS, Cohen JD, Norman KA (2005) Category-specific cortical\nactivity precedes retrieval during memory search. Science 310: 1963–1966.\n7. Haxby JV, Gobbini MI, Furey ML, Ishai A, Schouten JL, et al. (2001)\nDistributed and overlapping representations of faces and objects in ventral\ntemporal cortex. Science 293: 2425–2430.\n8. O’Toole A, Jiang F, Abdi H, Haxby JV (2005) Partially distributed\nrepresentations of objects and faces in ventral temporal cortex. Journal of\nCognitive Neuroscience 17: 580–590.\n9. Rosch E, Mervis CB, Gray WD, Johnson DM, Boyes-Braem P (1976) Basic\nobjects in natural categories. Cognitive Psychology 8: 382–439.\n10. Tzourio-Mazoyer N, Landeau B, Papathanassiou D, Crivello F, Etard O, et al.\n(2002) Automated anatomical labeling of activations in SPM using a\nmacroscopic anatomical parcellation of the MNI MRI single-subject brain.\nNeuroImage 15: 273–289.\n11. Rademacher J, Galaburda AM, Kennedy DN, Filipek PA, Caviness VS (1992)\nHuman cerebral cortex: localization, parcellation and morphometry with\nmagnetic resonance imaging. Journal of Cognitive Neuroscience 4: 352–374.\n12. Rorden C, Brett M (2000) Stereotaxic display of brain lesions. Behavioural\nNeurology 12: 191–200.\n13. Mitchell TM, Hutchinson R, Niculescu RS, Pereira F, Wang X, et al. (2004)\nLearning to decode cognitive states from brain images. Machine Learning 57:\n145–175.\n14. Lavit C, Escoufier Y, Sabatier R, Traissac P (1994) The ACT (STATIS\nmethod). Computational statistics and data analysis 18: 97–119.\n15. Abdi H, Valentin D (2007) STATIS. In: Salkind NJ, ed. Encyclopedia of\nmeasurement and statistics. Thousand Oaks (CA): Sage. pp 955–962.\n16. Robert P, Escoufier Y (1976) A unifying tool for linear multivariate statistical\nmethods: the RV-coefficient. Applied Statistics 25: 257–265.\n17. Kherif F, Poline JB, Meriaux S, Benali H, Flandin G, et al. (2003) Group\nanalysis in functional neuroimaging: selecting subjects using similarity measures.\nNeuroImage 20: 2197–2208.\n18. Shinkareva SV, Ombao HC, Sutton BP, Mohanty A, Miller GA (2006)\nClassification of functional brain images with a spatio-temporal dissimilarity\nmap. NeuroImage 33: 63–71.\nRepresentation of Objects\nPLoS ONE | www.plosone.org 8 January 2008 | Issue 1 | e1394\n19. Chao LL, Martin A (2000) Representation of manipulable man-made objects in\nthe dorsal stream. NeuroImage 12: 478–484.\n20. Phillips JA, Noppeney U, Humphreys GW, Price CJ (2002) Can segregation\nwithin the semantic system account for category-specific deficits? Brain 125:\n2067–2080.\n21. Culham JC, Valyear KF (2006) Human parietal cortex in action. Current\nOpinion in Neurobiology 16: 205–212.\n22. Epstein R, Harris A, Stanley D, Kanwisher N (1999) The parahippocampal\nplace area: recognition, navigation, or encoding? Neuron 23: 115–125.\n23. James TW, Humphrey GK, Gati JS, Menon RS, Goodale MA (2002)\nDifferential effects of viewpoint on object-driven activation in dorsal and ventral\nstreams. Neuron 35: 793–801.\n24. Grill-Spector K, Kushnir T, Edelman S, Avidan G, Itzchak Y, et al. (1999)\nDifferential processing of objects under various viewing conditions in the human\nlateral occipital complex. Neuron 24: 187–203.\n25. Vuilleumier P, Henson RN, Driver J, Dolan RJ (2002) Multiple levels of visual\nobject constancy revealed by event-related fMRI of repetition priming. Nature\nNeuroscience 5: 491–499.\n26. Hasson U, Nir Y, Levy I, Fuhrmann G, Malach R (2004) Intersubject\nsynchronization of cortical activity during natural vision. Science 303:\n1634–1640.\n27. Ishai A, Ungerleider LG, Martin A, Haxby JV (2000) The representation of\nobjects in the human occipital and temporal cortex. Journal of Cognitive\nNeuroscience 12: 35–51.\n28. Ishai A, Ungerleider LG, Martin A, Schouten JL, Haxby JV (1999) Distributed\nrepresentation of objects in human ventral visual pathway. Proceedings of the\nNational Academy of Sciences 96: 9379–9384.\n29. Mechelli A, Price C, Friston KJ, Ishai A (2004) Where bottom-up meets top-\ndown: neuronal interactions during perception and imagery. Cerebral Cortex\n14: 1256–1265.\n30. Martin A, Ungerleider LG, Haxby JV (2000) Category specificity and the brain:\nthe sensory/motor model of semantic representations of objects. In:\nGazzaniga MS, ed. The new cognitive neurosciences. Cambridge: MIT Press.\npp 1023–1035.\n31. Goldberg RF, Perfetti CA, Schneider W (2006) Perceptual knowledge retrieval\nactivates sensory brain regions. Journal of Neuroscience 26: 4917–4921.\n32. Aminoff E, Gronau N, Bar M (2006) The parahippocampal cortex mediates\nspatial and nonspatial associations. Cerebral Cortex 17: 1493–1503.\nRepresentation of Objects\nPLoS ONE | www.plosone.org 9 January 2008 | Issue 1 | e1394",
  "topic": "Perception",
  "concepts": [
    {
      "name": "Perception",
      "score": 0.6777787804603577
    },
    {
      "name": "Cognition",
      "score": 0.6065216660499573
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5982165932655334
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5538479685783386
    },
    {
      "name": "Object (grammar)",
      "score": 0.5386552214622498
    },
    {
      "name": "Psychology",
      "score": 0.5357216000556946
    },
    {
      "name": "Visual perception",
      "score": 0.5268137454986572
    },
    {
      "name": "Voxel",
      "score": 0.5171462893486023
    },
    {
      "name": "Neural correlates of consciousness",
      "score": 0.5094881653785706
    },
    {
      "name": "Brain activity and meditation",
      "score": 0.4874406158924103
    },
    {
      "name": "Functional magnetic resonance imaging",
      "score": 0.4530966579914093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3160175085067749
    },
    {
      "name": "Computer science",
      "score": 0.28368932008743286
    },
    {
      "name": "Electroencephalography",
      "score": 0.2564997673034668
    },
    {
      "name": "Neuroscience",
      "score": 0.2087506353855133
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}