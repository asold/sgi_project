{
  "title": "Temporal Difference-Based Graph Transformer Networks For Air Quality PM2.5 Prediction: A Case Study in China",
  "url": "https://openalex.org/W4283592975",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2099127306",
      "name": "Zhen Zhang",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2101420493",
      "name": "Shiqing Zhang",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2103022835",
      "name": "Xiaoming Zhao",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2186225118",
      "name": "Linjian Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011523473",
      "name": "Jun Yao",
      "affiliations": [
        "Taizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2099127306",
      "name": "Zhen Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101420493",
      "name": "Shiqing Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103022835",
      "name": "Xiaoming Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186225118",
      "name": "Linjian Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011523473",
      "name": "Jun Yao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3110010218",
    "https://openalex.org/W3024071797",
    "https://openalex.org/W3215971971",
    "https://openalex.org/W2077865456",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3021936800",
    "https://openalex.org/W3000104524",
    "https://openalex.org/W3039016011",
    "https://openalex.org/W3091985306",
    "https://openalex.org/W2084005109",
    "https://openalex.org/W2970055828",
    "https://openalex.org/W1974540239",
    "https://openalex.org/W2990955039",
    "https://openalex.org/W6789350765",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W1968840994",
    "https://openalex.org/W3189604654",
    "https://openalex.org/W3013537470",
    "https://openalex.org/W590735017",
    "https://openalex.org/W2009692134",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2041471468",
    "https://openalex.org/W3190284897",
    "https://openalex.org/W3133183845",
    "https://openalex.org/W2297152540",
    "https://openalex.org/W6786282642",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2559759128",
    "https://openalex.org/W2174574259",
    "https://openalex.org/W3083561918",
    "https://openalex.org/W3124650867",
    "https://openalex.org/W1598069255",
    "https://openalex.org/W6770291295",
    "https://openalex.org/W3146366485",
    "https://openalex.org/W2965521597",
    "https://openalex.org/W3081466337",
    "https://openalex.org/W2892341857",
    "https://openalex.org/W2569758175",
    "https://openalex.org/W3108703399",
    "https://openalex.org/W4200546284",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2031478711",
    "https://openalex.org/W3117062925",
    "https://openalex.org/W2813865948",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2949262395",
    "https://openalex.org/W6751826626",
    "https://openalex.org/W2975879014",
    "https://openalex.org/W2899742462",
    "https://openalex.org/W2898407312",
    "https://openalex.org/W2888926545",
    "https://openalex.org/W3086892812",
    "https://openalex.org/W2789849108",
    "https://openalex.org/W3217128657",
    "https://openalex.org/W3042384558",
    "https://openalex.org/W2009331722",
    "https://openalex.org/W3046584416",
    "https://openalex.org/W3137937485",
    "https://openalex.org/W6786852218",
    "https://openalex.org/W2076485554",
    "https://openalex.org/W2989156240",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2966783050",
    "https://openalex.org/W3170630188",
    "https://openalex.org/W3167456680",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "Air quality PM2.5 prediction is an effective approach for providing early warning of air pollution. This paper proposes a new deep learning model called temporal difference-based graph transformer networks (TDGTN) to learn long-term temporal dependencies and complex relationships from time series PM2.5 data for air quality PM2.5 prediction. The proposed TDGTN comprises of encoder and decoder layers associated with the developed graph attention mechanism. In particular, considering the similarity of different time moments and the importance of temporal difference between two adjacent moments for air quality PM2.5prediction, we first construct graph-structured data from original time series PM2.5 data at different moments without explicit graph structure. Then we improve the self-attention mechanism with the temporal difference information, and develop a new graph attention mechanism. Finally, the developed graph attention mechanism is embedded into the encoder and decoder layers of the proposed TDGTN to learn long-term temporal dependencies and complex relationships from a graph prospective on air quality PM2.5 prediction tasks. Experiment results on two collected real-world datasets in China, such as Beijing and Taizhou PM2.5 datasets, show that the proposed method outperforms other used methods on both short-term and long-term air quality PM2.5 prediction tasks.",
  "full_text": "Temporal Difference-Based Graph\nTransformer Networks For Air Quality\nPM2.5 Prediction: A Case Study in\nChina\nZhen Zhang1, Shiqing Zhang2, Xiaoming Zhao2*, Linjian Chen3 and Jun Yao1\n1Zhejiang Provincial Key Laboratory of Evolutionary Ecology and Conservation, Taizhou University, Taizhou, China,2Institute of\nIntelligent Information Processing, Taizhou University, Taizhou, China,3Yuhuan Branch of Taizhou Ecological Environment\nBureau, Yuhuan, China\nAir quality PM2.5 prediction is an effective approach for providing early warning of air\npollution. This paper proposes a new deep learning model called temporal difference-\nbased graph transformer networks (TDGTN) to learn long-term temporal\ndependencies and complex relationships from time series PM2.5 data for air\nquality PM2.5 prediction. The proposed TDGTN comprises of encoder and\ndecoder layers associated with the developed graph attention mechanism. In\nparticular, considering the similarity of d ifferent time moments and the importance\nof temporal difference between two adjacent moments for air quality PM2.5prediction,\nwe ﬁrst construct graph-structured data from original time series PM2.5 data at\ndifferent moments without explicit graph structure. Then we improve the self-attention\nmechanism with the temporal difference information, and develop a new graph\nattention mechanism. Finally, the de veloped graph attention mechanism is\nembedded into the encoder and decoder layers of the proposed TDGTN to learn\nlong-term temporal dependencies and complex relationships from a graph\nprospective on air quality PM2.5 prediction tasks. Experiment results on two\ncollected real-world datasets in China, such as Beijing and Taizhou PM2.5\ndatasets, show that the pr oposed method outperforms other used methods on\nboth short-term and long-term air quality PM2.5 prediction tasks.\nKeywords: air quality prediction, deep learning, temporal difference, graph attention, transformer, long-term\ndependency\n1 INTRODUCTION\nWith the rapid development of economy, industrialization, and urbanization, a large number of\nurban cities throughout the world are undergoing increasingly serious air pollution problems,\nthereby threatening human health and lives, the environment, and sustainable social development\n(Darçın, 2014; Ke et al., 2021). In pariticular, long exposure to polluted air leads to a variety of\ncardiovascular and respiratory sicknesses like lung cancer, bronchial asthma, atherosclerosis, chronic\nobstructive pulmonary diseases,etc (Schwartz, 1993; Chang Q. et al., 2020; Yan et al., 2020). Wherein,\nPM2.5 (ﬁne particulate with an aerodynamic diameter of 2.5μm or smaller) has become the primary\nfactor of air pollution, and the increasing PM2.5 concentration directly threats to human health\nEdited by:\nJason G. Su,\nUniversity of California, Berkeley,\nUnited States\nReviewed by:\nPatricio Perez,\nUniversity of Santiago, Chile\nYing Zhu,\nXi’an University of Architecture and\nTechnology, China\n*Correspondence:\nXiaoming Zhao\ntzxyzxm@163.com\nSpecialty section:\nThis article was submitted to\nEnvironmental Informatics and Remote\nSensing,\na section of the journal\nFrontiers in Environmental Science\nReceived: 21 April 2022\nAccepted: 07 June 2022\nPublished: 27 June 2022\nCitation:\nZhang Z, Zhang S, Zhao X, Chen L and\nYao J (2022) Temporal Difference-\nBased Graph Transformer Networks\nFor Air Quality PM2.5 Prediction: A\nCase Study in China.\nFront. Environ. Sci. 10:924986.\ndoi: 10.3389/fenvs.2022.924986\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249861\nORIGINAL RESEARCH\npublished: 27 June 2022\ndoi: 10.3389/fenvs.2022.924986\n(Zhang B. et al., 2020). Aa a result, real-time, accurate and long-\nterm PM2.5 concentration predictions in advance play a\nsigniﬁcant role in preventing and curbing air pollution,\ngovernment decision-making, as well as protecting human\nhealth, and so on.\nSo far, a large number of studies have explored the\nperformance of various methods for air quality PM2.5\nprediction (Liao et al., 2020; Liu et al., 2021). Prior methods\nfor air quality prediction can be mainly grouped into two\ncategories, namely physical prediction methods and statistical\nprediction methods. The physical prediction methods are a\nnumerical simulation model on the basis of aerodynamics,\natmospheric physics, and chemical reactions for studying\npollutant diffusion mechanism (Geng et al., 2015). The well-\nknown physical prediction models include chemical transport\nmodels (CTMs) (Mihailovic et al., 2009; Ponomarev et al., 2020),\ncommunity multiscale air quality (CMAQ) (Zhang et al., 2014),\nweather research and forecasting (WRF) (Powers et al., 2017), the\nGEOS-Chem model (Lee et al., 2017), and so on. Nevertheless,\nowing to the complicated pollutant diffusion mechanism,\nleveraging these models leads to several limitations such as\nexpensitive computation, the complexity of processing,\nuncertainty of parameters, and low prediction accuracy (Wang\nJ. et al., 2019). Statistical prediction methods employ a statistical\nmodeling strategy to forecast future air quality on the basis of the\nobserved historical time series PM2.5 data. In comparison with\nphysical prediction methods, statistical prediction methods have\nlow compuation since they can avoid the complicated pollutant\ndiffusion mechanism. In this case, they can still obtain\ncompetitive performance to physical prediction methods on\nair quality PM2.5 prediction tasks ( Suleiman et al., 2019 ).\nOwing to these advantages, leverage statistical prediction\nmethods for air quality PM2.5 prediction is more extensive.\nThere are two kinds of statistical prediction models: linear and\nnonlinear. The commonly-used linear statistical prediction\nmodels, which is based on the supposed linearity of real-world\nobserved data, are autoregressive moving average (ARMA)\n(Graupe et al., 1975 ), and autoregressive integrated moving\naverage (ARIMA) ( Jian et al., 2012 ; Cekim, 2020 ).\nConsiderning the nonlinearity of real-world observed data, the\nconventional nonlinear statistical prediction methods are\nmachine learning (ML) models. At present, Various common\nmachine learning (ML) algorithms, including multiple linear\nregression (MLR) ( Donnelly et al., 2015 ), arti ﬁcial neural\nnetwork (ANN) (Arhami et al., 2013; Agarwal et al., 2020),\nsupport vector regression (SVR) (Yang et al., 2018; Chu et al.,\n2021), random forest (RF) (Gariazzo et al., 2020), as well as\nensemble learning of multple ML models (Xiao et al., 2018), have\nbeen employed for air quality PM2.5 prediction. Among these\nmodels, as non-linear tool ANN has become the most popular\none, since ANN is able to effectively simulate nonlinearities and\ninteractive relationships when dealing with non-linear systems,\nespecially when theoretical models are hard to be developed (Feng\net al., 2015). The widely-used ANN models contain multilayer\nperceptron (MLP), back propagation neural network (BPNN)\n(Wang W. et al., 2019), and general regression neural network\n(GRNN) models (Zhou et al., 2014). These ML methods have\ndistinct mathematical logic in which the correlation between\ninput and output data is relatively deﬁnite. Additionally, they\nhave relatively shallow network structure, resulting in the limited\nability of modelling dependency on time series PM2.5 data.\nTo address the above-mentioned issue, the recently-emerged\ndeep learning (Hinton and Salakhutdinov, 2006; LeCun et al.,\n2015) methods may provide a possible clue. With the aid of multi-\nlayer network architecture, deep learning algorithms are able to\naumomatally extract multiple levels of abstract feature\nrepresentations from input data. Due to such powerful feaure\nlearning capability, deep learning methods have made great\nbreakthroughs (LeCun et al., 2015; Pouyanfar et al., 2018)i n\nobject detection and image classi ﬁ\ncation, natural language\nprocessing (NLP), speech signal processing, and so on.\nIn recent years, various deep learning models have also been\nsuccessfully employed for air quality PM2.5 prediction (Liao\net al., 2020; Aggarwal and Toshniwal, 2021; Saini et al., 2021;\nSeng et al., 2021; Zaini et al., 2021). In particular, Ragabet al.,\npresented a method of air pollution index (AQI) prediction by\nmeans of using one-dimensional convolutional neural network\n(1D-CNN) and exponential adaptive gradients optimization for\nKlang city, in Malaysia (Ragab et al., 2020). In addition, recurrent\nneural network (RNN) (Elman, 1990), and its variants such as\nlong short term memory (LSTM) (Hochreiter and Schmidhuber,\n1997) and gated recurrent unit (GRU) (Chung et al., 2014), have\nbecome popular techniques for forecasting time series PM2.5\ndata. This is attributed to the fact these RNN-based models have\nexcellent capablity of capturing temporal dependency from input\ntime series PM2.5 data. A bidirectional LSTM (BiLSTM)\nconsisting of both forward and backward LSTM units was\nprovided for univariate air quality PM2.5 prediction (De Melo\net al., 2019). In this study, they also adopted transfer learning\ntechniques to further improve air quality prediction performance.\nat wider daily and weekly temporal intervals. Jinet al., proposed a\nnew model integrating multiple nested long short term memory\nnetworks (MN-LSTMs) for accurate AQI forecasting enlightened\nwith the federated learning (Jin et al., 2021).\nAt present, several hybrid deep learning framework (Chang\nY.-S. et al., 2020; Aggarwal and Toshniwal, 2021; Du et al., 2021;\nZhang et al., 2021) have attracted extentive attention for air\nquality PM2.5 forecasting. Specially, a hybrid deep learning\nmodel, based on one-dimensional CNNs (1D-CNN) and\nbidirectional LSTMs for spatial-temporal feature learning, was\ndeveloped for air quality prediction (Du et al., 2021). This hybrid\ndeep learning framework focused on learning the spatial-\ntemporal correlation features and interdependence of\nmultivariate air quality data. A spatio-temporal CNN and\nLSTM (CNN-LSTM) model (Pak et al., 2020) was provided to\nforecast the next day’s daily average PM2.5 concentration in\nBeijing City. In this CNN-LSTM model, the mutual information\n(MI) was used for the spatio-temporal correlation analysis, which\ntook into account both the linear and nonlinear correlation\nbetween target and observed parameter values. A new spatial-\ntemporal deep learning method with bidirectional gated recurrent\nunit integrated with attention mechanism (BiAGRU) (Zhang K.\net al., 2020), was proposed for accurate air quality forecasting. A\nhierarchical deep learning framework comprising of three\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249862\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\ncomponents like the encoder, STAA-LSTM, and the decoder was\npresented for forecasting the real-world air quality data of Delhi\n(Abirami and Chitra, 2021). In their work, the encoder was used\nto encode all spatial relations from input data. The STAA-LSTM,\nas a variant of LSTM, aimed to forecast future spatiotemporal\nrelations in the latent space. The decoder was leveraged to decode\nthese relations for the actual forecasting.\nIn addition, graph neural networks (GNN) (Scarselli et al.,\n2008) have become an emerging active research subject in machine\nlearning, and obtained great success in processing graph-structured\ndata owing to their powerful graph-based feature learning ability.\nThe representative GNN method is graph convolutional neural\nnetwork (GCN) (Kipf and Welling, 2016). GCN is a generalization\nof conventional CNNs to deal with homogeneous graph, in which\nthe graph nodes and edge types should be identical. Considering\nthe fact that different air quality monitoring stations may have\ndifferent topological structure in space, GCNs can be intuitively\nused to capture the spatial dependencies among multiple air quality\nmonitoring stations. Specially, Xuet al. proposed a hierarchical\nGCN Method called HighAir (Xu et al., 2021) for air quality\nprediction, in which a city graph and station graphs were\nconstructed to take into account the city-level and station-level\npatterns of air quality, respectively. Chenet al.presented the group-\naware graph neural network (GAGNN) (Chen et al., 2021)f o r\nnationwide city air quality prediction. GAGNN aimed to build up a\ncity graph and a city group graph to learn the spatial and latent\ndependencies between cities, respectively. In addition, combining\nGNN with LSTM has recently become a popular method to model\nspatio-temporal dependencies for air quality PM2.5 forecasting.\nSpecially, a graph-based LSTM (GLSTM) model (Gao and Li,\n2021) was developed to forecast PM2.5 concentration in Gansu\nProvince of Northwest China. They regarded all air quality\nmonitoring stations as a graph, and yielded a parameterized\nadjacency matrix based on the adjacency matrix of the graph.\nThen, integrating the parameterized adjacency matrix with LSTMs\nwas employed to learn spatio-temporal dependecies for air quality\nPM2.5 prediction. These existing graph-based works aim to\ncapture the spatial dependencies among multiple air quality\nmonitoring stations rather than the single air quality\nmonitoring station.\nMore recently, the attention mechanism (Niu et al., 2021) has\nbecome an important direction in theﬁeld of deep learning. In\nparticular, the temporal attention mechanism is capable of\nadaptively assigning greater weights to input data at different\ntimes from a sequence with higher correlations for target\nprediction tasks. Moreover, it can be also calculated in parallel,\nthereby improving the computational ef ﬁciency. Among\nattention-based deep learning methods, the recently-developed\nTransformer (Vaswani et al., 2017) technique achieving great\nsuccess for machine translation tasks in NLP, has become\nfashionable at present. The original Transformer model does\nnot contain any recurrent structures and convolutions and aims\nto model temporal dependencies in machine translation tasks\nwith the aid of the powerful self-attention mechanism. So far, the\nTransformer models have exhibit better performance than RNN\nand LSTM in capable of learning long-range dependencies in a\nnumber of areas ranging from NLP (Vaswani et al., 2017; Neishi\nand Yoshinaga, 2019), object detection and classiﬁcation (Bazi\net al., 2021; Duke et al., 2021 ; Lanchantin et al., 2021 ), to\nelectricity consuming load analysis ( Yue et al., 2020 ; Zhou\net al., 2021).\nAlthough the recently-emerged Transformer techniques have\nachieved promising performance in various domains, few studies\nfocus on the applications of Transformer techniques to air quality\nPM2.5 prediction. Additionally, as a typical graph-based deep\nlearning method, GNNs have a powerful graph-based feature\nlearning ability when processing graph-structured data. As a\ntypical attention-based deep learning method, Transformer\ntechniques are able to effectively model temporal dependencies\ndue to the used self-attention mechanism. In this case, how to\nintegrate the advantages of GNNs and Transformer techniques\nbased on a graph attention mechanism for air quality PM2.5\nprediction is a challenging problem, which is under-exploited in\nexisting works.\nInspired by the recent great success of GNNs and Transformer,\nthis paper combines the advantages of GNNs processing graph-\nstructured data and Transformer modeling temporal\ndependencies, and proposes a novel graph attention-based deep\nlearning model called temporal difference-based graph transformer\nnetworks (TDGTN) for air quality PM2.5 prediction. The main\ncontributions of this paper are summarized as follows:\n1) Considering the similarity of different time moments and the\nimportance of temporal difference between two adjacent\nmoments for air quality prediction, for the single air\nmonitoring station we aim to construct graph-structured\ndata from the obtained time series PM2.5 data at different\nmoments without explicit graph structure. To the best of our\nknowledge, this is theﬁrst attempt to exploit graph-based air\nquality PM2.5 prediction for the single air monitoring station\nfrom a graph-based perspective.\n2) This paper combines the advantages of both GNNs and\nTransformer, and proposes a new deep learning model\ncalled TDGTN to learn long-term temporal dependencies\nand complex relationships from time series PM2.5 data for\nair quality PM2.5 prediction. In the proposed TDGTN model,\nwe improve the self-attention mechanism with the temporal\ndifference information and develop a new graph attention\nmechanism.\n3) This paper evaluates the performance of the proposed\nTDGTN on two real-world datasets, in China, including\nBeijing and Taizhou PM2.5 datasets and compares it with\nthe state-of-the-art models such as ARMA, SVR, CNN, LSTM,\nand the original Transformer. Experimental results\ndemonstrate that TDGTN outperforms existing models\nboth short-term (1 h) and long-term (6, 12, 24, 48 h) air\nquality prediction tasks.\n2 DATA AND METHODS\n2.1 Study Area and Data Collection\nTo verify the effectiveness of the proposed method on air quality\nprediction tasks, we adopt two real-world hourly air quality\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249863\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nPM2.5 datasets to perform air quality prediction experiments.\nThey are Beijing PM2.5 dataset (Liang et al., 2015), and Taizhou\nPM2.5 dataset. Figure 1 shows the location of Beijing and\nTaizhou air quality monitoring stations in China. In\nparticular, Beijing city is located at 116\n°66′ east longitude and\n40°13′ north latitude. Taizhou is located at 121°42′ east longitude\nand 28°65′ north latitude. Taizhou city lies in the southeast of\nZhejiang Province, China. These two cities represent two distinct\nclimate areas in China. Specially, Beijing city is a typical dry area\nin the north of China, whereas Taizhou city is a typical wet area in\nthe south of China.\nThe used Beijing PM2.5 dataset contains around 43,800\nsamples, each of which was recorded with an hourly interval\nranging from 01/01/2010 to 12/31/2014. In this dataset, the\nPM2.5 data (http://www.mee.gov.cn/) was collected from the\nUnited States Embassy in Beijing, and the corresponding\nmeteorological data (http://tianqi.2345.com/) was collected\nfrom Beijing Capital International Airport. This dataset\ncomprises of eight feature items, including PM2.5\nconcentration (ug/m\n3), dew point, temperature, pressure,\ncombined wind direction, cumulated wind speed (m/s),\ncumulated hours of snow, cumulated hours of rain.Figure 2\npresents an illustration of hourly PM2.5 values from 5/01/2014 to\n5/31/2014 on Beijing PM2.5 dataset.\nThe used Taizhou PM2.5 dataset contains about 26,000 hourly\nrecords ranging from 01/01/2017 to 12/31/2019. They were\nFIGURE 1 |The location of Beijing and Taizhou air quality monitoring stations in China.\nFIGURE 2 |An illustration of hourly PM2.5 values (ug/m3) from 5/01/2014 to 5/31/2014 on Beijing PM2.5 dataset [Each observation point in the horizontal axis\ndenotes a timescale (hour) related to the collected PM2.5 value, as described in the vertical axis in thisﬁgure].\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249864\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\ncollected by our teams from the single Hongjia monitoring\nstation, which is located in Jiaojiang urban district from\nTaizhou city in the southeast of Zhejiang Province. This\ndataset also include eight feature items, such as PM2.5\nconcentration (ug/m\n3), dew point, temperature, pressure,\ncombined wind direction, cumulated wind speed (m/s),\ncumulated hours of rain, cumulated hours of relative\nhumidity. Figure 3 provides an illustration of hourly PM2.5\nvalues from 10/01/2019-10/31/2019 on Taizhou PM2.5 dataset.\n2.2 Method\nFigure 4 presents an overview of our proposed temporal\ndifference-based graph transformer networks (TDGTN) for\nair quality PM2.5 prediction. Like the original Transformer\n(V a s w a n ie ta l . ,2 0 1 7), our proposed TDGTN model\ncomprises of encoder and decoder layers associated with the\ngraph attention mechanism, as depicted inFigure 4.C o m p a r e d\nwith the original Transformer (Vaswani et al., 2017), TDGTN\nhas two distinct properties. One is that we embed the graph\nattention into the encoder and decoder instead of the common\nmulti-head attention in the original Transformer except for the\nused masked multi-head attention. The other is that based on\ntime series PM2.5 data, the ﬁrst-order backward difference\ninformation between two adjacent moments is embedded\ninto the constructed graph so as to learn long-term\ndependency and complex relationships from a graph\nperspective. In the following, we will describe the relevant\ndetails of TDGTN.\nFIGURE 3 |An illustration of hourly PM2.5 values (ug/m3) from 10/01/2019-10/31/2019 on Taizhou PM2.5 dataset [Each observation point in the horizontal axis\ndenotes a timescale (hour) related to the collected PM2.5 value, as described in the vertical axis in thisﬁgure].\nFIGURE 4 |An overview of our proposed temporal difference-based graph transformer networks (TDGTN) for air quality PM2.5 prediction.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249865\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\n2.2.1 Problem Description\nGiven a lengthLx of input time series dataX /equals{ x1,x 2, ... ,x Lx}\n(xi ∈ Rdx ) with feature dimension dx, the used air quality\nprediction methods aim to forecast the corresponding time\nseries PM2.5 data Y /equals{ y\n1,y 2, ... ,y Ly}( yi ∈ Rdy ) with a\nlength Ly and feature dimension dy. The encoder aims to\nlearn hidden continuous feature representations Z /equals\n{z1,z 2, ... ,z Lz} with a length Lz from input time series data\nX. Then, the decoder produces an output ofY /equals{ y1,y 2, ... ,y Ly}\nfrom the obtained hidden continuous feature representations in\nthe encoder. This inference is performed by means of an step-by-\nstep implementation, in which the decoder computes a new\nhidden feature representation z\nk+1 from the previous feature\nrepresentation zk and other outputs in k-th step, and then\npredict (k + 1)-th time series datayk+1.\n2.2.2 Graph Construction\nGraphs, as a special form of data, aim to characterize the\nrelationships between different entities. GNNs endow each\nnode in a graph with an ability of learning its neighborhood\ncontext by means of propagating information through graph-\nbased structures. In this case, air quality PM2.5 prediction for the\nsingle air monitoring station can be intuitively regarded as a\nproblem of graph-based multivariate time series forecasting from\na view point of graphs. Considering the similarity of different\ntime moments and the importance of temporal difference\nbetween two adjacent moments for air quality prediction, for\nthe single air monitoring station we ﬁrst construct graph-\nstructured data from the obtained time series PM.25 data at\ndifferent moments without explicit graph structure, as described\nbelow. Based on the constructed graph-structured data, modeling\ntime series PM2.5 data from a graph prospective may be a good\nway to maintain their temporal trajectory while exploring the\ntemporal dependencies among time series PM2.5 data.\nA graph is deﬁned asG /equals( Λ, Γ) where Λ represents its nodes,\nand Γ denotes its edges. The number of nodes in a graph is\ndenoted by n. The graph adjacency matrixU ∈ R\nn×n is used to\ncharacterize the relationships among nodes.\nAs shown inFigure 5, the momentti (i /equals 1, 2, ...n) from time\nseries PM2.5 data can be regarded as thei-th node in a graph, and\nthey are interconnected by using their hidden dependency\nrelationships. Therefore, all the nodes in a graph can be\ndeﬁned as\nΛ /equals (t\n1,t 2, /tn) (1)\nThe graph adjacency matrix is computed by a Hadamard\nproduct\nU /equals A ⊙ E (2)\nWhere A ∈ Rn×n denotes the initial multiplicative attention scores\ncalculated by A /equals XX′, and E ∈ Rn×n represents the ﬁrst-order\nbackward difference matrix, which is obtained by\nE /equals E′ · W (3)\nE′ /equals (∇10, ∇21, ∇32, ...,∇n,n−1) (4)\n∇i,i−1 /equals xi − xi−1,i /equals 1, 2, /,n (5)\nWhere W is the linear transformation of the originalﬁrst-order\nbackward difference matrixE′ ∈ Rn×1, and∇i,i−1 is theﬁrst-order\nbackward difference between two adjacent nodes in a graph\nrepresenting the meteorological dynamical changes between\ntwo adjacent moments. In this way, the edge values inΓ from\na graph correspond to the element values in the produced graph\nadjacency matrix U, as depicted inFigure 5A.\nIt’s worth pointing that there are two distinct properties about\nour constructed graph-structured data from original time series\nPM2.5 data without explicit graph structures. First, the attention\nscore A is used to weigh the similarity of different nodes (i.e., time\nmoments). The higher the similarity of different nodes is, the\nlarger the attention score values are. Moreover, the dynamical\nchanging information in time series PM2.5 data between two\nadjacent moments is very important for air quality prediction.\nTherefore, the Hadamard productA ⊙ E between the attention\nscore A and ﬁrst-order backward difference matrixE is designed\nto weigh the similarity of different nodes, and the dynamical\nchanging in time series PM2.5 data simultaneously. Second, it is\nknown that in long-term time series data the obtained\ninformation with close nodes (such as two neighboring nodes)\nis more important for air quality prediction than the obtained\ninformation with far nodes. Multiplying the attention scores of far\nFIGURE 5 | (A)an illustration of constructing graph-structured data from time series PM2.5 data at four different moments,(B) the graph adjacency matrix is\ncomputed by using a Hadamard productA ⊙ E between the attention scoresA and ﬁrst-order backward difference matrixE.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249866\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nnodes with the ﬁrst-order backward difference of neighboring\nnodes, thus makes the graph adjacency matrixU not only focuses\non the information of far nodes, but also pays more attention to\nthe neighboring nodes.\nAfter constructing graph-structured data, the self-attention\nmechanism in Transformer ( Vaswani et al., 2017 ) can be\noperated on these graph-structured data. Then, we improve\nthe self-attention mechanism with the temporal difference\ninformation. This yields our graph attention mechanism used\nin the proposed TDGTN model for learning long-term temporal\ndependencies from a graph prospective on air quality PM2.5\nprediction tasks.\n2.2.3 The Details of TDGTN Model\nSimilar to the original Transformer ( Vaswani et al., 2017 ),\nTDGTN contains encoder and decoder blocks with the\ndeveloped graph attention mechanism, as described inFigure 4.\nEncoder: The encoder is composed of a graph attention layer\nand a fully connected feed-forward network layer. Around each of\ntwo sub-layers, the residual connection ( He et al., 2016 )i s\nadopted, each of them is followed by an addition and layer-\nnormalization layer (Add and Norm). Given input time series\ndata X, the encoder aims to learn the interrelationship of PM2.5\nrelated data in time series data from a graph perspective.\nDecoder: The decoder comprises of a masked multi-head\nattention layer, a graph attention layer and a fully connected\nfeed-forward network layer, and each of them is followed by an\naddition and layer-normalization layer (Add and Norm). Similar\nto the encoder, the residual connection is employed around each\nof two sub-layers. The decoder accepts the input time series data\nX\nde /equals{ Xtoken,X 0}, in which Xtoken denotes the started tokens,\nand X0 represents the placeholder for target time series data. The\ndecoder aims to produce the output of predicted PM2.5\nconcentration data in a generative manner based on the\nobtained hidden continuous feature representations in the\nencoder.\nGraph attention: Based on the constructed graph, we improve\nthe self-attention mechanism with the temporal difference\ninformation and embed it into the produced graph-structured\ndata to calculate the hidden representations of each node in the\ngraph. As shown in (Vaswani et al., 2017), the canonical self-\nattention consists of three parts: query, key and value, and is\ncomputed by performing scaling dot product calculation:\nAttention(Q, K, V) /equals softmax(\nQKΤ\n/radicaltpext/radicaltpext\nd\n√ )V (6)\nWhere Q ∈ RL×d is the query matrix,K ∈ RL×d is the key matrix,\nV ∈ RL×d is the value matrix,L is the length of input data, andd is\nthe feature dimension of input data.\nAs mentioned-above, the ﬁrst-order backward difference\nbetween two adjacent nodes in a graph can be used to\nrepresent the meteorological dynamical changes between two\nadjacent moments. This difference information is useful for air\nquality prediction, and can be embedded into the canonical self-\nattention. In order to simultaneously capture the interrelation\nand dynamical changes among different nodes, we modify the\nattention calculation in Eqn. 6 by multiplying the ﬁrst-order\nbackward difference matrixE as follows:\nAttention /equals softmax((\nQ · KΤ\n/radicaltpext/radicaltpext\nd\n√ ) · E)V (7)\nFor graph-based time series PM2.5 data prediction, we\nemployed ﬁxed position encoding with the nonlinear sine and\ncosine functions (Vaswani et al., 2017) to provide the temporal\ninformation of time series data for graph attention calculation.\n3 EVALUATION CRITERIA\nTo verify the performance of air quality PM2.5 prediction\nmethods, three representative evaluation metrics, including\nmean absolute error (MAE), root mean square error (RMSE),\nand mean absolute percentage error (MAPE), were employed for\nexperiments. MAE, RMSE, and MAPE are deﬁned as:\nMAE(y, y\n∧\n) /equals 1\nm ∑\nm\ni/equals 1\n⏐⏐⏐⏐⏐⏐yi − yi\n∧ ⏐⏐⏐⏐⏐⏐ (8)\nRMSE(y, y\n∧\n) /equals\n/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext/radicaltpext\n1\nm ∑\nm\ni/equals 1\n(yi − yi\n∧\n)\n2\n√\n(9)\nMAPE(y, y\n∧\n) /equals 1\nm ∑\nm\ni/equals 1\n⏐⏐⏐\n⏐⏐⏐y\ni − yi\n∧ ⏐⏐⏐\n⏐⏐⏐\ny\ni\n(10)\nWhere y and y\n∧\nseparately denotes the ground-truth and\npredicted PM2.5 value, andm represents the whole number of\ntesting data. The smaller the values of MAE, RMSE, and MAPE\nare, the higher theﬁnal prediction results are. Since MAPE is very\nsensitive to outlier data, the obtained MAPE values are often\nhigher than MAE and RMSE. In this case, MAE, RMSE and\nMAPE are employed simultaneously to evaluate the performance\nof all used methods.\n3.1 Implementation Details\nWe implement all the experiments on a PC server with a GPU\nNVIDIA Quadro P6000 with 24G memory. The open source\nPytorch tools are leveraged to conduct all machine learning\nmodels for air quality prediction. For deep learning models,\nthe open source Tensorﬂow library is installed and conﬁgured.\nThe Adam optimizer is adopted, and the initial learning rate is set\nto le-4. The batch size is set to 32, the maximum epoch number is\n200, and the mean squared error loss function is employed.\nNormalization is conducted to be [0, 1] for air quality time\nseries data. The lookup size (window size), which is used to\nrepresent historical observations as input size of all machine\nlearning models, is 24 for its promising performance. We evaluate\nthe performance of our method in comparison with other\nrepresentative methods, such as the traditional ARMA and\nSVR, as well as the recently-emerged deep models like CNNs,\nLSTMs, original Transformer methods, as describe below.\nARMA is a traditional linear statistical method for time series\ndata prediction. Here, ARMA is just used for singe-step air quality\nprediction since it is limited multi-step air quality prediction\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249867\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nstrategy. For ARMA, there are two key parameters ARMA (p, q)\naffecting its performance, in which p is the order of the AR part\nand q is the order of the MA part. In this work, we seek the\noptimal p and q in a simple exhausting search way in the range of\n[1, 10] with an interval of 1 to produce the best performance for\nARMA. As a result, we separately employ ARMA (4, 1) on Beijing\ndataset and ARMA (1, 9) on Taizhou dataset for experiments due\nto its best performance. SVR is a kernel method on the basis of\nnon-linear statistical machine learning theories. We adopt the\nlinear kernel for SVR on air quality PM2.5 prediction tasks.\nCNNs are a well-known deep model originally processing two-\ndimension (2D) image data. Due to the used 1D time-series\nPM2.5 data, 1D-CNN is adopted in this work. The network\nconﬁguration for 1D-CNN is that it consists of 256 convolution\nkernels with a kernel width of 5 and a stride of 1. Then, a batch\nnormalization layer, max-pooling layer, recti ﬁed linear units\n(RLU) layer, a dropout (0.3) layer, and a fully-connected (FC)\nlayer are used after the convolution layers.\nLSTMs are a typical kind of recurrent architecture modeling\nlong-range dependencies of time series data. Bidirectional LSTM\n(BiLSTM) is employed for air quality prediction. BiLSTM\ncontains a forward LSTM and a backward LSTM. We\nleveraged a two-layer BiLSTM for air quality forecasting in\nthis work. Each layer of BiLSTM contains 256 hidden\nneurons, followed by a dropout (0.05) layer. For the original\nTransformer model (Vaswani et al., 2017) and our proposed\nTDGTN model, we leverage three encoders and two decoders for\ntheir promising performance on air quality PM2.5 prediction.\nMoreover, in these two Transformer-based models the number of\nmulti-head attention is 8, and the used single feed-forward\nnetwork has 2048 nodes.\nIn this work, we adopt a year-independent strategy for air\nquality forecasting experiments which is deﬁnitely close to the\nreal-world sceneries. More specially, the training, and testing sets\nare selected from different years. In detail, on the used Beijing\nPM2.5 dataset, theﬁrst four-year data (01/01/2010 to 12/31/2013)\nis selected as the training net, and the last year data (01/01/2014-\n12/31/2014) is adopted for testing. On the used Taizhou PM2.5\ndataset, the ﬁrst two-year data (01/01/2017 to 12/31/2018) is\nemployed for training, and the last year data (01/01/2019 to 12/\n31/2019) is adopted for testing. During the training of deep\nmodels, we randomly select 10% of the entire training set as\nthe validation set for model validation.\n3.2 Results and Analysis\nTo verify the performance of different air quality PM2.5\nprediction methods, we presented two types of experimental\nresults: single-step prediction for the next 1 h, and multi-step\nprediction for the next multiple hours.\n3.2.1 Single-Step Prediction Results\nFigure 6 provides performance comparisons of different air\nquality prediction methods on Beijing and Taizhou PM2.5\ndatasets for single-step PM2.5 prediction tasks when the\nforward-step prediction size is 1 for the next 1 h (h1). These\ncomparing methods contain ARMA, the linear SVR, CNN,\nLSTM, the original Transformer (abbreviated as Transformer),\nas well as our method. As shown inFigure 6, it can be seen that\nour method outperforms other used methods on Beijing and\nTaizhou PM2.5 datasets for single-step PM2.5 prediction tasks. In\ndetail, our method obtains the lowest RSME, MAE, and MAPE on\nthese two datasets. More specially, our method is able to reduce\nRMSE to 18.51 (ug/m\n3), MAE to 11.06 (ug/m3), and MAPE to\n22.91 (%) on Beijing PM2.5 dataset, whereas on Taizhou PM2.5\ndataset our method can reduce RMSE to 5.70 (ug/m\n3), MAE to\n3.66 (ug/m3), and MAPE to 20.23 (%). This indicates the\neffectiveness of our proposed method for air quality PM2.5\nprediction from a graph perspective. In comparison with other\nmethods like ARMA, SVR, CNN, LSTM, and Transformer, our\nmethod has stronger capability of capturing long-term\ndependency and complex relationships from time series PM2.5\ndata for air quality prediction. In addition, our method yields\nbetter performance than Transformer, showing the advantages of\nour method on the basis of graph attention.\nBesides, compared with traditional shallow learning\nmethods like ARMA and SVR, deep learning methods,\nincluding LSTM, Transformer and our method, produce\nbetter performance for air quality prediction. This\ndemonstrates the superiority of deep learning techniques\nover traditional shallow learning techniques on air quality\nprediction tasks. However, the used 1D-CNN obtains slight\nlower performance than SVR on single-step PM2.5 prediction\ntasks. This indicates that CNN may not very effective to learn\nlong-term dependency and complex relationships from 1D\ntime series PM2.5 data.\n3.2.2 Multi-Step Prediction Results\nFor multi-step prediction results, we provided performance\ncomparisons of different air quality prediction methods for the\nnext multiple hours (6, 12, 24, 48). For the next 6 h, the average\nprediction results in the next forward 6 h were reported as the\ntesting error of different methods. For more than the next 6 h, we\ndivided them into a number of adjacent intervals and trained\nindividual models corresponding to every interval. Then, we\nﬁgured out the average prediction results for every interval. In\nparticular, for the next 12 h prediction, we split it into three\nintervals: 0– 3h ,3– 6 h, and 6– 12 h. For the next 24 h prediction,\nwe split it into four intervals: 0– 3h ,3– 6h ,6– 12 h, and 12– 24 h.\nFIGURE 6 |Comparisons of different methods for single-step PM2.5\nprediction tasks for the next 1 h.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249868\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nFor the next 48 h prediction, we split it into four intervals: 0– 6h ,\n6– 12 h, 12– 24 h and 24– 48 h.\nFigure 7 presents the obtained results (RMSE, MAE and\nMAPE) of different methods for the next 6 h on Beijing and\nTaizhou PM2.5 datasets. It can be seen from the results in\nFigure 7, compared with other methods, our method achieves\nthe smaller RSME, MAE and MAPE on Beijing and Taizhou\nPM2.5 datasets. This indicates the superiority of the proposed\nmethod on long-term air quality prediction tasks. More specially,\nour method reduces RMSE to 36.27 (ug/m\n3), MAE to 22.61 (ug/\nm3), MAPE to 51.88 (%) on Beijing PM2.5 dataset, and RMSE to\n11.19 (ug/m3), MAE to 7.40 (ug/m3), and MAPE to 44.37 (%) on\nTaizhou PM2.5 dataset, respectively. The ranking order for other\nmethods is Transformer, LSTM, CNN, and SVR. Note that CNN\nprovides slightly smaller RMSE, MAE, and MAPE than SVR on\nmulti-step PM2.5 prediction tasks for the next 6 h. This is\nopposite to single-step PM2.5 prediction tasks for the next 1 h,\nas shown in Figure 6. This shows that CNN is capable of\npromoting the prediction performance with the increasing\nforward-step prediction size from the next 1 h to the next 6 h.\nThis ﬁnding of CNN will be veriﬁed further in the next 12, 24\nand 48 h.\nFigures 8, 9 separately show the prediction results (RMSE,\nMAE and MAPE) of different methods for the next 12 h (three\nintervals) on Beijing and Taizhou PM2.5 datasets.Figures 10, 11\nindividually depict the prediction results (RMSE, MAE and\nMAPE) of different methods for the next 24 h (four intervals)\non Beijing and Taizhou PM2.5 datasets. Figures 12 , 13\nindependently present the prediction results (RMSE, MAE and\nMAPE) of different methods for the next 48 h (four intervals) on\nBeijing and Taizhou PM2.5 datasets. From the results inFigures\n8– 13, we can observe that when the forward prediction size\nincreases from 12 to 48 h, the multi-step PM2.5 prediction\naccuracies of all used methods clearly drop down. This may be\nattributed to the fact that the larger the forward prediction size is,\nthe more dif ﬁcult and challenging the accurate air quality\nprediction task is. In addition, Figures 8– 13 show that our\nmethod still presents the lowest prediction error (RMSE,\nMAE, MAPE) among all used methods when the forward\nprediction size changes from 12 to 48 h. Besides, CNN\nFIGURE 7 |Comparisons of different methods for multi-step prediction\nresults for the next 6 h.\nFIGURE 8 |Comparisons of different methods for multi-step prediction results for the next 12 h on Beijing dataset.\nFIGURE 9 |Comparisons of different methods for multi-step prediction results for the next 12 h on Taizhou dataset.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 9249869\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nFIGURE 10 |Comparisons of different methods for multi-step prediction results for the next 24 h on Beijing dataset.\nFIGURE 11 |Comparisons of different methods for multi-step prediction results for the next 24 h on Taizhou dataset.\nFIGURE 12 |Comparisons of different methods for multi-step prediction results for the next 48 h on Beijing dataset.\nFIGURE 13 |Comparisons of different methods for multi-step prediction results for the next 48 h on Taizhou dataset.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 92498610\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nperforms better than SVR again for the next 12 – 48 h, and\noutperforms LSTM for the next 48 h. This shows that CNN is\nmore appropriate to implement long-term air quality prediction\ncompared with short-term air quality prediction tasks.\nTo intuitively exhibit the superiority of our method over the\noriginal Transformer method,Figures 14, 15 separately provide\nthe visualization of their single-step ground truth and predicted\nPM2.5 values for the next 1 h, and multi-step ground truth and\nFIGURE 14 |Comparisons of our method and Transformer on single-step (h1) and multi-step (h48) ground truth and air quality prediction tasks during 1 month (4/\n01/2014-4/30/2014) on Beijing dataset.\nFIGURE 15 |Comparisons of our method and Transformer on single-step (h1) and multi-step (h48) ground truth and air quality prediction tasks during 1 month (4/\n01/2019-4/30/2019) on Taizhou dataset.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 92498611\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\npredicted PM2.5 values for the next 48 h on Beijing and Taizhou\nPM2.5 datasets. The forward prediction size is 4/01/2014-4/30/\n2014 on Beijing PM2.5 dataset, and 4/01/2019-4/30/2019 on\nTaizhou PM2.5 dataset. Here, an illustration of their difference\nis labeled with a red circle inFigures 14, 15.\nAs shown inFigures 14, 15, we can observe that both of them\nobtain promising performance on single-step prediction tasks for\nthe next 1 h. Nevertheless, our method slightly outperforms\nTransformer on subtle changes in the time period of wave\nvalley and the wave peak of air quality PM2.5 testing data\nfrom these two datasets. Moreover, such superiority of our\nmethod over Transformer is more obvious for multi-step\nprediction results for the next 48 h. The visualization in\nFigures 14 , 15 show the advantages of our method over\nTransformer on short-term and long-term air quality PM2.5\nprediction tasks, again.\nCompared with the results obtained on single-step PM2.5\nprediction tasks, all used methods for multi-step PM2.5\nprediction achieves much larger RMSE, MAE and MAPE,\ndemonstrating the difﬁculty in long-term air quality prediction\nwhen adopting a year-independent strategy widely used in real-\nword sceneries. Specially, the obtained MAPE values are much\nhigher than RMSE, and MAE, due to the inherent drawback of\nMAPE as an error measure, that is, MAPE is very sensitive to\noutlier data (Kim and Kim, 2016). This is consistent with previous\nﬁndings (Wen et al., 2019; Du et al., 2021). Nevertheless, the\nobtained results on multi-step PM2.5 prediction tasks\ndemonstrate the advantage of the proposed TDGTN again,\noutperforming other methods.\n4 CONCLUSION AND FUTURE WORK\nIn this work, a new deep learning model called TDGTN is proposed\nto learn long-term temporal dependencies and complex\nrelationships from time series PM2.5 data for air quality PM2.5\nprediction. The proposed TDGTN model contains a number of\nencoder and decoder layers associated with the newly-developed\ngraph attention mechanism. Specially, the conventional self-\nattention mechanism in the original Transformer model is\nimproved by means of integrating the temporal difference\ninformation, which gives rise to a new graph attention\nmechanism used in the proposed TDGTN model. Based on the\nconstructed graph-structured data, we are theﬁrst to implement air\nquality PM2.5 prediction tasks for the single air monitoring station\nfrom a view point of graphs. Experiment results on Beijing and\nTaizhou PM2.5 datasets demonstrate the promising performance of\nthe proposed TDGTN method on both short-term and long-term air\nquality prediction tasks.\nIt is noted that for air quality prediction on different cities, all\nused machine learning methods should be trained according to the\ncollected data corresponding to this city. Otherwise, their obtained\nair quality prediction performance is usually poor due to the\ndistribution difference of collected data from different cities.\nThis is an inherent drawback for all machine learning\nalgorithms. To alleviate this problem, combining deep learning\nwith transfer learning techniques (Wang L. et al., 2019) may be a\npossible solution for cross-city air quality prediction. Moreover, it\nis also interesting to exploit a physics-based method (Ponomarev\net al., 2020) that is applicable over different locations or regions in\nfuture. Besides, this work focuses on air quality prediction on a\nsingle city (Beijing or Taizhou) rather than a special region with\nmultiple cities. In particular, we aim to integrate the advantages of\nGNNs and Transformer techniques and evaluate their\nperformance of air quality PM2.5 prediction from a single\nmonitoring station in two cities. Considering the fact that\nGNNs are able to capture the spatial dependencies among\nmultiple air quality monitoring stations, it is interesting to\nextend our model for a regional estimation of PM2.5 from\nmultiple cities throughout the world. Additionally, the proposed\nmethod was designed for a single monitoring station from a single\ncity, thereby failing to analyze the spatial variations. To address this\nissue, it is also meaningful for air quality prediction to incorporate\nsatellite-based air pollution data (Xu et al., 2019) from satellite\nmeasurements, which can map air pollution for a broad region\ninstead of a single city.\nDATA AVAILABILITY STATEMENT\nThe raw data supporting the conclusions of this article will be\nmade available by the authors, without undue reservation.\nAUTHOR CONTRIBUTIONS\nZZ Conceptualization, Methodology, Software, Writing-Original\ndraft. SZ: Conceptualization, Software. XZ: Project\nadministration, Writing- Reviewing and Editing. LC and JY:\nData collection, Pre-processing.\nFUNDING\nThis work was supported by Zhejiang Provincial National Science\nFoundation of China under Grant No. LY20E080013,\nLZ20F020002, and National Science Foundation of China\n(NSFC) under Grant No. 61976149.\nREFERENCES\nAbirami, S., and Chitra, P. (2021). Regional Air Quality Forecasting Using Spatiotemporal\nDeep Learning.J. Clean. Prod.283, 125341. doi:10.1016/j.jclepro.2020.125341\nAgarwal, S., Sharma, S., R., S., Rahman, M. H., Vranckx, S., Maiheu, B., et al. (2020).\nAir Quality Forecasting Using Artiﬁcial Neural Networks with Real Time\nDynamic Error Correction in Highly Polluted Regions. Sci. Total Environ.\n735, 139454. doi:10.1016/j.scitotenv.2020.139454\nAggarwal, A., and Toshniwal, D. (2021). A Hybrid Deep Learning Framework for\nUrban Air Quality Forecasting. J. Clean. Prod. 329, 129660. doi:10.1016/j.\njclepro.2021.129660\nArhami, M., Kamali, N., and Rajabi, M. M. (2013). Predicting Hourly Air Pollutant\nLevels Using Artiﬁcial Neural Networks Coupled with Uncertainty Analysis by\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 92498612\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nMonte Carlo Simulations.Environ. Sci. Pollut. Res.20 (7), 4777– 4789. doi:10.\n1007/s11356-012-1451-6\nBazi, Y., Bashmal, L., Rahhal, M. M. A., Dayil, R. A., and Ajlan, N. A. (2021). Vision\nTransformers for Remote Sensing Image Classiﬁcation. Remote Sens. 13 (3),\n516. doi:10.3390/rs13030516\nCekim, H. O. (2020). Forecasting PM10 Concentrations Using Time Series Models:\na Case of the Most Polluted Cities in Turkey.Environ. Sci. Pollut. Res.27 (20),\n25612– 25624. doi:10.1007/s11356-020-08164-x\nChang, Q., Zhang, H., and Zhao, Y. (2020). Ambient Air Pollution and Daily\nHospital Admissions for Respiratory System-Related Diseases in a Heavy\nPolluted City in Northeast China. Environ. Sci. Pollut. Res. Int. 27,\n10055– 10064. doi:10.1007/s11356-020-07678-8\nChang, Y.-S., Abimannan, S., Chiao, H.-T., Lin, C.-Y., and Huang, Y.-P. (2020). An\nEnsemble Learning Based Hybrid Model and Framework for Air Pollution\nForecasting. Environ. Sci. Pollut. Res. 27 (30), 38155– 38168. doi:10.1007/\ns11356-020-09855-1\nChen, L., Xu, J., Wu, B., Qian, Y., Du, Z., Li, Y., et al. (2021). Group-Aware Graph\nNeural Network for Nationwide City Air Quality Forecasting. Available at:\nhttps://arxiv.org/abs/2108.12238.\nChu, J., Dong, Y., Han, X., Xie, J., Xu, X., and Xie, G. (2021). Short-term Prediction\nof Urban PM2.5 Based on a Hybrid Modiﬁed Variational Mode Decomposition\nand Support Vector Regression Model.Environ. Sci. Pollut. Res.28 (1), 56– 72.\ndoi:10.1007/s11356-020-11065-8\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical Evaluation of\nGated Recurrent Neural Networks on Sequence Modeling. Available at: https://\narxiv.org/abs/1412.3555.\nDarçın, M. (2014). Association between Air Quality and Quality of Life.Environ.\nSci. Pollut. Res.21 (3), 1954– 1959. doi:10.1007/s11356-013-2101-3\nDe Melo, W. C., Granger, E., and Hadid, A. (2019).“Depression Detection Based\non Deep Distribution Learning,” in 2019 IEEE International Conference on\nImage Processing (ICIP), Taipei, Taiwan, 22-25 September 2019, 4544– 4548.\ndoi:10.1109/ICIP.2019.8803467\nDonnelly, A., Misstear, B., and Broderick, B. (2015). Real Time Air Quality\nForecasting Using Integrated Parametric and Non-parametric Regression\nTechniques. Atmos. Environ. 103, 53– 65. doi:10.1016/j.atmosenv.2014.12.011\nDu, S., Li, T., Yang, Y., and Horng, S.-J. (2021). Deep Air Quality Forecasting Using\nHybrid Deep Learning Framework. IEEE Trans. Knowl. Data Eng. 33 (6),\n2412– 2424. doi:10.1109/tkde.2019.2954510\nDuke, B., Ahmed, A., Wolf, C., Aarabi, P., and Taylor, G. W. (2021).“Sstvos: Sparse\nSpatiotemporal Transformers for Video Object Segmentation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\nNashville, TN, USA, 20-25 June 2021, 5912– 5921. doi:10.1109/CVPR46437.\n2021.00585\nElman, J. L. (1990). Finding Structure in Time.Cognitive Sci. 14 (2), 179– 211.\ndoi:10.1207/s15516709cog1402_1\nFeng, X., Li, Q., Zhu, Y., Hou, J., Jin, L., and Wang, J. (2015). Artiﬁcial Neural\nNetworks Forecasting of PM2.5 Pollution Using Air Mass Trajectory Based\nGeographic Model and Wavelet Transformation. Atmos. Environ.\n107,\n118– 128. doi:10.1016/j.atmosenv.2015.02.030\nGao, X., and Li, W. (2021). A Graph-Based LSTM Model for PM2.5\nForecasting. Atmos. Pollut. Res. 12 (9), 101150. doi:10.1016/j.apr.2021.\n101150\nGariazzo, C., Carlino, G., Silibello, C., Renzi, M., Finardi, S., Pepe, N., et al.\n(2020). A Multi-City Air Pollution Population Exposure Study: Combined\nUse of Chemical-Transport and Random-Forest Models with Dynamic\nPopulation Data. Sci. Total Environ. 724, 138102. doi:10.1016/j.scitotenv.\n2020.138102\nGeng, G., Zhang, Q., Martin, R. V., van Donkelaar, A., Huo, H., Che, H., et al.\n(2015). Estimating Long-Term PM2.5 Concentrations in China Using Satellite-\nBased Aerosol Optical Depth and a Chemical Transport Model.Remote Sens.\nEnviron. 166, 262– 270. doi:10.1016/j.rse.2015.05.016\nGraupe, D., Krause, D., and Moore, J. (1975). Identiﬁcation of Autoregressive\nMoving-Average Parameters of Time Series.IEEE Trans. Autom. Contr.20 (1),\n104– 107. doi:10.1109/tac.1975.1100855\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).“Deep Residual Learning for Image\nRecognition,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, Las Vegas, NV, USA, 27-30 June 2016, 770– 778. doi:10.\n1109/CVPR.2016.90\nHinton, G. E., and Salakhutdinov, R. R. (2006). Reducing the Dimensionality of\nData with Neural Networks.science 313 (5786), 504– 507. doi:10.1126/science.\n1127647\nHochreiter, S., and Schmidhuber, J. (1997). Long Short-Term Memory.Neural\nComput. 9 (8), 1735– 1780. doi:10.1162/neco.1997.9.8.1735\nJian, L., Zhao, Y., Zhu, Y. P., Zhang, M. B., and Bertolatti, D. (2012). An Application\nof ARIMA Model to Predict Submicron Particle Concentrations from\nMeteorological Factors at a Busy Roadside in Hangzhou, China.Sci. Total\nEnviron. 426, 336– 345. doi:10.1016/j.scitotenv.2012.03.025\nJin, N., Zeng, Y., Yan, K., and Ji, Z. (2021). Multivariate Air Quality Forecasting\nwith Nested Long Short Term Memory Neural Network.IEEE Trans. Ind. Inf.\n17 (12), 8514– 8522. doi:10.1109/tii.2021.3065425\nKe, J., Zhang, J., and Tang, M. (2021). Does City Air Pollution Affect the Attitudes\nof Working Residents on Work, Government, and the City? an Examination of\na Multi-Level Model with Subjective Well-Being as a Mediator.J. Clean. Prod.\n295, 126250. doi:10.1016/j.jclepro.2021.126250\nKim, S., and Kim, H. (2016). A New Metric of Absolute Percentage Error for\nIntermittent Demand Forecasts.Int. J. Forecast.32 (3), 669– 679. doi:10.1016/j.\nijforecast.2015.12.003\nKipf, T. N., and Welling, M. (2016). Semi-supervised Classiﬁcation with Graph\nConvolutional Networks. Available at: https://arxiv.org/abs/1609.02907.\nLanchantin, J., Wang, T., Ordonez, V., and Qi, Y. (2021).“General Multi-Label\nImage Classiﬁcation with Transformers,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, Nashville, TN, USA,\n20-25 June 2021, 16478– 16488. doi:10.1109/cvpr46437.2021.01621\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep Learning.Nature 521 (7553),\n436– 444. doi:10.1038/nature14539\nLee, H.-M., Park, R. J., Henze, D. K., Lee, S., Shim, C., Shin, H.-J., et al. (2017).\nPM2.5 Source Attribution for Seoul in May from 2009 to 2013 Using GEOS-\nChem and its Adjoint Model. Environ. Pollut. 221, 377– 384. doi:10.1016/j.\nenvpol.2016.11.088\nLiang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., et al. (2015). Assessing\nBeijing’s PM 2.5 Pollution: Severity, Weather Impact, APEC and Winter\nHeating. Proc. R. Soc. A471 (2182), 20150257. doi:10.1098/rspa.2015.0257\nLiao, Q., Zhu, M., Wu, L., Pan, X., Tang, X., and Wang, Z. (2020). Deep Learning\nfor Air Quality Forecasts: a Review.Curr. Pollut. Rep.6 (4), 399– 409. doi:10.\n1007/s40726-020-00159-z\nLiu, H., Yan, G., Duan, Z., and Chen, C. (2021). Intelligent Modeling Strategies for\nForecasting Air Quality Time Series: A Review.Appl. Soft Comput.102, 106957.\ndoi:10.1016/j.asoc.2020.106957\nMihailovic, D. T., Alapaty, K., and Podrascanin, Z. (2009). Chemical Transport\nModels. Environ. Sci. Pollut. Res. 16 (2), 144– 151. doi:10.1007/s11356-008-\n0086-0\nNeishi, M., and Yoshinaga, N. (2019). “On the Relation between Position\nInformation and Sentence Length in Neural Machine Translation, ” in\nProceedings of the 23rd Conference on Computational Natural Language\nLearning (CoNLL), Hong Kong, China, November 3 – 4, 2019, 328 – 338.\ndoi:10.18653/v1/k19-1031\nNiu, Z., Zhong, G., and Yu, H. (2021). A Review on the Attention Mechanism of\nDeep Learning. Neurocomputing 452, 48 – 62. doi:10.1016/j.neucom.2021.\n03.091\nPak, U., Ma, J., Ryu, U., Ryom, K., Juhyok, U., Pak, K., et al. (2020). Deep Learning-\nBased PM2.5 Prediction Considering the Spatiotemporal Correlations: A Case\nStudy of Beijing, China.Sci. Total Environ.699, 133561. doi:10.1016/j.scitotenv.\n2019.07.367\nPonomarev, N. A., Elansky, N. F., Kirsanov, A. A., Postylyakov, O. V., Borovski, A.\nN., and Verevkin, Y. M. (2020). Application of Atmospheric Chemical\nTransport Models to Validation of Pollutant Emissions in Moscow.Atmos.\nOcean. Opt. 33 (4), 362– 371. doi:10.1134/s1024856020040090\nPouyanfar, S., Sadiq, S., Yan, Y., Tian, H., Tao, Y., Reyes, M. P., et al. (2018). A\nSurvey on Deep Learning: Algorithms, Techniques, and Applications.ACM\nComput. Surv. (CSUR)51 (5), 1– 36. doi:10.1145/3234150\nPowers, J. G., Klemp, J. B., Skamarock, W. C., Davis, C. A., Dudhia, J., Gill, D. O.,\net al. (2017). The Weather Research and Forecasting Model: Overview, System\nEfforts, and Future Directions.Bull. Am. Meteorological Soc.98 (8), 1717– 1737.\ndoi:10.1175/bams-d-15-00308.1\nRagab, M. G., Abdulkadir, S. J., Aziz, N., Al-Tashi, Q., Alyousiﬁ, Y., Alhussian, H.,\net al. (2020). A Novel One-Dimensional CNN with Exponential Adaptive\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 92498613\nZhang et al. TDGTN For Air Quality PM2.5 Prediction\nGradients for Air Pollution Index Prediction.Sustainability 12 (23), 10090.\ndoi:10.3390/su122310090\nSaini, T., Chaturvedi, P., and Dutt, V. (2021). Modelling Particulate Matter Using\nMultivariate and Multistep Recurrent Neural Networks.Front. Environ. Sci.\n614. doi:10.3389/fenvs.2021.752318\nScarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2008).\nThe Graph Neural Network Model.IEEE Trans. Neural Netw.20 (1), 61– 80.\ndoi:10.1109/TNN.2008.2005605\nSchwartz, J. (1993). Particulate Air Pollution and Chronic Respiratory Disease.\nEnviron. Res. 62 (1), 7– 13. doi:10.1006/enrs.1993.1083\nSeng, D., Zhang, Q., Zhang, X., Chen, G., and Chen, X. (2021). Spatiotemporal\nPrediction of Air Quality Based on LSTM Neural Network.Alexandria Eng. J.\n60 (2). doi:10.1016/j.aej.2020.12.009\nSuleiman, A., Tight, M. R., and Quinn, A. D. (2019). Applying Machine Learning Methods\nin Managing Urban Concentrations of Trafﬁc-Related Particulate Matter (PM10 and\nPM2.5).Atmos. Pollut. Res.10 (1), 134– 144. doi:10.1016/j.apr.2018.07.001\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention Is All You Need,” in Advances in Neural Information\nProcessing Systems, 5998– 6008.\nWang, J., Bai, L., Wang, S., and Wang, C. (2019). Research and Application of the\nHybrid Forecasting Model Based on Secondary Denoising and Multi-Objective\nOptimization for Air Pollution Early Warning System.J. Clean. Prod. 234,\n54– 70. doi:10.1016/j.jclepro.2019.06.201\nWang, L., Geng, X., Ma, X., Liu, F., and Yang, Q. (2019).“Cross-city Transfer\nLearning for Deep Spatio-Temporal Prediction,” in Proceedings of the 28th\nInternational Joint Conference on Artiﬁcial Intelligence (IJCAI), Macao, China,\nAugust 10– 16, 2019, 1893– 1899. doi:10.24963/ijcai.2019/262\nWang, W., Zhao, S., Jiao, L., Taylor, M., Zhang, B., Xu, G., et al. (2019). Estimation\nof PM2.5 Concentrations in China Using a Spatial Back Propagation Neural\nNetwork. Sci. Rep. 9 (1), 13788– 13810. doi:10.1038/s41598-019-50177-1\nWen, C., Liu, S., Yao, X., Peng, L., Li, X., Hu, Y., et al. (2019). A Novel Spatiotemporal\nConvolutional Long Short-Term Neural Network for Air Pollution Prediction.\nSci. total Environ.654, 1091– 1099. doi:10.1016/j.scitotenv.2018.11.086\nX i a o ,Q . ,C h a n g ,H .H . ,G e n g ,G . ,a n dL i u ,Y .( 2 0 1 8 ) .A nE n s e m b l eM a c h i n e - L e a r n i n g\nModel to Predict Historical PM2.5 Concentrations in China from Satellite Data.\nEnviron. Sci. Technol.52 (22), 13260– 13269. doi:10.1021/acs.est.8b02917\nXu, H., Bechle, M. J., Wang, M., Szpiro, A. A., Vedal, S., Bai, Y., et al. (2019).\nNational PM2.5 and NO2 Exposure Models for China Based on Land Use\nRegression, Satellite Measurements, and Universal Kriging.Sci. Total Environ.\n655, 423– 433. doi:10.1016/j.scitotenv.2018.11.125\nXu, J., Chen, L., Lv, M., Zhan, C., Chen, S., and Chang, J. (2021). HighAir: A\nHierarchical Graph Neural Network-Based Air Quality Forecasting Method.\nAvailable at: https://arxiv.org/abs/2101.04264.\nYan, X., Zang, Z., Luo, N., Jiang, Y., and Li, Z. (2020). New Interpretable Deep\nLearning Model to Monitor Real-Time PM2.5 Concentrations from Satellite\nData. Environ. Int. 144, 106060. doi:10.1016/j.envint.2020.106060\nYang, W., Deng, M., Xu, F., and Wang, H. (2018). Prediction of Hourly PM2.5\nUsing a Space-Time Support Vector Regression Model.Atmos. Environ. 181,\n12– 19. doi:10.1016/j.atmosenv.2018.03.015\nYue, Z., Witzig, C. R., Jorde, D., and Jacobsen, H.-A. (2020).“BERT4NILM: A\nBidirectional Transformer Model for Non-intrusive Load Monitoring, ” in\nProceedings of the 5th International Workshop on Non-Intrusive Load\nMonitoring, Virtual conference, November 18, 2020, 89– 93.\nZaini, N. a., Ean, L. W., Ahmed, A. N., and Malek, M. A. (2021). A Systematic\nLiterature Review of Deep Learning Neural Network for Time Series Air Quality\nForecasting. Environ. Sci. Pollut. Res.29, 4958– 4990. doi:10.1007/s11356-021-\n17442-1\nZhang, B., Wu, B., and Liu, J. (2020). PM2.5 Pollution-Related Health Effects and\nWillingness to Pay for Improved Air Quality: Evidence from China ’s\nPrefecture-Level Cities. J. Clean. Prod. 273, 122876. doi:10.1016/j.jclepro.\n2020.122876\nZhang, H., Chen, G., Hu, J., Chen, S.-H., Wiedinmyer, C., Kleeman, M., et al.\n(2014). Evaluation of a Seven-Year Air Quality Simulation Using the Weather\nResearch and Forecasting (WRF)/Community Multiscale Air Quality (CMAQ)\nModels in the Eastern United States.Sci. Total Environ. 473-474, 275– 285.\ndoi:10.1016/j.scitotenv.2013.11.121\nZhang, K., Thé, J., Xie, G., and Yu, H. (2020). Multi-step Ahead Forecasting of\nRegional Air Quality Using Spatial-Temporal Deep Neural Networks: A Case\nStudy of Huaihai Economic Zone.J. Clean. Prod.277, 123231. doi:10.1016/j.\njclepro.2020.123231\nZhang, Z., Zeng, Y., and Yan, K. (2021). A Hybrid Deep Learning Technology for\nPM2.5 Air Quality Forecasting.Environ. Sci. Pollut. Res.28 (29), 39409– 39422.\ndoi:10.1007/s11356-021-12657-8\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., et al. (2021).“Informer:\nBeyond Efﬁcient Transformer for Long Sequence Time-Series Forecasting,” in\nProceedings of AAAI, Virtual conference, February 2– 9, 2021, 11106– 11115.\nZhou, Q., Jiang, H., Wang, J., and Zhou, J. (2014). A Hybrid Model for PM 2.5\nForecasting Based on Ensemble Empirical Mode Decomposition and a General\nRegression Neural Network. Sci. Total Environ. 496, 264– 274. doi:10.1016/j.\nscitotenv.2014.07.051\nConﬂict of Interest:The authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be construed as a\npotential conﬂict of interest.\nPublisher’s Note:All claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their afﬁliated organizations, or those of\nthe publisher, the editors and the reviewers. Any product that may be evaluated in\nthis article, or claim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nCopyright © 2022 Zhang, Zhang, Zhao, Chen and Yao. This is an open-access article\ndistributed under the terms of the Creative Commons Attribution License (CC BY).\nThe use, distribution or reproduction in other forums is permitted, provided the\noriginal author(s) and the copyright owner(s) are credited and that the original\npublication in this journal is cited, in accordance with accepted academic practice.\nNo use, distribution or reproduction is permitted which does not comply with\nthese terms.\nFrontiers in Environmental Science | www.frontiersin.org June 2022 | Volume 10 | Article 92498614\nZhang et al. TDGTN For Air Quality PM2.5 Prediction",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6252720952033997
    },
    {
      "name": "Graph",
      "score": 0.6011271476745605
    },
    {
      "name": "Data mining",
      "score": 0.5160036087036133
    },
    {
      "name": "Attention network",
      "score": 0.47526830434799194
    },
    {
      "name": "Air quality index",
      "score": 0.4730360209941864
    },
    {
      "name": "Encoder",
      "score": 0.438035249710083
    },
    {
      "name": "Time series",
      "score": 0.4323081970214844
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38861191272735596
    },
    {
      "name": "Machine learning",
      "score": 0.27358898520469666
    },
    {
      "name": "Theoretical computer science",
      "score": 0.24446004629135132
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}