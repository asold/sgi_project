{
  "title": "Cross-Utterance Language Models with Acoustic Error Sampling",
  "url": "https://openalex.org/W3082335021",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5077772423",
      "name": "Guangzhi Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100460206",
      "name": "Chao Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002191410",
      "name": "Philip C. Woodland",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2889152503",
    "https://openalex.org/W2901265786",
    "https://openalex.org/W2026149468",
    "https://openalex.org/W3008623409",
    "https://openalex.org/W2545177271",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2207587218",
    "https://openalex.org/W2533523411",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W2888867175",
    "https://openalex.org/W3096471021",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W648786980",
    "https://openalex.org/W2986102593",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W2949640357",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2963386218",
    "https://openalex.org/W3007776460",
    "https://openalex.org/W2005708641",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2886180730",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2964864831",
    "https://openalex.org/W2937649809",
    "https://openalex.org/W2963362078",
    "https://openalex.org/W2748679025"
  ],
  "abstract": "The effective exploitation of richer contextual information in language models (LMs) is a long-standing research problem for automatic speech recognition (ASR). A cross-utterance LM (CULM) is proposed in this paper, which augments the input to a standard long short-term memory (LSTM) LM with a context vector derived from past and future utterances using an extraction network. The extraction network uses another LSTM to encode surrounding utterances into vectors which are integrated into a context vector using either a projection of LSTM final hidden states, or a multi-head self-attentive layer. In addition, an acoustic error sampling technique is proposed to reduce the mismatch between training and test-time. This is achieved by considering possible ASR errors into the model training procedure, and can therefore improve the word error rate (WER). Experiments performed on both AMI and Switchboard datasets show that CULMs outperform the LSTM LM baseline WER. In particular, the CULM with a self-attentive layer-based extraction network and acoustic error sampling achieves 0.6% absolute WER reduction on AMI, 0.3% WER reduction on the Switchboard part and 0.9% WER reduction on the Callhome part of Eval2000 test set over the respective baselines.",
  "full_text": "Cross-Utterance Language Models with Acoustic Error Sampling\nG. Sun, C. Zhang and P . C. Woodland\nCambridge University Engineering Dept., Trumpington St., Cambridge, CB2 1PZ U.K\n{gs534, cz277, pcw}@cam.ac.uk\nAbstract\nThe effective exploitation of richer contextual information\nin language models (LMs) is a long-standing research problem\nfor automatic speech recognition (ASR). A cross-utterance LM\n(CULM) is proposed in this paper, which augments the input\nto a standard long short-term memory (LSTM) LM with a con-\ntext vector derived from past and future utterances using an ex-\ntraction network. The extraction network uses another LSTM\nto encode surrounding utterances into vectors which are inte-\ngrated into a context vector using either a projection of LSTM\nﬁnal hidden states, or a multi-head self-attentive layer. In addi-\ntion, an acoustic error sampling technique is proposed to reduce\nthe mismatch between training and test-time. This is achieved\nby considering possible ASR errors into the model training pro-\ncedure, and can therefore improve the word error rate (WER).\nExperiments performed on both AMI and Switchboard datasets\nshow that CULMs outperform the LSTM LM baseline WER.\nIn particular, the CULM with a self-attentive layer-based ex-\ntraction network and acoustic error sampling achieves 0.6% ab-\nsolute WER reduction on AMI, 0.3% WER reduction on the\nSwitchboard part and 0.9% WER reduction on the Callhome\npart of Eval2000 test set over the respective baselines.\nIndex Terms: cross-utterance, language models, speech recog-\nnition, context information\n1. Introduction\nA language model (LM) estimates the probability of a word\nsequence which is often decomposed into a product of condi-\ntional word prediction probabilities using the chain rule. LMs\nare widely used in many machine learning tasks, such as natu-\nral language understanding, machine translation and automatic\nspeech recognition (ASR). In ASR, high-performance LMs are\nfound to be critical to achieve good performance for both tra-\nditional source-channel model-based systems [1, 2] and recent\nend-to-end systems [3, 4, 5]. While traditional n-gram LMs\ncan only provide the contextual information from n preceding\nwords [14], recurrent neural network (RNN) LMs [6, 7], partic-\nularly long short-term memory (LSTM) LMs [8, 15], can pro-\nvide richer contextual information from the entire utterance to\nachieve better ASR performance [9, 10]. More recently, contex-\ntual information from past and future utterances has been taken\ninto account in language modelling [11, 12, 13].\nAlthough effective incorporation of cross-utterance contex-\ntual information remains an open research problem, improve-\nments have been found in ASR performance using such infor-\nmation. Early work tried to incorporate a short-term cache [17]\nor document-level semantic information [18] into LMs. With\nthe advent of RNN LMs, model adaptation using statistical anal-\nysis to represent global topics [16, 19] also showed promising\nresults. More recently, neural networks, such as hierarchical\nThanks to Yu Wang for his help with the Kaldi acoustic model. G.\nSun is funded by Cambridge Trust.\nRNNs and pre-trained BERT LMs [25], were used to encode\ncontextual information into vector representations for LM adap-\ntation [21, 22, 23, 24]. To use the history information from\nthe previous utterances at test-time, an RNN LM was trained\nwith conversational data without resetting its recurrent state at\nthe beginning of each new utterance [20]. Moreover, the his-\ntory representation can be extracted with an attention model and\nconcatenated with the hidden states [26, 36].\nThis paper proposes an LSTM-based cross-utterance LM\n(CULM) structure to explore the ﬂexible and effective use of\ncontext information embedded in the past and future utterances.\nThe CULM comprises of a main LSTM and a context extraction\nnetwork [19]. The main LSTM takes a context vector as an aux-\niliary input on top of a standard LSTM LM structure. The con-\ntext vector encapsulating information from surrounding utter-\nances is generated from the extraction network which contains\nan encoder LSTM and a fusion component. To extract the con-\ntext vector, the surrounding utterances are ﬁrst split into short\nsegments and encoded into vectors using the encoder LSTM.\nThe fusion component is then used to transform these vectors\ninto the context vector. Two extraction methods, the ﬁnal-state\nencoding and the self-attentive encoding, have been investi-\ngated. In ﬁnal-state encoding, a fully-connected (FC) layer is\nused to transform the concatenation of the encoder LSTM ﬁ-\nnal output states corresponding to each segment into the con-\ntext vector. Alternatively, the self-attentive encoding employs a\nself-attentive layer [30] to fuse the entire encoder LSTM output\nsequence instead of only the ﬁnal state for each segment before\nsending it to the FC layer. The main LSTM and the context ex-\ntractor are jointly optimised so that the extractor learns useful\ninformation for predicting the next word.\nSince in real ASR applications, only decoding hypothe-\nses of the surrounding utterances are available at test-time, the\nCULM trained on the reference text of surrounding utterances\nhas a mismatch between training and test [34, 35]. Therefore, an\nacoustic error sampling method is proposed to introduce simu-\nlated ASR errors into surrounding utterances in training. Exper-\niments were performed on both augmented multi-party interac-\ntion (AMI) and Switchboard corpora, and results are measured\nin both (pseudo) perplexity (PPL) and word error rate (WER).\nThe remainder of this paper is as follows: Sec. 2 describes\nthe CULM structure, followed by the introduction of the two\ncontext extraction networks. Acoustic error sampling method is\nexplained in Sec. 3. Experiments and results are discussed in\nSec. 4 followed by conclusions in Sec. 5.\n2. CULM structure\nThe model structure is shown in Fig. 1 which includes the con-\ntext vector as an auxiliary input to the main LSTM [32]. When\nprocessing the j-th utterance, a context vector is extracted from\nthe surrounding utterances excluding the current one. The in-\nput to the main LSTM is the concatenation of the context vec-\ntor, the word embedding and hidden states. The output is a\narXiv:2009.01008v1  [cs.CL]  19 Aug 2020\nprobability distribution over the vocabulary for the i-th word\ngiven previous words in the utterance and the context of the ut-\nterance, PLSTM = P(wj\ni |wj\n1:i−1, cj). LSTM hidden states are\nre-initialised at the beginning of each utterance to avoid infor-\nmation leaking from the content of previous utterances to the\ncurrent utterance when future information is used. The model\ntogether with the context vector extraction network is jointly\ntrained using the cross-entropy criterion.\nLSTM \nLayerscontext vector \nPLSTM\nword embedding \nhidden states \nc j\nw i \u0000 1\nh i \u0000 1\nh i \u0000 2\nFigure 1: CULM structure predicting the i-th word in the j-th\nutterance. Hidden state hi−1 is carried to the next prediction.\nPLSTM is the model output probability distribution for the next\nword. w and c are used to represent word embeddings and\ncontext vectors, while w and c are used for words and contexts.\n2.1. Extracting information from surrounding utterances\n2.1.1. Final-state encoding\nThe ﬁnal-state encoding extraction method is shown in Fig. 2\nwhere the segments from the context are directly encoded by\nthe ﬁnal hidden state of the encoder LSTM.\ncontext vector \ncj\nw j \u0000 3\n0 w j \u0000 3\n1 w j \u0000 3\nn\nw j +3\n0 w j +3\n1\nu j +3\nu j \u0000 3\nFC layer\nw j +3\nn\nFigure 2: Context vector extraction using the encoder LSTM\nﬁnal hidden state. Three past and future segments relative to\nthe current utterancej are shown here for illustration. FC is the\nfully-connected layer and u’s are LSTM ﬁnal state encodings.\nWhen training the LSTM, the output gate learns to fetch\nuseful information for speciﬁc tasks from the cell state which is\nthe memory in the network. The ﬁnal-state encoding method\ntakes advantage of this design and encourages the encoder\nLSTM ﬁnal hidden state to extract useful context information.\nFirst, the context is arranged by concatenating the past and fu-\nture utterances separately, excluding the current one. In con-\ntrast to [37], where the utterance structure is retained, segments\nare obtained by splitting the context into sequences with equal\nnumber of words ignoring utterance boundaries. This facilitates\nparallel processing when the extraction network is jointly opti-\nmised with the LM network. Then, each segment is processed\nby an LSTM and the ﬁnal hidden state, u, is used as the seg-\nment encoding. Finally, segment encodings are concatenated\nand projected through a fully-connected layer to form the con-\ntext vector cj in Eqn (1).\ncj = ReLU(WT [uj−l, ...,uj−1, uj+1, ...,uj+k]). (1)\n2.1.2. Self-attentive encoding\nThe ﬁnal hidden state has a limited ability to capture the content\nof the entire segment and may easily have a bias towards the\nending words. Therefore, a self-attentive layer is introduced\nto enrich the representation and explicitly indicate the relative\nimportance among context words, as illustrated in Fig. 3.\ncontext vector cj\nSelf-attentive Layer\nw j \u0000 1\nm \u0000 1 w j \u0000 1\n0 w j +1\n0 w j +1\n1\nu j +1\nu j \u0000 1\nFC layer\nw j +1\nnw j \u0000 1\nn\nFigure 3: Context vector extraction using multi-head self-\nattentive segment encodings. One past segment and one future\nsegment, each with n words, are shown.\nAs before, the context is arranged into segments which\nare sent to the encoder LSTM. The entire sequence of hidden\nstates from the encoder LSTM is then combined by a multi-\nhead self-attentive layer [30, 31]. The self-attentive layer com-\nbines the sequence of output hidden states using a weighted av-\nerage where the attention combination weights are generated\nthrough a two-layer feed-forward neural network from the hid-\nden state sequence itself. The relative importance of each word\nin the context is directly reﬂected by the attention combination\nweight it is assigned. This network extracts the past and future\nsegment encodings separately, and fuses them together with a\nfully-connected layer whose output is the context vector. This\nprocess is shown in Eqn (2).\nuj−1 = SelfAtten[LSTM(wj−1\nn , ...,wj−1\n0 )],\nuj+1 = SelfAtten[LSTM(wj+1\n0 , ...,wj+1\nn )],\ncj = FC(uj−1, uj+1), (2)\nwhere u’s represent the self-attentive segment encodings.\nLSTM(·) refers to the encoder LSTM. SelfAtten(·) refers to the\nself-attentive layer in [30] with a multi-head output to generate\nthe context vector cj. FC(·) refers to the fully-connected layer.\n3. Acoustic error sampling for context\nIn order to reduce the difference between LM training and de-\ncoding, insertion, deletion and substitution errors are added to\nthe context before the extraction, which is referred to as acous-\ntic error sampling. This process occurs at the beginning of each\ntraining epoch. Each word in the training set will be either\ndeleted, substituted, or inserted with a following word accord-\ning to an error distribution as shown in Eqn. (3).\nˆcj ∼P(cj|O), (3)\nwhere cj is the context of utterance j and O is the acoustic\nobservation of the context. Two ways of approximating the er-\nror distribution are proposed. The ﬁrst one manually sets the\nprobability of occurrence for each error type, and adopts a uni-\nform distribution across all words for substitutions and inser-\ntions. The second one samples according to the error analysis\nof the training set after ﬁrst-pass decoding. For instance, in the\nﬁrst method, each word will be deleted, substituted, or inserted\nwith any other word, with probabilities 0.10, 0.08 and 0.04. In\nthe second method, if the word ”they” in the training set has\n1000 occurrences where 30 of them are deleted and 50 are sub-\nstituted with ”he”, the sampling distribution will be 0.03 for\ndeletion and 0.05 for substitution with ”he”. As before, it also\nhas a probability of 0.04 to be inserted with a word according\nto the table of insertion frequency counts.\n4. Experiments\n4.1. Experimental setup\n4.1.1. Data\nThe proposed techniques were evaluated by performing exper-\niments on two tasks: AMI [33] and the Switchboard (SWBD).\nAMI contains 100 hours of group meetings recorded at different\nsites while SWBD contains conversations between two people\non deﬁned topics. Data size and partition used in LM training\nis shown in Table. 1. The LM train set used for the SWBD task\ncombines data from the SWBD and Fisher corpora which also\ncontains conversations between two people. 10% of conversa-\ntions were randomly selected to form the held-out validation set.\nEval2000 test set, which contains conversations from Callhome\n(CH) and SWBD, is used to evaluate the SWBD system.\nData Train Validation Test Vocabulary\nAMI 911K 108K 103K 13K\nSWBD 24M 3M 0.05M 30K\nTable 1: Number of words in each split and the vocabulary.\n4.1.2. Evaluation\nThe performance of the proposed CULMs are measured by\npseudo-PPL and WER. The pseudo-PPL is deﬁned in Eqn. 4.\nlog2(pseudo-PPL) =−1\nN\nM∑\nj=1\nNj∑\ni=1\nlog P(wj\ni |wj\n1:i−1, cj), (4)\nwhere N = MNj is the total number of words in the test\nset which contains M utterances and the j-th utterance has\nNj words. −log P(wj\ni |wj\n1:i−1, cj) is the cross-entropy loss for\neach word prediction. It is different to standard PPL in that\nthe context contains future words. However, when hidden state\nre-initialisation is applied at utterance boundaries and the con-\ntext excludes the current utterance, the pseudo-PPL reﬂects the\nmodel performance of individual utterances in the test set.\n4.2. AMI Experiments\nThe ﬁrst set of experiments were performed on AMI. Single-\nlayer LSTMs implemented in PyTorch [40] with 768d hid-\nden states and 256d word embeddings were used, and were\ntrained on in-domain data only, using stochastic gradient de-\nscent (SGD) algorithm with a newbob training scheduler. For\nWER experiments, a TDNN-F [41] acoustic model was trained\non the 81 hours of AMI training data using the lattice-free\nmaximum mutual information [39] criterion without any data-\naugmentation, speaker adaptation or voice tract length normal-\nisation following the simpliﬁed Kaldi recipe [38, 43]. It was\nFigure 4: pseudo-PPL by varying the context range on AMI dev\nset. The context range is measured by the number of words in\nboth past and future contexts.\nused to generate 100-best lists with a 4-gram LM. The 100-\nbest list were then rescored using CULMs whose context is de-\nrived from the hypothesis rescored by a standard LSTM-LM.\nThe code for LM training and rescoring can be found here1.\n4.2.1. Context coverage investigation\nTo determine suitable context coverages for CULMs, pseudo-\nPPLs are plotted against different context ranges for the AMI\ndev set in Figure 4. The lowest pseudo-PPL achieved by the\nself-attentive encoding is at around 36 words, while the lowest\npseudo-PPL for the ﬁnal-state encoding is at around 72 words.\nFurthermore, the self-attentive context encoding degrades faster\nthan the ﬁnal-state encoding when the context becomes longer,\nbecause the self-attentive structure is also possible to assign\nhigh weights to further context irrelevant to the current utter-\nance. As a result, 36 words for self-attentive encoding and 72\nwords for ﬁnal-state encoding were selected.\n4.2.2. System comparison\nUsing the best context range, the LM performance for both\nPPL and WER are compared in Table 2. Baseline refers to the\nstandard single layer LSTM-LM with 768d hidden states and\n256d word embeddings. For completeness, using LSTM hidden\nstates from 1-best history to initialise the baseline model at the\nstart of each utterance during rescore is also included which is a\nsimple but powerful incorporation of context information. The\ncorresponding PPL for this is to use the ﬁnal hidden state from\nthe previous utterance at the beginning of the current utterance\nat the sentence boundaries without resetting.\nSystems (pseudo-) PPL WER\n4-gram - 20.2\nbaseline 61.3 18.4\nbaseline + 1-best 56.9 18.0\nﬁnal-state encoding 55.9 18.0\nself-attentive encoding 55.8 17.9\nTable 2: Comparison among different LMs for (pseudo-)PPL\nand WER on AMI eval set. Baseline + 1-best uses 1-best history\nto initialise the LSTM hidden states during rescoring.\nBy initialising with 1-best history, PPL is reduced by 4.4\nand WER is reduced by 0.4% absolute compared to the base-\nline. Both CULMs outperform the baseline the baseline with\n1-best history. While requiring more computation than the ﬁnal-\n1 https://github.com/BriansIDP/Cross Utterance Clean\nSampling method Final-state Self-atten.\nno sampling 55.9 55.8\nsubstitution 56.1 55.9\n+ insertion & deletion 55.9 55.4\nfrom error analysis 55.5 55.3\nTable 3: pseudo-PPL with error sampling under two context\nvector extraction networks on AMI eval set. Pseudo-PPLs are\nmeasured under the true context.\nSampling method Final-state Self-atten.\nno sampling 18.0 17.9\nno sampling true context 17.9 17.9\nsubstitution 18.0 18.0\n+ insertion & deletion 17.9 17.8\nfrom error analysis 17.9 17.8\nTable 4: WER with error sampling using two context vector ex-\ntraction networks on AMI eval set.\nstate encoding, the self-attentive encoding CULM achieves the\nbest performance in pseudo-PPL and WER with a further 0.1%\nWER reduction over the baseline with 1-best history.\nThe improvements from using acoustic error sampling tech-\nnique are shown in Table 3 and Table 4 for PPL and WER re-\nspectively. The middle rows in both tables are the results using\nthe ﬁrst error sampling method and the bottom rows are using\nthe second method with error distributions from analysis ﬁles.\nAdding insertion and deletion errors that change the position of\nwords in the context performs better than using substitution er-\nrors only. By using the second method, both systems achieved\nanother 0.1% absolute WER reduction. In particular, the self-\nattentive context encoding system performs better than the no\nsampling case even when the true context is given. This means\nthe model with error sampling generalises better to other con-\ntexts. Therefore, the error sampling not only narrows the gap\nbetween training and rescoring, but also has a data augmenta-\ntion effect to the self-attentive context extraction network.\n4.3. SWBD Experiments\nTo further validate the proposed method on a larger corpus, ex-\nperiments are performed on Switchboard. A two-layer LSTM\nwith 256d word embedding and 2048d hidden states is used as\nthe baseline LM and also for the CULM. Around 260 hours of\nSWBD training data was used to train the acoustic model with\nother set-up the same as for AMI [42].\n4.3.1. System comparison\nDifferent LMs are compared in Table 5 where similar improve-\nments to the AMI case were found. Besides, the effect of error\nsampling is more signiﬁcant for the CH part than for the SWBD\npart as the latter has context closer to the ground truth. Over-\nall, reductions of 0.3% and 0.9% absolute WER were found by\nusing a CULM with self-attentive encoding and acoustic error\nsampling on the SWBD and CH parts respectively.\n4.3.2. Error analysis\nFinally, by comparing the error pattern analysis from differ-\nent systems, three examples of common improvements obtained\nSystems (pseudo-) WER\nPPL SWBD CH\n4-gram 87.1 9.9 20.7\nbaseline 52.1 7.8 17.8\nbaseline + 1-best 39.9 7.7 17.1\nﬁnal-state enc. 40.1 7.6 17.1\nself-atten. 37.9 7.5 17.0\nﬁnal-state enc. + sampling 39.8 7.6 17.1\nself-atten. + sampling 37.7 7.5 16.9\nTable 5: PPL and WER for LM trained on SWBD + Fisher\ndatasets and evaluated on the Eval2000 dataset which is split\ninto the Switchboard (SWBD) part and the Callhome (CH) part.\nPseudo-PPLs are measured with the true context.\nfrom CULMs are summarised in Figure 5.\nTrue text \nRNNLM \nRNNLM no reset \nCross-utterance\n• Tense Consistency \n*. She says she has started walking every morning before she goes to work \n1. She said she started walking every morning before she goes to work \n2. She says she started walking every morning before she goes to work \n3. She says she has started walking every morning before she goes to work \n• Content Coherency \n*. …and lost an ounce. How could you tell if you have lost an ounce \n1. …and lost an ounce. How could you tell if you have lost an hour \n2. …and lost an ounce. How could you tell if you have lost an hour \n3. …and lost an ounce. How could you tell if you have lost an ounce \n• Personal Pronoun Consistency \n*. I got scared and I cut him off, and I had not talked to him for … \n1. I got scared and I cut him off, and I had not talked to them for … \n2. I got scared and I cut him off, and I had not talked to him for … \n3. I got scared and I cut him off, and I had not talked to him for …\nFigure 5: Examples of errors from different LMs. From top to\nbottom: ground truth, standard LSTM, standard LSTM with 1-\nbest history, and self-attentive encoding CULM.\nThe most frequent error being corrected is the tense. If the\nsurrounding context provides the correct tense, the CULM will\nselect the utterance with matched tense in the n-best list. A sec-\nond type of correction is the repetition of a word. This correc-\ntion is also explicitly indicated by the high attention weight as-\nsigned to the speciﬁc word in the context when the self-attentive\nextraction is used. Finally, there are a couple of places where\nthe consistency of personal pronoun use is retained.\n5. Conclusions\nA cross-utterance LM which effectively incorporates the sur-\nrounding context information for ASR is proposed in this pa-\nper, together with two context vector extraction methods. In\naddition, an acoustic error sampling technique is introduced\nto achieve a better match between LM training and testing as\nwell as a better generalisation to other contexts. Experiments\nperformed on two conversational corpora showed that the pro-\nposed CULM structure outperforms the baseline in both PPL\nand WER, and with acoustic error sampling, further improve-\nments were found. Speciﬁcally, the best-performing system\nwith error sampling yielded WER reductions of 0.6% absolute\non AMI eval set, 0.3% on the Switchboard part and 0.9% the\non Callhome part of the Eval2000 test set. Furthermore, analy-\nsis with examples of corrected error patterns demonstrated the\neffectiveness of the proposed method.\n6. References\n[1] F. Jelinek, Statistical Methods for Speech Recognition, The MIT\npress, 1997.\n[2] T. Mikolov, M. Karaﬁ ´at, L. Burget, J. Cernock ´y, and S. Khudan-\npur, “Recurrent neural network based language model”, Proc.\nInterspeech, Makuhari, 2010.\n[3] T. Hori, J. Cho, and S. Watanabe, “End-to-end speech recognition\nwith word-based RNN language models”,Proc. SLT, Athens, 2018.\n[4] S. Toshniwal, A. Kannan, C.C. Chiu, Y . Wu, T.N. Sainath, and\nK. Livescu, “A comparison of techniques for language model\nintegration in encoder-decoder speech recognition”, Proc. SLT,\nAthens, 2018.\n[5] A. Sriram, H. Jun, S. Satheesh, and A. Coates, “Cold fu-\nsion: Training seq2seq models together with language models”,\narXiv:1708.06426, 2017.\n[6] X. Chen, X. Liu, Y . Wang, M.J.F. Gales and P.C. Woodland “Efﬁ-\ncient training and evaluation of recurrent neural network language\nmodels for automatic speech recognition”, in IEEE/ACM Transac-\ntions on Audio, Speech, and Language Processing, vol. 24, no. 11,\npp. 2146-2157, Nov. 2016.\n[7] X. Liu, Y . Wang, X. Chen, M.J.F. Gales, and P.C. Woodland, “Ef-\nﬁcient lattice rescoring using recurrent neural network language\nmodels”, Proc. ICASSP, Florence, 2014.\n[8] S. Hochreiter, J. Schmidhuber, “Long short-term memory”, Neural\ncomputation 9(8), 1997.\n[9] A. Graves, N. Jaitly and A. Mohamed “Hybrid speech recognition\nwith deep bidirectional LSTM”, Proc. ASRU, Olomouc, 2013.\n[10] H. Soltau, H. Liao and H. Sak “Neural speech recognizer:\nAcoustic-to-word LSTM model for large vocabulary speech recog-\nnition”, Proc. Interspeech, Stockholm, 2017.\n[11] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q.V . Le, R. Salakhutdi-\nnov “Transformer-XL: Attentive language models beyond a ﬁxed-\nLength context”, Proc. ACL, Florence, 2019.\n[12] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke,\nD. Yu and G. Zweig “Achieving human parity in conversational\nspeech recognition”, arXiv:1610.05256, 2016.\n[13] G. Sun Cross-utterance Language Models for Automatic Speech\nRecognition, MEng. Thesis, Cambridge University Engineering\nDepartment, Cambridge, UK, 2019.\n[14] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural\nprobabilistic language model”, Journal of Machine Learning Re-\nsearch, vol. 3, pp. 11371155, 2003.\n[15] M. Sundermeyer, R. Schluter, and H. Ney, “LSTM neural net-\nworks for language modelling”, Proc. Interspeech, Portland, 2012.\n[16] T. Mikolov and G. Zweig. “Context dependent recurrent neural\nnetwork language model”, Proc. SLT, Miami, 2012.\n[17] R. Kuhn and R. De Mori. “A cache-based natural language model\nfor speech recognition”, IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 12.6, pp. 570583, 1990.\n[18] J. Bellegarda. “Exploiting latent semantic information in statis-\ntical language modeling”, Proceedings of the IEEE. , vol. 88, pp.\n12791296, 2000.\n[19] X. Chen, T. Tan, X. Liu, P. Lanchantin, M. Wan, M.J.F. Gales,\nand P.C. Woodland, “Recurrent neural network language model\nadaptation for multi-genre broadcast speech recognition”, Proc.\nInterspeech, Dresden, 2015.\n[20] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke,\n“The Microsoft 2017 conversational speech recognition system”,\narXiv 1708.06073, 2017.\n[21] K. Li, H. Xu, Y . Wang, D. Povey, and S. Khudanpur, “Recur-\nrent neural network language model adaptation for conversational\nspeech recognition”, Proc. Interspeech, Hyderabad, 2018.\n[22] R. Masumura, T. Tanaka, T. Moriya, Y . Shinohara, T. Oba, and\nY . Aono, “Large context end-to-end automatic speech recognition\nvia extension of hierarchical recurrent encoder-decoder models”,\nProc. ICASSP, Brighton, 2019.\n[23] S. Kim, S. Dalmia and F. Metze, “Gated embeddings in end-to-\nend speech recognition for conversational-context fusion”, Proc.\nACL, Florence, 2019.\n[24] M.E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,\nK. Lee, and L. Zettlemoyer, “Deep contextualized word represen-\ntations”, Proc. NAACL, New Orleans, 2018.\n[25] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding”, Proc. NAACL, Minneapolis, 2019.\n[26] S. Parthasarathy, W. Gale, X. Chen, G. Polovets and\nS. Chang, “Long-span language modelling for speech recognition”,\narXiv:1911.04571, 2019.\n[27] T. Wang and K. Cho, “Larger-context language modelling”, Proc.\nACL, Berlin, 2016.\n[28] B. Liu and I. Lane, “Dialog context language modelling with\nrecurrent neural networks”, Proc. ICASSP, New Orleans, 2017.\n[29] W. Xiong, L. Wu, J. Zhang, and A. Stolcke, “Session-level lan-\nguage modelling for conversational speech”, Proc. EMNLP, Brus-\nsels, 2018.\n[30] Z. Lin, M. Feng, C.N. dos Santos, M. Yu, B. Xiang, B. Zhou, and\nY . Bengio, “A structured self-attentive sentence embedding”,Proc.\nICLR, Toulon, 2017.\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA.N. Gomez, L. Kaiser, and I. Polosukhin “Attention is all you\nneed”, Proc. NIPS, Long Beach, 2017.\n[32] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer and Y . Wu\n“Exploring the limits of language modelling”, arXiv:1602.02410,\n2016.\n[33] J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot,\nT. Hain, J. Kadlec, V . Karaiskos, W. Kraaij, M. Kronenthal,\nG. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post,\nD. Reidsma, and P. Wellner, “The AMI meeting corpus: A pre-\nannouncement”, Proc. MLMI, Bethesda, 2006.\n[34] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer “Scheduled\nsampling for sequence prediction with recurrent neural networks”,\nProc. NIPS, Montreal, 2015.\n[35] R. V oleti, J. M. Liss, and V . Berisha “Investigating the effects of\nword substitution errors on sentence embeddings”, Proc. ICASSP,\nBrighton, 2019.\n[36] K. Irie1, A. Zeyer, R. Schluter and H. Ney “Training language\nmodels for long-span cross-sentence evaluation”,Proc. ASRU, Sen-\ntosa, 2019.\n[37] R. Masumura, M. Ihori, T. Tanaka, I. Saito, K. Nishida and T. Oba,\n“Generalized large-context language models based on forward-\nbackward hierarchical recurrent encoder-decoder models”, Proc.\nASRU, Sentosa, 2019.\n[38] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian and P. Schwarz,\n“The Kaldi speech recognition toolkit”,Proc. ASRU, Hawaii, 2011.\n[39] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang, and S. Khudanpur, “Purely sequence-trained neu-\nral networks for ASR based on lattice-free MMI”, Proc. Inter-\nspeech, San Francisco, 2016.\n[40] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De-\nVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic\ndifferentiation in PyTorch”, Proc. NIPS Autodiff Workshop, Long\nBeach, 2017.\n[41] D. Povey, G. Cheng, Y . Wang, K. Li, H. Xu, M. Yarmohamadi,\nand S. Khudanpur, “Semi-orthogonal low-rank matrix factorization\nfor deep neural networks”, Proc. Interspeech, Hyderabad, 2018.\n[42] Y . Lu, M.J.F. Gales, K.M. Knill, P. Manakul, Y . Wang “Disﬂuency\ndetection for spoken learner english”, Proc. SLaTE, Graz, 2019.\n[43] P. Manakul, M.J.F. Gales, L. Wang, “Abstractive spoken doc-\nument summarisation using hierarchical model with multi-stage\nattention diversity optimization”, In submission to Interspeech ,\nShanghai, 2020.",
  "topic": "Utterance",
  "concepts": [
    {
      "name": "Utterance",
      "score": 0.8142687678337097
    },
    {
      "name": "Computer science",
      "score": 0.806401252746582
    },
    {
      "name": "Speech recognition",
      "score": 0.7309640645980835
    },
    {
      "name": "Word error rate",
      "score": 0.7104344367980957
    },
    {
      "name": "Test set",
      "score": 0.6284343600273132
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5826830863952637
    },
    {
      "name": "Reduction (mathematics)",
      "score": 0.5393540263175964
    },
    {
      "name": "Projection (relational algebra)",
      "score": 0.530852198600769
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.5162108540534973
    },
    {
      "name": "Word (group theory)",
      "score": 0.4759514033794403
    },
    {
      "name": "Language model",
      "score": 0.46008017659187317
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4509515166282654
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4432685673236847
    },
    {
      "name": "Phone",
      "score": 0.4272751212120056
    },
    {
      "name": "Acoustic model",
      "score": 0.4112958312034607
    },
    {
      "name": "Speech processing",
      "score": 0.2896718382835388
    },
    {
      "name": "Algorithm",
      "score": 0.15359455347061157
    },
    {
      "name": "Detector",
      "score": 0.0887266993522644
    },
    {
      "name": "Mathematics",
      "score": 0.08517885208129883
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}