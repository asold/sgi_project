{
  "title": "WormSwin: Instance segmentation of C. elegans using vision transformer",
  "url": "https://openalex.org/W4383533473",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4202210380",
      "name": "Maurice Deserno",
      "affiliations": [
        "University Hospital Cologne",
        "University of Cologne"
      ]
    },
    {
      "id": "https://openalex.org/A2083576006",
      "name": "Katarzyna Bozek",
      "affiliations": [
        "Cologne Excellence Cluster on Cellular Stress Responses in Aging Associated Diseases",
        "University Hospital Cologne",
        "University of Cologne"
      ]
    },
    {
      "id": "https://openalex.org/A4202210380",
      "name": "Maurice Deserno",
      "affiliations": [
        "University of Cologne",
        "TH Köln - University of Applied Sciences",
        "University Hospital Cologne"
      ]
    },
    {
      "id": "https://openalex.org/A2083576006",
      "name": "Katarzyna Bozek",
      "affiliations": [
        "University Hospital Cologne",
        "Cologne Excellence Cluster on Cellular Stress Responses in Aging Associated Diseases"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3111687703",
    "https://openalex.org/W3191130978",
    "https://openalex.org/W2037478113",
    "https://openalex.org/W4220806894",
    "https://openalex.org/W2141533058",
    "https://openalex.org/W2035694260",
    "https://openalex.org/W2884436577",
    "https://openalex.org/W3157861089",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2014829418",
    "https://openalex.org/W3196897371",
    "https://openalex.org/W4294104518",
    "https://openalex.org/W6893711219",
    "https://openalex.org/W4226329107",
    "https://openalex.org/W3188082098",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3110482695",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2762439315",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W3091259769",
    "https://openalex.org/W2019062120",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2989676862",
    "https://openalex.org/W4250482878",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2113242953",
    "https://openalex.org/W4384652490"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports\nWormSwin: Instance segmentation \nof C. elegans using vision \ntransformer\nMaurice Deserno 1,2,4* & Katarzyna Bozek 1,2,3\nThe possibility to extract motion of a single organism from video recordings at a large-scale provides \nmeans for the quantitative study of its behavior, both individual and collective. This task is particularly \ndifficult for organisms that interact with one another, overlap, and occlude parts of their bodies \nin the recording. Here we propose WormSwin—an approach to extract single animal postures of \nCaenorhabditis elegans (C. elegans) from recordings of many organisms in a single microscope well. \nBased on transformer neural network architecture our method segments individual worms across a \nrange of videos and images generated in different labs. Our solutions offers accuracy of 0.990 average \nprecision ( AP 0.50 ) and comparable results on the benchmark image dataset BBBC010. Finally, it \nallows to segment challenging overlapping postures of mating worms with an accuracy sufficient to \ntrack the organisms with a simple tracking heuristic. An accurate and efficient method for C. elegans \nsegmentation opens up new opportunities for studying of its behaviors previously inaccessible due to \nthe difficulty in the worm extraction from the video frames.\nBehaviour is the external output of an animal’s nervous system. The possibility to systematically observe, extract, \nand quantify an animal’s motion is a prerequisite to investigate and ultimately understand its behavioral reper-\ntoire. Alterations to an organism’s natural behavior is a phenotypic readout of the neural and other molecular \nchanges that are causing them. To fully understand the functioning of neural mechanisms it is therefore essential \nto dissect their effect on an animal’s behavior.\nCapturing behavior requires video acquisition systems allowing to either view or infer an entire posture of an \norganism and its change in time. One of the main challenges in obtaining complete and precise posture measure-\nments are the occlusions of animal body parts in a 2D video recording, especially if more than one individual \nis being imaged. To resolve this, extensive 3D motion capture systems have been  developed1 as well as methods \nthat allow to impute the occluded parts of the  posture2.\nThese challenges have not yet been resolved for the model organism C. elegans. While imaging the nematode’s \nbehavior is less complex than imaging of larger organisms and massively parallel recording systems allow to \ncapture thousands of worms at a  time3,4, there are currently no end-to-end methods that resolve their postures \nwhen occlusions occur. The quantification of C. elegans strains’ behavior and characterization of their phenotypes \nis therefore based on segments of worm motion in which it does not coil or intersect with another worm. As a \nresult, a large portion of the worm behavior, including its group behavior, cannot be quantitatively analyzed.\nHere we propose an automated method for C. elegans  posture extraction from 2D video recordings. Based \non deep learning transformer architecture and a classical instance segmentation training objective, our solution \nallows to correctly infer an outline of an individual worm body in overlapping and occluded configurations. We \ntrain the neural network on randomly generated image data, obtaining a solution that generalizes to various real \ndatasets. With the segmentation outputs of our method we are able to correctly infer worm trajectories with a \nsimple position matching heuristics. WormSwin opens up new opportunities to study the full repertoire of C. \nelegans behavior including behaviors such as mating that were previously inaccessible to quantitative analysis.\nOPEN\n1Center for Molecular Medicine Cologne (CMMC), Faculty of Medicine and University Hospital Cologne, University \nof Cologne, Cologne, North Rhine-Westphalia, Germany. 2Institute for Biomedical Informatics, Faculty of Medicine \nand University Hospital Cologne, University of Cologne, Cologne, North Rhine- Westphalia, Germany. 3Cologne \nExcellence Cluster on Cellular Stress Responses in Aging-Associated Diseases (CECAD), University of Cologne, \nCologne, North Rhine-Westphalia, Germany. 4Faculty of Mathematics and Natural Sciences, University of Cologne, \nCologne, North Rhine-Westphalia, Germany. *email: maurice.deserno@uni-koeln.de\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nRelated work\nOver the past years different methods for C. elegans detection and segmentation have been proposed, either as \npart of a general approach to tracking and behavioral studies, or as a stand-alone method.\nOne of the first methods for automated worm tracking and behavior quantification was proposed by Baek \net al.5. The method used a computer-controlled tracker for single worms, recording grayscale videos. The gray-\nscale frames of a video were binarized based on the mean and standard deviation of pixel intensities and a pre -\ndefined threshold. The method computes features such as the area of foreground or the movement between two \nframes in the binarized videos and uses them as input to the  algorithm6 for classification of different C. elegans \nstrains. Swierczek et al. 7 proposed a tracking approach called Multi Worm Tracker. The method calculates a \nbackground estimate using pixel intensity values. Moving objects are found by searching for pixels darker than \nthe background by a specific threshold. In the next frame, the objects are searched for in the vicinity of their \nprevious location.\nThe arrival of deep learning offered new opportunities to build more accurate methods for worm segmenta-\ntion and tracking. Javer et al. 8 developed a multi-object tracking framework able to track C. elegans as well as \nfish and drosophila larvae. The method requires manual tuning of segmentation parameters to best perform \nwith the given recorded data and comes with a graphical user interface for the ease of use and evaluation of the \nresults. Using the motion data, the framework extracts a large number of features characterizing worm move-\nment. Hebert et al. 9 proposed a pose estimation method for videos of single moving C. elegans in challenging \nposes like coiling. Using a  ResNetV210-like architecture the centerline of worms is predicted. With the help of \ntemporal information the head and tail position is determined. Wählby et al. 11 proposed a phenotype analysis \ntoolbox based on the open-source  CellProfiler 12 project. To untangle clusters of worms the authors describe \nthem as a mathematical graph and, using a learned model of worm postures, search for the best representation \nof true worms. The worm posture model is based on a training dataset of isolated single C. elegans shapes and on \ncomputed angle-based shape descriptors. One of the downsides of this approach is that unexpected phenotypes \nare likely to be discarded as debris. Banerjee et al. 13 introduced a deep learning C. elegans tracking method in \nwhich the detection is based on  YOLOv514 and tracking on Strong SORT  algorithm15. For each detected object \nthe method outputs its bounding box, then threshold-based segmentation and skeletonization are applied to \ninfer shapes of the detected objects. Fudickar et al.16 developed a two-shot segmentation method based on Mask \nR-CNN17 with ResNet-10118 backbone, to segment C. elegans imaged in petri-dishes with a low-cost image cap-\nturing system. However, the method did not solve the problem of segmenting overlapping worms and segments \nthem as one object. Mais et al. 19 developed a proposal-free instance segmentation method, called PatchPerPix, \nbased on a convolutional neural network (CNN) trained to predict the shape of a C. elegans  in a small patch \nof the whole image (local shape patches). The method uses a modified U-Net 20 deep neural network and patch \naffinity graphs to reconstruct individual worm shapes. For each pixel the method predicts which shape patch it \nbelongs to and, using a patch affinity graph, merges the patches to form complete instance shapes. Lalit et al. 21 \nproposed an embedding-based instance segmentation method for 2D and 3D microscopy data, called EmbedSeg. \nThe method is based on ERF-Net 22, predicting spatial embeddings of pixels. These embeddings are then clus-\ntered into object instances. To train this method, an additional step of pre-processing the dataset is required to \ngenerate object-centered image patches for every object. The method was tested on different datasets including \nthe C. elegans BBBC010 dataset.\nAmong the methods described above there are one- and two-shot detectors. One shot-detector architectures \nlike  YOLO23 detect objects in one step. Pre-defined boxes (also called anchors) are placed onto a grid, laid over \nthe image. For each box, the network predicts if the box contains an object. On the other hand, two-shot detectors \nconsist of a region proposal network (RPN) proposing regions of interest (RoI) to a second network, refining \nthese proposals to form the actual predictions. One-shot object detection methods  (like13) are in general less \ncomputationally expensive compared to two-shot approaches (e.g.16), although the latter ones achieve a higher \nprecision especially in more challenging scenes. This is one of the reasons for the high popularity of two-shot \nnetworks such as Mask R-CNN in the domain of instance segmentation.\nUsually more than one box is predicted per object. To only keep the best matching box, many methods apply \nnon-maximum suppression (NMS). This approach consists of removing from the predicted highly overlapping \nbounding boxes those with lower probability values as potential false positive detections of the same object. \nHowever, NMS can lead to removal of correct detections, especially in dense scenes, where many objects in the \nimage overlap.\nIn this paper we address the problem of segmenting objects in dense scenes by combining the well established \narchitecture of two-shot detectors with state of the art vision transformer. To avoid the pitfalls of the NMS algo-\nrithm, we apply Soft Non Maximum Suppression (Soft-NMS)24.\nMethods\nDatasets. CSB-1 dataset. The CSB-1 dataset consists of 56 videos with a length of ∼ 1.5 min, a frame rate \nof 5 Hz and frame size of 912 × 736 px which were generated to describe the new C. elegans csb-1  strain25. We \nannotated 10 of those videos, where nine videos were reserved for training and one for testing. The videos do not \ncontain any visible petri-dish edges, have different backgrounds and varying numbers of worms. We extracted \nframes from the videos using FFmpeg  (https:// ffmpeg. org) and used them to generate our synthetic training \ndataset described below.\nWorms were annotated individually with a binary mask labelling foreground pixels, resulting in one mask \nimage per worm per frame. These separate masks allow to mark all the worms also in cases where pixels of \nindividual C. elegans overlap. The labeled CSB-1 dataset contains more than 60,500 individual worm masks. C. \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nelegans at the image borders are ignored during the labelling process. Our data is available under https:// doi. org/ \n10. 5281/ zenodo. 74568 03 as a rich resource to develop better methods for animal tracking.\nSynthetic dataset. For training the model we generated a synthetic dataset using the nine annotated videos \nfrom the CSB-1 dataset described above. We automatically cut out foreground objects from the original gray-\nscale images, according to their polygon annotations and created patches with a worm in the foreground and \ntransparent background. Additionally, we created background images as templates by removing all foreground \nobjects using standard graphics software and filled them with patches of background, taken from the same \nbackground images.\nThe following pipeline was applied to create each image of the synthetic dataset: \n1. Randomly select 5–30 foreground objects and a background template\n2. Randomly flip and rotate foreground objects and their corresponding annotations\n3. Apply blurring to foreground objects by averaging the pixel values using a 2 × 2 px kernel\n4. Place foreground object patches on background image: \n(a) In 20% cases: place a foreground object on top of another one\n(b) In 80% cases: place a foreground object randomly on the background image\nThe generated training dataset consists of 10,000 grayscale images with a size of 912 × 736 px and more than \n175,000 labeled C. elegans and additional 1000 images for testing (see Fig.  1a). We randomly added grayscale \nrings of random sizes surrounding the center of the images (see Fig.  1b) to make the network robust against \nsimilar artifacts (e.g. petri-dish edges) in other test datasets. Foreground objects might overlap with the artificial \npetri-dish edges, but are only placed on the inside of the rings. Using the object masks of the original data, for \neach foreground object we generated a binary mask corresponding to its artificially generated location and shape. \nThese masks are used as ground truth for model training and testing on this dataset. Our synthetic training \ndataset is available at https:// doi. org/ 10. 5281/ zenodo. 74568 03.\nBBBC010. The “BBBC010—C. elegans live/dead assay” 26 (BBBC010) dataset consists of 200 images, divided \ninto 100 bright-field and 100 green fluorescent protein (GFP) microscopy images of the same scene. The images \nhave a size of 696 × 520 px and are saved as 16-bit grayscale TIFF files. For our experiments we converted the \nimages to 8-bit grayscale PNG images. The images contain a black border surrounding the region of interest \n(ROI) with the C. elegans in the center (Fig.  1c) which makes up around 50% of the image. Ground truth con-\nsists of binary foreground/background images for each worm separately, allowing to disentangle the overlapping \nshapes.\nThe images show C. elegans exposed to Enterococcus faecalis with a negative control group containing dead \nworms and a positive control group, which was treated with ampicillin and includes alive worms. While the alive \nC. elegans have the natural curved shapes (Fig.  1c), the negative control group appear rod-like with an uneven \ntexture (Fig. 1d).\nMating dataset. The mating dataset (MD) was created from a 10 min. long video with a frame rate of 25 Hz and \na frame size of 3036 × 3036 px. It contains freely moving worms as well as mating ones. Mating behavior is par-\nticularly difficult to segment as the two individuals are strongly overlapping and parallel to one another (Fig. 1e). \nThis dataset represents therefore the most challenging segmentation task for our method.\nWe downsampled the video to 5 Hz and selected 50 frames randomly for annotation and testing of our \napproach. More than 3900 individual worm postures were labeled in this dataset. The labeling includes only \nmature C. elegans, worms touching the image boundary were ignored. We split the frames into 450 images with \na size of 1012 × 1012 px without overlap. The grayscale images show C. elegans in a petri-dish with the edges \nvisible (see example patch in Fig. 1f).\nNetwork architecture\nTo predict bounding boxes and instance segmentation masks we use the Hybrid Task Cascade (HTC) 27 neural \nnetwork architecture, combined with Swin  Transformer28 as backbone (similar  to28).\nSwin Transformer is a Vision Transformer (ViT)-based backbone  architecture29, which can be applied to \ndifferent vision-related tasks (e.g. classification, detection or segmentation). Previous ViTs divided the input \nimage into relatively large patches and computed self attention among them. ViTs showed lower computational \ncomplexity, but did not account for small details in large images. To tackle this problem Swin Transformer intro-\nduced a Shifted Window approach to reduce the computational complexity of standard multi-head self attention \n(MSA) modules. Additionally, Swin Transformer builds hierarchical feature maps, merging image patches in \ndeeper layers, enabling small-sized patches, leading to more detailed predictions. We chose Swin-L architecture \nvariant in our study which was pre-trained on ImageNet-21K30 with an image size of 384 × 384 px (similar  to28).\nHTC improves the architecture of Cascade Mask R-CNN31 by introducing interleaved bounding box regres-\nsion and instance segmentation mask prediction. The information flow is optimized by adding direct connections \nbetween the individual mask branches (Fig.  2). Additionally, a semantic segmentation branch is added to the \noriginal architecture to help to distinguish between foreground and background. In our experiments we do not \nuse this additional semantic segmentation branch.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nTo further improve the accuracy when training on small batches, we exchanged the default Batch Normali -\nzation (BN)32 with Group Normalization (GN)33 and Weight Standardization (WS)34 in HTC (similar  to34). We \nalso replaced the Shared 2 Fully-Connected Bounding Box heads (Shared2FC) by Shared 4 Convolution + one \n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\nFigure 1.  Example images from the datasets used in this study: (a) synthetic dataset example with added ring, \n(b) synthetic dataset without ring, (c) BBBC010 dataset example with mostly alive C. elegans, (d) BBBC010 \ndataset patch with mostly dead C. elegans, (e) mating dataset with petri-dish ring, (f) zoomed-in mating dataset \npatch with many overlaps.\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nFully-Connected Bounding Box head (Shared4Conv1FC) (as described  in33). To suppress low quality detections \nbut keep high quality predictions in dense and overlapping scenes we use Soft-NMS instead of the traditional \nNMS algorithm for the R-CNN during test time (see HTC++28).\nTraining. We used multi-scale training with a size between 480 and 800 px for the shorter side and 1333 px \nat most for the longer side,  AdamW35 as optimizer, Cosine Annealing Learning Rate  Scheduler36 and Linear \nWarm-Up37 (similar  to28). The learning rate was set to 2.5e -5 and weight decay to 0.1. The number of warm-up \niterations of the linear warm-up and learning rate scheduler was set to 1000, warm-up ratio to 0.1 and mini-\nmum learning rate ratio to  1e-5. During training and testing the NMS threshold for the RPN was set to 0.7, the \nSoft-NMS of the R-CNN was set to 0.5 during test time. We used random flipping with a probability of 0.5 and \n AutoAugment38 for multiscale resizing and cropping. Additionally, we used the pre-trained weights for the Swin \nbackbone, trained on ImageNet-21K with an image size of 384 × 384 px (similar  to28). We tested our approach \non three different datasets: the publicly available BBBC010 dataset, MD and CSB-1 datasets. During testing all \nimages were resized to 800 px on the short side and to no more than 1333 px on the longer side, preserving the \noriginal ratio. We excluded all instances touching image borders as incomplete C. elegans instances.\nIn all our experiments we used the MMDetection  framework39. Our code and network configuration file for \nthe MMDetection framework are available at https:// github. com/ bozek lab/ worm- swin.\nWormSwin was trained using 4 Nvidia Tesla V100-SMX2 32 GB GPUs, 6 cores of an Intel Xeon Gold 6248 \nCPU @ 2.50 GHz and 100 GB of RAM. With a batch size of four (one image per GPU) and two workers per GPU, \ntraining for 36 epochs took ∼ 19 h. Evaluation on the test set runs at a speed of 2.7 images/s.\nResults\nWe trained WormSwin on data synthetically generated based on the CSB-1 dataset. The procedure of data gen-\neration allows us to control the degree of overlap among individuals and to train the network on a large number \nof images containing overlapping worms to improve segmentation of dense scenes. Once trained, we evaluated \nthe model on a synthetic test set (see Table 1) as well as on three independent datasets: BBBC010, MD and CSB-\n1. These datasets come from different labs, show visual variability, and contain different number and degree \nof overlapping C. elegans. We report our results mostly as COCO Average Precision (AP) 40 calculated using \npycocotools (https:// github. com/ cocod ataset/ cocoa pi). For the BBBC010 dataset, we report our results as DSB \nAP for comparison to other methods. AP is the area under the precision-recall curve and its values are between \n0 and 1, with a higher AP representing better performance. Precision and recall of the detection is calculated for \ninstances that show intersection over union (IoU) with the ground truth above a predefined threshold. DSB mAP \ncalculates a mean Jaccard Index by using different IoU thresholds. COCO mAP uses a more complex approach: \ndetections are sorted by descending confidence score. The calculation iterates over all detections in this order, \nmarks them as True Positive (TP) or False Positive (FP) and adds them to calculate the precision until a recall \nof 1.0 is reached or iterated over all detections. Different IoU thresholds are used to label detections as TP or FP .\nWe report our results mostly for two IoU thresholds: 0.5 and 0.75 as well as a mean AP (mAP) for thresholds \nfrom 0.5 to 0.95 with a step size of 0.05. One of the most challenging parts for instance segmentation of C. elegans, \nas well as other biological systems, are overlapping objects in dense configurations. To measure the accuracy of \nour approach explicitly for overlapping objects, we added a dedicated AP metric. We defined overlapping objects \nas those whose ground truth bounding boxes overlap by more than 25% or whose segmentation masks that any \noverlap (>0% IoU). We report the AP for all objects as well as for the overlapping objects separately (Table 2).\nCSB-1 dataset. Although trained on synthetically generated data, our method generalizes fairly well to \nthe real video data with a mAP of 0.819 and 0.585 for the bounding box and mask respectively, lower by only \n∼ 0.09 mAP compared to the synthetic data. The same metric on the overlapping worms in the CSB-1 dataset \nare 0.551 and 0.527. While the mAP is lower for the overlapping C. elegans compared to the results on the entire \ndataset, the AP 0.50 of the bounding box and mask on the overlapping worms are 0.883 and 0.975, respectively. \nThis result suggests that the worms are detected correctly in principle but there exist errors in mask prediction. \nWhat these mask prediction errors are, is however not clear at a first glance. Despite the difference between the \nAP 0.50 and AP 0.75 in the overlapping worms, we found that the segmentation masks align in general well with \nthe ground truth (Fig. 3), however pixels on the edges of each object tend to be imprecisely segmented. Due to \nFigure 2.  Network architecture based on Swin-L backbone and HTC. Batch norm (BN) layers in HTC are \nreplaced by group norm (GN) + weight standardization (WS). Bounding box heads are changed from the \noriginal Shared2FC architecture to Shared4Conv1FC.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\n(a)\n (b)\n (c)\nFigure 3.  Example from the CSB-1 dataset (box and mask colors are selected randomly). (a) Ground truth \nannotations, (b) predicted bounding boxes and masks, (c) TP (green), FP and FN (red) pixels.\nTable 1.  Test results on all instances. “Box” and “mask” refer to the accuracy of detection of the bounding \nbox and segmentation mask, respectively. PatchPerPix ppp+dec refers to the network variant, introduced  by19. \n(*Multi-scale testing, †additional training data).\nAP 0.50 AP 0.75 mAP 0.50:0.95\nCSB-1\n WormSwin (box) 0.990 0.976 0.819\n WormSwin (mask) 0.990 0.675 0.585\nSynthetic\n WormSwin (box) 0.989 0.978 0.909\n WormSwin (mask) 0.977 0.918 0.679\nBBBC010\n PatchPerPix ppp+dec (mask) 0.939 0.891 0.775\n WormSwin (box)† 0.985 0.949 0.823\n WormSwin (mask)† 0.954 0.801 0.622\n WormSwin (mask)*, † 0.964 0.815 0.629\nMD\n WormSwin (box)† 0.990 0.968 0.832\n WormSwin (mask)† 0.980 0.551 0.542\nTable 2.  Test results for overlapping worms only (* multi-scale testing, †additional training data).\nAP 0.50 AP 0.75 mAP 0.50:0.95\nCSB-1\n WormSwin (box) 0.883 0.643 0.551\n WormSwin (mask) 0.975 0.409 0.527\nSynthetic CSB-1\n WormSwin (box) 0.983 0.958 0.853\n WormSwin (mask) 0.959 0.821 0.613\nBBBC010\n WormSwin (box) † 0.911 0.821 0.661\n WormSwin (mask) † 0.873 0.565 0.488\n WormSwin (mask)*, † 0.895 0.573 0.501\nMD\n WormSwin (box)† 0.822 0.633 0.505\n WormSwin (mask)† 0.893 0.079 0.355\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nthe small size of a worm mask with ∼ 500 px, errors at the edges of the predicted masks represent ∼ 30% of all \nforeground pixels.\nTo test the hypothesis that most error occur on the mask edges, we implemented an alternative version of the \nIoU: if a pixel in either ground-truth or predicted mask is at the border of an object (when the 4-way neighbor-\nhood is not fully foreground) then it is set to the value of the pixel at this position in the other mask. This way, \nobject border pixels which otherwise would be considered as false negative (FN) or false positive (FP) do not \ninfluence the IoU calculation in a negative way. Using this calculation, the mean IoU on the test subset raised \nfrom 0.827 to 0.961 (+13.4% increase) on the CSB-1 dataset.\nBBBC010 dataset. Because of the very limited number of training samples (50 images) the predictions \nof the network trained on BBBC010 were of poor quality. Therefore, we used the network pre-trained on our \nsynthetic data and fine-tune it on 50 randomly selected images from the BBBC010 dataset. We compared the \nperformance of our approach to two existing methods:  PatchPerPix19 and  EmbedSeg21. To enable this compari-\nson, instead of the COCO AP metric (see Table  1) we used (Data Science Bowl) DSB AP (https:// www. kaggle. \ncom/ compe titio ns/ data- scien ce- bowl- 2018/ overv iew/ evalu ation) as accuracy evaluation on this dataset which \nwas used in the original EmbedSeg method  publication21 (see Table 3).\nWe used the alternative IoU calculation already used for the CSB-1 dataset, to calculate the DSB accuracy \nwithout considering object edges. With the IoU defined this way, using the DSB metric we achieve 0.769 mAP \n(+0.233), 0.929 AP 0.50 (+0.012) and 0.823 AP 0.80 (+0.487) (compare to Table 3).\nMating dataset. Finally, we tested WormSwin on the MD dataset using weights pre-trained on our syn-\nthetic dataset. In this dataset we annotated 50 images, which are larger in size and contain a higher number of C. \nelegans compared to the BBBC010 dataset. Further, we split them into patches of size 1024 × 1024 px. We report \nour results in Table  1). Despite the challenging configurations of worms in this dataset, our method correctly \nidentifies the segmented objects, as indicated by the AP 0.50 which is comparable to the AP 0.50 in other data-\nsets. However the AP 0.75 and mAP 0.50:0.95 suggest that, while correctly detected, the segmentation masks of the \ndetected objects are imprecise. Similar to other datasets, we hypothesise that these errors occur on the bounda-\nries of the segmentation masks (Fig. 4) as well as are due to the very challenging object overlaps in this dataset.\nTracking. To test if our segmentation results are sufficiently accurate to allow for worm tracking and fur -\nther behavioral analysis, we implemented a simple IoU-based matching method (Fig.  5) and applied it on our \npredicted instance segmentation masks in the CBS-1 test set. Between two consecutive frames, objects with the \nhighest overlap in mask are matched into a trajectory. Iterating the matching procedure over all video frames \nresults in object trajectories. In this simple approach, if an object is not detected in a frame but detected in a \nsubsequent frame its trajectory is disrupted and two separate trajectories are created instead. We attempt to \nreconnect such trajectories in a post-processing step: for 10 frames after loosing an object, starting points of \nnew trajectories are compared with the endpoint of the lost trajectory. If the segmentation masks at these points \noverlap with at least 50%, the trajectories are reconnected. In the frames with missing segmentation masks the \npositions of C. elegans can be interpolated between two ends of reconnected trajectories.\nWhile a tracking method is outside of the scope of this study, our simple approach allows to build trajectories \nof interacting mating worms (Fig. 5). Tracking these challenging C. elegans interactions opens up new possibili-\nties of studying its behavior.\nDiscussion\nIn this work we present WormSwin, a deep learning approach for instance segmentation of microscopy images \nof C. elegans. Our method combines several recent improvements in deep learning and instance segmentation \n(Transformer Networks, HTC, Group Normalization, Weight Standardization, Soft-NMS) into a single approach \ntrained end-to-end. WormSwin does not require any pre-processing of the image data, enabling researchers to \ndirectly apply it on their video or image data.\nTogether with our method we provide a large dataset of C. elegans images with instance mask annotations to \nhelp researchers develop better segmentation approaches in the future. The new dataset is by an order of magni-\ntude larger compared to the BBBC010 dataset, enabling training deeper network architectures.\nThe small size of the BBBC010 benchmark dataset is a limiting factor to extensively train and test our method \non this dataset. The accuracy of our method is lower on this dataset compared to the CSB-1 which might be \nTable 3.  Test results using DSB metric (* multi-scale testing, †additional training data, ‡alternative IoU \nwithout object edges, mAP for IoUs in range 0.5–0.95, step size 0.05).\nAP 0.50 AP 0.60 AP 0.70 AP 0.80 AP 0.90 mAP\nBBBC010\n PatchPerPix ppp+dec19 0.930 0.905 0.879 0.792 0.386 0.727\n  EmbedSeg21 0.965 0.934 0.896 0.762 0.326 –\n WormSwin (mask)*, † 0.917 0.884 0.785 0.336 0.005 0.536\n WormSwin (mask)*, †, ‡ 0.929 0.920 0.890 0.823 0.483 0.769\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nattributed to the differences in the color intensities, size and appearance of C. elegans between the two datasets. \nSince retraining of WormSwin on a small amount of BBBC010 images improved the methods performance, we \nsuggest that to accurately segment datasets differing from CSB-1 characteristics, a similar retraining is necessary.\nNotably, our method shows a decrease in AP in the higher IoU threshold categories (e.g. Table  3AP 0.80 ). \nDespite this precision drop, the segmentation masks appear overall correct (Figs. 3, 4). We therefore hypothesize \nthat the major errors in the segmentation masks occur on the boundaries of the foreground area and further \n(a)\n (b)\n(c)\n (d)\n(e)\n (f)\n(g)\n (h)\nFigure 4.  Results on the Mating Dataset (box and mask colors are selected randomly). (a,c,e,g) Segmentation \nresults, (b,d,f,h) TP (green), FP and FN (red) pixels.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nsubstantiate this by calculating accuracy metric that does not take into account boundary pixels. The reason for \nthis type of error might be e.g. variation in human-generated labeling. We introduce blurring in the synthetic \ntraining data which might additionally change the appearance of the object contours. Despite these errors, \nindividual C. elegans poses are captured by the predicted segmentation masks and can be subject to further \nquantitative analysis.\nAs a major future improvement of this work we see models exploring temporal information to improve \nsegmentation of overlapping objects. Information on how C. elegans individuals arrive in a specific configura -\ntion is of great help in disentangling their postures. Previous work by Fontaine et al. 41 model C. elegans using \nplanar curves and Central Difference Kalman Filter (CDKF) to track multiple worms. This approach shows good \nresults even when occlusion occurs. Similarly, Alonso et al. 42 proposed a deep learning approach for detection \nand tracking in high density microscopy data, based on splines as shape descriptors. They test their approach \non different dataset including videos of C. elegans and achieve high accuracy in dense scenes with a high degree \nof occlusion. Such methods are a step towards combining segmentation with tracking in a single training objec-\ntive. While generating training datasets for multi-object tracking is a massive work burden, the accuracy of our \nsegmentation approach allows to build preliminary trajectories in an automated fashion.\nData availability\nThe datasets (except for the BBBC010 dataset) generated during and/or analysed during the current study are \navailable in the Zenodo—WormSwin: C. elegans Video Datasets repository, https:// doi. org/ 10. 5281/ zenodo. 74568 \n03. The BBBC010 dataset is available at https:// bbbc. broad insti tute. org/ BBBC0 10. Source code and configuration \nfiles are available at https:// github. com/ bozek lab/ worm- swin.\nReceived: 15 May 2023; Accepted: 5 July 2023\nReferences\n 1. Marshall, J. D. et al. Continuous whole-body 3D kinematic recordings across the rodent behavioral repertoire. Neuron 109, 420-\n437e8. https:// doi. org/ 10. 1016/j. neuron. 2020. 11. 016 (2021).\n 2. Gosztolai, A. et al. LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in \nlaboratory animals. Nat. Methods 18, 975–981. https:// doi. org/ 10. 1038/ s41592- 021- 01226-z (2021).\n 3. Y emini, E., Jucikas, T., Grundy, L. J., Brown, A. E. X. & Schafer, W . R. A database of Caenorhabditis elegans behavioral phenotypes. \nNat. Methods 10, 877–879. https:// doi. org/ 10. 1038/ nmeth. 2560 (2013).\n 4. Barlow, I. L. et al. Megapixel camera arrays enable high-resolution animal tracking in multiwell plates. Commun. Biol. 5, 253. \nhttps:// doi. org/ 10. 1038/ s42003- 022- 03206-1 (2022).\n 5. Baek, J.-H., Cosman, P ., Feng, Z., Silver, J. & Schafer, W . R. Using machine vision to analyze and classify Caenorhabditis elegans \nbehavioral phenotypes quantitatively. J. Neurosci. Methods 118, 9–21. https:// doi. org/ 10. 1016/ S0165- 0270(02) 00117-6 (2002).\n 6. Breiman, L., Friedman, J., Olshen, R. & Stone, C. Classification and regression trees. Wadsworth Int. Group 37, 237–251 (1984).\n 7. Swierczek, N. A., Giles, A. C., Rankin, C. H. & Kerr, R. A. High-throughput behavioral analysis in C. elegans. Nat. Methods 8, \n592–598. https:// doi. org/ 10. 1038/ nmeth. 1625 (2011).\n(a) Tracked C. elegans.\n (b) Selected trajectories of interacting C. elegans.\nFigure 5.  Example of tracked C. elegans.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\n 8. Javer, A. et al. An open-source platform for analyzing and sharing worm-behavior data. Nat. Methods  15, 645–646. https:// doi. \norg/ 10. 1038/ s41592- 018- 0112-1 (2018).\n 9. Hebert, L., Ahamed, T., Costa, A. C., O’Shaughnessy, L. & Stephens, G. J. WormPose: Image synthesis and convolutional networks \nfor pose estimation in C. elegans. PLoS Comput. Biol. 17, e1008914. https:// doi. org/ 10. 1371/ journ al. pcbi. 10089 14 (2021).\n 10. He, K., Zhang, X., Ren, S. & Sun, J. Identity mappings in deep residual networks. In Computer Vision—ECCV 2016: 14th European \nConference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, 630–645 (Springer, 2016).\n 11. Wählby, C. et al. An image analysis toolbox for high-throughput C. elegans assays. Nat. Methods  9, 714–716. https:// doi. org/ 10. \n1038/ nmeth. 1984 (2012).\n 12. Stirling, D. R. et al. Cell Profiler 4: Improvements in speed, utility and usability. BMC Bioinform. 22, 433. https:// doi. org/ 10. 1186/ \ns12859- 021- 04344-9 (2021).\n 13. Banerjee, S. C., Khan, K. A. & Sharma, R. Deep-worm-tracker: Deep learning methods for accurate detection and tracking for \nbehavioral studies in C. elegans. Anim. Behav. Cogn.https:// doi. org/ 10. 1101/ 2022. 08. 18. 504475 (2022).\n 14. Jocher, G. YOLOv5 by Ultralytics. https:// doi. org/ 10. 5281/ zenodo. 39085 59 (2020).\n 15. Du, Y ., Song, Y ., Y ang, B. & Zhao, Y . Strongsort: Make deepsort great again. https:// doi. org/ 10. 48550/ ARXIV . 2202. 13514 (2022).\n 16. Fudickar, S., Nustede, E. J., Dreyer, E. & Bornhorst, J. Mask R-CNN based C. elegans detection with a DIY microscope. Biosensors \n11, 257. https:// doi. org/ 10. 3390/ bios1 10802 57 (2021).\n 17. He, K., Gkioxari, G., Dollár, P . & Girshick, R. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, \n2961–2969 (2017).\n 18. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, 770–778 (2016).\n 19. Mais, L., Hirsch, P . & Kainmueller, D. Patchperpix for instance segmentation. In European Conference on Computer Vision, 288–304 \n(Springer, 2020).\n 20. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Confer-\nence on Medical image computing and computer-assisted intervention, 234–241 (Springer, 2015).\n 21. Lalit, M., Tomancak, P . & Jug, F . Embedding-based instance segmentation in microscopy. In Proceedings of the Fourth Conference \non Medical Imaging with Deep Learning, 399–415 (PMLR, 2021).\n 22. Romera, E., Álvarez, J. M., Bergasa, L. M. & Arroyo, R. Erfnet: Efficient residual factorized convnet for real-time semantic seg-\nmentation. IEEE Trans. Intell. Transp. Syst. 19, 263–272. https:// doi. org/ 10. 1109/ TITS. 2017. 27500 80 (2018).\n 23. Redmon, J., Divvala, S., Girshick, R. & Farhadi, A. Y ou only look once: Unified, real-time object detection. In Proceedings of the \nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016).\n 24. Bodla, N., Singh, B., Chellappa, R. & Davis, L. S. Soft-nms–improving object detection with one line of code. In Proceedings of the \nIEEE International Conference on Computer Vision, 5561–5569 (2017).\n 25. Lopes, A. F . C. et al. A C. elegans model for neurodegeneration in Cockayne syndrome. Nucleic Acids Res. 48, 10973–10985. https:// \ndoi. org/ 10. 1093/ nar/ gkaa7 95 (2020).\n 26. Ljosa, V ., Sokolnicki, K. L. & Carpenter, A. E. Annotated high-throughput microscopy image sets for validation. Nat. Methods 9, \n637–637. https:// doi. org/ 10. 1038/ nmeth. 2083 (2012).\n 27. Chen, K. et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition (CVPR) (2019).\n 28. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International \nConference on Computer Vision (ICCV) (2021).\n 29. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference \non Learning Representations, ICLR 2021, Virtual Event, Austria, May 3–7, 2021 (OpenReview.net, 2021).\n 30. Deng, J. et al. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern \nRecognition, 248–255. https:// doi. org/ 10. 1109/ CVPR. 2009. 52068 48 (2009).\n 31. Cai, Z. & Vasconcelos, N. Cascade r-cnn: High quality object detection and instance segmentation. IEEE Trans. Pattern Anal. \nMach. Intell.https:// doi. org/ 10. 1109/ tpami. 2019. 29565 16 (2019).\n 32. Ioffe, S. & Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International \nConference on Machine Learning, 448–456 (PMLR, 2015).\n 33. Wu, Y . & He, K. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV), 3–19 (2018).\n 34. Qiao, S., Wang, H., Liu, C., Shen, W . & Yuille, A. Micro-batch training with batch-channel normalization and weight standardiza-\ntion. arXiv: 1903. 10520 (arXiv preprint) (2019).\n 35. Loshchilov, I. & Hutter, F . Decoupled weight decay regularization. In International Conference on Learning Representations (2018).\n 36. Loshchilov, I. & Hutter, F . SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Repre-\nsentations (2017).\n 37. Goyal, P . et al. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv: 1706. 02677 (arXiv preprint) (2017).\n 38. Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V . & Le, Q. V . Autoaugment: Learning augmentation strategies from data. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2019).\n 39. Chen, K. et al. MMDetection: Open mmlab detection toolbox and benchmark. arXiv: 1906. 07155 (arXiv preprint) (2019).\n 40. Lin, T.-Y . et al. Microsoft coco: Common objects in context. In Computer Vision—ECCV 2014 (eds Fleet, D. et al.) 740–755 \n(Springer, 2014).\n 41. Fontaine, E., Burdick, J. & Barr, A. Automated tracking of multiple C. Elegans. In Conference Proceedings: ... Annual International \nConference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual \nConference2006, 3716–3719. https:// doi. org/ 10. 1109/ IEMBS. 2006. 260657 (2006).\n 42. Alonso, A. & Kirkegaard, J. B. Fast spline detection in high density microscopy data. arXiv: 2301. 04460 (2023).\nAcknowledgements\nWe would like to thank Matthias Rieckher for supplying us the videos of the CSB-1 dataset, as well as Xiao-Liu \nChu for supplying the Mating Dataset video. We thank everyone who helped us annotating the data used in \nthis publication. Maurice Deserno and Katarzyna Bozek were supported by the North Rhine-Westphalia return \nprogram (311-8.03.03.02-147635), BMBF program Junior Group Consortia in Systems Medicine (01ZX1917B) \nand hosted by the Center for Molecular Medicine Cologne. We thank the Regional Computing Center of the \nUniversity of Cologne (RRZK) for providing computing time on the DFG-funded (Funding number: INST \n216/512/1FUGG) High Performance Computing (HPC) system CHEOPS as well as support.\nAuthor contributions\nM.D.: methodology, software, experiments and analysis. K.B.: supervision, data and funding acquisition. M.D. \nand K.B. writing the article. All authors reviewed the manuscript.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:11021  | https://doi.org/10.1038/s41598-023-38213-7\nwww.nature.com/scientificreports/\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.D.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.5850780010223389
    },
    {
      "name": "Artificial intelligence",
      "score": 0.579563558101654
    },
    {
      "name": "Computer science",
      "score": 0.5685142874717712
    },
    {
      "name": "Computer vision",
      "score": 0.5059129595756531
    },
    {
      "name": "Computational biology",
      "score": 0.3877333402633667
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3211858868598938
    },
    {
      "name": "Biology",
      "score": 0.2669025659561157
    }
  ]
}