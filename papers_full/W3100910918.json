{
    "title": "Using Transfer-based Language Models to Detect Hateful and Offensive Language Online",
    "url": "https://openalex.org/W3100910918",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A3105540012",
            "name": "Vebjørn Isaksen",
            "affiliations": [
                "Norwegian University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3739129",
            "name": "Björn Gambäck",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3115903740",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2740168486",
        "https://openalex.org/W2340954483",
        "https://openalex.org/W2741065173",
        "https://openalex.org/W2963793818",
        "https://openalex.org/W2972401993",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2796881724",
        "https://openalex.org/W2956090150",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2963481894",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W2963015912",
        "https://openalex.org/W4287704453",
        "https://openalex.org/W3103061166",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2806872289",
        "https://openalex.org/W2922580172",
        "https://openalex.org/W2890149277",
        "https://openalex.org/W2585712495",
        "https://openalex.org/W2953646920",
        "https://openalex.org/W2962932155",
        "https://openalex.org/W2949952652",
        "https://openalex.org/W2785615365",
        "https://openalex.org/W2995435108",
        "https://openalex.org/W2839648288",
        "https://openalex.org/W2563826943",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2953553271",
        "https://openalex.org/W2962855291",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2473555522",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2953692061",
        "https://openalex.org/W2594902547",
        "https://openalex.org/W2734862619",
        "https://openalex.org/W2760103715",
        "https://openalex.org/W3045733172",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W1521626219",
        "https://openalex.org/W2954346034",
        "https://openalex.org/W2613977835",
        "https://openalex.org/W2889016530",
        "https://openalex.org/W2954086794",
        "https://openalex.org/W3105113320",
        "https://openalex.org/W2954226438",
        "https://openalex.org/W2963532322",
        "https://openalex.org/W2954264738",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3022992164",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2783792814"
    ],
    "abstract": "Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either ‘Hateful’, ‘Normal’ or ‘Offensive’. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.",
    "full_text": "Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 16–27\nOnline, November 20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n16\nUsing Transfer-based Language Models to\nDetect Hateful and Offensive Language Online\nVebjørn Isaksen∗ and Bj¨orn Gamb¨ack\nDepartment of Computer Science\nNorwegian University of Science and Technology\nNO-7491 Trondheim, Norway\nvebjorni@me.com, gamback@ntnu.no\nAbstract\nDistinguishing hate speech from non-hate\noffensive language is challenging, as hate\nspeech not always includes offensive slurs\nand offensive language not always express\nhate. Here, four deep learners based on the\nBidirectional Encoder Representations from\nTransformers (BERT), with either general or\ndomain-speciﬁc language models, were tested\nagainst two datasets containing tweets labelled\nas either ‘Hateful’, ‘Normal’ or ‘Offensive’.\nThe results indicate that the attention-based\nmodels profoundly confuse hate speech with\noffensive and normal language. However, the\npre-trained models outperform state-of-the-art\nresults in terms of accurately predicting the\nhateful instances.\n1 Introduction\nThe majority of the tweets on Twitter or posts on\nFacebook are harmless and often posted purpose-\nfully, but some express hatred towards a targeted\nindividual or minority group and members. These\nposts are intended to be derogatory, humiliating or\ninsulting and are deﬁned as hate speech by David-\nson et al. (2017). Different from offensive lan-\nguage, hate speech is usually expressed towards\ngroup attributes such as religion, ethnic origin, sex-\nual orientation, disability or gender (Founta et al.,\n2018b). Some of the biggest ﬁrms invest heavily\nin tracking abusive language, e.g., automatic detec-\ntion of offensive language in comments (Systrom,\n2017, 2018) or giving a percentage of how likely\na text is to be perceived as toxic.1 However, these\nand other existing tools share a common ﬂaw of\nnot distinguishing between offensive and hateful\nlanguage. One important reason to keep these two\nseparate is that hate speech is considered a felony\nin many countries. The task of separating offensive\n∗Currently at Bekk Consulting AS\n1https://www.perspectiveapi.com\nand hateful language has shown to be demanding;\nhowever, with the recent scientiﬁc breakthroughs\nand the concept of transfer learning, we can take\nhuge steps in the right direction.\nThe paper investigates the effects of transferring\nknowledge from the Bidirectional Encoder Repre-\nsentations from Transformers (BERT; Devlin et al.,\n2019) language model on distinguishing hateful,\noffensive and normal language, by ﬁne-tuning the\npre-trained BERT language model with data con-\ntaining hateful and offensive language, and compar-\ning its performance to the state-of-the-art on two\nwidely used hate speech detection datasets. Those\ndatasets are presented Section 2. Section 3 then\ngives an overview of related work in the ﬁeld of\nhate speech detection. Section 4 describes the im-\nplemented system architecture. Section 5 presents\nthe experiments, including setup and results, while\nSection 6 evaluates and discusses those results. Sec-\ntion 7 concludes and suggests future work.\n2 Data\nMany existing datasets containing hate speech are\npublicly available for use and consist of data from\nseveral sources online, mainly Twitter (Waseem\nand Hovy, 2016; Waseem, 2016; Chatzakou et al.,\n2017; Golbeck et al., 2017; Davidson et al., 2017;\nRoss et al., 2016; ElSherief et al., 2018; Founta\net al., 2018b), while some cover other sources such\nas Fox News comments (Gao and Huang, 2017)\nand sentences from posts on the white supremacist\nonline forum Stormfront (de Gibert et al., 2018).\nAlmost all available datasets are labelled by hu-\nmans,2 which results in different approaches taken\nwhen creating and annotating the datasets. Some re-\nsearchers use expert annotators (Waseem and Hovy,\n2016), others use majority voting among several\n2Except for the 12M tweet SOLID dataset (Rosenthal et al.,\n2020). It is, however, distance-learned based on the manually\nannotated 14k OLID tweet set (Zampieri et al., 2019).\n17\nClass Normal Offensive Hateful\nTweets 4,163 19,190 1,430\nTable 1: The Davidson et al. (2017) dataset, D\nDataset Normal Offensive Hateful Spam\nOriginal 53,790 27,037 4,948 14,024\nAvailable 41,784 14,202 2,941 9,372\nTable 2: The Founta et al. (2018b) dataset, F\namateur annotators on platforms such as Crowd-\nFlower (Davidson et al., 2017). However, the task\nof hate speech detection lacks a shared benchmark\ndataset (Schmidt and Wiegand, 2017) that can be\nused to measure the performance of different ma-\nchine learning models. Further, most annotation\nschemata follow Waseem and Hovy (2016) by split-\nting the data into only two basic classes, either\nhate and none hate or offensive and non-offensive\n(classes that then also often are split, e.g., labelling\nhateful tweets as either sexist or racist). However,\nit is debatable whether those labels are sufﬁcient to\nrepresent hateful and abusive language. In contrast,\na few datasets make the distinction between hateful\nand offensive language, e.g., Davidson et al. (2017)\nand Founta et al. (2018b), which will be used here\nand abbreviated D and F, respectively.\nThe dataset by Davidson et al. (2017) consists of\n24,783 English tweets and their labels along with\nsome information including the number of annota-\ntors. The number of CrowdFlower annotators range\nfrom three to nine, and majority voting was used\nwhen deciding the ﬁnal class for a tweet: “Hate\nSpeech”, “Offensive Language” or “Neither”. The\nlabel distribution can be seen in Table 1.\nThe dataset created by Founta et al. (2018b) con-\ntains almost 100k annotated tweets with four labels,\n“Normal”, “Spam”, “Hateful” and “Abusive”. As\nthe authors only provide tweet IDs for researches\nto retrieve tweets through the Twitter Application\nProgramming Interface (API), some tweets may\nfor several reasons not be retrievable, e.g., a tweet\nor the user account behind a tweet may have been\ndeleted; thus, of the 99,799 provided tweet IDs,\nonly 68,299 tweets were retrieved. The label dis-\ntribution of those compared to the original label\ndistribution for dataset F is shown in Table 2.\n3 Related Work\nNobata et al. (2016) mention some challenges\nwithin hate speech, e.g., that the abusive language\nwith time evolves new slurs and clever ways to\navoid being detected. Hence they performed a lon-\ngitudinal study over one year to see how trained\nmodels react over time, employing n-grams, word\nembeddings, and other linguistic and syntactic fea-\ntures. All features combined yielded the best per-\nforming model; however, looking at individual fea-\ntures, character n-grams performed best, a result\nthat also Waseem and Hovy (2016) reported.\nTransferring knowledge from word embeddings\nto be used as input to neural networks has been a\ncommon technique. Gamb ¨ack and Sikdar (2017)\nexperimented with character n-grams in combi-\nnation with word embeddings from word2vec\nMikolov et al. (2013) in various Convolutional\nNeural Network (CNN) setups, with the best per-\nforming model using transferred knowledge from\nword2vec. Adding character n-grams boosted pre-\ncision, but lowered recall. Badjatiya et al. (2017)\nexperimented with several machine learners and\nneural networks, with the best performer being an\nLong Short-Term Memory (LSTM) with random\nword vectors where the network’s output was used\nas input to a Gradient Boosted Decision Tree. How-\never, their results have shown questionable and dif-\nﬁcult to reproduce (Mishra et al., 2018; Fortuna\net al., 2019). Pavlopoulos et al. (2017a,b) tested\nword embeddings from both GloVe and word2vec\nin an Recurrent Neural Network (RNN), while Pit-\nsilis et al. (2018) utilised an RNN ensemble, al-\nthough without use of word embeddings, but feed-\ning standard vectorized word uni-grams to multiple\nLSTM networks, aggregating the classiﬁcations, to\noutperform the previous state-of-the-art.\nPark and Fung (2017) created a hybrid system\nthat tried to capture features from two input lev-\nels, using two CNNs, one character-based and one\nword-based. Meyer and Gamb¨ack (2019) proposed\nan optimised architecture combining components\nwith CNNs and LSTMs into one system. One part\nof the system used character n-grams as input while\nthe other part used word embeddings. They used\nthe dataset from Waseem and Hovy (2016), obtain-\ning better results than previous solutions. Most of\nthe research discussed above used that dataset (with\nlabels ‘Sexist’, ‘Racist’ or ‘Neither’) or a slightly\nmodiﬁed version (Waseem, 2016).\nThe dataset by Davidson et al. (2017) in con-\ntrast separates hateful language from offensive and\nnormal language, making the task harder. Zhang\net al. (2018) used this dataset and six other, but\n18\nmerged the offensive class with the normal class.\nOn the 2-class hate vs normal language task, they\noutperformed the state-of-the-art on 6 out of 7\ndatasets with a system feeding word embeddings\nfrom word2vec into a CNN to produce input vec-\ntors for an LSTM network with GRU cells perform-\ning the ﬁnal classiﬁcation. Founta et al. (2018a)\nused the same dataset, but kept the offensive sam-\nples separate from the normal ones, thus taking\non the challenge of separating hateful and offen-\nsive language. They ran two networks in parallel,\none RNN with text input and one feed-forward\nnetwork with metadata input, followed by a con-\ncatenation layer and a classiﬁcation layer, perform-\ning slightly below the F1-score 0.900 Davidson\net al. (2017) achieved with a baseline LR model.\nHowever, Kshirsagar et al. (2018) surpassed the\nbaseline using pre-trained word embeddings as in-\nput to multiple Multilayer Perceptron (MLP) layers,\nachieving a total F1-score of 0.924. Still, the F-\nscore increase is due to better performance on the\n‘Normal’ and ‘Offensive’ classes, with the model\nactually performing worse on the ‘Hate’ class.\nThis agrees with Malmasi and Zampieri (2018)\nwho tested several supervised learners and ensem-\nble classiﬁers on the dataset, reporting a notice-\nable difﬁculty of distinguishing hateful language\nfrom profanity. Their extensive results analysis\nshowed that tweets with the highest probability\nof being tagged as hate usually are targeted at a\nspeciﬁc social group, so that contextual and se-\nmantic document features may be required to im-\nprove performance. Gaydhani et al. (2018) in con-\ntrast claimed near-perfect performance, misclassi-\nfying only 0.035% of true hate speech samples on\na combination of datasets from Davidson et al. and\nWaseem (2016) using n-grams as features and feed-\ning the TF-IDF values of these into classiﬁers such\nas Support Vector Machine, Na¨ıve Bayes and Lo-\ngistic Regression. However, analysing their train-\ning and test data3 shows that 74% of the test data\nis either duplicate or in the training data, giving a\nhighly biased test set and questionable results.\nBasile et al. (2019) and Zampieri et al. (2019,\n2020) present ﬁndings from SemEval-2019 Task\n5 and 6 resp. -2020 Task 12, observing that pre-\ntrained attention-based deep learning models were\nused by the top teams in all subtasks. P ´erez and\nLuque (2019) and Indurthi et al. (2019) were the\n3https://github.com/adityagaydhani14/\nToxic-Language-Detection-in-Online-Content\ntop teams in SemEval-2019 Task 5, using ELMo to-\ngether with LSTM networks. ELMo (Embeddings\nfrom Language Model; Peters et al., 2018) uses\na bidirectional Language Model to create deeply\ncontextualised word representations, with unsuper-\nvised pre-training. GPT (Generative Pre-training\nTransformer; Radford et al., 2018, 2019) expanded\nthe amount of text the language model can be\ntrained on by combining the ideas of unsupervised\npre-training (Dai and Le, 2015) and transformers\n(Vaswani et al., 2017) with attention. BERT (De-\nvlin et al., 2019) is a direct descendant of GPT,\nalthough instead of using a stack of transformer de-\ncoders, BERT uses a stack of transformer encoders,\nand while GPT only trains a forward language\nmodel, BERT is bidirectional. With the release\nof two pre-trained language models, BERTBASE\nand BERTLARGE, BERT can be used as a lan-\nguage model for tasks such as hate speech detec-\ntion. Liu et al. (2019) used BERTBASE to deliver\nsome of the best results in SemEval-2019 Task 6,\nwhile several SemEval-2020 tasks saw continu-\nous transformer multitask pre-training (ERNIE 2.0;\nSun et al., 2020) outperforming other solutions.\n4 Architecture\nWord embedding techniques based on bag-of-\nwords contexts, such as word2vec (Mikolov et al.,\n2013), only capture the semantic relations among\nwords (Vashishth et al., 2019), whereas language\nmodels are more complex and can capture the\nmeaning of a word in a sentence, i.e., its context.\nThis work focuses on such language models and\nexplores the effect of transferring knowledge from\na substantial pre-trained language model to a clas-\nsiﬁer predicting hateful and offensive expressions.\n4.1 Preprocessing\nTwitter authors often make use of abbreviations\nand internet slang. Many tweets in addition con-\ntain retweeted content, mentions of other users,\nURLs, hashtags, emojis, etc. As language mod-\nels can capture context between words and prefer\ncomplete sentences, only simple preprocessing was\nused to clean the data. NLTK’s (Bird et al., 2009)\nTweetTokenizer was used to remove URLs,\nnumbers, mentions and ‘RT’ retweet marks. Stop\nwords were not removed to keep as much context\nas possible for the language model to capture.\nHuggingFace’s BertTokenizer was used\nfor text normalisation and punctuation splitting\n19\nas well as WordPiece subword-level tokenisation.\nWords that do not occur in the vocabulary are seg-\nmented into subword units, so there are no out-of-\nvocabulary words.\n4.2 BERT Model Architecture\nBERT’s language models can be pre-trained from\nscratch using only a plain text corpus or ﬁne-\ntuned with a domain-speciﬁc corpus. Although\npre-training is a one-time procedure, it is relatively\nexpensive requiring a large amount of crawled text\nand computational power. However, Devlin et al.\n(2019) released several pre-trained models, two\nof which were used in the experiments: BERT\nBase, Uncased (12 encoder layers with 768 hidden\nunits and 12 attention heads; 110M parameters) and\nBERT Large, Uncased (24-layer, 1024-hidden, 16-\nheads; 340M parameters), that were trained on the\nEnglish Wikipedia and BookCorpus (Zhu et al.,\n2015) for 1M update steps. Both models are low-\nercased and have pre-trained checkpoints that can\neither be trained with more data or ﬁne-tuned with\ntask-speciﬁc data. Both of these approaches were\nimplemented and tested in the experiments. The\nmodels are trained with word sequence length up\nto 512, but this can be shorted when ﬁne-tuning,\nto save substantial memory. Each encoder in the\nstack applies self-attention and then passes the re-\nsults through a simple feed-forward network, be-\nfore handing the output over to the next encoder.\nMost language models pass each input token\nthrough a token embedding layer to achieve a nu-\nmerical representation. BERT solves this by pass-\ning each token through three different embedding\nlayers (token, segment and positional). Each of\nthese three layers converts an input sequence of\ntokens to a vector representation of size (n ,768),\nwhere n is the number of tokens in the input se-\nquence. These three vector representations are\nsummed element-wise to construct a single vec-\ntor used as input for BERT’s encoder stack.\nThe model output is where BERT separates itself\nfrom a traditional transformer: Each token position\nin the input sequence outputs a length 768 hidden\nvector for BERT Base and 1024 for BERT Large.\nEach encoder outputs hidden vectors that can be\nused as contextualised word embeddings that can\nbe fed into an existing model. For the ﬁne-tuning\napproach, only the hidden vectors from the ﬁnal en-\ncoder in the stack are relevant and only the hidden\nvector in the ﬁrst position is used for sentence clas-\nFigure 1: System architecture. The ﬁnal classiﬁer in-\ncludes a feedforward network with one input layer, one\nhidden layer, one output layer, and one softmax layer.\nsiﬁcation. This vector can be used as input to any\nclassiﬁer. Devlin et al. achieved great results using\nonly a single-layer network, but the ﬁnal systems\nused here are slightly modiﬁed with an additional\nlinear layer of size 2048 added to increase the com-\nplexity of the model. (An RNN model was also\ntested, but omitted as learning did not improve.)\nFor ﬁne-tuning, only the number of labels needs\nto be added as a new parameter, 3 and 4 for the\nsystems used here. All BERT parameters and the\nﬁnal classiﬁer network parameters are ﬁne-tuned\njointly to maximise the systems’ predictive capabil-\nities. The logits from the last linear layer are passed\nthrough a softmax layer to calculate the ﬁnal label\nprobabilities. Between BERT’s pooled output and\nthe ﬁrst linear layer, and between the ﬁrst and sec-\nond linear layers, dropout is utilised to regulate\nthe systems to reduce the risk of overﬁtting. In\naddition, cross entropy is used to calculate the clas-\nsiﬁcation error of each sample. To update the whole\nnetwork’s weights iteratively based on the training\ndata, HuggingFace’s version of the Adam optimiser\n(Kingma and Ba, 2017) is used with weight decay\nﬁx, warmup, and linear learning rate decay. Fig-\nure 1 gives an overview of the system architecture\nimplemented for the experiments.\n4.3 Further Language Model Training\nStarting from BERT’s Wikipedia and BookCorpus\ncheckpoint, it is possible to further train the lan-\nguage model with domain-speciﬁc corpora. This\ntechnique of using unlabelled data from the same\n20\ndomain as the target task to train the language\nmodel further using the original pre-training ob-\njective(s) was ﬁrst seen in ULMFiT (Howard and\nRuder, 2018). Since the approach taken here only\nuses two datasets, there are still a lot of datasets\nfrom the target domain available. Remember, the\npre-training only requires the raw text, and so the\nlabels are irrelevant. All available datasets men-\ntioned at the beginning of Section 2, except the\ntwo used for the target task, were collected and\nused to further train BERT on domain data. Fur-\nthermore, BERT’s English vocabulary consists of\n30,522 segmented subword units learned before-\nhand. Some vocabulary entries are placeholders\nthat can be replaced with new words. ElSherief\net al. (2018) created a list of keywords commonly\nused as hate speech, and most of those were placed\nin the unused placeholders when further training\nBERT from its checkpoints.\nOne of BERT’s pre-training objectives is next\nsentence prediction in which the model predicts\nwhether one sentence follows another sentence\nor not. As a result, the input format for further\ntraining BERT is a single ﬁle with untokenised\ntext and one sentence per line. Natural Language\nToolkit (NLTK)’ssent_tokenizer was used\nto split documents into sentences of at least one\nword. Since tweets rarely consist of multiple com-\nplete sentences due to Twitter’s 280 character limit,\nsome tweets were split in the middle to construct\ntwo sentences instead of discarding them.\nOther datasets were formatted more easily, e.g,\nthe Stormfront forum data from de Gibert et al.\n(2018) contained a large folder where each text\nﬁle was a sentence. All text data from the datasets\nwere merged into one ﬁle yielding one large text\nﬁle with nearly 170,000 lines. This ﬁle was then\nused to further train two language models from\nBERT Base and Large checkpoints on the two orig-\ninal pre-training objectives, masked LM and next\nsentence prediction. The output of this process, two\nlanguage models, trained on Wikipedia, BookCor-\npus, and domain data was used in the experiments\nto investigate the effect of further training the lan-\nguage model with domain-speciﬁc data.\n5 Experiments and Results\nThe two original pre-trained language models\nBERT Base and BERT Large from Devlin et al.\n(2019) were tested together with the two language\nmodels (BERT Base* and BERT Large*) further\ntrained with domain-speciﬁc data. Each system’s\nperformance was tested with the two datasets D\n(Davidson et al., 2017) andF (Founta et al., 2018b).\nDataset F annotates tweets as ‘Hateful’, ‘Offensive’\n‘Spam’ or ‘Normal’. When identifying hateful and\noffensive language, the ‘Spam’ class is redundant\nand was omitted. However, to compare to previ-\nous research, experiments with the original 4-class\ndataset F were also carried out.\nAll text data in the experiments were lowercased.\nBoth datasets were split into a training set con-\ntaining 80% of the total samples and a held-out\ntest set containing the remaining 20%, with Scikit-\nlearn’s stratiﬁed splitting function used to ensure\nequal class balance between the sets. The order\nof the training samples was shufﬂed before each\nrun. Cross-validation with multiple folds was not\nimplemented due to framework limitations.\nAll experiments were run on devices with at\nleast 64GB RAM, the amount recommended by\nthe creators of BERT. The two original language\nmodels were pre-trained with a sequence length\nof 512 and batch size 256. The ﬁne-tuned models\nhad a sequence length of 128 and batch size 32.\nAll four language models were trained with the\nAdam optimiser, with the optimal learning rates\nfound to be 3e-5 for the ﬁne-tuning process and\n2e-5 for the classiﬁcation process after an exhaus-\ntive search with parameters suggested by Devlin\net al. (2019). Other parameters shared by the four\nsystems are a dropout probability of 10% on all\nlayers, the number of training epochs which was 3,\nand an evaluation batch size of 8. The ﬁne-tuning\nof the language models took around 3 hours on two\nNvidia V100 GPUs with 32GB RAM each, while\nclassiﬁcation with BERT Base and Large took on\naverage around 1 and 2 hours, respectively.\nSystem performance will be measured by micro\naveraged Precision, Recall, and F1-score, as this\nis more suitable for unbalanced datasets and gives\ndetailed insights into how the models classify each\nsample. The macro averaged total for each metric\nwill also be presented for comparison reasons.\n5.1 Dataset from Davidson et al. (2017)\nDataset D is quite unbalanced with 77% of the\ntweets being annotated as ‘Offensive’ and only 6%\nbeing labelled ‘Hateful’. As seen in Table 3, all\nfour models perform more or less equally in almost\nall metrics, and are able to correctly classify tweets\nas ‘Normal’ and ‘Offensive’ fairly well. BERT\n21\nBERT Model: Base Large Base* Large*\nNormal\nP 0.867 0.889 0.883 0.883\nR 0.906 0.888 0.893 0.888\nF1 0.886 0.888 0.888 0.885\nOffensive\nP 0.941 0.938 0.929 0.932\nR 0.953 0.959 0.965 0.961\nF1 0.947 0.948 0.947 0.946\nHateful\nP 0.497 0.520 0.477 0.460\nR 0.343 0.364 0.213 0.259\nF1 0.406 0.428 0.294 0.331\nMicro avg. F1 0.910 0.913 0.909 0.908\nMacro avg. F1 0.751 0.759 0.725 0.729\nTable 3: Results from BERT experiments on dataset D\nLarge is the model that performs best with a ﬁnal\nmacro averaged F1-score of 0.759, with the other\nthree models not far behind. This is in line with\nDevlin et al. (2019) who found BERT Large out-\nperforming BERT Base across all tasks tested.\nOut of the four models, BERT Large also ob-\ntains the best scores for the ‘Hateful’ class, with\nprecision, recall and F1-score of 0.520, 0.364 and\n0.428, respectively. 52% of the examples the model\npredicted as hateful were correctly classiﬁed. Only\n36% of the total true hateful tweets were classiﬁed\ncorrectly, yielding low recall. The two models with\ngeneral language understanding, BERT Base and\nLarge, outperform the two models with domain-\nspeciﬁc language understanding on the ‘Hateful’\nclass. On this class, BERT Large* obtains a F1-\nscore of 0.331 compared to BERT Large’sF1-score\nof 0.428. This gap in F1-scores is unexpected as\nthe intention of further training the language mod-\nels with domain-speciﬁc data was to increase the\nhateful language understanding.\n5.2 Dataset from Founta et al. (2018b)\nDataset F is nearly three times the size ofD. The la-\nbel distribution is also more balanced with roughly\nhalf of the samples labelled ‘Normal’ and the rest\ndistributed between the other three classes. Al-\nthough only 6% of the tweets are annotated ‘Hate-\nful’, this is a fair representation of the real world\nwhere only a small portion of the online content\nis hate speech. The best scores for each metric\nwere then spread across the four models and there\nwas no clear difference between the models: all\nobtained an F1-score of 0.67. As with dataset D,\nthe models were able to correctly classify tweets\nas ‘Normal’ and ‘Offensive’ quite well while mis-\nclassifying most of the true ‘Hateful’ and ‘Spam’\nBERT Model: Base Large Base* Large*\nNormal\nP 0.956 0.956 0.956 0.957\nR 0.968 0.967 0.967 0.964\nF1 0.962 0.961 0.961 0.960\nOffensive\nP 0.865 0.861 0.860 0.865\nR 0.920 0.926 0.921 0.919\nF1 0.892 0.892 0.889 0.891\nHateful\nP 0.573 0.574 0.531 0.485\nR 0.299 0.264 0.264 0.284\nF1 0.393 0.362 0.353 0.358\nMicro avg. F1 0.923 0.922 0.921 0.919\nMacro avg. F1 0.762 0.756 0.748 0.745\nTable 4: Results from BERT experiments on dataset F\ntweets. The best F1-scores for the ‘Normal’ and\n‘Offensive’ classes were 0.869 and 0.884, respec-\ntively, obtained by BERT Base*, but the other mod-\nels were right behind. The only telling difference\nbetween the models was the scores on the ‘Hateful’\nclass, with BERT Base the clear winner.\nRemoving the ‘Spam’ class from the original\ndataset, we immediately see an increase in the mod-\nels’ scores for all three classes as shown in Table 4.\nAs expected, the increase is most noticeable for the\n‘Normal’ class which previously was highly con-\nfused with the ‘Spam’ class. The increase is less\nnotable for the ‘Hateful’ class although BERT Base\noutperforms the other models by a margin. BERT\nBase is surprisingly the model that performs best\noverall, beating the other three models on nearly ev-\nery metric. Remarkably 97% of the tweets labelled\nas ‘Normal’ are correctly classiﬁed by the model,\nbut only 30% of true hateful samples. Again, the\nmodels seem to recognise true hate speech as less\nhateful than the annotators. The two models trained\nwith domain-speciﬁc data, BERT Base* and BERT\nLarge*, perform worse on the ‘Hateful’ class than\nthe other two models. This is an interesting obser-\nvation as more training with domain-speciﬁc data\nhas shown to increase the performance of models\nin previous solutions.\n6 Evaluation and Discussion\nThe main difference between the two datasets used\nin the experiments is the size and label distribution.\nThe size of dataset F allows for more training sam-\nples than dataset D although systems transferring\nknowledge from pre-trained language models have\nshown that even small datasets can achieve similar\nperformance (Howard and Ruder, 2018). The four\nmodels’ overall performance on datasetsD and F\n22\nare the same despite the fact that the latter dataset\nallows for more language model ﬁne-tuning. The\nlabel distribution in dataset F is more realistic than\ndataset D, where a large portion of the samples is\nlabelled as ‘Offensive’. However, this unbalance of\ndataset D does not seem to affect the models’ per-\nformance noticeably. The reason is probably that\ndataset D contains a sufﬁcient amount of class sam-\nples for the models to learn the other two classes.\nThis ability to learn with a few training examples\nis one of the main advantages of using language\nmodels instead of traditional word embeddings.\n6.1 Language Model Selection\nAlthough datasets without the distinction between\noffensive and hateful language were irrelevant for\ntesting the models in the experiments, they were\nused as unlabelled data to further pre-train two\nBERT language models. This additional training\nis intended to give the language models domain-\nspeciﬁc language understanding and has shown to\nincrease the overall performance in other tasks (De-\nvlin et al., 2019). However, the results obtained\nfrom the experiments show that the two models\nwith domain-speciﬁc language understanding per-\nformed worse or equal to the language models with\ngeneral language understanding. As we can see\nin Table 3, the worst performance of the two ex-\ntended BERT models was on dataset D. BERT\nBase* and Large* obtained macro-averaged F1-\nscores of 0.725 and 0.729, respectively, while the\noriginal BERT models obtained F1-scores of 0.751\nfor BERT Base and 0.759 for BERT Large. The dif-\nference between these scores is a result of the mod-\nels’ performance on the ‘Hateful’ class as the per-\nformance on the ‘Normal’ and ‘Offensive’ classes\nare near identical for all four models. BERT Large\noutperforms the other three models on the ‘Hateful’\nclass with a F1-score of 0.428. This is in line with\nDevlin et al. (2019) who found that BERT Large\noutperformed BERT Base on several other tasks.\nHowever, this is not the case for the results ob-\ntained by BERT Large on dataset F. Looking at\nTable 4, we observe that the smaller model BERT\nBase outperforms BERT Large on nearly every\nmetric. The most compelling difference can again\nbe seen in the ‘Hateful’ row, where BERT Base\nachieved an F1-score of 0.393 compared to BERT\nLarge’sF1-score of 0.362, mainly as a result of\nbetter recall obtained by BERT Base.\nSurprisingly, there is no telling difference when\ncomparing the two models with general language\nunderstanding to the two models with domain-\nspeciﬁc language understanding. Further train-\ning with large domain-speciﬁc corpora is expected\nto be beneﬁcial and increase the performance on\ndownstream tasks like hate speech detection. How-\never, the results from the experiments do not reﬂect\nthis assumption, and it seems like all four models\nare able to capture similar features, thus perform-\ning equally well. Next sentence prediction is one\nof BERT’s two pre-training objectives. So in order\nto further pre-train the language model, it is nec-\nessary to obtain documents containing at least two\nsentences. This became a limitation, as the domain-\nspeciﬁc data used in the experiments mostly consist\nof tweets, that often contain only a single sentence\nand omitting every single-sentence tweet would\nlead to a much smaller training corpus. In order\nto include single-sentence tweets in the training\ncorpus, they were split at the middle. This is not\noptimal and may be one of the reasons why BERT\nBase* and Large* did not perform as expected.\n6.2 Error Analysis\nGenerally, the results from each dataset indicate\nthat it is hard to separate hateful language from\noffensive and normal language. This was also the\nkey ﬁnding stated by Malmasi and Zampieri (2018)\nand Davidson et al. (2017) when testing their mod-\nels’ performance on datasetD. For dataset D, most\nof the annotated hateful samples are confused with\nthe ‘Offensive’ class, and this may be due to the\nskewed dataset where the ‘Offensive’ samples dom-\ninate. With dataset F, there is roughly an equal\ndistribution of misclassiﬁcations between the ‘Of-\nfensive’ and ‘Normal’ class. This indicates that\nneither of the tested models using features from\nthe pre-trained language model is capable of dis-\ntinguishing hateful language from offensive and\nneutral language with acceptable accuracy.\nTo investigate BERT Base’s predictions on\ndataset F deeper, some correctly and incorrectly\nclassiﬁed instances were sampled and analysed.\nThe model tends to predict instances containing\nclear racist or homophobic slurs as hate speech,\nwhile obvious hate speech appears more straight-\nforward for the model to understand and accurately\npredict. Several instances annotated as ‘Hateful’,\nbut predicted as ‘Normal’ or Offensive’ by the\nmodel do not appear to be clear hate speech and\nare perhaps mislabelled by the human coders and\n23\nSystem P R F 1\nBERT Large 0.91 0.91 0.90\nDavidson et al. (2017) 0.91 0.90 0.90\nFounta et al. (2018a) 0.89 0.89 0.89\nKshirsagar et al. (2018) – – 0.92\nTable 5: Dataset D comparison (weighted averages)\ncorrectly predicted by the model. The text “ISIS\nmessage calls Trump ’foolish idiot”’ was found\nfour times in the original dataset with different\nauthors, being annotated twice as ‘Hateful’ and\ntwice as ‘Offensive’, with the model predicting the\nhuman-chosen label on only one of the instances.\nAs stated by Chatzakou et al. (2017), annotation\nis even hard for humans and this is an example of\nthe gold standard not being perfect even though the\nFounta et al. dataset was thoroughly constructed.\n6.3 Comparison to State-of-the-Art\nTable 5 shows the results obtained on dataset D\nby BERT Large compared to previous results. Al-\nthough the dataset is widely used, some researchers\n(e.g., Zhang et al., 2018) chose to merge the ‘Offen-\nsive’ and ‘Normal’ classes into one non-hate class;\nmaking them not comparable to the results carried\nout in the experiments. All four systems in Table 5\nperform equally well with F1-scores around 0.90.\nBERT Large is outperformed by Kshirsagar et al.\n(2018)’s Transformed Word Embedding Model\n(TWEM). BERT Large outperforms the solution\nfrom Founta et al. (2018a) and obtains similar re-\nsults as the baseline from Davidson et al. (2017).\nLee et al. (2018) tested several machine learn-\ning algorithms on dataset F intending to create a\nbaseline for this dataset. Table 6 shows that the\ntwo BERT models and Lee et al.’s word-based\nRNN-LTC model perform similarly on this dataset.\nHowever, BERT Base* achieves an F1-score of\n0.361 on the ‘Hateful’ class, compared to the RNN-\nLTC model’s F1-score of 0.302. This indicates\nthat BERT Base* is better at separating hateful lan-\nguage from the other types of language. RNN-LTC\noutperformed BERT Base* on the ‘Spam’ class\nresulting in the similar total average scores.\nThe experimental results on dataset F without\nthe ‘Spam’ class were compared to three baseline\nsystems, since no comparable research was found.\nThe macro-averaged scores are shown in Table 7.\nOut of the four tested models, BERT Base was the\nbest performing with an F1-score of 0.76. Again,\nBERT Base’s performance on the “Hateful” class\nSystem P R F 1\nBERT Base* 0.800 0.812 0.806\nBERT Large* 0.802 0.809 0.805\nLee et al. (2018) CNNw 0.789 0.808 0.783\nLee et al. (2018) RNN-LTCw 0.804 0.815 0.805\nTable 6: Dataset F comparison (weighted averages)\nSystem P R F 1\nBERT Base 0.80 0.73 0.76\nNa¨ıve Bayes 0.63 0.63 0.63\nSupport Vector Machine 0.87 0.65 0.74\nLogistic Regression 0.80 0.69 0.74\nTable 7: Dataset F without “Spam” (macro averages)\nis compellingly better than the best performing\nLogistic Regression model. BERT Base obtain an\nF1-score of 0.393 while the LR model achieves an\nF1-score of 0.310. The improved performance on\nthe “Hateful” class on both version of dataset F\nimplies that models transferring knowledge from\npre-trained language models are able to distinguish\nthe nuances of abusive language more accurately.\nModel selection is important when creating a\nhate speech predictor; however, Gr ¨ondahl et al.\n(2018) argue that model architecture is less im-\nportant than the type of data and labelling criteria.\nThey found that the tested models, which ranged\nfrom simple Logistic Regression to more complex\nLSTM, performed equally well when recreating\nseveral state-of-the-art solutions. Gr¨ondahl et al.’s\nresults are consistent with the investigations con-\nducted during the experiments, where changes in\nthe ﬁnal classiﬁer’s complexity did not reﬂect any\nchanges in the results.\n7 Conclusion and Future Work\nTo explore the effects of applying language models\nto the downstream task of hate speech detection,\nfour systems based on the BERT language models\nwere implemented and tested on two datasets anno-\ntated both for hateful and offensive language. Two\nof the systems were further pre-trained with unla-\nbelled domain-speciﬁc data. However, the results\ndid not reﬂect any notable improvement with the\nextended language models.\nAll four models achieved F1-scores close to or\nabove state-of-the-art solutions on both datasets,\nand their ability to correctly distinguish hate speech\nfrom offensive and ordinary language was con-\nsiderably better than the compared solutions, but\n24\nthe scores on the ‘Hateful’ class are not sufﬁcient\nenough to bring the systems into practical use, as\nhateful expressions would pass through the system\nor more benign cases would be incorrectly cen-\nsored. Still, language models bring a considerable\npotential to understanding all the nuances of hate-\nful utterances, and further exploration of how to\nmost effectively train and transfer knowledge from\nthem is necessary.\nThe models used in the experiments were all\npre-trained on the English Wikipedia and Book-\nCorpus to obtain general language understanding.\nTypically, the language that appears in Wikipedia\narticles and books are somewhat domain neutral\nand formal. This language may be too different\nfrom the hate speech domain in terms of words\nand sentences. Therefore, it may be beneﬁcial to\ncollect documents from hate speech datasets and\ncreate one large corpus, which can be used as input\ndata to pre-train BERT’s encoders from scratch.\nA problem with BERT is the vast number of\nparameters that need to be set, leading to mem-\nory problems and long training times. However,\nthe usage of transformers for language processing\nis a fast-moving ﬁeld, so several ideas and strate-\ngies have lately been introduced to improve on the\noriginal BERT setup. One of those — such as AL-\nBERT, ‘A lite BERT’ (Lan et al., 2020); GPT-3,\n‘Generative Pre-trained Transformer’ (Brown et al.,\n2020); continuous pre-training (‘ERNIE 2.0’; Sun\net al., 2020); transformers for longer sequences\n(‘BigBird’; Zaheer et al., 2020); or layerwise adap-\ntive large batch optimisation (‘LAMB’; You et al.,\n2020) — could be tested on the task.\nLan et al. (2020)’s ALBERT can drastically re-\nduce the number of parameters and help solve mem-\nory problems and reduce training times. Zaheer\net al. (2020)’s ‘BigBird’, with its sparse attention\nmechanism, allows for longer input sequences than\nBERT and is suitable for tasks where the datasets in-\nclude longer documents. You et al. (2020) utilised\nlarge batch stochastic optimisation methods to re-\nduce the training time of BERT remarkably.\nAs describe in Section 4.3, each tweet in the\ntraining set was split into two for the next sen-\ntence prediction task BERT is performing during\npre-training. This was done because tweets rarely\ncontain two full sentences. However, this strategy\ncan lead to some loss of linguistic information and\nit may be better to just skip next sentence predic-\ntion during training and only perform the masked\nlanguage model task.\nReferences\nPinkesh Badjatiya, Shashank Gupta, Manish Gupta,\nand Vasudeva Varma. 2017. Deep learning for hate\nspeech detection in tweets. In Proceedings of the\n26th International Conference on World Wide Web\nCompanion, pages 759–760, Perth, Australia. In-\nternational World Wide Web Conferences Steering\nCommittee.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019. SemEval-2019 task 5: Multilingual\ndetection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n54–63, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nSteven Bird, Ewan Klein, and Edward Loper.\n2009. Natural Language Processing with Python .\nO’Reilly Media.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. CoRR, abs/2005.14165.\nDespoina Chatzakou, Nicolas Kourtellis, Jeremy\nBlackburn, Emiliano De Cristofaro, Gianluca\nStringhini, and Athena Vakali. 2017. Mean birds:\nDetecting aggression and bullying on Twitter. In\nProceedings of the 2017 ACM on Web Science Con-\nference, pages 13–22, Troy, New York, USA. Asso-\nciation for Computing Machinery.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In Proceedings of\nthe 28th International Conference on Neural Infor-\nmation Processing Systems, volume 2, pages 3079–\n3087, Montr ´eal, Qu ´ebec, Canada. Curran Asso-\nciates, Inc.\nThomas Davidson, Dana Warmsley, Michael Macy,\nand Ingmar Weber. 2017. Automated hate speech\ndetection and the problem of offensive language.\nIn Proceedings of the Eleventh International Con-\nference on Web and Social Media , pages 512–515,\nMontr´eal, Qu´ebec, Canada. AAAI Press.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\n25\nMai ElSherief, Shirin Nilizadeh, Dana Nguyen, Gio-\nvanni Vigna, and Elizabeth Belding. 2018. Peer to\npeer hate: Hate speech instigators and their targets.\nIn Twelfth International Conference on Web and So-\ncial Media, pages 52–61, Stanford, California, USA.\nAAAI Press.\nPaula Fortuna, Juan Soler-Company, and S´ergio Nunes.\n2019. Stop PropagHate at SemEval-2019 tasks 5\nand 6: Are abusive language classiﬁcation results\nreproducible? In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n745–752, Minneapolis, Minnesota, USA. Associa-\ntion for Computational Linguistics.\nAntigoni Maria Founta, Despoina Chatzakou, Nicolas\nKourtellis, Jeremy Blackburn, Athena Vakali, and Il-\nias Leontiadis. 2018a. A uniﬁed deep learning archi-\ntecture for abuse detection. CoRR, abs/1802.00385.\nAntigoni Maria Founta, Constantinos Djouvas, De-\nspoina Chatzakou, Ilias Leontiadis, Jeremy Black-\nburn, Gianluca Stringhini, Athena Vakali, Michael\nSirivianos, and Nicolas Kourtellis. 2018b. Large\nscale crowdsourcing and characterization of Twit-\nter abusive behavior. In Twelfth International Con-\nference on Web and Social Media , pages 491–500,\nStanford, California, USA. AAAI Press.\nBj¨orn Gamb ¨ack and Utpal Kumar Sikdar. 2017. Us-\ning convolutional neural networks to classify hate-\nspeech. In Proceedings of the First Workshop on\nAbusive Language Online, pages 85–90, Vancouver,\nBritish Columbia, Canada. Association for Compu-\ntational Linguistics.\nLei Gao and Ruihong Huang. 2017. Detecting on-\nline hate speech using context aware models. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing, RANLP\n2017, pages 260–266, Varna, Bulgaria. INCOMA\nLtd.\nAditya Gaydhani, Vikrant Doma, Shrikant Kendre, and\nLaxmi Bhagwat. 2018. Detecting hate speech and\noffensive language on Twitter using machine learn-\ning: An N-gram and TFIDF based approach. CoRR,\nabs/1809.08651.\nOna de Gibert, Naiara Perez, Aitor Garc´ıa-Pablos, and\nMontse Cuadros. 2018. Hate speech dataset from\na white supremacy forum. In Proceedings of the\n2nd Workshop on Abusive Language Online (ALW2),\npages 11–20, Brussels, Belgium. Association for\nComputational Linguistics.\nJennifer Golbeck, Zahra Ashktorab, Rashad O. Banjo,\nAlexandra Berlinger, Siddharth Bhagwan, Cody\nBuntain, Paul Cheakalos, Alicia A. Geller, Quint\nGergory, Rajesh Kumar Gnanasekaran, Raja Ra-\njan Gunasekaran, Kelly M. Hoffman, Jenny Hot-\ntle, Vichita Jienjitlert, Shivika Khare, Ryan Lau,\nMarianna J. Martindale, Shalmali Naik, Heather L.\nNixon, Piyush Ramachandran, Kristine M. Rogers,\nLisa Rogers, Meghna Sardana Sarin, Gaurav Sha-\nhane, Jayanee Thanki, Priyanka Vengataraman, Zi-\njian Wan, and Derek Michael Wu. 2017. A large\nlabeled corpus for online harassment research. In\nProceedings of the 2017 ACM on Web Science Con-\nference, pages 229–233, Troy, New York, USA. As-\nsociation for Computing Machinery.\nTommi Gr ¨ondahl, Luca Pajola, Mika Juuti, Mauro\nConti, and N. Asokan. 2018. All you need is “love”:\nEvading hate speech detection. In Proceedings of\nthe 11th ACM Workshop on Artiﬁcial Intelligence\nand Security, AISec ’18, pages 2–12, Toronto, On-\ntario, Canada. Association for Computing Machin-\nery.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\nCoRR, abs/1801.06146.\nVijayasaradhi Indurthi, Bakhtiyar Syed, Manish Shri-\nvastava, Nikhil Chakravartula, Manish Gupta, and\nVasudeva Varma. 2019. FERMI at SemEval-2019\ntask 5: Using sentence embeddings to identify hate\nspeech against immigrants and women in twitter.\nIn Proceedings of the 13th International Workshop\non Semantic Evaluation , pages 70–74, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nRohan Kshirsagar, Tyrus Cukuvac, Kathy McKeown,\nand Susan McGregor. 2018. Predictive embeddings\nfor hate speech detection on Twitter. InProceedings\nof the 2nd Workshop on Abusive Language Online\n(ALW2), pages 26–32, Brussels, Belgium. Associa-\ntion for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Interna-\ntional Conference on Learning Representations, Ad-\ndis Ababa, Ethiopia. OpenReview.net.\nYounghun Lee, Seunghyun Yoon, and Kyomin Jung.\n2018. Comparative studies of detecting abusive lan-\nguage on twitter. In Proceedings of the 2nd Work-\nshop on Abusive Language Online (ALW2) , pages\n101–106, Brussels, Belgium. Association for Com-\nputational Linguistics.\nPing Liu, Wen Li, and Liang Zou. 2019. NULI at\nSemEval-2019 task 6: Transfer learning for offen-\nsive language detection using bidirectional trans-\nformers. In Proceedings of the 13th Interna-\ntional Workshop on Semantic Evaluation, pages 87–\n91, Minneapolis, Minnesota, USA. Association for\nComputational Linguistics.\nShervin Malmasi and Marcos Zampieri. 2018. Chal-\nlenges in discriminating profanity from hate speech.\nJournal of Experimental & Theoretical Artiﬁcial In-\ntelligence, 30(2):187–202.\n26\nJohannes Skjeggestad Meyer and Bj ¨orn Gamb ¨ack.\n2019. A platform agnostic dual-strand hate speech\ndetector. In Proceedings of the Third Workshop on\nAbusive Language Online, pages 146–156, Florence,\nItaly. Association for Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their composition-\nality. In C. J. C. Burges, L. Bottou, M. Welling,\nZ. Ghahramani, and K. Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems\n26, pages 3111–3119. Curran Associates, Inc.\nPushkar Mishra, Marco Del Tredici, Helen Yan-\nnakoudakis, and Ekaterina Shutova. 2018. Author\nproﬁling for abuse detection. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1088–1098, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nChikashi Nobata, Joel Tetreault, Achint Thomas,\nYashar Mehdad, and Yi Chang. 2016. Abusive lan-\nguage detection in online user content. In Proceed-\nings of the 25th International Conference on World\nWide Web, WWW ’16, pages 145–153, Republic and\nCanton of Geneva, Switzerland. International World\nWide Web Conferences Steering Committee.\nJi Ho Park and Pascale Fung. 2017. One-step and two-\nstep classiﬁcation for abusive language detection on\nTwitter. In Proceedings of the First Workshop on\nAbusive Language Online, pages 41–45, Vancouver,\nBritish Columbia, Canada. Association for Compu-\ntational Linguistics.\nJohn Pavlopoulos, Prodromos Malakasiotis, and Ion\nAndroutsopoulos. 2017a. Deep learning for user\ncomment moderation. In Proceedings of the First\nWorkshop on Abusive Language Online , pages 25–\n35, Vancouver, BC, Canada. Association for Com-\nputational Linguistics.\nJohn Pavlopoulos, Prodromos Malakasiotis, and Ion\nAndroutsopoulos. 2017b. Deeper attention to abu-\nsive user content moderation. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1125–1135, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nJuan Manuel P ´erez and Franco M. Luque. 2019. Ata-\nlaya at SemEval 2019 task 5: Robust embeddings for\ntweet classiﬁcation. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n64–69, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nGeorgios K. Pitsilis, Heri Ramampiaro, and Helge\nLangseth. 2018. Detecting offensive language in\ntweets using deep learning. Applied Intelligence ,\n48(12):4730–4742.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nSara Rosenthal, Pepa Atanasova, Georgi Karadzhov,\nMarcos Zampieri, and Preslav Nakov. 2020. A large-\nscale semi-supervised dataset for offensive language\nidentiﬁcation. CoRR, abs/2004.14454.\nBj¨orn Ross, Michael Rist, Guillermo Carbonell, Ben-\njamin Cabrera, Nils Kurowsky, and Michael Wo-\njatzki. 2016. Measuring the reliability of hate\nspeech annotations: The case of the European\nrefugee crisis. In Proceedings of the 3rd Workshop\non Natural Language Processing for Computer Me-\ndiated Communication , pages 6–9, Bochum, Ger-\nmany. Bochumer Linguistische Arbeitsberichte.\nAnna Schmidt and Michael Wiegand. 2017. A survey\non hate speech detection using natural language pro-\ncessing. In Proceedings of the Fifth International\nWorkshop on Natural Language Processing for So-\ncial Media , pages 1–10, Valencia, Spain. Associa-\ntion for Computational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020.\nERNIE 2.0: A continual pre-training framework for\nlanguage understanding. In 34th AAAI Conference\non Artiﬁcial Intelligence , pages 8968–8975, New\nYork, New York, USA. AAAI.\nKevin Systrom. 2017. Keeping Instagram a safe place\nfor self-expression. Instagram.com.\nKevin Systrom. 2018. Protecting our community from\nbullying comments. Instagram.com.\nShikhar Vashishth, Manik Bhandari, Prateek Yadav,\nPiyush Rai, Chiranjib Bhattacharyya, and Partha\nTalukdar. 2019. Incorporating syntactic and seman-\ntic information in word embeddings using graph con-\nvolutional networks. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3308–3318, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008, Long Beach,\nCalifornia, USA.\n27\nZeerak Waseem. 2016. Are you a racist or am I seeing\nthings? Annotator inﬂuence on hate speech detec-\ntion on Twitter. In Proceedings of the First Work-\nshop on NLP and Computational Social Science ,\npages 138–142, Austin, Texas, USA. Association for\nComputational Linguistics.\nZeerak Waseem and Dirk Hovy. 2016. Hateful sym-\nbols or hateful people? Predictive features for hate\nspeech detection on Twitter. In Proceedings of the\nNAACL Student Research Workshop , pages 88–93,\nSan Diego, California, USA. Association for Com-\nputational Linguistics.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learn-\ning: Training BERT in 76 minutes. In 8th Interna-\ntional Conference on Learning Representations, Ad-\ndis Ababa, Ethiopia. OpenReview.net.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big Bird: Transformers for\nlonger sequences. CoRR, abs/2007.14062.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 task 6: Identifying and\ncategorizing offensive language in social media\n(OffensEval). In Proceedings of the 13th Interna-\ntional Workshop on Semantic Evaluation, pages 75–\n86, Minneapolis, Minnesota, USA. Association for\nComputational Linguistics.\nMarcos Zampieri, Preslav Nakov, Sara Rosenthal, Pepa\nAtanasova, Georgi Karadzhov, Hamdy Mubarak,\nLeon Derczynski, Zeses Pitenis, and C ¸ a˘grı C ¸¨oltekin.\n2020. SemEval-2020 Task 12: Multilingual offen-\nsive language identiﬁcation in social media (Offens-\nEval 2020). CoRR, abs/2006.07235.\nZiqi Zhang, David Robinson, and Jonathan Tepper.\n2018. Detecting hate speech on Twitter using a\nconvolution-GRU based deep neural network. In\nThe Semantic Web: 15th European Semantic Web\nConference, pages 745–760, Cham, Switzerland.\nSpringer.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE In-\nternational Conference on Computer Vision , pages\n19–27, Los Alamitos, California, USA. IEEE Com-\nputer Society."
}