{
  "title": "Feedbackâ€based selfâ€learning in largeâ€scale conversational AI agents",
  "url": "https://openalex.org/W4320803111",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2985333450",
      "name": "Pragaash Ponnusamy",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A1967149857",
      "name": "Alireza Roshan Ghias",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2097286658",
      "name": "Yi Yi",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2042830454",
      "name": "Benjamin Yao",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2900894694",
      "name": "Chenlei Guo",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A116639046",
      "name": "Ruhi Sarikaya",
      "affiliations": [
        "Amazon (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2985333450",
      "name": "Pragaash Ponnusamy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967149857",
      "name": "Alireza Roshan Ghias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097286658",
      "name": "Yi Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042830454",
      "name": "Benjamin Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900894694",
      "name": "Chenlei Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A116639046",
      "name": "Ruhi Sarikaya",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2011034758",
    "https://openalex.org/W2080801582",
    "https://openalex.org/W3088653102",
    "https://openalex.org/W2743027853",
    "https://openalex.org/W4238097273",
    "https://openalex.org/W2584094115",
    "https://openalex.org/W2112420033",
    "https://openalex.org/W2046932483",
    "https://openalex.org/W3147409906",
    "https://openalex.org/W2572102653",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2125288949",
    "https://openalex.org/W4234228486",
    "https://openalex.org/W2215991614",
    "https://openalex.org/W2542459869",
    "https://openalex.org/W2053552153",
    "https://openalex.org/W2912500072",
    "https://openalex.org/W2606098075"
  ],
  "abstract": "Abstract Today, most of the largeâ€scale conversational AI agents such as Alexa, Siri, or Google Assistant are built using manually annotated data to train the different components of the system including automatic speech recognition (ASR), natural language understanding (NLU), and entity resolution (ER). Typically, the accuracy of the machine learning models in these components are improved by manually transcribing and annotating data. As the scope of these systems increase to cover more scenarios and domains, manual annotation to improve the accuracy of these components becomes prohibitively costly and time consuming. In this paper, we propose a system that leverages customer/system interaction feedback signals to automate learning without any manual annotation. Users of these systems tend to modify a previous query in hopes of fixing an error in the previous turn to get the right results. These reformulations, which are often preceded by defective experiences caused by either errors in ASR, NLU, ER, or the application. In some cases, users may not properly formulate their requests (e.g., providing partial title of a song), but gleaning across a wider pool of users and sessions reveals the underlying recurrent patterns. Our proposed selfâ€learning system automatically detects the errors, generates reformulations, and deploys fixes to the runtime system to correct different types of errors occurring in different components of the system. In particular, we propose leveraging an absorbing Markov Chain model as a collaborative filtering mechanism in a novel attempt to mine these patterns, and coupling it with a guardrail rewrite selection mechanism that reactively evaluates these fixes using feedback friction data. We show that our approach is highly scalable, and able to learn reformulations that reduce Alexaâ€user errors by pooling anonymized data across millions of customers. The proposed selfâ€learning system achieves a winâ€loss ratio of 11.8 and effectively reduces the defect rate by more than 30 percent on utterance level reformulations in our production A/B tests. To the best of our knowledge, this is the first selfâ€learning largeâ€scale conversational AI system in production.",
  "full_text": "DOI:10.1609/aaai.12025\nSPECIAL TOPIC ARTICLE\nFeedback-basedself-learninginlarge-scaleconversational\nAIagents\nPragaashPonnusamy AlirezaRoshanGhias YiYi BenjaminYao\nChenleiGuo RuhiSarikaya\nAmazonAlexa\nCorrespondence\nChenleiGuo,AmazonAlexa.\nEmail:chenlei.guo@gmail.com\nAbstract\nToday, most of the large-scale conversational AI agents such as Alexa, Siri, or\nGoogleAssistantarebuiltusingmanuallyannotateddatatotrainthedifferent\ncomponents of the system including automatic speech recognition (ASR), nat-\nural language understanding (NLU), and entity resolution (ER). Typically, the\naccuracy of the machine learning models in these components are improved\nby manually transcribing and annotating data. As the scope of these systems\nincrease to cover more scenarios and domains, manual annotation to improve\nthe accuracy of these components becomes prohibitively costly and time con-\nsuming.Inthispaper,weproposeasystemthatleveragescustomer/systeminter-\naction feedback signals to automate learning without any manual annotation.\nUsers of these systems tend to modify a previous query in hopes of fixing an\nerror in the previous turn to get the right results. These reformulations, which\nareoftenprecededbydefectiveexperiencescausedbyeithererrorsinASR,NLU,\nER, or the application. In some cases, users may not properly formulate their\nrequests(e.g.,providingpartialtitleofasong),butgleaningacrossawiderpool\nof users and sessions reveals the underlying recurrent patterns. Our proposed\nself-learningsystemautomaticallydetectstheerrors,generatesreformulations,\nanddeploysfixestotheruntimesystemtocorrectdifferenttypesoferrorsoccur-\nringindifferentcomponentsofthesystem.Inparticular,weproposeleveraging\nan absorbing Markov Chain model as a collaborative filtering mechanism in a\nnovel attempt to mine these patterns, and coupling it with a guardrail rewrite\nselectionmechanismthatreactivelyevaluatesthesefixesusingfeedbackfriction\ndata.Weshowthatourapproachishighlyscalable,andabletolearnreformula-\ntionsthatreduceAlexa-usererrorsbypoolinganonymizeddataacrossmillions\nofcustomers.Theproposedself-learningsystemachievesawin-lossratioof11.8\nandeffectivelyreducesthedefectratebymorethan30percentonutterancelevel\nreformulationsinourproductionA/Btests.Tothebestofourknowledge,thisis\nthefirstself-learninglarge-scaleconversationalAIsysteminproduction.\nAIMagazine. 2021;42:43â€“56. Â©2021,AssociationfortheAdvancementofArtificialIntelligence.Allrightsreserved. 43\n44 AIMAGAZINE\nINTRODUCTION\nLarge-scaleconversationalAIagents(2017)suchasAlexa,\nSiri, and Google Assistant are getting more and more\nprevalent,openingupinnewdomainsandtakingupnew\ntaskstohelpusersacrosstheglobe.Onekeyconsideration\nin designing such systems is how they can be improved\novertimeatthatscale.Usersinteractingwiththeseagents\nexperience frictions due to various reasons: (1) automatic\nspeech recognition (ASR) errors, such as â€œplay maj and\ndragonsâ€ (should be â€œplay imagine dragonsâ€), (2) natural\nlanguageunderstanding(NLU)errors,suchasâ€œ donâ€™tplay\nthissongagainskipâ€ (Alexa would understand if it is for-\nmulatedasâ€œthumbsdownthissong â€),andevenusererrors,\nsuchasâ€œ playbazziangelâ€(itshouldhavebeenâ€œplaybeau-\ntifulbybazziâ€).Itgoeswithoutsayingthatfixingthesefric-\ntions help users to have a more seamless experience and\nengagemorewiththeAIagents.\nOne common method to address frictions is to gather\nthese use cases and fix them manually using rules and\nfinitestatetransducers(FST)astheyareoftenthecasein\nspeechrecognitionsystems(2002).Thisofcourseisalabo-\nrious technique which is: (1) not scalable at Alexa scale,\n(2)pronetoerror,and(3)gettingstaleandevendefective\novertime.Anotherapproachcouldbetoidentifythesefric-\ntions,askannotatorstocomeupwiththecorrectformof\nquery,andthenupdateASRandNLUmodelstosolvethese\nproblems. This is also: (1) not a scalable solution, since it\nneedsalotofannotations,and(2)itisexpensiveandtime\nconsumingtoupdatethosemodels.Instead,wehavetaken\na â€œquery rewritingâ€ approach to solve customer frictions,\nmeaningthatwhennecessary,wereformulateacustomerâ€™s\nquery such that it conveys the same meaning/intent, and\nis actionable (i.e., interpretable) by Alexaâ€™s existing NLU\nsystems.\nInmotivatingourapproach,considertheexampleutter-\nance, â€œplay maj and dragons.â€ Now, without reformula-\ntion, Alexa would inevitably come up with the response,\nâ€œSorry,Icouldnâ€™tfindmajanddragons .â€Somecustomers\ngiveupatthispoint,whileothersmaytryenunciatingbet-\nterforAlexatounderstandthem:â€œ playimaginedragons.â€\nAlso note that there might be other customers who give\nupandchangethenextquerytoanotherintent,forexam-\nple: â€œplaypopmusic.â€ Here, frictions evidently cause dis-\nsatisfactionwithdifferentcustomersreactingdifferentlyto\nthem. However, quite clearly there are good rephrases by\nsomecustomersamongalltheseinteractions,whichbeck-\nons the question â€“ how can we identify and extract them\ntosolvecustomerfrictions?\nWe propose using a Markov-based collaborative filter-\ning approach to identify rewrites that lead to success-\nful customer interactions. We go on to discuss the the-\nory and implementation of the idea, as well as show that\nthis method is highly scalable and effective in signifi-\ncantly reducing customer frictions. We also discuss how\nthisapproachwasdeployedintocustomer-facingproduc-\ntion and what are some of the challenges and benefits of\nsuchapproach.\nRELATEDWORK\nCollaborativefilteringhasbeenusedextensivelyinrecom-\nmendersystems.Inamoregeneralsense,collaborativefil-\nteringcanbeviewedasamethodofminingpatternsfrom\nvariousagents(mostcommonly,people),inordertohelp\neachotherout(2001).Markovchainshavebeenusedprevi-\nouslyincollaborativefilteringapplicationstorecommend\ncourseenrollment(2016),personalizedrecommendersys-\ntems(2012),andwebrecommendation(2012).\nStudieshaveshownthatMarkovprocessescanbeused\ntoexplaintheuserwebquerybehavior(2005),andMarkov\nchains have since been used successfully for web query\nreformulationviaabsorbingrandomwalk(2015),andmod-\neling query utility (2012). We here present a new method\nfor query reformulation using Markov chain that is both\nhighly scalable and interpretable due to intuitive defini-\ntions of transition probabilities. Also, to the best of the\nauthorsâ€™ knowledge, this is the first work where Markov\nchain is used for query reformulation in voice-based vir-\ntualassistants.\nOneimportantdifferencebetweenthewebqueryrefor-\nm u l a t i o na n dA l e x a â€™ su s ec a s ei st h a tw en e e dt os e a m -\nlesslyreplacetheuserâ€™sutteranceinordertoremovefric-\ntion.Askingusersforconfirmationeverytimeweplanto\nreformulateisonitselfanaddedfriction,whichwetryto\navoidasmuchaspossible.Anotherdifferenceishowsuc-\ncessandfailurearedefinedforaninteractionbetweenuser\nandavoice-basedvirtualassistantsystem.Weuseimplicit\nandexplicituserfeedbackwheninteractingwithAlexato\nestablishthe absorbingstates ofsuccessandfailure.\nSYSTEMOVERVIEW\nTheAlexaconversationalAIsystemfollowsaratherwell-\nestablishedarchitecturaldesignpatternofcloud-baseddig-\nital voice assistants (2018), that is, comprising, in order,\nof an ASR system, an NLU system with a built-in dialog\nmanager,andatext-to-speech(TTS)system,asvisualized\nin Figure 1 Conventionally, as a user interacts with their\nAlexa-enableddevice,theirvoiceisfirstrecognizedbythe\nASR engine and decoded into a plain text surface form,\nwhich we refer to as an utterance. The utterance is then\ninterpreted by the NLU component to surface the afore-\nmentioneduserâ€™sintentbyalsoaccountingforthestateof\nAIMAGAZINE 45\nFIGURE 1 Ahigh-leveloverviewofthedeployedarchitecturewithourreformulationengineincontextoftheoverallsystemin(A)and\ntheofflinesub-systemthatupdatesitsonlinecounterpartonadailycadence\nFIGURE 2 Datapipelineforconstructingsessions\nuserâ€™sactivedialogsession.Thereafter,theintentandthe\ncorresponding action to execute is passed on to the TTS\ntogeneratetheappropriateresponseasspeechbacktothe\nuserviatheirAlexa-enableddevice,thusclosingtheinter-\naction loop. Also note that the metadata and logs associ-\nated with each of the above systems are deidentified and\nstoredasynchronouslyinanexternaldatastoragesystem,\nwhichthereafterisaccessibleoffline.\nIn deploying our self-learning system online, we aug-\nmenttheoriginalinteractionloopbyfirstinterceptingthe\nutterancebeingpassedontotheNLUsystemandrewriting\nit with our reformulation engine. We then subsequently\npass the rewrite in lieu of the original utterance back to\nNLU for interpretation, and thus restoring the original\ndataflow.Thisisshownasthepost-deploymentdataflow\npathinFigure1.Ourreformulationenginehereessentially\nimplementsaratherlightweightservice-orientedarchitec-\nture that encapsulates the access to a high-performance,\nlow-latency database, which is queried with the original\nutterancetoyielditscorrespondingrewritecandidate.This\nalongwiththefactthatthesystemisfundamentallystate-\nless across users translates to a rather scalable customer-\nfacing system with marginal impact to the overall per-\nceivedlatencyoftheirAlexa-enableddevice.\nNow, in order to discover new rewrite candidates and\nparticularly to maintain the recency of the rewrites, the\nMarkov-based model ingests the deidentified Alexa log\ndata on a daily cadence to learn from usersâ€™ reformula-\ntionsandsubsequentlyupdatestheaforementionedonline\ndatabase. This ingestion to update process takes place\nofflineinentiretywiththerewritesinthedatabaseupdated\nvia a low-maintenance feature-toggling (i.e., feature-flag)\nmechanism.Additionally,inverifyingtheviabilityofthese\nrewrites, particularly in the context of improving cus-\ntomer experience, the system is also supplemented by a\nrewrite selection mechanism. Here, the deidentified log\n46 AIMAGAZINE\ndataisleveragedtoevaluatetherewritesfromtheMarkov-\nbased model by independently comparing their friction\nrate against that of the original utterance both generi-\ncally via a proportionZ-test and contextually via gener-\nalizedlinearmodels(GLMs),andsubsequentlyremoving\nthemfrombeinguploadedtothedatabaseshouldtheyper-\nformworsethantheirno-rewritecounterpart.Thisrewrite\nselection process, which allows us to maintain a rather\nhighprecisionoverallsystematruntime,alsotakesplace\nentirelyofflineandupdatesthedatabaseinsimilarfashion\ntothatoftheMarkov-basedmodel,albeitinamuchmore\nfrequentcadenceofevery 4 h â€“ definedby striking a bal-\nancebetweenimmediacy,dataavailability,andexecution\ntime.\nIn subsequent sections of this paper, we discuss the\nnatureofourdataset,howtheMarkov-basedmodellearns\nfromthereformulations,andthefunctionalaspectsofour\nrewriteselectionmechanism.Itisalsoworthmentioning\nthattheaforementionedfrictionrateiscomputedbyaggre-\ngatingacrossutterances,theresultofapre-trainedneural\nmodel that leverages a userâ€™s utterance, the correspond-\ningAlexaâ€™sresponse,andothercontextualsignalstodetect\nfriction for every user-Alexa interaction exchange. While\nthe extended details of that model is beyond the scope of\nthepaper,inthesimplestsense,themodelisbasedonfine-\ntuningaBERT(2018)modelonaratherlimitedinternally\nannotateddataset.\nDATASET\nAsourobjectiveistolearnthepatternsfromuserinterac-\ntionswithAlexa,wepre-process3monthsofdeidentified\nAlexalogdataacrossmillionsofcustomers,whichconsti-\ntutes a highly randomized collection of time-series utter-\nancedata,toconstructourdataset, îˆ° comprisingofaset\nofsessions, ğ‘†\nğ‘–,thatis:\nîˆ° = {ğ‘†0,ğ‘†1,â€¦} (1)\nHere, in motivating the definition of a session, it is\nworthwalkingthroughthispre-processingpipelinefroma\nhigh-levelperspective.Giventhecollectionofrandomized\nlog, the data is first funneled through a partitioning and\nsorting stage where each grouping represents a finite\nordered set of successive utterances, ğ‘¢ for a unique\ncustomer-devicepair, (ğ‘,ğ‘‘).Thereafter,eachofthesesets\nare split into smaller sub-sets at points where the time\ndelay between any two consecutive utterances is at most\nğ›¿\nğœ.Thesesub-setsareuniquelydenotedby (ğ‘,ğ‘‘,ğ‘¡) where\nğ‘¡ representsthetimestampofthecorrespondingsub-setâ€™s\nfirst utterance. We also note that interjecting utterances,\nğ½, that is, those leading to StopIntent, CancelIntent, etc.,\nTABLE 1 Topsevendomainsspanningthedatasetwiththeir\nexampleutterances\nDomain ExampleUtterance\nGlobal Whattimeisit\nMusic Playwatermelonsugar\nHomeAutomation Turnonthepatiolight\nNotifications Setareminderfortwop.m.\nKnowledge Whowonthen.f.I.\nWeather Isitgoingtorainthisweek\nVideo Playthenewseasonofshameless\nFIGURE 3 Latentsessionofinterpretations\nthat occur before the end of the aforementioned ordered\nsets are removed. These processing steps as a collective\nprocessisillustratedinFigure2.\nThen, intuitively speaking, any particular session,ğ‘†\nğ‘– is\neffectivelyatime-delimitedsnapshotofauserâ€™sconversa-\ntionhistorywiththeirAlexa-enableddevice.Weillustrate\nthis in Figure 4Aâ€“C where each session is represented as\nalineardirectedchainofsuccessiveutterances,forexam-\nple, ğ‘¢\n2 â†’ğ‘¢ 3 â†’ğ‘¢ 4. In this paper, we choose the value of\nğ›¿ğœ =4 5 sasaresultfromaseparateinternalanalysis.\nAdditionally,asamatterofprinciple,wenoteherethat\nthis dataset of sessions still remains true to the underly-\ning natural distributions of the original log data. This is\nparticularly so as the process of constructing the dataset\nis entirely devoid of any stratification or sampling strate-\ngies.Tothatend,giventheanonymityofthesessions,we\nensurethateveryuserinteractionwiththeirAlexadevice\nis entirely independent and weighted equally across the\ndataset,wherefromalinguisticcontext,itbothstructurally\nand semantically spans over a dozen different domains â€“\nthetop7ofwhichalongwiththeirexamplesaresumma-\nrizedinT able1.\nABSORBINGMARKOVCHAIN\nIn this section, we show how encoding user interaction\nhistoryaspathsinanabsorbingMarkovChainmodelcan\nbe used to mine patterns for reformulating utterances. In\nparticular, we discuss in detail the concept of the inter-\npretation space,ğ», which functions as the vertex set of\nthemodelâ€™stransientstates.Wethenelaborateonthecon-\nstructionoftheabsorbingstates, ğ‘…,thecanonicalsolution\nAIMAGAZINE 47\nFIGURE 4 AvisualrepresentationoftheMarkovmodelconstructedintheinterpretationspace,H,overthreeseparatesessions,(a),(b),\nand(c),ofusersattemptingtoplaythealbumâ€œDespicableMeâ€,andhowsolvingforthepathwiththehighestlikelihoodofsuccess,( +),given\nbythedarkenededgedin(d),canallowforthedefectiveutterancestobereformulatedintoamoresuccessfulquery,assummarizedin(e).\nNotethathere,fordemonstrationpurposes,weonlyshow3interactions.However,inpractice,wehadahigherthresholdfortheminimum\nnumberofcustomersandinteractionstohavebetterestimatesfortheprobabilities\nFIGURE 5 Sub-graphoftheMarkovmodelinFigure4D\nto the model, and the practical implementation of the\nmodel. As the Markov Chain model is inherently a prob-\nabilistic graphical model, we can represent it as graph,\nğº=( ğ‘‰ , ğ¸ ),wherethevertexset, ğ‘‰ comprisesofboththe\ntransient and absorbing states, that is, vertices inğ» and\nğ‘…, respectively, while the edge set,ğ¸ comprises of vertex-\npairs,(ğ‘¥,ğ‘¦) withğ‘¥ beinganyvertexin ğ» andğ‘¦ beingany\nvertex inğ‘‰. We note that from here on out, we use the\nterms,Graph,andMarkovmodelinterchangeably.\nInterpretationspace\nWhile our definition of a session above naturally extends\ntowardhavingeachorderedlinearsequenceofutterances\nasapathinourMarkovmodel,thisencodingintheutter-\nancespace, ğ‘ˆ,thatis,thespaceofallutterances ğ‘¢,imposes\na limitation on the model by creating heavily sparse con-\nnections.Thisisprimarilyduetothehighdegreeofseman-\ntic and structural variance inğ‘ˆ, which would ultimately\nresultinalowercapacityforgeneralization.\nToresolvethis,weleveragethedomainandintentclas-\nsifieraswellasthenamedentityrecognition(NER)results\nfromAlexaâ€™sNLUsystemstosurfacestructuredrepresen-\ntationsofutterances,andthusencapsulatealatentdistri-\nbution overğ‘ˆ. Consequently, each utterance in a session\nis projected into this interpretation space,ğ» which com-\nprisesthesetofallinterpretations â„,todefinealatentses-\nsion,ğ‘†\nâ€²\nğ‘–:\nwhere placed in context of Figure 4B is illustrated as\nabove in Figure 3. To exemplify this, consider the utter-\nance, â€œplay despicable meâ€( i . e . ,u\n4 in Figure 4), which\nwouldbemappedintotheH-spaceas:\nMusic | PlayMusicIntent | AlbumName:despicable\nme\nwhichiscompactlyrepresentedas â„2 inFigure4.Asthe ğ»-\nspacecondensesthesemanticsof ğ‘ˆ,thismappingbetween\nğ‘ˆ and ğ» is inherently a many-to-one relationship. How-\never, given the stochasticity of Alexaâ€™s NLU, the original\nprojectionitselfisnotentirelybijectiveandthusresultsin\n48 AIMAGAZINE\namany-to-onerelationshipinboththeforwardandinverse\nmapping, that is,ğ‘ˆâ†’ğ» and ğ»â†’ğ‘ˆ , akin to a bipartite\nmapping. This in turn, yields the conditional probability\ndistributions,ğ‘ƒ(ğ»|ğ‘ˆ) andğ‘ƒ(ğ‘ˆ|ğ»),suchthatforapartic-\nularğ‘¢âˆˆğ‘ˆ andâ„âˆˆğ» ,theyaredefinedasfollows:\nğ‘ƒ (â„|ğ‘¢) = #(ğ‘¢â†’â„ )\n#ğ‘¢ ğ‘ƒ (ğ‘¢|â„) = #(ğ‘¢â†’â„ )\n#â„ (2)\nwhere#(ğ‘¢ â†’ â„)representsthetotalnumberoftimesboth\nğ‘¢ andâ„ aremappedontoeachotherwhile #ğ‘¢ and#â„ are\nthe total occurrences ofğ‘¢ and â„ in the entire dataset,îˆ°,\nrespectively.\nTransientstates\nGiven our transformed dataset,îˆ°â€² of latent sessionsğ‘†â€²,\nwe take each such session and the interpretations within\nit to represent paths and transient states respectively in\nourMarkovmodel,suchthateachsuccessivepairofinter-\npretations would represent an edge in the graph. Then,\nthecorrespondingprobabilitythatatransitionstate â„\nğ‘– âˆˆğ»\ntransitionsto â„ğ‘— âˆˆğ» inthegraphisgivenby:\nğ‘ƒ\n(\nâ„ğ‘—|â„ğ‘–\n)\n=\n#\n(\nâ„ğ‘– â†’â„ ğ‘—\n)\n#â„ğ‘–\n(3)\nwhere#(â„ğ‘– â†’â„ ğ‘—) and #â„ğ‘– represent the total number of\ntimes â„ğ‘— is adjacent, that is, directly linked toâ„ğ‘– and the\ntotal occurrence ofâ„ğ‘– respectively, aggregated across all\nsessions (i.e.,over 3 months and millions of customers)i n\nîˆ°â€².\nTakingthisincontextofFigure4,considerthetransition\nprobabilityğ‘ƒ(â„1|â„0).Fromthesessions(a),(b),and(c),we\ncannotethatthetransitionstate â„0 isadjacenttothestates,\n{â„0,â„1,â„3,(âˆ’)} witheachofthemhavingaco-occurrence\ncount of1 withâ„0. Here,(âˆ’) refers to the failure absorb-\ningstate( definedinthefollowingsub-section).Assuch,the\nprobabilityğ‘ƒ( â„1|â„0)= 1\n4 =0 . 2 5asshowninFigure4D.\nAbsorbingstates\nIn formulating the definition of the absorbing states of\nthe Markov model, we look toward encoding the notion\nofinterpreteddefectsasperceivedbytheuser.Aswehave\nbrieflyintroducedearlier,thisconceptofdefectsurfacesin\ntwokeyforms,thatis,via explicitandimplicitfeedback.\nHere, explicit feedbackrefers to the type of corrective\norreinforcingfeedbackreceivedfromdirectuserengage-\nment. This primarily includes events where users opt to\ninterrupt Alexa by means of an interjecting utterance (as\ndefinedaboveinDataset).Thisisillustratedintheexample\nbelow:\nUser: â€œplayaleverâ€\nAlexa: â€œHereâ€™sLeverbyTheMavisâ€™s,startingnow.â€\nUser: â€œstopâ€\nIncontrast, implicitfeedback istypicallyobservedwhen\nusers abandon a session following Alexaâ€™s failure to han-\ndlearequesteitherduetoaninternalexceptionorsimply\nunable to find a match for the entities resolved. Case in\npoint:\nUser: â€œplaymajanddragonsâ€\nAlexa: â€œSorry,Icanâ€™tfindtheartistmajanddragons.â€\nGiventhis,wedefinetwoabsorbingstates: failure(ğ‘Ÿâˆ’),\nandsuccess(ğ‘Ÿ+),where successisdefinedastheabsenceof\nfailure. These states are artificially injected to the end of\nallsessions,basedontheimplicitandexplicitfeedbackwe\ninferfromAlexaâ€™sresponse,anduserâ€™slastutterance.\nTo clarify this, let us walk through the examples above\nassuming that they are the last utterances of their corre-\nspondingsessions.Inthefirstexample,wewoulddropthe\nâ€œstopâ€turn,andadda failurestate.Inthesecondexample,\nwe simply add thefailure state to the end of the session.\nFinally,intheabsenceofanexplicitorimplicitfeedback,\nweadda successstatetotheendofthesession.Giventhis,\nwe can then define the probability that a given transient\nstate,â„\nğ‘– isabsorbedinmuchthesamewayasinEquation\n8,forexample:\nğ‘ƒ\n(\nğ‘Ÿ+|â„ğ‘–\n)\n=\n#\n(\nâ„ğ‘– â†’ğ‘Ÿ +)\n#â„ğ‘–\n(4)\nNote that in Figure 4, we refer to thefailure (ğ‘Ÿâˆ’), and\nsuccess(ğ‘Ÿ+)statesas (âˆ’) and(+),respectively.\nMarkovmodel\nWiththedistributionsoverboththetransitionandabsorb-\ning states defined above, recall that the interpretation\nspace,ğ» isthesetofalltransientstatesinthegraph.Then,\nwecansummarizetheMarkovmodelinitscanonicalform\nviathetransitionmatrix, ğ€ asfollows:\nğ€=\n[ğğ‘\n0ğˆ\n2\n]\n(5)\nwhere the sub-matrixğ encompasses the transient prob-\nabilities between all states in the interpretation spaceğ»,\nwhilethesub-matrix ğ‘encapsulatestheabsorptionproba-\nbilitiesofeverystatein ğ» goingtothetwoabsorbingstates,\nğ‘Ÿ+ and ğ‘Ÿâˆ’. Additionally, the sub-matrices0 and ğˆ2 both\nAIMAGAZINE 49\nrepresent the sub-matrix of zeros and the2Ã—2 identity\nmatrix,respectively.\nNow, we generalize the previous notation of probabili-\ntiesas ğ‘ƒ(ğ‘›),thatis,theprobabilityatdepth-ğ‘› ofthegraph,\nwith ğ‘ƒ implicitly referring toğ‘ƒ(1). Note that in the con-\ntext of transition probabilities, this generalization readily\nextends to the matrix form where the probability of tran-\nsitioning from some transient stateâ„\nğ‘– to some transient\nstate â„ğ‘— after exactlyğ‘› steps, that is,ğ‘ƒ(ğ‘›)(â„ğ‘—|â„ğ‘–) is thus\ngivenbythe (ğ‘–,ğ‘—)-thelementofthesub-matrix ğğ‘› (i.e.,ğ\nmultiplied itselfğ‘›times) and similarly as before withğğ‘–,ğ‘—\nimplicitlyreferringto ğ‘ƒ(â„ğ‘—|â„ğ‘–).Additionally,let â„ğ‘  andâ„ğ‘¡\nbeanygivensourceandtargettransientstatesinthegraph,\nrespectively. Then, in motivating the optimization objec-\ntive,considerforemosttheMarkovmodelinFigure4Dand\ntakeboth â„\nğ‘  andâ„ğ‘¡ tobe â„0 andâ„2,respectively.Thesub-\ngraphinquestioncanbeillustratedinFigure5.\nAt this point, we can observe the following key facts\nabout the sub-graph above: (1) as there is at least 1 path\nbetween â„\nğ‘  and â„ğ‘¡, the latter is considered to bereach-\nable by the former; (2) both paths linkingâ„ğ‘  and â„ğ‘¡ are\nseparatedby1othertransientstatewithnodirectlinkage\nresulting in 2 traversal steps being required, for example,\nâ„\n0 â†’â„ 1 â†’â„ 2;and(3)theself-edgeon â„0 furtherextends\ntheoriginaldepth-2pathintoinfinitelymanypossiblepath\ndepthsbyrepeatedlycirclingaround â„\n0,forexample, â„0 â†’\nâ€¦â†’â„ 0 â†’â„ 1 â†’â„ 2.\nHere,whilethepoint(2)aboveindicatesthat ğ2\nğ‘ ,ğ‘¡ >0 ,it\nisfarfromsufficientfromsummarizingthetotalprobabil-\nityoftransitioningfrom â„ğ‘  toâ„ğ‘¡.Infact,thepoint(3)above\ncements thatâ„ğ‘¡ can also be reached byâ„ğ‘  with traversals\nof depths 3, 4, and so on, which inevitably leads to the\ntotal probability being the(ğ‘ ,ğ‘¡)-th element of the infinite\ncompoundsumof ğ\n2 +ğ 3 +â‹¯+ .Wecanthengeneralize\nthis across all possible source and transient states by also\nincludingğ\n0 andğtoaccountforallpossibledepths.Now,\ngiventhat ğ isaconvergentsquarematrixofprobabilities\nsuch thatâˆ¥ğâˆ¥ <1 , the aforementioned summation then\nleadstoageometricseriesofmatrices,whichasgivenby\nDefinition 11.3 in (1997), corresponds to the fundamental\nmatrixoftheMarkovmodel,denotedby ğ:\nğ= ğ\n0 +ğ+ğ 2 +â‹¯ =\n(\nğˆ|ğ»| âˆ’ğ\n)âˆ’1\n(6)\nwhere ğˆ|ğ»| refers to the identity matrix which is equi-\ndimensional withğ. Given this, we can thus define the\nprobabilityof successaftertraversingfromanygiven â„ğ‘  to\nanygiven â„ğ‘¡ suchthat â„ğ‘¡ isreachablebyâ„ğ‘  asfollows:\nğ›‘âˆ (â„ğ‘  â†’â„ ğ‘¡) =ğ‘ƒ\n(\nğ‘Ÿ+|â„ğ‘¡\n)\nâ‹…ğ ğ‘ ,ğ‘¡ (7)\nAs such, in the context of reducing defects, we con-\nsiderâ„ğ‘¡ to be a possible reformulation candidate forâ„ğ‘  if\nitis reachablebyâ„ğ‘ ,suchthatconditionedon â„ğ‘ ,â„ğ‘¡ hasa\nhigherchanceof successthanâ„ğ‘  onitsown,thatis:\nğ›‘âˆ (â„ğ‘  â†’â„ ğ‘¡) >ğ‘ƒ\n(\nğ‘Ÿ+|â„ğ‘ \n)\n(8)\nWethenframeourobjectiveasidentifyingthe â„ğ‘¡ which\nmaximizestheaforementionedprobabilityforthegiven â„ğ‘ :\nâ„âˆ—\nğ‘¡ =a r g m a x\nâ„ğ‘¡\nğ›‘âˆ (â„ğ‘  â†’â„ ğ‘¡) (9)\nIntuitivelyspeaking,intheeventthat â„âˆ—\nğ‘¡ â‰  â„ğ‘ ,themodel\nshows that there exists areachable target interpretation\nthat when reformulated fromâ„ğ‘ , has a better chance at\na successfulexperience than not doing so. In reference to\nFigure 4E, we can see that reformulatingâ„0 to â„âˆ—\nğ‘¡ =â„ 2\nincreasesthelikelihoodofsuccessas:\nğ›‘âˆ (â„0 â†’â„ 2) = 2\n3 >ğ‘ƒ\n(\nğ‘Ÿ+|â„0\n)\n=0 (10)\nSuppose thatâ„âˆ—\nğ‘¡ =â„ ğ‘  . In which case, the source inter-\npretation is already successful on its own and hence\nrequires no reformulation. As such, the model is effec-\ntively able to automatically partition the vertex space,ğ»\nintosetsof successful(ğ»\n+)andunsuccessful (ğ»âˆ’)interpre-\ntations.Inextendingthisreformulationbacktotheutter-\nance space,ğ‘ˆ, we leverage the distributionsğ‘ƒ(ğ‘ˆ|ğ») and\nğ‘ƒ(ğ»|ğ‘ˆ) definedinEquation5andre-defineourobjective\nasfollowsforagivensourceutterance ğ‘¢\nğ‘  âˆˆğ‘ˆ :\nğ‘¢âˆ—\nğ‘¡ =a r gm a x\nğ‘¢ğ‘¡\nâˆ‘\nâ„ğ‘ ,â„ğ‘¡\nğ‘ƒ (ğ‘¢ğ‘¡|â„ğ‘¡) â‹…ğ›‘ âˆ (â„ğ‘  â†’â„ ğ‘¡) â‹…ğ‘ƒ (â„ğ‘ |ğ‘¢ğ‘ )\n(11)\nThe intuition described above can similarly be applied\nhere whereğ‘¢âˆ—\nğ‘¡ is the moresuccessfulreformulation ofğ‘¢ğ‘ .\nNotethattheself-partitioningfeatureofthemodeldirectly\nextendstotheutterancespace, ğ‘ˆ,allowingittobothsur-\ngicallyandactivelytargetonlyutterancesthatarelikelyto\nbe defective and surface their corresponding rewrite can-\ndidates.Thisisthecardinalaspectofthemodelthatdrives\nthe self-learning nature of the proposed system without\nrequiringanyhumanintheloop.\nImplementation\nWith|ğ»| âˆ¼1 06,constructingthematrix ğ,letaloneinvert-\ningit,posesakeychallengetowardsscalingoutthemodel,\nparticularlyinitsbatchedform.Assuch,weformulatean\napproximationincomputingtheprobability ğ›‘\nâˆ(â„ğ‘  â†’â„ ğ‘¡)\nforallsourceinterpretations, â„ğ‘  bymeansofadistributed\napproach.\n50 AIMAGAZINE\nWe note that from our dataset,ğ·â€², that in the event\nthatagivensourceutterance, ğ‘¢ğ‘  isdefective,userswould\nonly attempt at reformulating their query a few times\nbeforeeitherarrivingatasatisfactoryexperienceoraban-\ndoning their session entirely. This translates to most (âˆ¼\n97.3 percent) source interpretations, â„\nğ‘  in the Markov\nmodelhavingshortpathlengths(i.e.,typically â‰¤ 5)priorto\nthembeingabsorbedbyanabsorbingstate.Consequently,\nthisalongwiththefactthatthesereformulationsarerecur-\nrent across users, most high-confidence reformulations\noftenonlyinvolvevisitingarelativelymuchsmallersetof\ntargetinterpretations, â„\nğ‘¡ whencomparedtothecardinality\noftheinterpretationspace, |ğ»| asawhole.\nThisleadsustodeducethatthematrix ğishighlysparse\nandthecorrespondinggraphcontainsmanyclustered(i.e.,\ncommunity) structures. We then leverage these facts to\nfirst collect the paths for every source interpretation,â„\nğ‘ \ninaseriesofmap-reducetasks,bymeansofadistributed\nbreadth-firstsearchtraversaluptoafixeddepthof5using\nApache Spark (2016). Thereafter, each task receives the\npathscorrespondingtoasingle â„\nğ‘  andinturnusesthemto\nconstructanapproximatetransitionmatrix.Asthedimen-\nsionality of this matrix is much lower than that ofğ€,w e\ncaneasilycomputeapproximatesforboththefundamen-\ntalmatrixandtheprobabilities ğ›‘\nâˆ(â„ğ‘  â†’â„ ğ‘¡) forthe reach-\nabletargetstateswithinthesametask.Asaresult,wehave\nadistributedsolutionforparallelizingthecomputationof\ntheoptimizationprobleminEqn.[optim]forevery â„âˆˆğ» .\nThe breadth-first search traversal, which involves a\nseries of sort-mergejoins, does indeed introduce an algo-\nrithmicoverheadof îˆ»(ğ‘‘ â‹…|ğ¸| + |ğ¸|log|ğ¸|),where ğ‘‘ andğ¸\nrefertothedepthofthetraversalandthesetofalledgesin\nthegraph,respectively.Wedoalsonotethatasthisisadis-\ntributedjoin,theincurrednetworkcostduetodatashuf-\nflesareomittedhereforsimplicity.Thatbeingsaid,these\noverheadsareoffsetbytheadvantageofbeingabletoscale\noutthemodel.Forpurposesofoptimization,eachsucces-\nsive join is only performed on the set of paths which are\nnon-cyclic and have yet to be absorbed while paths with\nvanishingprobabilitiesareprunedoff.\nEXPERIMENTS\nBaseline:Pointer-generator\nsequence-to-sequencemodel\nSequence-to-sequence (seq2seq) architectures have been\nthe foundation for many neural machine translation and\nsequencelearningtasks(2014).Assuch,byformulatingthe\ntaskofqueryrewritingasanextensionofsequencelearn-\ning, we used a long short-term memory-based (LSTM)\nmodel as an alternative method to produce rewrites. In\nshort, we first mined 3 months of rephrase data using\na rephrase detection ML model such that the first utter-\nance was defective, and the rephrase was successful. We\nthen used this data to train the pointer-generator model,\nsuchthatgiventhefirstutterance,itproducesthesecond\nutterance.Themodelisbasedonwell-establishedencoder-\ndecoderarchitecturewithattentionandcopymechanisms\n(2017).Afterthemodelistrained,wethenusedittorewrite\nthesameutterancesthatthegraphrewrites.\nOfflineanalysis\nInordertoevaluatethequalityoftherewritesweobtained,\nwe annotated 5679 unique utterance-rewrite pairs gener-\nated via the graph, and estimated the accuracy and winâ€“\nloss ratio to be 93.4 percent and 12.0, respectively. The\nnotion of winâ€“loss ratio here is defined as the ratio of\nrewritesthatresultinbettercustomerexperienceandthe\nrewritesthatdeterioratecustomerexperience.Wefurther\nleveragedthepointer-generatormodeltogeneraterewrites\nfortheseutterancesasabaseline.\nApplying the pointer-generator model on this dataset\nresulted in accuracy of 55.2 percent, that is, significantly\nlower than the accuracy of graph. This is expected, since\nthegraph(1)aggregatesall3monthsofdata(andnotlim-\nited to merely rephrases), (2) takes into account the fre-\nquencyoftransitionswhereasthepointer-generatormodel\nonlyhasuniquerephrasepairsfortraining,and(3)utilizes\ntheinterpretationspacetofurthercompactandaggregate\ntheutterances.\nIncontrast,thepointer-generatormodelhasinsteadthe\nbenefitofahigherrecall(sinceitcanpotentiallyrewriteany\nutterance), and it learns the patterns, for example, Song-\nName â†’ play SongName. Another important difference\nbetween the graph and the pointer-generator method is\nthatthegraphiscapableofidentifyingwhenanutterance\nis successful via its interpretation, that is, whenâ„\nâˆ—\nğ‘¡ =â„ ğ‘ \nand thus maximize its precision. This is a signal tonot\nrewrite the utterance, since statistically speaking, rewrit-\ningcouldonlypotentiallyworsenitslikelihoodofsuccess.\nHowever, the pointer-generator model lacks this capabil-\nity, and it may rewrite an otherwise successful utterance,\nwhichthereafterwouldcauseafriction.\nTable2showssomeexamplesofgoodandbadrewrites\nfrom the graph. It is clear from the examples that the\nrewrites are capable of fixing ASR (no. 1â€“3), NLU (no. 4â€“\n7), and even user errors (no. 8). On the other hand, there\nare cases where the rewrites tend to fail (no. 9â€“10). One\nof the recurring cases of failure is when an utterance is\nrewrittentoagenericutterance,likeâ€œ play,â€orâ€œ shufflemy\nsongs.â€Thisusuallyhappensduetotheoriginalutterance\nnot beingsuccessful, and the users trying many different\nAIMAGAZINE 51\nTABLE 2 Someexamplerewritesfromthegraph\nNo. Originalutterance Rewrite Label\n1 Playmajanddragons Playimaginedragons Good\n2 Playshadowbyladygaga Playshallowbyladygaga Good\n3 Playrumer Playrumorbyleebrice Good\n4 Playsiriusx.m.chill Playchannelfiftythreeonsiriusx.m. Good\n5 Playa.b.c. Playthealphabetsong Good\n6 Donâ€™teverplaythatsongagain Thumbsdownthissong Good\n7 Turnthevolumetohalf Volumefive Good\n8 Playislandninetypointfive Playislandninetyeightpointfive Good\n9 Playswaggyplaylist Shufflemysongs Bad\n10 Playcarterfivebylilwayne Playcarterfourbylilwayne Bad\npathsthateventuallyloseinformation,andisconsequently\naggregatedinagenericutterance(duetoEq.[sumProbabil-\nities]).Anothercommoncaseoffailureiswhentherewrite\nchanges the intention of the original utterance by chang-\ning the song name or artist name. This happens because\nof various reasons. For example, the data that we use for\nbuildingthegraphmaycontainaperiodoftimewherethe\noriginal utterance was not usuallysuccessful, so the users\nchangedtheirmindbyaskingtoplayanothersimilarsong\n(likeno.10).\nWhilethefirsttypeoferroriseasytocorrect,byeither\napplying rules or building a learning-based ranker after\nthe graph generation, the second type, however, is rather\ntricky to detect, since more often than not, changes in\ntheinterpretationtendtohelp.Here,wecriticallyrelyon\ntherewriteselectionmechanismtoremovetheseformsof\nrewritesfromtheproductionsystem.\nREWRITESELECTION\nDespite the graphâ€™s ability to surgically target only utter-\nances which are potentially defective, it certainly is not\ndevoid of false positives and may very well over-trigger\nby surfacing rewrites that worsen user experiences as\nshown in Table 2. In fact, extending from the previous\nsection, while high path entropy leading to overly gener-\nalized utterances is a cause of concern, the greater req-\nuisite for a rewrite selection mechanism stems from the\nlatter issue of intent changes in the proposed rewrite,\nwhich is largely prompted by the unavailability of more\nrecent content requested by the original utterance. Here,\ntherewriteoffersbutonlyatemporarysolutiontoaddress\nuserdemand.However,whenthenewercontentbecomes\navailable,asystemmorereactivethanthegraphisimpera-\ntiveinsuppressingthis.Additionally,thecollaborativefil-\ntering nature of the graph translates to surfacing rewrite\npaths that while may improve the experience for most\nusers, may very well worsen for those who fall short of\nbeinginthemajoritypreferencepool.Thesefactorssuffice\nin giving rise to our rewrite selection system centered on\nthepremiseofcomparativelyevaluatingthefrictionrates\nbetweentherewritesandtheiroriginalcounterparts,and\nthus compensates for the graphâ€™s limitations by (1) reac-\ntivelysupplantinggraphâ€™slocalMarkovpropertyinassess-\ningtheviabilityofarewriteforanutterancepost-launch,\n(2)leveragingmoregranularcontextualandfeedbacksig-\nnalsasaffordedbythepre-trainedneuraldefectpredictor\nthan the coarser definitions used in the absorbing states,\nand(3)potentiallyfeedingbacktheactionsofrewritenot\nbeingselectedasimplicitfailurenodesallowingforabetter\nre-routingofpathsinthegraph.\nGeneralizedselection\nGiven the original utterance, the selection system can\ndecide to rewrite it to one or more candidate utterances,\nor not to rewrite at all if none of the candidates reduces\nfrictions.\nInitsrathersimplestform,thegeneralizedrewriteselec-\ntion both posits and attempts to answer the question of\nwhetherarewriteforagivenutterancewouldsignificantly\nworsenuserexperiencewhencomparedtothatoftheorig-\ninal utterance itself, solely by virtue of their respective\nfriction rates. This lends itself to the notion of evaluating\non the basis of hypothesis testing, and more specifically\na proportionğ‘-test between the friction distributions of\ntherewriteandno-rewritecases.Todrivethishome,con-\nsiderthecasewherewerewrite,â€œ playwalkhardbydewey\ncoxâ€â†’ â€œplaywalkhardâ€byexcisingtheartistnameforthe\nrequestedsong.Notethattherewriteisactivelyalteringa\nportionoforiginalinterpretationasfollows:\nArtistNameâˆ¶ğğğ°ğğ² ğœğ¨ğ±|SongNameâˆ¶walk hard\nâ†“\nSongNameâˆ¶walk hard\n52 AIMAGAZINE\nNow, letğœ‡0 and ğœ‡1 be the friction rates of the original\nandrewrite,respectively.Then,wesubsequentlyconstruct\ntheone-sidedproportion ğ‘-testtoquestioniftherewriteis\nindeedworseningtheexperiencebydefiningboththenull\nandalternativehypothesessuchthat:\nğ»\n0 âˆ¶ğœ‡ 0 =ğœ‡ 1\nğ»1 âˆ¶ğœ‡ 0 <ğœ‡ 1\n(12)\nLeveraging actual post-launch data, we observed that\nthe original resulted in about six friction experiences out\nof eight, that is, an effective friction rate of 75 percent\nwhiletherewritehadabouteightfrictionexperiencesout\nof24,thatis,aneffectivefrictionrateof33percent.These\nin combination yields ağ‘ƒ-value of 0.98 and with a sig-\nnificancelevelof 0.01 suggestthattheinversealternative\nhypothesis,thatis,where ğœ‡\n0 >ğœ‡ 1 isstatisticallysignificant\nand is thereafter sufficient to reject the null hypothesis â€“\nthus indicating that the rewrite actually improves overall\nuserexperience.Now,extendingthisbacktotheexample\nno. 10 in Table 2, where we rewrite â€œplay carter five by lil\nwayneâ€â†’â€œplaycarterfourbylilwayne ,â€wenotethatwhile\ninitiallythecontentunavailabilityoftheoriginalutterance\nwould push theğ‘-test in favor of the rewrite, this is only\ntemporaryasthe ğ‘-testwouldeventuallyfavorpreserving\nthe original as soon as the more recent content becomes\navailable and thus driving down the friction rates of the\noriginal. This reactive behavior is critical to ensure cus-\ntomersatisfactionatoptimalrates.\nContextualizedselection\nDespite the simplicity of the generalized selection mech-\nanism, its reactivity is very much limited by the vol-\nume of observations for the given hypothesis tests and\nthis is especially so for rewrites with rather low accrual\nrateswhereafarlongerperiodwouldberequisitetoward\nachievingsimilarstatisticalsignificanceforrewriteselec-\ntion.Thisinevitablynecessitatesasupplementarysystem\nto bridge this reactivity gap. Now, consider the two pro-\nposedrewritesforasimilaroriginalutterance,thatis,â€œplay\na.b.c.â€ â†’ â€œplaythesonga.b.c.â€andâ€œplaya.b.c.â€ â†’ â€œplay\nthea.b.c.song.â€Here,whiletheoriginalutteranceandthe\nformerrewritehasaneffectivefrictionratesof79percent\n(given approximately 100 observations) and 32.3 percent\n(givenapproximately3000observations),respectively,the\nlatter rewrite appeared to have only occurred once and\nwithout friction. Evidently, a single friction-free observa-\ntionofthelatterrewritewouldbeinsufficienttoperform\nany meaningful hypothesis testing as in the generalized\nsetting.However,giventhefactthatboththerewriteshere\nshareasimilarresolvedinterpretationasfollows:\nğ‘€ğ‘’ğ‘‘ğ‘–ğ‘ğ‘‡ğ‘¦ğ‘ğ‘’âˆ¶ğ‘ ğ‘œğ‘›ğ‘” |ğ‘†ğ‘œğ‘›ğ‘”ğ‘ğ‘ğ‘šğ‘’âˆ¶ğ‘¡â„ğ‘’ ğ‘ğ‘™ğ‘â„ğ‘ğ‘ğ‘’ğ‘¡ ğ‘ ğ‘œğ‘›ğ‘”\nItdoesindeedbeckonthequestionofwhetherthefric-\ntionrateandbyextensiontheoverallrewriteviabilityofthe\nlatterrewritecouldpossiblybeinferredfromthoseofthe\nformer.Infact,ourcontextualizedrewriteselection(CRS)\nproposes to answer this question by means of featurizing\nthe rewrites so as to leverage and transfer their content\ninformation across rewrites, thus scaling out the rewrite\nselectionimpactregardlessofthereactivityofthegeneral-\nizedsetting.\nFrictionrateprediction\nMotivated by the requisite for bridging the reactivity gap\nandgeneralizingfrictiondistributionsacrossutterancesin\ntraffic,theCRSaimstoinferthefrictionratesforrewrites\nin this contextualized setting by ultimately establish-\ning two conditional distributions,ğ‘ƒ(ğ‘Œ|ğ‘¢) and ğ‘ƒ(ğ‘Œ|ğ‘¢\nğ‘ ,ğ‘¢ğ‘¡)\nwhereğ‘Œ correspondstoabinaryresponsevariabledenot-\ning the friction indicator whileğ‘¢, ğ‘¢ğ‘ ,a n dğ‘¢ğ‘¡ each corre-\nspondtoanyvalidutterance,thesourceutterance,andthe\ntargetutterance,thatis,therewrite,respectively.Now,in\nlearningtheseconditionaldistributions,theCRSabstracts\ntheinputspaceleveragingafeatureextractor, ğœ™âˆ¶ğ‘¢â†’ğ‘‹\nğ‘¢\nto contextualize the inputs, that is, utterances and there-\nafteremployslogisticregressionclassifiersinconstructing\nthefollowingpredictivemodels:\nğ‘ƒ(ğ‘Œ|ğ‘¢) âˆ¼ ğ‘ƒ(ğ‘Œ |ğ‘‹\nğ‘¢)\nğ‘ƒ(ğ‘Œ|ğ‘¢ğ‘ ,ğ‘¢ğ‘¡)âˆ¼ ğ‘ƒ ( ğ‘Œ |ğ‘‹ğ‘ ,ğ‘‹ğ‘¡) (13)\nHere,thefeaturespaceforeachutteranceincludes(but\nisnotlimitedto)thedomain,intent,NER,andentityreso-\nlutionvalues.Extendingthisformulationbacktotheorig-\ninalexample,weobservethatboththerewrites,â€œplaythe\nsonga.b.c .â€andâ€œplaythea.b.c.song â€fortheoriginalutter-\nance, â€œplay a. b. c.â€ would fundamentally have the same\nfeaturespaces, ğ‘‹\nğ‘  andğ‘‹ğ‘¡ purelyonthebasisofhavingsim-\nilarresolvedinterpretations.Assuch,thepredictedfriction\nrateswouldalsoinevitablybethesameandthusallowing\nthemodeltogeneralizeitslearningacrossutterances.\nAsamatterofperformance,thesetwopredictivemodels\nachieveanROC-AUCof66percentand62percent,respec-\ntively within the confines of our testing dataset compris-\ningofmillionsofhistoricalutterancesbuiltupontheafore-\nmentionedAlexalogdata.Inadditiontothepredictedfric-\ntionbinaries,thestandarderrorsbasedontheasymptotics\nofgeneralizedlinearmethods(GLMs)(1985)andthedelta\nAIMAGAZINE 53\nTABLE 3 Z-testonpredictedfrictionrates\nFrictionrate\n(SE)\np-\nvalue Decision\nOriginalutterance:playa.\nb.c.\n78.5%(1.26%)\nRewritecandidate:play\nthea.b.c.song\n32.8%(0.73%) 1.0 Better\nRewritecandidate:play\nthealphabetsong\n34.1%(6.27%) 0.999 Better\nOriginalutterance:play\nbigshrimp\n15.3%(1.49%)\nRewritecandidate:play\nbigshrimpbyflatbush\nzombies\n17.1%(6.87%) 0.597 Tie\nOriginalutterance:play\nhappierbyd.j.\nmarshmello\n30.9%(9.43%)\nRewritecandidate:play\nhappier\n60.5%(8.14%) 0.009 Worse\nmethod(1935)arecomputedacrossthedatasettocapture\nrelevantuncertainties.\nIn the interest of better evaluating the functional\nperformance of the models in selecting rewrites, we\nannotated 152 random samples of rewrites that were\ndecidedly chosen to be removed by the models, and we\nobserved that 113 of them are veritable, leading toward\nan effective precision of 74.3 percent. Additionally, we\nestimated the overall percentage of bad distinct rewrites\nin our traffic to be approximately 6.4 percent, based on a\nseparate annotation of 921 random rewrites. Now, given\nthat approximately 12.9 percent of them are reactively\nremoved, we conclude that this effectively translates to\nthe recall of our rewrite selection system. It is worth\nmentioninghoweverthatthesenumberwouldbefurther\ninflatediftheywereotherwisere-weightedbytraffic.\nDecisionmodeling\nWiththepredictivemodelsbeingabletogeneralizefriction\ndistributionsacrossutterances,weturntoleveragingthem\ninmodelingthedecisionboundariesforrewriteselection.\nIn particular, consider the examples in Table 3 where the\nrewrites:\nâ€œplaya.b.c.â€ â†’ â€œplaythea.b.c.songâ€\nâ€œplaya.b.c.â€ â†’ â€œplaythealphabetsongâ€\nbothsignificantlyreducethefrictionratefrom78.5percent\n(with a standard error, SE of 1.26 percent) to 32.8 percent\n(0.73percent)and34.1percent(6.27percent),respectively,\nandthusevidentlyimprovingtheoveralluserexperience.\nIn contrast, it is far from arguable that the rewrite â€œplay\nhappier by d. j. marshmelloâ€â†’ â€œplay happierâ€h a ss i g n i f -\nicantly worsened the friction rate, that is, from 30.9 per-\ncent(9.43percent)to60.5percent(8.14percent).However,\nnotallrewriteshavesuchmarkeddecisionboundariesfor\nstrict selection. In the rewrite, â€œplay big shrimpâ€â†’ â€œplay\nbig shrimp by flatbush zombies,â€ with 95 percent confi-\ndence intervals of their predictions heavily overlapping\nwith each other, that is,15.3 percent Â± 1.96âˆ—1.49 percent\nand 17.1 percent Â± 1.96âˆ—6.87 percent, the decision was\nrestricted to a mere tie. Extending from this, we look\ntowardtwoseparatedecisionmakingstrategies,namelya\nconservativesettingandamoreexplorativeapproach.\nConservative:Thegoalhereistobemorerisk-averseso\nas to ensure safety of the system, whereby explorations\naredisfavoredanddecisionsaresolelybasedonsignificant\ninsurmountableevidence.Whilestillleveragingthenotion\nof ğ‘-tests, we directly utilize the predicted friction rates\nand their corresponding standard errors in lieu of infer-\nringthemeanandstandarderrorfromempiricalobserved\nfrictionrates.Applyingasignificancelevelof ğ›¼=0 . 0 5,in\nthesefourexamples,wewillonlyremovetherewriteâ€œplay\nhappierbyd.j.marshmello\" â†’ â€œplayhappierâ€andpreserve\ntheviabilityoftheotherrewritesinproductiontraffic.\nExplorative: The goal here instead would be to opti-\nmizethelong-termperformancebyaggressivelyexploring\nrewritesuntilsuchatimesignificantconfidencehasbeen\naccrued to reach a decision. Suppose that the initial pre-\ndicted friction rate is not too high, but the SE is rather\nsufficiently high as is the case with the rewrite, â€œplay big\nshrimpâ€â†’â€œplaybigshrimpbyflatbushzombiesâ€wherethe\nfrictionrateis17.1percent,butwitharatherhighSE6.87\npercent. This means its true friction rate could fluctuate\nbetweenlower(downto âˆ¼10percentifweuse1SEasthe\nfluctuation) and higher values. Now, should an increase\ninobservationvolumeloweritsstandarderror,wewould\ntheninevitablyconcludethatitasagoodrewriteifthenew\npredictedfrictionrateisclosertothetuneof13percent(1\npercent),andabadrewriteifinsteaditiscloserto18per-\ncent(1percent).Eveniftherewriteturnsouttobebadafter\nouractiveexplorations,thedataisstillnotwasted,asthe\ninvestmentisonlyshort-termandthedataforaparticular\nrewriteisalsosharedinmodellearningbyotherrewrites\nwithsimilarfeatures.\nThis kind of reward-driven exploration ideas has been\nwellresearchedinmulti-armbanditalgorithms.Asamat-\nter of reference, LinUCB (2010) and derived applications\nusingpenalizedlogisticregressionasmentionedin(2011).\nHere,UCBstandsforupperconfidencebound.Ourexam-\nple of using17.1 percent âˆ’ 6.87 percentfor the possible\ntruefrictionrateofâ€œ playbigshrimpâ€ â†’â€œplaybigshrimpby\nflatbushzombiesâ€issimilartoUCBideas.Strictlyapplying\n54 AIMAGAZINE\nUCB ideas in our example, we make the decision based\non the lower bound of predicted friction rate, that is,\nrewrite17.1 percent âˆ’ a âˆ—6.87 percent,asoppositetoorig-\ninal utterance15.3 percent âˆ’ ğ‘ âˆ— 1.49 percent.H e r eğ‘ is\na technical parameter related to sample size and confi-\ndence. Additionally, from a Thompson Sampling (1933)\nandrelatedapplicationsforexample(2010),and(2017)per-\nspective,onecanconsiderhavingthreearmsfortheexam-\nple â€œplay a. b. c.â€ in Table 3 â€“ no-rewrite, rewrite â€œplay\nthea.b .c.song ,â€ or rewrite â€œplaythealphabetsong.â€ The\nideawouldbealongthisline:everytimeweseeanutter-\nancetrafficwithâ€œ playa.b.c .,â€wedrawonerandomsam-\nplefromeacharmâ€™spredicteddefectdistributionapproxi-\nmatedbyGaussiandistributionwithestimatedmeanand\nSE,N(78.5 percent,1.26 percent),N(32.8 percent, 0.73 per-\ncent)and N(34.1percent,6.27percent).Thenweselectthe\narmwiththelowestsampledfrictionrateasourdecision\nforthatparticulartraffic.\nAPPLICATIONDEPLOYMENT\nOfflinerewritemining\nSince there are thousands of new utterances per day, and\nthere are constant changes to the upstream and down-\nstream systems in Alexa on a daily basis, it is important\nto update our rewrites on a regular basis to remove stale\nandineffectiverewrites.Werundailyjobstominethemost\nrecentrewritesinanofflinefashion.Thisallowsustofind\nthemostrecentrewritesandservethemtousers.Itisnote-\nworthy that in case of conflicts between the rewrites, we\npick the most recent rewrite, since it has the latest data.\nWehaveonlinealarmsandmetricstomonitordailyjobs,\nsincesometimeschangestotheupstreamanddownstream\nAlexa components can impact our rewrite mining algo-\nrithm.Incaseoflargechangesinourmetrics,wedoadive\ndeepintothedatatofindtherootcause.\nOnlineservice\nSince the graph is static during the period it is used, and\nthere are many repetitive utterances per day, we opted\nto mine the rewrites as key-value pairs, where the orig-\ninal utterance is the key, and the rewrite is the value.\nFor example, we store â€œplay babe sharkâ€ â†’ â€œplay baby\nsharkâ€ as one entry. We then serve these pairs in a high-\nperformance database to meet the low latency require-\nment.Thisallowsustodecoupletheofflineminingprocess\nandtheonlineservingprocessforhighavailabilityandlow\nlatencyrequirements.\nRewriteselectionsystem\nDuetothequickchangeofcontentsinAlexatrafficaswell\nas its evolving ASR & NLU systems, graph alone is not\nquickenoughtoadapttoallchanges.Theguardrailrewrite\nselectionsystemrunsitsalgorithmsevery4h,refreshand\nstoreselectionactionsforallrewritepairsinanotherhigh-\nperformance database that is coupled with graph online\nservice.\nOnlineperformance\nFollowing the offline analysis and traffic simulations, we\nlaunchedthegraphrewritesinproductioninanA/Btest-\ningsetup.Wemonitoredtheperformanceofourrewrites\nagainst no-rewrites for over 2 weeks, and we observed\nmorethan30percentaveragereductionindefectrate( ğ‘âˆ’\nvalue < 0.001),helpingmillionsofusers.Here,thenotion\nofdefectisbasedonanMLmodelwhichscoresuserdis-\nsatisfactionateveryturn.Inaseparate9weekrandomized\ncontroltrial,wealsonotedasdefectdecreased,anewdia-\nloginteractionwascreatedforeverytwocorrectionsmade\nbythesystem(ğ‘ âˆ’ value < 0.01),whichinturntranslates\ntogreateruserengagement.Wefurthermeasuredthewinâ€“\nlossratio3monthsafterthesystemâ€™sreleasebytallyingthe\nnumber of unique rewrites where rewriting significantly\nimproved i.e. wins â€“ or worsened i.e. losses over the no-\nrewriteoption(weused Z-testtotestthesignificance,and\nset ğ‘-value threshold of0.01). The post-launch winâ€“loss\nratiocloselymatchedourofflineestimate(11.8 onlinevs.\n12.0 offline).\nWe have been running this application for over 15\nmonths in production, and it has been serving millions\nofusers,sinceimprovingtheirexperienceonadailybasis\nwithoutgettingintheirway.Weknowthisforafactsince\nwehavebeenmonitoringcustomersatisfactionmetricson\na weekly basis. We monitor the total number of rewrites,\nand the average friction rate for the rewrites, along with\naverage friction for no-rewrites, where for the latter two,\ntheaforementioned30percentmarginstillprevails.Ontop\noftrackingonlinemetrics,wecontinuedoingofflineevalu-\nationsonaweeklybasis,wherewesampleourtraffic,and\nsend it for annotation. Combining the online and offline\nmetrics in a longitudinal fashion allows us to closely fol-\nlowthechangesinthecustomerexperience,whichisthe\nultimatemetricforoursystem.\nCONCLUSION\nAsconversationalagentsbecomemorepopularandgrow\ninto new scopes, it is critical for these systems to have\nAIMAGAZINE 55\nself-learning mechanisms to fix the recurring issues con-\ntinuouslywithminimalhumanintervention.Inthispaper,\nwe presented a self-learning system that is able to effi-\nciently target and rectify both systemic and customer\nerrors at runtime by means of query reformulation. In\nparticular, we proposed a highly scalable collaborative-\nfilteringmechanismbasedonanabsorbingMarkovchain\nto surfacesuccessful utterance reformulations in conver-\nsational AI agents. Our system achieves a high precision\nperformancethankstoaggregatinglargeamountsofcross-\nuserdatainanofflinefashion,withoutadverselyimpact-\ning usersâ€™ perceived latency by serving the rewrites in a\nlook-up manner online. This coupled with our rewrite\nselectionmechanismwhichreactivelyevaluatestheviabil-\nityofrewritesatagreatercadencehashelpedmaintainthe\naforementionedhighprecisionatruntime.Wehavetested\nand deployed our system into production across millions\nofusers,reducingcustomerfrictionsbymorethan30per-\ncent and achieving a winâ€“loss ratio of11.8. Our solution\nhas been customer-facing for over 36 months now, and it\nhashelpedmillionsofuserstohaveamoreseamlessexpe-\nriencewithAlexa.\nACKNOWLEDGEMENTS\nTheauthorswouldverymuchliketothankStevenWasik\nandhisteamfortheiranalysisintocustomerengagement\nandJinHockOngandhisteam,particularlyKarenStabile\nandVincentLyfortheircontinuedengineeringsupportin\nmaintainingandmonitoringthesystemasawhole.\nREFERENCES\nChapelle, O., and L. Li. 2011. â€œAn empirical evaluation of thomp-\nsonsampling.â€InAdvancesinNeuralInformationProcessingSys-\ntems,NIPSâ€™11,2249â€“57.Granada,Spain:CurranAssociates,Inc.\nDevlin, J., M. Chang, K. Lee, and K. Toutanova. 2018. â€œBERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding.â€CoRRabs/1810.04805.\nDoob,J.L.1935.â€œThelimitingdistributionsofcertainstatistics.â€ The\nAnnalsofMathematicalStatistics 6(3):160â€“9.\nFahrmeir,L.,andH.Kaufmann.1985.â€œConsistencyandasymptotic\nnormality of the maximum likelihood estimator in gener- alized\nlinearmodels.â€ TheAnnalsofStatistics 13:342â€“68.\nFouss,F.,S.Faulkner,M.Kolp,A.Pirotte,andM.Saerens2005.â€œWeb\nrecommendationsystembasedonamarkov-chainmodel.â€InPro-\nceedings of the Seventh International Conference on Enterprise\nInformationSystems,56â€“63.Miami,USA:SciTePress.\nGao, J., M. Galley, and L. Li. 2018. â€œNeural approaches to conversa-\ntionalAI.â€ CoRRabs/1809.08267.\nGraepel,T.,J.Q.Candela,T.Borchert,andR.Herbrich.2010.â€œWeb-\nscalebayesianclick-throughratepredictionforspon-soredsearch\nadvertisinginmicrosoftâ€™sbingsearchengine.â€â€™InProceedingsof\nthe27thInternationalConferenceonInternationalConferenceon\nMachine Learning, ICMLâ€™10, 13â€“20. Madison, WI, USA: Omni-\npress.\nGrinstead, C. M., and J. L. Snell. 1997.Introduction to Probability.\nAmericanMathematicalSociety.\nHill,D.N.,H.Nassif,Y.Liu,A.Iyer,andS.Vishwanathan.2017.â€œAn\nefficientbanditalgorithmforrealtimemultivariateoptimization.â€\nIn Proceedings of the 23rd ACM SIGKDD International Confer-\nenceonKnowledgeDiscoveryandDataMining,1813â€“21.Halifax,\nNS,Canada:AssociationforComputingMachinery.\nJansen,B.J.,D.L.Booth,andA.Spink.2005.â€œPatternsofqueryrefor-\nmulationduringwebsearching.â€ JournaloftheAmericanSociety\nforInformationScienceandTechnology 60(7):1358â€“71.\nKhorasani,E.S.,Z.Zhenge,andJ.Champaign.2016.â€œAmarkovchain\ncollaborativefilteringmodelforcourseenrollmentrecommenda-\ntions.â€InIEEEInternationalConferenceonBigData,3484-3490.\nWashington,DC,USA:IEEE.\nLi,L.,W.Chu,J.Langford,andR.E.Schapire.2010.â€œAcontextual-\nbanditapproachtopersonalizednewsarticlerecommen-dation.â€\nIn Proceedings of the 19th International Conference on World\nWideWeb,661â€“70.Raleigh,NorthCarolina,USA:Associationfor\nComputingMachinery.\nMohri,M.,F.Pereira,andM.Riley.2002.â€œWeightedfinite-statetrans-\nducersinspeechrecognition.â€ ComputerSpeech&Language 16(1):\n69â€“88.\nSahoo, N., P. V. Singh, and T. Mukhopadhyay. 2012. â€œA hidden\nMarkovmodelforcollaborativefiltering.â€ MISQuarterly 36:1329â€“\n56.\nSarikaya,R.2017.â€œThetechnologybehindpersonaldigitalassistants:\nAn overview of the system architecture and key compo- nents.â€\nIEEESignalProcessingMagazine 34(1):67â€“81.\nSee,A.,P.J.Liu,andC.D.Manning.2017.â€œGettothepoint:Summa-\nrizationwithpointer-generatornetworks.â€ Proceedingsofthe55th\nAnnualMeetingoftheAssociationforComputationalLinguistics 1:\n1073â€“83.\nSutskever, I., O. Vinyals, and Q. V. Le. 2014. â€œSequence to sequence\nlearningwithneuralnetworks.â€InProceedingsofthe27thInter-\nnational Conference on Neural Information Processing Systems,\n3104â€“3112.Montreal,Canada:MITPress.\nTerveen,L.,andW.Hill.2001.â€œBeyondrecommendersystems:Help-\ningpeoplehelpeachother.â€In TheNewMillennium,JackCarroll.\nNewYork,N.Y.,USA.\nThompson,W.R.1933.â€œOnthelikelihoodthatoneunknownprob-\nability exceeds another in view of the evidence of two samples.â€\nBiometrika25(3/4):285â€“94.\nWang,J.,J.Z.Huang,andD.Wu.2015.â€œRecommendinghighutility\nqueriesviaquery-reformulatinggraph.â€ MathematicalProblemsin\nEngineering,2015:1â€“14.\nZaharia,M.,R.S.Xin,P.Wendell,T.Das,M.Armbrust,A.Dave,X.\nMeng, et al. 2016. â€œApache spark: A unified engine for big data\nprocessing.â€CommunicationsoftheACM 59(11):56â€“65.\nZhu, X., J. Guo, X. Cheng, and Y. Lan. 2012. â€œMore than rel-\nevance: High utility query recommendation by mining usersâ€™\nsearchbehaviors.â€InCIKMâ€™12,October29â€“November2,Maui,HI,\nUSA.\nAUTHOR BIOGRAPHIES\nPragaashPonnusamy isanappliedscientistatAma-\nzonAlexa AI.Hereceived hisB.S.degreein electrical\nengineeringandcomputersciencefromtheUniversity\nofCalifornia,Berkeleyin2016.\n56 AIMAGAZINE\nAlirezaRoshanGhias isanappliedsciencemanager\natAmazon.HereceivedhisB.SdegreefromtheUniver-\nsityofTehranin2004andhisM.S.degreefromSharif\nUniversity of Technology in 2006, both in Mechani-\ncal Engineering. In 2012, he received his Ph.D. degree\nfrom Ecole Polytechnique FeÅ¥deÅ¥rale de Lausanne in\nBiomedicalEngineering.\nYi Yi is a senior applied scientist at Amazon. He\nreceived his B.S degree in Mathematics and Applied\nMathematicsfromShanghaiUniversityin2010;andhis\nPh.D.degreeinstatisticsfromtheUniversityofCalifor-\nnia,LosAngelesin2016.\nBenjaminYao isaseniorappliedsciencemanagerat\nAlexaAIatAmazon.HereceivedhisB.SdegreeinElec-\ntrical Engineering from the University of Science and\nTechnology of China in 2003 and his M.S. degree in\nImageAnalysisin2006fromChineseAcademyofSci-\nences.In2011,hereceievedhisPh.D.degreeinStatistics\nfromtheUniversityofCalifornia,LosAngeles.\nChenlei Guo is the director of applied science at\nAmazon. He received his B.S. degree in 2005 and his\nM.S. degree in 2008 from Fudan Univeristy, both in\nElectronicsEngineering.In2009,hereceivedhisM.S.\ndegree in Computer Engineering from Carnegie Mel-\nlon University. He was a principal engineering man-\nager at Microsoft from 2009 to 2017, where he led the\nteam to launch multiple products such as the entity\nranking for Bing search, the office insight and peo-\nple/location/timesearchinOffice365product.\nRuhi Sarikayais the director of applied science at\nAmazon. He received his B.S. degree from Bilkent\nUniversity, Ankara, Turkey, in 1995; his M.S. degree\nfrom Clemson University, South Carolina, in 1997;\nand his Ph.D. degree from Duke University Durham,\nNorth Carolina, in 2001, all in Electrical and Com-\nputer Engineering. He was a principal science man-\nageratMicrosoftfrom2011to2016,wherehefounded\nandmanagedtheteamthatbuiltlanguageunderstand-\ninganddialogmanagementcapabilitiesofCortanaand\nXboxOne.BeforeMicrosoft,hewaswithIBMResearch\nfor 10 years. Prior to joining IBM in 2001, he was a\nresearcherattheUniversityofColoradoatBoulderfor\n2years.\nHowtocitethisarticle: Ponnusamy,P.,A.R.\nGhias,Y.Yi,B.Yao,C.Guo,andR.Sarikaya.2021.\nâ€œFeedback-basedself-learninginlarge-scale\nconversationalAIagents.â€ AIMagazine 42:43â€“56.\nhttps://doi.org/10.1609/aaai.12025\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.856630802154541
    },
    {
      "name": "Annotation",
      "score": 0.6464449167251587
    },
    {
      "name": "Natural language understanding",
      "score": 0.6186366677284241
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5894371271133423
    },
    {
      "name": "Scope (computer science)",
      "score": 0.5587055683135986
    },
    {
      "name": "Dialog system",
      "score": 0.5309998989105225
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.45393455028533936
    },
    {
      "name": "Machine learning",
      "score": 0.40552830696105957
    },
    {
      "name": "Natural language processing",
      "score": 0.3859933316707611
    },
    {
      "name": "Natural language",
      "score": 0.3561522960662842
    },
    {
      "name": "World Wide Web",
      "score": 0.1394847333431244
    },
    {
      "name": "Dialog box",
      "score": 0.11802992224693298
    },
    {
      "name": "Programming language",
      "score": 0.0878690779209137
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210089985",
      "name": "Amazon (Germany)",
      "country": "DE"
    }
  ],
  "cited_by": 20
}