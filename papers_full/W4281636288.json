{
  "title": "Stabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer",
  "url": "https://openalex.org/W4281636288",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101613621",
      "name": "Minrui Wang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5003907234",
      "name": "Mingxiao Feng",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5046805800",
      "name": "Wengang Zhou",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5078141810",
      "name": "Houqiang Li",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2145761514",
    "https://openalex.org/W3035404531",
    "https://openalex.org/W3152881947",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2617547828",
    "https://openalex.org/W2768629321",
    "https://openalex.org/W2064031160",
    "https://openalex.org/W3131436982",
    "https://openalex.org/W3214748135",
    "https://openalex.org/W2998367975",
    "https://openalex.org/W3037822235",
    "https://openalex.org/W2757615817",
    "https://openalex.org/W3208346261",
    "https://openalex.org/W3020785977",
    "https://openalex.org/W2747213132"
  ],
  "abstract": "The increased integration of renewable energy poses a slew of technical\\nchallenges for the operation of power distribution networks. Among them,\\nvoltage fluctuations caused by the instability of renewable energy are\\nreceiving increasing attention. Utilizing MARL algorithms to coordinate\\nmultiple control units in the grid, which is able to handle rapid changes of\\npower systems, has been widely studied in active voltage control task recently.\\nHowever, existing approaches based on MARL ignore the unique nature of the grid\\nand achieve limited performance. In this paper, we introduce the transformer\\narchitecture to extract representations adapting to power network problems and\\npropose a Transformer-based Multi-Agent Actor-Critic framework (T-MAAC) to\\nstabilize voltage in power distribution networks. In addition, we adopt a novel\\nauxiliary-task training process tailored to the voltage control task, which\\nimproves the sample efficiency and facilitating the representation learning of\\nthe transformer-based model. We couple T-MAAC with different multi-agent\\nactor-critic algorithms, and the consistent improvements on the active voltage\\ncontrol task demonstrate the effectiveness of the proposed method.\\n",
  "full_text": "Stabilizing Voltage in Power Distribution Networks via\nMulti-Agent Reinforcement Learning with Transformer\nMinrui Wangâˆ—\nMingxiao Fengâˆ—\nwangminrui0804@mail.ustc.edu.cn\nfmxustc@mail.ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nWengang Zhouâ€ \nzhwg@ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nInstitute of Artificial Intelligence,\nHefei Comprehensive National\nScience Center\nHefei, Anhui, China\nHouqiang Liâ€ \nlihq@ustc.edu.cn\nUniversity of Science and Technology\nof China\nHefei, Anhui, China\nInstitute of Artificial Intelligence,\nHefei Comprehensive National\nScience Center\nHefei, Anhui, China\nABSTRACT\nThe increased integration of renewable energy poses a slew of\ntechnical challenges for the operation of power distribution net-\nworks. Among them, voltage fluctuations caused by the instability\nof renewable energy are receiving increasing attention. Utilizing\nMARL algorithms to coordinate multiple control units in the grid,\nwhich is able to handle rapid changes of power systems, has been\nwidely studied in active voltage control task recently. However,\nexisting approaches based on MARL ignore the unique nature of\nthe grid and achieve limited performance. In this paper, we in-\ntroduce the transformer architecture to extract representations\nadapting to power network problems and propose a Transformer-\nbased Multi-Agent Actor-Critic framework (T-MAAC) to stabilize\nvoltage in power distribution networks. In addition, we adopt a\nnovel auxiliary-task training process tailored to the voltage control\ntask, which improves the sample efficiency and facilitates the rep-\nresentation learning of the transformer-based model. We couple\nT-MAAC with different multi-agent actor-critic algorithms, and\nthe consistent improvements on the active voltage control task\ndemonstrate the effectiveness of the proposed method.1\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Multi-agent reinforcement\nlearning.\nKEYWORDS\nMulti-agent Reinforcement Learning, Active Voltage Control, Trans-\nformer\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding authors: Wengang Zhou and Houqiang Li.\n1Code will be released at https://github.com/cjdjr/T-MAAC.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9385-0/22/08. . . $15.00\nhttps://doi.org/10.1145/3534678.3539480\nACM Reference Format:\nMinrui Wang, Mingxiao Feng, Wengang Zhou, and Houqiang Li. 2022.\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Re-\ninforcement Learning with Transformer. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™22), Au-\ngust 14â€“18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 12 pages.\nhttps://doi.org/10.1145/3534678.3539480\n1 INTRODUCTION\nThe development and utilization of renewable energy is critical for\naddressing current energy and environmental concerns. In recent\nyears, distributed generations (DGs), e.g., roof-top photovoltaics\n(PVs), have been steadily connected to power distribution networks\nbecause of its particular environmental friendliness, economy and\nflexibility. However, the increasing penetration of PVs in the distri-\nbution network may cause voltage swing due to their rapid active\npower changes. The voltage fluctuation can be alleviated by uti-\nlizing the control flexibility provided by PV inverters and other\ncontrollable devices such as static var compensators (SVCs)[ 16].\nTherefore, an elaborate scheme is required to coordinate the control\nbetween these distributed devices based on local information to\nensure stable operation of the entire power system, which is called\nactive voltage control[26].\nThere have been some previous efforts dedicated to active volt-\nage control, which can be classified into three broad groups from\nthe perspective of the control framework: centralized, distributed\nautonomous, and distributed cooperative control[4]. As a promising\nsolution, the distributed cooperative control enables collaboration\nbetween distinct control units via limited communication links.\nAmong them, some approaches[3â€“5, 16, 27] apply multi-agent rein-\nforcement learning (MARL) to active voltage control. These MARL\nalgorithms are based on the centralized training and decentral-\nized execution framework, which extract knowledge from histori-\ncal data and simulation environment. The learned strategies can\nbe deployed to the grid and achieve cooperative control without\nany communication devices. These attempts of applying MARL to\npower network tasks have attracted a lot of attention because of\ntheir strong adaptability to the unknown dynamic in real-time.\nFor active voltage control problem, distributed control units are\ntreated as agents, observing information about nodes in a zone\nof the grid. Figure 1 shows an example, in which the whole grid\narXiv:2206.03721v1  [cs.MA]  8 Jun 2022\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\nFigure 1: An example in power distribution network. Each\nbus in the distribution network is considered as a node in\nthe graph. The whole network are divided into 4 zones con-\nnected to the main branch (node 1-4). We need to control\nvoltages on node 1-13 and node 0 represent the main grid\nwith constant voltage. G denotes an external generator; L de-\nnotes load connected to the node; and the sun emoji denotes\nthe location a PV installed in.\nis divided into three zones and the sun emoji represents the lo-\ncation where a PV is installed. Each PV inverter installed in a\nPV is regarded as a agent and observes information of nodes in\nthe corresponding zone. In prior works[ 3â€“5, 16, 27], the MLP-\nbased policy networks and MLP-based critic network are applied\nto parameterize the policy functions and action-value functions,\nrespectively[1, 10, 12, 19, 30]. Besides, additional paddings are ap-\npended to each observation to guarantee that all observations\nhave the same dimension. After that, the padded observations are\nmapped to control actions via policy networks whose parameters\nare shared among all agents.\nIt is worth noting that directly applying the above routine set-\ntings in the MARL community to the active voltage control task\nencounters a number of challenges:\n(1) Inconsistent number of nodes observed by various agents.\nFor example, as shown in Figure 1, there are 4 nodes in zone 1\nand 3, but only 1 node in zone 2.\n(2) Inconsistent topology of nodes in each observation. In\nFigure 1, both zone 1 and zone 3 include 4 nodes, however\nthey are connected in quite different ways.\n(3) Inconsistent importance of nodes in a zone. For example,\ntypically, node 6 installed with PV has more frequent voltage\nfluctuation than other nodes in zone 1, which means that the PV\n1 must take more attention to node 6 when making decisions.\nCurrent methods ignore these challenges and simply concatenate\nnodes information directly to obtain observations, with an assump-\ntion that neural networks are capable of automatically modeling\nthe relationship between nodes in a sub-grid and decoupling ob-\nservations smartly to address above challenges. By following these\nsettings, these approaches handle all information received from\ndifferent agents in a same way and treat nodes in a observation\nuniformly with no regard of the topology of these nodes in the\nzone, which leads to limited representation learning capability.\nTo address the above challenges, we propose a transformer [25]\nbased multi agent actor-critic framework (abbreviation as T-MAAC)\nfor active voltage control, which can couple with mainstream multi-\nagent actor-critic algorithms. Specifically, we propose a transformer-\nbased policy network and a transformer-based critic network to\nobtain discriminative representations. For the policy network, we\ndivide the whole observation into node-based entities and project\nthese node-based entities to high-dimensional semantic space via\na transformer encoder. Inspired by [22, 29], an adjacency matrix\nrepresenting the connectivity of nodes in the zone is treated as\nthe mask in the self-attention mechanism, introducing topologi-\ncal information to enhance representations. Then, we propose an\nembedding aggregation module to aggregate node-based informa-\ntion in the zone into the embedding from the agentâ€™s (control unit)\npoint of view. To address the instability of transformer architecture\nin MARL algorithms[14, 22], we develop a novel self-supervised\nauxiliary task in the training process of policy networks. For the\ncritic network, we exploit the vanilla transformer layer to approxi-\nmate the global Q-value function. We introduce the self-attention\nmechanism to model the correlations between agents from the scale\nof the entire grid. Experiments on the MAPDN environment[26]\ndemonstrate the effectiveness of our approach.\nIn summary, our main contributions are three-fold as follows:\nâ€¢We propose a novel transformer-based multi-agent actor-critic\nframework for active voltage control task and improve the perfor-\nmances of the existing multi-agent actor-critic algorithms from\nthe perspective of voltage regulation and energy loss.\nâ€¢We adopt a self-supervised auxiliary task to stabilize the training\nprocess of MARL algorithms with transformer, improving the\nsample efficiency and facilitating the representation learning.\nâ€¢We introduce the attention mechanism into the voltage control\nin power network tasks, assisting a control unit in elaborating\ncooperative control strategies with other control units. It is more\nexplainable and facilitates the MARL-based methods deploying\nto the realistic power system.\nThe rest of this paper is organized as follows. We first give a\nliterature review on the related work in Section 2. Then the back-\nground of active voltage control and the formulation of Markov\nGames are elaborated in Section 3. The methodology is described in\ndetail in Section 4. Simulation results and discussions are presented\nin Section 5. Finally, we conclude our work in Section 6.\n2 RELATED WORKS\n2.1 Multi-Agent Reinforcement Learning for\nActive Voltage Control\nAdvances in machine learning lead to widespread applications of\nmulti-agent reinforcement learning techniques to tackle active volt-\nage control problem. In [ 4], authors take advantage of spectral\nclustering algorithms to partition the large distribution power net-\nwork into several zones and formulate the control between each\nzones as Markov Game solved by MATD3. [ 3] introduced an at-\ntention mechanism in the critic network to enhance scalability of\nalgorithms. In contrast to [3], we not only introduce attention mech-\nanisms to model the relationship of agents in the critic network,\nbut also propose transformer-based architecture adapting to the\ngrid topology in the policy network. [ 27] developed a approach\nwith a manually designed voltage inner loop for the autonomous\nvoltage control of transmission network based on MADDPG. [5]\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nleverages the sparse pseudo-Gaussian process to build a surrogate\nmodel using few-shot recorded data and control actions based on\nmulti-agent soft actor critic algorithms. Above prior works divided\nthe whole grid into several zones, with each zone controlled by\na single agent. However, MAPDN[26] modeled the active voltage\ncontrol problem as a Dec-POMDP[21] and each PV inverter is con-\ntrolled by an agent. In this paper, we also follow the settings of\nDec-POMDP proposed in MAPDN, which enables the presence of\nmultiple agents with similar observations in a zone.\n2.2 Attention Mechanisms and Transformer in\nMulti-Agent Reinforcement Learning\nTransformers[25] have been applied successfully to solve a wide\nvariety of tasks in natural language processing[8] and computer\nvision[6, 9]. However, transformers have not yet been fully explored\nin multi-agent reinforcement learning, mostly due to differing na-\nture of problem, such as high variance in training. UPDet[14] pro-\nposed the transformer-based individual value function and policy\ndecoupling strategy based on value-based MARL algorithms to\nachieve improvements on multi-agent games in the StarCraft II\nenvironment with discrete action space. Hierarchical RNNS-Based\nTransformers MADDPG (HRTMADDPG)[28] combined transform-\ners with RNNs, capturing the causal relationship between sub-time\nsequences. [17] proposed a self-attention-based multi-agent contin-\nuous control algorithms to solve the problem of uneven learning\ndegree and improved learning efficiency when faced with more\nagents. The above works showed promising performance on game-\nlike environments with a few agents. However, it is not yet clear\nwhether multi-agent algorithms with transformer can still achieve\ncompetitive performance if applied to the power system with large-\nscale agents, i.e. there are 38 agents on 322-bus network in MAPDN\nenvironment[26].\n3 PROBLEM FORMULATION\n3.1 Active Voltage Control on Power\nDistribution Networks\nIn this paper, a power distribution network installed with roof-top\nphotovoltaics (PVs) is modeled as a tree graph structure G= (ğ‘‰,ğ¸ )\nshown in Figure 1, where ğ‘‰ = {0,1,...,ğ‘ }and ğ¸ = {1,2,...,ğ‘ }de-\nnote the set of nodes (buses) and edges (branches), respectively[11].\nFor bus ğ‘– âˆˆğ‘‰, let ğ‘£ğ‘– and ğœƒğ‘– be the magnitude and phase angle of\ncomplex voltage and ğ‘ ğ‘– = ğ‘ğ‘– +ğ‘—ğ‘ğ‘– denotes the complex power in-\njection. There are complex and non-linear relationships between\nthese physical quantities that satisfy the power system dynam-\nics rules[26]. In particular, node 0 is connected to the main grid,\nwhich serves to balance the active and reactive power in the dis-\ntribution network. Nodes in the distribution network are divided\ninto several zones based on their shortest path from the termi-\nnal to the main branch[26]. Also, loads (e.g. residential and indus-\ntrial clients) and PVs are connected to some of nodes. Each PV\nis equipped with an PV inverter that generates reactive power to\ncontrol the voltage around the standard value denoted as ğ‘‰ğ‘Ÿğ‘’ğ‘“ . For\nsafe operation of the distribution network, 5% voltage fluctuation\nis usually allowed. Let ğ‘£0 = 1.0 per unit (ğ‘.ğ‘¢.), the voltage ampli-\ntude of each bus must satisfy the following inequality condition:\n0.95ğ‘.ğ‘¢. â‰¤ğ‘£ğ‘– â‰¤1.05ğ‘.ğ‘¢.,âˆ€ğ‘– âˆˆğ‘‰ \\{0}. In the middle of day, the\nsolar energy is converted into electrical energy and injected into\nthe distribution network via PVs, which would increaseğ‘£ğ‘– out of\nthe safe range. In contrast, ğ‘£ğ‘– may drop below the 0.95ğ‘.ğ‘¢. due to\nthe heavy load at night. In this paper, we consider each PV inverter\ninstalled in a PV as the control unit.\n3.2 Formulation of Markov Games in Active\nVoltage Control\nThe collaborative control process of PV inverters can be mod-\neled as a Dec-POMDP[ 21] for ğ‘ agents. A Dec-POMDP is usu-\nally defined by a tuple (N,S,{Ağ‘–}ğ‘–âˆˆN,T,ğ‘Ÿ, {Oğ‘–}ğ‘–âˆˆN,Î©,ğ›¾), where\nN = {1,...,ğ‘› }denotes the set of ğ‘› agents, Sdenotes the state\nspace observed by all agents, Ağ‘– denotes the action space of agent\nğ‘–. Let A= Ã—ğ‘–âˆˆNAğ‘–, then T : SÃ—AÃ—Sâ†’[ 0,1]denotes the\ntransition probability from any state ğ‘  âˆˆS to any state ğ‘ â€² âˆˆS\nafter taking a joint action ğ‘ âˆˆ A; ğ‘Ÿ : SÃ—A â†’ R is a global\nreward function that determines the immediate reward received\nby whole agents for a transition from (ğ‘ ,ğ‘)to ğ‘ â€²; O = Ã—ğ‘–âˆˆNOğ‘–\ndenotes the joint observation set, where Oğ‘– is each agentâ€™s obser-\nvation; Î© : SÃ—AÃ—Oâ†’[ 0,1]denotes the perturbation of the\nobservers for agentsâ€™ joint observations over the states after deci-\nsions; ğ›¾ âˆˆ[0,1)is the discount factor. We can formulate the policy\nof the i-th agentâ€™s policy as ğœ‹ğ‘–, and the objective of Dec-POMDP is\nfinding an optimal joint policy ğœ‹ = Ã—ğ‘–âˆˆNğœ‹ğ‘– to maximize expected\nlong-term reward Eğœ‹[Ãâˆ\nğ‘¡=0 ğ›¾ğ‘¡ğ‘Ÿğ‘¡]. Considering the active voltage\ncontrol problem, we describe specific elements in the Dec-POMDP\nin detail as follows, simulated to [26]:\nâ€¢Agent. As shown in Figure 1, each PV is an agent that injects the\nreactive power generated by itâ€™s PV inverter into the distribution\nnetwork so as to maintain the voltage of all buses within the safe\nrange.\nâ€¢Observation. Let ğ‘œğ‘— = (ğ‘ğ¿\nğ‘—,ğ‘ğ¿\nğ‘—,ğ‘£ğ‘—,ğœƒğ‘—,ğ‘“ğ‘™ğ‘ğ‘” ğ‘ƒğ‘‰\nğ‘— )represents the\nnode-based feature of node ğ‘—. ğ‘ğ¿\nğ‘— âˆˆ(0,âˆ)and ğ‘ğ¿\nğ‘— âˆˆ(0,âˆ)are\nactive and reactive power of the load connected to node ğ‘— respec-\ntively; ğ‘£ğ‘— âˆˆ(0,âˆ)and ğœƒğ‘— âˆˆ[âˆ’ğœ‹,ğœ‹ ]denote the voltage magni-\ntude and voltage phase of node ğ‘— respectively; ğ‘“ğ‘™ğ‘ğ‘”ğ‘ƒğ‘‰\nğ‘— âˆˆ{0,1}\nindicates whether the node ğ‘— is installed with a PV. Moreover, ad-\nditional physical quantities (ğ‘ğ‘ƒğ‘‰,ğ‘ğ‘ƒğ‘‰)are appended to the node-\nbased feature for those nodes installed with PV. ğ‘ğ‘ƒğ‘‰ âˆˆ(0,âˆ)\ndenotes the active power generated by PV ğ‘–and ğ‘ğ‘ƒğ‘‰ âˆˆ(âˆ’âˆ,âˆ)\nis the reactive power generated by the PV inverter. Nodes in the\ngrid are partitioned into several zones and the observation of\nagent ğ‘–(denoted as Oğ‘–) is obtained by concatenating node-based\nfeatures in the zone in which agentğ‘–is located. It is worth noting\nthat agents in the same zones have similar observations.\nâ€¢Action. Each agent ğ‘– âˆˆN has a continuous action set Ağ‘– =\n{ğ‘ğ‘– : âˆ’ğ‘ â‰¤ğ‘ğ‘– â‰¤ğ‘,ğ‘ > 0}that denotes the ratio of maximum\nreactive power it can generate. And the joint action set is defined\nas A= Ã—ğ‘–âˆˆNAğ‘–.\nâ€¢Reward Function. The reward function is defined as follows:\nğ‘Ÿ = âˆ’ 1\n|ğ‘‰|\nâˆ‘ï¸\nğ‘–âˆˆğ‘‰\nğ‘™ğ‘£ (ğ‘£ğ‘–)âˆ’ğ›¼Â·ğ‘™ğ‘\n\u0010\nqğ‘ƒğ‘‰\n\u0011\n, (1)\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\nwhere ğ‘™ğ‘£(Â·)is a voltage barrier function and ğ‘™ğ‘ = 1\n|N|\n\r\rqğ‘ƒğ‘‰\r\r1 is\nthe reactive power generation loss. The objective is to learn a opti-\nmal strategy to control the voltage within the safety range around\nğ‘‰ğ‘Ÿğ‘’ğ‘“ (i.e. [0.95ğ‘£ğ‘Ÿğ‘’ğ‘“ ,1.05ğ‘£ğ‘Ÿğ‘’ğ‘“ ]) while minimizing the power loss of\nthe whole distribution network. The hyper-parameter ğ›¼ âˆˆ(0,1)\nis set in advance by simulation environment, which plays the\nrole of balancing the two losses. In practice, we use the reactive\npower generation loss instead of power loss of the whole distri-\nbution network, because obtaining overall power loss of the grid\nin real time is difficult. The voltage barrier function ğ‘™ğ‘£(Â·)penal-\nizes the voltage rise deviation and the voltage drop deviation.\nAs with [26], there voltage barrier functions are considered to\nform reward functions: L1-shape, L2-shape and bowl-shape (see\nAppendix A.3).\nAdditionally, the topology of the grid will be exploited as a\npriori knowledge in our framework. Formally, the connectivity\nbetween nodes in a zone is represented by a adjacency matrix ğ·ğ‘–.\nFurthermore, we divide the raw observation of agent ğ‘– into node-\nbased features based on the prior knowledge:\nOğ‘– = {ğ‘œğ‘–,1,ğ‘œğ‘–,2,Â·Â·Â· ,ğ‘œğ‘–,ğ‘šğ‘– }. (2)\nHere, ğ‘– âˆˆ{1,2,Â·Â·Â· ,ğ‘›}is the index of agents,ğ‘šğ‘– denotes the number\nof nodes in the zone. Let ğ‘ğ‘– âˆˆ{1,2,Â·Â·Â· ,ğ‘šğ‘–}denotes the index of\nnode installed with PV ğ‘–, which makes agent ğ‘– aware of its own\nlocation in the zone.\n4 METHOD\nIn order to apply MARL algorithms to the active voltage control\nproblem while considering the characteristics of grid, we present a\nnovel transformer-based multi-agent actor critic (T-MAAC) frame-\nwork. Specifically, we propose a policy network and a critic network\nbased on transformer as well as auxiliary-task based training pro-\ncess, which is compatible with mainstream multi-agent actor-critic\nalgorithms such as MADDPG[19] and MATD3[1]. In this section,\nwe describe the structure of the policy network and the critic net-\nwork in Section 4.1 and Section 4.2, respectively. The auxiliary task\nto stabilize the training process is discussed in Section 4.3.\n4.1 Transformer-based Policy Network\nTo extract more relevant representations for the active voltage\ncontrol task, we develop a transformer-based policy network as\nshown in Figure 2 to handle various types of observations. We\npresent a mathematical formulation of our transformer-based model\nin this section.\n4.1.1 Projection layer. First of all, we transform the raw observa-\ntion Ointo node-based embeddings via a projection layer.\nIf the observation of agent ğ‘– at time step ğ‘¡ (denoted as Oğ‘¡\nğ‘–) is\nmade up of ğ‘šnode-based features, then all of them are embedded\nvia a projection layer ğ‘ƒ as follows:\nğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘¡\nğ‘– = {ğ‘ƒ(ğ‘œğ‘¡\nğ‘–,1),ğ‘ƒ(ğ‘œğ‘¡\nğ‘–,2),Â·Â·Â· ,ğ‘ƒ(ğ‘œğ‘¡\nğ‘–,ğ‘šğ‘– )}. (3)\nIn Eq. (3), ğ‘– âˆˆ{1,Â·Â·Â· ,ğ‘›}is the index of the agent; ğ‘— âˆˆ{1,Â·Â·Â· ,ğ‘šğ‘–}\nis the index of nodes; ğ‘šdenotes the number of nodes in the zone.\nIn the vanilla transformer[25], Vaswani adds \"positional encod-\nings\" to the input embeddings to inject position information of the\ntokens in a sequence. However, a distribution network has a radial\nFigure 2: The architecture of the policy network based on\ntransformer. The transformer encoder and embedding ag-\ngregation module are used to obtain the embedding of raw\nobservation. Then, the embedding is mapped to action via\nthe GRU head. Additionally, the auxiliary-task head pre-\ndicts the voltage out of control ratio (VR) of the raw observa-\ntion, which stabilizes the training process via extra auxiliary\nloss. Details can be found in Section 4.1.\ntopology instead of sequential structure. So instead of positional\nencodings, we inject position information via an adjacency matrix\nDğ‘– used in the attention mechanism that will be elaborated in the\nnext section.\n4.1.2 Transformer Encoder and Embedding Aggregation Module.\nNext, The transformer encoder and embedding aggregation module\nare designed to extract more robust representation from the ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘¡\nğ‘–\nabove as shown in Figure 2.\nThe vanilla self-attention mechanism proposed in [25] is com-\nputed as follows:\nAttention (Q,K,V)= softmax\n \nQKğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\n!\nV. (4)\nEq. (4) can be extended to self-attention with mask mechanism:\nMaskAttention(Q,K,V,mask)= softmax\n \nQKğ‘‡\nâˆšï¸\nğ‘‘ğ‘˜\nÂ·mask\n!\nV. (5)\nThree matrices K,Q,V represent a set of keys, queries and values\nrespectively; ğ‘‘ğ‘˜ is a scaling factor equal to dimension of queries\nand keys; mask is a binary matrix with the same shape as QKğ‘‡ and\nğ‘šğ‘ğ‘ ğ‘˜ğ‘¥,ğ‘¦ âˆˆ{0,1}indicates whether perform an attention operation\nbetween position ğ‘¥ and position ğ‘¦.\nOur transformer encoder utilizes the self-attention with mask\nmechanism to establish the correlation between nodes in the zone.\nWe formulate our transformer encoder as follows:\nğ¸0 = ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘¡\nğ‘–, (6)\nğ‘„(ğ‘™),ğ¾(ğ‘™),ğ‘‰ (ğ‘™)= ğ‘Š(ğ‘™)\nğ‘„ ğ¸(ğ‘™âˆ’1),ğ‘Š (ğ‘™)\nğ¾ ğ¸(ğ‘™âˆ’1),ğ‘Š (ğ‘™)\nğ‘‰ ğ¸(ğ‘™âˆ’1), (7)\nÂ¯ğ‘Œ(ğ‘™)= MaskAttention\n\u0010\nğ‘„(ğ‘™),ğ¾(ğ‘™),ğ‘‰ (ğ‘™),ğ·ğ‘–\n\u0011\n, (8)\nğ¸(ğ‘™)= LayerNorm\n\u0010\nğ¸(ğ‘™âˆ’1)+Linear\n\u0010\nÂ¯ğ‘Œ(ğ‘™)\n\u0011\u0011\n, (9)\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nğ‘‚\n1\nğœ‡\n1\nğ‘‚\n2\nğœ‡\n2\nğ‘‚\nğ‘\nğœ‡\nğ‘\nâ€¦\nTransformer\nLinear\nğ‘„ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™\nÃ—ğ¿\nğ’†ğŸ ğ’†ğŸ ğ’†ğŸ‘ ğ’†ğŸ’ ğ’†ğ‘µâ€¦\nFigure 3: The architecture of the global critic network. Our\nframework replace the widely used MLP-based critic net-\nwork with the transformer-based critic network.\nwhere ğ‘Š(ğ‘™)\nğ‘„ ,ğ‘Š (ğ‘™)\nğ¾ ,ğ‘Š (ğ‘™)\nğ‘‰ represent the learnable parameters to com-\npute Q,K,V and ğ‘™ âˆˆ{1,2,Â·Â·Â· ,ğ¿}is the index of transformer layers.\nThe adjacency matrix ğ·ğ‘– indicates how nodes are connected. For\nexample, if node ğ‘¥ and node ğ‘¦are adjacent, ğ·ğ‘–ğ‘¥ğ‘¦ equals 1; if they\nare not, ğ·ğ‘–ğ‘¥ğ‘¦ equals 0.\nThen, the embedding aggregation module aggregates ğ¸(ğ‘™)into\nglobal information Â¯ğ¸ğ‘– from the view of agentğ‘–. In practice, we select\nan additional transformer layer without mask as our embedding\naggregation module:\nğ‘„ğ¸ğ´,ğ¾ğ¸ğ´,ğ‘‰ğ¸ğ´ = ğ‘Šğ¸ğ´\nğ‘„ ğ¸(ğ‘™),ğ‘Šğ¸ğ´\nğ¾ ğ¸(ğ‘™),ğ‘Šğ¸ğ´\nğ‘‰ ğ¸(ğ‘™), (10)\nÂ¯ğ‘Œğ¸ğ´ = Attention\n\u0010\nğ‘„ğ¸ğ´,ğ¾ğ¸ğ´,ğ‘‰ğ¸ğ´\n\u0011\n, (11)\nğ¸ğ¸ğ´ = LayerNorm\n\u0010\nğ¸(ğ‘™)+Linear\n\u0010\nÂ¯ğ‘Œğ¸ğ´\n\u0011\u0011\n, (12)\nÂ¯ğ¸ğ‘– = SelectEmbedding\n\u0010\nğ¸ğ¸ğ´,ğ‘ğ‘–\n\u0011\n. (13)\nSelectEmbedding is an operation to select the embedding whose\nindex is ğ‘ğ‘– (The index of node installed with PV ğ‘–). Intuitively, Â¯ğ¸ğ‘–\ndenotes the representation extracted from the agent ğ‘–perspective\nin a higher semantic space.\n4.1.3 GRU head and Auxiliary-task head. In the last part of the\npolicy network, a GRU[7] layer is applied to project the representa-\ntion Â¯ğ¸ğ‘– to the control action ğœ‡ğ‘–. â„ğ‘¡ in Figure 2 denotes the temporal\nhidden state at the time step ğ‘¡.\nMeanwhile, we introduce the auxiliary-task head to predict the\nvoltage out of control ratio (VR) in Oğ‘–. VR indicates the ratio of\nvoltage outside the safety range (i.e. 0.95-1.05 p.u.) in a zone, which\nis a critical metric that corresponds to the optimization objectives\nfor active voltage control task. Thus, the auxiliary-task head helps\nthe upstream transformer encoder module implicitly recover this\nessential information from raw observation. Also, extra auxiliary\nloss helps stabilize the learning process, which is introduced in\nSection 4.3 in detail.\n4.2 Transformer-based Critic Network\nFor the active voltage control task, the correlations between agents\nare related to their position in the grid. In a radial distribution\nnetwork, the voltage of each node is influenced by all other nodes,\nbut the impact decreases as the distance increases. Therefore, agents\nneed to be aware of the relationship between nodes in the grid to\nmake cooperative control decisions. For example, if two agents are\ntopologically close to each other, they must consider more carefully\nwhen making decisions. Additionally, the voltage of most nodes can\nbe controlled within the safety range (i.e. from 0.95ğ‘.ğ‘¢. to 1.05ğ‘.ğ‘¢.),\nwhile only a few parts have the risk of voltage exceeding the safety\nvalue. Thus, agents may pay more attention to those zones in danger\nto control the voltage of all nodes in the distribution network.\nThe conventional centralized critic network constructed with\npure MLPs[26] suffers from credit assignment issues, especially in\na large-scale cooperative environment. The large number of agents\nand their complex relationships complicate policy learning[18]. To\nmake the learning process more robust, we model the correlation\nbetween agents via the self-attention mechanism and design a\ntransformer-based critic network. The architecture of critic network\nis shown in Figure 3.\nWe describe the calculation of the global Q-function parame-\nterized by the transformer-based critic network. First of all, let Oğ‘–\ndenotes the raw observation of agent ğ‘– and ğœ‡ğ‘– denotes the corre-\nsponding action it performed. Then, ğ‘ tuples (Oğ‘–,ğœ‡ğ‘–)are trans-\nformed into ğ‘ embeddings through several vanilla transformer\nlayers[25]:\n{ğ‘’1,Â·Â·Â· ,ğ‘’ğ‘}= Transformer ({(O1,ğœ‡1),Â·Â·Â· ,(Oğ‘,ğœ‡ğ‘)}). (14)\nNext, we project the embeddings to the output space of the central-\nized action-value function ğ‘„ğœ‹\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ through a linear function:\nğ‘„ğœ‹\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™(O1,ğœ‡1,O2,ğœ‡2,Â·Â·Â· ,Oğ‘,ğœ‡ğ‘)= Linear(ğ‘’1,ğ‘’2,Â·Â·Â· ,ğ‘’ğ‘).\n(15)\nIn addition, our proposed transformer-based critic network reduces\nthe number of parameters that benefits from compact semantic\nrepresentations calculated by transformer architecture.\n4.3 Auxiliary-task Training Process\nIn this section, we select MADDPG[ 19] as the base algorithm to\ndescribe the entire auxiliary-task training process of T-MAAC. Sup-\npose there are a total ofğ‘ agents with continuous policies ğğ‘–(Â·; ğœƒğ‘–)\nparameterized by ğœƒ = {ğœƒ1,Â·Â·Â· ,ğœƒğ‘}. Let s = (ğ‘œ1,Â·Â·Â· ,ğ‘œğ‘)denotes\nthe observations of each agents, then we formulate the gradient of\nthe expected return for agent ğ‘–, ğ½(ğğ‘–)= E[ğ‘…]as:\nâˆ‡ğœƒğ‘– ğ½\u0000ğğ‘–\n\u0001 =Es,ğ‘âˆ¼D\n\u0014\nâˆ‡ğœƒğ‘– ğğ‘–(ğ‘ğ‘– |ğ‘œğ‘–)âˆ‡ğ‘ğ‘– ğ‘„ğ\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™(s,ğ‘1,...,ğ‘ ğ‘)\n\f\f\fğ‘ğ‘–=ğğ‘– (ğ‘œğ‘–)\n\u0015\n. (16)\nHere, Dis the experience replay buffer recording transitions of all\nagents. And the centralized global action-value functions ğ‘„ğ\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™\nis updated as:\nğ‘¦ = ğ‘Ÿ +ğ›¾ğ‘„ğâ€²\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™\n\u0000sâ€²,ğ‘â€²\n1,...,ğ‘ â€²\nğ‘\n\u0001\f\f\fğ‘â€²\nğ‘— =ğâ€²\nğ‘— (ğ‘œğ‘— ), (17)\nL(ğœƒğ‘–)= Es,ğ‘,ğ‘Ÿ,sâ€²\n\u0014\u0010\nğ‘„ğ\nğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (s,ğ‘1,...,ğ‘ ğ‘)âˆ’ğ‘¦\n\u00112\u0015\n. (18)\nIn addition, we adopt an additional self-supervised loss to stabilize\nthe learning process. vğ‘–(Â·; ğœƒğ‘–)is the output of auxiliary-task head\nin the policy network, which predicts the voltage out of control\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\nratio (VR) in the zone. And itâ€™s ground truth ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘– is calculated\nfrom the raw observation ğ‘œğ‘–. The auxiliary loss is as follows:\nğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘– =\nğ‘šğ‘–âˆ‘ï¸\nğ‘—=1\n[I(ğ‘‰ğ‘œğ‘™ğ‘¡ğ‘ğ‘”ğ‘’ ğ‘— < 0.95)+I(ğ‘‰ğ‘œğ‘™ğ‘¡ğ‘ğ‘”ğ‘’ ğ‘— > 1.05)], (19)\nLğ‘ğ‘¢ğ‘¥ (ğœƒğ‘–)= Es,ğ‘,ğ‘Ÿ,sâ€²\n\u0002\n(vğ‘– (ğ‘œğ‘– ; ğœƒğ‘–)âˆ’ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘–)2\u0003\n, (20)\nwhere I is the indicator function and ğ‘šğ‘– is the number of nodes in\nthe ğ‘œğ‘–. In order to improve the sample efficiency and scalability in\nMARL algorithms, the parameters of policy networks are shared\namong agents in T-MAAC. Details of transformer-based MADDPG\n(T-MADDPG) are given in Algorithm 1.\nAlgorithm 1: Transformer-based MADDPG (T-MADDPG)\n1 for episode â†1 to ğ‘€ do\n2 Initialize a random process Nfor action exploration and\nReplay Buffer D;\n3 for t â†1 to max-episode-length do\n4 for each agent ğ‘–, select a action ğ‘ğ‘– = ğœ‡ğœƒğ‘– (ğ‘œğ‘–)+Nğ‘–;\n5 Execute actions ğ‘= (ğ‘1,Â·Â·Â· ,ğ‘ğ‘)from state ğ‘ ;\n6 Get reward ğ‘Ÿ by going to new state ğ‘ â€²;\n7 Store (ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘  â€²)in replay buffer Dand ğ‘  â†ğ‘ â€²;\n8 for agent ğ‘– â†1 to N do\n9 Sample a random mini-batch of Ssamples\n(ğ‘ ğ‘—,ğ‘ğ‘—,ğ‘Ÿğ‘—,ğ‘ â€²ğ‘—)from D;\n10 Update critic by minimizing the loss in Eq. (18) ;\n11 Update actor by minimizing the auxiliary loss in\nEq. (20) ;\n12 Update actor by gradient ascent in Eq. (16) ;\n13 end\n14 Update target network parameters for each agent ğ‘–:\nğœƒâ€²\nğ‘– â†ğœğœƒğ‘– +(1 âˆ’ğœ)ğœƒâ€²\nğ‘–;\n15 end\n16 end\n5 EXPERIMENTS\nIn this section, we conduct a series of experiments based on the\nMAPDN environment[26] to evaluate the performance of T-MAAC.\nWe first introduce the experiment setups and implementation de-\ntails of the algorithms. Then, we compare the evaluation results of\nour algorithm with the baseline methods and the ablated variants.\nFurthermore, we show a case study that visualizes the attention\nweights in the self-attention mechanism to analyze which nodes\nin the distribution network agents should focus on (see Appendix\nB.1).\n5.1 Experiment Setups\nThe MAPDN[26] is an environment of distributed/decentralized ac-\ntive voltage control on power distribution networks, which supports\nnumerical studies for the 33-bus, 141-bus, and 322-bus network.\nWe conduct experiments in the 141-bus network scenario with 22\nagents and the 322-bus network scenario with 38 agents.\nIn order to evaluate different algorithms fairly, we randomly\nselect some test scenarios from different seasons to construct a\ntest dataset and a validation dataset(details in Appendix A.1). Each\nexperiment is run with 5 random seeds and the test results during\ntraining are given by the median and the 25%-75% quartile shading.\nAnd each experiment is evaluated in the validation dataset every\n20 episodes during training. After the training phase, we evaluate\nthe learned strategy on the whole test dataset.\nFollowing the proposal of [26], we conduct main experiments\nwith three different voltage barrier functions (see Appendix A.3) on\nthe 141-bus and the 322-bus networks. In experiments, we use two\nmetrics to evaluate the performance of algorithms.Controllable rate\n(CR): It calculates the ratio of all busesâ€™ voltage being under control\nwithin safety range. Q loss (QL) : It calculates the mean reactive\npower generations by agents per time step, which is the same as\nğ‘™ğ‘(Â·)defined in Eq.(1). QL is an alternative metric to power loss\nbecause the power loss of the whole grid is hard to obtain in the\nactual distribution network. CR is the most critical metric for the\nactive voltage control task, and a low CR indicates that the entire\ngrid is currently perilous.\n5.2 Baseline Methods and Implementation\nDetails\nAccording to [ 26], MADDPG[19] and MATD3[ 1] achieve excel-\nlent performance in the MAPDN environment compared to other\nstate-of-the-art MARL algorithms. Thus, we separately couple our\nproposed T-MAAC with MADDPG and MATD3 to evaluate the\nperformance of our framework:\nâ€¢MADDPG and MATD3 . In our experiments, the MLP-based\npolicy network in MADDPG and MATD3 consists of one hidden\nlayer and a GRU layer. And the MLP-based critic network is\nconstructed with one hidden layer.\nâ€¢T-MADDPG and T-MATD3 . Compared to the baseline algo-\nrithms above, T-MADDPG and T-MATD3 replace the MLP-based\nnetwork architecture with the transformer-based network ar-\nchitecture proposed in Section 4.1 and 4.2. Moreover, additional\nauxiliary loss is introduced during training (see Algorithm 1). In\nthe policy network, the transformer encoder is composed of a\nstack of 2 transformer layers, and the embedding aggregation\nmodule is an extra transformer layer. The architecture of GRU\nhead remains the same as the baseline algorithms above.\nFollowing [13], all algorithms are trained with the normalized re-\nward and the action bound enforcement trick. We perform gradient\nclipping with L1 norm and the clip bound is set to 1. Moreover, the\nparameters in policy networks are shared among agents, and the\nagent ID is concatenated with observation to distinguish different\nagents. The hyper-parameters of algorithms are shown in Table 2\n(see Appendix A.4).\n5.3 Result\nThe median CR and QL of algorithms during training for MADDPG,\nMATD3, T-MADDPG and T-MATD3 are shown in Figure 4. As\nthe figure shows, our proposed T-MAAC framework consistently\nimproves the performance of baseline methods under three types of\nrewards and two different scale grid scenarios. Owing to the learned\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\n(a) CR-L1-141\n (b) CR-L2-141\n (c) CR-BL-141\n (d) CR-L1-322\n (e) CR-L2-322\n (f) CR-BL-322\n(g) QL-L1-141\n (h) QL-L2-141\n (i) QL-BL-141\n (j) QL-L1-322\n (k) QL-L2-322\n (l) QL-BL-322\nFigure 4: Median CR and QL of algorithms with different voltage barrier functions. \"T-\" indicates the combination of T-MAAC\nand the baseline algorithm. The sub-caption indicates metric-Barrier-scenario and BL is the abbreviation of bowl.\nTable 1: The mean test results in the test dataset. CR denotes the control rate; QL denotes the Q loss; PL denotes the power\nloss.\nMethod Spring Summer Fall Winter\nCR (%) QL ( MW\nMVAR) PL (MW) CR (%) QL ( MW\nMVAR) PL (MW) CR (%) QL ( MW\nMVAR) PL (MW) CR (%) QL ( MW\nMVAR) PL (MW)\n322-MADDPG 79.2 0.031 0.036 75.0 0.031 0.040 92.5 0.031 0.029 97.8 0.031 0.027\n322-T-MADDPG 88.6 0.027 0.037 86.3 0.027 0.044 96.7 0.025 0.026 98.0 0.024 0.021\n322-MATD3 59.4 0.033 0.037 54.1 0.033 0.040 77.9 0.034 0.033 89.4 0.035 0.033\n322-T-MATD3 90.6 0.028 0.039 89.5 0.029 0.046 97.9 0.028 0.028 99.3 0.027 0.024\n141-MADDPG 75.8 1.88 0.78 72.9 1.85 0.91 75.4 1.95 0.53 71.1 1.99 0.45\n141-T-MADDPG 97.8 0.77 0.93 97.8 0.78 1.10 100 0.79 0.61 100 0.85 0.50\n141-MATD3 78.7 1.83 0.93 73.4 1.77 1.08 90.1 1.95 0.67 89.9 2.03 0.59\n141-T-MATD3 97.4 0.77 0.89 97.4 0.79 1.06 99.9 0.79 0.57 100 0.85 0.47\nbetter representations relevant to the active voltage control task, T-\nMADDPG and T-MATD3 improve the controllable rate(CR) in the\ngrid while reducing the reactive power generation (QL). T-MAAC\nsignificantly performs well on the 141-bus network, verifying the\nimportance of better representations. As for the 322-bus network,\na large-scale scenario with 38 agents, T-MADDG increases the\nCR while maintaining the low QL as same as MADDPG, and T-\nMATD3 achieves a higher CR with a lower QL. By comparing\nthe performance with three different reward functions, T-MAAC\nalso stabilizes the training process and alleviates the phenomenon\nmentioned in [26] that algorithms is sensitive to reward functions.\nThe better representations improve sample efficiency and resolve\nthe issue that different reward functions lead agents to different\nlocal optimal policies.\nWe also evaluate all algorithms trained by L2-shape voltage\nbarrier function in the test dataset. We show the performances in\nTable 1 (including actual power loss PL in the whole distribution\nnetwork). The metricCR shows that it is more challenging to control\nthe voltage in spring and summer than in fall and winter due to\nexcessive active power injection produced by PVs in the former. Our\nproposed T-MAAC overcomes such difficulties and achieves better\nperformances in all scenarios, especially in spring and summer.\nMeanwhile, penalized byQ loss, T-MAAC learns a strategy with less\nreactive power generation than baseline methods. It is worth noting\nthat QL is a proxy for power loss during training, thus, the learned\nstrategy with lessQL may still lead to morePL. Such a matter can be\nalleviated by more related and easy-to-obtain surrogate metric[5].\n5.4 Ablation Study\nIn this section, we conduct a series of experiments to examine fur-\nther which particular components of T-MAAC are essential for the\nperformance. We fix the baseline algorithm to be MADDPG trained\nby L2-shape voltage barrier function. The performances of the vari-\nants of T-MADDPG are evaluated in the following experimental\nsettings.\n5.4.1 Auxiliary Task. We add an additional auxiliary task during\ntraining to stabilize the training process (see Section 4.3). The fol-\nlowing ablated variants are designed to verify the effectiveness of\nthe auxiliary task:\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\n(a) CR-L2-141\n (b) CR-L2-322\n(c) QL-L2-141\n (d) QL-L2-322\nFigure 5: Performance comparison with or w/o auxiliary\ntask. The sub-caption indicates metric-Barrier-scenario.\nâ€¢T-MADDPG w/o aux task : We remove the auxiliary task and\noptimize model by Eq. (16) and Eq. (18).\nâ€¢MADDPG with aux task : We also add the auxiliary task to the\nMLP-based MADDPG to figure out whether the auxiliary task\ncan improve performance with different network architectures.\nThe result is shown in Figure 5. The performance of T-MADDPG is\nbetter than T-MADDPG without auxiliary task during almost the\nentire training process especially on the 322-bus network that is a\nmore challenging large-scale scenario. T-MADDPG without auxil-\niary task performs little differently from MADDPG, which indicates\ntraining with auxiliary task is essential for our transformer-based\nnetwork architectures.\nIn addition, training with auxiliary task also improves the perfor-\nmance of MADDPG on the 141-bus network, which means that the\nauxiliary loss during training helps convergence to a better policy.\nHowever, training with auxiliary task doesnâ€™t work on the 322-bus\nnetwork. This may be due to the fact that the simple policy/critic\nnetwork constructed with MLPs is no longer able to capture the\nnature in the grid, especially in the large-scale grid.\n5.4.2 Ablation Study on the Transformer-based Policy Network.In\nT-MAAC, we design a novel policy network based on transformer\nto achieve better performance on active voltage control task (see\nSection 4.1). We introduce two ablated variants to examine modules\nin the transformer-based policy network as follows:\nâ€¢T-MADDPG w/o EA : We remove the embedding aggregation\nmodule, and the final representation of Ois obtained by average\noutputs of the transformer encoder.\nâ€¢T-MADDPG w/o topology : We remove the adjacency matrix\nand the mask in the self-attention mechanism. Attention opera-\ntions in Eq. (4) are implemented between all nodes regardless of\nwhether they are connected in the zone or not.\nThe embedding aggregation module integrate node-based in-\nformation to global information from the point of the agent, and\nthe result in Figure 6 shows that it further improves performance\non both 141-bus network and 322-bus network. Further more, by\n(a) CR-L2-141\n (b) CR-L2-322\n(c) QL-L2-141\n (d) QL-L2-322\nFigure 6: Performance comparison on variants of our\npolicy network. The sub-caption indicates metric-Barrier-\nscenario.\n(a) CR-L2-141\n (b) CR-L2-322\n(c) QL-L2-141\n (d) QL-L2-322\nFigure 7: Performance comparison on variants of our\ncritic network. The sub-caption indicates metric-Barrier-\nscenario.\ncomparing the performance of T-MADDPG w/o EA and MADDPG\nin the 322-bus network, it can be seen that vanilla transformer\narchitecture without EA canâ€™t handle various observation space.\nThis result verifies the opinions discussed in Section 4.1 that aggre-\ngating information from the perspective of decision maker allows\ntransformer encoder to obtain better representations suitable for\nthis task.\nIf we donâ€™t inject the position information into the transformer,\nall nodes are treated equally in the early stages of training. Thus,\ninspired by [22, 29], we select the adjacency matrix as the mask\nin the self-attention mechanism to assist agents in capturing the\nnature of the grid. As shown in Figure 6, the utilization of topology\nimproves sample efficiency in the baseline algorithm.\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\n5.4.3 Transformer-based Critic. In this section, we conduct abla-\ntion experiments to figure out the effect of introducing transformers\ninto the global critic network.\nâ€¢T-MADDPG w/o T-critic : We replace the transformer-based\ncritic network with the widely used MLP-based critic network\nas same as MADDPG.\nWe show the result in Figure 7. Compared to T-MADDPG w/o\ntransformer-based critic, T-MADDPG achieves better performance\non both scenarios. Moreover, the transformer-based critic also sta-\nbilizes the training process in 141-bus network as shown in Figure\n7(b).\n6 CONCLUSIONS\nIn this paper, we propose T-MAAC, a transformer-based multi-agent\nactor-critic framework, for voltage stabilization in power distribu-\ntion networks. Our framework consists of a policy network and a\nglobal critic network. The policy network based on transformer cap-\ntures the characteristics of grid, obtaining better representations for\npower network task. In the global critic network, we introduce the\nself-attention mechanism to model the correlation between agents\nand achieving better performance. Additionally, we adopt the aux-\niliary task, predicting the voltage out of control ratio in a zone\nfor active voltage control task, to stabilize the training process and\nimprove the embedding learning. We conduct extensive evaluations\nas well as ablation studies in the real-world scale grid scenarios\nprovided by MAPDN. The experimental results demonstrate that\nT-MAAC significantly improves the performance of existing MARL\nalgorithms for voltage stabilization.\nACKNOWLEDGMENTS\nThis work was supported in part by the National Natural Science\nFoundation of China under Contract 61836011 and in part by the\nYouth Innovation Promotion Association CAS under Grant 2018497.\nIt was also supported by the GPU cluster built by MCC Lab of\nInformation Science and Technology Institution, USTC.\nREFERENCES\n[1] Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. 2019.\nReducing overestimation bias in multi-agent domains using double centralized\ncritics. arXiv preprint arXiv:1910.01465 (2019).\n[2] Mesut E Baran and Felix F Wu. 1989. Network reconfiguration in distribution\nsystems for loss reduction and load balancing. IEEE Power Engineering Review 9,\n4 (1989), 101â€“102.\n[3] Di Cao, Weihao Hu, Junbo Zhao, Qi Huang, Zhe Chen, and Frede Blaabjerg.\n2020. A multi-agent deep reinforcement learning based voltage regulation using\ncoordinated PV inverters. IEEE Transactions on Power Systems 35, 5 (2020), 4120â€“\n4123.\n[4] Di Cao, Junbo Zhao, Weihao Hu, Fei Ding, Qi Huang, and Zhe Chen. 2020.\nDistributed voltage regulation of active distribution system based on enhanced\nmulti-agent deep reinforcement learning. arXiv preprint arXiv:2006.00546 (2020).\n[5] Di Cao, Junbo Zhao, Weihao Hu, Fei Ding, Qi Huang, Zhe Chen, and Frede\nBlaabjerg. 2021. Data-driven multi-agent deep reinforcement learning for distri-\nbution system decentralized voltage control with high penetration of PVs. IEEE\nTransactions on Smart Grid 12, 5 (2021), 4137â€“4150.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\nKirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with trans-\nformers. In Proceedings of the European conference on computer vision . Springer,\n213â€“229.\n[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.\nEmpirical evaluation of gated recurrent neural networks on sequence modeling.\narXiv preprint arXiv:1412.3555 (2014).\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.arXiv\npreprint arXiv:1810.04805 (2018).\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at Scale. InInternational\nConference on Learning Representations .\n[10] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shi-\nmon Whiteson. 2018. Counterfactual multi-agent policy gradients. In Proceedings\nof the AAAI Conference on Artificial Intelligence , Vol. 32.\n[11] Lingwen Gan, Na Li, Ufuk Topcu, and Steven H Low. 2013. Optimal power flow\nin tree networks. In Proceedings of the IEEE Conference on Decision and Control .\nIEEE, 2313â€“2318.\n[12] Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. 2017. Cooperative\nmulti-agent control using deep reinforcement learning. In Proceedings of the\nInternational Conference on Autonomous Agents and Multiagent Systems . Springer,\n66â€“83.\n[13] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon\nHa, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. 2018.\nSoft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905\n(2018).\n[14] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. 2021. UPDeT: Univer-\nsal Multi-agent RL via Policy Decoupling with Transformers. In International\nConference on Learning Representations .\n[15] HM Khodr, FG Olsina, PM De Oliveira-De Jesus, and JM Yusta. 2008. Maximum\nsavings approach for location and sizing of capacitors in distribution systems.\nElectric Power Systems Research 78, 7 (2008), 1192â€“1203.\n[16] Haotian Liu and Wenchuan Wu. 2021. Online multi-agent reinforcement learning\nfor decentralized inverter-based volt-var control. IEEE Transactions on Smart\nGrid 12, 4 (2021), 2980â€“2990.\n[17] Kai Liu, Yuyang Zhao, Gang Wang, and Bei Peng. 2022. Self-attention-based\nmulti-agent continuous control method in cooperative environments.Information\nSciences 585 (2022), 454â€“470.\n[18] Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao.\n2020. Multi-agent game abstraction via graph attention neural network. In\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 7211â€“7218.\n[19] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch.\n2017. Multi-agent actor-critic for mixed cooperative-competitive environments.\narXiv preprint arXiv:1706.02275 (2017).\n[20] Steffen Meinecke, DÅ¾anan SarajliÄ‡, Simon Ruben Drauz, Annika Klettke, Lars-\nPeter Lauven, Christian Rehtanz, Albert Moser, and Martin Braun. 2020. Sim-\nbenchâ€”a benchmark dataset of electric power systems to compare innovative\nsolutions based on power flow analysis. Energies 13, 12 (2020), 3290.\n[21] Frans A Oliehoek and Christopher Amato. 2016. A concise introduction to decen-\ntralized POMDPs . Springer.\n[22] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Sid-\ndhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb\nNoury, et al. 2020. Stabilizing transformers for reinforcement learning. In Pro-\nceedings of the International Conference on Machine Learning . PMLR, 7487â€“7498.\n[23] Leon Thurner, Alexander Scheidler, Florian SchÃ¤fer, Jan-Hendrik Menke, Julian\nDollichon, Friederike Meier, Steffen Meinecke, and Martin Braun. 2018. pan-\ndapowerâ€”an open-source python tool for convenient modeling, analysis, and\noptimization of electric power systems. IEEE Transactions on Power Systems 33, 6\n(2018), 6510â€“6521.\n[24] Tijmen Tieleman, Geoffrey Hinton, et al . 2012. Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magnitude. COURSERA: Neural\nNetworks for Machine Learning 4, 2 (2012), 26â€“31.\n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the Advances in Neural Information Processing Systems .\n5998â€“6008.\n[26] Jianhong Wang, Wangkun Xu, Yunjie Gu, Wenbin Song, and Tim C Green. 2021.\nMulti-agent reinforcement learning for active voltage control on power distri-\nbution networks. Advances in Neural Information Processing Systems 34 (2021),\n3271â€“3284.\n[27] Shengyi Wang, Jiajun Duan, Di Shi, Chunlei Xu, Haifeng Li, Ruisheng Diao, and\nZhiwei Wang. 2020. A data-driven multi-agent autonomous voltage control\nframework using deep reinforcement learning. IEEE Transactions on Power\nSystems 35, 6 (2020), 4644â€“4654.\n[28] Xiaolong Wei, Xianglin Huang, LiFang Yang, Gang Cao, Zhulin Tao, Bing Wang,\nand Jing An. 2021. Hierarchical RNNs-Based transformers MADDPG for mixed\ncooperative-competitive environments. Journal of Intelligent & Fuzzy Systems\nPreprint (2021), 1â€“12.\n[29] Deunsol Yoon, Sunghoon Hong, Byung-Jun Lee, and Kee-Eung Kim. 2020. Win-\nning the L2RPN challenge: Power grid management via semi-markov afterstate\nactor-critic. In International Conference on Learning Representations .\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\n[30] Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu.\n2021. The surprising effectiveness of mappo in cooperative, multi-agent games.\narXiv preprint arXiv:2103.01955 (2021).\nStabilizing Voltage in Power Distribution Networks via Multi-Agent Reinforcement Learning with Transformer KDD â€™22, August 14â€“18, 2022, Washington, DC, USA.\nA EXPERIMENTAL SETTINGS\nA.1 MAPDN Environment and Datasets\nIn MAPDN, the 33-bus and 141-bus networks are modified from\nIEEE 33-bus[2] and IEEE 141-bus[15], respectively, while the 322-\nbus network is constructed by topology from SimBench[20]. The\ndata supporting simulation (i.e., load profile, load and PV data,\nactive and reactive power consumption) are collected from the real\nworld and then interpolated with the 3-min resolution consistent\nwith the gridâ€™s real-time control period. To guarantee the safety of\ndistribution network, the environment manually sets the range of\nactions with [âˆ’0.6,0.6]for the 141-bus case and [âˆ’0.8,0.8]for the\n322-bus case suggested by MAPDN[26]. As for the reward function,\nğ›¼ in Eq.(1) is set to 0.1 in the 322-bus case and 0.01 in the 141-bus\ncase to tune the trade-off between voltage control performance and\nreactive power generation loss.\nIt is worth noting that the difficulty of voltage control problem\nvaries during different months of a year. For example, during the\nmidday summer, excessive active power from intense sunlight is\ninjected into the grid, creating a more significant challenge for the\nvoltage control task than in winter. Thus, a series of fixed scenarios\nmust be chosen to evaluate algorithms fairly. We randomly select\n10 episodes per month, a total of 120 episodes, which constitute the\ntest dataset. Each episode lasts for 480 time steps (i.e., a day). Then,\nwe split the test dataset into four parts by month: Spring (Mar., Apr.,\nMay.), Summer (Jun., Jul., Aug.), Fall (Sept., Oct., Nov.), Winter (Dec.,\nJan., Feb). A validation dataset is obtained by randomly selecting\n10 episodes from the test dataset. During the training phase, we\nrandomly sample the initial state for an episode and each episode\nlasts for 240 time steps (i.e., a half day). Each experiment is run with\n5 random seeds and the test results during training are given by the\nmedian and the 25%-75% quartile shading as same as MAPDN[26].\nMoreover, each experiment is evaluated in the validation dataset\nevery 20 episodes during training. After the training phase, we\nevaluate the learned strategy on the whole test dataset.\nA.2 Network Topology\n(a) 141-bus network\n (b) 322-bus network\nFigure 8: The topologies of power networks. Figures are vi-\nsualized by the PandaPower toolkit[23].\nThe network topologies of the 141-bus network and the 322-bus\nnetwork are shown in Figure 8. In the 141-bus network, there are\n84 loads connected to some specific nodes, and 22 PVs (agents) are\ninstalled in some specific nodes. As for the 322-bus network, 337\nloads and 38 PVs are connected to some specific nodes.\nA.3 Reward Functions\nFigure 9: Three different voltage barrier functions proposed\nby [26].\nIn this work, reward functions are configured in accordance with\nthe guidelines in MAPDN environment [26]. We also conduct main\nexperiments with different voltage barrier functions using the same\nsettings as [26]: L1-shape, L2-shape and Bowl-shape as shown in\nFigure 9. Although the action range has been limited, there is still\na possibility that the whole power system crashes due to incorrect\ncontrol actions. To address this issue, if the power system crash,\nthe system would backtrack to the last state and terminate the\nsimulation with a reward of -200 regard as extra penalty.\nA.4 Hyper-parameters of Algorithms.\nThe hyper-parameters in our algorithms are shown in Table 2.\nTable 2: Hyper-parameters in experiments.\nName Value\nCommon:\noptimizer RMSProp[24]\npolicy learning rate 10âˆ’4\nvalue learning rate 10âˆ’4\npolicy update epochs 1\nvalue update epochs 10\ntarget update learning rate 0.1\ndiscount factor 0.99\nreplay buffer size 5000\nbatch size 32\nMADDPG and MATD3:\nMLP hidden dimension (policy) [64]\nGRU hidden dimension (policy) 64\nMLP hidden dimension (critic) [64]\nT-MADDPG and T-MATD3:\nauxiliary task learning rate 10âˆ’5\nauxiliary task update epochs 10\ntransformer hidden dimension (policy) 64\ntransformer layers (policy) 3\ntransformer hidden dimension (critic) 64\ntransformer layers (critic) 3\nnumber of multi-attention heads 4\nKDD â€™22, August 14â€“18, 2022, Washington, DC, USA. Wang and Feng, et al.\nB EXTRA EXPERIMENTAL RESULTS\nB.1 Case Study\n(a) A zone in the 322-bus network. The sun emoji represents\nthe location where a PV is installed. And the PV installed in\nnode 15 is selected as a special agent to visualize attention\nweights.\n(b) Voltage\n (c) Attention Weight\nFigure 10: A case study on the 322-bus network. We visualize\nthe attention weights of the special agent in (a). The voltages\nand attention weights of each nodes are shown in (b) and (c),\nrespectively.\nAs shown in Figure 10, we visualize the attention weights in the\nembedding aggregation module to figure out which nodes the agent\nfocuses on. We illustrate a zone in the 322-bus network in Figure\n10(a). Five PVs are installed at different locations in the zone. We\nselect the PV installed in node 15 as the special agent, and record\nattention weights of the final self-attention layer in the embedding\naggregation module. As shown in Figure 10(b), the voltage of all\nnodes is within safety range (0.95ğ‘.ğ‘¢. - 1.05ğ‘.ğ‘¢.). However, due to\nthe radial topology of the distribution network, nodes at the end of\nthe grid face a greater risk of voltage deviations[11]. The trend of\nattention weights in Figure 10(c) shows that the agent pays more\nattention to the nodes with high voltage based on the topology\nof this zone. It is also worthwhile to note that the agent is most\nconcerned with the nodes installed with PVs, which demonstrates\nthat the agent becomes aware of the locations of other PVs in the\nzone. This phenomenon verifies the opinion discussed in Section 4\nthat the T-MAAC captures the nature of the gird and extracts better\nrepresentations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7367686033248901
    },
    {
      "name": "Transformer",
      "score": 0.7114173769950867
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5933352708816528
    },
    {
      "name": "Grid",
      "score": 0.5086487531661987
    },
    {
      "name": "AC power",
      "score": 0.4857497811317444
    },
    {
      "name": "Voltage",
      "score": 0.48138144612312317
    },
    {
      "name": "Renewable energy",
      "score": 0.4468275308609009
    },
    {
      "name": "Electric power system",
      "score": 0.42676854133605957
    },
    {
      "name": "Voltage regulation",
      "score": 0.41712215542793274
    },
    {
      "name": "Control engineering",
      "score": 0.3681279420852661
    },
    {
      "name": "Artificial intelligence",
      "score": 0.27638715505599976
    },
    {
      "name": "Power (physics)",
      "score": 0.21524488925933838
    },
    {
      "name": "Electrical engineering",
      "score": 0.20431911945343018
    },
    {
      "name": "Engineering",
      "score": 0.16063201427459717
    },
    {
      "name": "Mathematics",
      "score": 0.07199212908744812
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ],
  "cited_by": 12
}