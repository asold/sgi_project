{
  "title": "Enhancing gastroenterology with multimodal learning: the role of large language model chatbots in digestive endoscopy",
  "url": "https://openalex.org/W4410571502",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2155904656",
      "name": "Yuanyuan Qin",
      "affiliations": [
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2095859947",
      "name": "Jian-Ming Chang",
      "affiliations": [
        "Southeast University"
      ]
    },
    {
      "id": "https://openalex.org/A1964094874",
      "name": "Li Li",
      "affiliations": [
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2124718693",
      "name": "Wu Mianhua",
      "affiliations": [
        "Nanjing University of Chinese Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2155904656",
      "name": "Yuanyuan Qin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095859947",
      "name": "Jian-Ming Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964094874",
      "name": "Li Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124718693",
      "name": "Wu Mianhua",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6856606850",
    "https://openalex.org/W4402264291",
    "https://openalex.org/W4312639100",
    "https://openalex.org/W4401387482",
    "https://openalex.org/W4376226279",
    "https://openalex.org/W4390872007",
    "https://openalex.org/W4319336214",
    "https://openalex.org/W4386065546",
    "https://openalex.org/W4317780575",
    "https://openalex.org/W6853459301",
    "https://openalex.org/W4378212544",
    "https://openalex.org/W4315648806",
    "https://openalex.org/W4390781727",
    "https://openalex.org/W4387967986",
    "https://openalex.org/W4295938041",
    "https://openalex.org/W3168463823",
    "https://openalex.org/W4313639237",
    "https://openalex.org/W3176404283",
    "https://openalex.org/W4361855028",
    "https://openalex.org/W4225323780",
    "https://openalex.org/W4405737212",
    "https://openalex.org/W4405264647",
    "https://openalex.org/W4386076642",
    "https://openalex.org/W4214842915",
    "https://openalex.org/W4362579348",
    "https://openalex.org/W6810788666",
    "https://openalex.org/W4283722442",
    "https://openalex.org/W4404327546",
    "https://openalex.org/W3184326922",
    "https://openalex.org/W4205476610",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W4281386687",
    "https://openalex.org/W4404612934",
    "https://openalex.org/W4379984088",
    "https://openalex.org/W3137249732",
    "https://openalex.org/W3214622234",
    "https://openalex.org/W4391664360",
    "https://openalex.org/W4321634665",
    "https://openalex.org/W4392936957",
    "https://openalex.org/W6854165291",
    "https://openalex.org/W3104050923",
    "https://openalex.org/W4396878041",
    "https://openalex.org/W4388501300",
    "https://openalex.org/W3022894865",
    "https://openalex.org/W4403124194",
    "https://openalex.org/W4403841197",
    "https://openalex.org/W4312592767",
    "https://openalex.org/W3196798710",
    "https://openalex.org/W4312378223",
    "https://openalex.org/W4403649890",
    "https://openalex.org/W4386148428",
    "https://openalex.org/W4382202943",
    "https://openalex.org/W4388998990",
    "https://openalex.org/W4379473336",
    "https://openalex.org/W3140022118"
  ],
  "abstract": "Introduction Advancements in artificial intelligence (AI) and large language models (LLMs) have the potential to revolutionize digestive endoscopy by enhancing diagnostic accuracy, improving procedural efficiency, and supporting clinical decision-making. Traditional AI-assisted endoscopic systems often rely on single-modal image analysis, which lacks contextual understanding and adaptability to complex gastrointestinal (GI) conditions. Moreover, existing methods struggle with domain shifts, data heterogeneity, and interpretability, limiting their clinical applicability. Methods To address these challenges, we propose a multimodal learning framework that integrates LLM-powered chatbots with endoscopic imaging and patient-specific medical data. Our approach employs self-supervised learning to extract clinically relevant patterns from heterogeneous sources, enabling real-time guidance and AI-assisted report generation. We introduce a domain-adaptive learning strategy to enhance model generalization across diverse patient populations and imaging conditions. Results and discussion Experimental results on multiple GI datasets demonstrate that our method significantly improves lesion detection, reduces diagnostic variability, and enhances physician-AI collaboration. This study highlights the potential of multimodal LLM-based systems in advancing gastroenterology by providing interpretable, context-aware, and adaptable AI support in digestive endoscopy.",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/one.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nOPEN ACCESS\nEDITED BY\nRaﬀaele Pellegrino,\nUniversity of Campania Luigi Vanvitelli, Italy\nREVIEWED BY\nDianzhe Tian,\nPeking Union Medical College Hospital\n(CAMS), China\nJoowon Chung,\nEulji University School of Medicine,\nRepublic of Korea\n*CORRESPONDENCE\nMianhua Wu\n/one.tnum/one.tnum/zero.tnum/one.tnum/one.tnum/three.tnum@njucm.edu.cn\nRECEIVED /two.tnum/six.tnum February /two.tnum/zero.tnum/two.tnum/five.tnum\nACCEPTED /two.tnum/four.tnum April /two.tnum/zero.tnum/two.tnum/five.tnum\nPUBLISHED /two.tnum/one.tnum May /two.tnum/zero.tnum/two.tnum/five.tnum\nCITATION\nQin Y, Chang J, Li L and Wu M (/two.tnum/zero.tnum/two.tnum/five.tnum)\nEnhancing gastroenterology with multimodal\nlearning: the role of large language model\nchatbots in digestive endoscopy.\nFront. Med./one.tnum/two.tnum:/one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/five.tnum Qin, Chang, Li and Wu. This is an\nopen-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nEnhancing gastroenterology with\nmultimodal learning: the role of\nlarge language model chatbots in\ndigestive endoscopy\nYuanyuan Qin/one.tnum,/two.tnum, Jianming Chang /three.tnum, Li Li /one.tnum,/two.tnumand Mianhua Wu /one.tnum,/two.tnum*\n/one.tnumFirst Clinical Medical College, Nanjing University of Chinese Medicine, Nanjing, China, /two.tnumJiangsu\nCollaborative Innovation Center of Traditional Chinese Medicin e Prevention and Treatment of Tumor,\nNanjing University of Chinese Medicine, Nanjing, China, /three.tnumSchool of Computer Science and\nEngineering, Southeast University, Nanjing, China\nIntroduction: Advancements in artiﬁcial intelligence (AI) and large language\nmodels (LLMs) have the potential to revolutionize digestiv e endoscopy by\nenhancing diagnostic accuracy, improving procedural eﬃciency, and supporting\nclinical decision-making. Traditional AI-assisted endoscopi c systems often rely\non single-modal image analysis, which lacks contextual underst anding and\nadaptability to complex gastrointestinal (GI) conditions. Mo reover, existing\nmethods struggle with domain shifts, data heterogeneity, and interpretability,\nlimiting their clinical applicability.\nMethods: To address these challenges, we propose a multimodal learning\nframework that integrates LLM-powered chatbots with endoscopic imaging and\npatient-speciﬁc medical data. Our approach employs self-supe rvised learning\nto extract clinically relevant patterns from heterogeneous so urces, enabling\nreal-time guidance and AI-assisted report generation. We in troduce a domain-\nadaptive learning strategy to enhance model generalization acr oss diverse\npatient populations and imaging conditions.\nResults and discussion: Experimental results on multiple GI datasets\ndemonstrate that our method signiﬁcantly improves lesion de tection, reduces\ndiagnostic variability, and enhances physician-AI collaborati on. This study\nhighlights the potential of multimodal LLM-based systems i n advancing\ngastroenterology by providing interpretable, context-aware, and adaptable AI\nsupport in digestive endoscopy.\nKEYWORDS\nmultimodal learning, large language models, digestive endoscopy, AI-assisted diagnosis,\ndomain adaptation\n/one.tnum Introduction\nGastroenterology has witnessed signiﬁcant advancements with the integration of\nartiﬁcial intelligence (AI), particularly in digestive endoscopy, where precise diagnosis,\ndecision support, and workﬂow optimization are critical\n(1). Traditional endoscopic\nassessments rely heavily on expert interpretation, which can be time-consuming, subject\nto inter-operator variability, and prone to misdiagnosis. Multimodal learning, which\ncombines visual, textual, and real-time patient data, has emerged as a promising approach\nto enhance endoscopic decision-making\n(2). Large Language Model (LLM)-based chatbots\nare at the forefront of this transformation, providing real-time guidance, diﬀerential\ndiagnosis suggestions, and automated report generation by synthesizing multiple sources\nof information\n(3). Not only do these AI-driven tools reduce the cognitive load on\nphysicians, but they also enable standardization in endoscopic interpretations and\nFrontiers in Medicine /zero.tnum/one.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nimprove diagnostic accuracy. Integrating multimodal AI in\ngastroenterology allows for more eﬃcient data-driven decision-\nmaking by leveraging real-time endoscopic imagery, electronic\nhealth records, and clinical guidelines\n(4). Despite these beneﬁts,\ncurrent AI solutions still face challenges related to interpretability,\nreal-time responsiveness, and clinical integration. To address these\nlimitations, researchers have explored various approaches, evolving\nfrom traditional knowledge-based systems to data-driven machine\nlearning techniques and, more recently, deep learning and large\npre-trained models. This paper reviews the progression of these\ntechniques and discusses their impact on digestive endoscopy\n(5).\nEarly approaches to AI-assisted digestive endoscopy primarily\nrelied on symbolic reasoning and knowledge-based systems. These\nrule-based systems utilized predeﬁned expert knowledge and\nontologies to analyze endoscopic ﬁndings and recommend possible\ndiagnoses\n(6). For example, early expert systems integrated\nstructured image descriptors with endoscopic procedural\nguidelines to identify abnormalities such as ulcers, polyps, and\nmalignancies. Similarly, ontology-driven frameworks enabled\nAI tools to standardize reporting by mapping visual ﬁndings to\nstructured diagnostic terms\n(7). While these systems provided\ninterpretability and consistency, they suﬀered from limited\nadaptability to new endoscopic techniques and variations in\nimaging conditions. The reliance on handcrafted rules restricted\ntheir ability to generalize across diverse patient populations and\nevolving clinical knowledge\n(8). The static nature of these systems\nmade it challenging to incorporate continuous learning from\nnew data, limiting their eﬀectiveness in real-world endoscopic\npractice. To overcome these drawbacks, researchers shifted toward\ndata-driven machine learning approaches, which oﬀered improved\nﬂexibility and learning capabilities\n(9).\nThe advent of machine learning models revolutionized AI-\nassisted digestive endoscopy by facilitating automated pattern\nrecognition from extensive endoscopic datasets. Techniques\nsuch as support vector machines (SVMs), random forests, and\nconvolutional neural networks (CNNs) were employed to classify\nendoscopic images and detect lesions with greater accuracy\n(10).\nFor instance, machine learning-based image segmentation allowed\nautomated detection of polyps and early-stage cancers, reducing\nthe need for manual annotation. Probabilistic models improved\nendoscopic decision support by analyzing multimodal patient\ndata, including clinical history and histopathological reports\n(11). Despite their improved adaptability compared to rule-based\nsystems, traditional machine learning methods required extensive\nfeature engineering and manual tuning to optimize performance.\nMoreover, these models struggled with real-time inference in\nendoscopic procedures due to computational constraints\n(12).\nAnother limitation was the lack of contextual understanding, as\nmachine learning models primarily focused on single-modality\ndata, such as images or structured patient records, without\nintegrating textual and conversational aspects. The advent of deep\nlearning and large pre-trained language models provided a solution\nto these challenges\n(13).\nDeep learning and multimodal learning techniques have greatly\npropelled AI-driven innovations in digestive endoscopy, allowing\nfor automatic feature extraction and seamless real-time integration\nof multimodal data. Large-scale CNNs and transformer-based\narchitectures have demonstrated exceptional performance in\nanalyzing endoscopic videos, detecting abnormalities, and\nproviding diagnostic predictions with high accuracy\n(14). More\nrecently, Large Language Model (LLM)-driven chatbots have\nrevolutionized AI-assisted gastroenterology by facilitating real-\ntime interaction between physicians and AI systems. These chatbots\nintegrate multimodal learning by combining visual endoscopic\nﬁndings with clinical text-based insights, enhancing decision\nsupport\n(15). For example, an LLM-powered chatbot can analyze\nendoscopic images, retrieve relevant clinical literature, and suggest\ndiﬀerential diagnoses in real time, assisting gastroenterologists in\ncomplex cases. Transformer-based architectures enable dynamic\nadaptation to evolving medical knowledge by continuously\nlearning from new datasets\n(16). However, challenges remain in\nterms of interpretability, potential biases in training data, and\nreal-time deployment in high-stakes clinical settings. Addressing\nthese issues requires advancements in explainable AI and real-time\nprocessing frameworks\n(17).\nBuilding on these developments, we propose a novel\nframework that leverages multimodal learning and LLM-driven\nchatbots to enhance digestive endoscopy. Our approach integrates\ntransformer-based AI models with real-time endoscopic imaging\nand structured clinical knowledge to provide interactive decision\nsupport. Unlike traditional symbolic AI, our framework is not\nrestricted by static rules and can dynamically adapt to new\nendoscopic techniques and imaging modalities. It surpasses\nconventional machine learning methods by incorporating\nmultimodal fusion, enabling a more comprehensive understanding\nof patient conditions. To improve clinical trustworthiness, our\napproach incorporates explainable AI mechanisms, ensuring that\nendoscopic ﬁndings and chatbot-generated recommendations are\ntransparent and interpretable. By leveraging pre-trained language\nmodels and real-time data processing, our method enhances\ndiagnostic accuracy, procedural eﬃciency, and physician-AI\ninteraction in gastroenterology.\nThe proposed method has several key advantages:\n• Our framework introduces a transformer-based multimodal\nlearning approach that integrates endoscopic imaging, clinical\nreports, and LLM-driven chatbots to enhance diagnostic\naccuracy and procedural decision-making.\n• Unlike conventional machine learning models, our method\nprocesses multimodal data in real-time, providing interactive\ndecision support for gastroenterologists, improving workﬂow\neﬃciency in digestive endoscopy.\n• Experimental evaluations demonstrate that our approach\noutperforms existing AI-assisted endoscopy methods in\naccuracy, adaptability, and physician usability, ensuring\nseamless clinical integration and improved patient outcomes.\n/two.tnum Related work\n/two.tnum./one.tnum Multimodal learning for enhanced\ngastrointestinal diagnostics\nMultimodal learning has emerged as a transformative approach\nin gastroenterology, integrating various data sources such as\nendoscopic imaging, clinical records, and genetic information\nFrontiers in Medicine /zero.tnum/two.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nto improve diagnostic accuracy and patient management (18).\nBy leveraging multimodal data, AI-driven models can provide\ncomprehensive insights into gastrointestinal conditions, aiding\nin both early detection and treatment planning. A prominent\nexample is the application of multimodal AI in diagnosing\npancreatic lesions. A randomized crossover study demonstrated\nthat combining endoscopic ultrasound images with patient-\nspeciﬁc clinical data resulted in superior diagnostic performance\ncompared to conventional single-modal approaches\n(19). This\nhighlights the value of integrating multiple data types to enhance\nclinical decision-making. Multimodal machine learning models\nhave also shown promise in endoscopy by improving the\ndetection and characterization of gastrointestinal abnormalities\n(20). By synthesizing visual endoscopic data with patient\nhistory and histopathological reports, these models enable\nreal-time, highly accurate assessments, assisting endoscopists\nin making informed decisions during procedures\n(21). The\nintegration of advanced diagnostic tools, such as white-light\nendoscopy combined with confocal laser endomicroscopy, has\nfacilitated real-time in vivo histological assessment of tissues\n(22). This approach has signiﬁcantly improved the detection of\nconditions such as Barrett’s esophagus and other precancerous\nlesions, demonstrating the potential of multimodal AI in\ngastroenterology\n(23).\n/two.tnum./two.tnum The role of large language models in\ndigestive endoscopy\nLarge Language Models (LLMs) have introduced new\npossibilities in digestive endoscopy, particularly in areas such as\npatient education, clinical decision support, and AI-assisted\nreport generation. These AI-driven chatbots can process\nand generate human-like text, making them highly valuable\ntools in modern gastroenterological practice\n(24). One key\napplication of LLMs is personalized patient education. AI-driven\nchatbots can provide tailored information regarding upcoming\nendoscopic procedures, post-procedure care, and common\npatient concerns. This personalized approach not only enhances\npatient comprehension but also increases overall satisfaction with\nmedical procedures\n(25). In diagnostic applications, integrating\nLLMs with multimodal AI models has proven eﬀective in\nassessing complex gastrointestinal conditions. For instance,\ndeep-learning systems trained on combined white-light and\nweak-magnifying endoscopic images have demonstrated real-time\ndiagnostic capabilities\n(26), accurately identifying neoplastic\nlesions and aiding endoscopists during procedures. Beyond\ndiagnostics, LLM chatbots have shown potential in clinical\ndecision support by synthesizing multimodal data—such as\nendoscopic imaging, histopathological ﬁndings, and electronic\nhealth records—to provide tailored treatment recommendations\n(27). By analyzing a patient’s medical history and current\nsymptoms, these chatbots can suggest treatment options, dietary\nmodiﬁcations, and follow-up schedules, improving adherence to\nmedical advice and personalized patient care\n(28). LLMs facilitate\nadvanced training for healthcare professionals by simulating\ncomplex clinical scenarios that incorporate diverse data types.\nThese AI-driven simulations enhance diagnostic reasoning and\ndecision-making skills, making them valuable tools in medical\neducation\n(29).\nRecent clinical studies have begun to demonstrate the\nmeasurable beneﬁts of LLM-based assistance in real-world\nmedical workﬂows. For instance, Pellegrino et al.\n(21)\nconducted a concordance analysis in colonoscopy, showing\nthat ChatGPT-4-assisted scoring of bowel preparation quality\nachieved comparable results to expert gastroenterologists, while\nimproving documentation consistency and reducing inter-rater\nvariability\n(28). Similarly, Chai and Wang reported that LLM-\npowered clinical decision support systems, when integrated with\nEHR data, improved diagnostic agreement rates in complex\ngastrointestinal cases by over 12% compared to standard rule-\nbased systems\n(27). These ﬁndings support the claim that\nLLMs can eﬀectively augment physician decision-making and\ndocumentation processes, especially when applied in structured,\nsupervised clinical settings.\n/two.tnum./three.tnum Challenges and future directions in\nAI-driven gastroenterology\nDespite the signiﬁcant advancements in AI-driven\ngastroenterology, challenges remain in integrating multimodal\nlearning and LLMs into clinical practice. Ensuring interoperability\nbetween diverse data sources, maintaining patient privacy\n(30),\nand achieving high clinical accuracy are key obstacles that\nrequire further research and development. One of the primary\nchallenges is the variability in data sources and imaging techniques\nacross diﬀerent institutions. AI models must be trained to\nhandle domain shifts and variations in endoscopic imaging\nconditions to ensure reliable and consistent performance\n(31).\nDomain-adaptive learning strategies have been proposed to\nimprove generalization, but further validation is needed for\nwidespread clinical adoption. Another critical challenge is the\ninterpretability and transparency of AI-driven decision support\nsystems\n(32). While deep learning models oﬀer superior accuracy,\ntheir black-box nature poses diﬃculties in clinical acceptance.\nExplainable AI (XAI) techniques are crucial for increasing trust\namong clinicians by providing insights into how AI models\ngenerate their recommendations\n(33). Real-time deployment of\nAI models in high-stakes clinical settings presents computational\nchallenges. Multimodal learning frameworks require substantial\nprocessing power to analyze large-scale endoscopic video\ndata alongside patient-speciﬁc records\n(34). Advancements in\nmodel eﬃciency, including optimization techniques such as\nquantization and pruning, are necessary to facilitate seamless\nintegration into real-world healthcare workﬂows. Ethical\nconsiderations surrounding AI applications in gastroenterology\nmust be addressed\n(35). Ensuring unbiased training datasets,\npreserving patient conﬁdentiality, and adhering to regulatory\nframeworks are essential to the responsible deployment of AI\nin medical practice. Collaborative eﬀorts among clinicians, AI\nresearchers, and regulatory bodies are critical for overcoming\nthese challenges and fully realizing the potential of AI in digestive\nendoscopy\n(36).\nFrontiers in Medicine /zero.tnum/three.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\n/three.tnum Method\n/three.tnum./one.tnum Overview\nArtiﬁcial Intelligence (AI) has emerged as a transformative\ntechnology in gastroenterology, enhancing diagnostic accuracy,\noptimizing treatment strategies, and improving patient outcomes.\nWith the increasing complexity and volume of medical data, AI-\ndriven approaches oﬀer new possibilities for automating image\ninterpretation, predicting disease progression, and personalizing\npatient care. This section provides an overview of our proposed\nmethodology, which integrates AI models into gastroenterology\nworkﬂows, covering key components such as problem formulation,\nmodel development, and novel optimization strategies.\nIn Section 3.2, we present the preliminaries necessary to\nunderstand the application of AI in gastroenterology. This includes\ndeﬁning the imaging modalities commonly used in gastrointestinal\n(GI) diagnostics, such as endoscopy, radiology, and histopathology,\nand formulating the AI-driven decision-making process. Key\nmathematical representations of data acquisition, preprocessing,\nand feature extraction are introduced to establish a structured\nfoundation for AI integration. In Section 3.3, we introduce\nour novel AI-based model tailored for gastroenterology. Unlike\nconventional rule-based or handcrafted feature extraction methods,\nour approach employs deep learning architectures to automatically\nlearn discriminative features from GI images and clinical data.\nBy incorporating self-supervised learning and multi-modal data\nfusion, our model achieves robust performance across diverse\npatient populations and varying imaging conditions. In Section 3.4,\nwe propose a new strategy to optimize AI deployment in clinical\nsettings. This involves designing interpretable AI systems that\nprovide explainable decision support for gastroenterologists. We\nintroduce a domain-adaptive learning technique to enhance model\ngeneralization, mitigating biases associated with dataset variations.\nThe proposed strategy also includes an uncertainty quantiﬁcation\nmechanism to assist clinicians in assessing model conﬁdence\nand reliability. We systematically develop a comprehensive\nAI framework for gastroenterology, leveraging state-of-the-art\nmachine learning techniques to advance disease detection, risk\nassessment, and therapeutic planning.\nTo further contextualize the application of our proposed\nframework, we illustrate how DGDN can be deployed within real-\nworld clinical workﬂows. DGDN is designed to support both\nreal-time diagnostic assistance during endoscopic procedures and\nretrospective decision support for clinical reporting and triage.\nIn a real-time scenario, the DGDN model ingests endoscopic\nvideo frames on-the-ﬂy, applies the AGD module to highlight\ndiagnostically relevant regions, and generates live predictions\nwith uncertainty quantiﬁcation. This assists gastroenterologists\nin identifying suspicious lesions, guiding biopsy decisions, or\nconﬁrming visual impressions during procedures. Alternatively, in\na retrospective setting, the model processes archived endoscopic\nimages, structured clinical records, and transcribed doctor–\npatient dialogue from electronic health systems. By fusing these\nmultimodal inputs, DGDN can generate structured diagnostic\nsummaries, suggest follow-up actions, or prioritize cases based on\nrisk levels. This supports applications such as endoscopy reporting\nautomation, post-procedure quality assurance, and early-stage\nFIGURE /one.tnum\nIllustration of DGDN in clinical workﬂow. Left: real-time support\nduring live endoscopy. Right: retrospective analysis for diagnostic\nreporting and triage. The multimodal AI model integrates live ima ge\ninput, electronic health records, and clinical dialogue to provi de\ncontext-aware, interpretable decision support at multiple st ages of\npatient care.\ntriage. A schematic diagram of this workﬂow is shown in Figure 1,\nhighlighting the ﬂexibility of DGDN in adapting to various points\nof care in gastroenterology.\n/three.tnum./two.tnum Preliminaries\nThe application of Artiﬁcial Intelligence (AI) in\ngastroenterology primarily focuses on analyzing medical imaging\ndata, automating disease detection, and enhancing clinical\ndecision-making. To formalize this problem mathematically, we\ndeﬁne the structure of AI-assisted gastroenterological diagnostics\nthrough a rigorous formulation of the data, feature space, and\ninference mechanism.\nMedical imaging plays a central role in gastroenterology,\nencompassing modalities such as endoscopy, computed\ntomography (CT), magnetic resonance imaging (MRI), and\nhistopathological slides. Each imaging modality provides a\ndiﬀerent data structure, which we deﬁne as follows.\nGiven an imaging modality m, let Im represent the space of all\npossible images captured using this modality. An image sample is\nthen denoted as:\nX ∈Im, X ={xi,j,c |i ∈[1, H], j ∈[1, W], c ∈[1, C]}, (1)\nwhere H and W are the height and width of the image, and C is\nthe number of channels.\nEach image X is associated with a diagnostic label y ∈Y, where\nY represents the set of possible conditions. The goal of AI-based\nFrontiers in Medicine /zero.tnum/four.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\ndiagnosis is to learn a function f : Im →Y that accurately maps an\ninput image to its corresponding diagnosis.\nTo enable eﬀective AI modeling, we deﬁne a feature space\nF that captures relevant patterns in gastrointestinal imaging. A\nfeature vector z extracted from an image X is deﬁned as:\nz =φ(X;θ), (2)\nwhere φ(·;θ) is a feature extraction function parameterized by\nθ, typically learned using deep neural networks.\nIn the case of endoscopy images, features may include\ntextural patterns, lesion boundaries, and color variations, while\nin histopathological images, cellular morphology and tissue\norganization are key factors. The extracted features z ∈ Rd\nform a high-dimensional representation that serves as input to\nclassiﬁcation or segmentation models.\nGiven an image X, the AI model predicts the likelihood of\ndiﬀerent conditions by computing:\np(y|X) =g(z;θg ), (3)\nwhere g(·;θg ) is a classiﬁcation function, typically modeled as a\nneural network with softmax output:\np(yk|X) = exp(w⊤\nk z +bk)\n∑ |Y|\nj=1 exp(w⊤\nj z +bj)\n, (4)\nwhere wk and bk are the parameters corresponding to class k.\nThe predicted class ˆy is then given by:\nˆy =arg max\nyk∈Y\np(yk|X). (5)\nIn real-world gastroenterology applications, data often exhibits\nspatial and temporal dependencies. For example, an endoscopy\nvideo provides sequential frames {Xt}T\nt=1 capturing dynamic\nviews of the gastrointestinal tract. A temporal AI model can be\nformulated as:\nzt =φ(Xt;θ), (6)\nht =ψ(zt, ht−1;θψ), (7)\nwhere ψrepresents a recurrent function that accumulates past\ninformation through a hidden state ht.\nBeyond imaging, gastroenterology AI systems can leverage\nmulti-modal data, including patient history, laboratory test results,\nand genetic proﬁles. Given N data modalities {m1, m2, ..., mN }, each\nproviding a feature set z(m), a fused representation is obtained via:\nzfusion =/Omega1 (z(m1), z(m2), ..., z(mN )), (8)\nwhere /Omega1 (·) is a fusion function, which may include\nconcatenation, attention mechanisms, or graph-based integration.\nA major challenge in AI-based gastroenterology is ensuring\nrobustness across diverse imaging conditions and patient\npopulations. A domain adaptation approach can be formulated as:\nLadapt =EX∼Ds L(f (X), y) +λD(Fs, Ft), (9)\nwhere Ds and Dt are the source and target domain\ndistributions, and D(·, ·) measures the feature space discrepancy,\noften implemented using Maximum Mean Discrepancy (MMD) or\nadversarial alignment.\n/three.tnum./three.tnum Deep gastrointestinal diagnosis\nnetwork\nTo address the challenges in AI-assisted gastroenterology, we\npropose the Deep Gastrointestinal Diagnosis Network (DGDN),\na novel deep learning architecture designed to improve disease\ndetection, segmentation, and classiﬁcation in gastrointestinal\n(GI) imaging. Unlike traditional models, DGDN integrates\nmultiple learning paradigms to enhance diagnostic accuracy and\ngeneralization (As shown in\nFigure 2).\n/three.tnum./three.tnum./one.tnum Multi-scale feature fusion\nDGDN employs a multi-scale feature fusion strategy to\neﬀectively capture both ﬁne-grained pathological features and\nbroader structural patterns in gastrointestinal imaging (As shown\nin\nFigure 3). Given an input image X ∈ RH×W×C, the network\nextracts features at diﬀerent spatial resolutions using convolutional\nlayers with varying kernel sizes. This allows the model to learn local\ntextures as well as global contextual information. The multi-scale\nfeature maps are deﬁned as:\nFmulti =Concat(Conv3×3(X), Conv5×5(X), Conv7×7(X)). (10)\nWhile concatenation preserves spatial information from\ndiﬀerent receptive ﬁelds, directly using these features can introduce\nredundancy. To address this, DGDN employs a learnable weighting\nmechanism to dynamically adjust the contribution of each feature\nmap, ensuring optimal information retention:\nFagg =\n∑\ni\nλiFmulti,i, where\n∑\ni\nλi =1. (11)\nTo further reﬁne the extracted multi-scale features, DGDN\napplies a channel attention mechanism that emphasizes\ninformative feature channels while suppressing irrelevant\nones. This is achieved by generating attention weights through a\nglobal pooling operation followed by two fully connected layers:\nAch =σ\n(\nW2 ReLU(W1 GAP(Fagg))\n)\n, (12)\nwhere GAP( ·) represents global average pooling, W1 and\nW2 are learnable parameters, and σ(·) is the sigmoid activation\nfunction. The attention-reﬁned feature representation is then\ncomputed as:\nFreﬁned =Ach ⊙Fagg, (13)\nwhere ⊙denotes element-wise multiplication. This ensures that\ndiagnostically signiﬁcant features receive higher attention, thereby\nimproving model interpretability and robustness. A spatial pyramid\npooling (SPP) module is incorporated to further enhance spatial\nrelationships across multiple scales. The feature map is divided into\ndiﬀerent-sized pooling bins, and the outputs are concatenated to\nform a multi-scale descriptor:\nFspp =Concat(Pool1×1(Freﬁned), Pool2×2(Freﬁned), Pool4×4(Freﬁned)).\n(14)\nBy combining multi-scale convolutional processing, adaptive\nfeature weighting, channel attention, and spatial pooling, DGDN\nFrontiers in Medicine /zero.tnum/five.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /two.tnum\nOverview of the Deep Gastrointestinal Diagnosis Network (DGDN) . The architecture consists of three key components, Multi-Sca le Feature Fusion\n(MSFF), Attention-Guided Diagnosis (AGD), and Domain-Adaptiv e Learning (DAL). MSFF extracts and integrates multi-scale s patial features to capture\nboth ﬁne-grained pathological details and global contextual patter ns. AGD employs an adaptive attention mechanism to enhance dia gnostically\nrelevant regions while suppressing background noise. DAL ensure s robust feature generalization across diﬀerent imaging domains t hrough\nadversarial domain adaptation and contrastive learning. The mod el leverages spatio-temporal data representation, hierarchic al feature extraction,\nand a self-adaptive fusion mechanism to improve disease detecti on, segmentation, and classiﬁcation in gastrointestinal imaging.\neﬀectively learns hierarchical representations that improve lesion\ndetection and classiﬁcation accuracy. This multi-scale feature\nfusion mechanism signiﬁcantly enhances the model’s capability to\ngeneralize across diverse gastrointestinal imaging conditions.\n/three.tnum./three.tnum./two.tnum Attention-guided diagnosis\nTo enhance the localization of key diagnostic regions,\nDGDN applies an attention-based spatial encoding mechanism\nthat adaptively reﬁnes feature representations. Traditional\nconvolutional networks struggle to highlight diagnostically\nrelevant regions consistently, particularly in complex medical\nimages with varying textures and lighting conditions. To\naddress this, we introduce an adaptive attention mechanism\nthat selectively enhances important features while suppressing\nirrelevant background information. The attention weights A are\ncomputed as:\nA =σ\n(\nConv1×1(Fagg)\n)\n, (15)\nwhere σ(·) denotes the sigmoid activation function, and Fagg\nrepresents the aggregated multi-scale feature map. The reﬁned\nfeature representation is obtained via element-wise multiplication:\nFattn =A ⊙Fagg, (16)\nwhere ⊙denotes Hadamard (element-wise) multiplication.\nHowever, static attention maps may not suﬃciently capture\ncomplex spatial dependencies. To enhance spatial selectivity, we\nintroduce an attention-based gating mechanism that leverages\nsecond-order interactions between feature channels:\nG =tanh\n(\nConv3×3(Fattn) +W ·Fattn\n)\n, (17)\nwhere W represents a learnable transformation matrix that\nenhances contextual interactions. This reﬁned attention map G is\nused to reweight the input feature representation:\nFﬁnal =G ⊙Fattn +Fagg. (18)\nTo ensure stable and reliable feature extraction across varying\nclinical conditions, an auxiliary supervision term is incorporated to\nregularize the attention distribution:\nLattn =\n∑\ni,j\n⏐\n⏐\n⏐\n⏐\n⏐Ai,j − exp(Ai,j)\n∑\nm,n exp(Am,n)\n⏐\n⏐\n⏐\n⏐\n⏐ , (19)\nwhich enforces a smooth and spatially coherent attention map.\nBy integrating this enhanced attention-guided mechanism, DGDN\nsigniﬁcantly improves interpretability and diagnostic accuracy,\nensuring more robust AI-driven medical image analysis.\nIt is important to note that the primary function of the AGD\nmodule is to enhance the interpretability of the model by focusing\nattention on diagnostically relevant regions within gastrointestinal\nFrontiers in Medicine /zero.tnum/six.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /three.tnum\nMulti-scale feature fusion (MSFF) in DGDN. The architecture i ntegrates multi-scale feature extraction, adaptive weightin g, and spatial attention to\nenhance gastrointestinal image analysis. The MSFF module captu res both ﬁne-grained and high-level structural details using c onvolutional layers of\ndiﬀerent kernel sizes. Feature selection is optimized through l earnable weighting and attention mechanisms, ensuring robust lesion detection. Token\nand channel selection reﬁne feature representations, while t he spatial attention mechanism (SAM) enhances spatial depen dencies. This hierarchical\nfusion strategy signiﬁcantly improves model generalization across d iverse imaging conditions.\nimages. The AGD mechanism serves as a spatial reﬁnement layer\nand does not perform classiﬁcation of speciﬁc disease categories\nsuch as polyps, ulcers, or tumors. Instead, diagnostic labeling\nis conducted by subsequent modules in the DGDN architecture\nthat utilize the reﬁned feature representations produced by AGD.\nMoreover, there is no direct or hard-coded coupling between\nthe generated attention maps and predeﬁned diagnostic classes.\nThe AGD module identiﬁes regions of interest based on feature\nsaliency, which indirectly supports classiﬁcation performance and\nmodel explainability without acting as a deterministic classiﬁer.\nWhile attention maps may vary in pattern across diﬀerent disease\ncases, their purpose is to guide, rather than determine, the\ndiagnostic outcome.\n/three.tnum./three.tnum./three.tnum Domain-adaptive learning\nTo improve generalization across diﬀerent imaging conditions\nand medical datasets, the proposed Domain-Generalized Deep\nNetwork (DGDN) leverages adversarial domain adaptation\ntechniques. These techniques enable DGDN to learn invariant\nfeature representations, reducing domain shifts between source\nand target distributions. A domain discriminator D is introduced\nto distinguish whether a feature representation originates from the\nsource domain Xs or the target domain Xt. The adversarial loss for\ndomain adaptation is formulated as:\nLdomain =−EXs [log D(Fattn,s)] −EXt [log(1 −D(Fattn,t))]. (20)\nHere, Fattn,s and Fattn,t represent attention-based feature\nembeddings extracted from the source and target domains,\nrespectively. The model is trained in an adversarial manner, where\nthe feature extractor aims to generate domain-invariant features by\nmaximizing Ldomain, while the domain discriminator D attempts\nto distinguish between source and target features. This adversarial\ninterplay leads to a more generalized feature space.\nTo domain adaptation, DGDN integrates both classiﬁcation\nand segmentation objectives, ensuring that the learned\nrepresentations retain clinical relevance. The total loss function is\nformulated as:\nL =Lcls +λsegLseg +λdomainLdomain. (21)\nHere, Lcls denotes the classiﬁcation loss, Lseg represents the\nsegmentation loss, and λseg, λdomain are weighting hyperparameters\ncontrolling the relative contributions of segmentation and\ndomain adaptation.\nTo further enhance domain robustness, we incorporate\ncontrastive learning into the feature space. Given a set of positive\nand negative feature pairs, contrastive loss encourages intra-\ndomain similarity while enforcing inter-domain separation:\nLcontrast =−\nN∑\ni=1\nlog exp(sim(Fs\ni , Ft\ni )/τ)\n∑ N\nj=1 exp(sim(Fs\ni , Ft\nj )/τ)\n, (22)\nFrontiers in Medicine /zero.tnum/seven.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nwhere sim(·, ·) is the cosine similarity function, Fs\ni and Ft\ni are the\nfeature representations from the source and target domains, and τ\nis a temperature scaling parameter.\nTo stabilize domain adaptation, we also introduce an entropy-\nbased regularization term that enforces prediction consistency\nacross domains. This is achieved by minimizing the entropy of the\nsoftmax output:\nLentropy =−EXt\n∑\nc\npc log pc, (23)\nwhere pc represents the predicted probability for class c in the\ntarget domain. This constraint encourages conﬁdent predictions\nwhile discouraging ambiguous outputs.\nWe deﬁne the total optimization objective as a weighted sum\nof classiﬁcation, segmentation, domain adaptation, contrastive, and\nentropy regularization losses:\nLtotal =Lcls +λsegLseg +λdomainLdomain\n+λcontrastLcontrast +λentropyLentropy. (24)\nThis comprehensive loss formulation enables DGDN\nto mitigate domain shifts, improve robustness, and ensure\nhigh performance in real-world clinical applications, where\nimaging conditions may vary signiﬁcantly across datasets and\nmedical institutions.\n/three.tnum./four.tnum Hierarchical adaptive fusion strategy\nTo further enhance the robustness and interpretability\nof AI-driven gastrointestinal (GI) diagnostics, we propose\nthe Hierarchical Adaptive Fusion Strategy (HAFS). This\nstrategy optimally integrates multi-scale features, uncertainty\nquantiﬁcation, and domain-aware adaptation to improve\ndiagnostic accuracy and generalization across diverse clinical\nenvironments. Unlike conventional fusion methods that\nrely on static feature aggregation, HAFS dynamically reﬁnes\ninformation from diﬀerent modalities and spatial resolutions using\na hierarchical optimization framework (As shown in\nFigure 4).\nIn this work, we deﬁne patient-speciﬁc optimization as the\nmodel’s ability to dynamically adapt its inference process based on\nindividual-level clinical context, rather than relying on population-\naveraged assumptions. This is achieved by incorporating\nmultimodal inputs–such as patient history, laboratory ﬁndings,\nand conversational cues–into the model’s internal decision-\nmaking pipeline. For example, during retrospective analysis,\nDGDN can weigh symptom descriptions or emotional tone\ndiﬀerently for elderly patients with a history of gastrointestinal\nbleeding, compared to younger patients undergoing routine\nscreening. In real-time settings, the uncertainty-aware module\nenables the model to ﬂag ambiguous predictions in patients with\ncomorbidities, prompting additional human review or more\nconservative diagnostic recommendations. This optimization\noccurs not through explicit per-patient retraining, but via\nfusion mechanisms that condition the feature representation on\nindividual data characteristics. In this way, DGDN supports a form\nof personalized inference, enhancing safety, interpretability, and\nclinical relevance.\nTo endoscopic imagery and structured clinical data, DGDN\nincorporates conversational inputs derived from patient–clinician\ndialogue, as exempliﬁed by the CMU-MOSEI dataset. While\nsuch dialogue-based sentiment or emotion signals are rarely\nexploited in current gastrointestinal diagnostic systems, they hold\nmeaningful clinical value. For example, during live endoscopy\nprocedures, real-time emotion recognition could alert physicians\nwhen patients express elevated anxiety, discomfort, or hesitation—\nserving as an early warning signal for adverse reactions or\nconsent issues. This functionality can enhance patient safety\nand personalized care, particularly in semi-conscious procedures\ninvolving sedation or discomfort. In post-procedure contexts,\ndialogue analysis can help summarize patient emotional responses,\ncontributing to counseling quality and patient satisfaction tracking.\nSentiment-aware models may identify patients who require\nadditional explanation, reassurance, or psychological follow-up.\nThese capabilities position DGDN not only as a diagnostic assistant\nbut also as a comprehensive patient interaction support system,\nenabling emotionally intelligent care in gastroenterology.\n/three.tnum./four.tnum./one.tnum Adaptive multi-scale fusion\nGastrointestinal imaging presents signiﬁcant variations in\nspatial resolution and texture across diﬀerent anatomical regions,\nrequiring a robust fusion mechanism to integrate multi-scale\ninformation eﬀectively. To address this, the Hierarchical Adaptive\nFusion Strategy (HAFS) organizes feature representations into a\nstructured hierarchy that captures both local details and global\ncontextual information. Given an input image X ∈ RH×W×C,\nHAFS employs convolutional layers with diﬀerent receptive ﬁelds\nto extract multi-scale features:\nFl =Conv7×7(X), Fm =Conv5×5(X), Fs =Conv3×3(X),\n(25)\nwhere Fl, Fm, and Fs correspond to feature maps with large,\nmedium, and small receptive ﬁelds, respectively. While simple\nconcatenation of these features may retain all spatial scales, it fails\nto consider their relative importance. To overcome this limitation,\nHAFS applies an adaptive weighting mechanism that dynamically\nselects the most relevant feature representations:\nFfused =\n∑\ni∈{s,m,l}\nαiFi, where\n∑\ni\nαi =1. (26)\nTo optimize the weight parameters αi, a self-attention\nmechanism is employed, which assigns higher importance to more\ninformative features. This attention is computed by normalizing\nactivation responses across scales:\nαi = exp(WiFi)\n∑\nj exp(WjFj) , (27)\nwhere Wi are learnable parameters that enable dynamic feature\nadaptation. To preserve spatial coherence and enhance global\ninformation ﬂow, HAFS introduces a residual fusion module that\nreﬁnes the aggregated feature representation:\nFﬁnal =Ffused +Wres ·GAP(Ffused), (28)\nFrontiers in Medicine /zero.tnum/eight.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /four.tnum\nHierarchical adaptive fusion strategy (HAFS) for AI-driven gast rointestinal diagnostics. A multi-scale fusion framework incorpora ting Transformer and\nConvolutional branches, uncertainty-aware prediction, and dom ain-invariant learning to enhance diagnostic accuracy and rob ustness across diverse\nclinical datasets. The adaptive multi-scale fusion module int egrates local and global features dynamically, ensuring eﬀecti ve representation learning.\nAn uncertainty-aware mechanism leverages Monte Carlo Dropout t o quantify prediction conﬁdence, improving reliability in medi cal applications.\nDomain-invariant learning mitigates distribution shifts usin g adversarial adaptation and statistical alignment, enhanci ng generalization across\ndiﬀerent imaging conditions.\nwhere GAP( ·) denotes global average pooling, and Wres\nscales the pooled feature map before reintroducing it to the fused\nrepresentation. This residual enhancement ensures that spatial\ndetails are preserved while incorporating high-level contextual\ninformation. By integrating hierarchical feature extraction,\nadaptive weighting, self-attention, and residual reﬁnement, HAFS\nsigniﬁcantly improves the robustness of multi-scale fusion,\nenabling superior performance in gastrointestinal lesion detection\nand classiﬁcation.\n/three.tnum./four.tnum./two.tnum Uncertainty-aware prediction\nTo improve reliability in clinical practice, HAFS incorporates\nan uncertainty-aware mechanism that quantiﬁes conﬁdence\nlevels in AI predictions, ensuring robust decision-making\nin high-stakes medical applications. Uncertainty estimation\nis particularly crucial in gastrointestinal diagnostics, where\nvariations in image quality, lighting conditions, and anatomical\ndiﬀerences can signiﬁcantly impact model predictions. To\ncapture epistemic uncertainty, we employ Monte Carlo Dropout\n(MC-Dropout), which approximates Bayesian inference by\nperforming multiple stochastic forward passes during inference.\nThe probability distribution of the model’s prediction is\nestimated as:\np(y|X) =1\nT\nT∑\nt=1\ng(Ffused;θt), (29)\nwhere θt represents model weights sampled from a dropout\ndistribution, and T is the number of stochastic forward passes. The\nvariance of these predictions quantiﬁes uncertainty, highlighting\nregions requiring additional scrutiny:\nσ2 =1\nT\nT∑\nt=1\n(g(Ffused;θt) −p(y|X))2. (30)\nTo further reﬁne uncertainty quantiﬁcation, we integrate\nan entropy-based regularization term that stabilizes uncertain\npredictions by penalizing high entropy in the output distribution:\nLentropy =−\n∑\nc\npc log pc, (31)\nFrontiers in Medicine /zero.tnum/nine.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nwhere pc represents the probability of class c. This entropy\nloss encourages conﬁdent predictions while maintaining\nmodel ﬂexibility. An uncertainty-aware decision threshold is\nintroduced to adaptively adjust classiﬁcation sensitivity based on\npredicted uncertainty:\nˆy =\n{\narg max p(y|X), if σ <τ,\nﬂag for review, otherwise.\n(32)\nHere, τ is a dynamic threshold that balances sensitivity and\nspeciﬁcity. By incorporating these techniques, HAFS ensures that\nhigh-uncertainty cases are ﬂagged for manual review, improving\ndiagnostic trustworthiness and enhancing real-world applicability\nin clinical settings.\n/three.tnum./four.tnum./three.tnum Domain-invariant learning\nTo mitigate domain shifts in medical imaging and enhance\nmodel generalization, Hybrid Adversarial Feature Selection\n(HAFS) employs adversarial domain adaptation. In real-world\nmedical applications, variations in imaging protocols, acquisition\ndevices, and patient populations often lead to discrepancies\nbetween training (source) and deployment (target) datasets.\nHAFS addresses this challenge by enforcing domain-invariant\nfeature learning through adversarial training (As shown in\nFigure 5). Given a labeled source dataset Ds = {(Xs, Ys)}\nand an unlabeled target dataset Dt = {Xt}, the model\nlearns transferable features using a domain discriminator\nD that attempts to diﬀerentiate between source and target\nrepresentations. The adversarial domain adaptation loss is\ndeﬁned as:\nLdomain =−EXs [log D(Ffused,s)] −EXt [log(1 −D(Ffused,t))]. (33)\nHere, Ffused,s and Ffused,t represent multi-scale fused feature\nembeddings extracted from the source and target domains,\nrespectively. The objective of the feature extractor f (·) is to generate\ndomain-invariant representations that deceive the discriminator D,\nthereby ensuring that Ffused,s and Ffused,t become indistinguishable.\nThis is achieved through a min-max optimization process:\nθf =arg min\nθf\nmax\nθD\nLdomain. (34)\nHere, θf and θD denote the parameters of the feature extractor\nand domain discriminator, respectively. The feature extractor is\noptimized to minimize the domain loss, while the discriminator is\ntrained to maximize it, leading to an adversarial equilibrium that\nenhances domain invariance.\nTo further ensure the transferability of learned representations,\nwe incorporate Maximum Mean Discrepancy (MMD), which\nexplicitly reduces statistical diﬀerences between source and\ntarget distributions in the feature space. The MMD loss is\ndeﬁned as:\nLMMD =\n\n\n\n\n\n\n1\nNs\nNs∑\ni=1\nk(Fi\nfused,s) − 1\nNt\nNt∑\nj=1\nk(Fj\nfused,t)\n\n\n\n\n\n\n2\n, (35)\nwhere k(·) is a kernel function, and Ns, Nt are the sample\nsizes from the source and target domains, respectively. This loss\nencourages the feature extractor to learn embeddings that have\nsimilar statistical properties across domains, improving adaptation\nwithout requiring labeled target samples.\nTo prevent catastrophic forgetting and ensure\nrobustness in the target domain, we introduce\nEntropy Minimization, which encourages the\nmodel to make conﬁdent predictions for target\ndomain samples:\nLentropy =−EXt\n∑\nc\npc log pc, (36)\nwhere pc denotes the predicted probability distribution over\nclasses. By minimizing entropy, the model is encouraged to learn\nwell-separated, high-conﬁdence predictions in the target domain.\nThe ﬁnal optimization objective of HAFS\ncombines classiﬁcation loss, adversarial domain\nadaptation, MMD-based statistical alignment, and\nentropy minimization:\nLtotal =Lcls +λdomainLdomain +λMMDLMMD +λentropyLentropy.\n(37)\nThis joint training framework enables HAFS to achieve\ndomain-invariant learning, thereby enhancing model robustness\nacross diverse imaging datasets and real-world clinical scenarios.\n/four.tnum Experimental setup\n/four.tnum./one.tnum Dataset\nThe CMU-MOSEI Dataset (37) is a large-scale multimodal\ndataset designed for sentiment and emotion analysis. It contains\nthousands of videos collected from online platforms, where\nspeakers express opinions on various topics. Each video is\nannotated with ﬁne-grained sentiment scores and multiple\nemotional labels, making it a valuable resource for studying\nhuman aﬀect in a multimodal context. The dataset includes\naudio, visual, and textual modalities, enabling researchers to\ndevelop and evaluate models that integrate diﬀerent data sources.\nIts diverse and well-annotated samples make it widely used in\nsentiment classiﬁcation and aﬀective computing research. The\nMIMIC-IV Dataset\n(38) is a comprehensive medical dataset\nderived from real-world intensive care unit (ICU) records. It\nincludes de-identiﬁed electronic health records, physiological\nwaveforms, laboratory test results, and medication histories of\npatients. The dataset provides a rich foundation for clinical\nresearch, enabling the development of predictive models for\ndisease progression, patient outcomes, and treatment optimization.\nWith its longitudinal structure and diverse patient demographics,\nMIMIC-IV supports studies in machine learning for healthcare,\nparticularly in critical care analytics and early warning systems.\nIts accessibility has contributed to signiﬁcant advancements\nin medical AI and decision support systems. The Kvasir-SEG\nDataset\n(39) is a high-quality medical dataset focused on\nFrontiers in Medicine /one.tnum/zero.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /five.tnum\nDomain-invariant learning framework with hybrid adversarial feature\nselection (HAFS). The framework illustrates a multi-stage\narchitecture designed to mitigate domain shifts in medical imagi ng\nand enhance model generalization. The core consists of a\nTransformer Encoder that processes multi-source feature input s,\nintegrating Engagement Feedback Mechanisms (EFM) for adaptive\nlearning. The system includes components like Dynamic Incenti ve\nAllocation (DIA) for task-speciﬁc adjustments and Retention\nProbability Estimation for long-term learning impact. The\nDomain-Invariant Learning process is enforced through adversar ial\ntraining using a domain discriminator, Maximum Mean Discrepa ncy\n(MMD) for statistical alignment, and entropy minimization to en sure\nrobust, conﬁdent predictions across both source and target\ndatasets. This uniﬁed framework supports eﬀective cross-domain\nfeature adaptation, essential for real-world clinical applic ations.\ngastrointestinal disease segmentation. It consists of annotated\nendoscopic images primarily depicting polyp regions, aiding\nin the development of automated segmentation and detection\nmodels. The dataset contains pixel-level annotations, ensuring\nprecise localization of abnormalities and enhancing the reliability\nof deep learning-based diagnostic systems. Its diverse sample\nset, covering various polyp appearances and sizes, makes it a\ncrucial benchmark for evaluating segmentation algorithms in\ngastroenterology. Researchers utilize Kvasir-SEG to improve early\npolyp detection, which plays a key role in preventing colorectal\ncancer through timely intervention. The GastroVision Dataset\n(40)\nis a multimodal dataset curated for the advancement of AI-driven\ngastroenterology applications. It contains endoscopic images and\nvideos annotated with diagnostic labels, supporting research in\nautomated lesion detection, classiﬁcation, and segmentation. The\ndataset captures a wide range of gastrointestinal conditions,\nincluding ulcers, polyps, and inﬂammation, making it a valuable\nresource for clinical decision support systems. Its inclusion of real-\nworld variability, such as diﬀerences in imaging conditions and\npatient demographics, enhances model robustness. GastroVision\nserves as a benchmark for developing computer-aided diagnosis\ntools that assist endoscopists in improving diagnostic accuracy\nand eﬃciency.\n/four.tnum./two.tnum Experimental details\nIn our experiments, we evaluate our model on four widely\nused text classiﬁcation datasets: CMU-MOSEI Dataset, MIMIC-\nIV Dataset, Kvasir-SEG Dataset, and GastroVision Dataset. These\ndatasets cover diverse text classiﬁcation tasks, including sentiment\nanalysis, topic categorization, and document classiﬁcation. Our\nmodel is implemented using PyTorch and trained on an NVIDIA\nA100 GPU with 40GB memory. We use the Adam optimizer with\nan initial learning rate of 3 ×10−5, which is scheduled to decay\nusing a cosine annealing strategy. The batch size is set to 32,\nand we use early stopping with a patience of 5 epochs based on\nvalidation loss. For text preprocessing, we tokenize all input data\nusing a pre-trained WordPiece tokenizer and truncate sequences\nto a maximum length of 512 tokens to maintain computational\neﬃciency. Stopwords are removed, and special characters are\nnormalized. We experiment with both word-level and subword-\nlevel tokenization to ensure robust text representation. Our model\nleverages a Transformer-based architecture with a bidirectional\nattention mechanism for better contextual understanding. We\nadopt a BERT-based encoder to extract deep semantic features\nfrom input texts. The encoder outputs are passed through a\nfully connected layer with a softmax activation function for\nclassiﬁcation. For training, we employ a cross-entropy loss function\nfor both binary and multi-class classiﬁcation tasks. The learning\nrate is ﬁne-tuned using a grid search over {1 ×10−5, 3 ×\n10−5, 5 ×10−5}, while the dropout rate is set to 0.1 to prevent\noverﬁtting. The number of Transformer layers is set to 12, and\nthe hidden dimension is 768. Positional encoding and layer\nnormalization are applied to enhance feature extraction. The\nmodel is trained for a maximum of 10 epochs, with evaluation\nconducted after each epoch on a held-out validation set. We use\nstandard classiﬁcation metrics for evaluation, including Accuracy,\nPrecision, Recall, and F1-score. For the CMU-MOSEI and Kvasir-\nSEG datasets, we report results for both binary and multi-class\nsentiment classiﬁcation tasks. For MIMIC-IV and GastroVision,\nwe evaluate performance on topic classiﬁcation. We adopt macro-\naveraged F1-score for datasets with imbalanced class distributions.\nThe results are averaged over ﬁve independent runs to ensure\nstability. To compare our approach with state-of-the-art models,\nwe benchmark against traditional machine learning classiﬁers and\ndeep learning architectures. Ablation studies are performed to\nanalyze the impact of diﬀerent components, including attention\nmechanisms, pre-trained embeddings, and ﬁne-tuning strategies.\nWe measure inference time per sample to evaluate computational\neﬃciency. To ensure fair evaluation, we follow the oﬃcial\ntraining/testing splits for each dataset. For GastroVision, we apply\nstratiﬁed sampling to maintain class balance. We also investigate\ndomain adaptation performance by training on one dataset\nand testing on another, analyzing generalization across diﬀerent\ntext classiﬁcation tasks. The experimental setup is designed to\nprovide comprehensive insights into our model’s eﬀectiveness and\neﬃciency (\nAlgorithm 1).\nFrontiers in Medicine /one.tnum/one.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nInput: Pretrained datasets: D ={CMU-MOSEI,\nMIMIC-IV, Kvasir-SEG, GastroVision}\nOutput: Trained DGDN model with optimal parameters\n1 Initialize model parameters θ\n2 Set learning rate η, batch size B, max epochs T\n3 for each dataset Di ∈D do\n4 Load dataset Di and split into training,\nvalidation, and test sets\n5 Tokenize and preprocess textual data\n6 for each epoch t =1 to T do\n7 Shuffle training data Dtrain\ni\n8 for each batch b =1 to\n|Dtrain\ni |\nB do\n9 Sample mini-batch B from Dtrain\ni\n10 Compute multi-scale feature maps:\nFmulti =Concat(Conv3×3(X),\nConv5×5(X), Conv7×7(X)) (38)\nCompute attention-weighted feature\nrepresentation:\nFattn =σ(Conv1×1(Fmulti)) ⊙Fmulti (39)\nApply classification and segmentation\nbranches:\nˆy =softmax(Wcls GAP(Fattn) +bcls) (40)\nCompute classification loss:\nLcls =−\n∑\nc\nyc log ˆyc (41)\nCompute domain adaptation loss:\nLdomain =−EXs [log D(Fattn,s)]\n−EXt [log(1 −D(Fattn,t))] (42)\nCompute total loss:\nL =Lcls +λdomainLdomain (43)\nUpdate model parameters:\nθ ←θ−η∇θL (44)\n11 end\n12 Evaluate on validation set and compute:\nAccuracy, Precision, Recall, F1-score\n(45)\nif validation loss does not improve for p\nepochs then\n13 Stop training (Early Stopping)\n14 end\n15 end\n16 end\n17 Evaluate on test set and report final performance\nmetrics\n18 return Trained DGDN Model\nAlgorithm /one.tnum. Training Procedure for DGDN.\n/four.tnum./three.tnum Comparison with SOTA methods\nTo evaluate the eﬀectiveness of our proposed method, we\ncompare it against state-of-the-art (SOTA) models on four\nbenchmark datasets: CMU-MOSEI, MIMIC-IV , Kvasir-SEG, and\nGastroVision. The results are reported in\nTables 1, 2. We assess\nmodel performance using standard classiﬁcation metrics, including\nAccuracy, Precision, Recall, and F1-score, where higher values\nindicate better performance.\nIn\nFigures 6, 7, our model consistently outperforms existing\nSOTA methods on CMU-MOSEI and MIMIC-IV datasets. For\nCMU-MOSEI, our method achieves an Accuracy of 90.3%,\nsurpassing the previous best model, ALBEF, which attains 88.7%.\nIn terms of F1-score, our approach improves upon ALBEF by 1.6%,\ndemonstrating superior sentiment classiﬁcation capability. On the\nMIMIC-IV dataset, our method achieves an Accuracy of 92.1%,\noutperforming UNITER’s 90.5%, while also attaining the highest\nPrecision and Recall scores. The improvements suggest that our\nmodel eﬀectively captures text semantics and topic distinctions\nin large-scale classiﬁcation tasks. It extends the comparison to\nthe Kvasir-SEG and GastroVision datasets. Our model continues\nto show strong performance, achieving an Accuracy of 88.2%\non Kvasir-SEG, outperforming ALBEF’s 86.9%. Similarly, the F1-\nscore reaches 86.0%, highlighting improved sentiment classiﬁcation\naccuracy. On GastroVision, our approach attains an Accuracy of\n83.7%, surpassing ALBEF’s 82.3%. The gains in Precision and Recall\nindicate that our model can better diﬀerentiate between document\ncategories despite the presence of overlapping topics.\nThe superior performance of our model can be attributed\nto several key factors. Our Transformer-based architecture\nleverages contextual embeddings more eﬀectively, capturing\nlong-range dependencies in text. Our multi-stage ﬁne-\ntuning approach ensures better adaptation to diﬀerent text\nclassiﬁcation tasks. The use of data augmentation techniques\nenhances model generalization across datasets. Our model\nincorporates adaptive attention mechanisms, allowing it\nto dynamically focus on relevant textual features. These\nresults demonstrate that our approach provides robust and\ngeneralizable improvements over existing SOTA methods in text\nclassiﬁcation tasks.\n/four.tnum./four.tnum Ablation study\nTo analyze the impact of diﬀerent components in our proposed\nmethod, we conduct an ablation study on four benchmark datasets:\nCMU-MOSEI, MIMIC-IV , Kvasir-SEG, and GastroVision. The\nresults are presented in\nTables 3, 4. We systematically remove key\nFrontiers in Medicine /one.tnum/two.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nTABLE /one.tnum Performance evaluation of our approach against state-of-the-art methods on CMU-MOSEI and MIMIC-IV datasets.\nModel CMU-MOSEI dataset MIMIC-IV dataset\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nMMBERT (45) 87.5 ±0.3 85.2 ±0.4 83.9 ±0.3 84.5 ±0.3 89.3 ±0.3 86.7 ±0.4 85.1 ±0.3 85.9 ±0.3\nCLIP (46) 88.1 ±0.4 86.0 ±0.3 84.7 ±0.3 85.2 ±0.3 90.2 ±0.3 87.4 ±0.4 86.0 ±0.3 86.5 ±0.3\nVisualBERT (47) 86.9 ±0.3 84.8 ±0.4 83.5 ±0.3 84.0 ±0.3 88.7 ±0.3 86.2 ±0.3 84.6 ±0.4 85.2 ±0.3\nUNITER (48) 88.4 ±0.4 86.5 ±0.3 85.0 ±0.3 85.7 ±0.3 90.5 ±0.3 87.8 ±0.3 86.3 ±0.4 86.9 ±0.3\nLXMERT (49) 87.3 ±0.3 85.4 ±0.3 84.1 ±0.4 84.6 ±0.3 89.5 ±0.3 86.9 ±0.3 85.4 ±0.4 85.8 ±0.3\nALBEF (50) 88.7 ±0.3 86.8 ±0.3 85.2 ±0.4 85.9 ±0.3 90.8 ±0.3 88.1 ±0.3 86.7 ±0.4 87.2 ±0.3\nOurs 90.3 ± 0.3 88.2 ± 0.3 86.9 ± 0.4 87.5 ± 0.3 92.1 ± 0.3 89.7 ± 0.3 88.2 ± 0.4 88.8 ± 0.3\nBold values indicate the best performance in each column.\nTABLE /two.tnum Evaluating the performance of our approach against state-of-the-art methods on Kvasir-SEG and GastroVision datasets.\nModel Kvasir-SEG dataset GastroVision dataset\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nMMBERT (45) 85.4 ±0.3 83.9 ±0.4 82.7 ±0.3 83.2 ±0.3 80.5 ±0.3 78.3 ±0.4 77.0 ±0.3 77.6 ±0.3\nCLIP (46) 86.1 ±0.4 84.5 ±0.3 83.1 ±0.3 83.7 ±0.3 81.2 ±0.3 79.0 ±0.4 77.8 ±0.3 78.3 ±0.3\nVisualBERT (47) 84.8 ±0.3 83.2 ±0.4 82.1 ±0.3 82.5 ±0.3 79.9 ±0.3 77.5 ±0.3 76.3 ±0.4 76.8 ±0.3\nUNITER (48) 86.6 ±0.4 85.1 ±0.3 83.8 ±0.3 84.3 ±0.3 81.8 ±0.3 79.7 ±0.3 78.2 ±0.4 78.8 ±0.3\nLXMERT (49) 85.7 ±0.3 84.0 ±0.3 82.9 ±0.4 83.4 ±0.3 80.9 ±0.3 78.8 ±0.3 77.4 ±0.4 77.9 ±0.3\nALBEF (50) 86.9 ±0.3 85.4 ±0.3 84.0 ±0.4 84.6 ±0.3 82.3 ±0.3 80.1 ±0.3 78.7 ±0.4 79.2 ±0.3\nOurs 88.2 ± 0.3 86.7 ± 0.3 85.5 ± 0.4 86.0 ± 0.3 83.7 ± 0.3 81.5 ± 0.3 80.2 ± 0.4 80.7 ± 0.3\nBold values indicate the best performance in each column.\nFIGURE /six.tnum\nBenchmarking advanced methods on CMU-MOSEI and MIMIC-IV datas ets: a comparative performance study.\ncomponents of our model and assess their eﬀects on Accuracy,\nPrecision, Recall, and F1-score.\nIn\nFigures 8, 9, the ﬁrst ablation removes Attention-Guided\nDiagnosis. This results in a notable performance drop across all\ndatasets. For instance, on the CMU-MOSEI dataset, Accuracy\ndecreases from 90.3% to 89.1%, while the F1-score drops from\n87.5% to 86.5%. Similarly, on the Kvasir-SEG dataset, Accuracy\ndecreases from 88.2% to 87.4%. This demonstrates that the\nFrontiers in Medicine /one.tnum/three.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /seven.tnum\nComparative analysis of state-of-the-art methods on Kvasir-SE G and GastroVision datasets.\nattention mechanism plays a crucial role in capturing contextual\ndependencies, leading to better text representations. The second\nablation removes Domain-Adaptive Learning, which adjusts token\nrepresentations based on sentence-level context. This degradation\nis noticeable, with Accuracy dropping to 88.5% on CMU-MOSEI\nand 86.8% on Kvasir-SEG. The reduced F1-score suggests that the\nabsence of contextual reﬁnement leads to weaker generalization, as\nseen in the MIMIC-IV dataset, where Accuracy drops from 92.1%\nto 90.6%. This highlights the importance of ﬁne-grained contextual\nmodeling in classiﬁcation tasks. The third ablation eliminates\nUncertainty-Aware Prediction, which integrates information from\ndiﬀerent layers. This results in a moderate drop in performance,\nparticularly aﬀecting Recall values. On CMU-MOSEI, Recall\ndecreases from 86.9% to 85.6%, indicating that removing this\nmodule causes the model to miss subtle sentiment indicators.\nThe same pattern is observed on the GastroVision dataset, where\nRecall drops from 80.2% to 78.3%, demonstrating the module’s\nimportance in long-text classiﬁcation.\nOur complete model consistently outperforms all ablation\nvariants across all datasets. The results conﬁrm that each\ncomponent plays a signiﬁcant role in improving text classiﬁcation\nperformance. Attention-Guided Diagnosis enhances contextual\nunderstanding, Domain-Adaptive Learning strengthens feature\nrepresentation, and Uncertainty-Aware Prediction ensures\neﬀective integration of hierarchical information. These ﬁndings\nvalidate the eﬀectiveness of our architectural choices in achieving\nstate-of-the-art performance in text classiﬁcation tasks.\nTo further investigate the eﬀectiveness of our domain\nadaptation strategy under real-world deployment conditions, we\nconducted a cross-institutional generalization experiment. In this\nsetup, the model was trained exclusively on the Kvasir-SEG\ndataset, which features endoscopic images captured using a speciﬁc\nclinical protocol and equipment setup, and then evaluated on\nthe GastroVision dataset, which includes data collected from\nmultiple institutions with heterogeneous imaging conditions,\ndevice manufacturers, and acquisition styles. This simulation\nclosely mimics practical domain shifts encountered in clinical\npractice, such as diﬀerences in illumination, resolution, staining,\nand operating habits across hospitals. As presented in\nTable 5, the\nbaseline model without any domain adaptation showed a marked\ndecrease in generalization performance when applied to the out-\nof-domain GastroVision dataset. Accuracy dropped to 80.2%, and\nthe F1-score fell to 77.5%, indicating limited robustness in cross-\nsite deployment scenarios. The precision and recall also suﬀered,\nsuggesting that the model failed to reliably identify and characterize\nlesions under unfamiliar imaging styles. When Domain-Adaptive\nLearning (DAL) was introduced–leveraging adversarial domain\nalignment and contrastive representation learning–the model’s\nperformance improved across all evaluation metrics. The F1-\nscore rose to 80.5% (a gain of 3.0 percentage points over the\nbaseline), with recall improving from 76.8% to 80.1%. This\nimprovement highlights the DAL module’s capacity to reduce the\nfeature space discrepancy between source and target domains. The\nmost signiﬁcant improvement was observed when the full HAFS\n(Hierarchical Adaptive Fusion Strategy) framework was applied.\nThis conﬁguration achieved an accuracy of 85.6%, precision of\n83.5%, recall of 82.9%, and F1-score of 83.2%—representing\nan absolute gain of 5.7% in F1-score over the baseline and\n2.7% over DAL alone. These gains demonstrate the eﬀectiveness\nof HAFS in achieving cross-domain robustness by combining\ndomain-aware fusion, uncertainty-aware prediction, and residual\nadaptation. These results conﬁrm that while domain adaptation\nsigniﬁcantly enhances generalization under distributional shifts,\nthere remains a non-negligible performance gap compared to in-\ndomain evaluations. Future eﬀorts should explore multi-source and\nfederated training paradigms to further bridge this generalization\ngap in heterogeneous clinical environments.\nThis evaluation involved a comparison between four diﬀerent\nconﬁgurations: an image-only baseline, a text-only baseline, a\nmodel that performs early fusion by simply concatenating visual\nand textual features, and the complete DGDN model which\nemploys hierarchical adaptive fusion. According to the results\nFrontiers in Medicine /one.tnum/four.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nTABLE /three.tnum Exploring the impact of model components through ablation study on CMU-MOSEI and MIMIC-IV datasets.\nModel CMU-MOSEI dataset MIMIC-IV dataset\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nw./o. Attention-\nGuided\nDiagnosis\n89.1 ±0.3 87.4 ±0.4 85.9 ±0.3 86.5 ±0.3 91.0 ±0.3 88.3 ±0.4 86.8 ±0.3 87.3 ±0.3\nw./o. Domain-\nAdaptive\nLearning\n88.5 ±0.4 87.0 ±0.3 85.4 ±0.3 86.0 ±0.3 90.6 ±0.3 88.0 ±0.3 86.5 ±0.4 87.0 ±0.3\nw./o.\nUncertainty-\nAware\nPrediction\n88.9 ±0.3 87.2 ±0.4 85.6 ±0.3 86.2 ±0.3 90.8 ±0.3 88.1 ±0.3 86.6 ±0.4 87.1 ±0.3\nOurs 90.3 ± 0.3 88.2 ± 0.3 86.9 ± 0.4 87.5 ± 0.3 92.1 ± 0.3 89.7 ± 0.3 88.2 ± 0.4 88.8 ± 0.3\nBold values indicate the best performance in each column.\nTABLE /four.tnum Comprehensive ablation analysis of our method on Kvasir-SEG and GastroVision datasets.\nModel Kvasir-SEG dataset GastroVision dataset\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nw./o. Attention-\nGuided\nDiagnosis\n87.4 ±0.3 85.9 ±0.4 84.6 ±0.3 85.2 ±0.3 82.1 ±0.3 80.0 ±0.4 78.5 ±0.3 79.1 ±0.3\nw./o. Domain-\nAdaptive\nLearning\n86.8 ±0.4 85.3 ±0.3 84.0 ±0.3 84.5 ±0.3 81.6 ±0.3 79.5 ±0.3 78.1 ±0.4 78.6 ±0.3\nw./o.\nUncertainty-\nAware\nPrediction\n87.1 ±0.3 85.6 ±0.4 84.3 ±0.3 84.8 ±0.3 81.9 ±0.3 79.8 ±0.3 78.3 ±0.4 78.9 ±0.3\nOurs 88.2 ± 0.3 86.7 ± 0.3 85.5 ± 0.4 86.0 ± 0.3 83.7 ± 0.3 81.5 ± 0.3 80.2 ± 0.4 80.7 ± 0.3\nBold values indicate the best performance in each column.\nFIGURE /eight.tnum\nIn-Depth ablation analysis of our approach on CMU-MOSEI and MIMI C-IV datasets. Attention-guided diagnosis (AGD), Domain-adapt ive learning\n(DAL), and uncertainty-aware prediction (UAP).\npresented in Table 6, the DGDN model demonstrates superior\nperformance over all baseline methods, achieving higher scores\nin accuracy, precision, recall, and F1-score across both evaluated\ndatasets. On the MedICaT dataset\n(41), which involves disease\ntagging based on medical illustrations and captions, the image-only\nmodel achieved an F1-score of 73.8%, and the text-only model\nFrontiers in Medicine /one.tnum/five.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nFIGURE /nine.tnum\nIn-depth analysis of our method through ablation study on Kvasir-S EG and GastroVision datasets.\nTABLE /five.tnum Evaluation of domain adaptation under cross-institutional setting.\nModel variant Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum Score↑\nBaseline (no domain adaptation) 80.2 ±0.4 78.3 ±0.3 76.8 ±0.4 77.5 ±0.3\n+ Domain-adaptive learning (DAL) 83.0 ±0.3 81.0 ±0.3 80.1 ±0.3 80.5 ±0.3\n+ Full HAFS Framework 85.6 ± 0.3 83.5 ± 0.3 82.9 ± 0.4 83.2 ± 0.3\nBold values indicate the best performance in each column.\nslightly improved to 75.6%. Early fusion increased the F1-score\nto 78.8%, suggesting some beneﬁt from combining modalities.\nHowever, DGDN further elevated performance to 81.3%, marking\na +7.5% absolute gain over the image-only setting and a +2.5%\ngain over early fusion. This improvement indicates that DGDN’s\narchitecture not only supports multimodal input but eﬀectively\nlearns synergistic representations from both modalities. On the\nMIMIC-CXR dataset\n(42), which is composed of radiology images\nand structured report text, the pattern is consistent. The image-\nonly model attained a 72.8% F1-score, while text-only reached\n74.0%. Early fusion lifted performance to 76.5%, and DGDN\nachieved 78.6%, reﬂecting a +5.8% boost over the image-only\nmodel. This conﬁrms that DGDN’s multimodal fusion mechanisms\nare eﬀective even in complex, report-driven classiﬁcation tasks.\nThey also demonstrate that the proposed fusion design yields\nsigniﬁcant gains over simple fusion baselines, both in diagnostic\naccuracy and semantic alignment across modalities. While we\nacknowledge the absence of currently available endoscopy-speciﬁc\ndatasets containing all three modalities (image, report, dialogue)\nfor the same patient, this experiment serves as a validated proxy\nand a proof-of-concept for DGDN’s design. We plan to pursue\nuniﬁed gastrointestinal multimodal data collection as part of\nfuture work.\nWe conducted an evaluation of the DGDN framework using\nthree well-established retrospective datasets: MIMIC-IV\n(43),\nMIMIC-CXR (42), and NIH ChestX-ray14 (44). These datasets\nwere all collected in authentic clinical environments without\nprospective study design, encompassing imaging or clinical data at\nthe patient level, captured as part of routine hospital operations.\nThe evaluation was carried out under two distinct data split\nprotocols. In the in-hospital split, the training and testing sets may\ninclude partially overlapping patient cohorts, reﬂecting scenarios\nwhere models are deployed within the same healthcare institution.\nIn contrast, the out-of-hospital split ensures that all patients in\nthe test set are entirely unseen during training, thereby simulating\ndeployment in new clinical contexts or across diﬀerent institutions\nand oﬀering a stringent test of the model’s generalization capability.\nAs shown in\nTable 7, across both evaluation settings, the proposed\nDGDN model consistently outperforms all baselines in terms of\nAccuracy, Precision, Recall, and F1 Score. On the in-hospital split,\nDGDN achieves an F1 score of 85.6%, outperforming the NIH\nChestX-ray14 baseline (83.9%) and MIMIC-CXR baseline (82.4%),\nwith a relative improvement of +1.7% and +3.2% respectively.\nThis suggests that even when evaluated on familiar institutional\ndata, DGDN provides tangible gains through its multimodal\nintegration and adaptive fusion mechanisms. More importantly, in\nthe out-of-hospital split, which evaluates the model’s robustness\nto unseen patient distributions and clinical protocols, DGDN\nmaintains strong performance with an F1 score of 80.6%, clearly\nsurpassing NIH ChestX-ray14 (77.7%), MIMIC-CXR (76.4%), and\nFrontiers in Medicine /one.tnum/six.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nTABLE /six.tnum Evaluation of DGDN on multimodal integration datasets(Image + Text).\nModel variant MedICaT dataset MIMIC-CXR dataset\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nMedICaT (41) 76.5 ±0.3 74.2 ±0.3 73.5 ±0.4 73.8 ±0.3 75.3 ±0.3 73.0 ±0.4 72.6 ±0.3 72.8 ±0.3\nMIMIC-CXR (42) 78.2 ±0.3 76.5 ±0.4 74.8 ±0.3 75.6 ±0.3 76.8 ±0.3 74.9 ±0.3 73.2 ±0.4 74.0 ±0.3\nOurs (DGDN) 83.2 ± 0.3 81.6 ± 0.3 81.0 ± 0.4 81.3 ± 0.3 80.8 ± 0.3 79.2 ± 0.3 78.1 ± 0.4 78.6 ± 0.3\nBold values indicate the best performance in each column.\nMIMIC-IV (74.1%). The performance gap between in- and out-of-\nhospital settings is also smallest for DGDN (5.0 percentage points),\ncompared to 6.2% for NIH ChestX-ray14 and 8.3% for MIMIC-IV ,\nconﬁrming that DGDN exhibits superior generalization and lower\noverﬁtting risk in retrospective clinical contexts. These results\ndirectly validate the practical reliability of DGDN for real-world\ndeployment. By demonstrating stable performance on retrospective\ndatasets with diﬀerent data sources, DGDN is shown to be more\nresilient to inter-institutional variation–an essential property for\nAI systems used in large-scale clinical environments. This evidence\nfurther supports the claim that our model is not merely overﬁtting\nbenchmark datasets, but is capable of handling diverse, historically\ncollected patient data with robustness and consistency.\n/five.tnum Discussion\nWhile the proposed DGDN framework was evaluated across\ndiverse datasets to demonstrate its multimodal capabilities, we\nacknowledge that not all datasets reﬂect real-world endoscopic\ndiagnostic scenarios. CMU-MOSEI and MIMIC-IV , though\nrepresentative of conversational and structured clinical data\nrespectively, are not inherently imaging-based nor collected in\ndirect endoscopy contexts. Their inclusion in our study primarily\nserves to validate the model’s cross-modal adaptability, rather than\nclinical integration in its current form. This distinction is crucial\nto interpret our ﬁndings accurately. The lack of uniﬁed datasets\nencompassing synchronized endoscopic images, patient dialogue,\nand structured EHR for the same individuals remains a barrier to\ncomprehensive clinical validation. Future work should focus on\nbuilding or accessing such integrated multimodal clinical datasets\nto enable end-to-end deployment and evaluation of systems like\nDGDN in practical gastroenterological workﬂows.\nWe acknowledge that the real-time clinical deployment of\nlarge language models (LLMs) remains technically challenging,\nparticularly in high-speed procedural environments such as\nendoscopic surgery. The computational demands, latency, and\ninfrastructure requirements of current LLM architectures limit\ntheir feasibility for synchronous interaction during procedures. In\nour proposed framework, LLMs are primarily intended to support\nnear-real-time interaction outside of critical surgical loops–such\nas automated documentation, post-procedure summarization,\nand asynchronous clinical decision support. For example,\nLLMs can be used to generate structured endoscopy reports\nbased on multimodal inputs (images, patient data, dialogue\ntranscripts) shortly after the procedure, reducing physician\ndocumentation workload and improving consistency. Real-\ntime intra-procedural guidance remains an aspirational goal,\npotentially realizable through future developments such as on-\ndevice LLM distillation, model compression, or hybrid cloud-edge\ndeployments. Furthermore, a layered deployment strategy can be\nadopted, wherein lightweight decision rules or vision-language\nmodules provide intra-operative cues, while full LLM-based\nsynthesis is performed post-operatively. This hybrid paradigm\nbalances responsiveness and computational tractability while\npreserving clinical utility.\nEthical considerations are paramount in the clinical application\nof AI models, particularly those involving sensitive patient data\nand automated diagnostic reasoning. Although this study utilizes\npublicly available de-identiﬁed datasets, real-world deployment\nwould necessitate stringent adherence to privacy regulations such\nas HIPAA and GDPR. Furthermore, ensuring fairness across\ndiverse patient populations is critical; AI systems must be evaluated\nfor demographic biases that may arise from training data imbalance\nor institutional heterogeneity. Another concern is the explainability\nof model outputs. In high-stakes clinical settings, black-box\npredictions can undermine clinician trust and pose medico-\nlegal challenges. Our framework addresses this partially through\nattention visualization and uncertainty quantiﬁcation; however,\nfurther development of transparent reasoning mechanisms is\nessential. Future implementations should also account for informed\nconsent regarding AI usage, clearly delineating the boundaries\nof machine-augmented recommendations vs. physician decision-\nmaking authority. Establishing oversight protocols, continuous\nauditing, and ethical review processes will be essential to safeguard\npatient safety, trust, and autonomy as AI tools like DGDN\ntransition from research to clinical environments.\n/six.tnum Conclusions and future work\nThe integration of large language models (LLMs) with\nmultimodal learning presents a transformative opportunity in\ngastroenterology, particularly in digestive endoscopy. Traditional\nAI-assisted endoscopic systems primarily rely on single-modal\nimage analysis, which lacks contextual awareness and adaptability\nto complex gastrointestinal (GI) conditions. These conventional\napproaches face critical limitations, such as domain shifts, data\nheterogeneity, and interpretability issues, which hinder their\nclinical applicability. To overcome these challenges, we propose a\nmultimodal learning framework that seamlessly integrates LLM-\npowered chatbots with endoscopic imaging and patient-speciﬁc\nmedical data. Our method leverages self-supervised learning to\nFrontiers in Medicine /one.tnum/seven.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nTABLE /seven.tnum Retrospective validation of DGDN on MIMIC-CXR dataset.\nModel variant In-hospital split (seen patients) Out-of-hospital split (unseen patients)\nAccuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑ Accuracy ↑ Precision ↑ Recall ↑ F/one.tnum score↑\nMIMIC-IV (43) 83.6 ±0.3 81.4 ±0.4 79.5 ±0.3 80.4 ±0.3 77.5 ±0.3 75.2 ±0.4 73.0 ±0.3 74.1 ±0.3\nMIMIC-CXR (42) 85.1 ±0.3 83.0 ±0.4 81.8 ±0.3 82.4 ±0.3 79.2 ±0.3 77.0 ±0.4 75.8 ±0.3 76.4 ±0.3\nNIH ChestX-ray14\n(44)\n86.5 ±0.3 84.6 ±0.3 83.2 ±0.4 83.9 ±0.3 80.4 ±0.3 78.1 ±0.3 77.3 ±0.4 77.7 ±0.3\nOurs (DGDN) 88.0 ± 0.3 86.2 ± 0.3 85.0 ± 0.4 85.6 ± 0.3 83.1 ± 0.3 81.2 ± 0.3 80.0 ± 0.4 80.6 ± 0.3\nBold values indicate the best performance in each column.\nextract clinically relevant patterns from heterogeneous sources,\nenabling real-time guidance and AI-assisted report generation. A\ndomain-adaptive learning strategy enhances model generalization\nacross diverse patient populations and imaging conditions.\nExperimental evaluations on multiple GI datasets conﬁrm that our\napproach improves lesion detection, reduces diagnostic variability,\nand enhances physician-AI collaboration, highlighting its potential\nto advance AI-driven gastroenterology.\nDespite these promising results, our approach presents two\nprimary limitations. Real-time processing eﬃciency remains a\nchallenge due to the computational demands of multimodal data\nfusion and LLM inference. The integration of high-dimensional\nimage data with LLM-based text processing requires substantial\ncomputational resources, which may limit deployment in resource-\nconstrained clinical environments. Future research should focus\non model optimization techniques, including quantization,\npruning, and hardware acceleration, to improve eﬃciency. Model\ngeneralization across diﬀerent medical institutions and populations\nrequires further validation. While our domain-adaptive learning\nstrategy mitigates some generalization issues, real-world\nvariations in endoscopic equipment, clinical protocols, and\npatient demographics may introduce biases. Future work should\nexplore continual learning and federated learning approaches to\nenhance adaptability while preserving patient privacy. Addressing\nthese challenges will be essential for the successful integration\nof LLM-driven multimodal AI systems in digestive endoscopy,\nultimately improving diagnostic accuracy, procedural eﬃciency,\nand clinical decision-making in gastroenterology.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nEthics statement\nEthical review and approval was not required for the study\non human participants in accordance with the local legislation\nand institutional requirements. Written informed consent from the\npatients/participants or patients/participants’ legal guardian/next\nof kin was not required to participate in this study in accordance\nwith the national legislation and the institutional requirements.\nAuthor contributions\nYQ: Writing – original draft, Writing – review & editing. JC:\nWriting – original draft, Writing – review & editing. LL: Writing –\noriginal draft, Writing – review & editing. MW: Writing – original\ndraft, Writing – review & editing.\nFunding\nThe author(s) declare that ﬁnancial support was received\nfor the research and/or publication of this article. This work\nwas supported by the National Natural Science Foundation\nof China [grant number 81774266], the Second National\nFamous Traditional Chinese Medicine Practitioner Inheritance\nWorkshop [grant number National Oﬃce of Traditional\nChinese Medicine Human Education Letter (2022) No. 245],\nWu Mianhua National Famous Elderly Chinese Medicine\nExperts Inheritance Workshop [grant number National\nTraditional Chinese Medicine Human Education Letter (2022)\nNo. 75], Wu Mianhua Jiangsu Famous Elderly Chinese\nMedicine Experts Inheritance Workshop [grant number\nJiangsu Chinese Medicine Science and Education (2021) No.\n7], the Seventh Batch of National Old Chinese Medicine\nExperts’ Academic Experience Inheritance Work Program\nof the State Administration of Traditional Chinese Medicine\n(SATCM) [grant number National TCM Human Education\nLetter (2022) No. 76], and the Graduate Student Research\nand Practice Innovation Program in Jiangsu Province [grant\nnumber SJCX23_0875].\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nGenerative AI statement\nThe author(s) declare that no Gen AI was used in the creation\nof this manuscript.\nFrontiers in Medicine /one.tnum/eight.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\n1. Hu J, Yao Y, Wang C, Wang S, Pan Y, Chen QA, et al. Large multiling ual models\npivot zero-shot multimodal learning across languages. In: International Conference on\nLearning Representations. (2023).\n2. Han X, Wu Y, Zhang Q, Zhou Y, Xu Y, Qiu H, et al. Backdooring mult imodal\nlearning. In: 2024 IEEE Symposium on Security and Privacy (SP). (2024). p. 3385–403.\ndoi: 10.1109/SP54263.2024.00031\n3. Peng X, Wei Y, Deng A, Wang D, Hu D. Balanced multimodal learnin g via on-\nthe-ﬂy gradient modulation. In: 2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) . (2022). p. 8228–37. doi: 10.1109/CVPR52688.2022.00806\n4. Zong Y, Aodha OM, Hospedales T. Self-supervised multimodal\nlearning: a survey. IEEE Trans Pattern Anal Mach Intell . (2024) 12:1–20.\ndoi: 10.1109/TPAMI.2024.3429301\n5. Xu P , Zhu X, Clifton DA. Multimodal learning with transformer s:\na survey. IEEE Trans Pattern Anal Mach Intell . (2023) 45:12113–32.\ndoi: 10.1109/TPAMI.2023.3275156\n6. Wang Y, Cui Z, Li Y. Distribution-consistent modal recove ring for incomplete\nmultimodal learning. In: 2023 IEEE/CVF International Conference on Computer Vision\n(ICCV). (2023). p. 21968–77. doi: 10.1109/ICCV51070.2023.02013\n7. Xu W, Wu Y, Ouyang F. Multimodal learning analytics of collabora tive patterns\nduring pair programming in higher education. Int J Educ Technol High Educ . (2023)\n20:8. doi: 10.1186/s41239-022-00377-z\n8. Wei S, Luo C, Luo Y. “MMANet: margin-aware distillation and m odality-\naware regularization for incomplete multimodal learning, ” in 2023 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR). (2023). p. 20039–49.\ndoi: 10.1109/CVPR52729.2023.01919\n9. Zhang H, Zhang C, Wu B, Fu H, Zhou JT, Hu Q. Calibrating multimo dal learning.\nIn: International Conference on Machine Learning . (2023).\n10. Hao Y, Stuart T, Kowalski M, Choudhary S, Hoﬀman P , Hartman A, et al.\nDictionary learning for integrative, multimodal, and scalable s ingle-cell analysis. Nat\nBiotechnol. (2022) 42:293–304. doi: 10.1038/s41587-023-01767-y\n11. Song B, Miller S, Ahmed F. Attention-enhanced multimodal lea rning for\nconceptual design evaluations. J Mech Des . (2023) 145:041410. doi: 10.1115/1.4056669\n12. Joseph J, Thomas B, Jose J, Pathak N. Decoding the growth of multimodal\nlearning: a bibliometric exploration of its impact and inﬂuence. Intell Dec Technol .\n(2024) 18:151–67. doi: 10.3233/IDT-230727\n13. Zhou Y, Wang X, Chen H, Duan X, Zhu W. Intra- and inter-moda l curriculum\nfor multimodal learning. In: Proceedings of the 31st ACM International Conference on\nMultimedia. (2023). p. 3724–35. doi: 10.1145/3581783.3612468\n14. Shi B, Hsu WN, Lakhotia K, Rahman Mohamed A. Learning audi o-visual speech\nrepresentation by masked multimodal cluster prediction. In: International Conference\non Learning Representations. (2022). Available online at: https://iclr.cc/virtual/2022/\nposter/6707\n15. Zhang Y, He N, Yang J, Li Y, Wei D, Huang Y, et al. mmFormer: mu ltimodal\nmedical transformer for incomplete multimodal learning of brai n tumor segmentation.\nIn: Medical Image Computing and Computer Assisted Intervention – MICCAI.\nLecture Notes in Computer Science, Vol. 13435. Cham: Springer (2022). p. 107–17.\ndoi: 10.1007/978-3-031-16443-9_11\n16. Bayoudh K, Knani R, Hamdaoui F, Mtibaa A. A survey on deep m ultimodal\nlearning for computer vision: advances, trends, applications, a nd datasets. Vis Comput.\n(2021) 38:2939–70. doi: 10.1007/s00371-021-02166-7\n17. Lian Z, Chen L, Sun L, Liu B, Tao J. GCNet: graph completion netw ork for\nincomplete multimodal learning in conversation. IEEE Trans Pattern Anal Mach Intell .\n(2023) 45:1–14. doi: 10.1109/TPAMI.2023.3234553\n18. Ma M, Ren J, Zhao L, Tulyakov S, Wu C, Peng X. SMIL: multimodal learning\nwith severely missing modality. In: Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence. (2021). p. 2302–10. doi: 10.1609/aaai.v35i3.16330\n19. Du C, Fu K, Li J, He H. Decoding visual neural representatio ns by multimodal\nlearning of brain-visual-linguistic features. IEEE Trans Pattern Anal Mach Intell . (2023)\n45:10760–77. doi: 10.1109/TPAMI.2023.3263181\n20. Chango W, Lara JA, Cerezo R, Romero C. A review on data fusi on in multimodal\nlearning analytics and educational data mining. WIREs Data Min Knowl Disc . (2022)\n12:e1458. doi: 10.1002/widm.1458\n21. Pellegrino R, Palladino G, Pagliuca F, Lucá S, Federico A, Gravin a AG. Cutaneous\nkaposi’s sarcoma following long-term inﬂiximab treatment in a pat ient with HIV-\nnegative antibiotic-dependent chronic pouchitis: consider ations on an exceptional\nﬁnding. Gastrointest Disor. (2024) 6:984–92. doi: 10.3390/gidisord6040069\n22. Pellegrino R, Palladino G, Izzo M, De Costanzo I, Landa F, Feder ico A,\net al. Water-assisted colonoscopy in inﬂammatory bowel disease s: From technical\nimplications to diagnostic and therapeutic potentials. World J Gastrointest Endosc .\n(2024) 16:647–60. doi: 10.4253/wjge.v16.i12.647\n23. Fan Y, Xu W, Wang H, Wang J, Guo S. PMR: prototypical modal reba lance for\nmultimodal learning. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). (2023). p. 20029–38. doi: 10.1109/CVPR52729.2023.01918\n24. Yan L, Zhao L, Gasevic D, Martinez-Maldonado R. Scalability , sustainability,\nand ethicality of multimodal learning analytics. In: LAK22: LAK22: 12th International\nLearning Analytics and Knowledge Conference. (2022). doi: 10.1145/3506860.3506862\n25. Ektefaie Y, Dasoulas G, Noori A, Farhat M, Zitnik M. Multimod al learning with\ngraphs. Nat Mach Intell. (2023) 5:340–50. doi: 10.1038/s42256-023-00624-6\n26. Yang Z, Fang Y, Zhu C, Pryzant R, Chen D, Shi Y, et al. i-Code: an integrative\nand composable multimodal learning framework. In: AAAI Conference on Artiﬁcial\nIntelligence. (2022).\n27. Chai W, Wang G. Deep vision multimodal learning: methodology , benchmark,\nand trend. Appl Sci. (2022) 12:6588. doi: 10.3390/app12136588\n28. Pellegrino R, Federico A, Gravina AG. Conversational LLM Cha tbot ChatGPT-\n4 for colonoscopy boston bowel preparation scoring: an artiﬁcia l intelligence-to-head\nconcordance analysis. Diagnostics. (2024) 14:2537. doi: 10.3390/diagnostics14222537\n29. Song Y, Mao X, Zhou X, He S, Chen Y, Zhang L, et al. Use of artiﬁ cial intelligence\nto improve the quality control of gastrointestinal endoscopy. Front Med . (2021)\n8:709347. doi: 10.3389/fmed.2021.709347\n30. Wu X, Li M, Cui X, Xu G. Deep multimodal learning for lymph node\nmetastasis prediction of primary thyroid cancer. Phys Med Biol . (2022) 67:035008.\ndoi: 10.1088/1361-6560/ac4c47\n31. Yu W, Xu H, Yuan Z, Wu J. Learning modality-speciﬁc represent ations\nwith self-supervised multi-task learning for multimodal sentim ent analysis. In:\nProceedings of the AAAI Conference on Artiﬁcial Intelligence. (2021). p. 10790–7.\ndoi: 10.1609/aaai.v35i12.17289\n32. Zhou X, Verma RM. Vulnerability detection via multimodal lea rning: datasets\nand analysis. In: Proceedings of the 2022 ACM on Asia Conference on Computer and\nCommunications Security. (2022). doi: 10.1145/3488932.3527288\n33. Liu S, Cheng H, Liu H, Zhang H, Li F, Ren T, et al. LLaV A-Plus: le arning to\nuse tools for creating multimodal agents. Comput Vision ECCV . (2024) 2024:126–42.\ndoi: 10.1007/978-3-031-72970-6_8\n34. Yao J, Zhang B, Li C, Hong D, Chanussot J. Extended vision t ransformer (ExViT)\nfor land use and land cover classiﬁcation: a multimodal deep learn ing framework. IEEE\nTrans Geosci Remote Sens . (2023) 61:1–15. doi: 10.1109/TGRS.2023.3284671\n35. Jiang K, Jiang X, Pan J, Wen Y, Huang Y, Weng S, et al. Corrige ndum: Current\nevidence and future perspective of accuracy of artiﬁcial inte lligence application for\nearly gastric cancer diagnosis with endoscopy: a systematic a nd meta-analysis. Front\nMed. (2021) 8:698483. doi: 10.3389/fmed.2021.698483\n36. Chen S, Yu J, Ruan R, Li Y, Tao Y, Shen Q, et al. “Pink Pattern” visualized\nin magnifying endoscopy with narrow-band imaging is a novel f eature of early\ndiﬀerentiated gastric cancer: a bridge between endoscopic i mages and histopathological\nchanges. Front Med. (2021) 8:763675. doi: 10.3389/fmed.2021.763675\n37. Sravani B, Mohan P , Hussein AHA, Kumar GR, Umaeswari P. Mult imodal\nsentimental classiﬁcation using long-short term memory. In: 2023 International\nConference on Integrated Intelligence and Communication Systems (ICIICS). (2023). p.\n1–5. doi: 10.1109/ICIICS59993.2023.10421563\n38. Gupta M, Gallamoza B, Cutrona N, Dhakal P , Poulain R, Beheshti R . An extensive\ndata processing pipeline for MIMIC-IV. In: Machine Learning for Health . PMLR\n(2022). p. 311–325.\n39. Karthikha R, Najumnissa Jamal D, Syed Raﬁammal S. An approa ch\nof polyp segmentation from colonoscopy images using Dilated-U-N et-Seg –\nA deep learning network. Biomed Signal Process Control . (2024) 93:106197.\ndoi: 10.1016/j.bspc.2024.106197\nFrontiers in Medicine /one.tnum/nine.tnum frontiersin.org\nQin et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmed./two.tnum/zero.tnum/two.tnum/five.tnum./one.tnum/five.tnum/eight.tnum/three.tnum/five.tnum/one.tnum/four.tnum\n40. Jha D, Sharma V , Dasu N, Tomar NK, Hicks S, Bhuyan MK, et al. G astroVision:\na multi-class endoscopy image dataset for computer aided gastro intestinal disease\ndetection. In: Machine Learning for Multimodal Healthcare Data . Springer (2023). p.\n125–40. doi: 10.1007/978-3-031-47679-2_10\n41. Subramanian S, Wang LL, Bogin B, Mehta S, van Zuylen M, Para sa S,\net al. MedICaT: a dataset of medical images, captions, and text ual references. In:\nFindings of the Association for Computational Linguistics: EMNL P. (2020). p.2112–20.\ndoi: 10.18653/v1/2020.ﬁndings-emnlp.191\n42. Uslu EE, Sezer E, Guven ZA. NLP-powered insights: a comparati ve analysis\nfor multi-labeling classiﬁcation with MIMIC-CXR dataset. IEEE Access . (2024)\n12:67314–24. doi: 10.1109/ACCESS.2024.3400007\n43. Zheng R, Qian S, Shi Y, Lou C, Xu H, Pan J. Association betwe en\ntriglyceride-glucose index and in-hospital mortality in criti cally ill patients with\nsepsis: analysis of the MIMIC-IV database. Cardiovasc Diabetol . (2023) 22:307.\ndoi: 10.1186/s12933-023-02041-w\n44. Bassi PRAS, Attux R. A deep convolutional neural network f or\nCOVID-19 detection using chest X-rays. Res Biomed Eng . (2021) 38:139–48.\ndoi: 10.1007/s42600-021-00132-9\n45. Sharma BK, Manral C, Deogaonkar A, Sinha KY, Varshney N, C haru. Integration\nof symbolic and implicit knowledge representation for open domain . In: 2024 Second\nInternational Conference on Intelligent Cyber Physical Systems and Inter net of Things\n(ICoICI), (2024). p. 1023–1027. doi: 10.1109/ICoICI62503.2024.106 96090\n46. Zhang B, Zhang P , Dong X, Zang Y, Wang J. Long-CLIP: unlocki ng\nthe long-text capability of CLIP. Comput Vis ECCV . (2024) 2024:310–25.\ndoi: 10.1007/978-3-031-72983-6_18\n47. Ramesh K, Koh YS. Investigation of explainability technique s for\nmultimodal transformers. In: Park LAF, Gomes HM, Doborjeh M, B oo YL,\nKoh YS, Zhao Y, Williams G, Simoﬀ S, editors. Data Mining. Communications\nin Computer and Information Science, Vol. 1741. Singapore: Springer (2022).\ndoi: 10.1007/978-981-19-8746-5_7\n48. Ishikawa S, Sugiura K. Target-Dependent UNITER: a transf ormer-\nbased multimodal language comprehension model for domestic se rvice\nrobots. IEEE Robot Autom Lett . (2021) 6:8401–8. doi: 10.1109/LRA.2021.\n3108500\n49. Wijerathna V , Raveen H, Abeygunawardhana S, Ambegoda TD . Chest\nX-ray caption generation with CheXNet. In: 2022 Moratuwa Engineering\nResearch Conference (MERCon). (2022). p. 1–6. doi: 10.1109/MERCon55799.2022.\n9906263\n50. Urooj Khan A, Garrett J, Bradshaw T, Salkowski L, Jeong J, T ariq\nA, et al. Knowledge-grounded adaptation strategy for vision-la nguage\nmodels: building a unique case-set for screening mammograms fo r\nresidents training. In: Medical Image Computing and Computer Assisted\nIntervention – MICCAI. (2024) 2024:587–98. doi: 10.1007/978-3-031-72390-\n2_55\nFrontiers in Medicine /two.tnum/zero.tnum frontiersin.org",
  "topic": "Endoscopy",
  "concepts": [
    {
      "name": "Endoscopy",
      "score": 0.6208088397979736
    },
    {
      "name": "Medicine",
      "score": 0.4776569604873657
    },
    {
      "name": "Gastroenterology",
      "score": 0.4402283728122711
    },
    {
      "name": "Internal medicine",
      "score": 0.39378997683525085
    },
    {
      "name": "Computer science",
      "score": 0.34557920694351196
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I131434179",
      "name": "Nanjing University of Chinese Medicine",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76569877",
      "name": "Southeast University",
      "country": "CN"
    }
  ]
}