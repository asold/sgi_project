{
  "title": "RACE: Retrieval-augmented Commit Message Generation",
  "url": "https://openalex.org/W4385572856",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3036867347",
      "name": "Ensheng Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106982061",
      "name": "Yan-Lin Wang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A1925330709",
      "name": "Wei Tao",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2513058980",
      "name": "Lun Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103519435",
      "name": "Hong-Yu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105084222",
      "name": "Shi Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097661864",
      "name": "Dongmei Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112369643",
      "name": "Hongbin Sun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2955654168",
    "https://openalex.org/W2081749632",
    "https://openalex.org/W4284667247",
    "https://openalex.org/W4302520418",
    "https://openalex.org/W2097227214",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W4376530446",
    "https://openalex.org/W2057049321",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3036257804",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2964322208",
    "https://openalex.org/W2999553660",
    "https://openalex.org/W4255898812",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2888312537",
    "https://openalex.org/W4226395508",
    "https://openalex.org/W3185176031",
    "https://openalex.org/W3110845443",
    "https://openalex.org/W4231404931",
    "https://openalex.org/W3161120529",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2964772344",
    "https://openalex.org/W3196992070",
    "https://openalex.org/W4306985937",
    "https://openalex.org/W2772902362",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3210968241",
    "https://openalex.org/W2061504941",
    "https://openalex.org/W4232691406",
    "https://openalex.org/W3035657086",
    "https://openalex.org/W3186179984",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2019246026",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W2963958373",
    "https://openalex.org/W2511833051",
    "https://openalex.org/W3199225770"
  ],
  "abstract": "Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5520–5530\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRACE: Retrieval-Augmented Commit Message Generation\nEnsheng Shia Yanlin Wangb,§,† Wei Taoc Lun Dud\nHongyu Zhange Shi Hand Dongmei Zhangd Hongbin Suna,§\naXi’an Jiaotong University bSchool of Software Engineering, Sun Yat-sen University\ncFudan University dMicrosoft Research eThe University of Newcastle\ns1530129650@stu.xjtu.edu.cn, hsun@mail.xjtu.edu.cn\nwangylin36@mail.sysu.edu.cn, wtao18@fudan.edu.cn\n{lun.du, shihan, dongmeiz}@microsoft.com\nhongyu.zhang@newcastle.edu.au\nAbstract\nCommit messages are important for software\ndevelopment and maintenance. Many neu-\nral network-based approaches have been pro-\nposed and shown promising results on auto-\nmatic commit message generation. However,\nthe generated commit messages could be repet-\nitive or redundant. In this paper, we pro-\npose RACE, a new retrieval-augmented neu-\nral commit message generation method, which\ntreats the retrieved similar commit as an ex-\nemplar and leverages it to generate an accu-\nrate commit message. As the retrieved com-\nmit message may not always accurately de-\nscribe the content/intent of the current code diff,\nwe also propose an exemplar guider, which\nlearns the semantic similarity between the re-\ntrieved and current code diff and then guides\nthe generation of commit message based on\nthe similarity. We conduct extensive experi-\nments on a large public dataset with five pro-\ngramming languages. Experimental results\nshow that RACE can outperform all base-\nlines. Furthermore, RACE can boost the per-\nformance of existing Seq2Seq models in com-\nmit message generation. Our data and source\ncode are available at https://github.com/\nDeepSoftwareAnalytics/RACE.\n1 Introduction\nIn software development and maintenance, source\ncode is frequently changed. In practice, code\nchanges are often documented as natural language\ncommit messages, which summarize what (con-\ntent) the code changes are or why (intent) the code\nis changed (Buse and Weimer, 2010; Cortes-Coy\net al., 2014). High-quality commit messages are\nessential to help developers understand the evo-\nlution of software without diving into implemen-\ntation details, which can save a large amount of\n§Yanlin Wang and Hongbin Sun are the corresponding\nauthors.\n†Work done during the author’s employment at Microsoft\nResearch Asia\ntime and effort in software development and main-\ntenance (Dias et al., 2015; Barnett et al., 2015).\nHowever, it is difficult to write high-quality com-\nmit messages due to lack of time, clear motivation,\nor experienced skills. Even for seasoned develop-\ners, it still poses a considerable amount of extra\nworkload to write a concise and informative com-\nmit message for massive code changes (Nie et al.,\n2021). It is also reported that around 14% of com-\nmit messages over 23,000 projects in SourceForge\nare left empty (Dyer et al., 2013). Thus, automat-\nically generating commit messages becomes an\nimportant task.\nOver the years, many approaches have been\nproposed to automatically generate commit mes-\nsages. Early studies (Shen et al., 2016; Cortes-Coy\net al., 2014) are mainly based on predefined rules\nor templates, which may not cover all situations or\ncomprehensively infer the intentions behind code\nchanges. Later, some studies (Liu et al., 2018;\nHuang et al., 2017, 2020) adopt information re-\ntrieval (IR) techniques to reuse commit messages of\nsimilar code changes. They can take advantage of\nsimilar examples, but the reused commit messages\nmight not correctly describe the content/intent of\nthe current code change. Recently, some Seq2Seq-\nbased neural network models (Loyola et al., 2017;\nJiang et al., 2017; Xu et al., 2019; Liu et al., 2019;\nJung, 2021) have been proposed to understand code\ndiffs and generate the high-quality commit mes-\nsages. These approaches show promising perfor-\nmance, but they tend to generate high-frequency\nand repetitive tokens and the generated commit\nmessages have the problem of insufficient infor-\nmation and poor readability (Wang et al., 2021a;\nLiu et al., 2018). Some studies (Liu et al., 2020;\nWang et al., 2021a) also explore the combination\nof neural-based and IR-based techniques. Liu et al.\n(2020) propose an approach to rank the retrieved\ncommit message (obtained by a simple IR-based\nmodel) and the generated commit message (ob-\n5520\ntained by a neural network model). Wang et al.\n(2021a) propose to use the similar code diff as aux-\niliary information in the inference stage, while the\nmodel is not trained to learn how to effectively uti-\nlize the information of retrieval results. Therefore,\nboth of them fail to take advantage of the informa-\ntion of retrieved similar results well.\nIn this paper, we propose a novel model RACE\n(Retrieval-Augmented Commit mEssage genera-\ntion), which retrieves a similar commit message as\nan exemplar, guides the neural model to learn the\ncontent of the code diff and the intent behind the\ncode diff, and generates the readable and informa-\ntive commit message. The key idea of our approach\nis retrieval and augmentation. Specifically, we first\ntrain a code diff encoder to learn the semantics\nof code diffs and encode the code diff into high-\ndimensional semantic space. Then, we retrieve the\nsemantically similar code diff paired with the com-\nmit message on a large parallel corpus based on the\nsimilarity measured by vectors’ distance. Next, we\ntreat the similar commit message as an exemplar\nand leverage it to guide the neural-based models to\ngenerate an accurate commit message. However,\nthe retrieved commit messages may not accurately\ndescribe the content/intent of current code diffs and\nmay even contain wrong or irrelevant information.\nTo avoid the retrieved samples dominating the pro-\ncessing of commit message generation, we propose\nan exemplar guider, which first learns the semantic\nsimilarity between the retrieved and current code\ndiff and then leverages the information of the ex-\nemplar based on the learned similarity to guide the\ncommit message generation.\nTo evaluate the effectiveness of RACE, we\nconduct experiments on a large-scale dataset\nMCMD (Tao et al., 2021) with five programming\nlanguage (Java, C#, C++, Python and JavaScript)\nand compare RACE with 11 state-of-the-art ap-\nproaches. Experimental results show that: (1)\nRACE significantly outperforms existing state-of-\nthe-art approaches in terms of four metrics (BLUE,\nMeteor, Rouge-L and Cider) on the commit mes-\nsage generation. (2) RACE can boost the per-\nformance of existing Seq2Seq models in com-\nmit message generation. For example, it can\nimprove the performance of NMTGen (Loyola\net al., 2017), CommitBERT (Jung, 2021), CodeT5-\nsmall (Wang et al., 2021b) and CodeT5-base (Wang\net al., 2021b) by 43%, 11%, 15%, and 16% on av-\nerage in terms of BLEU, respectively. In addition,\nwe also conduct human evaluation to confirm the\neffectiveness of RACE.\nWe summarize the main contributions of this\npaper as follows:\n• We propose a retrieval-augmented neural com-\nmit message generation model, which treats\nthe retrieved similar commit as an exemplar\nand leverages it to guide neural network model\nto generate informative and readable commit\nmessages.\n• We apply our retrieval-augmented framework\nto four existing neural network-based ap-\nproaches (NMTGen, CommitBERT, CodeT5-\nsmall, and CodeT5-base) and greatly boost\ntheir performance.\n• We perform extensive experiments includ-\ning human evaluation on a large multi-\nprogramming-language dataset and the results\nconfirm the effectiveness of our approach over\nstate-of-the-art approaches.\n2 Related Work\nCode intelligence, which leverages machine learn-\ning especially deep learning-based method to un-\nderstand source code, is an emerging topic and\nhas obtained the promising results in many soft-\nware engineering tasks, such as code summariza-\ntion (Zhang et al., 2020; Shi et al., 2021a, 2022b;\nWang et al., 2020) and code search (Gu et al., 2018;\nDu et al., 2021; Shi et al., 2022a). Among them,\ncommit message generation plays an important role\nin the software evolution.\nIn early work, information retrieval techniques\nare introduced to commit message generation (Liu\net al., 2018; Huang et al., 2017, 2020). For instance,\nChangeDoc (Huang et al., 2020) retrieves the most\nsimilar commits according to the syntax or seman-\ntics in the code diff and reuses commit messages\nof similar code diffs. NNGen (Liu et al., 2018) is a\nsimple yet effective retrieval-based method using\nthe nearest neighbor algorithm. It firstly recalls the\ntop-k similar code diffs in the parallel corpus based\non cosine similarity between bag-of-words vectors\nof code diffs. Then select the most similar result\nbased on BLEU scores between each of them (top-\nk results) and the input code diff. These approaches\ncan reuse similar examples and the reused commit\nmessages are usually readable and understandable.\nRecently, many neural-based approaches (Loy-\nola et al., 2017; Jiang et al., 2017; Xu et al., 2019;\n5521\nLiu et al., 2019, 2020; Jung, 2021; Dong et al.,\n2022; Nie et al., 2021; Wang et al., 2021a) have\nbeen used to learn the semantic of code diffs and\ntranslate them into commit messages. For exam-\nple, NMTGen (Loyola et al., 2017) and Commit-\nGen (Jiang et al., 2017) treat the code diffs as plain\ntexts and adopt the Seq2Seq neural network with\ndifferent attention mechanisms to translate them\ninto commit messages. CoDiSum (Xu et al., 2019)\nextracts both code structure and code semantics\nfrom code diffs and jointly models them with a\nmulti-layer bidirectional GRU to better learn the\nrepresentations of code diffs. PtrGNCMsg (Liu\net al., 2019) incorporates the pointer-generator net-\nwork into the Seq2Seq model to handle out-of-\nvocabulary (OOV) words. CommitBERT lever-\nage CodeBERT (Feng et al., 2020), a pre-trained\nlanguage model for source code, to learn the se-\nmantic representations of code diffs and adopt a\nTransformer-based (Vaswani et al., 2017) decoder\nto generate the commit message. These approaches\nshow promising results on the generation of com-\nmit messages.\nRecently, introducing retrieved relevant results\ninto the training process has been found useful in\nmost generation tasks (Lewis et al., 2020; Yu et al.,\n2021; Wei et al., 2020). Some studies (Liu et al.,\n2020; Wang et al., 2021a) also explore the combi-\nnation of neural-based models and IR-based tech-\nniques to generate commit messages. ATOM (Liu\net al., 2020) ensembles the neural-based model and\nthe IR-based technique through the hybrid rank-\ning. Specifically, it uses BiLSTM to encode ASTs\npaths extracted from ASTs of code diffs and adopt\na decoder to generate commit messages. It also\nuses TF-IDF technique to represent code diffs as\nvectors and retrieves the most similar commit mes-\nsage based on cosine similarity. The generated and\nretrieved commit messages are finally prioritized\nby a hybrid ranking module. CoRec (Wang et al.,\n2021a) is also a hybrid model and only considers\nthe retrieved result during the inference. Specif-\nically, at the training stage, they use an encoder-\ndecoder neural model to encode the input code diffs\nby an encoder and generate commit messages by a\ndecoder. At the inference stage, they first use the\ntrained encoder to retrieve the most similar code\ndiff from the training set. Then they reuse a trained\nencoder-decoder to encode the input and retrieved\ncode diff, combine the probability distributions (ob-\ntained by two decoders) of each word, and generate\nthe final commit message step by step. In summary,\nATOM does not learn to refine the retrieved results\nor the generated results, and CoRec is not trained\nto utilize the information of retrieval results. There-\nfore, both of them fail to take full advantage of\nthe retrieved similar results. In this paper, we treat\nthe retrieved similar commit as an exemplar and\ntrain the model to leverage the exemplar to enhance\ncommit message generation.\n3 Proposed Approach\nThe overview of RACE is shown in Figure 1. It\nincludes two modules: retrieval module and gener-\nation module. Specifically, RACE firstly retrieves\nthe most semantically similar code diff paired with\nthe commit message from the large parallel training\ncorpus. The semantic similarity between two code\ndiffs is measured by the cosine similarity of vectors\nobtained by a code diff encoder. Next, RACE treats\nthe retrieved commit message as an example and\nuses it to guide the neural network to generate an\nunderstandable and concise commit message.\n3.1 Retrieval module\nIn this module, we aim to retrieve the most seman-\ntically similar result. Specifically, we first train\nan encoder-decoder neural network on the large\ncommit message generation dataset. The encoder\nis used to learn the semantics of code diffs and\nencode code diffs into a high-dimension seman-\ntic space. Then we retrieve the most semantically\nsimilar code diff paired with the commit message\nfrom the large parallel training corpus. The seman-\ntic similarity between two code diffs is measured\nby the cosine similarity of vectors obtained by a\nwell-trained code diff encoder.\nRecently, encoder-decoder neural network mod-\nels (Loyola et al., 2017; Jiang et al., 2017; Jung,\n2021), which leverage an encoder to learn the\nsemantic of code diff and employ a decoder to\ngenerate the commit message, have shown their\nsuperiority in the understanding of code diffs\nand commit messages generation. To enable the\ncode diff encoder to understand the semantics\nof code diffs, we train it with a commit mes-\nsage generator on a large commit message gen-\neration dataset, which consists of about 0.9 million\n<code diff , commit message> pairs.\nTo capture long-range dependencies (e.g. a vari-\nable is initialized before the changed line) and\nmore contextual information of code diffs, we em-\n5522\nⅠ: Retrieval Module\n Similar Code Diff\n Similar Commit Message\n  Training Corpus   \nRetriever\nCode Diff \nEncoder\n Input Code Diff\n Input Code Diff CodeDiff\nEncoder\n①\n⑤ Similar Code Diff CodeDiff\nEncoder\n①\n Similar Commit \nMessage\nCommit Message\nEncoder\n① Commit \nMessage\nExemplar\nGuider\nX\nX\n②\n③\n③\n④\n...\n...\n...\n...\n...\n②\nDecoder\nⅡ: Generation Module\nλ\n1-λ\nFigure 1: The architecture of RACE. It includes two modules: retrieval module and generation module. The retrieval\nmodule is used to retrieve the most similar code diff and commit message. The generation module leverages the\nretrieved result to enhance the performance of neural network models.\nploy a Transformer-based encoder to learn the se-\nmantic representations of input code diffs. As\nshown in Figure 1, a Transformer-based encoder is\nstacked with multiple encoder layers. Each layer\nconsists of four parts, namely, a multi-head self-\nattention module, a relative position embedding\nmodule, a feed forward network (FFN) and an add\n& norm module. In b-th attention head, the input\nXb =\n(\nxb\n1,xb\n2,..., xb\nl\n)\n( where Xb = X[(b−1) ∗\nheaddim : b∗headdim], X is the sequence of code\ndiff embedding, headdim is the dimension of each\nhead and lis the input sequence length. ) is trans-\nformed to (Headb = headb\n1,headb\n2,..., headb\nl )\nby:\nheadb\ni =\nl∑\nj=1\nαij\n(\nWVxb\nj + pV\nij\n)\neij = (WQxb\ni )T (\nWKxb\nj + pK\nij\n)\n√dk\n(1)\nwhere αij = exp eij∑n\nk=1 exp eik\n, WQ, WK and WV are\nlearnable matrix for queries, keys and values. dk\nis the dimension of queries and keys; pK\nij and pV\nij\nare relative positional representations for positions\niand j.\nThe outputs of all heads are concatenated and\nthen fed to the FFN modules which is a multi-layer\nperception. The add & norm operation are em-\nployed after the multi-head attention and FFN mod-\nules. The calculations are as follows:\nHead = Concat\n(\nHead1,Headd,HeadB\n)\nHid = add& norm(Head,X)\nEnc = add& norm(FFN (Hid) ,Hid)\n(2)\nwhere add& norm(A1,A2) =LN(A1 + A2),\nB is the number of heads and LN is layer nor-\nmalization. The final output of encoder is sent to\nTransformer-based decoder to generate the commit\nmessage step by step. We use cross-entropy as loss\nfunction and adopt AdamW (Loshchilov and Hut-\nter, 2019) to optimize the parameters of the code\ndiff encoder and the decoder at the top of Figure 1.\nNext, the retrieval module is used to retrieve the\nmost similar result from a large parallel training\ncorpus. We firstly use the above code diff encoder\nto map code diffs into a high-dimensional latent\nspace and retrieve the most similar example based\non cosine similarity.\nSpecifically, after being trained in the commit\nmessage generation dataset, the code diff encoder\ncan capture the semantic of code diff well. We use\nwell-trained code diff encoder following a mean-\npooling operation to map the code diff into a high\ndimensional space. Mathematically, given the in-\nput code diff embedding X = (x1,x2,..., xl),\nthe code diff encoder can transformed them to\nEnc = (enc1,enc2,..., encl). Then we obtain\nthe semantic vector of the code diff by pooling\noperation:\nvec = pooling(Enc) =mean(enc1,enc2,..., encl)\n(3)\nwhere mean is a dimension-wise average operation.\nWe measure the similarity of two code diffs by co-\nsine similarity of their semantic vectors and retrieve\nthe most similar code diff paired with the commit\nmessage from the parallel training corpus. For each\n5523\ncode diff, we return the first-ranked similar result.\nBut, for the code diff in the training dataset, we\nreturn the second-ranked similar result because the\nfirst-ranked result is itself.\n3.2 Generation module\nAs shown at the bottom of Figure 1, in the genera-\ntion module, we treat the retrieved commit message\nas an exemplar and leverage it to guide the neural\nnetwork model to generate an accurate commit\nmessage. Our generation module consists of three\ncomponents: three encoders, an exemplar guider,\nand a decoder.\nFirst, following Equation 1, 2, three Transformer-\nbased encoders are adopted to obtain the rep-\nresentations of the input code diff ( Encd =\nencd\n1,encd\n2,..., encd\nl ), the similar code diff\n(Encs = encs\n1,encs\n2,..., encs\nm), and similar com-\nmit message ( Encm = encm\n1 ,encm\n2 ,..., encm\nn )\n(step 1⃝ in Figure 1), where subscripts l,m,n are\nthe length of the input code diff, the similar code\ndiff, and the similar commit message, respectively.\nSecond, since the retrieved similar commit mes-\nsages may not always accurately describe the con-\ntent/intent of the input code diffs even express to-\ntally wrong or irrelevant semantics. Therefore, we\npropose an exemplar guiderwhich first learns the\nsemantic similarity between the retrieved and in-\nput code diff and then leverages the information of\nthe similar commit messages based on the learned\nsimilarity to guide the commit message generation\n(step 2⃝ ). Mathematically, exemplar guidercalcu-\nlate the semantic similarity (λ) between the input\ncode diff and the similar code diff based on their\nrepresentation Encd\nl and Encs\nm (step 2⃝ and 3⃝):\nλ= σ(Ws[mean(Encd),mean(Encs)]) (4)\nwhere σis the sigmoid activation function, Ws is\na learnable matrix, and meanis a dimension-wise\naverage operation.\nThird, we weight representations of code diff\nand similar commit message by 1 − λand λ, re-\nspectively and then concatenate them to obtain the\nfinal input encoding.\nEncdm = [(1−λ) ∗Encd : λ∗Encs] (5)\nFinally, we use a Transformer-based decoder\nto generate the commit message. The decoder\nconsists of multiply decoder layer and each lay-\ners includes a masked multi-head self-attention, a\nLanguage Training Validation Test\nJava 160,018 19,825 20,159\nC# 149,907 18,688 18,702\nC++ 160,948 20,000 20,141\nPython 206,777 25,912 25,837\nJavaScript 197,529 24,899 24,773\nTable 1: Statistics of the evaluation dataset.\nmulti-head cross-attention module, a FFN mod-\nule and an add & norm module. Different from\nmulti-head self-attention module in the encoder,\nin terms of one token, masked multi-head self-\nattention in the decoder can only attend to the pre-\nvious tokens rather than the before and after con-\ntext. In b-th cross-attention layer, the input encod-\ning ( Encdm =\n(\nencdm\n1 ,encdm\n2 ,..., encdm\nl+m\n)\n) is\nqueried by the output of the preceding commit mes-\nsage representations Msg = (msg1,..., msgt)\nobtained by masked multi-head self-attention mod-\nule.\nDecheadb\ni\n=\nl+m∑\nj=1\nαij\n(\nWDec\nV encb\nj\n)\nDeceij =\n(\nWDec\nQ msgb\nj )T (WDec\nK encb\ni\n)\n√dk\n(6)\nwhere αij =\nexp Deceij∑n\nk=1 exp Deceik\n, WDec\nQ , WDec\nK and\nWDec\nV are trainable projection matrices for queries,\nkeys and values of the decoder layer.t is the length\nof preceding commit message.\nNext, we use Equation 2 to obtain the hidden\nstates of each decoder layer. In the last decoder\nlayers, we employ a MLP and softmax operator to\nobtain the generation probability of each commit\nmessage token on the vocabulary. Then we use\nthe cross-entropy as the loss function and apply\nAdamW for optimization.\n4 Experimental Setup\n4.1 Dataset\nIn our experiment, we use a large-scale dataset\nMCMD (Tao et al., 2021) with five programming\nlanguages (PLs): Java, C#, C++, Python and\nJavaScript. For each PL, MCMD collects commits\nfrom the top-100 starred repositories on GitHub\nand then filters the redundant messages (such as\nrollback commits) and noisy messages defined in\nLiu et al. (2018). Finally, to balance the size of data,\nthey randomly sample and retain 450,000 commits\nfor each PL. Each commit contains the code diff,\nthe commit message, the name of the repository,\n5524\nand the timestamp of commit, etc. To reduce the\nnoise data in the dataset, we further filter out com-\nmits that contain multiple files or files that cannot\nbe parsed (such as .jar, .ddl, .mp3, and .apk).\n4.2 Data pre-processing\nThe code diff in MCMD are based on line-\nlevel code change. To obtain more fine-grained\ncode change, following previous study (Pan-\nthaplackel et al., 2020), we use a sequence\nof span of token-level change actions to rep-\nresent the code diff. Each action is struc-\ntured as <action> span of tokens <action end> .\nThere are four <action> types, namely, <keep>,\n< insert >, <delete> , and <replace> . <keep>\nmeans that the span of tokens are unchanged.\n< insert > means that adding span of tokens.\n<delete> means that deleting span of tokens.\n<replace> means that the span of tokens in the old\nversion that will be replaced with different span\nof tokens in the new version. Thus, we extend\n<replace> to <replace old> and <replace new>\nto indicate the span of old and new tokens, respec-\ntively. We use difflib 1 to extract the sequence of\ncode change actions.\n4.3 Hyperparameters\nWe follow (Tao et al., 2021) to set the maximum\nlengths of code diff and commit message to 200\nand 50, respectively. We use the weight of the\nencoder of CodeT5-base (Wang et al., 2021b) to\ninitialize the code diff encoders and use the de-\ncoder of CodeT5-base to initialize the decoder in\nFigure 1. The original vocabulary sizes of CodeT5\nis 32,100. We add nine special tokens ( <keep>,\n<keep_end>, < insert >, <insert_end> , <delete> ,\n<delete_end>, <replace_old> , <replace_new>,\nand <replace_end>) and the vocabulary sizes of\ncode and queries become 32109. For the optimizer,\nwe use AdamW with the learning rate 2e-5. The\nbatch size is 32. The max epoch is 20. In addi-\ntion, we run the experiments 3 times with random\nseeds 0,1,2 and display the mean value in the paper.\nThe experiments are conducted on a server with 4\nGPUs of NVIDIA Tesla V100 and it takes about\n1.2 hours each epoch.\n4.4 Evaluation metrics\nWe evaluate the quality of the generated mes-\nsages using four metrics: BLEU (Papineni et al.,\n1https://docs.python.org/3/library/difflib.\nhtml\n2002), Meteor (Banerjee and Lavie, 2005), Rouge-\nL (Lin, 2004), and Cider (Vedantam et al., 2015).\nThese metrics are prevalent metrics in machine\ntranslation, text summarization, and image cap-\ntioning. There are many variants of BLEU be-\ning used to measure the generated message, We\nchoose B-Norm (the BLEU result in this paper is\nB-Norm), which correlates with human perception\nthe most (Tao et al., 2021). The detailed metrics\ncalculation can be found in Appendix.\n4.5 Baselines\nWe compare RACE with four end-to-end neural-\nbased models, two IR-based methods, two hybrid\napproaches which combine IR-based techniques\nand end-to-end neural-based methods, and three\npre-trained-based models. Four end-to-end neural-\nbased models include CommitGen (Jiang et al.,\n2017), CoDiSum (Xu et al., 2019), NMTGen (Loy-\nola et al., 2017), PtrGNCMsg (Liu et al., 2019)\nand ATOM (Liu et al., 2020). They all train\nmodels from scratch. Two IR-based methods are\nNNGen (Liu et al., 2018) and Lucene (Apache,\n2011), they retrieve the similar code diff based on\ndifferent similarity measurements and reuse the\ncommit message of the similar code diff as the fi-\nnal result. CoRec and ATOM are all hybrid models\nwhich combine the neural-based models and IR-\nbased techniques. Three pre-trained models are\nCommitBERT, CodeT5-small, and CodeT5-base.\nThey are pre-trained on the large parallel code and\nnatural language corpus and fine- tuned on the com-\nmit message generation dataset. All baselines ex-\ncept Lucene, CodeT5-small and CodeT5-base are\nintroduced in Section 2. Lucene is a traditional IR\nbaseline, which uses TF-IDF to represent a code\ndiff as a vector and searches the similar code diff\nbased on the cosine similarity between two vectors.\nCodeT5-small and CodeT5-base are source code\npre-trained models and have achieved promising\nresults in many code-related tasks (Wang et al.,\n2021b). We fine-tune them on MCMD as strong\nbaselines. In addition, we only evaluate ATOM\non Java dataset as the current implementation of\nATOM only supports Java.\n5 Experimental Results\n5.1 How does RACE perform compared with\nbaseline approaches?\nTo evaluate the effectiveness of RACE, we con-\nduct the experiment by comparing it with the 11\n5525\nModel Java C# C++ Python JavaScript\nBLEU Met. Rou. Cid. BLEU Met. Rou. Cid. BLEU Met. Rou. Cid. BLEU Met. Rou. Cid. BLEU Met. Rou. Cid.\nIR-based NNGen 19.41 12.40 25.15 1.23 22.15 14.77 26.46 1.55 13.61 9.39 18.21 0.73 16.06 10.91 21.69 0.92 18.65 12.50 24.45 1.21\nLucene 15.61 10.56 19.43 0.94 20.68 13.34 23.02 1.36 13.43 8.81 16.78 0.67 15.16 9.63 18.85 0.85 17.66 11.25 21.75 1.02\nEnd-to-end\nCommitGen 14.07 7.52 18.78 0.66 13.38 8.31 17.44 0.63 11.52 6.98 16.75 0.45 11.02 6.43 16.64 0.42 18.67 11.88 24.10 1.08\nCoDiSum 13.97 6.02 16.12 0.39 12.71 5.56 14.40 0.36 12.44 6.00 14.39 0.42 14.61 8.59 17.02 0.42 11.22 5.32 13.26 0.28\nNMTGen 15.52 8.91 21.13 0.86 12.71 8.11 17.16 0.62 11.57 7.06 17.46 0.51 11.41 7.18 18.43 0.48 18.22 12.07 24.43 1.12\nPtrGNCMsg 17.71 11.33 24.32 0.99 15.98 10.18 21.16 0.83 14.06 9.63 20.17 0.63 15.89 11.36 23.49 0.76 20.78 14.52 27.87 1.29\nHybrid ATOM 16.42 11.66 22.67 0.91 / / / / / / / / / / / / / / / /\nCoRec 18.51 11.26 24.78 1.13 18.41 11.70 23.73 1.12 14.02 8.63 20.10 0.72 15.09 9.60 22.35 0.80 21.30 13.84 27.53 1.40\nPre-trained\nCommitBERT 22.32 12.63 28.03 1.42 20.67 12.31 25.76 1.25 16.16 10.05 19.90 0.94 17.29 11.31 22.36 1.01 23.40 15.64 30.51 1.54\nCodeT5-small 22.28 14.16 29.71 1.37 18.92 11.71 24.95 1.05 16.08 11.19 21.60 0.79 17.49 12.46 24.65 0.90 21.97 14.48 28.65 1.42\nCodeT5-base 22.76 14.57 30.23 1.43 22.21 14.51 29.08 1.33 16.73 11.69 22.86 0.85 17.99 12.74 25.27 0.96 22.87 15.12 29.81 1.50\nOurs RACE 25.66 15.46 32.02 1.76 26.33 16.37 31.31 1.84 19.13 12.55 24.52 1.14 21.79 14.68 28.35 1.40 25.55 16.31 31.79 1.84\n↑13% ↑6% ↑6% ↑23% ↑19% ↑13% ↑8% ↑38% ↑14% ↑7% ↑7% ↑34% ↑21% ↑15% ↑12% ↑46% ↑12% ↑8% ↑7% ↑23%\nAblation RACE -Guider 23.37 13.98 30.01 1.53 21.33 13.56 27.33 1.31 17.43 12.10 22.03 0.95 19.44 13.89 26.4 1.01 23.39 15.64 30.51 1.54\nTable 2: Comparison of RACE with baselines under four metrics on five programming languages. Met., Rou., and\nCide. are short for Meteor, Rouge-L, and Cider, respectively. All results are statistically significant (with p< 0.01).\nbaselines including two IR-based approaches, four\nend-to-end neural-based approaches, two hybrid ap-\nproaches, and three pre-train-based approaches in\nterms of four evaluation metrics. The experimental\nresults are shown in Table 2.\nWe can see that IR-based models NNGen and\nLucene generally outperform end-to-end neural\nmodels on average in terms of four metrics. It\nindicates that retrieved similar results can provide\nimportant information for commit message genera-\ntion. CoRec, which combines the IR-based method\nand neural method, performs better than NNGen on\nC++ and JavaScript dataset but lower than NNGen\non Java, C# and Python. This is because CoRec\nonly leverages the information similar code diff\nat the inference stage. ATOM, which priorities\nthe generated result of the neural-based model and\nretrieved result of the IR-based method, also out-\nperforms the IR-based approach Lucene and three\nneural-based models CommitGen, CoDiSum, and\nNMTGen. Three pre-trained-based approaches out-\nperform other baselines in terms of four metrics\non average. CodeT5-base performs best among\nthem on average. Our approach performs the best\namong all approaches on 5 programming languages\nin terms of four metrics. This is because RACE\ntreats the retrieved similar commit message as an\nexemplar and leverages it to guide the neural net-\nwork model to generate an accurate commit mes-\nsage.\nWe also give an example of commit messages\ngenerated by our approach and the baselines in Fig-\nure 2. IR-based methods NNGen and Lucene can\nretrieve semantically similar but not completely\ncorrect commit message. Specifically, retrieved\ncommit messages contain not only the important\nsemantic (“Filter out unavailable databases”) of\nthe current code diff but also the extra informa-\ntion (“Revert”). Neural network models generally\ncapture the action of “add” but fail to further un-\nderstand the intend of the code diff. The hybrid\nmodel CoRec cannot generate the correct commit\nmessage either. Our model treats the retrieved re-\nsult (Revert \"Filter out unavailable databases”) as\nan exemplar, and guides the neural network model\nto generate the correct commit message.\n5.2 What is the effectiveness of exemplar\nguider?\nWe conduct the ablation study to verify the effec-\ntiveness of exemplar guidermodule. Specifically,\nas shown at the bottom of Figure 1, we directly con-\ncatenated the representations of retrieved results\nand fed them to the decoder to generate commit\nmessages without using the exemplar guider. As\nshown at the bottom of the Table 2, we can see\nthat the performance of the ablated model (RACE\n-Guide) degrades in all programming languages in\nterms of four metrics. It demonstrates the effective-\nness of our exemplar guider.\n5.3 What is the performance when we reteieve\nkrelevant commits?\nWe also conduct experiments to recall k (k=1, 3,\n5, 7, 9) most relevant commits to augment the gen-\neration model. Specifically, as shown in Figure 1\nthe relevance of the code diffs is measured by the\ncosine similarity their semantic vectors obtained by\nEquation 3. Then retrieved krelevant commits are\nencoded and fed to the exemplar guidar to obtain\n5526\nCode Diff\napache: superset/views/core.py\n...\n+ class DatabaseFilter(SupersetFilter):\n+    def apply(self, query, func):\n+        if self.has_all_datasource_access():\n+            return query\n+        perms = self.get_view_menus('database_access')\n+       return query.filter(self.model.perm.in_(perms))\n...\n+    base_filters = [['perm', DatabaseFilter, lambda:[]]]\nReference Filter out unavailable databases\nBaselines\nNNGen Revert “ Filter out unavailable databases”\nLucene Revert “ filter out unavailable databases ”\nCommitGen Merge pull request from mistercrunch / UNK\nNMTGen Add <unk> to <unk>\nPtrGNCMsg Add support for dashboards in database\nCoRec Remove <unk>\nCommitBERT Add DatabaseFilter ( )\nCodeT5-small [database] Add databasefilter to filter all users\nCodeT5-base [hotfix] Adding databasefilter to core.py\nRACE Stage I : Revert “ Filter out unavailable databases ”\nStage II :Filter out unavailable databases\nFigure 2: An example of generated commit messages.\nReference is the developer-written commit message.\nThe results of our approach in stage I and II are re-\nturned by the retrieved module and generation module,\nrespectively.\nsemantic similarities by Equation 4, respectively.\nFinally, we weight representations of code diff and\nsimilar commit messages according to the seman-\ntic similarities and feed them to the decoder to\ngenerate commit messages step by step. The exper-\nimental results are shown in Figure 3. We can see\nthat the performance is generally stable on different\nk. In our future work, we will continue to study\nalternatives on leveraging the information of the re-\ntrieved results, e.g., how many commits to retrieve\nand how to model the corresponding information.\n5.4 Can our framework boost the\nperformance of existing models?\nWe further study whether our framework can en-\nhance the performance of the existing Seq2Seq\nneural network model in commit message gen-\neration. Therefore, we adapt our framework to\nfour Seq2Seq-based models, namely NMTGen\n(M1), CommitBERT (M2), CodeT5-small (M3)\nand CodeT5-base (M4). Specifically, we use the\nencoder of these models as our code diff encoder\nand obtain the high-dimensional semantic vectors\nin the retrieval module (Figure 1). In the genera-\ntion module, we use the encoder of their models\nto encode input code diffs, similar code diffs, and\nsimilar commit messages. We also use the decoder\n1 3 5 7 9\nk most relevant results\n0\n10\n20\n30\n40Score\nBLEU Meteor Rough-L Cider\nFigure 3: Performance of models augemented with k\nretrieved relevant commits.\nJava C# C++ Python JavaScript\n     M1 M2 M3 M4 M1 M2 M3 M4 M1 M2 M3 M4 M1 M2 M3 M4 M1 M2 M3 M4\n+ Gain 21.52 24.06 23.91 25.66 22.05 22.34 21.33 26.33          18.69   19.12   19.13   17.58   20.45   20.88   21.79   22.24   25.19   25.71   25.55\nOriginal 15.52 22.32 22.28 22.76 12.71 20.67 18.92 22.21 11.57 16.16 16.08 16.73 11.41 17.29 17.49 17.99 18.22 21.97   22.87\n0\n5\n10\n15\n20\n25\n30BLEU\n39%\n73%\n37%\n54%\n22%\n8%\n8%\n16%\n18%\n8%\n7%\n13%\n19%\n19%\n17%13% 19%\n14%\n21%\n12%\nM1:NMTGen \nM2:CommitBERT\nGain from    ur F  ramework\n23.4\n0\n0\n15.9 \noM3:CodeT 5-small \nM4:CodeT 5-base\nFigure 4: Performance gains on four models. The origi-\nnal performance of the models are in yellow and gains\nfrom our framework are in green. The percentage value\nin each bar is the rate of improvement.\nof their models to generate commit messages.\nThe experimental results are shown in Figure 4,\nwe present the performance of four original models\n(yellow) and gains (green) from our framework on\nfive programming languages in terms of BLEU 2\nscore. Overall, we can see that our framework\ncan improve the performance of all four neural\nmodels in all programming languages. Our frame-\nwork can improve the performance of the original\nmodel from 7% to 73%. Especially, after applying\nour framework, the performance of NMTGen has\nmore than 20% improvement on all programming\nlanguages. In addition, Our framework can boost\nthe performance of NMTGen on BLUE, Meteor,\nRouge-L, and Cider by 43%, 49%, 33%, and 61%\non average, boost CommitBERT by 11%, 9%, 11%,\nand 12%, boost CodeT5-small by 15%, 14%, 11%,\nand 26%, and boost CodeT5-base by 16%, 10%,\n8%, and 32% 3.\n2We show results of other three metrics in Appendix due\nto space limitation. Our conclusions also hold.\n3The result can be found in 1-4 of Appendix\n5527\nModel Informativeness Conciseness Expressiveness\nCommitBERT 1.22 ( ±1.02) 2.03 ( ±1.04) 2.46 ( ±0.99)\nNNGen 1.03 ( ±1.00) 1.74 ( ±1.01) 2.36 ( ±0.95)\nNMTGen 0.74 ( ±0.92) 1.56 ( ±0.93) 2.11 ( ±0.94)\nCoRec 1.05 ( ±1.09) 1.80 ( ±1.05) 2.43 ( ±0.88)\nRACE 2.49 (±1.10) 3.08 (±0.96) 2.85 (±0.84)\nTable 3: Results of human evaluation (standard devia-\ntion in parentheses).\n5.5 Human evaluation\nWe also conduct a human evaluation by following\nthe previous works (Moreno et al., 2013; Panichella\net al., 2016; Shi et al., 2021b) to evaluate the se-\nmantic similarity of the commit message generated\nby RACE and four baselines NNGen, NMTGen,\nCommitBERT, and CoRec. The four baselines\nare IR-based, end-to-end neural network-based,\nhybrid, and pre-trained-based approaches, respec-\ntively. We randomly choose 50 code diff from\nthe testing sets and their commit message gen-\nerated by four approaches. Finally, we sample\n250 <code diff , commit message> pairs to score.\nSpecifically, we invite 4 volunteers with excellent\nEnglish ability and more than three years of soft-\nware development experience. Each volunteer is\nasked to assign scores from 0 to 4 (the higher the\nbetter) to the generated commit message from the\nthree aspects: Informativeness (the amount of im-\nportant information about the code diff reflected\nin the commit message), Conciseness (the extend\nof extraneous information included in the commit\nmessage), and Expressiveness (grammaticality and\nfluency). Each pair is evaluated by four volunteers,\nand the final score is the average of them.\nTo verify the agreement among the volunteers,\nwe calculate the Krippendorff’s alpha (Hayes and\nKrippendorff, 2007) and Kendall rank correlation\ncoefficient (Kendall’s Tau) values (Kendall, 1945).\nThe value of Krippendorff’s alpha is 0.90 and the\nvalues of pairwise Kendall’s Tau range from 0.73 to\n0.95, which indicates that there is a high degree of\nagreement between the 4 volunteers and that scores\nare reliable. Table 3 shows the result of human\nevaluation. RACE is better than other approaches\nin Informative, Conciseness, and Expressiveness,\nwhich means that our approach tends to generate\nconcise and readable commit messages with more\ncomprehensive semantics. In addition, we confirm\nthe superiority of our approach using Wilcoxon\nsigned-rank tests (Wilcoxon et al., 1970) for the\nhuman evaluation. Results 4 show that the improve-\nment of RACE over other approaches is statistically\nsignificant with all p-values smaller than 0.05 at\n95% confidence level.\n6 Conclusion\nThis paper proposes a new retrieval-augmented\nneural commit message generation method, which\ntreats the retrieved similar commit message as\nan exemplar and uses it to guide the neural net-\nwork model to generate an accurate and read-\nable commit message. Extensive experimental re-\nsults demonstrate that our approach outperforms\nrecent baselines and our framework can signifi-\ncantly boost the performance of four neural net-\nwork models. Our data, source code and Ap-\npendix are available at https://github.com/\nDeepSoftwareAnalytics/RACE.\nLimitations\nWe have identified the following main limitations:\nProgramming Languages.We only conduct ex-\nperiments on five programming languages. Al-\nthough in principle, our framework is not specifi-\ncally designed for certain languages, models per-\nform differently in different programming lan-\nguages. Therefore, more experiments are needed\nto confirm the generality of our framework. In the\nfuture, we will extend our study to other program-\nming languages.\nCode base. Compared with purely neural\nnetwork-based models, our method needs a code\nbase to retrieve the most similar example from\nthat. This limitation is inherited from IR-based\ntechniques.\nTraining Time.In addition to modeling the in-\nformation of input code diffs, our model needs to\nretrieve similar diffs and encode them. Thus, our\nmodel takes a long time to train (about 35 hours to\ntrain the model).\nLong Code Diffs. Longer code diffs may contain\nmore complex semantics or behaviors. Long diffs\n(over 512 tokens) are truncated in our approach\nand some information would be lost. In our future\nwork, we will design mechanisms to better handle\nlong diffs.\nAcknowledgement\nWe thank reviewers for their valuable comments\non this work. This research was supported\n4Available in Appendix\n5528\nby National Key R&D Program of China (No.\n2017YFA0700800). We would like to thank Jiaqi\nGuo and Wenchao Gu for their valuable sugges-\ntions and feedback during the work discussion pro-\ncess. We also thank the participants of our human\nevaluation for their time.\nReferences\nApache. 2011. Apache lucene.\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments. In IEEvalua-\ntion@ACL.\nMike Barnett, Christian Bird, João Brunet, and Shu-\nvendu K. Lahiri. 2015. Helping developers help\nthemselves: Automatic decomposition of code re-\nview changesets. In ICSE (1), pages 134–144. IEEE\nComputer Society.\nRaymond P. L. Buse and Westley Weimer. 2010. Au-\ntomatically documenting program changes. In ASE,\npages 33–42. ACM.\nLuis Fernando Cortes-Coy, Mario Linares Vásquez,\nJairo Aponte, and Denys Poshyvanyk. 2014. On\nautomatically generating commit messages via sum-\nmarization of source code changes. In SCAM, pages\n275–284. IEEE Computer Society.\nMartin Dias, Alberto Bacchelli, Georgios Gousios,\nDamien Cassou, and Stéphane Ducasse. 2015. Un-\ntangling fine-grained code changes. In SANER, pages\n341–350. IEEE Computer Society.\nJinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin\nLi, Wenjie Zhang, and Dan Hao. 2022. Fira: Fine-\ngrained graph-based code change representation for\nautomated commit message generation.\nLun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi\nHan, and Dongmei Zhang. 2021. Is a single model\nenough? mucos: A multi-model ensemble learning\napproach for semantic code search. In Proceedings of\nthe 30th ACM International Conference on Informa-\ntion & Knowledge Management, pages 2994–2998.\nRobert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and\nTien N. Nguyen. 2013. Boa: a language and in-\nfrastructure for analyzing ultra-large-scale software\nrepositories. In ICSE, pages 422–431. IEEE Com-\nputer Society.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan,\nXiaocheng Feng, Ming Gong, Linjun Shou, Bing\nQin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020.\nCodebert: A pre-trained model for programming and\nnatural languages. In EMNLP (Findings), volume\nEMNLP 2020 of Findings of ACL, pages 1536–1547.\nAssociation for Computational Linguistics.\nXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018.\nDeep code search. In ICSE, pages 933–944. ACM.\nAndrew F Hayes and Klaus Krippendorff. 2007. An-\nswering the call for a standard reliability measure for\ncoding data. Communication methods and measures,\n1(1):77–89.\nYuan Huang, Nan Jia, Hao-Jie Zhou, Xiangping Chen,\nZibin Zheng, and Mingdong Tang. 2020. Learning\nhuman-written commit messages to document code\nchanges. J. Comput. Sci. Technol., 35(6):1258–1277.\nYuan Huang, Qiaoyang Zheng, Xiangping Chen,\nYingfei Xiong, Zhiyong Liu, and Xiaonan Luo. 2017.\nMining version control system for automatically gen-\nerating commit comment. In ESEM, pages 414–423.\nIEEE Computer Society.\nSiyuan Jiang, Ameer Armaly, and Collin McMillan.\n2017. Automatically generating commit messages\nfrom diffs using neural machine translation. In ASE.\nTae Hwan Jung. 2021. Commitbert: Commit mes-\nsage generation using pre-trained programming lan-\nguage model. In Proceedings of the 1st Workshop\non Natural Language Processing for Programming\n(NLP4Prog 2021), pages 26–33.\nMaurice G Kendall. 1945. The treatment of ties in\nranking problems. Biometrika, 33(3):239–251.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In NeurIPS.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out.\nQin Liu, Zihe Liu, Hongming Zhu, Hongfei Fan, Bowen\nDu, and Yu Qian. 2019. Generating commit mes-\nsages from diffs using pointer-generator network. In\nMSR, pages 299–309. IEEE / ACM.\nShangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie,\nand Yang Liu. 2020. ATOM: commit message gener-\nation based on abstract syntax tree and hybrid ranking.\nTSE, PP:1–1.\nZhongxin Liu, Xin Xia, Ahmed E. Hassan, David Lo,\nZhenchang Xing, and Xinyu Wang. 2018. Neural-\nmachine-translation-based commit message genera-\ntion: how far are we? In ASE, pages 373–384. ACM.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In ICLR.\nPablo Loyola, Edison Marrese-Taylor, and Yutaka Mat-\nsuo. 2017. A neural architecture for generating natu-\nral language descriptions from source code changes.\nIn ACL (2), pages 287–292. Association for Compu-\ntational Linguistics.\n5529\nLaura Moreno, Jairo Aponte, Giriprasad Sridhara, An-\ndrian Marcus, Lori L. Pollock, and K. Vijay-Shanker.\n2013. Automatic generation of natural language sum-\nmaries for java classes. In ICPC, pages 23–32. IEEE\nComputer Society.\nLun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam,\nYang Liu, and Zenglin Xu. 2021. Coregen: Con-\ntextualized code representation learning for commit\nmessage generation. Neurocomputing, 459:97–107.\nSebastiano Panichella, Annibale Panichella, Moritz\nBeller, Andy Zaidman, and Harald C. Gall. 2016.\nThe impact of test case summaries on bug fixing per-\nformance: an empirical investigation. In ICSE, pages\n547–558. ACM.\nSheena Panthaplackel, Pengyu Nie, Milos Gligoric,\nJunyi Jessy Li, and Raymond J. Mooney. 2020.\nLearning to update natural language comments based\non code changes. In ACL, pages 1853–1868. Associ-\nation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nJinfeng Shen, Xiaobing Sun, Bin Li, Hui Yang, and\nJiajun Hu. 2016. On automatic summarization of\nwhat and why information in source code changes. In\nCOMPSAC, pages 103–112. IEEE Computer Society.\nEnsheng Shi, Wenchao Gub, Yanlin Wang, Lun Du,\nHongyu Zhang, Shi Han, Dongmei Zhang, and Hong-\nbin Sun. 2022a. Enhancing semantic code search\nwith multimodal contrastive learning and soft data\naugmentation. arXiv preprint arXiv:2204.03293.\nEnsheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi\nHan, Hongyu Zhang, Dongmei Zhang, and Hong-\nbin Sun. 2022b. On the evaluation of neural code\nsummarization. In ICSE.\nEnsheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,\nShi Han, Dongmei Zhang, and Hongbin Sun. 2021a.\nCast: Enhancing code summarization with hierar-\nchical splitting and reconstruction of abstract syntax\ntrees. In EMNLP.\nEnsheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,\nShi Han, Dongmei Zhang, and Hongbin Sun. 2021b.\nCAST: enhancing code summarization with hierar-\nchical splitting and reconstruction of abstract syntax\ntrees. In EMNLP (1), pages 4053–4062. Association\nfor Computational Linguistics.\nWei Tao, Yanlin Wang, Ensheng Shi, Lun Du, Shi\nHan, Hongyu Zhang, Dongmei Zhang, and Wen-\nqiang Zhang. 2021. On the evaluation of commit\nmessage generation models: An experimental study.\nIn ICSME.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR.\nHaoye Wang, Xin Xia, David Lo, Qiang He, Xinyu\nWang, and John Grundy. 2021a. Context-aware\nretrieval-based deep commit message generation.\nACM Trans. Softw. Eng. Methodol., 30(4):56:1–\n56:30.\nYanlin Wang, Lun Du, Ensheng Shi, Yuxuan Hu, Shi\nHan, and Dongmei Zhang. 2020. Cocogum: Contex-\ntual code summarization with multi-relational gnn on\numls. Technical report, Microsoft, MSR-TR-2020-\n16. [Online].\nYue Wang, Weishi Wang, Shafiq R. Joty, and Steven\nC. H. Hoi. 2021b. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code under-\nstanding and generation. In EMNLP (1), pages 8696–\n8708. Association for Computational Linguistics.\nBolin Wei, Yongmin Li, Ge Li, Xin Xia, and Zhi Jin.\n2020. Retrieve and refine: exemplar-based neural\ncomment generation. In 2020 35th IEEE/ACM In-\nternational Conference on Automated Software Engi-\nneering (ASE), pages 349–360. IEEE.\nFrank Wilcoxon, SK Katti, and Roberta A Wilcox. 1970.\nCritical values and probability levels for the wilcoxon\nrank sum test and the wilcoxon signed rank test. Se-\nlected tables in mathematical statistics, 1:171–259.\nShengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hang-\nhang Tong, and Jian Lu. 2019. Commit message\ngeneration for source code changes. In IJCAI, pages\n3975–3981. ijcai.org.\nHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021.\nImproving query representations for dense retrieval\nwith pseudo relevance feedback. In CIKM, pages\n3592–3596. ACM.\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and\nXudong Liu. 2020. Retrieval-based neural source\ncode summarization. In ICSE.\n5530",
  "topic": "Commit",
  "concepts": [
    {
      "name": "Commit",
      "score": 0.988184928894043
    },
    {
      "name": "Computer science",
      "score": 0.8216444253921509
    },
    {
      "name": "Code (set theory)",
      "score": 0.4960804879665375
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4746423661708832
    },
    {
      "name": "Message passing",
      "score": 0.45241376757621765
    },
    {
      "name": "Programming language",
      "score": 0.30058735609054565
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2335416078567505
    },
    {
      "name": "Database",
      "score": 0.11675259470939636
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.07502907514572144
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 31
}