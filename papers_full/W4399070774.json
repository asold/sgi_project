{
  "title": "Learning robust correlation with foundation model for weakly-supervised few-shot segmentation",
  "url": "https://openalex.org/W4399070774",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2268433610",
      "name": "Huang Xinyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355735921",
      "name": "Zhu, Chuang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229310240",
      "name": "Liu Kebin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2371152776",
      "name": "Ren Ruiying",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2225032127",
      "name": "Liu Sheng-jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2115733720",
    "https://openalex.org/W3034942609",
    "https://openalex.org/W6735236233",
    "https://openalex.org/W3036534997",
    "https://openalex.org/W3103819522",
    "https://openalex.org/W4383646091",
    "https://openalex.org/W2990230185",
    "https://openalex.org/W2963599420",
    "https://openalex.org/W3034985049",
    "https://openalex.org/W3047258141",
    "https://openalex.org/W4214573368",
    "https://openalex.org/W4386075956",
    "https://openalex.org/W4385617508",
    "https://openalex.org/W4386076165",
    "https://openalex.org/W6736057607",
    "https://openalex.org/W4205556549",
    "https://openalex.org/W3209453546",
    "https://openalex.org/W6800856238",
    "https://openalex.org/W6848342932",
    "https://openalex.org/W3208004344",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4312443348",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W4313153210",
    "https://openalex.org/W2994528761",
    "https://openalex.org/W4214660208",
    "https://openalex.org/W3176065502",
    "https://openalex.org/W6848056308",
    "https://openalex.org/W2997490332",
    "https://openalex.org/W6810606939",
    "https://openalex.org/W6717697761",
    "https://openalex.org/W1967005434",
    "https://openalex.org/W6846377438",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4250482878",
    "https://openalex.org/W6779089016",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4386066138",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W4386075882",
    "https://openalex.org/W6857661428",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2148534289",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W6840013202",
    "https://openalex.org/W2031489346",
    "https://openalex.org/W6630336748",
    "https://openalex.org/W2981787211",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2963603913",
    "https://openalex.org/W2441255125",
    "https://openalex.org/W2963078159",
    "https://openalex.org/W4306309433",
    "https://openalex.org/W2794363191",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3197177058",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4312693171",
    "https://openalex.org/W4387323307",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4285604202",
    "https://openalex.org/W3033188311",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W4321319299"
  ],
  "abstract": null,
  "full_text": "Learning Robust Correlation with Foundation Model for\nWeakly-Supervised Few-Shot Segmentation\nXinyang Huanga, Chuang Zhua,∗, Kebin Liua, Ruiying Rena, Shengjie Liua\naSchool of Artificial Intelligence, Beijing University of Posts and Telecommunications,\nBeijing, China\nAbstract\nExisting few-shot segmentation (FSS) only considers learning support-query\ncorrelation and segmenting unseen categories under the precise pixel masks.\nHowever, the cost of a large number of pixel masks during training is expen-\nsive. This paper considers a more challenging scenario, weakly-supervised\nfew-shot segmentation (WS-FSS), which only provides category ( i.e. image-\nlevel) labels. It requires the model to learn robust support-query information\nwhen the generated mask is inaccurate. In this work, we design a Cor-\nrelation Enhancement Network (CORENet) with foundation model, which\nutilizes multi-information guidance to learn robust correlation. Specifically,\ncorrelation-guided transformer (CGT) utilizes self-supervised ViT tokens to\nlearn robust correlation from both local and global perspectives. From the\nperspective of semantic categories, the class-guided module (CGM) guides\nthe model to locate valuable correlations through the pre-trained CLIP.\nFinally, the embedding-guided module (EGM) implicitly guides the model\nto supplement the inevitable information loss during the correlation learn-\ning by the original appearance embedding and finally generates the query\nmask. Extensive experiments on PASCAL-5 i and COCO-20 i have shown\nthat CORENet exhibits excellent performance compared to existing meth-\nods. Our code will be available soon after acceptance.\n∗Corresponding author\nEmail: hsinyanghuang7@gmail.com; czhu@bupt.edu.cn\narXiv:2405.19638v1  [cs.CV]  30 May 2024\n(a) FSS\n (b) WS-FCS\n (c) WS-FSS\nFig. 1. Comparison between (a) few-shot segmentation (FSS) task [7], (b) weakly-\nsupervised few-shot classification and segmentation (WS-FCS) task [15], and (c) our\nweakly-supervised few-shot segmentation (WS-FSS) task settings. (a) The FSS task re-\nquires many support-query masks during training. (b) The classification and segmentation\ntasks are decoupled in the WS-FCS task. It provides supervisory information on whether\nimages belong to the same category without providing specific category assistance for\nsegmentation. (c) The WS-FSS task assists the model in segmentation through specific\ncategories of supervised information in the presence of noise in the mask generated by the\nmodel.\n1. Introduction\nFew-shot learning [1, 2, 3, 4, 5, 6] is a machine learning method that\nuses very little labeled data to help the model quickly adapt to new tasks\nor categories. It is crucial in applications where data collection is costly or\nrequires intensive annotation, such as image segmentation. Consequently,\nfew-shot segmentation (FSS) has been proposed and extensively studied [7,\n8, 9, 10, 11, 12, 13, 14].\nExisting FSS methods are typically trained based on the meta-learning\nparadigm [16, 17, 18, 19, 20]. They often assume the presence of a large\namount of accurately annotated data for model training and learn the support-\nquery correlation by abundant support and query masks, as shown in Fig. 1a.\nLikewise, during testing, several ground-truth (GT) support masks are re-\nquired during the reference of the model. However, the cost of obtaining the\n2\nsegmentation masks required for these models is very expensive and cumber-\nsome.\nAlthough related works [21, 22] have explored the setting of few-shot\nsegmentation in weakly-supervised scenarios, they are unable to generate su-\npervised masks for unseen categories during the testing phase or require ad-\nditional training in a mask generation module. CST [15] solved this problem\nand proposed the weakly-supervised few-shot classification and segmentation\n(WS-FCS), as shown in Fig. 1b. However, there are two issues when directly\napplying it to WS-FSS tasks: firstly, it simply considers the correlation in-\nformation of the support-query pair in the presence of the GT mask, which\nmay introduce a lot of matching noise in the case of inaccurate masks; sec-\nondly, the provided category information is whether the two images belong\nto the same category, ignoring the benefits of semantic level information for\nsegmentation. [23] is closest to the problem setting of this paper, but it ig-\nnores the exploration of robust correlation and the contribution and role of\nthe foundation model in the WS-FSS task. Similar to [23], this paper focuses\non a weakly-supervised few-shot segmentation (WS-FSS) scenario where the\nmodel should learn robust support-query matching information and perform\nsegmentation on query images with only image-level category informa-\ntion and no access to GT masks , as depicted in Fig. 1c.\nTo solve the WS-FSS, this paper introduces a Correlation Enhance-\nment Network (CORENet) with foundation model assistance that helps\nthe model learn robust correlation from multiple perspectives, even in the\npresence of inaccurate masks generated by the model. Specifically, we first\ndesign a Correlation-Guided Transformer (CGT), which takes high-quality\ntokens obtained from a self-supervised Vision Transformer (ViT) [24] as in-\nput. It fuses information from local and global perspectives to guide the\nmodel in better utilizing correlation information. Therefore, CGT can be\nrelatively robust in the face of generated imprecise masks. Furthermore, a\nwell-designed self-distillation loss helps CGT generate higher-quality correla-\ntion maps in the early stages. However, when the model generates inaccurate\nmasks, the effect of segmenting the query from the perspective of correlation\nis limited. To address the above issues, the Class-Guided Module (CGM)\nhelps the model to roughly locate specific objects from inaccurate masks\nusing prior knowledge by using the provided class information. Although\nexisting works [25, 15] utilize category supervision information by classify-\ning support and query during FSS. They provide category information to\nsupport and query whether the images belong to the same category (0/1 la-\n3\nbel) without providing specific category semantic information assistance for\nsegmentation. By using pre-trained CLIP [26] to generate coarse attention,\nCGM utilizes existing correlation features to filter out background features\nunrelated to the query foreground, helping the model roughly locate valu-\nable correlation information. Finally, to further reduce potential information\nloss during correlation processing and implicitly guide the model in refining\nmatching information, we propose an Embedding-Guided Module (EGM).\nEGM uses efficient tokens generated by ViT to supplement the information\nof the original embeddings, resulting in the final masks.\nTo generate supervised masks, inspired by previous works [24, 15], the pa-\nper utilizes attention maps generated by pre-trained self-supervised ViT to\ncreate pseudo-masks. Furthermore, we leverage pixel relationships within the\nimage to generate more accurate pseudo-masks through the Pixel-Adaptive\nRefinement (PAR) module [27], which helps the model learn robust correla-\ntions from the perspective of mask enhancement. Even when encountering\nunseen categories during testing, the model can provide relatively accurate\npseudo-masks. Our main contributions are summarized as follows:\n• We propose a Correlation Enhancement Network (CORENet) with\nfoundation model assistance to guide models from multiple perspec-\ntives to learn robust correlation in WS-FSS.\n• We propose a Correlation-Guided Transformer (CGT) that learns to\nsupport-query knowledge from a knowledge aggregation perspective\nand apply the Pixel Adaptive Refinement (PAR) module in a few-shot\nscenario for the first time.\n• We propose a Class-Guided Module (CGM) and an Embedding-Guided\nModule (EGM) to mine and supplement target information in correla-\ntion features from category semantics and appearance embedding per-\nspective.\n• Our CORENet achieved state-of-the-art results compared to the latest\nFSS and WS-FSS methods in two WS-FSS scenarios ( i.e. PASCAL-5i\nand COCO-20i).\nThe remainder of this paper is as follows: Section 2 reviews recent work\nrelated to WS-FSS. Sections 3 and 4 elaborate on the entire process of our\n4\nproposed CORENet. Then, comprehensive quantitative and qualitative re-\nsults are reported in Section 5, followed by a series of ablation studies. Fi-\nnally, Section 6 gives the conclusion of this work.\n2. Related Work\nFew-Shot Semantic Segmentation. Few-shot semantic segmenta-\ntion (FSS) aims to segment new semantic objects in images, with only a\nfew densely labeled examples available. The current methods mainly focus\non the improvement of the meta-learning stage. They can be classified as\nprototype-based methods and relational-based methods. The intuition be-\nhind the prototype-based methods [28, 8, 9, 29, 30, 31, 14] is to extract repre-\nsentative foreground or background prototypes from the supporting samples\nusing the method, and then use different strategies to interact between differ-\nent prototypes or between prototypes and query features. Relational-based\nmethods [10, 11, 12, 25, 15, 13] have also achieved great success in the few-\nshot semantic segmentation. However, these methods only focus on learning\nto support and query matching information between images under precise su-\npervision. This paper considers a more challenging weak supervision version\nof FSS, which completes the segmentation of query images without providing\nany mask information, only providing support images and category informa-\ntion.\nWeakly-Supervised Few-Shot Segmentation. Due to the severe\nchallenge of data scarcity, many works currently study few-shot segmentation\nin a weakly-supervised environment. However, the definition of weakly su-\npervised few-shot segmentation (WS-FSS) in existing methods is still flawed\nand inconsistent. WS Co-FCN [32] generated a pseudo-mask to support the\nimage by retaining pixels not classified as background. However, it cannot\nhandle supporting images that contain multiple new classes. Some methods\n[21, 33] use supervision information such as bounding boxes. WRCAM [22]\nrequires pre-training of a mask generation module for all image categories\nin advance, including test image categories that have not been seen during\nthe training phase, which does not follow the training paradigm of few-shot\nlearning during the training stage. The problem setting of CST [15] is sim-\nilar to that of this paper. However, the provided category information is\nwhether the two images belong to the same category and does not provide\nspecific category assistance for segmentation. [23] is closest to the problem\nsetting of this paper, but this paper focuses on exploring the contribution\n5\nand role of the foundation model in the WS-FSS task. This paper focuses\non the few-shot segmentation in a weakly-supervised scenario, where no GT\nmask information is provided at any stage. It provides category information\nassistance to complete the segmentation of the query image.\n3. Problem Definition\nSimilar to the few-shot segmentation [7, 8, 9, 10, 11, 12, 13], in order\nto avoid overfitting risks caused by insufficient training data, we adopted a\nwidely used meta-learning method called episodic training [34]. In weakly-\nsupervised few-shot segmentation, we define two datasets, Dtrain and Dtest,\nwith category sets Ctrain and Ctest respectively, where Ctrain ∩ Ctest = ∅.\nThe model trained on Dtrain is directly transferred to Dtest for evaluation\nand testing. We train the model in an episode manner [34]. Under the\nweak-supervised setting, each episode only comprises support set S = {Is},\nquery set Q = {Iq}, and their corresponding category c. Unlike few-shot\nsegmentation, we do not provide mask information at any stage. Under the\nK-shot setting, it includes the support set S = {Ii\ns}K\ni=1, query set Q = {Iq}\nand the corresponding category c. Training set Dtrain and test set Dtest\nmeans Dtrain = {Ii\ns, Ii\nq, c}Ntrain\ni=1 and Dtest = {Ii\ns, Ii\nq, c}Ntest\ni=1 , where Ntrain and\nNtest is a series of quantitative training and testing. During training, the\nmodel iteratively samples an episode from Dtrain to generate a pseudo-mask\nusing limited information and to learn segmentation knowledge through the\ngenerated pseudo-masks. During the testing, the model changed from Dtest\nrandomly samples {Ii\ns, Ii\nq, c} to predict the query mask.\n4. Methodology\n4.1. Overview\nAs shown in Fig. 2, the Correlation Enhancement Network (CORENet)\nis composed of three key modules, namely, correlation-guided transformer\n(CGT), class-guided module (CGM), and embedded-guided module (EGM).\nPrecisely, we extract high-quality features through pretrained DINO ViT [24]\nand calculate the correlation between the token of the supporting image pair\nand the query image pair. Then, the robust cross-correlation information is\nlearned from the local and global perspectives through CGT. With the assis-\ntance of CLIP [26], CGM uses category information to guide the generation\nof a coarse attention map and filters out the irrelevant information in the\n6\nFig. 2. The overall architecture of our Correlation Enhancement Network (CORENet).\nFirstly, the Correlation-Guided Transformer (CGT)is introduced to generate robust\ncorrelation features using the local and global similarity calculations of ViT tokens. Then,\nwith the assistance of CLIP, theClass-Guided Module (CGM)transforms the category\ninformation into coarse attention and further refines them to filter irrelevant information in\nthe relevant features. Meanwhile, the Embedding-Guided Module (EGM)combines\nthe support query appearance of each layer with the enhanced correlation features, further\nreducing the potential information loss of the model in correlation-enhanced learning under\nweakly-supervised settings and obtaining the final query mask.\nquery features through the generated cross-correlation features. To reduce\nthe potential information loss of the model in correlation reinforcement, we\npropose EGM, which further aggregates the matching information by us-\ning the embedded information obtained from the feature graph to guide the\nmodel to learn the matching information implicitly. Then, the model sends\nthe learned robust features into the segmentation header to predict the fi-\nnal segmentation mask ˜Mq of the query image. Next, each module will be\ndescribed in detail in the following paragraphs.\n4.2. Correlation-Guided Transformer\nThe correlation between support and query plays a crucial role in FSS.\nThe existing methods [12, 25, 15] help the model segment the query image on\nthe existing support foreground information by using the similarity between\nthe support and query image pixels. However, due to the lack of a GT mask,\nit is not comprehensive to only consider the correlation information of this\nlocal-to-local matching. In this paper, the correlation-guided transformer\n(CGT) is proposed. From the perspective of local-to-local and local-to-global,\nCGT uses the features extracted by self-supervised pretrained ViT to learn\nthe multi-view robust correlation information between support images and\nquery images.\n7\nFig. 3. Illustration of Multi-kernel information fusion in CGT.\nLocal-to-local correlation. Specifically, CGT uses DINO [24] as the\nbackbone of pretrained frozen ViT. It gets K-layers patch tokens fq, fs and\nclass tokens fq,cls, fs,cls by inputting support images and query images and\nthrough multi-head attention. Then, we calculate the local-to-local ( i.e.\npixel-to-pixel) correlation between the query and support patch tokens in\neach layer and preserve the semantic diversity of the M heads of the ViT,\ni.e., we calculate the M ×K cosine similarities of the query to support tokens\nand concatenate them along the new dimension:\nClocal = (fq)T fs\n||fq||||fs|| ∈ RMK ×hqwq×hsws , (1)\nwhere hsws and hqwq represent the product of length and width of supports\nand query images, || · ||means l2 regularization.\nLocal-to-global correlation. From the global view, we use the support\nmask to cut out the foreground and background regions from fs. Unlike the\nforeground area, which is cut off as a whole area, the background area is di-\nvided into N local areas because the background may not be uniform. To this\nend, we use the Voronoi-based method [35, 36] to divide the background into\nN different regions. Then, the global features of foreground and background\n8\nare obtained by mask average pooling:\npf = 1\n|Ms|\nhswsX\ni=1\nfs,iMs,i,\npb,n = 1\n|Bn\ns |\nhswsX\ni=1\nfs,iBn\ns,i,\n(2)\nwhere Ms is the pseudo-mask for support images, its generation will be in-\ntroduced in Section 4.5. Bn\ns = 1 − Ms is the n-th background mask for the\nsupport mask. Similar to Eq. 1, the local-to-global correlation between the\nquery and support token is calculated as follows:\nCf\nglobal = (fq)T pf\n||fq||||pf || ∈ RMK ×hqwq×1,\nCb\nglobal,n = (fq)T pb,n\n||fq||||pb,n|| ∈ RMK ×hqwq×N ,\n(3)\nwhere N = 5 is the number of the background. We further concatenate\nthe features to obtain the correlation token C0\ni ∈ R(1+N+hsws)×ML , where\ni ∈ [1, ··· , hqwq] is an index over the query token and L is the number of\nthe transformer layers. The correlation token refers to the token obtained\nafter feeding the correlation map into the transformer. Then following CST\n[15], it takes C0\ni and support mask Ms as input and returns three types of\ntoken: foreground, background, and local correlation token through a two-\nlayer transformer [37]. Each transformer layer can be described as follows:\nCl′\ni = LNl(MHSAl(Cl\ni, Ms,i) + Cl\ni),\nCl+1\ni = LNl(MLPl(Cl′\nl ) + Cl′\ni ) ∈ RCl×hqwq×hlwl , (4)\nwhere l means the transformer layer index, Cl means its dimension, and\nLNl, MHSAl, MLPl correspond to a multi-head self-attention (MHSA) [37],\na group normalization [38], and a linear layer, respectively. Similar to related\nworks [39, 15], in each MHSA layer, the generated query is embedded into a\nspatial pool, and the output size changes from hsws to 1. Then, we split the\ntensor C into foreground, background, and local correlation token along the\nsecond dimension, i.e. Cf\nglobal, Cb\nglobal,n, Clocal.\nFore-background fusion. After obtaining the global correlation tokens,\nwe propose an adaptive fusion method for different global foreground and\n9\nbackground features. For different backgrounds, we select them with adaptive\nweighting, which consists of a simple linear layer.\nCb\nglobal = w1Cb\nglobal,1 + ... + wnCb\nglobal,n + β, (5)\nwhere wn means the n-th weight of the linear layer, and β means the bias.\nThen, we fuse the merged background and foreground correlation features\nby a convolutional layer:\nCglobal = Conv1(Cat(Cf\nglobal, Cb\nglobal)), (6)\nwhere Cat( ·, ·) is the concatenation operation. This method can help the\nmodel integrate necessary support knowledge from different backgrounds and\nforegrounds.\nMulti-kernel information fusion. We use the multi-kernel informa-\ntion fusion mechanism after obtaining the local and global features. Multi-\nkernel information fusion uses different receptive field convolution kernels\nto fuse the local and global correlation information, reducing the noise of\ndifferent matching information due to the lack of GT masks. We process\nthe features by concatenating two parts of the features, utilizing convolu-\ntional kernels of different receptive fields, and helping the model learn robust\nknowledge:\nCi = Convi(Cat(Clocal, Cglobal)), (7)\nwhere Conv i means the i × i convolutional operation and i ∈ {1, 3, 5, 7}.\nThen, we will integrate the obtained feature knowledge of different receptive\nfields:\nC′ = Conv1(Cat(C1, C3, C5, C7)) + Clocal. (8)\nFinally, the final correlation token is obtained through the residual con-\nnection layer [40]:\nCfusion = Conv3(C′) + C′ ∈ RC×hq×wq . (9)\nSelf-distillation loss. We propose a self-distillation loss for our CGT\nto help the model generate higher-quality robust correlation diagrams in the\nearly stage. We average the feature dimensions for the correlation map of\neach layer to get the correlation map ˆC ∈ Rhq×wq , and use the high-level\ncorrelation map to guide the low-level correlation feature map, as follows:\nLdistill = 1\nL\nLX\nl=1\nhlwlX\ni=1\nζl( ˆCl+1\nlocal,i) · log ζl( ˆCl+1\nlocal,i)\nˆCl\nlocal,i\n, (10)\n10\nwhere ζl(·) is the resize function of the l-th layer, L is the number of the\nlayers and hlwl is the the product of length and width of the l-th layer. The\nguidance of the high-level correlation graph to the low-level feature graph\nhelps the model retain the fine-grained segmentation quality, reduces the\nimpact of noise, and does not discard the context information [41], which\ncan help learn robust correlations.\n4.3. Class-Guided Module\nThe knowledge learned by the model from the correlation between support\nand query is limited, especially in the case of imprecise support masks in\nthe WS-FSS scenarios. To further assist the model in filtering potential\nnoise in correlation features, we use additional category information from\nthe perspective of category semantics to help the model locate more valuable\ncorrelation information. Pre-trained CLIP [26] has been proven to generate\nrelatively coarse CAM based on category information by using Grad-CAM\n[42, 43]. After large-scale pre-training, CLIP already has powerful zero-shot\nlearning capabilities. Even without seeing specific supervision labels during\ntraining, CLIP is able to understand and generate output for tasks for which\nit was not explicitly trained [44, 45]. This paper utilizes this to construct the\nCGM that helps the model roughly locate the approximate positions of the\nobjects that need to be segmented.\nTo simplify our method, this paper will not discuss obtaining more ac-\ncurate masks for CLIP. Instead, we will choose a simple mask generation\nmethod and discuss utilizing the generated coarse masks. CGM can also be\nseen as a simple zero-shot method, and it does not rely on various complex\ncue engineering and other zero-shot models but can still achieve satisfactory\nperformance.\nWe first input the query image and its category prompt “a photo of\n[class]”, where class represents its corresponding category c, into the pre-\ntrained CLIP and then use Grad-CAM to obtain a coarse attentionAc. Next,\nwe multiply Ac by the obtained correlation token Cfusion and use FCGM to\nrefine the attention:\nAr = FCGM (Cfusion ⊗ ζ(Ac)) ∈ Rhq×wq , (11)\nwhere FCGM consists of two convolutional layers and a sigmoid function, ⊗\nis the Hadamard product and ζ(·) is the resize function. Finally, we combine\nthe features with the attention Ar to obtain filtered correlation features that\n11\ndiscard irrelevant background information:\n˜C = (Cfusion ⊗ Ar) ⊕ Cfusion , (12)\nwhere ⊕ stands for the element-wise sum. Combined with backpropagation,\nthe parameters of FCGM in CGM are updated. Therefore, the refined Ar\ncan focus more on the objects that need to be segmented based on the Ac\ngenerated by CLIP. Through the coarse-to-fine training strategy, when the\nmodel encounters unfamiliar categories, even without the precise support of\nmask supervision, it can combine the powerful zero-shot capability of CLIP\nto capture the approximate location of the segmented object.\n4.4. Embedding-Guided Module\nTowards the goal of reducing the potential information loss of the model\nin correlation-enhanced learning under weakly-supervised segmentation set-\ntings, we suggest embedding the original appearance of each layer obtained\nfrom support and query feature maps into the decoder for further aggregation\nto implicitly guide the model in utilizing the learned robust support-query\nmatching information.\nFirst, add the features of each layer and project them:\nFs = FProj. (\nKX\nk=1\nfk\ns ),\nFq = FProj. (\nKX\nk=1\nfk\nq ),\n(13)\nwhere FProj. denotes 1 ×1 convolution and K means the layer number of the\nbackbone ViT. Then they are concatenated to the similarity feature ˜C, and\nthe final prediction mask ˜Mq is obtained through the EGM composed of two\nlayers of transformers [46] and a segmentation header:\n˜Mq = EGM (Cat( ˜C, Fs, Fq)). (14)\nThe original appearance information implicitly helps the model reduce\ninformation loss in learning robust correlation. Meanwhile, the appearance\nembedding information is an effective guide for filtering noise in matching\nscores [47, 48, 49], while self-supervised pre-trained ViT can provide an effi-\ncient multi-layer feature. It helps the model learn the relevant information\nobtained in the presence of certain mismatches through embedding guidance\nat each layer.\n12\n4.5. Training Objective\nPseudo-mask. As demonstrated in previous studies [50, 24, 15], query\nkey attention maps can capture semantically significant foreground objects.\nInspired by this, we generate a pseudo-GT mask for dynamic queries and\nimage support by calculating the cross attention of the last ViT layer:\nMm\ns,i = (fm\ns,i)T fm\nq,cls\n||fm\ns,i||||fm\nq,cls|| ∈ Rhsws×1,\nMm\nq,i = (fm\nq,i)T fm\ns,cls\n||fm\nq,i||||fm\ns,cls|| ∈ Rhqwq×1,\n(15)\nwhere fm\nq,cls, fm\ns,cls means m-th head query or support class token. Meanwhile,\nwe use the Pixel-Adaptive Refinement (PAR) module [27] to generate pseudo-\nmasks based on the relationship information between various pixels within\nthe image, generating more accurate supervision information:\nMs,i = 1(P AR(ζ( 1\nM\nMX\nm=1\nMm\ns,i) > α)),\nMq,i = 1(P AR(ζ( 1\nM\nMX\nm=1\nMm\nq,i) > α)),\n(16)\nwhere α = 0 .4 is the prediction threshold and 1(·) is the indicator func-\ntion. Unlike existing WS-FSS methods [22], our mask generation module\nalso applies to unseen categories without additional training stages.\nTraining loss. There are two parts of training loss: segmentation loss\nand self-distillation loss. The segmentation loss Lseg is calculated by the final\nprediction ˜Mq and Ms using the cross-entropy function. The self-distillation\nloss is obtained from Eq. 10. The final loss is:\nL = Lseg + λdistillLdistill, (17)\nwhere λdistill is the balance parameter set to 0.5. The whole training process\nfor CORENet is summarized in Algorithm 1.\n5. Experiments\nIn this section, we evaluate the proposed method, compare it with recent\nstate-of-the-art, and provide in-depth analyses of the results of the ablation\nstudy.\n13\nAlgorithm 1: Training Process for CORENet.\nInput: A training set Dtrain and a training category set Ctrain.\nOutput: The final trained model ϕ.\nfor each episode (S, Q) ∈ Dtrain and categoryc ∈ Ctrain do\nExtract features by pretrained DINO ViT.\nGenerate the pseudo-mask using Eq.15 and 16.\n# Correlation-Guided Transformer\nCompute the local-to-local and local-to-global correlation using\nEq. 1, 2 and 3.\nObtain foreground, background, and local correlation tokens\nusing Eq. 4.\nObtain the final correlation token Cfusion using Eq. 5, 6, 7, 8\nand 9.\nCompute the self-distillation loss Ldistill as in Eq. 10.\n# Class-Guided Module\nObtain filtered correlation feature ˜C using Eq. 11 and 12.\n# Embedding-Guided Module\nPredict the query mask ˜Mq using Eq. 13 and 14.\nCompute the final loss L as in Eq. 17.\nCompute gradients and optimize via SGD.\nend\nReturn the final trained model ϕ.\n5.1. Experimental Settings\nDatasets. To evaluate our method, experiments are conducted on two\ncommonly used few-shot segmentation datasets, PASCAL-5i and COCO-20i.\nPASCAL-5i is created according to PASCAL VOC 2012 [51] with additional\nnotes of SBD [52]. A total of 20 classes in the dataset are evenly divided into\nfour folds i ∈ {0, 1, 2, 3 }, and each fold contains five classes. COCO-20 i is\nproposed by [53] and is based on MSCOCO [54]. Similar to PASCAL-5 i, the\n80 classes in COCO-20i are divided into four folds, and each fold contains 20\nclasses.\nEvaluation metrics. We use union average intersection (mIoU) as our\nevaluation indicators. The mood indicator averages the IoU values of all\nclasses in the fold: mIoU = 1\nC\nPC\nc=1 IoUc, where C is the number of classes in\nthe target fold and IoU c is the intersection on the union of class c. Because\nmIoU better reflects the generalization ability and prediction quality of the\n14\nTable 1\nPerformance of PASCAL-5i [51] in mIoU. The superscript ∗ indicates that the model is\ntrained on the pseudo-mask generated by CST [15]. Bold numbers indicate the best\nperformance, and underlined numbers indicate the second best.\n1-shot 5-shot\nMethods 5 0 51 52 53 mean 50 51 52 53 mean\nHSNet∗ [12] 47.6 45.4 41.0 37.0 42.8 48.0 46.1 41.6 37.3 43.3\nASNet∗ [25] 49.0 44.6 43.8 35.2 43.2 50.1 45.6 45.0 35.8 44.1\nMIANet∗ [13] 44.9 34.3 41.2 35.9 39.1 46.4 45.1 46.5 36.4 43.6\nCST [15] 48.2 45.1 42.4 34.6 42.5 49.5 45.5 42.8 35.1 43.2\nCORENet (Ours)50.3 51.6 47.6 39.4 47.2 50.7 51.8 47.8 39.6 47.5\nmodel, we mainly focus on mIoU in our experiments.\nImplementation details. To compare with previous works based on\nResNet50 [12, 25, 13], we use the ViT-small backbone [55]. The feature\nextraction backbone network conducts self-monitoring and pre-training on\nImageNet 1K [56] through DINO [24]. Following the CST [15], the reason\nfor choosing this ViT is that its training data size and the number of model\nparameters are similar to ResNet50 [40]. It is also trained on ImageNet 1K\nbut uses class labels as supervision. The backbone of CLIP is ResNet101.\nHowever, our framework based on DINO and CLIP can easily replace the\nbackbone network with a foundation model such as ViT-G/14 [57] with a\nhuge parameter amount (2.5B), which distinguishes our method from existing\nmethods. As in the previous works [12, 25, 15], the backbone is frozen during\ntraining. The learning rate is initialized to 0.0005, the batch size is 16, and\nthe additional layer is trained using Adam [58]. The loss balance parameter\nλdistill is set to 0.5, and the number of backgrounds N is set to 5. Consistent\nwith CST [15], our CORENet uses a 1-way 1-shot segment for training and\nany N-way K-shot inference.\n5.2. Comparison with State-of-the-Arts.\nDue to the lack of supervision masks, the existing FSS model cannot be\ndirectly migrated to the WS-FSS scenario. To better compare existing FSS\nmethods, we combine them with the mask generation method in CST [15] to\ngenerate supervised information and guide the model in predicting the final\nquery mask. We label them with the superscript ∗.\nPASCAL-5i. Table 1 compares mIoU performance between our method\nand existing representative models. From this, it can be seen that: (i)\n15\nTable 2\nPerformance of COCO-20 i [53] in mIoU. The superscript ∗ indicates that the model is\ntrained on the pseudo-mask generated by CST [15]. Bold numbers indicate the best\nperformance, and underlined numbers indicate the second best.\n1-shot 5-shot\nMethods 20 0 201 202 203 mean 200 201 202 203 mean\nHSNet∗ [12] 19.9 22.5 22.1 23.0 21.9 21.0 24.2 22.7 23.8 22.9\nMIANet∗ [13] 20.5 22.8 21.6 22.3 21.8 21.5 23.2 21.8 22.5 22.3\nCST [15] 21.0 21.9 22.4 22.5 22.0 21.3 22.1 22.7 22.6 22.1\nCORENet (Ours)22.1 22.822.3 23.4 22.7 22.3 24.722.6 24.0 23.4\nFig. 4. Qualitative results of our CORENet on PASCAL-5i and COCO-20i benchmarks.\nZoom in for details.\nCORENet achieved state-of-the-art performance in both 1-shot and 5-shot\nsettings. Compared to the recent FSS model MIANet [13] and the weakly-\nsupervised classification & segmentation model CST [15], we have improved\nby 8.1%, 4.7% (1-shot), and 3.9%, 4.3% (5-shot), respectively. (ii) MIANet\nhas performed poorly in certain situations, which holds the previous state-\nof-the-art results of FSS. This is because in WS-FSS scenarios, the generated\nmask contains noise, and excessive dependence on correlated features with\nnoise can lead to a decrease in model performance. This also confirms that\nthe method proposed in this paper can effectively handle scenarios with mask\nnoise in weakly-supervised few-shot segmentation.\nCOCO-20i. COCO-20i is a more challenging dataset containing mul-\ntiple objects and more significant variance. Table 2 shows the performance\n16\nTable 3\nAblation studies of main model components. Bold numbers indicate the best performance.\nBaseline CGT CGM EGM PAR 1-shot 5-shot\n✓ 42.5 43.2\n✓ ✓ 43.1 43.9\n✓ ✓ ✓ 44.0 44.7\n✓ ✓ ✓ ✓ 45.1 45.8\n✓ ✓ ✓ ✓ ✓ 47.2 47.5\ncomparison of mIoU. Overall, the mean mIoU of MIANet in the 1-shot and\n5-shot settings surpasses all previous methods. Under the 1-shot setting, our\nCORENet exceeded MIANet and CST by 0.9% and 0.7%. This proves the\nsuperiority of our method despite the challenging scenarios.\nQualitative results. Fig. 4 reports quantitative results from CORENet\nand baseline models based on PASCAL-5 i and COCO-20i benchmark tests.\nWe can see that CORENet performs well in capturing object details. For\nexample, more subtle details are retained in segmenting dogs and cars.\n5.3. Ablation Study\nWe conducted extensive ablation studies of PASCAL-5 i to verify the ef-\nfectiveness of the critical modules (CGT, CGM, and EGM) we proposed. In\naddition, we provide experimental details and additional experiments in the\nsupplementary materials.\nComponents analysis. Our CORENet consists of four key components:\nCorrelation-Guided Transformer (CGT), Class-Guided Module (CGM), Emb-\nedding-Guided Module (EGM), and Pixel-Adaptive Refinement (PAR). Ta-\nble 3 shows our validation of the effectiveness of each component. PAR\ncan further reduce imprecise noise in masks and achieve a performance im-\nprovement of 2.1% in 1-shot by fully utilizing the information of surrounding\npixels. EGM is an essential component of our model, which increases mIoU\nby 1.1% in 1-shot. CGT and CGM are also indispensable. By combining all\nthree modules, CORENet achieves state-of-the-art performance.\nMain components in CGT. CGT constructs local-to-global correla-\ntions to help the model fully understand matching information. Table 4 shows\nthe impact of each element in CGT on model performance. “FBC” means the\nfore-background concatenation, “FBF” means the fore-background fusion,\n“SIF” denotes the single-kernel information fusion, and “MIF” denotes the\nmulti-kernel information fusion. We can see that the fusion using adaptive\n17\nTable 4\nAblation studies of main components in CGT.\nBold numbers indicate the best performance.\nFBC FBF SIF MIF 1-shot 5-shot\n✓ ✓ 46.6 46.9\n✓ ✓ 46.9 47.1\n✓ ✓ 46.7 47.2\n✓ ✓ 47.2 47.5\nTable 5\nAnalysis of background regions in\nCGT.\nBackground regions1-shot\nN = 4 47.0\nN = 5 47.2\nN = 6 47.1\nN = 7 47.2\nTable 6\nAblation studies of CGM.\nCLIP Refinement 1-shot\n46.5\n✓ 46.8\n✓ ✓ 47.2\nTable 7\nAnalysis of projection dimen-\nsion in EGM.\nDimension 1-shot\n32 46.8\n64 47.2\n128 46.9\n384 47.1\nweights is 0.3% better than the method of directly concatenating background\nfeatures along the channel. In addition, by establishing path information be-\ntween different receptive fields, the proposed multi-kernel information fusion\nmethod is compared to the single-kernel fusion method (using only 3 × 3\nconvolutions), which can further reduce the impact of mismatches on the\nmodel and achieve better performance. Through the proposed fusion mech-\nanism, CGT can help the model learn more robust correlation with more\ninformation while support mask inaccuracies.\nNumber of background regions for CGT. In Section 4.2, we men-\ntioned that the Voronoi-based method [35, 36] helps the model learn complex\nbackground correlation knowledge by dividing different background regions.\nWe further discuss the impact of this region on the final results of the model,\nas shown in Table 5. It can be seen from the results in the table that the\nmodel results are relatively robust for different numbers of regions. This\nshows that our CGT can perform robust correlation modeling for different\ncomplex background knowledge, which can help the model still learn valuable\ncorrelation knowledge when facing the generated imprecise masks.\n18\nTable 8\nAnalysis of differences in pro-\njection operations of EGM.\nDimension 1-shot\nConcatenation 46.9\nSum 47.2\nTable 9\nPerformance differences with related methods [23]\nin PASCAL-5i.\nMethod 1-shot 5-shot\nPixel-level meta-learner [23]42.4 45.5\nCORENet (Ours) 47.2 47.5\nRefinement of CGM. In Section 4.3, we mentioned using pre-trained\nCLIP [26] assisted models for segmentation by constructing CGM. We further\ndiscuss the necessity of pre-trained CLIP and the proposed enhancement\nmodule, as shown in Table 6. When no additional components are added, the\nmodel directly feeds the correlation features obtained through CGT into the\nEGM module. For the different approaches to fusing attention map of CLIP,\nwe compare the approach of simply fusing it into the original correlation\nfeatures (denoted as “CLIP”) with the approach of fusing it by designing\nadditional learnable refinement modules (denoted as “Refinement”). Due\nto the zero-shot capability of CLIP, the performance of the model can be\nimproved to a certain extent when only CLIP is used to weight features. The\nexperimental results show that after the refinement module, the model can\nbetter combine the knowledge provided by CLIP to help the model filter out\nirrelevant background areas and achieve the best results.\nDifferenct backbone of CLIP in CGM. We use the GradCAM of the\npre-trained CLIP in the CGM module to generate initialization attention\nmaps. As shown in Fig. 5, we visualized the thermal maps obtained by\nCLIP for different backbones. When using a deeper network as the backbone\nof CLIP, the initial attention map obtained is visually better. A better\ninitial attention map can help the model focus on more critical correlation\ninformation. Therefore, we chose ResNet101 [40] as the backbone of our\nCGM module.\nProjection dimension of EGM. In Section 4.4, we mentioned pro-\njecting the original feature representation onto a particular dimension and\nconcatenating it into the enhanced correlation features to reduce potential\ninformation loss during enhancement. We conducted experiments on differ-\nent projection dimensions, and the results are shown in Table 7. Different\nprojection dimensions have little impact on the experimental results, and the\nbest effect is achieved when the dimension is 64.\n19\n(a) Original image\n (b) ResNet50\n (c) ResNet101\nFig. 5. Visualization of GradCAM obtained from different backbones of CLIP in CGM.\nDifferences in projection operations of EGM. In Eq. 13, we men-\ntioned the operation of adding all features during the feature projection pro-\ncess. A feasible alternative is to concatenate all features and feed them into\na dimensionally reduced convolutional layer. The features obtained in this\nway are of the same size as those obtained by direct addition, where we note\nthis scheme as ”concatenation”. The comparison results of the two schemes\nare shown in Table 8. It can be seen that directly adding features can obtain\nbetter results than concatenation without the need for additional convolution\noperations.\nComparison with existing similar work. It is worth noting that the\nrecent related work [23] also considers a similar problem setting. Different\nfrom [23], our method focuses more on exploring the performance capabilities\nof the foundation model in WS-FSS. To further demonstrate the difference\nbetween the two methods, we compare the differences between the two meth-\nods on PASCAL-5i, and the results are shown in Table 9. It can be seen from\nthe results that our method can help the model perform better in the WS-\nFSS scenario due to its strong generalization ability based on the foundation\nmodel and the robustness of the proposed method.\nParameter sensitivity. For our CGT, we have designed a self-distillation\nloss aid model to generate higher-quality correlation maps. We conducted\nsensitivity experiments on different loss balance parameters λdistill, as shown\nin Fig. 6. Under different λdistill, the mIoU variation of the model is rela-\ntively robust and reaches its optimal value at 0.5. However, without using\nself-distillation loss, i.e. λdistill = 0, the model performance decreases by\n1.1%, further proving the advantage of our proposed loss.\n20\nFig. 6. Results with loss balance parameter λdistill on the 1-shot setting.\nTable 10\nThe difference between related work and weakly-supervised few-shot segmentation.\nRelated task settings Difference\nFew-shot semantic segmentation [7] Dependence on accurate ground-truth mask.\nWeakly-supervised semantic segmentation [60] The model can only segment seen categories.\nWeakly-supervised few-shot classification & segmentation [15]The provided category information is whether the twoimages belong to the same category (0/1 label) and doesnot provide specific category assistance for segmentation.\n5.4. Discussions\nDiscussions about related settings. We demonstrate the differences\nbetween WS-FSS and related work settings in Table 10. Compared to WS-\nFSS, few-shot segmentation methods [49, 13, 8, 12, 29, 28, 10] rely more\non precise ground-truth masks and learn to support and query the corre-\nlation of images based on this. Due to differences in application scenar-\nios, weakly-supervised segmentation methods [59, 60, 27] cannot segment\nnew classes that have not been seen before. Most relevant to WS-FSS, the\nweakly-supervised few-shot classification & segmentation method [15] not\nonly performs weakly-supervised segmentation on query images containing\nunseen categories but also allows the model to output whether they belong to\nthe same category as the support images. However, the category supervision\ninformation it provides is whether the query image is in the same category\nas the support image. It does not provide specific category information to\nassist the model in FSS.\nFeature work. Compared to state-of-the-art methods on relatively sim-\nple datasets, our method has succeeded considerably. However, model perfor-\n21\nmance can continue to improve when faced with more complex datasets (such\nas COCO-20i). We will explore in the future how to better learn correlations\nbetween more complex images. On the other hand, our random division of\nbackground region in CGT also considers this problem. More complex corre-\nlations are learned by setting a more significant background number N, and\nthe problem becomes part of parameter selection. In the future, we will have\na more in-depth discussion on this issue to help the model learn more robust\ncorrelation knowledge without GT masks.\n6. Conclusion\nThis paper proposes a framework to address the issue of requiring precise\nmasks for existing FSS tasks, which address weakly-supervised few-shot seg-\nmentation tasks with only category information. To better mine the robust\ncorrelation between support queries, this paper proposes that CGT calcu-\nlate similarity information from global and local perspectives. Then, from\nthe perspective of category semantics, we designed CGM to help the model\nroughly locate targets using the pre-trained CLIP. In addition, the EGM\nmodule was designed to implicitly guide the model in filtering noise in corre-\nlation from the perspective of appearance embedding. Extensive experiments\nhave shown that our CORENet has achieved state-of-the-art results in our\nweakly-supervised few-shot segmentation tasks.\nReferences\n[1] L. Fei-Fei, R. Fergus, P. Perona, One-shot learning of object categories,\nIEEE transactions on pattern analysis and machine intelligence 28 (4)\n(2006) 594–611.\n[2] Y. Wang, Q. Yao, J. T. Kwok, L. M. Ni, Generalizing from a few ex-\namples: A survey on few-shot learning, ACM computing surveys (csur)\n53 (3) (2020) 1–34.\n[3] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learn-\ning, Advances in neural information processing systems 30 (2017).\n[4] Y. Xie, H. Wang, B. Yu, C. Zhang, Secure collaborative few-shot learn-\ning, Knowledge-Based Systems 203 (2020) 106157.\n22\n[5] Y. Qin, W. Zhang, C. Zhao, Z. Wang, X. Zhu, J. Shi, G. Qi, Z. Lei,\nPrior-knowledge and attention based meta-learning for few-shot learn-\ning, Knowledge-Based Systems 213 (2021) 106609.\n[6] Y. Zhang, M. Gong, J. Li, K. Feng, M. Zhang, Autonomous perception\nand adaptive standardization for few-shot learning, Knowledge-Based\nSystems 277 (2023) 110746.\n[7] A. Shaban, S. Bansal, Z. Liu, I. Essa, B. Boots, One-shot learning for\nsemantic segmentation, arXiv preprint arXiv:1709.03410 (2017).\n[8] K. Wang, J. H. Liew, Y. Zou, D. Zhou, J. Feng, Panet: Few-shot image\nsemantic segmentation with prototype alignment, in: proceedings of\nthe IEEE/CVF international conference on computer vision, 2019, pp.\n9197–9206.\n[9] C. Zhang, G. Lin, F. Liu, R. Yao, C. Shen, Canet: Class-agnostic seg-\nmentation networks with iterative refinement and attentive few-shot\nlearning, in: Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2019, pp. 5217–5226.\n[10] W. Liu, C. Zhang, G. Lin, F. Liu, Crnet: Cross-reference networks for\nfew-shot segmentation, in: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp. 4165–4173.\n[11] Z. Tian, H. Zhao, M. Shu, Z. Yang, R. Li, J. Jia, Prior guided feature\nenrichment network for few-shot segmentation, IEEE transactions on\npattern analysis and machine intelligence 44 (2) (2020) 1050–1065.\n[12] J. Min, D. Kang, M. Cho, Hypercorrelation squeeze for few-shot seg-\nmentation, in: Proceedings of the IEEE/CVF international conference\non computer vision, 2021, pp. 6941–6952.\n[13] Y. Yang, Q. Chen, Y. Feng, T. Huang, Mianet: Aggregating unbiased\ninstance and general information for few-shot semantic segmentation,\nin: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 7131–7140.\n[14] Q. Li, B. Sun, B. Bhanu, Lite-fenet: Lightweight multi-scale feature en-\nrichment network for few-shot segmentation, Knowledge-Based Systems\n278 (2023) 110887.\n23\n[15] D. Kang, P. Koniusz, M. Cho, N. Murray, Distilling self-supervised vi-\nsion transformers for weakly-supervised few-shot classification & seg-\nmentation, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023, pp. 19627–19638.\n[16] C. Finn, P. Abbeel, S. Levine, Model-agnostic meta-learning for fast\nadaptation of deep networks, in: International conference on machine\nlearning, PMLR, 2017, pp. 1126–1135.\n[17] J. Schmidhuber, Evolutionary principles in self-referential learning, or on\nlearning how to learn: the meta-meta-... hook, Ph.D. thesis, Technische\nUniversit¨ at M¨ unchen (1987).\n[18] A. Nichol, J. Schulman, Reptile: a scalable metalearning algorithm,\narXiv preprint arXiv:1803.02999 2 (3) (2018) 4.\n[19] A. Rivolli, L. P. Garcia, C. Soares, J. Vanschoren, A. C. de Carvalho,\nMeta-features for meta-learning, Knowledge-Based Systems 240 (2022)\n108101.\n[20] Y. Feng, J. Chen, J. Xie, T. Zhang, H. Lv, T. Pan, Meta-learning as\na promising approach for few-shot cross-domain fault diagnosis: Al-\ngorithms, applications, and prospects, Knowledge-Based Systems 235\n(2022) 107646.\n[21] P. H. T. Gama, H. N. Oliveira, J. Marcato, J. Dos Santos, Weakly\nsupervised few-shot segmentation via meta-learning, IEEE Transactions\non Multimedia (2022).\n[22] M. Zhang, Y. Zhou, B. Liu, J. Zhao, R. Yao, Z. Shao, H. Zhu, Weakly su-\npervised few-shot semantic segmentation via pseudo mask enhancement\nand meta learning, IEEE Transactions on Multimedia (2022).\n[23] Y.-H. Lee, F.-E. Yang, Y.-C. F. Wang, A pixel-level meta-learner for\nweakly supervised few-shot semantic segmentation, in: Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision,\n2022, pp. 2170–2180.\n[24] M. Caron, H. Touvron, I. Misra, H. J´ egou, J. Mairal, P. Bojanowski,\nA. Joulin, Emerging properties in self-supervised vision transformers,\n24\nin: Proceedings of the IEEE/CVF international conference on computer\nvision, 2021, pp. 9650–9660.\n[25] D. Kang, M. Cho, Integrative few-shot learning for classification and seg-\nmentation, in: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 9979–9990.\n[26] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transfer-\nable visual models from natural language supervision, in: International\nconference on machine learning, PMLR, 2021, pp. 8748–8763.\n[27] L. Ru, Y. Zhan, B. Yu, B. Du, Learning affinity from attention: End-\nto-end weakly-supervised semantic segmentation with transformers, in:\nProceedings of the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 2022, pp. 16846–16855.\n[28] M. Siam, B. N. Oreshkin, M. Jagersand, Amp: Adaptive masked proxies\nfor few-shot segmentation, in: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 2019, pp. 5249–5258.\n[29] L. Yang, W. Zhuo, L. Qi, Y. Shi, Y. Gao, Mining latent classes for\nfew-shot segmentation, in: Proceedings of the IEEE/CVF international\nconference on computer vision, 2021, pp. 8721–8730.\n[30] G. Li, V. Jampani, L. Sevilla-Lara, D. Sun, J. Kim, J. Kim, Adaptive\nprototype learning and allocation for few-shot segmentation, in: Pro-\nceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2021, pp. 8334–8343.\n[31] A. Okazawa, Interclass prototype relation for few-shot segmentation, in:\nEuropean Conference on Computer Vision, Springer, 2022, pp. 362–378.\n[32] H. Raza, M. Ravanbakhsh, T. Klein, M. Nabi, Weakly supervised one\nshot segmentation, in: Proceedings of the IEEE/CVF International\nConference on Computer Vision Workshops, 2019, pp. 0–0.\n[33] O. Saha, Z. Cheng, S. Maji, Improving few-shot part segmentation us-\ning coarse supervision, in: European Conference on Computer Vision,\nSpringer, 2022, pp. 283–299.\n25\n[34] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al., Matching net-\nworks for one shot learning, Advances in neural information processing\nsystems 29 (2016).\n[35] F. Aurenhammer, Voronoi diagrams—a survey of a fundamental geo-\nmetric data structure, ACM Computing Surveys (CSUR) 23 (3) (1991)\n345–405.\n[36] J.-W. Zhang, Y. Sun, Y. Yang, W. Chen, Feature-proxy transformer\nfor few-shot segmentation, Advances in Neural Information Processing\nSystems 35 (2022) 6575–6588.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural\ninformation processing systems 30 (2017).\n[38] Y. Wu, K. He, Group normalization, in: Proceedings of the European\nconference on computer vision (ECCV), 2018, pp. 3–19.\n[39] Z. Dai, G. Lai, Y. Yang, Q. Le, Funnel-transformer: Filtering out se-\nquential redundancy for efficient language processing, Advances in neu-\nral information processing systems 33 (2020) 4271–4282.\n[40] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image\nrecognition, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2016, pp. 770–778.\n[41] B. Peng, Z. Tian, X. Wu, C. Wang, S. Liu, J. Su, J. Jia, Hierarchical\ndense correlation distillation for few-shot segmentation, in: Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, 2023, pp. 23641–23651.\n[42] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,\nGrad-cam: Visual explanations from deep networks via gradient-based\nlocalization, in: Proceedings of the IEEE international conference on\ncomputer vision, 2017, pp. 618–626.\n[43] H. Wang, L. Liu, W. Zhang, J. Zhang, Z. Gan, Y. Wang, C. Wang,\nH. Wang, Iterative few-shot semantic segmentation from image label\ntext, arXiv preprint arXiv:2303.05646 (2023).\n26\n[44] Z. Zhou, Y. Lei, B. Zhang, L. Liu, Y. Liu, Zegclip: Towards adapting clip\nfor zero-shot semantic segmentation, in: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n11175–11185.\n[45] S. Jiao, Y. Wei, Y. Wang, Y. Zhao, H. Shi, Learning mask-aware clip\nrepresentations for zero-shot segmentation, Advances in Neural Infor-\nmation Processing Systems 36 (2023) 35631–35653.\n[46] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin\ntransformer: Hierarchical vision transformer using shifted windows, in:\nProceedings of the IEEE/CVF international conference on computer\nvision, 2021, pp. 10012–10022.\n[47] A. Hosni, C. Rhemann, M. Bleyer, C. Rother, M. Gelautz, Fast cost-\nvolume filtering for visual correspondence and beyond, IEEE transac-\ntions on pattern analysis and machine intelligence 35 (2) (2012) 504–511.\n[48] D. Sun, X. Yang, M.-Y. Liu, J. Kautz, Pwc-net: Cnns for optical flow\nusing pyramid, warping, and cost volume, in: Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 8934–\n8943.\n[49] S. Hong, S. Cho, J. Nam, S. Lin, S. Kim, Cost aggregation with 4d\nconvolutional swin transformer for few-shot segmentation, in: European\nConference on Computer Vision, Springer, 2022, pp. 108–126.\n[50] S. Amir, Y. Gandelsman, S. Bagon, T. Dekel, Deep vit features as dense\nvisual descriptors, arXiv preprint arXiv:2112.05814 2 (3) (2021) 4.\n[51] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, A. Zisserman,\nThe pascal visual object classes (voc) challenge, International journal of\ncomputer vision 88 (2010) 303–338.\n[52] B. Hariharan, P. Arbel´ aez, R. Girshick, J. Malik, Simultaneous detection\nand segmentation, in: Computer Vision–ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart VII 13, Springer, 2014, pp. 297–312.\n27\n[53] K. Nguyen, S. Todorovic, Feature weighting and boosting for few-shot\nsegmentation, in: Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 2019, pp. 622–631.\n[54] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ ar, C. L. Zitnick, Microsoft coco: Common objects in context,\nin: Computer Vision–ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13, Springer,\n2014, pp. 740–755.\n[55] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al.,\nAn image is worth 16x16 words: Transformers for image recognition at\nscale, arXiv preprint arXiv:2010.11929 (2020).\n[56] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al., Imagenet large\nscale visual recognition challenge, International journal of computer vi-\nsion 115 (2015) 211–252.\n[57] X. Zhai, A. Kolesnikov, N. Houlsby, L. Beyer, Scaling vision transform-\ners, in: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 2022, pp. 12104–12113.\n[58] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization,\narXiv preprint arXiv:1412.6980 (2014).\n[59] D. Li, J.-B. Huang, Y. Li, S. Wang, M.-H. Yang, Weakly supervised\nobject localization with progressive domain adaptation, in: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition,\n2016, pp. 3512–3520.\n[60] H. Bilen, A. Vedaldi, Weakly supervised deep detection networks, in:\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 2846–2854.\n28",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7452180981636047
    },
    {
      "name": "Segmentation",
      "score": 0.6269574165344238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6108099222183228
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5897040963172913
    },
    {
      "name": "Correlation",
      "score": 0.5585438013076782
    },
    {
      "name": "Computer science",
      "score": 0.44748812913894653
    },
    {
      "name": "One shot",
      "score": 0.4415537714958191
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4008128046989441
    },
    {
      "name": "Mathematics",
      "score": 0.2967846393585205
    },
    {
      "name": "Geography",
      "score": 0.18379414081573486
    },
    {
      "name": "Geometry",
      "score": 0.12556064128875732
    },
    {
      "name": "Engineering",
      "score": 0.11501136422157288
    },
    {
      "name": "Materials science",
      "score": 0.06725424528121948
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Metallurgy",
      "score": 0.0
    }
  ]
}