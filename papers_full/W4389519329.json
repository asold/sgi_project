{
    "title": "Conceptual structure coheres in human cognition but not in large language models",
    "url": "https://openalex.org/W4389519329",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2239179153",
            "name": "Siddharth Suresh",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2978686985",
            "name": "Kushin Mukherjee",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2551537243",
            "name": "Xizheng Yu",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A2161858336",
            "name": "Wei-Chun Huang",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        },
        {
            "id": "https://openalex.org/A5065676264",
            "name": "Lisa Padua",
            "affiliations": [
                "Albany State University"
            ]
        },
        {
            "id": "https://openalex.org/A2074342450",
            "name": "Timothy Rogers",
            "affiliations": [
                "University of Wisconsin–Madison"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3122547876",
        "https://openalex.org/W2158997610",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3085332162",
        "https://openalex.org/W1974275080",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W1488309867",
        "https://openalex.org/W4230838588",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W4250234291",
        "https://openalex.org/W4321499403",
        "https://openalex.org/W4320009668",
        "https://openalex.org/W4282053625",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W1965580172",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4246354968",
        "https://openalex.org/W1830239953",
        "https://openalex.org/W4221159014",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W3047863423",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4306294770",
        "https://openalex.org/W4221161799",
        "https://openalex.org/W2020438140",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4248358431",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W4387617694",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4319166659",
        "https://openalex.org/W2979401726",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W2032618685",
        "https://openalex.org/W2070586582",
        "https://openalex.org/W2014440992",
        "https://openalex.org/W4313451803",
        "https://openalex.org/W4365460912",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W4365205276",
        "https://openalex.org/W4253085775",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W2078894097",
        "https://openalex.org/W2147152072",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4223947928",
        "https://openalex.org/W2185823043",
        "https://openalex.org/W4254816979"
    ],
    "abstract": "Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses three common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known large language model, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses–responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates. The results suggest one important way that knowledge inhering in contemporary LLMs can differ from human cognition.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 722–738\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nConceptual structure coheres in human cognition\nbut not in large language models\nSiddharth Suresh1, Kushin Mukherjee1, Xizheng Yu1,\nWei-Chun Huang1, Lisa Padua2 and Timothy T. Rogers1\n1University of Wisconsin-Madison,2Albany State University\nsiddharth.suresh@wisc.edu\nAbstract\nNeural network models of language have long\nbeen used as a tool for developing hypotheses\nabout conceptual representation in the mind\nand brain. For many years, such use involved\nextracting vector-space representations of\nwords and using distances among these\nto predict or understand human behavior\nin various semantic tasks. Contemporary\nlarge language models (LLMs), however,\nmake it possible to interrogate the latent\nstructure of conceptual representations using\nexperimental methods nearly identical to those\ncommonly used with human participants. The\ncurrent work utilizes three common techniques\nborrowed from cognitive psychology to\nestimate and compare the structure of concepts\nin humans and a suite of LLMs. In humans,\nwe show that conceptual structure is robust\nto differences in culture, language, and\nmethod of estimation. Structures estimated\nfrom LLM behavior, while individually fairly\nconsistent with those estimated from human\nbehavior, vary much more depending upon\nthe particular task used to generate responses–\nacross tasks, estimates of conceptual structure\nfrom the very same model cohere less with\none another than do human structure estimates.\nThese results highlight an important difference\nbetween contemporary LLMs and human\ncognition, with implications for understanding\nsome fundamental limitations of contemporary\nmachine language.\n1 Introduction\nSince Elman’s pioneering work (Elman, 1990)\nshowcasing the ability of neural networks\nto capture many aspects of human language\nprocessing (Rumelhart et al., 1986), such models\nhave provided a useful tool, and sometimes\na gadfly, for developing hypotheses about the\ncognitive and neural mechanisms that support\nlanguage. When trained on a task that seems\nalmost absurdly simplistic–continuous, sequential\nprediction of upcoming words in sentences–early\nmodels exhibited properties that upended received\nwisdom about what language is and how it works.\nThey acquired internal representations that blended\nsyntactic and semantic information, rather than\nkeeping these separate as classic psycho-linguistics\nrequired. They handled grammatical dependencies,\nnot by constructing syntactic structure trees, but\nby learning and exploiting temporal patterns\nin language. Perhaps most surprisingly, they\nillustrated that statistical structure latent in natural\nlanguage could go a long way toward explaining\nhow we acquire knowledge of semantic similarity\nrelations among words. Because words with\nsimilar meanings tend to be encountered in similar\nlinguistic contexts (Firth, 1957; Osgood, 1952;\nHarris, 1954), models that exploit contextual\nsimilarity when representing words come to\nexpress semantic relations between them. This\nenterprise of learning by predicting has persisted\ninto the modern era of Transformer-based language\nmodels (Devlin et al., 2018; Brown et al., 2020a).\nThough early work was limited in the nature and\ncomplexity of the language corpora used to train\nmodels (Elman, 1991; McClelland et al., 1990),\nthese ideas spurred a variety of computational\napproaches that could be applied to large corpora\nof written text. Approaches such as latent semantic\nanalysis (Deerwester et al., 1990) and skip-\ngram models (Mikolov et al., 2013a; Bojanowski\net al., 2017), for instance, learn vector-space\nrepresentations of words from overlap in their\nlinguistic contexts, which turn out to capture a\nvariety of semantic relationships amongst words,\nincluding some that are highly abstract (Grand\net al., 2022; Elman, 2004; Lupyan and Lewis,\n2019).\nIn all of this work, lexical-semantic repre-\nsentations are cast as static points in a high-\ndimensional vector space, either computed directly\nfrom estimates of word co-occurrence in large\n722\ntext corpora (Deerwester et al., 1990; Burgess,\n1998), or instantiated as the learned activation\npatterns arising in a neural network model trained\non such corpora. To evaluate whether a given\napproach expresses semantic structure similar\nto that discerned by human participants, the\nexperimenter typically compares the similarities\nbetween word vectors learned by a model to\ndecisions or behaviors exhibited by participants in\nsemantic tasks. For instance, LSA models were\ntested on synonym-judgment tasks drawn from\na common standardized test of English language\ncomprehension by comparing the cosine distance\nbetween the vectors corresponding to a target word\nand each of several option words, and having\nthe model \"choose\" the option with the smallest\ndistance (Landauer et al., 1998). The model was\ndeemed successful because the choice computed\nin this way often aligned with the choices of\nnative English speakers. Such a procedure was\nnot just a useful way for assessing whether model\nrepresentations are human-like—it was just about\nthe only way to do so for this class of models.\nIn the era of large language models such as\nOpen AI’s GPT3 (Brown et al., 2020a), Meta’s\nLLaMa family (Touvron et al., 2023; Taori et al.,\n2023), Google’s FLAN (Wei et al., 2021), and\nmany others (Zhang et al., 2022; Chowdhery et al.,\n2022; Hoffmann et al., 2022; Du et al., 2022),\nthis has changed. Such models are many orders\nof magnitude larger than classical connectionist\napproaches, employ a range of architectural and\ntraining innovations, and are optimized on truly\nvast quantities of data—but nevertheless they\noperate on principles not dissimilar to those\nthat Elman and others pioneered. That is, they\nexploit patterns of word co-occurrence in natural\nlanguage to learn distributed, context-sensitive\nrepresentations of linguistic meaning at multiple\nlevels, and from these representations they generate\nprobabilistic predictions about likely upcoming\nwords. Current models generate plausible and\ngrammatically well-formed responses, created by\niteratively predicting what words are likely to come\nnext and sampling from this distribution using\nvarious strategies (Wei et al., 2022; Wang et al.,\n2022; Yao et al., 2023). So plausible is the text\nthat recent iterations like ChatGPT (Ouyang et al.,\n2022) can write essays sufficiently well to earn a\nA in an undergraduate setting (Elkins and Chun,\n2020), pass many text-based licensing exams in\nlaw and medicine (Newton and Xiromeriti, 2023;\nChoi et al., 2023; Kung et al., 2023) produce\nworking Python code from a general description\nof the function (OpenAI, Year of the webpage,\ne.g., 2023), generate coherent explanations for a\nvariety of phenomena, and answer factual questions\nwith remarkable accuracy 1. Even in the realm of\nfailures, work has shown that the kinds of reasoning\nproblems LLMs struggle with are often the same\nones that humans tend to find difficult (Dasgupta\net al., 2022). In short, if solely evaluated based on\ntheir generated text, such models appear to show\nseveral hallmarks of conceptual abilities that until\nrecently were uniquely human.\nThese innovations allow cognitive scientists, for\nthe first time, to measure and evaluate conceptual\nstructure in a non-human system using precisely\nthe same natural-language-based methods that\nwe use to study human participants. Large\nlanguage models can receive written instructions\nfollowed by a series of stimuli and generate\ninterpretable, natural-language responses for each.\nThe responses generated can be recorded and\nanalyzed in precisely the same manner as responses\ngenerated by humans, and the results of such\nanalyses can then be compared within and between\nhumans and LLMs, as a means of understanding\nwhether and how these intelligences differ.\nThe current paper uses this approach to\nunderstand similarities and differences in the\nway that lexical semantic representations are\nstructured in humans vs LLMs, focusing on one\nremarkable aspect of human concepts–specifically,\ntheir robustness. As Rosch showed many years\nago (Rosch, 1975, 1973), the same conceptual\nrelations underlie behavior in a variety of tasks,\nfrom naming and categorization to feature-listing\nto similarity judgments to sorting. Similar\nconceptual relations can be observed across distinct\nlanguages and cultures (Thompson et al., 2020).\nRobustness is important because it allows for\nshared understanding and communication across\ncultures, over time, and through generations:\nHomer still speaks to us despite the astonishing\ndifferences between his world and ours, because\nmany of the concepts that organized his world\ncohere with those that organize ours. Our\ngoal was to assess whether conceptual structure\n1While it is likely that GPT-3 has been trained on examples\nof many of these exams and tasks, that it can efficiently\nretrieve this knowledge when queried with natural language is\nnonetheless worth noting.\n723\nin contemporary LLMs is also coherent when\nevaluated using methods comparable to those\nemployed with human participants, or whether\nhuman and LLM “mind” differ in this important\nregard.\nTo answer this question, we first measured\nthe robustness of conceptual structure in humans\nby comparing estimates of such structure for a\ncontrolled set of concepts using three distinct\nbehavioral methods – feature-listing, pairwise\nsimilarity ratings, and triadic similarity judgements\n– across two distinct groups – Dutch and North\nAmerican – differing in culture and language. We\nthen conducted the same behavioral experiments\non LLMs, and evaluated (a) the degree to\nwhich estimated conceptual relations in the LLM\naccord with those observed in humans, and\n(b) whether humans and LLMs differ in the\napparent robustness of such structure. We\nfurther compared the structures estimated from\nthe LLM’s overt patterns of behavior to those\nencoded in its internal representations, and also to\nsemantic vectors extracted from two other common\nmodels in machine learning. In addition to\nsimply demonstrating how methods from cognitive\npsychology can be used to better understand\nmachine intelligence, the results point to an\nimportant difference between current state of the\nart LLMs and human conceptual representations.\n2 Related work\nIn addition to many of the studies highlighted in\nthe previous section, here we note prior efforts\nto model human semantics using NLP models.\nMany recent papers have evaluated ways in which\nLLMs are and are not humanlike in their patterns\nof behavior when performing tasks similar to\nthose used in psychology–enough that, despite the\nrelative youth of the technology, there has been\na recent review summarising how LLMs can be\nused in psychology(Demszky et al., 2023), along\nwith work highlighting cases where LLMs can\nreplicate classical findings in the social psychology\nliterature (Dillion et al., 2023). Looking further\nback, several authors have evaluated the semantic\nstructure of learned word embeddings in static-\nvector spaces (Mikolov et al., 2013b; Marjieh et al.,\n2022b), while others have examined the semantic\nstructure of more fine-grained text descriptions of\nconcepts in language models capable of embedding\nsequences (Marjieh et al., 2022a). A few studies\nhave used models to generate lists of features and\nestimated semantic structure from feature overlap\n(Hansen and Hebart, 2022; Suresh et al., 2023;\nMukherjee et al., 2023), or have asked models\nto produce explicit pairwise similarity ratings\n(Marjieh et al., 2023). While such work often\ncompares aspects of machine and human behaviors,\nto our knowledge no prior study has evaluated the\ncoherence of elicited structures across tasks within\na given model, or between structures elicited from\nhumans and machines using the same set of tasks.\n3 Measuring Human Conceptual\nStructure\nFor both human and LLM experiments, we focused\non a subset of 30 concepts (as shown in Table\n1) taken from a large feature-norming study\nconducted at KU Leuven (De Deyne et al., 2008).\nThe items were drawn from two broad categories–\ntools and reptiles/amphibians–selected because\nthey span the living/nonliving divide and also\npossess internal conceptual structure. Additionally\nthis dataset has been widely used and validated in\nthe cognitive sciences.\nTo measure the robustness of conceptual\nstructure in humans, we estimated similarities\namongst the 30 items using 3 different tasks:\n(1) semantic feature listing and verification\ndata collected from a Dutch-speaking Belgian\npopulation in the early 2000s, (2) triadic similarity-\nmatching conducted in English in the US in 2022,\nand (3) Likert-scale pairwise similarity judgments\ncollected in English in the US in 2023. The\nresulting datasets thus differ from each other in\n(1) the task used (feature generation vs triadic\nsimilarity judgments vs pairwise similarity ratings),\n(2) the language of instruction and production\n(Dutch vs English), and (3) the population from\nwhich the participants were recruited (Belgian\nstudents in early 2000’s vs American MTurk\nworkers in 2022/2023). The central question\nwas how similar the resulting estimated structures\nare to one another, a metric we call structure\ncoherence. If estimated conceptual similarities\nvary substantially with language, culture, or\nestimation method, the structural coherence\nbetween groups/methods will be relatively low; if\nsuch estimates are robust to these factors, it will\nbe high. The comparison then provides a baseline\nagainst which to compare structural coherence in\nthe LLM.\n724\n3.1 Methods\n3.1.1 Feature listing study\nData were taken from the Leuven feature-\nlisting norms(De Deyne et al., 2008). In an\ninitial generation phase, this study asked 1003\nparticipants to list 10 semantic features for 6-10\ndifferent stimulus words which were were one of\n295 (129 animals and 166 artifacts) concrete object\nconcepts. The set of features produced across all\nitems were tabulated into a 2600d feature vector. In\na second verification phase, four independent raters\nconsidered each concept-feature pair and evaluated\nwhether the feature was true of the concept. The\nfinal dataset thus contained a C (concept) by F\n(feature) matrix whose entries indicate how many\nof the four raters judged concept C to have feature\nF. Note that this endeavour required the raters to\njudge hundreds of thousands of concept-property\npairs.\nFrom the full set of items, we selected 15 tools\nand 15 reptiles for use in this study (as shown\nin Table 1). We chose these categories because\nthey express both broad, superordinate distinctions\n(living/nonliving) as well as finer-grained internal\nstructure (e.g. snakes vs lizards vs crocodiles).\nThe raw feature vectors were binarized by\nconverting all non-zero entries to 1, with the\nrationale that a given feature is potentially true of a\nconcept if at least one rater judged it to be so. We\nthen estimated the conceptual similarity relations\namongst all pairs of items by taking the cosine\ndistance between their binarized feature vectors,\nand reduced the space to three dimensions via\nclassical multidimensional scaling (Kruskal and\nWish, 1978). The resulting embedding expresses\nconceptual similarity amongst 30 concrete objects,\nas estimated via semantic feature listing and\nverification, in a study conducted in Dutch on a\nlarge group of students living in Belgium in the\nearly 2010s.\n3.1.2 Triadic comparison study\nAs a second estimate of conceptual structure\namongst the same 30 items, we conducted a triadic\ncomparison or triplet judgment task in which\nparticipants must decide which of two option words\nis more similar in meaning to a third reference word.\nFrom many such judgments, ordinal embedding\ntechniques (Jamieson et al., 2015; Hebart et al.,\n2022; Hornsby and Love, 2020) can be used to\nsituate words within a low-dimensional space in\nwhich Euclidean distances between two words\ncapture the probability that they will be selected\nas \"most similar\" relative to some arbitrary third\nword. Like feature-listing, triplet judgment studies\ncan be conducted completely verbally, and so can\nbe simulated using large language models.\nParticipants were 18 Amazon Mechanical\nTurk workers recruited using CloudResearch.\nEach participant provided informed consent in\ncompliance with our Institutional IRB and was\ncompensated for their time.\nStimuli were English translations of the 30 item\nnames listed above, half reptiles and half tools.\nProcedure. On each trial, participants viewed a\ntarget word displayed above two option words, and\nwere instructed to choose via button press which\nof the two option words was most similar to the\ntarget in its meaning. Each participant completed\n200 trials, with the triplet on each trial sampled\nrandomly with uniform probability from the space\nof all possible triplets. The study yielded a total\nof 3600 judgments, an order of magnitude larger\nthan the minimal needed to estimate an accurate\n3D embedding from random sampling according\nto estimates of sample complexity in this task\n(Jamieson et al., 2015). Ninety percent of the\njudgments were used to find a 3D embedding\nin which pairwise Euclidean distances amongst\nwords minimize the crowd-kernel triplet loss\non the training set (Tamuz et al., 2011). The\nresulting embedding was then tested by assessing\nits accuracy in predicting human judgments on the\nheld-out ten percent of data. The final embeddings\npredicted human decisions on held-out triplets with\n75% accuracy, which matched the mean level of\ninter-subject agreement on this task.\n3.1.3 Pairwise similarity study\nOur final estimate of conceptual structure relied\non participants making similarity ratings between\npairs of concepts from the set of 30 items using a\nstandard 7 point Likert scale. Unlike the previous\ntwo methods whichimplicitly arrive at a measure of\nsimilarity between concepts, this approach elicits\nexplicit numerical ratings of pairwise similarity. To\naccount for the diversity in ratings between people,\nwe had multiple participants rate the similarity\nbetween each concept pair in our dataset, with\neach participant seeing each pair in a different\nrandomized order.\nParticipants were 10 MTurk workers recruited\nusing CloudResearch. Each participant provided\n725\nTools Reptiles\nAxe\nNail\nKnife\nSaw\nSpanner\nChisel...\nCobra\nTurtle\nGecko\nLizard\nToad\nCaiman...\nFeature Listing\nCobra\nGecko Spanner\nTriadic Comparison\nConcepts from \nRuts et al. (2004)\nHuman\nParticipants\nGPT-3\n(text-davinci-002)\nWhich of the bottom two \nwords is more similar \nto the word at the top?\nList all the properties of \nCobras\nCobra: has scales, \nis venomous, \nis cold-blooded...\nOn a scale of 1-7, how \nsimilar is a \nGecko to a Cobra?\nGecko-Cobra \nSimlarity: 5\nPairwise Similarity\nTasks\nConcepts\nFigure 1: The three tasks used to estimate conceptual structure in both LLMs and Humans. The exact prompts used\nin our experiments with Humans and LLMs are shown in Table 4\ninformed consent in compliance with our\nInstitutional IRB and was compensated for their\ntime.\nStimuli were each of the 435 (\n(30\n2\n)\n) possible pairs\nof the 30 tool and reptile concepts introduced in\nthe earlier sections.\nProcedure. On each trial of the experiment,\nparticipants were presented with a question of\nthe form - ‘How similar are these two things?\n{concept 1} and {concept 2}’ and were provided\nwith a Likert scale below the question with the\noptions — 1: Extremely dissimilar, 2: Very\ndissimilar, 3: Likely dissimilar, 4: Neutral, 5:\nLikely similar, 6: Very similar, 7: Extremely\nsimilar. On each trial { concept 1} and {concept\n2} were randomly sampled from the set of 435\npossible pairs of concepts and each participant\ncompleted 435 ratings trials rating each of the\npossible pairs.\n3.2 Results\nWe found that the inter-rater reliability within the\nfeature-listing and pairwise rating tasks were quite\nhigh ( r = .81 and r = .98 respectively). We could\nnot compute a similar metric in a straightforward\nmanner for the triadic judgement study because\neach participant was provided with a unique set\nof triplets. Figure 2 top row shows hierarchical\ncluster plots of the semantic embeddings from\nfeature lists (left), the triadic comparison task\n(middle), and pairwise judgement task (right). Both\nembeddings strongly differentiate the living from\nnonliving items, and show comparatively little\ndifferentiation of subtypes within each category\n(though such subtypes are clearly apparent amongst\nthe feature-listing embeddings). To assess the\nstructural coherence among the three different\nembedding spaces, we computed the square of\nthe Procrustes correlation pairwise between the\ndissimilarity matrices for the 30 concepts derived\nfrom the three tasks (Gower, 1975). This metric,\nanalogous to r2, indicates the extent to which\nvariations in pairwise distances from one matrix\nare reflected in the other. The metric yielded\nsubstantial values of 0.96, 0.84, and 0.72 when\ncomparing representations from the feature-listing\ntask to the triplet-judgement task, the feature-\nlisting task to the pairwise comparison task, and\nthe triplet task to the pairwise comparison task,\nrespectively. All these values were significantly\nbetter than chance (p < 0.001), suggesting that in\neach comparison, distances in one space accounted\nfor 96%, 84%, and 72% of the variation observed\nin the other.\". Thus despite differences in language,\ntask, and cultures, the three estimates of conceptual\nstructure were well-aligned, suggesting that human\nconceptual representations of concrete objects are\nremarkably robust. We next consider whether the\nsame is true of large language models.\n4 Measuring Machine Conceptual\nStructure\nIn this section, we consider whether one of the most\nperformant LLMs, OpenAI’s GPT 3, expresses\ncoherence in the structural organization of concepts\nwhen tested using the same methods used in\nthe human behavioral experiments. Using the\nOpenAI API, we conducted the feature listing and\nverification task, triadic comparison task, and the\npairwise similarity rating task on GPT 3. Given\nthe recent deluge of open-source LLMs, we also\ntested FLAN-T5 XXL, and FLAN-U2 on the\ntriadic comparison and pairwise ratings tasks to\nsee how they perform relative to larger closed\nmodels. Finally for completeness we also tested\n726\nFeature listing Triadic comparisons Pairwise ratings\nHumans\nblindworm\nsnake\nboacobradinosaur\ncrocodile\nalligator\ncaiman\nvacuum_cleaner\ngrinding_disc\nlawn_mowernail\nscrewdriverspannerknife\nshovel\ntortoise\nturtle\ntoad\nlizard\nsalamander\nchameleongecko\nchiselhammer\naxe\nsaw\nanvil\noilcan\npaintbrush\nChameleon\nGecko\nLizardCaiman\nSnake\nBoa python\nCrocodile\nSalamander\nLawn Mower\nOil can\nVacuum\nHammerNail\nAxe\nPaint brush\nKnife\nBlindworm\nTortoise\nToad\nDinosaur\nAlligator\nTurtleCobra\nChiselGrinding disk\nShovel\nSpanner\nSaw\nAnvil\nScrewdriver\nCrocodile\nTortoise\nTurtle\nGecko\nLizard\nToad\nChameleon\nSalamander\nSpanner\nHammer\nNail\nGrinding diskKnifeAxe\nSaw\nChisel\nBlindworm\nCobra\nBoa python\nSnake\nDinosaur\nCaimanAlligator\nShovelAnvil\nScrewdriver\nLawn Mower\nVacuum\nOil can\nPaint brush\nGPT-3 (002)\ntoad\ndinosaur\ncaiman\nalligator\ncrocodile\nboa\ncobra\nsnake\nlawn_mower\nvacuum_cleaneraxe\nshovel\nsaw\nchiselknife\ngrinding_disc\nblindworm\nchameleon\ngecko\nlizard\ntortoise\nturtlesalamander\nnailanvil\npaintbrush\nspanner\noilcan\nhammer\nscrewdriver\nSnakeTurtle\nSalamander\nAlligatorBoa python\nChameleon\nLizard\nVacuum\nLawn Mower\nOil can\nCobra\nPaint brush\nGrinding disk\nKnifeSaw\nShovel\nCrocodile\nTortoise\nCaiman\nGecko\nToad\nBlindwormDinosaur\nAxe Screwdriver\nHammer\nAnvil\nSpanner\nChisel\nNail\nSnakeTurtle\nSalamander\nAlligatorBoa python\nChameleon\nLizard\nVacuum\nLawn Mower\nOil can\nCobra\nPaint brush\nGrinding disk\nKnifeSaw\nShovel\nCrocodile\nTortoise\nCaiman\nGecko\nToad\nBlindwormDinosaur\nAxe Screwdriver\nHammer\nAnvil\nSpanner\nChisel\nNail\nFLAN-XXL\nblindworm\ntortoise\nturtle\nsalamander\ntoadchameleon\ngecko\nlizard\naxe\nknife\nchisel\nsaw\nlawn_mower\nvacuum_cleaner\nanvil\ngrinding_disc\ncobra\nboa\nsnake\ndinosaur\ncaiman\nalligatorcrocodile\nnailspanner\nhammer\nscrewdriver\nshovel\noilcan\npaintbrush\ncrocodile\nnailtoad\npaintbrush\ndinosaur\nlawn_mower\nshovel\nknife\nchameleon\ngecko\nscrewdriver\ntortoise\ncaiman\nsnakeaxe boa\nanvil\ncobra\nspanner\nlizard\ngrinding_disc\nalligatorchisel\nblindwormsalamander\noilcan\nturtle\nvacuum_cleaner\nhammer\nsaw\nscrewdriver\nsaw\nsalamander\npaintbrush\noilcan\nnail\nlizard\nlawn_mower\nknife\nhammer\ngrinding_discgecko\ndinosaur\ncrocodilecobrachisel\nvacuum_cleaner\nturtle\ntortoise\ntoad\nspanner\nsnakeshovel\nchameleoncaiman\nboa\nblindworm\naxe\nalligator\nanvil\nFigure 2: Organization of conceptual representations estimated from Humans, GPT-3, and FLAN-XXL using\nfeature listing, triadic comparisons, and pairwise ratings. Grouping of concepts is based on hierarchical clustering\nof representations. Tools are shown in cooler colors and reptiles are in warmer colors.\nthe similarity between embeddings extracted from\nGPT 3, word2vec, and the language component of\nCLIP. While word2vec embeddings are a staple of\nNLP research, relatively fewer works have explored\nthe structure of the language models that are jointly\ntrained in the CLIP procedure.\nAfter computing the similarity structure between\nconcepts expressed by the NLP methods outlined\nabove, we considered (a) how well these estimates\naligned with structures estimated from human\nbehaviors within each task, and (b) the structural\ncoherence between the embeddings estimated via\ndifferent methods from LLM behavior.\n4.1 Methods\n4.1.1 Feature listing simulations\nTo simulate the feature-generation phase of the\nLeuven study, We queried GPT-3 with the prompt\n\"List the features of a [concept]\" and recorded the\nresponses (see Table 3). The model was queried\nwith a temperature of 0.7, meaning that responses\nwere somewhat stochastic so that the model\nproduced different responses from repetitions of\nthe same query. For each concept We repeated\nthe process five times and tabulated all responses\nacross these runs for each item. The responses\nwere transcribed into features by breaking down\nphrases or sentence into constituent predicates; for\ninstance, a response such as \"a tiger is covered in\nstripes\" was transcribed as \"has stripes.\" Where\nphrases included modifiers, these were transcribed\nseparately; for instance, a phrase like \"has a long\nneck\" was transcribed as two features, \"has neck\"\nand \"has long neck.\" Finally, alternate wordings\nfor the same property were treated as denoting a\nsingle feature; for instance, \"its claws are sharp\"\nand \"has razor-sharp claws\" would be coded as the\ntwo features \"has claws\" and \"has sharp claws.\" We\ndid not, however, collapse synonyms or otherwise\nreduce the feature set. This exercise generated a\ntotal of 580 unique features from the 30 items.\nTo simulate the feature verification phase of\nthe Leuven study, we then asked GPT to decide,\nfor each concept C and feature F, whether the\n727\nconcept possessed the feature. For instance, to\nassess whether the model \"thinks\" that alligators\nare ectothermic, we probed it with the prompt \"In\none word, Yes/No: Are alligators ectothermic?\"\n(temperature 0). Note that this procedure requires\nthe LLM to answer probes for every possible\nconcept/feature pair–for instance, does an alligator\nhave wheels? Does a car have a heart? etc.\nThese responses were used to flesh out the\noriginal feature-listing matrix: every cell where\nthe LLM affirmed that concept C had feature F\nwas filled with a 1, and cells where the LLM\nresponded \"no\" were filled with zeros. We refer\nto the resulting matrix as the verified feature\nmatrix. Before the feature verification process,\nthe concept by feature matrix was exceedingly\nsparse, containing 786 1’s (associations) and\n16614 0’s (no associations). After the verification\nprocess, the concept by feature matrix contained\n7845 1’s and 9555 0’s. Finally, we computed\npairwise cosine distances between all items based\non the verified feature vectors, and used classical\nmultidimensional scaling to reduce these to three-\ndimensional embeddings, exactly comparable to\nthe human study.\n4.1.2 Triadic comparison simulations\nTo simulate triplet judgment, we used the prompt\nshown in Figure 1 for each triplet, using the\nexact same set of triplets employed across all\nparticipants in the human study. We recorded\nthe model’s response for each triplet (see Table\n2) and from these data fit a 3D embedding using\nthe same algorithm and settings as the human\ndata. The resulting embedding predicted GPT-\n3 judgements for the held-out triplets at a 78 %\naccuracy, comparable to that observed in the human\nparticipants.\n4.1.3 Pairwise similarity simulations\nTo simulate the pairwise similarity task, we used\nthe prompt shown in Table 4 for all the possible\npairs of concepts (435 (\n(30\n2\n)\n)).\n4.2 Results\nHierarchical cluster plots for embeddings generated\nfrom the LLM’s feature lists, triadic judgements,\nand pairwise judgments are shown in the second\nand third rows of Figure 2, immediately below\nthe corresponding plots from human data. Most\napproaches reliably separate living and nonliving\nthings (although see the pairwise representations\nfor Flan-XXL to see a failure case). The verified\nfeature lists additionally yield within-domain\nstructure similar to that observed in human lists,\nwith all items relatively similar to one another,\nand with some subcategory structure apparent\n(e.g. turtle/tortoise, snake, crocodile). within-\ndomain structure estimated from triplet judgments,\nin contrast, looks very different.\nThese qualitative observations are borne out\nby the squared Procrustes correlations between\ndifferent embedding spaces, shown in Figure\n3. Similarities expressed in the space estimated\nfrom LLM verified feature lists capture 89% of\nthe variance in distances estimated from human\nfeature lists, 78% of the variance in those\nestimated from human triplet judgments, and 89%\nof the variance estimated from human pairwise\nsimilarities. Similarities expressed in the space\nestimated from LLM triplet judgments account for\n82% of the variance in distances estimated from\nhuman feature lists, 80% of the variance in those\nestimated from human triplet judgments, and 69%\nof the variance estimated from human pairwise\nsimilarities. Finally similarities expressed in the\nspace estimated from LLM Pairwise comparisons\naccount for 32% of the variance in distances\nestimated from human feature lists, 14% of the\nvariance in those estimated from human triplet\njudgments, and 51% of the variance estimated\nfrom human pairwise similarities. Similarities\nestimated from LLM pairwise comparisons, in\ncontrast to those estimated from LLM feature lists\nand LLM triplets, account for less than half the\nvariance in embeddings generated from human\njudgment. More interestingly, they account for\nless than half the variance in the embeddings\ngenerated from the LLM verified feature lists\nand LLM triplet judgements. Unlike the human\nembeddings, conceptual structures estimated from\ndifferent behaviors in the very same model do not\ncohere very well with each other.\nFigure 4 also shows the squared Procrustes\ncorrelations for semantic embeddings generated\nvia several other approaches including (a) the\nraw (unverified) feature lists produced by GPT-3,\n(b) the word embedding vectors extracted from\nGPT-3’s internal hidden unit activation patterns,\n(c) word embeddings from the popular word2vec\napproach, and (d) embeddings extracted from a\nCLIP model trained to connect images with their\nnatural language descriptions. None of these\n728\nFeature Listing Triplet Pairwise\nFeature ListingTripletPairwise\n1 0.96 0.84\n0.96 1 0.72\n0.84 0.72 1\nHumans\nFeature Listing Triplet Pairwise\n1 0.72 0.47\n0.72 1 0.25\n0.47 0.25 1\nGPT-3 (002)\n1\nFeature Listing Triplet Pairwise\n1 0.12 NA*\n0.12\nFLAN-XXL\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1\n1\nNA*\nNA* 0.12\n*All the pairwise \ncomparisons by FLAN-XXL\nwere the same so a \nProcrustes correlation \ncannot be calculated.\nFigure 3: Matrices showing conceptual coherence across testing methods for Humans, GPT-3, and FLAN-XXL.\nThe value in each cell corresponds to the squared Procustes pairwise correlation.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHuman Feature Listing\nFeature List\nTriadic\nPairwise\nEmbedding\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-3 (002) (ver)FLAN-XXL (ver)\nGPT-3 (002) (unver)GPT-3 (002) tripletFLAN-XXL TripletFLAN-UL2 Triplet\nGPT-3 (002) PairwiseFLAN-UL2 PairwiseGPT-3 (003) Pairwise\nWord2Vec\nGPT-3 embedding\nCLIP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 Human Pairwise Similarity\nR2\nHuman Triadic Comparison\nFigure 4: Human-machine alignment using human\nsemantic representations estimated from feature listing\n(top), triadic comparisons (middle), and pairwise ratings\n(bottom). Heights of bars correspond to squared\nProcrustes correlations.\napproaches accord with human-based embeddings\nas well as do the embeddings estimated from the\nLLM verified-feature lists, nor are the various\nstructures particularly coherent with one another.\nNo pair of LLM-estimated embeddings shows\nthe degree of coherence observed between the\nestimates derived from human judgments.\nIn general, while LLMs vary in their degree\nof human alignment with respect to conceptual\nstructure depending on the probing technique, the\ncritical finding is that they are not coherent within-\nthemselves across probing techniques. While\nthere might be ways to optimize human-machine\nconceptual alignment using in-context learning\n(Brown et al., 2020b; Chan et al., 2022) or\nspecialized prompting strategies (Wei et al., 2022)\nto the extent that the community wants to deploy\nLLMs to interact with humans in natural language\nand use them as cognitive models, it is crucial to\ncharacterize how stable the models’ concepts are\nwithout the use of specialized prompting.\n5 Conclusion\nIn this study, we compared the conceptual\nstructures of humans and LLMs using three\ncognitive tasks: a semantic feature-listing task, a\ntriplet similarity judgement task, and a pairwise\nrating task. Our results showed that the conceptual\nrepresentations generated from human judgments,\ndespite being estimated from quite different tasks,\nin different languages, across different cultures,\nwere remarkably coherent: similarities captured\nin one space accounted for 96% of the variance\nin the other. This suggests that the conceptual\nstructures underlying human semantic cognition\nare remarkably robust to differences in language,\ncultural background, and the nature of the task at\nhand.\nIn contrast, embeddings obtained from analo-\ngous behaviors in LLMs differed depending upon\non the task. While embeddings estimated from\nverified feature lists aligned moderately well with\nthose estimated from human feature norms, those\nestimated from triplet judgments or from the raw\n(unverified) feature lists did not, nor did the two\nembedding spaces from the LLM cohere well with\neach other. Embedding spaces extracted directly\nfrom model hidden representations or from other\ncommon neural network techniques did not fare\nbetter: in most comparisons, distances captured\nby one model-derived embedding space accounted\n729\nfor, at best, half the variance in any other. The\nsole exception was the space estimated from LLM-\nverified feature vectors, which cohered modestly\nwell with embeddings taken directly from the GPT-\n3 embeddings obtained using the triplet task (72%\nof the variance) and the hidden layer (66% of\nvariance)5.\nWhile recent advances in prompting techniques\nincluding chain-of-thought prompting (Wei et al.,\n2022), self-consistence (Wang et al., 2022), and\ntree-of-thoughts (Yao et al., 2023) have been shown\nto improve performance in tasks with veridical\nsolutions such as mathematical reasoning and\nknowledge retrieval, we highlight here through\nboth direct and indirect tasks that the underlying\nconceptual structure learned by LLMs is brittle.\nWe implemented chain-of-thought reasoning for\nsome models and found that this led to LLM\nmodel representations being more aligned with\nhuman conceptual structure (Fig 6). However,\nthe conceptual coherence within a model only\nincreased for only some of the models but\nstill nothing comparable to human conceptual\nrobustness.\nTogether these results suggest an important\ndifference between human cognition and current\nLLM models. Neuro-computational models of\nhuman semantic memory suggest that behavior\nacross many different tasks is undergirded by\na common conceptual \"core\" that is relatively\ninsulated from variations arising from different\ncontexts or tasks (Rogers et al., 2004; Jackson\net al., 2021). In contrast, representations of\nword meanings in large language models depend\nessentially upon the broader linguistic context.\nIndeed, in transformer architectures like GPT-\n3, each word vector is computed as a weighted\naverage of vectors from surrounding text, so it\nis unclear whether any word possesses meaning\noutside or independent of context. Because\nthis is so, the latent structures organizing its\novert behaviors may vary considerably depending\nupon the particular way the model’s behavior\nis probed. That is, the LLM may not have a\ncoherent conceptual \"core\" driving its behaviors,\nand for this reason, may organize its internal\nrepresentations quite differently with changes to\nthe task instruction or prompt. Context-sensitivity\nof this kind is precisely what grants such models\ntheir notable ability to simulate natural-seeming\nlanguage, but this same capacity may render them\nill-suited for understanding human conceptual\nrepresentation.\n6 Limitations\nWhile there are benefits to studying the coherence\nof a constrained set of concepts, as we have\ndone here, human semantic knowledge is vast and\ndiverse and covers many domains beyond tools\nand reptiles. While it was reasonable to conduct\nour experiments on 30 concepts split across these\ndomains both due to resource limitations and to\nlimit the concept categories to those that are largely\nfamiliar to most people, a larger scale study on\nlarger concept sets (Hebart et al., 2022; Devereux\net al., 2014; McRae et al., 2005) might reveal\na different degree of coherence in conceptual\nstructure across probing methods in LLMs.\nWhen conducting LLM simulations, we didn’t\nemploy any prompting technique like tree-of-\nthought(Yao et al., 2023), self-consistency(Wang\net al., 2022), etc.. While we think that the\npurpose of this work is to highlight the fragility\nof the conceptual ‘core’ of models as measured by\nincoherent representations across tasks, it remains\npossible that representations might cohere to a\ngreater extent using these techniques and might\nalign closer with human representations.\nFinally, Human semantic knowledge is the\nproduct of several sources of information including\nvisual, tactile, and auditory properties of the\nconcept. While LLMs can implicitly acquire\nknowledge about these modalities via the corpora\nthey are trained on, they are nevertheless bereft of\nmuch of the knowledge that humans are exposed\nto that might help them organize concepts into a\nmore coherent structure. In this view, difference in\nthe degree in conceptual coherence between LLMs\nand humans should not be surprising.\n7 Code\nHere’s the link to the repository containing the code\nand data we used to run our experiments.\nReferences\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors\nwith subword information. Transactions of the\nassociation for computational linguistics, 5:135–146.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\n730\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020b. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCurt Burgess. 1998. From simple associations to the\nbuilding blocks of language: Modeling meaning in\nmemory with the hal model. Behavior Research\nMethods, Instruments, & Computers, 30(2):188–198.\nStephanie CY Chan, Ishita Dasgupta, Junkyung Kim,\nDharshan Kumaran, Andrew K Lampinen, and Felix\nHill. 2022. Transformers generalize differently from\ninformation stored in context vs in weights. arXiv\npreprint arXiv:2210.05675.\nJonathan H Choi, Kristin E Hickman, Amy Monahan,\nand Daniel Schwarcz. 2023. Chatgpt goes to law\nschool. Available at SSRN.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Language\nmodels show human-like content effects on reasoning.\narXiv preprint arXiv:2207.07051.\nSimon De Deyne, Steven Verheyen, Eef Ameel, Wolf\nVanpaemel, Matthew J Dry, Wouter V oorspoels, and\nGert Storms. 2008. Exemplar by feature applicability\nmatrices and other dutch normative data for semantic\nconcepts. Behavior research methods , 40:1030–\n1048.\nScott Deerwester, Susan T Dumais, George W Furnas,\nThomas K Landauer, and Richard Harshman. 1990.\nIndexing by latent semantic analysis. Journal of the\nAmerican Society for Information Science, 41(6):391–\n407.\nDorottya Demszky, Diyi Yang, David S Yeager,\nChristopher J Bryan, Margarett Clapper, Susannah\nChandhok, Johannes C Eichstaedt, Cameron Hecht,\nJeremy Jamieson, Meghann Johnson, et al. 2023.\nUsing large language models in psychology. Nature\nReviews Psychology, pages 1–14.\nBarry J Devereux, Lorraine K Tyler, Jeroen Geertzen,\nand Billi Randall. 2014. The centre for speech,\nlanguage and the brain (cslb) concept property norms.\nBehavior research methods, 46:1119–1127.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805.\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt\nGray. 2023. Can ai language models replace human\nparticipants? Trends in Cognitive Sciences.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.\nGlm: General language model pretraining with\nautoregressive blank infilling. In Proceedings of\nthe 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 320–335.\nKatherine Elkins and Jon Chun. 2020. Can gpt-3 pass\na writer’s turing test? Journal of Cultural Analytics,\n5(2).\nJeffrey L Elman. 1990. Finding structure in time.\nCognitive science, 14(2):179–211.\nJeffrey L Elman. 2004. An alternative view of\nthe mental lexicon. Trends in cognitive sciences ,\n8(7):301–306.\nJL Elman. 1991. Distributed representations, simple\nrecurrent networks, and grammatical structure.\nMachine learning, 7(2-3):195–225.\nJohn Firth. 1957. A synopsis of linguistic theory, 1930-\n1955. Studies in linguistic analysis, pages 10–32.\nJohn C Gower. 1975. Generalized procrustes analysis.\nPsychometrika, 40:33–51.\nGabriel Grand, Idan Asher Blank, Francisco Pereira,\nand Evelina Fedorenko. 2022. Semantic projection\nrecovers rich human knowledge of multiple object\nfeatures from word embeddings. Nature human\nbehaviour, 6(7):975–987.\nHannes Hansen and Martin N Hebart. 2022. Semantic\nfeatures of object concepts generated with gpt-3.\narXiv preprint arXiv:2202.03753.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nMartin N Hebart, Oliver Contier, Lina Teichmann,\nAdam Rockter, Charles Y Zheng, Alexis Kidder,\nAnna Corriveau, Maryam Vaziri-Pashkam, and\nChris I Baker. 2022. Things-data: A multimodal\ncollection of large-scale datasets for investigating\nobject representations in brain and behavior. bioRxiv,\npages 2022–07.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. Training compute-\noptimal large language models. arXiv preprint\narXiv:2203.15556.\n731\nAdam N Hornsby and Bradley C Love. 2020. How\ndecisions and the desire for coherency shape\nsubjective preferences over time. Cognition,\n200:104244.\nRebecca L Jackson, Timothy T Rogers, and Matthew A\nLambon Ralph. 2021. Reverse-engineering\nthe cortical architecture for controlled semantic\ncognition. Nature human behaviour, 5(6):774–786.\nKevin G Jamieson, Lalit Jain, Chris Fernandez,\nNicholas J Glattard, and Robert D Nowak. 2015.\nNext: A system for real-world development,\nevaluation, and application of active learning. In\nNIPS, pages 2656–2664. Citeseer.\nJoseph B Kruskal and Myron Wish. 1978. Multidimen-\nsional scaling, volume 11. Sage.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla,\nCzarina Sillos, Lorie De Leon, Camille Elepaño,\nMaria Madriaga, Rimel Aggabao, Giezel Diaz-\nCandido, James Maningo, et al. 2023. Performance\nof chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models. PLoS digital\nhealth, 2(2):e0000198.\nThomas K Landauer, Peter W Foltz, and Darrell Laham.\n1998. An introduction to latent semantic analysis.\nDiscourse processes, 25(2-3):259–284.\nGary Lupyan and Molly Lewis. 2019. From words-as-\nmappings to words-as-cues: The role of language\nin semantic knowledge. Language, Cognition and\nNeuroscience, 34(10):1319–1337.\nRaja Marjieh, Ilia Sucholutsky, Theodore R Sumers,\nNori Jacoby, and Thomas L Griffiths. 2022a.\nPredicting human similarity judgments using large\nlanguage models. arXiv preprint arXiv:2202.04728.\nRaja Marjieh, Ilia Sucholutsky, Pol van Rijn, Nori\nJacoby, and Thomas L Griffiths. 2023. What\nlanguage reveals about perception: Distilling\npsychophysical knowledge from large language\nmodels. arXiv preprint arXiv:2302.01308.\nRaja Marjieh, Pol van Rijn, Ilia Sucholutsky,\nTheodore R Sumers, Harin Lee, Thomas L Griffiths,\nand Nori Jacoby. 2022b. Words are all you need?\ncapturing human sensory similarity with textual\ndescriptors. arXiv preprint arXiv:2206.04105.\nJL McClelland, DE Rumelhart, and the PDP\nResearch Group. 1990. The development of\ndistributed representations for words. Explorations\nin parallel distributed processing: a handbook of\nmodels, programs, and exercises, pages 77–109.\nKen McRae, George S Cree, Mark S Seidenberg, and\nChris McNorgan. 2005. Semantic feature production\nnorms for a large set of living and nonliving things.\nBehavior research methods, 37(4):547.\nTomas Mikolov, Kai Chen, Greg Corrado, and\nJeffrey Dean. 2013a. Efficient estimation of word\nrepresentations in vector space. In International\nconference on machine learning, pages 1188–1196.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S\nCorrado, and Jeff Dean. 2013b. Distributed\nrepresentations of words and phrases and their\ncompositionality. Advances in neural information\nprocessing systems, pages 3111–3119.\nKushin Mukherjee, Siddharth Suresh, and Timothy T\nRogers. 2023. Human-machine cooperation\nfor semantic feature listing. arXiv preprint\narXiv:2304.05012.\nPhilip Mark Newton and Maira Xiromeriti. 2023.\nChatgpt performance on mcq exams in higher\neducation. a pragmatic scoping review.\nOpenAI. Year of the webpage, e.g., 2023. GPT-4: A\npowerful language model. Accessed on: Date of\nyour access, e.g., September 27, 2023.\nCharles E Osgood. 1952. The nature and measurement\nof meaning. Psychological bulletin, 49(3):197.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\net al. 2022. Training language models to follow\ninstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nTimothy T Rogers, James L McClelland, et al. 2004.\nSemantic cognition: A parallel distributed processing\napproach. MIT press.\nEleanor Rosch. 1975. Cognitive representations\nof semantic categories. Journal of experimental\npsychology: General, 104(3):192.\nEleanor H Rosch. 1973. On the internal structure of\nperceptual and semantic categories. In Cognitive\ndevelopment and acquisition of language, pages 111–\n144. Elsevier.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams. 1986. Learning representations by back-\npropagating errors. nature, 323(6088):533–536.\nSiddharth Suresh, Kushin Mukherjee, and Timothy T\nRogers. 2023. Semantic feature verification in flan-t5.\narXiv preprint arXiv:2304.05591.\nOmer Tamuz, Ce Liu, Serge Belongie, Ohad Shamir,\nand Adam Tauman Kalai. 2011. Adaptively learning\nthe crowd kernel. arXiv preprint arXiv:1105.1033.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023. Alpaca:\nA strong, replicable instruction-following model.\nStanford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\n732\nBill Thompson, Seán G Roberts, and Gary Lupyan.\n2020. Cultural influences on word meanings revealed\nthrough large-scale semantic alignment. Nature\nHuman Behaviour, 4(10):1029–1038.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and\nefficient foundation language models. arXiv preprint\narXiv:2302.13971.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. 2021. Finetuned\nlanguage models are zero-shot learners. arXiv\npreprint arXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\n733\nA Appendix\nTable 1: Concepts used in the experiment\nConcepts Categories\n15*Reptiles Turtle\nAlligator\nLizard\nTortoise\nCobra\nSnake\nBlindworm\nGecko\nBoa python\nToad\nCrocodile\nChameleon\nCaiman\nSalamander\nDinosaur\n15*Tools Hammer\nScrewdriver\nGrinding disc\nVacuum cleaner\nSpanner\nLawn mower\nAxe\nSaw\nKnife\nNail\nChisel\nShovel\nAnvil\nOilcan\nPaint brush\nHead Option 1 Option 2 FLAN-T5-XXL FLAN-UL2 davinci-002 davinci-003\nShovel Alligator Spanner Alligator Alligator Spanner Spanner\nAnvil Caiman Tortoise Tortoise Caiman Caiman Caiman\nNail Boa python Snake Snake Snake Boa Python Boa python\nPaint brush Chisel Toad Chisel Toad Chisel Chisel\nShovel Caiman Crocodile Crocodile Dangerous Crocodile Crocodile\nTable 2: Example responses to triplet judgement task by different models.\n734\nConcepts Prompt Features\nAlligator List all features of an Alligator.have tail, can stay underwater, have tough skin ...\nAnvil List all features of an Anvil. have wings, have hammer, have hole ...\nAxe List all features of an Axe. have blade, can be used for chopping wood, are a tool ...\nBlindworm List all features of a Blindworm.have no legs, can smell with tongue, are small ...\nBoa python List all features of a Boa python.have pits, can be green, are dimorphic ...\nCaiman List all features of a Caiman.have short body,can swim, are reptile ...\nChameleon List all features of a Chameleon.have tongue, can change color, are a good swimmer ...\nChisel List all features of a Chisel. have a blade, can cut material, are hand held ...\nCobra List all features of a Cobra. have long body, can inject venom, are carnivorous ...\nCrocodile List all features of a Crocodile.have teeth, can breathe air, are a swimmer ...\nDinosaur List all features of a Dinosaur.have claws, can lay eggs, are reptiles ...\nGecko List all features of a Gecko. have tail, can stick to surfaces, are nocturnal ...\nGrinding diskList all features of a Grinding disk.have diameter, can be used for grinding. are abrasive ...\nHammer List all features of a Hammer.have handle, can be used for pounding nails, are a tool ...\nKnife List all features of a Knife. have blade , can cut things, are sharp ...\nLawn MowerList all features of a Lawn Mower.have engine, can mulch, are powered by gas ...\nLizard List all features of a Lizard. have legs,can climb, are carnivores ...\nNail List all features of a Nail. have covering, have point, are metal ...\nOil can List all features of an Oil can.have a spout, can pour, are used to hold oil ...\nPaint brush List all features of a Paint brush.have handle, can be used for painting, are a tool ...\nSalamander List all features of a Salamander.have tail, can be a variety of colors, are an amphibian ...\nSaw List all features of a Saw. have handle, can be used to cut lumber, are a tool ...\nScrewdriver List all features of a Screwdriver.have a handle, have a tip, have a cap ...\nShovel List all features of a Shovel. have point, have blade, have handle ...\nSnake List all features of a Snake. have long body, can be dangerous to humans, are carnivorous ...\nSpanner List all features of a Spanner.have different tips, can grip a bolt, are a tool ...\nToad List all features of a Toad. have glands, can jump, are good jumpers ...\nTortoise List all features of a Tortoise.have shell, can live long, are cold blooded ...\nTurtle List all features of a Turtle. have shell, can swim, are reptiles ...\nVacuum List all features of a Vacuum.have nozzle, can suck up dirt, have filter ...\nTable 3: Examples of features produced by GPT-3.\n735\nHuman norms\nHuman triplet\nHuman Pairwise\nGPT-3(002)(ver)\nGPT-3(002) triplet\nGPT-3(002) Pairwise\nGPT-3(002) triplet\nGPT-3(003) Pairwise\nFLAN-XXL(ver)\nFLAN-XXL Triplet\nFLAN-UL2 Triplet\nFLAN-UL2 Pairwise\nGPT-3(002)(unver)\nGPT embedding\nWord2Vec\nCLIP\nHuman norms\nHuman triplet\nHuman Pairwise\nGPT-3(002)(ver)\nGPT-3(002) triplet\nGPT-3(002) Pairwise\nGPT-3(002) triplet\nGPT-3(003) Pairwise\nFLAN-XXL(ver)\nFLAN-XXL Triplet\nFLAN-UL2 Triplet\nFLAN-UL2 Pairwise\nGPT-3(002)(unver)\nGPT embedding\nWord2Vec\nCLIP\n1 0.96 0.84 0.89 0.82 0.32 0.82 0.24 0.88 0.099 0.0086 0.25 0.29 0.49 0.63 0.2\n0.96 1 0.72 0.78 0.8 0.2 0.8 0.14 0.77 0.078 0.0021 0.13 0.17 0.33 0.57 0.12\n0.84 0.72 1 0.88 0.69 0.51 0.69 0.42 0.89 0.14 0.039 0.49 0.53 0.71 0.61 0.33\n0.89 0.78 0.88 1 0.72 0.47 0.72 0.38 0.93 0.12 0.026 0.42 0.48 0.66 0.6 0.28\n0.82 0.8 0.69 0.72 1 0.25 1 0.2 0.73 0.12 0.0094 0.18 0.21 0.39 0.57 0.2\n0.32 0.2 0.51 0.47 0.25 1 0.25 0.64 0.5 0.11 0.099 0.65 0.63 0.67 0.27 0.4\n0.82 0.8 0.69 0.72 1 0.25 1 0.2 0.73 0.12 0.0094 0.18 0.21 0.39 0.57 0.2\n0.24 0.14 0.42 0.38 0.2 0.64 0.2 1 0.4 0.098 0.081 0.55 0.55 0.59 0.26 0.39\n0.88 0.77 0.89 0.93 0.73 0.5 0.73 0.4 1 0.12 0.023 0.46 0.54 0.68 0.62 0.32\n0.099 0.078 0.14 0.12 0.12 0.11 0.12 0.098 0.12 1 0.059 0.13 0.098 0.1 0.085 0.069\n0.00860.0021 0.039 0.026 0.0094 0.099 0.0094 0.081 0.023 0.059 1 0.13 0.08 0.083 0.036 0.054\n0.25 0.13 0.49 0.42 0.18 0.65 0.18 0.55 0.46 0.13 0.13 1 0.79 0.74 0.24 0.42\n0.29 0.17 0.53 0.48 0.21 0.63 0.21 0.55 0.54 0.098 0.08 0.79 1 0.75 0.23 0.42\n0.49 0.33 0.71 0.66 0.39 0.67 0.39 0.59 0.68 0.1 0.083 0.74 0.75 1 0.43 0.44\n0.63 0.57 0.61 0.6 0.57 0.27 0.57 0.26 0.62 0.085 0.036 0.24 0.23 0.43 1 0.19\n0.2 0.12 0.33 0.28 0.2 0.4 0.2 0.39 0.32 0.069 0.054 0.42 0.42 0.44 0.19 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 5: A correlation matrix showing square of the Procrustes correlations between semantic representations\nestimated from different models across different tasks.\n736\nHuman norms\nHuman triplet\nHuman Pairwise\nGPT3(002) COT Triplet\nGPT3(002) COT Pairwise\nGPT3(003) COT Triplet\nGPT3(003) COT Pairwise\nFLAN-UL2 COT Triplet\nFLAN-UL2 COT Pairwise\nHuman norms\nHuman triplet\nHuman Pairwise\nGPT3(002) COT Triplet\nGPT3(002) COT Pairwise\nGPT3(003) COT Triplet\nGPT3(003) COT Pairwise\nFLAN-UL2 COT Triplet\nFLAN-UL2 COT Pairwise\n1 0.96 0.84 0.91 0.24 0.94 0.16 0.87 0.3\n0.96 1 0.72 0.91 0.14 0.97 0.074 0.85 0.18\n0.84 0.72 1 0.73 0.45 0.74 0.37 0.75 0.54\n0.91 0.91 0.73 1 0.17 0.9 0.093 0.88 0.22\n0.24 0.14 0.45 0.17 1 0.16 0.61 0.17 0.61\n0.94 0.97 0.74 0.9 0.16 1 0.098 0.85 0.21\n0.16 0.074 0.37 0.093 0.61 0.098 1 0.14 0.57\n0.87 0.85 0.75 0.88 0.17 0.85 0.14 1 0.27\n0.3 0.18 0.54 0.22 0.61 0.21 0.57 0.27 1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 6: A correlation matrix showing square of the Procrustes correlations between semantic representations\nestimated from different models across different tasks using only Chain of Thought prompting.\n737\nHumans LLMs\nFeature generation This bundle contains up to 10 sheets\nwith a word written on top of the\npage. We would like you to write\ndown preferably 10 features underneath\nthe word. Try to give different sorts\nof features, such as, for example,\nphysical or perceptual features (what\nit looks like, how it smells, how it\ntastes, ...), functional features (what it\nis used for, when and where it is used,\n...), background information (where it\ncomes from, some historical facts, ...),\netc. ((De Deyne et al., 2008))\nList all the properties of {concept1}\nFeature verification The participants were instructed to\njudge, for every feature-exemplar pair,\nwhether the feature characterizes the\nexemplar, and to write down a 1 or a 0\nin the corresponding matrix entry. (De\nDeyne et al., 2008)\nIn one word, Yes/No : Are {concept1}\n{property1} (or) In one word, Yes/No :\nDo {concept1} have {property1}/\nTriplet task Which of the bottom two words is more\nsimilar to the word at the top?\nAnswer using only one word - {con-\ncept1} or {concept2} and not {anchor}.\nWhich is more similar in meaning to\n{anchor}?\"\nPairwise similarity On a scale of 1-7, how similar is a\n{concept1} to a {concept2}?\nAnswer with only one number from\n1 to 7, considering 1 as ’extremely\ndissimilar’, 2 as ’very dissimilar’, 3 as\n’likely dissimilar’, 4 as ’neutral’, 5 as\n’likely similar’, 6 as ’very similar’, and\n7 as ’extremely similar’: How similar is\n{concept1} and {concept2}?\nTable 4: Prompts used across the three tasks for humans and LLMs\n738"
}