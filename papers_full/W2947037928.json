{
  "title": "Pre-training of Graph Augmented Transformers for Medication Recommendation",
  "url": "https://openalex.org/W2947037928",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4227503016",
      "name": "Shang, Junyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2233630775",
      "name": "Ma Tengfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122836570",
      "name": "Xiao Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350181399",
      "name": "Sun, Jimeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963208729",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2141173017",
    "https://openalex.org/W2742491462",
    "https://openalex.org/W2744140371",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2557074642",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2799690436",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2964068143"
  ],
  "abstract": "Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.",
  "full_text": "Pre-training of Graph Augmented Transformers for Medication Recommendation\nJunyuan Shang1,3 , Tengfei Ma2 , Cao Xiao1 and Jimeng Sun3\n1Analytics Center of Excellence, IQVIA, Cambridge, MA, USA\n2IBM Research AI, Yorktown Heights, NY , USA\n3Georgia Institute of Technology, Atlanta, GA, USA\njunyuan.shang@iqvia.com, Tengfei.Ma1@ibm.com, cao.xiao@iqvia.com, jsun@cc.gatech.edu\nAbstract\nMedication recommendation is an important\nhealthcare application. It is commonly formulated\nas a temporal prediction task. Hence, most existing\nworks only utilize longitudinal electronic health\nrecords (EHRs) from a small number of patients\nwith multiple visits ignoring a large number\nof patients with a single visit ( selection bias ).\nMoreover, important hierarchical knowledge such\nas diagnosis hierarchy is not leveraged in the\nrepresentation learning process. To address these\nchallenges, we propose G-BERT, a new model\nto combine the power of G raph Neural Networks\n(GNNs) and BERT (Bidirectional Encoder Rep-\nresentations from Transformers) for medical code\nrepresentation and medication recommendation.\nWe use GNNs to represent the internal hierarchical\nstructures of medical codes. Then we integrate\nthe GNN representation into a transformer-based\nvisit encoder and pre-train it on EHR data from\npatients only with a single visit. The pre-trained\nvisit encoder and representation are then ﬁne-tuned\nfor downstream predictive tasks on longitudinal\nEHRs from patients with multiple visits. G-BERT\nis the ﬁrst to bring the language model pre-training\nschema into the healthcare domain and it achieved\nstate-of-the-art performance on the medication\nrecommendation task.\n1 Introduction\nThe availability of massive electronic health records (EHR)\ndata and the advances of deep learning technologies\nhave provided unprecedented resource and opportunity\nfor predictive healthcare, including the computational\nmedication recommendation task. A number of deep\nlearning models were proposed to assist doctors in mak-\ning medication recommendation [Xiao et al. , 2018a;\nShang et al. , 2019; Baytas et al. , 2017; Choi et al. , 2018;\nMa et al., 2018]. They often learn representations for medical\nentities (e.g., patients, diagnosis, medications) from patient\nEHR data, and then use the learned representations to predict\nmedications that are suited to the patient’s health condition.\nHeart failure\nCONGESTIVE \nHEART FAILURE\nOther forms of \nheart disease\n1\n2\n3\n4 5\nindex icd-9 description\n1\n2\n3\n4\n5\nroot\n428.0\n428\n420-429\n428.1\nLEFT \nHEART FAILURE\nroot\nFigure 1: Graphical illustration of ICD-9 Ontology.\nTo provide effective medication recommendation, it is im-\nportant to learn accurate representation of medical codes. De-\nspite that various considerations were handled in previous\nworks for improving medical code representations[Ma et al.,\n2018; Baytas et al., 2017; Choi et al., 2018], there are two\nlimitations with the existing work:\n1. Selection bias: Data that do not meet training data crite-\nria are discarded before model training. For example, a\nlarge number of patients who only have one hospital visit\nwere discarded from training in [Shang et al., 2019].\n2. Lack of hierarchical knowledge: For medical knowl-\nedge such as diagnosis code ontology (Figure. 1), their\ninternal hierarchical structures were rarely embedded in\ntheir original graph form when incorporated into repre-\nsentation learning.\nTo mitigate the aforementioned limitations, we propose\nG-BERT that combines the pre-training techniques and\ngraph neural networks for better medical code representation\nand medication recommendation. G-BERT is enabled and\ndemonstrated by the following technical contributions:\n1. Pre-training to leverage more data: Pre-training tech-\nniques, such as ELMo [Peters et al. , 2018 ], OpenAI\nGPT [Radford et al. , 2018 ] and BERT [Devlin et al. ,\n2018], have demonstrated a notably good performance\nin various natural language processing tasks. These\ntechniques generally train language models from unla-\nbeled data, and then adapt the derived representations to\ndifferent tasks by either feature-based (e.g. ELMo) or\nﬁne-tuning (e.g. OpenAI GPT, BERT) methods. We de-\nveloped a new pre-training method based on BERT for\narXiv:1906.00346v2  [cs.AI]  27 Nov 2019\npre-training on each visit of EHR so that the data with\nonly one hospital visit can also be utilized. We revised\nBERT to ﬁt EHR data in both input and pre-training ob-\njectives. To our best knowledge, G-BERT is the ﬁrst\nmodel that leverages Transformers and language model\npre-training techniques in healthcare domain. Compared\nwith other supervised models, G-BERT can utilize dis-\ncarded/unlabeled data more efﬁciently.\n2. Medical ontology embedding with graph neural net-\nworks: We enhance the representation of medical codes\nvia learning medical ontology embedding for each med-\nical codes with graph neural networks. We then in-\nput the ontology embedding into a multi-layer Trans-\nformer [Vaswani et al. , 2017 ] for BERT-style pre-\ntraining and ﬁne-tuning.\n2 Related Work\nMedication Recommendation. Medication Recom-\nmendation can be categorized into instance-based and\nlongitudinal recommendation methods [Shang et al., 2019].\nInstance-based methods focus on current health conditions.\nAmong them, Leap [Zhang et al. , 2017 ] formulates a\nmulti-instance multi-label learning framework and pro-\nposes a variant of sequence-to-sequence model based on\ncontent-attention mechanism to predict combination of\nmedicines given patient’s diagnoses. Longitudinal-based\nmethods leverage the temporal dependencies among clin-\nical events, see [Choi et al. , 2016; Xiao et al. , 2018b;\nLipton et al. , 2015 ]. Among them, RETAIN [Choi et\nal., 2016 ] uses a two-level neural attention model to de-\ntect inﬂuential past visits and signiﬁcant clinical variables\nwithin those visits for improved medication recommendation.\nPre-training Techniques. The goal of pre-training tech-\nniques is to provide model training with good initializations.\nPre-training has been shown extremely effective in various\nareas such as image classiﬁcation [Hinton et al. , 2006 ]\nand machine translation [Ramachandran et al. , 2016 ].\nThe unsupervised pre-training can be considered as a\nregularizer that supports better generalization from the\ntraining dataset [Erhan et al. , 2010 ]. Recently, language\nmodel pre-training techniques such as [Peters et al. , 2018;\nRadford et al. , 2018; Devlin et al. , 2018 ] have shown to\nlargely improve the performance on multiple NLP tasks. As\nthe most widely used one, BERT [Devlin et al., 2018] builds\non the Transformer [Vaswani et al., 2017 ] architecture and\nimproves the pre-training using a masked language model\nfor bidirectional representation. In this paper, we adapt the\nframework of BERT and pre-train our model on each visit of\nthe EHR data to leverage the single-visit data that were not\nﬁt for training in other medication recommendation models.\nGraph Neural Networks (GNN). GNNs are neural net-\nworks that learn node or graph representations from graph-\nstructured data. Various graph neural networks have been\nproposed to encode the graph-structure information, includ-\ning graph convolutional neural networks (GCN) [Kipf and\nWelling, 2017], message passing networks (MPNN) [Gilmer\net al., 2017], graph attention networks (GAT) [Velickovic et\nal., 2017 ]. GNNs have already been demonstrated useful\non EHR modeling [Choi et al. , 2017; Shang et al. , 2019 ].\nGRAM [Choi et al., 2017] represented a medical concept as\na combination of its ancestors in the medical ontology using\nan attention mechanism. It’s different from G-BERT from\ntwo aspects as described in Section 4.2. Another work worth\nmentioning is GAMENet [Shang et al. , 2019 ], which also\nused graph neural network to assist the medication recom-\nmendation task. However, GAMENet has a different moti-\nvation which results in using graph neural networks on drug-\ndrug-interaction graphs instead of medical ontology.\n3 Problem Formalization\nDeﬁnition 1 (Longitudinal Patient Records). In lon-\ngitudinal EHR data, each patient can be represented\nas a sequence of multivariate observations: X(n) =\n{X(n)\n1 ,X(n)\n2 ,··· ,X(n)\nT(n) }where n∈{1,2,...,N }, N is the\ntotal number of patients; T(n) is the number of visits of the\nnth patient. Here we choose two main medical code to rep-\nresent each visit Xt = Ct\nd ∪Ct\nm of a patient which is a union\nset of corresponding diagnoses codes Ct\nd ⊂Cd and medica-\ntions codes Ct\nm ⊂Cm. For simplicity, we use Ct\n∗to indicate\nthe uniﬁed deﬁnition for different type of medical codes and\ndrop the superscript (n) for a single patient whenever it is un-\nambiguous. C∗denotes the medical code set and |C∗|the size\nof the code set. c∗∈C∗is the medical code.\nDeﬁnition 2 (Medical Ontology). Medical codes are usu-\nally categorized according to a tree-structured classiﬁcation\nsystem such as ICD-9 ontoloy for diagnosis and ATC ontol-\nogy for medication. We use Od,Om to denote the ontology\nfor diagnosis and medication. Similarly, we use O∗to indi-\ncate the uniﬁed deﬁnition for different type of medical codes.\nIn detial, O∗= C∗∪C∗where C∗denotes the codes excluding\nleaf codes. For simplicity, we deﬁne two functionpa(·),ch(·)\nwhich accept target medical code and return ancestors’ code\nset and direct child code set.\nProblem Deﬁnition (Medication Recommendation).\nGiven diagnosis codes Ct\nd of the visit at time t, patient\nhistory X1:t = {X1,X2,··· ,Xt−1}, we want to recommend\nmultiple medications by generating multi-label output\nˆyt ∈{0,1}|Cm|.\n4 Method\nThe overall framework of G-BERT is described in Figure 2.\nG-BERT ﬁrst derives the initial embedding of medical codes\nfrom medical ontology using graph neural networks. Then, in\norder to fully utilize the rich EHR data, G-BERT constructs\nan adaptive BERT model on the discarded single-visit data\nfor visit representation. Finally we add a prediction layer and\nﬁne-tune the model in the medication recommendation task.\nIn the following we will describe G-BERT in detail. But\nﬁrstly, we give a brief background of BERT especially for\nthe two pre-training objectives which will be later adapted to\nEHR data in Section 4.3.\nOntology Tree BERT\n……\nVisit \nEmbedding\nOntology \nEmbedding\nFine-tuned\nClassiﬁer\nSigmoid \nOutput\n...\nroot\nleaf\nroot\nleaf\nICD9\nATC\nŏ\nŏ\n[CLS]\nŏ\nŏ\n[CLS]\nŏ\nŏ\nhd\nhm\nod\nom\ncd\ncm\nFigure 2: The framework of G-BERT. It consists of three main parts: ontology embedding, BERT and ﬁne-tuned classiﬁer. Firstly, we\nderive ontology embedding for medical code laid in leaf nodes by cooperating ancestors information by Eq. 1 and 2 based on graph attention\nnetworks (Eq. 3, 4). Then we input set of diagnosis and medication ontology embedding separately to shared weight BERT which is pre-\ntrained using Eq. 6, 7, 8. Finally, we concatenate the mean of all previous visit embeddings and the last visit embedding as input and ﬁne-tune\nthe prediction layers using Eq. 10 for medication recommendation tasks.\nNotation Description\nX(n) longitudinal observations for n-th patient\nCd,Cm diagnoses and medications codes set\nOd,Om diagnoses and medications codes ontology\nC∗⊂O∗ non-leaf medical codes of type∗\nc∗∈O∗ single medical code of type∗\npa(c∗) function retrievec∗’ ancestors’ code\nch(c∗) function retrievec∗’ direct children’s code\nWe ∈R|O∗|×d initial medical embedding matrix inO∗\nHe ∈R|O∗|×d enhanced medical embeddings instage 1\noc∗ ∈Rd ontology embedding instage 2\nαk\ni,j k-th attention between nodesi,j\nWk ∈Rd×d k-th weight matrix applied to each node\ng(·,·,·) graph aggregator function\nvt\n∗ t-th visit embedding of type∗\nˆyt ∈{0,1}|C∗| multi-label prediction\nTable 1: Notations used in G-BERT.\n4.1 Background of BERT\nBased on a multi-layer Transformer encoder [Vaswani et al.,\n2017] (The transformer architecture has been ubiquitously\nused in many sequence modeling tasks recently, so we will\nnot introduce the details here), BERT is pre-trained using two\nunsupervised tasks:\n•Masked Language Model. Instead of predicting words\nbased on previous words, BERT randomly selects words\nto mask out and then predicts the original vocabulary IDs\nof the masked words from their (bidirectional) context.\n•Next Sentence Prediction. Many of BERT’s downstream\ntasks are predicting the relationships of two sentences,\nthus in the pre-training phase, BERT has am a binary\nsentence prediction task to predict whether one sentence\nis the next sentence of the other.\nA typical input to BERT is as follows ([Devlin et al., 2018]):\nInput = [CLS] the man went to [MASK] store\n[SEP] he bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nwhere [CLS] is the ﬁrst token of each sentence pair to repre-\nsent the special classiﬁcation embedding, i.e. the ﬁnal state\nof this token is used as the aggregated sequence represen-\ntation for classiﬁcation tasks; [SEP] is used to separate two\nsentences; [MASK] is used to mask out the predicted words\nin the masked language model. Using this form, these inputs\nfacilitate the two tasks described above, and they will also be\nused in our method description in the following section.\n4.2 Input Representation\nThe G-BERT model takes medical codes’ ontology em-\nbeddings as input, and obtains intermediate representations\nfrom a Transformer encoder as the visit embeddings. It is\nthen pre-trained on EHR from patients who only have one\nhospital visit. The derived encoder and visit embedding\nwill be fed into a classiﬁer and ﬁne-tuned to make predictions.\nOntology Embedding\nWe constructed ontology embedding from diagnosis ontol-\nogy Od and medication ontology Om. Since the medical\ncodes in raw EHR data can be considered as leaf nodes\nin these ontology trees, we can enhance the medical code\nembedding using graph neural networks (GNNs) to integrate\nthe ancestors’ information of these codes. Here we perform\na two-stage procedure with a specially designed GNN for\nontology embedding.\nTo start, we assign an initial embedding vector to every\nmedical code c∗ ∈ O∗ with a learnable embedding matrix\nWe ∈R|O∗|×d where dis the embedding dimension.\nStage 1. For each non-leaf node c∗∈C∗, we obtain its en-\nhanced medical embedding hc∗ ∈Rd as follows:\nhc∗ = g(c∗,ch(c∗),We) (1)\nwhere g(·,·,·) is an aggregation function which accepts\nthe target medical code c∗, its direct child codes ch(c∗)\nand initial embedding matrix. Intuitively, the aggregation\nfunction can pass and fuse information in target node from its\ndirect children which result in the more related embedding of\nancestor’ code to child codes’ embedding.\nStage 2. After obtaining enhanced embeddings, we pass the\nenhance embedding matrix He ∈R|O∗|×d back to get ontol-\nogy embedding for leaf codes c∗∈C∗as follows:\noc∗ = g(c∗,pa(c∗),He) (2)\nwhere g(·,·,·) accepts ancestor codes of target medical code\nc∗. Here, we use pa(c∗) instead of ch(c∗), since utilizing\nthe ancestors’ embedding can indirectly associate all medical\ncodes instead of taking each leaf code as independent input.\nThe option for the aggregation function g(·,·,·) is ﬂexible,\nincluding sum, mean. Here we choose the one from graph\nattention networks (GAT) [Velickovic et al. , 2017 ], which\nhas shown efﬁcient embedding learning ability on graph-\nstructured tasks, e.g., node classiﬁcation and link prediction.\nIn particular, we implement the aggregation function g(·,·,·)\nas follows (taking stage 2 for an example):\ng(c∗,pa(c∗),He) =\nK\n∥\nk=1\nσ\n\n ∑\nj∈Nc∗\nαk\nc∗,jWkhj\n\n (3)\nwhere N∗is the neighboring nodes of c∗including c∗itself\nwhich depends on what stage we are on. Since we are on the\nstage 2 now, N∗ = {{c∗}∪ pa(c∗)}(likewise for stage 1,\nN∗ = {{c∗}∪ ch(c∗)}), ∥represents concatenation which\nenables the multi-head attention mechanism, σ is a nonlin-\near activation function, Wk ∈Rm×d is the weight matrix\nfor input transformation, and αk\nc∗,j are the corresponding k-\nth normalized attention coefﬁcients computed as follows:\nαk\nc∗,j = exp\n(\nLeakyReLU(a⊺[Wkhc∗||Wkhj])\n)\n∑\nk∈Nc∗\nexp (LeakyReLU(a⊺[Wkhc∗||Wkhk]))\n(4)\nwhere a ∈R2m is a learnable weight vector and LeakyReLU\nis a nonlinear function. (we assume m= d/K).\nAs shown in Figure 2, we construct ICD-9 tree for diag-\nnosis and ATC tree for medication using the same structure.\nHere the direction of arrow shows the information ﬂow where\nancestor nodes can get information from their direct children\n(in stage 1 ) and similarly leaf nodes can get information\nfrom their connected ancestors (in stage 2).\nIt is worth mentioning that our graph embedding method\non medical ontology is different from GRAM [Choi et al.,\n2017] from the following two aspects:\nBERT Visit \nEmbedding\nŏ\nŏ\nŏ\nŏ\nMasking\n……\n[CLS]\n[CLS]\nod\nom\n[MASK]\n[MASK]\n……\nPre-training\nTasks\ndual-prediction task\nself-prediction task\nself-prediction task\nFigure 3: Graphical illustration of pre-training procedure. We ﬁrstly\nrandomly mask the input medical codes using a [MASK] symbol.\nOrange arrow: self-prediction task takes vm or vd as input to re-\nstore the original medical codes with the same type. Green arrow:\ndual-prediction task takes one type of visit embedding such as vm\nor vd and tries to predict the other type of medical codes.\n1. Initialization: we initialize all the node embeddings\nfrom a learnable embedding matrix, while GRAM learns\nthem using Glove from the co-occurrence information.\n2. Updating: we develop a two-step updating function for\nboth leaf nodes and ancestor nodes; while in GRAM,\nonly the leaf nodes are updated (as a combination of\ntheir ancestor nodes and themselves).\nVisit Embedding\nSimilar to BERT, we use a multi-layer Transformer architec-\nture [Vaswani et al., 2017] as our visit encoder. The model\ntakes the ontology embedding as input and derive visit em-\nbedding vt\n∗∈Rd for a patient at t-th visit:\nvt\n∗= Transformer({[CLS]}∪{ot\nc∗|c∗∈Ct\n∗})[0] (5)\nwhere [CLS] is a special token as in BERT. It is put in the\nﬁrst position of each visit of type ∗and its ﬁnal state can be\nused as the representation of the visit. Intuitively, it is more\nreasonable to use Transformers as encoders (multi-head\nattention based architecture) than RNN or mean/sum to\naggregate multiple medical embedding for visit embedding\nsince the set of medical codes within one visit is not ordered.\nNote that symbol [SEP] is also ignored considering there is\nno clear separate among codes within one visit.\nIt is worth noting that our Transformer encoder is different\nfrom the original one in the position embedding part. Position\nembedding, as an important component in Transformers and\nBERT, is used to encode the position and order information\nof each token in a sequence. However, one big difference\nbetween language sentences and EHR sequences is that the\nmedical codes within the same visit do not generally have an\norder, so we remove the position embedding in our model.\n4.3 Pre-training\nWe adapted the original BERT model to be more suitable\nfor our data and task. In particular, we pre-train the model\non each single EHR visit (within both single-visit EHR\nsequences and multi-visit EHR sequences). We modiﬁed the\ninput and pre-training objectives of the BERT model: (1)\nFor the input, we built the Transformer encoder on the GNN\noutputs, i.e. ontology embeddings, for visit embedding. For\nthe original EHR sequence, it means essentially we combine\nthe GNN model with a Transformer to become a new\nintegrated encoder. In addition, we removed the position em-\nbedding as we explained before. (2) As for the pre-training\nprocedures, we modiﬁed the original pre-training tasks\ni.e., Masked LM (language model) task and Next Sentence\nprediction task to self-prediction task and dual-prediction\ntask. The idea to conduct these tasks is to make the visit\nembedding absorb enough information about what it is made\nof and what it is able to predict . (note that we omit super-\nscript tas only single visit is used for pre-training procedure).\nThus, for the self-prediction task, we want the visit em-\nbedding v∗to recover what it is made of, i.e., the input medi-\ncal codes C∗limited by the same type for each visit as follows:\nLse(v∗,C(n)\n∗ ) =−log p(C(n)\n∗ |v∗)\n= −\n∑\nc∗∈C(n)\n∗\nlog p(c∗|v∗) +\n∑\nc∗∈{C∗\\C(n)\n∗ }\nlog p(c∗|v∗) (6)\nwhere C(n)\n∗ is the medical codes set of n-th patient,\n∗∈{ d,m}and we minimize the binary cross entropy loss\nLse. For instance, assume that the n-th patient takes 10\ndifferent medications out of total 100 medications which\nmeans |C(n)\nm | = 10 and |Cm| = 100. In such case, we\ninstantiate and minimize Lse(vm,C(n)\nm ) to produce high\nprobabilities among 10 taken medications captured by\n−∑\nc∗∈C(n)\nm\nlog p(c∗|vm) and lower the probabilities among\n90 non-taken ones captured by ∑\nc∗∈{Cm\\C(n)\nm }log p(c∗|vm).\nIn practise, Sigmoid (f(v∗)) should be transformed by\napplying a fully connected neural network f(·) with one\nhidden layer. With an analogy to the Masked LM task in\nBERT, we also used speciﬁc symbol [MASK] to randomly\nreplace the original medical code c∗∈C∗. So there are 15%\ncodes in C∗which will be replaced randomly and the model\nshould have the ability to predict the masked code based on\nothers.\nLikewise, for the dual-prediction task, since the visit em-\nbedding v∗carries the information of medical codes of type\n∗, we can further expect it has the ability to do more task-\nspeciﬁc prediction as follows:\nLdu = −log p(Cd|vm) −log p(Cm|vd) (7)\nwhere we use the same transformation function\nSigmoid(f1(vm)), Sigmoid (f2(vd))1 with different weight\nmatrix to transform the visit embedding and optimize the\nbinary cross entropy loss Ldu expanded same as Lse in Eq. 6.\nThis is a direct adaptation of the next sentence prediction\n1f1, f2 are the multiple layer perceptron (MLP) with one hidden\nlayer\n…\n}\n…\n}\nMulti-Visit \nEmbeddings\nAggregating \nEmbeddings\nFine-tuning\nClassiﬁer\nSigmoid \nOutput\n...\nŏ\nŏ\nFigure 4: Graphical illustration of ﬁne-tuning procedure. We sep-\narately aggregate the visit embeddings produced by diagnoses and\nmedications before t-visit. Then, these two aggregated visit embed-\ndings are concatenated with diagnosis visit embedding at t-visit as\ninput for ﬁne-tuning.\ntask. In BERT, the next sentence prediction task facilitates\nthe prediction of sentence relations, which is a common task\nin NLP. However, in healthcare, most predictive tasks do not\nhave a sequence pair to classify. Instead, we are often inter-\nested in predicting unknown disease or medication codes of\nthe sequence. For example, in medication recommendation,\nwe want to predict multiple medications given only the\ndiagnosis codes. Inversely, we can also predict unknown\ndiagnosis given the medication codes.\nThus, our ﬁnal pre-training optimization objective can sim-\nply be the combination of the aforementioned losses, as\nshown in Eq. 8.\nLpr = Lse(vd,Cd) +Lse(vm,Cm) +Ldu (8)\nIn practise, we could integrate using mini-batch technique to\ntrain on EHR data from all patients with a single visit.\n4.4 Fine-tuning\nAfter obtaining pre-trained visit representation for each visit,\nfor a prediction task on a multi-visit sequence data, we ag-\ngregate all the visit embedding and add a prediction layer for\nthe medication recommendation task as shown in Figure. 4.\nTo be speciﬁc, from pre-training on all visits, we have a pre-\ntrained Transformer encoder, which can then be used to get\nthe visit embedding vτ\n∗at time τ. The known diagnosis codes\nCt\nd at the prediction time tis also represented using the same\nmodel as vt\n∗. Concatenating the mean of previous diagnoses\nvisit embeddings and medication visit embeddings, also the\nlast diagnoses visit embedding, we built an MLP based pre-\ndiction layer to predict the recommended medication codes\nas in Equation 9.\nyt = Sigmoid(W1[( 1\nt−1\n∑\nτ<t\nvτ\nd)||( 1\nt−1\n∑\nτ<t\nvτ\nm)||vt\nd]+ b)\n(9)\nwhere W1 ∈R|Cm|×3d is a learnable transformation matrix.\nGiven the true labels ˆyt at each time stamp t, the loss func-\nStats Single-Visit Multi-Visit\n# of patients 30,745 6,350\navg # of visits 1.00 2.36\navg # of dx 39 10.51\navg # of rx 52 8.80\n# of unique dx 1,997 1,958\n# of unique rx 323 145\nTable 2: Statistics of the Data (dx for diagnosis, rx for medication).\ntion for the whole EHR sequence (i.e. a patient) is\nL= − 1\nT −1\nT∑\nt=2\n(y⊺\nt log( ˆyt) + (1−y⊺\nt) log(1−ˆyt)) (10)\n5 Experiment\n5.1 Experimental Setting\nData\nWe used EHR data from MIMIC-III [Johnson et al., 2016 ]\nand conducted all our experiments on a cohort where patients\nhave more than one visit. We utilize data from patients with\nboth single visit and multiple visits in the training dataset as\npre-training data source (multi-visit data are split into visit\nslices and duplicate codes within a single visit are removed\nin order to avoid leakage of information). In this work, we\ntransform the drug coding from NDC to ATC Third Level for\nusing the ontology information. The statistics of the datasets\nare summarized in Table 2.\nBaselines\nWe compared G-BERT2 with the following baselines. All\nmethods are implemented in PyTorch [Paszke et al., 2017 ]\nand trained on an Ubuntu 16.04 with 8GB memory and\nNvidia 1080 GPU.\n1. Logistic Regression (LR) is logistic regression with\nL1/L2 regularization. Here we represent sequential mul-\ntiple medical codes by sum of multi-hot vector of each\nvisit. Binary relevance technique [Luaces et al., 2012]\nis used to handle multi-label output.\n2. LEAP [Zhang et al., 2017] is an instance-based medi-\ncation combination recommendation method which for-\nmalizes the task in multi-instance and multi-label learn-\ning framework. It utilizes a encoder-decoder based\nmodel with attention mechanism to build complex de-\npendency among diseases and medications.\n3. RETAIN [Choi et al., 2016 ] makes sequential predic-\ntion of medication combination and diseases prediction\nbased on a two-level neural attention model that detects\ninﬂuential past visits and clinical variables within those\nvisits.\n4. GRAM [Choi et al., 2017 ] injects domain knowledge\n(ICD9 Dx code tree) to tanh via attention mechanism.\n5. GAMENet [Shang et al., 2019] is the method to recom-\nmend accuracy and safe medication based on memory\n2https://github.com/jshang123/G-Bert\nneural networks and graph convolutional networks by\nleveraging EHR data and Drug-Drug Interaction (DDI)\ndata source. For fair comparison, we use a variant\nof GAMENet without DDI knowledge and procedure\ncodes as input renamed as GAMENet−.\n6. G-BERT is our proposed model which integrated the\nGNN representation into Transformer-based visit en-\ncoder with pre-training on single-visit EHR data.\nWe also evaluated 3G-BERT variants for model ablation.\n1. G-BERTG−,P−: We directly use medical embedding\nwithout ontology information as input and initialize the\nmodel’s parameters without pre-training.\n2. G-BERTG−: We directly use medical embedding with-\nout ontology information as input with pre-training.\n3. G-BERTP−: We use ontology information to get ontol-\nogy embedding as input and initialize the model’s pa-\nrameters without pre-training.\nMetrics\nTo measure the prediction accuracy, we used Jaccard Simi-\nlarity Score (Jaccard), Average F1 (F1) and Precision Recall\nAUC (PR-AUC). Jaccard is deﬁned as the size of the intersec-\ntion divided by the size of the union of ground truth set Y(k)\nt\nand predicted set ˆY(k)\nt .\nJaccard = 1∑N\nk\n∑Tk\nt 1\nN∑\nk\nTk∑\nt\n|Y(k)\nt ∩ˆY(k)\nt |\n|Y(k)\nt ∪ˆY(k)\nt |\nwhere N is the number of patients in test set and Tk is the\nnumber of visits of the kth patient.\nImplementation Details\nWe randomly divide the dataset into training, validation\nand testing set in a 0.6 : 0.2 : 0.2 ratio. For G-BERT,\nthe hyperparameters are adjusted on evaluation set: (1)\nGAT part: input embedding dimension as 75, number of\nattention heads as 4; (2) BERT part: hidden dimension as\n300, dimension of position-wise feed-forward networks as\n300, 2 hidden layers with 4 attention heads for each layer.\nSpecially, we alternated the pre-training with 5 epochs and\nﬁne-tuning procedure with 5 epochs for 15 times to stabilize\nthe training procedure.\nFor LR, we use the grid search over typical range of hyper-\nparameter to search the best hyperparameter values which re-\nsult in L1 norm penalty with weight as1.1. For deep learning\nmodels, we implemented RNN using a gated recurrent unit\n(GRU) [Cho et al. , 2014 ] and utilize dropout with a prob-\nability of 0.4 on the output of embedding. We test several\nembedding choice for baseline methods and determine the di-\nmension for medical embedding as 300 and thershold for ﬁ-\nnal prediction as 0.3 for better performance. Training is done\nthrough Adam [Kingma and Ba, 2014 ] at learning rate 5e-4.\nWe ﬁx the best model on evaluation set within 100 epochs\nand report the performance in test set.\n5.2 Results\nTable. 3 compares the performance on the medication rec-\nommendation task. For variants of G-BERT, G-BERTG−,P−\nperforms worse compared with G-BERTG− and G-BERTP−\nwhich demonstrate the effectiveness of using ontology infor-\nmation to get enhanced medical embedding as input and em-\nploy an unsupervised pre-training procedure on larger abun-\ndant data. Incorporating both hierarchical ontology infor-\nmation and pre-training procedure, the end-to-end model\nG-BERT has more capacity and achieve comparable results\nwith others.\nMethods Jaccard PR-AUC F1 # of parameters\nLR 0.4075 0.6716 0.5658 -\nGRAM 0.4176 0.6638 0.5788 3,763,668\nLEAP 0.3921 0.5855 0.5508 1,488,148\nRETAIN 0.4456 0.6838 0.6064 2,054,869\nGAMENet− 0.4401 0.6672 0.5996 5,518,646\nGAMENet 0.4555 0.6854 0.6126 5,518,646\nG-BERTG−,P− 0.4186 0.6649 0.5796 2,634,145\nG-BERTG− 0.4299 0.6771 0.5903 2,634,145\nG-BERTP− 0.4236 0.6704 0.5844 3,034,045\nG-BERT 0.4565 0.6960 0.6152 3,034,045\nTable 3: Performance on Medication Recommendation Task.\nAs for baseline models, LR and Leap are worse than\nour most basic model ( G-BERTG−,P−) in terms of most\nmetrics. Comparing G-BERTP− and GRAM, which both\nused medical ontology information without pre-training, the\nscores of our G-BERTP− is slightly higher in all metrics.\nThis can demonstrate the validness of using Transformer\nencoders and the speciﬁc prediction layer for medication\nrecommendation. Our ﬁnal model G-BERT is also better\nthan the attention based model, RETAIN, and the recently\npublished state-of-the-art model, GAMENet. Speciﬁcally,\neven adding the extra information of DDI knowledge and\nprocedure codes, GAMENet still performs worse than\nG-BERT.\nIn addition, we visualized the pre-training medical code\nembeddings of G-BERTG− and G-BERT to show the ef-\nfectiveness of ontology embedding using online embed-\nding projector 3 shown in (https://raw.githubusercontent.com/\njshang123/G-Bert/master/saved/tsne.png/).\n6 Conclusion\nIn this paper we proposed a pre-training model named\nG-BERT for medical code representation and medication rec-\nommendation. To our best knowledge, G-BERT is the ﬁrst\nthat utilizes language model pre-training techniques in health-\ncare domain. It adapted BERT to the EHR data and integrated\nmedical ontology information using graph neural networks.\nBy additional pre-training on the EHR from patients who only\nhave one hospital visit which are generally discarded before\nmodel training, G-BERT outperforms all baselines in predic-\ntion accuracy on medication recommendation task. One di-\nrection for the future work is to add more auxiliary and struc-\n3https://projector.tensorﬂow.org/\ntural tasks to improve the ability of code representaion. An-\nother direction may be to adapt our model to be suitable for\neven larger datasets with more heterogeneous modalities.\nAcknowledgments\nThis work was supported by the National Science Founda-\ntion award IIS-1418511, CCF-1533768 and IIS-1838042, the\nNational Institute of Health award 1R01MD011682-01 and\nR56HL138415.\nReferences\n[Baytas et al., 2017] Inci M. Baytas, Cao Xiao, Xi Zhang,\nFei Wang, Anil K. Jain, and Jiayu Zhou. Patient subtyp-\ning via time-aware lstm networks. In Proceedings of the\n23rd ACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, 2017.\n[Cho et al., 2014] Kyunghyun Cho, Bart Van Merri ¨enboer,\nDzmitry Bahdanau, and Yoshua Bengio. On the proper-\nties of neural machine translation: Encoder-decoder ap-\nproaches. arXiv preprint arXiv:1409.1259, 2014.\n[Choi et al., 2016] Edward Choi, Mohammad Taha Ba-\nhadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and\nWalter Stewart. Retain: An interpretable predictive model\nfor healthcare using reverse time attention mechanism.\nIn Advances in Neural Information Processing Systems ,\npages 3504–3512, 2016.\n[Choi et al., 2017] Edward Choi, Mohammad Taha Ba-\nhadori, Le Song, Walter F Stewart, and Jimeng Sun. Gram:\nGraph-based attention model for healthcare representation\nlearning. In SIGKDD, 2017.\n[Choi et al., 2018] Edward Choi, Cao Xiao, Walter Stew-\nart, and Jimeng Sun. Mime: Multilevel medical embed-\nding of electronic health records for predictive healthcare.\nIn S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in\nNeural Information Processing Systems 31 , pages 4547–\n4557. Curran Associates, Inc., 2018.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Erhan et al., 2010] Dumitru Erhan, Yoshua Bengio, Aaron\nCourville, Pierre-Antoine Manzagol, Pascal Vincent, and\nSamy Bengio. Why does unsupervised pre-training help\ndeep learning? Journal of Machine Learning Research ,\n11(Feb):625–660, 2010.\n[Gilmer et al., 2017] J. Gilmer, S.S. Schoenholz, P.F. Riley,\nO. Vinyals, and G.E. Dahl. Neural message passing for\nquantum chemistry. In ICML, 2017.\n[Hinton et al., 2006] Geoffrey E Hinton, Simon Osindero,\nand Yee-Whye Teh. A fast learning algorithm for deep\nbelief nets. Neural computation, 18(7):1527–1554, 2006.\n[Johnson et al., 2016] Alistair EW Johnson, Tom J Pollard,\nLu Shen, H Lehman Li-wei, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. Mimic-iii, a freely ac-\ncessible critical care database. Scientiﬁc data, 3:160035,\n2016.\n[Kingma and Ba, 2014] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2014.\n[Kipf and Welling, 2017] Thomas N Kipf and Max Welling.\nSemi-supervised classiﬁcation with graph convolutional\nnetworks. In ICLR, 2017.\n[Lipton et al., 2015] Zachary C Lipton, David C Kale,\nCharles Elkan, and Randall Wetzel. Learning to diag-\nnose with lstm recurrent neural networks. arXiv preprint\narXiv:1511.03677, 2015.\n[Luaces et al., 2012] Oscar Luaces, Jorge D´ıez, Jos´e Barran-\nquero, Juan Jos´e del Coz, and Antonio Bahamonde. Binary\nrelevance efﬁcacy for multilabel classiﬁcation. Progress in\nArtiﬁcial Intelligence, 1(4):303–313, 2012.\n[Ma et al., 2018] Tengfei Ma, Cao Xiao, and Fei Wang.\nHealth-atm: A deep architecture for multifaceted patient\nhealth record representation and risk prediction. In Pro-\nceedings of the 2018 SIAM International Conference on\nData Mining, pages 261–269. SIAM, 2018.\n[Paszke et al., 2017] Adam Paszke, Sam Gross, Soumith\nChintala, Gregory Chanan, Edward Yang, Zachary De-\nVito, Zeming Lin, Alban Desmaison, Luca Antiga, and\nAdam Lerer. Automatic differentiation in pytorch. 2017.\n[Peters et al., 2018] Matthew Peters, Mark Neumann, Mohit\nIyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representa-\ntions. In NAACL, volume 1, pages 2227–2237, 2018.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan,\nTim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[Ramachandran et al., 2016] Prajit Ramachandran, Peter J\nLiu, and Quoc V Le. Unsupervised pretraining\nfor sequence to sequence learning. arXiv preprint\narXiv:1611.02683, 2016.\n[Shang et al., 2019] Junyuan Shang, Cao Xiao, Tengfei Ma,\nHongyan Li, and Jimeng Sun. Gamenet: Graph augmented\nmemory networks for recommending medication combi-\nnation. AAAI, 2019.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, pages 5998–6008, 2017.\n[Velickovicet al., 2017] Petar Velickovic, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Lio, and\nYoshua Bengio. Graph attention networks. arXiv preprint\narXiv:1710.10903, 1(2), 2017.\n[Xiao et al., 2018a] Cao Xiao, Edward Choi, and Jimeng\nSun. Opportunities and challenges in developing deep\nlearning models using electronic health records data: a\nsystematic review. Journal of the American Medical In-\nformatics Association, 2018.\n[Xiao et al., 2018b] Cao Xiao, Tengfei Ma, Adji B. Dieng,\nDavid M. Blei, and Fei Wang. Readmission prediction via\ndeep contextual embedding of clinical concepts. PLOS\nONE, 13(4):1–15, 04 2018.\n[Zhang et al., 2017] Yutao Zhang, Robert Chen, Jie Tang,\nWalter F Stewart, and Jimeng Sun. Leap: Learning to pre-\nscribe effective and safe treatment combinations for mul-\ntimorbidity. In SIGKDD, pages 1315–1324, 2017.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7348591685295105
    },
    {
      "name": "Encoder",
      "score": 0.7098475694656372
    },
    {
      "name": "Transformer",
      "score": 0.6131247878074646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5394237041473389
    },
    {
      "name": "Machine learning",
      "score": 0.5350979566574097
    },
    {
      "name": "Health records",
      "score": 0.5151921510696411
    },
    {
      "name": "Inductive bias",
      "score": 0.5066385865211487
    },
    {
      "name": "Graph",
      "score": 0.4870646297931671
    },
    {
      "name": "Training set",
      "score": 0.4585014283657074
    },
    {
      "name": "Feature learning",
      "score": 0.4405536353588104
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.4276922345161438
    },
    {
      "name": "Health care",
      "score": 0.367202490568161
    },
    {
      "name": "Task (project management)",
      "score": 0.32183289527893066
    },
    {
      "name": "Multi-task learning",
      "score": 0.2767266035079956
    },
    {
      "name": "Theoretical computer science",
      "score": 0.11788713932037354
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210108991",
      "name": "IQVIA (United States)",
      "country": "US"
    }
  ]
}