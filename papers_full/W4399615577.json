{
  "title": "Unleashing the transformers: NLP models detect AI writing in education",
  "url": "https://openalex.org/W4399615577",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5069607722",
      "name": "José Campino",
      "affiliations": [
        "Universidade Nova de Lisboa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4214927078",
    "https://openalex.org/W3199189488",
    "https://openalex.org/W3017131514",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4309242593",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2992323123",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4319318680",
    "https://openalex.org/W4304614714",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W6692563993",
    "https://openalex.org/W6629028937",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W4383815588",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W4320011332",
    "https://openalex.org/W4322217015",
    "https://openalex.org/W6727099177",
    "https://openalex.org/W2273396394",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2781626870",
    "https://openalex.org/W3093288613",
    "https://openalex.org/W2970474271",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2613092605",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4390175962",
    "https://openalex.org/W4296369454",
    "https://openalex.org/W2950813464"
  ],
  "abstract": "Abstract Artificial Intelligence (AI) has witnessed widespread application across diverse domains, with education being a prominent focus for enhancing learning outcomes and tailoring educational approaches. Transformer models, exemplified by BERT, have demonstrated remarkable efficacy in Natural Language Processing (NLP) tasks. This research scrutinizes the current landscape of AI in education, emphasizing the utilization of transformer models. Specifically, the research delves into the influence of AI tools facilitating text generation through input prompts, with a notable instance being the GPT-4 model developed by OpenAI. The study employs pre-trained transformer models to discern whether a given text originates from AI or human sources. Notably, BERT emerges as the most effective model, fine-tuned using a dataset comprising abstracts authored by humans and those generated by AI. The outcomes reveal a heightened accuracy in distinguishing AI-generated text. These findings bear significance for the educational realm, suggesting that while endorsing the use of such tools for learning, vigilance is warranted to identify potential misuse or instances where students should independently develop their reasoning skills. Nevertheless, ethical considerations must be paramount when employing such methodologies. We have highlighted vulnerabilities concerning the potential bias of AI models towards non-native English speakers, stemming from possible deficiencies in vocabulary and grammatical structure. Additionally, users must ensure that there is no complete reliance on these systems to ascertain students' performance. Further research is imperative to unleash the full potential of AI in education and address ethical considerations tied to its application.",
  "full_text": "Vol.:(0123456789)\nJournal of Computers in Education (2025) 12(2):645–673\nhttps://doi.org/10.1007/s40692-024-00325-y\nUnleashing the transformers: NLP models detect AI writing \nin education\nJosé Campino1 \nReceived: 14 September 2023 / Revised: 29 April 2024 / Accepted: 7 May 2024 /  \nPublished online: 13 June 2024 \n© The Author(s) 2024\nAbstract\nArtificial Intelligence (AI) has witnessed widespread application across diverse \ndomains, with education being a prominent focus for enhancing learning outcomes \nand tailoring educational approaches. Transformer models, exemplified by BERT, \nhave demonstrated remarkable efficacy in Natural Language Processing (NLP) \ntasks. This research scrutinizes the current landscape of AI in education, empha-\nsizing the utilization of transformer models. Specifically, the research delves into \nthe influence of AI tools facilitating text generation through input prompts, with a \nnotable instance being the GPT-4 model developed by OpenAI. The study employs \npre-trained transformer models to discern whether a given text originates from AI \nor human sources. Notably, BERT emerges as the most effective model, fine-tuned \nusing a dataset comprising abstracts authored by humans and those generated by \nAI. The outcomes reveal a heightened accuracy in distinguishing AI-generated text. \nThese findings bear significance for the educational realm, suggesting that while \nendorsing the use of such tools for learning, vigilance is warranted to identify poten-\ntial misuse or instances where students should independently develop their reasoning \nskills. Nevertheless, ethical considerations must be paramount when employing such \nmethodologies. We have highlighted vulnerabilities concerning the potential bias of \nAI models towards non-native English speakers, stemming from possible deficien-\ncies in vocabulary and grammatical structure. Additionally, users must ensure that \nthere is no complete reliance on these systems to ascertain students’ performance. \nFurther research is imperative to unleash the full potential of AI in education and \naddress ethical considerations tied to its application.\nKeywords Transformer models · Artificial intelligence · Natural language \nprocessing · ChatGPT · BERT · Education\n * José Campino \n josepedrocampino@gmail.com\n1 Nova School of Business and Economics, Carcavelos, Portugal\n646 Journal of Computers in Education (2025) 12(2):645–673\nIntroduction\nThe interest on Artificial Intelligence (AI) and its possible use in education is \nnot new (Devedžić, 2004). However, the recent developments in this field, the \nimprovement of computational resources and their availability, allowed the devel-\nopment of new technologies and further applications, inciting changes in the edu-\ncation system (Lund & Wang, 2023). In this field, the interest has been increasing \nand in recent years the number of papers published on the topic of AI are increas-\ning (Chen et al., 2020). The application of AI in education has covered a wide \nrange of activities according to Chen et  al., (2020): (i) assessment of students \nand schools; (ii) grading and evaluation of papers and exams; (iii) personalized \nteaching; (iv) smart school; and (v) online and remote education. Its application \nin education has gained momentum, as an increasing number of institutions are \nexploring the use of AI for improving learning outcomes. AI has the potential to \nrevolutionize the way of teaching and learning, posing advantages and dangers as \nany other technology.\nConcerning the advantages of integrating AI in education, several key benefits \nhave been recognized. Firstly, AI facilitates personalized learning by tailoring \neducational experiences to individual students based on their strengths, weak -\nnesses, and preferred learning styles. This approach enables students to progress \nat their own pace, fostering heightened engagement, motivation, and ultimately \nyielding improved learning outcomes (Dimitriadou & Lanitis, 2023; Xu & Ouy -\nang, 2022). Secondly, intelligent tutoring systems powered by AI offer instantane-\nous feedback to students, aiding in the identification and correction of mistakes. \nThis not only alleviates the workload on teachers but also enhances the overall \neffectiveness of instructional methods (Xu & Ouyang, 2022). Thirdly, AI’s capa-\nbility for data analysis proves instrumental in scrutinizing extensive datasets. \nThis analytical prowess assists teachers in discerning students’ learning patterns, \nallowing for the customization of teaching strategies to enhance education quality \nand elevate student performance. Lastly, the integration of AI fosters accessibil-\nity in education, particularly benefiting students with disabilities. Notably, tech-\nnologies such as speech recognition enable students with hearing impairments to \nactively participate in classroom discussions, thereby promoting inclusivity (Xu \n& Ouyang, 2022).\nThe incorporation of AI in education also presents inherent risks encapsulated. \nFirstly, the issue of bias arises, as AI algorithms may exhibit biases leading to \ndiscriminatory outcomes (Akgun & Greenhow, 2022). Liang et al. (2021)unveil \na notable bias in GPT detectors against non-native English writers, evident in the \nhigh misclassification rate of non-native-authored TOEFL essays compared to \nthe near-zero misclassification rate of presumed native-authored college essays. \nThe discrepancy is attributed to limited linguistic variability and word choices \nby non-native authors, resulting in lower perplexity text. The study raises con-\ncerns about the reliability of current detection methods underscoring the need for \nmore robust detection techniques that account for nuances introduced by prompt \ndesign. Secondly, the integration of AI in education raises legitimate privacy \n647\nJournal of Computers in Education (2025) 12(2):645–673 \nconcerns. The collection of personal data by AI systems, including biometric \ninformation and browsing history, raises the specter of misuse or theft, posing \npotential harm to students and educators alike (Akgun & Greenhow, 2022). Fur -\nthermore, there is the risk of dependence, wherein an over-reliance on AI has the \npotential to diminish critical thinking and problem-solving skills among students, \nleading to a scenario where students become overly dependent on AI systems for \nlearning, thereby compromising their ability to think independently. Moreover, \nthe introduction of such disruptive technology necessitates adaptation by both \nstudents and lecturers, encompassing understanding, potential applications, and \ncorrect administration of the technology. Lastly, there is a tangible risk of exclu-\nsion, wherein the imperative for simultaneous adaptation to such technologies \nmay not be universally feasible due to various factors such as disparate levels of \nknowledge, economic disparities, or accessibility challenges, potentially resulting \nin exclusion from the benefits of AI in education.\nThe integration of AI technologies into education presents a noteworthy chal-\nlenge related to plagiarism, necessitating the incorporation of effective control \nmechanisms to identify texts generated by AI (Abd-Elaal et al., 2022). While pre-\nvious solutions have been put forth (Tien & Labbé, 2017; Shahmohammadi et al., \n2020), they were not able to adopt recent advancements, particularly in Natural Lan-\nguage Processing (NLP). The emergence of NLP models based on the Transformer \nmethodology, exemplified by Large Language Models (LLMs) like BERT and GPT \n(Generative Pre-trained Transformer), requires novel approaches. In this context, \nour contribution lies in leveraging such models to detect AI-generated content in \nessay-type texts within the context of university-level education. To achieve this \ngoal, pre-existing models were fine-tuned using a database comprising both human \nand AI-generated academic abstracts. Notably, the BERT model exhibited superior \nperformance, boasting high accuracy levels. The application of the fine-tuned model \nto an experimental database, comprising 200 human-generated texts and those pro-\nduced by various AI models, provided compelling evidence of the practicality and \nefficacy of this approach. In essence, this paper aims to contribute to the ongoing \ndiscourse surrounding the integration of AI in education by proposing a novel meth-\nodology to address critical challenges associated with essay grading systems.\nFrom a theoretical perspective, our research proposes a new approach to essay \ngrading contributing to advancing our understanding of the intersection between AI, \neducation, and ethics. By examining the challenges and implications of AI-gener -\nated content in academic settings, we shed light on broader questions of algorith-\nmic fairness and educational equity. Moreover, our study provides insights into the \nevolving role of educators in navigating the complexities of AI-enhanced teaching \nand learning, highlighting the importance of pedagogical adaptation and ethical \nawareness in the digital age. In addition to its theoretical implications, our research \nhas practical significance for a range of stakeholders in the education ecosystem. For \neducators, our findings offer valuable insights into the potential risks and opportuni-\nties associated with AI technologies in the classroom, empowering them to make \ninformed decisions about assessment practices. Similarly, educational administrators \ncan use our research to inform the development of guidelines governing the respon-\nsible use of AI in education, ensuring that ethical considerations are prioritized in \n648 Journal of Computers in Education (2025) 12(2):645–673\nthe adoption and implementation of AI-driven educational tools. For students, our \nfindings hold the promise of enhanced academic integrity awareness through per -\nsonalized mechanisms powered by AI, which have the potential to allow a faster and \nmore accurate grading process using the methods proposed in this research together \nwith future developments.\nLiterature review\nNatural language processing (NLP) models\nNLP models are machine learning models that are designed to process and under -\nstand human language. These models are used for a wide range of tasks, includ-\ning machine translation, sentiment analysis or text classification. Previously, NLP \nmodels relied on hand-crafted rules and heuristics to analyze and process language \n(François et al., 2012). These models were limited by the complexity and ambiguity \nof natural language, and they often struggled to perform well on real-world tasks. \nMore recently, however, there has been a shift towards using machine learning tech-\nniques to build NLP models. These models are trained on large datasets of human \nlanguage, and they learn to recognize patterns and make predictions based on that \ndata.\nOne of the key innovations in NLP models has been the development of pre-\ntrained language models. These models are trained on massive amounts of text data \nand learn to represent the meaning of words and sentences in a high-dimensional \nvector space. These representations can then be fine-tuned on specific downstream \ntasks, such as sentiment analysis or machine translation, to improve their perfor -\nmance. There are many different types of NLP models, including rule-based models, \nstatistical models, and deep learning models. Deep learning models, such as con-\nvolutional neural networks (CNNs) (O’Shea & Nash, 2015) and recurrent neural \nnetworks (RNNs) (Salehinejad et  al., 2017), have become increasingly popular in \nrecent years due to their ability to capture complex patterns in language data (Cho, \net al., 2014; Peters, Neumann et al., 2018).\nRecently, the Transformer architecture introduced by Vaswani et  al., 2017, has \nbecome the standard for pre-training large-scale language models, with a different \napproach from RNN models (Raffel, et al., 2020). RNNs process input data sequen-\ntially and maintain a hidden state that captures information from the previous inputs. \nThis hidden state is passed to the next time step and updated based on the current \ninput, allowing the model to capture temporal dependencies in the input sequence. \nTransformers, on the other hand, use an attention mechanism to directly model the \nrelationships between all input positions, rather than processing them sequentially \n(Dai, et al., 2019). The innovation brought by the Transformer architecture has led to \na new era of pre-trained language models, where large-scale models are pre-trained \non massive amounts of text data and then fine-tuned on specific downstream tasks. \nThis has led to significant improvements in many NLP tasks, including machine \ntranslation, sentiment analysis and question answering. The recently developed criti-\ncal models are summarized in Table  1 and can be categorized as follows: (i) BERT \n649\nJournal of Computers in Education (2025) 12(2):645–673 \n(Bidirectional Encoder Representations from Transformers) introduced by Devlin \net al., (2019); ALBERT (A Lite BERT) which is a modification of BERT introduced \nby Lan et al., (2020); RoBERTa (Robustly Optimized BERT Pretraining Approach) \nintroduced by Liu et al., (2019); ELECTRA (Efficiently Learning an Encoder that \nClassifies Token Replacements Accurately) introduced by Clark et al., (2020) and; \nXLNet (eXtreme MultiLabelNet) introduced by Yang et al., (2019).\nPre-training a transformer model involves training the model on a large corpus \nof unlabeled data before fine-tuning it on a downstream task. The objective of pre-\ntraining is to learn general representations of language that can be reused for multi-\nple downstream tasks, rather than optimizing the model for a specific task. Previo-\nsuly, training was done using specific databases such as news articles (Jozefowicz \net al., 2016), Wikipedia articles (Merity et al., 2016) or books (Kiros, et al., 2015). \nThe current approach is to train the transformer model in the most diverse database \npossible (Radford, et al., 2019), this will allow a posterior fine-tuning for specific \ntasks with a better performance. The most common pre-training objective for trans-\nformer models is masked language modeling (MLM) (Sinha, et al., 2021). In MLM, \na certain percentage of the input tokens are randomly masked, and the model is \ntrained to predict the masked tokens based on the context provided by the unmasked \ntokens. This objective encourages the model to capture the context-dependent rela-\ntionships between different tokens in the input sequence. Another pre-training objec-\ntive used in transformer models is next sentence prediction (NSP) (Shi & Demberg, \n2019). In NSP, the model is trained to predict whether two sentences are contigu-\nous or not, based on a given pair of sentences. This objective encourages the model \nto capture the relationships between different sentences in a document. Pre-trained \nmodels have several advantages over traditional NLP models. First, pre-trained \nmodels require less labeled data for fine-tuning. This is because the pre-training pro-\ncess allows the model to learn general language structures, which can be applied \nto new, specific tasks. This reduces the amount of labeled data required to achieve \nhigh performance on new tasks. Second, pre-trained models are transferable across \nlanguages and domains. This means that a model pre-trained on one language can \nbe fine-tuned for a different language, without the need for large amounts of labeled \ndata. Additionally, a pre-trained model can be fine-tuned on specific domains, such \nas legal or medical language, without requiring large amounts of labeled data in \nthose domains.\nTable 1  Summary of the pre-trained models used\nTrans-\nformer \nlayers\nHidden size Atten-\ntion \nheads\nParameters Processing Length of training\nBERT base 12 768 12 110 million 4 TPUs 4 days\nALBERT base 12 768 12 11 million 64 TPUs 1 day\nRoBERTa base 12 768 12 125 million 32 TPUs 4 days\nELECTRA base 12 768 12 110 million 1 TPU 3 days\nXLNet base 12 768 12 110 million 32 TPUs 3.5 days\n650 Journal of Computers in Education (2025) 12(2):645–673\nAI development and the link with education\nThe use of AI in education has large impacts and consensually positive outcomes, \nparticularly in administration, instruction and learning tasks. For the purpose of this \nresearch, it is particularly interesting the positive impacts the use of AI has in the \nengagement and improved learning of students and the needed control mechanisms \n(Chen et al., 2020). A remarkable advance in accessible and user-friendly AI is the \nwide publicly available ChatGPT by OpenAI in 2018.\nThe technology is an AI-powered chatbot or conversational agent developed by \nOpenAI, a leading artificial intelligence research organization. ChatGPT is based on \nthe GPT (Generative Pre-trained Transformer) architecture, which is a type of deep \nlearning neural network that has been trained on massive amounts of text data to \ngenerate natural language responses to user queries. ChatGPT is designed to engage \nin human-like conversation on a wide range of topics, from answering questions and \nproviding information to engaging in casual chat. It is capable of understanding and \nresponding to natural language inputs, using its knowledge of language and con-\ntext to generate responses that are relevant, informative, and engaging. ChatGPT is \naccessible through various interfaces, such as web browsers, messaging platforms, \nor applications that integrate with the model’s API. It can be used for a variety of \npurposes, such as customer service, education, entertainment, and more. Since the \nfirst generation lauch in 2018, OpenAI has developed the technology which had a \nlarge improvement with the 3.5 version. The latest version, to the date of the writing, \nis the GTP 4 which builds on the previous version and provides impressive solutions \nmainly concerning video, image recognition and code writing.\nChatGPT has tremendous impacts in academia, libraries and consequently in \neducation (Lund & Wang, 2023). However, there are fragilities in the technology \n(Mathew, 2023) and the model still needs improvement has it still provides biased \nand wrong answers which can compromise its overall accuracy (Johnson, et  al., \n2023). This should be improved in the future as new generations will be released. \nThe use of technologies such as ChatGPT will disrupt the current academic environ-\nment and will likely become as common as current tools such as a calculator or a \ncomputer (McMurtrie, 2023). The writing has become automated, and the grading \nof essays might be biased due to the use of this technology. This might have impacts \nat the level of simple essays but it may also affect the academic thesis writing. Argu-\ning the possible end of essays does not seem to be a solution (Rudolph et al., 2023). \nEssays play an important role in the development of critical thinking, develop-\nment of ideas and writing and speaking capabilities (Taghizadeh et al., 2020) from \nyoung ages until the higher education. Therefore, although AI tools usage should \nbe supported due to the tremendous advantages it poses, this implies an extra step \nwhen evaluating essays which should be written individually and should be used to \nimprove students’ capabilities.\nThe necessity of assessing if an essay is written by a computer program was \nalready raised in academia. Abd-Elaal et al., (2022) highlight the growing concern \nof computer-generated writing tools and their potential to undermine academic \nintegrity and standards. It underscores the need for raising awareness among \nacademics about these tools, developing ways to identify its characteristics, and \n651\nJournal of Computers in Education (2025) 12(2):645–673 \nimplementing clear policies to regulate its usage. Such mechanisms of computer \nwriting detection were already developed, for instance, in the paper by Tien & \nLabbé, (2017). This work addresses the need for automatic detection of automat-\nically generated texts to uphold the quality of bibliometric services. The study \nexplores various text generation methods, demonstrating that documents pro-\nduced by these methods can be reasonably well-classified. The paper introduces \nthe Grammatical Structure Similarity (GSS) system, demonstrating an 80% posi-\ntive detection rate and less than 1% false detection rate for sentences from known \nProbabilistic Context Free Grammar (PCFG) generators. The system’s efficacy is \nhighlighted against other machine learning techniques, though practicality dimin-\nishes when applied to generators using different techniques. Although the authors \nuse machine learning techniques, the paper was an early research and thus not \ncapable of capturing recent developments in this field. Hence, Shahmohammadi \net  al., (2020) discuss paraphrase detection, which is a fundamental task in the \narea of natural language processing. Paraphrase refers to sentences or phrases that \nconvey the same meaning but use different wording. In this research, the authors \npropose a new deep-learning based model which can generalize well despite the \nlack of training data for deep models. The evaluation results show that the pro-\nposed model outperforms almost all the previous works in terms of F-measure \nand accuracy. Notably, the authors suggest that future research incorporates addi-\ntional word embeddings, including ELMo, and leveraging state-of-the-art models \nlike BERT and attention techniques that have gained recent attention in the field. \nHowever, as noticed by Weber-Wulff et al., (2023), the current AI writing detec-\ntion tools are not as accurate as expected showing poor performance. The authors \nstate that all detection tools scored below 80% of accuracy and only 5 over 70%.\nMethodology\nThe main objective of this research is to use Transformer models to predict if a \ntext is either written by AI or by a human. This is a difficult task because the NLP \nmodels are trained on large datasets to perfectly mimic human writing. However, \nthere are patterns used by these models which can be captured by another trained \nTransformer model. The pre-trained Transformer model should then be fine-\ntuned to a specific task to perform well on text classification or any other assign-\nment. By preprocessing the data and employing the AdamW optimizer, the model \nundergoes iterative refinement to enhance its performance. Unlike conventional \ntechniques that often rely on simpler word frequency-based representations and \nlack the ability to capture semantic such as bag of words, TF-IDF, or n-grams, \nthis method exploits the power of transformer models, enabling the capture of \nintricate patterns and contextual nuances. This is possible by leveraging the con-\ntextual understanding and self-attention mechanisms inherent in transformer \narchitectures. The incorporation of performance measures ensures rigorous evalu-\nation, providing a robust validation of the model’s efficacy in distinguishing AI-\ngenerated content.\n652 Journal of Computers in Education (2025) 12(2):645–673\nComputer used\nThe task of text classification will most likely be performed at individual or at small \ngroups’ level. This is probably so due to the need of fine-tuning to a specific task and \ndue to individual or small groups’ needs. Therefore, the computational resources \nare likely to be limited. The estimations were performed on an individual computer \nwith average characteristics that can be confirmed in Table 2. Since fine-tuning does \nnot require much computational resources, the task can be well performed using a \ncomputer with the characteristics mentioned. However, when replicating the results, \nthe user should expect slow outputs, particularly when running several epochs, and \nsome limitations in terms of data usage. In other words, the number of observations \nper database should be smaller in such machines.\nDatabases\nFollowing the previous reasoning, small databases have been used to perform the \nfine-tuning, which characteristics can be confirmed on Table  3. The main reason is \nto test if the task of fine-tuning can easily be performed at individual level in a daily \nuse.\nTwo databases were created: (i) training and (ii) testing. The test database is par -\nticularly large compared with the training database with the objective of accurately \nshow the performance of the models. The databases are composed of three columns: \n(i) text to be analyzed, (ii) label identifying if a text is written by AI (binary vari-\nable); (iii) source of the text for future identification.\nFor this research, the text used for classification was composed of abstracts from \nacademic research papers. These abstracts were collected from a search in Scimago \nusing the simple keyword “leadership”. Then, a random selection of the abstracts \nfrom several research fields and from the year of 2023 was performed. Thereafter, \nTable 2  Characteristics of the \ncomputer used to fine-tune and \nrun the model\nCharacteristics\nOperatingFsystem MacOS\nMemory 16FGB\nCPU AppleFM1\nGPU AppleFM1F(no\nFCUDAFavai\nlable)\nTable 3  Descriptive statistics of \nthe database used Small train Small test Large train Large test\nNb.FOfFtexts 80 70 120 100\nMin.FTokens 94 39 82 39\nMax.FTokens 464 555 464 555\nAvg.FTokens 200 184 202 178\n653\nJournal of Computers in Education (2025) 12(2):645–673 \nthe title of the research papers selected was used as a prompt to ask a text genera-\ntion model to write an abstract for a paper with that title. Hence, it is possible to \napproximate the themes of the human written abstracts with the ones written by AI, \nimproving the quality of the performance results. The model used to generate the \ntexts was the GPT-3.5 developed and trained by OpenAI. This can be done via a \nsimple code in Python with the respective API or simply by using the available tools \nsuch as ChatGPT which is constantly being updated and currently is based on the \nGPT-4 version.\nFine‑tuning\nAs already mentioned, the task of fine-tuning is essential for a good performance of \nthe transformer model on a specific task. This fine-tuning allows a small adjustment \non a pre-trained model to drastically improve its performance on a designated task.\nHowever, first the model was tested without any previous fine-tuning and with \ncommon treatments of the data. The data was simply inputted as a Comma-Sep-\narated Values (CSV) file with the format described before. Then, a treatment was \napplied to the database which includes the following: (i) removal of special char -\nacters and numbers: remove all the special characters in the text such as commas or \nparentheses. Numbers were also removed from the text. The removal of such char -\nacters did not show meaningful differences in the final result as the model is focused \non the tokens (i.e.: words) and not on punctuation, for example; (ii) convert to lower \ncase: convert the text to lower case to make it completely uniform; (iii) combine \nthe text into a single line: in case an abstract has several paragraphs, it was assured \nthat all the text fits to a single line; (iv) remove extra spaces: delete any extra spaces \nbetween words resulting of previous data treatment, leaving a single space between \nwords.\nThe application of this treatment to the data is crucial for enabling the model to \ncapture significant patterns while mitigating noise and potential bias resulting from \nvariations in the text or distinctive writing styles. Two additional treatments were \nconsidered, but their incorporation into the code was ultimately excluded. The initial \ntreatment involves eliminating stop words (e.g., a, the, is), and the second treatment \ninvolves applying stemming to the text, reducing each word to its base form (e.g., \ntransforming \"creating\" and \"creative\" to \"create\"). The rationale behind not apply -\ning these treatments is rooted in the endeavor to execute a complex text classification \ntask. In this context, the composition and structure of the phrase hold paramount \nimportance. The implementation of stop word removal and stemming could com-\npromise the contextual integrity of the phrase, rendering the AI-generated text indis-\ntinguishable from human-generated text. The test utilizing the standard transformer \nmodel without any fine-tuning was conducted using the code chunk presented in \nTable 4 (i.e., in this instance, for BERT). The test database and model were loaded, \nenabling the computation of performance metrics.\nSubsequently, the fine-tuning operation was executed using the code chunk \nprovided in Table  5. In this step, the AdamW optimizer was employed to refine \nthe model. AdamW represents a variation of the Adam optimization algorithm \n654 Journal of Computers in Education (2025) 12(2):645–673\n(Adaptive Moment Estimation) initially introduced by Loshchilov & Hutter \n(2019). Notably, AdamW incorporates an additional weight decay term compared \nto the conventional Adam, specifically designed to mitigate overfitting. The inclu-\nsion of the weight decay term in AdamW serves to penalize substantial weights \nwithin the model, a factor known to contribute to overfitting. Although Adam \nhas demonstrated effectiveness across a broad spectrum of deep learning tasks, \nit occasionally grapples with overfitting challenges. The introduction of AdamW \naddresses this concern by explicitly regulating the model through weight decay.\nTable 4  Code chuck used to test the transformer models without fine-tuning\nTable 5  Code chuck used to fine-tune the models\n655\nJournal of Computers in Education (2025) 12(2):645–673 \nThe code in Table  5 defines an optimizer using the AdamW algorithm with \na learning rate of 2e−5 , and then fine-tunes a pre-trained model for a specified \nnumber of epochs. During each epoch, the optimizer’s gradients are reset to zero \nusing the optimizer.zero_grad() method. Then, the model’s forward propaga-\ntion is performed with the input data train_inputs and target labels train_labels, \nand the resulting loss is calculated. The backward propagation is then performed \nusing the loss.backward() method to compute gradients, and the optimizer’s \nstep() method is called to update the model parameters based on these gradients. \nOverall, this code implements the basic training loop for fine-tuning a pre-trained \nlanguage model using the AdamW optimizer.\nThe learning rate of 2e−5 means that the optimizer adjusts the model’s param-\neters by a factor of 2e−5 times the computed gradients during each update step. \nThe learning rate is a hyperparameter that controls the step size of the optimizer \nduring parameter updates. It determines how much the model’s parameters should \nbe adjusted based on the computed gradients. A smaller learning rate means that \nthe model parameters will be updated more slowly and cautiously, which can help \nprevent overshooting the optimal values. However, it also means that the optimi-\nzation process may take longer to converge. On the other hand, a larger learning \nrate means that the model parameters will be updated more aggressively, which \ncan result in faster convergence but may also cause the optimizer to overshoot the \noptimal values.\nPerformance measures\nEvaluating the model’s performance is a crucial aspect of this study, encompass-\ning assessments for both non-fine-tuned models and at each epoch during the fine-\ntuning process. This comprehensive approach facilitates the computation of average \nperformance metrics over multiple epochs. The evaluation employs the test database \nfor all metrics except for loss, which is computed during fine-tuning. The measures \nutilized include firstly the loss, which is a measure of the error of the model on the \ntraining data. It is computed using the cross-entropy loss function as per Eq. (1).\nwhere: yi is the true probability of class i, and p i is the predicted probability of class \ni\nSecondly, accuracy is the proportion of correctly classified examples out of \nall examples. It is computed as the number of true positives and true negatives \ndivided by the total number of examples as per Eq. (2 ).\nwhere: TP (true positives), TN (true negatives), FP (false positives), and FN (false \nnegatives)\n(1)Cross Entropy Loss =−\n/uni2211.s1\niyi ∙ log(pi)\n(2)Accuracy= TP + TN\nTP + TN + FP + FN\n656 Journal of Computers in Education (2025) 12(2):645–673\nThirdly, precision is the proportion of true positives among all examples classi-\nfied as positive. It is computed as the number of true positives divided by the total \nnumber of positive predictions as per Eq. (3).\nwhere: TP (true positives) and FP (false positives)\nFourthly, recall is the proportion of true positives among all actual positive exam-\nples. It is computed as the number of true positives divided by the total number of \npositive examples as per Eq. (4).\nwhere: TP (true positives) and FN (false negatives)\nFifthly, F1-score is the mean of precision and recall. It is a measure of the bal-\nance between these two measures as per Eq. (5).\nSixthly, AUC-ROC is the area under the receiver operating characteristic (ROC) \ncurve. It is a measure of the model’s ability to distinguish between positive and neg-\native examples. The ROC curve plots the true positive rate (TPR) against the false \npositive rate (FPR) for different threshold values. The AUC-ROC is computed as the \narea under the ROC curve. Lastly, AUC-PR: AUC-PR is the area under the preci-\nsion-recall curve. It is a measure of the model’s ability to retrieve positive examples. \nThe precision-recall curve plots the precision against the recall for different thresh-\nold values. The AUC-PR is computed as the area under the precision-recall curve.\nAnalyze an entire document\nAfter the fine-tuning and the definition of the performance measures, the model can \nbe saved and is ready to use in the task defined. In real world scenario, it may be \nimportant to input an entire document and verify the probability of such a document \nbeing written by AI. This can be a biased analysis as parts of the document may be \nwritten by AI and other by humans, and the result will depend on the proportions of \nthe text authorship. The best option would be to analyze smaller portions of text but \nas this is not always possible or comfortable, the transformer models allow an analy-\nsis of an entire document.\nThe suggestion is that for such analysis, the user should import an entire PDF file \nand convert it into simple text as in Table  6. After that, the text preprocessing tech-\nniques already explored should be applied. If the process is well conducted, the data \nwill end up being exactly as a simple text input but with a considerable larger con-\ntent. The transformer models have a limit of tokens which they can analyze at once. \nFor example, the BERT base model can analyze 512 tokens. Therefore, it is crucial \nto divide the imported file into text chucks not exceeding the capacity limit of the \n(3)Precision= TP\nTP + FP\n(4)Recall= TP\nTP + FN\n(5)F 1 = 2 ∗ Precision+ Recall\nPrecision+ Recall\n657\nJournal of Computers in Education (2025) 12(2):645–673 \nmodel selected. Then, a loop should be created to sum the probability of each text \nchunk being generated by AI and later a simple mean should be performed.\nPython and libraries used\nTo execute the analysis proposed in this research, it is essential to utilize the latest \nversion of the Python coding language (currently Python 3.11). In this case, Python \nwas employed, installed through Anaconda, allowing for the straightforward con-\nstruction of environments and library installation. Specifically for the tasks outlined, \nheavy reliance was placed on the Transformers library, which offers state-of-the-art \nNatural Language Processing (NLP) capabilities. This library is constructed on top \nof PyTorch and TensorFlow, featuring pre-trained models for various NLP tasks, \nincluding text classification, question answering, and language translation. Addi-\ntionally, the Torch library, designed for building and training neural networks, was \nutilized, particularly in the realm of deep learning and scientific computing. The \nSklearn library, with a focus on the Metrics module, played a crucial role, provid-\ning a suite of functions for evaluating machine learning model performance. Finally, \nfoundational libraries such as Numpy and Pandas were incorporated into the toolkit.\nResults\nAs examined earlier, the primary aim of the research is to assess the probability of \na text being composed by AI. To accomplish this objective, pre-trained Transformer \nmodels were employed. Initially, pre-trained models were utilized without any fine-\ntuning. Following this, the models underwent fine-tuning to discern whether a sci-\nentific abstract was authored by an AI tool, relying on the distinctive writing styles \ninherent in both human and AI-generated content.\nStandard models\nWhen employing the standard version of Transformer models (i.e., without fine-\ntuning), the results of performance measures exhibit inconsistency, as detailed in \nTable 7. Subpar outcomes were observed across all performance metrics. Notably, \nthere were elevated values for loss and diminished values for accuracy. Among the \nmodels assessed, BERT demonstrated the most favorable performance, albeit with \nan accuracy of only 53.75% and a precision of 52.17%. BERT and XLNet displayed \nTable 6  Code chuck used to import the entire PDF document\n658 Journal of Computers in Education (2025) 12(2):645–673\ncommendable results for recall, indicating their ability to correctly identify posi-\ntive values. However, considering all the other metrics, utilizing the models in their \nstandard configuration for text classification, especially in the context of the intricate \ntask outlined in this research, may be no more precise than a random guess.\nFine‑tuned models\nConsidering the results obtained for the use of the standard models without any fine-\ntuning, it becomes clear that this step is crucial to obtain a more precise model. \nHence, the fine-tuning of the models was performed and the results are summarized \nin Table 8.\nNotable advancements are apparent with the implementation of the fine-tuning \nstep, leading to improved performance measures across all models. Even after only \n12 epochs, a relatively limited number of training periods, BERT emerges as the \nmost superior model among its peers. The model demonstrates robust values, with \nan accuracy of 94.52% and a precision of 93.66%. Significantly, the recall value is \nexceptionally high at 96.90%, second only to the ELECTRA model with a value \nof 97.62%. Other metrics also reveal similarly elevated results, hovering around the \n95% mark. In the context of our specific objective, emphasis is placed on the loss \nmetric, representing the model’s error. BERT maintains an average loss of 0.3513, \nwhich, though improved, remains relatively high compared to the lowest recorded \nvalue of 0.1572. Additionally, BERT distinguishes itself for its swift convergence, \nachieving this in 87.35 min in the described environment. The BERT model une-\nquivocally stands out as the top performer, closely followed by the ELECTRA \nmodel after fine-tuning. Consequently, the number of epochs was extended to 20, \nand both the BERT and ELECTRA models were fine-tuned, with the results detailed \nin Table 9.\nWith an increased number of epochs, there is evident improvement in the results \nfor performance measures. Considering the loss as a pivotal metric, BERT reduces \nthe average to 0.3179, with the minimum value reaching a mere 0.0521. While the \naverage values for all other measures remain similar, the noteworthy enhancement \nTable 7  Results of the transformer models without fine-tuning\nThe bold means that were the best results in a given test\nALBERT BERT ELECTRA RoBERTa XLNet\nLoss 0.7478 0.6868 0.6904 0.6961 0.7183\nAccuracy 33.75% 53.75% 50.00% 50.00% 51.25%\nPrecision 38.98% 52.17% 0.00% 0.00% 50.70%\nRecall 57.50% 90.00% 0.00% 0.00% 90.00%\nF1 46.46% 66.06% 0.00% 0.00% 64.86%\nROC-AUC 33.75% 53.75% 50.00% 50.00% 51.25%\nPR-AUC 43.67% 51.96% 50.00% 50.00% 50.63%\nTimeF (min.) 1.77 1.67 1.72 1.69 3.49\n659\nJournal of Computers in Education (2025) 12(2):645–673 \nTable 8  Results of the transformer models after fine-tuning\nThe bold means that were the best results in a given test\nALBERT BERT ELECTRA RoBERTa XLNet\nMinimum Average Maximum Minimum Average Maximum Minimum Average Maximum Minimum Average Maximum Minimum Average Maximum\nLoss 0.1321 0.3714 0.7856 0.1572 0.3513 0.7171 0.4134 0.5872 0.6994 0.3263 0.5838 0.6912 0.0411 0.2728 0.6868\nAccuracy 50.00% 88.81% 100.00% 75.71% 94.52% 100.00% 52.86% 87.38% 95.71% 50.00% 77.38% 98.57% 65.71% 87.26% 95.71%\nPrecision 0.00% 82.26% 100.00% 67.31% 93.66% 100.00% 51.47% 84.30% 94.44% 50.00% 85.99% 100.00% 65.38% 84.27% 92.11%\nRecall 0.00% 91.43% 100.00% 71.43% 96.90% 100.00% 94.29% 97.62% 100.00% 48.57% 80.71% 100.00% 40.00% 92.62% 100.00%\nF1 0.00% 86.04% 97.22% 74.63% 94.92% 100.00% 67.96% 89.68% 95.89% 60.00% 78.43% 98.59% 53.85% 87.23% 95.89\nROC-AUC 50.00% 88.81% 100.00% 75.71% 94.52% 100.00% 52.86% 87.38% 95.71% 50.00% 77.38% 98.57% 65.71% 87.26% 95.71\nPR-AUC 50.00% 86.31% 100.00% 67.61% 92.64% 100.00% 51.47% 83.32% 93.17% 50.00% 76.46% 97.22% 65.94% 81.83% 92.11\nEpochs 12 12 12 12 12\nTimeF(min.) 96.98 87.35 98.48 94.59 173.18\n660 Journal of Computers in Education (2025) 12(2):645–673\nlies in the lowest values for these measures, which are distinctly higher after the \nepoch increase. BERT maintains its position as the fastest model, converging in \n209.01 min compared to ELECTRA’s 226.80 min. However, ELECTRA still exhib-\nits poorer metrics when compared to BERT, underscoring the need for a further \nincrease in the number of epochs to mitigate the loss. It is essential to approach \nthis increase cautiously, as an excessively high number of epochs poses the risk of \noverfitting. The determination of a reasonable number of epochs is crucial, con-\nsidering that performance values tend to plateau and show minimal improvement \nbeyond a certain point. Graph 1 reinforces this observation, illustrating that as the \nloss decreases, the remaining metrics reach a plateau with only marginal differences \nin their results as the number of epochs increases. To facilitate comparison with the \nloss metric, all measures were converted to their original values rather than being \npresented as percentages. In practice, continuous testing of the model with real sce-\nnarios after several epochs is deemed crucial. Following the fine-tuning with 12 \nepochs, a real test to assess the model’s performance on the proposed task is recom-\nmended. The same applies after increasing the number of epochs to 20 and beyond \nif warranted. In the context of this study and the task at hand, the BERT model \nexhibits robust results after 20 epochs, particularly considering the low value for \nthe loss. However, ELECTRA evidently requires a higher number of epochs to yield \nimproved results.\nExperiment with the fine‑tuned BERT model\nThe BERT model has been identified as exhibiting the most promising accuracy \nresults across all conducted steps. Consequently, it has been chosen for a conclusive \nreal test, and the ensuing results are presented. Emphasizing the significance of con-\nducting such tests throughout the fine-tuning phase to calibrate the number of epochs \nis crucial. In this study, 20 epochs are deemed adequate to achieve a commendable \nTable 9  Results of the transformer models fine-tuned with 20 epochs\nThe bold means that were the best results in a given test\nBERT ELECTRA \nMinimum Average Maximum Minimum Average Maximum\nLoss 0.0521 0.3179 0.7012 0.1895 0.4833 0.7055\nAccuracy 79.00% 94.50% 98.00% 50.00% 89.30% 97.00%\nPrecision 72.31% 92.53% 97.56% 50.00% 89.22% 100.00%\nRecall 74.00% 97.30% 100.00% 76.00% 95.30% 100.00%\nF1 81.74% 94.61% 100.00% 66.67% 91.01% 97.09%\nROC-AUC 79.00% 94.50% 98.00% 50.00% 89.30% 97.00%\nPR-AUC 70.97% 91.33% 96.15% 50.00% 86.95% 95.04%\nEpochs 20 20\nTimeF(min.) 209.01 226.80\n661\nJournal of Computers in Education (2025) 12(2):645–673 \n0\n0,1\n0,2\n0,3\n0,4\n0,5\n0,6\n0,7\n0,8\n0,9\n1\n123456 78 91 01 11 21 31 41 51 61 71 81 92 0\nResults\nNumber of Epochs\nLoss Accuracy Precision Recall F1 ROC-AUC PR-AUC\nGraph 1  Results for the BERT model along fine-tuning with 20 epochs\n662 Journal of Computers in Education (2025) 12(2):645–673\nmodel performance. However, in more intricate real-world scenarios, the necessary \nnumber of epochs might escalate.\nTo this end, BERT was fine-tuned, and both the model and the tokenizer \nemployed (the model’s tokenizer available in the Transformers’ library) were saved. \nSubsequently, both elements were loaded, and functions were defined for text pre-\nprocessing and computing the probability of inputted text being generated by AI, \nbased on the outcomes of our pre-trained and fine-tuned BERT model. The pre-pro-\ncessing of text holds significance to maintain uniformity with the input text format \nutilized during model fine-tuning. In our research, special characters and numbers \nwere removed, the text was converted to lowercase, words were combined into a sin-\ngle line, and unnecessary spaces between words were eliminated.\nUltimately, the probability of a text being AI-generated is computed using the \ndesignated function. The Softmax function is applied to normalize the outputs, \nyielding probabilities that sum up to 1—forming a valid probability distribution \nover the binary output classes (AI-generated or not). Consequently, the probabil-\nity returned by the predict function signifies the model’s confidence in the input \ntext being AI-generated, considering the model’s training data and the fine-tuning \nprocess. The probability is then scaled up by 100 and presented with two decimal \npoints, depicting it as a percentage value. The entire process is outlined in Table  10, \naccompanied by an example of the utilized code chunk.\nTable 10  Code chuck used to test the fine-tuned models\n663\nJournal of Computers in Education (2025) 12(2):645–673 \nTo evaluate the outcomes and assess their practical relevance, a novel database \ncomprising 150 observations was meticulously curated. This dataset is equally \ndivided between authentic abstracts extracted from Scopus in 2023, employing the \nkeyword \"machine learning,\" and synthetic abstracts generated for the same papers \nby three distinct models: (i) ChatGPT (supplied by OpenAI using GPT-3.5); (ii) \nBing (supplied my Microsoft using GPT-4); (iii) Bard (supplied by Google). The \nmodels were invoked with identical commands. Graph 2 visually represents the out-\ncomes, illustrating the precision of predictions made by the fine-tuned model. Our \nmodel adeptly distinguishes human-authored texts, assigning them significantly low \nprobabilities of being machine-generated. Conversely, it consistently attributes high \nprobabilities to texts originated from AI models. Specifically, when analyzing texts \nproduced by GPT-3.5, the model consistently yields elevated probability values, \nconsistently exceeding the 95% threshold. A similar scenario is verified when ana-\nlyzing texts produced by Bing which used GPT-4. Nevertheless, the results unveil a \ndiscernible decline in accuracy when Bard is utilized in lieu of the other two models.\nAccording to the data presented in Table  11, 100% of the predictions associated \nwith texts generated by GPT-3.5 exhibit probabilities exceeding 95%, indicating a \nhigh confidence in identifying these texts as AI-generated. For Bing produced texts, \n100% of the predictions made by our model are above the 90% probability thresh-\nold and 94% of the predictions are above the 95% probability threshold. However, \nthe consistency of these values diminishes when employing Bard, where only 44% \nof predictions for texts generated by Bard surpass the 95% probability threshold of \nbeing AI-generated. Despite this variability, the predictions for Bard-generated texts \nmaintain a commendable level of reliability. Specifically, 94% of the predictions sur-\npass a 70% probability threshold, and 70% of the predictions exceed a 90% probabil-\nity threshold, showcasing the robustness of the model in ascertaining the AI origin \nof the generated texts.\nThe obtained results, particularly the diminished accuracy observed for Bard-gen-\nerated texts, align with the fine-tuning process applied to the model. The fine-tuning \nexclusively utilized texts from GPT-3.5 and human-authored sources, showcasing \noptimal performance in those specific scenarios. The same scenario is verified with \nBing as it is based on the same baseline GPT model, although a more recent version \nis employed. However, as the evaluation to different generative models is extend, \nthe efficacy of our fine-tuned detection model varies, contingent upon the generative \nmodel employed and the data used in its training. Consequently, the adaptability of \ntools designed for detecting AI-generated writing necessitates adjustments according \nto the specific generative model in use. Fine-tuning the detection model with texts \nfrom diverse sources is a potential approach. However, this may compromise overall \naccuracy as patterns between different types of AI writing become more challenging \nto discern. Such an approach would also demand increased computational resources, \nowing to the requirement for larger datasets and extended fine-tuning epochs.\nOur research methodology suggests the prospect of creating distinct tools tailored \nto different AI writing models. These specialized tools, fine-tuned for specific mod-\nels, could be more efficient and accurate. Notably, the availability of sufficient data \nfor successful generative model training remains a challenge, limiting the diversity \nof models. As a result, while certain models may proliferate based on established \n664 Journal of Computers in Education (2025) 12(2):645–673\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n05 10 15 20 25 30 35 40 45 50\nProbabilityofbeingAIgenerated\nText number\nBard Bing GPT-3.5 Human\nGraph 2  Probability of a text being generated by AI for Bard, Bing, GPT-3.5 and human texts\n665\nJournal of Computers in Education (2025) 12(2):645–673 \nmethodologies, this study provides compelling evidence that tools developed for \ndetecting text generated by prominent generative models can achieve high accuracy.\nIt is essential to acknowledge that the accuracy of our fine-tuned model is contin-\ngent upon data consistency between the fine-tuning and testing phases. In this study, \nthe model was fine-tuned using abstracts, and its effectiveness in detecting AI writ-\ning is demonstrated on texts of the same type. However, caution is warranted when \nextending this accuracy to other text types, as the optimal fine-tuning approach for \ndifferent text categories remains uncertain, even though accuracy may remain high.\nDiscussion\nThe Transformer methodology, initially introduced by Vaswani et  al. (2017), has \nbrought about a revolutionary shift in machine learning, particularly in Natural Lan-\nguage Processing (NLP) tasks. This study proposes a classification task aimed at \nidentifying texts generated by AI, assigning a probability percentage to such occur -\nrences. This task proves valuable in essay grading and control tasks designed for \nuniversity-level students.\nThe Transformer methodology facilitated the development of pre-trained models \nthat leverage extensive data, a resource typically unavailable at an individual level. \nThis scarcity complicates the training process, rendering it exceptionally intricate. \nThe utility of pre-trained models lies in their ability to achieve high performance \nacross various tasks with a less complex fine-tuning process. This research substan-\ntiates the significance of this process by comparing performance measures of lead-\ning pre-trained Transformer models. When employing the standard model without \nfine-tuning, all models exhibited poor performance. This contrasts with the signifi-\ncantly improved results observed post fine-tuning, utilizing a database comprising \nabstracts from real academic articles and those generated by GPT-3.5.\nThe BERT model introduced by Devlin et al., (2019) was confirmed as the top-\nperformer model analyzed for the task proposed in this research. Using the model \nwithout any fine-tuning process reveals levels of loss of 0.6868, accuracy of 53.75%, \nTable 11  Results breakdown for \nthe probability of a text being \ngenerated by AI\n%AI Bard (%) Bing (%) GPT-3.5 (%) Human (%)\n > 10% 100 100 100 6\n > 20% 100 100 100 4\n > 30% 100 100 100 2\n > 40% 100 100 100 0\n > 50% 100 100 100 0\n > 60% 100 100 100 0\n > 70% 94 100 100 0\n > 80% 84 100 100 0\n > 90% 70 100 100 0\n > 95% 44 94 100 0\n666 Journal of Computers in Education (2025) 12(2):645–673\nand precision of 52.17% for the BERT model, which was still the best performer \namong its peers. After a fine-tuning process of 20 epochs, it achieved average values \nof 0.3179 for loss with a minimum value in the last epoch of 0.0521. As for accu-\nracy, the model showed average values of 94.50% with a minimum value of 79% and \na maximum value of 98%. Similarly, the precision results show an average value of \n92.53% with a minimum value of 73.31% and a maximum value of 97.56%. Several \nstudies have previously highlighted the need for AI writing detection tools as Abd-\nElaal et al., (2022) showed the growing concern regarding this issue and the need to \nraise awareness among academics of the utilization of AI tools. Many tools for AI \nwriting detection were developed, particularly since the launch of the disruptive tool \nChatGPT in 2018 by OpenAI. However, as noticed by Weber-Wulff et al., (2023), \nthe current AI writing detection tools show poor levels of accuracy as all detection \ntools scored below 80% of accuracy and only 5 over 70%. The reasoning of this \npaper is that the lack of accuracy is due to generalization as tools try to identify and \nbe compatible with every type of text. Generative AI models can adapt to different \ncircumstances and the result will show this adaptability. For example, a command to \nwrite in academic manner will provide a different result than a command to write in \na creative manner. Hence, a generic tool will likely show reasonable levels of accu-\nracy but still lacking specificity and eventually showing poorer results when com-\npared with fine-tuned tools. This can be verified in the experiment conducted with \nthe fine-tuned model. After this process, BERT was able to clearly discern between \nhuman written texts and the ones generated by GPT-3.5 and GPT-4. However, while \nusing the same model on Bard generated texts, a reduction in accuracy was verified, \nshowing the need for a fine-tuning process on a specific type of text.\nHence, if the classification task purpose is to detect AI writing in abstracts, the \ntools should be fine-tuned with this type of data. Similarly, the data used for fine-\ntuning should be adapted to the specific writing level or style. This research pro-\nposes the need to implement a control for detection of AI writing at university level. \nTherefore, the tool used needs to be fine-tuned to the type of writing at this school-\ning level. Analogously, if such tool would be used in different levels of the education \nsystem, it would need adaptation for the expected type of writing as the fine-tuning \nprocess needs to account for data provided by students of the schooling level ana-\nlyzed. Hence, the fine-tuning process is a simple but crucial process to improve the \nmodel’s performance. This process needs adaptations according to the pre-trained \nTransformer model used (i.e.: can be found in the accompanying information), par -\nticularly at the model and tokenizer steps identified in Table  10. It is important to \nemphasize the need for removing noise from the data with the preliminary clean-\ning process. The approach followed suggests removing special characters, convert-\ning the entire text to lower case, combining the text into a single line, and removing \nextra blanks spaces. This process will allow the model to focus only on the essen-\ntial tokens providing better outputs. Lastly, the definition of the optimal number of \nepochs is a tricky process as the model should not be fine-tuned excessively to avoid \noverfitting to the data. The fine-tuning (i.e.: as well as a training process) should \nstop as soon as the performance measures achieve a plateau. Therefore, the process \nshould be stopped manually, or with an early stop mechanism activated as soon as \n667\nJournal of Computers in Education (2025) 12(2):645–673 \nthe measures are not improving beyond the desired value. Several models should be \nfine-tuned and tested to get a perception of which process provided better outcomes.\nIn the attempt to provide solutions for AI detection tools, several research pro-\nposed early solutions such as Tien & Labbé, (2017) and Shahmohammadi et  al., \n(2020). However, they were not able to build on the current techniques and mod-\nels mainly developed after the introduction of BERT in 2019. Shahmohammadi \net al., (2020) proposes the use of such techniques in future research as the authors \nhave found this gap in the literature. This research tries to contribute to this line of \nthought including Transformer models in such AI writing detecting tools. Indeed, \nseemingly due to a fine-tuning process of a pre-trained viable model and the use in \na specific context, the approach proposed here provides better results than the ones \npreviously identified by Weber-Wulff et al., (2023) and Johnson and et al., (2023). \nIndeed, Shahmohammadi et al., (2020) have identified the best performance model \nto show an accuracy of 88.5% and a f-measure of 70.3%. After fine-tuning, the \nBERT model used in this research showed average values of accuracy of 94.5% with \na minimum value of 79%, and average F1 values of 94.61% with a minimum value \nof 87.74%. A caveat for these results is the amount of data tested (i.e.: less amount \nof data needed due to previous training), and the specificity of the task requested \nas other studies seem to have adopted general detection of AI writing. Another dif-\nferentiated feature of our analysis is the use of the probability and not a binary out-\ncome. The BERT model allows the computation of a percentage showing the prob-\nability of a certain text being written by AI tools. This approach allows a nuanced \nview instead of a static one. The percentage allows the user to understand the level \nof certainty of the model and if it falls in uncomfortable levels (i.e.: between 40 and \n60%), this should be an alert that a definitive conclusion should not be taken.\nThe use of tools such as the one proposed here, is in its initial phase and tremen-\ndous improvements and adjustments must be made. As this research proposes an \nalternative based on previous pre-trained models, several caveats arise from the use \nof such solutions. Liang et al. (2021) have shown convincing proof of a bias of AI \nwriting detection tools towards non-native English speakers. Indeed, while in theo-\nretical realms it might not be crucial, the existence of false negatives but particular \nthe existence of false positives, might be an important issue. Any tool developed \nwill show errors while in use, but the consequences of such errors might have sev -\neral implications that are not currently foreseen. To account for true positives, our \napproach was to compute the recall value which for BERT showed an average value \nof 97.30% with a minimum value of 74% in the fine-tuning process. While the per -\ncentage of true positives is promising, it will certainly vary according to the task \nand in non-controlled environments. Furthermore, if the average value is considered, \nthere are still 2.7% of wrongly classified observations.\nAs Lund & Wang, (2023) have identified, AI tools have tremendous impacts in \nsociety particularly in academia and education. Chen et al., (2020) have identified \npositives aspects of the adoption of such tools in education such as positive results \nin engagement and learning. However, several studies have identified possible fragil-\nities of AI tools which are still in an early stage of development (Liang et al. 2021; \nMathew, 2023). Certainly, the BERT or other similar model needs further research \nand experimentation before being largely applied. These tools should be adapted to \n668 Journal of Computers in Education (2025) 12(2):645–673\nthe reality of the task they are proposed to solve and undergo a long and thorough \ntesting time before being implemented. Plagiarism tools are already implemented \nin most universities to make the scientific writing more rigorous and avoid devi-\nant behaviors. The implementation of AI writing detection tools might undergo the \nsame process of verifying their adaptability to the organization. It is crucial that, \nonce the correct model was defined, it goes through an experimental period in which \nit is tested in real world scenarios. This experimental period should confirm the \nadaptability and accuracy of the tool as well as the capability of the infrastructure \navailable.\nAs Chugh et al., (2023) point out in a study on the implementation of tech-\nnologies in higher education, the type of technology being implemented plays a \ncrucial role for the success of its implementation. Therefore, it is critical that the \nusers know the technology and their fundamentals. As Esteve-Mon et al. (2021)\nstates, the success of the implementation of digital tools depends critically on \ntraining and training strategies. Therefore, it is crucial to highlight the need for \ntraining of the users who are the ones evaluating the outputs of the tool. Human \nintelligence will always be crucial and irreplaceable, being the last resort to \nascertain critically the accuracy of the model outputs. It is also interesting to note \nan apparent inherent contradiction of using AI to undo AI’s work of convincingly \nmimicking the conventions of human writing. The implementation of AI tools in \neducation has positive impacts (Chen et al., 2020) and its usage is inevitable. As \ntechnology implementation proceeds, negative effects might be found and they \nwill need to be corrected. This research is focused on the detection of AI writ-\ning in essays at university level where such tools should not be used as a primary \nsource, as they could create a bias in students’ evaluation and jeopardize their \ncritical thinking. Therefore, AI tools could be implemented under close human \nsupervision to detect undesirable behaviors and improve fairness while techno-\nlogical solutions are fomented.\nThe present study holds significant educational implications owing to its innova-\ntive approach to the subject of AI-generated writing. The primary implication lies in \nthe enhancement of essay grading efficiency through the automation of identifying \nAI-generated content, especially in contexts where its use is inappropriate. When \ncombined with other tools like automatic grading, this process stands to become \nmore efficient, less prone to bias, and quicker. Consequently, the promotion of \nacademic integrity can be achieved by identifying and preventing instances of AI-\ngenerated content, thereby curbing plagiarism and fostering originality in academic \nendeavors. Such tools also have the potential to cultivate critical thinking skills. \nHowever, it’s imperative to exercise critical thinking in their utilization. Maintaining \nhuman oversight and critically evaluating outputs, particularly in tasks such as essay \ngrading, is indispensable. Educators face the challenge of striking a balance between \nthe benefits of AI technology and its inherent limitations, ensuring it complements \nrather than supplants human intelligence and critical thinking abilities. Moreover, \nthere’s a pressing need for training and awareness regarding the technology and its \nunderlying principles to effectively evaluate the outputs of AI writing detection tools \nand comprehend their limitations. This becomes especially crucial with models that \nrequire fine-tuning to enhance their effectiveness. The integration of critical thinking \n669\nJournal of Computers in Education (2025) 12(2):645–673 \nand comprehensive training is paramount in mitigating ethical concerns, particularly \nthose pertaining to biases and potential inaccuracies. Educators must remain vigilant \nin addressing these issues to ensure fair and impartial assessments.\nThe process outlined in this research holds promise for broader future develop-\nments that warrant attention. While this study serves as a preliminary exploration, \nshowcasing a functional methodology capable of yielding compelling results, it’s \nessential to recognize that the data compiled thus far is primarily for testing pur -\nposes. Consequently, it becomes imperative to apply this methodology to real-world \ndata with genuine objectives. This approach facilitates the refinement of the model \nthrough a fine-tuning process and enables rigorous testing of its accuracy. Although \nthe methodology demonstrates potential applicability across various domains, it’s \nadvisable to tailor its implementation specifically to academic contexts. Therefore, \nthe methods employed herein should be adapted and applied across different edu-\ncational levels to assess the consistency of results, given the varying nature of writ-\ning styles encountered. For instance, tools deployed at the university level should \nundergo fine-tuning to discern AI-generated writing specific to that academic set-\nting, ensuring precision and relevance in detection. Moreover, it’s crucial to employ \nthese methods with diverse datasets comprising students from varied backgrounds \nand academic years, following a panel data structure. This approach enables the \nassessment of accuracy while also facilitating the identification of potential biases.\nConclusion\nThis research explores the impact of the Transformer methodology, particularly in \nthe realm of Natural Language Processing (NLP). The proposed classification task, \naimed at identifying texts generated by AI, presents a valuable application in the \ncontext of essay grading and control tasks for university-level students. Leveraging \npre-trained Transformer models, this study highlights the significance of fine-tuning \nprocesses in enhancing model performance. Even without fine-tuning, BERT dem-\nonstrated superiority over its counterparts, and after a fine-tuning process, it exhib-\nited improvements in key performance metrics.\nAddressing the limitations observed in existing AI writing detection tools, this \nresearch underscores the importance of task-specific fine-tuning to enhance model \nreliability and accuracy. The proposed methodology emphasizes the need to tailor \nthe fine-tuning process to the specific writing level, style, and context. Moreover, the \nsignificance of data cleaning, optimal epoch determination, and noise reduction in \nthe preliminary stages of the process is highlighted.\nWhile acknowledging the initial phase of AI writing detection tools, this research \nsuggests caution in their implementation due to potential biases. As technology, \nincluding AI tools, continues to impact academia and education, it is crucial to \napproach their implementation with care and consideration. The study reinforces the \nimportance of thorough testing and adaptation to the specific organizational context \nbefore the widespread deployment of AI writing detection tools. Human intelligence \nremains indispensable, serving as the ultimate arbiter to critically evaluate model \noutputs.\n670 Journal of Computers in Education (2025) 12(2):645–673\nPractical implications\nThis research can have important implications in several fields and tasks. Particu-\nlarly, we have applied our approach to education, as this area will most certainly be \nhighly affected by the development of AI. The use of AI shows significant benefits \nin improving learning outcomes and providing personalized education. However, \nthe development of tools that allow for automated text generation, such as GPT-4, \ncan raise concerns about academic integrity and individual reasoning. The find-\nings of the study suggest that pre-trained transformer models can effectively detect \nwhether a given text was written by AI or by humans, which could help in identify -\ning instances of academic dishonesty. This technology could be utilized by educa-\ntional institutions to monitor the use of automated text generation tools and ensure \nthat students are developing their own reasoning and critical thinking skills.\nIt is likely that such tools will be present more often in the future in other areas \napart from education. This study may also have provided a contribution to this appli-\ncation and to the discussion of AI tools capable of generating text via the input of \nprompts. The study raises the awareness of the importance of developing human \ncapabilities such as critical thinking and reasoning while adopting disruptive solu-\ntions that can certainly improve the way of teaching and learning, but also innumer -\nous other aspects of daily life.\nLimitations and suggestions\nAlthough the interesting and encouraging results of this research, it is crucial to \nhighlight some if its limitations. First, it is important to highlight the use of pre-\ntrained models for a text classification task. The BERT model has revealed to be \nparticularly accurate in this task. However, it can provide different results for differ -\nent tasks or for different types of text. Second, and in order to guarantee accuracy, it \nis crucial to fine-tune the model. Nevertheless, the success of the fine-tuning process \nis completely dependent on the database’s quality. The data should be as similar as \npossible to the one to which the model will be finally applied. Third, this study con-\ncluded that 20 epochs of fine-tuning process provide very accurate results. Neverthe-\nless, this can differ depending on the task, on the database used, or the complexity of \nthe real-world text. Fourth, the computational resources used were limited to show \nevidence of the application of such approach to real world scenarios. However, this \nalso limits the amount of data used in the fine-tuning process and may make the pro-\ncess much slower. The approach proposed will become more accurate as the amount \nof data used for fine-tuning increases. Fifth, it is clear that the potential of trans-\nformer models in detecting text written by AI is large. Nonetheless, the approach \nproposed is not definitive and certainly has margin for improvement. It is crucial \nto bear in mind the possibility for error in such tools, particularly dependent on the \nfine-tuning process. This limitation is visible in the experiment conducted where \napplying a model fine-tuned using GPT-3.5 text in texts generated by Bard, revealed \nlower performances. Hence, the fine-tuning process should be adapted to the spe-\ncific task requested and might not show similar performance levels for other tasks \n671\nJournal of Computers in Education (2025) 12(2):645–673 \n(e.g.: the model fine-tuned with the simple purpose of detecting purely AI generated \ntext might show lower performance when detecting paraphrasing). Furthermore, the \nbias propensity of such AI tools must not be overlooked and should also be consid-\nered when applying these solutions.\nThe approach here proposed will benefit from testing the results in much larger \ndatabases. This will improve the certainty that our proposed approach is reliable for \nlarger sources of data. This should also be done using other data types and not only \nabstracts of academic papers. Furthermore, the need to test this approach with real \nexperimental data is crucial. The approach here developed should be tested in real \nworld scenarios to ascertain its real accuracy. In real world scenarios several limi-\ntations and drawbacks might arise which were not foreseen in controlled environ-\nments. For instance, the need to analyze an entire document might reveal several \nchallenges as only part of the document might have been generated by an AI model. \nAlthough the tool might reveal effective in such scenarios, this will certainly affect \nthe final percentage result leading to uncertainty. Therefore, the approach here pro-\nposed might also be tested with full documents and not only plain text.\nDue to the high pace at which technology evolves, the approach here proposed \nmight be outdated soon and other more promising solutions might arise. Neverthe-\nless, we aim at contributing to the development of this field and to the discussion of \nsuch pressing matters. Furthermore, it is important to notice that our approach is not \nthe only available to perform the task of text classification. It should be possible to \nperform such task using a logit model or a RNN model, if trained and designed for \nsuch purpose. This alternative approach will require a large amount of data. This \nburden is overcome using the pre-trained Transformer model. Nevertheless, it would \nbe important to confirm the accuracy and usability of such approaches. Lastly, we \nsuggest the application of the proposed approach to different languages. In this \nresearch, we have applied our approach only to the English language. Therefore, we \ncannot assume the same behavior in other languages, but it is expected that the pre-\ntrained fine-tuned model will also show a good performance due to its pre-training \nphase.\nAcknowledgements The author José Campino acknowledges the financial support of Fundação para a \nCiência e Tecnologia through the project number PTDC/EGE-ECO/7493/2020.\nFunding Open access funding provided by FCT|FCCN (b-on). The authors declare that no funds, grants, \nor other support were received during the preparation of this manuscript.\nData availability The datasets generated during and/or analyzed during the current study are available \nfrom the corresponding author on reasonable request.\nDeclarations \nConflict of interest The authors have no relevant financial or non-financial interests to disclose.\nEthical approval All of the followed procedures were in accordance with the ethical and scientific stand-\nards. This article does not contain any studies with human participants performed by the author.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \n672 Journal of Computers in Education (2025) 12(2):645–673\nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ \nlicen ses/ by/4. 0/.\nReferences\nAbd-Elaal, E.-S., Gamage, S. H., & Mills, J. E. (2022). Assisting academics to identify computer gener -\nated writing. European Journal of Engineering Education. https:// doi. org/ 10. 1080/ 03043 797. 2022. \n20467 09\nAkgun, S., & Greenhow, C. (2022). Artificial intelligence in education: Addressing ethical challenges in \nK-12 settings. AI Ethics.\nChen, L., Chen, P., & Lin, Z. (2020). Artificial Intelligence in Education: A Review. IEEE.\nCho, K., Merrienboer, B. v., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. \n(2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine \nTranslation. Conference on Empirical Methods in Natural Language Processing. https:// aclan tholo \ngy. org/ D14- 1179. pdf.\nChugh, R., Turnbull, D., Cowling, M. A., Vanderburg, R., & Vanderburg, M. A. (2023). Implementing \neducational technology in Higher Education Institutions: a review of technologies, stakeholder \nperceptions, frameworks and metrics. Education and Information Technologies. https:// doi. org/ 10. \n23919/ EECSI 56542. 2022. 99465 79\nClark, K., Luong, M.-T., Le, Q. V., & Manning, C. D. (2020). ELECTRA: Pre-training Text Encoders as \nDiscriminators Rather Than Generators. ICLR 2020. https:// arxiv. org/ pdf/ 2003. 10555. pdf.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-XL: atten-\ntive language models beyond a fixed-length context. ACL. https:// arxiv. org/ pdf/ 1901. 02860. pdf.\nDevedžić, V. (2004). Web intelligence and artificial intelligence in education. Educational Technology & \nSociety.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding. In Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technology. https:// arxiv. org/ pdf/ \n1810. 04805. pdf.\nDimitriadou, E., & Lanitis, A. (2023). A critical evaluation, challenges, and future perspectives of \nusing artificial intelligence and emerging technologies in smart classrooms. Smart Learning \nEnvironments.\nEsteve-Mon, F. M., Postigo-Fuentes, A. Y., & Castañeda, L. (2021). A strategic approach of the crucial \nelements for the implementation of digital tools and processes in higher education. Higher Educa-\ntion Quarterly. https:// doi. org/ 10. 1111/ hequ. 12411\nFrançois, T., & Miltsakaki, E. (2012). Do NLP and machine learning improve traditional readability for -\nmulas? NAACL-HLT 2012. https:// aclan tholo gy. org/ W12- 2207. pdf.\nJohnson, D., Goodman, R., Patrinely, J., Stone, C., Zimmerman, E., Donald, R., Wheless, L. (2023). \nAssessing the accuracy and reliability of AI-generated medical responses: an evaluation of the chat-\nGPT model. Research Square.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., & Wu, Y. (2016). Exploring the limits of language \nmodeling.\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). Skip-\nthought vectors. Advances in Neural Information Processing Systems.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). ALBERT: a lite BERT \nfor self-supervised learning of language representations. ICLR 2020. https:// arxiv. org/ pdf/ 1909. \n11942. pdf.\nLiang, W., Yuksekgonul, M., Mao, Y., Wu, E., & Zou, J. (2021). GPT detectors are biased against non-\nnative English writers. Cell Press. https:// doi. org/ 10. 1016/j. patter. 2023. 100779\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Stoyanov, V. (2019). RoBERTa: a robustly opti-\nmized BERT pretraining approach.\n673\nJournal of Computers in Education (2025) 12(2):645–673 \nLoshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. ICLR. https:// arxiv. org/ pdf/ \n1711. 05101. pdf.\nLund, B. D., & Wang, T. (2023). Chatting about ChatGPT: how may AI and GPT impact academia and \nlibraries? Library Hi Tech News.\nMathew, A. (2023). Is artificial intelligence a world changer? a case study of OpenAI’s chat GPT. Recent \nProgress in Science and Technology.\nMcMurtrie, B. (2023). AI and the future of undergraduate writing. Retrieved from The Chronicle of of \nHigher Education: https:// www. chron icle. com/ artic le/ ai- and- the- future- of- under gradu ate- writi ng\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models.\nO’Shea, K., & Nash, R. (2015). An introduction to convolutional neural networks.\nPeters, M. E., Neumann, M., Iyyer, M., & Gardner, M. (2018). Deep contextualized word representations. \nNAACL-HLT 2018. https:// aclan tholo gy. org/ N18- 1202. pdf.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsu-\npervised multitask learners. OpenAI Blog.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Li, W. (2020). Exploring the limits \nof transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research.\nRudolph, J., Tan, S., & Tan, S. (2023). ChatGPT: Bullshit spewer or the end of traditional assessments in \nhigher education? Journal of Applied Learning & Teaching.\nSalehinejad, H., Sankar, S., Barfett, J., Colak, E., & Valaee, S. (2017). Recent advances in recurrent neu-\nral networks.\nShahmohammadi, H., Dezfoulian, M., & Mansoorizadeh, M. (2020). Paraphrase detection using LSTM \nnetworks and handcrafted features. Multimedia Tools and Applications. https:// doi. org/ 10. 1007/ \ns11042- 020- 09996-y\nShi, W., & Demberg, V. (2019). Next sentence prediction helps implicit discourse relation classification \nwithin and across domains. In Proceedings of the 2019 Conference on Empirical Methods in Natural \nLanguage Processing and the 9th International Joint Conference on Natural Language Processing \n(EMNLP-IJCNLP). https:// aclan tholo gy. org/ D19- 1586. pdf.\nSinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., & Kiela, D. (2021). Masked language modeling \nand the distributional hypothesis: order word matters pre-training for little.\nTaghizadeh, M. E., Abidin, M. J., Naseri, E., & Hosseini, M. (2020). In the importance of EFL learn-\ners’ writing skill: is there any relation between writing skill and content score of english essay test? \nSciPress Ltd.\nTien, N. M., & Labbe, C. (2017). Detecting automatically generated sentences with grammatical struc-\nture similarity. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 018- 2789-4\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Polosukhin, I. (2017). \nAttention Is All You Need. In 31st Conference on Neural Information Processing Systems. https:// \narxiv. org/ pdf/ 1706. 03762. pdf.\nWeber-Wulff, D., Anohina-Naumeca, A., Bjelobaba, S., Foltýnek, T., Guerrero-Dib, J., Popoola, O., & \nWaddington, L. (2023). Testing of detection tools for AI-generated text. International Journal for \nEducational Integrity. https:// doi. org/ 10. 1007/ s40979- 023- 00146-z\nXu, W., & Ouyang, F. (2022). The application of AI technologies in STEM education: a systematic \nreview from 2011 to 2021. International Journal of STEM Education.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q. V. (2019). XLNet: Generalized \nAutoregressive Pretraining for Language Understanding. 33rd Conference on Neural Information \nProcessing Systems. https:// arxiv. org/ pdf/ 1906. 08237. pdf.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\nJosé Campino PhD professor and researcher with a track record of international publications and par -\nticipation in international events. Has coordinated several classes, including international classes, in \nthe areas of management and strategy. Experienced professional in the financial/banking area who has \nworked for internationally recognized institutions. The research interests concern innovation, manage-\nment, strategy, entrepreneurship, and finance.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6747415661811829
    },
    {
      "name": "Transformer",
      "score": 0.667395830154419
    },
    {
      "name": "Computer science",
      "score": 0.657476544380188
    },
    {
      "name": "Realm",
      "score": 0.6125199198722839
    },
    {
      "name": "Vocabulary",
      "score": 0.5148529410362244
    },
    {
      "name": "Applications of artificial intelligence",
      "score": 0.4770640432834625
    },
    {
      "name": "Natural language processing",
      "score": 0.4620387554168701
    },
    {
      "name": "Language model",
      "score": 0.4159160256385803
    },
    {
      "name": "Machine learning",
      "score": 0.36729955673217773
    },
    {
      "name": "Engineering",
      "score": 0.14204418659210205
    },
    {
      "name": "Linguistics",
      "score": 0.11418068408966064
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I83558840",
      "name": "Universidade Nova de Lisboa",
      "country": "PT"
    }
  ]
}