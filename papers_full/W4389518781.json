{
    "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning",
    "url": "https://openalex.org/W4389518781",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3004075532",
            "name": "Quanyu Long",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2122265056",
            "name": "Wen-Ya Wang",
            "affiliations": [
                "Nanyang Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A3011006640",
            "name": "Sinno Pan",
            "affiliations": [
                "Nanyang Technological University",
                "Chinese University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2296283641",
        "https://openalex.org/W3156636935",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W2927746189",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2032566933",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2889774592",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4286769130",
        "https://openalex.org/W2760505947",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W2971277088",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W3104055036",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2787423662",
        "https://openalex.org/W3095594087",
        "https://openalex.org/W2153353890",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4287888012",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2757947833",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W3166720688",
        "https://openalex.org/W2346452181",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2962902328",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3173727191"
    ],
    "abstract": "Large language models (LLMs) have showcased their capability with few-shot inference known as in-context learning. However, in-domain demonstrations are not always readily available in real scenarios, leading to cross-domain in-context learning. Besides, LLMs are still facing challenges in long-tail knowledge in unseen and unfamiliar domains. The above limitations demonstrate the necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study the UDA problem under an in-context learning setting to adapt language models from the source domain to the target domain without any target labels. The core idea is to retrieve a subset of cross-domain elements that are the most similar to the query, and elicit language model to adapt in an in-context manner by learning both target domain distribution and the discriminative task signal simultaneously with the augmented cross-domain in-context examples. We devise different prompting and training strategies, accounting for different LM architectures to learn the target distribution via language modeling. With extensive experiments on Sentiment Analysis (SA) and Named Entity Recognition (NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer and demonstrate significant improvements over baseline models.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6525–6542\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAdapt in Contexts: Retrieval-Augmented Domain Adaptation via\nIn-Context Learning\nQuanyu Long1 Wenya Wang1 Sinno Jialin Pan1,2\n1Nanyang Technological University, Singapore\n2The Chinese University of Hong Kong\n{quanyu001, wangwy}@ntu.edu.sg sinnopan@cuhk.edu.hk\nAbstract\nLarge language models (LLMs) have show-\ncased their capability with few-shot inference\nknown as in-context learning. However, in-\ndomain demonstrations are not always readily\navailable in real scenarios, leading to cross-\ndomain in-context learning. Besides, LLMs\nare still facing challenges in long-tail knowl-\nedge in unseen and unfamiliar domains. The\nabove limitations demonstrate the necessity of\nUnsupervised Domain Adaptation (UDA). In\nthis paper, we study the UDA problem under\nan in-context learning setting to adapt language\nmodels from the source domain to the target\ndomain without any target labels. The core\nidea is to retrieve a subset of cross-domain ele-\nments that are the most similar to the query, and\nelicit language model to adapt in an in-context\nmanner by learning both target domain distri-\nbution and the discriminative task signal simul-\ntaneously with the augmented cross-domain in-\ncontext examples. We devise different prompt-\ning and training strategies, accounting for dif-\nferent LM architectures to learn the target dis-\ntribution via language modeling. With exten-\nsive experiments on Sentiment Analysis (SA)\nand Named Entity Recognition (NER) tasks,\nwe thoroughly study the effectiveness of ICL\nfor domain transfer and demonstrate significant\nimprovements over baseline models.\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated remarkable success in various tasks via in-\ncontext learning (ICL) with task instructions and\nfew-shot demonstrations (input-label pairs) (Zhao\net al., 2021; Liu et al., 2022; Min et al., 2022), elim-\ninating the need for fine-tuning from task-specific\nlabels. Nevertheless, in-domain demonstrations\nare usually absent in real scenarios since the target\ndomain labels are unavailable. Sourcing labeled ex-\namples from other domains may suffer from huge\nsyntactic and semantic domain shifts. Moreover,\nLMs \nSource input:\nIn the study at the University's Institute for \nHuman Gene Therapy, researchers altered \na common-cold virus to carry a version of \nthe working dystrophin gene.\nTarget contexts：\nOur oganization finds structural and \ndevelopmental expression pattern of the \nmouse WD - repeat gene DMR - N9 \nimmediately upstream of the myotonic \nDystrophy Locus.\nThe XPR2 gene from Yarrowia lipolytica \nencodes an inducible alkaline extracellular \nprotease.\n…… Biomedical Literature\nNews article\n+\nLanguage \nModeling\nEntity \nPrediction\nQuery\nRetrieved examples\nAugmenting\nFigure 1: A motivating example of retrieval-augmented\nin-context adaptation for NER: biomedical texts re-\ntrieved from the target unlabeled domain will serve\nas demonstrative contexts to help LMs correctly pre-\ndict entities \"Institute for Human Gene Therapy\" and\n\"dystrophin gene\" (solid arrow). The language model\ntransfers the knowledge to the target domain to identify\nunknown entities with a similar structure like \"XPR2\ngene\" or \"dystrophy locus\" by learning target distribu-\ntion with language modeling (dotted arrow).\nLLMs are prone to generating unpredictable out-\nputs in undesired formats, and they are struggling\nwith long-tail knowledge for unseen and unfamil-\niar domains where topics and genres are less fre-\nquently encountered in the training corpus (Asai\net al., 2023). Therefore, the limitations above call\nfor effective adaptation strategies to transfer knowl-\nedge of LMs from a labeled source domain to the\nunlabeled target domain, known as Unsupervised\nDomain Adaptation (UDA).\nTo bridge the domain gap, UDA aims to adapt\nmodels that learn domain-agnostic features from\nlabeled source samples and unlabeled target sam-\nples. Some studies have proposed discrepancy mea-\nsures to align source and target distributions in the\nrepresentation space (Ganin et al., 2016; Ye et al.,\n2020; Long et al., 2022). However, these methods\nmainly focus on feature alignment and only ap-\nply to encoder-based LMs. Other studies focus on\nadaptive pre-training including an additional post\n6525\npre-training phase of masked language modeling\n(MLM) on target unlabeled data to learn the tar-\nget domain distribution (Han and Eisenstein, 2019;\nKarouzos et al., 2021). However, different training\nphases make the learned diverse knowledge hard to\nremember, and such methods are also only applica-\nble to encoder-only LMs which are usually smaller\nin scale. Therefore, few studies have investigated\nhow to update knowledge of unfamiliar domains\nfor larger LMs (e.g., decoder-only LMs). And few\nstudies try to relate source-labeled samples to tar-\nget unlabeled examples in a single training stage,\nwhile vast amounts of target unlabeled data can\nserve as a knowledge-rich datastore.\nIn this paper, we propose to retrieve similar ex-\namples from the target unlabeled corpus to serve\nas the context of a source query and perform adap-\ntive in-context learning by concatenating the source\nquery and target contexts as the input prompt. The\ncore idea is to elicit LMs to learn target distribu-\ntion and discriminative task signals simultaneously\nwith the retrieved cross-domain examples. Fig. 1\nshows an illustrative example. For each input from\nthe source domain, we compose its context with\nsemantically similar texts retrieved from the target\nunlabeled domain to enrich semantics and reduce\nthe domain difference in the surface form. Then\nthe model will learn the task discrimination taking\nboth the source input and the target context. To\nfurther mitigate domain shift, we propose to learn\nthe target distribution using the language modeling\nmechanism (causal or masked language modeling)\nsimultaneously by predicting tokens from the target\ncontext, which acts as a proxy to the target distri-\nbution. Combining the two goals encourages the\nmodel to learn domain-agnostic and task-aware in-\nformation which is beneficial for knowledge trans-\nfer.\nWe propose a domain-adaptive in-context learn-\ning (DAICL) framework for different LM archi-\ntectures, including encoder-only and decoder-only\nmodels, and observe consistent advantages. To ac-\ncount for the architectural difference, we devise\ndistinct prompting and fine-tuning strategies. For\nthe encoder-only model, we append contexts re-\ntrieved from the target domain to each source input.\nThe model is trained to predict source input labels\nand masked tokens in the appended contexts. For\nthe decoder-only model, we instead prepend exam-\nples before the source input. The model is trained\nto predict each token autoregressively in the prompt\nas well as the response output.\nOverall, we make the following contributions:\n• We propose domain-adaptive in-context learn-\ning with retrieval augmentation in which we\nmix the source input and semantically rich tar-\nget contexts to learn two in-context objectives\nsimultaneously;\n• We proposed a unified framework with effi-\ncient prompting and fine-tuning strategies ac-\ncounting for different architectures (encoder-\nonly LMs and decoder-only LMs);\n• We thoroughly study the effectiveness of\nin-context learning for UDA. Our experi-\nments surprisingly reveal that retrieving out-\nof-distribution demonstrations fails for LLMs’\nfew-shot inference and fine-tuning is still ben-\neficial for domain adaptation.\n2 Problem Definition\nConsider a scenario where we have access to two\ndistinct domains: a source domain and a target\ndomain. The source domain dataset, denoted\nas DS, consists of n labeled data sampled i.i.d.\nfrom the source distribution, DS = {xS\ni ,yi}1,...,n,\nwhere xS\ni represents sequences of tokens, yi repre-\nsents the corresponding label. On the other hand,\nthe unlabeled target domain dataset, denoted as\nDT = {xT\nj }1,...,m, comprises m unlabeled data\npoints, which are also sampled i.i.d. from the target\ndomain. The primary objective of Unsupervised\nDomain Adaptation (UDA) is to adapt the knowl-\nedge learned from the source domain in such a way\nthat allows it to generalize on the target domain\neffectively. This adaptation process involves lever-\naging the unlabeled data from the target domain to\nlearn the target distribution and mitigate the domain\nshift.\nThis paper focuses on the UDA problem over\ntwo application scenarios: Named Entity Recog-\nnition (NER)1 and Sentiment Analysis (SA). We\ndescribe our method and pivot discussions around\nthese two tasks in the following sections.\n3 Method\nWe propose a novel framework, Domain Adaptive\nIn-Context Learning (DAICL), capable of training\n1In our work, we only need to predict the entity spans,\nignoring the entity type, because the label spaces for different\ndomains are different when considering entity type. However,\nwe still refer to this task as NER for short.\n6526\nTransformer-based Encoder\nVx V[SEP] V[MASK]\n… E[SEP] E[MASK]\nCRF \nSource domain input:\nTarget domain unlabeled corpus: \n……\n……\n……….\n………….\n…………..\nOur oganization finds structural and developmental expression \npattern of the mouse WD - repeat gene DMR - N9 immediately \nupstream of the myotonic Dystrophy Locus.\nIn the study at the University's Institute for Human Gene \nTherapy, researchers altered a common-cold virus to carry a \nversion of the working dystrophin gene.\nquery\nThe XPR2 gene from Yarrowia lipolytica encodes an inducible \nalkaline extracellular protease.\n……\n MASK\nEx … … …\n… … … …\nFigure 2: An overview of training encoder-only NER models with retrieved context via in-context learning.\nLMs to adapt with the help of retrieved contexts.\nWe begin by introducing the overall framework\nin Section 3.1. Next, we present specific designs\nfor Encoder-only language models in Section 3.2;\nand Decoder-only language models in Section 3.3.\nFor decoder-only models, we present two settings:\ninference-only (Section 3.3.1) and fine-tuning (Sec-\ntion) 3.3.2.\n3.1 In-Context Adaptation\nThe term In-Context Learninghas been commonly\nreferred to as few-shot prompting in LLMs. To be\nclear, in this work, we instead useIn-Context Learn-\ning to emphasize the idea of learning a model with\nsemantically rich contexts. Here context should\nbe differentiated with demonstration, the latter one\nrepresents input-label pairs in few-shot prompting.\nUnder the setting of UDA where target labels are\nnot accessible, context is composed of input-only\nexamples from the unlabeled target domain. Next,\nwe present an overall framework to construct suit-\nable contexts and adapt LMs with suitable contexts.\n3.1.1 Context Construction with Retrieval\nGiven an input sentence from the source domain,\nwe first search for semantically similar examples\nfrom the unlabeled target domain. This is analo-\ngous to retrieval and re-rank given a search query.\nRetrieval-augmented LM approaches (Guu et al.,\n2020; Lewis et al., 2020; Asai et al., 2023) ap-\nply a parametrized dense retriever to train with\nthe task model. In this paper, we fix the retriever\npart and use the off-the-shelf scoring language\nmodels. For the SA task, we use SimCSE (Gao\net al., 2021) which produces semantically meaning-\nful sentence embeddings after being trained with\ncontrastive learning (Chen et al., 2020; He et al.,\n2020). Here cosine similarity is used to retrieve\ntop-ranked (most similar) examples from the tar-\nget domain. For NER, we use BERTScore (Zhang\net al., 2020; Wang et al., 2021), because it gives a\nmetric for each sentence based on the similarity of\ntoken representation, which is more crucial for the\ntask of NER.\nSpecifically, given a source sentence xS paired\nwith label y, we retrieve top- k relevant chunks\nof texts from the target unlabeled dataset DT.\nThe retrieved examples are denoted as xT =\n{xT\n1 ,··· ,xT\nk}which will serve as the contexts to\nenrich the semantics for the source input.\n3.1.2 Domain-Adaptive In-Context Learning\nWith the retrieved context consisting of kmost se-\nmantically similar examples to the source input,\nwe seek a strategy to integrate this context into\nthe source input and design a training objective\nthat could learn target distribution and at the same\ntime be able to discriminate the task label. To\nthis end, we propose to combine the following two\nobjectives given the concatenated text sequences\n[xS; xT]. Objective 1: In-context Task Learning–\na supervised task to predict the task label y. Objec-\ntive 2: In-context Language Modeling – a token\nprediction task to predict tokens from the target\ncontext xT:\nLSup(θ) =−log Prθ\n(\ny\n⏐⏐xS,xT)\n; (1)\nLLM(θ) =−log Prθ\n(\ntT\ni\n⏐⏐xS,xT)\n,tT\ni ∈xT, (2)\nwhere θrepresents the parameters for a language\nmodel. Ideally, the first objective (1) aims to learn\ntask discrimination with the help of context. Note\nthat unlike single-domain task prediction which\nonly takes xS as input, here we augment the source\ninput with target contexts to learn task-aware infor-\nmation across domains. The second objective (2)\n6527\nencourages the model to learn the target distribu-\ntion by predicting tokens in the target context xT.\nBy mixing with a source input, the model learns to\nfuse the distributions from two different domains\nin order to bridge the domain gap. When combin-\ning these two objectives, we expect that the model\nlearns task-aware knowledge that is indistinguish-\nable from the two domains.\n3.2 Encoder-only LMs with ICL\nThis section describes domain-adaptive in-context\nlearning with encoder-only LMs, e.g., BERT (De-\nvlin et al., 2019). As discussed in Section 3.1, for\neach input xS, we first retrieve top-ksentences xT\nfrom the target domain as the context for xS. For\nencoder-only models, the retrieved sentences are\nthen concatenated at the end of the source input.\n[xS; xT] =\n[\nxS; ⟨SEP⟩; xT\n1 ; ··· ; xT\nk\n]\n, (3)\nwhere ⟨SEP⟩is a separation token.\nTo perform in-context learning, recall from Sec-\ntion 3.1, two objectives (language modeling and\ntask learning) are involved. An overview of the\ntraining process for encoder-only models on the\nNER task is shown in Fig. 2. For the language mod-\neling objective, we perform unsupervised Masked\nLanguage Modeling (MLM) on the target domain.\nWe randomly sample 15% tokens from the target\ncontext [xT\n1 ; ··· ; xT\nk] and replace them with the\n[MASK] token. We denote the set of indices for the\nmasked tokens as M and the original ground-truth\ntokens for these masked positions are referred to\nas tT\nM = {ti|i∈M}. The masked input becomes\n[x; xT\nM], where xT\nM denotes the collection of tar-\nget contexts after masking. With the bidirectional\nstructure of the encoder-only LMs, the represen-\ntation for each masked token in the target domain\nencodes both the target context and the source in-\nput. As such, the MLM objective encourages the\nencoder to learn the target distribution that is indis-\ntinguishable from the source domain.\nFor the task objective, we use different predic-\ntion mechanisms for different tasks. For SA, we\nuse average pooling on top of each token in the\nsource input xS before being fed into the classifier.\nFor NER, we apply an additional CRF layer (Ma\nand Hovy, 2016; Lample et al., 2016) on top of\nthe LM feature which is a common practice for\ntoken-level classifications.\nFormally, the joint objective is to minimize the\nnegative log-likelihood of the ground truth task\nlabel yand masked tokens tT\nM:\nmin\nθ\n∑\n(xS,y)∼DS\n−\n[\nlog Prθ(y|xS,xT\nM)\n+ λlog Prθ(tT\nM|xS,xT\nM)\n]\n,\n(4)\nwhere λrepresents a scaling factor.\n3.3 Decoder-only LMs with ICL\nRecently, decoder-only LMs have received exces-\nsive attention and have motivated continuous devel-\nopments to scale up in order to solve various NLP\ntasks under zero-shot or few-shot settings, such\nas GPT-3 (Brown et al., 2020), LLaMA (Touvron\net al., 2023), and ChatGPT. Despite the increasing\nscalability, they are still prone to producing unpre-\ndictable outputs in undesired formats. For example,\nChatGPT gives subpar performance for NER (see\nTable 1). This reflects the necessity of decoder-only\nLMs for learning to adapt to the target domain.\n3.3.1 Cross-Domain Few-Shot Inference\nRecent works show that providing few-shot ICL\ndemonstrations (input-label pairs) contributes to\nperformance gains (Zhao et al., 2021; Liu et al.,\n2022; Min et al., 2022). However, there are no\nin-distribution demonstrations available when per-\nforming inference on the unlabeled target domain.\nTherefore, in many real scenarios, we often select\nout-of-distribution (OOD) input-label pairs from\nanother domain irrespective of the possible huge\ndomain shift from the target query. In UDA, we\nhave access to the entire labeled source dataset,\nthus we could retrieve similar demonstrations from\nthe source domain given a target query. We pro-\nvide prompts and examples in Fig. 3 showing how\nto use retrieved input-label pairs from the source\ndomain as demonstrations.\nIn our experiments with ChatGPT (see Table.\n1 and Table. 2), surprisingly we find that re-\ntrieving OOD demonstrations fails in most adap-\ntation scenarios; even randomly sampling cross-\ndomain demonstrations can bring non-trivial per-\nformance gains comparing with the retrieval ap-\nproaches. However, fine-tuning much smaller LMs\nwith in-context domain adaptation gives the best\nperformances in most cases in our experiments.\nThis phenomenon suggests we still need to fine-\ntune decoder-only LMs to update specific domain\nknowledge which will be discussed in the next sec-\ntion.\n6528\nNamed Entity Span Prediction Sentiment Analysis\nPlease identify all entities from the input sentence. If there is no \nentity, please output None.\nGiven the input sentence, assign a sentiment label from ['positive', \n'neutral', 'negative']. Return label only without any other text.\nSentence: In the study at the University's Institute for Human Gene Therapy, \nresearchers altered a common-cold virus to carry a version of the working \ndystrophin gene.\nEntity: Institute for Human Gene Therapy, dystrophin gene\nSentence: The virus, which also was altered to minimise its susceptibility to the \nimmune system, was then injected into the muscle cells of mice bred to lack \ndystrophin genes.\nEntity: dystrophin genes\nSentence: Both drugs are types of interferon.\nEntity: None\n……\nSentence: This cable works well with my best thing is that it is long so you don't \nhave to be right next to your TV to review your photos and video.\nLabel: positive\nSentence: Very good and normal quality, you can use it instead of originals \ncheap suitable for Canon. I haven't more words but the site required more words \nso i wrote that.\nLabel: positive\nSentence: I don't use the headset every day but it seems not bad.\nLabel: neutral\n……\nSentence: Physical mapping 220 kb centromeric of the human MHC and \nDNA sequence analysis of the 43 - kb segment including the RING1, HKE6, \nand HKE4 genes.\nEntity:\nSentence: Spiritually and mentally inspiring! A book that allows you to \nquestion your morals and will help you discover who you really are!\nLabel: \nSource Source\nTarget Target\nFigure 3: Examples with prompts for inference. Different from fine-tuning setting that uses target unlabeled dataset\nas the retrieval corpus, for inference setting, we search input-label pairs from the source labeled dataset given a\ntarget test query. Dotted boxes contain demonstrations retrieved from the source.\nSentiment Analysis\nBelow is an instruction that describes a task. Write a response that appropriately \ncompletes the request.\n###Instruction:\nGiven the input sentence, assign a sentiment label from ['positive', 'neutral', \n'negative']. Return label only without any other text.\nSentence: The lyrics of this song are so powerful and heartening.\nSentence: Love this album! It is absolutely uplifting I would recommend it to \nanyone it deserves 5 stars and more.\nSentence: This CD is banging from start to finish a few of the hooks stopped me \nfrom giving it 5 stars but this is a must have.\nInput sentence: This is one my must have books since it is a masterpiece of \nspirituality, the message behind it is so powerful that you have to read it it \nwill take you to enlightenment.\n###Response: positive\nFigure 4: Illustration of crafted training example\n[prompt; xT ; xS; y], dotted box contains k = 3input-\nonly demonstrations from target unlabeled dataset.\n3.3.2 Fine-tuning\nIn this work, we fine-tune LLaMA (Touvron et al.,\n2023) with a parameter efficient approach, i.e.,\nLow-Rank Adaptation (LoRA) (Hu et al., 2021).\nLoRA maintains the weights of pre-trained LMs\nwhile introducing trainable rank decomposition ma-\ntrices into each transformer layer, making it feasi-\nble to fine-tune larger LMs with much fewer com-\nputational resources 2.\nSimilar to the method proposed in Section 3.2,\nwe first retrieve top-k contexts from the target unla-\nbeled set, given a source input query. We then\ninsert these contexts in between the instruction\nand the source input sentence 3 (see an example\nin Fig. 4). Next, we finetune the decoder-only LMs\ngiven the crafted example [prompt; xT; xS; y] =\n[t0,t1,t2,···] and the source label. Specifically,\nwith the Casual Language Modeling (CLM) mech-\n2In our experiment, trainable parameters only account for\n0.24% of the entire LLaMA-7B parameters.\n3We follow the template from Standford Alpaca (Taori\net al., 2023)\nanism, the objective is to predict the next token\nti:\nmin\nθ\n∑\ni\n−log Pr\nθ\n(ti|t0,t1,··· ,ti−1). (5)\nDifferent from section 3.2, for decoder-only\nLMs, the retrieved target contexts xT need to be\npositioned before the source input xS as the model\nwill learn in an autoregressive manner. Moreover,\ninstead of only calculating token prediction loss\non the response/output ywhich is adopted for the\nIn-context Task Learning objective as discussed\nin Section 3.1, we propose to compute the loss\non every token within [prompt; xT; xS; y]. Objec-\ntive (5) can be decomposed into two objectives:\n1) When ti ∈xT, the loss corresponds to token\npredictions in the target domain, analogous to the\nin-context language modeling objective; 2) When\nti ∈y, the loss relates to in-context task learning\nwhich aims to generate task label given both tar-\nget contexts and the source input. The objective\n(5) thus merges two proposed in-context objectives\ninto a unified function.\n4 Experiments\n4.1 Datasets\nNER datasets We experiment on 7 NER datasets\ncovering four domains: News, Social media, Fi-\nnancial, and Biomedical. Under the News domain,\nCoNLL-03 English dataset (Sang and De Meulder,\n2003) is the most popular NER dataset, and we treat\nit as the source domain dataset. The other three do-\nmains serve as target domains. For the Social Me-\ndia domain, we use WNUT-16 (Strauss et al., 2016)\nand WNUT-17 (Derczynski et al., 2017) collected\nfrom Twitter. For the Financial domain, we use\n6529\nFIN (Alvarado et al., 2015) which is a dataset of\nfinancial agreements. For the Biomedical domain:\nwe use BC2GM (Smith et al., 2008), BioNLP09\n(Kim et al., 2009), and BC5CDR (Li et al., 2016).\nNote that for different domains, entity types are\ndifferent. For unsupervised domain adaptation, to\nensure source and target domains share the same\nlabel space, we remove the entity types and convert\nall label formats to the BIO scheme4, similar to the\nproblem of entity span prediction.\nSentiment Analysis datasets We use the Amazon\nreview dataset (He et al., 2018) which contains four\ndomains: Book (BK), Electronics (E), Beauty (BT),\nand Music (M). The original crawled reviews con-\ntain star ratings (1 to 5 stars). Following previous\nwork (He et al., 2018; Ye et al., 2020), we label\nthem with rating < 3, > 3, = 3as negative, pos-\nitive, and neutral respectively. There are in total\n12 adaptation scenarios, and we select 6 of them in\nour experiment.\nStatistics and the data splits of all the datasets\ncan be found in Appendix A.\n4.2 Experiment Configurations\nFor our retrieval system, we use SimCSE Roberta-\nLarge (Gao et al., 2021) trained on NLI datasets5\nas the retrieval model for the SA task, and use\nRoBERTa-large (Liu et al., 2019) for BERTScore\n(Zhang et al., 2020) for the NER task 6. We set\nk = 5 for top- k retrieval from the target do-\nmain. For the encoder-only model, we select\nXLM-RoBERTa-large (Conneau et al., 2020) as\na basis which has 561M parameters. For the\ndecoder-only model, we use LLaMA-7B7 (Touvron\net al., 2023) and fine-tune it with LoRA (Hu et al.,\n2021). For inference-only LMs, we choose Chat-\nGPT and LLaMA-Alpaca8. For ChatGPT, we use\ngpt-3.5-turbo9. LLaMA-Alpaca uses LoRA to\nfine-tune the LLaMA-7B model on Alpaca (Taori\net al., 2023) dataset which is generated with Self-\nInstruct (Wang et al., 2022).\n4.3 Implementation Details\nFor training the RoBERTa model, we fine-tune con-\ntextualized embeddings using AdamW (Kingma\n4For example, entity “Los Angeles” has the label “B-LOC\nI-LOC”, we convert it to “B I”.\n5https://github.com/princeton-nlp/SimCSE\n6https://github.com/Tiiiger/bert_score\n7For computing efficiency, we adopt the 7B model, but our\nmethod can be easily extended to larger LM families.\n8https://huggingface.co/tloen/alpaca-lora-7b\n9May 24 version of ChatGPT is used for experiments.\nand Ba, 2015; Loshchilov and Hutter, 2018). In\nthe experiments on NER datasets, the learning rate\nis set to 1e-5 for RoBERTa and 0.05 for CRF. For\nSA datasets, we set the learning rate to 5e-5 and\nuse a linear scheduler with warm-up steps 10% of\nthe total training steps. The weight factor λin (4)\nequals to 0.2.\nFor LLaMA-LoRA, we set the rank rto be 16,\ndropout rate to be 0.05. Trainable parameters only\naccount for 0.24% of the entire LLaMA-7B param-\neters. We fine-tune LLaMA-LoRA with batch size\n256, learning rate 3e-4, and train 5 epochs with\nearly stopping. With the help of LoRA, each adap-\ntation scenario only requires less than 1 hour of\ntraining time on a single A100 GPU.\n4.4 Results\nWe experiment with the following settings and base-\nlines. For Inference-only experiments:\nNo demo performs zero-shot inference on each\ntarget test input without demonstrations.\nRand demo samples demonstrations randomly\nfrom the source domain.\nRetr demo retrieves top-5 demonstrations from\nthe source domain, corresponding to the approach\nmentioned in Section 3.3.1.\nFor fine-tuning experiments:\nNo-ICL does not retrieve any target context for\neach source input. The model is only trained on\nsource inputs.\nICL-rand investigates the effectiveness of the task\nobjective (1). Instead of retrieval, we randomly\nsample contexts from the target domain. In this\ncase, the model is not exposed to semantically sim-\nilar contexts in the target domain to enhance knowl-\nedge transfer via (1).\nICL-sup only trains the model via the task objec-\ntive (1). This investigates the effectiveness of the\nlanguage modeling objective (2). For the encoder-\nonly model, we do not mask any token. For the\ndecoder-only model, we calculate the loss corre-\nsponding to the response/output positions.\nICL-source further investigates the effectiveness\nof the target contexts. Here we retrieve contexts\nsolely from the source domain instead of the tar-\nget domain. Hence, the model learns to perform\nthe task and language modeling within the source\ndistribution.\nDAICL is our proposed method domain-adaptive\nin-context learning as shown in Section 3.2 and\nSection 3.3.2. As described in Section 3.1, this\n6530\nFinancial Social Media Biomedical\nFIN WNUT-16 WNUT-17 BC2GM BioNLP09 BC5CDR\nInference only\nLLaMA-Alpaca\nNo demo 16.69 14.71 16.20 13.74 16.44 21.64\nRand demo 13.59 23.20 22.10 20.69 24.46 25.83\nRetr demo 20.18 29.5 26.73 22.56 22.98 27.26\nChatGPT\nNo demo 19.60 32.10 33.45 19.90 15.44 37.16\nRand demo 20.82 39.73 39.45 26.92 21.71 37.85\nRetr demo 19.88 38.28 38.10 24.17 18.98 35.71\nFine-tuning\nRoBERTa\nVu et al. (2020) 23.38 47.11 – 30.81 29.24 –\nNo-ICL 24.171.3 68.490.2 63.180.3 27.691.6 33.671.1 21.842.2\nICL-rand 24.912.9 69.26†\n0.6 64.66†\n0.3 30.68†\n1.6 35.19†\n0.9 26.93†\n1.9\nICL-sup 26.24†\n2.1 70.89†\n0.4 65.40†\n0.4 28.071.4 34.110.9 23.20†\n2.2\nICL-source 24.911.2 68.840.3 63.380.2 26.961.8 32.071.2 22.062.0\nDAICL 27.22†\n2.1 71.79†\n0.4 65.79†\n0.2 32.51†\n1.1 36.81†\n0.6 25.92†\n1.8\nLLaMA-LoRA\nNo-ICL 15.20 45.22 53.92 24.24 26.29 25.35\nICL-rand 12.68 42.09 51.08 23.06 21.66 21.28\nICL-sup 15.81 45.70 54.32 24.83 25.00 26.91\nICL-source 14.73 45.30 53.29 24.54 23.92 24.96\nDAICL 14.82 46.51 55.08 26.02 24.21 28.96\nTable 1: F1 results of Named Entity Span prediction tasks. The source domain is the CoNLL-03 dataset, and the\ntarget domains are financial, social media, and biomedical. For RoBERTa, results are reported with average and\nstandard deviation in 5 runs, †represents the model is significantly stronger than the baseline model No-ICL with\np< 0.05. For LLaMA, due to the cost of inference computation, we only perform a single run.\nmethod retrieves related contexts from the target\ndomain and combines two objectives to perform\ndomain-adaptive ICL.\nThe experiment results for NER and SA are illus-\ntrated in Table 1 and Table 2, respectively. Below\nwe conclude with some interesting findings.\nAdaptive ICL benefits UDA by learning two ob-\njectives simultaneously. Given the results in Table\n1 and Table 2, we can observe that our proposed\nmethod DAICL which learns two objectives simul-\ntaneously surpasses baselines with a large margin\nin most adaptation scenarios. From the result of\nICL-sup, we find that training with the task objec-\ntive alone could slightly help UDA. As discussed in\nSection 3, the benefit originates from incorporating\nthe target contexts for task discrimination. By com-\nparing DAICL with ICL-sup and ICL-source, we\ncan conclude that the proposed in-context adapta-\ntion strategy enhances domain adaptation by jointly\nlearning the task signal and language modeling si-\nmultaneously.\nRetrieving OOD examples could be disappoint-\ning for LLMs. From the RoBERTa results of ICL-\nrand, we find that random target contexts can im-\nprove NER (compared with No-ICL) by a small\nmargin. One possible reason is that random con-\ntexts from the target domain could still encourage\nthe model to learn the target distribution via (2).\nHowever, ICL-rand significantly impedes the per-\nformance of Sentiment Analysis. We conjecture\nthat ICL-rand might select target contexts with op-\nposite sentiment labels from the source input, neg-\natively affecting the learning process.\nSurprisingly, ChatGPT with random out-of-\ndistribution (OOD) demonstrations achieves higher\nscores than retrieval in all NER and SA experi-\nments (Rand demo vs. Retr demo). Previous work\nreveals that choosing demonstration examples that\nare close to the test input significantly enhances\nthe effectiveness of ICL (Liu et al., 2022; Rubin\net al., 2022). However, they retrieve from a labeled\ntraining set in which the distributions of the text\nand label space are identical with the test input. In\ncontrast, in transfer setting which is close to the\nreal-world scenario, we only have OOD input-label\npairs from another labeled dataset. We make a\nhypothesis regarding this observation, for cross-\ndomain ICL, providing diverse and distinct OOD\ndemonstrations is more beneficial for LLMs to un-\nderstand the task and generalize.\nFine-tuning is still beneficial for UDA. Under the\nUDA setting where labels only exist in the source\ndomain, we can prompt LLMs with input-label\npairs from the source domain to infer the target la-\nbel (inference-only). Another option is to fine-tune\nsmaller LMs to adapt task-aware knowledge from\nthe source to the target domains. A natural ques-\n6531\nE→BK BT →BK BK →BT BK →M BK →E M →BT Ave.\nInference only\nLLaMA-Alpaca\nNo demo 61.53 61.53 63.72 58.86 59.41 63.72 61.46\nRand demo 54.33 55.45 60.48 49.09 51.98 63.78 55.85\nRetr demo 60.9 63.58 69.35 60.33 61.36 67.82 64.06\nChatGPT\nNo demo 72.68 72.68 72.27 70.06 69.83 72.27 71.63\nRand demo 73.10 73.27 74.37 71.18 71.44 74.3 72.94\nRetr demo 73.07 71.92 73.82 69.69 71.00 73.57 72.18\nFine-tuning\nRoBERTa\nLong et al. (2022) 70.330.3 70.920.6 64.131.4 64.671.7 62.360.7 65,400.8 66.30\nYe et al. (2020) 70.900.4 71.380.8 67.480.4 67.160.6 64.001.2 70.710.3 68.61\nNo-ICL 68.330.5 69.850.6 65.921.1 61.471.7 61.360.7 67.430.8 65.73\nICL-rand 67.610.8 68.740.6 64.801.3 61.591.9 61.440.9 66.721.7 65.15\nICL-sup 69.68†\n0.6 71.15†\n0.5 68.79†\n1.4 64.88†\n1.1 63.16†\n1.0 69.15†\n1.1 67.80\nICL-source 68.700.8 70.64†\n0.8 65.291.4 61.812.2 61.751.4 66.891.9 65.84\nDAICL 71.21†\n0.5 72.81†\n0.9 68.64†\n1.7 66.93†\n0.8 66.08†\n0.7 71.44†\n0.9 69.52\nLLaMA-LoRA\nNo-ICL 74.15 74.30 72.97 70.42 70.08 70.15 72.01\nICL-rand 65.22 64.17 60.48 61.95 59.36 63.44 62.43\nICL-sup 76.10 75.20 72.25 71.63 71.78 70.54 72.75\nICL-source 70.18 68.45 68.46 63.27 67.23 67.94 67.59\nDAICL 77.30 76.30 74.02 73.40 70.38 72.37 74.13\nTable 2: Accuracy(%) results of Amazon Review Sentiment Analysis. For example, E→BK represents training on\nElectronics (E) and adapting to Book (BK). There are 4 domains available, we choose 6 out of 12 adaptation tasks.\ntion to ask is can the few-shot prompting paradigm\nsubstitute the fine-tuning paradigm? In NER exper-\niments, ChatGPT achieves very low performances,\nbut fine-tuning a much smaller RoBERTa model\nachieves state-of-the-art scores in most adaptation\nscenarios. In SA experiments, fine-tuning LLaMA\nwith even fewer trainable parameters (1.7M) out-\nperforms all the other methods. Hence, we hypothe-\nsize that although LLMs have strong generalization\nability, they cannot tackle problems in all domains.\nWhen it comes to UDA, designing an effective\nadaptation strategy is still beneficial.\n4.5 Analysis\nAdaptive ICL or Adaptive Pre-training? In Sec-\ntion 3.1, we propose to learn the two objectives\nsimultaneously with the help of the target contexts.\nWhat if we separate the two objectives into dif-\nferent training stages? In the first stage, we con-\ntinue pre-training LMs on the unlabeled target do-\nmain dataset with the language modeling objec-\ntive. In the second stage, supervised fine-tuning\nis performed on the labeled source domain dataset\nwith the task objective. This two-step UDA pro-\ncedure is called adaptive pre-training or post pre-\ntraining (Han and Eisenstein, 2019; Vu et al., 2020;\nKarouzos et al., 2021). There are two differences\nbetween adaptive pre-training and adaptive ICL\nwhich we propose: 1) adaptive ICL mixes a source\ninput with target contexts when performing task\npredictions while adaptive pre-training only takes\nthe source input; 2) adaptive ICL learns two losses\nsimultaneously, and for decoder-only model, we\nonly have one type of task which merges these two\nlosses intrinsically.\nWNUT17 BC2GM E→BK M →BT\npre-train 54.62 25.78 74.20 70.45\nNo-ICL 53.92 24.24 74.15 70.15\nDAICL 55.08 26.02 77.30 72.37\nTable 3: A comparison of adaptive ICL and adaptive\npre-training for LLaMA. On NER, we use CoNLL-\n03→WNUT17 and CoNLL-03→BC2GM. For SA, we\nuse E→BK and M→BT.\nTo compare the two approaches, we conduct\nexperiments on LLaMA-LoRA to perform adaptive\npre-training. In the first stage, we pre-train LoRA\nweights using target unlabeled texts. In the second\nstage, we start from the LoRA checkpoint obtained\nin the previous stage and continue fine-tuning it\nwith task supervision. We use the same Alpaca\ntemplate but do not provide demonstrative context.\nResults can be found in Table 3. No ICL is identical\nto the second stage in adaptive pre-training.\nWe could observe that pre-training only gains\nmarginal benefits for SA tasks compared with No-\nICL. We conjecture that the domain gap is smaller\nin SA datasets than in NER datasets. The pro-\nposed adaptive ICL strategy outperforms adaptive\npre-training, which could be attributed to the fact\nthat the decoder-only model under adaptive ICL\ncan learn the two objectives with demonstrative\ncontexts.\n6532\n5 Related Work\nUnsupervised Domain Adaptation\nTraditional methods include Pseudo-labeling (Ye\net al., 2020), Pivot-based approach (Pan et al.,\n2010), and adversarial neural network (Ganin et al.,\n2016). Recently, Adaptive pre-training on domain-\nspecific corpora has proven to be an effective pro-\ncess for adaptation, such as BioBERT (Lee et al.,\n2019) which is a specialized variant of BERT.\nHan and Eisenstein (2019) proposes AdaptaBERT,\nwhich includes a second phase of unsupervised pre-\ntraining for BERT in unsupervised domain adap-\ntation. Karouzos et al. (2021) proposes a mixed\nmulti-task loss to learn classification and MLM.\nChronopoulou et al. (2019) utilizes an auxiliary\nLM loss to prevent catastrophic forgetting in trans-\nfer learning.\nRetrieval-Augmented Language Models\nRetrieval-based LMs have shown to be effective in\nimproving LM performance (Asai et al., 2023). The\nretriever with various knowledge datastores can\nprovide up-to-date information since LMs cannot\nmemorize all long-tail knowledge in the parame-\nters. REALM (Guu et al., 2020) pre-trains and fine-\ntunes an encoder-only model jointly with a knowl-\nedge retriever by modeling documents as latent\nvariables and marginalizing over all possible docu-\nments. While RAG (Lewis et al., 2020) fine-tunes\nan encoder-decoder model with a non-parametric\nretriever by fixing the search index. Atlas (Izacard\net al., 2022) combines RAG with pre-training on\nopen-domain QA and knowledge-intensive tasks.\nReplug (Shi et al., 2023) proposes adapting the\ndense retriever to the black-box large language\nmodels to reduce the generating perplexity.\nIn-Context Learning\nIn the context of ICL, previous studies indicate\nthat it primarily exposes the model’s infrastructure\nlearned during pre-training. Xie et al. (2022) pro-\nvides evidence that ICL can be interpreted as a type\nof Bayesian inference, where demonstrations act\nas noisy evidence. (Min et al., 2022) shows that\nthe advantages of ICL mainly stem from having\nthe appropriate distribution of inputs and labels,\nrather than solely focusing on the correctness of\nindividual labels. Previous research has revealed\nthat in scenarios where abundant training data is ac-\ncessible, retrieving examples that are similar to the\ntest input as demonstrations significantly enhances\nICL performance. Liu et al. (2022) introduces a\nretrieval module for GPT-3 (Brown et al., 2020)\nand they also fine-tune the retrieval model, leading\nto stronger ICL performance. Rubin et al. (2022)\ntrains a dense retriever to select demonstrations that\nhave a positive impact on the learning process.\n6 Conclusion\nIn this work, we propose domain-adaptive in-\ncontext learning to acquire knowledge of both the\ntarget domain distribution and the discriminative\ntask signal simultaneously. We develop different\nprompting and fine-tuning strategies that take into\naccount various LM architectures and different lan-\nguage modeling mechanisms. Overall, our frame-\nwork demonstrates significant performance gains\nover an extensive spectrum of cross-domain ex-\nperiments, and we perceive that fine-tuning is still\neffective and promising in the era of large language\nmodels when it involves domain shift.\n7 Limitations\nOur retrieval system is based on SimCSE and\nBERTScore to choose semantically similar con-\ntexts following previous work. However, we do\nnot explore other scoring and re-ranking metrics,\nor explore methods to train a dense retriever. On\nthe other hand, it is hard to tell what makes a good\ndemonstration simply based on a retrieval system,\nconsidering that the retrieval system does not have\naccess to the inference task. We leave this for fu-\nture work to explore what is a good demonstrative\nexample when encountering domain shift.\n8 Ethics Statement\nTo ensure the ethical use of Artificial Intelligence\nin the legal field, we have taken measures such as\nanonymizing sensitive information in real-world\ndatasets. In addition, our model’s predictions\nshould be served as supportive references for\njudges, assisting them in making judgments more\nefficiently, rather than solely determining the judg-\nments.\nAcknowledgements\nThis work is partially supported by the 2020 Mi-\ncrosoft Research Asia collaborative research grant.\nSinno J. Pan thanks for the support from HK Global\nSTEM Professorship and the JC STEM Lab of Ma-\nchine Learning and Symbolic Reasoning.\n6533\nReferences\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timo-\nthy Baldwin. 2015. Domain adaption of named entity\nrecognition to support credit risk assessment. In Pro-\nceedings of the Australasian Language Technology\nAssociation Workshop 2015, pages 84–90.\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi\nChen. 2023. Acl 2023 tutorial: Retrieval-based lan-\nguage models and applications. ACL 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and\nGeoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational Conference on Machine Learning, pages\n1597–1607. PMLR.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pretrained\nlanguage models. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 2089–2095.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nLeon Derczynski, Eric Nichols, Marieke Van Erp, and\nNut Limsopatham. 2017. Results of the wnut2017\nshared task on novel and emerging entity recognition.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 140–147.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan,\nPascal Germain, Hugo Larochelle, François Lavi-\nolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The\njournal of machine learning research, 17(1):2096–\n2030.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6894–6910.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsu-\npervised domain adaptation of contextualized em-\nbeddings for sequence labeling. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4238–4248.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for un-\nsupervised visual representation learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9729–9738.\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. 2018. Adaptive semi-supervised learning\nfor cross-domain sentiment classification. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 3467–\n3476.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nConstantinos Karouzos, Georgios Paraskevopoulos, and\nAlexandros Potamianos. 2021. Udalm: Unsuper-\nvised domain adaptation through language modeling.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2579–2590.\nJin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-\nnobu Kano, and Jun’ichi Tsujii. 2009. Overview of\nbionlp’09 shared task on event extraction. In Pro-\nceedings of the BioNLP 2009 workshop companion\nvolume for shared task, pages 1–9.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\n6534\nIn Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 260–270.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2019. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36:1234 – 1240.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nJiao Li, Yueping Sun, Robin J. Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J. Mattingly, Thomas C. Wiegers,\nand Zhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase: The Journal of Biological Databases and\nCuration, 2016.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (Dee-\nLIO 2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nQuanyu Long, Tianze Luo, Wenya Wang, and Sinno\nPan. 2022. Domain confused contrastive learning for\nunsupervised domain adaptation. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2982–2995.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional lstm-cnns-crf. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1064–1074.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. 2015. Image-based recom-\nmendations on styles and substitutes. In Proceedings\nof the 38th international ACM SIGIR conference on\nresearch and development in information retrieval,\npages 43–52.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nSinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang\nYang, and Zheng Chen. 2010. Cross-domain senti-\nment classification via spectral feature alignment. In\nProceedings of the 19th international conference on\nWorld wide web, pages 751–760.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2655–2671.\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142–147.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nLarry L. Smith, Lorraine K. Tanabe, Rie Ando,\nCheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-\nShi Lin, Roman Klinger, C. Friedrich, Kuzman\nGanchev, Manabu Torii, Hongfang Liu, Barry Had-\ndow, Craig A. Struble, Richard J. Povinelli, An-\ndreas Vlachos, William A. Baumgartner, Lawrence E.\nHunter, Bob Carpenter, Richard Tzong-Han Tsai,\nHong-Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,\nSophia Katrenko, Pieter W. Adriaans, Christian\nBlaschke, Rafael Torres, Mariana L. Neves, Preslav\nNakov, Anna Divoli, Manuel Maña-López, Jacinto\nMata, and W. John Wilbur. 2008. Overview of biocre-\native ii gene mention recognition. Genome Biology,\n9:S2 – S2.\nBenjamin Strauss, Bethany Toma, Alan Ritter, Marie-\nCatherine De Marneffe, and Wei Xu. 2016. Results\nof the wnut16 named entity recognition shared task.\nIn Proceedings of the 2nd Workshop on Noisy User-\ngenerated Text (WNUT), pages 138–144.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\n6535\nThuy Vu, Dinh Phung, and Gholamreza Haffari. 2020.\nEffective unsupervised domain adaptation with ad-\nversarially trained language models. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6163–\n6173.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nZhongqiang Huang, Fei Huang, and Kewei Tu. 2021.\nImproving named entity recognition by external con-\ntext retrieving and cooperative learning. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1800–1812.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference. In Interna-\ntional Conference on Learning Representations.\nHai Ye, Qingyu Tan, Ruidan He, Juntao Li, Hwee Tou\nNg, and Lidong Bing. 2020. Feature adaptation of\npre-trained language models across languages and\ndomains with robust self-training. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 7386–\n7399.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2020. Bertscore: Evaluating\ntext generation with bert. In International Confer-\nence on Learning Representations.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n6536\nA Datasets\n# Train # Dev # Test\nCONLL-03 14,987 3,466 3,684\nFIN 1169 – 306\nWNUT-16 2,394 1,000 3,849\nWNUT-17 3,394 1,009 1,287\nBC2GM 12574 2519 5038\nBIONLP09 7462 1448 2446\nBC5CDR 4,560 4,581 4,797\nTable 4: Statistics of the dataset split of NER dataset.\nFor NER datasets, we select CoNLL-03 training\nas the source labeled dataset and a CoNLL-03 dev\nset as the validation set for adaptation. When adapt-\ning to a target domain, for example, WNUT16,\nwe use WNUT16 training set as the unlabeled\ntarget dataset by discarding all labels from this\ntraining set. That is, in our approach, for the fine-\ntuning setting, we retrieve text-only examples from\nWNUT16 training dataset as the contexts of source\ninput CoNLL03. Statistics can be found in Table 4\nDOMAIN # Neg # Neu # Pos Total\nBOOK Set 1 2000 2000 2000 6000\nSet 2 513 663 4824 6000\nELEC Set 1 2000 2000 2000 6000\nSet 2 694 489 4817 6000\nBEAUTY Set 1 2000 2000 2000 6000\nSet 2 616 675 4709 6000\nMUSIC Set 1 2000 2000 2000 6000\nSet 2 785 774 4441 6000\nTable 5: Statistics of the dateset split of NER dataset.\nFor the Amazon review dataset, it does not re-\nmove neutral labels, which is advantageous in un-\nsupervised domain adaptation (UDA) scenarios\nwhere label information from the target domain\nis unavailable. A summary of this dataset can be\nfound in Table 5. For SA, each dataset consists of\ntwo sets. Set 1 contains 6,000 instances with bal-\nanced class labels, while Set 2 comprises instances\nrandomly sampled from a larger dataset (McAuley\net al., 2015), preserving the authentic label distribu-\ntion. It is important to note that there is no overlap\nbetween the examples in these two sets. Following\nthe approach outlined in (He et al., 2018), Set 1\nis used as the training set for the source domain.\nWhile the label distribution in the target domain is\nunpredictable and beyond our control in real-life\nscenarios, it is more reasonable to use Set 2 as the\nunlabeled set for the target domain. Finally, the\nmodel is evaluated on Set 1 from the target domain.\nRegarding the data split, a validation set is created\nby randomly sampling 1000 instances from the\nsource labeled Set 1. For example, when perform-\ning E→BK adaptation task, we use Electronics Set\n1 as the training set and validation set, we use Book\nSet 2 as the target unlabeled set, and we retrieve\nsimilar examples from this set as contexts. The\nevaluation will be performed in Book Set 1.\nB Example Input and Output Pairs of\nChatGPT and LLaMA\n6537\nTable 6: Example input and output pairs for ChatGPT on NER dataset BC2GM. No demonstration.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: Physical mapping 220 kb centromeric of the human MHC and DNA sequence analysis of\nthe 43 - kb segment including the RING1 , HKE6 , and HKE4 genes .\nEntity: - Physical mapping\n- human MHC\n- DNA sequence analysis\n- RING1 gene\n- HKE6 gene\n- HKE4 gene\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: DNA elements recognizing NF - Y and Sp1 regulate the human multidrug - resistance\ngene promoter .\nEntity:\n1. DNA elements\n2. NF-Y\n3. Sp1\n4. human multidrug-resistance gene promoter\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: Like other IAPs , ch - IAP1 contains the N - terminal baculovirus IAP repeats and C -\nterminal RING finger motifs .\nEntity:\nIAPs, ch-IAP1, baculovirus IAP repeats, RING finger motifs.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: To clarify the difference , both the Crk II and Crk II - 23 , proteins were expressed in E .\ncoli and examined their binding capacity in vitro .\nEntity:\n- Crk II\n- Crk II - 23\n- E. coli\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: A GT - rich sequence binding the transcription factor Sp1 is crucial for high expression of\nthe human type VII collagen gene ( COL7A1 ) in fibroblasts and keratinocytes .\nEntity:\n1. GT-rich sequence\n2. Transcription factor Sp1\n3. Human type VII collagen gene (COL7A1)\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: Manual sample clean - up procedures as well as the addition of an internal standard are not\nneeded .\nEntity:\nNone.\n6538\nTable 7: Example input and output pairs for ChatGPT on NER dataset BC2GM. Retrieved demonstrations are from\nCoNLL03.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: In the study at the University ’s Institute for Human Gene Therapy , researchers altered a\ncommon-cold virus to carry a version of the working dystrophin gene .\nEntity: Institute for Human Gene Therapy\nSentence: The virus , which also was altered to minimise its susceptibility to the immune system ,\nwas then injected into the muscle cells of mice bred to lack dystrophin genes .\nEntity: None\nSentence: \" We agreed that following detailed scientific analysis using a methodology which would\ntake out the maximum number of BSE cases possible .\nEntity: BSE\nSentence: In the experiment , between 30 to 40 percent of the muscle fibers in one group of mice\nproduced dystrophin for two weeks before diminishing .\nEntity: None\nSentence: Stork H1 results breakdown per sector .\nEntity: None\nSentence: Physical mapping 220 kb centromeric of the human MHC and DNA sequence analysis of\nthe 43 - kb segment including the RING1 , HKE6 , and HKE4 genes .\nEntity:\nhuman MHC, RING1, HKE6, HKE4 genes.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: In the study at the University ’s Institute for Human Gene Therapy , researchers altered a\ncommon-cold virus to carry a version of the working dystrophin gene .\nEntity: Institute for Human Gene Therapy\nSentence: Individuals with the disease have a non-working version of a gene responsible for producing\na crucial muscle protein called dystrophin .\nEntity: None\nSentence: The virus , which also was altered to minimise its susceptibility to the immune system ,\nwas then injected into the muscle cells of mice bred to lack dystrophin genes .\nEntity: BSE\nSentence: Both drugs are types of interferon .\nEntity: None\nSentence: When it approved Avonex in May , the FDA said both Biogen ’s product and Betaseron\nwere developed under the incentives of the Ophran Drug Act which provides seven years of marketing\nexclusivity for products that treat rare diseases .\nEntity: Avonex, FDA, Biogen, Betaseron, Ophran Drug Act\nSentence: DNA elements recognizing NF - Y and Sp1 regulate the human multidrug - resistance\ngene promoter .\nEntity:\nNF-Y , Sp1, human multidrug-resistance gene promoter.\n6539\nTable 8: Example input and output pairs for ChatGPT on NER dataset BC2GM. Demonstrations are randomly\nsampled from CoNLL03.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: The 32-year-old defender played seven seasons with Nantes and was with Paris St Germain\nfor five seasons .\nEntity: Nantes, Paris St Germain\nSentence: 29,582\nEntity: None\nSentence: The Palestinian Authority was set up under the 1993 PLO-Israel interim peace deal .\nEntity: Palestinian Authority, PLO-Israel\nSentence: In Home Health said it previously recorded a reserve equal to 16 percent of all revenue\nrelated to the community liaison costs .\nEntity: In Home Health\nSentence: \" I realised this year , that without putting 99.9 percent of your mind into tennis , I do n’t\nthink you can successful , \" said the 22-year-old Medvedev .\nEntity: Medvedev\nSentence: Physical mapping 220 kb centromeric of the human MHC and DNA sequence analysis of\nthe 43 - kb segment including the RING1 , HKE6 , and HKE4 genes .\nEntity:\nhuman MHC, RING1, HKE6, HKE4.\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: The blue-chip CAC-40 index ended 2.43 points or 0.12 percent lower at 2,017.99 points\nafter a brief foray into positive territory when the New York stock market opened higher .\nEntity: CAC-40, New York\nSentence: American League\nEntity: American League\nSentence: 1886 - At Skeleton Canyon in Arizona , Geronimo , Apache chief and leader of the last\ngreat Red Indian rebellion finally surrendered to General Nelson Miles .\nEntity: Skeleton Canyon, Arizona, Geronimo, Red Indian, Nelson Miles\nSentence: ( Formula Shell leads best-of-seven series 1-0 )\nEntity: Formula Shell\nSentence: PRESS DIGEST - Jordan - Aug 25 . Entity: Jordan Sentence: DNA elements recognizingNF - Y and Sp1 regulate the human multidrug - resistance gene promoter .\nEntity:\nNF-Y , Sp1, DNA elements.\n6540\nTable 9: Example input and output pairs for LLaMA-Alpaca on NER dataset BC2GM. No demonstration. Note that\nAlpaca-lora is not fine-tuned on NER datasets.\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nInput sentence: Physical mapping 220 kb centromeric of the human MHC and DNA sequence analysis\nof the 43 - kb segment including the RING1 , HKE6 , and HKE4 genes .\n### Response:\nNone\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nInput sentence: DNA elements recognizing NF - Y and Sp1 regulate the human multidrug - resistance\ngene promoter .\n### Response:\nDNA elements, NF - Y , Sp1\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nInput sentence: Like other IAPs , ch - IAP1 contains the N - terminal baculovirus IAP repeats and C -\nterminal RING finger motifs .\n### Response:\nNone\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nInput sentence: A GT - rich sequence binding the transcription factor Sp1 is crucial for high expression\nof the human type VII collagen gene ( COL7A1 ) in fibroblasts and keratinocytes .\n### Response:\nhuman type VII collagen gene\n6541\nTable 10: Example input and output pairs for our fine-tuned LLaMA-7B on NER dataset BC2GM. Inference with\nretrieved demonstration.\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: In the study at the University ’s Institute for Human Gene Therapy , researchers altered a\ncommon-cold virus to carry a version of the working dystrophin gene .\nEntity: Institute for Human Gene Therapy\nSentence: The virus , which also was altered to minimise its susceptibility to the immune system ,\nwas then injected into the muscle cells of mice bred to lack dystrophin genes .\nEntity: None\nSentence: \" We agreed that following detailed scientific analysis using a methodology which would\ntake out the maximum number of BSE cases possible .\nEntity: BSE\nSentence: In the experiment , between 30 to 40 percent of the muscle fibers in one group of mice\nproduced dystrophin for two weeks before diminishing .\nEntity: None\nSentence: Stork H1 results breakdown per sector .\nEntity: None\nInput sentence: Physical mapping 220 kb centromeric of the human MHC and DNA sequence analysis\nof the 43 - kb segment including the RING1 , HKE6 , and HKE4 genes .\n### Response:\nHuman MHC, RING1, HKE6, HKE4 genes\nBelow is an instruction that describes a task, paired with an input that provides further context. Write\na response that appropriately completes the request.\n### Instruction:\nPlease identify all entities from the input sentence. If there is no entity, please output None.\nSentence: In the study at the University ’s Institute for Human Gene Therapy , researchers altered a\ncommon-cold virus to carry a version of the working dystrophin gene .\nEntity: Institute for Human Gene Therapy\nSentence: Individuals with the disease have a non-working version of a gene responsible for producing\na crucial muscle protein called dystrophin .\nEntity: None\nSentence: The virus , which also was altered to minimise its susceptibility to the immune system ,\nwas then injected into the muscle cells of mice bred to lack dystrophin genes .\nEntity: BSE\nSentence: Both drugs are types of interferon .\nEntity: None\nSentence: When it approved Avonex in May , the FDA said both Biogen ’s product and Betaseron\nwere developed under the incentives of the Ophran Drug Act which provides seven years of marketing\nexclusivity for products that treat rare diseases .\nEntity: Avonex, FDA, Biogen, Betaseron, Ophran Drug Act\nInput sentence: DNA elements recognizing NF - Y and Sp1 regulate the human multidrug - resistance\ngene promoter .\n### Response:\nNF - Y , Sp1\n6542"
}