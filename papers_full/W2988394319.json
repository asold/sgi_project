{
  "title": "Fast Transformer Decoding: One Write-Head is All You Need",
  "url": "https://openalex.org/W2988394319",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224379215",
      "name": "Shazeer, Noam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2626778328"
  ],
  "abstract": "Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention \"heads\", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.",
  "full_text": "arXiv:1911.02150v1  [cs.NE]  6 Nov 2019\nF ast T ransformer Decoding: One W rite-Head is All\nY ou Need\nNoam Shazeer\nGoogle\nnoam@google.com\nNovember 7, 2019\nAbstract\nMulti-head attention layers, as used in the T ransformer neu ral sequence model, are a powerful alter-\nnative to RNNs for moving information across and between seq uences. While training these layers is\ngenerally fast and simple, due to parallelizability across the length of the sequence, incremental inference\n(where such paralleization is impossible) is often slow, du e to the memory-bandwidth cost of repeatedly\nloading the large \"keys\" and \"values\" tensors. W e propose a v ariant called multi-query attention, where\nthe keys and values are shared across all of the diﬀerent atte ntion \"heads\", greatly reducing the size of\nthese tensors and hence the memory bandwidth requirements o f incremental decoding. W e verify exper-\nimentally that the resulting models can indeed be much faste r to decode, and incur only minor quality\ndegradation from the baseline.\n1 Introduction\nThe T ransformer neural sequence model [ V aswani et al. , 2017] has emerged as a popular alternative to\nrecurrent sequence models. T ransformer relies on attentio n layers to communicate information between\nand across sequences. One major challenge with T ransformer is the speed of incremental inference. As we\nwill discuss, the speed of incremental T ransformer inferen ce on modern computing hardware is limited by\nthe memory bandwidth necessary to reload the large \"keys\" an d \"values\" tensors which encode the state\nof the attention layers. In the following sections, we will r eview the multi-head-attention layers used by\nT ransformer, provide a performance analysis, and propose a n architectural variation (multi-query attention)\nwhich greatly improves inference speed with only minor qual ity degradation.\n2 Background: Neural Attention\nNeural Attention, introduced by [\nBahdanau et al. , 2014], is a powerful tool for manipulating variable-length\nrepresentations. A neural attention function takes a singl e query-vector q and a set of m diﬀerent (key-vector,\nvalue-vector) pairs (represented by the matrices K and V ), and produces an output vector y. The output y\nis computed as a weighted sum of the diﬀerent value vectors, w here the weights are derived by comparing\nthe query to the keys.\n2.1 Dot-Product Attention\nThe following code describes a common formulation, where th e weights are computed as the softmax of the\ndot-products of the query with the diﬀerent keys.\n1\nd e f D o t P r o d u c t A t t e n t i o n ( q , K, V ) :\n\" \" \" Dot −P r o d u c t A t t e n t i o n on o n e q u e r y .\nA r g s :\nq : a v e c t o r w i t h s h a p e [ k ]\nK: a m a t r i x w i t h s h a p e [ m, k ]\nV : a m a t r i x w i t h s h a p e [ m, v ]\nR e t u r n s :\ny : a v e c t o r w i t h s h a p e [ v ]\n\" \" \"\nl o g i t s = t f . e i n s u m ( \" k , mk−> m\", q , K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s )\nr e t u r n t f . e i n s u m ( \"m, mv−>v \" , w e i g h t s , V)\nOur code samples use einsum notation, as deﬁned in T ensorFlow and numpy , for generalize d contractions\nbetween tensors of arbitrary dimension. In this notation, a n equation names the dimensions of the input and\noutput T ensors. The computation is numerically equivalent to broadcasting each input to have the union of\nall dimensions, multiplying component-wise, and summing a cross all dimensions not in the desired output\nshape.\n2.2 Multi-head Attention\nThe \"T ransformer\" seuqence-to-sequence model [\nV aswani et al. , 2017] uses h diﬀerent attention layers (heads)\nin parallel, which the authors refer to as \"Multi-head atten tion\". The query vectors for the h diﬀerent layers\nare derived from h diﬀerent learned linear projections Pq of an input vector x. Similarly , the keys and\nvalues are derived from h diﬀerent learned linear projections Pk , P v of a collection M of m diﬀerent input\nvectors. The outputs of the h layers are themselves passed through diﬀerent learned line ar projections Po ,\nthen summed. F or simplicity , we give the input and output vec tors identical dimensionality d. The The\ncomputation can be expressed as follows:\nd e f M u l t i h e a d A t t e n t i o n (\nx , M, P_q, P_k, P_v, P_o ) :\n\" \" \" M u l t i −h e a d A t t e n t i o n on o n e q u e r y .\nA r g s :\nx : a v e c t o r w i t h s h a p e [ d ]\nM: a m a t r i x w i t h s h a p e [ m, d ]\nP_q : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_k : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_v : a t e n s o r w i t h s h a p e [ h , d , v ]\nP_o : a t e n s o r w i t h s h a p e [ h , d , v ]\nR e t u r n s :\ny : a v e c t o r w i t h s h a p e [ d ]\n\" \" \"\nq = t f . e i n s u m ( \" d , hdk −>hk \" , x , P_q)\nK = t f . e i n s u m ( \"md , hdk −> hmk\" , M, P_k)\nV = t f . e i n s u m ( \"md , hdv −> hmv\" , M, P_v)\nl o g i t s = t f . e i n s u m ( \" hk , hmk −> hm\" , q , K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s )\no = t f . e i n s u m ( \"hm , hmv−>hv \" , w e i g h t s , V)\ny = t f . e i n s u m ( \" hv , hdv −> d \" , o , P_o)\nr e t u r n y\nNote: [ V aswani et al. , 2017] include a constant scaling factor on the logits. W e omit thi s in our code, as\nit can be folded into the linear projections Pq or Pk .\n2\n2.3 Multi-head Attention (Batched)\nIn practice, it is far more eﬃcient to batch together multipl e queries. The code below adds two types of\nbatching. First, we generate queries from n diﬀerent positions in a sequence. These queries all interac t with\nthe same keys and values. In addition, we process a batch of b diﬀerent non-interacting sequences at once.\nF ollowing [\nV aswani et al. , 2017], in an autoregressive model, we can prevent backward-info rmation-ﬂow by\nadding a \"mask\" to the logits containing the value −∞ in the illegal positions.\nd e f M u l t i h e a d A t t e n t i o n B a t c h e d (\nX, M, mask , P_q, P_k, P_v, P_o ) :\n\" \" \" M u l t i −h e a d A t t e n t i o n .\nA r g s :\nX : a t e n s o r w i t h s h a p e [ b , n , d ]\nM: a t e n s o r w i t h s h a p e [ b , m, d ]\nmask : a t e n s o r w i t h s h a p e [ b , h , n , m]\nP_q : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_k : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_v : a t e n s o r w i t h s h a p e [ h , d , v ]\nP_o : a t e n s o r w i t h s h a p e [ h , d , v ]\nR e t u r n s :\nY : a t e n s o r w i t h s h a p e [ b , n , d ]\n\" \" \"\nQ = t f . e i n s u m ( \" bnd , hdk −>bhnk \" , X, P_q)\nK = t f . e i n s u m ( \" bmd , hdk −> bhmk \" , M, P_k)\nV = t f . e i n s u m ( \" bmd , hdv −> bhmv \" , M, P_v)\nl o g i t s = t f . e i n s u m ( \" bhnk , bhmk −> bhnm \" , Q, K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s + mask )\nO = t f . e i n s u m ( \" bhnm , bhmv−> bhnv \" , w e i g h t s , V)\nY = t f . e i n s u m ( \" bhnv , hdv −>bnd \" , O, P_o)\nr e t u r n Y\n2.3.1 Performance Analysis of Batched Multi-head Attentio n\nT o simplify the performance analysis, we will make several s implifying assumptions:\n• m = n\n• k = v = d\nh , as suggested by [ V aswani et al. , 2017]\n• n ≤ d\nThe total number of arithmetic operations is Θ( bnd2). (Since the complexity of each of the tf.einsum\noperations above is O(bnd2 ) given the simplifying assumptions.\nThe total size of memory to be accessed is equal to the sum of th e sizes of all the tensors involved:\nO(bnd + bhn2 + d2). The ﬁrst term is due to X , M , Q, K , V , O and Y , the second term due to the logits\nand weights, and the third term due to the projection tensors Pq , Pk , Pv and Po .\nDividing the two, we ﬁnd that the ratio of memory access to ari thmetic operations is O( 1\nk + 1\nbn ). This low\nratio is necessary for good performance on modern GPU/TPU ha rdware, where the computational capacity\ncan be two orders of magnitude higher than the memory bandwid th.\n2.4 Multihead Attention (Incremental)\nIn some settings, data dependencies make it is impossible to process queries from multiple positions in parallel.\nAn example is a self-attention layer in an autoregressive la nguage model such as T ransformer [\nV aswani et al. ,\n2017]. The queries produced at each position attend to key-value pairs produced at all positions up to and\nincluding that position. During training, the ground-trut h target sequence is known, and we can use an\n3\neﬃcient parallel implementation similar to that in section 2.3. However, when generating from the trained\nmodel, the output of the self-attention layer at a particula r position aﬀects the token that is generated at\nthe next position, which in turn aﬀects the input to that laye r at the next position. This prevents parallel\ncomputation. Code for incrementally computing this self-a ttention layer is shown below.\nd e f M u l t i h e a d S e l f A t t e n t i o n I n c r e m e n t a l (\nx , prev_K , prev_V , P_q, P_k, P_v, P_o ) :\n\" \" \" M u l t i −h e a d S e l f −A t t e n t i o n ( o n e s t e p ) .\nA r g s :\nx : a t e n s o r w i t h s h a p e [ b , d ]\nprev_K : t e n s o r w i t h s h a p e [ b , h , m, k ]\nprev_V : t e n s o r w i t h s h a p e [ b , h , m, v ]\nP_q : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_k : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_v : a t e n s o r w i t h s h a p e [ h , d , v ]\nP_o : a t e n s o r w i t h s h a p e [ h , d , v ]\nR e t u r n s :\ny : a t e n s o r w i t h s h a p e [ b , d ]\nnew_K : t e n s o r w i t h s h a p e [ b , h , m+1 , k ]\nnew_V : t e n s o r w i t h s h a p e [ b , h , m+1 , v ]\n\" \" \"\nq = t f . e i n s u m ( \" bd , hdk −>bhk \" , x , P_q)\nnew_K = t f . c o n c a t (\n[ prev_K , t f . expand_dims ( t f . e i n s u m ( \" bd , hdk −>bhk \" , M, P_k) , a x i s = 2 ) ] ,\na x i s =2)\nnew_V = t f . c o n c a t (\n[ prev_V , t f . expand_dims ( t f . e i n s u m ( \" bd , hdv −>bhv \" , M, P_v) , a x i s = 2 ) ] ,\na x i s =2)\nl o g i t s = t f . e i n s u m ( \" bhk , bhmk −> bhm \", q , new_K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s )\no = t f . e i n s u m ( \" bhm , bhmv−>bhv \" , w e i g h t s , new_V)\ny = t f . e i n s u m ( \" bhv , hdv −> bd \" , O, P_o)\nr e t u r n y , new_K, new_V\n2.4.1 Performance Analysis\nW e make the same simplifying assumptions as in section 2.3.1.\nAcross n calls, the total number of arithmetic operations is again Θ( bnd2).\nAcross n calls, the total amount of memory access is Θ( bn2d + nd2 ), the ﬁrst term due to K and V and\nthe second term due to Pq , Pk , Pv and Po .\nDividing the memory by the computations, we ﬁnd that the rati o of memory access to arithmetic opera-\ntions is Θ( n\nd + 1\nb ). When n ≈ d or b ≈ 1, the ratio is close to 1, causing memory bandwidth to be a maj or\nperformance bottleneck on modern computing hardware. In or der to make incremental generation eﬃcient,\nwe must reduce both of these terms to be ≪ 1. The 1\nb term is the easier one - we can just use a larger batch\nsize, memory size permitting.\nReducing the n\nd term is harder. This term is related to the expense of reloadi ng at each step the K and V\ntensors representing the memory which have size bhmk = bn2. One solution is to limit the sequence length n.\nAnother is to reduce the number of positions being attended- to, either by attending to a local neighborhood,\nor by otherwise compressing the number of memory positions, as in [ Liu et al. , 2018], [ Zhang et al. , 2018],\n[Povey et al. , 2018]. In this paper we present an orthogonal approach to reducin g the size of the K and V\ntensors - namely removing their \"heads\" dimension, while ma intaining the \"heads\" dimension in the queries.\n4\n3 Multi-Query Attention\nW e introduce multi-query Attention as a variation of multi-head attention as described in [ V aswani et al. ,\n2017]. Multi-head attention consists of multiple attention lay ers (heads) in parallel with diﬀerent linear\ntransformations on the queries, keys, values and outputs. M ulti-query attention is identical except that the\ndiﬀerent heads share a single set of keys and values. The code for (incremental) multi-query (self) attention\nis identical to the code listed above for multi-head attenti on, except that we remove the letter \"h\" from the\ntf.einsum equations where it represents the \"heads\" dimension of K , V , Pk , or Pv .\nd e f M u l t i q u e r y A t t e n t i o n B a t c h e d (\nX, M, mask , P_q, P_k, P_v, P_o ) :\n\" \" \" M u l t i −Query A t t e n t i o n .\nA r g s :\nX : a t e n s o r w i t h s h a p e [ b , n , d ]\nM: a t e n s o r w i t h s h a p e [ b , m, d ]\nmask : a t e n s o r w i t h s h a p e [ b , h , n , m]\nP_q : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_k : a t e n s o r w i t h s h a p e [ d , k ]\nP_v : a t e n s o r w i t h s h a p e d , v ]\nP_o : a t e n s o r w i t h s h a p e [ h , d , v ]\nR e t u r n s :\nY : a t e n s o r w i t h s h a p e [ b , n , d ]\n\" \" \"\nQ = t f . e i n s u m ( \" bnd , hdk −>bhnk \" , X, P_q)\nK = t f . e i n s u m ( \" bmd , dk −> bmk\" , M, P_k)\nV = t f . e i n s u m ( \" bmd , dv −> bmv\" , M, P_v)\nl o g i t s = t f . e i n s u m ( \" bhnk , bmk −> bhnm \" , Q, K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s + mask )\nO = t f . e i n s u m ( \" bhnm , bmv−>bhnv \" , w e i g h t s , V)\nY = t f . e i n s u m ( \" bhnv , hdv −>bnd \" , O, P_o)\nr e t u r n Y\n5\nd e f M u l t i q u e r y S e l f A t t e n t i o n I n c r e m e n t a l (\nx , prev_K , prev_V , P_q, P_k, P_v, P_o ) :\n\" \" \" M u l t i −q u e r y S e l f −A t t e n t i o n ( o n e s t e p ) .\nA r g s :\nx : a t e n s o r w i t h s h a p e [ b , d ]\nprev_K : t e n s o r w i t h s h a p e [ b , m, k ]\nprev_V : t e n s o r w i t h s h a p e [ b , m, v ]\nP_q : a t e n s o r w i t h s h a p e [ h , d , k ]\nP_k : a t e n s o r w i t h s h a p e [ d , k ]\nP_v : a t e n s o r w i t h s h a p e [ d , v ]\nP_o : a t e n s o r w i t h s h a p e [ h , d , v ]\nR e t u r n s :\ny : a t e n s o r w i t h s h a p e [ b , d ]\nnew_K : t e n s o r w i t h s h a p e [ b , m+1 , k ]\nnew_V : t e n s o r w i t h s h a p e [ b , m+1 , v ]\n\" \" \"\nq = t f . e i n s u m ( \" bd , hdk −>bhk \" , x , P_q)\nK = t f . c o n c a t (\n[ prev_K , t f . expand_dims ( t f . e i n s u m ( \" bd , dk −>bk \" , M, P_k) , a x i s = 2 ) ] ,\na x i s =2)\nV = t f . c o n c a t (\n[ prev_V , t f . expand_dims ( t f . e i n s u m ( \" bd , dv −>bv \" , M, P_v) , a x i s = 2 ) ] ,\na x i s =2)\nl o g i t s = t f . e i n s u m ( \" bhk , bmk −> bhm \", q , K)\nw e i g h t s = t f . s o f t m a x ( l o g i t s )\no = t f . e i n s u m ( \" bhm , bmv−>bhv \" , w e i g h t s , V)\ny = t f . e i n s u m ( \" bhv , hdv −> bd \" , O, P_o)\nr e t u r n y , K, V\n3.1 Performance Analysis for Incremental Multi-Query Atte ntion\nW e make the same simplifying assumptions as in section 2.3.1.\nAcross n calls, the total number of arithmetic operations is again Θ( bnd2).\nAcross n calls, the total amount of memory access is Θ( bnd + bn2k + nd2), the ﬁrst term due to x, q, o\nand y, the second term due to K and V and the third term due to Pq , Pk , Pv , Po .\nDividing the memory by the computations, we ﬁnd that the rati o of memory access to arithmetic opera-\ntions is Θ( 1\nd + n\ndh + 1\nb ). W e have reduced the oﬀensive n\nd by a factor of h. Theoretically , given large batch\nsize b, this should dramatically improve performance of incremen tal generation. In our experimental section,\nwe will show that the performance gains are real and that mode l quality remains high.\n4 Experiments and Results\n4.1 Experimental Setup\nF ollowing [\nV aswani et al. , 2017], we evaluate on the WMT 2014 English-German translation ta sk. As a\nbaseline, we use an encoder-decoder T ransformer model with 6 layers, using dmodel = 1024 df f = 4096,\nh = 8, dk = dv = 128, learned positional embeddings, and weight-sharing b etween the token-embedding and\noutput layers. The baseline model and all variations have 21 1 million parameters. All models were trained\nfor 100,000 steps ( 20 epochs). Each training batch consiste d of 128 examples, each of which consisted of\na 256-token input sequence and a 256-token target sequence ( multiple training sentences were concatenated\ntogether to reach this length). Models were trained on a 32-c ore TPUv3 cluster, with each model taking\nabout 2 hours to train. W e used an implementation from the ten sor2tensor and mesh-tensorﬂow libraries.\n6\nThe conﬁgurations used can be found at [to be added before pub lication] , including details about learning\nrates, dropout, label smoothing, etc.\nIn our \"multi-query\" model, we replace all of the attention l ayers in the model to multi-query attention.\nThis includes the encoder-self-attention, decoder-self- attention and encoder-decoder-attention layers. W e\nwiden the feed-forward hidden layers from 4096 to 5440 to mak e the total parameter-count equal to that of\nthe baseline.\nT o demonstrate that local-attention and multi-query atten tion are orthogonal, we also trained \"local\"\nversions of the baseline and multi-query models, where the d ecoder-self-attention layers (but not the other\nattention layers) restrict attention to the current positi on and the previous 31 positions.\nA simpler alternative way to reduce the sizes of K and V is to reduce the number of heads h and/or to\nreduce the dimensionalities k and v of the keys and values. W e trained several such models for com parison,\nagain widening the feed-forward hidden layers to make the to tal parameter-count equal to that of the baseline.\nW e preformed a similar set of experiments using \"transforme r-decoder\" language models on the Billion-\nW ord Language Modeling Benchmark [ Chelba et al. , 2013]. F or the baseline, we use a model with 6 layers,\ndmodel = 1024 df f = 8192, h = 8, dk = dv = 128. The total parameter count is 192 million for the baseli ne\nand for all variations. W e trained for 136K steps (10 epochs) at a batch size of 64K tokens. Again, we used\na 32-core TPUv3 cluster for approximately 3 hours to train ea ch model.\n4.2 Model Quality\nT able 1 shows results for the machine-translation experiments. W e decoded the dev set using greedy\nmaximum-likelihood decoding and computed BLEU score with s acrebleu \"sacrebleu -t wmt13 -l en-de\n-tok intl\" . W e also list per-subword-token perplexity on the dev set. A ccording to both of these metrics,\nthe multi-query attention model seems to be slightly worse t han the baseline, but much closer than any of\nthe alternatives involving decreasing h, dk and dv .\nW e validated the results by decoding the test set using both g reedy decoding and beam search (beam\n4, α = 0 . 6), and evaluated with sacrebleu \"sacrebleu -t wmt14 -l en-de -tok intl\" . Again, the multi-\nquery model performed similarly to the baseline, and actual ly had the highest BLEU score (28.5) with\nbeam-4 decoding.\nT able 3 shows results for the billion-word language modeling bench mark. Models were evaluated by per-\nword (not per-subword-token) perplexity on the dev set. The results paint a similar picture to the translation\nresults. The multi-query attention model was slightly wors e than the baseline, but signiﬁcantly better than\nany of the alternatives involving decreasing h, dk and dv .\n4.3 Speed\nT able 2 shows training and inference times for the various models. B oth training and inference speeds were\nevaluated on one TPUv2 (8 cores). A training step (consistin g of 32,768 input tokens and 32,768 target\ntokens, as described above) took 433ms for the base model and 425ms for the multi-query model. Dividing\nby 32,768, we ﬁnd that the training time is 13.2 µs per (input-token + target-token), as listed in T able 2.\nW e ran incremental greedy inference on a batch of 1024 sequen ces (128 per core) using a source-sequence\nlength of 128 tokens and a target sequence length of 128. 1 F or the baseline model, the encoder part of the\nmodel took 222ms and each incremental step of the decoder too k 47ms. Dividing by the respective numbers\nof tokens, we ﬁnd that the amortized inference time is 1 . 7µs per token for the encoder and a much larger\n46µs per token for the decoder, as listed in T able 2. F or the multi-query model, the encoder took 195ms\nand the decoder took 3.9ms per step, for amortized per-token costs of 1 . 5µs and 3 . 8µs respectively . T able 2\nshows these values as well as similar results for beam-searc h.\n1 Due to system limitations requiring ﬁxed shapes, we used pad ding and masking in our decoder-self-attention implementa tion.\nThe memory tensors were thus padded to the maximum length (12 8), or to the window-size (32) in the case of local attention.\nEach decoding step thus took the same amount of time. An alter native implementation of incrementally growing the tensor s\ncould save time near the beginning of the sequence.\n7\nT able 1: WMT14 EN-DE Results.\nAttention h d k , d v df f ln(PPL) BLEU BLEU (test)\nType (dev) (dev) beam 1 / 4\nmulti-head 8 128 4096 1.424 26.7 27.7 / 28.4\nmulti-query 8 128 5440 1.439 26.5 27.5 / 28.5\nmulti-head local 8 128 4096 1.427 26.6 27.5 / 28.3\nmulti-query local 8 128 5440 1.437 26.5 27.6 / 28.2\nmulti-head 1 128 6784 1.518 25.8\nmulti-head 2 64 6784 1.480 26.2 26.8 / 27.9\nmulti-head 4 32 6784 1.488 26.1\nmulti-head 8 16 6784 1.513 25.8\nT able 2: Amortized training and inference costs for WMT14 EN -DE T ranslation T ask with sequence length\n128. V alues listed are in TPUv2-microseconds per output tok en.\nAttention T raining Inference Beam-4 Search\nType enc. + dec. enc. + dec.\nmulti-head 13.2 1.7 + 46 2.0 + 203\nmulti-query 13.0 1.5 + 3.8 1.6 + 32\nmulti-head local 13.2 1.7 + 23 1.9 + 47\nmulti-query local 13.0 1.5 + 3.3 1.6 + 16\nT able 3: Billion-W ord LM Benchmark Results.\nAttention h d k , d v df f dev-PPL\nmulti-head 8 128 8192 29.9\nmulti-query 8 128 9088 30.2\nmulti-head 1 128 9984 31.2\nmulti-head 2 64 9984 31.1\nmulti-head 4 32 9984 31.0\nmulti-head 8 16 9984 30.9\n5 Conclusion\nW e have proposed multi-query attention - an alternative to m ulti-head attention with much lower memory-\nbandwidth requirements in the incremental setting. W e beli eve that this enables wider adoption of attention-\nbased sequence models in inference-performance-critical applications.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. Neural machine translation by jointly learning to\nalign and translate, 2014.\nCiprian Chelba, T omas Mikolov, Mike Schuster, Qi Ge, Thorst en Brants, and Phillipp Koehn. One billion\nword benchmark for measuring progress in statistical langu age modeling. CoRR, abs/1312.3005, 2013.\nURL\nhttp://arxiv.org/abs/1312.3005.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer.\n8\nGenerating wikipedia by summarizing long sequences. In Proceedings of the International Conference on\nLearning Representations, 2018.\nDaniel Povey , Hossein Hadian, Pegah Ghahremani, Ke Li, and S anjeev Khudanpur. A time-restricted self-\nattention layer for ASR. In Proceddings of the IEEE International Conference on Acoust ics, Speech and\nSignal Processing (ICASSP). IEEE, 2018.\nAshish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit , Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nBiao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average attention network,\n2018.\n9",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.8698722720146179
    },
    {
      "name": "Computer science",
      "score": 0.7704623937606812
    },
    {
      "name": "Transformer",
      "score": 0.6793254017829895
    },
    {
      "name": "Inference",
      "score": 0.6113296151161194
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5105441808700562
    },
    {
      "name": "Memory bandwidth",
      "score": 0.501258134841919
    },
    {
      "name": "Bandwidth (computing)",
      "score": 0.4517197608947754
    },
    {
      "name": "Computer engineering",
      "score": 0.3701629340648651
    },
    {
      "name": "Algorithm",
      "score": 0.36638474464416504
    },
    {
      "name": "Parallel computing",
      "score": 0.32771748304367065
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31545254588127136
    },
    {
      "name": "Computer network",
      "score": 0.10826337337493896
    },
    {
      "name": "Voltage",
      "score": 0.09091293811798096
    },
    {
      "name": "Engineering",
      "score": 0.07524913549423218
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 26
}