{
  "title": "Character-based NMT with Transformer",
  "url": "https://openalex.org/W2984171820",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2319204204",
      "name": "Gupta, Rohit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225944136",
      "name": "Besacier, Laurent",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202063709",
      "name": "Dymetman, Marc",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226831782",
      "name": "Gallé, Matthias",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2116599427",
    "https://openalex.org/W2949454572",
    "https://openalex.org/W1788779403",
    "https://openalex.org/W2916835973",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2914056350",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W48507986",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2952770479",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2945969861",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2162651021",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2553397501",
    "https://openalex.org/W2951336364",
    "https://openalex.org/W2220350356",
    "https://openalex.org/W2960002028",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W22168010",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2924801999",
    "https://openalex.org/W3038058348",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2963011474",
    "https://openalex.org/W2580723344",
    "https://openalex.org/W1816313093",
    "https://openalex.org/W3211259717",
    "https://openalex.org/W2788760202",
    "https://openalex.org/W2048967369",
    "https://openalex.org/W2888958984",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2173361515",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2896691342",
    "https://openalex.org/W1862810382",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W1905522558"
  ],
  "abstract": "Character-based translation has several appealing advantages, but its performance is in general worse than a carefully tuned BPE baseline. In this paper we study the impact of character-based input and output with the Transformer architecture. In particular, our experiments on EN-DE show that character-based Transformer models are more robust than their BPE counterpart, both when translating noisy text, and when translating text from a different domain. To obtain comparable BLEU scores in clean, in-domain data and close the gap with BPE-based models we use known techniques to train deeper Transformer models.",
  "full_text": "Character-based NMT with Transformer\nRohit Gupta⋆, Laurent Besacier†§, Marc Dymetman†, Matthias Gall´e†\n†Naver Labs Europe, France\n§LIG - Universit´e Grenoble Alpes, France\nAbstract\nCharacter-based translation has several appealing advan-\ntages, but its performance is in general worse than a carefully\ntuned BPE baseline. In this paper we study the impact of\ncharacter-based input and output with the Transformer archi-\ntecture. In particular, our experiments on EN-DE show that\ncharacter-based Transformer models are more robust than\ntheir BPE counterpart, both when translating noisy text, and\nwhen translating text from a different domain. To obtain\ncomparable BLEU scores in clean, in-domain data and close\nthe gap with BPE-based models we use known techniques to\ntrain deeper Transformer models.\n1. Introduction\nCharacter-level NMT models have some compelling charac-\nteristics. They do not suffer from out-of-vocabulary problem\nand avoid tedious and language-speciﬁc pre-processing that\nadds yet another hyper-parameter to tune. In addition, they\nhave been reported to be more robust when translating noisy\ntext [1] and – when using the same architecture – are more\ncompact to store. Those two characteristics are particularly\nimportant for translating user-generated content or spoken\nlanguage, which is often noisy due to transcription errors.\nOn the drawbacks, they tend to perform worse in translation\nquality than using words or Byte-Pair Encoding (BPE) [2, 3]\nsegmentations. In this paper we perform extensive experi-\nments to measure the possibilities of character-based Trans-\nformer models in such scenarios.\nFor LSTM-based architectures, [4] showed that it was\npossible to obtain similar performance at the cost of train-\ning deeper networks. The current state-of-the-art model for\nNMT (and NLP in general) however is Transformer [5] and\nto our knowledge no equivalent study has been reported for\nthat architecture. In this paper, we analyze the impact of\ncharacter-level Transformer models versus BPE-based and\nevaluate them on four axes:\n• translating on clean vs noisy text,\n• in-domain vs out-of-domain conditions,\n⋆ rohitg1594@gmail.com Work done while at Naver Labs Eu-\nrope.\n§ laurent.besacier@univ-grenoble-alpes.fr\n† {marc.dymetman,matthias.galle}@naverlabs.com\n• training in low and high-resource conditions,\n• impact of different network depths.\nOur experiments are for EN-DE, on news (WMT) and\nTED talks (IWSLT). The results show that:\n• it is possible to narrow the gap between BPE and\ncharacter-level models with deeper encoders\n• character-level models are more robust to lexicograph-\nical noise than BPE models out of the box\n• character-level models cope better with test data that is\nfar apart from the training data set\n2. Related Work\nInput representations For deciding what should be the\natomic input symbols, the most intuitive way seems to\nbe to use words as tokens. Continous word representa-\ntion [6, 7] have shown tremendous impact in NLP applica-\ntion [8, 9, 10, 11]. While some of those representations ex-\nploit morphological features [12], such representation still\nface challenges due to the large vocabulary needed and to\nout of vocabulary words. To circumvent these issues, some\nworks learn language representations directly at the charac-\nter level and disregard any notion of word segmentation. This\napproach is attractive due to its simplicity and ability to adapt\nto different languages. It has been used in a wide range of\nNLP tasks such as language modeling [13, 14], question an-\nswering [15] and parsing [16].\nFor translation, character level models in NMT initially\nshowed unsatisfactory performance [17, 18]. The two ear-\nliest models with positive results were [19] and [20]. They\ncompose word representations from their constituent charac-\nters and as such require an ofﬂine segmentation step to be\nperformed beforehand. [21] was able to obviate this step\nby composing representations of “pseudo” words from char-\nacters using convolutional ﬁlters and highway layers [22].\nThe previous methods introduce special modiﬁcations to the\nNMT architecture in order to work at the character level. [4],\non the other hand, uses a vanilla (LSTM based) NMT system\nto achieve superior results at the character level. In a differ-\nent direction, [23] proposes to dynamically learn segmenta-\ntion informed by the NMT objective. The authors discov-\nered that their model prefers to operate on (almost) character\nlevel, providing support for purely character-based NMT.\narXiv:1911.04997v1  [cs.CL]  12 Nov 2019\nA common approach for dealing with the open vocab-\nulary issue is to break up rare words into sub-word units\n[24, 25]. BPE [24] is the standard technique in NMT and\nhas been applied to great success to many systems [26, 27].\nBPE has one hyperparameter: number of merge operations\nn. The optimal ndepends on many factors including NMT\narchitecture, language characteristics and size of training\ndataset. [26] explored several hyperparameter settings, in-\ncluding number of BPE merge operations, to establish strong\nbaselines for NMT in LSTM-based architectures. They rec-\nommended “32K as a generally effective vocabulary size and\n16K as a contrastive condition when building systems on less\nthan 1 million parallel sentences” . [28] does a thorough\nstudy on the impact of n for both LSTM and Transformer\narchitectures. The authors conclude that there is in fact no\noptimal nfor LSTM; it can be very different depending on\nthe dataset and language pair. However, for the Transformer,\nthe best BPE size is between character level and 10k.\nDeep modelsUntil recently it was very hard to train very\ndeep models with the standard Transformer architecture. The\ntraining dynamics tended to be unstable with degradation of\nperformance for deeper models. [29] argued that the main\nculprit was the then in vogue non-linearity function: sig-\nmoid. It saturates for deep models, blocking gradient infor-\nmation from ﬂowing backward. Consequently, [30] proposed\nthe ReLU non-linearity, which is the de facto standard to-\nday. Though this simple technique allows one to train deeper\nmodels than before, it is not sufﬁcient for very deep models\nwith more than 30 layers. Residual connections [31] were\nformulated so that the consequent layers have direct access\nto the layer inputs in addition to the usual forward functions.\nThis simple tweak makes it possible to train models of up\nto 1 000 layers, achieving SOTA on an image classiﬁcation\nbenchmark. [32] ﬁnd it hard to train the Transformer with\nmore than 10 encoder layers. It proposes transparent atten-\ntion, wherein encoder-decoder attention is computed over a\nlinear combination of the outputs of all encoder layers. This\nalleviates the gradient vanishing or exploding problem and\nis sufﬁcient to train Transformer with an encoder of 24 lay-\ners. [33] extends [32] and achieves slight but robust improve-\nments.\nRobustness Machine learning systems can be brittle.\nSmall changes to the input can lead to dramatic failures of\ndeep learning models [34, 35]. For NMT, [36] studied ro-\nbustness to lexicographical errors. They found both char-\nacter and BPE to be very sensitive to such errors with se-\nvere degradation in performance (out-of-the-box robustness\nof character models was however slightly better than the one\nof BPE models). They proposed two techniques to improve\nrobustness of NMT models: structure-invariant word repre-\nsentations and training on noisy texts. These techniques are\nsufﬁcient to make a character based model simultaneously\nrobust to multiple kinds of noise. [37] and [38] also report\nsimilar ﬁndings, namely that training on a balanced diet of\nsynthetic noise can dramatically improve robustness on syn-\nthetic noise. While [38] leverage the noise distribution in the\ntest set, [37] does not. Dealing with noisy data for NMT\ncan also be seen as a domain adaptation problem [39]. The\nmain discrepancy is between the distribution of the training\ndata and test data, also known as domain shift [40]. Many\ndifferent approaches have been studied to train with multiple\ndomains: [41] and [42] include data from the target domain\ninto the training set directly without any modiﬁcations, [43]\nintroduce domain tags to differentiate between different do-\nmain, and ﬁnally, [44] and [45] use a topic model to add topic\ninformation about the domain during training. [46] describes\nthe winning entry to the WMT’19 robustness challenge.\n3. Representation units for Transformer\n3.1. Character vs BPE models\nWe experimented on two language directions, namely,\nGerman-English (DE-EN) and English-German (EN-DE).\nFor DE-EN, we consider two settings: high resourceand low\nresource. For high resource, we concatenate the common-\ncrawl [47] and Europarl [48] corpora. We used the WMT\n2015 news translation test set as our validation set and WMT\n2016 as the test set. For the low resource setting, we used\nthe IWSLT 2014 corpus [49], consisting of transcriptions and\ntranslations of TED Talks.1 We used the ofﬁcial train, valid\nand test splits. In EN-DE, we used the same setup as the\nlow resource setting of DE-EN in the opposite direction. The\nIWSLT14 dataset is much smaller than the WMT corpus used\noriginally by [5]. Therefore, for the low resource setting we\nuse a modiﬁed version of the Transformer base architecture\nwith approximately 50M parameters as compared to 65M for\nTransformer base. For the high resource setting we use the\nunmodiﬁed Transformer base architecture.\nThe training details for the low resource setting are as\nfollows. Training is done on 4 GPUs with a max batch size\nof 4 000 tokens (per GPU). We train for 150 and 60 epochs in\nthe low and high resource settings respectively, while saving\na checkpoint after every epoch and average the 3 best check-\npoints according to their perplexity on a validation set. In the\nlow resource setting, we test all 6 combinations of dropout in\n[0.3,0.4,0.5] and learning rate in [5,10] ×10−4. Using the\nbest dropout and learning rate combination, 5 models (with\ndifferent random seeds) are trained. Whereas for the high re-\nsource setting, we tune dropout in [0.1,0.2,0.3] and set the\nmax learning rate to be 5 ×10−4. Due to the signiﬁcantly\nlarger computational requirements for this dataset, we only\ntrain one model.\nThe average performance and standard deviation (over 5\nmodels) on test set are shown Table 1. The following conclu-\nsions can be drawn from the numbers:\n1. Vocabulary matters for low resource. The impact of\nvocabulary size is signiﬁcant in the low resource set-\nting, BLEU scores differ by over 8 points for DE-EN\nand close to 5 BLEU for EN-DE. For the high resource\n1https://www.ted.com/talks\nV ocab Size DE-EN (low) EN-DE (low) DE-EN (high)\nChar 33.7 ±0.1 26.7 ±0.1 36.3\n1 000 34.0 ±0.2 26.8 ±0.1 –\n2 000 34.4 ±0.2 27.1 ±0.1 –\n5 000 35.0 ±0.0 27.4 ±0.1 36.2\n10 000 34.6 ±0.2 27.6 ±0.1 –\n20 000 30.5 ±0.2 25.3 ±0.1 –\n30 000 28.3 ±0.1 24.3 ±0.1 37.2\n40 000 27.1 ±0.1 23.7 ±0.2 –\n50 000 26.2 ±0.3 23.1 ±0.2 –\nTable 1: Impact of BPE vocab size on BLEU.\nsetting, the effect of vocabulary size is minimal over a\nlarge range.\n2. Optimal BPE is small for low resource. The optimal\nvocabulary size is either 5 000 for DE-EN or 10 000 for\nEN-DE. In the high resource setting, 30K is optimal\nand we corroborate the standard choice.\n3. Character level models are competitive. Though the\ncharacter-level models are not able to beat the best\nBPE models, they are surprisingly competitive with-\nout any modiﬁcations to the architecture.\n3.2. Noisy vs clean\nWe introduce the following four different types of character\nlevel synthetic noise with an associated noise probability p.\nWe respect word boundaries by only applying noise within\nthe word.\n1. delete. Randomly delete a character except for\npunctuation or space.\n2. insert. Insert a random character.\n3. replace. Replace the current character,\n4. switch. Switch the position of two consecutive char-\nacters. We do not apply this for ﬁrst and last character\nof a word.\n5. all. With a probability of p/4, introduce one of the\nnoises listed above.\nFor DE-EN, we also experiment with natural noise.\nWe follow [36] and use their dataset of naturally occur-\nring noise in German. 2 It combines two projects: RWSE\nWikipedia Revision Dataset [50] and the MERLIN corpus of\nlanguage learners [51]. These corpora were created to mea-\nsure spelling difﬁculty and consist of word lists, wherein a\ncorrect German word has associated with it a list of common\nmistakes. For example word “Familie” can be replaced in\nour natural test set by “Famielie”, “Fammilie”, etc.\n2We accessed the dataset from https://github.com/ybisk/\ncharNMT-noise/blob/master/noise/de.natural.\nFigure 1: Degradation of translation quality with increasing\nnoise (character insertion). The slope of the curves (sensibil-\nity) is smaller and shows character level is more robust here.\nFor each noise type, we create ten different noisy ver-\nsions of the test set with different noise probabilities. For\nsynthetic noise, noise proportions were p = 1, 2, ... , 10%,\nwhereas for natural noise, noise proportions werep= 10,\n20, ... , 100%. Note that the 100% natural noise test set\ndoes not have all its tokens transformed. A majority of words\nhave no naturally occurring spelling error in [36]’s dataset.\nWe then compute the BLEU test score on that noisy test data,\nfor each pand for models trained with different vocabulary\nsizes. A representative such plot can be seen in Fig. 1, for\nthe case of using insertion and where each line corresponds\nto one vocabulary size.\nWe calculate the BLEU scores on noisy test sets with dif-\nferent noise probabilities and for each data series, we com-\npute a linear regression:\nBLEU ≈βp+ α, (1)\nwhere pis the noise probability, the slope β is the “sensitiv-\nity” of the NMT system to that type of noise and α is the\nintercept. Closer to 0 means that the system is more robust\nto that kind of noise, and a value of −100 indicates that for\neach additional percentage point of noise the system loses 1\nBLEU point. Those values can be seen in Fig. 2 where we\nplot the values of β vs the vocabulary size. We conclude\nfrom that:\n1. Degradation with noise. Out of the box, both BPE\nand character level models are very sensitive to lexico-\ngraphic noise. BPE models lose as much as 2 BLEU\n(a) DE-EN (low)\n (b) EN-DE (low)\n(c) DE-EN (high)\nFigure 2: Noise sensitivity vs vocab size for models trained on clean data. Character-level models are shown with zero vocabulary\nsize. Sensitivity values closer to zero mean that the model is more robust to that kind of noise.\npoints for each percentage increase in noise, whereas\ncharacter level models lose as much as 1.5 BLEU.\n2. Behaviour of different noises. BPE models are\nroughly equally sensitive to all kinds of synthetic\nnoise. Character level models are more sensitive to\ncertain kinds of noises than other. They are rela-\ntively very robust to switch, approximately equally\nrobust to delete and insert and least robust to\nreplace. We hypothesize that this could be due\nto switch only changing the positional encodings\nlocally. The content embeddings remain intact. In\ncontrast, replace preserves positional encodings\nbut changes the content embeddings. Sensitivity to\nnatural noise is much smaller than to synthetic\nnoise overall, probably due to the fact that increasing\nthe noise level does not have any effect on words that\nare not listed in the [36] dataset.\n3. Character level models are more robust. For each\nkind of noise, character level models are less sensitive\nthan all of the BPE models. They are particularly ro-\nbust to switch, where they are more than twice as\nrobust as the best performing BPE models. Though\ncharacter level models start out at a worse footing than\nthe best BPE models, after applying only 1-2 % (for\nsynthetic noise) of noise in the test set, character level\nmodels perform better.\nWe also experiment with a simple method to robustify\nmodels to the noise. We introduce the aforementioned noises\ninto the training data as well. Thereafter, we test on both\nclean and noisy data. In consideration for time and com-\nputational resources, we choose two representational BPE\nvocabularies – 5 000 for small vocab size and 30 000 for\nlarge vocab size. We also train character level models with\nnoisy data. We only consider synthetic character level noises\nwith noisy probability set to 5% in the low resource setting.\nThe results are shown in Fig. 3 and 4 for DE-EN and EN-\nDE respectively. Each group of two represents training on\nclean/matched noise; for 3 different vocabulary sizes and 6\ndifferent types of noise. The full results of these sets of ex-\nperiments are in Appendix B. The following conclusions are\napparent.\n1. Adding noise helps. Training on similar type of noisy\ndata improves performance for all vocabularies.3\n3We also observed that certain kinds of noise also improve robustness for\nother noises (results with unmatched train and test conditions not reported\nhere).\nFigure 3: BLEU scores for DE-EN models trained and tested on different noise conditions. The ﬁrst (orange) column refers to\ntraining on clean data and testing on noised data; the second (blue) trained and tested on matched noise (which is the same for\nthe clean group)\nFigure 4: BLEU scores for EN-DE models trained and tested on different noise conditions. The ﬁrst (orange) column refers to\ntraining on clean data and testing on noised data; the second (blue) trained and tested on matched noise (which is the same for\nthe clean group)\n2. BPE is as robust as character. By training on similar\nkinds of noise in the training data, we are able to ro-\nbustify BPE models to the same level as character level\nmodels without sacriﬁcing too much performance on\nthe clean test set.\n3. Effect on clean data. We observed (see Tables 6\nand 7 in the Appendix) that for small vocabularies\n(character-level and BPE 5 000), training with noise\nin training data had a small detrimental effect when\ntesting on clean data. However, in the case of BPE\n30 000, training on noisy data signiﬁcantly boosted\nperformance (eg, improvement of 6 BLEU for DE-\nEN and 1.7 for EN-DE when training with delete\nand testing on clean data). We hypothesize that the in-\ncreased diversity of tokens during training (due to the\npresence of noise) acts as a regularizer boosting per-\nformance on the test set.\n3.3. In-domain vs out-of-domain\nWe test the low and high resource models on the following\nin and out of domain datasets:\n1. newstest 2016. News text from WMT 2016 news\ntranslation task\n2. WMT Biomedical. Medline abstracts from WMT\n2018 biomedical translation task.\n3. WMT-IT. Hardware and software troubleshooting an-\nswers from WMT 2016 IT domain translation task.\n4. Europarl. The ﬁrst 3 000 sentences from Europarl\ncorpus [48]. Proceedings of the European Parliament.\n5. commoncrawl. The ﬁrst 3 000 sentences from com-\nmoncrawl parallel text corpus [47].\nWe provide two similarity metrics between the training\nand test sets in Tables 2. “% Unseen” is the percentage of\nwords in the test set that are not present in the training cor-\npus. “PPL” is the perplexity measure of the test set using\na language model trained on the training data. We used the\nkenlm4 toolkit with Kneser-Ney smoothing [52] and context\nsize of 4.\nWe show results in Figure 5 and conclude the following:\n1. DE-EN low resource. Character level models are bet-\nter for all out of domain datasets, except for Europarl.\nRecall from Table 2 that Europarl also has the least\nproportion of unseen words. This suggests that char-\nacter level models outperform BPE when evaluated on\ndata sufﬁciently different from the training domain in\nthis low resource setting.\n2. DE-EN high resource. Character level models are\nnow only better when testing on the WMT-Biomedical\ntest set. We see from Table 2 that it also has the largest\n4https://github.com/kpu/kenlm\nproportion of unseen words. For all other test sets,\nBPE 30 000 leads to the best BLEU scores.\n3. EN-DE low resource. We see similar performance for\nin and out of domain data. Good BPE models on in-\ndomain test set are still better on out-of-domain test\nsets. A possible explanation is the lower proportion of\nunseen words as compared to German and seeing the\nwords more frequently in the training corpus.\n3.4. Deeper character-based Transformers\nFor other architectures, training deeper models had a very\npositive impact on character-based translations [4], however\nsimilar studies have not been reported using the Transformer.\nDue to computational constraints, we experiment only on\nDE-EN language pair in the low and high resource settings\nand train only one model for each conﬁguration.\n3.4.1. Low resource\nWe train models from 6 to 16 encoder layers for character\nlevel and BPE 5 000, the best performing vocabulary size in\nour preliminary experiments. We ﬁx learning rate to 5 ×\n10−4 and tune dropout in [0.3,0.4,0.5]. First, we do not\nperform any modiﬁcations to the Transformer architecture.\nIn particular, this means that layer normalization takes place\nafter each sub layer. To train deeper models, following [33],\nwe place the layer normalization step before each layer, and\nalso experiment with transparent attention [32].\nIn contrast to [4], we see a degradation of performance\nwith increasing depth for post-normalization (Figure 6 illus-\ntrates this with the standard Transformer architecture when\ngoing from 10 to 12 layers), but the simple trick of switching\nthe sequence of performing layer normalization is sufﬁcient\nto train models with up to 32 layers in the encoder. We there-\nfore report only results using pre-normalization in Table 3.\nWhile adding transparent attention is beneﬁcial for almost all\ndepths in the character level models, it gives mixed results for\nthe BPE 5 000 model. Hence, we see that by training deeper\nmodels we are able to marginally improve performance for\nboth vocabularies. For character level models we improve by\n1 BLEU points (from 33.7 to 34.7). For 5 000, the gain is\na more modest 0.4 BLEU points (from 35 to 35.4). We are\nable to narrow but not close the gap between character and\nBPE.\n3.4.2. High resource\nIn light of aforementioned experiments, we no longer train\nmodels with post layer normalization and restrict ourselves to\npre layer normalization and transparent attention. Here, we\nalso experiment with the BPE 30 000 vocabulary. The results\nare shown in Table 4. Here again we see an improvement in\nBLEU scores with increasing depth of 1-2 points when go-\ning beyond 6 encoder layers. Transparent attention seems to\nhelp consistently for character level models but barely does\nDE-EN EN-DE DE-EN (high)\nDataset # Sents % Unseen PPL % Unseen PPL % Unseen PPL\nIWSLT14 6 750 4.4 583 2.2 282 2.0 740\nWMT-IT 2 000 14.4 2 540 13.2 2 322 5.8 996\nWMT-Bio. 321 20.0 5 540 12.1 3 035 9.2 3 404\nnewstest 2016 2 999 12.7 2,712 9.0 1,659 4.5 1 703\nEuroparl 3 000 9.0 1,765 4.4 771 0 10\ncommoncrawl 3 000 17.8 5 024 12.6 2 711 0 9\navg 3 011 13.0 3 022 8.9 1,797 3.6 1 144\nTable 2: Similarity metrics between test sets and training sets.\n(a) DE-EN (low)\n (b) EN-DE (low)\n (c) DE-EN (high)\nFigure 5: BLEU scores for different vocabularies on test sets from different domains.\nChar 5 000\nenc PreN PreN+T PreN PreN+T\n6 33.4 33.2 34.6 34.6\n12 33.8 34.5 34.8 34.8\n16 33.5 34.5 35.2 34.9\n20 34.4 34.7 35.3 34.9\n24 34.1 34.7 35.1 35.4\n28 34.4 34.3 35 35\n32 34.1 34.5 34.7 35.2\nTable 3: Results for the low resource setting. “PreN + T”\nrefers to an architecture with layer normalization before each\nsub-layer and transparent attention. Best results for each\nlayer depth are shown in bold.\nanything for the two BPE models. Further, with increased\ndepth, BPE 5 000 and 30 000 perform similarly in contrast to\nshallow models where there is a 1 BLEU difference. How-\never, character level models are still slightly worse than the\nBPE models with a max score of 37.7 rather than 38 for the\nBPE models.\n4. Discussion and recommendations\nVocabulary Size.We observed that in the low resource set-\nting, BPE vocab size can be a very important parameter to\ntune, having a large impact on BLEU. However, the effect\nvanishes for the high resource setting, where performance is\nChar 5 000 30 000\nenc PreN PreN + T PreN PreN+T PreN PreN+T\n6 36.3 36.5 36.2 36.4 37.2 36.9\n12 36.8 37.5 37.1 36.9 37.4 37.5\n18 37.3 37.4 37.6 37.7 37.9 37.8\n24 37.7 37.7 37.6 37.6 37.8 37.8\n32 37.2 37.4 37.9 38 38 37.9\nTable 4: Results for the high resource setting.\nsimilar for a large range of vocabulary sizes. Character level\nmodels also tend to be competitive with BPE.\nLexicographical noise. When trained on clean data,\ncharacter-based models are more robust to natural and syn-\nthetic lexicographical noise than BPE-based models (these\nresults conﬁrm a trend already observed in [36]), however\nthe trend fades away when similar kind of noise is intro-\nduced in the training data as well. Surprisingly, we observed\nthat noise on training data might be acting as a regularizer\nfor the large BPE vocabularies (breaking up large tokens into\nsmaller ones) and improves results on clean inputs.\nDomain shift.Better results on 4 datasets over 5 with char-\nacter based models for DE-EN; character level models out-\nperform BPE when evaluated on data sufﬁciently different\nfrom the training domain in low resource; no signiﬁcant\ndifferences for EN-DE. In DE-EN (high resource), charac-\nter level models are only better when testing on the WMT-\nBiomedical test set which has the largest proportion of un-\nFigure 6: Degradation observed with the standard Transformer architecture when going from 10 to 12 layers.\nseen words.\nDeep models. In contrast to Cherry et al. (2018), we see\na degradation of performance with increasing depth without\nany modiﬁcation of the Transformer architecture. We can\ntrain deeper and more efﬁcient character-based Transformers\nby switching the sequence of performing layer normaliza-\ntion. Doing so, we can train models with up to 32 layers\nin the encoder. We are able to narrow but not close the gap\nbetween character and BPE. These tricks may also hold for\nother use cases where longer input sequences are needed (for\ninstance: document level NMT).\n5. Conclusion\nIn this work, we have studied the characteristics of different\nrepresentation units in NMT including character-level mod-\nels and BPE models with different vocabulary sizes. We ob-\nserved that different representations can have very different\nbehaviours with distinct advantages and disadvantages. In\nthe future, we would like to investigate methods to combine\ndifferent representations in order to get the best of all worlds.\n6. References\n[1] H. S. Y . B. Nadir Durrani, Fahim Dalvi and P. Nakov,\n“What is in a translation unit? comparing character and\nsubword representations beyond translation,” 2018.\n[Online]. Available: https://openreview.net/pdf?id=\nB1x0E2C5tQ\n[2] B. Heinzerling and M. Strube, “Bpemb: Tokenization-\nfree pre-trained subword embeddings in 275 lan-\nguages,” CoRR, vol. abs/1710.02187, 2017. [Online].\nAvailable: http://arxiv.org/abs/1710.02187\n[3] J. Bradbury, S. Merity, C. Xiong, and R. Socher,\n“Quasi-recurrent neural networks,” CoRR, vol.\nabs/1611.01576, 2016. [Online]. Available: http:\n//arxiv.org/abs/1611.01576\n[4] C. Cherry, G. Foster, A. Bapna, O. Firat, and\nW. Macherey, “Revisiting character-based neural\nmachine translation with capacity and compression,”\nCoRR, vol. abs/1808.09943, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1808.09943\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,”CoRR, vol. abs/1706.03762,\n2017. [Online]. Available: http://arxiv.org/abs/1706.\n03762\n[6] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado,\nand J. Dean, “Distributed representations of words and\nphrases and their compositionality,” in Advances in\nNeural Information Processing Systems 26 , C. J. C.\nBurges, L. Bottou, M. Welling, Z. Ghahramani, and\nK. Q. Weinberger, Eds. Curran Associates, Inc., 2013,\npp. 3111–3119.\n[7] J. Pennington, R. Socher, and C. D. Manning, “Glove:\nGlobal vectors for word representation,” in In EMNLP,\n2014.\n[8] P. Wang, Y . Qian, F. K. Soong, L. He, and\nH. Zhao, “Part-of-speech tagging with bidirectional\nlong short-term memory recurrent neural network,”\nCoRR, vol. abs/1510.06168, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1510.06168\n[9] I. Yamada, H. Shindo, H. Takeda, and Y . Take-\nfuji, “Learning distributed representations of texts\nand entities from knowledge base,” CoRR,\nvol. abs/1705.02494, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1705.02494\n[10] M. Tan, B. Xiang, and B. Zhou, “Lstm-based deep\nlearning models for non-factoid answer selection,”\nCoRR, vol. abs/1511.04108, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1511.04108\n[11] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and\nA. Bordes, “Supervised learning of universal sentence\nrepresentations from natural language inference data,”\nCoRR, vol. abs/1705.02364, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1705.02364\n[12] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov,\n“Enriching word vectors with subword information,”\nTransactions of the Association for Computational Lin-\nguistics, vol. 5, pp. 135–146, 2017.\n[13] T. Mikolov, I. Sutskever, A. Deoras, H.-S. Le, S. Kom-\nbrink, and J. Cernock ´y, “Subword language modeling\nwith neural networks,” 2011.\n[14] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and\nL. Jones, “Character-level language modeling with\ndeeper self-attention,” CoRR, vol. abs/1808.04444,\n2018. [Online]. Available: http://arxiv.org/abs/1808.\n04444\n[15] T. Kenter, L. Jones, and D. Hewlett, Eds.,\nByte-level Machine Reading across Morpho-\nlogically Varied Languages , 2018. [Online].\nAvailable: https://www.aaai.org/ocs/index.php/AAAI/\nAAAI18/paper/download/16605/16145\n[16] M. Ballesteros, C. Dyer, and N. A. Smith, “Improved\ntransition-based parsing by modeling characters instead\nof words with lstms,” CoRR, vol. abs/1508.00657,\n2015. [Online]. Available: http://arxiv.org/abs/1508.\n00657\n[17] D. Vilar, J.-T. Peter, and H. Ney, “Can we translate\nletters?” in Proceedings of the Second Workshop\non Statistical Machine Translation , ser. StatMT ’07.\nStroudsburg, PA, USA: Association for Computational\nLinguistics, 2007, pp. 33–39. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=1626355.1626360\n[18] G. Neubig, T. Watanabe, S. Mori, and\nT. Kawahara, “Substring-based machine transla-\ntion,” Machine Translation , vol. 27, no. 2,\npp. 139–166, June 2013. [Online]. Available:\nhttp://dx.doi.org/10.1007/s10590-013-9136-6\n[19] W. Ling, I. Trancoso, C. Dyer, and A. W. Black,\n“Character-based neural machine translation,” CoRR,\nvol. abs/1511.04586, 2015. [Online]. Available: http:\n//arxiv.org/abs/1511.04586\n[20] M. R. Costa-juss `a and J. A. R. Fonollosa,\n“Character-based neural machine translation,” CoRR,\nvol. abs/1603.00810, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1603.00810\n[21] J. Lee, K. Cho, and T. Hofmann, “Fully character-\nlevel neural machine translation without explicit\nsegmentation,” CoRR, vol. abs/1610.03017, 2016.\n[Online]. Available: http://arxiv.org/abs/1610.03017\n[22] R. K. Srivastava, K. Greff, and J. Schmidhuber,\n“Highway networks,” CoRR, vol. abs/1505.00387,\n2015. [Online]. Available: http://arxiv.org/abs/1505.\n00387\n[23] J. Kreutzer and A. Sokolov, “Learning to segment\ninputs for NMT favors character-level processing,”\nCoRR, vol. abs/1810.01480, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1810.01480\n[24] R. Sennrich, B. Haddow, and A. Birch, “Neural\nmachine translation of rare words with subword units,”\nCoRR, vol. abs/1508.07909, 2015. [Online]. Available:\nhttp://arxiv.org/abs/1508.07909\n[25] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y . Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser,\nS. Gouws, Y . Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean, “Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation,” CoRR, vol. abs/1609.08144,\n2016. [Online]. Available: http://arxiv.org/abs/1609.\n08144\n[26] M. J. Denkowski and G. Neubig, “Stronger baselines\nfor trustable results in neural machine translation,”\nCoRR, vol. abs/1706.09733, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1706.09733\n[27] Proceedings of the 4th Workshop on Asian Translation\n(WAT2017). Taipei, Taiwan: Asian Federation of Nat-\nural Language Processing, Nov. 2017. [Online]. Avail-\nable: https://www.aclweb.org/anthology/W17-5700\n[28] S. Ding, A. Renduchintala, and K. Duh, “A call\nfor prudent choice of subword merge operations,”\nCoRR, vol. abs/1905.10453, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1905.10453\n[29] X. Glorot and Y . Bengio, “Understanding the dif-\nﬁculty of training deep feedforward neural net-\nworks,” in Proceedings of the Thirteenth International\nConference on Artiﬁcial Intelligence and Statistics ,\nser. Proceedings of Machine Learning Research,\nY . W. Teh and M. Titterington, Eds., vol. 9.\nChia Laguna Resort, Sardinia, Italy: PMLR, 13–\n15 May 2010, pp. 249–256. [Online]. Available:\nhttp://proceedings.mlr.press/v9/glorot10a.html\n[30] V . Nair and G. E. Hinton, “Rectiﬁed lin-\near units improve restricted boltzmann machines,”\nin Proceedings of the 27th International Con-\nference on International Conference on Ma-\nchine Learning , ser. ICML’10. USA: Omni-\npress, 2010, pp. 807–814. [Online]. Available:\nhttp://dl.acm.org/citation.cfm?id=3104322.3104425\n[31] K. He, X. Zhang, S. Ren, and J. Sun, “Deep\nresidual learning for image recognition,” CoRR, vol.\nabs/1512.03385, 2015. [Online]. Available: http:\n//arxiv.org/abs/1512.03385\n[32] A. Bapna, M. X. Chen, O. Firat, Y . Cao, and\nY . Wu, “Training deeper neural machine trans-\nlation models with transparent attention,” CoRR,\nvol. abs/1808.07561, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1808.07561\n[33] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong,\nand L. S. Chao, “Learning deep transformer models\nfor machine translation,” CoRR, vol. abs/1906.01787,\n2019. [Online]. Available: http://arxiv.org/abs/1906.\n01787\n[34] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna,\nD. Erhan, I. Goodfellow, and R. Fergus, “Intriguing\nproperties of neural networks,” in International Con-\nference on Learning Representations , 2014. [Online].\nAvailable: http://arxiv.org/abs/1312.6199\n[35] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining\nand harnessing adversarial examples,” in Interna-\ntional Conference on Learning Representations , 2015.\n[Online]. Available: http://arxiv.org/abs/1412.6572\n[36] Y . Belinkov and Y . Bisk, “Synthetic and natural\nnoise both break neural machine translation,” CoRR,\nvol. abs/1711.02173, 2017. [Online]. Available: http:\n//arxiv.org/abs/1711.02173\n[37] V . Karpukhin, O. Levy, J. Eisenstein, and\nM. Ghazvininejad, “Training on synthetic noise\nimproves robustness to natural noise in machine trans-\nlation,” CoRR, vol. abs/1902.01509, 2019. [Online].\nAvailable: http://arxiv.org/abs/1902.01509\n[38] Vaibhav, S. Singh, C. Stewart, and G. Neubig,\n“Improving robustness of machine translation with\nsynthetic noise,” CoRR, vol. abs/1902.09508, 2019.\n[Online]. Available: http://arxiv.org/abs/1902.09508\n[39] P. Koehn and R. Knowles, “Six challenges for\nneural machine translation,” in Proceedings of the\nFirst Workshop on Neural Machine Translation .\nVancouver: Association for Computational Linguistics,\nAug. 2017, pp. 28–39. [Online]. Available: https:\n//www.aclweb.org/anthology/W17-3204\n[40] J. Quionero-Candela, M. Sugiyama, A. Schwaighofer,\nand N. D. Lawrence, Dataset Shift in Machine Learn-\ning. The MIT Press, 2009.\n[41] M. Li, Y . Zhao, D. Zhang, and M. Zhou, “Adaptive\ndevelopment data selection for log-linear model in\nstatistical machine translation,” in Proceedings of\nthe 23rd International Conference on Computational\nLinguistics (Coling 2010) . Beijing, China: Coling\n2010 Organizing Committee, Aug. 2010, pp. 662–\n670. [Online]. Available: https://www.aclweb.org/\nanthology/C10-1075\n[42] A. Axelrod, X. He, and J. Gao, “Domain adaptation\nvia pseudo in-domain data selection,” in Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing, ser. EMNLP ’11. Stroudsburg,\nPA, USA: Association for Computational Linguistics,\n2011, pp. 355–362. [Online]. Available: http://dl.acm.\norg/citation.cfm?id=2145432.2145474\n[43] C. Kobus, J. M. Crego, and J. Senellart, “Domain\ncontrol for neural machine translation,” CoRR, vol.\nabs/1612.06140, 2016. [Online]. Available: http:\n//arxiv.org/abs/1612.06140\n[44] J. Zhang, L. Li, A. Way, and Q. Liu, “Topic-\ninformed neural machine translation,” in Proceedings\nof COLING 2016, the 26th International Conference on\nComputational Linguistics: Technical Papers. Osaka,\nJapan: The COLING 2016 Organizing Committee,\nDec. 2016, pp. 1807–1817. [Online]. Available:\nhttps://www.aclweb.org/anthology/C16-1170\n[45] W. Chen, E. Matusov, S. Khadivi, and J. Peter,\n“Guided alignment training for topic-aware neural\nmachine translation,” CoRR, vol. abs/1607.01628,\n2016. [Online]. Available: http://arxiv.org/abs/1607.\n01628\n[46] A. B ´erard, I. Calapodescu, and C. Roux, “Naver labs\neurope’s systems for the wmt19 machine translation ro-\nbustness task,”arXiv preprint arXiv:1907.06488, 2019.\n[47] J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,\nC. Callison-Burch, and A. Lopez, “Dirt cheap web-\nscale parallel text from the common crawl,” vol. 1, 07\n2013.\n[48] P. Koehn, “Europarl: A parallel corpus for statistical\nmachine translation,” vol. 5, 11 2004.\n[49] M. Federico, S. Stcker, and F. Yvon, “International\nworkshop onspoken language translation,” in Proceed-\nings of theInternational Workshop onSpoken Language\nTranslation. Lake Tahoe, CA, USA: Association for\nComputational Linguistics, Dec. 2014.\n[50] T. Zesch, “Measuring contextual ﬁtness using error\ncontexts extracted from the Wikipedia revision history,”\nin Proceedings of the 13th Conference of the European\nChapter of the Association for Computational Linguis-\ntics. Avignon, France: Association for Computational\nLinguistics, Apr. 2012, pp. 529–538. [Online]. Avail-\nable: https://www.aclweb.org/anthology/E12-1054\n[51] K. Wisniewski, K. Schne, L. Nicolas, C. Vettori,\nA. Boyd, D. Meurers, A. Abel, and J. Hana, “Merlin:\nAn online trilingual learner corpus empirically ground-\ning the european reference levels in authentic learner\ndata,” 10 2013.\n[52] H. Ney, U. Essen, and R. Kneser, “On structuring\nprobabilistic dependencies in stochastic language mod-\nelling,” Computer Speech and Language, vol. 8, pp. 1–\n38, 1994.\nA. Training Details\nThe training details for the low resource setting are as fol-\nlows. Training is done on 4 GPUs with a max batch size\nof 4000 tokens (per GPU). We train for 150 epochs, while\nsaving a checkpoint after every epoch and average the 3 best\ncheckpoints according to their perplexity on a validation set.\nFor each setting, we test all 6 combinations of dropout in\n[0.3,0.4,0.5] and learning rate in [5,10] ×10−4. Using the\nbest dropout and learning rate combination, 5 models (with\ndifferent random seeds) are trained.\nThe training details for the high resource setting are as\nfollows. Training is done on 4 GPUs with a max batch size\nof 3,500 tokens (per GPU). We train for 60 epochs, while\nsaving a checkpoint after every epoch and average the 3 best\ncheckpoints according to their perplexity on a validation set.\nFor each setting, we test all 3 combinations of dropout in\n[0.1,0.2,0.3] and set the max learning rate to be 5 ×10−4.\nDue to the signiﬁcantly larger computational requirements\nfor this dataset, we only train one model for each conﬁgura-\ntion.\nB. Robustness to Noise\nWe include here the full tables of training and testing on clean\nand noised datasets. A summary of those tables can be found\nin Figure 3 and 4 in the main part of the paper.\nHyperparameter Transformer Base Our Version\nEncoder embedding dimension 512 512\nEncoder fully forward embedding dimension 2048 1024\nEncoder layers 6 6\nEncoder attention heads 8 4\nDecoder embedding dimension 512 512\nDecoder fully forward embedding dimension 2048 1024\nDecoder layers 6 6\nDecoder attention heads 8 4\nShare encoder-decoder embeddings X ✓\nTable 5: Hyperparameter Settings for the modiﬁed Transformer architecture.\ntrain \\test clean all delete insert replace switch avg\nclean 34.1 27.6 27.9 27.7 25.4 30.6 28.9\nall 32.7 33.1 30.4 32.1 28.1 32.4 31.5\ndelete 33.0 28.1 30.7 25.9 24.6 30.6 28.8\ninsert 32.7 28.8 25.5 32.4 25.7 30.2 29.2\nreplace 32.9 29.7 28.2 30.3 31.1 30.2 30.4\nswitch 33.1 28.4 26.8 28.0 24.8 32.9 29.0\n(a) Character\ntrain \\test clean all delete insert replace switch avg\nclean 35.0 23.5 24.9 23.5 22.7 25.0 25.8\nall 34.4 32.9 32.0 33.4 31.6 33.6 33.0\ndelete 34.3 29.6 32.7 27.3 25.5 31.1 30.1\ninsert 34.4 30.9 29.1 33.8 30.4 30.8 31.6\nreplace 34.0 30.7 29.2 32.1 31.9 29.9 31.3\nswitch 34.1 29.3 29.2 28.0 25.2 33.7 29.9\n(b) 5,000\ntrain \\test clean all delete insert replace switch avg\nclean 28.2 17.3 18.2 17.0 17.9 19.0 19.6\nall 34.0 32.0 31.5 32.3 30.2 32.9 32.2\ndelete 34.2 28.1 32.5 25.5 24.2 29.3 29.0\ninsert 34.2 30.4 28.8 32.9 29.6 29.7 30.9\nreplace 33.8 30.4 28.8 31.5 30.9 29.6 30.8\nswitch 34.2 28.0 28.2 25.1 23.2 33.7 28.7\n(c) 30,000\nTable 6: BLEU scores for DE-EN models trained and tested on different noises.\n(a) delete\n (b) insert\n(c) replace\n (d) switch\n(e) all\n (f) natural\nFigure 7: Degradation of translation quality with lexicographical noise for model trained on DE-EN language pair in the low\nresource setting. Character level model is shown in light green (best viewed in color).\n(a) delete\n (b) insert\n(c) replace\n (d) switch\n(e) all\nFigure 8: Degradation of translation quality with lexicographical noise for model trained on EN-DE language pair in the low\nresource setting. Character level model is shown in light green (best viewed in color).\n(a) delete\n (b) insert\n(c) replace\n (d) switch\n(e) all\n (f) natural\nFigure 9: Degradation of translation quality with lexicographical noise for model trained on DE-EN language pair in the high\nresource setting. Character level model is shown in light green (best viewed in color).\ntrain \\test clean all delete insert replace switch avg\nclean 26.9 21.2 21.3 21.4 19.3 22.5 22.1\nall 25.7 25.3 24.1 26.2 24.2 25.5 25.2\ndelete 25.3 22.0 24.9 20.2 19.8 23.2 22.6\ninsert 25.9 22.8 20.7 26.5 21.1 22.7 23.3\nreplace 26.0 23.5 21.9 24.3 25.3 22.8 24.0\nswitch 25.6 22.1 21.9 21.5 19.5 25.9 22.8\n(a) Character\ntrain \\test clean all delete insert replace switch avg\nclean 27.4 17.6 18.0 17.5 17.2 17.4 19.2\nall 25.9 25.4 24.7 26.0 24.9 25.5 25.4\ndelete 26.1 22.1 25.5 20.1 19.8 22.9 22.8\ninsert 26.2 23.7 21.9 26.6 23.4 22.6 24.1\nreplace 26.0 24.1 22.1 25.5 25.4 22.5 24.3\nswitch 26.3 22.4 22.6 20.9 19.9 26.2 23.1\n(b) 5,000\ntrain \\test clean all delete insert replace switch avg\nclean 24.5 14.9 15.4 14.3 14.5 14.8 16.4\nall 26.2 25.2 25.1 25.7 24.6 25.7 25.4\ndelete 26.2 21.6 25.6 19.7 19.3 22.1 22.4\ninsert 26.3 23.8 21.9 26.2 23.7 22.4 24.1\nreplace 26.2 23.8 22.1 24.9 25.2 22.4 24.1\nswitch 26.6 21.7 21.5 19.5 19.0 26.4 22.5\n(c) 30,000\nTable 7: BLEU scores for EN-DE models trained and tested on different noises.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.861967921257019
    },
    {
      "name": "Computer science",
      "score": 0.6735098361968994
    },
    {
      "name": "Character (mathematics)",
      "score": 0.5446504950523376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43439334630966187
    },
    {
      "name": "Speech recognition",
      "score": 0.3768678903579712
    },
    {
      "name": "Natural language processing",
      "score": 0.3340443968772888
    },
    {
      "name": "Engineering",
      "score": 0.15495315194129944
    },
    {
      "name": "Mathematics",
      "score": 0.13618174195289612
    },
    {
      "name": "Voltage",
      "score": 0.11963343620300293
    },
    {
      "name": "Electrical engineering",
      "score": 0.08628177642822266
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    }
  ],
  "cited_by": 8
}