{
  "title": "Cyberbullying Text Identification based on Deep Learning and Transformer-based Language Models",
  "url": "https://openalex.org/W4392051252",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2098026964",
      "name": "Saifullah Khalid",
      "affiliations": [
        "Chittagong University of Engineering & Technology"
      ]
    },
    {
      "id": null,
      "name": "Khan, Muhammad Ibrahim",
      "affiliations": [
        "Chittagong University of Engineering & Technology"
      ]
    },
    {
      "id": null,
      "name": "Jamal, Suhaima",
      "affiliations": [
        "Georgia Southern University"
      ]
    },
    {
      "id": "https://openalex.org/A4287424419",
      "name": "Sarker, Iqbal H.",
      "affiliations": [
        "Edith Cowan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2912431116",
    "https://openalex.org/W4306877294",
    "https://openalex.org/W3153771145",
    "https://openalex.org/W4384938272",
    "https://openalex.org/W3016831976",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2595653137",
    "https://openalex.org/W2579442881",
    "https://openalex.org/W4200127519",
    "https://openalex.org/W2785615365",
    "https://openalex.org/W2963532322",
    "https://openalex.org/W4389762492",
    "https://openalex.org/W4379117903",
    "https://openalex.org/W3179438688",
    "https://openalex.org/W4380928679",
    "https://openalex.org/W3180896826",
    "https://openalex.org/W4360980398",
    "https://openalex.org/W2972657613",
    "https://openalex.org/W2912102236",
    "https://openalex.org/W3031939012",
    "https://openalex.org/W2252009349",
    "https://openalex.org/W2952629768",
    "https://openalex.org/W2954643340",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2031126038",
    "https://openalex.org/W3031484690",
    "https://openalex.org/W2962977603"
  ],
  "abstract": "In the contemporary digital age, social media platforms like Facebook, Twitter, and YouTube serve as vital channels for individuals to express ideas and connect with others. Despite fostering increased connectivity, these platforms have inadvertently given rise to negative behaviors, particularly cyberbullying. While extensive research has been conducted on high-resource languages such as English, there is a notable scarcity of resources for low-resource languages like Bengali, Arabic, Tamil, etc., particularly in terms of language modeling. This study addresses this gap by developing a cyberbullying text identification system called BullyFilterNeT tailored for social media texts, considering Bengali as a test case. The intelligent BullyFilterNeT system devised overcomes Out-of-Vocabulary (OOV) challenges associated with non-contextual embeddings and addresses the limitations of context-aware feature representations. To facilitate a comprehensive understanding, three non-contextual embedding models GloVe, FastText, and Word2Vec are developed for feature extraction in Bengali. These embedding models are utilized in the classification models, employing three statistical models (SVM, SGD, Libsvm), and four deep learning models (CNN, VDCNN, LSTM, GRU). Additionally, the study employs six transformer-based language models: mBERT, bELECTRA, IndicBERT, XML-RoBERTa, DistilBERT, and BanglaBERT, respectively to overcome the limitations of earlier models. Remarkably, BanglaBERT-based BullyFilterNeT achieves the highest accuracy of 88.04% in our test set, underscoring its effectiveness in cyberbullying text identification in the Bengali language.",
  "full_text": "EAI Endorsed Transactions\non Industrial Networks and Intelligent Systems Research Article\nCyberbullying Text Identification: A Deep Learning\nand Transformer-based Language Modeling\nApproach\nKhalid Saifullah1,*, Muhammad Ibrahim Khan1, Suhaima Jamal2, Iqbal H. Sarker3,*\n1Department of Computer Science and Engineering, Chittagong University of Engineering and Technology; \nChittagong-4349, Bangladesh\n2Dept. of Information Technology, Georgia Southern University, Statesboro GA, USA\n3Centre for Securing Digital Futures, School of Science, Edith Cowan University, Perth, WA-6027, Australia\nAbstract\nIn the contemporary digital age, social media platforms like Facebook, Twitter, and YouTube serve as vital\nchannels for individuals to express ideas and connect with others. Despite fostering increased connectivity,\nthese platforms have inadvertently given rise to negative behaviors, particularly cyberbullying. While\nextensive research has been conducted on high-resource languages such as English, there is a notable\nscarcity of resources for low-resource languages like Bengali, Arabic, Tamil, etc., particularly in terms\nof language modeling. This study addresses this gap by developing a cyberbullying text identification\nsystem called BullyFilterNeT tailored for social media texts, considering Bengali as a test case. The\nintelligent BullyFilterNeT system devised overcomes Out-of-Vocabulary (OOV) challenges associated with\nnon-contextual embeddings and addresses the limitations of context-aware feature representations. To\nfacilitate a comprehensive understanding, three non-contextual embedding models GloVe, FastText, and\nWord2Vec are developed for feature extraction in Bengali. These embedding models are utilized in the\nclassification models, employing three statistical models (SVM, SGD, Libsvm), and four deep learning models\n(CNN, VDCNN, LSTM, GRU). Additionally, the study employs six transformer-based language models:\nmBERT, bELECTRA, IndicBERT, XML-RoBERTa, DistilBERT, and BanglaBERT, respectively to overcome the\nlimitations of earlier models. Remarkably, BanglaBERT-based BullyFilterNeT achieves the highest accuracy\nof 88.04% in our test set, underscoring its eff ectiveness in cyberbullying text identification in the Bengali\nlanguage.\nReceived on 28 December 2023; accepted on 19 February 2024; published on 22 February 2024\nKeywords: Cyberbullying; large language modeling; deep learning; transformers models; natural language processing\n(NLP); fine tuning; OOV; harmful messages\nCopyright © 2024 K. Saifullah et al., licensed to EAI. This is an open access article distributed under the terms of the CC\nBY-NC-SA 4.0, which permits copying, redistributing, remixing, transformation, and building upon the material in any\nmedium so long as the original work is properly cited.\ndoi:10.4108/eetinis.v11i1.4703\n1. Introduction\nCyberbullying text identification refers to the process of\ndetecting and recognizing instances of cyberbullying in\nwritten digital communications, such as text messages,\nemails, social media posts, or other online interactions.\nThe rapid growth of media platforms like Facebook,\nTwitter, and YouTube has transformed communication,\nallowing individuals to express opinions on various\n∗Email: mdkhalidsaifullah@hotmail.com, m.sarker@ecu.edu.au\ntopics. However, this has also led to the spread of offen-\nsive and hateful content. Cyberbullying, a significant\nissue, can cause psychological distress and undermine\nrespectful conversations. According to research, this\nkind of conduct happened on Facebook and Twitter\nquite a bit. Among Bangladesh’s 80.83 million Internet\nusers [27], over 90% use Facebook regularly, and the\nbulk of these users are young, delicate, and in desper-\nate need of protection. Lately, numerous studies have\nfocused on high-resource languages such as English.\n1\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\nThis emphasis is attributed to the existence of ade-\nquately annotated cyberbullying corpora, pre-trained\nmodels for text-to-feature extraction, pre-trained cyber-\nbullying identification models, and a suite of finely\ntuned hyperparameters [9].\nNevertheless, Bengali stands as the seventh most\nwidely spoken language globally, with approximately\n245 million people in Bangladesh and two states of\nIndia conversing in Bengali [16]. The growing popu-\nlarity of Bangla on social media is attributed to the\nwidespread acceptance of the Unicode system and the\nincreasing use of the Internet [17]. Consequently, a\nsubstantial volume of Bengali bullying texts has pro-\nliferated across the unstructured web. Manually iden-\ntifying these unstructured Bengali texts is impractical\nand financially burdensome. To address these chal-\nlenges, the development of a cyberbullying identifica-\ntion system becomes imperative for government poli-\ncymakers and security agencies. However, the scarcity\nof annotated corpora related to bullying, the absence\nof domain-specific pre-trained feature extraction and\nclassification models, and the unavailability of well-\ntuned hyperparameters for domain-centric tasks pose\nsignificant obstacles.\nIn recent years, considerable research has been ded-\nicated to Bengali text classification [15], authorship\nattribution [14], sentiment analysis [2], and emotion\nclassification [5]. However, the exploration of Ben-\ngali cyberbullying identification from textual data has\nbeen relatively limited [1, 3, 4]. Notably, the predom-\ninant approaches in existing research involve TF-IDF\nand non-contextual embedding-based (i.e., GloVe, Fast-\nText, Word2Vec) feature extraction, alongside statisti-\ncal, CNN, and LSTM-based classification models. It is\nworth noting that the TF-IDF-based feature extractor\nfalls short in capturing semantic meaning-based text\nfeatures, while non-contextual embeddings like GloVe\n[26], FastText [6], and Word2Vec [22] struggle to extract\ncontext-aware features. To address these shortcomings,\nour research employs transformer-based language mod-\nels. These models excel in extracting contextual text\nfeatures, thus overcoming the limitations associated\nwith traditional classification models. This shift is cru-\ncial for advancing the eff ectiveness of cyberbullying\nidentification in the Bengali language.\nIn conclusion, to encapsulate the findings of the\nresearch, this study centers around the following\nResearch Questions (RQs):\n• RQ1: How to develop an intelligent cyberbul-\nlying text identification model for low-resource\nlanguage?\n• RQ2: How can extract the context-aware text fea-\ntures and overcome the limitations of statistical,\nconvolutional, and sequential cyberbullying text\nclassification models?\nThe noteworthy contributions of this research and\npotential answers to the Research Questions (ARQs) are\noutlined as follows:\n• ARQ1: We develop an intelligent framework for\nidentifying cyberbullying text. This framework\nsystematically gathers a cyberbullying text cor-\npus, extracts features from the text, and ulti-\nmately builds the model for textual cyberbullying\nidentification (Section 3).\n• ARQ2: We implement the transformer-based lan-\nguage models which capture context-aware tex-\ntual features during the text-to-feature extraction\nphase and fine-tune the transformer-based lan-\nguage model using the cyberbullying corpus. The\nfine-tuned model overcomes the limitations of\nstatistical, convolutional, and sequential models\n(Section 3.2).\n• Constructed a cyberbullying text identification\ncorpus comprising 34,433 labeled texts. Within\nthis corpus, 17,901 are categorized as “Bully”,\nand 16,521 are labeled as “Not-Bully”. The\ncollection process involved manual gathering\nfrom social media, followed by annotation and\nverification tasks using a manual annotation\napproach (Section 3.1).\n• We trained a total of 12 cyberbullying text\nidentification models, employing a diverse range\nof methodologies. This includes three statistical\nmodels (SVM, Libsvm, SGD), four deep learning\nmodels (CNN, LSTM, VDCNN, GRU), and six\ntransformer-based models (BanglaBERT, mBERT,\nDistilBERT, IndicBERT, XML-RoBERTa, bELEC-\nTRA). Through empirical analysis, we identify\nthe top-performing model to detect cyberbullying\ntexts (Section 3.2).\n2. Background\nThe continually evolving landscape of online platforms,\nincluding Twitter, Facebook, Reddit, and others, has\ninstigated extensive research into the identification\nand categorization of undesirable texts in recent\nyears. This research spans diverse domains, addressing\naggression classification [23], hate speech detection\n[11], abuse detection [24], toxicity classification [18],\nmisogyny classification [21], trolling identification\n[8], cyberbullying detection [25], and off ensive text\nclassification [25]. While a substantial body of research\nhas been dedicated to various languages, with a\npredominant focus on English, this work provides a\nconcise summary of studies addressing violence, hate,\noffense detection/classification, and related topics in\nboth non-Bengali and Bengali languages. The section\nincludes an overview of studies conducted in English,\n2\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nCyberbullying Text Identification: A Deep Learning and Transformer-based Language Modeling Approach\nHindi, Arabic, and other languages. Additionally,\nkumar et al. [19] presents an aggressive language\nidentification dataset featuring three categories covert,\nand non-aggressive with annotations in both English\nand Hindi, encompassing 15,000 posts/comments on\naggression.\nAroyehun et al. developed deep neural network-\nbased English models employing data augmentation\nand a pseudo-labeling method. Employing LSTM and\nCNN-LSTM approaches, their system achieved macro\nF1-scores of 0.64 and 0.59, respectively. In another\nstudy, [28] utilized the TRAC-2 [20] dataset and\na bootstrap aggregating-based ensemble with fine-\ntuned BERT models to detect violence and misogyny.\nThey attained an 80.3% weighted F1 score on the\ntest set of English social media posts. [30] compiled\nthe Offensive Language Identification Dataset (OLID)\nconsisting of 14,000 English tweets. They established\na three-layer hierarchical annotation schema to detect,\nclassify, and identify the targets of texts, utilizing\nSVM, BiLSTM, and CNN for baseline evaluation.\nCNN outperformed competitors in all three levels,\nachieving macro F1 values of 0.80, 0.69, and 0.47.\nFurthermore, Founta et al. [10] provided a dataset\ncomprising 80,000 tweets categorized into hateful,\nabusive, spam, and normal classes. They employed\na comprehensive methodology to address ambiguous\ncategories. Additionally, Davidson et al. [7] created a\ndataset of 25,000 tweets categorized into hate, off ense,\nand neither. The best macro F1-score of 0.90 was\nachieved using logistic regression with TF-IDF and n-\ngram features.\nPrevious research predominantly focused on high-\nresource languages, employing transformer-based lan-\nguage models with extensive gold-standard bully iden-\ntification corpora. However, limited attention has been\ngiven to low-resource languages like Bengali and Hindi.\nResearch in low-resource bully identification often\nrelies on non-contextual embedding models, such as\nGloVe, FastText, and Word2Vec. These embeddings face\nchallenges in overcoming Out-of-Vocabulary (OOV)\nissues and extracting local and global contextual fea-\ntures specific to low-resource languages. In response to\nthese challenges, this study introduces the BullyFilter-\nNeT system. The system systematically develops a Ben-\ngali bully identification corpus and empirically evalu-\nates various statistical, deep learning, and transformer-\nbased language models. Ultimately, the top-performing\nmodel is selected to address the specific requirements\nof Bengali bully identification.\n3. Methodology\nAnswer to RQ1 and the primary objective of this study\nis to develop an intelligent Bengali cyberbullying text\nidentification system called BullyFilterNeT which can\nintelligently distinguish between pieces of text as either\ncontaining bullying content or not. To fulfill this objec-\ntive, the research progresses through three steps: (i)\nCyberbullying Corpus Development (ii) Cyberbullying\nText Identification Models Development (iii) Cyberbul-\nlying Text Identification Models Verification and Selec-\ntion. The abstract view is presented in Figure 1. The\nsubsequent subsections elaborate on each of these steps.\n3.1. Cyberbullying Corpus Development\nIn this research, we have collected the bully and not\nbully-related texts corpus built in six steps. The overall\nprocedure is presented in Figure 2.\nEach of the steps is described in the following.\nData Source Selection. In the realm of data source selec-\ntion, particularly for social media and blogs, the choice\nof platforms plays a pivotal role in shaping the nature\nand quality of the data acquired. Social media platforms\nlike Twitter, Facebook, and Instagram provide real-\ntime and diverse user-generated content, making them\nvaluable sources for understanding public opinions,\ntrends, and sentiments. The informal and conversa-\ntional nature of social media content can off er insights\ninto current events and user interactions. Similarly,\nblogs, with their more extended and often reflective\nnarratives, contribute to a deeper understanding of\nindividual perspectives and experiences. However, the\nselection process should consider the specific objectives\nof the research, the target audience, and the potential\nbiases inherent in each platform. Striking a balance\nbetween the immediacy of social media and the depth\nof blogs is essential for obtaining a comprehensive and\nrepresentative dataset for various analyses and applica-\ntions.\nManually Collection. Manual data collection from social\nmedia and blogs involves a meticulous and hands-\non approach to gathering information directly from\nthese online platforms. Researchers or data collectors\nnavigate through social media channels such as Twitter,\nFacebook, and blogs, identifying relevant content based\non predefined criteria. This method allows for a more\ntargeted selection of data, ensuring that specific themes,\nsentiments, or user interactions are captured. The\nmanual collection enables the inclusion of context-rich\ncontent that automated tools might overlook, such as\nnuanced expressions, subtleties, or cultural references.\nHowever, this process is resource-intensive and time-\nconsuming, as it requires human reviewers to sift\nthrough vast amounts of data. Additionally, ethical\nconsiderations, such as user privacy and consent, must\nbe carefully addressed when manually collecting data\nfrom social media and blogs. Despite its challenges,\nmanual data collection remains valuable for its ability\nto provide a nuanced understanding of online content,\n3\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\nFigure 1. Abstract view of BullyFilterNeT\nFigure 2. Abstract view of cyberbully corpus development\nmaking it a preferred approach for certain research\nobjectives.\nData Preprocessing. The Bengali full stop is replaced\nwith a newline character. Various forms of whitespace\n(including non-standard ones) are identified using a\nregex pattern. These are replaced with a single space\nto standardize the text. The text is then cleaned of\nthese punctuations using regex substitutions. The text\nis filtered to retain only Bengali characters and spaces.\nIt achieves this by keeping characters with Unicode\nvalues greater than the letter ’z’ (which effectively filters\nout Latin characters) and spaces. Consecutive spaces\nare reduced to a single space. Finally, the cleaned and\nprocessed text is returned. The preprocessing function\nis to be created specifically to cleanse Bengali text\ndata, rendering it more appropriate for later analysis or\ntraining of models.\nData Annotation. Taking into consideration the episte-\nmological issues highlighted by Ross et al. [29], our\nresearch employed a tiered approach in the selection\nof annotators. Two annotators consisted of individuals\nwith diverse academic backgrounds, including under-\ngraduate and postgraduate students. The study estab-\nlished a comprehensive annotation framework to inter-\npret textual content, following rigorous methodological\nstandards. Annotators applied labels like ’Bully’ and\n’NotBully’ based on semantic and pragmatic analysis.\nTable 1 presents the demographic categorization and\nepistemic stances of annotators. These measures ensure\nreliability, and effectiveness, and minimize errors. The\nprocess involved two annotators for intersubjective\nvalidity.\nData Annotation Verification. Data annotation verifica-\ntion, a crucial step in ensuring the quality and accuracy\nof labeled datasets, involves the expertise of linguistics\nprofessionals who assess and validate annotations based\non the opinions of two annotators. In this process, two\nindividuals independently annotate the data, and their\nannotations are then reviewed by a linguistics expert.\nThe linguist examines the annotations for consistency,\ncoherence, and adherence to predefined guidelines. Any\ndiscrepancies or disagreements between the two anno-\ntators are carefully scrutinized, and the linguist, draw-\ning on their linguistic expertise, resolves ambiguities\nand refines the annotations to maintain a high standard\nof quality. This meticulous verification process helps\nenhance the reliability of annotated datasets, particu-\nlarly in linguistically nuanced tasks, ensuring that the\nlabeled data accurately reflects the intended linguistic\nfeatures or patterns.\nData Annotation Quality Measures. The statistical metric\ndenoted as [63] is utilized to assess the level of\nagreement in annotations, as represented by the below\nequation,\nk= p0 −pe\n1 −pe\n(1)\nThe sign p(o) is used to represent the observed\nand hypothetical probability of annotative agreement,\ndenoting them as separate entities. The Kappa coef-\nficient, which measures inter-annotator agreement,\nyielded a score of 0.87 for the ’Bully’ category. This\nindicates that the lexico-semantic elements associated\nwith this category are notably unambiguous, facilitat-\ning a high level of agreement across annotators. In\n4\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nCyberbullying Text Identification: A Deep Learning and Transformer-based Language Modeling Approach\nTable 1. Demographic Categorization and Epistemic Stances of Annotators\nAnnotator Academic\nLevel\nResearch\nArea\nExperience Gender Viewed\nCyberbul-\nlying\nTargeted\nby Cyber-\nbullying\nAN-1 Undergraduate NLP 1 year Male Yes No\nAN-2 Undergraduate NLP 2 years Female Yes Yes\nExpert Senior NLP,\nEthics\n10 years Male Yes Yes\ncontrast, the category labeled ’NotBully’ demonstrated\na Kappa score of 0.73, suggesting a greater degree of\nintricacy and the possibility of variations in annotation.\nTable 2 shows the Kappa Scores for Annotation Classes\nin ’CyberBullyDetect’.\nTable 2. Kappa Scores for Annotation Classes in ’CyberBully-\nDetect’\nAnnotation Category Kappa Score\nBully 0.87\nNotBully 0.73\nMean Kappa Score 0.75\nCorpus Statistics Following the completion of the\ncrucial six steps, this study successfully establishes\na Cyberbullying identification corpus. The statistics\nsummarizing the corpus are detailed in Table 3.\nThis table provides a comprehensive overview of a\nBengali corpus categorized into “Bully” and “Not-\nbully” classes. The corpus consists of a total of 34,422\nsamples, with 24,108 samples allocated for training and\n10,314 for testing. In the training set, there are 12,543\nsamples labeled as \"Bully\" and 11,565 samples labeled\nas \"Not-bully.\" The testing set comprises 5,358 \"Bully\"\nsamples and 4,956 \"Not-bully\" samples. These statistics\noffer a clear distribution of the dataset, indicating\nthe balance or imbalance between the two classes\nand the total number of samples available for model\ntraining and evaluation. Such information is essential\nfor understanding the characteristics of the dataset and\nguiding the development and assessment of models for\nBengali cyberbullying detection.\n3.2. Development, Verification, and Selection of the\nCyberbullying Text Identification Model\nThe primary goal of this study is to create a text\nidentification system for low-resource cyberbullying.\nTo achieve this objective, the research is structured\ninto three main steps: (i) Text-to-Feature extraction,\n(ii) Development of statistical, deep learning, and\ntransformer-based language models, and (iii) Evalua-\ntion of models and selection of the top-performing\nmodel. The overall methodological procedure is pre-\nsented in Figure 3. In the following subsequent subsec-\ntions, each of the steps is described.\nText-to-Feature extraction. In this study, we have\nemployed two types of feature extraction methods, i.e.,\n(1) Non-contextual and (2) Contextual.\n(1) Non-contextual The three non-contextual embed-\nding models are used for Bengali text-to-feature extrac-\ntion purposes, i.e., GloVe [26], FastText [6], and\nWord2Vec [22]. In the realm of Bengali cyberbully-\ning text analysis, the process of text-to-feature extrac-\ntion plays a crucial role in uncovering meaningful\npatterns and representations. This research leverages\nthree prominent word embedding techniques, namely\nGloVe, FastText, and Word2Vec, to extract informa-\ntive features from Bengali cyberbullying text. GloVe\ncaptures global word co-occurrence statistics, FastText\nconsiders sub-word information, and Word2Vec models\nword embeddings based on contextual similarity. By\nemploying these techniques, the study aims to harness\nthe unique linguistic nuances of Bengali cyberbully-\ning instances, enabling a more nuanced understanding\nof the language-specific characteristics associated with\nsuch content. The comparative analysis of these word\nembedding methods contributes to the development of\na robust text-to-feature extraction pipeline tailored for\nBengali cyberbullying detection.\n(2) Contextual In answer to the research question\nRQ2 this study deployed contextual feature extrac-\ntors, such as BanglaBERT, XML-RoBERTa, IndicBERT,\nand ELECTRA, which are advanced language models\ndesigned to capture contextual information from text\nin the Bengali language. These models belong to the\nfamily of transformer-based architectures, which have\ndemonstrated remarkable success in natural language\nprocessing tasks. BanglaBERT is specifically tailored for\nBengali, off ering contextualized embeddings by con-\nsidering the unique linguistic intricacies of the lan-\nguage. XML-RoBERTa extends this idea, emphasizing\nthe importance of contextual embeddings in handling\ncomplex structures and multiple languages. IndicBERT,\ndesigned for various Indic languages including Bengali,\n5\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\nTable 3. Summary of corpus\nAttributes Values\nTotal samples 34,422\nTotal Training Samples 24,108\nBully Training samples 12,543\nNot-bully Training Samples 11,565\nTotal Test Samples 10,314\nBully Testing Samples 5,358\nNot-bully Testing Samples 4,956\nFigure 3. Abstract view of cyberbullying text identification models development and top-performing models selection\nfocuses on contextualized representations for improved\nlanguage understanding.\nELCTRA (Efficiently Learning an Encoder that Clas-\nsifies Token Replacements Accurately) is an approach\nthat introduces an adversarial training strategy to pre-\ntrain transformer-based models, enhancing their effi-\nciency in handling contextual features. These con-\ntextual feature extractors contribute significantly to\ntasks like sentiment analysis, named entity recognition,\nand cyberbullying detection in Bengali text, as they\nempower models to comprehend the nuanced contex-\ntual meanings within the language. Their incorpora-\ntion in natural language processing pipelines enriches\nthe representation of Bengali text, facilitating more\naccurate and context-aware language understanding in\nvarious applications.\nDevelopment of Statistical, Deep Learning, and Trans-\nformer-based Language Models. In this study, we have\nemployed two statistical, four deep learning, and four\ntransformer-based language models are verify the Ben-\ngali Cyberbullying text identification system.\n(1) Statistical Models This study explores the eff ec-\ntiveness of three machine learning (ML) techniques:\nGPU-based Support Vector Machine (SVM), GPU-based\nLibsvm and Stochastic Gradient Descent (SGD) for Ben-\ngali for Cyberbullies text [13]. The ML-based classifier\n6\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nCyberbullying Text Identification: A Deep Learning and Transformer-based Language Modeling Approach\nmodels are developed and tuned on a created corpus.\nSVM and Libsvm share similar parameters, including a\nsigmoid kernel, tol of 0.00001, and a decision function\nshape of ’over,’ with Libsvm demonstrating faster per-\nformance. The SGD classifier employs parameters loss =\nmodified_huber and alpha = 0.001, with the remaining\nparameters set as defaults.\n(2) Deep Learning-based Models CNN: Employing\na single-layer, multi-kernel Convolutional Neural\nNetwork (CNN) architecture, this model investigates\nBengali Cyberbullies text performance with three\nembedding models (FastText, GloVe, & Word2Vec).\nThe distinct kernels are set to 3, 4, and 5, with\ncorresponding filter numbers of 128, 128, and 256.\nFollowing the convolution layer, there is a 1D max-\npool layer and activation layers [14]. Subsequently, the\npooled features are concatenated and subjected to a\ndropout operation with a threshold value of 0.3.\nVDCNN: Introducing the Variable-Size Deep Con-\nvolutional Neural Network (VDCNN) architecture for\nBengali for Cyberbullies text identification purpose\n[16]. Unlike the original VDCNN, which operates\non character-level embeddings, this adaptation com-\nbines VDCNN with diff erent embedding techniques\nto enhance Bengali text classification performance. By\nreducing certain convolution operations, it addresses\ntraining time and model overfitting issues encountered\nby the original VDCNN.\nLSTM: In this study, a two-layer LSTM is utilized\nwith the following parameters: max sequence length\n= 256, hidden dimensions = 128, 256, batch size =\n12, dropout rates = 0.50, 0.40, loss function = cate-\ngorical_crossentropy, optimizer = adam, and activation\nfunction = softmax [2]. The model is trained for a\nmaximum of 50 epochs on the developed corpus. It\nis noted that an increased number of sequences nega-\ntively impacts classification performance. Additionally,\nexperiments are conducted with max sequence lengths\nof 1024 and 2048 in this research.\nGRU: The two-layer GRU model is configured\nwith the following parameters: hidden states = 128,\n128, max sequence length = 512, batch size = 32,\nepochs = 80, dropout rates = 0.30, 0.25, loss function\n= categorical_crossentropy, optimizer = adam, and\nactivation functions = tanh, softmax [2]. The last GRU\nlayer is followed by a 1D max-pool layer. Subsequently,\nthe 512 feature values are concatenated for the softmax\nlayer, responsible for generating predictions in the\nexpected category.\n(3) Transformer-based Language Models The train-\ning module for Transformer-based Language Mod-\nels, including mBERT, bELECTRA, XML-RoBERTa,\nIndicBERT, DistilBERT, and BanglaBERT, involves\ninitial pre-training on a large multilingual corpus\nto learn contextualized representations [15]. Subse-\nquently, these models undergo task-specific hyperpa-\nrameters adaption, text-to-feature extraction, and fine-\ntuning to the intricacies of Bengali cyberbullying text\nidentification.\nHyperparameters Adaption: Due to shortage of\ntraining samples, we have adapted the transformer-\nbased language models using Eq. 2\nHO\ni = FHPO (HI\ni)),i = mBERT,...,BanglaBERT (2)\nhere HO\ni represents the optimised hyperparameters\nand HI\ni represents the initial hyperparameters of\ntransformer-based language models, i.e., mBERT to\nBanglaBERT. The function FHPO (.) indicates the hyper-\nparameters adaption function which adapted the maxi-\nmum sequence length and batch size.\nText-to-feature Extraction: The transformer-based\nlanguage models are extracted the linguistic features\nusing Eq. 3.\nTF\nj = FTFE (Tji),j = 1,..,N (3)\nhere TF\nj represent the extracted features of jth sample\nTji using the ith model. FTFE(.) indicate the feature\nextraction function using model i.\nFine-tuning: Fine-tuning is a crucial process in the\ncontext of machine learning, especially when working\nwith pre-trained language models like transformer-\nbased models. It involves adjusting the parameters of\na pre-trained model on a specific task or domain to\nenhance its performance. Fine-tuning is particularly\nvaluable when the available labeled data for a specific\ntask is limited. Fine-tuning allows the model to adapt\nto the specific characteristics and nuances of the target\ntask or dataset, improving its ability to make accurate\npredictions. The fine-tuning process typically involves\nfeeding the pre-trained model with task-specific data,\nand updating its weights based on the gradients\ncomputed during the training process. This helps the\nmodel to specialize in the target task while retaining\nthe knowledge gained during pre-training on a large\ncorpus. These transformer-based language models are\nfine-tuned using the Eq. 4.\nθfine-tuned = arg min\nθ\nLtask-specific(θ) + λ\nX\ni\n∥θi −θpre-trained,i∥2\n(4)\nhere θfine-tuned represents the fine-tuned model parame-\nters, Ltask-specific(θ) s the task-specific loss function,θi −\nθpre-trained denote the parameters of the fine-tuned and\n7\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\npre-trained models, respectively. The λis a regulariza-\ntion hyperparameter that controls the balance between\nthe task-specific loss and the regularization term. This\nequation captures the fine-tuning process where the\nmodel is optimized for a task-specific objective while\nleveraging knowledge gained from pre-training on a\nlarge corpus. Regularization helps prevent overfitting\nduring the fine-tuning process.\nEvaluation of Models and Selection of the Top-perform-\ning Model. In this research, we conducted a compre-\nhensive evaluation of the test set, comprising 10,314\ntext samples, to assess the performance of various\nmodels in the context of Bengali cyberbullying text\nidentification. Three statistical models, namely SVM,\nSGD, and Libsvm, were employed, along with four\ndeep learning models, including CNN, VDCNN, LSTM,\nand GRU. Additionally, six state-of-the-art transformer-\nbased language models, mBERT, bELECTRA, XML-\nRoBERTa, IndicBERT, DistilBERT, and BanglaBERT,\nwere included in the evaluation. The performance met-\nrics, encompassing accuracy and F1-score, for all mod-\nels, are summarized in Table 4. Notably, BanglaBERT\nemerged as the top-performing model, achieving the\nhighest accuracy and F1 score among the evaluated\nmodels. This underscores the efficacy of transformer-\nbased models, particularly BanglaBERT, in cyberbully-\ning text identification in the Bengali language within\nthe scope of this study.\n4. Experimental Results and Discussions\nWithin this section, we first provide an overview of\nthe experimental setup and the chosen evaluation\nmeasures. Subsequently, we delve into the presentation\nand discussion of the results.\n4.1. Experimental Setup and Evaluation Measures\nThe models were deployed on the Google Colaboratory\nplatform with Python 3 and a Google Cloud Engine\nbackend with GPU capability. This study’s computing\nresources included 12.5GB of RAM and 64GB of disk\nspace. The dataset was analyzed with Python’s Pandas\n(version 1.1.4) and NumPy (version 1.18.5) libraries.\nThe Scikit-Learn package (version 0.22.2) was used to\ncreate traditional machine learning models, while Keras\n(version 2.4.0) and TensorFlow (version 2.3.0) were\nused to create deep learning models. The ktrain library\n(version 0.25) was used for models using Transformer\narchitectures. The dataset was partitioned into three\nsets: training, validation, and test. The training set\nsupported the models’ learning phase, whereas the\nvalidation set aided in hyperparameter tuning.\nThe final evaluation was carried out on an unknown\ntest set using a variety of statistical measures, as\nspecified in the following equations: Precision (p):\nIt quantifies the proportion of true positive samples\nwithin the samples classified as positive.\nAccuracy = NumberofCorrectPredictions\nNumberofSamples ×100 (5)\nPrecision(p ) = Truepositive\nTruepositive + Falsepositive (6)\nRecall (r): It calculates the ratio of correctly labeled\npositive samples to total positive samples.\nr = Truepositive\nTruepositive + FalseNegative (7)\nError Rate (e): This is the percentage of misclassified\nsamples.\ne= FalsePositive + FalseNegative\nNumberofSamples (8)\nWeighted F1-Score: The F1-score is a harmonic mean\nof Precision and Recall. Because of the dataset’s\nimbalance, a weighted F1-score is produced as follows:\nF1 = (Truepositive )\n(Truepositive + 1/2(FalsePositive + FalseNegative ))\n(9)\nThis section will also measure the weighted average\n(WA) and macro average (MA) precision, recall, and F1\nscore. provide a full overview of the findings produced\nby the various models, with the weighted F1 score\nserving as the key criterion of evaluation. This part will\nalso include a comparison with existing methodologies,\noffering insight into the benefits and drawbacks of the\nproposed paradigm.\n4.2. Result Analysis\nIn this section, we have evaluated the Bengali\ncyberbullying text identification models with the test\ndataset. Table 4 presents the performance metrics\nof various models for Bengali cyberbullying text\nidentification based on a test dataset. The models are\nevaluated in terms of accuracy and F1 score, providing\ninsights into their classification capabilities. Notably,\ntraditional models such as GloVe combined with SVM,\nSGD, or Libsvm exhibit reasonable accuracy, ranging\nfrom 76.20% to 78.39%, with corresponding F1 scores\nin the 76.00-79.00 range. Moving to neural network-\nbased architectures, GloVe combined with CNN,\nVDCNN, LSTM, and GRU achieve higher accuracy, with\nscores ranging from 79.61% to 84.36%, and F1-scores in\nthe 80.00-84.00 range.\nThe performance further improves with the inte-\ngration of transformer-based models. mBERT, XML-\nRoBERTa, IndicBERT, and DistilBERT consistently\n8\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nCyberbullying Text Identification: A Deep Learning and Transformer-based Language Modeling Approach\nModels Accuracy (%) F1-score Precision Recall\nGloVe+SVM 77.73 77.35 76. 64 77.97\nGloVe+SGD 76.48 75.00 74.70 75.35\nGloVe+Libsvm 78.93 78.71 78.36 79.11\nFastText+Libsvm 76.87 75.82 75.03 76.53\nWord2Vec+Libsvm 76.20 74.84 73.40 76.18\nGloVe+CNN 84.36 83.03 82. 79 83.83\nGloVe+VDCNN 82.47 81.88 80. 61 82.96\nGloVe+LSTM 81.30 80.10 79. 47 80.88\nGloVe+GRU 79.61 79.25 78. 25 79.87\nmBERT 86.47 86.21 86. 12 86.22\nbELECTRA 85.25 85.12 84.87 85.25\nXML-RoBERTa 87.13 86.62 86.62 87.34\nIndicBERT 86.75 87.16 86.69 87.45\nDistilBERT 85.65 85.96 85.87 86.01\nBanglaBERT 88.04 87.85 85.80 90.0\nTable 4. Accuracy and F1-score of cyberbullying text identification system based on 10,314 test dataset\ndemonstrate superior accuracy, ranging from 85.65%\nto 87.13%, and F1-scores in the 86.00-87.00 range.\nNotably, BanglaBERT outperforms all other models,\nachieving the highest accuracy of 88.04% and an F1-\nscore of 88.00%. This underscores the eff ectiveness of\ntransformer-based models, particularly BanglaBERT, in\naccurately identifying cyberbullying text in Bengali,\nshowcasing their robust performance on the given test\ndataset.\nBanglaBERT achieves maximum performance in\nBengali cyberbullying text identification due to its\ntailored design for the Bengali language, extensive\npre-training on a large corpus, and the ability to\ngenerate contextual embeddings. The model’s fine-\ntuning process involves eff ective parameter tuning,\noptimizing its performance for the specific task.\nLeveraging transfer learning, BanglaBERT capitalizes\non its general language understanding capabilities,\nadapting them to the nuances of cyberbullying\nidentification in Bengali. These factors collectively\ncontribute to BanglaBERT’s superior accuracy, making\nit highly eff ective in discerning and classifying\ncyberbullying content in the given context.\nBased on Table 4 performance, the maximum accu-\nracy of Bengali cyberbullying text identification perfor-\nmance has been obtained from the transformer-based\nBanglaBERT models. The details of the BanglaBERT\nmodel’s performance are presented in Table5.\nThe table presents a detailed performance analysis\nof BanglaBERT in the context of cyberbullying text\nidentification, categorizing results into “Bully” and\n“Not-bully” classes. Precision (p), recall (r), macro\naverage percentage (MA%), and weighted average\npercentage (WA%) are reported for both categories.\nFor the “Bully” class, the model achieves a precision\nof 90.00%, recall of 86.00%, and both macro and\nweighted average percentages of 88.00% for precision\nand recall. Similarly, for the “Not-bully” class, the\nprecision is 96.00%, recall is 90.00%, and macro and\nweighted average percentages are 88.00% for both\nprecision and recall. The support column indicates the\nnumber of instances in each class, with 5358 instances\nfor “Bully” and 4956 instances for “Not-bully”.\nThese metrics collectively demonstrate BanglaBERT’s\nstrong performance in accurately identifying both\ncyberbullying and non-bullying content, with high\nprecision, recall, and consistent average percentages\nacross both categories.\n4.3. Error Analysis\nFigure 4 presents the confusion matrix of BanglaBERT\nmodels for the 10,314 test dataset. In the context of\nthe confusion matrix for Bengali cyberbullying text\nidentification, the terms “error” and “success” can be\ninterpreted as follows:\nThe model achieved success in correctly identifying\ninstances of cyberbullying (0 for Bully) with a count of\n4462. These are instances where the model’s prediction\naligns with the actual presence of cyberbullying\ncontent. The model also demonstrated success in\naccurately identifying instances of non-cyberbullying\ncontent (1 for Not-bully) with a count of 4619. These\nare instances where the model correctly recognized and\nclassified content as non-off ensive. The model made\nan error in 739 instances by incorrectly predicting\ncyberbullying (0 for Bully) when the content was, in\nfact, not offensive. These are instances of false alarms\nor instances where the model may have been overly\nsensitive. The model made an error in 494 instances\nby failing to identify instances of cyberbullying\nwhen the content was off ensive. These are instances\nwhere the model missed detecting actual instances of\n9\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\nCategory p(%) r(%) MA% (p) MA% (r) WA% (p) WA% (r) Support\nBully 90.00 86.00 88.00 88.00 88.00 88.00 5358\nNot-\nbully\n96.00 90.00 88.00 88.00 88.00 88.00 4956\nTable 5. Statistical summary of BanglaBERT model based on 10,314 test dataset\nMethod Accuracy(%)\nGloVe+Libsvm [15] 78.39\nGloVe+CNN [13] 84.36\nGloVe+LSTM [2] 81.30\nGloVe+VDCNN [16] 82.47\nIndicBERT [12] 86.75\nProposed(BanglaBERT) 88.04\nTable 6. Accuracy comparison of proposed BanglaBERT with existing methods\nFigure 4. Confusion matrix of BanglaBERT model for 10,314 text\ndataset\ncyberbullying. Understanding these success and error\ncategories provides valuable insights into the model’s\nstrengths and weaknesses in diff erentiating between\ncyberbullying and non-cyberbullying content.\nComparison with Existing Research. In the absence of a\nstandardized Bengali cyberbullying corpus and estab-\nlished standardization practices, this study employed\nexisting methods along with their associated hyperpa-\nrameters. The research involved training and validating\nthe test set, and the summarized performance is pre-\nsented in Table 6.\nVarious techniques, including GloVe combined with\nLibsvm [15], GloVe with CNN [13], GloVe with LSTM\n[2], and GloVe with VDCNN [16], have been previously\nemployed for Bengali cyberbullying text identification.\nAdditionally, IndicBERT [12] represents a transformer-\nbased language model specifically designed for the\nBengali language. The proposed model, BanglaBERT,\noutperforms all these methods, achieving the highest\naccuracy at 88.04%. This comparison underscores the\nsuperior performance of BanglaBERT in the specific\ntask of cyberbullying text identification in Bengali,\ndemonstrating its efficacy in surpassing existing\nmethods.\nOverall, our evaluation underscores the superiority\nof transformer-based models, particularly BanglaBERT,\nin accurately identifying cyberbullying text in Bengali.\nBanglaBERT’s tailored design, extensive pre-training,\nand fine-tuning contribute to its exceptional perfor-\nmance, surpassing traditional and other transformer-\nbased models. The model exhibits robust precision and\nrecall, as demonstrated by the confusion matrix anal-\nysis. Comparative assessment with existing methods\nfurther solidifies BanglaBERT’s position as a leading\nsolution for promoting a safer digital environment in\nthe Bengali language. The confusion matrix also pro-\nvides valuable insights into specific cases where models\neither succeeded or failed in correctly classifying the\ngiven inputs in cyberbullying detection.\n5. Conclusion\nThis study introduces a novel corpus comprising 34,422\nsamples, with 70% (24,108) designated for training\nand 30% (12,543) for testing. The corpus under-\ngoes evaluation employing statistical, deep learning,\nand transformer-based language models. BanglaBERT\nstands out, achieving the highest accuracy at 88.04%.\nDeep learning models utilize non-contextual embed-\ndings GloVe, FastText, and Word2Vec yet struggle\nwith Out-of-Vocabulary (OOV) issues. In contrast,\ntransformer-based language models excel in extracting\ncontextual features, mitigating OOV challenges. Statis-\ntical models fall short due to limitations in capturing\n10\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nCyberbullying Text Identification: A Deep Learning and Transformer-based Language Modeling Approach\nlocal and global word and sentence-level semantics.\nWhile deep learning models capture local semantics,\nthey lack context awareness. In summary, transformer-\nbased language models prove adept at extracting\ncontext-aware features, leading to superior accuracy.\nWhile we used Bengali datasets for experiments, this\nmodel is also applicable to other low-resource lan-\nguages such as Arabic, Tamil, etc.\nIn the future, we will collect other low-resource lan-\nguages datasets and conduct the relevant experiments.\nWe plan to further refine the large language models\n(LLMs) using the cyberbully dataset and explore the\neffects of Bengali to English translation data using the\ndeveloped model. Overall, this work opens up a promis-\ning pathway in cyberbullying research for low-resource\nlanguages.\nReferences\n[1] Abdhullah-Al-Mamun and Shahin Akhter. Social media\nbullying detection using machine learning on bangla\ntext. In 2018 10th International Conference on Electrical\nand Computer Engineering (ICECE), pages 385–388, 2018.\n[2] Sadia Afroze and Mohammed Moshiul Hoque. Sntiemd:\nSentiment specific embedding model generation and\nevaluation for a resource constraint language. In\nIntelligent Computing & Optimization, pages 242–252,\nCham, 2023. Springer International Publishing.\n[3] Md. Tofael Ahmed, Maqsudur Rahman, Shafayet Nur,\nAzm Islam, and Dipankar Das. Deployment of machine\nlearning and deep learning algorithms in detecting\ncyberbullying in bangla and romanized bangla text: A\ncomparative study. In 2021 International Conference on\nAdvances in Electrical, Computing, Communication and\nSustainable Technologies (ICAECT), pages 1–10, 2021.\n[4] Arnisha Akhter, Uzzal Kumar Acharjee, Md. Alamin\nTalukder, Md. Manowarul Islam, and Md Ashraf Uddin.\nA robust hybrid machine learning model for bengali\ncyber bullying detection in social media. Natural\nLanguage Processing Journal, 4:100027, 2023.\n[5] Sara Azmin and Kingshuk Dhar. Emotion detection from\nbangla text corpus using naïve bayes classifier. In 2019\n4th International Conference on Electrical Information and\nCommunication Technology (EICT), pages 1–5, 2019.\n[6] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. Enriching word vectors with subword\ninformation. Tran. ACL, 5:135–146, June 2017.\n[7] Thomas Davidson, Dana Warmsley, Michael Macy, and\nIngmar Weber. Automated hate speech detection and\nthe problem of offensive language. In Proceedings of the\ninternational AAAI conference on web and social media,\nvolume 11, pages 512–515, 2017.\n[8] Luis Gerardo Mojica de la Vega and Vincent Ng.\nModeling trolling in social media conversations. In\nProceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018), 2018.\n[9] Amirita Dewani, Mohsin Ali Memon, and Sania Bhatti.\nCyberbullying detection: advanced preprocessing tech-\nniques & deep learning architecture for roman urdu\ndata. Journal of Big Data, 8(1):160, December 2021.\n[10] Antigoni Founta, Constantinos Djouvas, Despoina\nChatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca\nStringhini, Athena Vakali, Michael Sirivianos, and\nNicolas Kourtellis. Large scale crowdsourcing and\ncharacterization of twitter abusive behavior. In\nProceedings of the international AAAI conference on web\nand social media, volume 12, 2018.\n[11] Lei Gao and Ruihong Huang. Detecting online hate\nspeech using context aware models. arXiv preprint\narXiv:1710.07395, 2017.\n[12] Md. Rajib Hossain and Mohammed Moshiul Hoque.\nCoberttc: Covid-19 text classification using transformer-\nbased language models. pages 179–186, Cham, 2023.\nSpringer Nature Switzerland.\n[13] Md. Rajib Hossain and Mohammed Moshiul Hoque.\nToward embedding hyperparameters optimization: Ana-\nlyzing their impacts on deep leaning-based text classifi-\ncation. In The Fourth Industrial Revolution and Beyond,\npages 501–512, Singapore, 2023. Springer Nature Singa-\npore.\n[14] Md. Rajib Hossain, Mohammed Moshiul Hoque,\nM. Ali Akber Dewan, Nazmul Siddique, Md. Nazmul\nIslam, and Iqbal H. Sarker. Authorship classification in a\nresource constraint language using convolutional neural\nnetworks. IEEE Access, 9:100319–100338, 2021.\n[15] Md. Rajib Hossain, Mohammed Moshiul Hoque, and\nNazmul Siddique. Leveraging the meta-embedding\nfor text classification in a resource-constrained lan-\nguage. Engineering Applications of Artificial Intelligence,\n124:106586, September 2023.\n[16] Md. Rajib Hossain, Mohammed Moshiul Hoque, Nazmul\nSiddique, and Iqbal H. Sarker. Bengali text document\ncategorization based on very deep convolution neural\nnetwork. Expert Systems with Applications, 184:115394,\n2021.\n[17] Md. Rajib Hossain, Mohammed Moshiul Hoque, Nazmul\nSiddique, and Iqbal H Sarker. CovTiNet: Covid text\nidentification network using attention-based positional\nembedding feature fusion. Neural Computing and\nApplications, 35(18):13503–13527, June 2023.\n[18] Mladen Karan and Jan Šnajder. Preemptive toxic\nlanguage detection in wikipedia comments using thread-\nlevel context. In Proceedings of the Third Workshop on\nAbusive Language Online, pages 129–134, 2019.\n[19] Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, and Mar-\ncos Zampieri. Benchmarking aggression identification\nin social media. In Proceedings of the first workshop on\ntrolling, aggression and cyberbullying (TRAC-2018), pages\n1–11, 2018.\n[20] Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, and\nMarcos Zampieri. Evaluating aggression identification\nin social media. In Proceedings of the second workshop on\ntrolling, aggression and cyberbullying, pages 1–5, 2020.\n[21] Todor Mihaylov, Georgi Georgiev, and Preslav Nakov.\nFinding opinion manipulation trolls in news community\nforums. In Proceedings of the nineteenth conference on\ncomputational natural language learning, pages 310–314,\n2015.\n[22] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient\nestimation of word representations in vector space.\npages 1–12, 2013.\n11\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |\nK. Saifullah et al.\n[23] Nishant Nikhil, Ramit Pahwa, Mehul Kumar Nirala, and\nRohan Khilnani. Lstms with attention for aggression\ndetection. arXiv preprint arXiv:1807.06151, 2018.\n[24] Endang Wahyu Pamungkas and Viviana Patti. Cross-\ndomain and cross-lingual abusive language detection: A\nhybrid approach with deep learning and a multilingual\nlexicon. In Proceedings of the 57th annual meeting of the\nassociation for computational linguistics: Student research\nworkshop, pages 363–370, 2019.\n[25] John Pavlopoulos, Nithum Thain, Lucas Dixon, and\nIon Androutsopoulos. Convai at semeval-2019 task\n6: Offensive language identification and categorization\nwith perspective and bert. In Proceedings of the 13th\ninternational Workshop on Semantic Evaluation, pages\n571–576, 2019.\n[26] J. Pennington, R. Socher, and C. Manning. Glove: Global\nvectors for word representation. In Proc. EMNLP, pages\n1532–1543, Doha, Qatar, 2014. ACL.\n[27] Eric Rice, Robin Petering, Harmony Rhoades, Hailey\nWinetrobe, Jeremy Goldbach, Aaron Plant, Jorge Mon-\ntoya, and Timothy Kordic. Cyberbullying perpetration\nand victimization among middle-school students. Amer-\nican journal of public health, 105(3):e66–e72, 2015.\n[28] Julian Risch and Ralf Krestel. Bagging bert models\nfor robust aggression identification. In Proceedings\nof the Second Workshop on Trolling, Aggression and\nCyberbullying, pages 55–61, 2020.\n[29] Björn Ross, Michael Rist, Guillermo Carbonell, Ben-\njamin Cabrera, Nils Kurowsky, and Michael Wojatzki.\nMeasuring the reliability of hate speech annotations:\nThe case of the european refugee crisis. arXiv preprint\narXiv:1701.08118, 2017.\n[30] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara\nRosenthal, Noura Farra, and Ritesh Kumar. Predicting\nthe type and target of off ensive posts in social media.\narXiv preprint arXiv:1902.09666, 2019.\n12\nEAI Endorsed Transactions on \nIndustrial Networks and Intelligent Systems \n| Volume 11 | Issue 1 | 2024 |",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6204307079315186
    },
    {
      "name": "Computer science",
      "score": 0.551047682762146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48526403307914734
    },
    {
      "name": "Identification (biology)",
      "score": 0.4829225242137909
    },
    {
      "name": "Deep learning",
      "score": 0.467698335647583
    },
    {
      "name": "Natural language processing",
      "score": 0.4660227298736572
    },
    {
      "name": "Engineering",
      "score": 0.1928664743900299
    },
    {
      "name": "Electrical engineering",
      "score": 0.1354723572731018
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}