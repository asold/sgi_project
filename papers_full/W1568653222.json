{
    "title": "A Factored Language Model for Prosody Dependent Speech Recognition",
    "url": "https://openalex.org/W1568653222",
    "year": 2007,
    "authors": [
        {
            "id": "https://openalex.org/A2105476041",
            "name": "Ken Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4272538594",
            "name": "Mark A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106484423",
            "name": "Jennifer S",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2065388812",
        "https://openalex.org/W2007605886",
        "https://openalex.org/W1997485640",
        "https://openalex.org/W2030664433",
        "https://openalex.org/W190621901",
        "https://openalex.org/W2118714763",
        "https://openalex.org/W1577006985",
        "https://openalex.org/W2100506586",
        "https://openalex.org/W2165487751",
        "https://openalex.org/W2166317648",
        "https://openalex.org/W2101694857",
        "https://openalex.org/W2426479676",
        "https://openalex.org/W2125428621",
        "https://openalex.org/W272647413",
        "https://openalex.org/W2044552743",
        "https://openalex.org/W1895315011",
        "https://openalex.org/W2114858359",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2116952749",
        "https://openalex.org/W2093476462",
        "https://openalex.org/W116380143",
        "https://openalex.org/W192736094",
        "https://openalex.org/W1560370201",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W1797231893"
    ],
    "abstract": "In this chapter, we proposed a novel approach that improves the robustness of prosody dependent language modeling by leveraging the dependence between prosody and syntax. In our experiments on Radio News Corpus, a factorial prosody dependent language model estimated using our proposed approach has achieved as much as 31% reduction of the joint perplexity over a prosody dependent language model estimated using the standard Maximum Likelihood approach. In recognition experiments, our approach results in a 1% improvement in word recognition accuracy, 0.7% improvement in accent recognition accuracy and 1.5% improvement in intonational phrase boundary (IPB) recognition accuracy over the baseline prosody dependent recognizer. The study in the chapter shows that",
    "full_text": "18\nA Factored Language Model for Prosody \nDependent Speech Recognition \nKen Chen, \nMark A. Hasegawa-Johnson and Jennifer S. Cole \nUniversity of Illinois at Urbana-Champaign \nU.S.A. \n 1. Introduction \nProsody refers to the suprasegmental features of natural speech (such as rhythm and \nintonation) that are used to convey linguistic and paralinguistic information (such as \nemphasis, intention, attitude, and emotion). Hu mans listening to natural prosody, as \nopposed to monotone or foreign prosody, are able to understand the content with lower \ncognitive load and higher accuracy (Hahn, 1999). In automatic speech understanding \nsystems, prosody has been previously used to disambiguate syntactically distinct sentences \nwith identical phoneme strings (Price et al., 1991), infer punctuation of a recognized text \n(Kim & Woodland, 2001), segment speech into sentences and topics (Shriberg et al., 2000), \nrecognize the dialog act labels (Taylor et al., 1997), and detect speech disfluencies (Nakatani \nand Hirschberg, 1994).  None of these applications use prosody for the purpose of \nimproving word recognition (i.e., the word recognition module in these applications does \nnot utilize any prosody information). Chen et al. (Chen et al., 2003) proposed a prosody \ndependent speech recognizer that uses prosody for the purpose of improving word \nrecognition accuracy.  In their approach, the task  of speech recognition is to find the \nsequence of word labels \u0002\nW = (w1,\u0015,wM ) that maximizes the recognition probability: \n[ ˆW]= argmax p(O |W,P)p(W,P)\n= argmax p(O |Q,H)p(Q,H |W,P)p(W,P),\n (1) \nwhere \u0002\u0002P = (p1,\u0015,pM ) is a sequence of prosody label s, one associated with each word, \n\u0002\u0002O = (o1,\u0015,oT ) is a sequence of observed acoustic feature vectors, \u0002Q = (q1,\u0015,qL ) is a sequence \nof sub-word units, typically allophones dependent on phonetic context, and \u0002H = (h1,\u0015,hL ) is \na sequence of discrete ``hi\ndden mode'' vectors describing the prosodic states of each \nallophone.  The combination [wm , pm ] is called a prosody-dependent word label, the \ncombination [ql ,hl ] is called a prosody-dependent allophone label, p(O |Q,H) is a proso dy-\ndependent acoustic model, p(Q,H |W,P) is a prosody-dependent pronunciation model, and \np(W,P) is a prosody-dependent language model.  In this framework, word and prosody are  \nconditioned on each other and are recognized at the same time. The system described in \nequation (1) has the advantage that both the acoustic model and the language model can be \npotentially improved through their dependence on prosody. \nSource: Robust Speech Recognition and Understanding, Book edited by: Michael Grimm and Kristian Kroschel,\nISBN 987-3-90213-08-0, pp.460, I-Tech, Vienna, Austria, June 2007\nOpen Access Database www.i-techonline.com\nRobust Speech Recognition and Understanding 320\nIn (Chen et\n al. 2006), the prosody variable pm  takes 8 possible values composed by 2 discrete \nprosodic variables: a variable a that marks a word as either ``a'' (pitch-accented) or ``u'' \n(pitch-unaccented), and a variable b that marks a word as ``i,m,f,o'' (phrase-initial, phrase-\nmedial, phrase-final, one-word phrase) according to its position in an intonational phrase. \nThus, in this scheme, a prosody-dependent word transcription may contain prosody-\ndependent word tokens of the form wab .  For example, the sentence ``well, what's next,'' \nuttered as two intonational phrases with two accented words, might be transcribed as \n``wellao what'sui nextaf.''\nA prosody dependent language model p(W,P) that models the joint probability distribution \nof concurrent word and prosody sequences, is different from a standard prosody \nindependent language model p(W) in the sense that not only word context but also  \nprosody context affect the prediction of the next possible word  and its prosody.  This model \nis useful in at least two respects.  First, it can be used to effectively reduce the search space \nof possible word hypotheses. (Kompe, 1997) have shown that a prosody dependent \nlanguage model can be used to speed up the word recognition process without sacrificing \naccuracy. Second, it is potentially useful in improving word recognition accuracy. Waibel \n(Waibel, 1988) reports that prosodic knowledge sources, when added to a phonetic speaker-\nindependent word hypothesizer, are able to reduce the average rank of the correct word \nhypothesis by a factor of 3. Arnfield (Arnfield, 1994) gives an example in his dissertation: the \nwords ``witch\" and ``which\", having identical acoust ic observations, can be distinguished \nprosodically (``witch\" is more likely to be accented than is ``which\" because it is a content \nw o r d  w h i l e  ` ` w h i c h \"  i s  a  f u n c t i o n  w o r d ) .   T h e  w o r d  t o  b e  p r e d i c t e d  i s  m o r e  l i k e l y  t o  b e  \n``witch\" instead of ``which\" if an accent is predicted from the current word-prosody context. \nIn the results reported by (Chen et al., 2006), a prosody dependent language model can \nsignificantly improve word recognition accuracy  over a prosody independent language \nmodel, given the same acoustic model. \nN-gram models can be conveniently used for prosody dependent language modeling. The \nn-gram probabilities are estimated from their maximum likelihood estimators (the relative \nfrequency count of the n-grams). For example, the bigram probability p(wj ,pj |wi ,pi ) (the \nprobability of observing token [wj , pj ] given token [wi , pi ]) can be estimated using the \nfollowing equation: \np(wj , pj |wi , pi )= n(wj ,pj ,wi ,pi )\nn(wi ,pi ) ,  (2) \nwhere n(⋅) is the \nnumber of the n-grams observed in the training set. Equation (2) treats \neach prosody dependent word token [wj , pj ] as a distinct unit, resulting in a recognizer that \nhas | p | times larger vocabulary size than does a standard prosody independent recognizer \n(where | p | is the number of options for tag pi ).  If any word-prosody combination can \noccur in English, the number of prosody dependent n-grams is equal to | p |\nn\n times the \nnumber of prosody independent n-grams.  In practice, the number of possible prosody \ndependent n-grams increases by far less than | p |\nn\n times, because a considerable amount of \nprosody dependent n-grams never occur in natural English.  Nevertheless, the number of \npossible prosody dependent n-grams still greatly increases as | p | increases due to the \nA Factored Language Model for Prosody Dependent Speech Recognition 321\nprosody variation induced by high level contex\ntual information and by different speaking \nstyles. Hence, robust esti mation of prosody dependent language modeling using equation \n(2) requires an increasingly large amount of prosodically labeled data which are normally \nexpensive to acquire. When the size of training text is limited, increasing | p | decreases the \ntrainability of the n-gram models and reduces the consistency between the training and test \ntext: the accuracy of the estimated probability mass functions (PMFs) decreases due to the \nprosody induced data sparseness and the number of possible unseen prosody dependent n-\ngrams increases. \nIn this chapter, we propose to improve the robustness of prosody dependent language \nmodeling by utilizing the dependence between  prosody and syntax. There is evidence \nindicating that syntax is a strong conditioning factor for prosody. For example, conjunctions \n(e.g., ``but\",``so\") occur more frequently at phrase initial positions than at phrase medial or \nfinal positions in fluent speech; content words (e.g., nouns) have much higher probability of \nbeing accented than function words (e.g., prepositions, articles). In a corpus based study, \nArnfield (Arnfield, 1994) proved empirically that although differing prosodies are possible \nfor a fixed syntax, the syntax of an utterance can be used to generate an underlying \n``baseline\" prosody regardless of actual words, semantics or context. The bigram models \ndeveloped by Arnfield were able to predict prosody from parts-of-speech with a high \naccuracy (91% for stress presence prediction).  The experiments conducted by (Hirschberg, \n1993) and (Chen et al., 2004) also indicate that parts-of-speech can predict the presence of \npitch accent with accuracies of around 82%-85% on the Radio New Corpus. \nThis chapter is organized as following: Section 2 reviews previous research on factored \nlanguage models and provides a Bayesian network view of spoken language that further \nexplains our motivation, Section 3 describes our methods for creating prosody dependent \nfactored language models, Section 4 reports our experiments on the Radio News Corpus \nand discusses results, and conclusions are given in Section 5. \n2.Background: Factored Language Models \n2.1 Previous work \nThe objective of a statistical language model is to accurately predict the next word wj  from \ncurrent history \u0002hj =[w0 ,\u0015wj−1].  In the past two decades, enormous efforts have been \nreported in the literature to find the factors in hj  that best predict wj  (Rosenfeld, 2000) \nincluding the use of syntactic and semantic information extracted from hj  (Khudanpur & \nWu, 2000; Bellegarda, 2000). Language modeling for speech recognition has been shown to \nbe a difficult task due to the many sou rces of variability existing in spoken language \nincluding disfluency, sentence fragments, dialect, and stylistic and colloquial language use. \nThe existence of these intrinsic properties of spoken language (which are quit different from \nwritten language) have forced researchers to expand the space hj  to include additional \nstreams of knowledge. \nOne example is the system proposed by Heeman and Allen (Heeman and Allen, 1999), in \nwhich the word sequences and parts-of-speech (POS) sequences are modeled jointly and \nrecognized simultaneously in a unified framework: \n[ ˜W, ˜S]= argmax p(O |W,S)p(W,S)\n≈ argmax p(O |W)p(W,S).\n  (3) \nRobust Speech Recognition and Understanding 322\nThe langua\nge model p(wj ,sj |W0, j−1,S0, j−1) can be factored into two component language  \nmodels, which makes it possible to utilize the syntactic knowledge encoded in the joint \nhistory of word and POS to improve the predictability of the next word (and its POS): \np(wj ,sj |W0, j−1,S0, j−1)\n= p(wj |W0, j−1,S0, j−1,sj )p(sj |W0, j−1,S0, j−1), (4) \nwhere sj  is the POS of wj , W0, j−1 i s the word history up to wj−1, and S0, j−1 is the POS history \nup to sj−1. Heeman used decision trees to cluster the word and POS history into equivalence \nclasses. This multi-stream POS-based language model p(W,S) achieved a 7% reduction of \nword perplexity over the single stream word-based n-gram language \nmodel p(W) and  \nimproved the prediction of word and POS simultaneously. Heeman further extended this \nmulti-stream language modeling frame work to include more knowledge sources (e.g., \nintonational phrase boundaries, speech repairs, discourse markers) and found that the inter-\ndependence among these knowledge sources can further improve the quality of the \nlanguage model for the modeling of conversational spoken language. \nIn a different context, Kirchhoff (Kirchhoff et al., 2003) applied the idea of multi-stream \nlanguage modeling to handle the morphological complexity in Arabic text, where she \nmodeled multiple streams of word featur es such as the morphological class ( mi ), patterns \n( pi ) and roots (\nri ) in place of the single stream of words.  For example, represent \nwi = (ri , pi ,mi ),\np(wi |wi−1,wi−2 )\n= p(ri ,pi ,mi |ri−1,pi−1,mi−1,ri−2 , pi−2 ,mi−2 )\n= p(ri | pi ,mi ,ri−1,pi−1,mi−1,ri−2 , pi−2 ,mi−2 )\np(pi |mi ,ri−1,pi−1,mi−1,ri−2 , pi−2 ,mi−2 )\np(mi |ri−1, pi−1,mi−1,ri−2 ,pi−2 ,mi−2 ).\n (5) \nThe three factored probability functions in equation (5) can be modeled indivi\ndually using \nn-grams or other modeling techniques.  Since each word feature has much smaller \ncardinality than the word vocabulary, and is less fractured by the nuances of morphological \nvariation, this factored language model can effectively help reduce the data sparseness in \ndialectal Arabic. \nThe language models we are proposing can be viewed as an extension to these previous \nworks. Rather than modeling POS explicitly in the language model, we propose to model \nprosody explicitly while using POS implicit ly to reduce the data sparseness induced by \nprosody. We argue that this method of modeling prosody makes the acoustic models and \nlanguage models fuse more tightly through their interaction with prosody and brings the \npotential of improving both word recognition and prosody recognition performance. \nA Factored Language Model for Prosody Dependent Speech Recognition 323\n2.2 \nA Bayesian Network View for Spoken Language \nFigure 1. A Bayesian network representi ng the complex  relationship among the acoustic \nobservation sequence ( O), word sequence ( W ), prosody sequence ( P) and syntax sequence \n( S) of an utterance. \nTo better understand our reasoning behind\n the idea of prosody dependent speech \nrecognition, we plot in Fig. 1 the complex relationship among the sequences of acoustic \nobservations O, words W , prosody P and syntax S for an arbitrary utterance in terms of a \nBayesian Netw\nork. The dependence of O over P is well defined because it is well known \nthat prosody affects the a\ncoustic realization of words in systematic ways. For example, \nunaccented vowels tend to be centralized and reduced in a function word, accented vowels \ntend to be longer and less subject to coarticulatory variation (Cho, 2001); accented \nconsonants are produced with greater closure duration (DeJong, 1995), greater linguopalatal \ncontact (Fougeron & Keating, 1997), longer voice onset time, and greater burst amplitude \n(Cole et al., 2007). Conditioning O over both P and W  brings us a framework in which \nprosody induced acousti\nc variations can be accurately modeled. The dependence of W  over \nS is well-established and has been used to build various types of POS-based language \nmodels. The dependence of P over S is supported by the experiments of Arnfield and \nothers, de\nscribed at the end of Section 1. The inter-dependence between P and W  has been \ndepicte\nd by a dashed arrow to express the fact that P can be assumed to be independent of \nW  given S with no knowledge about the pragmatic context (i.e., there is no reason to \nb\ne l i e v e  t h a t  o n e  n o u n  i s  m o r e  l i k e l y  t o  b e  a c c e n t e d  t h a n  a n y  o t h e r  g i v e n  n o  p r a g m a t i c  \ncontext). This assumption is useful in our later derivation in Section 3. \nModeling W  and P jointly in this prosody dependent framework creates a new search \nspace in which\n the candidate word sequences are weighted in terms of their conformability \nwith natural prosody. An information-theoretic analysis in (Chen & Hasegawa-Johnson, \n2004; Hasegawa-Johnson et al, 2005; Chen et al., 2006) showed that it is possible for a \nRobust Speech Recognition and Understanding 324\nprosody-dependent speech re\ncognizer to improve word recognition accuracy even if the \nacoustic model and the language model do not separately lead to improvements. Even if \nprosody does not improve the recognition of words in isolation, the likelihood of the correct \nsentence-level transcription may be improved by a language model that correctly predicts \nprosody from the word string, and an acoustic model that correctly predicts the acoustic \nobservations from the prosody. In their experiments on the Radio News Corpus (Chen et al., \n2006), as large as 11% word recognition accuracy improvement over a prosody independent \nspeech recognizer was achieved by a prosody dependent recognizer that has comparable \ntotal parameter count. \n3. Method \nIn this section, we propose an approach that creates prosody dependent factored language \nmodels by utilizing the dependence between prosody and syntax. For notational \nconvenience and clarity, we used bigram models for our derivation. The equations \npresented in this section can be easily extended to higher order n-gram models. \n3.1 Prosody Dependent Factored Language Model \nThe semi prosody dependent bigram probability (the probability of observing a word wj\ngiven the previous prosody dependent word label [wi , pi ]) can be calculated from the \nprosody independent bigram probability p(wj |wi ) using the following equation: \np(wj |wi , pi )\n= p(pi ,wj |wi )\np(pi |wi )\n= p(pi |wj ,wi )p(wj |wi )\np(pi |wi )\n≈\np(pi |si ,sj )p(si ,sj |wi ,wj )p(wj |wi )si,s j\n¦\np(pi |si ,sj )p(si ,sj |wi ,wj )p(wj |wi )si,s j¦wj¦ ,\n (6) \nwhere si  and sj  are the POS of wi  and wj  respective ly. The approximation in equation (6) \nassumes that pi  (the prosody on the previous word) is dependent on the POS context but \nindependent of the actual word context: \np(pi |si ,sj ) ≈ p(pi |si ,sj ,wi ,wj ). (7) \nSimilarly, the prosody dependent bigra m probability (the probability of observing a \nprosody dependent word token [wj , pj ] given the previous prosody dependent word token \n[wi , pi ]) can be calculated from the semi prosody dependent bigram probability \np(wj |wi , pi ):\nA Factored Language Model for Prosody Dependent Speech Recognition 325\np(wj , pj |wi , pi )\n= p(pj |wj ,wi ,pi )p(w |wi ,pi )\n= p(pj |sj ,si ,wj ,wi , pi )p(sj ,si |wj ,wi , pi )p(wj |wi ,pi )\nsi,s j\n¦\n≈ p(pj |sj ,si , pi )p(sj ,si |wj ,wi )p(wj |wi , pi ).\nsi,s j\n¦\n (8) \nThe follow\ning approximations are required in deriving equation (8): \np(pj |si ,sj , pi ) ≈ p(pj |si ,sj ,wi ,wj , pi ), (9) \nand\np(si ,sj |wi ,wj ) ≈ p(si ,sj |wi ,wj , pi ). (10) \nEquation (9) assumes that prosody is dependent on its syntactic context represented by the \nPOS of current word and the previous word but independent of the actual words. Equation \n(10) assumes that prosody does not affect the probability distribution of POS given the \nactual word context. This assumption is plausible except for the cases where prosody is used \nto resolve syntactic ambiguities (Price et al., 1991).  In this chapter, we assume that the use of \nprosody to resolve POS ambiguity is statistically rare. \nEquations (6) and (8) provide an approach to calculate the prosody dependent bigram \nprobability p(wj , pj |wi , pi ) based on the regular prosody independent bigram probability \np(wj |wi ) and three additional probability mass functions: p(pi |si ,sj ), p(pj |si ,sj , pi ), and \np(si ,sj |wi ,wj ). p(si ,sj |wi ,wj ) describes th e stochastic mapping between a word pair and \nthe associated POS pair. In most cases, this probability  is a delta function, meaning that a \nword pair can only be associated with a unique POS pair.  In a few cases, it is possible for a \nword pair to have more than one associated POS pairs.  The probability mass functions \np(pi |si ,sj ) and p(pj |si ,sj , pi ) describ e the inter-dependence between prosody and parts-\nof-speech, and can be very robustly estimated from a small database due to the small \ncardinality of the POS set and the prosody set. Note that equation (8) is possibly more \naccurate than equation (6) because the approximations are made only in the numerator \nwhile equation (6) has approximations in both numerator and denominator. \n3.2 Methods for Smoothing the Language Models\nTwo popular techniques can be used to smooth the resulting language model: the backoff \nscheme and linear interpolation. When a prosody dependent bigram can not be estimated \nfrom the training data, it can be backed off to a prosody dependent unigram using Katz's \nbackoff scheme (Katz, 1987): \npb (wj , pj |wi , pi )=\ndr p(wj , pj |wi , pi ),  if exists\nb(wi , pi )p(wj ,pj ),  else\n­ \n® ° \n¯ ° , (11) \nwhere 0< dr ≤ 1 is a \nconstant discount ratio and the backoff weight b(wi , pi ) is computed to \nensure that the bigram probabilities conditioned on [wi , pi ] sum up to 1: \nRobust Speech Recognition and Understanding 326\nb(wi , pi )= 1− p(wj , pj |wi , pi )j∈B¦\n1− p(pj |wj )j∈B¦ , (12) \nwhere B is the set of all prosody dependent word labels [wj , pj ] whose bigram probabilities \ncan be \ncalculated from equations (6) and (8). \nThe bigram probabilities calculated from equations (6) and (8) can be interpolated with the \nbigram probabilities estimated directly from the data (equation (2)).  Let pc  be the \nprobabilities calculated by equation (6) and (8), and pm  the probabilities estimated by \nequation (2), the interpolated probability pi  can be obtained using: \npi (wj ,pj |wi , pi )\n= λpc (wj , pj |wi , pi )+ (1− λ)pm (wj , pj |wi , pi ), (13) \nwhere λ is a constant weight optimized using an EM algorithm to minimize the cross \nentropy of the interpolated lan\nguage model over an independent development-test set. \n3.3 Joint Perplexity and Word Perplexity \nThe quality of a standard prosody independent language model p(W) can be measured by \nits perplexity E over a test set \u0002T =[w0 ,w1,\u0015,wN ]:\nE(T )= 2\nH(T )\n, (14) \nwhere the cross-entropy H(T) can be calculated as: \nH(T )=− 1\nN log2 p(wk |wk−1).\nk=1\nN\n¦  (15) \nSimilarly, the q\nuality of a prosody dependent language model p(W,P) can be measured by \nits perplexity Ep  \nover the test set \u0002Tp =[w0 ,p0 ,w1,p1,\u0015,wN , pN ] that contains the same  word \nsequence as T  does but is transcribed prosodically: \nEp (Tp )= 2\nHp (Tp )\n,  (16) \nwhere Hp (Tp ) can be\n calculated as: \nHp (Tp )=− 1\nN log2 p(wk ,pk |wk−1,pk−1).\nk=1\nN\n¦  (17) \nTo avoid confusion, we name E the Word Perplexity, and Ep  the Joint Perplexity.  \nObviously, E and Ep  are not dire ctly comparable because they are calculated over different \nhypothesis spaces: E is an estimate of how many possible words can appear in the next spot \ngiven current word history, while Ep  is a\nn estimate of how many possible prosody \ndependent word tokens can appear in the next spot given current word and prosody \nhistory.\nTo directly compare the quality of a prosody dependent language model with that of a \nprosody independent language model, we need to compute the word perplexity for the \nprosody dependent language model. Note that equation (15) can be expanded as \nA Factored Language Model for Prosody Dependent Speech Recognition 327\nH(T )=− 1\nN log2\np(W0,kP0,k )P0,k\n¦W0,k−1\n¦\np(W0,k−1P0,k−1)P0,k−1¦W0,k−2¦k=1\nN\n¦ , (18) \nwhere \u0002\u0002W0,k =[w0 ,w1,\u0015wk ], P0,k  includes all possibl e prosody paths that can be assigned to \nW0,k , and p(W0,kP0,k ) is calculated using the estimated prosody dependent language model. \nNote that equation (18) can be computed efficiently using the forward algorithm, one of the \nstandard algorithms for HMM. \n4. Experiments and Results \n4.1 The Corpus \nTo train prosody dependent speech recognizers, a large prosodically labeled speech \ndatabase is required. The Boston University Radio News Corpus is one of the largest \ncorpora designed for study of prosody (Ostendorf et al., 1995). The corpus consists of \nrecordings of broadcast radio news stories including original radio broadcasts and \nlaboratory broadcast simulations recorded from seven FM radio announcers (4 male, 3 \nfemale).  Radio announcers usually use more clear and consistent prosodic patterns than \nnon-professional readers, thus the Radio News Corpus comprises speech with a natural but \ncontrolled style, combining the advantages of both read speech and spontaneous speech.  In \nthis corpus, a majority of paragraphs are annotated with the orthographic transcription, \nphone alignments, part-of-speech tags and prosodic labels. The part-of-speech tags used in \nthis corpus are the same as those used in the Penn Treebank.  This tag set includes 47 parts-\nof-speech: 22 open class categories, 14 closed class categories and 11 punctuation labels. \nPart-of-speech labeling is carried out automatically using the BBN tagger. The tagger uses a \nbigram model of the tag sequence and a probability of tag given word taken from either a \ndictionary or, in the case of an unknown word, based on features of the word related to \nendings, capitalization and hyphenation.  The tagger was trained on a set of Wall Street \nJournal sentences that formed part of the Penn Treebank corpus. For the labnews stories (a \nsubset of the Radio New Corpus recorded without noise in a phonetics laboratory), only 2% \nof the words were incorrectly labeled. \nThe prosodic labeling system represents prosodic phrasing, phrasal prominence and \nboundary tones, using the Tones and Brea k Indices (ToBI) system for American English \n(Beckman & Ayers, 1994).  The ToBI system labels pitch accent tones, phrase boundary \ntones, and prosodic phrase break indices.  Br eak indices indicate the degree of decoupling \nbetween each pair of words; intonational phrase boundaries are marked by a break index of \n4 or higher.  Tone labels indicate phrase boundary tones and pitch accents. Tone labels are \nconstructed from the three basic elements H, L, and !H.  H and L represent high tone, low \ntone respectively, while !H represents a high tone produced at a pitch level that is stepped \ndown from the level of the preceding high tone. There are four primary types of intonational \nphrase boundary tones: L-L%, representing the pitch fall at the end of a declarative phrase \nor sentence; H-L%, representing a fall or plateau at a mid-level pitch such as occurs in the \nmiddle of a longer declarative dialog turn; H-H%, representing the canonical, upward pitch \ncontour at the end of a yes-no question; and L-H%, representing the low-rising contour \nfound at the end of each non-final item on a list. The contours !H-L% and !H-H% are down-\nstepped variants that may occur following a H* pitch accent and are less frequently \nobserved. Seven types of accent tones are labeled: H*, !H*, L+H*, L+!H*, L*, L*+H and \nRobust Speech Recognition and Understanding 328\nH+!H*. The To\nBI system has the advantage that it can be used consistently by labelers for a \nvariety of styles.  For example, if one allows a level of uncertainty in order to account for \ndifferences in labeling style, it can be shown that the different transcribers of the Radio \nNews Corpus agree on break index with 95% inter-transcriber agreement (Ostendorf et al., \n1995).  Presence versus absence of pitch accent is transcribed with 91% inter-transcriber \nagreement.\nIn the experiments we report in this chapter, the original ToBI labels are simplified: accents \nare only distinguished by presence versu s absence, word boundaries are distinguished by \nthose in intonational phrase-final position, and those that are medial in an intonational \nphrase. Applying this simplification, we create prosody dependent word transcriptions in \nwhich a word can only have 4 possible prosodic variations: unaccented phrase medial \n(``um''), accented phrase medial (``am''), unaccented phrase final (``uf'') and accented phrase \nfinal (``af''). \n4.2 Perplexity \nThe prosodically labeled data used in our experiments consist of 300 utterances, 24944 \nwords (about 3 hours of speech sampled at  16Khz) read by five professional announcers (3 \nfemale, 2 male) containing a vocabulary of 3777 words. Training and test sets are formed by \nrandomly selecting 85% of the utterances for training, 5% of the utterances for development \ntest and the remaining 10% for testing (2503 words). \nWe first measured the quality of the language models in terms of their perplexity on the test \nset. Four language models are trained from the same training set: a standard prosody \nindependent backoff bigram language model LPI, a prosody dependent backoff bigram \nlanguage model LPDM computed using equation (2), a prosody dependent backoff bigram \nlanguage model LPDC1 computed using equation (8) only, and a model LPDC2 computed \nusing both equation (6) and equation (8).  The difference between LPDC1 and LPDC2 is that \nin LPDC1, the semi prosody dependent bigram probabilities p(wj |wi , pi ) required by \nequation (8) are estimated directly from training data using their maximum likelihood \nestimators; whereas in LPDC2 they are computed from the prosody independent bigram \nprobabilities p(wj |wi ) using equation (6). The models LPDC1 and LPDC2 were linearly \ninterpolated with LPDM using equation (13), with interpolation weights \nλ optimized over \nthe deve\nlopment-test set.  Table I lists the results of this experiment. \nLPI LPDM LPDC1 LPDC2 \nJoint Perp.  340 282 235 \nWord Perp. 130 60 54 47 \nUnseen bigrams 931 1244 1103 956 \nTotal bigrams 12100 14461 37373 81950 \nTable 1. The joint perplexity, word perplexity, number of unseen bigrams in the test set and \ntotal number of estimate\nd bigrams in the prosody independent language model (LPI), the \nprosody dependent language model estimated using the standard ML approach (LPDM) \nand the prosody dependent language model calculated using the proposed algorithm \n(LPDC1 and LPDC2). \nA Factored Language Model for Prosody Dependent Speech Recognition 329\nCompare the \nperformance among the prosody dependent language models: LPDM, LPDC1, \nand LPDC2. Both LPDC1 and LPDC2 have much smaller joint perplexity than LPDM: the \njoint perplexity of LPDC1 is 17% less than that of LPDM, while that of LPDC2 is 31% \nsmaller. Factorial modeling increases the number of bigrams whose probabilities can be \nestimated more accurately than their backed-off unigrams: the number of total estimated \nbigrams increased by 2 and 7 times respectively in LPDC1 and LPDC2, and the number of \nunseen bigrams in the test data reduced by around 25%, approaching the number of unseen \nbigrams in LPI. \nTo compare the perplexity of the prosody dependent language models with the prosody \nindependent language model LPI, we computed the word perplexity for the prosody \ndependent language models using equation (18). As can be seen in the third row of Table 1, \nword perplexity of LPDC2 is reduced by 64% relative to LPI.  Note that the word \nperplexities of the prosody dependent language models are only weakly comparable with \nthat of the prosody independent language models in terms of predicting the word \nrecognition performance. The word recognition power of a prosody dependent language \nmodel is prominent only when it is coupled with an effective prosody dependent acoustic \nmodel. \n4.3 Word Recognition \nEncouraged by the great reduction in perplexity, we conducted word and prosody \nrecognition experiments on the same training and test sets.  Two acoustic models are used in \nthis experiment: a prosody independent acoustic model API and a prosody dependent \nacoustic model APD.  All phonemes in API and APD are modeled by HMMs consisting of 3 \nstates with no skips. Within each state, a 3 mixture Gaussi an model is used to model the \nprobability density of a 32-dimensional acou stic-phonetic feature stream consisting of 15 \nMFCCs, energy and their deltas.  The allophone models in APD contain an additional one-\ndimensional Gaussian acoustic-prosodic observation PDF which is used to model the \nprobability density of a nonlinearly-transformed pitch str eam, as described in (Chen et al, \n2004; Chen et al, 2006).  API contains monophone models adopted from the standard \nSPHINX set (Lee, 1990) and is unable to detect any prosody related acoustic effects.  APD \ncontains a set of prosody dependent allophones constructed from API by splitting the \nmonophones into allophones according to a four-way prosodic distinction (unaccented \nmedial, accented medial, unaccented final, accented final): each monophone in API has 4 \nprosody dependent allophonic variants in APD.  Allophone models in APD that are split \nfrom the same monophone share a single tied acoustic-phonetic observation PDF, but each \nallophone distinctly models the state transition probabilities and the acoustic-prosodic \nobservation PDF.  The APD allophones are therefore able to detect two of the most salient \nprosody induced acoustic effects: preboundary lengthening, and the pitch excursion over \nthe accented phonemes.  The parameter count of the acoustic-phonetic observation PDF (195 \nparameters per state) is much larger than the parameter count of the acoustic-prosodic \nobservation PDF (2 parameters per state) or the transition probabilities (1 parameter per \nstate); since the acoustic-phonetic parameters are shared by all allophones of a given \nmonophone, the total parameter count of the APD model set is only about 6% larger than \nthe parameter count of API. \nFive recognizers are tested: a standard prosody independent recognizer RII using API and \nLPI, a semi prosody independ ent recognizer RID using APD and LPI, a prosody dependent \nRobust Speech Recognition and Understanding 330\nrecognizer RD\nM using APD and LPDM, a prosody dependent recognizer RDC1 using APD \nand LPDC1, and a prosody dependent recognizer RDC2 using APD and LPDC2.  The word \nrecognition accuracy, accent recogn ition accuracy and intonational phrase boundary \nrecognition accuracy of these recognizers over the same training and test set are reported in \nTable 2. \n RII RID RDM RDC1 RDC2 \nAM API APD APD APD APD \nLM LPI LPI LPDM LPDC1 LPDC2 \nWord 75.85 76.02 77.29 78.27 77.08 \nAccent 56.07 56.07 79.59 79.71 80.26 \nIPB 84.97 84.97 85.06 85.80 86.62 \nTable 2. Percent word, accent, and intonational phrase bou ndary recognition accuracy for \nrecognizers RII, RID, RDM, RDC, and RDC2. \nOverall, the prosody dependent speech recognizers significantly improve the word \nrecognition accuracy (WRA) over the prosody independent speech recognizer. RDM \nimproved the word recognition accuracy by 1.4% over RII and 1.2% over RID.  RDC1 further \nimproved the WRA by 1% over RDM, apparently benefiting from the improved prosody \nlanguage model LPDC1. The pitch accent re cognition accuracy (ARA) and the intonational \nphrase boundary recognition accuracy (BRA) are also significantly improved.  Since RII and \nRID classify every word as unaccented and every word boundary as phrase-medial, the \nARA and BRA listed in RII and RID are the chance levels.  RDM showed a great \nimprovement in ARA but only slight improvement in BRA mostly due to the already high \nchance level 84.97%.  RDC2 used the language model LPDC2 that has the smallest \nperplexity.  However, it only achieved improvement over RDM on ARA and BRA (0.7% and \n1.5% respectively), but not on WRA. The failure of LPDC2 to outperform the WRA of \nLPDC1 may not be meaningful: it is well known that perplexity does not always correlate \nwith recognition performance.  However, it is poss ible to speculatively assign some \nmeaning to this result.  The flexible class-dependent structure of LPDC2 is able to model a \nnumber of prosody-dependent bigrams that is seven times larger than the number observed \nin the training data (Table I). It is possible that the approximations in equation (6) do not \naccurately represent the probabilities of all of these bigrams, and that therefore the increased \nflexibility harms word recognition accuracy. \n 5. Conclusion \nIn this chapter, we proposed a novel approach that improves the robustness of prosody \ndependent language modeling by leveraging the dependence between prosody and syntax.  \nIn our experiments on Radio News Corpus, a factorial prosody dependent language model \nestimated using our proposed approach has achieved as much as 31% reduction of the joint \nperplexity over a prosody dependent language model estimated using the standard \nMaximum Likelihood approach. In recognition experiments, our approach results in a 1% \nimprovement in word recognition accuracy, 0.7% improvement in accent recognition \naccuracy and 1.5% improvement in intonational phrase boundary (IPB) recognition accuracy \nover the baseline prosody dependent recognizer.  The study in the chapter shows that \nA Factored Language Model for Prosody Dependent Speech Recognition 331\nprosody-syntax dependence can be used to\n reduce the uncertainty in modeling concurrent \nword-prosody sequences. \n 6. Acknowledgment \nThis work was supported in part by NSF award number 0132900, and in part by a grant \nfrom the University of Illinois Critical Research Initiative.  Statements in this chapter reflect \nthe opinions and conclusions of the authors, and are not endorsed by the NSF or the \nUniversity of Illinois. \n7.  References \nArnfield, S. (1994). Prosody and syntax in corpus based analysis of spoken English, Ph.D. \nthesis, University of Leeds \nBeckman, M. E. and Ayers, G. M. (1994). Guidelines for ToBI Labelling: the Very \nExperimental HTML Version, http://www.ling.ohio-\nstate.edu/research/phonetics/E ToBI/singer tobi.html \nBellegarda, J. (2000). Exploiting latent semantic information in statistical language modeling, \nProceedings of the IEEE 88, 8, 1279-1296 \nChen, K., Borys, S., Hasegawa-Johnson, M., and Cole, J. (2003). Prosody dependent speech \nrecognition with explicit duration mode ling at intonational phrase boundaries, \nProc. EUROSPEECH, Geneva, Switzerland \nChen, K. and Hasegawa-Johnson, M. (2004). How prosody improves word recognition, Proc.\nISCA International Conference on Speech Prosody, Nara, Japan \nChen, K., Hasegawa-Johnson, M., and Cohen, A. 2004. An automatic prosody labeling \nsystem using ANN-based syntactic-prosodic model and GMM-based acoustic-\nprosodic model, Proc. ICASSP, Montreal, Canada \nChen, K., Hasegawa-Johnson, M., Cohen, A., Borys, S., Kim, S., Cole, J., and Choi, J. (2006). \nProsody dependent speech recognition on radio news, IEEE Trans. Speech and Audio \nProcessing 14(1): 232-245\nCho, T. 2001. Effects of prosody on articulation in English, Ph.D. thesis, UCLA \nCole, J., Kim H., Choi H., & Hasegawa-Johnson M. (2007). Prosodic effects on acoustic cues \nto stop voicing and place of articulation: Evidence from Radio News speech. Journal \nof Phonetics 35: 180-209 \nDeJong, K. (1995). The supraglottal articulation of prominence in English: Linguistic stress \nas localized hyperarticulation, J. Acoust. Soc. Am 89, 1, 369-382 \nFougeron, C. and Keating, P. (1997). Articulatory strengthening at edges of prosodic \ndomains, J. Acoust. Soc. Am 101, 6, 3728-3740 \nHahn, L. (1999). Native speakers’ reactions to non-native stress in English discourse, Ph.D. \nthesis, UIUC \nHasegawa-Johnson M., Chen K., Cole J., Borys S., Kim S., Cohen A., Zhang T., Choi J., Kim \nH., Yoon T., and Chavarria S. (2005). Simultaneous Recognition of Words and \nProsody in the Boston University Radio Speech Corpus, Speech Communication ,\n46(3-4): 418-439 \nHeeman, P. and Allen, J. (1999). Speech repairs, intonational phrases and discourse markers: \nmodeling speakers’ utterances in spoken dialog, Computational Linguistics 25, 4 \nRobust Speech Recognition and Understanding 332\nHirschberg, J. (\n1993). Pitch accent in context: Predicting intonational prominence from text, \nArtificial Intelligence 63, 1-2 \nKatz, S. M. (1987). Estimation of probabilities from sparse data for the language model \ncomponent of a speech recognizer, IEEE Trans. Speech and Audio Processing  35, 3 \n(Mar.), 400-401 \nKhudanpur, S. and Wu, J. (2000). Maximum entropy techniques for exploiting syntactic, \nsemantic and collocational dependencies in language modeling, Computer Speech \nand Language 14, 355-372 \nKim, J. H. and Woodland, P. C. (2001). The use of prosody in a combined system for \npunctuation generation and speech recognition, Proc. EUROSPEECH\nKirchhoff, K., Bilmes, J., Das, S., Duta, N., Egan, M., Jin, G., He, F., Henderson, J., Liu, D., \nNoamany, M., Schone, P., Schwartz, R., and Vergyri, D. (2003). Novel approaches \nto Arabic speech recognition: report from the 2002 Johns-Hopkins summer \nworkshop, Proc. ICASSP, Hong Kong, China \nKompe, R. (1997). Prosody in Speech Understanding Systems, Springer-Verlag \nLee, K. F. (1990). Context-dependent phonetic hidden Markov models for speaker-\nindependent continuous speech recognition, IEEE Trans. Speech and Audio Processing\n38, 4 (Apr.), 599-609 \nNakatani, C. H. and Hirschberg, J. (1994). A corpus-based study of repair cues in \nspontaneous speech, J. Acoust. Soc. Am 95, 3, 1603-1616 \nOstendorf, M., Price, P. J., and Shattuck-Hufnagel, S. (1995). The Boston University Radio \nNews Corpus, Linguistic Data Consortium \nPrice, P. J., Ostendorf, M., Shattuck-Hufnagel, S., and Fong, C. (1991). The use of prosody in \nsyntactic disambiguation, J. Acoust. Soc. Am 90, 6 (Dec.), 2956-2970 \nRosenfeld, R. (2000). Two decades of statistical language modeling: where do we go from \nhere? Proceedings of the IEEE 88, 8, 1270-1278 \nShriberg, E., Stolcke, A., Hakkani-Tur, D., and Tur, G. (2000). Prosody-based automatic \nsegmentation of speech into sentences and topics, Speech Communication  32, 1-2 \n(Sep.), 127-154 \nTaylor, P., King, S., Isard, S., Wright, H., and Kowtko, J. (1997). Using intonation to constrain \nlanguage models in speech recognition, Proc. EUROSPEECH\nWaibel, A. (1988). Prosody and Speech Recognition, London: Pitman \nRobust Speech Recognition and Understanding\nE\ndited by Michael Grimm and Kristian Kroschel\nISBN 978-3-902613-08-0\nHard cover, 460 pages\nPublisher I-Tech Education and Publishing\nPublished online 01, June, 2007\nPublished in print edition June, 2007\nInTech Europe\nUniversity Campus STeP Ri \nSlavka Krautzeka 83/A \n51000 Rijeka, Croatia \nPhone: +385 (51) 770 447 \nFax: +385 (51) 686 166\nwww.intechopen.com\nInTech China\nUnit 405, Office Block, Hotel Equatorial Shanghai \nNo.65, Yan An Road (West), Shanghai, 200040, China \nPhone: +86-21-62489820 \nF\nax: +86-21-62489821\nThis book on Robust Speech Recognition and Understanding brings together many different aspects of the\ncurrent research on automatic speech recognition and language understanding. The first four chapters\naddress the task of voice activity detection which is considered an important issue for all speech recognition\nsystems. The next chapters give several extensions to state-of-the-art HMM methods. Furthermore, a number\nof chapters particularly address the task of robust ASR under noisy conditions. Two chapters on the automatic\nrecognition of a speaker's emotional state highlight the importance of natural speech understanding and\ninterpretation in voice-driven systems. The last chapters of the book address the application of conversational\nsystems on robots, as well as the autonomous acquisition of vocalization skills.\nHow to reference\nIn order to correctly reference this scholarly work, feel free to copy and paste the following:\nKen Chen, Mark A. Hasegawa-Johnson and Jennifer S. Cole (2007). A Factored Language Model for Prosody\nDependent Speech Recognition, Robust Speech Recognition and Understanding, Michael Grimm and Kristian\nKroschel (Ed.), ISBN: 978-3-902613-08-0, InTech, Available from:\nhttp://www.intechopen.com/books/robust_speech_recognition_and_understanding/a_factored_language_mod\nel_for_prosody_dependent_speech_recognition\n\n© 2007 The Author(s). Licensee IntechOpen. This chapter is distributed under the terms of the\nCreative Commons Attribution-NonCommercial-ShareAlike-3.0 License , which permits use,\ndistribution and reproduction for non-commercial purposes, provided the original is properly cited\nand derivative works building on this content are distributed under the same license."
}