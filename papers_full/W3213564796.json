{
  "title": "APIRecX: Cross-Library API Recommendation via Pre-Trained Language Model",
  "url": "https://openalex.org/W3213564796",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2897036700",
      "name": "Yuning Kang",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2128849231",
      "name": "Zan Wang",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2103519435",
      "name": "Hong-Yu Zhang",
      "affiliations": [
        "University of Newcastle Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2125231704",
      "name": "Junjie Chen",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A3040978346",
      "name": "Hanmo You",
      "affiliations": [
        "Tianjin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W90447038",
    "https://openalex.org/W2923372094",
    "https://openalex.org/W2971207485",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W2992721737",
    "https://openalex.org/W2014577207",
    "https://openalex.org/W1970607969",
    "https://openalex.org/W2619465136",
    "https://openalex.org/W2142403498",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2899418668",
    "https://openalex.org/W2787481829",
    "https://openalex.org/W2888017562",
    "https://openalex.org/W2018389835",
    "https://openalex.org/W2798742628",
    "https://openalex.org/W2888559334",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2547212960",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W2971031524",
    "https://openalex.org/W3105398568",
    "https://openalex.org/W2921792613",
    "https://openalex.org/W2985808369",
    "https://openalex.org/W3034407863",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W4213053623",
    "https://openalex.org/W4288562606",
    "https://openalex.org/W2148190602",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2971863715",
    "https://openalex.org/W2548627465",
    "https://openalex.org/W3006367553",
    "https://openalex.org/W3005855585",
    "https://openalex.org/W3102516861",
    "https://openalex.org/W4245415816",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2757222607",
    "https://openalex.org/W2139380585",
    "https://openalex.org/W3105242324",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2965234662",
    "https://openalex.org/W2143861926"
  ],
  "abstract": "For programmers, learning the usage of APIs (Application Programming Interfaces) of a software library is important yet difficult. API recommendation tools can help developers use APIs by recommending which APIs to be used next given the APIs that have been written. Traditionally, language models such as N-gram are applied to API recommendation. However, because the software libraries keep changing and new libraries keep emerging, new APIs are common. These new APIs can be seen as OOV (out of vocabulary) words and cannot be handled well by existing API recommendation approaches due to the lack of training data. In this paper, we propose APIRecX, the first cross-library API recommendation approach, which uses BPE to split each API call in each API sequence and pre-trains a GPT based language model. It then recommends APIs by fine-tuning the pre-trained model. APIRecX can migrate the knowledge of existing libraries to a new library, and can recommend APIs that are previously regarded as OOV. We evaluate APIRecX on six libraries and the results confirm its effectiveness by comparing with two typical API recommendation approaches.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3425–3436\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3425\nAPIRecX: Cross-Library API Recommendation via Pre-Trained\nLanguage Model\nYuning Kang1, Zan Wang1, Hongyu Zhang2, Junjie Chen1∗, Hanmo You1\n1College of Intelligence of and Computing, Tianjin University, Tianjin, China\n2The University of Newcastle, Callaghan, Australia\n{kangyuning,wangzan,junjiechen,youhanmo}@tju.edu.cn\nzhang.hongyu@newcastle.edu.au\nAbstract\nFor programmers, learning the usage of APIs\n(Application Programming Interfaces) of a\nsoftware library is important yet difﬁcult. API\nrecommendation tools can help developers use\nAPIs by recommending which APIs to be\nused next given the APIs that have been writ-\nten. Traditionally, language models such as\nN-gram are applied to API recommendation.\nHowever, because the software libraries keep\nchanging and new libraries keep emerging,\nnew APIs are common. These new APIs can\nbe seen as OOV (out of vocabulary) words and\ncannot be handled well by existing API recom-\nmendation approaches due to the lack of train-\ning data. In this paper, we propose APIRecX,\nthe ﬁrst cross-library API recommendation ap-\nproach, which uses BPE to split each API call\nin each API sequence and pre-trains a GPT-\nbased language model. It then recommends\nAPIs by ﬁne-tuning the pre-trained model.\nAPIRecX can migrate the knowledge of ex-\nisting libraries to a new library, and can rec-\nommend APIs that are previously regarded as\nOOV . We evaluate APIRecX on six libraries\nand the results conﬁrm its effectiveness by\ncomparing with two typical API recommenda-\ntion approaches.\n1 Introduction\nApplication Programming Interface (API) is an in-\ntegral part of software libraries. Being familiar\nwith APIs could help improve programming pro-\nductivity. However, a library tends to contain a\nlarge number of APIs and there could be complex\ndependencies among APIs, and thus understanding\nall APIs in a library is very challenging, especially\nfor new developers. To facilitate correct and ef-\nﬁcient usage of APIs during programming, many\nAPI recommendation approaches (Zhong et al.,\n2009; Nguyen et al., 2016; Xie et al., 2019; Bruch\net al., 2009; Huang et al., 2018) have been pro-\nposed. More speciﬁcally, API recommendation\n∗ ∗Junjie Chen is the corresponding author.\naims to automatically recommend a correct API\ncall at the current programming location based on\nits preceding part of code information.\nAs an example, Listing 1 shows a Java code snip-\npet about opening a text ﬁle. Assuming a program-\nmer forgets what to write in Line 6. API recommen-\ndation tool can help the programmer by prompting\nthe most likely API call to be used next. In this\ncase, printStackTrace () will be returned. The\nAPI recommendation tools do so by learning API\nusage pattern from a large code corpus. Some tools\n(Nguyen et al., 2016; Nguyen and Nguyen, 2015)\nuse probabilistic models to learn API usage pat-\ntern , while others (Zhong et al., 2009; Wang et al.,\n2013) use data mining methods to ﬁnd API usage\npatterns . Recently, deep learning based language\nmodels are proposed to model the API sequences\nand have obtained promising results in recommend-\ning APIs (Raychev et al., 2014; Yan et al., 2018;\nWhite et al., 2015; Nguyen and Nguyen, 2015).\nHowever, the existing API recommendation\ntools only focus on improving the performance\nof API recommendation when API usage data are\nsufﬁcient (i.e., the usage data of the APIs to be\nrecommended are sufﬁcient in training data). That\nis, they mostly ignored the OOV (out of vocabu-\nlary) problem, which could have negative impact\non the performance of API recommendation. More\nspeciﬁcally, when some APIs are unseen in train-\ning data, these approaches cannot recommend them\ncorrectly. The OOV problem could be more serious\nfor a new library, since it is very difﬁcult to collect\nsufﬁcient API usage data.\nTo conduct API recommendation for new li-\nbraries, cross-library API recommendation is a po-\ntentially feasible solution, which aims to recom-\nmend APIs in new libraries based on the usage data\nof APIs in other libraries, but it is still an open\nchallenge due to the inherent OOV problem. For\nexample, as shown in Listing 2, we may rarely (or\neven never) see SQLException.printStackTrace()\n3426\nin the training set, but the usage of Exception is\nvery common in the training set and the usage of\nSQLException and Exception are similar. So if we\nuse a word segmentation algorithm to split SQLEx-\nception.printStackTrace() into the sequence: SQL-\nException-.-print-StackTrace(), we can use the Ex-\nception usage pattern learned during the training\nprocess to predict theprintStackTrace()method and\nﬁnally synthesize SQLException.printStackTrace()\nas the recommendation result.\u0007 \u0004\n1 p u b l i c s t a t i c v o i d main ( . . . ) {\n2 F i l e I n p u t S t r e a m i n p u t S t r e a m = n u l l ;\n3 t r y {\n4 F i l e f i l e = new F i l e ( \" tmp . t x t \" ) }\n5 c a t c h ( E x c e p t i o n e ) {\n6 e . ____ ) ; / / To w r i t e a c a t c h b l o c k\n7 }\n8 }\n9 −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\n10 API Sequence : F i l e I n p u t S t r e a m . new ( ) −\nTRY−TryBlock − F i l e . new ( S t r i n g ) −\nCATCH− E x c e p t i o n . _____ ( )\n\r\u0006 \u0005\nListing 1: An Example of API Recommendation\n\u0007 \u0004\n1 p u b l i c s t a t i c C o n n e c t i o n\n2 g e t C o n n e c t i o n ( ) {\n3 C o n n e c t i o n c o n n e c t i o n = n u l l ;\n4 t r y {\n5 c o n n e c t i o n = D r i v e r m a n a g e r . g e t (URL,\n6 username , password ) ;\n7 } c a t c h ( SQLException e ) {\n8\n9 e . p r i n t S t a c k T r a c e ( ) ;\n10 }\n11 r e t u r n c o n n e c t i o n ;\n12 }\n\r\u0006 \u0005\nListing 2: An OOV Example in API Recommendation\nTo achieve the goal of cross-library API recom-\nmendation, we draw lessons from the area of text\ngeneration in relieving the OOV problem (Sennrich\net al., 2016; Hermann et al., 2021). More speciﬁ-\ncally, we design a framework of cross-library API\nrecommendation, called APIRecX, which consists\nof three main components, i.e., API segmentation,\nsubword language model building, and API synthe-\nsis for recommendation. Since the OOV problem\nat the API level hampers cross-library API recom-\nmendation, APIRecX ﬁrst incorporates BPE (Byte\nPair Encoding) (Provilkov et al., 2020; Sennrich\net al., 2016), one of the most widely-used word\nsegmentation methods in text generation, to split\neach API call into a sequence of subwords. That\nis, the OOV problem at the API level could be\nlargely relieved at the subword level. Based on\na large number of subword data, APIRecX then\nadopts the “pre-training&ﬁne-tuning” mechanism\nto build a GPT-based(Generative Pre-Training) pre-\ntrained language model, which can recommend a\nsubword in each prediction. Since the recommen-\ndation process is conducted at the subword level,\nit is necessary to compose a complete API call\nfor recommendation based on predicted subwords.\nHere, APIRecX incorporates beam search for API\nsynthesis.\nTo evaluate the performance of APIRecX, we\nconducted an extensive study based on 1,711 Java\nprojects from GitHub involving six libraries in\nthree domains as subjects for mimicking new li-\nbraries in the scenario of cross-library API recom-\nmendation, and over 14,000 GitHub Java projects\nthat do not involve the former six libraries as train-\ning corpus. By comparing with two typical API\nrecommendation approaches, i.e., LSTM-based lan-\nguage model(Yan et al., 2018; White et al., 2015)\nand N-gram-based language model (Raychev et al.,\n2014; Karampatsis and Sutton, 2019; Hindle et al.,\n2012), our experimental results demonstrate the\neffectiveness of APIRecX for cross-library API\nrecommendation in terms of recommendation ac-\ncuracy.\nTo sum up, this work makes the following major\ncontributions:\n• We propose the ﬁrst framework for cross-\nlibrary API recommendation, consisting of\nBPE-based API segmentation, subword lan-\nguage model building, and beam-search based\nAPI synthesis.\n• We are the ﬁrst to build a GPT-based language\nmodel in the area of API recommendation,\nwhich is more effective than the existing lan-\nguage models.\n• We conduct an extensive study to evaluate our\nproposed approach, demonstrating its effec-\ntiveness in the scenario of cross-library API\nrecommendation.\n2 Approach\nIn the paper, we propose APIRecX, the ﬁrst\napproach for cross-library API recommendation.\nWith APIRecX, we can recommend APIs in some\nlibraries (especially new libraries) by learning from\na large amount of API usage data of some other\nlibraries.\n3427\nOriginal \nLibraries\nBPE \nSegmentation\nFine-tuned \nGPT model\nAPI List\nPre-train Language Model\nNew \nLibraries\nCross-library Fine-tune\nAPI Code \nSnippet\nBeam\nSearch\nRecommend\nDevelopers\nAPI Recommendation\nInput\nBPE \nSegmentation\nPre-trained \nGPT Model\nPre-train\nFine-tune\nFigure 1: An Overview of APIRecX\n2.1 Overview\nAchieving the goal of cross-library API recommen-\ndation is challenging.\n• First, different libraries tend to not contain\nAPIs with the same names, and thus it is hard\nto adopt existing approaches to recommend\nAPIs that are not seen in training data. That is,\nthe ﬁrst challenge is due to the OOV problem\nat the API level. To overcome it, APIRecX\naims to recommend APIs at the subword level\nthrough API segmentation. The insight is that\nan API call usually consists of a set of rela-\ntively commonly-used subwords such as Ex-\nception, print, etc. Therefore, the OOV prob-\nlem at the API level can be largely relieved at\nthe subword level.\n• Second, APIRecX recommends each subword\nin turn and then composes a complete API call\nfor recommendation based on predicted sub-\nwords. That means that an API call can be cor-\nrectly recommended only if all the subwords\nin the API call are recommended correctly,\nwhich largely aggravates the recommendation\ndifﬁculty. To relieve the inaccuracy of API\nrecommendation caused by inaccurate sub-\nword prediction, APIRecX incorporates beam\nsearch to enlarge the search space of API syn-\nthesis instead of directly recommending an\nAPI call composed by Top-1 subword in each\nprediction.\nWith the above two insights, we design a novel\nGPT-based method in APIRecX to build a subword\nlanguage model. Here, APIRecX ﬁrst pre-trains a\nsubword language model based on a large number\nof API usage data of other libraries in an ofﬂine pro-\ncess. When new libraries are released, APIRecX\nthen directly ﬁne-tunes the pre-trained model af-\nter collecting a certain amount of API usage data\nof new libraries, which is much more efﬁcient\nthan retraining based on all API usage data (i.e.,\npre-training data and ﬁne-tuning data). Also, to\nmake APIRecX a light-weight approach, APIRecX\ndoes not build complex data-ﬂow and control-ﬂow\ngraphs, but directly represents a method as an API\nsequence following the existing work (Gu et al.,\n2017; Yan et al., 2018; Nguyen et al., 2017). The\noverview of APIRecX is shown in Figure 1.\n2.2 BPE-based API Segmentation\nAPIRecX extracts API sequences following the\npractice in the existing work (Gu et al., 2017),\nwhich extracts all API calls (identiﬁer&arguments,\ne.g.DriverManager.getConnection(String)), and\ncontrol statements with API call in a method to\nform an API sequence. Here, all variables in API\nsequences are replaced with their types. For exam-\nple, for an API call o.m() where o is an instance\nof a class C, APIRecX adds C.m to the API se-\nquence.\nAlthough API names tend to be unique, they\nusually consists of a set of relatively commonly-\nused subwords. That is, different API names may\ninclude common subwords, and thus the OOV prob-\nlem at the API level could be largely relieved at the\nsubword level. With this insight, APIRecX splits\nan API call in an API sequence into a sequence of\nsubwords and conducts follow-up learning and pre-\ndiction at the subword level, and ﬁnally composes\n3428\na complete API call for recommendation based on\npredicted subwords. In this way, it is possible to\ncompose an unseen API call in training data with\nsubwords, which makes cross-library API recom-\nmendation become feasible.\nHere, APIRecX adopts BPE (Provilkov et al.,\n2020; Sennrich et al., 2016; Devlin et al., 2018),\none of the most widely-used word segmentation\nmethods in text generation, for splitting an API call\nto subwords. The reason why choosing BPE is that\nit achieves a good balance between effectiveness\nand efﬁciency. More speciﬁcally, compared with\ncharacter segmentation (Gao et al., 2020) ,whites-\npace segmentation (Tezcan et al., 2020; Mikolov\net al., 2013),and CamelCase segmentation,BPE\nis more effective, since character segmentation is\ntoo ﬁne-grained and thus leads to much semantic\nloss while whitespace segmentation is too coarse-\ngrained for API calls and thus cannot effectively\nrelieve the OOV problem.Although CamelCase seg-\nmentation can achieve a relatively appropriate seg-\nmentation granularity, compared with BPE,it has\na larger granularity, which will cause more OOV\nwords.By taking the domain of Swing as an exam-\nple, there are 61.9% common subwords between\ntraining and test data achieved by BPE, while there\nare only 50.9% common subwords achieved by\nCamelCase.Compared with more advanced meth-\nods (e.g., WordPiece (Devlin et al., 2018) and\nULM (Chen et al., 2005), BPE is more efﬁcient but\nnot much less effective, since these methods need\nto build language models during word segmenta-\ntion while BPE is based on frequency. Besides,\nAPIRecX adds a special subword (/t) to mark\nthe end of each API call, which helps APIRecX\ndetermine the termination of subword recommen-\ndation for an API call. Through this step, APIRecX\nobtains a large amount of API usage data at the sub-\nword level. As an example, for the code in Listing\n1, the API sequence after BPE-based segmenta-\ntion is: Connection–.–new()(/t)–TRY(/t)–Driver–\nManager–.–get–Connection()(/t) –CATCH(/t)–\nException–.–print–StackTrace()(/t).\n2.3 Building a Subword Language Model\nTo build a subword language model, APIRecX\nadopts the “pre-training & ﬁne-tuning” mechanism\nas presented above. That is, APIRecX ﬁrst pre-\ntrains a subword language model based on a large\namount of subword data that do not involve APIs\nof new libraries, and then ﬁne-tunes the pre-trained\nmodel by including a small amount of subword\ndata involving the APIs of the library to be recom-\nmended. Besides the efﬁciency beneﬁt presented\nabove, ﬁne-tuning has been demonstrated to be\nmore effective than the strategy of direct training\nbased on the mixed data of pre-training data and\nﬁne-tuning data (Mao et al., 2015), since the vol-\nume of API usage data of new libraries is signiﬁ-\ncantly smaller than that of other libraries, leading\nto very difﬁcult to learn usage patterns of the APIs\nof new libraries via the latter strategy.\nIn APIRecX, we design a GPT-based subword\nlanguage modeling building method. GPT ﬁrst\nmaps an API subword sequence S =a1, ..., at into\na vector matrix through the embedding layer Emb\nwhere t represents the total number of subword\nin the API subword sequence , and then we can\nget the embedding matrix H0 of the API subword\nsequence after adding the position information\nthrough the position embedding matrix Wp.\nHx =\n{\nEmb(S) +Wp x = 0\nTblock (Hx−1) 1 ≤x ≤n (1)\nThen, GPT inputs the obtained embedding ma-\ntrix into the decoder block of the transformer for\ncalculation. where x represents the order number\nof Transformer layers, and the vector matrix Hn\noutputted by the last layer of decoder block rep-\nresents the attention weight for each subword in\nthis sequence. Then, Hn is multiplied by the trans-\npose of embedding layer matrix, and normalized\nby softmax to obtain P(S) which represents the\nprobabilities of all subwords in the vocabulary at\neach position in the sequence.\nP(S) =Softmax (Hn ∗EmbT ) (2)\nIn the training phase, we calculate the loss between\nground truth and P(S) through cross-entropy, and\noptimize GPT through the Adam optimization al-\ngorithm.\n2.4 Beam-search based API Synthesis\nWith a subword language model, APIRecX recom-\nmends a subword in each prediction based on a\nsequence of subwords before the current position\nto be predicted. Given that an API call to be recom-\nmended is denoted as Am = {s1\nm, s2\nm, . . . , snm\nm }\nwhere sj\nm refers to the jth subword in Am and nm\nrefers to the number of subwords in Am, API calls\nbefore Am are denoted as {A1, A2, . . . , Am−1}\n3429\nwhere Ai = {s1\ni , s2\ni , . . . , sni\ni }. When pre-\ndicting at the position of s1\nm, APIRecX\ninputs {s1\n1, . . . , sn1\n1 , . . . , s1\nm−1, . . . , snm−1\nm−1 }\nto the model, and when predicting at\nthe position of sj\nm, APIRecX inputs\n{s1\n1, . . . , sn1\n1 , s1\nm−1, . . . , snm−1\nm−1 , w1\nm, .., wj−1\nm },\nwhere wj−1\nm is the predicted subword at the\nposition of sj−1\nm and wj−1\nm is the same as sj−1\nm\nif the prediction is correct. That is, the cur-\nrently predicted subword is used to predict\nsubsequent subwords. When the prediction\nresult ends with (/t), APIRecX outputs the\nchain of predicted subwords as the API call\nfor recommendation. For example, in listing 2,\nwhen the developer enters e. on line 6, APIRecX\ninputs {A1, A2, A3, A4, S1\n5 , S2\n5 }, where A1 =\n{File, Input, Stream, ., new()(/t)}, A2 =\n{TRY (/t)}, A3 = {File, ., new(String)(/t)}\n, A4 = {CATCH (/t)}, S1\n5 = Exception\nand S2\n5 = .. Then APIRecX predicts the next\nsubword based on the input. When APIRecX\npredicts a subword ending with (/t) such as\n{print, StackTrace()(/t)}, it will merge pre-\ndicted subwords with S1\n5 and S2\n5 and return the\nresult to the developer.\nHowever, subword prediction aggregates the dif-\nﬁculty of API recommendation, since it is hard to\nguarantee the accurate prediction of each subword\nin an API call. Especially, when a wrong subword\nis predicted in a certain position, the predictions of\nall the subsequent subwords could be also affected,\nsince the wrong subword will be used to predict\nsubsequent subwords. Actually, each subword is\nassigned as a probability in each prediction. By\nconsidering all the subwords in each prediction and\nusing each subword for subsequent predictions, the\ncorrect chain of subwords (used for composing a\ncomplete API call) cannot be missing, but explor-\ning such enormous combination space is unafford-\nable. Therefore, it is still challenging to recom-\nmend a complete API call based on subword-level\nprediction.\nTo achieve the balance between the accuracy of\nAPI recommendation and the efﬁciency, APIRecX\nadopts widely-used beam search (Freitag and Al-\nOnaizan, 2017; Shu and Nakayama, 2018; Huang\net al., 2017). More speciﬁcally, beam search consid-\ners Top-K subwords (K refers to beam size) in each\nprediction rather than only Top-1 subword or all the\nsubwords. For each of Top-K subwords in a predic-\ntion, it then produces Top-K subwords and obtains\nK2 chains of subwords, and then preserves Top-K\nchains according to their chain probabilities for the\nnext prediction. Following the existing work (Shu\nand Nakayama, 2018; Huang et al., 2017; Freitag\nand Al-Onaizan, 2017; Karampatsis et al., 2020),\nwe use Formula 3 to calculate the chain probability\nof a chain of subwords:\nP(w1\nm, ..., wi\nm|s1\n1, ..., sn1\n1 , ..., s\nnm−1\nm−1 ) =\ni∏\nj=1\np(wj\nm) (3)\nwhere, p(wj\nm) (which is short for\np(wj\nm|s1\n1, . . . , sn1\n1 , . . . , snm−1\nm−1 , w1\nm, . . . , wj−1\nm )) is\nthe probability of the jth subword in the chain of\n(w1\nm, . . . , wi\nm) predicted by the subword model.\nTo relieve the effectiveness problem caused\nby the monotonicity of traditional beam search,\nAPIRecX preserves the memory of poor-quality\nincomplete chains produced during the process\nof beam search following the existing work in\ntext generation (Shu and Nakayama, 2018). More\nspeciﬁcally, APIRecX constructs a candidate pool\nthat stores the remaining incomplete chains except\nTop-K chains among k2 chains produced in each\nprediction. When k2 chains produced based on\nTop-K chains selected from the last prediction have\nsmaller chain probabilities than those of chains\nin the candidate pool, APIRecX chooses Top-K\nchains among the k2 chains produced in the current\nprediction and all the chains in the candidate pool\nrather than only the current k2 chains. In this way,\nAPIRecX has a chance to make up wrong choice in\nprevious predictions. Besides, APIRecX improves\nthe condition of terminating the beam search pro-\ncess following the existing work (Huang et al.,\n2017) in text generation, i.e., the searching stops\nuntil the smallest chain probability among all the\nproduced complete chains is larger than the largest\nchain probability among all incomplete chains (in-\ncluding incomplete chains in both the candidate\npool and current Top-K chains).\n3 Evaluation\n3.1 Experimental Setup\n3.1.1 Datasets\nWe used six JDK libraries from three domains to\nmimic new libraries in the scenario of cross-library\nAPI recommendation. They are java.sql and\njavax.sql in the domain of JDBC (which is the\ndomain about database operations), java.awt\nand javax.swing in the domain of Swing\n3430\nDomain #API #Project #Sequence\nJDBC 909 784 42,298\nSwing 10,622 722 63,249\nIO 1,192 205 15,356\nTable 1: Statistical information on three domains\n#Projects 14,807\n#LOC 352,312,696\n#Methods 15,201,014\n#Sequence 5,120,310\nTable 2: Statistical information on pre-train corpus\n(which is the domain about user interfaces), and\njava.io and java.nio in the domain of IO\n(which is the domain about stream-based inputs and\noutputs), respectively. Based on the three domains,\nwe conducted three groups of experiments, each of\nwhich uses the two libraries in the corresponding\ndomain as the new libraries for recommendation.\nTable 1 shows the information about the three ex-\nperiments. where Column “#API” is the number\nof APIs in the corresponding domain libraries, Col-\numn “#Project” is the number of Java projects that\nare collected from GitHub and use the APIs in the\ndomain libraries, and Column “#Sequence” is the\nnumber of API sequences that are extracted from\nthe collected projects.\nBesides, we adopted the corpus provided by the\nexisting work (Allamanis and Sutton, 2013) for pre-\ntraining. The corpus has over 14,000 Java projects\nfrom GitHub after removing the projects involv-\ning the above three domains. From these projects,\nwe extracted over 5,000,000 API sequences as pre-\ntraining data.Table 2 shows the information about\nthe pre-train corpus. where Column “#Projects”\nis the number of Java projects in pre-train corpus,\nColumn “#LOC” is the total number of lines of\ncode, Column “#Methods” is the total number of\njava methods, and Column “#Sequence” is the num-\nber of API sequences that are extracted from this\ncorpus.\n3.1.2 Selecting test and ﬁne-tune data\nWe used 10 projects (splitting domain projects\ninto 10 groups and then selecting the one with the\nlargest number of domain API calls in each group)\nas test projects, and extract API call sequences from\nthem.For each sequence of API calls, we produced\na set of API call sequences, each of which contain\na \"hole\", as test data. Speciﬁcally, we produced\nthem by digging a \"hole\" from the second API call\nin the sequence in turn respectively. Then, for each\nAPI call sequence with a \"hole\", we used the se-\nquence of API calls before the \"hole\" as input for\npredicting the API call in the \"hole\". After select-\ning the test data, we sample a certain amount of\ndata from the remained data at 5 different sampling\nratios which are 0.2%, 1%, 10%, 50%, and 100%\nas ﬁne-tune data.Then we use these sampled data\nto ﬁne-tune the pre-trained model following the\nﬁne-tuning process presented in Section 2.3\n3.1.3 Baselines\nWe adopted traditional LSTM-based API recom-\nmendation approach (Yan et al., 2018; White et al.,\n2015; Zhang et al., 2019; Chen et al., 2019) and N-\ngram based API recommendation approach (Ray-\nchev et al., 2014; Karampatsis and Sutton, 2019)\nfor comparison in order to quantitatively investi-\ngate the superiority of APIRecX over traditional\nAPI recommendation approaches. We refer to the\nparameter settings in these two works(Yan et al.,\n2018; Raychev et al., 2014) to train baseline tools\non the data we collected, and the speciﬁc parameter\nsettings are shown in Table 6.\n3.1.4 Parameters\nThe parameters comprise the model training param-\neters and the beam search parameters in the API\nrecommendation process. Table 6 lists all the pa-\nrameters of APIRecX and baselines. The structure\nof original GPT contains a 12-layer transformer\ndecoder block with 12-head attention, containing\nnearly 100 million parameters, which requires an\nextremely huge amount of data to support train-\ning. However, compared with collecting text data,\nit is harder to collect such a huge amount of API\nusage data to support training such a complicated\nmodel, and thus we tailored the structure of the\noriginal GPT to match with the scale of our train-\ning data. Speciﬁcally, our tailored GPT uses a 6-\nlayer transformer decoder block with 8-head atten-\ntion. Besides, GPT handles ﬁxed-length sequences,\nthus we set the subword-sequence length to be 512.\nIn our context, the ﬁxed-length sequence refers\nto the ﬁxed-length subword sequence processed\nfrom an API call sequence. For the API call se-\nquences in our dataset, the average length is 41,\nthe largest length is 2,280, and the percentage of\nsubword sequences that are longer than 512 is only\n0.4%. Moreover, the longer the sequence is, the\nmore difﬁcult it is to model. Therefore, our setting\n(512) could reach a good trade-off following the\nexisting study(Devlin et al., 2018). The baseline\n3431\nmodel parameters were set according to the previ-\nous work(Yan et al., 2018; Raychev et al., 2014).\nWe trained the APIRecX for 15 epochs in the pre-\ntraining stage, and then we adopted the early stop\nstrategy to terminate the ﬁne-tuning process in the\nﬁne-tuning stage. For baseline approaches, we\nadopted early stop strategy to terminate the training\nprocess according to the previous work.\nBeam search process contains two parameters:\nbeam size and max iteration. Beam size represents\nthe width of beam search and max iteration repre-\nsents the maximum search epoch. More details of\nparameters setting will be shown in the Appendix.\n3.1.5 Evaluation Metric\nTo evaluate the performance of APIRecX, we\nadopted Top-N accuracy following the existing\nwork on API recommendation (Xie et al., 2019;\nNguyen et al., 2016; Nguyen and Nguyen, 2015).\nEach API recommendation approach can produce\na ranking list of API calls for recommendation.\nTop-N accuracy measures the percentage of the\ncases that the correct API call is included in Top-\nN results among all the locations in the test set,\nand higher Top-N accuracy indicates better perfor-\nmance. Following the existing work (Nguyen et al.,\n2016; Nguyen and Nguyen, 2015; Xie et al., 2019;\nYan et al., 2018), we setN to be 1, 5, and 10 respec-\ntively. Note that We focus on the recommendation\nof domain APIs, so we only report the accuracy of\nTop-N recommendation of domain APIs.\n3.2 Results and Analysis\n3.2.1 Overall effectiveness\nTable 3 presents the comparison results between\nAPIRecX and baselines under ﬁve sampling ratios\nin three domains, respectively.\nFrom this table, APIRecX performs better than\nthe two baselines under all the studied sampling\nratios in all the three domains in terms of all the\nmetrics. For example, under the sampling ratio of\n0.2% in the domain of IO, APIRecX has achieved\n52.9% Top-1 accuracy while the two baselines are\nonly 30.6% and 16.5%. The improvements are\n72.87% and 220.61%, respectively. We also per-\nformed a Wilcoxon rank sum test to investigate\nwhether our approach can signiﬁcantly outperform\nLSTM and N-gram across all the domains respec-\ntively. The results show that all the p-values are\nsmaller than 0.004 (<0.05) regardless of Top-1/Top-\n5/Top-10 accuracy, demonstrating the effectiveness\nof our approach in statistics.\nWe then analyzed why APIRecX performs well\nas shown in Table 4. In this table, the fast three\nrows present the percentage that training data cover\ndomain APIs in the test set, the percentage that\ntraining data cover subwords from domain APIs\nin the test set, percentage of unseen APIs in the\ncorrect recommendation result, and the last rows\npresent the number of API call types that success-\nfully recommended by APIRecX under the sam-\npling ratio of 0.2%.\nFrom Table 4, under the sampling ratio of\n0.2%, the API coverage is small (10.9 ∼49.3%),\nonly 25.5% APIs are covered by training data\non average, but the subword coverage is large\n(61.9∼89.3%) and the average subword coverage\nrate reached 77.7%, indicating the power of API\nsegmentation to handle the OOV problem. Indeed,\nAPIRecX is able to recommend unseen APIs in\nboth pre-training and ﬁne-tuning data. For exam-\nple, Among the APIs correctly recommended by\nthe APIRecX, an average of 28.1%, 131.3 types is\nfrom the unseen APIs,demonstrating its ability for\ncross-library API recommendation.\n3.2.2 Effectiveness of Beam Search\nWe compare our beam search strategy in APIRecX\nand the traditional beam search (Freitag and Al-\nOnaizan, 2017; Shu and Nakayama, 2018) under\ndifferent beam sizes. Here, we use the JDBC do-\nmain with and the sampling ratio of 10% as the rep-\nresentative, whose comparison results are shown in\nTable 5. From this table, our used beam search per-\nforms better than traditional beam search under all\nthe studied beam sizes in terms of all the metrics,\ndemonstrating the contribution of the improved\nbeam search strategy. In the meanwhile, its contri-\nbution becomes more obvious in Top-5 accuracy\nand Top-10 accuracy than Top-1 accuracy because\nthe rescued chains of subwords by the improved\nbeam search are difﬁcult to have larger chain prob-\nabilities than Top-1 chain due to the small proba-\nbility of certain subword prediction. More speciﬁ-\ncally, the probability of a complete API call (e.g.,\nprintStackTrace() in Line-6 of Listing-1) is the\nproduct of the probabilities of a chain of subwords\n(e.g., print, StackTrace, ()). Although the candi-\ndate pool storage of the improved beam search can\nrelieve the effectiveness problem caused by the\nmonotonicity of traditional beam search through\npreserving the memory of poor-quality incomplete\nchains produced during the beam-search process,\nthe small probabilities of poor-quality incomplete\n3432\nSample Approach JDBC Swing IO\nTop-1 Top-5 Top-10 Top-1 Top-5 Top-10 Top-1 Top-5 Top-10\n0.2%\nAPIRecX 37.9 74.7 81.2 25.0 43.8 51.2 52.9 69.5 73.7\nLSTM 26.8 52.6 65.9 15.1 31.3 39.1 30.6 53.4 63.3\nN-gram 11.9 41.5 56.5 7.9 26.3 31.6 16.5 45.9 57.0\n1%\nAPIRecX 42.8 77.7 83.7 25.3 46.9 54.5 56.4 75.5 79.8\nLSTM 31.6 67.4 74.8 17.2 34.3 44.0 36.7 56.5 66.4\nN-gram 16.0 40.6 58.5 10.2 28.2 36.7 16.7 45.9 57.8\n10%\nAPIRecX 46.9 79.9 85.7 40.6 67.8 74.5 56.9 75.9 80.5\nLSTM 33.7 69.1 75.3 30.6 53.1 60.9 36.1 60.8 70.0\nN-gram 18.6 43.8 59.3 16.3 37.5 46.9 18.1 48.3 59.2\n50%\nAPIRecX 56.6 86.3 93.0 48.7 79.0 80.9 60.6 81.9 85.3\nLSTM 41.8 73.8 84.7 32.8 56.7 65.0 39.1 64.1 70.9\nN-gram 25.4 55.1 63.7 16.3 39.4 48.7 18.6 48.5 62.8\n100%\nAPIRecX 60.0 89.4 94.5 54.8 77.2 83.7 63.9 84.2 88.7\nLSTM 43.4 76.2 85.6 36.7 61.1 69.2 40.1 67.1 75.3\nN-gram 28.6 56.1 65.5 18.7 41.4 50.7 21.6 52.8 68.8\nTable 3: Overall effectiveness of APIRecX\nCriterion Domain Avg.JDBC Swing IO\nAPI Coverage 49.3% 10.9% 16.3% 25.5%\nSubword Coverage 82.0% 61.9% 89.3% 77.7%\nOOV Correct Rate 8.7% 26.4% 49.6% 28.1%\nOOV Correct API type 14.7 317.6 61.5 131.3\nTable 4: Analysis of the results\nBeam size Method Top-1 Top-5 Top-10\n10 Ours 45.2 79.4 84.3\nTraditional 44.1 69.7 76.1\n15 Ours 46.6 78.6 84.6\nTraditional 44.5 69.1 75.8\n20 Ours 46.9 79.9 85.7\nTraditional 44.1 70.1 77.1\n25 Ours 46.6 83.1 85.7\nTraditional 44.0 72.7 77.2\n30 Ours 45.3 80.1 86.7\nTraditional 44.3 71.9 78.0\nTable 5: The results of different beam search methods\non JDBC\nchains could lead to the small probability of the\ncorresponding complete API call, making it hard to\nbe ranked as Top-1. Taking Line-6 in Listing-1 as\nan example, if “StackTrace” has a small probability,\nits small probability could make the probability of\nthe complete API call small, causing it hard to be\nranked as Top-1. Therefore, the improved beam\nsearch has less apparent improvement in terms of\nTop-1 accuracy. Also, APIRecX performs stably\nunder different beam sizes.\n4 Related work\n4.1 API recommendation\nIn the literature, some statistical learning based\n(Nguyen and Nguyen, 2015; Liu et al., 2018; Ray-\nchev et al., 2014; Xie et al., 2019) and pattern\nmining based API recommendation approaches\n(Zhong et al., 2009; Wang et al., 2013; Fowkes and\nSutton, 2016; Xie et al., 2019) have been proposed\nwithout dealing with the OOV problem, and thus\nall of them cannot be effective in the scenario of\ncross-library API recommendation. For example,\nXie et al. (2019) proposed HiRec, which improves\npattern-mining based approaches by utilizing the\nhidden information of project-speciﬁc code via call\ngraph in mining API usage patterns. Nguyen and\nNguyen (2015) designed a graph-based statistical\nlanguage model by representing source code as\ngraphs for API recommendation. Different from\nthem, APIRecX is the ﬁrst approach for cross-\nlibrary API recommendation by handling the OOV\nproblem via GPT-based pre-trained subword lan-\nguage model.\n4.2 Pre-trained models across languages\nOur approach is inspired by pre-training in the mul-\ntilingual scenario (Chi et al., 2020; Huang et al.,\n2019; Yang et al., 2020a,b, 2019). For example,\nLample and Conneau (2019) proposed the XLM\nmodel, which processes multiple languages via\nBPE so that all the languages can share subword\ndictionaries. Ren et al. (2019) proposed the cross-\nlingual masked language model, which uses more\nexplicit cross-lingual information (such as trans-\nlation table). More speciﬁcally, they used the\nmonolingual corpus of two languages to train the\nmonolingual N-gram vector through FastText (Bo-\n3433\njanowski et al., 2017), and then used the unsuper-\nvised cross-lingual word vector method, VecMap,\n(Garneau et al., 2020) to obtain the cross-lingual\nN-gram vector. The translation table between the\ntwo languages is inferred from the similarity of the\nN-gram vectors of the two languages.\nDifferent from them, our work targets the prob-\nlem of API recommendation rather than cross-\nlingual problems, which have different character-\nistics, and APIRecX builds a GPT-based subword\nlanguage model for API recommendation. Code-\nBERT(Feng et al., 2020) gets a general language\nmodel about programming language by pre-trained\non six different programming languages, and can\nbe applied to different downstream tasks,It seems\nthat codebert can be our baseline but the reason\nwhy not use CodeBERT as the baseline for com-\nparison is that it needs two-way information and\nwe regard API recommendation as a one-way text\ngeneration task. When developers use API, they\nusually write API calls sequentially (forward) and\nthe task of API recemmendation is to predict the\nfuture API calls, there is no reverse information\n(backward) in practice. Therefore, CodeBERT can-\nnot be applied to our problem.\n5 Conclusions\nWe propose the ﬁrst approach APIRecX for cross-\nlibrary API recommendation, which can automat-\nically recommend API calls for new libraries.\nAPIRecX ﬁrst splits each API call into a sequence\nof subwords to relieve the OOV problem at the\nAPI level. It then pre-trains a GPT-based sub-\nword language model based on a large number\nof API usage data from other libraries. By ﬁne-\ntuning the pre-trained model with a sample of\nAPI usage data of new libraries, APIRecX con-\nducts subword prediction and incorporates beam\nsearch to compose a complete API call for recom-\nmendation. We conduct an extensive study based\non six libraries of three domains for mimicking\nnew libraries and 14,000 GitHub Java projects for\npre-training, demonstrating the effectiveness of\nAPIRecX. However, our work also has certain limi-\ntation, which is the generalization of our results and\nﬁndings. Although we invested signiﬁcant time and\neffort to prepare datasets, conducted experiments\nand analyzed results, our experiments involved only\none program language with three domains. The per-\nformance of our neural architecture, and especially\nthe ﬁndings on transfer learning, could be different\nwith other programming languages or libraries. In\nthe future, we will try to get rid of this limitation by\napplying our approach to more languages/libraries.\nThe source code of APIRecX and experimen-\ntal data can be found in https://github.com/\nyuningkang/APIRecX.\n6 Acknowledgements\nThis work was funded by National Natural Science\nFoundation of China (Nos. 61872263, 62002256,\n20201180), Intelligent Manufacturing Special Fund\nof Tianjin. We are also very grateful to reviewers\nfor their helpful comments.\nReferences\nMiltiadis Allamanis and Charles Sutton. 2013. Min-\ning source code repositories at massive scale us-\ning language modeling. In Proceedings of the 10th\nWorking Conference on Mining Software Reposito-\nries, MSR ’13, San Francisco, CA, USA, May 18-19,\n2013, pages 207–216. IEEE Computer Society.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomás Mikolov. 2017. Enriching word vectors with\nsubword information. Trans. Assoc. Comput. Lin-\nguistics, 5:135–146.\nMarcel Bruch, Martin Monperrus, and Mira Mezini.\n2009. Learning from examples to improve code\ncompletion systems. In Proceedings of the 7th Joint\nMeeting of the European Software Engineering Con-\nference and the ACM SIGSOFT Symposium on The\nFoundations of Software Engineering , ESEC/FSE\n’09, page 213–222, New York, NY , USA. Associa-\ntion for Computing Machinery.\nAitao Chen, Yiping Zhou, Anne Zhang, and Gordon\nSun. 2005. Unigram language model for chinese\nword segmentation. In Proceedings of the Fourth\nSIGHAN Workshop on Chinese Language Process-\ning, SIGHAN@IJCNLP 2005, Jeju Island, Korea,\n14-15, 2005. ACL.\nChi Chen, Xin Peng, Jun Sun, Zhenchang Xing, Xin\nWang, Yifan Zhao, Hairui Zhang, and Wenyun Zhao.\n2019. Generative API usage code recommendation\nwith parameter concretization. Sci. China Inf. Sci. ,\n62(9):192103:1–192103:22.\nZewen Chi, Li Dong, Furu Wei, Wenhui Wang, Xian-\nLing Mao, and Heyan Huang. 2020. Cross-lingual\nnatural language generation via pre-training. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artiﬁcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020 , pages 7570–\n7577. AAAI Press.\n3434\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, and Ming Zhou. 2020. Code-\nbert: A pre-trained model for programming and nat-\nural languages. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: Findings, EMNLP 2020, Online Event,\n16-20 November 2020 , pages 1536–1547. Associa-\ntion for Computational Linguistics.\nJaroslav Fowkes and Charles Sutton. 2016. Parameter-\nfree probabilistic api mining across github. FSE\n2016, page 254–265, New York, NY , USA. Associa-\ntion for Computing Machinery.\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam\nsearch strategies for neural machine translation. In\nProceedings of the First Workshop on Neural Ma-\nchine Translation, NMT@ACL 2017, Vancouver,\nCanada, August 4, 2017 , pages 56–60. Association\nfor Computational Linguistics.\nYingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, and\nRichard H. R. Hahnloser. 2020. Character-level\ntranslation with self-attention. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 1591–1604. Association for Computa-\ntional Linguistics.\nNicolas Garneau, Mathieu Godbout, David Beau-\nchemin, Audrey Durand, and Luc Lamontagne.\n2020. A robust self-learning method for fully un-\nsupervised cross-lingual mappings of word embed-\ndings: Making the method robustly reproducible as\nwell. In Proceedings of The 12th Language Re-\nsources and Evaluation Conference, LREC 2020,\nMarseille, France, May 11-16, 2020 , pages 5546–\n5554. European Language Resources Association.\nXiaodong Gu, Hongyu Zhang, Dongmei Zhang, and\nSunghun Kim. 2017. Deep api learning.\nEnno Hermann, Herman Kamper, and Sharon Gold-\nwater. 2021. Multilingual and unsupervised sub-\nword modeling for zero-resource languages. Com-\nput. Speech Lang., 65:101098.\nAbram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel,\nand Premkumar T. Devanbu. 2012. On the natu-\nralness of software. In 34th International Confer-\nence on Software Engineering, ICSE 2012, June 2-\n9, 2012, Zurich, Switzerland, pages 837–847. IEEE\nComputer Society.\nHaoyang Huang, Yaobo Liang, Nan Duan, Ming Gong,\nLinjun Shou, Daxin Jiang, and Ming Zhou. 2019.\nUnicoder: A universal language encoder by pre-\ntraining with multiple cross-lingual tasks. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019 , pages 2485–2494. As-\nsociation for Computational Linguistics.\nLiang Huang, Kai Zhao, and Mingbo Ma. 2017. When\nto ﬁnish? optimal beam search for neural text gen-\neration (modulo beam size). In Proceedings of the\n2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2134–2139, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nQiao Huang, Xin Xia, Zhenchang Xing, David Lo,\nand Xinyu Wang. 2018. API method recommenda-\ntion without worrying about the task-api knowledge\ngap. In Proceedings of the 33rd ACM/IEEE Interna-\ntional Conference on Automated Software Engineer-\ning, ASE 2018, Montpellier, France, September 3-7,\n2018, pages 293–304. ACM.\nRafael-Michael Karampatsis, Hlib Babii, Romain\nRobbes, Charles Sutton, and Andrea Janes. 2020.\nBig code != big vocabulary: Open-vocabulary mod-\nels for source code. CoRR, abs/2003.07914.\nRafael-Michael Karampatsis and Charles Sutton. 2019.\nMaybe deep neural networks are the best choice for\nmodeling source code. CoRR, abs/1903.05734.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nXiaoyu Liu, LiGuo Huang, and Vincent Ng. 2018. Ef-\nfective api recommendation without historical soft-\nware repositories. ASE 2018, page 282–292, New\nYork, NY , USA. Association for Computing Machin-\nery.\nJunhua Mao, Xu Wei, Yi Yang, Jiang Wang, Zhiheng\nHuang, and Alan L. Yuille. 2015. Learning like a\nchild: Fast novel visual concept learning from sen-\ntence descriptions of images. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 2533–\n2541. IEEE Computer Society.\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In 1st International Con-\nference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop\nTrack Proceedings.\nAnh Tuan Nguyen, Michael Hilton, Mihai Codoban,\nHoan Anh Nguyen, Lily Mast, Eli Rademacher,\nTien N. Nguyen, and Danny Dig. 2016. Api code\nrecommendation using statistical learning from ﬁne-\ngrained changes. In Proceedings of the 2016 24th\nACM SIGSOFT International Symposium on Foun-\ndations of Software Engineering , FSE 2016, page\n511–522, New York, NY , USA. Association for\nComputing Machinery.\nAnh Tuan Nguyen and Tien N. Nguyen. 2015. Graph-\nbased statistical language model for code. In Pro-\nceedings of the 37th International Conference on\n3435\nSoftware Engineering - Volume 1 , ICSE ’15, page\n858–868. IEEE Press.\nTrong Duc Nguyen, Anh Tuan Nguyen, Hung Dang\nPhan, and Tien N. Nguyen. 2017. Exploring API\nembedding for API usages and applications. In Pro-\nceedings of the 39th International Conference on\nSoftware Engineering, ICSE 2017, Buenos Aires, Ar-\ngentina, May 20-28, 2017 , pages 438–449. IEEE /\nACM.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. Bpe-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2020, Online, July 5-10, 2020 , pages\n1882–1892. Association for Computational Linguis-\ntics.\nVeselin Raychev, Martin Vechev, and Eran Yahav. 2014.\nCode completion with statistical language models.\n49(6):419–428.\nShuo Ren, Yu Wu, Shujie Liu, Ming Zhou, and Shuai\nMa. 2019. Explicit cross-lingual pre-training for un-\nsupervised machine translation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 770–779. Association for Com-\nputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nRaphael Shu and Hideki Nakayama. 2018. Improving\nbeam search by removing monotonic constraint for\nneural machine translation. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 339–344, Melbourne, Australia. Association\nfor Computational Linguistics.\nArda Tezcan, Véronique Hoste, and Lieve Macken.\n2020. Estimating word-level quality of statistical\nmachine translation output using monolingual infor-\nmation alone. Nat. Lang. Eng., 26(1):73–94.\nJue Wang, Yingnong Dang, Hongyu Zhang, Kai Chen,\nTao Xie, and Dongmei Zhang. 2013. Mining suc-\ncinct and high-coverage API usage patterns from\nsource code. In Proceedings of the 10th Working\nConference on Mining Software Repositories, MSR\n’13, San Francisco, CA, USA, May 18-19, 2013 ,\npages 319–328. IEEE Computer Society.\nMartin White, Christopher Vendome, Mario Linares-\nVásquez, and Denys Poshyvanyk. 2015. Toward\ndeep learning software repositories. In Proceedings\nof the 12th Working Conference on Mining Software\nRepositories, MSR ’15, page 334–345. IEEE Press.\nRensong Xie, Xianglong Kong, Lulu Wang, Ying Zhou,\nand Bixin Li. 2019. Hirec: API recommendation us-\ning hierarchical context. In 30th IEEE International\nSymposium on Software Reliability Engineering, IS-\nSRE 2019, Berlin, Germany, October 28-31, 2019 ,\npages 369–379. IEEE.\nJinpei Yan, Yong Qi, Qifan Rao, and Hui He. 2018.\nLearning API suggestion via single LSTM network\nwith deterministic negative sampling. In The 30th\nInternational Conference on Software Engineering\nand Knowledge Engineering, Hotel Pullman, Red-\nwood City, California, USA, July 1-3, 2018 , pages\n137–136. KSI Research Inc. and Knowledge Sys-\ntems Institute Graduate School.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020a. Alternat-\ning language modeling for cross-lingual pre-training.\nIn The Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2020, The Thirty-Second Inno-\nvative Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2020, The Tenth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 9386–9393. AAAI Press.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020b. Alternat-\ning language modeling for cross-lingual pre-training.\nIn The Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence, AAAI 2020, The Thirty-Second Inno-\nvative Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2020, The Tenth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 9386–9393. AAAI Press.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nHaoyu Zhang, Jingjing Cai, Jianjun Xu, and Ji Wang.\n2019. Pretraining-based natural language genera-\ntion for text summarization. In CoNLL, pages 789–\n797. Association for Computational Linguistics.\nHao Zhong, Tao Xie, Lu Zhang, Jian Pei, and Hong\nMei. 2009. Mapo: Mining and recommending api\nusage patterns. In Proceedings of the 23rd European\nConference on ECOOP 2009 — Object-Oriented\nProgramming, Genoa, page 318–343, Berlin, Hei-\ndelberg. Springer-Verlag.\n3436\nAppendix\nA Parameter settings\nSection Approach Hyperparameter Value\nModel\nGPT\nffn_hidden 512\nhidden 256\nnum_head 8\nnum_layer 6\nbatch size 32\nsequence length 512\nlearning rate 0.00015\nepoch(pre-train) 15\nepoch(ﬁne-tune) Early Stop\nLSTM\nhidden 128\nnum_layer 2\nbatch size 128\nsequence length 60\nlearning rate 0.005\nepoch Early Stop\nN-gram\nhidden 300\ncontext size 3\nbatch size 40000\nlearning rate 0.005\nepoch Early Stop\nBeam Search - beam size 20\nmax iteration 10\nTable 6: Parameters of APIRecX and baseline\nB Retrain and pre-train\nOur experiments shows that the “pre-train&ﬁne-\ntune” mechanism is effective and efﬁcient than\nthe one-step training strategies. Table 8 lists\nthe domain API recommendation accuracy of the\nmodel trained in three training strategies. “Pre-\ntrain&ﬁne-tune” represents the strategy used in\ntraining APIRecX introduced in Section 2.3, “re-\ntrain” means training APIRecX from scratch us-\ning three different proportions of ﬁne-tuning data\ncombined with pre-training data in three domains.\nBeam size Sample Top-1 Top-5 Top-10\n0.2% 38.4 71.3 76.5\n10 10% 45.2 79.4 84.3\n100% 58.7 88.1 92.7\n0.2% 38.2 73.1 79.3\n15 10% 46.6 78.6 84.6\n100% 58.8 88.1 93.7\n0.2% 38.2 74.8 81.2\n20 10% 46.9 79.9 85.7\n100% 60.0 89.4 94.5\n0.2% 38.5 74.1 81.5\n25 10% 46.6 83.1 85.7\n100% 59.6 88.7 93.1\n0.2% 38.4 73.5 81.9\n30 10% 45.3 80.1 86.7\n100% 58.8 88.2 93.9\nTable 7: Different beam size results in JDBC domain\nDomain Ratio Strategy Top-1 Top-5 Top-10\nJDBC\n100%\npre-train&ﬁne-tune 60 89.4 94.5\nretrain 54.5 85.6 91.1\nscratch 52.9 85.4 91.9\n10%\npre-train&ﬁne-tune 46.9 79.9 85.7\nretrain 42.1 71.5 79.4\nscratch 30.2 56.9 64.7\n0.2%\npre-train&ﬁne-tune 37.6 75 81.2\nretrain 27.7 50.4 53.1\nscratch 13.2 33.2 33.3\nSwing\n100%\npre-train&ﬁne-tune 54.8 77.2 83.7\nretrain 44.4 71.3 77.6\nscratch 48.8 75.3 80.8\n10%\npre-train&ﬁne-tune 40.6 67.8 74.5\nretrain 33.6 57.9 64.2\nscratch 25.9 50.7 60.6\n0.2%\npre-train&ﬁne-tune 25 43.8 51.2\nretrain 23.6 43.1 47.7\nscratch 3.2 4.9 8.8\nIO\n100%\npre-train&ﬁne-tune 63.9 84.2 88.7\nretrain 62.7 81.9 87.2\nscratch 32.4 62.8 71.4\n10%\npre-train&ﬁne-tune 56.9 75.9 80.5\nretrain 56.9 74.9 79.1\nscratch 0.7 10.2 20.8\n0.2%\npre-train&ﬁne-tune 52.9 69.5 73.7\nretrain 51.7 68.4 71.3\nscratch 0.05 0.05 1.4\nTable 8: Pre-train and retrain result\n“Scratch” means training APIRecX from scratch us-\ning only three different proportions of ﬁne-tuning\ndata. As shown in Table 8, the “pre-train&ﬁne-\ntune” mechanism is better than the other two one-\nstep strategy at three sampling ratios, and proves\nsuperiority under low sampling ratios.\nC Beam Size evaluation\nWe evaluate the effectiveness of different beam\nsize under three different sampling ratios of JDBC\ndomain to ﬁnd the suitable beam size. Table 7\nlists the average recommendation accuracy rates\nachieved in 5 different beam sizes under three dif-\nferent sampling ratios in JDBC domain. Table 7\nshows that, as the beam size increases, the dura-\ntion and the accuracy both increases. After the\nbeam size reaches 20, the accuracy increases rather\nslowly and remains basically unchanged. To bal-\nance the performance and efﬁciency of APIRecX,\nwe set beam size to be 20 as the parameter of other\ncomparative experiments.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8706648349761963
    },
    {
      "name": "Application programming interface",
      "score": 0.5770122408866882
    },
    {
      "name": "World Wide Web",
      "score": 0.5061318278312683
    },
    {
      "name": "Software",
      "score": 0.5028514266014099
    },
    {
      "name": "Information retrieval",
      "score": 0.32214978337287903
    },
    {
      "name": "Programming language",
      "score": 0.2654607892036438
    }
  ]
}