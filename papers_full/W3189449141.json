{
  "title": "HiFT: Hierarchical Feature Transformer for Aerial Tracking",
  "url": "https://openalex.org/W3189449141",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2743004273",
      "name": "Cao, Ziang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2013756148",
      "name": "Fu, Changhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120996495",
      "name": "Ye, Junjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2014885863",
      "name": "Li, Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2060314455",
      "name": "Li, Yiming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3167454939",
    "https://openalex.org/W2592463526",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2520477759",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W3179950005",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W3035571898",
    "https://openalex.org/W3092714706",
    "https://openalex.org/W2987346479",
    "https://openalex.org/W2964069521",
    "https://openalex.org/W2776035257",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2898200825",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W3169909246",
    "https://openalex.org/W3108516375",
    "https://openalex.org/W2963227409",
    "https://openalex.org/W3035466700",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2886910176",
    "https://openalex.org/W2599547527",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3207718618",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035511673",
    "https://openalex.org/W2518013266",
    "https://openalex.org/W3168663926",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1612645646",
    "https://openalex.org/W3003412316",
    "https://openalex.org/W2604701225",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W2606221507",
    "https://openalex.org/W2798842862",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2955747520",
    "https://openalex.org/W3102624093",
    "https://openalex.org/W2963030525",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W2927438889"
  ],
  "abstract": "Most existing Siamese-based tracking methods execute the classification and regression of the target object based on the similarity maps. However, they either employ a single map from the last convolutional layer which degrades the localization accuracy in complex scenarios or separately use multiple maps for decision making, introducing intractable computations for aerial mobile platforms. Thus, in this work, we propose an efficient and effective hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical similarity maps generated by multi-level convolutional layers are fed into the feature transformer to achieve the interactive fusion of spatial (shallow layers) and semantics cues (deep layers). Consequently, not only the global contextual information can be raised, facilitating the target search, but also our end-to-end architecture with the transformer can efficiently learn the interdependencies among multi-level features, thereby discovering a tracking-tailored feature space with strong discriminability. Comprehensive evaluations on four aerial benchmarks have proven the effectiveness of HiFT. Real-world tests on the aerial platform have strongly validated its practicability with a real-time speed. Our code is available at https://github.com/vision4robotics/HiFT.",
  "full_text": "HiFT: Hierarchical Feature Transformer for Aerial Tracking\nZiang Cao‚Ä†, Changhong Fu‚Ä†,*, Junjie Ye‚Ä†, Bowen Li‚Ä†, and Yiming Li‚Ä°\n‚Ä†Tongji University ‚Ä°New York University\ncaoang233@gmail.com, changhongfu@tongji.edu.cn, yimingli@nyu.edu\nAbstract\nMost existing Siamese-based tracking methods execute\nthe classiÔ¨Åcation and regression of the target object based\non the similarity maps. However, they either employ a sin-\ngle map from the last convolutional layer which degrades\nthe localization accuracy in complex scenarios or sepa-\nrately use multiple maps for decision making, introduc-\ning intractable computations for aerial mobile platforms.\nThus, in this work, we propose an efÔ¨Åcient and effective\nhierarchical feature transformer (HiFT) for aerial track-\ning. Hierarchical similarity maps generated by multi-level\nconvolutional layers are fed into the feature transformer\nto achieve the interactive fusion of spatial (shallow layers)\nand semantics cues (deep layers). Consequently, not only\nthe global contextual information can be raised, facilitat-\ning the target search, but also our end-to-end architecture\nwith the transformer can efÔ¨Åciently learn the interdepen-\ndencies among multi-level features, thereby discovering a\ntracking-tailored feature space with strong discriminabil-\nity. Comprehensive evaluations on four aerial benchmarks\nhave proven the effectiveness of HiFT. Real-world tests on\nthe aerial platform have strongly validated its practicability\nwith a real-time speed. Our code is available at https:\n//github.com/vision4robotics/HiFT.\n1. Introduction\nVisual object tracking1, aiming to estimate the location\nof object frame by frame given the initial state, has drawn\nconsiderable attention due to its prosperous applications es-\npecially for unmanned aerial vehicles (UA Vs), e.g., aerial\ncinematography [5], visual localization [48], and collision\nwarning [19]. Despite the impressive progress, efÔ¨Åcient and\neffective aerial tracking remains a challenging task due to\nlimited computational resources and various difÔ¨Åculties like\nfast motion, low-resolution, frequent occlusion, etc.\nIn the visual tracking community, deep learning (DL)-\nbased trackers [44, 35, 9, 2, 31, 53, 18, 17, 6] stand out on\n*Corresponding Author\n1This work targets single object tracking (SOT).\n#0247\n#0015\n#0262\n #0291\n#0167\n #0182\nHiFT (ours) SiamCAR SiamRPN++_Res\n#0270\n #0321\n #0338\nSiamBAN\nFigure 1. Qualitative comparison of the proposed HiFT with state-\nof-the-arts [23, 8, 31] on three challenging sequences ( BMX4,\nRaceCar1 from DTB70 [34], andCar16 from UA V20L [39]). Ow-\ning to the effective tracking-tailored feature space produced by\nthe hierarchical feature transformer, our HiFT tracker can achieve\nrobust performance under various challenges with a satisfactory\ntracking speed while other trackers lose effectiveness.\naccount of using the convolutional neural network (CNN)\nwith robust representation capability. However, lightweight\nCNNs like AlexNet [30] can hardly extract robust features\nwhich are vital for tracking performance in complex aerial\nscenarios. Using a larger kernel size or a deeper back-\nbone [31] can alleviate the aforementioned shortcoming yet\nthe efÔ¨Åciency and practicability will be sacriÔ¨Åced. In liter-\nature, the dilated convolution [49] proposed to expand the\nreceptive Ô¨Åeld and avoid the loss of resolution caused by\nthe pooling layer. Unfortunately, it still suffers from unsta-\nble performance while handling small objects.\nRecently, the transformer has demonstrated huge poten-\ntial in many domains with an encoder-decoder structure [1].\nInspired by the superior performance of the transformer in\nmodeling global relationships, we try to exploit its architec-\nture in aerial tracking to effectively fuse multi-level 2 fea-\ntures to achieve promising performance. Meanwhile, the\nloss of efÔ¨Åciency caused by the computations of multiple\nlayers and the deÔ¨Åciency of the transformer in handling\n2We use the hierarchical feature to denote the feature maps from mul-\ntiple convolutional layers.\narXiv:2108.00202v3  [cs.CV]  2 Oct 2021\nsmall objects (pointed out in [52]) can be mitigated simul-\ntaneously.\nIn speciÔ¨Åc, since the target object in visual tracking can\nbe an arbitrary object, the learned object queries in the orig-\ninal transformer structure hardly generalize well in visual\ntracking. Therefore, we adopt low-resolution features from\nthe deeper layer to replace object queries. Meantime, we\nalso feed the shallow layers into the transformer to discover\na tracking-tailored feature space with strong discriminabil-\nity by end-to-end training, which implicitly models the re-\nlationship of spatial information from high-resolution lay-\ners and semantic cues from low-resolution layers. More-\nover, to further handle the insufÔ¨Åciency faced with low-\nresolution objects [52], we design a novel feature modu-\nlation layer in the transformer to fully explore the interde-\npendencies among multi-level features. The proposed hier-\narchical feature transformer (HiFT) tracker has efÔ¨Åciently\nachieved robust performance under complex scenarios, as\nshown in Fig. 1. The main contributions of this work are as\nfollows:\n‚Ä¢ We propose a novel hierarchical feature transformer\nto learn relationships amongst multi-level features,\nthereby discovering a tracking-tailored feature space\nwith strong discriminability for aerial tracking.\n‚Ä¢ We design a neat feature modulation layer and classiÔ¨Å-\ncation label to further exploit the hierarchical features\nin Siamese networks and improve the tracking accu-\nracy in handling the small objects.\n‚Ä¢ Comprehensive evaluations on four authoritative aerial\nbenchmarks have validated the promising performance\nof HiFT against other state-of-the-art (SOTA) trackers,\neven those equipped with deeper backbones.\n‚Ä¢ Real-world tests are conducted on a typical aerial plat-\nform, demonstrating the superior efÔ¨Åciency and effec-\ntiveness of HiFT in real-world scenarios.\n2. Related Works\n2.1. Visual Tracking Methods\nAfter MOSSE [4], a variety of achievements have been\nwitnessed in handcrafted discriminative correlation Ô¨Ålter\n(DCF)-based trackers [21, 36, 29, 9]. By calculating in\nthe Fourier domain, DCF-based trackers can achieve com-\npetitive performance with high efÔ¨Åciency [20]. Neverthe-\nless, those trackers hardly maintain robustness under vari-\nous tracking conditions due to the poor representation abil-\nity of the handcrafted feature. To improve the tracking per-\nformance, several works introducing deep learning to DCF-\nbased methods have been released [9, 50, 35]. Despite the\ngreat progress, they are still faced with inferior robustness\nand efÔ¨Åciency for aerial tracking.\nAnother outstanding branch in the SOT community is the\nSiamese-based methods [2, 24, 32, 53, 31], which beneÔ¨Åt\nfrom massive ofÔ¨Çine training data and end-to-end learning\nstrategy. As one of the pioneering works, SiameseFC [2] ex-\nposed the advantage of the Siamese framework, formulating\nthe tracking task as the similarity matching process of tem-\nplate and search patches. Based on SiameseFC, DSiam [24]\nwas proposed to effectively handle the object appearance\nvariation and background interference. Inspired by region\nproposal network (RPN) [40], SiamRPN [32] considered\ntracking as two subtasks, applying the classiÔ¨Åcation and\nregression branches respectively. DaSiamRPN [53] intro-\nduced a novel distractor-aware module and an effective\nsampling strategy, further promoting its robustness. More\nrecently, the potential of adopting very deep networks as\nthe backbone is extensively tapped [31], while the efÔ¨Å-\nciency is sacriÔ¨Åced largely. Obviously, RPN-based track-\ners [32, 53, 31] provide an effective tracking strategy. How-\never, the hyper-parameters associated with anchors signif-\nicantly decrease the generalization of trackers. In order to\neliminate such a drawback, the anchor-free method is pro-\nposed [23, 8].\nIn Siamese-based trackers, robust features make a vi-\ntal inÔ¨Çuence on tracking performance. However, the\ntrackers [2, 32, 53, 18] with lightweight backbone like\nAlexNet [30] suffer from the lack of global context while\nthe trackers [31, 8, 23] utilizing deep CNN like ResNet [25]\nare far from real-time requirements onboard UA V . Al-\nbeit several works proposed to explore multi-level features\nin visual tracking [31, 16], they introduce cumbersome\ncomputation inevitably which is unaffordable for mobile\nplatforms. Differently, this work proposes a brand-new\nlightweight hierarchical feature transformer (HiFT) for ef-\nfective and efÔ¨Åcient multi-level feature fusion, achieving ro-\nbust aerial tracking efÔ¨Åciently.\n2.2. Transformer in Computer Vision\nVaswani et al. [1] Ô¨Årstly proposed the transformers\nfor machine translation based on the attention mecha-\nnism. BeneÔ¨Åting from its high representation ability, the\ntransformer structure is expanded to the domain of com-\nputer vision such as video captioning [51], image enhance-\nment [47], and pose estimation [27]. After DETR [7] ini-\ntiates the research of transformer in object detection, de-\nformable DETR [52] proposed the deformable attention\nmodule for efÔ¨Åciently convergence, providing inspirations\nabout the combination of transformer and CNN. Some stud-\nies attempted to introduce the transformer to multi-object\ntracking and achieved promising performance [38], while\nthe study of transformer in SOT is still blocked so far.\nAlthough the attention mechanism in the transformer\nshows good performance in extensive visual tasks, its su-\nperiority struggles to be extended to SOT, since predeÔ¨Åned\nConv & Correlation  \nTemplate patch\nBackbone\nùêåùüë\n‚Ä≤\nùêå4\nFeature \nencoder\n#k\nClassification &\nRegression\nSearch patch\n#k\n#1\nPositional encoding\nTransformed features\nResult\nElement-wise sum\nFeature \ndecoder\nùêå3\nùêåùüí\n‚Ä≤\nùêå5\nFigure 2. Overview of the HiFT tracker. The modules from the left to right are feature extraction network, hierarchical feature transformer,\nand classiÔ¨Åcation & regression network. Three arrows with different colors represent the workÔ¨Çow of features from different layers\nrespectively. Note that only the input of the encoder is combined with position encoding. Best viewed in color. (Image frames are from\nUA V20L [39].)\n(or learned) object queries hardly maintain effectiveness\nwhen facing an arbitrary object. Moreover, the transformer\nhardly deals with the low-resolution object which is fre-\nquently encountered in aerial tracking. In this work, in-\nstead of redesigning object queries and related structures,\nwe propose a hierarchical feature transformer to construct-\ning a novel as well as robust tracking-tailored feature space.\nBy virtue of the introduction of global context and interde-\npendencies among multi-level features, the discriminabil-\nity in the feature space is signiÔ¨Åcantly raised to improve\nthe tracking performance. Meanwhile, HiFT possesses a\nlightweight encoder-decoder structure which is desirable\nfor mobile platforms.\n3. Proposed Method\nThe workÔ¨Çow of HiFT is presented in Fig. 2. It can be di-\nvided into three submodules, feature extraction network, hi-\nerarchical feature transformer, and classiÔ¨Åcation & regres-\nsion network. Note that we utilize features from the last\nthree layers to build the hierarchical feature transformation\nin this paper.\n3.1. Feature Extraction Network\nDeep CNNs, e.g., ResNet [25], MobileNet [42], and\nGoogLeNet [43], have demonstrated their surprising capa-\nbility, serving as popular feature extraction backbones in\nSiamese frameworks [31]. However, the heavy computa-\ntion brought by the deep structure hardly be afforded by the\naerial platform. To this concern, HiFT adopts a lightweight\nbackbone, i.e., AlexNet [30], which serves in both template\nand search branches. For clarity, the template/search images\nare respectively denoted by Z and X. œÜk(X) represents the\nk-th layer output of the search branch.\nRemark 1: Despite the weaker feature extraction capabil-\nity of AlexNet compared with those deeper networks, the\nproposed feature transformer can make up such a drawback\nsigniÔ¨Åcantly, at the same time saving computation resources\nfor real-time aerial tracking.\n3.2. Hierarchical Feature Transformer\nThe proposed hierarchical feature transformer can be\nmainly divided into two steps: high-resolution features en-\ncoding and low-resolution feature decoding. The former\naims at learning interdependencies between different fea-\nture layers and spatial information to raise attention to ob-\njects with different scales (especially low-resolution ob-\njects). While the latter aggregates the semantic information\nfrom the low-resolution feature map. BeneÔ¨Åting from the\nabundant global context and interdependencies among hier-\narchical features, our method discovers a tracking-tailored\nfeature space. Thus, the discriminability and representa-\ntive capabilities of transformed features under various aerial\ntracking conditions are raised signiÔ¨Åcantly. SpeciÔ¨Åcally,\nfeatures from the last three layers are utilized. The fea-\nture map from k-th layer is convoluted and reshaped to\nMi ‚àà RWH √óC (C, W, Hrepresents the channel, width,\nand height of the feature map respectively) before being fed\ninto the feature transformer:\nMi = F(œÜi(Z) ‚ãÜœÜi(X)) ,i = 3,4,5 , (1)\nwhere Fdenotes the convolution layer and ‚ãÜrepresents the\ncross-correlation operator. Then, M‚Ä≤\n3 ‚àà RWH √óC and\nM‚Ä≤\n4 ‚àà RWH √óC can be obtained by supplementing with\na learnable positional encoding.\n3.2.1 Feature Encoding\nTo fully explore the interdependencies between hierarchi-\ncal features, we use the combination of M‚Ä≤\n3 and M‚Ä≤\n4 as\nthe input of multi-head attention module [1] as M1\nE =\nNorm(M‚Ä≤\n3 + M‚Ä≤\n4), where Norm represents the normaliza-\ntion layer. Generally, the scaled dot-product attention Att\ncan be expressed by:\nAtt(Q,K,V) = Softmax(QKT\n‚àöc )V , (2)\nwhere ‚àöcis the scaling factor to avoid gradient vanishment\nin the softmax function. Then the calculation process of the\nmulti-head attention module mAtt is expressed as:\nmAtt(Q,K,V) =\n(\nCat(a1,...,a N )\n)\nWc\naj = Att(QWj\n1,KWj\n2,VWj\n3)\n, (3)\nwhere Wc ‚àà RC√óC, Wj\n1 ‚àà RC√óCd , Wj\n2 ‚àà RC√óCd ,\nand Wj\n3 ‚àà RC√óCd (Cd=C/N, N is the number of parallel\nattention head) can all be regarded as fully connected layer\noperation. Please note that Q,K,V are only mathemati-\ncal symbols to clarify the function. Therefore, they do not\nhave practical meanings. Afterwards, the output of the Ô¨Årst\nmulti-head attention module, i.e., M2\nE ‚àà RWH √óC, can be\nobtained by:\nM2\nE = mAtt(M1\nE,M1\nE,M‚Ä≤\n3) . (4)\nAs a result, the interdependencies between M‚Ä≤\n3 and M‚Ä≤\n4\nare effectively learned to enrich the high-resolution feature\nmap M2\nE. Besides, the global context in the two feature\nmaps is also introduced in M2\nE. After that, we construct\nthe modulation layer to fully explore the potential of in-\nterdependencies between M3\nE and M‚Ä≤\n4 whose structure is\nshown in Fig. 3. SpeciÔ¨Åcally, the input of modulation layer\nM3\nE is obtained by normalization of M‚Ä≤\n3 and M2\nE, i.e.,\nM3\nE = Norm(M‚Ä≤\n3 + M2\nE). After a feed-forward network\n(FFN) and global average pooling operation (GAP), the out-\nput of modulation layer M4\nE can be formulated as:\nW‚Ä≤ = F(Cat(M3\nE,M‚Ä≤\n4)) ‚àóFFN(GAP(M‚Ä≤\n4))\nM4\nE = M3\nE + Œ≥1 ‚àóW‚Ä≤ ‚àóM3\nE\n, (5)\nwhere Œ≥1 represents a learning weight.\nOwing to the modulation layer, the internal spatial in-\nformation between M‚Ä≤\n4 and M3\nE are exploited efÔ¨Åciently,\nthereby effectively distinguishing the object from the com-\nplex background. Eventually, the encoded information can\nbe calculated through FFN and normalization.\nRemark 2: Attributing to the feature encoder, the global\ncontext and interdependencies between M‚Ä≤\n3 and M‚Ä≤\n4 are\nfully explored. Additionally, to overcome the deÔ¨Åciency\nof handling small objects, the modulation layer is proposed\nto further explore spatial information for enriching the en-\ncoded information. Finally, based on it, the decoder can\nbuild an effective feature transformation for robust tracking.\n3.2.2 Feature Decoding\nBefore decoding, the low-resolution feature map is Ô¨Årstly\nreshaped to M5 ‚ààRWH √óC in Eq. (1). The feature decoder\nfollows the similar structure of standard transformer [1].\nDifferently, we build the effective feature decoder without\nposition encoding. Since we treat the number of locations as\nMulti-Head Attention\nAdd & Norm\nFFN\nAdd & Norm\nCat & Conv\nAdd & Norm\nMulti-Head Attention\nAdd & Norm\nMulti-Head Attention\nQKV\nAdd & Norm\nFFN\nAdd & Norm\nFeature encoder\nFeature decoder\nGAP & FFN \nModulation layer\nTransformed feature maps\nùêå3\n‚Ä≤ ùêå4\n‚Ä≤ ùêå5\nElement-wise sumChannel-wise multiplication\nQKV\nQKV\nFigure 3. Detailed workÔ¨Çow of HiFT. The left sub-window illus-\ntrates the feature encoder. The right one shows the structure of the\ndecoder. Best viewed in color.\nthe sequence length in our method, the position encoding is\nintroduced to distinguish each location on feature maps. For\navoiding the direct inÔ¨Çuence on the transformed feature, we\ndecide to introduce the position information through the en-\ncoder implicitly. Analysis of the positional encoding strat-\negy is conducted later in Sec. 4.3.3. The structure of the\ndecoder is exhibited in Fig. 3.\nRemark 3: By the hierarchical feature transformer, the spa-\ntial/semantic information in the high-/low-resolution fea-\ntures is fully utilized to improve the discriminability of the\nÔ¨Ånal transformed feature. Meanwhile, the modulation layer\nachieves the aggregation of interdependencies among dif-\nferent feature layers, enhancing the robustness of tracking\nobjects with various scales.\n3.3. DeÔ¨Ånition of ClassiÔ¨Åcation Label\nThe structures of classiÔ¨Åcation and regression are imple-\nmented by several convolution layers. To achieve accurate\nclassiÔ¨Åcation, we apply two classiÔ¨Åcation branches. One\nbranch aims to classify via the area involved in the ground\ntruth box. The other branch focuses on determining the pos-\nitive samples measured by the distance between the center\nof ground truth and the corresponding point. Besides, to\naccelerate the convergence, we use pseudo-random number\ngenerators denoted asTto constrain the number of negative\nlabels.\nRemark 4: The detailed calculation process of classiÔ¨Åcation\nand regression can be found in the supplementary material.\nTherefore, the overall loss function can be determined\nas:\nLoverall = Œª1Lcls1 + Œª2Lcls2 + Œª3Lloc , (6)\nwhere Lcls1, Lcls2, Lloc represent the cross-entropy, binary\ncross-entropy, and IoU loss. Œª1, Œª2, and Œª3 are the coefÔ¨Å-\ncients to balance the contributions of each loss.\n4. Experiments\n4.1. Implementation Details\nDuring the training of 70 epochs, the last three layers\nof AlexNet are Ô¨Åne-tuned in the last 60 epochs while the\nÔ¨Årst two layers are frozen. The learning rate is initialized as\n5√ó10‚àí4 and decreased in the log space from 10‚àí2 to 10‚àí4.\nBesides, the sizes of Z and X are set to 3 √ó127 √ó127 and\n3√ó287√ó287 respectively. The feature transformer consists\nof one encoder layer and two decoder layers. We use image\npairs extracted from COCO [37], ImageNet VID [41], GOT-\n10K [28], and Youtube-BB [15] to train HiFT. In addition,\nthe stochastic gradient descent (SGD) is adopted, and batch\nsize, momentum, and weight decay are set to 220, 0.9, and\n10‚àí4, respectively. Our tracker is trained on a PC with an\nIntel i9-9920X CPU, a 32GB RAM, and two NVIDIA TI-\nTAN RTX GPUs. More experimental results can be found\nin the supplementary.\n4.2. Evaluation Metrics\nThe one-pass evaluation (OPE) metrics [39] including\nprecision and success rate are applied to assess the track-\ning performance. SpeciÔ¨Åcally, the success rate is measured\nby the intersection over union (IoU) of the ground truth and\nestimated bounding boxes. The percentage of frames whose\nIoU is beyond a pre-deÔ¨Åned threshold is drawn as the suc-\ncess plot (SP). Besides, the center location error (CLE) be-\ntween the estimated location and the ground truth is em-\nployed to evaluate the precision. The percentage of frames\nwhose CLE is within a certain threshold is drawn as the\nprecision plot (PP). Meanwhile, the area under the curve\n(AUC) of the SP and the precision at a threshold of 20 pix-\nels is adopted to rank the trackers.\n4.3. Evaluation on Aerial Benchmarks\n4.3.1 Overall Performance\nFor overall evaluation, HiFT is tested on four challenging\nand authoritative aerial tracking benchmarks, and compre-\nhensively compared with other 19 state-of-the-art (SOTA)\ntrackers including SiamRPN++ [31], DaSiamRPN [53],\nUDT [44], UDT+ [44], TADT [35], CoKCF [50],\nARCF [29], AutoTrack [36], ECO [9], C-COT [13],\nMCCT [45], DeepSTRCF [33], STRCF [33], BACF [21],\nSRDCF [11], fDSST [12], SiameseFC [2], DSiam [24],\nand KCF [26]. For fairness, all the Siamese-based track-\ners adopt the same backbone, i.e., AlexNet [30] pre-trained\non ImageNet [41].\nUA V123 [39]: UA V123 is a large-scale UA V benchmark\nincluding 123 high-quality sequences with more than 112K\nframes which covers a variety of challenging aerial scenar-\nios such as frequent occlusion, low resolution, out-of-view,\netc. Therefore, UA V123 can help to exhaustively assess\ntracking performance in aerial tracking. As illustrated in\nTable 1, HiFT outperforms other trackers in both precision\nand success. In terms of precision, HiFT gains Ô¨Årst place\nwith a precision score of 0.787, surpassing the second-\nand third-best SiamRPN++ (0.769) and ECO (0.752) by\n2.3% and 4.7% respectively. As for the success rate, HiFT\n(0.589) also improves over SiamRPN++ (0.579) and ranks\nÔ¨Årst place. In a word, HiFT demonstrates superior perfor-\nmance in all kinds of aerial tracking scenarios.\nUA V20L [39]:UA V20L is composed of 20 long-term track-\ning sequences with 2934 frames on average and over 58 K\nframes in total. In this paper, it is utilized to evaluate our\ntracker in realistic long-term aerial tracking scenes. As pre-\nsented in Table 2, attributing to the global contextual infor-\nmation introduced by the feature transformer, our tracker\nachieves competitive performance compared to other SOTA\ntrackers. SpeciÔ¨Åcally, HiFT yields the best precision score\n(0.763), surpassing the second-best SiamRPN++ (0.696)\nand the third-best DaSiamRPN (0.665) by 9.6% and\n14.7%. Similarly, in success rate, HiFT achieves the best\nscore (0.566), followed by SiamRPN++ (0.528) and DaSi-\namRPN (0.465). The extraordinary performance veriÔ¨Åes\nthat HiFT could be a desirable choice in long-term aerial\ntracking scenarios.\nDTB70 [34]: Compared to the aforementioned two bench-\nmarks, DTB70 contains 70 challenging UA V sequences\nwith a large number of severe motion scenes. The robust-\nness of trackers in fast motion scenarios could be appropri-\nately evaluated on this benchmark. Experimental results are\nshown in Fig. 4a, HiFT ranks Ô¨Årst place in both precision\n(0.802) and success rate (0.594), followed by SiamRPN++\nTable 1. Quantitative evluation on UA V123 [39]. The top three\nperformances are respectively highlighted byred, green, and blue\ncolor. Prec. and Succ. respectively denote precision score at 20\npixels and AUC of success plot.\nTrackers Prec. Succ. Trackers Prec. Succ.\nAutoTrack [36] 0.689 0.472 C-COT [13] 0.729 0.502\nARCF [29] 0.671 0.468 UDT+ [44] 0.732 0.502\nSTRCF [33] 0.681 0.481 UDT [44] 0.668 0.477\nfDSST [12] 0.583 0.405 TADT [35] 0.727 0.520\nSRDCF [11] 0.676 0.463 DeepSTRCF [33] 0.705 0.508\nCoKCF [50] 0.652 0.399 MCCT [45] 0.734 0.507\nKCF [26] 0.523 0.331 DSiam [24] 0.608 0.400\nBACF [21] 0.662 0.461 ECO [9] 0.752 0.528\nSiamRPN++ [31]0.769 0.579 SiameseFC [2] 0.725 0.494\nDaSiamRPN [53] 0.725 0.501HiFT (ours) 0.787 0.589\nTable 2. Overall evaluation on UA V20L [39]. The top nine track-\ners are reported.The top three trackers are respectively marked by\nred, green, and blue color. Prec. and Succ. respectively denote\nprecision score at 20 pixels and AUC of success plot.\nUDT+ ECO TADT DeepST- Siames- DSiam DaSiam SiamRP-HiFT\n[44] [9] [35] RCF [33] eFC [2] [24] RPN [53] N++ [31](ours)\nPrec. 0.585 0.589 0.609 0.588 0.599 0.6030.665 0.696 0.763\nSucc. 0.401 0.427 0.459 0.443 0.402 0.3910.465 0.528 0.566\n0 10 20 30 40 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nPrecision\nPrecision plots\nHiFT [0.802]\nSiamRPN++ [0.795]\nC-COT [0.769]\nDeepSTRCF [0.734]\nMCCT [0.725]\nECO [0.722]\nSiameseFC [0.719]\nAutoTrack [0.716]\nARCF [0.694]\nDaSiamRPN [0.694]\nTADT [0.693]\nUDT+ [0.658]\nSTRCF [0.649]\nUDT [0.602]\nCoKCF [0.599]\nBACF [0.590]\nfDSST [0.534]\nSRDCF [0.512]\nDSiam [0.495]\nKCF [0.468]\n0 0.2 0.4 0.6 0.8 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSuccess rate\nSuccess plots\nHiFT [0.594]\nSiamRPN++ [0.589]\nC-COT [0.517]\nDeepSTRCF [0.506]\nECO [0.502]\nMCCT [0.484]\nSiameseFC [0.483]\nAutoTrack [0.478]\nARCF [0.472]\nDaSiamRPN [0.472]\nTADT [0.464]\nUDT+ [0.462]\nSTRCF [0.437]\nUDT [0.422]\nBACF [0.402]\nCoKCF [0.378]\nSRDCF [0.363]\nfDSST [0.357]\nDSiam [0.337]\nKCF [0.280]\n(a) Results on DTB70.\n0 10 20 30 40 50\nLocation error threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Precision\nPrecision plots\nHiFT [0.754]\nSiamRPN++ [0.740]\nECO [0.709]\nC-COT [0.704]\nDaSiamRPN [0.689]\nTADT [0.684]\nMCCT [0.681]\nDeepSTRCF [0.680]\nSiameseFC [0.678]\nAutoTrack [0.676]\nUDT+ [0.673]\nARCF [0.663]\nSTRCF [0.632]\nDSiam [0.622]\nCoKCF [0.612]\nSRDCF [0.578]\nBACF [0.576]\nUDT [0.572]\nfDSST [0.518]\nKCF [0.409]\n0 0.2 0.4 0.6 0.8 1\nOverlap threshold\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Success rate\nSuccess plots\nHiFT [0.574]\nSiamRPN++ [0.555]\nECO [0.519]\nTADT [0.507]\nC-COT [0.502]\nDeepSTRCF [0.499]\nMCCT [0.492]\nDaSiamRPN [0.481]\nAutoTrack [0.481]\nUDT+ [0.478]\nSiameseFC [0.472]\nARCF [0.471]\nSTRCF [0.460]\nUDT [0.428]\nSRDCF [0.426]\nDSiam [0.425]\nBACF [0.416]\nCoKCF [0.387]\nfDSST [0.381]\nKCF [0.267] (b) Results on UA V123@10fps.\nFigure 4. PPs and SPs of HiFT and other SOTA trackers on (a) DTB70 and (b) UA V123@10fps. Our tracker achieves superior performance\nin the two benchmarks\nTable 3. Average evaluation on four aerial tracking benchmarks. Our tracker outperforms all other trackers with an obvious improvement.\nThe best three performances are respectively highlighted with red, green, and blue color.\nTrackers\nHiFT SiamRPN++ DaSiamRPN AutoTrack ARCF C-COT SiameseFC UDT+ TADT DeepSTRCF MCCT ECO\n(ours) [31] [53] [36] [29] [13] [2] [44] [35] [33] [45] [9]\nAvg. Prec. 0.776 0.750 0.693 0.648 0.643 0.691 0.680 0.662 0.678 0.677 0.686 0.693\nAvg. Succ. 0.581 0.563 0.480 0.445 0.448 0.479 0.463 0.461 0.488 0.489 0.472 0.494\nwith a precision of 0.795 and a success rate of 0.589. The\npromising ability of HiFT in handling fast motion can be\nattributed to the proposed hierarchical feature transformer\nwhich is able to promote the discrimination ability of HiFT.\nUA V123@10fps [39]:UA V123@10fps is created by down-\nsampling from the original 30FPS recording. Consequently,\nthe issue of strong motion in UA V123@10fps is more se-\nvere compared to UA V123. The PPs and SPs shown in\nFig. 4b demonstrate that HiFT can consistently obtain sat-\nisfactory performance, achieving the best precision (0.754)\nand success rate (0.574). To sum up, HiFT provides a more\nstable performance comparing to other SOTA trackers, veri-\nfying its favorable robustness in various aerial tracking sce-\nnarios.\nRemark 5: Table 3 reports the average precision and suc-\ncess rate of the top 11 trackers on four benchmarks. It\nshows that HiFT has improved the second-best tracker\nSiamRPN++ by 3.5% and 3.2% in precision and success\nrate respectively.\nTable 4. Attribute-based evaluation of top 6 trackers on four bench-\nmarks. The best two performances are respectively highlighted by\nred and green color. HiFT keeps achieving the best performance\nin different attributes. ‚àÜ denotes the improvement in comparison\nwith the second best tracker.\nAttributes Low-resolution Scale variation Occlusion Fast motion\nTrackers Prec. Succ. Prec. Succ. Prec. Succ. Prec. Succ.\nSiamRPN++ 0.5910.390 0.728 0.559 0.601 0.405 0.680 0.489\nDaSiamRPN 0.592 0.347 0.678 0.482 0.583 0.361 0.617 0.409\nC-COT 0.586 0.331 0.643 0.451 0.571 0.359 0.644 0.411\nTADT 0.604 0.366 0.632 0.466 0.598 0.387 0.628 0.412\nECO 0.581 0.343 0.644 0.471 0.583 0.375 0.620 0.407\nHiFT (ours) 0.626 0.416 0.772 0.584 0.638 0.431 0.751 0.537\n‚àÜ(%) 3.63 6.81 5.98 4.40 6.20 6.43 10.42 9.79\n4.3.2 Attribute-based Comparison\nTo exhaustively evaluate HiFT under various challenges,\nattribute-based comparisons are conducted, seen in Table 4.\nHiFT ranks Ô¨Årst place in terms of both precision and success\nrate in comparison with other top 5 trackers. SpeciÔ¨Åcally,\nHiFT signiÔ¨Åcantly exceeds the second-best performance in\nattributes of low-resolution, scale variation, occlusion, and\nfast motion. HiFT improves the second-best performance\nby around 10% in fast motion scenarios. The satisfactory\nresults demonstrate that our hierarchical feature transformer\ncan help exploit the global contextual information to over-\ncome issues of severe motion. In addition, when the ob-\njects are severely occluded, HiFT can learn more robust fea-\ntures to discriminate the occluded objects. Therefore, HiFT\nalso yields prominent improvement in the scenarios of oc-\nclusion. Moreover, since the multi-scale feature maps are\nutilized for building the feature transformation, our tracker\nis endowed with the ability to track objects with various\nscales, as veriÔ¨Åed by its performance in the attributes of\nlow-resolution and scale variation.\n4.3.3 Ablation Study\nTo verify the effectiveness of each module of the proposed\nmethod, detailed studies amongst HiFT with different mod-\nules enabled are conducted on UA V20L.\nSymbol introduction: For clarity, we Ô¨Årst introduce the\nmeaning of symbols used in Table 5. This work consid-\ners Baseline as the model with only feature extraction\nand regression & classiÔ¨Åcation network. OT denotes origi-\nnal standard transformer (with object query). FT indicates\nthe original transformer with the feature map (instead of ob-\nject query) but without the proposed modulation layer.HFT\nTable 5. Ablation study of different components of HiFT. For the\ndetailed explanation of Baseline, OT, FT, HFT, PE, and RL, please\nkindly refer to the text in Sec. 4.3.3. ‚àÜ denotes the improvement\ncompared with the Baseline tracker.\nTrackers Precision ‚àÜpre(%) Success ‚àÜsuc(%)\nBaseline 0.611 ‚Äì 0.463 ‚Äì\nBaseline+OT 0.597 -2.29 0.446 -3.67\nBaseline+FT 0.675 +10.47 0.496 +7.13\nBaseline+HFT+PE 0.689 +12.77 0.523 +12.96\nBaseline+HFT+RL 0.629 +2.95 0.486 +4.97\nBaseline+HFT (HiFT) 0.763 +24.88 0.566 +22.25\ndenotes the full version of the proposed hierarchical fea-\nture transformer. PE represents direct positional encoding\nto M5 (HiFT leaves out position encoding inM5 as demon-\nstrated in Sec. 3.2.2). RL represents the rectangle label used\nin the traditional trackers. For fairness, each version of the\ntracker adopts the same training strategy except for the in-\nvestigated module.\nDiscussion on transformer architecture: As shown\nin Table 5, adding the original transformer with object\nqueries (Baseline+OT) directly lowers the performance\nof Baseline by about 2.29% on precision and 3.67% on\nsuccess rate, which proves that object queries hardly per-\nform well in SOT with novel target objects. Replacing\nobject query with the feature map, Baseline+FT raises\ntracking precision by 10.47%. Further adopting the modu-\nlation layer, Baseline+HFT, yields the best performance\nby 24.88%. All the aforementioned results can be com-\nFrames Baseline Baseline+OT HiFT\nAnimal4\nSpeedCar4\nMotor2\ngroup3\nYacht4\ncar8\nFigure 5. Visualization of the conÔ¨Ådence map of three track-\ning methods on several sequences from UA V20L [39] and\nDTB70 [34]. The target objects are marked out by red boxes in\nthe original frames. HiFT gets more robust performance for visual\ntracking in the air.\nbined together to validate the efÔ¨Åcacy of the elaborately de-\nsigned hierarchical feature transformer with the modulation\nlayer in aerial tracking.\nDiscussion on position encoding&classiÔ¨Åcation label:\nThis part aims at proving the 2 strategies, position en-\ncoding in Sec. 3.2.2 and new classiÔ¨Åcation label in\nSec. 3.3. For position decoding, in Table 5, the tracker\nBaseline+HFT+PE hurts the performance of HiFT\ntremendously (from 24.88% improvements to 12.77%),\nproving that direct position encoding is indeed not proper\nfor feature M5. Considering the distance of ground\ntruth and sample points, the circular strategy utilized in\nHiFT achieves a notable improvement (24.88%) compared\nto the traditional rectangle label in Baseline+HFT+RL\n(2.95%).\nRemark 6: Please note that more ablation studies are re-\nported in supplement material.\n4.3.4 Qualitative Evaluation\nAs shown in Fig. 5, the conÔ¨Ådence map of our HiFT tracker\nconsistently focuses on the object under onerous challenges\nin aerial tracking,e.g., fast motion inMotor2, low resolution\nin SpeedCar4, and occlusion in group3 and Yacht4. Despite\nthat the Baseline and Baseline+OT are trained with\nthe same strategy as HiFT, they still fail to concentrate on\nthe target object in those complex tracking scenarios, which\nproves the robustness of the proposed hierarchical feature\ntransformer.\n4.3.5 Comparison to Trackers with Deeper Backbone\nThe proposed hierarchical feature transformer dedicates to\nmodel effective feature mapping among multi-level fea-\ntures, so as to achieve SOTA performance without intro-\nducing a huge computational burden. To further evaluate its\neffectiveness, we employ the trackers equipped with deeper\nbackbones for comparison. The state-of-the-art track-\ners, including SiamRPN++ (ResNet-50) [31], SiamRPN++\n(MobileNet) [31], SiamMask (ResNet-50) [46], ATOM\n(ResNet-18) [10], DiMP (ResNet-50) [3], PrDiMP\n(ResNet-18) [14], SiamCAR (ResNet-50) [23], SiamGAT\n(GoogleNet) [22], and SiamBAN (ResNet-50) [8], are\ninvolved in the comparison. As illustrated in Fig. 6,\nHiFT achieves a satisfactory balance of tracking robust-\nness and speed. On UA V20L, adopting AlexNet as the\nbackbone, HiFT (0.763) surpasses the second-best tracker\nSiamRPN++ ResNet-50 (0.749) in precision and achieves\na speed of 127 FPS, which is 1.8 times faster than the lat-\nter. Similarly, on DTB70, HiFT achieves comparable per-\nformance compared to those deeper CNN-based trackers.\nEventually, the average precision and tracking speed are re-\nported in Table 6, HiFT yields the best average precision\n(0.783) with a promising speed of 129.87 FPS, proving that\n127 FPS\n71 FPS 133 FPS\n74 FPS\nFigure 6. Precision-speed trade-off analysis by quantitative comparison between HiFT and trackers with deeper backbone on UA V20L [39]\n(left) and DTB70 [34] (right). Our method realizes an excellent trade-off on both two benchmarks.\nTable 6. Average precision and tracking speed of HiFT and the trackers with deeper backbone. The proposed approach runs at a satisfactory\nspeed of ‚àº130 FPS, while achieving comparable tracking performance with those trackers equipped with a deeper backbone. The best\nthree performances are respectively highlighted with red, green, and blue color.\nTracker HiFT (ours)SiamGAT [22] SiamCAR [23] SiamBAN [8] PrDiMP [14] SiamRPN++ [31] SiamRPN++ [31] SiamMask [46] ATOM [10]\nBackbone AlexNet GoogleNet ResNet-50 ResNet-50 ResNet-18 ResNet-50 MobileNet ResNet-50 ResNet-18\nAvg. Prec. 0.783 0.751 0.739 0.763 0.741 0.774 0.748 0.740 0.738\nAvg. FPS 129.87 90.01 71.74 73.25 25.94 71.59 115.03 77.30 34.94\nHiFT achieves an excellent balance between tracking per-\nformance and efÔ¨Åciency.\n5. Real-World Tests\nIn this section, HiFT is further implemented on a typi-\ncal UA V platform including an embedded onboard proces-\nsor, i.e., NVIDIA AGX Xavier, to testify its practicability\nin real-world applications. Figure 7 presents three tests in\nthe wild, including day and night scenes. The main chal-\nlenges in the tests are partial occlusion, viewpoint change\n(the Ô¨Årst row), low-resolution, camera motion (the second\nrow), small object, and similar object around (the third\nrow). Attributing to the effective feature transformer, HiFT\nmaintains satisfying tracking robustness in various chal-\nlenging scenarios. Moreover, our tracker remains at an av-\nerage speed of 31.2 FPS during the tests without using Ten-\nsorRT. Therefore, the real-world tests onboard the embed-\nded system directly validate the superior performance and\nefÔ¨Åciency of HiFT under various UA V-speciÔ¨Åc challenges.\n6. Conclusion\nIn this work, a novel hierarchical feature transformer\nfor efÔ¨Åcient aerial tracking is proposed for streamlining the\nprocess of exploiting the global contextual information and\nmulti-level features. By virtue of both low-resolution se-\nmantics information and high-resolution spatial details, the\ntransformed feature can achieve promising performance in\ndiscriminating the object from clutters via a lightweight\nstructure. Meanwhile, attributing to the modulation layer\nand the new classiÔ¨Åcation label, the effectiveness of the fea-\nture transformer can reach its full potential. Comprehensive\nexperiments have validated that HiFT can achieve an ex-\ncellent precision-speed trade-off and can be utilized in real-\nworld aerial tracking scenarios. Moreover, even compared\n0 100 200 300 400 500 600 700\nFrame\n0\n10\n20CLE\n0 100 200 300 400 500 600 700\nFrame\n0\n10\n20CLE\n0 100 200 300 400 500\nFrame\n0\n10\n20CLE\n#0001 #0296 #0549\n#0001 #0217 #0699\n#0001 #0201 #0448\n(#)\n(#)\n(#)\nFigure 7. Visualization of real-world tests on the embedded plat-\nform. The tracking results and ground truth are marked with red\nand green boxes. The CLE score below the blue dotted line is\nconsidered as the success tracking result in the real-world tests.\nto the trackers with deeper backbones, HiFT can achieve\ncomparable performance. We are convinced that our work\ncan advance the development of aerial tracking and promote\nthe real-world applications of visual tracking.\nAcknowledgment: This work is supported by the\nNational Natural Science Foundation of China (No.\n61806148) and the Natural Science Foundation of Shanghai\n(No. 20ZR1460100). We thank the anonymous reviewers\nfor their efforts to help us improve our work.\nReferences\n[1] V . Ashish, S. Noam, P. Niki, U. Jakob, J. Llion, N. G. Aidan,\nK. Lukasz, and P. Illia. Attention Is All You Need. In\nAdvances in neural information processing systems (NIPS) ,\npages 6000‚Äì6010, 2017. 1, 2, 3, 4\n[2] L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and\nP. H. Torr. Fully-Convolutional Siamese Networks for Object\nTracking. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 850‚Äì865, 2016. 1, 2, 5, 6\n[3] G. Bhat, M. Danelljan, L. Van Gool, and R. Timofte. Learn-\ning discriminative model prediction for tracking. InProceed-\nings of the IEEE International Conference on Computer Vi-\nsion (ICCV), pages 6181‚Äì6190, 2019. 7\n[4] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y . M. Lui.\nVisual Object Tracking Using Adaptive Correlation Filters.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 2544‚Äì2550, 2010. 2\n[5] R. Bonatti, C. Ho, W. Wang, S. Choudhury, and S. Scherer.\nTowards a Robust Aerial Cinematography Platform: Local-\nizing and Tracking Moving Targets in Unstructured Envi-\nronments. In Proceedings of the IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages\n229‚Äì236, 2019. 1\n[6] Z. Cao, C. Fu, J. Ye, B. Li, and Y . Li. SiamAPN++:\nSiamese Attentional Aggregation Network for Real-Time\nUA V Tracking. In Proceedings of the IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems (IROS),\npages 1‚Äì7, 2021. 1\n[7] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,\nand S. Zagoruyko. End-to-End Object Detection with Trans-\nformers. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 213‚Äì229, 2020. 2\n[8] Z. Chen, B. Zhong, G. Li, S. Zhang, and R. Ji. Siamese Box\nAdaptive Network for Visual Tracking. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 6668‚Äì6677, 2020. 1, 2, 7, 8\n[9] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg. ECO:\nEfÔ¨Åcient Convolution Operators for Tracking. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 6931‚Äì6939, 2017. 1, 2, 5, 6\n[10] M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg. ATOM:\nAccurate Tracking by Overlap Maximization. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4655‚Äì4664, 2019. 7, 8\n[11] M. Danelljan, G. H ¬®ager, F. S. Khan, and M. Felsberg.\nLearning Spatially Regularized Correlation Filters for Visual\nTracking. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV) , pages 4310‚Äì4318, 2015.\n5\n[12] M. Danelljan, G. H ¬®ager, F. S. Khan, and M. Felsberg.\nDiscriminative Scale Space Tracking. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 39(8):1561‚Äì\n1575, 2017. 5\n[13] M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg. Be-\nyond Correlation Filters: Learning Continuous Convolution\nOperators for Visual Tracking. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , pages 472‚Äì\n488, 2016. 5, 6\n[14] M. Danelljan, L. Van Gool, and R. Timofte. Probabilis-\ntic Regression for Visual Tracking. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 7181‚Äì7190, 2020. 7, 8\n[15] E.Real, J.Shlens, S.Mazzocchi, X.Pan, and V .Vanhoucke.\nYouTube-BoundingBoxes: A Large High-Precision Human-\nAnnotated Data Set for Object Detection in Video. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 7464‚Äì7473, 2017. 5\n[16] H. Fan and H. Ling. Siamese Cascaded Region Proposal Net-\nworks for Real-Time Visual Tracking. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 7944‚Äì7953, 2019. 2\n[17] C. Fu, Z. Cao, Y . Li, J. Ye, and C. Feng. Onboard Real-\nTime Aerial Tracking With EfÔ¨Åcient Siamese Anchor Pro-\nposal Network. IEEE Transactions on Geoscience and Re-\nmote Sensing, pages 1‚Äì13, 2021. 1\n[18] C. Fu, Z. Cao, Y . Li, J. Ye, and C. Feng. Siamese Anchor\nProposal Network for High-Speed Aerial Tracking. In Pro-\nceedings of the IEEE International Conference on Robotics\nand Automation (ICRA), pages 1‚Äì7, 2021. 1, 2\n[19] C. Fu, A. Carrio, M. A. Olivares-Mendez, R. Suarez-\nFernandez, and P. Campoy. Robust real-time vision-based\naircraft tracking from Unmanned Aerial Vehicles. In Pro-\nceedings of the IEEE International Conference on Robotics\nand Automation (ICRA), pages 5441‚Äì5446, 2014. 1\n[20] Changhong Fu, Bowen Li, Fangqiang Ding, Fuling Lin, and\nGeng Lu. Correlation Filter for UA V-Based Aerial Tracking:\nA Review and Experimental Evaluation. IEEE Geoscience\nand Remote Sensing Magazine, pages 1‚Äì28, 2020. 2\n[21] H. K. Galoogahi, A. Fagg, and S. Lucey. Learning\nBackground-Aware Correlation Filters for Visual Tracking.\nIn Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), pages 1144‚Äì1152, 2017. 2, 5\n[22] D. Guo, Y . Shao, Y . Cui, Z. Wang, L. Zhang, and C. Shen.\nGraph Attention Tracking. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 1‚Äì10, 2021. 7, 8\n[23] D. Guo, J. Wang, Y . Cui, Z. Wang, and S. Chen. SiamCAR:\nSiamese Fully Convolutional ClassiÔ¨Åcation and Regression\nfor Visual Tracking. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n6268‚Äì6276, 2020. 1, 2, 7, 8\n[24] Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, and S.\nWang. Learning Dynamic Siamese Network for Visual Ob-\nject Tracking. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), pages 1781‚Äì1789,\n2017. 2, 5\n[25] K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning\nfor Image Recognition. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 770‚Äì778, 2016. 2, 3\n[26] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-\nSpeed Tracking with Kernelized Correlation Filters. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n37(3):583‚Äì596, 2015. 5\n[27] L. Huang, J. Tan, J. Liu, and J. Yuan. Hand-Transformer:\nNon-Autoregressive Structured Modeling for 3D Hand Pose\nEstimation. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 17‚Äì33, 2020. 2\n[28] L. Huang, X. Zhao, and K. Huang. GOT-10k: A Large\nHigh-Diversity Benchmark for Generic Object Tracking in\nthe Wild. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, pages 1‚Äì17, 2019. 5\n[29] Z. Huang, C. Fu, Y . Li, F. Lin, and P. Lu. Learning Aberrance\nRepressed Correlation Filters for Real-time UA V Tracking.\nIn Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), pages 2891‚Äì2900, 2019. 2, 5, 6\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Ima-\ngenet ClassiÔ¨Åcation with Deep Convolutional Neural Net-\nworks. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), pages 1097‚Äì1105, 2012. 1, 2, 3, 5\n[31] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan.\nSiamRPN++: Evolution of Siamese Visual Tracking With\nVery Deep Networks. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 4277‚Äì4286, 2019. 1, 2, 3, 5, 6, 7, 8\n[32] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu. High Performance\nVisual Tracking with Siamese Region Proposal Network. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 8971‚Äì8980, 2018.\n2\n[33] F. Li, C. Tian, W. Zuo, L. Zhang, and M. Yang. Learning\nSpatial-Temporal Regularized Correlation Filters for Visual\nTracking. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 4904‚Äì\n4913, 2018. 5, 6\n[34] S. Li and D. Yeung. Visual Object Tracking for Unmanned\nAerial Vehicles: A Benchmark and New Motion Models.\nIn Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intel-\nligence (AAAI), pages 1‚Äì7, 2017. 1, 5, 7, 8\n[35] X. Li, C. Ma, B. Wu, Z. He, and M. Yang. Target-Aware\nDeep Tracking. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n1369‚Äì1378, 2019. 1, 2, 5, 6\n[36] Y . Li, C. Fu, F. Ding, Z. Huang, and G. Lu. AutoTrack:\nTowards High-Performance Visual Tracking for UA V With\nAutomatic Spatio-Temporal Regularization. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11920‚Äì11929, 2020. 2, 5, 6\n[37] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll¬¥ar, and C. L. Zitnick. Microsoft coco: Com-\nmon objects in context. In Proceedings of the European con-\nference on computer vision (ECCV) , pages 740‚Äì755, 2014.\n5\n[38] T. Meinhardt, A. Kirillov, L. Laura, and C. Feichtenhofer.\nTrackFormer: Multi-Object Tracking with Transformers.\narXiv preprint arXiv:2101.02702, 2021. 2\n[39] M. Mueller, N. Smith, and B. Ghanem. A Benchmark and\nSimulator for UA V Tracking. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV) , pages 445‚Äì\n461, 2016. 1, 3, 5, 6, 7, 8\n[40] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN:\nTowards Real-Time Object Detection with Region Proposal\nNetworks. In Advances in Neural Information Processing\nSystems (NeurIPS), pages 91‚Äì99, 2015. 2\n[41] O. Russakovsky, J. Deng, H. Su, J. Krause, et al. Imagenet\nLarge Scale Visual Recognition Challenge. International\nJournal of Computer Vision, 115(3):211‚Äì252, 2015. 5\n[42] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.\nChen. Mobilenetv2: Inverted Residuals and Linear Bottle-\nnecks. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 4510‚Äì4520,\n2018. 3\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going Deeper with\nConvolutions. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 1‚Äì\n9, 2015. 3\n[44] N. Wang, Y . Song, C. Ma, W. Zhou, W. Liu, and H. Li. Unsu-\npervised Deep Tracking. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 1308‚Äì1317, 2019. 1, 5, 6\n[45] N. Wang, W. Zhou, Q. Tian, R. Hong, M. Wang, and H.\nLi. Multi-cue Correlation Filters for Robust Visual Track-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 4844‚Äì4853, 2018. 5,\n6\n[46] Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr.\nFast online object tracking and segmentation: A unifying ap-\nproach. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 1328‚Äì1338,\n2019. 7, 8\n[47] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning Texture\nTransformer Network for Image Super-Resolution. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 5791‚Äì5800, 2020. 2\n[48] J. Ye, C. Fu, F. Lin, F. Ding, S. An, and G. Lu. Multi-\nRegularized Correlation Filter for UA V Tracking and Self-\nLocalization. IEEE Transactions on Industrial Electronics ,\npages 1‚Äì10, 2021. 1\n[49] F. Yu and V . Koltun. Multi-Scale Context Aggregation by\nDilated Convolutions. In Proceedings of the International\nConference on Learning Representations (ICLR), pages 1‚Äì9,\n2016. 1\n[50] L. Zhang and P. N. Suganthan. Robust Visual Tracking via\nCo-Trained Kernelized Correlation Filters. Pattern Recogni-\ntion, 69:82‚Äì93, 2017. 2, 5\n[51] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong. End-\nto-End Dense Video Captioning with Masked Transformer.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 8739‚Äì8748, 2018. 2\n[52] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. De-\nformable DETR: Deformable Transformers for End-to-End\nObject Detection. arXiv preprint arXiv:2010.04159, 2020. 2\n[53] Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W.\nHu. Distractor-Aware Siamese Networks for Visual Object\nTracking. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 101‚Äì117, 2018. 1, 2, 5, 6",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7385455369949341
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6356092691421509
    },
    {
      "name": "Transformer",
      "score": 0.5828191637992859
    },
    {
      "name": "Computation",
      "score": 0.5075849294662476
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4619126617908478
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4074052572250366
    },
    {
      "name": "Computer vision",
      "score": 0.3827302157878876
    },
    {
      "name": "Data mining",
      "score": 0.3686010241508484
    },
    {
      "name": "Engineering",
      "score": 0.12848934531211853
    },
    {
      "name": "Algorithm",
      "score": 0.07923164963722229
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}