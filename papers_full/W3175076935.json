{
  "title": "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation",
  "url": "https://openalex.org/W3175076935",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5101667541",
      "name": "Feilong Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5024849044",
      "name": "Fandong Meng",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5009070585",
      "name": "Xiuyi Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100726749",
      "name": "Peng Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A5100770462",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963668159",
    "https://openalex.org/W2966158321",
    "https://openalex.org/W2622980782",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2950009015",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963243930",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2892245540",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W1575833922",
    "https://openalex.org/W3034291519",
    "https://openalex.org/W2987734933",
    "https://openalex.org/W3035052826",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W2463565445",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2189070436",
    "https://openalex.org/W3104123491",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W4287806966",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2988023442",
    "https://openalex.org/W2963623904",
    "https://openalex.org/W3107092117",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2972438655",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3116651605",
    "https://openalex.org/W3022310886",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3099143471",
    "https://openalex.org/W2963643760",
    "https://openalex.org/W2917061951",
    "https://openalex.org/W3023074479",
    "https://openalex.org/W2963287297",
    "https://openalex.org/W2951508633",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2981902456",
    "https://openalex.org/W3035103424",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3035398197",
    "https://openalex.org/W3095309002",
    "https://openalex.org/W2996781902",
    "https://openalex.org/W2768661419"
  ],
  "abstract": "Visual dialogue is a challenging task since it needs to answer a series of coherent questions on the basis of understanding the visual environment.Previous studies focus on the implicit exploration of multimodal coreference by implicitly attending to spatial image features or object-level image features but neglect the importance of locating the objects explicitly in the visual content, which is associated with entities in the textual content.Therefore, in this paper we propose a Multimodal Incremental Transformer with Visual Grounding, named MITVG, which consists of two key parts: visual grounding and multimodal incremental transformer.Visual grounding aims to explicitly locate related objects in the image guided by textual entities, which helps the model exclude the visual content that does not need attention.On the basis of visual grounding, the multimodal incremental transformer encodes the multi-turn dialogue history combined with visual scene step by step according to the order of the dialogue and then generates a contextually and visually coherent response.Experimental results on the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the proposed model, which achieves comparable performance.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 436‚Äì446\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n436\nMultimodal Incremental Transformer with Visual Grounding\nfor Visual Dialogue Generation\nFeilong Chen, Fandong Meng, Xiuyi Chen, Peng Li, Jie Zhou\nPattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China\n{ivess.chan,hugheren.chan}@gmail.com\n{fandongmeng,patrickpli,withtomzhou}@tencent.com\nAbstract\nVisual dialogue is a challenging task since\nit needs to answer a series of coherent ques-\ntions on the basis of understanding the vi-\nsual environment. Previous studies focus\non the implicit exploration of multimodal co-\nreference by implicitly attending to spatial\nimage features or object-level image features\nbut neglect the importance of locating the ob-\njects explicitly in the visual content, which\nis associated with entities in the textual con-\ntent. Therefore, in this paper we propose\na Multimodal Incremental Transformer with\nVisual Grounding, named MITVG, which con-\nsists of two key parts: visual grounding and\nmultimodal incremental transformer. Visual\ngrounding aims to explicitly locate related ob-\njects in the image guided by textual entities,\nwhich helps the model exclude the visual con-\ntent that does not need attention. On the ba-\nsis of visual grounding, the multimodal incre-\nmental transformer encodes the multi-turn dia-\nlogue history combined with visual scene step\nby step according to the order of the dialogue\nand then generates a contextually and visually\ncoherent response. Experimental results on\nthe VisDial v0.9 and v1.0 datasets demonstrate\nthe superiority of the proposed model, which\nachieves comparable performance.\n1 Introduction\nRecently, there is increasing interest in vision-\nlanguage tasks, such as image caption (Xu et al.,\n2015; Anderson et al., 2016, 2018; Cornia et al.,\n2020) and visual question answering (Ren et al.,\n2015a; Gao et al., 2015; Lu et al., 2016; Ander-\nson et al., 2018). In the real world, our conver-\nsations (Chen et al., 2020b, 2019) usually have\nmultiple turns. As an extension of conventional\nsingle-turn visual question answering, Das et al.\n(2017) introduce a multi-turn visual question an-\nswering task named visual dialogue, which aims to\nQ1: how many people ? \nQ2: is anyone holding a frisbee ?\nQ3: is the coach on the right ?\nQ4: are they wearing matching \nuniforms ?\nCaption: there is a frisbee team with their \ncoach taking a team photo \nA1: 7 people\nA2: yes\nA3: yes, on the far right \nA4: all except the coach\nFigure 1: An example of visual dialogue. The color in\ntext background corresponds to the same color box in\nthe image, which indicates the same entity. Our model\nÔ¨Årstly associates textual entities with objects explicitly\nand then gives contextually and visually coherent an-\nswers to contextual questions.\nexplore the ability of an AI agent to hold a mean-\ningful multi-turn dialogue with humans in natural\nlanguage about visual content.\nVisual dialogue (Agarwal et al., 2020; Wang\net al., 2020; Qi et al., 2020; Murahari et al., 2020)\nrequires agents to give a response on the basis of\nunderstanding both visual and textual content. One\nof the key challenges in visual dialogue is how to\nsolve multimodal co-reference (Das et al., 2017;\nKottur et al., 2018). Therefore, some fusion-based\nmodels (Das et al., 2017) are proposed to fuse spa-\ntial image features and textual features in order to\nobtain a joint representation. Then attention-based\nmodels (Lu et al., 2017; Wu et al., 2018; Kottur\net al., 2018) are proposed to dynamically attend to\nspatial image features in order to Ô¨Ånd related visual\ncontent. Furthermore, models based on object-level\nimage features (Niu et al., 2019; Gan et al., 2019;\nChen et al., 2020a; Jiang et al., 2020a; Nguyen\n437\net al., 2020; Jiang et al., 2020b) are proposed to ef-\nfectively leverage the visual content for multimodal\nco-reference. However, as implicit exploration of\nmultimodal co-reference, these methods implic-\nitly attend to spatial or object-level image features,\nwhich is trained with the whole model and is in-\nevitably distracted by unnecessary visual content.\nIntuitively, speciÔ¨Åc mapping of objects and textual\nentities can reduce the noise of attention. As shown\nin Figure 1, the related objects can help the agent\nto understand the entities (e.g., Q1: ‚Äúpeople‚Äù, Q2:\n‚Äúfrisbee‚Äù, Q3: ‚Äúcoach‚Äù) for the generation of correct\nanswers. Then when it answers the question Q4\n‚Äúare they wearing matching uniforms ?‚Äù, the agent\nhas already comprehended ‚Äúpeople‚Äù and ‚Äúcoach‚Äù\nfrom the previous conversation. On this basis, it\ncan learn the entity ‚Äúuniforms‚Äù with the correspond-\ning object in the image, and generate the answer\n‚Äúall except the coach‚Äù. To this end, we need to 1)\nexplicitly locate related objects guided by textual\nentities to exclude undesired visual content, and 2)\nincrementally model the multi-turn structure of the\ndialogue to develop a uniÔ¨Åed representation com-\nbining multi-turn utterances with the corresponding\nrelated objects. However, previous work overlooks\nthese two important aspects.\nIn this paper, we thus propose a novel and ef-\nfective Multimodal Incremental Transformer with\nVisual Grounding, named MITVG, which contains\ntwo key parts: visual grounding and multimodal\nincremental transformer. Visual grounding aims\nto establish speciÔ¨Åc mapping of objects and tex-\ntual entities by explicitly locating related objects\nin the image with the textual entities. By doing\nso, our model can exclude undesired visual content\nand reduce attention noise. On the basis of visual\ngrounding, the multimodal incremental transformer\nis used to model the multi-turn dialogue history\ncombined with the speciÔ¨Åc visual content to gen-\nerate visually and contextually coherent responses.\nAs an encoder-decoder framework, MITVG con-\ntains a Multimodal Incremental Transformer En-\ncoder (MITE) and a Gated Cross-Attention De-\ncoder (GCAD).\nWe test the effectiveness of our proposed model\non large-scale datasets: VisDial v0.9 and v1.0 (Das\net al., 2017). Both automatic and manual evalu-\nations show that our model substantially outper-\nforms the competitive baselines and achieves the\nnew state-of-the-art results on substantial metrics.\nOur main contributions are as follows:\n‚Ä¢ To the best of our knowledge, we are the Ô¨Årst\nto leverage visual grounding to explicitly lo-\ncate related objects in the image guided by\ntextual entities for visual dialogue.\n‚Ä¢ We propose a novel multimodal incremental\ntransformer to encode the multi-turn dialogue\nhistory step by step combined with the visual\ncontent and then generate a contextually and\nvisually coherent response.\n‚Ä¢ We achieve comparable performance on Vis-\nDial v0.9 and v1.0 datasets.\n2 Approach\n2.1 Overview\nIn this section, we formally describe the visual\ndialogue task and then proceed to our proposed\nMultimodal Incremental Transformer with Visual\nGrounding (MITVG).\nFollowing Das et al.(2017), a visual dia-\nlogue agent is given three inputs, i.e., an im-\nage I, a dialogue history (the caption and\nquestion-answer pairs) till round t ‚àí1: H =\n(CapÓ¥ôÓ¥òÓ¥óÓ¥ö\nH0\n,(Q1,A1)Ó¥ô Ó¥òÓ¥ó Ó¥ö\nH1\n,¬∑¬∑¬∑ ,(Qt‚àí1,At‚àí1)Ó¥ô Ó¥òÓ¥ó Ó¥ö\nHt‚àí1\n) and the cur-\nrent question Qt at round t, where Cap is\nthe caption describing the image taken as H0\nand H1,...,H t‚àí1 are concatenations of question-\nanswer pairs. The goal of the visual dialogue agent\nis to generate a response (or answer)At to the ques-\ntion Qt. Cap, Q‚àóand A‚àóare sentences.\nFigure 2 shows the framework of MITVG, which\naims to explicitly model multi-turn dialogue his-\ntory step by step based on the explicit modeling\nrelationship between multiple modalities. MITVG\nÔ¨Årstly locates related objects in the image explicitly\nguided by the textual entities via visual ground-\ning, then encodes multi-turn dialogue history in\nthe order of the dialogue utterance based on visual\ngrounding via Multimodal Incremental Encoder\n(MITE), and Ô¨Ånally utilizes the outputs of both\nencoder and visual grounding to generate the re-\nsponse word by word via Gated Cross-Attention\nDecoder (GCAD).\n2.2 Input Representation\nBefore describing our method, we introduce the\ninput representation.\nImage Features. We use a pre-trained Faster R-\nCNN model (Ren et al., 2015b) to extract object-\n438\nMITE\nMITE\nMITE\nSelf-Attention\nGated Cross-\nAttention\nFFN\nTarget Inputs\nSoftmax Layer\nTarget Outputs\nCurrent\nQues. Input\n1-st Round \nHistory Inputs\nEmbedding Layer\nVG Model\ni-th Round \nQues. Input\nVG Model\ni-th Round\nHistory Inputs\nVG Model\nCaption\nInput\n„Éª„Éª„Éª\n„Éª„Éª„Éª\nùëÅ\"√ó\n1-st Round \nQues. Input\nCurrent\nQues. Input\nImage Input\nEncoder Decoder\nFigure 2: The framework of Multimodal Incremental Transformer with Visual Grounding (MITVG). ‚ÄúVG Model‚Äù\nindicates visual grounding model (Yang et al., 2019b) (Details are described in Sec. 2.3). ‚ÄúMITE‚Äù denotes the\nmultimodal incremental transformer encoder (Details are described in Sec. 2.4.1). MITVG Ô¨Årstly uses the VG\nmodel to explicitly model the relationship between the textual content and the visual content, and encodes multi-\nturn dialogue history in the order of the dialogue based on visual grounding, and Ô¨Ånally utilizes the outputs of both\nencoder and visual grounding to generate the response word by word in the decoding process.\nlevel image features. SpeciÔ¨Åcally, the image fea-\ntures vfor an image I are represented by:\nv= Faster R‚àíCNN(I) ‚ààRK√óV , (1)\nwhere Kdenotes the total number of the detected\nobjects per image and V denotes the dimension of\nfeatures for each object.\nLanguage Features. The current (at the t-th\nround) L-word question features are a sequence\nof M-dimension word embedding with positional\nencoding added (Vaswani et al., 2017), as follows:\nqt = [ st,1,st,2,...,s t,L] ‚ààRL√óM , (2)\nst,j = wj + PE(j), (3)\nwhere wj is the word embedding of the j-th word\nin the question Qt, and PE(¬∑) denotes positional\nencoding function (Vaswani et al., 2017). For the\ndialogue history H = {H0,H1,...,H t‚àí1}and\nthe answer At, the dialogue history features u =\n{u0,u1,...,u t‚àí1}and the answer features at are\nobtained in the same way as the question Qt.\n2.3 Visual Grounding\nTo exclude the needless visual content, we intro-\nduce visual grounding, which is deÔ¨Åned to ground\na natural language query (phrase or sentence) about\nan image onto a correct region of the image. First\nof all, we use NeuralCoref 1 for reference resolu-\ntion. For example, when it processes the question\nQ4 ‚Äúare they wearing matching uniforms ?‚Äù shown\nin Figure 1, NeuralCoref takes the question Q4 and\nits history as inputs, and then generates a new ques-\ntion ‚Äúare the people wearing matching uniforms\n?‚Äù as a new Q4. As shown in Figure 3 (a), visual\ngrounding model (Yang et al., 2019b) takes thei-th\nquestion Qi and the image I as inputs and gener-\nates initial visual grounding features, as follows:\nv(0)\ngi = VGM(Qi,I), (4)\nwhere VGM(¬∑) denotes visual grounding model2.\nThen v(0)\ngi is sent to the multi-head self-attention\n1Introduction and code of NeuralCoref are available at\nhttps://github.com/huggingface/neuralcoref. NeuralCoref is\nonly used for visual grounding.\n2Introduction and code are available at\nhttps://github.com/zyang-ur/onestage grounding.\n439\nImage Input\ni-th Round\nHistory Inputs\nCross-modal\nAttention\nEmbedding Layer\nSelf-Attention\nFFN\nHistory\nAttention\nSelf-Attention\nFFN\nùëÅ\"√ó\nVG Model\ni-th Round \nQues. Input\nùëÅ$√ó\nùë£&'\nùëê)*+\nùëê)\n(ùëé) (ùëè)\nùë£&'\nùë¢)\nFigure 3: Framework of (a) Visual Grounding and\n(b) Multimodal Incremental Transformer Encoder\n(MITE).\nlayer followed by a position wise feed-forward net-\nwork (FFN) layer (stacked Nv times) to generate\nthe i-th visual grounding features as follows3:\nÀÜvn\ngi = MultiHead\n(\nv(n‚àí1)\ngi ,v(n‚àí1)\ngi ,v(n‚àí1)\ngi\n)\n, (5)\nwhere n = 1,...,N v and MultiHead(¬∑) denotes\nthe multi-head self-attention layer (Vaswani et al.,\n2017), then\nv(n)\ngi = FFN\n(\nÀÜvn\ngi\n)\n, (6)\nwhere n= 1,...,N v and FFN(¬∑) denotes the po-\nsition wise feed-forward networks (Vaswani et al.,\n2017). After Nv layers computation, we obtain the\nÔ¨Ånal visual grounding features vgi by:\nvgi = v(Nv)\ngi , (7)\nActually, there are some questions that do not con-\ntain any entities in the visual dialogue, such as\n‚Äúanything else ?‚Äù. For such questions, we use the\nfeatures of the whole image instead, i.e. vgi = v.\n2.4 Multimodal Incremental Transformer\nInspired by the idea of incremental transformer (Li\net al., 2019) which is originally designed for the\nsingle-modal dialogue task, we make an extension\nand propose a multimodal incremental transformer,\nwhich is composed of a Multimodal Incremental\nTransformer Encoder (MITE) and a Gated Cross-\nAttention Decoder (GCAD). The MITE uses an\nincremental encoding scheme to encode multi-turn\n3For simplicity, we omit the descriptions of layer normal-\nization and residual connection.\ndialogue history with an understanding of the im-\nage. The GCAD leverages the outputs from both\nthe encoder and visual grounding via the gated\ncross-attention layer to fuse the two modal informa-\ntion in order to generate a contextually and visually\ncoherent response word by word.\n2.4.1 MITE\nTo effectively encode multi-turn utterances\ngrounded in visual content, we design the Mul-\ntimodal Incremental Transformer Encoder (MITE).\nAs shown in Figure 3 (b), at the i-th round, where\ni= 1,2,...,t ‚àí1, the MITE takes the visual ground-\ning features vgi , the dialogue history features ui\nand the context state ci‚àí1 as inputs, and utilizes\nattention mechanism to incrementally build up the\nrepresentation of the relevant dialogue history and\nthe associated image regions, and then outputs the\nnew context state ci. This process can be stated\nrecursively as follows:\nci = MITE (vgi ,ui,ci‚àí1) , (8)\nwhere MITE(¬∑) denotes the encoding function, ci\ndenotes the context state after the dialogue history\nfeatures ui and the visual grounding featuresvgi be-\ning encoded, and c0 is the dialogue history features\nu0.\nAs shown in Figure 3 (b), we use a stack of Nh\nidentical layers to encode vgi , ui and ci‚àí1, and to\ngenerate ci. Each layer consists of four sub-layers.\nThe Ô¨Årst sub-layer is a multi-head self-attention\nfor the dialogue history:\nA(n) = MultiHead\n(\nC(n‚àí1),C(n‚àí1),C(n‚àí1)\n)\n,\n(9)\nwhere n= 1,...,N h, C(n‚àí1) is the output of the\nlast layer Nn‚àí1, and C(0) is the dialog history fea-\ntures ui. The second sub-layer is a multi-head\ncross-modal attention:\nB(n) = MultiHead (An,vgi ,vgi ) , (10)\nwhere vgi is the visual grounding features. The\nthird sub-layer is a multi-head history attention:\nF(n) = MultiHead\n(\nB(n),ci‚àí1,ci‚àí1\n)\n, (11)\nwhere ci‚àí1 is the context state after the previous di-\nalogue history features ui‚àí1 being encoded. That‚Äôs\nwhy we call this encoder ‚ÄúMultimodal Incremental\nTransformer‚Äù. The fourth sub-layer is a position\nwise feed-forward network (FFN):\nC(n) = FFN\n(\nF(n)\n)\n. (12)\n440\nWe useci to denote the Ô¨Ånal representation atNh-th\nlayer:\nci = C(Nh). (13)\nThe mulitmodal incremental transformer encoder\nat the current turn t, i.e., the bottom one in Figure 2,\nhas the same structure as all the other MITEs but\ntakes the visual grounding features vgt , the current\nquestion features qt and the context state ct‚àí1 as\ninputs and generates the Ô¨Ånal context state ct.\n2.4.2 GCAD\nMotivated by the real-world human cognitive pro-\ncess, we design a Gated Cross-Attention Decoder\n(GCAD) shown in Figure 2, which takes the\nmasked answer features a<z (where z= 1,2,...,Z\nand Z is the length of the answer), encoder out-\nputs ct and visual grounding features vgt as inputs,\nand generates contextually and visually coherent\nresponses grounded in an image. GCAD is com-\nposed of a stack of Ny identical layers, each of\nwhich has three sub-layers.\nThe Ô¨Årst sub-layer is a multi-head self-\nattention as follows:\nJ(n) = MultiHead\n(\nR(n‚àí1),R(n‚àí1),R(n‚àí1)\n)\n,\n(14)\nwhere n= 1,...,N y, R(n‚àí1) is the output of the\nprevious layer, and R(0) is the masked answer fea-\ntures a<z.\nThe second sub-layer is a multi-head gated\ncross-modal attention layer (GCA) as shown in\nFigure 4, calculated as:\nP(n) = Œ±(n) ‚ó¶E(n) + Œ≤(n) ‚ó¶G(n), (15)\nwhere n= 1,...,N y, ‚ó¶denotes Hadamard prod-\nuct, E(n) and G(n) denote the outputs of two cross-\nattention functions, computed as follows:\nE(n) = MultiHead\n(\nJ(n),ct,ct\n)\n, (16)\nG(n) = MultiHead\n(\nJ(n),vgt ,vgt\n)\n, (17)\nwhere Œ±(n), Œ≤(n) are two gates4:\nŒ±(n) = œÉ\n(\nWE[J(n),E(n)] +bE\n)\n, (18)\nŒ≤(n) = œÉ\n(\nWG[J(n),G(n)] +bG\n)\n, (19)\nwhere œÉdenotes sigmoid function, WE, WG, bE,\nbG are learnable parameters, and [¬∑,¬∑] indicates con-\ncatenation.\n4Our inspiration comes from Cornia et al. (2020).\nKey\nùë£\"#\nValue\nQuery Key\nValue\nCross-\nAttention\nCross-\nAttention\nFC FC\nùúé ùúé\n‚àò ‚àò+\nùëê( ùêΩ(+)\nùëÉ(+)\nFigure 4: Framework of Gated Cross-Attention (GCA)\nin the Deocer.\nThe third sub-layer is a position wise feed-\nforward network (FFN):\nR(n) = FFN\n(\nP(n)\n)\n. (20)\nWe use rz to denote the Ô¨Ånal representation at Ny-\nth layer:\nrz = R(Ny). (21)\nFinally, we use softmax to get the word probabili-\nties ÀÜaz:\nÀÜaz = softmax(rz). (22)\n3 Experiments\n3.1 Datasets\nWe conduct experiments on the VisDial v0.9 and\nv1.0 datasets (Das et al., 2017) to verify our ap-\nproach. VisDial v0.9 contains 83k dialogs on\nCOCO-train (Lu et al., 2017) and 40k dialogs on\nCOCO-val images as test set, for a total of 1.23M\ndialog question-answer pairs. VisDial v1.0 dateset\nis an extension of VisDial v0.9 dateset with addi-\ntional 10k COCO-like images from Flickr. VisDial\nv1.0 dateset contains 123k, 2k and 8k images as\ntrain, validation and test splits, respectively.\n3.2 Implementation and Evaluation\nImplementation Details. Following previous\nwork (Das et al., 2017), in order to represent words\nwe Ô¨Årstly lowercase all the texts and convert digits\nto words, and then remove contractions before tok-\nenization. The captions, questions and answers are\nfurther truncated to ensure that they are not longer\nthan 40, 20 and 20 tokens, respectively. We con-\nstruct the vocabulary of tokens that appear at least\n441\nModel ObjectVis-GMRR‚Üë R@1‚Üë R@5‚Üë R@10‚Üë Mean‚Üì\nAP (Das et al., 2017) √ó √ó 37.35 23.55 48.52 53.23 26.50\nNN (Das et al., 2017) √ó √ó 42.74 33.13 50.83 58.69 19.62\nLF (Das et al., 2017) √ó √ó 51.99 41.83 61.78 67.59 17.07\nHREA (Das et al., 2017) √ó √ó 52.42 42.28 62.33 68.71 16.79\nMN (Das et al., 2017) √ó √ó 52.59 42.29 62.85 68.88 17.06\nHCIAE (Lu et al., 2017) √ó √ó 53.86 44.06 63.55 69.24 16.01\nCorefNMN (Kottur et al., 2018)√ó √ó 53.50 43.66 63.54 69.93 15.69\nCoAtt (Wu et al., 2018) √ó √ó 55.78 46.10 65.69 71.74 14.43\nRvA (Niu et al., 2019) ‚úì √ó 55.43 45.37 65.27 72.97 10.71\nDV AN (Guo et al., 2019b)‚úì √ó 55.94 46.58 65.50 71.25 14.79\nVDBERT (Wang et al., 2020)‚úì √ó 55.95 46.83 65.43 72.05 13.18\nLTMI (Nguyen et al., 2020)‚Ä† ‚úì √ó 55.85 46.07 65.97 72.44 14.17\nDMRM (Chen et al., 2020a)‚úì √ó 55.96 46.20 66.02 72.43 13.15\nMITVG ‚úì ‚úì 56.83 47.14 67.19 73.72 11.95\nTable 1: Performance on VisDial val v0.9 (Das et al., 2017). ‚Ä†indicates that we re-implement the model. ‚ÄúObject‚Äù\nand ‚ÄúVis-G‚Äù denote if the model uses object-level image features and visual grounding, respectively. Underline\ndenotes the highest score among baselines. Our MITVG exceeds previous work on most of the metrics and achieves\ncomparable performance.\nModel ObjectVis-GMRR‚Üë R@1‚Üë R@5‚Üë R@10‚Üë Mean‚Üì NDCG‚Üë\nMN (Das et al., 2017)‚Ä° ‚úì √ó 47.99 38.18 57.54 64.32 18.60 51.86\nHCIAE (Lu et al., 2017)‚Ä° ‚úì √ó 49.07 39.72 58.23 64.73 18.43 59.70\nCoAtt (Wu et al., 2018)‚Ä° ‚úì √ó 49.64 40.09 59.37 65.92 17.86 59.24\nPrimary (Guo et al., 2019a)‚úì √ó 49.01 38.54 59.82 66.94 16.60 -\nReDAN (Gan et al., 2019)‚úì √ó 50.02 40.27 59.93 66.78 17.40 60.47\nDMRM (Chen et al., 2020a)‚úì √ó 50.16 40.15 60.02 67.21 15.19 -\nLTMI (Nguyen et al., 2020)‚Ä† ‚úì √ó 50.38 40.30 60.72 68.44 15.73 61.61\nDAM (Jiang et al., 2020b)‚úì √ó 50.51 40.53 60.84 67.94 16.65 60.93\nKBGN (Jiang et al., 2020a)‚úì √ó 50.05 40.40 60.11 66.82 17.54 60.42\nMITVG ‚úì ‚úì 51.14 41.03 61.25 68.49 14.37 61.47\nTable 2: Performance on VisDial val v1.0 (Das et al., 2017). ‚Ä°denotes that all the models are re-implemented by\nGan et al. (2019). Our MITVG outperforms previous work and achieves comparable performance.\n5 times in the training split. To represent image\nregions, we use Faster R-CNN (Ren et al., 2015b)\nwith ResNet-101 (He et al., 2016) Ô¨Ånetuned on the\nVisual Genome dataset (Krishna et al., 2017), thus\nobtaining a 2048-dimensional feature vector for\neach region. The layers of our encoder, decoder\nand visual grounding module are all set to 3. The\nnumber of attention heads in multi-head attention is\n8 and the Ô¨Ålter size is 2048. The word embedding\nis shared by the history, questions and responses.\nThe dimension of word embedding is set to 512 em-\npirically. We use Adam (Kingma and Ba, 2014) for\noptimization, following the learning rate schedul-\ning strategy of Vaswani et al. (2017). Our model\nis implemented using PyTorch v1.0, Python v3.6,\nand provides out of the box support with CUDA\n9 and CuDNN 7. We train our model on TITAN\nXP with 8 GPUs. For each epoch, we spend about\n9,000 seconds on training the model. The total\nparameters are about 56.79M.\nBefore we train our model, we use three exter-\nnal tools for image features extracting, reference\nresolution and visual grounding.\nImage Features Extracting We extract im-\nage features of VisDial images, using a Faster-\nRCNN (Ren et al., 2015b) with ResNet-101 (He\net al., 2016) pre-trained on Visual Genome (Kr-\nishna et al., 2017), introduction and code\nfrom https://github.com/peteanderson80/bottom-\nup-attention.\nReference Resolution we use NeuralCoref v4.0\nfor reference resolution, which is developed by\nhuggingface. Introduction and code are available\nat https://github.com/huggingface/neuralcoref.\nVisual Grounding We use One-Stage Visual\nGrounding Model (Yang et al., 2019b) to ob-\ntain the visual grounding features. Introduction\nand code are available at https://github.com/zyang-\nur/onestage grounding.\nAutomatic Evaluation. We use a retrieval set-\nting to evaluate individual responses at each round\nof a dialogue, following Das et al. (2017). Specif-\n442\nically, at test time, apart from the image, ground\ntruth dialogue history and the question, a list of\n100-candidate answers is also given. The model is\nevaluated on retrieval metrics: (1) rank of human\nresponse (Mean, the lower the better), (2) existence\nof the human response in top‚àíkranked responses,\ni.e., R@k(3) mean reciprocal rank (MRR) of the\nhuman response (the higher the better) and (4) nor-\nmalized discounted cumulative gain (NDCG) for\nVisDial v1.0 (the higher the better). During eval-\nuation, we use the log-likelihood scores to rank\ncandidate answers.\nHuman Evaluation. We randomly extract 100\nsamples for human evaluation according to Wu et al.\n(2018), and then ask 3 human subjects to guess\nwhether the last response in the dialogue is human-\ngenerated or machine-generated. If at least 2 of\nthem agree it is generated by a human, we think it\npasses the Truing Test (M1). In addition, we record\nthe percentage of responses that are evaluated better\nthan or equal to human responses (M2), according\nto the human subjects‚Äô evaluation.\n3.3 Main Results\nWe compare our proposed model to the state-\nof-the-art generative models developed in previ-\nous work. Current encoder-decoder based gen-\nerative models can be divided into tree facets.\n(1) Fusion-based models: LF (Das et al., 2017)\nand HREA (Das et al., 2017) directly encode the\nmultimodal inputs and decode the answer. (2)\nAttention-based models: HCIAE (Lu et al., 2017),\nCoAtt (Wu et al., 2018), Primary (Guo et al.,\n2019a), ReDAN (Gan et al., 2019), DV AN (Guo\net al., 2019b) and DMRM (Chen et al., 2020a),\nDAM, LTMI, KBGN. (3) Visual co-reference res-\nolution models: CorefNMN (Kottur et al., 2018),\nRvA (Niu et al., 2019). (4) The pretraining model:\nVDBERT (Wang et al., 2020).\nAs shown in Table 1 and Table 2, our MITVG,\nwhich explicitly locates related objects guided by\nthe textual entities and implements a multimodal\nincremental transformer to incrementally build the\nrepresentation of the dialogue history and the im-\nage, achieves comparable performance on the Vis-\nDial v0.9 and v1.0 datasets. SpeciÔ¨Åcally, our model\noutperforms previous work by a signiÔ¨Åcant margin\nboth on the VisDial v0.9 dataset (0.87 on MRR,\n0.31 on R@1, 1.17 on R@5, 0.75 on R10) and the\nVisDial v1.0 dataset (0.98 on MRR, 0.76 on R@1,\n1.23 on R@5, 1.28 on R10, 0.82 on Mean, and\nDMRM MITVG\nMethod 1 (M1) 0.62 0.76\nMethod 2 (M2) 0.59 0.70\nTable 3: Human evaluation on 100 sampled responses\non VisDial val v1.0. M1: percentage of responses pass\nthe Turing Test. M2: percentage of responses evaluated\nbetter than or equal to human responses.\n1.00 on NDCG). The improvement of R@10 is the\nlargest and our method also gains a large increase\non MRR and R@1 due to the explicit modeling\nof multiple modalities (Seeing Sec 3.5 for further\nquantitative analysis).\nAs shown in Table 3, we conduct human study\nto further prove the effectiveness of our model.\nOur model achieves the highest scores both on the\nmetric M1 (0.76) and M2 (0.70) compared with\nthe previous model, DMRM (Chen et al., 2020a).\nThese results show that our model can generate a\nbetter contextually and visually coherent response.\n3.4 Ablation Study\nWe also conduct an ablation study to illustrate the\nvalidity of our proposed Multimodal Incremental\nTransformer with Visual Grounding. The results\nare shown in Table 4.\nWe implement Multimodal Incremental Trans-\nformer without Visual Grounding (‚ÄòMITVG w/o\nVG‚Äô) to verify the validity of visual grounding.\nAs shown in Table 4, comparing ‚ÄòMITVG w/o\nVG‚Äô with MITVG, we Ô¨Ånd the metrics decrease\nobviously (0.46 on MRR, 0.60 on R@1, 0.68 on\nR@5, 0.46 on R@10 and 0.59 on Mean) if visual\ngrounding is deleted from MITVG. This observa-\ntion demonstrates the validity of visual grounding.\nTo verify the effectiveness of the incremental\ntransformer architecture, we implement a Multi-\nmodal Incremental LSTM without Visual Ground-\ning (‚ÄòMI-LSTM w/o VG‚Äô). A 3-layer bidirectional\nLSTM (Schuster and Paliwal, 1997) with multi-\nhead attention and a 1-layer LSTM with GCA are\napplied for encoder and decoder, respectively. All\nthe LSTM hidden state size is 512. Results in\nTable 4 demonstrate the effectiveness of our incre-\nmental transformer architecture (compare ‚ÄòMITVG\nw/o VG‚Äô with ‚ÄòMI-LSTM w/o VG‚Äô). Results from\nthe comparison between ‚ÄòMITVG w/o VG‚Äô and\nDMRM (Chen et al., 2020a) also show the validity\nof our incremental transformer to some extent.\n443\nQ1: how tall is the stack ?\nGT: 3 suitcases             Ours: 3 suitcases \nQ2: what color are they ?\nGT: blue and 2 red       Ours : blue and 2 red\nQ3: what do you think they contain ?\nGT: probably clothes   Ours: probably clothes\nCaption: a stack of luggage below a framed photo\nof a map\nQ1: is the photo in color ?\nGT: yes                  Ours: yes\nQ2: how many giraffes ?\nGT: more than 3    Ours: 3\nQ3: is it daytime ?\nGT: yes                  Ours: yes\nCaption: several giraffes gather at an elevated \nplatform to take food from zoo visitors\n(ùëé) (ùëè)\nFigure 5: Case study. The text marked in blue indicates the dialogue topic. The answers marked in green and red\nindicate the right and wrong answers, respectively. Our MITVG often generates right responses (marked in green)\nin keeping with human answers.\nModel MRR R@1 R@5 R@10 Mean\nDMRM 50.16 40.15 60.02 67.21 15.19\nMITVG 51.14 41.03 61.25 68.49 14.37\nMITVG w/o VG50.68 40.43 60.57 68.03 14.96\nMI-LSTM w/o VG50.02 39.85 59.86 67.16 15.78\nTable 4: Ablation study of our proposed model on Vis-\nDial val v1.0. ‚ÄúMI-LISM‚Äù indicates Multimodal Incre-\nmental LSTM. ‚ÄúVG‚Äù indicates visual grounding.\nTrain ValidationTest\nVisDial v0.92.04 1.95 -\nVisDial v1.02.05 1.93 1.93\nTable 5: Average number of the grounded objects in\neach question.\n3.5 Case Study\nAs shown in Table 5, we calculate the average num-\nber of the objects associated with entities in each\nquestion for assistant analysis. As shown in Fig-\nure 5 (a), owing to the explicit understanding of\nvisual content via visual grounding and the mul-\ntimodal incremental transformer architecture, our\nMITVG generates responses in keeping with hu-\nman answers. For example, while answering the\nquestion Q1 ‚Äò‚Äòhow tall is the stack ?‚Äù and Q2 ‚Äúwhat\ncolor are they ?‚Äù, our model grounds the three suit-\ncases accurately via visual grounding, thus giving\nthe accurate responses ‚Äú3 suitcases‚Äù and ‚Äúblue and\n2 red‚Äù. However, as shown in Figure 5 (b), for\nquestions Q2, MITVG gives a wrong answer be-\ncause it focuses on wrong number of objects in the\nquestion by visual grounding.\n4 Related Work\nVisual Dialogue. Our work touches two\nbranches of the research in visual dialogue. One\nis how to leverage image features. Niu et al.\n(2019) utilize object-level image features as visual\nattention and reÔ¨Åne it by recursively reviewing the\ndialog history. Gan et al. (2019) and Chen et al.\n(2020a) regard the object-level image features\nas visual memory to infer answers progressively\nthrough multiple steps. The other is how to model\ndialogue history. Yang et al. (2019a) propose a new\ntraining paradigm inspired by actor-critic policy\ngradient (Sutton et al., 1999) for history-advantage\ntraining. Guo et al. (2020) represent each turn\ndialogue history with visual content as a node in\na context-aware graph neural network. Park et al.\n(2020) reÔ¨Åne history information from both topic\naggregation and context matching. Different from\nthese approaches, we explicitly establish speciÔ¨Åc\nmapping of objects and textual entities to exclude\nundesired visual content via visual grounding,\nand model multi-turn structure of the dialogue\nbased on visual grounding to develop a uniÔ¨Åed\nrepresentation combining multi-turn utterances\n444\nalong with the relevant objects.\nIncremental Structures. There are some suc-\ncesses on introducing the incremental structure into\ntasks related to dialog systems (Zilka and Jurcicek,\n2015; Coman et al., 2019; Li et al., 2019; Das et al.,\n2017). In particular, Coman et al. (2019) propose\nan incremental dialog state tracker which is updated\non a token basis from incremental transcriptions.\nLi et al. (2019) devise an incremental transformer\nto encode multi-turn utterances along with knowl-\nedge in related documents for document grounded\nconversations. Das et al. (2017) propose a dialog-\nRNN to produce an encoding for this round and a\nstate for next round. Our model is different from\nthese approaches mainly in two aspects: 1) we ex-\nplicitly model the relationship between modalities,\ni.e., textual utterance and image objects, in visual\ndialogue through visual grounding; 2) based on the\nexplicit association between modalities, our model\nincrementally encodes the dialogue history and the\nimage with well-designed incremental multimodal\narchitecture to sufÔ¨Åciently understand the dialogue\ncontent, thus generating better responses.\n5 Conclusion\nWe propose a novel Multimodal Incremental Trans-\nformer with Visual Grounding for visual dia-\nlogue, named MITVG, which consists of two key\nparts: visual grounding and multimodal incremen-\ntal transformer. Visual grounding aims to explicitly\nmodel the relationship between multiple modalities.\nBased on visual grounding, multimodal incremen-\ntal transformer aims to explicitly model multi-turn\ndialogue history in the order of the dialogue. Exper-\niments on the VisDial v0.9 and v1.0 datasets show\nthat our model achieves comparable performance.\nReferences\nShubham Agarwal, Trung Bui, Joon-Young Lee, Ioan-\nnis Konstas, and Verena Rieser. 2020. History for\nvisual dialog: Do we really need it? arXiv preprint\narXiv:2005.07493.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. SPICE: Semantic proposi-\ntional image caption evaluation. Adaptive Behavior,\n11(4):382‚Äì398.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei\nZhang. 2018. Bottom-up and top-down attention for\nimage captioning and visual question answering. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6077‚Äì6086.\nFeilong Chen, Fandong Meng, Jiaming Xu, Peng Li,\nBo Xu, and Jie Zhou. 2020a. DMRM: A dual-\nchannel multi-hop reasoning model for visual dialog.\nThirty-Fourth AAAI Conference on ArtiÔ¨Åcial Intelli-\ngence.\nXiuyi Chen, Fandong Meng, Peng Li, Feilong Chen,\nShuang Xu, Bo Xu, and Jie Zhou. 2020b. Bridg-\ning the gap between prior and posterior knowledge\nselection for knowledge-grounded dialogue genera-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 3426‚Äì3437.\nXiuyi Chen, Jiaming Xu, and Bo Xu. 2019. A work-\ning memory model for task-oriented dialog response\ngeneration. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2687‚Äì2693.\nAndrei C Coman, Koichiro Yoshino, Yukitoshi Murase,\nSatoshi Nakamura, and Giuseppe Riccardi. 2019.\nAn incremental turn-taking model for task-oriented\ndialog systems. arXiv preprint arXiv:1905.11806.\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi,\nand Rita Cucchiara. 2020. Meshed-memory trans-\nformer for image captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi\nSingh, Deshraj Yadav, Jos¬¥e MF Moura, Devi Parikh,\nand Dhruv Batra. 2017. Visual dialog. In Proceed-\nings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 326‚Äì335.\nZhe Gan, Yu Cheng, Ahmed EI Kholy, Linjie Li,\nJingjing Liu, and Jianfeng Gao. 2019. Multi-step\nreasoning via recurrent dual attention for visual di-\nalog. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 6463‚Äì6474.\nHaoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang,\nLei Wang, and Wei Xu. 2015. Are you talking to a\nmachine? dataset and methods for multilingual im-\nage question. In Advances in Neural Information\nProcessing Systems, pages 2296‚Äì2304.\nDalu Guo, Chang Xu, and Dacheng Tao. 2019a. Image-\nquestion-answer synergistic network for visual dia-\nlog. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 10434‚Äì\n10443.\nDan Guo, Hui Wang, and Meng Wang. 2019b. Dual\nvisual attention network for visual dialog. pages\n4989‚Äì4995.\nDan Guo, Hui Wang, Hanwang Zhang, Zheng-Jun\nZha, and Meng Wang. 2020. Iterative context-aware\ngraph inference for visual dialog. arXiv preprint\narXiv:2004.02194.\n445\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages\n770‚Äì778.\nXiaoze Jiang, Siyi Du, Zengchang Qin, Yajing Sun,\nand Jing Yu. 2020a. KBGN: Knowledge-bridge\ngraph network for adaptive vision-text reasoning in\nvisual dialogue. Proceedings of the 28th ACM Inter-\nnational Conference on Multimedia.\nXiaoze Jiang, Jing Yu, Yajing Sun, Zengchang Qin, Zi-\nhao Zhu, Yue Hu, and Qi Wu. 2020b. DAM: De-\nliberation, abandon and memory networks for gener-\nating detailed and non-repetitive responses in visual\ndialogue. arXiv preprint arXiv:2007.03310.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nSatwik Kottur, Jos¬¥e M. F. Moura, Devi Parikh, Dhruv\nBatra, and Marcus Rohrbach. 2018. Visual corefer-\nence resolution in visual dialog using neural module\nnetworks. ArXiv, abs/1809.01816.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Vi-\nsual genome: Connecting language and vision us-\ning crowdsourced dense image annotations. Interna-\ntional Journal of Computer Vision, 123(1):32‚Äì73.\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\nQian Li, and Jie Zhou. 2019. Incremental\ntransformer with deliberation decoder for docu-\nment grounded conversations. arXiv preprint\narXiv:1907.08854.\nJiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh,\nand Dhruv Batra. 2017. Best of both worlds: Trans-\nferring knowledge from discriminative learning to a\ngenerative visual dialog model. In Advances in Neu-\nral Information Processing Systems, pages 314‚Äì324.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\n2016. Hierarchical question-image co-attention for\nvisual question answering. In Advances In Neural\nInformation Processing Systems, pages 289‚Äì297.\nVishvak Murahari, Dhruv Batra, Devi Parikh, and Ab-\nhishek Das. 2020. Large-scale pretraining for visual\ndialog: A simple state-of-the-art baseline. Proceed-\nings of the European Conference on Computer Vi-\nsion.\nVan-Quang Nguyen, Masanori Suganuma, and\nTakayuki Okatani. 2020. EfÔ¨Åcient attention mech-\nanism for visual dialog that can handle all the\ninteractions between multiple inputs. Proceedings\nof the European Conference on Computer Vision.\nYulei Niu, Hanwang Zhang, Manli Zhang, Jianhong\nZhang, Zhiwu Lu, and Ji-Rong Wen. 2019. Recur-\nsive visual attention in visual dialog. InProceedings\nof the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 6679‚Äì6688.\nSungjin Park, Taesun Whang, Yeochan Yoon, and\nHueiseok Lim. 2020. Multi-view attention networks\nfor visual dialog. arXiv preprint arXiv:2004.14025.\nJiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang\nZhang. 2020. Two causal principles for improving\nvisual dialog. Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition.\nMengye Ren, Ryan Kiros, and Richard Zemel. 2015a.\nExploring models and data for image question an-\nswering. In Advances in Neural Information Pro-\ncessing Systems, pages 2953‚Äì2961.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015b. Faster R-CNN: Towards real-time ob-\nject detection with region proposal networks. In Ad-\nvances in Neural Information Processing Systems ,\npages 91‚Äì99.\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE Transactions\non Signal Processing, 45(11):2673‚Äì2681.\nRichard S Sutton, David A. McAllester, Satinder P.\nSingh, and Yishay Mansour. 1999. Policy gradient\nmethods for reinforcement learning with function ap-\nproximation. In Advances in Neural Information\nProcessing Systems, volume 12, pages 1057‚Äì1063.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998‚Äì6008.\nYue Wang, ShaÔ¨Åq Joty, Michael R Lyu, Irwin King,\nCaiming Xiong, and Steven CH Hoi. 2020. VD-\nBERT: A uniÔ¨Åed vision and dialog transformer with\nbert. arXiv preprint arXiv:2004.13278.\nQi Wu, Peng Wang, Chunhua Shen, Ian Reid, and An-\nton van den Hengel. 2018. Are you talking to me?\nreasoned visual dialog generation through adversar-\nial learning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n6106‚Äì6115.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In Proceedings of International Conference on\nMachine Learning, pages 2048‚Äì2057.\nTianhao Yang, Zheng-Jun Zha, and Hanwang Zhang.\n2019a. Making history matter: History-advantage\nsequence training for visual dialog. In The IEEE In-\nternational Conference on Computer Vision (ICCV).\n446\nZhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing\nHuang, Dong Yu, and Jiebo Luo. 2019b. A fast and\naccurate one-stage approach to visual grounding. In\nProceedings of the IEEE International Conference\non Computer Vision, pages 4683‚Äì4693.\nLukas Zilka and Filip Jurcicek. 2015. Incremental lstm-\nbased dialog state tracker. In 2015 IEEE Workshop\non Automatic Speech Recognition and Understand-\ning (Asru), pages 757‚Äì762. IEEE.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.769629716873169
    },
    {
      "name": "Transformer",
      "score": 0.6873636245727539
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5818697810173035
    },
    {
      "name": "Visualization",
      "score": 0.4766864776611328
    },
    {
      "name": "Ground",
      "score": 0.46400538086891174
    },
    {
      "name": "Computer vision",
      "score": 0.43060582876205444
    },
    {
      "name": "Natural language processing",
      "score": 0.37323981523513794
    },
    {
      "name": "Engineering",
      "score": 0.09282591938972473
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 17
}