{
  "title": "DrugGen enhances drug discovery with large language models and reinforcement learning",
  "url": "https://openalex.org/W4409568991",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4376447844",
      "name": "Mahsa Sheikholeslami",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5114763006",
      "name": "Navid Mazrouei",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1999438708",
      "name": "Yousof Gheisari",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A5114763007",
      "name": "Afshin Fasihi",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4288305009",
      "name": "Matin Irajpour",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A3045858910",
      "name": "Ali Motahharynia",
      "affiliations": [
        "Isfahan University of Medical Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4376447844",
      "name": "Mahsa Sheikholeslami",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114763006",
      "name": "Navid Mazrouei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1999438708",
      "name": "Yousof Gheisari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114763007",
      "name": "Afshin Fasihi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288305009",
      "name": "Matin Irajpour",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3045858910",
      "name": "Ali Motahharynia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4386807703",
    "https://openalex.org/W3112559629",
    "https://openalex.org/W4211238337",
    "https://openalex.org/W3201230437",
    "https://openalex.org/W4307447012",
    "https://openalex.org/W3165171933",
    "https://openalex.org/W2998571806",
    "https://openalex.org/W4391931607",
    "https://openalex.org/W4308891232",
    "https://openalex.org/W4391130256",
    "https://openalex.org/W4391878438",
    "https://openalex.org/W3124172763",
    "https://openalex.org/W4382132796",
    "https://openalex.org/W3196174677",
    "https://openalex.org/W4395053803",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2087535060",
    "https://openalex.org/W2970257897",
    "https://openalex.org/W6774364786",
    "https://openalex.org/W4402797674",
    "https://openalex.org/W4390747020",
    "https://openalex.org/W2496623717",
    "https://openalex.org/W4256732118",
    "https://openalex.org/W4400420147",
    "https://openalex.org/W4321483180",
    "https://openalex.org/W3093283395",
    "https://openalex.org/W4404391482",
    "https://openalex.org/W2170146596",
    "https://openalex.org/W2767891136",
    "https://openalex.org/W2154139219",
    "https://openalex.org/W4404458588",
    "https://openalex.org/W2055043387",
    "https://openalex.org/W4388455891",
    "https://openalex.org/W3094640617",
    "https://openalex.org/W2537679995",
    "https://openalex.org/W2151697120",
    "https://openalex.org/W2044834685",
    "https://openalex.org/W2768092025",
    "https://openalex.org/W2031168104",
    "https://openalex.org/W3167019654",
    "https://openalex.org/W1985588649",
    "https://openalex.org/W4311218872"
  ],
  "abstract": null,
  "full_text": "DrugGen enhances drug discovery \nwith large language models and \nreinforcement learning\nMahsa Sheikholeslami1,2, Navid Mazrouei1, Yousof Gheisari1,3, Afshin Fasihi2, \nMatin Irajpour 1,4 & Ali Motahharynia 1,5\nTraditional drug design faces significant challenges due to inherent chemical and biological \ncomplexities, often resulting in high failure rates in clinical trials. Deep learning advancements, \nparticularly generative models, offer potential solutions to these challenges. One promising algorithm \nis DrugGPT, a transformer-based model, that generates small molecules for input protein sequences. \nAlthough promising, it generates both chemically valid and invalid structures and does not incorporate \nthe features of approved drugs, resulting in time-consuming and inefficient drug discovery. To address \nthese issues, we introduce DrugGen, an enhanced model based on the DrugGPT structure. DrugGen \nis fine-tuned on approved drug-target interactions and optimized with proximal policy optimization. \nBy giving reward feedback from protein–ligand binding affinity prediction using pre-trained \ntransformers (PLAPT) and a customized invalid structure assessor, DrugGen significantly improves \nperformance. Evaluation across multiple targets demonstrated that DrugGen achieves 100% valid \nstructure generation compared to 95.5% with DrugGPT and produced molecules with higher predicted \nbinding affinities (7.22 [6.30–8.07]) compared to DrugGPT (5.81 [4.97–6.63]) while maintaining \ndiversity and novelty. Docking simulations further validate its ability to generate molecules targeting \nbinding sites effectively. For example, in the case of fatty acid-binding protein 5 (FABP5), DrugGen \ngenerated molecules with superior docking scores (FABP5/11, -9.537 and FABP5/5, -8.399) compared \nto the reference molecule (Palmitic acid, -6.177). Beyond lead compound generation, DrugGen also \nshows potential for drug repositioning and creating novel pharmacophores for existing targets. By \nproducing high-quality small molecules, DrugGen provides a high-performance medium for advancing \npharmaceutical research and drug discovery.\nKeywords Drug design, Drug repurposing, Large language model, Reinforcement learning, Molecular \ndocking\nTraditional drug design often falls short in handling the vast chemical and biological space features involved in \nligand-receptor interactions1,2. Usually, a major proportion of suggested drug candidates fail in clinical trials 3, \nmaking drug discovery a time-consuming and costly process. Recent advances in deep learning (DL), particularly \nin generative models, offer promising solutions for these obstacles4,5. Deep learning models have been extensively \nused in molecular design6,7, pharmacokinetics8–11, pharmacodynamics predictions12, and toxicity assessments10. \nThese models improve the efficiency and accuracy of various tasks in drug development, contributing to \ndifferent stages of drug discovery and optimization projects 13,14. However, due to the insufficiency of available \ndatasets, complexity of drug-target interactions, and complication of manipulating complex chemical structures, \ngenerative DL models also seem to be insufficient in proposing optimal answers to drug design problems 15. \nNevertheless, with the advancement of transformer-based architecture in large language models (LLMs), new \nhorizons have opened up in various biological contexts. ProGen, a model developed to design new proteins with \ndesired functionality and protein–ligand binding affinity prediction using pre-trained transformers (PLAPT), \na model for protein–ligand binding affinity prediction, are successful examples of the application of LLMs in \nbioinformatics16,17. DrugGPT, an LLM based on the generative pre-trained transformer (GPT) architecture 18 \n1Regenerative Medicine Research Center, Isfahan University of Medical Sciences, Isfahan 81746 73461, Iran. \n2Department of Medicinal Chemistry, School of Pharmacy, Isfahan University of Medical Sciences, Isfahan, Iran. \n3Department of Genetics and Molecular Biology, Isfahan University of Medical Sciences, Isfahan, Iran. 4Isfahan \nCardiovascular Research Center, Cardiovascular Research Institute, Isfahan University of Medical Sciences, Isfahan, \nIran. 5Isfahan Neuroscience Research Center, Isfahan University of Medical Sciences, Isfahan, Iran. email: \n2012irajpour@gmail.com; alimotahharynia@gmail.com\nOPEN\nScientific Reports |        (2025) 15:13445 1| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports\n\nis another example that has shown potential in generating novel drug-like molecules having interactions with \nbiological targets19.\nDrugGPT leverages the transformer architecture to comprehend structural properties and structure–activity \nrelationships. Receiving the amino acid sequence of a given target protein, this model generates simplified \nmolecular input line entry system (SMILES) 20 strings of interacting small molecules. By learning from large \ndatasets of known drugs and their targets, DrugGPT can propose new compounds with desired properties by \nemploying autoregressive algorithms for a stable and effective training process 21, thus accelerating the lead \ndiscovery phase in drug development. However, the effectiveness of generative models in drug discovery relies \nheavily on the quality and relevance of the training data 5. Models trained on comprehensive and accurately \ncurated datasets are more likely to produce viable drug candidates22. Additionally, fine-tuning these models can \nenhance their performance for predictive applications23.\nIn this study, we developed “DrugGen” , an LLM based on the DrugGPT architecture, finetuned using a \ncurated dataset of approved drug-target pairs; which is further enhanced using a policy optimization method. \nBy utilizing this approach, DrugGen is optimized to generate drug candidates with optimized properties. \nFurthermore, we evaluated the model’s performance using custom metrics—validity, diversity, and novelty—\nto comprehensively assess the quality and properties of the generated compounds. Our results indicated that \nDrugGen generates chemically sound and valid molecules in comparison with DrugGPT while maintaining \ndiversity and validity of generated structures. Notably, DrugGen excels in generating molecules with higher \npredicted binding affinities, increasing the likelihood of strong interactions with biological targets. Docking \nsimulations further demonstrated the model’s capability to accurately target binding sites and suggest new \npharmacophores. These findings highlight DrugGen’s promising potential to advance pharmaceutical research. \nMoreover, we proposed evaluation metrics that can serve as objective and practical measures for comparing \nfuture models.\nResults\nIn order to develop an algorithm to generate drug-like structures, we gathered a curated dataset of approved \ndrug-target pairs. We began by selecting a pre-trained model and then enhanced its performance through a two-\nstep process. First, we employed supervised fine-tuning (SFT) on a dataset of approved sequence-SMILES pairs \nto fine-tune the model. Next, we utilized a reinforcement learning algorithm—proximal policy optimization \n(PPO)—along with a customized reward system to further optimize its performance. The final model was named \nDrugGen. The schematic design of the study is illustrated in Fig. 1.\nFig. 1. Schematic representation of model development and evaluation. The top section illustrates the \ndataset creation and the training of DrugGen through supervised fine-tuning (SFT) and proximal policy \noptimization (PPO) using a customized reward function. The bottom section outlines the assessment process, \nbased on validity, diversity, novelty, and binding affinity for both DrugGen and DrugGPT, along with docking \nsimulations for DrugGen.\n \nScientific Reports |        (2025) 15:13445 2| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nDrugGen is effectively fine-tuned on a dataset of approved drug-target\nSupervised fine-tuning using the SFT trainer exhibited a steady decrease in training and validation loss over the \nepochs, indicating effective learning (Fig. 2A and Supplementary file 1). After three epochs of training, the loss \nof both the training and validation datasets reached a plateau. Therefore, checkpoint number three was selected \nfor the second phase. In the second phase, the model was further optimized using PPO based on the customized \nreward system. Over 20 epochs of optimization, the model generated 30 unique small molecules for each target \nin each epoch, ultimately reaching a plateau in the reward diagram (Fig. 2B and Supplementary file 2).\nDrugGen generates valid, diverse, and novel small molecules\nEight proteins were selected for models assessments which include two targets with a high probability of \nassociation with diabetic kidney disease (DKD) from the DisGeNet database, angiotensin-converting enzyme \n(ACE) and peroxisome proliferator-activated receptor gamma (PPARG) and six proteins without known approved \ndrugs, i.e., galactose mutarotase (GALM), putative fatty acid-binding protein 5-like protein 3 (FB5L3), short-\nwave-sensitive opsin 1 (OPSB), nicotinamide phosphoribosyltransferase (NAMPT), phosphoglycerate kinase 2 \n(PGK2), and fatty acid-binding protein 5 (FABP5), that identified as having a high probability of being targeted \nby approved small molecules through our newly developed druggability scoring algorithm, DrugTar24. For each \ntarget, 500 molecules were generated. The validity of generated molecules was 95.45% and 99.90% for DrugGPT \nand DrugGen, respectively (Chi-Squared, P < 10–38, Supplementary file 3). These molecules had an average \ndiversity of 84.54% [74.24–90.48] for DrugGPT and 60.32% [38.89–92.80] for DrugGen ( U = 358,245,213,849, \nP = 0, Fig. 3A and Supplementary file 4), indicating the generation of more similar molecules in DrugGen. These \nresults suggest that DrugGen still generates a wide range of structurally diverse drug candidates rather than \nproducing similar or redundant molecules. To assess the novelty of generated molecules, 100 unique small \nmolecules were generated for each target. The validity scores for DrugGPT and DrugGen were in agreement with \nprevious results (95.5% and 100%, respectively, Chi-Squared, P < 10–8, Supplementary file 5). After removing \ninvalid structures, the novelty scores for DrugGPT and DrugGen were 66.84% [55.28–73.57] and 41.88% [24–\n59.66], respectively ([Mann–Whitney, U = 475980, P < 10–80], Fig. 3B and Supplementary file 5), indicating that \nfewer novel molecules were generated in DrugGen. These values indicate a good balance between diversity and \nnovelty for DrugGen.\nDrugGen generates small molecules with high affinity for their targets\nWe used two different measures to assess the binding affinity of the generated molecules to their respective \ntargets: PLAPT, an LLM for predicting binding affinity, and molecular docking.\nPLAPT: The same set of small molecules generated in novelty assessment (100 unique small molecules for \neach target) were used for assessing the quality of generated structures. Except for FABP5, DrugGen consistently \nproduced small molecules with significantly higher binding affinities compared to DrugGPT ([7.22 [6.30–8.07] \nvs. 5.81 [4.97–6.63], U = 137,934, P < 10–85], Fig. 3C, Table 1, and Supplementary file 5). This finding underscores \nDrugGen’s superior capability to generate high-quality structures.\nMolecular docking: Docking simulations were performed on the targets that had reliable protein data bank \nfiles and could be successfully re-docked, i.e., FABP5, NAMPT, and ACE. To further evaluate the model’s ability \nto generate potential drug candidates, docking was also performed against approved drugs, assessing how \nwell the generated molecules compared to existing therapeutics. GALM protein was included to emphasize \nthe model’s capability to create molecules for unexplored targets with no reference molecules. The results \nshowed that the generated molecules included agents with high binding affinities for the binding site of their \nrespective targets (Table 2 and Supplementary file 6). Except for ACE which has multiple proven binding sites \nwith docked molecules binding to different locations than the reference molecule, all other docked molecules \nwere positioned in the same binding site as the reference in their best-docked poses (Fig.  4). Furthermore, the \nmodel has generated molecules with better docking scores than the reference for FABP5 (-9.537 and -8.399 vs. \n-6.177) and NAMPT (-8.381 vs. -8.300). Notably, for NAMPT, the model suggested a novel pharmacophore that \nFig. 2. Training process of DrugGen. (A) Learning curve of the model during supervised fine-tuning (SFT) \nand (B) reward trend during proximal policy optimization (PPO).\n \nScientific Reports |        (2025) 15:13445 3| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nFig. 3. Comparison of molecular diversity, novelty, and binding affinity across different targets. (A) \nComparison of molecular diversity distribution is shown as the frequency of the “1 – Tanimoto similarity” \nin percent for each target. (B) Scatter plots comparing novelty, with the “1 – Tanimoto similarity” in percent \nplotted against the molecular index. (C) Violin plots depicting the distribution of binding affinity for each \ntarget.\n \nScientific Reports |        (2025) 15:13445 4| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\noccupies the same active site as the reference molecule (Fig. 5). ID cards of generated small molecules with their \nrelated SMILES are presented in Supplementary file 7.\nDiscussion\nIn this study, we developed DrugGen, a large language model designed to generate small molecules based on \nthe desired targets as input. DrugGen is based on a previously developed model known as DrugGPT, achieving \nimprovements by supervised fine-tuning on approved drugs and reinforcement learning. These improvements \naim to facilitate the generation of novel small molecules with stronger binding affinities and a higher probability \nof approval in future clinical trials. The results indicate that DrugGen can produce high-affinity molecules with \nrobust docking scores, highlighting its potential to accelerate the drug discovery process.\nDrugGen is primarily based on the DrugGPT, which utilizes a GPT-2 architecture trained on datasets \ncomprising SMILES and SMILES-protein sequence pairs for generation of small molecules. Although DrugGPT \nshows promise, it became evident that the creation of high-quality small molecules requires more than merely \nensuring ligand-target interactions. These molecules may also exhibit essential properties, including favorable \nchemical characteristics (such as stability and the absence of cytotoxic substructures), pharmacokinetic profiles \n(acceptable ADME properties—absorption, distribution, metabolism, and excretion), pharmacodynamic \nattributes (efficacy and potency)25–27. Hence, due to the hypothesis that approved drugs have intrinsic properties \nthat make them become approved 28, DrugGen was fine-tuned on approved sets of small molecules. This fine-\ntuning was enhanced through binding affinity feedback from another LLM, PLAPT, resulting in improved quality \nof generated molecules. Our findings demonstrate that DrugGen produces small molecules with significantly \nbetter chemical validity and binding affinity compared to DrugGPT while maintaining chemical diversity.\nTo assess the capability of DrugGen in generating high-quality molecules, we selected eight targets. The \ninclusion of six targets without known approved small molecules demonstrates DrugGen’s potential to introduce \nnovel candidates for previously untargeted or unexplored therapeutic areas. Among the assessed targets, \ngenerated molecules showed enhanced validity and stronger binding affinities compared to those produced \nby DrugGPT. This consistency suggests that DrugGen’s reinforcement learning process effectively enhances \nits ability to generate potent drug candidates. Moreover, docking simulations further confirmed the quality of \nSmall molecules XP GScore\nNAMPT40 -8.381\nDaporinad -8.3\nNAMPT23 -8.187\nLisinopril -19.489\nACE17 (Enalapril) -15.538\nACE14 (Captopril) -9.677\nACE28 -8.964\nACE29 -6.405\nGALM13 -8.905\nGALM2 -7.061\nGALM7 -6.913\nFABP5/11 -9.537\nFABP5/5 -8.399\nPalmitic acid -6.177\nTable 2. The extra precision (XP) docking scores of the generated ligands and their respective references (if \navailable).\n \nTargets DrugGPT DrugGen U statistics P\nACE 5.71 [5.10–6.71] 8.43 [6.65–9.06] 1475  < 10–16*\nPPARG 6.32 [5.75–6.74] 7.39 [6.61–7.95] 2208  < 10–10*\nGALM 6.12 [4.96–6.73] 6.92 [6.04–7.73] 2767  < 10–6*\nFB5L3 5.35 [4.73–6.38] 6.94 [6.36–7.48] 1723  < 10–14*\nOPSB 5.43 [4.26–6.14] 7.62 [6.84–8.07] 842  < 10–22*\nNAMPT 5.84 [5.16–6.75] 7.00 [6.19–7.70] 2616  < 10–7*\nPGK2 5.23 [4.62–6.22] 7.34 [6.36–8.52] 1212  < 10–18*\nFABP5 6.30 [5.35–7.18] 6.69 [5.80–7.60] 4382 1\nTable 1. Statistical analysis of binding affinities of DrugGPT vs. DrugGen. All data are presented as median \n[Q1-Q3]. Data are compared using Mann–Whitney U test. P is corrected for multiple comparisons using \nBonferroni method. *P < 0.05.\n \nScientific Reports |        (2025) 15:13445 5| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nFig. 5. Comparison of pharmacophores of NAMPT inhibitors in the same active site. Daporinad (left) and \nNAMPT40 (right) with fundamentally different pharmacophores, both placed in the active site of NAMPT.\n \nFig. 4. Visualization of ligand binding in active sites across selected targets. (A) FABP5/11 (dark pink) and \nPalmitic acid (light pink, reference) in the FABP5 active site. (B) GALM13 (light green) and GALM2 (lime) \nassociated with GALM. (C) NAMPT40 (teal) and Daporinad (blue, reference) in the NAMPT active site. (D) \nACE29 (red), ACE28 (yellow), ACE17 (peach), ACE14 (orange), and Lisinopril (copper, reference) associated \nwith ACE.\n \nScientific Reports |        (2025) 15:13445 6| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nDrugGen in generating high-quality small molecules. The comparison of docking scores between generated \nand reference molecules, NAMPT40 vs. Daporinad and FABP5/11 and FABP5/5 vs. Palmitic acid, shows that \nDrugGen can design molecules with predicted interactions stronger than the known drugs. This observation \nhighlights DrugGen’s capability to innovate beyond the existing drug design approaches. Furthermore, the \ndiversity of generated molecules, reflected in the wide range of docking scores, emphasizes the model’s flexibility \nin producing varied chemical structures. Additionally, in the case of NAMPT, the model generated one structure \nwith a strong docking score possessing a pharmacophore very different from that of the reference molecules, \nmeaning that core drug structure was dissimilar to the reference molecule. This structure occupied the same \nbinding site as the reference molecule, which is a potentially new pharmacophore for this target. In addition to \nthese improvements, in the process of reinforcement learning, penalties were applied for generating repetitive \nstructures, resulting in a diverse and valid set of molecules whilst retaining the possibility of regenerating \napproved drugs in the case of drug repurposing 29. Thus, DrugGen demonstrates applicability in both de novo \ndrug design and repurposing efforts.\nDespite these achievements, our study has some limitations that should be considered in future research. \nVariability in binding affinity results across assessed targets was observed. For instance, FABP5’s performance \nimprovement was less pronounced compared with others. This might suggest that with certain target classes \nor protein sequences, unique challenges emerge for our model, requiring additional fine-tuning or alternative \nstrategies for further optimization. In addition, DrugGen cannot target a specific binding site, as can be seen \nin the case of ACE, which has multiple binding sites 30. Ligand prediction using the DrugGen model led to \nmolecules with fairly strong ligand binding to different binding sites; however, this may not be desirable in \nsome cases. The existing reward function relies on an affinity-predictor deep learning model that has inherent \naccuracy and specificity limitations due to the limitations of the databases and input representation, which could \nbe addressed in future works. Our model is primarily focused on predicting novel cores and structures for \ntargets with limited bioactive molecules, thus it does not generate fully optimized structures. These predicted \nstructures should undergo structural manipulation for structural optimization to better fit the active site of \nthe target. Future improvements will involve incorporating active site interactions into the reward system to \nenhance structural accuracy. Additionally, integrating this model with others like PocketGen 31, which focuses \non designing ligand-binding pockets, could be a promising approach that enables the joint optimization of both \nligand generation and protein pocket design. This combined approach could enhance drug discovery, leading \nto more precise, effective, and innovative therapeutics for a wide range of diseases. Finally, the reliance on in \nsilico validation, while useful, needs to be complemented with experimental validation to confirm the practical \nefficacy and safety of the generated molecules.\nIn conclusion, DrugGen represents a powerful tool for early-stage drug discovery, with the potential to \nsignificantly accelerate the process of identifying novel lead compounds. With further refinement and integration \nwith experimental validation, DrugGen could become an integral part of future drug discovery pipelines, \ncontributing to the development of new therapeutics across a wide range of diseases.\nMaterials and methods\nDataset preparation\nA dataset of small molecules, each approved by at least one regulatory body, was collected to enhance the safety \nand relevance of the generated molecules. First, 1710 small molecules from the DrugBank database (version: \n5.1.10) were retrieved 32,33, 117 of which were labeled as withdrawn. After initial assessments of withdrawn \ndrugs by a physician (Ali Motahharynia) and a pharmacist (Mahsa Sheikholeslami) through a literature review, \nconsensus was reached to omit 50 entries due to safety concerns. Consequently, 1660 approved small molecules \nand their respective targets were selected to retrieve target-related sequences from UniProt database 34,35. From \nthe total of 2116 approved drug targets, retrieved from DrugBank database, 27 were not present in the UniProt \ndatabase. After further assessment, these 27 proteins were manually replaced with equivalently reviewed UniProt \nIDs by searching the UniProt database, identifying identical proteins via NCBI Gene, and confirming the matches \nusing basic local alignment search tools (BLAST) 36. The protein with UniProt ID “Q5JXX5” was deleted from \nthe UniProt database and therefore, omitted from the collected dataset as well. Finally, 1660 small molecules and \n2093 related protein targets were selected. Available SMILEs (1634) were retrieved from DrugBank, ChEMBL37, \nand ZINC20 databases38. Protein sequences were retrieved from the UniProt database.\nData preprocessing\nSimilar to the structure used by DrugGPT, the small molecules and target sequences were merged into the pair \nof a string consisting of protein sequence and SMILES in the following format: “ <|startoftext|> +  < P >  + target \nprotein sequence +  < L >  + SMILES + <|endoftext|> .\" To ensure the compatibility of this input format with the \noriginal model, the resulting strings were tokenized using the trained DrugGPT’s byte-pair encoding (BPE) \ntokenizer (53,083 tokens). The strings were padded to the maximum length of 768, and longer strings were \ntruncated. The “ <|startoftext|> ” , “ <|endoftext|> ” , and “ < PAD > ” were defined as special tokens.\nDrugGen development overview\nUsing the supervised fine-tuning (SFT) trainer module from the transformer reinforcement learning (TRL) \nlibrary (version: 0.9.4)39, the original DrugGPT model was finetuned on our dataset. Afterward, reinforcement \nlearning was applied to further improve the model. For this purpose, a Tesla V100 GPU with 32 GB of VRAM, \n64 GB of RAM, and a 4-core CPU were utilized for both phases, i.e., SFT and reinforcement learning using a \nPPO trainer.\nScientific Reports |        (2025) 15:13445 7| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nSupervised fine-tuning\nThe training dataset consisted of 9398 strings. The base model was trained using the SFT trainer class for five \nepochs with the following configuration: Learning rate: 5e-4, batch size: 8, warmup steps (linear warmup \nstrategy): 100, and eval steps: 50. AdamW optimizer with a learning rate of 5e-4 and epsilon value of 1e-8 was \nused for optimizing the model parameters. The model’s performance on the training and validation sets (ratio of \n8:2) was evaluated using the cross-entropy loss function during the training phase.\nProximal policy optimization\nHugging Face’s PPO Trainer, which is based on OpenAI’s original method for “Summarize from Feedback” 40 \nwas used in this study. PPO is a reinforcement learning algorithm that improves the policy by taking small steps \nduring optimization, avoiding overly large updates that could lead to instability. The key formula used in PPO is:\n LCLIP (θ)= Et[min(rt(θ)At, clip(rt(θ), 1 − ϵ, 1+ ϵ)At)] (1)\nIn this equation, LCLIP(θ) represents the clipped objective function that PPO aims to optimize during training. \nThe expectation Et denotes the average over time steps t, capturing the overall performance of the policy. The \nterm rt(θ) is the probability ratio of taking action ɑt under the new policy compared to the old policy, defined as \nrt (θ)= πθ( at|st)\nπθold ( at|st) . The advantage estimate At quantifies the relative value of the action taken in relation to \nthe expected value of the policy. The clipping function, clip(rt(θ), 1 − ϵ, 1+ ϵ), restricts the ratio to a defined \nrange, preventing large updates to the policy that could destabilize training. This formulation allows PPO to \nbalance exploration and stability, enabling effective policy updates while minimizing the risk of performance \ndegradation. There are three main phases in training a model with PPO. First, the language model generates \na response based on an input query in a phase called the rollout phase. In our study, the queries were protein \nsequences, and the generated responses were SMILES strings. Then in the evaluation phase, the generated \nmolecules were assessed with a custom model that predicts binding affinity. Finally, the log probabilities of the \ntokens in the generated SMILES sequences were calculated based on the query/response pairs. This step is also \nknown as the optimization phase. Additionally, to maintain the generated responses within a reasonable range \nfrom the reference language model, a reward signal was introduced in the form of the Kullback–Leibler (KL) \ndivergence between the two outputs. This additional signal ensures that the new responses do not deviate too far \nfrom the original model’s outputs. Thus, PPO was applied to train the active language model.\nIn our study, the rollout section had the following generation parameters: “do_sample”: True, “top_k”: 9, \n“top_p”: 0.9, “max_length”: 1024, and “num_return_sequences”: 10. In each epoch, generation was continued \nuntil 30 unique small molecules were generated for each target. Keeping initial model’s structure in mind, the \ndataset was filtered based on the length of each protein sequence. After creating the prompts according to the \nspecified format, i.e., “ <|startoftext|> +  < P >  + target protein sequence +  < L > ” , prompts with a tensor size \ngreater than 768 were omitted, resulting in 2053 proteins (98.09% of the initial dataset).\nThe PPO trainer configuration included: “mini_batch_size”: 8, “batch_size”: 240, and “learning_rate”: 1.41e-\n5. Score scaling and normalization were handled with the PPO trainer’s built-in functions.\nReward function\nPLAPT: PLAPT, a cutting-edge model designed to predict binding affinities with remarkable accuracy was \nused as a reward function. PLAPT leverages transfer learning from pre-trained transformers, ProtBERT and \nChemBERTa, to process one-dimensional protein and ligand sequences, utilizing a branching neural network \narchitecture for the integration of features and estimation of binding affinities. The superior performance of \nPLAPT has been validated across multiple datasets, where it achieved state-of-the-art results 16. The affinities \nof the generated structures with their respective targets were evaluated using PLAPT’s neg_log10_affinity_M \noutput.\nCustomized invalid structure assessor: We developed a customized algorithm using RDKit library (version: \n2023.9.5)41 to assess invalid structure, where specific checks were performed to identify potential issues such as \natom count, valence errors, and parsing errors. Invalid structures, including those with fewer than two atoms, \nincorrect valence states, or parsing failures were flagged and penalized accordingly. To promote the generation \nof valid molecules, a reward value of 0 was assigned to any invalid SMILES structures. These reward systems \nprovide a rigorous scoring system for model development.\nTo further shift the model toward generating novel molecules, a multiplicative penalty was applied to the \nreward score when a generated SMILES string matched a molecule already present in the approved SMILES \ndataset. Specifically, the reward was multiplied by 0.7 for such occurrences, to retain a balance between generating \nnew structures as well as repurposing approved drugs.\nDrugGen assessment\nTo evaluate the performance of DrugGen, several metrics were employed to measure its efficacy in generating \nviable and high-affinity drug candidates. For this purpose, eight targets consisting of two DKD targets with the \nhighest score in DisGeNet database (version 3.12.1) 42, i.e., “ ACE” and “PPARG” and six targets without any \nknown approved small molecules for them were selected. The selection of these six targets was according to \nour recent study “DrugTar Improves Druggability Prediction by Integrating Large Language Models and Gene \nOntologies”24. According to this study, 6 out of the 10 most probable proteins for future targets were selected. \nThe selected targets are “GALM” , “FB5L3” , “OPSB” , “NAMPT” , “PGK2” , and “FABP5” . The generative quality of \nDrugGPT and DrugGen in terms of validity, diversity, novelty, and binding affinity was assessed. Additionally, \nwe performed in silico validation of the molecules generated by DrugGen using a rigorous docking method.\nScientific Reports |        (2025) 15:13445 8| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nValidity assessment The validity of the generated molecules was evaluated using the previously mentioned cus-\ntomized invalid structure assessor. The percentage of valid to total generation was reported as models’ capability \nto construct valid structures.\nDiversity assessment To assess the diversity of the generated molecules, 500 ligands were generated for each \ntarget by DrugGPT and DrugGen. The diversity of the generated molecules was quantitatively assessed using the \nTanimoto similarity index43. The diversity evaluation process involved the following steps: First, each generated \nmolecule was converted to its corresponding molecular fingerprint using Morgan fingerprints (size = 2048 bits, \nradius = 2)44. For each molecule, pairwise Tanimoto similarities were calculated between all possible pairs of \nfingerprints, and the average value was calculated. Thus, the diversity of the generated set was determined as the \n“1—average of Tanimoto similarity” within a generated batch. The distribution of diversity for each target was \nplotted. The invalid structures were not involved in diversity assessments. Statistical analyses were performed \nusing Mann–Whitney U test.\nNovelty assessment For each target, a set of 100 unique molecules was generated by DrugGPT and DrugGen. \nThe novelty of the generated molecules was evaluated by comparing them to a dataset of approved drugs. After \nconverting the molecules into Morgan fingerprints, the similarity of each generated molecule to the approved \ndrugs was calculated using Tanimoto similarity index, retaining only the maximum similarity value. The novelty \nwas reported as the “1—max_Tanimoto similarity” . The invalid structures were not included in the novelty as-\nsessments. Statistical analyses were performed using Mann–Whitney U test.\nPLAPT binding affinity assessment  The same set of molecules generated during the novelty assessment was \nused to evaluate the binding affinities of the compounds produced by DrugGPT and DrugGen. The invalid \nstructures were involved in the binding affinity assessments. Statistical analysis was conducted using the Mann–\nWhitney U test, and corrections for multiple comparisons were applied using the Bonferroni method.\nMolecular docking Molecular docking was conducted for selected targets with available protein data bank \n(PDB) structures, specifically ACE, NAMPT, GALM, and FABP5. A set of 100 newly generated molecules, fol -\nlowing duplicate removal, were docked into the crystal structures of ACE (PDB ID: 1o86), NAMPT (PDB ID: \n2gvj), GALM (PDB ID: 1snz), and FABP5 (PDB ID: 1b56). Overall, blind docking 45 was employed for all 122 \ngenerated molecules and their references to thoroughly search the entire protein surface for the most favorable \nactive site (Supplementary file 6 and 7). The reference ligands used were Lisinopril for ACE and Palmitic acid \nfor FABP5, both of which were bound in the active site. For NAMPT, Daporinad, a molecule currently in phase \n2 clinical trials, served as the highest available reference. In the case of GALM, no reference ligand was found. \nThe retrieved PDB files were prepared using the protein preparation wizard46 available in the Schrödinger suite, \nensuring the addition of missing hydrogens, assignment of appropriate charge states at physiological pH, and \nreconstruction of incomplete side chains and rings. LigPrep 47 with the OPLS4 force field 48 was employed to \ngenerate all possible stereoisomers and ionization states at pH 7.4 ± 0.5. The prepared structures were used for \ndocking.\nDocking simulations were performed using the GLIDE program 49. Ligands were docked using the extra \nprecision (XP) protocol. Ligands were allowed full flexibility during the docking process, while the protein was \nheld rigid. The information of the grid boxes is summarized in Table 3.\nThe GLIDE XP scoring function was used to evaluate docking poses. Negative values of the GLIDE score (XP \nGScore) were reported for readability. The robustness of the docking procedures was validated by redocking the \nreference ligands into their respective binding sites. The computed root-mean-squared deviation (RMSD) values \nwere 0.7233Å, 0.2961Å, and 2.0119Å for ACE, NAMPT, and FABP5, respectively, confirming the reliability of \nthe docking protocol.\nData availability\nAll data generated or analyzed during this study are included in the manuscript and supporting files. The \nsequence-SMILES dataset of approved drug-target pairs used in this study is publicly available at “alimotah -\nharynia/approved_drug_target” on Hugging Face ( h t t p s :  / / h u g g  i n g f a c  e . c o / d  a t a s e  t s / a l i  m o t a h h  a r y n i a  / a p p r o v e d _ \nd r u g _ t a r g e t).\nCode availability\nThe checkpoints, code for generating small molecules, and customized invalid structure assessor are publicly \navailable at https://huggingface.co/alimotahharynia/DrugGen and https://github.com/mahsasheikh/DrugGen. \nExplore the interactive user interface of DrugGen at  h t t p s :   /  / h u g g i n g f a c  e . c  o /  s p a c  e  s / a l i m  o t a h h a  r y n  i a  /  G P  T - 2 -  D \nTarget ligxrange ligyrange ligzrange xcent xrange ycent yrange zcent zrange\n2GVJ—NAMPT 40 40 40 14.616 76 -7.569 76 14.046 76\n1O86—ACE 40 40 40 40.657 76 37.169 76 43.527 76\n1SNZ—GALM 40 40 40 -10.433 58 5.656 58 50.197 58\n1B56—FABP5 30 30 30 49.969 52 22.227 52 32.492 52\nTable 3. Gridbox generation properties for performing blind docking.\n \nScientific Reports |        (2025) 15:13445 9| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\nr u g - G e n e r a t o r\nReceived: 1 January 2025; Accepted: 14 April 2025\nReferences\n 1. Bai, L. et al. AI-enabled organoids: construction, analysis, and application. Bioactive Materials 31, 525–548 (2024).\n 2. Coley, C. W . Defining and exploring chemical spaces. Trends Chem. 3, 133–145 (2021).\n 3. Sun, D., Gao, W ., Hu, H. & Zhou, S. Why 90% of clinical drug development fails and how to improve it?. Acta Pharm. Sin. B 12, \n3049–3062 (2022).\n 4. Tong, X. et al. Generative models for de novo drug design. J. Med. Chem. 64, 14011–14027 (2021).\n 5. Zeng, X. et al. Deep generative molecular design reshapes drug discovery. Cell Rep. Med. https://doi.org/10.1016/j.xcrm.2022.100794 \n(2022).\n 6. Meyers, J., Fabian, B. & Brown, N. D. novo molecular design and generative models. Drug Discovery Today 26, 2707–2715 (2021).\n 7. Méndez-Lucio, O., Baillif, B., Clevert, D.-A., Rouquié, D. & Wichard, J. D. novo generation of hit-like molecules from gene \nexpression signatures using artificial intelligence. Nat. Commun. 11, 10 (2020).\n 8. Janssen, A. et al. A Generative and causal pharmacokinetic model for factor VIII in hemophilia a: a machine learning framework \nfor continuous model refinement. Clin. Pharmacol. Ther. 115, 881–889 (2024).\n 9. Ota, R. & Y amashita, F . Application of machine learning techniques to the analysis and prediction of drug pharmacokinetics. J. \nControl. Release 352, 961–969 (2022).\n 10. Horne, R. I. et al. Using generative modeling to endow with potency initially inert compounds with good bioavailability and low \ntoxicity. J. Chem. Inf. Model. 64, 590–596 (2024).\n 11. Ghayoor, A. & Kohan, H. G. Revolutionizing pharmacokinetics: the dawn of AI-powered analysis. J. Pharm. Pharm. Sci. 27, 12671 \n(2024).\n 12. Menke, J. & Koch, O. Using domain-specific fingerprints generated through neural networks to enhance ligand-based virtual \nscreening. J. Chem. Inf. Model. 61, 664–675 (2021).\n 13. Qureshi, R. et al. AI in drug discovery and its clinical relevance. Heliyon 9 (2023).\n 14. Zhuang, D. & Ibrahim, A. K. Deep learning for drug discovery: a study of identifying high efficacy drug compounds using a \ncascade transfer learning approach. Appl. Sci. 11, 7772 (2021).\n 15. Gangwal, A. & Lavecchia, A. Unlocking the potential of generative AI in drug discovery. Drug Discovery Today, 103992 (2024).\n 16. Rose, T., Monti, N., Anand, N. & Shen, T. PLAPT: protein-ligand binding affinity prediction using pretrained transformers. bioRxiv \n2024(2002), 575577 (2008).\n 17. Madani, A. et al. Progen: Language modeling for protein generation. Preprint https:\\\\arXiv.org.\\2004.03497 (2020).\n 18. Brown, T. B. Language models are few-shot learners. Preprint https:\\\\arXiv.org.\\2005.14165 (2020).\n 19. Li, Y . et al. DrugGPT: A GPT-based strategy for designing potential ligands targeting specific proteins. bioRxiv 2023(2006), 543848 \n(2023).\n 20. Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. \nInform. Comput. Sci. 28, 31–36 (1988).\n 21. Michailidis, G. & d’ Alché-Buc, F . Autoregressive models for gene regulatory network inference: Sparsity, stability and causality \nissues. Math. Biosci. 246, 326–334 (2013).\n 22. Kim, T. K., Paul, H. Y ., Hager, G. D. & Lin, C. T. Refining dataset curation methods for deep learning-based automated tuberculosis \nscreening. J. Thorac. Dis. 12, 5078 (2020).\n 23. Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell 180(688–702), e613 (2020).\n 24. Borhani, N., Izadi, I., Motahharynia, A., Sheikholeslami, M. & Gheisari, Y . DrugTar improves druggability prediction by integrating \nlarge language models and gene ontologies. biorxiv https://doi.org/10.1101/2024.09.21.614218 (2024).\n 25. Roskoski, R. Jr. Properties of FDA-approved small molecule protein kinase inhibitors: A 2024 update. Pharmacol Res. 200, 107059. \nhttps://doi.org/10.1016/j.phrs.2024.107059 (2024).\n 26. Loftsson, T. in Essential Pharmacokinetics (ed Thorsteinn Loftsson) 85–104 (Academic Press, 2015).\n 27. Di, L. & Kerns, E. H. in Drug-Like Properties (Second Edition) (eds Li Di & Edward H. Kerns) 1–3 (Academic Press, 2016).\n 28. Li, B. et al. DrugMetric: quantitative drug-likeness scoring based on chemical space distance. Briefings Bioinform.  h t t p s : / / d o i . o r g / \n1 0 . 1 0 9 3 / b i b / b b a e 3 2 1     (2024).\n 29. Kulkarni, V . S., Alagarsamy, V ., Solomon, V . R., Jose, P . A. & Murugesan, S. Drug repurposing: an effective tool in modern drug \ndiscovery. Russ J Bioorg Chem 49, 157–166. https://doi.org/10.1134/s1068162023020139 (2023).\n 30. Cozier, G. E., Lubbe, L., Sturrock, E. D. & Acharya, K. R. Angiotensin-converting enzyme open for business: structural insights \ninto the subdomain dynamics. Febs j 288, 2238–2256. https://doi.org/10.1111/febs.15601 (2021).\n 31. Zhang, Z., Shen, W . X., Liu, Q. & Zitnik, M. Efficient generation of protein pockets with PocketGen. Nat. Machine Intell. 6, 1382–\n1395. https://doi.org/10.1038/s42256-024-00920-9 (2024).\n 32. Wishart, D. S. et al. DrugBank: a comprehensive resource for in silico drug discovery and exploration. Nucleic Acids Res. 34, D668–\nD672 (2006).\n 33. Wishart, D. S. et al. DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic Acids Res. 46, D1074-d1082.  h t t p s : \n/ / d o i . o r g / 1 0 . 1 0 9 3 / n a r / g k x 1 0 3 7     (2018).\n 34. UniProt: the universal protein knowledgebase in 2023. Nucleic acids research 51, D523-D531 (2023).\n 35. Consortium & T. U,. UniProt: the Universal Protein Knowledgebase in 2025. Nucleic Acids Res. 53, D609–D617.  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 9 3 / n a r / g k a e 1 0 1 0     (2024).\n 36. Sf, A. Basic local alignment search tool. J Mol Biol 215, 403–410 (1990).\n 37. Zdrazil, B. et al. The ChEMBL Database in 2023: a drug discovery platform spanning multiple bioactivity data types and time \nperiods. Nucleic Acids Res. 52, D1180–D1192 (2024).\n 38. Irwin, J. J. et al. ZINC20—a free ultralarge-scale chemical database for ligand discovery. J. Chem. Inf. Model. 60, 6065–6073 (2020).\n 39. Transformer Reinforcement Learning, <https://huggingface.co/docs/trl/en/index> (\n 40. Summarize from Feedback, <https://github.com/openai/summarize-from-feedback> (\n 41. RDKit, <https://www.rdkit.org/> (\n 42. Piñero, J. et al. DisGeNET: a comprehensive platform integrating information on human disease-associated genes and variants. \nNucleic acids research, gkw943 (2016).\n 43. Bajusz, D., Rácz, A. & Héberger, K. Why is Tanimoto index an appropriate choice for fingerprint-based similarity calculations?. J. \nCheminformatics 7, 1–13 (2015).\n 44. Morgan, H. L. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts \nservice. J. Chem. Doc. 5, 107–113 (1965).\n 45. Hassan, N. M., Alhossary, A. A., Mu, Y . & Kwoh, C.-K. Protein-ligand blind docking using QuickVina-W with inter-process \nspatio-temporal integration. Sci. Rep. 7, 15451 (2017).\nScientific Reports |        (2025) 15:13445 10| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/\n 46. Madhavi Sastry, G., Adzhigirey, M., Day, T., Annabhimoju, R. & Sherman, W . Protein and ligand preparation: parameters, \nprotocols, and influence on virtual screening enrichments. J. Comput. Aided Mol. Design 27, 221–234 (2013).\n 47. Schrödinger Release 2024–2: LigPrep (Schrödinger, LLC, New Y ork, NY , 2024).\n 48. Lu, C. et al. OPLS4: Improving force field accuracy on challenging regimes of chemical space. J. Chem. Theory Comput. 17, 4291–\n4300 (2021).\n 49. Friesner, R. A. et al. Glide: a new approach for rapid, accurate docking and scoring. 1. Method and assessment of docking accuracy. \nJ. Med. Chem. 47, 1739–1749 (2004).\nAcknowledgements\nWe sincerely thank Dr. Mehdi Rahmani for his invaluable assistance with technical and software issues related to \ntraining our model on the cluster servers.\nAuthor contributions\nConceptualization: M.S, Y .G, M.I, A.M. Dataset preparation: M.S, A.M. Model development: M.S, N.M, M.I, \nA.M. Statistical analysis: M.S, N.M, A.M. In silico validation: M.S, A.F . Data interpretation: All authors. Drafting \noriginal manuscript: M.S, N.M. Revising the manuscript: Y .G, A.F , M.I, A.M. All the authors have read and ap-\nproved the final version for publication and agreed to be responsible for the integrity of the study.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 9 8 6 2 9 - 1     .  \nCorrespondence and requests for materials should be addressed to M.I. or A.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:13445 11| https://doi.org/10.1038/s41598-025-98629-1\nwww.nature.com/scientificreports/",
  "topic": "Drug discovery",
  "concepts": [
    {
      "name": "Drug discovery",
      "score": 0.7381942272186279
    },
    {
      "name": "Reinforcement learning",
      "score": 0.6819658875465393
    },
    {
      "name": "Computer science",
      "score": 0.5922484397888184
    },
    {
      "name": "Drug",
      "score": 0.45527759194374084
    },
    {
      "name": "Computational biology",
      "score": 0.3640839755535126
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32192426919937134
    },
    {
      "name": "Bioinformatics",
      "score": 0.2915947437286377
    },
    {
      "name": "Biology",
      "score": 0.17612460255622864
    },
    {
      "name": "Pharmacology",
      "score": 0.1573149859905243
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114982161",
      "name": "Isfahan University of Medical Sciences",
      "country": "IR"
    }
  ],
  "cited_by": 7
}