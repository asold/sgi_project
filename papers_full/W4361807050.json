{
  "title": "Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching",
  "url": "https://openalex.org/W4361807050",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1362095416",
      "name": "Yuan, Jiayi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743429926",
      "name": "Tang Ruixiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378273847",
      "name": "Jiang, Xiaoqian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115715217",
      "name": "Hu Xia",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W330229616",
    "https://openalex.org/W2903777941",
    "https://openalex.org/W2911661483",
    "https://openalex.org/W3021833419",
    "https://openalex.org/W2961396908",
    "https://openalex.org/W2114847203",
    "https://openalex.org/W3124357233",
    "https://openalex.org/W3122120695",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3011079674"
  ],
  "abstract": "The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case studies to further illustrate the effectiveness of our approach and provide a deeper understanding of its underlying principles.",
  "full_text": "Large Language Models for Healthcare Data Augmentation: An Example on\nPatient-Trial Matching\nJiayi Yuan1, Ruixiang Tang1, Xiaoqian Jiang, PhD2, Xia Hu, PhD1\n1 Rice University, Houston, TX; 2University of Texas Health Science Center, Houston, TX.\nAbstract\nThe process of matching patients with suitable clinical trials is essential for advancing medical research and providing\noptimal care. However, current approaches face challenges such as data standardization, ethical considerations, and\na lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we\nexplore the potential of large language models (LLMs) to address these challenges by leveraging their advanced\nnatural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We\npropose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM),\nwhich balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our\nexperiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and\nthe generalizability to new data is improved by 12.12%. Additionally, we present case studies to further illustrate the\neffectiveness of our approach and provide a deeper understanding of its underlying principles.\n1 Introduction\nIdentifying suitable clinical trials for patients is crucial for the reliable assessment and examination of medical treat-\nments. This process not only enables patients to obtain the best possible care but also supports advancements in\nmedical research. Models that facilitate clinical trial matching compare patient profiles with the eligibility require-\nments of ongoing studies to pinpoint potential matches [1]. Nonetheless, discovering the ideal clinical trial for a\npatient can be a complex and lengthy procedure. This is where advanced clinical trial matching methods come into\nplay. By employing artificial intelligence (AI) or specific algorithms, researchers can pair patients with fitting clinical\ntrials [2, 3], a task that is growing increasingly vital as the number of studies continues to expand. Please refer to our\npreliminary section for more details.\nIn spite of the potential benefits, integrating AI into clinical trial processes encounters various obstacles, such as issues\npertaining to data availability, standardization, and ethical considerations [4]. A significant challenge lies in the dis-\ncordance between the ontology and terminology utilized in Electronic Health Records (EHRs) and those implemented\nin clinical trial inclusion and exclusion criteria. Although recent research has produced solutions to address these\nconcerns through black-boxed embedding matching [5, 6], the efficacy of AI-driven clinical trial matching services\nmay still be impeded by the difficulty in interoperating the information from the two disparate sources.\nThe advent of large language models (LLM) [7] presents an opportunity to enhance the compatibility between EHRs\nand clinical trial descriptions, promoting more accurate patient-trial matching. By capitalizing on their sophisticated\nnatural language processing abilities, these models can efficiently decipher, comprehend, and harmonize the diverse\nterminologies and ontologies present in both EHRs and clinical trial inclusion and exclusion criteria. This enhanced\ninteroperability not only streamlines the matching process but also ensures greater accuracy in identifying suitable\ntrials for patients. Consequently, large language models hold the potential to transform clinical trial matching, enabling\nbetter patient outcomes and contributing to more efficient medical research.\nThe foremost challenge when implementing LLMs for clinical trial matching lies in managing privacy concerns that\narise due to the handling of sensitive patient data, which may include personal and health-related information. En-\nsuring the security and confidentiality of this data is crucial to maintain patient trust and adhere to legal and ethical\nstandards. To overcome this challenge, we propose an innovative data augmentation that prioritizes data privacy while\nmaintaining the benefits offered by LLMs. During the actual implementation, instead of feeding original patient data\ndirectly into LLMs, we use desensitized patient data as a prompt to guide the LLM in the augmentation process of the\ntrial data. Through comprehensive experimentation, our proposed LLM-PTM demonstrates an average improvement\nin performance by 7.32%, it also improves generalizability by12.12%. Moreover, we conduct case studies to elucidate\nthe efficacy of the method and provide a broader insight into its underlying principles.\narXiv:2303.16756v2  [cs.CL]  5 Aug 2023\nMatching Algorithm\nClinical Trial #xyz\nPatient Record #abc\nGenderAge\nPatients Information\nDiagnosis\nMRIMedication\nClinical Trial #xyz\nAvailable Trials\nInclusion Criteria\nExclusion Criteria\nFigure 1: An illustration of patient-trial matching.\n2 Preliminary\n2.1 Patient Trial Matching\nIn the realm of clinical research, clinical trials serve as the solely established methodology for the development of novel\ndisease treatments. However, these trials frequently encounter challenges such as costly, imprecise, and inadequate\npatient recruitment. A substantial number of trials grapple with obtaining the necessary patient population, with\n50% experiencing delays due to recruitment issues, while others fail to secure a sufficient number of participants to\ninitiate the trial [8]. The advent of automated patient-trial matching offers a promising avenue for optimizing the\ntrial recruitment process. The crux of this approach lies in identifying eligible patients for clinical trials based on\ntheir longitudinal electronic health records (EHR) and trial eligibility criteria (EC), which encompass both inclusion\nand exclusion criteria as depicted in Fig. 1. The problem can be framed as a classification problem. Given input\nconsisting of a patient’s complete Electronic Health Record (EHR) data and a single trial’s eligibility criteria (EC),\nthe output can be classified as either match, mismatch, or unknown. A match between a patient and a trial occurs if\nand only if the patient satisfies all of the trial’s ECs. Mathematically, let P represent the patient’s EHR data and T =\n[t1, ..., tn] represent the trial’s eligibility criteria. The matching function m(P, t) ={match, mismatch, unknown}\nand M(P, T) ={match, mismatch}.\n2.2 Data Augmentation\nText data augmentation, involving diverse transformations, is widely utilized to improve model training for text clas-\nsification tasks within the domain of natural language processing (NLP). Contemporary data augmentation techniques\nin NLP function at distinct levels of granularity, including characters, words, sentences, and documents. The objective\nof data augmentation is to generate intelligible and varied additional instances that uphold semantic congruity. In the\ncontext of patient-trial matching, data augmentation assumes particular importance due to the limited richness of the\navailable training data. The implementation of an effective augmentation method can facilitate the generation of a\nmore varied text dataset, allowing machine learning models to better capture the intricacies of patient and eligibility\ncriteria information. Consequently, this leads to enhanced training of classifiers, resulting in more accurate and reliable\noutcomes for patient-trial matching tasks.\n2.3 Data Privacy in Healthcare\nThe potential of big data to revolutionize healthcare is acknowledged [9], but privacy concerns remain. Privacy issues\nlead to both consequentialist and deontological concerns in data usage [9]. Restricting access to patient data can\nimpede data-driven innovation [10], while data deidentification complicates linking patient data from different sources\n[11, 12]. Alternative privacy-preserving approaches, such as pseudonymized data or differential privacy techniques,\ncan be applied in certain contexts [13, 14, 15]. Privacy audits and security standards can also help, with data holders\nacting as stewards rather than privacy-agnostic intermediaries. Yet, a privacy-innovation tradeoff may still persist in\nmany situations. In the context patient-trial matching, privacy concerns also arise due to the inherent nature of big data.\nThis study explores privacy-conscious augmentation techniques that interact with open-source components, aiming to\nimprove match identification accuracy while preventing the leakage of private-source data.\n3 Methodology\n3.1 Problem Setting\nAs previously elucidated, the process of patient-trial matching entails the identification of suitable patients for a spe-\ncific clinical trial based on their Electronic Health Records (EHR). These records contain comprehensive medical\ninformation about the patient, while clinical trials are characterized by detailed descriptions, eligibility criteria, and\nother pertinent data. In the following section, we will systematically introduce the problem setting.\nInput 1: Patient Records . We extract patient records from raw clinical documentation and subsequently trans-\nform them into a structured tabular format. We represent patient records by the symbol P and formally define it as:\nP = [d1, d2, . . . , dnd , m1, m2, . . . , mnm , p1, p2, . . . , pnp ]. Here, di refers to a diagnosis belonging to the set D of\ndiagnoses, mi represents a medication from the set M of medications, and pi denotes a procedure within the set\nP of procedures. All elements, i.e., diagnoses, medications, and procedures, are character strings. The quantities\nnd, nm, and np are the total numbers of diagnoses, medications, and procedures in the electronic health record (EHR)\ncategories, respectively.\nInput 2: Clinical Trials. In our problem setting, we focus on the criteria of clinical trials. Analogous to the extraction\nof patient notes, we retrieve eligibility criteria from unprocessed patient trial documents. Let us denote clinical trials\nby the symbol T and express it as: T = [i1, i2, . . . , ini , e1, e2, . . . , ene ]. In this representation, ii and ei symbolize the\ninclusion and exclusion criteria, respectively, both of which are character strings. The variablesni and ne indicate the\ntotal number of inclusion and exclusion criteria, correspondingly.\nTask 1: Patient-Criteria Matching . Given a patient’s visit records P and a collection of inclusion or exclusion\ncriteria, we frame the patient-criteria matching task as a multi-class classification problem. The objective is to cat-\negorize the matching outcomes between patients and Eligibility Criteria (ECs) into three distinct classes: “match”,\n“mismatch”, and “unknown”, according to the similarity between patient records and trial criteria. We can represent\nthis as: ˆy(c, P) ∈ {match, mismatch, unknown}, where c ∈ T.\nTask 2: Patient-Trial Matching. Given a patient’s visit records P and a clinical trial C consisting of a collection of\ninclusion and exclusion criteria, we assert that a patient and a trial constitute a match only if the patient satisfies all\ninclusion criteria and contradicts all exclusion criteria in the trial. We can represent this condition as:Match(C, P) ⇔\n(∀i ∈ {i1, . . . , ini }, : ˆy(i, P) =match) ∧ (∀e ∈ {e1, . . . , ene }, : ˆy(e, P) =match).\n3.2 Proposed Pipeline\nTrial Eligibility Criteria Augmentation\nAs previously discussed, the acquisition of comprehensive and high-quality data in patient-trial matching presents\nsignificant challenges, including considerable expenses and potential privacy infringement. Recognizing the necessity\nfor data augmentation in this context, we have introduced LLM-PTM. In this work, we put forth a data augmentation\ntechnique utilizing Language Models (LLMs) to create supplementary data points while preserving the semantic co-\nherence of the original trial’s inclusion (i) and exclusion (e) criteria. We first employ Chain-of-Thought to direct the\nLLMs to gradually generate the prompts. These prompts incorporate our requirement that the output data be more\ncomprehensible to machine learning models, while preserving the exact semantic content. Subsequently, we use de-\nsensitized patient data, clinical trial data, and the previously generated prompts to execute a data augmentation process\nthat preserves privacy.\nThe illustration and examples of the augmentation part of LLM-PTM can be found in Fig. 2. Given the criteria of\na clinical trial: T = [i1, i2, . . . , ini , e1, e2, . . . , ene ], we aim to utilize an LLM to generate a set of augmented data\npoints T that adhere to these constraints. Formally, for each criteria ik and el ∈ T, we construct input strings i′\nk\nand e′\nl as follows: i′\nk = o ⊕ ik, e′\nl = o ⊕ el where o is the designed prompt, ⊕ denotes the concatenation operation.\nSubsequently, we feed the input strings i′\nk and e′\nl into the LLM to generate a set of augmented data points Aik and\nAel , respectively. Regard the LLM as a function LLM(): Aik = LLM(i′\nk), Ael = LLM(e′\nl). The final augmented\ntrial dataset T can then be represented as:\nPrompt\nAugmented Criteria\nOriginal\nCriteria\nClinical diagnosis of ischemic stroke involving cerebral\ncortex\nPlease return me restructured sentences, modifying the\nlexical choice and syntax where appropriate\nClinical examination confirmed an ischemic stroke affecting the cerebral cortex.\nAn ischemic stroke involving the cerebral cortex was clinically diagnosed.\nThere was a severe case of systemic hemorrhage in the last month.\nSerious systemic hemorrhage in the past 30 days\nThe patient experienced a significant episode of internal bleeding\nwithin the past 30 days.They must maintain identical semantic meaning\nFigure 2: Illustration of LLM-PTM augmented criteria.\nT =\nn[\nk=1\nAik ∪\nm[\nl=1\nAel (1)\nPatient and Criteria Embedding\nAfter getting preprocessed text data, we then do embedding with the taxonomy-guided deep learning method. Latent\nrepresentations of a patient’s visit record and trial criteria can be acquired through the utilization of extensive language\nmodels (LLMs). In this work, we employ the pretrained BERT [16] as our text encoder. For patient embedding, a\nmemory network [17], Mem(·), is utilized to manage the patient records, thereby effectively maintaining the sequence\nof visit data within the embedding space. Formally, the patient record embedding, represented as xP , is derived from\nthe encoding function fP (·), which can be expressed as:\nxP = fP (P) =Mem(BERT (a1), BERT(a2), . . . , BERT(an)), ai ∈ {D, M, P}. (2)\nIn order to capture and encode these essential features within the embedding space, we implement a prior approach\nthat employs a convolutional neural network (CNN) and a highway layer [18] to extract patterns at various levels for\nthe semantic matching task [19]. Formally, the encoding function fc(·) is employed to encode an EC embedding xc,\nwhich can be described as:\nxi/e = fc(c) =Highway (BERT (c)), (3)\nwhere c ∈ T. The outputs of highway networks are calculated asHighway (·) =Sigmoid(·)Conv(·)+ Conv(·)(1−\nSigmoid(Conv(·))).\nPrediction and Embedding Learning\nThroughout the model optimization process, we aim to maximize patient-trial matching while explicitly addressing\nthe differences between inclusion and exclusion criteria. To achieve this, we design a composite loss function that\ncomprises the following loss terms. The first loss is classification loss. To optimize the classification performance\nbetween the predicted outcome ˆy and the ground truth y, we employ a cross-entropy loss term:\nLcla = −yT log(ˆy) − (1 − y)T log(1 − ˆy). (4)\nFurthermore, we construct inclusion/exclusion contrastive loss term to explicitly address the match between patient\nembedding and EC embedding for both inclusion and exclusion criteria. This loss term enables the model to extract\ndistinct features (e.g., negation words) within the inclusion/exclusion criteria, thereby aiding the decision to include\nor exclude a patient. Mathematically, this involves maximizing the similarity between the retrieved patient memory\nand the embedding for inclusion criteria (i.e., (xP , xi)) while minimizing the similarity between the memory and the\nembedding for exclusion criteria (i.e., (xP , xe)). We formulate the loss term using the following pairwise distance\nloss:\nLcon =\nY\na=1,...,ni\n(1 − s(xia , xP )) ·\nY\nb=1,...,ne\nmax(0, s(xeb , xP ) − ε), (5)\nTrial EC\n(Inclusion&\n Exclusion)\nInclusion Criteria\nExclusion Criteria\nPatient\nPublic\nPrivate \nPatient\nRecords\n(EHR) Memory\nNetwork\nHighway\nNetwork\nFully\nConnected\nLayer\nMatch\nMismatch\nUnknown\nFigure 3: Overall model framework\nwhere s(·, ·) represents the similarity function between two vectors. In our work, we utilize the cosine similarity func-\ntion to determine the distance between two data modalities. The hyperparameter ε signifies the minimum similarity\nbetween the exclusion criteria embedding and patient memory. If a patient matches an inclusion criterion, the model\nmaximize the cosine similarity between the two embeddings, making1−s(xi, xP ) approach 0. If a patient is excluded\ndue to an exclusion criterion, the similarity between the two embeddings (i.e., max(0, s(xe, xP ) − ε)) is minimized\nand must be no less than ε. This ensures that xi and xe have distinct distances to xP in the latent embedding space.\nFinally, we minimize the loss functions jointly through backpropagation in an end-to-end manner in Eq.6,\nL = α · Lcla + (1− α) · Lcon, (6)\nwhere α controls the strength of classification loss. The overall framework can be found in Fig. 3.\n4 Experiment\n4.1 Dataset\nClinical Trial Data. We collected data from six different stroke clinical trials, namely NCT03735979, NCT03805308,\nNCT03263117, NCT03496883, NCT03876457, and NCT03545607, usingClinicalTrials.gov as our source. Our focus\nwas on both the inclusion and exclusion criteria, resulting in 150 sentence-level statements extracted.\nPatient EHR Data. The project was approved by the UTHealth Institutional Review Board (IRB) under HSC-SBMI-\n21-0529 - “Re-admission Risk Estimation for Stroke Patients”. Using the stroke patient database, we gathered patient\nclaims data for 825 patients who were enrolled in at least one of the six stroke trials. This data included longitudinal\nprescription and medical claims data for each patient, encompassing their diagnoses, procedures, and medications.\nAugment Data. In the development of the baseline methods, we utilized open-source libraries to generate augmented\ndata. For our LLM-PTM, which employed a LLM-based prompt augmentation, we incorporated OpenAI’s ChatGPT\nfrom the version released on March 5th.\nWe then matched each inclusion/exclusion criteria with its corresponding patient EHR, labeling them as either ”match”\nor ”mismatch”. To add an element of uncertainty, we also included ”unknown” labels for one inclusion criterion and\none exclusion criterion randomly selected from another trial, leading to a total of 100,000 labeled pairs.\n4.2 Model and Setting\nModel Configuration In the process of text embedding, we employed the Clinical BERT embeddings as our primary\nmethod, as delineated by [20]. This specific model was pre-trained on a substantial dataset comprising 2 million\nclinical notes, which were extracted from the MIMIC-III v1.4 database [21]. To optimize the performance of the\nmodel, a total of 150,000 iterations were executed during the training phase. The resultant embedding produced by\nthe Clinical BERT model featured a dimensionality of 768. As for the architectural design of the highway network,\nit consisted of a two-layered convolutional structure with each layer containing 128 channels. This configuration was\nstrategically chosen to enhance the learning capacity of the model while maintaining computational efficiency. The\nmachine learning models are built upon PyTorch.\nBaselines. We benchmarked the proposed data augmentation approach in comparison to original data and the standard\ndata augmentation technique on the dataset. Specifically, we tested 1) Swap Word Augmentation, a method that\nrandomly swaps words within the text and is a subset of the Easy Data Augmentation (EDA) [22] proposed by Wei\net al; 2) Context Word Augmentation [23, 24], which employs BERT to insert contextually relevant words by adding\na mask token at a random position in the input text and allowing BERT to predict the appropriate word; 3) Back\nTranslation Augmentation. This technique [25] involves translating the text to another language and then back to\nEnglish, yielding a distinct text that maintains the original meaning. In this work, we employed Google Translate in\ndeep translator as a translation framework and utilize German, French, and Spanish as intermediate languages.\nTraining Settings. In the process of developing our experimental framework, we established our training configura-\ntions in accordance with the prevailing settings that are widely accepted within the field. To elaborate, we commenced\nthe training of the entire neural networks from scratch for a predetermined duration of 12 epochs. Throughout this\nstage, we employed the Adam optimizer [26], accompanied by a batch size of 128 and a learning rate of 1e-4. The\nhyperparameter α was set to 0.5 and ε was set to 0.01.\nEvaluation Metrics. The outcomes of our study are presented at two distinct levels: the patient-criteria level and\nthe patient-trial level. To comprehensively evaluate the performance of the model, we employed a variety of widely\nrecognized evaluation metrics, including Precision, Recall, and F1 score.\n4.3 Experimental Results Analysis\n4.3.1 Overall Performance\nIn this section, we aggregate the results from both patient criteria and patient-trial matching across all trials under\ninvestigation. A comparative analysis between our proposed methods and the established baseline methods is presented\nin Tab. 1. The experimental results are obtained by calculating the average of three separate tests. Upon examining\nthe experimental outcomes, we discerned that the implementation of our LLM-based augmentation approach yielded\na substantial enhancement in the performance of machine learning models across all evaluation metrics.\nTable 1: Overall performance\nPerformance Patient-Criteria Level Patient-Trial Level\nPrec Rec F1 Prec Rec F1\nVanilla (No Augmentation) 0.858 ±1.1e−2 0.841±5.7e−3 0.850±8.1e−3 0.715±2.0e−2 0.748±1.7e−2 0.731±1.9e−2\nSwap Word Augmentation 0.854 ±1.8e−2 0.814±1.6e−2 0.833±1.7e−2 0.707±1.2e−2 0.759±1.4e−2 0.732±1.3e−2\nContext Word Augmentation 0.855±1.0e−2 0.835±1.4e−2 0.844±1.2e−2 0.752±1.7e−2 0.789±1.5e−2 0.770±1.6e−2\nBack Translation Augmentation 0.919±1.1e−2 0.845±1.3e−2 0.880±1.2e−2 0.777±1.4e−2 0.796±1.3e−2 0.787±1.3e−2\nLLM-PTM 0.964 ±8.0e−3 0.862±1.2e−2 0.910±1.0e−2 0.801±1.3e−2 0.830±1.1e−2 0.815±1.1e−2\nMore specifically, the average performance improvements for Precision, Recall, and F1 scores at the patient-criteria\nlevel are more than 10.6%, 2.1%, and 6.0%, respectively. A more pronounced performance gain is observed at the\npatient-trial level, with respective improvements of8.6%, 8.2%, and 8.4%. These performance gains can be attributed\nto the inclusion of a more diverse dataset (in comparison to the original dataset) and the incorporation of more precise\nsemantic information (relative to other augmentation techniques). In the forthcoming case study, we will delve deeper\ninto the implications and nuances of these findings, providing a more comprehensive understanding of the impact of\nour proposed LLM-based augmentation method on the performance of machine learning models in this domain.\n4.3.2 Performance Throughout Different Trials\nIn this section, we aim to evaluate the performance of our proposed model at the patient criteria level across various\ntrials. The primary objective of this analysis is to determine whether criteria in certain trials prove to be more chal-\nlenging than others and to assess the extent to which the models can achieve satisfactory results. The outcomes of this\nexamination are presented in Tab. 2. Our findings indicate that the performance of the baseline model does indeed\nfalter in some trials, whereas our proposed method demonstrates a more consistent and robust capacity to tackle these\nchallenging tasks. For instance, in Trial 1, the baseline model only managed to attain a precision of 40.2%, while our\nmethod exhibited an improvement of 24.0%. This suggests that our approach, which incorporates enriched and varied\nsemantic information, can significantly enhance the model’s performance, particularly when faced with difficult data.\nIn the subsequent case study, we will delve further into the nuances of these observations to better understand the\nfactors contributing to the success of our proposed method.\nTable 2: Performance throughout different trials\nPerformance Vanilla LLM-PTM\nPrec Rec F1 Prec Rec F1\nTrial1 (NCT03263117) 0.402 0.382 0.391 0.642 0.502 0.563\nTrial2 (NCT03496883) 1.000 0.925 0.961 1.000 0.976 0.987\nTrial3 (NCT03545607) 0.788 0.749 0.768 0.882 0.801 0.839\nTrial4 (NCT03735979) 0.601 0.557 0.578 0.633 0.557 0.592\nTrial5 (NCT03805308) 0.915 0.902 0.908 0.933 0.900 0.916\nTrial6 (NCT03876457) 0.498 0.496 0.496 0.593 0.405 0.481\n4.3.3 Generalizability\nIn this section, we focus on assessing the generalizability of our proposed model in comparison to the baseline (vanilla)\nmodel. Our objective is to determine the effectiveness of both models when trained on certain trials and subsequently\ntested on unseen trials. To conduct this analysis, we designed several experimental scenarios: 1) Training on easy\ntrials and testing on easy trials; 2) Training on easy trials and testing on hard trials; 3) Training on mixed trials and\ntesting on hard trials. The level of difficulty is determined by the performance demonstrated in Section 4.3.2. The\nresults of these experiments are presented in Tab. 3.\nTable 3: Performance of generalizability\nPerformance Vanilla LLM-PTM\nPrec Rec F1 Prec Rec F1\nCase 1 0.793 0.734 0.762 0.855 0.803 0.828\nCase 2 0.525 0.452 0.485 0.673 0.604 0.636\nCase 3 0.566 0.535 0.550 0.742 0.656 0.696\nUpon analyzing the outcomes, it becomes evident that the baseline model exhibits weak performance with respect\nto generalizability. In contrast, our proposed method demonstrates a notable enhancement in accuracy, achieving\nan average boost of 12.12% across the different scenarios. In an effort to understand the factors contributing to the\nobserved generalizability when training on easy and hard trials, we consider various scenarios. Both the vanilla model\nand our method perform similarly when training and testing on easy trials, with our method showing a slight accuracy\nimprovement due to its enriched semantic information. When training on easy trials and testing on hard ones, the\nbaseline model struggles to generalize, while our method excels thanks to its strong adaptability using varied semantic\ninformation. Even when training on mixed trials, our method outperforms the vanilla model during testing on hard\ntrials, as it can better comprehend the problem domain. Overall, our proposed method demonstrates the potential to\nenhance machine learning model performance across different contexts and problem domains by efficiently utilizing\nenriched semantic information.\n4.4 Case Study\nIn this section, we present a case study to illustrate the strengths of our proposed method through the use of concrete ex-\namples. Specifically, we discuss two examples that demonstrate augmentation’s capacity to alleviate challenges posed\nby hard data and to enrich the semantic information of the given problem domain. By showcasing these examples, we\naim to provide a deeper understanding of the benefits of our method in practice.\nCase 1: Hard Data Easing\nConsider a challenging case where traditional machine learning models struggle to accurately classify patients based\non their medical records. In this scenario, our augmentation method generates additional criteria by leveraging the\nenriched semantic information derived from the language model. These newly created criteria assist the model in\ncapturing nuanced relationships and patterns that may have been overlooked by the baseline model. Consequently, the\nmodel is better equipped to handle the complexity of hard data, resulting in improved performance and more accurate\nclassification of patients. As shown in Tab. 4, the initial data cause erroneous predictions, whereas the model trained\non augmented data displayed accurate classification. This improved performance indicates that augmented datasets\ncan enhance the efficacy of machine learning models.\nTable 4: Case study: ease of hard data. We highlight the clear expressions.\nCriteria-Level\nMatching Criteria Prediction Ground\nTruth\nVanilla Positive urine or serum pregnancy test for women of child bearing potential. Match\nMismatchLLM-PTM Women of reproductive age have received a positive result on their urine or serum preg-\nnancy test. Mismatch\nWomen who are capable of bearing children have a positive pregnancy test in their urine\nor serum. Mismatch\nCase 2: Semantic Enrichment\nIn a second example, let us consider a situation where the initial dataset contains limited semantic information, which\nmay hinder the model’s ability to make accurate predictions. Our augmentation method addresses this issue by gener-\nating a more diverse set of criteria based on the LLM’s understanding of the problem domain. By doing so, the method\nenriches the semantic information available to the model, allowing it to uncover previously undiscovered relationships\nand patterns within the data. This enriched representation of the problem domain subsequently contributes to better\nperformance. As shown in Tab. 5, the vanilla model exhibited inaccuracies in patient-trial level matching; however,\nwhen trained on augmented data, the model demonstrated improved prediction accuracy. This evidence suggests that\nthe incorporation of augmented datasets can enhance the machine learning model’s capacity to comprehend specific\ncriteria and establish associations with other related criteria.\nTable 5: Case study: semantic enrichment. We highlight the diverse expressions.\nTrial-Level\nMatching Criteria within one trial Prediction Ground\nTruth\nVanilla Acute ischemic stroke patients. Mismatch\nMatch\nLLM-PTM\nPatients suffering from a sudden blockage of blood flow to the brain due to ischemia.\nMatchIndividuals experiencing a sudden onset of neurological deficits resulting from a lack of\nblood supply to the brain.\nPeople with an abrupt interruption of blood flow to the brain caused by an ischemic event.\nThrough the exploration of these examples, the case study highlights the potential of our proposed method to tackle\nchallenges associated with hard data and enrich semantic information. This, in turn, contributes to a more robust\nand comprehensive understanding of the problem domain, ultimately leading to improved performance of machine\nlearning models in various contexts.\n5 Related Works\nPatient Trial Matching. Existing patient-trial matching methodologies can be classified into rule-based systems and\ndeep embedding-based models. Rule-based systems endeavor to extract named entities and relationships for trial\neligibility criteria (ECs) and devise rules for patient identification. These systems rely either on extensive human\nannotations such as EliXR [27], supervised learning classifiers for rule extraction [28], or a combination of machine\nlearning and rule-based approaches such as Criteria2Query [29] to establish rules for ECs. In recent years, deep\nembedding-based models, such as DeepEnroll [6], have emerged. These models jointly embed patient records and\ntrial ECs in a shared latent space, subsequently aligning them using attentive inference. COMPOSE[5] represents the\nstate-of-the-art method, it used a fine-grained model design to differentiate between inclusion and exclusion criteria.\nIn this work, we follow COMPOSE as our baseline model structure.\nData Augmentation. In natural language processing (NLP), data augmentation techniques operate at character, word,\nsentence, and document levels. Character-level methods, such as random insertion, exchange, replacement, or deletion,\nimprove model resilience against textual noise [30, 31]. Word-level techniques, like random swap, deletion [22],\nsynonym augmentation [32], and word embeddings-based approaches [33, 34], enhance text classification tasks while\nmaintaining semantic consistency. Contextual augmentation uses MLMs such as BERT [16] and RoBERTa [35] for\ngenerating relevant new text [36]. Sentence and document-level methods include back translation augmentation [25].\nPatient-trial matching faces similar issues in obtaining training data, our proposed method shows that LLMs can help\nacquire high-quality data.\n6 Conclusion\nIn summary, this study has successfully developed and implemented a novel data augmentation method called LLM-\nPTM, designed to enhance the patient-trial matching process while preserving patient privacy. By leveraging the power\nof LLMs for data augmentation, our approach demonstrated a significant performance improvement, with a 7.32%\nincrease over the baseline model. Moreover, LLM-PTM proved to be highly generalizable, exhibiting a substantial\nimprovement of 12.12% when applied to new datasets. The promising results obtained from the implementation of\nLLM-PTM highlight the effectiveness of using LLM-empowered data augmentation techniques in the patient-trial\nmatching pipeline. The success of this method has the potential to revolutionize patient-trial matching processes,\ncontribute to improved clinical trial outcomes, and expedite the development of novel therapeutic interventions. The\nstudy also paves the way for future research exploring the extension of LLM-PTM to other domains in the healthcare\nindustry, ultimately optimizing data-driven decision-making processes.\nReferences\n1. Mattson ME. Patient-Treatment Matching: Rationale and Results. Alcohol health and research world.\n1994;18(4):287-95.\n2. Hassanzadeh H, Karimi S, Nguyen A. Matching patients to clinical trials using semantically enriched document\nrepresentation. Journal of biomedical informatics. 2020 May;105:103406.\n3. Alexander M, Solomon B, Ball DL, Sheerin M, Dankwa-Mullan I, Preininger AM, et al. Evaluation of an artificial\nintelligence clinical trial matching system in Australian lung cancer patients. JAMIA open. 2020 Jul;3(2):209-15.\n4. Bhatt A. Artificial intelligence in managing clinical trial design and conduct: Man and machine still on the\nlearning curve? Perspectives in clinical research. 2021 Jan;12(1):1-3.\n5. Gao J, Xiao C, Glass LM, Sun J. COMPOSE: cross-modal pseudo-siamese network for patient trial matching. In:\nProceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining; 2020.\np. 803-12.\n6. Zhang X, Xiao C, Glass LM, Sun J. DeepEnroll: patient-trial matching with deep embedding and entailment\nprediction. In: Proceedings of The Web Conference 2020; 2020. p. 1029-37.\n7. OpenAI. GPT-4 Technical Report; 2023.\n8. Hargreaves B. Clinical trials and their patients: the rising costs and how to stem the loss. Pharmafile URL\nhttp://www pharmafile com/news/511225/clinical-trials-and-their-patients-rising-costs-and-how-stem-loss. 2016.\n9. Price WN, Cohen IG. Privacy in the age of medical big data. Nature medicine. 2019;25(1):37-43.\n10. Price W, Nicholson I. Drug Approval in a Learning Health System. Minn L Rev. 2017;102:2413.\n11. Eisenberg RS, Price WN. Promoting healthcare innovation on the demand side. Journal of Law and the Bio-\nsciences. 2017;4(1):3-49.\n12. Evans B. Big data and individual autonomy in a crowd. Big Data, Health Law, and Bioethics. 2018.\n13. Beaulieu-Jones BK, Wu ZS, Williams C, Lee R, Bhavnani SP, Byrd JB, et al. Privacy-preserving genera-\ntive deep neural networks support clinical data sharing. Circulation: Cardiovascular Quality and Outcomes.\n2019;12(7):e005122.\n14. Dwork C, Roth A, et al. The algorithmic foundations of differential privacy. Foundations and Trends® in Theo-\nretical Computer Science. 2014;9(3–4):211-407.\n15. Moussa M, Demurjian SA. Differential privacy approach for big data privacy in healthcare. In: Privacy and\nSecurity Policies in Big Data. IGI Global; 2017. p. 191-213.\n16. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:181004805. 2018.\n17. Weston J, Chopra S, Bordes A. Memory networks. arXiv preprint arXiv:14103916. 2014.\n18. Srivastava RK, Greff K, Schmidhuber J. Highway networks. arXiv preprint arXiv:150500387. 2015.\n19. You Q, Zhang Z, Luo J. End-to-end convolutional semantic embeddings. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition; 2018. p. 5735-44.\n20. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly available clinical BERT embed-\ndings. arXiv preprint arXiv:190403323. 2019.\n21. Johnson AE, Pollard TJ, Shen L, Lehman LwH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible\ncritical care database. Scientific data. 2016;3(1):1-9.\n22. Wei J, Zou K. Eda: Easy data augmentation techniques for boosting performance on text classification tasks.\narXiv preprint arXiv:190111196. 2019.\n23. Kobayashi S. Contextual augmentation: Data augmentation by words with paradigmatic relations. arXiv preprint\narXiv:180506201. 2018.\n24. Kumar V , Choudhary A, Cho E. Data augmentation using pre-trained transformer models. arXiv preprint\narXiv:200302245. 2020.\n25. Sennrich R, Haddow B, Birch A. Improving neural machine translation models with monolingual data. arXiv\npreprint arXiv:151106709. 2015.\n26. Kingma DP, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980. 2014.\n27. Weng C, Wu X, Luo Z, Boland MR, Theodoratos D, Johnson SB. EliXR: an approach to eligibility criteria extrac-\ntion and representation. Journal of the American Medical Informatics Association. 2011;18(Supplement 1):i116-\n24.\n28. Bustos A, Pertusa A. Learning eligibility in cancer clinical trials using deep neural networks. Applied Sciences.\n2018;8(7):1206.\n29. Yuan C, Ryan PB, Ta C, Guo Y , Li Z, Hardin J, et al. Criteria2Query: a natural language interface to clinical\ndatabases for cohort definition. Journal of the American Medical Informatics Association. 2019;26(4):294-305.\n30. Belinkov Y , Bisk Y . Synthetic and natural noise both break neural machine translation. arXiv preprint\narXiv:171102173. 2017.\n31. Liu Z, Jin H, Wang TH, Zhou K, Hu X. DivAug: plug-in automated data augmentation with explicit diversity\nmaximization. In: Proceedings of the IEEE/CVF International Conference on Computer Vision; 2021. p. 4762-70.\n32. Niu T, Bansal M. Adversarial over-sensitivity and over-stability strategies for dialogue models. arXiv preprint\narXiv:180902079. 2018.\n33. Wang WY , Yang D. That’s so annoying!!!: A lexical and frame-semantic embedding based data augmentation\napproach to automatic categorization of annoying behaviors using# petpeeve tweets. In: Proceedings of the 2015\nconference on empirical methods in natural language processing; 2015. p. 2557-63.\n34. Mrk ˇsi´c N, S´eaghdha DO, Thomson B, Gaˇsi´c M, Rojas-Barahona L, Su PH, et al. Counter-fitting word vectors to\nlinguistic constraints. arXiv preprint arXiv:160300892. 2016.\n35. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:190711692. 2019.\n36. Tang R, Han X, Jiang X, Hu X. Does Synthetic Data Generation of LLMs Help Clinical Text Mining? arXiv\npreprint arXiv:230304360. 2023.",
  "topic": "Generalizability theory",
  "concepts": [
    {
      "name": "Generalizability theory",
      "score": 0.7214974164962769
    },
    {
      "name": "Interoperability",
      "score": 0.672945499420166
    },
    {
      "name": "Confidentiality",
      "score": 0.6568947434425354
    },
    {
      "name": "Standardization",
      "score": 0.5835155844688416
    },
    {
      "name": "Computer science",
      "score": 0.546064019203186
    },
    {
      "name": "Health care",
      "score": 0.5458219647407532
    },
    {
      "name": "Clinical trial",
      "score": 0.528772234916687
    },
    {
      "name": "Matching (statistics)",
      "score": 0.4713646471500397
    },
    {
      "name": "Health records",
      "score": 0.4662325084209442
    },
    {
      "name": "Data science",
      "score": 0.37912559509277344
    },
    {
      "name": "Medicine",
      "score": 0.2372841238975525
    },
    {
      "name": "Computer security",
      "score": 0.23664698004722595
    },
    {
      "name": "Psychology",
      "score": 0.1929873824119568
    },
    {
      "name": "Political science",
      "score": 0.09848597645759583
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ]
}