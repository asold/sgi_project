{
  "title": "Optimal Tap Setting of Voltage Regulation Transformers Using Batch Reinforcement Learning",
  "url": "https://openalex.org/W2883076328",
  "year": 2019,
  "authors": [
    {
      "id": null,
      "name": "Xu, Hanchen",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4287544060",
      "name": "Dominguez-Garcia, Alejandro D.",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4207352866",
      "name": "Sauer, Peter W.",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": null,
      "name": "Dom\\'inguez-Garc\\'ia, Alejandro D.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2403666663",
    "https://openalex.org/W2137646724",
    "https://openalex.org/W1977158414",
    "https://openalex.org/W2101822270",
    "https://openalex.org/W2039523638",
    "https://openalex.org/W2520235949",
    "https://openalex.org/W2811333905",
    "https://openalex.org/W2098172344",
    "https://openalex.org/W2153573511",
    "https://openalex.org/W2092710777",
    "https://openalex.org/W2024998154",
    "https://openalex.org/W2277948250",
    "https://openalex.org/W2790306973",
    "https://openalex.org/W2920951612",
    "https://openalex.org/W6761669123",
    "https://openalex.org/W2591980212",
    "https://openalex.org/W4241079583",
    "https://openalex.org/W32403112",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W6677737365",
    "https://openalex.org/W2106424475",
    "https://openalex.org/W1499925099",
    "https://openalex.org/W41554520",
    "https://openalex.org/W2120346334",
    "https://openalex.org/W2345892448",
    "https://openalex.org/W2941693457",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W1763243278",
    "https://openalex.org/W2962848173",
    "https://openalex.org/W2145761514",
    "https://openalex.org/W2586680856",
    "https://openalex.org/W2908649203",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2770447299"
  ],
  "abstract": "In this paper, we address the problem of setting the tap positions of load tap changers (LTCs) for voltage regulation in radial power distribution systems under uncertain load dynamics. The objective is to find a policy to determine the tap positions that only uses measurements of voltage magnitudes and topology information so as to minimize the voltage deviation across the system. We formulate this problem as a Markov decision process (MDP), and propose a batch reinforcement learning (RL) algorithm to solve it. By taking advantage of a linearized power flow model, we propose an effective algorithm to estimate the voltage magnitudes under different tap settings, which allows the RL algorithm to explore the state and action spaces freely offline without impacting the system operation. To circumvent the \"curse of dimensionality\" resulted from the large state and action spaces, we propose a sequential learning algorithm to learn an action-value function for each LTC, based on which the optimal tap positions can be directly determined. The effectiveness of the proposed algorithm is validated via numerical simulations on the IEEE 13-bus and 123-bus distribution test feeders.",
  "full_text": "arXiv:1807.10997v2  [math.OC]  14 Feb 2019\n1\nOptimal T ap Setting of V oltage Regulation\nTransformers Using Batch Reinforcement Learning\nHanchen Xu, Student Member , IEEE, Alejandro D. Dom´ ınguez-Garc´ ıa, Member , IEEE and\nPeter W . Sauer, Life F ellow , IEEE\nAbstract—In this paper , we address the problem of setting the\ntap positions of load tap changers (L TCs) for voltage regula tion in\nradial power distribution systems under uncertain load dyn amics.\nThe objective is to ﬁnd a policy to determine the tap position s\nthat only uses measurements of voltage magnitudes and topol ogy\ninformation so as to minimize the voltage deviation across\nthe system. W e formulate this problem as a Markov decision\nprocess (MDP), and propose a batch reinforcement learning ( RL)\nalgorithm to solve it. By taking advantage of a linearized po wer\nﬂow model, we propose an effective algorithm to estimate the\nvoltage magnitudes under different tap settings, which all ows\nthe RL algorithm to explore the state and action spaces freel y\nofﬂine without impacting the system operation. T o circumve nt the\n“curse of dimensionality” resulted from the large state and action\nspaces, we propose a sequential learning algorithm to learn an\naction-value function for each L TC, based on which the optim al\ntap positions can be directly determined. The effectivenes s of the\nproposed algorithm is validated via numerical simulations on the\nIEEE 13-bus and 123-bus distribution test feeders.\nIndex T erms —voltage regulation, load tap changer , data-\ndriven, Markov decision process, reinforcement learning.\nI. I N T RO D U CT IO N\nV\nOL T AGE regulation transformers—also referred to as\nload tap changers (L TCs)—are widely utilized in power\ndistribution systems to regulate the voltage magnitudes al ong\na feeder. Conventionally, the tap position of each L TC is\ncontrolled through an automatic voltage regulator based on\nlocal voltage measurements [1]. This approach, albeit simp le\nand effective, is not optimal in any sense, and may result in f re-\nquent actions of the L TCs, thus, accelerating wear and tear [ 2].\nParticularly, the voltage deviation may not be minimized. I n\nthe context of transmission systems, transformer tap posit ions\nare optimized jointly with active and reactive power genera tion\nby solving an optimal power ﬂow (OPF) problem, which\nis typically cast as a mixed-integer programming problem\n(see, e.g., [3], [4] and references therein). Similar OPF-b ased\napproaches are also adopted in power distribution systems.\nFor example, in [2], the authors cast the optimal tap setting\nproblem as a rank-constrained semideﬁnite program that is\nfurther relaxed by dropping the rank-one constraint, which\navoids the non-convexity and integer variables, and thus, t he\nproblem can be solved efﬁciently. OPF-based approaches hav e\nalso been utilized to determine the optimal reactive power\ninjection from distributed energy resources so as to regula te\nvoltage in a distribution network [5], [6].\nThe authors are with the Department of Electrical and Comput er Engineer-\ning at the University of Illinois at Urbana-Champaign, Urba na, IL 61801,\nUSA. Email: {hxu45, aledan, psauer }@illinois.edu.\nWhile these OPF-based approaches are effective in reg-\nulating voltages, they require complete system knowledge,\nincluding active and reactive power injections, and transm is-\nsion/distribution line parameters. While it may be reasona ble\nto assume that such information in available for transmis-\nsion systems, the situation in distribution systems is quit e\ndifferent. Accurate line parameters may not be known and\npower injections at each bus may not be available in real\ntime, which prevents the application of OPF-based approach es\n[7]. In addition, OPF-based approaches typically deal with\none snapshot of system conditions, and assume loads remain\nconstant between two consecutive snapshots. Therefore, th e\noptimal tap setting problem needs to be solved for each\nsnapshot in real time.\nIn this paper, we develop an algorithm that can ﬁnd a policy\nfor determining the optimal tap positions of the L TCs in a\npower distribution system under uncertain load dynamics wi th-\nout any information on power injections or line parameters;\nthe algorithm requires only voltage magnitude measurement s\nand system topology information. Speciﬁcally, the optimal\ntap setting problem is cast as a Markov decision process\n(MDP), which can be solved using reinforcement learning\n(RL) algorithms. Y et, adequate state and action samples tha t\nsufﬁciently explore the MDP state and action spaces are\nneeded. However, it is hard to obtain such samples in real\npower systems since this requires changing tap settings and\nother controls to excite the system and record voltage re-\nsponses, which may jeopardize system operational reliabil ity\nand incur economic costs. T o circumvent this issue, we take\nadvantage of a linearized power ﬂow model and develop\nan effective algorithm to estimate voltage magnitudes unde r\ndifferent tap settings so that the state and action spaces ca n\nbe explored freely ofﬂine without impacting the real system .\nThe dimension of the state and action spaces increases\nexponentially as the number of L TCs grows, which causes\nthe issue known as the “curse of dimensionality” and makes\nthe computation of the optimal policy intractable [8]. T o ci r-\ncumvent the “curse of dimensionality, ” we propose an efﬁcie nt\nbatch RL algorithm—the least squares policy iteration (LSP I)\nbased sequential learning algorithm—to learn an action-va lue\nfunction sequentially for each L TC. Once the learning of\nthe action-value function is completed, we can determine th e\npolicy for optimally setting the L TC taps. W e emphasize that\nthe optimal policy can be computed ofﬂine, where most com-\nputational burden takes place. However, when executed onli ne,\nthe required computation to ﬁnd the optimal tap positions\nis minimal. The effectiveness of the proposed algorithm is\n2\nvalidated through simulations on two IEEE distribution tes t\nfeeders.\nThe remainder of the paper is organized as follows. Section\nII introduces a linearized power ﬂow model that includes\nthe effect of L TCs and describes the optimal tap setting\nproblem. Section III provides a primer on MDPs and the LSPI\nalgorithm. Section IV develops an MDP-based formulation\nfor the optimal tap setting problem and Section V proposes\nan algorithm to solve this problem. Numerical simulation\nresults on two IEEE test feeders are presented in Section VI.\nConcluding remarks are provided in Section VII.\nII. P RE L IM INA RIE S\nIn this section, we review a linearized power ﬂow model\nfor power distribution systems, and modify it to include the\neffect of L TCs. W e also describe the L TC tap setting problem.\nA. P ower Distribution System Model\nConsider a power distribution system that consists of a set\nof buses indexed by the elements in N= {0, 1, ··· , N }, and\na set of transmission lines indexed by the elements in L=\n{1, ··· , L}. Each line ℓ ∈L is associated with an ordered pair\n(i, j) ∈N×N . Assume bus 0 is an ideal voltage source that\ncorresponds to a substation bus, which is the only connectio n\nof the distribution system to the bulk power grid.\nLet Vi denote the magnitude of the voltage at bus i, i ∈N,\nand deﬁne vi := V 2\ni ; note that u0 is a constant since bus 0 is\nassumed to be an ideal voltage source. Let pi and qi denote the\nactive power injection and reactive power injection at bus i,\ni ∈N, respectively. For each line ℓ ∈L that is associated with\n(i, j), let pij and qij respectively denote active and reactive\npower ﬂows on line (i, j), which are positive if the ﬂow of\npower is from bus i to bus j and negative otherwise. Let rℓ\nand xℓ denote the resistance and reactance of line ℓ, ℓ ∈L.\nFor a radial power distribution system, the relation betwee n\nsquared voltage magnitudes, power injections, and line pow er\nﬂows, can be captured by the so-called LinDisfFlow model\n[9] as follows:\npij = −pj +\n∑\nk:(j,k)∈L\npjk , (1a)\nqij = −qj +\n∑\nk:(j,k)∈L\nqjk , (1b)\nvi −vj = 2( rℓpij + xℓqij), (1c)\nwhere ℓ is associated with (i, j).\nDeﬁne r = [ r1, ··· , rL]⊤ and x = [ x1, ··· , xL]⊤. Let\n˜M = [ ˜Miℓ] ∈(N+1)×L, with ˜Miℓ = 1 and ˜Mjℓ = −1 if line ℓ\nis associated with (i, j), and all other entries equal to zero. Let\nm⊤ denote the ﬁrst row of ˜M and M the matrix that results\nby removing m⊤ from ˜M. For a radial distribution system,\nL = N, and M is invertible. Deﬁne v = [ v1, ··· , vN ]⊤,\np = [ p1, ··· , pN ]⊤, and q = [ q1, ··· , qN ]⊤. Then, the\nLinDistFlow model in (1) can be written as follows:\nM⊤v+ mv0 = 2diag( r)M−1p+ 2diag(x)M−1q, (2)\nwhere diag(·) returns a diagonal matrix with the entries of the\nargument as its diagonal elements.\nFig. 1. Load tap changer model.\nThe standard model for an L TC in the literature is shown in\nFig. 1 (see, e.g., [1]), where i = √−1, line ℓ is associated\nwith (i, j), and tℓ is the tap ratio of the L TC on line ℓ.\nT ypically, the tap ratio can possibly take on 33 discrete\nvalues ranging from 0.9 to 1.1, by an increment of 5/8%\np.u., i.e., tℓ ∈T = {0.9, 0.90625, ··· , 1.09375, 1.1}[1]. Let\n∆ tℓ ∈∆ T = {0, ±0.00625, ··· , ±0.19375, ±0.2}denote the\nset of all feasible L TC tap ratio changes. W e index the 33 tap\npositions by −16, ··· , −1, 0, 1, ··· , 16 for convenience.\nLet Lt denote the set of lines with L TCs and let |Lt|= Lt,\nwhere |·| denotes the cardinality of a set. For line ℓ that\nis associated with (i, j), if ℓ ∈Lt, the voltage relation in the\nLinDistFlow model, i.e., (1c), needs to be modiﬁed as follow s:\n1\nt2\nℓ\nvi −vj = 2( rℓpij + xℓqij). (3)\nDeﬁne t = [ tℓ]⊤ and ∆ t = [∆ tℓ]⊤, ℓ ∈Lt. Let ˜M(t) =\n[ ˜Miℓ(t)] ∈R(N+1)×L, with ˜Miℓ(t) = 1 and ˜Mjℓ (t) = −1 if\nline ℓ ∈L\\L t, ˜Miℓ(t) = 1\nt2\nℓ\nand ˜Mjℓ(t) = −1 if line ℓ ∈Lt,\nand all other entries equal to zero. Let m(t)⊤ denote the ﬁrst\nrow of ˜M(t) and M(t) the matrix that results by removing\nm(t)⊤ from ˜M(t). The matrix M(t) is non-singular when\nthe power distribution system is connected. Then, the modiﬁ ed\nmatrix-form LinDistFlow model that takes into account the\nL TCs is given by:\nM(t)⊤v+ m(t)v0 = 2diag( r)M−1p+ 2diag(x)M−1q.\n(4)\nB. Optimal T ap Setting Problem\nT o effectively regulate the voltages in a power distributio n\nsystem, the tap positions of L TCs need to be set appropriatel y.\nThe objective of the optimal tap setting problem is to ﬁnd a\npolicy π that determines the L TC tap ratio so as to minimize\nthe voltage deviation from some reference value, denoted by\nv⋆, based on current tap ratios and measurements of the\nvoltage magnitudes, i.e., π : ( t, v) →∆ t, t ∈T Lt\n, v ∈\nRN , ∆ t ∈ ∆ TLt\n. Throughout this paper, we make the\nfollowing two assumptions:\nA1. The distribution system topology is known but the line\nparameters are unknown.\nA2. The active and reactive power injections are not measured\nand their probability distributions are unknown.\n3\nIII. M A RKOV DE CIS IO N PRO CE S S A N D BAT CH\nRE IN F O RCE M E N T LE A RN IN G\nIn this section, we provide some background on MDPs and\nthe batch RL algorithm, a type of data efﬁcient and stable\nalgorithm for solving MDPs with unknown models.\nA. Markov Decision Process\nAn MDP is deﬁned as a 5-tuple (S, A, P, R, γ), where S\nis a ﬁnite set of states, Ais a ﬁnite set of actions, P is a\nMarkovian transition model that denotes the probability of\ntransitioning from one state into another after taking an ac tion,\nR: S×A×S→ R is a reward function such that, for s, s′ ∈S\nand a∈A, r = R(s, a, s′) is the reward obtained when the\nsystem transitions from state sinto state s′ after taking action\na, and γ ∈[0, 1) is a discount factor (see, e.g., [10]). 1 W e refer\nto the 4-tuple (s, a, r, s′), where s′ is the state following s\nafter taking action aand r = R(s, a, s′), as a transition.\nLet Sk and Ak denote the state and action at time instant\nk, respectively, and Rk the reward received after taking\naction Ak in state Sk. Let P denote the probability operator;\nthen, Pk(s′|s, a) := P {Sk+1 = s′|Sk = s, Ak = a}is the\nprobability of transitioning from state s into state s′ after\ntaking action aat instant k. Throughout this paper, we assume\ntime-homogeneous transition probabilities, hence we drop the\nsubindex k and just write P(s′|s, a).\nLet ¯R : S×A → R denote the expected reward for a\nstate-action pair (s, a); then, we have\n¯R(s, a) = E [R] =\n∑\ns′∈S\nR(s, a, s′)P(s′|s, a), (5)\nwhere E [·] denotes the expectation operation. The total dis-\ncounted reward from time instant k and onwards, denoted by\nGk, also referred to as the return, is given by\nGk =\n∞∑\nk′=k\nγk′−kRk′ . (6)\nA deterministic policy π is a mapping from Sto A, i.e.,\na = π(s), s ∈S , a ∈A. The action-value function under\npolicy π is deﬁned as follows:\nQπ (s, a) = E [Gk|Sk = s, Ak = a; π] , (7)\nwhich is the expected return when taking action a in state\ns, and following policy π afterwards. Intuitively, the action-\nvalue function quantiﬁes, for a given policy π, how “good”\nthe state-action pair (s, a) is in the long run.\nLet Q∗(·, ·) denote the optimal action-value function—\nthe maximum action-value function over all policies, i.e.,\nQ∗(s, a) = max π Qπ (s, a). All optimal policies share the\nsame optimal action-value function. Also, the greedy polic y\nwith respect to Q∗(s, a), i.e., π∗(s) = arg max a Q∗(s, a)\nis an optimal policy. Then, it follows from (6) and (7) that\n1 These deﬁnitions can be directly extended to the case where t he the set of\nstates is inﬁnite. Due to space limitation, this case is not d iscussed in detail\nhere.\nQ∗(s, a) satisﬁes the following Bellman optimality equation\n(see, e.g., [8]):\nQ∗(s, a) = ¯R(s, a) + γ\n∑\ns′∈S\nP(s′|s, a) max\na′∈A\nQ∗(s′, a′).\n(8)\nThe MDP is solved if we ﬁnd Q∗(s, a), and correspondingly,\nthe optimal policy π∗. It is important to emphasize that (8) is\nkey in solving the MDP . For ease of notation, in the rest of\nthis paper, we simply write the Q∗(s, a) as Q(s, a).\nWhen both the state and the action sets are ﬁnite, the action-\nvalue function can be exactly represented in a tabular form t hat\ncovers all possible pairs (s, a) ∈S×A . In this case, if Pis\nalso known, then the MDP can be solved using, e.g., the so-\ncalled policy iteration and value iteration algorithms (se e, e.g.,\n[8]). If Pis unknown but samples of transitions are available,\nthe MDP can be solved by using RL algorithms such as the\nQ-learning algorithm (see, e.g., [11]).\nB. Batch Reinforcement Learning\nWhen S is not ﬁnite, conventional Q-learning based ap-\nproaches require discretization of S(see, e.g., [12] and [13]).\nThe discretized state space will better approximate the ori ginal\nstate space if a small step size is used in the discretization pro-\ncess, yet the resulting MDP will face the “curse of dimension -\nality. ” A large step size can alleviate the computational bu rden\ncaused by the high dimensionality of the state space, but at\nthe cost of potentially degrading performance signiﬁcantl y.\nMore practically, when the number of elements in S is\nlarge or S is not ﬁnite, the action-value function can be\napproximated by some parametric functions such as linear\nfunctions [10] and neural networks [14]. Let ˆQ(·, ·) denote\nthe approximate optimal action-value function. Using a lin ear\nfunction approximation, ˆQ(s, a) can be represented as fol-\nlows:\nˆQ(s, a) = w⊤φ(s, a), (9)\nwhere φ : S×A→ Rf is a feature mapping for (s, a), which\nis also referred to as the basis function, and w ∈Rf is the\nparameter vector.\nA class of stable and data-efﬁcient RL algorithms that can\nsolve an MDP with function approximations are the batch\nRL algorithms—“batch” in the sense that a set of transition\nsamples are utilized each time—such as the LSPI algorithm\n[10], which is considered to be the most efﬁcient one in this\nclass. W e next explain the fundamental idea behind the LSPI\nalgorithm. Let D= {(s, a, r, s′) : s, s′ ∈S, a∈A} denote\na set (batch) of transition samples obtained via observatio n\nor simulation. The LSPI algorithm ﬁnds the best w that\nﬁts the transition samples in Din an iterative manner. One\nway to explain the intuition behind the LSPI algorithm is as\nfollows (the readers are referred to [10] for a more rigorous\ndevelopment). Deﬁne\ng(w) =\n∑\n(s,a,r,s′)∈D\n(Q(s, a) −w⊤φ(s, a))2. (10)\n4\nLet wi denote the value of wthat is available at the beginning\nof iteration i. At iteration i, the algorithm ﬁnds wi+1 by\nsolving the following problem:\nwi+1 = arg min\nw\ng(w), (11)\nwhich is an unconstrained optimization problem. The soluti on\nof (11) can be computed by setting the gradient of g(·) to zero\nas follows:\n∂g\n∂w = −2\n∑\n(s,a,r,s′)∈D\n(Q(s, a) −w⊤φ(s, a))φ(s, a) = 0f .\n(12)\nNote that the true value of Q(s, a) is not known and is\nsubstituted by the so-called temporal-difference (TD) tar get,\nr+γw⊤φ(s′, a′), where a′ = arg max a∈A w⊤\ni φ(s′, a) is the\noptimal action in state s′ determined based on wi. Note that\nthe TD target is a sample of the right-hand-side (RHS) of (8),\nwhich serves as an estimate for the RHS of (8). W e emphasize\nthat despite Q(s, a) being substituted by r + γw⊤φ(s′, a′),\nthe true optimal action-value function is not a function of w;\ntherefore, the gradient of g with respect to wis taken before\nthe Q(s, a) is approximated by the TD target, which does\ndepends on w. Then, after replacing Q(s, a) with the TD\ntarget, (12) has the following closed-form solution:\nwi+1 =\n\n ∑\n(s,a,r,s′)∈D\nφ(s, a)(φ(s, a) −γφ(s′, a′))⊤\n\n\n−1\n×\n∑\n(s,a,r,s′)∈D\nφ(s, a)r. (13)\nIntuitively, at each iteration, the LSPI algorithm ﬁnds the w\nthat minimizes the mean squared error between the TD target\nand ˆQ(s, a) over all transition samples in D. This process is\nrepeated until change of w, deﬁned as ∥wi+1 −wi∥, where\n∥·∥denotes the L2-norm, becomes smaller than a threshold ε,\nupon which the algorithm is considered to have converged.\nThe LSPI algorithm has the following three nice properties.\nFirst, linear functions are used to approximate the optimal\naction-value function, which allows the algorithm to handl e\nMDPs with high-dimensional or continuous state spaces. Sec -\nond, at each iteration, a batch of transition samples is used to\nupdate the vector wparameterizing ˆQ(·, ·), and these samples\nare reused at each iteration, thus increasing data efﬁcienc y.\nThird, the optimal parameter vector is found by solving a\nleast-squares problem, resulting in a stable algorithm. W e refer\ninterested readers to [10] for more details on the convergen ce\nand performance guarantee of the LSPI algorithm.\nIV . O P T IM A L TA P SE T T IN G PRO BL E M A S AN MDP\nIn this section, we formulate the optimal tap setting proble m\nas an MDP as follows:\n1) State space: Deﬁne the squared voltage magnitudes\nat all buses but bus 0 and the tap ratios as the state, i.e.,\ns = ( t, v), which has both continuous and discrete variables.\nThen, the state space is S⊆T Lt\n×RN .\n2) Action space: The actions are the L TC tap ratio changes,\ni.e., a = ∆ t, and the action space is the set of all feasible\nvalues of L TC tap ratios, i.e., A= ∆ TLt\n. In the optimal tap\nsetting problem, the action is discrete. The size of the acti on\nspace increases exponentially with the number of L TCs.\n3) Reward function: The objective of voltage regulation is\nto minimize the voltage deviation as measured by the L2 norm.\nAs such, when the system transitions from state s= ( t, v) into\nstate s′ = ( t′, v′) after taking action a= ∆ t := t′ −t, the\nreward is computed by the following function:\nR(s, a, s′) = −1\nN ∥v′ −v⋆∥. (14)\n4) T ransition model: T o derive the transition model P, note\nthat it follows from (4) that\nv′ =(M(t′)⊤)−1(ξ+ M(t)⊤v+ m(t)v0 −m(t′)v0),\n(15)\nwhere ξ = 2diag( r)M−1(p′ −p) + 2diag( x)M−1(q′ −\nq), and p′ and q′ are active and reactive power injections\nthat results into v′, respectively. Then, the transition model\nP(s′|s, a) can be derived from the probability density function\n(pdf) of (v′|v, t, ∆ t), which can be further computed from the\npdf of (ξ|v, t, ∆ t). However, under Assumptions A1 and A2,\nthe line parameters as well as the probability distribution s of\nactive and reactive power injections are unknown; thus, the\ntransition model is not known a priori. Therefore, we need\nto resort to RL algorithms that do not require an explicit\ntransition model to solve the MDP .\nV . O P T IM A L TA P SE T T IN G AL G O RIT H M\nIn this section, we propose an optimal tap setting algorithm ,\nwhich consists of a transition generating algorithm that ca n\ngenerate samples of transitions in D, and an LSPI-based se-\nquential learning algorithm to solve the MDP . Implementati on\ndetails such as the feature selection are also discussed.\nA. Overview\nThe overall structure of the optimal tap setting framework\nis illustrated in Fig. 2. The framework consists of an enviro n-\nment that is the power distribution system, a learning agent\nthat learns the action-value function from a set of transiti on\nsamples, and an acting agent that determines the optimal\naction from the action-value function. Deﬁne the history to\nbe the sequence of states, actions, and rewards, and denote i t\nby H, i.e., H= {s0, a0, r0, s1, a1, r1, ···}. Speciﬁcally, the\nlearning agent will use the elements in the set Htogether with\na virtual transition generator to generate a set of transiti on\nsamples D according to some exploratory behavior deﬁned\nin the exploratory actor. The set of transition samples in Dis\nthen used by the action-value function estimator—also refe rred\nto as the critic—to ﬁt an approximate action-value function\nusing the LSPI algorithm described earlier. The learning ag ent,\nwhich has a copy of the up-to-date approximate action-value\nfunction from the learning agent, ﬁnds a greedy action for th e\ncurrent state and instructs the L TCs to follow it.\nNote that the learning of the action-value function can\nbe done ofﬂine by the learning agent, which is capable\n5\nPower Distribution \nSystem \nEnvironment \nGreedy Actor \nActing Agent \naction \nreward \nAction-Value Function \nEstimator (Critic) \nHistory \nLearning Agent \nVirtual Transition \nGenerator \nExploratory Actor \nstate \naction \naction-value \nfunction estimate \nFig. 2. The batch RL based framework for optimal tap setting. (Dotted line\nindicates the critic is optional for the exploratory actor . )\nof exploring various system conditions through the virtual\ntransition generator based on the history H, yet without\ndirectly interacting with the power distribution system. T his\navoids jeopardizing system operational reliability, whic h is a\nmajor concern when applying RL algorithms to power system\napplications [15].\nB. V irtual T ransition Generator\nThe LSPI algorithm (as well as all other RL algorithms)\nrequire adequate transition samples that spread over the st ate\nand action spaces S×A . However, this is challenging in\npower systems since the system operational reliability mig ht\nbe jeopardized when exploring randomly. One way to work\naround this issue is to use simulation models, rather than th e\nphysical system, to generate virtual transitions. T o this e nd, we\ndevelop a data-driven virtual transition generator that si mulates\ntransitions without any knowledge of the active and reactiv e\npower injections (neither measurements nor probability di stri-\nbutions) or the line parameters.\nThe fundamental idea is the following. For a transition\nsample (s, a†, r†, s† = ( t†, v†)) that is obtained from H, the\nvirtual transition generator generates a new transition sa mple\n(s, a‡, r‡, s‡ = ( t‡, v‡)), where a‡ is determined from s\naccording to some exploration policy (to be deﬁned later) th at\naims to explore the state and action spaces. Replacing a† in\nthe ﬁrst transition sample with a‡, the voltage magnitudes will\nchange accordingly. Assume the same transition of the power\ninjections in these two samples, then the RHS of (4) does not\nchange. Thus, v‡ can be readily computed from v† by solving\nthe following set of linear equations:\nM(t‡)⊤v‡ + m(t‡)v0 = M(t†)⊤v† + m(t†)v0. (16)\nSince the only unknown in (16) is v‡ ∈ R and M(t‡) ∈\nRN×N is invertible, we can solve for v‡ as follows:\nv‡ = ( M(t‡)⊤)−1(M(t†)⊤v† + m(t†)v0 −m(t‡)v0).\n(17)\nAlgorithm 1: V irtual transition Generating\nInput: H, D, v⋆, exploration policy\nOutput: D\nInitialize D← ∅\nfor d = 1 , ··· , D do\nChoose a transition sample (s, a†, r†, s† = ( t†, v†))\nfrom H\nSelect a‡ according to exploration policy and set\nt‡ = t† + a‡\nEstimate v‡ following a‡ as v‡ = ϕ(v†, t†, t‡)\nCompute the reward by r‡ = −1\nN ∥v‡ −v⋆∥\nAdd (s, a‡, r‡, s‡ = ( t‡, v‡)) to D\nend\nFor ease of notation, we simply write (17) as\nv‡ = ϕ(v†, t†, t‡). (18)\nThis nice property allows us to estimate the new values of\nvoltage magnitudes when the tap positions change without\nknowing the exact values of power injections and line parame -\nters. The virtual transition generating procedure is summa rized\nin Algorithm 1.\nC. LSPI-based Sequential Action-V alue Function Learning\nGiven the transition sample set D, we can now develop a\nlearning algorithm for ˆQ(s, a) based on the LSPI algorithm.\nWhile the LSPI is very efﬁcient when the action space is\nrelatively small, it becomes computationally intractable when\nthe action space is large, since the number of unknown pa-\nrameters in the approximate action-value function is typic ally\nproportional to |A|, which increases exponentially with the\nnumber of L TCs. T o overcome the “curse of dimensionality”\nthat results from the size of the action space, we propose an\nLSPI-based sequential learning algorithm to learn the acti on-\nvalue function.\nThe key idea is the following. Instead of learning an ap-\nproximate optimal action-value function for the action vec tor\na, we learn a separate approximate action-value function for\neach component of a. T o be more speciﬁc, for each L TC\nl, l = 1 , ··· , Lt, we learn an approximate optimal action-\nvalue function ˆQ(l)(s, a(l)) = φ(l)(s, a(l))⊤w(l), where a(l)\nis the lth component of a, φ(l)(·, ·) is a feature mapping from\nS× ∆ T to Rf . During the learning process of w(l), the\nrest of the L TCs are assumed to behave greedily according\nto their own approximate optimal action-value function. T o\nachieve this, we design the following exploration policy to\ngenerate the virtual transition samples Dused when learning\nw(l) for L TC l. In the exploration step in Algorithm 1, the tap\nratio change of L TC l is selected uniformly in ∆ T (uniform\nexploration), while those of others are selected greedily w ith\nrespect to the up-to-date ˆQ(l)(·, ·) (greedy exploration). Then,\nthe LSPI algorithm detailed in Algorithm 2, where c is a small\npositive pre-condition number and w(l)\n1 is the initial value for\nthe parameter vector, is applied to learn w(l). This procedure is\nrepeated in a round-robin fashion for all L TCs for J iterations,\nin each of which w(l)\n1 is set to the up-to-date w(l) learned in\n6\nAlgorithm 2: LSPI for Single L TC\nInput: l, D, φ, γ, ε, c, w(l)\n1\nOutput: w(l)\nInitialize w(l)\n0 = 0f and i = 1\nwhile ∥w(l)\ni −w(l)\ni−1∥> ε or i = 1 do\nInitialize B0 = cIf×f and b0 = 0f , set j = 1\nfor (s, a, r, s′) ∈D do\na(l)′\n= arg max a∈∆ T φ(s′, a)⊤w(l)\ni\nBj = Bj−1 + φ(s, a(l))(φ(s, a(l)) −γφ(s′, a(l)′\n))⊤\nbj = bj−1 + φ(s, a(l))r\nIncrease j by 1\nend\nw(l)\ni+1 = B−1\n|D|b|D|, increase i by 1\nend\nthe previous iteration or chosen if it is in the ﬁrst iteratio n.\nThe value of J is set to 1 if there is only one L TC and is\nincreased slightly when there are more L TCs. Note that a\nnew set of transitions Dis generated when learning w(l) for\ndifferent L TCs at each iteration. Using this sequential lea rning\nalgorithm, the total number of unknowns is then proportiona l\nto Lt|∆ T|, which is far fewer compared to |∆ TLt\n|as in the\ncase where the approximate optimal action-value function f or\nthe entire action vector, a, is learned.\nA critical step in implementing the LSPI algorithm is\nconstructing features from the state-action pair (s, a(l)) for\nL TC l; we use radial basis function (RBFs) to this end. The\nfeature vector for a state-action pair (s, a(l)), i.e., φ(l)(s, a(l)),\nis a vector in Rf , where f = ( κ + 1) ×|∆ T| and κ is a\npositive integer. φ(l)(s, a(l)) has |∆ T| segments, each one\nof length κ + 1 corresponding to a tap change in ∆ T, i.e,\nφ(l)(s, a(l)) = [ ψ⊤\n1 , ··· , ψ⊤\n|∆ T | ]⊤, where ψi ∈ Rκ+1, i =\n1, ··· , |∆ T|. Speciﬁcally, for s = ( t, v) and a(l) being\nthe ith tap change in ∆ T, ψj = 0κ+1 for j ̸= i, and\nψi = [1 , e− ∥ ˜v−¯v1∥\nσ2 , ··· , e− ∥ ˜v−¯vκ∥\nσ2 ]⊤, where σ > 0, ˜v =\nϕ(v, t, ˜t) with ˜t being obtained by replacing the lth entry\nin t with 1, and ¯vi, i = 1 , ··· , κ are pre-speciﬁed constant\nvectors in RN referred to as the RBF centers. The action\na(l) only determines which segment will be non-zero. Thus,\n˜v is indeed the squared voltage magnitudes under the same\npower injections if the tap of L TC l is at position 0. Each\nRBF computes the distance between v′ and some pre-speciﬁed\nsquared voltage magnitudes.\nD. T ap Setting Algorithm\nThe tap setting algorithm, the timeline of which is illustra ted\nin Fig. 3, works as follows. At time instant k, a new state\nsk as well as the reward following the action ak−1, rk−1, is\nobserved. Let ∆ T denote the time ellapsed between two time\ninstants. Every K time instants, i.e., every K∆ T units of time,\nw(l), is updated by the learning agent by executing the LSPI-\nbased sequential learning algorithm described in Section V -C.\nThe acting agent then ﬁnds a greedy action for the current sta te\nsk and sends it to the L TCs. In order to reduce the wear and\nAlgorithm 3: Optimal T ap Setting\nInput: φ, K, J, ǫ\nfor k = 1 , 2, ··· do\nObtain rk−1 and sk, and add them into H\nif k mod K = 0 then\nfor j = 1 , ··· , J do\nfor l = 1 , ··· , Lt do\nRun Algo. 1 to generate Dusing uniform\nexploration for L TC l and greedy\nexploration for other L TCs\nRun Algo. 2 with w(l)\n1 set to the current\nw(l)\nend\nend\nend\nfor l = 1 , ··· , Lt do\nSet a(l)\nk = arg max\na∈∆ T\nφ(sk, a)⊤w(l) if\nmax\na∈∆ T\nφ(sk, a)⊤w(l) −φ(sk, a(l)\nk−1)⊤w(l) > ǫ\nSet a(l)\nk = a(l)\nk−1 otherwise\nend\nAdd ak to Hand adjust tap ratios based on ak\nend\ntime \nLTC tap position adjustment \npolicy update policy update \nFig. 3. Timeline for L TC tap setting.\ntear on the L TCs, the greedy action for the current state sk is\nchosen only if the difference between the action-value resu lt-\ning from the greedy action, i.e., max\na∈∆ T\nφ(sk, a)⊤w(l), and that\nresulting from the previous action, i.e., φ(sk, a(l)\nk−1)⊤w(l), is\nlarger than a threshold ǫ. Otherwise, the tap positions do not\nchange. The above procedure is summarized in Algorithm 3.\nVI. N U M E RICA L SIM U L AT IO N\nIn this section, we apply the proposed methodology to the\nIEEE 13-bus and 123-bus test feeders from [16].\nA. Simulation Setup\nThe power injections for both these two test feeders are\nconstructed based on historical hourly active power load da ta\nfrom a residential building in San Diego over one year [17].\nSpeciﬁcally, the historical hourly active power load data a re\nﬁrst scaled up so that the maximum system total active\npower load over that year for the IEEE 13-bus and 123-\nbus distribution test feeders are 6.15 MW and 12.3 MW ,\nrespectively. These numbers are chosen so that the resultin g\nvoltage magnitudes fall outside of the desired range at some\n7\ntime instants. Then, the time granularity of the scaled syst em\ntotal active power load is increased to 5 minutes through a\nlinear interpolation. Each value in the resulting ﬁve-minu te\nsystem total active power load data time series is further\nmultiplied by a normally distributed variable, the mean and\nstandard deviation of which is 1 and 0.02, respectively. The\nactive power load proﬁle at each bus is constructed by pseudo -\nrandomly redistributing the system total active power load\namong all load buses. Each load bus is assumed to have a\nconstant power factor of 0.95. While only load variation is\nconsidered in the simulation, the proposed methodology can\nbe directly applied to the case with renewable-based resour ces,\nwhich can be modeled as negative loads.\nW e ﬁrst verify the accuracy of the virtual transition gener-\nating algorithm. Speciﬁcally, assume the voltage magnitud es\nare known for some unknown power injections under a known\ntap ratio of 1. Then, when the tap ratio changes, we compute\nthe true voltage magnitudes under the new tap ratio, denoted\nby v, by solving the full ac power ﬂow problem, and the\nestimated voltage magnitudes under new tap ratio, denoted\nby ˆv, via (18). Simulation results indicate that the maximum\nabsolute difference between the true and the estimated volt age\nmagnitude, i.e., ∥v−ˆv∥∞, is smaller than 0.001 p.u., which\nis accurate enough for the application of voltage regulatio n\naddressed in this paper.\nB. Case Study on the IEEE 13-bus T est F eeder\nAssume v⋆ = 1N , where 1N is an all-ones vector in RN\nIn the simulation, 21 RBF centers are used, i.e., κ = 21 .\nSpeciﬁcally, ¯vi = (0 .895 + 0 .005i)2 ×1N , i = 1 , ··· , 21.\nThe duration between two time instants is ∆ T = 5 min. The\npolicy is updated every 2 hours, i.e., K = 24 . In each update,\nactual transition samples are chosen from the history over t he\nsame time interval in the previous 5 days, which are part of\nH, and new actions are chosen according to the exploration\npolicy described in Section V -C. A total number of D = 6000\nvirtual transitions are generated using Algorithm 1. Since this\ntest feeder only has one L TC, there is no need to sequentially\nupdate the approximate action-value function, so we set J = 1 .\nOther parameters are chosen as follows: γ = 0 .9, ε = 1 ×10−5,\nǫ = 1 ×10−4, c = 0 .1, and σ = 1 .\nAssuming complete and perfect knowledge on the system\nparameters as well as active and reactive power injections\nfor all time instants, we can ﬁnd the optimal tap position\nthat results in the highest reward by exhaustively searchin g\nthe action space, i.e., all feasible tap ratios, at each time\ninstant. It is important to point out that, in practice, the\nexhaustive search approach is infeasible since we do not\nhave the necessary information, and not practical due to the\nhigh computational burden. Results obtained by the exhaust ive\nsearch approach and the conventional tap setting scheme (se e,\ne.g., [1]), in which the taps are adjusted only when the volta ge\nmagnitudes exceed a desired range, e.g., [0.9, 1.1] p.u., are\nused to benchmark the proposed methodology.\nFigure 4 shows the tap positions (top panel) and the re-\nwards (bottom panel) under different approaches. The rewar ds\nresulted from these two approaches are very close. The daily\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n−8\n−6\n−4\n−2\ntap position batch RL\nexhaustive search\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n−3\n−2\n−1\nreward (×10− 2)\nbatch RL\nexhaustive search\nconventional scheme\nFig. 4. T ap positions and rewards for IEEE 13-bus test feeder .\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n0.94\n0.96\n0.98\n1.00\nvoltage magnitude (p.u.)\nConventional scheme\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n0.98\n1.00\n1.02\nvoltage magnitude (p.u.)\nBatch RL\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n0.98\n1.00\n1.02\nvoltage magnitude (p.u.)\nExhaustive search\nFig. 5. V oltage magnitude proﬁles of IEEE 13-bus test feeder .\nmean reward, i.e., ρ = 1\n288\n∑ 288\nk=1 rk, where rk is the reward\nat time instant k as deﬁned in (14), obtained by the batch\nRL approach and the exhaustive search approach is ρ =\n−4.279 ×10−3 and ρ = −4.156 ×10−3, respectively, while\nthat under the conventional scheme is ρ = −19.26×10−3. The\ntap positions under the batch RL approach and the exhaustive\n8\n0 2 4 6 8 10 12 14 16 18 20 22 24\ntime (hour)\n−10\n−5\nreward (×10− 3)\nbatch RL\nexhaustive search\nconventional scheme\nFig. 6. Rewards for IEEE 123-bus test feeder .\nsearch approach are aligned during most of the time during\nthe day. Note that the tap position under the conventional\nscheme remains at 0 since the voltage magnitudes are within\n[0.9, 1.1] p.u., and is not plotted. Figure 5 shows the voltage\nmagnitude proﬁles under the different tap setting algorith ms.\nThe voltage magnitude proﬁles under the proposed batch RL\napproach (see Fig. 5, center panel) are quite similar to thos e\nobtained via the exhaustive search approach (see Fig. 5, bot tom\npanel), both result in a higher daily mean reward than that\nresulted from the conventional scheme (see Fig. 5, top panel ).\nW e also would like to point out that Algorithm 2 typically\nconverges within 5 iterations in less than 20 seconds, and\nthe batch RL approach is faster than the exhaustive search\napproach by several orders of magnitude.\nC. Case Study on the IEEE 123-bus T est F eeder\nW e next test the proposed methodology on the IEEE 123-\nbus test feeder. In the results for the IEEE 13-bus test feede r\nreported earlier, while the L TC has 33 tap positions, only a\nsmall portion of them is actually used. This motivates us to\nfurther reduce the action space by narrowing the action spac e\nto a smaller range. Speciﬁcally, we can estimate the voltage\nmagnitudes under various power injections and L TC tap posi-\ntions using (18). After ruling out tap positions under which\nthe voltage magnitudes will exceed the desired range, we\neventually allow 9 positions, from −8 to 0, for two L TCs, and\n5 positions, from 0 to 5, for the other two L TCs. Here, κ = 11\nRBF centers are used. Speciﬁcally, ¯vi = (0 .94 + 0.01i)2 ×1N\nfor all L TCs except for the one near the substation, for which\n¯vi = (0 .89 + 0 .01i)2 ×1N , i = 1 , 2, ··· , 11. A total number\nof D = 3600 virtual transitions are generated in a similar\nmanner as in the IEEE 13-bus test feeder case. The number\nof iterations in the LSPI-based sequential learning algori thm\nis set to J = 3 . Other parameters are the same as in the IEEE\n13-bus test feeder case.\nFigure 6 shows the rewards under the batch RL approach\nand the exhaustive search. The daily mean reward obtained by\nthe batch RL approach and the exhaustive search approach is\nρ = −1.646 ×10−3 and ρ = −1.402 ×10−3, respectively,\nwhile that under the conventional scheme is ρ = −7.513 ×\n10−3. Due to the space limitation, other simulation results such\nas voltage proﬁles are not presented.\nVII. C O N CL U D IN G RE M A RK S\nIn this paper, we formulate the optimal tap setting problem\nof L TCs in power distribution systems as an MDP and propose\na batch RL algorithm to solve it. T o obtain adequate state-\naction samples, we develop a virtual transition generator\nthat estimates the voltage magnitudes under different tap\nsettings. T o circumvent the “curse of dimensionality”, we\nproposed an LSPI-based sequential learning algorithm to le arn\nan action-value function for each L TC, based on which the\noptimal tap positions can be determined directly. The propo sed\nalgorithm can ﬁnd the policy that determines the optimal\ntap positions that minimize the voltage deviation across th e\nsystem, based only on voltage magnitude measurements and\nnetwork topology information, which makes it more desirabl e\nfor implementation in practice. Numerical simulation on th e\nIEEE 13- and 123-bus test feeders validated the effectivene ss\nof the proposed methodology.\nRE F E RE N CE S\n[1] P . Kundur, N. J. Balu, and M. G. Lauby , P ower system stability and\ncontrol. McGraw-hill New Y ork, 1994, vol. 7.\n[2] B. A. Robbins, H. Zhu, and A. D. Dom´ ınguez-Garc´ ıa, “Opt imal tap\nsetting of voltage regulation transformers in unbalanced d istribution\nsystems, ” IEEE Trans. P ower Syst. , vol. 31, no. 1, pp. 256–267, Jan\n2016.\n[3] W . H. E. Liu, A. D. Papalexopoulos, and W . F . Tinney , “Disc rete shunt\ncontrols in a newton optimal power ﬂow , ” IEEE Trans. P ower Syst. ,\nvol. 7, no. 4, pp. 1509–1518, Nov 1992.\n[4] M. R. Salem, L. A. T alat, and H. M. Soliman, “V oltage contr ol\nby tap-changing transformers for a radial distribution net work, ” IEE\nProceedings - Generation, Transmission and Distribution , vol. 144,\nno. 6, pp. 517–520, Nov 1997.\n[5] H. Zhu and H. J. Liu, “Fast local voltage control under lim ited reactive\npower: Optimality and stability analysis, ” IEEE Trans. P ower Syst. ,\nvol. 31, no. 5, pp. 3794–3803, Sept. 2016.\n[6] B. A. Robbins and A. D. Dom´ ınguez-Garc´ ıa, “Optimal rea ctive power\ndispatch for voltage regulation in unbalanced distributio n systems, ” IEEE\nTrans. P ower Syst., vol. 31, no. 4, pp. 2903–2913, July 2016.\n[7] H. Xu, A. D. Dom´ ınguez-Garc´ ıa, and P . W . Sauer, “ A data- driven voltage\ncontrol framework for power distribution systems, ” in Proc. of IEEE PES\nGeneral Meeting, Portland, OR, Aug. 2018, pp. 1–5.\n[8] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction .\nMIT press, 2018.\n[9] M. E. Baran and F . F . Wu, “Network reconﬁguration in distr ibution\nsystems for loss reduction and load balancing, ” IEEE Trans. P ower Del.,\nvol. 4, no. 2, pp. 1401–1407, Apr 1989.\n[10] M. G. Lagoudakis and R. Parr, “Least-squares policy ite ration, ” Journal\nof machine learning research , vol. 4, no. Dec, pp. 1107–1149, 2003.\n[11] C. J. W atkins and P . Dayan, “Q-learning, ” Machine learning, vol. 8, no.\n3-4, pp. 279–292, 1992.\n[12] J. G. Vlachogiannis and N. D. Hatziargyriou, “Reinforc ement learning\nfor reactive power control, ” IEEE Trans. P ower Syst., vol. 19, no. 3, pp.\n1317–1325, 2004.\n[13] Y . Xu, W . Zhang, W . Liu, and F . Ferrese, “Multiagent-bas ed reinforce-\nment learning for optimal reactive power dispatch, ” IEEE Trans. Syst.,\nMan, Cybern., Syst., P art C (Applications and Reviews) , vol. 42, no. 6,\npp. 1742–1751, 2012.\n[14] V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. V eness , M. G.\nBellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ost rovski\net al. , “Human-level control through deep reinforcement learnin g, ”\nNature, vol. 518, no. 7540, p. 529, 2015.\n[15] M. Glavic, R. Fonteneau, and D. Ernst, “Reinforcement l earning for\nelectric power system decision and control: Past considera tions and\nperspectives, ” IF AC-P apersOnLine, vol. 50, no. 1, pp. 6918–6927, 2017.\n[16] IEEE distribution test feeders. [Online]. A vailable:\nhttps://ewh.ieee.org/soc/pes/dsacom/testfeeders/\n[17] Commercial and residential hourly load proﬁles for all\nTMY3 locations in the United States. [Online]. A vailable:\nhttps://openei.org/doe-opendata/dataset",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8283133506774902
    },
    {
      "name": "Markov decision process",
      "score": 0.6486086845397949
    },
    {
      "name": "Transformer",
      "score": 0.636357307434082
    },
    {
      "name": "Computer science",
      "score": 0.5836184620857239
    },
    {
      "name": "Control theory (sociology)",
      "score": 0.5182830691337585
    },
    {
      "name": "Voltage",
      "score": 0.491823673248291
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.49054402112960815
    },
    {
      "name": "Electric power system",
      "score": 0.47528019547462463
    },
    {
      "name": "Mathematical optimization",
      "score": 0.47021201252937317
    },
    {
      "name": "Tap changer",
      "score": 0.4357945919036865
    },
    {
      "name": "Markov process",
      "score": 0.36894112825393677
    },
    {
      "name": "Power (physics)",
      "score": 0.2609279751777649
    },
    {
      "name": "Mathematics",
      "score": 0.23196694254875183
    },
    {
      "name": "Engineering",
      "score": 0.2101927399635315
    },
    {
      "name": "Artificial intelligence",
      "score": 0.15413522720336914
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Control (management)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 147
}