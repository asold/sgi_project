{
  "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation",
  "url": "https://openalex.org/W3193300915",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2074658184",
      "name": "Yang JinYu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1922531866",
      "name": "Liu JingJing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983314566",
      "name": "Xu Ning",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381687061",
      "name": "Huang, Junzhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2766897166",
    "https://openalex.org/W2963240485",
    "https://openalex.org/W2627183927",
    "https://openalex.org/W2097482982",
    "https://openalex.org/W2962808524",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2937843571",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2904706552",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2149933564",
    "https://openalex.org/W1565327149",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W2950333415",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2945328857",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2593768305",
    "https://openalex.org/W92894758",
    "https://openalex.org/W2981413347",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2804452283",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W1722318740",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2593814746",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2964278684",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3034526587",
    "https://openalex.org/W2949383447",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3111507638",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2981630749",
    "https://openalex.org/W2164943005",
    "https://openalex.org/W3147183491",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W2786808285",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2962687275"
  ],
  "abstract": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the transferability of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior transferability over its CNNs-based counterparts with a large margin, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT's intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned transferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
  "full_text": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation\nJinyu Yang1, Jingjing Liu2, Ning Xu2, Junzhou Huang1\n1University Of Texas at Arlington\n2Kuaishou Techlology\nAbstract\nUnsupervised domain adaptation (UDA) aims to transfer the\nknowledge learnt from a labeled source domain to an unla-\nbeled target domain. Previous work is mainly built upon con-\nvolutional neural networks (CNNs) to learn domain-invariant\nrepresentations. With the recent exponential increase in apply-\ning Vision Transformer (ViT) to vision tasks, the capability of\nViT in adapting cross-domain knowledge, however, remains\nunexplored in the literature. To ﬁll this gap, this paper ﬁrst com-\nprehensively investigates the transferability of ViT on a variety\nof domain adaptation tasks. Surprisingly, ViT demonstrates\nsuperior transferability over its CNNs-based counterparts with\na large margin, while the performance can be further improved\nby incorporating adversarial adaptation. Notwithstanding, di-\nrectly using CNNs-based adaptation strategies fails to take the\nadvantage of ViT’s intrinsic merits (e.g., attention mechanism\nand sequential image representation) which play an important\nrole in knowledge transfer. To remedy this, we propose an\nuniﬁed framework, namely Transferable Vision Transformer\n(TVT) 1, to fully exploit the transferability of ViT for domain\nadaptation. Speciﬁcally, we delicately devise a novel and ef-\nfective unit, which we term Transferability Adaption Module\n(TAM). By injecting learned transferabilities into attention\nblocks, TAM compels ViT focus on both transferable and\ndiscriminative features. Besides, we leverage discriminative\nclustering to enhance feature diversity and separation which\nare undermined during adversarial domain alignment. To ver-\nify its versatility, we perform extensive studies of TVT on four\nbenchmarks and the experimental results demonstrate that\nTVT attains signiﬁcant improvements compared to existing\nstate-of-the-art UDA methods.\nIntroduction\nDeep neural networks (DNNs) demonstrate unprecedented\nachievements on various machine learning problems and ap-\nplications. However, such impressive performance heavily\nrelies on massive amounts of labeled data which requires\nconsiderable time and labor efforts to collect. Therefore, it is\ndesirable to train models that can leverage rich labeled data\nfrom a different but related domain and generalize well on\ntarget domains with no or limited labeled examples. Unfor-\ntunately, the canonical supervised-learning paradigm suffers\nfrom the domain shift issue that poses a major challenge in\nadapting models across domains. This motivates the research\non unsupervised domain adaptation (UDA) (Wang and Deng\n1https://github.com/uta-smile/TVT\n2018) which is a special scenario of transfer learning (Pan and\nYang 2009). The key idea of UDA is to project data points of\nthe labeled source domain and the unlabeled target domain\ninto a common feature space, such that the projected features\nare both discriminative (semantic meaningful) and domain-\ninvariant, in turn, generalize well to bridge the domain gap.\nTo achieve this goal, various methods have been proposed\nin the past decades, among which adversarial adaptation has\nbecome the dominant technique in this ﬁeld, which attempts\nto align cross-domain representations by minimizing an ad-\nversarial loss through a domain discriminator (Ganin et al.\n2016; Tzeng et al. 2017; Long et al. 2017a).\nRecently, Vision Transformer (ViT) (Dosovitskiy et al.\n2020) has received increasing attention in the vision commu-\nnity. Different from CNNs that act on local receptive ﬁelds of\nthe given image, ViT models long-range dependencies among\nvisual features across the entire image, through the global\nself-attention mechanism. Speciﬁcally in ViT, each image is\nsplit into a sequence of ﬁxed-size non-overlapping patches,\nwhich are then linearly embedded and concatenated with po-\nsition embeddings. To be consistent with NLP paradigm, a\nclass token is prepended to the patch tokens, serving as the\nrepresentation of the whole image. Then, those sequential em-\nbeddings are fed into a stack of transformers to learn desired\nvisual representations. Due to its advantages in global con-\ntext modeling, ViT has obtained excellent results on various\nvision tasks, such as image classiﬁcation (Dosovitskiy et al.\n2020), object detection (Carion et al. 2020; Wang et al. 2021),\nsegmentation (Zheng et al. 2020; Liu et al. 2021), and video\nunderstanding (Girdhar et al. 2019; Neimark et al. 2021).\nDespite that ViT is becoming increasingly popular, two\nimportant questions related to domain adaption remain unan-\nswered. First, how transferable is ViT across different do-\nmains, compared to its CNNs counterparts? As ViT is\nconvolution-free and lacks some inductive bias inherent to\nCNNs (e.g., locality and translation equivariance), it relies on\nlarge-scale pre-training to trump inductive bias. Such training\nprerequisite along with the learned global attentions may pro-\nvide ViT with outstanding capability in domain transferring,\nyet this hypothesis has not been investigated. The second\nquestion is, how can we properly improve ViT in adapting\ndifferent domains? One intuitive approach is to directly apply\nadversarial discriminator onto the class tokens to perform\nadversarial alignment, where the state of a class token rep-\nresents the entire image. However, cross-domain alignment\narXiv:2108.05988v2  [cs.CV]  26 Nov 2021\nof such global features assumes all regions or aspects of\nthe image have the equal transferability and discriminative\npotential, which is not always tenable. For instance, back-\nground regions can be easier aligned across domains, while\nforeground regions are more discriminative. In other words,\nsome discriminative features may lack transferability, and\nsome transferable features may not contribute much to the\ndownstream task (e.g., classiﬁcation). Therefore, in order to\nproperly enhance the transferability of ViT, it is critical to\nidentify ﬁne-grained features that are both transferable and\ndiscriminative.\nIn this paper we aim to present our answers to the two\naforementioned questions. Firstly, to ﬁll the blank of under-\nstanding ViT’s transferability, we ﬁrst conduct a compre-\nhensive study of vanilla ViT (Dosovitskiy et al. 2020) on\npublic UDA benchmarks. As expected, our experimental re-\nsults demonstrate that ViT is more transferable than its strong\nCNNs-based counterparts, which can be partially explained\nby the global context modeling and large-scale pre-training.\nBesides, we observe further improvements by applying an ad-\nversarial discriminator to the class tokens of ViT, which only\naligns global representations. However, such strategy suffers\nfrom the oversimpliﬁed assumption and ignores the inherent\nproperties of ViT that are beneﬁcial for domain adaptation:\ni) sequential patch tokens actually give us the free access\nto ﬁne-grained features; ii) the self-attention mechanism in\ntransformer naturally works as a discriminative probe. In the\nlight of this, we propose an uniﬁed UDA framework that\nmakes full use of ViT’s inherent merits. We name it Transfer-\nable Vision Transformer (TVT).\nThe key idea of our method is to retain both transferable\nand discriminative features which are essential in knowl-\nedge adaptation. To achieve this goal, we ﬁrst introduce the\nnovel Transferability Adaption Module (TAM) built upon a\nconventional transformer. TAM uses a patch-level domain\ndiscriminator to measure the transferabilities of patch tokens,\nand injects learned transferabilities into the multi-head self-\nattention block of a transformer. On one hand, the attention\nweights of patch tokens in the self-attention block are used to\ndetermine their semantic importance, i.e., the features with\nlarger attention are more discriminative yet without transfer-\nability guarantees. On the other hand, as patch tokens can\nbe regarded as ﬁne-grained representations of an image, the\nhigher transferability of a token means the local features are\nmore transferable across domains though not necessarily dis-\ncriminative. By simply replacing the last transformer of ViT\nwith a plug-and-play TAM, we could drive ViT to focus on\nboth transferable and discriminative features.\nSince our method performs adversarial adaptation that\nforces the learned features of two domains to be similar, one\nunderlying side-effect is that the discriminative information\nof target domain might be destroyed during feature alignment.\nTo address this problem, we design a Discriminative Clus-\ntering Module (DCM) inspired by the clustering assumption.\nThe motivation is to enforce the individual target prediction\nclose to one-hot encoding (well separated) and the global\ntarget prediction to be uniformly distributed (global diverse),\nsuch that the learnt target-domain representation could retain\nmaximum discriminative information about the input values.\nContributions of this paper are summarized as follows:\n• As far as we know, we are the ﬁrst investigating the ca-\npability of ViT in transferring knowledge on the domain\nadaptation task. We believe this work gives good insights\nto understand and explore ViT’s transferability while ap-\nplied to various vision tasks.\n• We propose TAM that delicately leverages the intrinsic\ncharacteristics of ViT, such that our method can capture\nboth transferable and discriminative features for domain\nadaptation. Moreover, we adopt discriminative cluster-\ning assumption to alleviate the discrimination destruction\nduring adversarial alignment.\n• Without any bells and whistles, our method set up a new\ncompetitive baseline cross several public UDA bench-\nmarks.\nRelated Work\nUnsupervised Domain Adaptation Transfer learning\naims to learn transferable knowledge that are generalizable\nacross different domains with different distributions (Pan and\nYang 2009; Ying et al. 2018). This is built upon the evidence\nthat feature representations in machine learning models, es-\npecially in deep neural networks, are transferable (Yosinski\net al. 2014). The main challenge of transfer learning is to\nreduce the domain shift or the discrepancy of the marginal\nprobability distributions across domains (Wang and Deng\n2018). In the past decades, various methods have been pro-\nposed to address one canonical transfer learning problem, i.e.,\nunsupervised domain adaptation (UDA), where no labels are\navailable for the target domain. For instance, DDC (Tzeng\net al. 2014) attempted to learn domain-invariant features by\nminimizing Maximum Mean Discrepancy (MMD) (Borg-\nwardt et al. 2006) between two domains. Long et al. further\nimproved DDC by embedding hidden representations of all\ntask-speciﬁc layers in a reproducing Hilbert space and used\na multiple kernel variant of MMD to measure the domain\ndistance (Long et al. 2015). Long et al. proposed to align\njoint distributions of multiple domain-speciﬁc layers across\ndomains through a joint maximum mean discrepancy metric\n(Long et al. 2017b). Another line of effort was inspired by\nthe success of adversarial learning (Goodfellow et al. 2014).\nBy introducing a domain discriminator and modeling the\ndomain adaption as a minimax problem (Ganin et al. 2016;\nTzeng et al. 2017; Long et al. 2017a), an encoder is trained\nto generate domain-invariant features, through deceiving a\ndiscriminator which tries to distinguish features of source\ndomain from that of target domain.\nIt is noteworthy that all of these methods completely or\npartially used CNNs as the fundamental block (LeCun et al.\n1998; Krizhevsky, Sutskever, and Hinton 2012; He et al.\n2016). By contrast, our method explores ViT (Dosovitskiy\net al. 2020) to tackle the UDA problem, as we believe ViT has\nbetter potential and capability in domain adaptation owning\nto some of its properties. Although previous UDA methods\n(e.g., adversarial learning) are able to improve vanilla ViT\nto some extent, they were not well designed for transformer-\nbased models, and thereby cannot leverage ViT’s inherent\ncharacteristic of providing attention information and ﬁne-\ngrained representations. However, Our method is delicately\ndesigned with the nature of ViT and could effectively lever-\nages the transferability and discrimination of each feature\nfor knowledge transfer, thus having better chance in fully\nexploiting the adaptation power of ViT.\nVision Transformer Transformers (Vaswani et al. 2017)\nwas ﬁrstly proposed in the NLP ﬁeld and demonstrate record-\nbreaking performance on various language tasks, e.g., text\nclassiﬁcation and machine translation (Devlin et al. 2018;\nBeltagy, Peters, and Cohan 2020; Zhou et al. 2020). Much of\nsuch impressive achievement is attributed to the power of cap-\nturing long-range dependencies through attention mechanism.\nSpurred by this, some recent studies attempted to integrate\nattention into CNNs to augment feature maps, aiming to pro-\nvide the capability in modeling heterogeneous interactions\n(Wang et al. 2018; Bello et al. 2019; Hu et al. 2018). Another\npioneering work of completely convolution-free architecture\nis Vision Transformer (ViT), which applied transformers on a\nsequence of ﬁxed-size non-overlapping image patches. Differ-\nent from CNNs that rely on image-speciﬁc inductive biases\n(e.g., locality and translation equivariance), ViT takes the\nbeneﬁts from large-scale pre-training data and global context\nmodeling. One such method (Dosovitskiy et al. 2020), known\nfor its simplicity and accuracy/compute trade-off, competes\nfavorably against CNNs on the classiﬁcation task and lays\nthe foundation for applying transformer to different vision\ntasks. ViT and its variants have proved their wide applicabil-\nity in object detection (Carion et al. 2020; Zhu et al. 2020;\nWang et al. 2021), segmentation (Zheng et al. 2020; Wang\net al. 2020), and video understanding (Girdhar et al. 2019;\nNeimark et al. 2021), etc.\nDespite the success of ViT on different vision tasks, to\nthe best of our knowledge, neither their transferability nor\nthe design of UDA methods with ViT have been previously\ndiscussed in the literature. To this end, we focus in this paper\non the investigation of ViT’s capability in knowledge trans-\nferring across different domains. We propose a novel UDA\nframework tailored for ViT by exploring its intrinsic merits\nand prove its superiority over existing methods.\nPreliminaries\nAdversarial Learning UDA We consider the image clas-\nsiﬁcation task in UDA, where a labeled source domain\nDs{(xs\ni ,ys\ni )}ns\ni=1 with ns examples and an unlabeled target\ndomain Dt{xt\nj}nt\nj=1 with nt examples are given. The goal\nof UDA is to learn features that are both discriminative and\ninvariant to the domain discrepancy, and in turn guarantee\naccurate prediction on the unlabeled target data. Here, a com-\nmon practice is to jointly performs feature learning, domain\nadaptation, and classiﬁer learning by optimizing the follow-\ning loss function:\nLclc(xs,ys) + αLdis(xs,xt) (1)\nwhere Lclc is supervised classiﬁcation loss, Ldis is a transfer\nloss with various possible implementations, and αis used\nto control the importance of Ldis. One of the most com-\nmonly used Ldis is the adversarial loss which encourages a\ndomain-invariant feature space through a domain discrimina-\ntor (Ganin et al. 2016).\nSelf-attention Mechanism The main building block of\nViT is Multi-head Self-Attention (MSA), which is used in\nthe transformer to capture long-range dependencies (Vaswani\net al. 2017). Speciﬁcally, MSA concatenates multiple scaled\ndot-product attention (short for SA) modules, where each SA\nmodule takes a set of queries (Q), keys (K), and values (V)\nas inputs. In order to learn dependencies between distinct\npositions, SA computes the dot products of the query with all\nkeys, and applies a softmax function to obtain the weights on\nthe values.\nSA(Q,K,V) = softmax(QKT\n√\nd\n)V (2)\nwhere dis the dimension of Q and K. With SA(Q,K,V),\nMSA is deﬁned as:\nMSA(Q,K,V) = Concat(head1,..., headk)WO\nwhere headi = SA(QWQ\ni ,KWK\ni ,VWV\ni )\n(3)\nwhere WQ\ni , WK\ni , WV\ni are projections of different heads,\nWO is another mapping function. Intuitively, using multi-\nple heads allows MSA to jointly attend to information from\ndifferent representation subspaces at different positions.\nMethodology\nIn this section, we ﬁrst investigate ViT’s ability in knowledge\ntransfer on various adaptation tasks. After that, we conduct\nthe early attempts to improve ViT’s transferability by incorpo-\nrating adversarial learning. Finally, we introduce our method\nnamed Transferable Vision Transformer (TVT), which con-\nsists two new adaptation modules to further improve ViT’s\ncapability for cross-domain adaptation..\nViT’s Transferability\nTo the best of our knowledge, the transferability of ViT has\nnot been studied in the literature before, although ViT and its\nvariants have shown great success in various vision task. To\nprobe into ViT’s capability of domain adaptation, we choose\nthe vanilla ViT (Dosovitskiy et al. 2020) as the backbone\nin all of our studies, owing to its simplicity and popular-\nity. We train vanilla ViT by labeled source data only and\nassess its transferability by the classiﬁcation accuracy on\ntarget data. As mentioned above, CNNs-based approaches\ndominate UDA research in the past decades and demonstrate\ngreat successes. Therefore, we compare vanilla ViT with\nCNNs-based architectures, including LeNet (LeCun et al.\n1998), AlexNet (Krizhevsky, Sutskever, and Hinton 2012),\nand ResNet (He et al. 2016). All experiments are performed\non well-established benchmarks with standard evaluation\nprotocols.\nTake the results on Ofﬁce-31 dataset for example. As\nshown in Table 2, Source Only ViT obtains impressing clas-\nsiﬁcation accuracy 89.45%, which is much better than its\nstrong CNN opponents AlexNet (70.1%) and ResNet (76.1%).\nSimilar phenomenon can be observed in other benchmark\nresults, where ViT competes favorably against, if not better\nthan, the other state-of-the-arts CNNs backbones, as shown\nin Table 1,3,4. Surprisingly, Source Only ViT even outper-\nforms strong CNNs-based UDA approaches without any bells\nand whistles. For instance, it achieves an average accuracy\n78.74% on Ofﬁce-Home dataset (Table 3), beating all CNN-\nbased UDA methods. Compared to SHOT (Liang, Hu, and\nFeng 2020) recognized as the best UDA model nowadays,\nSource Only ViT obtains 7% absolute accuracy boost, a big\nstep in pushing the frontier of UDA research. These evidences\njustify our hypothesis that ViT is more transferable, partially\nexplained by its large-scale pre-training and global context\nmodeling. However, as observed in Table 1, a large gap still\nexists between the Source Only and Target Only models\n(88.3% vs 99.22%), which indicates further improvement\nspace of ViT’s transferability.\nViT w/ Adversarial Adaptation: Baseline\nWe ﬁrst investigate how ViT beneﬁts from adversarial adapta-\ntion (Ganin et al. 2016), which is widely used in CNNs-based\nUDA methods. We follow the typical adversarial adaptation\nfashion that employs an encoder Gf for feature learning, a\nclassiﬁer Gc for classiﬁcation, and a domain discriminator\nDg for global feature alignment. Here, Gf is implemented as\nViT andDg is applied to output state of the class tokens of the\nsource and target images. To accomplish domain knowledge\nadaptation, Gf and Dg play a minimax game: Gf learns\ndomain-invariant features to deceive Dg, while Dg distin-\nguishes source-domain features from that of target-domain.\nThe objective can be formulated as:\nLclc(xs,ys) = 1\nns\n∑\nxi∈Ds\nLce(Gc(Gf (xs\ni )),ys\ni )\nLdis(xs,xt) = −1\nn\n∑\nxi∈D\nLce(Dg(Gf (x∗\ni )),yd\ni ),\n(4)\nwhere n= ns + nt, D= Ds\n⋃Dt, Lce is cross-entropy loss,\nthe superscript ∗can be either sor tto denote a source or a\ntarget domain, and yd denotes the domain label (i.e., yd = 1\nis source, yd = 0 is target).\nWe denote ViT with adversarial adaptation as our Baseline.\nAs shown in Table 1,2,3,4, Baseline shows 7.8%, 0.78%,\n1.56%, and 3.21% absolute accuracy improvements over\nvanilla ViT, respectively on the four benchmarks. Those re-\nsults reveal that global feature alignment with a domain dis-\ncriminator helps ViT’s transferability. However, compared\nwith the digit recognition task, Baseline achieves limited im-\nprovements on object detection which is more complicated\nand challenging. We boils down such observation to a conclu-\nsion that simply applying global adversarial alignment cannot\nexploit ViT’s full transferable power, since it fails to consider\ntwo key factors: (i) not all regions/features are equally trans-\nferable or discriminative. For effective knowledge transfer,\nit is essential to focus on both transferable and discrimina-\ntive features; (ii) ViT naturally provides ﬁne-grained features\ngiven its forward passing sequential tokens, and attention\nweights in transformer actually convey discriminative poten-\ntials of patch tokens. To address these challenges and fully\nleverage the merits of ViT, a new UDA framework named\nTransferable Vision Transformer (TVT) is further proposed.\nTransferable Vision Transformer (TVT)\nAn overview of TVT is shown in Figure 1, which contains\ntwo main modules: (i) a Transferability Adaptation Module\n(TAM) and (ii) a Discriminative Clustering Module (DCM).\nLayer Norm\nLayer Norm\nTransferable \nMSA\nMLP\nq K\n...\nTransformer Layer\nsoftmax\nFigure 1: An overview of the proposed TVT framework. As\nin ViT, both source and target images are split into ﬁxed-\nsize patches which are then linearly mapped and embedded\nwith positional information. The generated patches are fed\ninto a transformer encoder whose last layer is replaced by\nTransferability Adaptation Module (TAM). Feature learning,\nadversarial domain adaptation and classiﬁcation are accom-\nplished by ViT-akin backbone, two domain discriminators\n(on patch-level and global-level), Discriminative Clustering\nModule (DCM) and the MLP-based classiﬁer.\nThese two modules are highly interrelated and play a comple-\nmentary role in transferring knowledge for ViT-based archi-\ntectures. TAM encourages the output state of class token to\nfocus on both transferable and semantic meaningful features,\nand DCM enforces the aligned features of target-domain sam-\nples to be clustered with large margins. As a consequence,\nthe features learnt by TVT are discriminative in classiﬁca-\ntion and transferable across domains as well. We detail each\nmodule in what follows.\nTransferability Adaptation Module As shown in Fig-\nure 1, we introduce the Transferability Adaptation Module\n(TAM) that explicitly considers the intrinsic merits of ViT,\ni.e., attention mechanisms and sequential patch tokens. As\nthe patch tokens are regarded as local features of an image,\nthey are corresponded to different image regions or captures\ndifferent visual aspects as ﬁne-grained representations of an\nimage. Assuming patch tokens of different semantic impor-\ntance and transferabilities, TAM aims at assigning different\nweights to those tokens, to encourage the learned image rep-\nresentations, i.e., the output state of class token, to attend\nto patch tokens that are both transferable and discriminative.\nWhile the self-attention weights in ViT could be employed\nas discriminative weights, one major hurdle here is, the trans-\nferability of each patch token is not available. To bypass this\ndifﬁculty, we adopt a patch-level domain discriminator Dl\nthat matches cross-domain local features (Pei et al. 2018;\nWang et al. 2019) by optimizing:\nLpat(xs,xt) = − 1\nnR\n∑\nxi∈D\nR∑\nr=1\nLce(Dl(Gf (x∗\nir)),yd\nir),\n(5)\nwhere Ris number of patches, and Dl(fir) is the probability\nof this region belonging to the source domain. During ad-\nversarial learning, Dl tries to assign 1 for a source-domain\npatch and 0 for the target-domain ones, while Gf combats\nsuch circumstances. Conceptually, a patch that can easily\ndeceive Dl (i.g., Dl is around 0.5) is more transferable across\ndomains and should be given a higher transferability. We\ntherefore use tir = T(fir) = H(Dl(fir)) ∈[0,1] to mea-\nsure the transferability of rth token of ith image, where H(·)\nis the standard entropy function. An other explanation of the\ntransferability is: by assigning weights to different patches,\nit disentangles an image into common space representations\nand domain-speciﬁc representations, while the passing paths\nof domain-speciﬁc features are softly suppressed.\nWe then convert the conventional MSA into the transfer-\nable MSA (T-MSA) by transferability adaptation, i.e., in-\njecting the learned transferabilities into attention weights of\nthe class token. Our T-MSA is built upon the transferable\nself-attention (TSA) block that is formally deﬁned as:\nTSA(q,K,V) = softmax(qKT\n√\nd\n) ⊙[1; T(Kpatch)]V (6)\nwhere q is the query of the class token, Kpatch is the\nkey of the patch tokens, ⊙is Hadamard product, and [; ]\nis concatenation operation. Obviously, softmax(qKT\n√\nd ) and\n[1; T(Kpatch)] indicate the discrimination (semantic impor-\ntance) and the transferability of each patch token, respectively.\nTo jointly attend to the transferabilities of different represen-\ntation subspaces and of different locations, we thus deﬁne\nT-MSA as:\nT-MSA(q,K,V) = Concat(head1,..., headk)WO\nwhere headi = TSA(qWq\ni ,KWK\ni ,VWV\ni )\n(7)\nTaken them together, we get the TAM as follows:\nˆzl = T-MSA(LN(zl−1)) + zl−1\nzl = MLP(LN(ˆzl)) + ˆzl (8)\nWe only apply TAM to the last transformer layer where patch\nfeatures are spatially non-local and of higher semantic mean-\nings. By this means, TAM focuses on ﬁne-grained features\nthat are transferable across domains and are discriminative\nfor classiﬁcation. So we have l = L, where Lis the total\nnumber of transformer layers in ViT.\nDiscriminative Clustering Module Towards the challeng-\ning problem of learning a probabilistic discriminative classi-\nﬁer with unlabeled target data, it is desirable to minimize the\nexpected classiﬁcation error on the target domain. However,\ncross-domain feature alignment through TAM by forcing the\ntwo domains to be similar may destroy the discriminative\ninformation of the learned representation, if no semantic con-\nstrains of the target domain is introduced. As shown in Fig-\nure 2, although the target feature is indistinguishable from the\nsource feature, it is distributed in a mess which limits its dis-\ncriminative power. To address this limitation, we are inspired\nby the assumptions that: (i) pt = softmax(Gc(Gf (xt))) are\nexpected to retain as much information about xt as possi-\nble (Bridle, Heading, and MacKay 1992); and (ii) decision\nboundary should not cross high density regions, but instead\nlie in low density regions, which is also known as cluster\nassumption (Chapelle and Zien 2005). Fortunately, these two\nassumptions can be met by maximizing mutual information\nbetween the empirical distribution on the target inputs and\nthe induced target label distribution (Gomes, Krause, and\nPerona 2010; Shi and Sha 2012; Hu et al. 2017), which can\nbe formally deﬁned as:\nI(pt; xt) = H( ¯pt) − 1\nnt\nnt∑\nj=1\nH(pt\nj)\n= −\nK∑\nk=1\n¯pt\nklog( ¯pt\nk) + 1\nnt\nnt∑\nj=1\nK∑\nk=1\npt\njklog(pt\njk)\n(9)\nwhere pt\nj = softmax(Gc(Gf (xt\nj))), ¯pt = Ext [pt],\nand K is the number of classes. Note that maximizing\n−1\nnt\n∑nt\nj=1 H(pt\nj) enforces the target predictions close to\none-hot encoding, therefore the cluster assumption is guaran-\nteed. To ensure the global diversity, we also maximizeH( ¯pt)\nto avoid that every target data is assigned to the same class.\nWith I(pt; xt), our model is encouraged to learn tightly clus-\ntered target features with uniform distribution, such that the\ndiscriminative information in the target domain are retained.\nTo summarize, the objective function of TVT is:\nLclc(xs,ys) + αLdis(xs,xt) + βLpat(xs,xt) −γI(pt; xt)\n(10)\nwhere α, β, and γare hyper-parameters.\nExperiments\nTo verify the effectiveness of our model, we conduct compre-\nhensive studies on commonly used benchmarks and present\nexperimental comparisons against state-of-the-art UDA meth-\nods as shown below.\nDigits is an UDA benchmark on digit classiﬁcation. We\nfollow the same setting in previous work to perform adap-\ntations on MNIST (LeCun et al. 1998), USPS, and Street\nView House Numbers (SVHN) (Netzer et al. 2011). For each\nsource-target domain pair, we train our model using the train-\ning sets of each domain, and perform evaluations on the\nstandard test set of the target domain.\nOfﬁce-31 (Saenko et al. 2010) contains 4,652 images of\n31 categories, which were collected from three domains:\nAmazon (A), DSLR (D), and Webcam (W). The Amazon\n(A) image were downloaded from amazon.zom, while the\nDSLR (D), and Webcam (W) were photoed under the ofﬁce\nenvironment by web and digital SLR camera, respectively.\nAlgorithm S→M U →M M →U Avg\nSource Only\nLeNet\n67.1 69.6 82.2 73.0\nRevGrad 73.9 73.0 77.1 74.7\nADDA 76.0 90.1 89.4 85.2\nSHOT-IM 89.6 96.8 91.9 92.8\nCyCADA 90.4 96.5 95.6 94.2\nCDAN 89.2 98.0 95.6 94.3\nMCD 96.2 94.1 94.2 94.8\nTarget Only 99.4 99.4 98.0 98.9\nSource Only\nViT\n88.58 88.23 73.09 88.30\nBaseline 92.70 98.60 97.01 96.10\nTVT 99.01 99.38 98.21 98.87\nTarget Only 99.70 99.70 98.26 99.22\nTable 1: Performance comparison on Digits dataset.\nOfﬁce-Home (Venkateswara et al. 2017) consists of im-\nages from four different domains: Artistic images (Ar), Clip\nArt (Cl), Product im- ages (Pr), and Real-World images (Rw).\nA total of 65 categories are covered within each domain.\nVisDA-2017 (Peng et al. 2017) is a synthesis-to-real object\nrecognition task used for the 2018 VisDA challenge. It covers\n12 categories. The source domain contains 152,397 synthetic\n2D renderings generated from different angles and under\ndifferent lighting conditions, while the target domain contains\n55,388 real-world images.\nBaseline Methods We compare with RevGrad (Ganin and\nLempitsky 2015; Ganin et al. 2016), ADDA (Tzeng et al.\n2017), SHOT (Liang, Hu, and Feng 2020), CDAN (Long\net al. 2017a), CyCADA (Hoffman et al. 2018), MCD (Saito\net al. 2018), DDC (Tzeng et al. 2014), DAN (Long et al.\n2015), JAN (Long et al. 2017b), PFAN (Chen et al. 2019),\nTADA (Wang et al. 2019), ALDA (Chen et al. 2020), TAT\n(Liu et al. 2019), and DTA (Lee et al. 2019), under the close-\nset setting where the source and the target domain share\nthe same label space. We use the results in their original\npapers for fair comparison. For each type of backbone, we\nreport its lower bound performance, denoted as Source Only,\nmeaning the models are trained with source data only. For\ndigit recognition, we also show the Target Only results as the\nhigh-end performance, which is obtained by both training and\ntesting on the labeled target data. Baseline denotes vanilla\nViT with adversarial adaptation (Ganin et al. 2016).\nImplementation Details The ViT-Base with 16×16 input\npatch size (or ViT-B/16) (Dosovitskiy et al. 2020) pre-trained\non ImageNet (Deng et al. 2009) is used as our backbone. The\ntransformer encoder of ViT-B/16 contains 12 transformer\nlayers in total. We train all ViT-based models using mini-\nbatch Stochastic Gradient Descent (SGD) optimizer with\nthe momentum of 0.9. We initialized the learning rate as 0\nand linearly increase it to lr= 0.03 after 500 training steps.\nWe then decrease it by the cosine decay strategy. The only\nexception is that we set lr= 0.003 for D→A and W→A in\nOfﬁce-31 dataset.\nResults of Digit Recognition For the digit recogni-\ntion task, we perform evaluations on SVHN →MNISt,\nAlgorithm A→W D→W W→D A→D D→A W→A Avg\nSource Only\nAlexNet\n61.6 95.4 99.0 63.8 51.1 49.8 70.1\nDDC 61.8 95.0 98.5 64.4 52.1 52.2 70.6\nDAN 68.5 96.0 99.0 67.0 54.0 53.1 72.9\nRevGrad 73.0 96.4 99.2 72.3 53.4 51.2 74.3\nJAN 75.2 96.6 99.6 72.8 57.5 56.3 76.3\nCDAN 78.3 97.2 100.0 76.3 57.3 57.3 77.7\nPFAN 83.0 99.0 99.9 76.3 63.3 60.8 80.4\nSource Only\nResNet\n68.4 96.7 99.3 68.9 62.5 60.7 76.1\nDDC 75.6 96.0 98.2 76.5 62.2 61.5 78.3\nDAN 80.5 97.1 99.6 78.6 63.6 62.8 80.4\nRevGrad 82.0 96.9 99.1 79.7 68.2 67.4 82.2\nJAN 86.0 96.7 99.7 85.1 69.2 70.7 84.6\nCDAN 94.1 98.6 100.0 92.9 71.0 69.3 87.7\nTADA 94.3 98.7 99.8 91.6 72.9 73.0 88.4\nTAT 92.5 99.3 100.0 93.2 73.1 72.1 88.4\nSHOT 90.1 98.4 99.9 94.0 74.7 74.3 88.6\nALDA 95.6 97.7 100.0 94.0 72.2 72.5 88.7\nSource Only\nViT\n89.18 98.87 100.0 88.76 80.09 79.77 89.45\nBaseline 91.57 98.99 100.0 90.56 80.16 80.12 90.23\nTVT 96.35 99.37 100.0 96.39 84.91 86.05 93.85\nTable 2: Performance comparison on Ofﬁce-31 dataset.\nUSPS→MNIST, and MNIST →USPS, following the stan-\ndard evaluation protocol of UDA. Shown in Table 1, TVT\nobtains the best mean accuracy for each task and outperforms\nprior work in terms of the average classiﬁcation accuracy.\nTVT also performs better than Baseline (+2.7%) due to the\ncontribution of the proposed TAM and DCM. In particular,\nTVT achieves comparable results to Target Only model, indi-\ncating that the domain shift problem is well alleviated.\nResults of Object Recognition For object recognition\ntask, Ofﬁce-31, Ofﬁce-Home, and VisDA-2017 are used in\nevaluation. As shown in Table 2 3, 4, TVT sets up new bench-\nmark results for all the three datasets. On the medium-sized\nOfﬁce-Home dataset (Table 3), we achieve the signiﬁcant\nimprovement over the best prior UDA method (83.56% vs\n71.8%). Results on the large-scale VisDA-2017 dataset (Ta-\nble 4) show that we not only achieve a higher average accu-\nracy, but also compete favorably against ALDA and SHOT\nthat rely on pseudo labels. We believe training with pseudo\nlabel would give TVT extra accuracy gain, while it is out of\nour current scope. Note that DTA also enforces the cluster\nassumption to learn discriminative features, but it fails to\nencourage the global diversity which may leads to a degener-\nate solution where every point is assigned to the same class.\nBesides, TVT surpasses both Source Only and Baseline, re-\nvealing its effectiveness in transferring domain knowledge by\n(i) capturing both transferable and discriminative ﬁne-grained\nfeatures and (ii) retaining discriminative information while\nsearching for the domain-invariant representations. This is\nalso evidenced by the t-SNE visualization of learned features\nas showcased in Figure 2. Obviously, TAM can effectively\nalign source and target domain features by exploiting the\nlocal feature transferability. However, the target feature is\nnot well-separated due to that target labels in training are\nabsent and the discriminative information are destroyed by\nadversarial alignment. Fortunately, this problem is alleviated\nby DCM by assuming that datapoints should be classiﬁed\nAlgorithm Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg\nSource Only\nAlexNet\n26.4 32.6 41.3 22.1 41.7 42.1 20.5 20.3 51.1 31.0 27.9 54.9 34.3\nDAN 31.7 43.2 55.1 33.8 48.6 50.8 30.1 35.1 57.7 44.6 39.3 63.7 44.5\nRevGrad 36.4 45.2 54.7 35.2 51.8 55.1 31.6 39.7 59.3 45.7 46.4 65.9 47.3\nJAN 35.5 46.1 57.7 36.4 53.3 54.5 33.4 40.3 60.1 45.9 47.4 67.9 48.2\nSource Only\nResNet\n34.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 53.9 41.2 59.9 46.1\nDAN 43.6 57.0 67.9 45.8 56.5 60.4 44.0 43.6 67.7 63.1 51.5 74.3 56.3\nRevGrad 45.6 59.3 70.1 47.0 58.5 60.9 46.1 43.7 68.5 63.2 51.8 76.8 57.6\nJAN 45.9 61.2 68.9 50.4 59.7 61.0 45.8 43.4 70.3 63.9 52.4 76.8 58.3\nCDAN 50.7 70.6 76.0 57.6 70.0 70.0 57.4 50.9 77.3 70.9 56.7 81.6 65.8\nTAT 51.6 69.5 75.4 59.4 69.5 68.6 59.5 50.5 76.8 70.9 56.6 81.6 65.8\nALDA 53.7 70.1 76.4 60.2 72.6 71.5 56.8 51.9 77.1 70.2 56.3 82.1 66.6\nTADA 53.1 72.3 77.2 59.1 71.2 72.1 59.7 53.1 78.4 72.4 60.0 82.9 67.6\nSHOT 57.1 78.1 81.5 68.0 78.2 78.1 67.4 54.9 82.2 73.3 58.8 84.3 71.8\nSource Only\nViT\n66.16 84.28 86.64 77.92 83.28 84.32 75.98 62.73 88.66 80.10 66.19 88.65 78.74\nBaseline 71.94 80.67 86.67 79.93 80.38 83.52 76.89 70.93 88.27 83.02 72.91 88.44 80.30\nTVT 74.89 86.82 89.47 82.78 87.95 88.27 79.81 71.94 90.13 85.46 74.62 90.56 83.56\nTable 3: Performance comparison on Ofﬁce-Home dataset.\nAlgorithm plane bcycl bus car house knife mcycl person plant sktbrd train truck Avg\nSource Only\nResNet\n55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5 52.4\nRevGrad 81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8 57.4\nMCD 87.0 60.9 83.7 64.0 88.9 79.6 84.7 76.9 88.6 40.3 83.0 25.8 71.9\nALDA 93.8 74.1 82.4 69.4 90.6 87.2 89.0 67.6 93.4 76.1 87.7 22.2 77.8\nDTA 93.7 82.2 85.6 83.8 93.0 81.0 90.7 82.1 95.1 78.1 86.4 32.1 81.5\nSHOT 94.3 88.5 80.1 57.3 93.1 94.9 80.7 80.3 91.5 89.1 86.3 58.2 82.9\nSource Only\nViT\n98.16 72.98 82.52 62.00 97.34 63.52 96.46 29.80 68.74 86.72 96.74 23.65 73.22\nBaseline 94.60 81.55 81.81 69.85 93.54 69.93 88.60 50.45 86.79 88.47 91.45 20.10 76.43\nTVT 92.92 85.58 77.51 60.48 93.60 98.17 89.35 76.40 93.56 92.02 91.69 55.73 83.92\nTable 4: Performance comparison on VisDA-2017 dataset.\nwith large margin, as illustrated in Figure 2 (D).\nAblation Study To learn the individual contribution of\nTAM and DCM in improving the knowledge transferability\nof ViT, we conduct the ablation study in Table 5. Compared\nto Source Only, TAM consistently improves the classiﬁcation\naccuracy with average 4.82% boost, indicating the signiﬁ-\ncance of capturing both transferable and discriminative fea-\ntures. The performance is further improved by incorporating\nDCM, justifying the necessary of retaining the discriminative\ninformation of the learned representation. It is noteworthy\nthat DCM brings the largest improvement on the large-scale\nsynthetic-to-real VisDA-2017 dataset. We suspect that the\nlarge domain gap in VisDA-2017 (synthetic 2D rendering\nto natural image) is the leading reason, since simply align-\ning two domains with large domain shift results in a mess\ndistributed feature space. This challenge, however, can be\nlargely addressed by DCM that enables retaining discrimina-\ntive information based on a cluster assumption.\nAttention Visualization We visualize the attention map\nof the class token in TAM to verify that our model can\nattend to local features that are both transferable and dis-\ncriminative. Without loss of generality, we randomly sample\ntarget-domain images in VisDA-2017 dataset for comparison.\nAs shown in Figure 3, our method captures more accurate\nregions than Source Only and Baseline. For instance, to rec-\nMethods Digits Ofﬁce-31 Ofﬁce-Home VisDA-2017 Avg\nSource Only 88.30 89.45 78.74 73.22 82.43\n+TAM 97.20 91.21 81.30 79.30 87.25\n+DCM 98.87 93.85 83.56 83.92 90.05\nTable 5: Ablation study of each module.\nognize the person in the top-left image, Source Only mainly\nfocus on women’s shoulder which is discriminative yet not\nhighly transferable. Moving beyond the shoulder region, the\nbaseline also attends to faces and hands that can general-\nize well across domains. Our method, instead, ignores the\nshoulder and only highlight those regions that are important\nfor classiﬁcation and transferable. Certainly, by leveraging\nthe intrinsic attention mechanism and ﬁne-grained features\ncaptured by sequential patches, our method promotes the\ncapability of ViT in transferring domain knowledge.\nConclusion\nIn this paper, we perform the ﬁrst-of-its-kind investigation\nof ViT’s transferability in UDA task and observe that ViT\nare more transferable than CNNs counterparts. To further\nimprove the power of ViT in transferring domain knowledge,\nwe propose TVT by explicitly considering the intrinsic mer-\nits of transformer architecture. Speciﬁcally, TVT captures\n(A) Source Only (B) Baseline\n(C) TAM (D) TVT\nFigure 2: t-SNE visualization of VisDA-2017 dataset, where\nred and blue points indicate the source (synthetic rendering)\nand the target (real images) domain, respectively.\n(A) Image (C) Baseline(B) Source Only (D) TVT\nFigure 3: Attention map visualization of person, truck, and\nbicycle in VisDA-2017 dataset. The hotter the color, the\nhigher the attention.\nboth transferable and discriminative features in the given\nimage, and retains discriminative information of the learnt\ndomain-invariant representations. Experimental results on\nwidely used benchmarks show that TVT outperforms prior\nUDA methods by a large margin.\nReferences\nBello, I.; Zoph, B.; Vaswani, A.; Shlens, J.; and Le, Q. V .\n2019. Attention augmented convolutional networks. In Pro-\nceedings of the IEEE/CVF International Conference on Com-\nputer Vision, 3286–3295.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150.\nBorgwardt, K. M.; Gretton, A.; Rasch, M. J.; Kriegel, H.-P.;\nSch¨olkopf, B.; and Smola, A. J. 2006. Integrating struc-\ntured biological data by kernel maximum mean discrepancy.\nBioinformatics, 22(14): e49–e57.\nBridle, J. S.; Heading, A. J.; and MacKay, D. J. 1992. Un-\nsupervised classiﬁers, mutual information and’phantom tar-\ngets’.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nChapelle, O.; and Zien, A. 2005. Semi-supervised classiﬁca-\ntion by low density separation. In International workshop on\nartiﬁcial intelligence and statistics, 57–64. PMLR.\nChen, C.; Xie, W.; Huang, W.; Rong, Y .; Ding, X.; Huang,\nY .; Xu, T.; and Huang, J. 2019. Progressive feature align-\nment for unsupervised domain adaptation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 627–636.\nChen, M.; Zhao, S.; Liu, H.; and Cai, D. 2020. Adversarial-\nlearned loss for domain adaptation. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 34, 3521–\n3528.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, 248–255. Ieee.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGanin, Y .; and Lempitsky, V . 2015. Unsupervised domain\nadaptation by backpropagation. In International conference\non machine learning, 1180–1189. PMLR.\nGanin, Y .; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle,\nH.; Laviolette, F.; Marchand, M.; and Lempitsky, V . 2016.\nDomain-adversarial training of neural networks. The journal\nof machine learning research, 17(1): 2096–2030.\nGirdhar, R.; Carreira, J.; Doersch, C.; and Zisserman, A.\n2019. Video action transformer network. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 244–253.\nGomes, R.; Krause, A.; and Perona, P. 2010. Discriminative\nclustering by regularized information maximization.\nGoodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial networks. arXiv preprint\narXiv:1406.2661.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 770–\n778.\nHoffman, J.; Tzeng, E.; Park, T.; Zhu, J.-Y .; Isola, P.; Saenko,\nK.; Efros, A.; and Darrell, T. 2018. Cycada: Cycle-consistent\nadversarial domain adaptation. In International conference\non machine learning, 1989–1998. PMLR.\nHu, H.; Gu, J.; Zhang, Z.; Dai, J.; and Wei, Y . 2018. Relation\nnetworks for object detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n3588–3597.\nHu, W.; Miyato, T.; Tokui, S.; Matsumoto, E.; and Sugiyama,\nM. 2017. Learning discrete representations via information\nmaximizing self-augmented training. In International con-\nference on machine learning, 1558–1567. PMLR.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Ima-\ngenet classiﬁcation with deep convolutional neural networks.\nAdvances in neural information processing systems, 25: 1097–\n1105.\nLeCun, Y .; Bottou, L.; Bengio, Y .; and Haffner, P. 1998.\nGradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11): 2278–2324.\nLee, S.; Kim, D.; Kim, N.; and Jeong, S.-G. 2019. Drop\nto adapt: Learning discriminative features for unsupervised\ndomain adaptation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 91–100.\nLiang, J.; Hu, D.; and Feng, J. 2020. Do we really need\nto access the source data? source hypothesis transfer for\nunsupervised domain adaptation. InInternational Conference\non Machine Learning, 6028–6039. PMLR.\nLiu, H.; Long, M.; Wang, J.; and Jordan, M. 2019. Trans-\nferable adversarial training: A general approach to adapting\ndeep classiﬁers. In International Conference on Machine\nLearning, 4013–4022. PMLR.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. arXiv preprint\narXiv:2103.14030.\nLong, M.; Cao, Y .; Wang, J.; and Jordan, M. 2015. Learning\ntransferable features with deep adaptation networks. In Inter-\nnational conference on machine learning, 97–105. PMLR.\nLong, M.; Cao, Z.; Wang, J.; and Jordan, M. I. 2017a.\nConditional adversarial domain adaptation. arXiv preprint\narXiv:1705.10667.\nLong, M.; Zhu, H.; Wang, J.; and Jordan, M. I. 2017b. Deep\ntransfer learning with joint adaptation networks. In Interna-\ntional conference on machine learning, 2208–2217. PMLR.\nNeimark, D.; Bar, O.; Zohar, M.; and Asselmann, D.\n2021. Video transformer network. arXiv preprint\narXiv:2102.00719.\nNetzer, Y .; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and\nNg, A. Y . 2011. Reading digits in natural images with unsu-\npervised feature learning.\nPan, S. J.; and Yang, Q. 2009. A survey on transfer learn-\ning. IEEE Transactions on knowledge and data engineering,\n22(10): 1345–1359.\nPei, Z.; Cao, Z.; Long, M.; and Wang, J. 2018. Multi-\nadversarial domain adaptation. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 32.\nPeng, X.; Usman, B.; Kaushik, N.; Hoffman, J.; Wang, D.;\nand Saenko, K. 2017. Visda: The visual domain adaptation\nchallenge. arXiv preprint arXiv:1710.06924.\nSaenko, K.; Kulis, B.; Fritz, M.; and Darrell, T. 2010. Adapt-\ning visual category models to new domains. In European\nconference on computer vision, 213–226. Springer.\nSaito, K.; Watanabe, K.; Ushiku, Y .; and Harada, T. 2018.\nMaximum classiﬁer discrepancy for unsupervised domain\nadaptation. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3723–3732.\nShi, Y .; and Sha, F. 2012. Information-theoretical learning of\ndiscriminative clusters for unsupervised domain adaptation.\narXiv preprint arXiv:1206.6438.\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\nAdversarial discriminative domain adaptation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 7167–7176.\nTzeng, E.; Hoffman, J.; Zhang, N.; Saenko, K.; and Darrell,\nT. 2014. Deep domain confusion: maximizing for domain\ninvariance (2014). Preprint. arXiv, 1412.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nis all you need. arXiv preprint arXiv:1706.03762.\nVenkateswara, H.; Eusebio, J.; Chakraborty, S.; and Pan-\nchanathan, S. 2017. Deep hashing network for unsupervised\ndomain adaptation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 5018–5027.\nWang, M.; and Deng, W. 2018. Deep visual domain adapta-\ntion: A survey. Neurocomputing, 312: 135–153.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.; Lu,\nT.; Luo, P.; and Shao, L. 2021. Pyramid vision transformer: A\nversatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 7794–7803.\nWang, X.; Li, L.; Ye, W.; Long, M.; and Wang, J. 2019.\nTransferable attention for domain adaptation. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 33,\n5345–5352.\nWang, Y .; Xu, Z.; Wang, X.; Shen, C.; Cheng, B.; Shen, H.;\nand Xia, H. 2020. End-to-End Video Instance Segmentation\nwith Transformers. arXiv preprint arXiv:2011.14503.\nYing, W.; Zhang, Y .; Huang, J.; and Yang, Q. 2018. Transfer\nlearning via learning to transfer. In International Conference\non Machine Learning, 5085–5094. PMLR.\nYosinski, J.; Clune, J.; Bengio, Y .; and Lipson, H. 2014. How\ntransferable are features in deep neural networks? arXiv\npreprint arXiv:1411.1792.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu, Y .;\nFeng, J.; Xiang, T.; Torr, P. H.; et al. 2020. Rethinking Seman-\ntic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers. arXiv preprint arXiv:2012.15840.\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\nand Zhang, W. 2020. Informer: Beyond Efﬁcient Transformer\nfor Long Sequence Time-Series Forecasting. arXiv preprint\narXiv:2012.07436.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7736164331436157
    },
    {
      "name": "Discriminative model",
      "score": 0.7449789643287659
    },
    {
      "name": "Feature learning",
      "score": 0.6524648070335388
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6295677423477173
    },
    {
      "name": "Exploit",
      "score": 0.5882018208503723
    },
    {
      "name": "Machine learning",
      "score": 0.564049482345581
    },
    {
      "name": "Domain adaptation",
      "score": 0.5298495292663574
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5288770198822021
    },
    {
      "name": "Transformer",
      "score": 0.49079760909080505
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.46306103467941284
    },
    {
      "name": "Categorization",
      "score": 0.4484449028968811
    },
    {
      "name": "Transferability",
      "score": 0.44513779878616333
    },
    {
      "name": "Transfer of learning",
      "score": 0.41151589155197144
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39717555046081543
    },
    {
      "name": "Engineering",
      "score": 0.10494542121887207
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ],
  "institutions": []
}