{
  "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
  "url": "https://openalex.org/W2526471240",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2149004533",
      "name": "Yishu Miao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A297118547",
      "name": "Phil Blunsom",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1850742715",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2964036520",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W854541894",
    "https://openalex.org/W2963773425",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W1909320841",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W1484210532",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2951527505",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2952264928",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2962944953",
    "https://openalex.org/W2251654079",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2964165364",
    "https://openalex.org/W2467240462",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2962965405",
    "https://openalex.org/W2122262818",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2173681125",
    "https://openalex.org/W2949416428",
    "https://openalex.org/W2108501770",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2327562811"
  ],
  "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution.We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences.In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary.In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data.Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.",
  "full_text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 319–328,\nAustin, Texas, November 1-5, 2016.c⃝2016 Association for Computational Linguistics\nLanguage as a Latent Variable:\nDiscrete Generative Models for Sentence Compression\nYishu Miao1, Phil Blunsom1,2\n1University of Oxford, 2Google Deepmind\n{yishu.miao, phil.blunsom}@cs.ox.ac.uk\nAbstract\nIn this work we explore deep generative mod-\nels of text in which the latent representation\nof a document is itself drawn from a discrete\nlanguage model distribution. We formulate a\nvariational auto-encoder for inference in this\nmodel and apply it to the task of compressing\nsentences. In this application the generative\nmodel ﬁrst draws a latent summary sentence\nfrom a background language model, and then\nsubsequently draws the observed sentence con-\nditioned on this latent summary. In our em-\npirical evaluation we show that generative for-\nmulations of both abstractive and extractive\ncompression yield state-of-the-art results when\ntrained on a large amount of supervised data.\nFurther, we explore semi-supervised compres-\nsion scenarios where we show that it is possi-\nble to achieve performance competitive with\npreviously proposed supervised models while\ntraining on a fraction of the supervised data.\n1 Introduction\nThe recurrent sequence-to-sequence paradigm for\nnatural language generation (Kalchbrenner and Blun-\nsom, 2013; Sutskever et al., 2014) has achieved re-\nmarkable recent success and is now the approach\nof choice for applications such as machine transla-\ntion (Bahdanau et al., 2015), caption generation (Xu\net al., 2015) and speech recognition (Chorowski et\nal., 2015). While these models have developed so-\nphisticated conditioning mechanisms, e.g. attention,\nfundamentally they are discriminative models trained\nonly to approximate the conditional output distribu-\ntion of strings. In this paper we explore modelling the\njoint distribution of string pairs using a deep genera-\ntive model and employing a discrete variational auto-\nencoder (V AE) for inference (Kingma and Welling,\n2014; Rezende et al., 2014; Mnih and Gregor, 2014).\nWe evaluate our generative approach on the task of\nsentence compression. This approach provides both\nalternative supervised objective functions and the\nopportunity to perform semi-supervised learning by\nexploiting the V AEs ability to marginalise the latent\ncompressed text for unlabelled data.\nAuto-encoders (Rumelhart et al., 1985) are a typi-\ncal neural network architecture for learning compact\ndata representations, with the general aim of perform-\ning dimensionality reduction on embeddings (Hinton\nand Salakhutdinov, 2006). In this paper, rather than\nseeking to embed inputs as points in a vector space,\nwe describe them with explicit natural language sen-\ntences. This approach is a natural ﬁt for summarisa-\ntion tasks such as sentence compression. According\nto this, we propose a generative auto-encoding sen-\ntence compression(ASC) model, where we intro-\nduce a latent language model to provide the variable-\nlength compact summary. The objective is to perform\nBayesian inference for the posterior distribution of\nsummaries conditioned on the observed utterances.\nHence, in the framework of V AE, we construct an in-\nference network as the variational approximation of\nthe posterior, which generates compression samples\nto optimise the variational lower bound.\nThe most common family of variational auto-\nencoders relies on the reparameterisation trick, which\nis not applicable for our discrete latent language\nmodel. Instead, we employ the REINFORCE al-\ngorithm (Mnih et al., 2014; Mnih and Gregor, 2014)\n319\ns0\ns1 s2\ns1\ns3\ns2\ns4\ns3\nh1\nd\nh2\nd\nh3\nd\nh4\nd\nDecoder\ns1 s2 s3 s4\nh1\ne\nc1 c2 c3\nc1 c2c0\nh1\nc\nh2\ne\nh3\ne\nh4\ne\nh2\nc\nh3\nc\nEncoder Compressor\nh0\nc\nCompression (Pointer  Networks)\n3α2α1α\n^h2\nc ^h3\nc^h1\nc\nReconstruction (Soft Attention)\nFigure 1: Auto-encoding Sentence Compression Model\nto mitigate the problem of high variance during\nsampling-based variational inference. Nevertheless,\nwhen directly applying the RNN encoder-decoder to\nmodel the variational distribution it is very difﬁcult to\ngenerate reasonable compression samples in the early\nstages of training, since each hidden state of the se-\nquence would have |V|possible words to be sampled\nfrom. To combat this we employ pointer networks\n(Vinyals et al., 2015) to construct the variational dis-\ntribution. This biases the latent space to sequences\ncomposed of words only appearing in the source\nsentence (i.e. the size of softmax output for each\nstate becomes the length of current source sentence),\nwhich amounts to applying an extractive compression\nmodel for the variational approximation.\nIn order to further boost the performance on sen-\ntence compression, we employ a supervised forced-\nattention sentence compression model (FSC)\ntrained on labelled data to teach the ASC model to\ngenerate compression sentences. The FSC model\nshares the pointer network of the ASC model and\ncombines a softmax output layer over the whole vo-\ncabulary. Therefore, while training on the sentence-\ncompression pairs, it is able to balance copying a\nword from the source sentence with generating it\nfrom the background distribution. More importantly,\nby jointly training on the labelled and unlabelled\ndatasets, this shared pointer network enables the\nmodel to work in a semi-supervised scenario. In\nthis case, the FSC teaches the ASC to generate rea-\nsonable samples, while the pointer network trained\non a large unlabelled data set helps the FSC model to\nperform better abstractive summarisation.\nIn Section 6, we evaluate the proposed model by\njointly training the generative (ASC) and discrimina-\ntive (FSC) models on the standard Gigaword sentence\ncompression task with varying amounts of labelled\nand unlabelled data. The results demonstrate that by\nintroducing a latent language variable we are able to\nmatch the previous benchmakers with small amount\nof the supervised data. When we employ our mixed\ndiscriminative and generative objective with all of the\nsupervised data the model signiﬁcantly outperforms\nall previously published results.\n2 Auto-Encoding Sentence Compression\nIn this section, we introduce the auto-encoding sen-\ntence compression model (Figure 1) 1 in the frame-\nwork of variational auto-encoders. The ASC model\nconsists of four recurrent neural networks – an en-\ncoder, a compressor, a decoder and a language model.\nLet sbe the source sentence, andcbe the compres-\nsion sentence. The compression model (encoder-\ncompressor) is the inference network qφ(c|s) that\ntakes source sentences s as inputs and generates\nextractive compressions c. The reconstruction\n1The language model, layer connections and decoder soft\nattentions are omitted in Figure 1 for clarity.\n320\nmodel (compressor-decoder) is the generative net-\nwork pθ(s|c) that reconstructs source sentences s\nbased on the latent compressions c. Hence, the for-\nward pass starts from the encoder to the compressor\nand ends at the decoder. As the prior distribution, a\nlanguage model p(c) is pre-trained to regularise the\nlatent compressions so that the samples drawn from\nthe compression model are likely to be reasonable\nnatural language sentences.\n2.1 Compression\nFor the compression model (encoder-compressor),\nqφ(c|s), we employ a pointer network consisting of a\nbidirectional LSTM encoder that processes the source\nsentences, and an LSTM compressor that generates\ncompressed sentences by attending to the encoded\nsource words.\nLet si be the words in the source sentences, he\ni be\nthe corresponding state outputs of the encoder.he\ni are\nthe concatenated hidden states from each direction:\nhe\ni = f−→enc(⃗he\ni−1,si)||f←−enc( ⃗he\ni+1,si) (1)\nFurther, let cj be the words in the compressed sen-\ntences, hc\nj be the state outputs of the compressor. We\nconstruct the predictive distribution by attending to\nthe words in the source sentences:\nhc\nj =fcom(hc\nj−1,cj−1) (2)\nuj(i) =wT\n3 tanh(W1hc\nj+W2he\ni) (3)\nqφ(cj|c1:j−1,s)= softmax(uj) (4)\nwhere c0 is the start symbol for each compressed\nsentence and hc\n0 is initialised by the source sentence\nvector of he\n|s|. In this case, all the words cj sampled\nfrom qφ(cj|c1:j−1,s) are the subset of the words\nappeared in the source sentence (i.e. cj ∈s).\n2.2 Reconstruction\nFor the reconstruction model (compressor-decoder)\npθ(s|c), we apply a soft attention sequence-to-\nsequence model to generate the source sentence s\nbased on the compression samples c∼qφ(c|s).\nLet sk be the words in the reconstructed sentences\nand hd\nk be the corresponding state outputs of the\ndecoder:\nhd\nk = fdec(hd\nk−1,sk−1) (5)\nIn this model, we directly use the recurrent cell of\nthe compressor to encode the compression samples2:\nˆh\nc\nj =fcom(ˆh\nc\nj−1,cj) (6)\nwhere the state outputs ˆh\nc\nj corresponding to the word\ninputs cj are different from the outputs hc\nj in the\ncompression model, since we block the information\nfrom the source sentences. We also introduce a start\nsymbol s0 for the reconstructed sentence and hd\n0\nis initialised by the last state output ˆh\nc\n|c|. The soft\nattention model is deﬁned as:\nvk(j) =wT\n6 tanh(W4hd\nk + W5ˆh\nc\nj) (7)\nγk(j) = softmax(vk(j)) (8)\ndk =\n∑|c|\nj\nγk(j)ˆh\nc\nj(vk(j)) (9)\nWe then construct the predictive probability distribu-\ntion over reconstructed words using a softmax:\npθ(sk|s1:k−1,c) = softmax(W7dk) (10)\n2.3 Inference\nIn the ASC model there are two sets of parameters, φ\nand θ, that need to be updated during inference. Due\nto the non-differentiability of the model, the repa-\nrameterisation trick of the V AE is not applicable in\nthis case. Thus, we use the REINFORCE algorithm\n(Mnih et al., 2014; Mnih and Gregor, 2014) to reduce\nthe variance of the gradient estimator.\nThe variational lower bound of the ASC model is:\nL=E qφ(c|s)[log pθ(s|c)] −DKL[qφ(c|s)||p(c)]\n⩽log\n∫qφ(c|s)\nqφ(c|s)pθ(s|c)p(c)dc= logp(s) (11)\nTherefore, by optimising the lower bound (Eq. 11),\nthe model balances the selection of keywords for the\nsummaries and the efﬁcacy of the composed com-\npressions, corresponding to the reconstruction error\nand KL divergence respectively.\nIn practise, the pre-trained language model prior\np(c) prefers short sentences for compressions. As\none of the drawbacks of V AEs, the KL divergence\nterm in the lower bound pushes every sample drawn\n2The recurrent parameters of the compressor are not updated\nby the gradients from the reconstruction model.\n321\ns1 s2 s3 s4\nh1\ne\nc1 c2 c3\nc1 c2c0\nh1\nc\nh2\ne\nh3\ne\nh4\ne\nh2\nc\nh3\nc\nEncoder Compresser\nh0\nc\nα β1 2 3α α1 β β2 3\nCompression (Combined Pointer Networks)\nSelected from V\nFigure 2: Forced Attention Sentence Compression Model\nfrom the variational distribution towards the prior.\nThus acting to regularise the posterior, but also to\nrestrict the learning of the encoder. If the estimator\nkeeps sampling short compressions during inference,\nthe LSTM decoder would gradually rely on the con-\ntexts from the decoded words instead of the informa-\ntion provided by the compressions, which does not\nyield the best performance on sentence compression.\nHere, we introduce a co-efﬁcient λ to scale the\nlearning signal of the KL divergence:\nL=E qφ(c|s)[log pθ(s|c)]−λDKL[qφ(c|s)||p(c)] (12)\nAlthough we are not optimising the exact variational\nlower bound, the ultimate goal of learning an effec-\ntive compression model is mostly up to the recon-\nstruction error. In Section 6, we empirically apply\nλ= 0.1 for all the experiments on ASC model. In-\nterestingly, λ controls the compression rate of the\nsentences which can be a good point to be explored\nin future work.\nDuring the inference, we have different strategies\nfor updating the parameters of φand θ. For the pa-\nrameters θin the reconstruction model, we directly\nupdate them by the gradients:\n∂L\n∂θ = E qφ(c|s)[∂log pθ(s|c)\n∂θ ]\n≈ 1\nM\n∑\nm\n∂log pθ(s|c(m))\n∂θ (13)\nwhere we draw M samples c(m) ∼qφ(c|s) indepen-\ndently for computing the stochastic gradients.\nFor the parameters φin the compression model,\nwe ﬁrstly deﬁne the learning signal,\nl(s,c) = logpθ(s|c) −λ(log qφ(c|s) −log p(c)).\nThen, we update the parameters φby:\n∂L\n∂φ = E qφ(c|s)[l(s,c)∂log qφ(c|s)\n∂φ ]\n≈ 1\nM\n∑\nm\n[l(s,c(m))∂log qφ(c(m)|s)\n∂φ ] (14)\nHowever, this gradient estimator has a big variance\nbecause the learning signal l(s,c(m)) relies on the\nsamples from qφ(c|s). Therefore, following the RE-\nINFORCE algorithm, we introduce two baselines\nb and b(s), the centred learning signal and input-\ndependent baseline respectively, to help reduce the\nvariance.\nHere, we build an MLP to implement the input-\ndependent baseline b(s). During training, we learn\nthe two baselines by minimising the expectation:\nE qφ(c|s)[(l(s,c) −b−b(s))2]. (15)\nHence, the gradients w.r.t. φare derived as,\n∂L\n∂φ≈1\nM\n∑\nm\n(l(s,c(m))−b−b(s))∂log qφ(c(m)|s)\n∂φ\n(16)\nwhich is basically a likelihood-ratio estimator.\n3 Forced-attention Sentence Compression\nIn neural variational inference, the effectiveness of\ntraining largely depends on the quality of the in-\nference network gradient estimator. Although we\nintroduce a biased estimator by using pointer net-\nworks, it is still very difﬁcult for the compression\nmodel to generate reasonable natural language sen-\ntences at the early stage of learning, which results in\n322\nhigh-variance for the gradient estimator. Here, we\nintroduce our supervised forced-attention sentence\ncompression (FSC) model to teach the compression\nmodel to generate coherent compressed sentences.\nNeither directly replicating the pointer network\nof ASC model, nor using a typical sequence-to-\nsequence model, the FSC model employs a force-\nattention strategy (Figure 2) that encourages the com-\npressor to select words appearing in the source sen-\ntence but keeps the original full output vocabulary\nV. The force-attention strategy is basically a com-\nbined pointer network that chooses whether to select\na word from the source sentence sor to predict a\nword from V at each recurrent state. Hence, the\ncombined pointer network learns to copy the source\nwords while predicting the word sequences of com-\npressions. By sharing the pointer networks between\nthe ASC and FSC model, the biased estimator obtains\nfurther positive biases by training on a small set of\nlabelled source-compression pairs.\nHere, the FSC model makes use of the compres-\nsion model (Eq. 1 to 4) in the ASC model,\nαj = softmax(uj), (17)\nwhere αj(i), i∈(1,..., |s|) denotes the probability\nof selecting si as the prediction for cj.\nOn the basis of the pointer network, we further\nintroduce the probability of predicting cj that is se-\nlected from the full vocabulary,\nβj = softmax(Whc\nj), (18)\nwhere βj(w),w ∈(1,..., |V|) denotes the probabil-\nity of selecting the wth from V as the prediction for\ncj. To combine these two probabilities in the RNN,\nwe deﬁne a selection factor tfor each state output,\nwhich computes the semantic similarities between\nthe current state and the attention vector,\nηj =\n∑|s|\ni\nαj(i)he\ni (19)\ntj = σ(ηT\nj Mhc\nj). (20)\nHence, the probability distribution over compressed\nwords is deﬁned as,\np(cj|c1:j−1,s)=\n{tjαj(i) + (1−tj)βj(cj), cj=si\n(1 −tj)βj(cj), cj̸∈s\n(21)\nEssentially, the FSC model is the extended compres-\nsion model of ASC by incorporating the pointer net-\nwork with a softmax output layer over the full vocab-\nulary. So we employ φto denote the parameters of\nthe FSC model pφ(c|s), which covers the parameters\nof the variational distribution qφ(c|s).\n4 Semi-supervised Training\nAs the auto-encoding sentence compression (ASC)\nmodel grants the ability to make use of an unla-\nbelled dataset, we explore a semi-supervised train-\ning framework for the ASC and FSC models. In\nthis scenario we have a labelled dataset that contains\nsource-compression parallel sentences, (s,c) ∈L ,\nand an unlabelled dataset that contains only source\nsentences s∈U . The FSC model is trained on L so\nthat we are able to learn the compression model by\nmaximising the log-probability,\nF =\n∑\n(c,s)∈L\nlog pφ(c|s). (22)\nWhile the ASC model is trained on U , where we\nmaximise the modiﬁed variational lower bound,\nL=\n∑\ns∈U\n(E qφ(c|s)[log pθ(s|c)]−λDKL[qφ(c|s)||p(c)]).\n(23)\nThe joint objective function of the semi-supervised\nlearning is,\nJ=\n∑\ns∈U\n(E qφ(c|s)[log pθ(s|c)]−λDKL[qφ(c|s)||p(c)])\n+\n∑\n(c,s)∈L\nlog pφ(c|s). (24)\nHence, the pointer network is trained on both un-\nlabelled data, U , and labelled data, L , by a mixed\ncriterion of REINFORCE and cross-entropy.\n5 Related Work\nAs one of the typical sequence-to-sequence tasks,\nsentence-level summarisation has been explored by a\nseries of discriminative encoder-decoder neural mod-\nels. Filippova et al. (2015) carries out extractive\nsummarisation via deletion with LSTMs, while Rush\net al. (2015) applies a convolutional encoder and an\n323\nattentional feed-forward decoder to generate abstrac-\ntive summarises, which provides the benchmark for\nthe Gigaword dataset. Nallapati et al. (2016) fur-\nther improves the performance by exploring multi-\nple variants of RNN encoder-decoder models. The\nrecent works Gulcehre et al. (2016), Nallapati et al.\n(2016) and Gu et al. (2016) also apply the similar idea\nof combining pointer networks and softmax output.\nHowever, different from all these discriminative mod-\nels above, we explore generative models for sentence\ncompression. Instead of training the discriminative\nmodel on a big labelled dataset, our original intuition\nof introducing a combined pointer networks is to\nbridge the unsupervised generative model (ASC) and\nsupervised model (FSC) so that we could utilise a\nlarge additional dataset, either labelled or unlabelled,\nto boost the compression performance. Dai and Le\n(2015) also explored semi-supervised sequence learn-\ning, but in a pure deterministic model focused on\nlearning better vector representations.\nRecently variational auto-encoders have been ap-\nplied in a variety of ﬁelds as deep generative mod-\nels. In computer vision Kingma and Welling (2014),\nRezende et al. (2014), and Gregor et al. (2015) have\ndemonstrated strong performance on the task of im-\nage generation and Eslami et al. (2016) proposed\nvariable-sized variational auto-encoders to identify\nmultiple objects in images. While in natural language\nprocessing, there are variants of V AEs on modelling\ndocuments (Miao et al., 2016), sentences (Bowman\net al., 2015) and discovery of relations (Marcheg-\ngiani and Titov, 2016). Apart from the typical initi-\nations of V AEs, there are also a series of works that\nemploys generative models for supervised learning\ntasks. For instance, Ba et al. (2015) learns visual\nattention for multiple objects by optimising a varia-\ntional lower bound, Kingma et al. (2014) implements\na semi-supervised framework for image classiﬁcation\nand Miao et al. (2016) applies a conditional varia-\ntional approximation in the task of factoid question\nanswering. Dyer et al. (2016) proposes a generative\nmodel that explicitly extracts syntactic relationships\namong words and phrases which further supports the\nargument that generative models can be a statistically\nefﬁcient method for learning neural networks from\nsmall data.\n6 Experiments\n6.1 Dataset & Setup\nWe evaluate the proposed models on the standard Gi-\ngaword3 sentence compression dataset. This dataset\nwas generated by pairing the headline of each article\nwith its ﬁrst sentence to create a source-compression\npair. Rush et al. (2015) provided scripts 4 to ﬁlter\nout outliers, resulting in roughly 3.8M training pairs,\na 400K validation set, and a 400K test set. In the\nfollowing experiments all models are trained on the\ntraining set with different data sizes5 and tested on a\n2K subset, which is identical to the test set used by\nRush et al. (2015) and Nallapati et al. (2016). We\ndecode the sentences by k= 5Beam search and test\nwith full-length Rouge score.\nFor the ASC and FSC models, we use 256 for the\ndimension of both hidden units and lookup tables.\nIn the ASC model, we apply a 3-layer bidirectional\nRNN with skip connections as the encoder, a 3-layer\nRNN pointer network with skip connections as the\ncompressor, and a 1-layer vanilla RNN with soft at-\ntention as the decoder. The language model prior is\ntrained on the article sentences of the full training\nset using a 3-layer vanilla RNN with 0.5 dropout. To\nlower the computational cost, we apply different vo-\ncabulary sizes for encoder and compressor (119,506\nand 68,897) which corresponds to the settings of\nRush et al. (2015). Speciﬁcally, the vocabulary of\nthe decoder is ﬁltered by taking the most frequent\n10,000 words from the vocabulary of the encoder,\nwhere the rest of the words are tagged as ‘<unk>’.\nIn further consideration of efﬁciency, we use only\none sample for the gradient estimator. We optimise\nthe model by Adam (Kingma and Ba, 2015) with a\n0.0002 learning rate and 64 sentences per batch. The\nmodel converges in 5 epochs. Except for the pre-\ntrained language model, we do not use dropout or\nembedding initialisation for ASC and FSC models.\n6.2 Extractive Summarisation\nThe ﬁrst set of experiments evaluate the models on\nextractive summarisation. Here, we denote the joint\n3https://catalog.ldc.upenn.edu/LDC2012T21\n4https://github.com/facebook/NAMAS\n5The hyperparameters where tuned on the validation set to\nmaximise the perplexity of the summaries rather than the recon-\nstructed source sentences.\n324\nModel Training Data Recall Precision F-1\nLabelled Unlabelled R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nFSC 500K - 30.817 10.861 28.263 22.357 7.998 20.520 23.415 8.156 21.468\nASC+FSC1 500K 500K 29.117 10.643 26.811 28.558 10.575 26.344 26.987 9.741 24.874\nASC+FSC2 500K 3.8M 28.236 10.359 26.218 30.112 11.131 27.896 27.453 9.902 25.452\nFSC 1M - 30.889 11.645 28.257 27.169 10.266 24.916 26.984 10.028 24.711\nASC+FSC1 1M 1M 30.490 11.443 28.097 28.109 10.799 25.943 27.258 10.189 25.148\nASC+FSC2 1M 3.8M 29.034 10.780 26.801 31.037 11.521 28.658 28.336 10.313 26.145\nFSC 3.8M - 30.112 12.436 27.889 34.135 13.813 31.704 30.225 12.258 28.035\nASC+FSC1 3.8M 3.8M 29.946 12.558 27.805 35.538 14.699 32.972 30.568 12.553 28.366\nTable 1: Extractive Summarisation Performance. (1) The extractive summaries of these models are decoded\nby the pointer network (i.e the shared component of the ASC and FSC models). (2) R-1, R-2 and R-L\nrepresent the Rouge-1, Rouge-2 and Rouge-L score respectively.\nmodels by ASC+FSC1 and ASC+FSC2 where ASC\nis trained on unlabelled data and FSC is trained on\nlabelled data. The ASC+FSC1 model employs equiv-\nalent sized labelled and unlabelled datasets, where\nthe article sentences of the unlabelled data are the\nsame article sentences in the labelled data, so there\nis no additional unlabelled data applied in this case.\nThe ASC+FSC2 model employs the full unlabelled\ndataset in addition to the existing labelled dataset,\nwhich is the true semi-supervised setting.\nTable 1 presents the test Rouge score on extractive\ncompression. We can see that the ASC+FSC1 model\nachieves signiﬁcant improvements on F-1 scores\nwhen compared to the supervised FSC model only\ntrained on labelled data. Moreover, ﬁxing the labelled\ndata size, the ASC+FSC2 model achieves better per-\nformance by using additional unlabelled data than the\nASC+FSC1 model, which means the semi-supervised\nlearning works in this scenario. Interestingly, learn-\ning on the unlabelled data largely increases the preci-\nsions (though the recalls do not beneﬁt from it) which\nleads to signiﬁcant improvements on the F-1 Rouge\nscores. And surprisingly, the extractive ASC+FSC1\nmodel trained on full labelled data outperforms the\nabstractive NABS (Rush et al., 2015) baseline model\n(in Table 4).\n6.3 Abstractive Summarisation\nThe second set of experiments evaluate performance\non abstractive summarisation (Table 2). Consistently,\nwe see that adding the generative objective to the\ndiscriminative model (ASC+FSC1) results in a sig-\nniﬁcant boost on all the Rouge scores, while em-\nploying extra unlabelled data increase performance\nfurther (ASC+FSC2). This validates the effectiveness\nof transferring the knowledge learned on unlabelled\ndata to the supervised abstractive summarisation.\nIn Figure 3, we present the validation perplexity\nto compare the abilities of the three models to learn\nthe compression languages. The ASC+FSC 1(red)\nemploys the same dataset for unlabelled and labelled\ntraining, while the ASC+FSC2(black) employs the\nfull unlabelled dataset. Here, the joint ASC+FSC 1\nmodel obtains better perplexities than the single dis-\ncriminative FSC model, but there is not much dif-\nference between ASC+FSC1 and ASC+FSC2 when\nthe size of the labelled dataset grows. From the per-\nspective of language modelling, the generative ASC\nmodel indeed helps the discriminative model learn to\ngenerate good summary sentences. Table 3 displays\nthe validation perplexities of the benchmark models,\nwhere the joint ASC+FSC1 model trained on the full\nlabelled and unlabelled datasets performs the best on\nmodelling compression languages.\nTable 4 compares the test Rouge score on ab-\nstractive summarisation. Encouragingly, the semi-\nsupervised model ASC+FSC2 outperforms the base-\nline model NABS when trained on 500K supervised\npairs, which is only about an eighth of the super-\nvised data. In Nallapati et al. (2016), the authors\nexploit the full limits of discriminative RNN encoder-\ndecoder models by incorporating a sampled soft-\nmax, expanded vocabulary, additional lexical fea-\ntures, and combined pointer networks6, which yields\nthe best performance listed in Table 4. However,\nwhen all the data is employed with the mixed ob-\n6The idea of the combined pointer networks is similar to the\nFSC model, but the implementations are slightly different.\n325\nModel Training Data Recall Precision F-1\nLabelled Unlabelled R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nFSC 500K - 27.147 10.039 25.197 33.781 13.019 31.288 29.074 10.842 26.955\nASC+FSC1 500K 500K 27.067 10.717 25.239 33.893 13.678 31.585 29.027 11.461 27.072\nASC+FSC2 500K 3.8M 27.662 11.102 25.703 35.756 14.537 33.212 30.140 12.051 27.99\nFSC 1M - 28.521 11.308 26.478 33.132 13.422 30.741 29.580 11.807 27.439\nASC+FSC1 1M 1M 28.333 11.814 26.367 35.860 15.243 33.306 30.569 12.743 28.431\nASC+FSC2 1M 3.8M 29.017 12.007 27.067 36.128 14.988 33.626 31.089 12.785 28.967\nFSC 3.8M - 31.148 13.553 28.954 36.917 16.127 34.405 32.327 14.000 30.087\nASC+FSC1 3.8M 3.8M 32.385 15.155 30.246 39.224 18.382 36.662 34.156 15.935 31.915\nTable 2: Abstractive Summarisation Performance. The abstractive summaries of these models are decoded by\nthe combined pointer network (i.e. the shared pointer network together with the softmax output layer over the\nfull vocabulary).\nModel Labelled Data Perplexity\nBag-of-Word (BoW) 3.8M 43.6\nConvolutional (TDNN) 3.8M 35.9\nAttention-Based (NABS) 3.8M 27.1\n(Rush et al., 2015)\nForced-Attention (FSC) 3.8M 18.6\nAuto-encoding (ASC+FSC1) 3.8M 16.6\nTable 3: Comparison on validation perplexity. BoW,\nTDNN and NABS are the baseline neural compres-\nsion models with different encoders in Rush et al.\n(2015)\nModel Labelled Data R-1 R-2 R-L\n(Rush et al., 2015) 3.8M 29.78 11.89 26.97\n(Nallapati et al., 2016) 3.8M 33.17 16.02 30.98\nASC + FSC2 500K 30.14 12.05 27.99\nASC + FSC2 1M 31.09 12.79 28.97\nASC + FSC1 3.8M 34.17 15.94 31.92\nTable 4: Comparison on test Rouge scores\njective ASC+FSC1 model, the result is signiﬁcantly\nbetter than this previous state-of-the-art. As the semi-\nsupervised ASC+FSC2 model can be trained on un-\nlimited unlabelled data, there is still signiﬁcant space\nleft for further performance improvements.\nTable 5 presents the examples of the compression\nsentences decoded by the joint model ASC+FSC 1\nand the FSC model trained on the full dataset.\n7 Discussion\nFrom the perspective of generative models, a sig-\nniﬁcant contribution of our work is a process for\nreducing variance for discrete sampling-based vari-\national inference. The ﬁrst step is to introduce two\nbaselines in the control variates method due to the\nfact that the reparameterisation trick is not applica-\n0 500K 1M 2M 4M\nLabelled Data size\n20\n40\n60\n80\n100Perplexity\n100\n49\n35.4\n27.4\n18.6\n87.7\n43\n32\n25.2 16.6\n83.3\n42.5\n33.6\n25.4\n16.6\nFSC\nASC+FSC1\nASC+FSC2\nFigure 3: Perplexity on validation dataset.\nble for discrete latent variables. However it is the\nsecond step of using a pointer network as the biased\nestimator that makes the key contribution. This re-\nsults in a much smaller state space, bounded by the\nlength of the source sentence (mostly between 20\nand 50 tokens), compared to the full vocabulary. The\nﬁnal step is to apply the FSC model to transfer the\nknowledge learned from the supervised data to the\npointer network. This further reduces the sampling\nvariance by acting as a sort of bootstrap or constraint\non the unsupervised latent space which could encode\nalmost anything but which thus becomes biased to-\nwards matching the supervised distribution. By using\nthese variance reduction methods, the ASC model is\nable to carry out effective variational inference for the\nlatent language model so that it learns to summarise\nthe sentences from the large unlabelled training data.\nIn a different vein, according to the reinforce-\nment learning interpretation of sequence level train-\ning (Ranzato et al., 2016), the compression model\nof the ASC model acts as an agent which iteratively\ngenerates words (takes actions) to compose the com-\n326\npression sentence and the reconstruction model acts\nas the reward function evaluating the quality of the\ncompressed sentence which is provided as a reward\nsignal. Ranzato et al. (2016) presents a thorough\nempirical evaluation on three different NLP tasks by\nusing additional sequence-level reward (BLEU and\nRouge-2) to train the models. In the context of this\npaper, we apply a variational lower bound (mixed re-\nconstruction error and KL divergence regularisation)\ninstead of the explicit Rouge score. Thus the ASC\nmodel is granted the ability to explore unlimited unla-\nbelled data resources. In addition we introduce a su-\npervised FSC model to teach the compression model\nto generate stable sequences instead of starting with\na random policy. In this case, the pointer network\nthat bridges the supervised and unsupervised model\nis trained by a mixed criterion of REINFORCE and\ncross-entropy in an incremental learning framework.\nEventually, according to the experimental results, the\njoint ASC and FSC model is able to learn a robust\ncompression model by exploring both labelled and\nunlabelled data, which outperforms the other sin-\ngle discriminative compression models that are only\ntrained by cross-entropy reward signal.\n8 Conclusion\nIn this paper we have introduced a generative model\nfor jointly modelling pairs of sequences and evalu-\nated its efﬁcacy on the task of sentence compression.\nThe variational auto-encoding framework provided\nan effective inference algorithm for this approach\nand also allowed us to explore combinations of dis-\ncriminative (FSC) and generative (ASC) compression\nmodels. The evaluation results show that supervised\ntraining of the combination of these models improves\nupon the state-of-the-art performance for the Giga-\nword compression dataset. When we train the su-\npervised FSC model on a small amount of labelled\ndata and the unsupervised ASC model on a large\nset of unlabelled data the combined model is able to\noutperform previously reported benchmarks trained\non a great deal more supervised data. These results\ndemonstrate that we are able to model language as a\ndiscrete latent variable in a variational auto-encoding\nframework and that the resultant generative model is\nable to effectively exploit both supervised and unsu-\npervised data in sequence-to-sequence tasks.\nsrc the sri lankan government on wednesday announced the closure of\ngovernment schools with immediate effect as a military campaign\nagainst tamil separatists escalated in the north of the country .\nref sri lanka closes schools as war escalates\nasca sri lanka closes government schools\nasce sri lankan government closure schools escalated\nfsca sri lankan government closure with tamil rebels closure\nsrc factory orders for manufactured goods rose #.# percent in septem-\nber , the commerce department said here thursday .\nref us september factory orders up #.# percent\nasca us factory orders up #.# percent in september\nasce factory orders rose #.# percent in september\nfsca factory orders #.# percent in september\nsrc hong kong signed a breakthrough air services agreement with the\nunited states on friday that will allow us airlines to carry freight to\nasian destinations via the territory .\nref hong kong us sign breakthrough aviation pact\nasca us hong kong sign air services agreement\nasce hong kong signed air services agreement with united states\nfsca hong kong signed air services pact with united states\nsrc a swedish un soldier in bosnia was shot and killed by a stray bul-\nlet on tuesday in an incident authorities are calling an accident ,\nmilitary ofﬁcials in stockholm said tuesday .\nref swedish un soldier in bosnia killed by stray bullet\nasca swedish un soldier killed in bosnia\nasce swedish un soldier shot and killed\nfsca swedish soldier shot and killed in bosnia\nsrc tea scores on the fourth day of the second test between australia\nand pakistan here monday .\nref australia vs pakistan tea scorecard\nasca australia v pakistan tea scores\nasce australia tea scores\nfsca tea scores on #th day of #nd test\nsrc india won the toss and chose to bat on the opening day in the\nopening test against west indies at the antigua recreation ground\non friday .\nref india win toss and elect to bat in ﬁrst test\nasca india win toss and bat against west indies\nasce india won toss on opening day against west indies\nfsca india chose to bat on opening day against west indies\nsrc a powerful bomb exploded outside a navy base near the sri lankan\ncapital colombo tuesday , seriously wounding at least one person ,\nmilitary ofﬁcials said .\nref bomb attack outside srilanka navy base\nasca bomb explodes outside sri lanka navy base\nasce bomb outside sri lankan navy base wounding one\nfsca bomb exploded outside sri lankan navy base\nsrc press freedom in algeria remains at risk despite the release on\nwednesday of prominent newspaper editor mohamed <unk> after\na two-year prison sentence , human rights organizations said .\nref algerian press freedom at risk despite editor ’s release <unk>\npicture\nasca algeria press freedom remains at risk\nasce algeria press freedom remains at risk\nfsca press freedom in algeria at risk\nTable 5: Examples of the compression sentences.\nsrc and ref are the source and reference sentences\nprovided in the test set. asca and asce are the abstrac-\ntive and extractive compression sentences decoded\nby the joint model ASC+FSC1, and fsca denotes the\nabstractive compression obtained by the FSC model.\n327\nReferences\n[Ba et al.2015] Jimmy Ba, V olodymyr Mnih, and Koray\nKavukcuoglu. 2015. Multiple object recognition with\nvisual attention. In Proceedings of ICLR.\n[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. 2015. Neural machine trans-\nlation by jointly learning to align and translate. In\nProceedings of ICLR.\n[Bowman et al.2015] Samuel R Bowman, Luke Vilnis,\nOriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and\nSamy Bengio. 2015. Generating sentences from a\ncontinuous space. arXiv preprint arXiv:1511.06349.\n[Chorowski et al.2015] Jan K Chorowski, Dzmitry Bah-\ndanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua\nBengio. 2015. Attention-based models for speech\nrecognition. In Proceedings of NIPS, pages 577–585.\n[Dai and Le2015] Andrew M Dai and Quoc V Le. 2015.\nSemi-supervised sequence learning. In Proceedings of\nNIPS, pages 3061–3069.\n[Dyer et al.2016] Chris Dyer, Adhiguna Kuncoro, Miguel\nBallesteros, and Noah A Smith. 2016. Recurrent neural\nnetwork grammars. In Proceedings of NAACL.\n[Eslami et al.2016] SM Eslami, Nicolas Heess, Theophane\nWeber, Yuval Tassa, Koray Kavukcuoglu, and Geof-\nfrey E Hinton. 2016. Attend, infer, repeat: Fast scene\nunderstanding with generative models. arXiv preprint\narXiv:1603.08575.\n[Filippova et al.2015] Katja Filippova, Enrique Alfonseca,\nCarlos A Colmenares, Lukasz Kaiser, and Oriol Vinyals.\n2015. Sentence compression by deletion with lstms. In\nProceedings of EMNLP, pages 360–368.\n[Gregor et al.2015] Karol Gregor, Ivo Danihelka, Alex\nGraves, and Daan Wierstra. 2015. Draw: A recurrent\nneural network for image generation. In Proceedings\nof ICML.\n[Gu et al.2016] Jiatao Gu, Zhengdong Lu, Hang Li, and\nVictor OK Li. 2016. Incorporating copying mecha-\nnism in sequence-to-sequence learning. arXiv preprint\narXiv:1603.06393.\n[Gulcehre et al.2016] Caglar Gulcehre, Sungjin Ahn,\nRamesh Nallapati, Bowen Zhou, and Yoshua Bengio.\n2016. Pointing the unknown words. arXiv preprint\narXiv:1603.08148.\n[Hinton and Salakhutdinov2006] Geoffrey E Hinton and\nRuslan R Salakhutdinov. 2006. Reducing the di-\nmensionality of data with neural networks. Science,\n313(5786):504–507.\n[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and\nPhil Blunsom. 2013. Recurrent continuous translation\nmodels. In Proceedings of EMNLP.\n[Kingma and Ba2015] Diederik P. Kingma and Jimmy Ba.\n2015. Adam: A method for stochastic optimization. In\nProceedings of ICLR.\n[Kingma and Welling2014] Diederik P Kingma and Max\nWelling. 2014. Auto-encoding variational bayes. In\nProceedings of ICLR.\n[Kingma et al.2014] Diederik P Kingma, Shakir Mo-\nhamed, Danilo Jimenez Rezende, and Max Welling.\n2014. Semi-supervised learning with deep generative\nmodels. In Proceedings of NIPS.\n[Marcheggiani and Titov2016] Diego Marcheggiani and\nIvan Titov. 2016. Discrete-state variational autoen-\ncoders for joint discovery and factorization of relations.\nTransactions of the Association for Computational Lin-\nguistics, 4.\n[Miao et al.2016] Yishu Miao, Lei Yu, and Phil Blunsom.\n2016. Neural variational inference for text processing.\nIn Proceedings of ICML.\n[Mnih and Gregor2014] Andriy Mnih and Karol Gregor.\n2014. Neural variational inference and learning in be-\nlief networks. In Proceedings of ICML.\n[Mnih et al.2014] V olodymyr Mnih, Nicolas Heess, and\nAlex Graves. 2014. Recurrent models of visual atten-\ntion. In Proceedings of NIPS.\n[Nallapati et al.2016] Ramesh Nallapati, Bowen Zhou,\nÇa glar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nrnns and beyond. arXiv preprint arXiv:1602.06023.\n[Ranzato et al.2016] Marc’Aurelio Ranzato, Sumit\nChopra, Michael Auli, and Wojciech Zaremba. 2016.\nSequence level training with recurrent neural networks.\n[Rezende et al.2014] Danilo J Rezende, Shakir Mohamed,\nand Daan Wierstra. 2014. Stochastic backpropagation\nand approximate inference in deep generative models.\nIn Proceedings of ICML.\n[Rumelhart et al.1985] David E Rumelhart, Geoffrey E\nHinton, and Ronald J Williams. 1985. Learning in-\nternal representations by error propagation. Technical\nreport, DTIC Document.\n[Rush et al.2015] Alexander M Rush, Sumit Chopra, and\nJason Weston. 2015. A neural attention model for\nabstractive sentence summarization. In Proceedings of\nEMNLP.\n[Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. 2014. Sequence to sequence learning with\nneural networks. In Proceedings of NIPS.\n[Vinyals et al.2015] Oriol Vinyals, Meire Fortunato, and\nNavdeep Jaitly. 2015. Pointer networks. In Proceed-\nings of NIPS, pages 2674–2682.\n[Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron\nCourville, Ruslan Salakhutdinov, Richard Zemel, and\nYoshua Bengio. 2015. Show, attend and tell: Neural\nimage caption generation with visual attention.\n328",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7692800164222717
    },
    {
      "name": "Latent variable",
      "score": 0.7386033535003662
    },
    {
      "name": "Generative model",
      "score": 0.6840216517448425
    },
    {
      "name": "Generative grammar",
      "score": 0.6728566884994507
    },
    {
      "name": "Latent variable model",
      "score": 0.6267802715301514
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6166053414344788
    },
    {
      "name": "Sentence",
      "score": 0.6089926362037659
    },
    {
      "name": "Inference",
      "score": 0.5841020345687866
    },
    {
      "name": "Language model",
      "score": 0.5563993453979492
    },
    {
      "name": "Natural language processing",
      "score": 0.523522138595581
    },
    {
      "name": "Representation (politics)",
      "score": 0.4580049514770508
    },
    {
      "name": "Task (project management)",
      "score": 0.41987907886505127
    },
    {
      "name": "Machine learning",
      "score": 0.364822655916214
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 186
}