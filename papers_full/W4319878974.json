{
  "title": "A Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer",
  "url": "https://openalex.org/W4319878974",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5039682885",
      "name": "Gon Woo Kim",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5100444699",
      "name": "Sang Won Lee",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5110959430",
      "name": "Ha Young Son",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5111876537",
      "name": "Kae Won Choi",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2124607194",
    "https://openalex.org/W2799062425",
    "https://openalex.org/W2827033964",
    "https://openalex.org/W2997222501",
    "https://openalex.org/W3152753423",
    "https://openalex.org/W3021587713",
    "https://openalex.org/W3047553107",
    "https://openalex.org/W2990165697",
    "https://openalex.org/W4220890766",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W4214624153",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W4225598394"
  ],
  "abstract": "In this paper, we propose a human pose estimation algorithm for an impulse radio ultra-wideband (IR-UWB) radar based on the transformer-based deep learning model. We have built an IR-UWB radar system with an 8-by-8 multiple-input multiple-output (MIMO) antenna array. The IR-UWB radar system in our paper is advantageous for the through-wall detection application since it operates on a very low frequency range (i.e., 0.45 to 3.55 GHz) compared to other existing works on RF-based human pose estimation. Moreover, the human pose estimation by an IR-UWB radar has not been studied very well in other existing works since all existing works have used a frequency-modulated continuous wave (FMCW) radar or a WiFi device. We propose a 3D-TransPOSE algorithm for the 3D human pose estimation from the IR-UWB radar signals. The proposed algorithm is designed based on the transformer architecture. While the transformer has actively been studied in the natural language processing (NLP) or vision domains, no prior work has applied the transformer model to the RF-based human pose estimation problem. The attention mechanism of the proposed algorithm is able to focus on the relevant time segments of the IR-UWB radar signals for detecting the human pose, eliminating the needs of converting radar signals to a voxelized 3D image. We have gathered a large dataset of IR-UWB radar signals labeled with 3D human skeletons, and shown that the proposed algorithm is able to detect human skeletons with a high accuracy.",
  "full_text": "Received 2 January 2023, accepted 5 February 2023, date of publication 10 February 2023, date of current version 16 February 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3244017\nA Study on 3D Human Pose Estimation Using\nThrough-Wall IR-UWB Radar and Transformer\nGON WOO KIM, SANG WON LEE\n, HA YOUNG SON,\nAND KAE WON CHOI\n, (Senior Member, IEEE)\nDepartment of Electrical and Computer Engineering, Sungkyunkwan University (SKKU), Suwon 03063, South Korea\nCorresponding author: Kae Won Choi (kaewonchoi@skku.edu)\nThis work was supported in part by the Institute of Information and Communications Technology Planning and Evaluation (IITP) through\nthe Korean Government Ministry of Science and ICT (MSIT), Reconstruction of Non-Line-of-Sight Scene for VR/AR Contents, under\nGrant 2020-0-00973; and in part by MSIT, South Korea, under the ICT Creative Consilience Program supervised by the IITP, under Grant\nIITP-2023-2020-0-01821.\nABSTRACT In this paper, we propose a human pose estimation algorithm for an impulse radio ultra-\nwideband (IR-UWB) radar based on the transformer-based deep learning model. We have built an IR-UWB\nradar system with an 8-by-8 multiple-input multiple-output (MIMO) antenna array. The IR-UWB radar\nsystem in our paper is advantageous for the through-wall detection application since it operates on a very\nlow frequency range (i.e., 0.45 to 3.55 GHz) compared to other existing works on RF-based human pose\nestimation. Moreover, the human pose estimation by an IR-UWB radar has not been studied very well in\nother existing works since all existing works have used a frequency-modulated continuous wave (FMCW)\nradar or a WiFi device. We propose a 3D-TransPOSE algorithm for the 3D human pose estimation from the\nIR-UWB radar signals. The proposed algorithm is designed based on the transformer architecture. While the\ntransformer has actively been studied in the natural language processing (NLP) or vision domains, no prior\nwork has applied the transformer model to the RF-based human pose estimation problem. The attention\nmechanism of the proposed algorithm is able to focus on the relevant time segments of the IR-UWB radar\nsignals for detecting the human pose, eliminating the needs of converting radar signals to a voxelized 3D\nimage. We have gathered a large dataset of IR-UWB radar signals labeled with 3D human skeletons, and\nshown that the proposed algorithm is able to detect human skeletons with a high accuracy.\nINDEX TERMS IR-UWB radar, MIMO, attention, transformer, keypoint detection, pose estimation.\nI. INTRODUCTION\nRecently, three-dimensional object recognition techniques to\naccurately detect the position of an object in a visually-\nobstructed area has been attracting attentions from research\ncommunities. The radio frequency (RF) wave is known to\npenetrate through a wall, and is able to give information on\nthe objects behind the wall. The through-wall radar makes\nuse of this property of the RF signal to detect objects in\nan invisible area [1]. The RF signal transmitted from the\nantenna of the radar passes through the obstructing material,\nis reflected by the object behind the obstacle, and is received\nby the radar. This reflected signal can be processed by a deep\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Guolong Cui\n.\nlearning network for three-dimensional object detection or\npose estimation behind a wall.\nThere have been research works on the deep learning-based\npose estimation by using the radar signal [2], [3], [4], [5],\n[6], [7], [8], [9]. The authors of [2] have proposed RF-Pose\nthat is able to estimate 2D human pose in an occluded area\nby using a convolutional neural network (CNN). This work\nis later extended to RF-Pose3D [3] which can estimate 3D\nkeypoints of human body. In [3], the authors have proposed\na planar decomposition of an 3D radar image to reduce the\ncomplexity of 4D CNN. In another works [4] and [5], 2D\nand 3D pose estimation algorithms based on commodity Wi-\nFi devices have been proposed, respectively. In [4] and [5],\na channel state information (CSI) image is extracted from\nWi-Fi signals to be used as an input to a CNN. The WiPose\n15082 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nproposed in [6] has also made use of a Wi-Fi device to\ndetect 3D human skeleton. The WiPose has encoded the prior\nknowledge of a skeletal structure of the human body into\nthe skeleton construction process, and employed a recurrent\nneural network (RNN) to enforce smooth movements of the\nskeleton over frames. A mmWave radar is used for detecting\nthe human pose in [7], [8], [9]. The authors of [7] have\nused a 77 GHz millimeter wave radar for 2D human pose\nestimation. The algorithm in [8] projects a radar point cloud\ninto range-azimuth and range-elevation planes. Then, the\nprojected point cloud is voxelized into an RGB image, which\nis put into a forked CNN architecture. The same authors\nhave proposed another RF pose estimation algorithm in [9],\nin which the voxelized image is processed by the sequence-\nto-sequence (Seq2Seq) model.\nBeside few exceptions, almost all deep learning-based\nRF pose estimation algorithms use a CNN [2], [3], [4],\n[5], [6], [7], [8]. The signals from a frequency-modulated\ncontinuous wave (FMCW) radar or the CSI from Wi-Fi\ndevices can be pre-processed to produce an 2D or 3D image-\nlike data structure. This voxelized image can be treated as\na typical vision image, and is put into the conventional\nobjection detection or pose estimation networks that consist\nof a CNN and a multi-layer perceptron (MLP). The WiPose\nin [6] also uses long-short term memory (LSTM) networks\nin addition to CNNs to relate skeletons over time. However,\nthe main backbone of the WiPose is still a CNN. The only\nexception we have found is the Seq2Seq model in [9], which\nis inspired by a famous model for the natural language\nprocessing (NLP) domain. In [9], multiple time frames of\npoint clouds from radars are treated as a paragraph of\nmultiple sentences in NLP, while output skeletal keypoints\ncorrespond to the extracted keywords. The Seq2Seq model\nin [9] consists of gated recurrent units (GRUs) and attention\nmodules without CNN. However, the Seq2Seq model is\nregarded as an outdated model in the NLP domain, which is\nobsoleted by the transformer.\nThe transformer [10] has become a de facto standard\nfor an NLP task because of its unmatched performance.\nThe transformer is based solely on the attention mechanism\nwithout RNNs. Recently, the transformer has successfully\nextended its application to vision tasks such as object\nclassification and detection [11], [12], [13], [14], [15]. For\nobject classification, the vision transformer (ViT) in [11]\nsplits an image into fixed-size patches and embeds theses\npatches into a transformer encoder. The ViT has outper-\nformed CNN in terms of accuracy and efficiency in the object\nclassification task. However, the ViT is required to be pre-\ntrained by extremely large datasets. The data-efficient image\ntransformers (DeiT) [12] has solved this problem by means of\nthe knowledge distillation procedure. The transformer model\nis applied to the object detection task by the detection trans-\nformer (DETR) [13]. In the DETR, an image is processed by\na CNN backbone to learn the features, which is processed by\nthe self-attention modules of the transformer encoder. Then,\nthe transformer decoder extracts the information from the\ntransformer encoder by using the cross attention to detect\nobjects. Some variants of the DETR have been proposed, for\nexample, the deformable DETR [14] incorporates the sparse\nspatial sampling of the deformable CNN into the transformer\nmodel, and the 3DETR [15] detects objects from 3D point\nclouds.\nIn this paper, we propose the 3D-TransPOSE model,\nwhich is built from the transformer, for 3D pose estimation\nby the impulse radio ultra-wideband (IR-UWB) radar.\nIn consideration of great success of the transformer in the\nNLP and vision domains, it would be meaningful to explore\nthe potential of the transformer in the radar-based pose\nestimation. However, to the authors’ knowledge, there is no\nprior work proposing to use the transformer for the radar-\nbased pose estimation. The transformer matches well with\nthe radar signal processing tasks in the following reason.\nIn the multiple-input multiple-output (MIMO) radar, the radar\nsignals from the pairs of transmit and receive antennas can\nbe processed to construct a voxelized 3D image. The typical\nprocedure of the radar-based pose estimation is to process\nthe voxelized 3D image by using a CNN. However, the\ntransformer can take a different approach by its attention\nmechanism. During the voxelization procedure, each voxel\ncan be constructed by adding up the radar signals at some\nspecific time points depending on different antenna pairs.\nIn other words, the information of an object in a 3D space\nshould be collected from different parts of time-domain\nradar signals depending on the antenna pairs. To collect the\ninformation about an object, the attention mechanism of the\ntransformer can naturally give a high attention to the relevant\ntime periods of radar signals from different antenna pairs.\nWith this capability, the proposed 3D-TransPOSE model can\ndetect a human pose directly from the time-domain radar\nsignal without voxelization.\nFor this work, we have built an IR-UWB MIMO radar\nsystem consisting of 8 transmit and 8 receive antennas. This\nsystem is unique in that it uses an IR-UWB radar while\nall other existing works use a FMCW radar [2], [3], [7],\n[8], [9] or a Wi-Fi device [4], [5], [6]. The IR-UWB radar\ntransmits a very short impulse to the air, and the reflected\nsignal is directly digitized by a high-speed analog-to-digital\nconverter (ADC). The IR-UWB radar has the advantages\nof higher resolution and lower signal-to-noise ratio (SNR)\nin comparison to the FMCW radar. Since the signal from\nthe IR-UWB radar is a time-domain one, it needs different\nprocessing than that from the FMCW radar. In addition, the\nradar system in this paper works on a very low frequency\nrange, that is, 0.45 to 3.55 GHz, while the frequency bands\nof other works are 5.4-7.2 GHz [2], [3], 5.8 GHz [4], [5],\n[6], and 77 GHz [7], [8], [9]. It is required to use a low\nfrequency RF signal to penetrate through a concrete wall\nwith a moderate thickness. Therefore, the radar system in this\npaper is more appropriate for a through-wall radar than these\nin other previous works. However, it is more challenging to\ndetect the human pose with the RF signal in a low frequency\nrange because of its low resolution in the angular domain.\nVOLUME 11, 2023 15083\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 1. IR-UWB MIMO radar system.\nBy using this radar system, we have gathered a large radar\ndataset labeled with keypoints of human skeletons, which are\nobtained by an optical motion capture system. The proposed\n3D-TransPOSE model is trained and tested based on this\nradar dataset.\nIn summary, the contribution of this paper is twofold.\n• First, we have built an IR-UWB radar system with an\n8 × 8 MIMO antenna array for detecting a human\npose behind a wall. This radar system has a high range\nresolution and excellent through-wall capability with\nits very wide bandwidth in a low frequency range.\nA massive dataset labeled with 3D human skeletons is\ncollected by using this radar system.\n• Second, we propose a 3D-TransPOSE model, which is\nbased on the transformer architecture, for 3D human\npose estimation. As far as we know, there has been no\nwork applying the transformer to the RF-based human\npose estimation. The proposed 3D-TransPOSE model\ncan focus on relevant time periods of the time-domain\nIR-UWB radar signals by using its attention mechanism.\nThe rest of this paper is organized as follows. In Section II,\nwe give a detailed explanation on the IR-UWB MIMO\nradar system and its principle. The 3D-TransPOSE model is\nproposed in Section III. The performance of the proposed\n3D-TransPOSE is demonstrated by experiments in Sec-\ntion IV, and the paper is concluded in Section V. The\nvideo clip showing the prediction results by the proposed\n3D-TransPOSE model can be seen in [16].\nII. IR-UWB MIMO RADAR SYSTEM FOR THROUGH-WALL\nHUMAN POSE ESTIMATION\nA. IR-UWB MIMO RADAR SYSTEM\nIn this section, we describe the IR-UWB MIMO radar\nsystem as shown in Fig. 1. The IR-UWB MIMO radar\nsystem consists of a radar chip, an amplifier, transmit and\nFIGURE 2. Antenna array layout.\nreceive RF switches, and 8 transmit and 8 receive antennas.\nThe radar chip is an IR-UWB radar chip (i.e., Novelda X1)\nthe frequency range of which is 0.45 to 3.55 GHz and the\nsampling rate of the ADC of which is up to 40 Gsps. The\nrange resolution of the radar chip in terms of the sampling\nrate is about 7.88 mm. The radar chip sends a UWB impulse\nfrom the transmit port and samples the received signal from\nthe receive port by using the ADC.\nThe transmitted impulse is amplified by a broadband\namplifier (i.e., SHF S126 A). The frequency of this amplifier\nis very wide (i.e., 80 kHz to 20 GHz), and the gain is 29 dB.\nThe output port of the amplifier is connected to a transmit\nRF switch (i.e., pSemi PE42412, PE42512), the frequency\nrange of which is 10 MHz to 8 GHz. This RF switch can\ndeliver the output signal from the amplifier to the selected\ntransmit antenna out of 8 transmit antennas according to a\ncontrol input. The transmit and receive antennas are vivaldi\nantennas with a 1.3 to 4.4 GHz frequency range. The gain of\nthe Vivaldi antenna is 9.15 dBi. The signal received by each\nreceive antenna is delivered to a receive RF switch, which\nis the same as the transmit one. The RF radar chip receives\nthe signal from the selected antenna, amplifies it by a low\nnoise amplifier (LNA), and digitizes it by using the ADC at\nthe sampling rate of 40 Gsps.\nIn this paper, the MIMO antenna array is arranged\naccording to Fig. 2. As shown in Fig. 2(a), 8 transmit\nantennas are placed vertically at both ends, and 8 receive\nantennas are arranged horizontally to form a two-dimensional\narrangement. With this antenna layout, the virtual antenna\narray is formed as shown in Fig. 2(b). Each antenna element\nin the virtual antenna array corresponds one relative position\nof a pair of transmit and receive antennas. We can see that\na high density virtual antenna array is formed so that a high\nangular resolution can be achieved.\nB. OPERATION PRINCIPLE OF MIMO IR-UWB RADAR\nSYSTEM\nThe IR-UWB radar system sends a short pulse to the air. The\nradar pulse is a monocycle signal which is defined as\nu(t) = 2α0t\nτ2 exp\n(\n−\n(t\nτ\n)2)\n, (1)\n15084 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 3. Near-field 3D radar imaging with back-projection algorithm.\nwhere α0 is the amplitude and τ is the scaling factor of the\npulse.\nLet us suppose that there are Ntgt point targets. The three-\ndimensional position of the jth target is denoted by xtgt\nj .\nLet Mtx and Mrx denote the number of transmit and receive\nantennas, respectively. The positions of the mth transmit and\nnth receive antennas are denoted by xtx\nm and xrx\nn , respectively.\nWhen the transmit and receive switches have respectively\nselected the mth transmit and nth receive antennas, the\nreceived signal is\nsm,n(t) =\nNtgt∑\nj=1\nγju(t − dm,n(xtgt\nj )/c) + η(t), (2)\nwhere γj is the reflectivity of the jth target, η(t) is the noise, c\nis the speed of light, and dm,n(x) is the travelling distance of\nthe pulse from the mth transmit antenna to nth receive antenna\nvia a three-dimensional point x. Then, dm,n(x) is given by\ndm,n(x) = ∥x− xtx\nm ∥ + ∥x− xrx\nm ∥, (3)\nwhere ∥ · ∥is the Euclidean norm.\nThe back-projection algorithm can be used for converting\nthe received signals to a radar image as shown in Fig. 3. The\nimage at point x is calculated as\ng(x) =\nMtx∑\nm=1\nMrx∑\nn=1\nsm,n(dm,n(x)/c). (4)\nThis imaging equation shows that, for the pair of the mth\ntransmit and nth receive antennas, the signal around the time\npoint dm,n(x)/c contains the information about the object\nlocated at x. The attention mechanism of the transformer\ncan learn the relevant time points in the signal, which have\nrich information about the object at a specific position.\nTherefore, the transformer-based algorithm can perform\nthe radar-based human pose estimation without the radar\nimaging.\nThe radar signals for all antenna pairs are repeatedly\nobtained for each frame. The Mtx × Mrx × Nsamp tensor of\nthe sampled radar signal in frame i is defined as\nSi = {si\nm,n(Tsj)}m=1,...,Mtx , n=1,...,Mrx , j=0,...,Nsamp−1, (5)\nwhere Ts is the sampling period and Nsamp is the number of\nthe samples. In our experiments, 1024 samples are obtained\nfor each frame, that is, Nsamp = 1024.\nIn Fig. 4(a) and (b), we show the sample radar signal from\nthe IR-UWB radar system and the radar imaging results,\nrespectively.\nC. HUMAN BODY KEYPOINTS\nWe use an optical motion capture system (i.e., OptiTrack)\nconsisting of 12 motion capture cameras. While the RF\nradar system records the received radar signals, the motion\ncapture system captures in real time the body skeletons of\nhumans wearing motion capture suits with markers. The body\nskeleton consists of 21 keypoints as shown in Fig. 5. The\nnumber of body skeletons at frame i is denoted by Bi. The\nK × 3 tensor representing the bth body skeleton at frame i is\ndenoted by\nPi\nb = {pi\nb,k }k=1,...,K , (6)\nwhere K is the number of keypoints and pi\nb,k is the three-\ndimensional vector of the kth keypoint of the bth human body\nat frame i.\nIn Fig. 6, we show the sample human keypoints tracked by\nthe motion capture system.\nD. EXPERIMENTAL ENVIRONMENT\nHumans can move around within a test space with the size of\n4 m, 3 m, and 2 m in x-, y-, and z-axes, respectively, as shown\nin Fig. 7(a). The motion capture cameras are placed above the\ntest space to track the keypoints of human bodies.\nA 4.5 cm-thick plywood is placed between the RF radar\nsystem and the test space to make the test space invisible\nfrom the radar as shown in Figs. 7(a) and (b). The RF radar\nsystem is positioned in four different locations A, B, C, and\nD, as shown in Fig. 7(c). We have gathered the radar signals\nand human body keypoints for all four locations to build a\ndataset. The human body keypoints are relocated into the\nradar-centric coordinate system in which the x-, y-, z-axes\npoint to the lateral, front, and upward directions of the antenna\narray, respectively.\nIII. PROPOSED 3D-TRANSPOSE ALGORITHM\nA. OVERVIEW\nIn this section, we describe the proposed 3D-TransPOSE\nmodel shown in Fig. 8. The 3D-TransPOSE takes the radar\nVOLUME 11, 2023 15085\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 4. Visualization of radar signal.\nsignal Si in (5) as an input and estimates the human pose Pi in\n(6). The radar signal is preprocessed to remove clutters, and is\nput into the feature extraction module. The feature extraction\nmodule is a CNN model that works as a time-domain filter.\nThe output of the feature extraction module is the feature for\neach time point of the radar signal for each pair of transmit\nand receive antennas.\nThe transformer module has NQ input query embeddings,\neach of which corresponds to a single human body.\nThe transformer has NL layers, and each layer consists\nFIGURE 5. Keypoints of a human body.\nFIGURE 6. Samples of human keypoints.\nthe self-attention and cross-attention modules. The cross-\nattention module brings the relevant features from the radar\nsignal by using an attention mechanism, while the self-\nattention is performed between the query embeddings.\nAn output query embedding from the transformer goes\nthrough a multi-layer perceptron (MLP) and linear layers\nto produce an objectness, bounding box, and keypoints. For\ntraining, output query embeddings are matched to the human\nskeletons in Pi, and the weighted sum of the objectness\nloss, L1 box loss, L1 keypoint loss, and generalized\nintersection over union (GIoU) loss is computed between\nthe matched query embedding and ground-truth human\nskeleton.\nIn this section, we will explain the feature extraction,\ntransformer, and loss computation in Sections III-B, III-C,\nand III-D, respectively.\n15086 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 7. Experimental environment.\nB. PREPROCESSING AND FEATURE EXTRACTION\nTo remove the radar clutter, we eliminate the static reflection\nsignal by subtracting the moving average from the radar\nsignal. That is, the decluttered radar signal is\n¯Si = Si − 1\nNA\nNA−1∑\nj=0\nSi−j, (7)\nwhere NA is the moving average window length. From now\non, we will omit the frame index i in the notation.\nTABLE 1. CNN layers of feature extraction module.\nThe first two dimensions of the Mtx × Mrx × Nsamp\ndecluttered radar signal tensor are flattened, and the radar\nsignal tensors over NT frames are stacked over the second\ndimension. Then, the input tensor to the CNN feature\nextraction module is a three-dimensional tensor XCNN,in, the\nshape of which is (M tx Mrx )×NF ×Nsamp. In our experiments,\nXCNN,in has the shape of 64 × NF × 1024.\nThe feature extraction module in Fig. 9 consists of 1D\nCNN layers that perform a convolutional operation over the\nlast dimension with size Nsamp while treating the second\ndimension with size NF as a channel. In Table 1, we show\nthe CNN layers used for our experiments. Each CNN layer\nis followed by a batch normalization and a rectified linear\nunit (ReLU) activation. The output channel size of the last\nCNN layer is dmodel , which is the same as the embedding\nsize of the transformer. The output tensor of the feature\nextraction module is denoted by XCNN,out , the shape of which\nis (M tx Mrx ) × dmodel × NCNN,out\nsamp . Here, NCNN,out\nsamp is the\nnumber of the time samples of the output of the CNN layers,\nand NCNN,out\nsamp = 62 in our experiments. Finally, XCNN,out is\nrearranged so that the first and last dimensions are combined\nand flattened. The resulting tensor is the radar signal feature\ntensor used by the transformer module, which is denoted by\nF ∈ RNF ×dmodel , (8)\nwhere NF = Mtx Mrx NCNN,out\nsamp = 3968.\nC. TRANSFORMER\nIn this subsection, we give a detailed explanation on the\ntransformer as shown in Fig. 10. The transformer consists of\nNL layers. Each layer has a self-attention module, a cross-\nattention module, and a feed-forward neural network (FFNN)\nmodule. Let NQ denote the number of queries and dmodel\ndenote the embedding size. Then, the input and output of\neach transformer layer is a tensor of shape NQ × dmodel . Let\nxl,in ∈ RNQ×dmodel and xl,out ∈ RNQ×dmodel denote the input\nand output of the lth transformer layer.\nLet us first explain the multi-head attention (MHA) mod-\nule. The multi-head attention module is a basic building block\nfor realizing the self-attention and cross-attention modules of\nthe transformer. The multi-head attention operation is defined\nas\ny = MHA(xQ, xK , xV ), (9)\nwhere xQ ∈ RNQ×dmodel , xK ∈ RNKV ×dmodel , and\nxV ∈ RNKV ×dmodel are the query, key, and value inputs and\ny ∈ RNQ×dmodel is the output. The number of the heads\nVOLUME 11, 2023 15087\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 8. Proposed 3D-TransPOSE architecture.\nFIGURE 9. Feature extraction module.\nis denoted by Nhead . The embedding size for each head\nis dhead = dmodel /Nhead given that dmodel is divisible by\nNhead . The inputs to the nth head are xQ\nn ∈ RNQ×dhead ,\nxK\nn ∈ RNKV ×dhead , xV\nn ∈ RNKV ×dhead which are produced by\nequally dividing xQ, xK , and xV along the second dimension,\nrespectively. The weight matrices for the query, key, and value\nof the nth head are all tensors with shape dhead ×dhead , which\nare denoted by WQ\nn , WK\nn , and WV\nn , respectively. Then, the\nquery Qn, key Kn, and value Vn for the nth head are computed\nas\nQn = xQ\nn WQ\nn , Kn = xK\nn WK\nn , Vn = xV\nn WV\nn . (10)\nThe attention matrix, denoted by An ∈ RNQ×NKV , is calcu-\nlated based on the query and key as\nAn = softmax\n(Qn(Kn)T\n√dhead\n)\n. (11)\nThen, the output of the nth head is xhead\nn ∈ RNQ×dhead such\nthat\nxhead\nn = AnVn. (12)\nFinally, we concatenate the output of all heads along the\nsecond dimension to make the output of the multi-head\nattention module as\ny = concat(xhead\n1 , . . . ,xhead\nNhead )W0, (13)\nFIGURE 10. Transformer architecture.\nwhere W0 ∈ Rdmodel ×dmodel is a weight matrix for combining\nthe outputs of all heads.\nSince all embeddings are treated equally regardless their\npositions by the multi-head attention module, it is required to\nuse a positional encoding to include the information about\nthe position of the embeddings. Let pF ∈ RNF ×dmodel and\npQ ∈ RNQ×dmodel denote the positional encodings for the\nradar signal feature and queries, respectively. The positional\nencoding for the radar signal feature, pF , is a sinusoidal\npositional embedding [10], while that for the queries, pQ, is a\nlearnable tensor.\nThe input to the lth layer, i.e., xl,in, first goes through the\nself-attention module as follows.\nxl,SA = MHA(xl,in + pQ, xl,in + pQ, xl,in). (14)\n15088 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nThen, the residual connection is made and the layer norm is\napplied as\nxl,LN1 = LayerNorm(xl,in + xl,SA), (15)\nwhere the LayerNorm normalizes the tensor along the last\ndimension. The next sublayer is the cross-attention module\nsuch that\nxl,CA = MHA(xl,LN1 + pQ, F + pF , F). (16)\nBy this cross-attention module, relevant radar signal features\nare brought to the respective queries. The residual connection\nand the layer normalization are applied again.\nxl,LN2 = LayerNorm(xl,LN1 + xl,CA), (17)\nThe final sublayer is the position-wise FFNN The FFNN\nsublayer consists of two fully-connected layers such that\nxl,FF = GELU(xl,LN2Wl,FF\n1 + bl,FF\n1 )Wl,FF\n2 + bl,FF\n2 ,\n(18)\nwhere ‘GELU’ is the GELU activation function and\nWl,FF\n1 ∈ Rdmodel ×dFF , bl,FF\n1 ∈ RdFF , Wl,FF\n2 ∈ RdFF ×dmodel ,\nand bl,FF\n2 ∈ Rdmodel are the weights and biases, and dFF is\nthe hidden dimension. Finally, the output of the lth layer is\ncomputed as\nxl,out = LayerNorm(xl,LN2 + xl,FF ). (19)\nThe input of the first layer is the query embeddings x1,in,\nwhich can be a zero or learnable tensor. The output of each\nlayer is fed into the next layer as an input. The output\nof the whole transformer is the output of the final layer\n(i.e., xNL ,out ).\nD. OBJECTNESS, BOUNDING BOX, AND KEYPOINT\nHEADS AND LOSS FUNCTION\nThe number of body skeletons is denoted by B, and the bth\nbody skeleton is denoted by Pb = {pb,k }k=1,...,K ∈ RK×3\nas given in (6). The coordinates of body skeletons are given\nin the radar-centric coordinate system. As shown in Fig. 11,\nwe calculate a 3D bounding box Ub ∈ R2×3 that encompasses\nall keypoints of the bth body skeleton. The first and second\nrows of Ub, respectively denoted by ub,1 ∈ R3 and ub,2 ∈ R3,\nare the center point and size of the bounding box in the x-\n, y-, and z-coordinates, respectively. We have added a small\nmargin in the x-, y-, and z-directions (i.e., (0.6 m, 0.6 m, 0 m)\nin this paper) to a bounding box. The keypoints of a body\nskeleton are normalized by representing them as relative\npositions within its bounding box. That is, the normalized\nkeypoints are computed as\n¯pb,k = (pb,k − (ub,1 − ub,2/2))/ub,2, (20)\nwhere (ub,1 − ub,2/2) is the minimum point of the bounding\nbox and the division by ub,2 is the component-wise division.\nWe define a staging 3D box in the radar coordinate system,\nwithin which body skeletons can appear. The staging 3D\nbox is represented by by the center point g1 ∈ R3 and the\nFIGURE 11. Bounding box and keypoints of body skeletons.\nsize g2 ∈ R3. In our experiment, the center point and size\nof the staging 3D box are g1 = (0 m, 2 m, 1.25 m) and\ng2 = (3 m, 3 m, 2.5 m), respectively. A bounding box is\nnormalized by the staging 3D box as\n¯ub,1 = (ub,1 − (g1 − g2/2))/g2, (21)\n¯ub,2 = ub,2/g2. (22)\nThe output of the transformer (i.e., xNL ,out ) is put into the\nobjectness, bounding box, and keypoint heads. The bounding\nbox head is an MLP with 3 hidden layers, ReLU activation,\nhidden dimension of dmodel , and output dimension of 6,\nfollowed by a sigmoid activation layer, and is applied to\neach query of the transformer. The output of the bounding\nbox head for the qth query from the transformer is reshaped\nto produce the bounding box prediction ˆUq ∈ R2×3 for\nq = 1, . . . ,NQ. On the other hand, the keypoint head consists\nof a single linear layer with output dimension of 3K followed\nby a sigmoid activation layer, which outputs the prediction\nof the body keypoints ˆPq ∈ RK×3 for the qth query of the\ntransformer. The objectness head is also a single linear layer\nwith output dimension of 1, followed by a sigmoid activation\nlayer. The objectness of the qth query of the transformer is\ndenoted by ˆoq ∈ R.\nThe bounding box prediction ˆUq predicts the normalized\nbounding box ¯Ub. We define two different losses for the\nbounding box prediction, one is the L1 box loss and the other\nis the GIoU loss. The L1 box loss between two bounding\nboxes U and U′ is\nLbox (U, U′) = ∥U− U′∥1, (23)\nwhere ∥ · ∥1 is the L1 norm with U and U′ treated as\nvectors of size 6. On the other hand, the GIoU loss between\nU and U′ is denoted by LGIoU (U, U′). The GIoU [17] is\nthe generalization of the intersection over union (IoU) that\nis effective even when two boxes are not overlapping. The\nGIoU loss is computed as one minus GIoU. The normalized\nkeypoints ¯Pb is predicted by the keypoint prediction ˆPq.\nWe define the L1 keypoint loss between P and P′ as\nLkey(P, P′) = ∥P− P′∥1. (24)\nEach body skeleton is matched with the output of each\nquery by using the Hungarian algorithm. The matching\nVOLUME 11, 2023 15089\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\ncost between the qth query and bth body skeleton is\ndefined as\nCmatch\nq,b = −λobj ˆoq + λbox Lbox ( ˆUq, ¯Ub)\n+λGIoU LGIoU ( ˆUq, ¯Ub) + λkeyLkey( ˆPq, ¯Pb), (25)\nwhere λobj, λbox , λGIoU , and λkey are the matching weights\nfor the objectness, L1 box, GIoU, and L1 keypoint losses.\nThe Hungarian algorithm derives the set of matching\nindices of the query and body skeleton, which is denoted\nby I, that minimizes the sum of the matching costs∑\n(q,b)∈I Cmatch\nq,b . In addition, we define Q as the set of the\nindices of queries that are matched with a body skeleton\n(i.e., Q = {q|(q,b) ∈ I}).\nFinally, the total loss for training is computed as\nLtotal\n= wobj\n(\n− wexist\n∑\nq∈Q\nlog(ˆoq) −\n∑\nq∈Qc\nlog(1 − ˆoq)\n)\n/NQ\n+\n∑\n(q,b)∈I\n(\nwbox Lbox ( ˆUq, ¯Ub)\n+wGIoU LGIoU ( ˆUq, ¯Ub) + wkeyLkey( ˆPq, ¯Pb)\n)\n, (26)\nwhere Qc is the complement of Q, wobj, wbox , wGIoU , and\nwkey are the weights for the objectness, L1 box, GIoU, and\nL1 keypoint losses, and wexist is the weight for the positive\nobjectness. In (26), we can see that the binary cross entropy\n(BCE) loss is used for the objectness.\nThe training is done by computing the gradients of the total\nloss in (26) by using the backpropagation and updating the\nweights according to the gradients.\nIV. EXPERIMENTAL RESULTS\nA. DATASET\nWe have built a large dataset of IR-UWB radar signals and\nhuman skeletons. We show the summary of the dataset in\nTable 2. The datasets are gathered in four different radar\npositions (i.e., A, B, C, and D) with one or two humans\ninvolved. Each session is contiguously recorded, and the\nnumber of frames for each session is given in Table 2. Table 2\nalso shows if each session is for training or test. The total\nnumber of frames in the training set is 105,400, while that in\nthe test set is 30,750.\nB. TRAINING\nTable 3 summarizes the model and training parameters\nused for training the proposed 3D-TransPOSE algorithm.\nWe use the AdamW optimizer with the learning rate of\n1e-4 and the weight decay of 1e-4. The learning rate\n(LR) scheduler decays the learning rate by multiplying\n0.1 after 72 epochs. The batch size is 32 and the total\nnumber of epochs is 84. The hyperparameters for the\nmodel in Table 3 have been optimized by sweeping the\nhyperparameters.\nFig. 12(a) shows the total loss over steps. On the other\nhand, Fig. 12(b) and (c) show the average precision (AP)\nTABLE 2. Datasets.\nwith the IoU threshold of 0.5 and the percentage of correct\nkeypoints (PCK) with the distance threshold of 0.2 m,\nrespectively. The AP and PCK are the main performance\nmetrics measuring the accuracy of bounding boxes and\nkeypoints, respectively. While the total loss decreases over\nsteps, the AP and PCK gradually increase without overfitting.\nC. HUMAN POSE ESTIMATION EXAMPLES\nThe trained model is used to detect the human pose in the test\ndataset. The query outputs with the objectness higher than\n0.9 are considered as a detected human body. We have used\nthe non-maximum suppression (NMS) with the IoU threshold\nof 0.5 on the bounding boxes for removing the query outputs\nwith redundant bounding boxes. The body keypoints in\nthe radar-centric coordinate system are reconstructed by\nreversing the normalization procedure.\nFig. 13 shows the examples of the estimated and ground-\ntruth body skeletons. In this figure, we can see that the human\nskeletons are generally well estimated. It is observed that the\nresults are less accurate when there are two human bodies.\nThis is because a human body can block the radar signal\nreflected by other human body, or make additional reflections\n15090 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 12. Training result.\nTABLE 3. Training and model parameters.\nresulting in signal distortions. However, we can still see\nmultiple human bodies can simultaneously be detected with\na moderate accuracy.\nThe video clip showing the prediction results by the\nproposed 3D-TransPOSE model can be seen in [16].\nD. HUMAN DETECTION PERFORMANCE\nIn this subsection, we present the performance of predicting\n3D bounding boxes. The predicted bounding boxes of the\nqueries with the objectness higher than a given objectness\nthreshold are matched with the ground-truth bounding boxes.\nA ground-truth bounding boxes can be matched with a\npredicted bounding box if the IoU between them is higher\nthan a given matching IoU threshold. The precision is defined\nas the number of matched predicted bounding boxes over the\nnumber of all predicted bounding boxes, while the recall is\nTABLE 4. AP for different IoU thresholds.\nTABLE 5. PCK for different distance thresholds.\nthe number of matched ground-truth bounding boxes over the\nnumber of all ground-truth bounding boxes.\nIn Fig. 14, we show the precision-recall curve drawn by\nvarying the objectness threshold. In this figure, multiple\nprecision-recall curves for different matching IoU thresholds\nare presented. The AP is the area under the precision-recall\ncurve. Table 4 shows the APs for different IoU thresholds.\nThe precision-recall curves and APs show that the bounding\nboxes can be detected with high probabilities when the IoU\nthresholds are 0.1, 0.3, 0.5, and 0.75.\nE. POSE ESTIMATION PERFORMANCE\nThe performance of the body pose estimation is measured\nby the PCK, which is the number of the correctly estimated\nkeypoints over the total number of keypoints. A keypoint\nis considered to be correctly estimated if and only if\nthe Euclidean distance between the estimated and ground-\ntruth keypoints are less than a given distance threshold.\nWe compute the PCK between the predicted and ground-truth\nkeypoints only when the corresponding bounding boxes are\nmatched.\nIn Fig. 15, we show the PCK curve in which the x-axis is\nthe distance threshold. This PCK curve is actually the cumu-\nlative density function (cdf) of the error distance between the\npredicted and ground-truth keypoints. In addition, Table 5\npresents the PCKs for several distance thresholds. It is noted\nthat the PCK is evaluated based on the absolute distance\nbetween keypoints, which means that the PCK can be\ndegraded if the bounding box is shifted even when the body\npose is accurate. As seen by the examples in Fig. 13, this PCK\nvalue is high enough to reconstruct an accurate human pose.\nF. ATTENTION WEIGHTS\nIn this subsection, we show the attention weights in (11) of the\ncross-attention modules. The attention weights of the cross-\nattention module represent the amount of information that a\nquery brings from each radar signal feature in (8).\nFig. 16 shows the attention weights of some sample\nqueries that are successfully matched with ground-truth\nhuman bodies. The attention weights are averaged over\nmultiple heads. In this figure, each row corresponds to\nan example query while each column corresponds to one\nof eight transformer layers. In each plot of Fig. 16, the\nx-axis corresponds to the time samples while the y-axis\ncorresponds to the antenna pairs. Although it is very difficult\nVOLUME 11, 2023 15091\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 13. Examples of the estimated and ground-truth body skeletons. The estimated and ground-truth body skeletons are shown in blue and red\ncolors, respectively.\nFIGURE 14. Precision-recall curve.\nto directly interpret the meaning of the attention weights,\nit is conjectured that the functionalities of transformer layers\nmay differ since the attention weights from different layers\nhave quite different patterns. For example, first two layers\ngather information from all over the radar signal features,\nwhile the rest of layers have a narrow focus on specific\nfeatures.\nFIGURE 15. PCK curve.\nG. ABLATIONS\nIn Table 6, we show the results of the ablation study. We have\nvaried three most important model hyperparameters, the\nnumber of transformer layers (N L ), the embedding dimension\n(dmodel ), and the number of queries (N Q). The AP with the\nIoU threshold of 0.5 and the PCK with the distance threshold\n15092 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 16. Example of attention weights.\nTABLE 6. Variations on 3D-TransPOSE architecture. Unlisted values are\nidentical to those of the base model.\nof 0.2 m are presented. The first row is the base model.\nIn Table 6, we can see that the base model performs the best.\nH. COMPARISON WITH CNN-BASED ALGORITHM\nIn this section, we compare the proposed 3D-TransPOSE\nmodel with the CNN-based pose estimation algorithm. Since\nthere is no existing pose estimation algorithm based on the\nIR-UWB radar signals, we have designed a new CNN-based\nalgorithm for comparison.\nThe Nsamp samples of the radar signals from Mtx transmit\nand Mrx receive antenna arrays are reordered to match the\nvirtual array in Fig. 2b to form the input tensor with the shape\nof Mtx ×Mrx ×Nsamp, which is 8×8×1024 in our experiments.\nTable 7 presents the CNN structure which is composed\nof 4 convolution layers. The input tensor with the shape of\nMtx × Mrx × Nsamp, the first dimension of which is regarded\nas a channel, is put into the CNN, and the output tensor has\nthe shape of 70 × GS × GS , where GS × GS represents the\nTABLE 7. CNN layer structure.\ntwo-dimensional grid on the x-y plane. In our experiment,\nwe have GS = 4. For each grid point, we have a vector of size\n70. Among them, 6 components are used for predicting the\ncoordinate and size of a 3D bounding box, and 1 component\nis used for the confidence score, and 63 components are used\nfor 21 three-dimensional keypoints.\nFor computing the loss function, each ground-truth bound-\ning box is matched to the predicted bounding box in the same\ngrid. Since the prediction from the CNN-based algorithm has\nthe same structure as that of the 3D-TransPOSE, the loss\nfunction in (26) is reused with a minor modification. The only\ndifference is that we have removed the GIoU loss from (26)\nsince the ground truth bounding box is always close to the\npredicted one in the grid structure used for the CNN-based\nalgorithm.\nThe training parameters for the CNN algorithm are almost\nidentical to those for the 3D-TransPOSE model. In evaluating\nthe CNN algorithm, the predictions with the confidence score\nhigher than 0.9 are considered detected, and the NMS with\nthe IoU threshold of 0.5 is applied as in the 3D-TransPOSE\nmodel.\nFigs. 17 and 18 show the precision-recall and PCK\ncurves of the CNN algorithms. Note that the CNN model is\nVOLUME 11, 2023 15093\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\nFIGURE 17. Precision-recall curve of CNN algorithm.\nFIGURE 18. PCK curve of CNN algorithm.\nTABLE 8. Comparison of AP for different IoU thresholds between\n3D-TransPOSE and CNN algorithms.\nTABLE 9. Comparison of PCK for different distance thresholds between\n3D-TransPOSE and CNN algorithms.\noptimized by extensively sweeping the model hyperparam-\neters. While the CNN algorithm has decent performances,\nTables 8 and 9 show that the proposed 3D-TransPOSE model\noutperforms the CNN algorithm by a wide margin in terms\nof both AP and PCK. This shows that it is beneficial to learn\nrelational information about time-domain RF signals with the\nhuman pose through an attention mechanism.\nV. CONCLUSION\nIn this paper, we have presented the 3D-TransPOSE model\nto predict the 3D human keypoints based on the IR-UWB\nradar signals. A large radar signal dataset labeled with\nhuman keypoints has been constructed by the IR-UWB radar\nsystems and the motion capture devices. The proposed 3D-\nTransPOSE model has been trained by the dataset, and the\nhuman detection and pose estimation performances have been\nevaluated in terms of the AP and PCK, respectively. We have\nshown that the propose model successfully estimates the\nhuman keypoints behind the wall based on the IR-UWB radar\nsignals.\nACKNOWLEDGMENT\n(Gon Woo Kim and Sang Won Lee contributed equally to this\nwork.)\nREFERENCES\n[1] J. Li, Z. Zeng, J. Sun, and F. Liu, ‘‘Through-wall detection of human\nbeing’s movement by UWB radar,’’ IEEE Geosci. Remote Sens. Lett.,\nvol. 9, no. 6, pp. 1079–1083, Nov. 2012.\n[2] M. Zhao, T. Li, M. A. Alsheikh, Y. Tian, H. Zhao, A. Torralba, and\nD. Katabi, ‘‘Through-wall human pose estimation using radio signals,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Salt Lake City, UT,\nUSA, Jun. 2018, pp. 7356–7365.\n[3] M. Zhao, Y. Tian, H. Zhao, M. A. Alsheikh, T. Li, R. Hristov, Z. Kabelac,\nD. Katabi, and A. Torralba, ‘‘RF-based 3D skeletons,’’ in Proc. Conf. ACM\nSpecial Interest Group Data Commun., Budapest, Hungary, Aug. 2018,\npp. 267–281.\n[4] L. Guo, Z. Lu, X. Wen, S. Zhou, and Z. Han, ‘‘From signal to image:\nCapturing fine-grained human poses with commodity Wi-Fi,’’ IEEE\nCommun. Lett., vol. 24, no. 4, pp. 802–806, Apr. 2020.\n[5] Y. Wang, L. Guo, Z. Lu, X. Wen, S. Zhou, and W. Meng, ‘‘From\npoint to space: 3D moving human pose estimation using commod-\nity WiFi,’’ IEEE Commun. Lett., vol. 25, no. 7, pp. 2235–2239,\nJul. 2021.\n[6] W. Jiang, H. Xue, C. Miao, S. Wang, S. Lin, C. Tian, S. Murali, H. Hu,\nZ. Sun, and L. Su, ‘‘Towards 3D human pose construction using WiFi,’’\nin Proc. 26th Annu. Int. Conf. Mobile Comput. Netw., London, U.K.,\nApr. 2020, pp. 1–14.\n[7] G. Li, Z. Zhang, H. Yang, J. Pan, D. Chen, and J. Zhang, ‘‘Capturing human\npose using mmWave radar,’’ in Proc. IEEE Int. Conf. Pervasive Comput.\nCommun. Workshops (PerCom Workshops), Austin, TX, USA, Mar. 2020,\npp. 1–6.\n[8] A. Sengupta, F. Jin, R. Zhang, and S. Cao, ‘‘mm-Pose: Real-time human\nskeletal posture estimation using mmWave radars and CNNs,’’ IEEE\nSensors J., vol. 20, no. 17, pp. 10032–10044, May 2020.\n[9] A. Sengupta and S. Cao, ‘‘mmPose-NLP: A natural language processing\napproach to precise skeletal pose estimation using mmWave radars,’’ early\naccess, Mar. 1, 2022, doi: 10.1109/TNNLS.2022.3151101.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. NIPS,\nLong Beach, CA, USA, 2017, pp. 6000–6010.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words:\nTransformers for image recognition at scale,’’ in Proc. ICLR, 2020,\npp. 1–22.\n[12] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. Jégou, ‘‘Training data-efficient image transformers & distillation\nthrough attention,’’ in Proc. ICML, 2021, pp. 10347–10357.\n[13] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in Proc.\nECCV, 2020, pp. 213–229.\n[14] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‘‘Deformable DETR:\nDeformable transformers for end-to-end object detection,’’ in Proc. ICLR,\n2020, pp. 1–16.\n[15] I. Misra, R. Girdhar, and A. Joulin, ‘‘An end-to-end transformer model for\n3D object detection,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2021, pp. 2906–2917.\n15094 VOLUME 11, 2023\nG. W. Kim et al.: Study on 3D Human Pose Estimation Using Through-Wall IR-UWB Radar and Transformer\n[16] CIoT Lab. A Study on 3D Human Pose Estimation Using Through-wall\nIR-UWB Radar and Transformer. Accessed: Oct. 19, 2022. [Online].\nAvailable: https://www.youtube.com/watch?v=Duhg8TNhJP8\n[17] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,\n‘‘Generalized intersection over union: A metric and a loss for bounding\nbox regression,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Long Beach, CA, USA, Jun. 2019, pp. 658–666.\nGON WOO KIM received the B.S. degree\nin electronic and electrical engineering from\nSungkyunkwan University, Suwon, South Korea,\nin 2021, and the M.S. degree from the Depart-\nment of Electrical and Computer Engineer-\ning, Sungkyunkwan University, in 2022. He is\ncurrently an Associate Researcher with LG Elec-\ntronics Inc., South Korea. His current research\ninterests include artificial intelligence, machine\nlearning, and sensor intelligence.\nSANG WON LEE received the B.S. degree\nin electronic and electrical engineering from\nSungkyunkwan University, Suwon, South Korea,\nin 2021, where he is currently pursuing the\nmaster’s degree with the Department of Electrical\nand Computer Engineering. His research interests\ninclude radar signal processing, machine learning,\nand deep learning.\nHA YOUNG SON received the B.S. degree\nin physics from Konkuk University, Seoul,\nSouth Korea, in 2020, and the M.S. degree\nfrom the Department of Electrical and Computer\nEngineering, Sungkyunkwan University, Suwon,\nSouth Korea, in 2022. Her research interests\ninclude artificial intelligence, machine learning,\nand computer vision.\nKAE WON CHOI (Senior Member, IEEE)\nreceived the B.S. degree in civil, urban, and\ngeosystem engineering, and the M.S. and Ph.D.\ndegrees in electrical engineering and computer\nscience from Seoul National University, Seoul,\nSouth Korea, in 2001, 2003, and 2007, respec-\ntively. From 2008 to 2009, he was with Telecom-\nmunication Business of Samsung Electronics\nCompany Ltd., South Korea. From 2009 to 2010,\nhe was a Postdoctoral Researcher at the Depart-\nment of Electrical and Computer Engineering, University of Manitoba,\nWinnipeg, MB, Canada. From 2010 to 2016, he was an Assistant Professor\nat the Department of Computer Science and Engineering, Seoul National\nUniversity of Science and Technology, South Korea. In 2016, he joined as\na Faculty Member at Sungkyunkwan University, South Korea, where he\nis currently an Associate Professor with the College of Information and\nCommunication Engineering. His research interests include machine learn-\ning, radar signal processing, reconfigurable intelligent surface, RF energy\ntransfer, cellular communication, and radio resource management. He has\nbeen the Editor of IEEE COMMUNICATIONS SURVEYS AND TUTORIALS, since 2014,\nIEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, since 2017, and IEEE\nTRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, since 2019.\nVOLUME 11, 2023 15095",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7246683835983276
    },
    {
      "name": "Radar",
      "score": 0.6795470118522644
    },
    {
      "name": "Continuous-wave radar",
      "score": 0.5942976474761963
    },
    {
      "name": "Artificial intelligence",
      "score": 0.529133677482605
    },
    {
      "name": "Pose",
      "score": 0.5129184722900391
    },
    {
      "name": "Transformer",
      "score": 0.4789462685585022
    },
    {
      "name": "Computer vision",
      "score": 0.47575920820236206
    },
    {
      "name": "Radar engineering details",
      "score": 0.4716331660747528
    },
    {
      "name": "Radar imaging",
      "score": 0.4662564992904663
    },
    {
      "name": "Telecommunications",
      "score": 0.21792566776275635
    },
    {
      "name": "Engineering",
      "score": 0.1652117371559143
    },
    {
      "name": "Electrical engineering",
      "score": 0.1071191132068634
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I848706",
      "name": "Sungkyunkwan University",
      "country": "KR"
    }
  ],
  "cited_by": 22
}