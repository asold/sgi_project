{
    "title": "Transformer-Based Acoustic Modeling for Hybrid Speech Recognition",
    "url": "https://openalex.org/W2981857663",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2116193461",
            "name": "Yongqiang Wang",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2125491050",
            "name": "Abdelrahman Mohamed",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3045845440",
            "name": "Due Le",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2121222603",
            "name": "Chunxi Liu",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2246215992",
            "name": "Alex Xiao",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2981541133",
            "name": "Jay Mahadeokar",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2130905166",
            "name": "Hongzhao Huang",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2307572011",
            "name": "Andros Tjandra",
            "affiliations": [
                "Nara Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2109967554",
            "name": "Xiaohui Zhang",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2131834514",
            "name": "Frank Zhang",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2761475506",
            "name": "Christian Fuegen",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2090483681",
            "name": "Geoffrey Zweig",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2250457939",
            "name": "Michael L. Seltzer",
            "affiliations": [
                "Meta (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2116193461",
            "name": "Yongqiang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2125491050",
            "name": "Abdelrahman Mohamed",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3045845440",
            "name": "Due Le",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121222603",
            "name": "Chunxi Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2246215992",
            "name": "Alex Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2981541133",
            "name": "Jay Mahadeokar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130905166",
            "name": "Hongzhao Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2307572011",
            "name": "Andros Tjandra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2109967554",
            "name": "Xiaohui Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131834514",
            "name": "Frank Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2761475506",
            "name": "Christian Fuegen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2090483681",
            "name": "Geoffrey Zweig",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2250457939",
            "name": "Michael L. Seltzer",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6631362777",
        "https://openalex.org/W6629717138",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2131342762",
        "https://openalex.org/W6713762819",
        "https://openalex.org/W2964084166",
        "https://openalex.org/W2962760690",
        "https://openalex.org/W2107878631",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W1995562189",
        "https://openalex.org/W2530876040",
        "https://openalex.org/W6696934422",
        "https://openalex.org/W6712930963",
        "https://openalex.org/W6769495571",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2394932179",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W2160815625",
        "https://openalex.org/W2802023636",
        "https://openalex.org/W2911291251",
        "https://openalex.org/W2097268427",
        "https://openalex.org/W2402146185",
        "https://openalex.org/W2293634267",
        "https://openalex.org/W2981740924",
        "https://openalex.org/W2184045248",
        "https://openalex.org/W1553004968",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3008525923",
        "https://openalex.org/W2520160253",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2896060389",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2977728428",
        "https://openalex.org/W2979286696",
        "https://openalex.org/W2941814890",
        "https://openalex.org/W2964089206",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3006827623",
        "https://openalex.org/W2962778134",
        "https://openalex.org/W2964045208",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2944255943",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W2798657914",
        "https://openalex.org/W1524333225",
        "https://openalex.org/W3103005696",
        "https://openalex.org/W2407080277",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1920942766",
        "https://openalex.org/W2795138957",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4288400010",
        "https://openalex.org/W3015960524"
    ],
    "abstract": "We propose and evaluate transformer-based acoustic models (AMs) for hybrid\\nspeech recognition. Several modeling choices are discussed in this work,\\nincluding various positional embedding methods and an iterated loss to enable\\ntraining deep transformers. We also present a preliminary study of using\\nlimited right context in transformer models, which makes it possible for\\nstreaming applications. We demonstrate that on the widely used Librispeech\\nbenchmark, our transformer-based AM outperforms the best published hybrid\\nresult by 19% to 26% relative when the standard n-gram language model (LM) is\\nused. Combined with neural network LM for rescoring, our proposed approach\\nachieves state-of-the-art results on Librispeech. Our findings are also\\nconfirmed on a much larger internal dataset.\\n",
    "full_text": "TRANSFORMER-BASED ACOUSTIC MODELING FOR HYBRID SPEECH RECOGNITION\nYongqiang Wang1, Abdelrahman Mohamed1, Duc Le1, Chunxi Liu1, Alex Xiao1,\nJay Mahadeokar 1,⋆ , Hongzhao Huang 1,⋆, Andros Tjandra 2,⋆†, Xiaohui Zhang 1,⋆, Frank Zhang 1,⋆,\nChristian Fuegen1,⋆, Geoffrey Zweig1,⋆, Michael L. Seltzer1,⋆\n1Facebook AI, USA 2Nara Institute of Science and Technology, Japan\nABSTRACT\nWe propose and evaluate transformer-based acoustic models (AMs)\nfor hybrid speech recognition. Several modeling choices are dis-\ncussed in this work, including various positional embedding meth-\nods and an iterated loss to enable training deep transformers. We also\npresent a preliminary study of using limited right context in trans-\nformer models, which makes it possible for streaming applications.\nWe demonstrate that on the widely used Librispeech benchmark, our\ntransformer-based AM outperforms the best published hybrid result\nby 19% to 26% relative when the standard n-gram language model\n(LM) is used. Combined with neural network LM for rescoring, our\nproposed approach achieves state-of-the-art results on Librispeech.\nOur ﬁndings are also conﬁrmed on a much larger internal dataset.\nIndex Terms— hybrid speech recognition, acoustic modeling,\ntransformer, recurrent neural networks\n1 Introduction\nSince the introduction of deep learning in automatic speech recogni-\ntion (ASR) [1], a variety of neural network architectures for acoustic\nmodeling have been explored [2–6]. Among them, recurrent neu-\nral networks (RNNs), especially long short-term memory (LSTM)\n[7] neural networks, are widely used, either in conventional hybrid\nsystems (e.g., [3, 8]), sequence-to-sequence-based (e.g. [9, 10]) or\nneural-transducer-based end-to-end systems (e.g. [11]). However,\nRNNs have several well-known limitations: 1) due to the vanish-\ning or exploding gradient problem discovered in [12], RNNs cannot\nmodel long term temporal dependencies well; 2) the recurrence na-\nture of RNNs makes it difﬁcult to process speech signal in parallel.\nTo address these issues, a variety of neural network architectures\nhave been proposed to replace RNNs, including time delay neural\nnetworks (TDNN) [5], feed-forward sequential memory networks\n(FSMN) [6], and convolution neural networks (CNN) [4, 13], while\nonly limited success has been achieved.\nRecently, self-attention network [14] has demonstrated promis-\ning results in a variety of natural language processing tasks (e.g.,\n[14–16]). Different from RNNs and CNNs, self-attention connects\narbitrary pairs of positions in the input sequence directly. To forward\n(or backward) signals between two positions that are nsteps away\nin the input, it only needs one step to traverse the network, com-\npared with O(n) steps in RNNs and O(log n) in CNNs. Moreover,\ncomputation in self-attention can be easily parallelized. On top of\nself-attention, the transformer model [14] leverages multi-head at-\ntention and interleaves with feed-forward layers. Self-attention and\ntransformer models were also used for ASR, mostly in the sequence-\nto-sequence architecture [17–19] with notable exceptions of [20,21].\nIn this work, we propose and evaluate transformer-based acous-\n⋆ Equal contribution;\n† Work was done when Andros was an intern at Facebook.\ntic models (AMs) for hybrid ASR. We explore several modeling\nchoices, including methods to encode either absolute or relative\npositional information into the input of transformer and an iterated\nloss to enable training deep transformers. Though our focus in\nthis work is to investigate the potential of transformer-based AMs\nwithout any constraint, we do explore streamable transformers and\npresent our initial experimental results. We show that our proposed\ntransformer-based AMs can yield signiﬁcant word error rate (WER)\nimprovement over very strong bi-directional LSTM (BLSTM) base-\nlines, both on the widely-used Librispeech benchmark and our\ninternal dataset. The results we obtained on Librispeech improve\nover the previous best hybrid WER by 19% to 26% when the stan-\ndard 4-gram language model (LM) is used; combined with neural\nLM rescoring, our system achieves state-of-the-art performance on\nthis dataset.\n2 Hybrid Architecture\nIn hybrid ASR [22], an acoustic encoder is used to encode an input\nsequence x1,··· ,xT to a sequence of high level embedding vectors\nz1,··· ,zT. These embedding vectors are used to produce a poste-\nrior distribution of tied states of hidden Markov model (HMM), such\nas senone [23] or chenone [24], for each frame. These posterior dis-\ntributions are then combined with other knowledge sources such as\nlexicons and LMs to construct a search graph. A decoder is then\nused to ﬁnd the best hypothesis. Different neural networks can be\nused as the encoder: in DNN, TDNN and CNN, zt is a function of\nxt and its ﬁxed number of neighboring frames; in uni-directional\nRNNs, zt is a function of x1 to xt, while in bi-directional RNNs,\nzt is a function of the entire input sequence.\nThough compared with the sequence-to-sequence or neural trans-\nducer architecture, the hybrid approach is admittedly less appealing\nas it is not end-to-end trained, it is still the best performing system\nfor authors’ practical problems. It also has the advantage that it can\nbe easily integrated with other knowledge sources (e.g., personalized\nlexicon) that may not be available during training. In this work, we\naim to leverage the transformer to improve hybrid acoustic model-\ning.\n3 Acoustic Modeling Using Transformer\nIn this section, we ﬁrst brieﬂy review the transformer network and\ndiscuss various modeling choices when using the transformer as the\nacoustic encoder. Relation to other works is also discussed in Section\n3.5.\n3.1 Self-Attention and Multi-Head Attention\nSelf-attention ﬁrst computes the attention distribution over the input\nsequence using dot-product attention, i.e., for every xt ∈ Rdi , a\narXiv:1910.09799v2  [cs.CL]  30 Apr 2020\ndistribution αt is obtained by:\nαtτ = exp(β·xT\ntWT\nq Wkxτ)∑\nτ′ exp(β·xT\ntWTq Wkxτ′ ) (1)\nwhere Wq,Wk ∈Rdk×di transforms xt to query and key space,\nβ = 1√\ndi\nis a scaling factor. Note that for language modeling, the\ndot-products between the current position and future positions are\nmasked to −∞to prevent future information leaking to the current\nembedding. Though for acoustic modeling, it is possible to attend\nto the entire sequence, in many applications, we only attend to lim-\nited right context frames to enable streaming processing of speech\nsignals (i.e., dot-product between tand τ,τ > t+ Ris masked to\n−∞). Given αt, the output embedding of self-attention is obtained\nvia:\nzt =\n∑\nτ\nDropout(αtτ) ·Wvxτ (2)\nwhere Wv ∈Rdv×di maps the input vectors to value space.\nSelf-attention is often combined with multi-head attention (MHA),\nwhere h self-attention heads are applied individually on the input\nsequences, and the output of each head are concatenated and linearly\ntransformed to a common space, i.e.,\nzt = Wo\n\n\n...∑\nτ Dropout(α(i)\ntτ ) ·W(i)\nv xτ\n...\n\n (3)\nwhere Wo ∈Rdi×hdv , α(i)\ntτ and W(i)\nv are the attention weights and\nthe value matrix of the i-th head.\n3.2 Architecture of Transformer\nIn addition to the MHA sub-layer, each transformer layer contains a\nfully-connected feed-forward network (FFN), which is composed by\ntwo linear transformations and a nonlinear activation function in be-\ntween. The FFN network is applied to each position in the sequence\nseparately and identically. To allow stacking many transformer layer\ntogether, residual connections are added to the MHA and FFN sub-\nlayers. Dropouts are also applied after MHA and linear transforma-\ntion as a form of regularization. Figure 1 summarizes the architec-\nture of one transformer layer. Note that different from [14], layer\nnormalization [25] is applied before MHA and FFN and the third\nlayer normalization ( LN3 in Figure 1) is necessary to prevent by-\npassing the transformer layer entirely. Note, following [15], we use\n“gelu” non-linearity [26] in the FFN network.\nFig. 1: Architecture of one transformer layer. “LN” means layer nor-\nmalization [25]; “FC” means fully connected linear transformation;\n“gelu” means the gelu nonlinear activation [26].\n3.3 Positional Embedding\nOne obvious limitation of the transformer layer is that the output is\ninvariant to the input order permutation, i.e., for any permutation π\napplied on the input sequence x1,··· ,xT, the output of the trans-\nformer layer can be obtained by applying the same permutation π\non z1,··· ,zT. This means that transformer does not model the\norder of the input sequence. In the original transformer work [14],\nthis is solved by injecting information about absolute positions into\nthe input sequence via sinusoid positional embeddings. We argued\nthat different from NLP applications, relative position could be more\nuseful for speech signals. In this work, we compare a few ways to\nencode positional information into the input of transformer:\n• Sinusoid positional embedding: a sinusoid positional em-\nbedding pt is added to xt, where the i-th element of pt is\nsin((t/10000)i/di ) for even iand cos((t/10000)(i−1)/di for\nodd i. This encodes absolute positional information;\n• Frame stacking: a simple way to break the permutation in-\nvariance is to stack ncontextual vectors together, i.e., xt =\n(xT\nt,xT\nt+1,··· ,xT\nt+n−1)T. This encodes the relative posi-\ntional information;\n• Convolutional embedding: inspired by [27], we use 2D con-\nvolutional layers to implicitly encode the relative positional\ninformation. Convolutional embedding implicitly performs\nframe stacking as well as learns useful short-range spectral-\ntemporal patterns [28].\n3.4 Training Deep Transformers\nTransformer layers can be stacked many times to form a very deep\nnetwork. In our initial experiments, we found better accuracies can\nbe obtained by deeper networks. However, after stacking many lay-\ners, it becomes difﬁcult to train and often gets stuck in a bad lo-\ncal optimum. To enable training deep transformer, we used iterated\nloss [29], in which output of some intermediate transformer layers\nis also used to calculate auxiliary cross entropy losses. These aux-\niliary losses are interpolated to make the ﬁnal loss function. Note\nthat intermediate-layer-speciﬁc parameters (e.g., the linear transfor-\nmation before the softmax operation) are discarded after training.\n3.5 Relation to Other Works\nThe original transformer paper [14] proposed to use self-attention\nand cross-attention to replace the recurrence in encoder and decoder\nin a sequence-to-sequence model. Since we focus on hybrid speech\nrecognition, we only use self-attention to replace the RNNs in the\nacoustic encoder in this work.\nSelf-attention based acoustic modeling has been explored in the\npast. In [20], self-attention is modiﬁed to attend to a ﬁxed number of\nleft and right context frames, and only one attention layer was used.\nBy comparison, in our work attention heads attend to all the past\nframes, and we use both self-attention and FFN networks with a very\ndeep structure, which is critical to achieve a good model accuracy.\nIn [30], transformers are compared with RNNs in the sequence-to-\nsequence architecture. In [18], various positional embedding meth-\nods were investigated for a sequence-to-sequence model, where it is\nfound that replacing the FFN network with a LSTM layer to make\nthe self-attention layer position aware yielded better performance.\nFollowing [27], we use convolution layers as pre-processors for the\ntransformer layer’s input and compare it with other positional en-\ncoding methods in Section 4.2. In [31], a loss function similar to\nthe iterated loss is used to enable training very deep transformers\nfor character-level LMs; we demonstrate that it is also crucial for\ntraining deep transformer-based AMs.\n4 Experiments\nTo evaluate the effectiveness of the proposed transformer-based\nacoustic model, we ﬁrst perform experiments on the Librispeech\ncorpus [32]. This corpus contains about 960 hours of read speech\ndata for training, and 4 development and test sets ( {dev, test}\n- {clean,other}), where other sets are more acoustic chal-\nlenging. No segmentation is performed for these test sets. The\nstandard 4-gram language model (LM) with a 200K vocabulary is\nused for all ﬁrst-pass decoding.\n4.1 Experiment Setups\nFollowing [24], we use context- and position-dependent graphemes\n(i.e., chenones) in all experiments. We bootstrap our HMM-GMM\nsystem using the standard Kaldi [33] Librispeech recipe. We use\n1-state HMM topology with ﬁxed self-loop and forward transition\nprobability (both 0.5). 80-dimensional log Mel-ﬁlter bank features\nare extracted with a 10ms frame shift. A reduced 20ms frame rate\nis achieved either by stacking-and-striding 2 consecutive frames or\nby a stride-2 pooling in the convolution layer if it is used. We found\nthat this not only reduces the computation but also slightly improves\nthe recognition accuracy. Speed perturbation [34] and SpecAug-\nment [10] (LD policy without time warping) are used. We focus\non cross-entropy (CE) trained models and only selectively perform\nsMBR [35] training on top of the best CE setup.\nNeural network training is performed using an in-house developed\nspeech extension of the PyTorch-based fairseq [36] toolkit. Adam\noptimizer [37] is used in all experiments; the learning rate linearly\nwarms up from 1e-5 to 1e-3 in the ﬁrst 8000 iterations and stays\nat 1e-3 during the rest of training. We mainly compare full-context\ntransformer with BLSTM in this work though we do have an initial\ninvestigation of transformers using limited right context. Dropout\nis used in all experiments: 0.1 for transformer and 0.2 for BLSTM.\nTo improve training throughput, our batch size is dynamically de-\ntermined so that we can occupy as much GPU memory as possible.\nFor most of the experiments in this work, a batch contains around\n10,000 to 20,000 frames, including padding frames. We train mod-\nels using 32 Nvidia P100 GPUs for at most 100 epochs; training is\nusually done within 4 days. We did not perform thorough architec-\nture searches for either transformer or BLSTM. For transformers, we\nmainly use a 12-layer transformer architecture with di = 768: per-\nhead dimension is always 64 and the FFN dimension is always set to\n4di. This model has about 90M parameters. For BLSTMs, we fol-\nlow [24] and consider two architectures, a 5-layer BLSTM with 800\nunits per layer per direction (about 94M parameters), and a 6-layer\nBLSTM with 1000 units (about 163M parameters)1.\nTraining transformers requires some tricks. Due to the quadrat-\nically growing computation cost with respect to the input sequence\nlength, we segment the training utterances into segments that are\nnot longer than 10 seconds 2. Though this creates a mismatch be-\ntween training and testing, preliminary results show that training on\nshorter segments not only increases the training throughput but also\nhelps the ﬁnal WERs. We also found that transformers are more\nprone to over-ﬁtting, thus require some regularization. We found\nSpecAugment [10] is effective: without it, WER starts to increase af-\nter only 3 epochs, while WER continues to improve during training\nwith SpecAugment.\nA fully-optimized, static 4-gram decoding graph is built using\nKaldi. This decoding graph is used for ﬁrst-pass decoding and n-best\ngeneration for neural LM rescoring. Test set WERs are obtained us-\ning the best model based on WER on the development set3. Follow-\n1We did not obtain further WER improvements by increasing number of\nparameters in BLSTM beyond 163M.\n2 This is achieved by aligning audio against the reference using an existing\nlatency-controlled BLSTM acoustic model.\n3We also average the last 10 epoch checkpoints to form an extra candidate.\ning [38], the best checkpoints fortest-clean and test-other\nare selected separately on the corresponding development sets 4\n4.2 Effect of Positional Embedding\nIn the ﬁrst set experiment, we investigate the effect of four positional\nembeddings (PE) methods for transformer-based acoustic models.\nIn the ﬁrst method, we stack-and-stride every 2 frames: it does not\nbreak the permutation invariance in transformers, thus denoted as\nNone. In the second method, the Sinusoid PE proposed in the orig-\ninal transformer paper [14], which encodes the absolute positional\ninformation, is used. In the third method, Frame Stacking, we stack\nthe current frame and next 8 future frames followed by a stride-\n2 sampling to form a new input sequence to transformers. Note\nthat since the stacked frames are partially overlapped with its neigh-\nboring stacked frames, the permutation invariance no longer holds.\nThis method encodes relative positional information. In the fourth\nmethod, Convolution, we use two VGG blocks [39] beneath trans-\nformer layers: each VGG block contains 2 consecutive convolution\nlayers with a 3-by-3 kernel followed by a ReLu non-linearity and\na pooling layer; 32 channels are used in the convolution layer of\nthe ﬁrst VGG block and increase to 64 for the second block. Max-\npooling is performed at a 2-by-2 grid, with stride 2 in the ﬁrst block\nand 1 in the second block. For an input sequence of 80-dim feature\nvector at a 10ms rate, this VGG network produces a 2560-dim fea-\nture vector sequence at a 20ms rate. Note that the perception ﬁeld\nof each feature vector output by the VGG network consists of 80ms\nleft-context and 80ms right context, the same right context length as\nFrame Stacking. A linear projection is used to project the feature\nvector to the dimension accepted by transformers, in this case, 768.\nTable 1: Effect of Positional Embeddings (PE) for Transformer.\nPE test-clean test-other\nNone 3.11 6.94\nSinusoid 3.13 6.67\nFrame Stacking 3.04 6.64\nConvolution 2.87 6.46\n4.3 Transformer vs. BLSTM\nIn the second set of experiments, we compare the transformer ar-\nchitecture with BLSTM. For a fair comparison, we try to build\ntransformer and BLSTM-based models using similar number of\nparameters. First we compare a BLSTM model, BLSTM(800, 5) ,\ni.e., 5 layers with 800 hidden units per layer per direction, with\nthe transformer model in row 3, Table 1, dubbed Trf-FS since it\nuses Frame Stacking. To be able to compare our best performing\ntransformer-based model with Convolution PE, we combine the\nsame VGG blocks in row 4, Table 1 with BLSTM, producing vg-\ngBLSTM(800, 5) . Lastly, with about 163M parameters, we build\nthe largest vggBLSTM model, vggBLSTM(1000,6). To match the\nnumber of parameters of this model, we increase the number of\ntransformer layers from 12 to 20. As shown in Table 2, transformer-\nbased models consistently outperform BLSTM-based models by\n2–4% on test-clean and 7–11% on test-other.\n4.4 Effect of Iterated Loss\nTable 2 shows that simply increasing the depth of transformers to\n20 layers, we obtained about 5.5% relative WER reduction (6.10 vs.\n6.46). Inspired by this, we try to increase the number of transformer\n4This is only to follow the same experimental protocol set by the prior\nwork in [38] – most of the experimental results on both test sets, including\nthe best WERs we reported in Table 4, are actually achieved by the same\nmodel.\nTable 2: Architecture comparison on the Librispeech benchmark\nModel Arch #Params (M) test-clean test-other\nBLSTM (800,5) 79 3.11 7.44\nTrf-FS (768,12) 91 3.04 6.64\nvggBLSTM (800,5) 95 2.99 6.95\nvggTrf. (768,12) 93 2.87 6.46\nvggBLSTM (1000,6) 163 2.86 6.63\nvggTrf. (768, 20) 149 2.77 6.10\nlayers further. To make the model size manageable, we use a smaller\nembedding dimension, 512, for deep transformer models. Our initial\nattempt was not successful; deep transformer models (deeper than\n20 layers) often got stuck in training and made little progress for a\nlong time. We solved the problem with the iterated loss used in [29]:\nthe output embeddings of the 6/12/18-th transformer layers are non-\nlinearly transformed (projected to a 256-dimensional space with a\nlinear transformation followed by a Relu non-linearity) and auxiliary\nCE losses are calculated separately. These additional CE losses are\ninterpolated with the original CE loss with a 0.3 weight. With this\niterated loss, we were able to train a 24-layer transformer model with\nonly 81M model parameters in decoding5 and obtain a 7% and 13%\nWER reduction on test-clean and test-other, respectively,\nover the vggTrf(768, 12) baseline.\nTable 3: Using iterated loss to train deep transformer models.\nModel Arch Iter Loss test-clean test-other\nvggTrf. (768, 12) N 2.87 6.46\n(Params: 93M) Y 2.77 6.10\nvggTrf. (512, 24) N not converged\n(Params: 81M) Y 2.66 5.64\nOn top of this vggTrf(512, 24) model, we further perform\nsMBR training and it slightly improves to 2.60% and 5.59% on\ntest-clean and test-other. We compare our results with\nsome published state-of-the-art systems on Librispeech in Table\n4: when the standard 4-gram LM is used in decoding, our sys-\ntem achieves 19% and 26% WER reduction on test-clean and\ntest-other respectively, over previous best 4-gram only hybrid\nsystem [24]6. We also built a transformer LM similar to the setup\nin [16] on the 800M text tokens provided by the Librispeech bench-\nmark and performed n-best rescoring on the ﬁrst pass decoding\noutput. To the best of our knowledge, our ﬁnal WERs (2.26/4.85)\nare state-of-the-art results on this widely used benchmark.\nTable 4: Comparison with previous best results on Librispeech.\n“4g” means the stand 4-gram LM is used; “NNLM” means a neu-\nral LM is used.\nArch. System LM test-\nclean\ntest-\nother\nLAS Park et al. [10] NNLM + 4g 2.5 5.8\nKarita et al. [30] NNLM 2.6 5.7\nHybrid\nRWTH [38] 4g 3.8 8.8\n+NNLM 2.3 5.0\nHan et al. [41] 4g 2.9 8.3\n+NNLM 2.2 5.8\nLe et al. [24] 4g 3.2 7.6\nOurs 4g 2.60 5.59\n+NNLM 2.26 4.85\n5There are 6M extra parameters only used in training.\n6Note that [24] used LC-BLSTM [40] instead of full-context BLSTM.\nTable 5: Forcing transformer models to use limited right context\n(RC) per layer during inference. Given a 12-layer transformer, an\nRC of 10 frames translates to 2.48 seconds of total lookahead.\nRC test-clean test-other\n∞ 2.87 6.45\n50 3.01 7.12\n20 3.29 8.10\n10 3.65 9.01\n4.5 Limited Right Context\nAll the transformer-based experiments so far used full context. To\nunderstand to what extent the transformer relies on future frames to\nderive embeddings for the current frames, we take the vggTrf(768,\n12) model (row 4, Table 2) and force every layer to attend to a ﬁxed\nlimited right context during inference. Interestingly, though this cre-\nates a large mismatch between training and inference, the resultant\nsystems can still yield reasonable WERs if the number of right con-\ntext frames is large enough. Note that though each layer only re-\nquires limited right context frames, the overall right context length\nis added up by the right context length of every transformer layer,\ntherefore we still end up with a large look-ahead window into the\nfuture, which makes it less possible to be used in a streaming ASR\napplication. We will investigate transformer-based acoustic models\nwith the streaming constraint in our future study.\n4.6 Large Scale Experiments\nFinally, we perform a large scale experiment on one of our internal\ntasks, English video ASR. The training set consists of 13.7K hours of\nvideos (from 941.6K video clips) shared publicly by users; only the\naudio part of those videos are used in our experiments. These data\nare completely anonymized; both transcribers and researchers do not\nhave access to any user-identiﬁable information. Due to the data\nnature, it is a very diverse and challenging task. About 9 hours (from\n620 video clips) data are held out for dev set. 3 test sets are used\nfor evaluation purpose: an 8.5-hour curated set of carefully select\nvery clean videos, an 19-hour clean set and a 18.6-hour noisy\nset. For our initial evaluation purpose, both training and test sets are\nsegmented into maximum 10 second segments.\nDue to time limit, we only built vggTrf(768, 12) without the iter-\nated loss and vggBLSTM(800, 5) on this task. Table 6 shows that on\nthis task, the proposed transformer-based acoustic model outperform\nvggBLSTM by 4.0-7.6%. We will report more results in our future\nwork.\nTable 6: Experiment results on our internal English video ASR task.\nModel curated clean noisy\nvggBLSTM(800,5) 10.72 15.97 22.13\nvggTrf(768,12) 9.90 15.26 21.25\n5 Discussions And Conclusions\nIn this work, we proposed and evaluated transformer-based acous-\ntic models for hybrid speech recognition. A couple of model mod-\neling choices are discussed and compared. We demonstrated that\ntransformer can signiﬁcantly outperforms BLSTM and give the best\nacoustic models on Librispeech benchmark. Initial study on a much\nlarger and more challenging dataset also conﬁrms our ﬁndings.\nThere are many works we are yet to explore. For example, our\nexperiments did not show to what extent transformer’s superior per-\nformance comes from replacing recurrence with self-attention, while\nother modeling techniques from transformer can be borrowed to im-\nprove RNNs as well [42]. The quadratically growing cost with re-\nspect to the length of speech signals is still a major blocker for\ntransformer-based acoustic models to be used in practice. These\nquestions will be studied in our future work.\n6 References\n[1] G. Hinton, L. Deng, D. Yu, et al., “Deep neural networks for acoustic\nmodeling in speech recognition,” IEEE Signal processing magazine ,\nvol. 29, 2012.\n[2] F. Seide, G. Li, and D. Yu, “Conversational speech transcription using\ncontext-dependent deep neural networks,” in Proc. Interspeech, 2011.\n[3] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory recur-\nrent neural network architectures for large scale acoustic modeling,” in\nProc. Interspeech, 2014.\n[4] O. Abdel-Hamid, A. Mohamed, H. Jiang, et al., “Convolutional neural\nnetworks for speech recognition,” IEEE/ACM Transactions on audio,\nspeech, and language processing, vol. 22, no. 10, pp. 1533–1545, 2014.\n[5] V . Peddinti, D. Povey, and S. Khudanpur, “A time delay neural network\narchitecture for efﬁcient modeling of long temporal contexts,” in Proc.\nInterspeech, 2015.\n[6] S. Zhang, H. Jiang, S. Wei, and L. Dai, “Feedforward sequential\nmemory neural networks without recurrent feedback,” arXiv preprint\narXiv:1510.02693, 2015.\n[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[8] D. Bahdanau, J. Chorowski, D. Serdyuk, et al., “End-to-end attention-\nbased large vocabulary speech recognition,” in Proc. ICASSP, 2016.\n[9] C.-C. Chiu, T.N. Sainath, Y . Wu, et al., “State-of-the-art speech recog-\nnition with sequence-to-sequence models,” in Proc. ICASSP, 2018.\n[10] D. S. Park, W. Chan, Y . Zhang, et al., “Specaugment: A simple\ndata augmentation method for automatic speech recognition,” arXiv\npreprint arXiv:1904.08779, 2019.\n[11] Y . He, T. N. Sainath, R. Prabhavalkar, et al., “Streaming end-to-end\nspeech recognition for mobile devices,” in Proc. ICASSP, 2019.\n[12] Y . Bengio, P. Simard, et al., “Learning long-term dependencies with\ngradient descent is difﬁcult,” IEEE transactions on neural networks ,\nvol. 5, no. 2, pp. 157–166, 1994.\n[13] R. Collobert, C. Puhrsch, and G. Synnaeve, “Wav2letter: an end-\nto-end convnet-based speech recognition system,” arXiv preprint\narXiv:1609.03193, 2016.\n[14] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,”\nin Proc. NIPS, 2017, pp. 5998–6008.\n[15] J. Devlin, M.-W. Chang, K. Lee, et al., “Bert: Pre-training of deep\nbidirectional transformers for language understanding,” arXiv preprint\narXiv:1810.04805, 2018.\n[16] A. Radford, K. Narasimhan, Tim S., et al., “Improving language un-\nderstanding by generative pre-training,” 2018.\n[17] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,” inProc. ICASSP,\n2018.\n[18] M. Sperber, J. Niehues, G. Neubig, et al., “Self-attentional acoustic\nmodels,” arXiv preprint arXiv:1803.09519, 2018.\n[19] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence-to-\nsequence speech recognition with the transformer in mandarin Chi-\nnese,” arXiv preprint arXiv:1804.10752, 2018.\n[20] D. Povey, Hossein Hadian, P. Ghahremani, et al., “A time-restricted\nself-attention layer for asr,” in Proc. ICASSP, 2018, pp. 5874–5878.\n[21] J. Salazar, K. Kirchhoff, and Z. Huang, “Self-attention networks for\nconnectionist temporal classiﬁcation in speech recognition,” in Proc.\nICASSP, 2019, pp. 7115–7119.\n[22] H. A. Bourlard and N. Morgan, Connectionist speech recognition: a\nhybrid approach, vol. 247, Springer Science & Business Media.\n[23] M.-Y . Hwang and X. Huang, “Subphonetic modeling with markov\nstates-senone,” in Proc. ICASSP, 1992, vol. 1, pp. 33–36.\n[24] D. Le, X. Zhang, W. Zheng, et al., “From senones to chenones: Tied\ncontext-dependent graphemes for hybrid speech recognition,” arXiv\npreprint arXiv:1910.01493, 2019.\n[25] J. Lei Ba, J. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[26] D. Hendrycks and K. Gimpel, “Gaussian error linear units (gelus),”\narXiv preprint arXiv:1606.08415, 2016.\n[27] A. Mohamed, D. Okhonko, and L. Zettlemoyer, “Transformers with\nconvolutional context for asr,”arXiv preprint arXiv:1904.11660, 2019.\n[28] Y . Zhang, W. Chan, and N. Jaitly, “Very deep convolutional networks\nfor end-to-end speech recognition,” in Proc. ICASSP, 2017, pp. 4845–\n4849.\n[29] A. Tjandra, C. Liu, F. Zhang, et al., “Deja-vu: Double feature pre-\nsentation and iterated loss in deep transformer networks,” To appear\nICASSP, 2020.\n[30] S. Karita, N. Chen, T. Hayashi, et al., “A Comparative Study\non Transformer vs RNN in Speech Applications,” arXiv preprint\narXiv:1909.06317, 2019.\n[31] R. Al-Rfou, D. Choe, N. Constant, et al., “Character-level language\nmodeling with deeper self-attention,” in Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, 2019, vol. 33, pp. 3159–3166.\n[32] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:\nan asr corpus based on public domain audio books,” in Proc. ICASSP,\n2015, pp. 5206–5210.\n[33] D. Povey, A. Ghoshal, G. Boulianne, et al., “The kaldi speech recogni-\ntion toolkit,” in IEEE Workshop on Automatic Speech Recognition and\nUnderstanding, 2011.\n[34] T. Ko, V . Peddinti, D. Povey, et al., “Audio augmentation for speech\nrecognition,” in Proc. Interspeech, 2015.\n[35] K. Vesel `y, A. Ghoshal, L. Burget, and D. Povey, “Sequence-\ndiscriminative training of deep neural networks.,” inProc. Interspeech,\n2013, vol. 2013, pp. 2345–2349.\n[36] O. Myle, E. Sergey, B. Alexei, F. Angela, et al., “fairseq: A Fast,\nExtensible Toolkit for Sequence Modeling,” inProceedings of NAACL-\nHLT 2019: Demonstrations, 2019.\n[37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980, 2014.\n[38] C. L ¨uscher, E. Beck, K. Irie, et al., “RWTH ASR Systems for Lib-\nriSpeech: Hybrid vs Attention-w/o Data Augmentation,”arXiv preprint\narXiv:1905.03072, 2019.\n[39] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.\n[40] Y . Zhang, G. Chen, D. Yu, et al., “Highway long short-term memory\nRNNs for distant speech recognition,” in Proc. ICASSP. IEEE, 2016,\npp. 5755–5759.\n[41] K. J. Han, R. Prieto, K. Wu, and T. Ma, “State-of-the-art speech recog-\nnition using multi-stream self-attention with dilated 1d convolutions,”\narXiv preprint arXiv:1910.00716, 2019.\n[42] M. Chen, O. Firat, A. Bapna, et al., “The best of both worlds: Com-\nbining recent advances in neural machine translation,” arXiv preprint\narXiv:1804.09849, 2018."
}