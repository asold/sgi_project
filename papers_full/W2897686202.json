{
  "title": "Universal Language Model Fine-Tuning with Subword Tokenization for Polish",
  "url": "https://openalex.org/W2897686202",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288916081",
      "name": "Czapla, Piotr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290221552",
      "name": "Howard, Jeremy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3030930279",
      "name": "Kardas, Marcin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W2767321762",
    "https://openalex.org/W2757554342",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2795285343",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W172230311",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2775005957",
    "https://openalex.org/W2951672049"
  ],
  "abstract": "Universal Language Model for Fine-tuning [arXiv:1801.06146] (ULMFiT) is one of the first NLP methods for efficient inductive transfer learning. Unsupervised pretraining results in improvements on many NLP tasks for English. In this paper, we describe a new method that uses subword tokenization to adapt ULMFiT to languages with high inflection. Our approach results in a new state-of-the-art for the Polish language, taking first place in Task 3 of PolEval'18. After further training, our final model outperformed the second best model by 35%. We have open-sourced our pretrained models and code.",
  "full_text": "arXiv:1810.10222v1  [cs.CL]  24 Oct 2018\nUniversal Language Model Fine-T uning with\nSubword T okenization for Polish\nPiotr Czapla 1, Jeremy Howard 2, and Marcin Kardas 1\n1 n-waves, W rocław\npiotr.czapla@n-waves.com, marcin.kardas@n-waves.com\n2 fast.ai\nUniversity of San F rancisco\nj@fast.ai\nAbstract. Universal Language Model for Fine-tuning [ 6] (ULMFiT) is\none of the ﬁrst NLP methods for eﬃcient inductive transfer le arning.\nUnsupervised pretraining results in improvements on many N LP tasks\nfor English. In this paper, we describe a new method that uses subword\ntokenization to adapt ULMFiT to languages with high inﬂecti on. Our\napproach results in a new state-of-the-art for the Polish la nguage, taking\nﬁrst place in T ask 3 of PolEval’18. After further training, o ur ﬁnal model\noutperformed the second best model by 35%. W e have open-sour ced our\npretrained models and code. 3\n1 Introduction\nLanguage Modeling recently gained in importance as it is bei ng used as a base\nfor transfer learning in multiple supervised tasks, obtain ing impressive improve-\nments over state-of-the-art [\n6, 11, 13]. F or example the error in text classiﬁcation\ntasks was reduced by 18% – 24% [ 6]. More complex tasks like commonsense rea-\nsoning and question answering were signiﬁcantly improved b y applying transfer\nlearning from a Language Model [ 13]. Use of unsupervised learning and transfer\nlearning has the additional beneﬁts of greatly reduced comp uting time and data\nrequirements for downstream supervised tasks. In some case s data requirements\nwere reduced by 100 times [ 6].\nUse of transfer learning is even more important for language s such as Polish,\nwhere access to large supervised data sets is very limited. M ost of the language\nmodels published to date are n-gram models, that do not allow for transfer\nlearning and are very memory hungry .\n1.1 Our contribution\nW e adapt Universal Language Model Fine-T uning (ULMFiT) [\n6] to handle Polish\ninﬂection with subword tokenization using SentencePiece [ 8]. W e trained multiple\nmodels on the PolEval 2018 LM dataset. Our best model achieve d a perplexity\n3 https://n-waves.com/poleval2018, http://nlp.fast.ai\nof 117.7 on the test set, resulting in ﬁrst place in the compet ition (second place\nscored a perplexity of 146.7). With further tuning after the competition of the\nmodel’s hyperparameters, we lowered the perplexity to 95.0 .\nW e hope to see the use of F astT ext [ 2] as the most common way of rep-\nresenting text in Polish replaced with our combination of Se ntencePiece and\nULMFiT.\n2 Related W ork\nLanguage models traditionally were approximated with non- parametric mod-\nels based on counting statistics. This were recently replac ed with deep neural\nnetwork for popular languages like English. However most of the literature de-\nvoted to the Polish language considers n-gram models [\n12, 18, 19, 21]. Brocki et\nal. [ 4] showed that a simple neural network (5 context words with 50 dimensional\nembeddings and one hidden layer) greatly outperforms a 4-gr am solution on a\nPolish corpus. Regardless of performance, the n-gram model s tend to be large\n(several dozens gigabytes for 5-gram [ 19]), making their use in web or mobile\napplications infeasible. F or comparison, our best perform ing model is around\n150 MB without compression. Moreover non-parametric model s do not allow for\ntransfer learning, which is the key to good performance on ma ny NLP tasks.\nNatural language processing tasks show the best performanc e when transfer\nlearning is applied either from an LSTM language model [ 6, 11] or from self-\nattention language models [ 13].\nThe latter may hold the most promise as has been shown to work w ell on\nadvanced NLP tasks like question answering, however, it is h ard to train and\nrequires extensive computing power and time [ 1]. Therefore, we decided to ﬁrst\nadopt an LSTM based model for Polish.\nLSTMs are the most widely used RNNs. Recent state of the art pe rformance\nof language models can be tracked to Merity et al. [ 9], who propose a way\nto eﬃciently use dropout in LSTM networks as well as other reg ularization and\nperformance techniques like averaged stochastic gradient descent, or randomized-\nlength backpropagation through time (BPTT). This work was l ater extended to\ntransfer learning and classiﬁcation by [ 6]. T ransfer learning in language modeling\nwas shown to beneﬁt from slanted triangular learning rates a nd other techniques\ndescribed by [ 17], originally used to quickly train computer vision models w ith\nminimal resources.\nLSTM based language models can be improved with use of adapti ve methods\nduring inference (neural cache [ 5] and dynamic evaluation [ 7]). Both methods\ndepend on observing sequence elements after they are predic ted in order to per-\nform adaptation. As our Polish language model is intended fo r transfer learning\nand not just the language modeling, we intentionally ignore d any approaches\nthat do not beneﬁt downstream tasks.\nA few papers investigate using some more sophisticated acti vation functions\nfor the output layer (e.g., mixture of softmaxes [ 20] and Hebbian softmax [ 14]).\nThe use of mixture of softmaxes has been criticized for large computing and\nmemory requirements. Whilst Hebbian softmax is a new work th at holds a\npromise for a better language model for downstream tasks, it requires signif-\nicant computing power. Their models where trained for 6 days with 8 P100s,\nwhile ULMFiT can be trained in around 6 to 10 hours on one P100.\nULMFiT’s approach [ 6] contributes a number of training tactics that allow\nfor inexpensive training of language models. It introduced a successful approach\nto transfer learning and ﬁne-tuning for NLP tasks. W e select ed it as our base for\npractical reasons such as small memory footprint, quick tra ining time and the\ndirect applicability to other downstream tasks like sentim ent analysis.\nA popular approach to transfer learning explored earlier in NLP was word\nembeddings. They appear in the Polish NLP space in form of wor d2vec [ 10, 15]\nand F astT ext [ 2]. However this approach only pretrains the ﬁrst layer of a mo del,\nwhich greatly limits its eﬀectiveness.\nAll of the word embeddings before F astT ext were hindered by t he inﬂection\nof the Polish language, which renders most approaches to ﬁnd ing embeddings\nfor full words incapable of learning useful features. The mo st successful attempt\nwas F astT ext, which uses pieces of words.\nAnother approach to address inﬂections in Polish is to use by te pair en-\ncoding [ 16], character level language models [ 11] or unigram subword tokeniza-\ntion [ 8]. W e used the unigram algorithm as its representation of Pol ish words\nmost closely ﬁtted the training pipeline of ULMFiT, and beca use it has shown\nstate of the art performance in downstream tasks such as mach ine translation.\n3 Model\n3.1 Dataset\nOur language model was trained only on PolEval 2018 LM data\n4. A summary\nof the datasets is presented in T able 1.\nThe vocabulary is created from all tokens appearing at least 3 times in the\ntraining data, yielding a vocabulary of 1 .38 M tokens.\nT able 1.Summary of PolEval 2018 LM datasets. The tokens denoting beg inning and\nend of sentence are not included.\ndataset sentences tokens OOV rate\ntrain 23.0 M 451.8 M 0.73%\ntrain (dedup.) 21.3 M 423.9 M 0.78%\ntest 2.6 M 50.2 M 0.91%\ntest (dedup.) 2.4 M 48.6 M 0.94%\n4 https://n-waves.com/poleval2018/competition - the url to the competition will\nmost likely change in 2019 so here is an up to date redirection .\nP (· | <s>) P (· | <s> Bez) P (· | <s> Bezbar) · · · · · · · · ·\nLSTM3 LSTM3 LSTM3 LSTM3 LSTM3 LSTM3\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nLSTM0 LSTM0 LSTM0 LSTM0 LSTM0 LSTM0\n<s> Bez bar wn e zielone\nFig. 1. An ULMFiT architecture with 4 recurrent layers.\n3.2 Subword tokenization\nSimilarly as in the requirements of the competition, ULMFiT represents tokens\nusing a ﬁxed-size vocabulary . T okens not in the vocabulary a re replaced by the\nspecial <unk> token. However, by mapping tokens to integer identiﬁers we g et\nrid of information regarding words structure. As a result, a language model\noperating on full words needs much more data to learn rules of highly inﬂected\nlanguages like Polish.\nOne of solutions to this problem is to use a character level mo del [\n1, 7]. Com-\npared to word-based models, character level models are larg er and require higher\ncomputational costs to achieve the same performance [ 3]. T o gain the advantages\nof both approaches we trained a model working on parts of word s. The subword\nvocabulary is created by training a SentencePiece 5 tokenization model. W e use\na unigram segmentation algorithm [ 8]. T able 2 shows an example of subword\ntokenization of a sentence for various vocabulary sizes. An important property\nof SentencePiece tokenization, necessary for us to obtain a valid word-based lan-\nguage model, is its reversibility . W e do not use subword regu larization as we\ndecided that the available training dataset is large enough to avoid overﬁtting.\n5 https://github.com/google/sentencepiece\nT able 2. An example split of sentence \"Bezbarwne zielone idee wściek le śpią .\" (Col-\norless green ideas sleep furiously) into subword tokens usi ng SentencePiece models\ndiﬀering by vocabulary sizes. Ratio denotes an average numb er of subword tokens used\nto encode an input token. The bottom part of the table was obta ined by applying a\nlowercasing preprocessing step (see Section 3.3).\n|V | ratio\n4k 1.90 B e z bar w ne zielon e i de e w ści ek le ś pi ą .\n8k 1.67 Bez bar w ne zielon e ide e w ści ek le ś pi ą .\n25k 1.42 Bez bar wn e zielone ide e w ście kle śpi ą .\n50k 1.34 Bez bar wne zielone idee w ście kle śpi ą .\n100k 1.29 Bez bar wne zielone idee w ście kle śpią .\n4k 2.04 <up> bez bar w ne zielon e i de e w ści ek le ś pi ą .\n8k 1.83 <up> bez bar w ne zielone ide e w ści ek le ś pi ą .\n25k 1.61 <up> bezbarwn e zielone ide e w ście kle śpi ą .\n50k 1.53 <up> bezbarwn e zielone idee w ście kle śpi ą .\n100k 1.49 <up> bezbarwne zielone idee w ście kle śpią .\nW e now present a formal justiﬁcation of our approach. F or a mu ltiset of\nsentences S = {s1, . . . , sN } and LM q : W ∗ → [0, 1], the empirical perplexity per\ntoken is given by\npplS (q)\ndef\n= 2 H( ˜p,q )/ Es∼ ˜p (|s|W ) ,\nwhere\nH ( ˜p, q) = − 1\nN\n∑\ns∈ S\nlg q(s)\nis an empirical cross-entropy and\nEs∼ ˜p (|s|W ) = 1\nN\n∑\ns∈ S\n|s|W\nis the average sentence length (in tokens).\nLet F : W ∗ 1:1\n− − →V ∗ be a one-to-one mapping from sentences/sequences over\ntokens in W into sequences over tokens in V . Having a LM qV : V ∗ → [0, 1] we\ncan create a LM qW : W ∗ → [0, 1] with qW (s) = qV (F (s)). F being injective\nguarantees that Z\ndef\n= ∑\nw∈ W ∗ qW (s) ≤ 1. T o make qW a valid distribution\nwe could normalize it by Z (computing of which could be infeasible) or simply\nassume that qW (#) = 1 − Z for some additional symbol # /∈ W (and 0 for any\nother sequence containing #). With ( F ◦ qV )(s)\ndef\n= qV (F (s)) we have\nlg (ppl S (qW )) = lg (ppl S (F ◦ qV )) = H ( ˜p, F ◦ qV )\nEs∼ ˜p (|s|W )\n= H ( ˜p, F ◦ qV )\nEs∼ ˜p (|F (s)|V ) · Es∼ ˜p (|F (s)|V )\nEs∼ ˜p (|s|W ))\n= lg\n(\npplF (S) (qV )\n)\n· Es∼ ˜p (|F (s)|V )\nEs∼ ˜p (|s|W )\nor equivalently\npplS (qW ) = (ppl F (S) (qV ))Es∼ ˜p (|F (s)|V )/ Es∼ ˜p (|s|W ) . (1)\nIn our case, W consists of 3 control tokens ( <unk>, <s> and </s>) and\n1 378 027 tokens 6 occurring 3 or more times in the training data. V is constructed\nby unigram model [ 8] using SentencePiece subword tokenizer and consists of 4\ncontrol tokens (additional <pad> token) and 24 996 subword tokens. F or any\nsentence s ∈ W ∗ we use the most probable tokenization as F (s). T o get even\nbetter results we could sum over all possible splits of s. W e believe, however,\nthat the normalization factor Z can be neglected as model should learn to ignore\nnon-existent words or alternative tokenizations.\n3.3 Universal Language Model Fine-tuning\nOur model is based on the fast.ai\n7 implementation of ULMFiT. T able 3 gives\ndetails of our ﬁnal submission as well as the best model train ed after the com-\npetition.\nData preprocessing Our preprocessing pipeline for the training data starts\nwith counting occurrences of word tokens and extracting a di ctionary consisting\nof words with at least 3 occurrences. The tokenized ﬁle is the n deduplicated.\nDuring development we experimented with an optional step of encoding\nwords with an initial letter being the only capital letter. S uch words are pre-\nceded with a special <up> token and the initial letter is lower-cased (see T able\n2\nfor an example). However, the experiments showed that there is no signiﬁcant\ndiﬀerence.\nAfter the deduplication (and optional lower-casing) the fu ll dataset is used to\ntrain a SentencePiece unigram model. The dictionary extrac ted in the ﬁrst step is\nused to remove rare (i.e., out-of-vocabulary) word tokens. The resulting sentences\nare encoded by the SentencePiece model. Due to large size of t he training dataset\n6 Even though not all tokens in PolEval datasets are words (e.g ., there are tokens\nconsisting of punctuation marks) and some tokens produced b y SentencePiece are\nvalid words, for simplicity we call the former word tokens and the later subword\ntokens.\n7 http://nlp.fast.ai/\nT able 3. Details of our submission and the best model trained after co mpetition.\nPolEval submission tuned model\nvocabulary size 50K 25 K\nRNN type LSTM\nrecurrent layers 4\nembeddings dimension 400\nhidden state dimension 1150\ntraining time 18 epochs 30 epochs\ndata set used for training ≈ 25% 100%\nbatch size 192 128\nsampled softmax 15 K samples no\ntext transforms none\nperplexity 117.8 95.0\nwe do not use subword regularization – each sentence is token ized only once\nwith the best encoding. The ﬁnal dataset is randomly shuﬄed a nd split into a\nvalidation dataset (around 10 million subword tokens) and a training dataset.\nF or the test dataset we optionally perform a lower-casing st ep, remove the\nout-of-vocabulary words and encode word tokens into subwor d tokens with Sen-\ntencePiece model. The deduplication step ensures that trai ning and validation\nsets are disjoint. However, because the test and the trainin g datasets share some\nsentences (around 0.23 M / 9.29% test sentences are present i n the training\ndataset), the cross validation perplexity was always highe r than the test one.\n4 Experiments\nW e run multiple experiments on around 10 M subword tokens of d ata to gain\nan intuition on how to tune ULMFiT hyperparameters for best p erformance on\nthe Polish language. Most promising solutions were trained further on the whole\ntraining set, and the best (based on validation perplexity) was selected. In this\nSection we present our ﬁndings regarding tuning various hyp erparameters of the\nULMFiT model.\n4.1 Results\nV ocabulary size Our experiments showed that out of all tested hyperparam-\neters, the vocabulary size has the greatest impact on model p erformance on\nPolish language. Unlike the English ULMFiT on full words, ou r vocabulary size\ninﬂuences how the subword tokens are formed. F or a large enou gh vocabulary\ntwo words with the same lemma are represented as two diﬀerent ids, and the\nsimilarity information is lost. The smaller the vocabulary , the closer we get to\ncharacter level models.\nFig. 2. Plot showing an impact of number of recurrent layers and voca bulary size on\nvalidation perplexity . Models trained for 12 epochs on a sma ll dataset consisting of\naround 10 M subword tokens. Models with vocabulary of size 25 K and more were\ntrained with sampled softmax (with 15K samples).\n4K 8K 12K 16K 20K 25K 50K 100K\n300\n400\n500\n600\n700\nV ocabulary size\nPerplexity on small dataset\n3 layers 4 layers 5 layers\nNumber of recurrent layers W e tested our models with 3, 4 and 5 recurrent\nlayers. Each additional layer noticeably increases memory usage of model and\ntime necessary for a single training epoch. On a small traini ng dataset the 5-layer\nmodels performed signiﬁcantly worse. W e do not know whether longer training,\nmore data or subword regularization could improve the perfo rmance relative\nto smaller models. The performance of 3-layer and 4-layer mo dels were almost\nidentical on a small dataset, but training on the full datase t proved that the\nlatter is more capable, and achieves lower validation perpl exity .\nT ext preprocessing In some experiments we applied lower-casing of the initial\nletter of each word. T o make the transform reversible, such w ords were preceded\nby <up> control token (see T able 2). F or most of the tested vocabulary sizes\nand number of layers there was no noticeable diﬀerence in per plexity , with an\nexception of 100 K tokens, where lower-casing resulted in de graded performance.\n5 Final Remarks\nW e showed that a subword tokenization can be used to achieve a high-performing\nlanguage model for Polish, a morphologically rich language . The presented model\nachieves state-of-the-art perplexity . However, we did not use the main advantage\nof ULMFiT, i.e., its ability for transfer learning. The natu ral next steps are\nto implement custom heads for common NLP tasks (named entity recognition,\nsentiment analysis) with a pretrained ULMFiT model as a back bone.\nBibliography\n[1] Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: C haracter-level lan-\nguage modeling with deeper self-attention. arXiv preprint arXiv:1808.04444\n(2018)\n[2] Bojanowski, P ., Grave, E., Joulin, A., Mikolov, T.: Enri ching word vectors\nwith subword information. T ransactions of the Association for Computa-\ntional Linguistics 5, 135–146 (2017)\n[3] Bojanowski, P ., Joulin, A., Mikolov, T.: Alternative st ructures for character-\nlevel rnns. CoRR abs/1511.06303 (2015)\n[4] Brocki, Ł., Marasek, K., Koržinek, D.: Connectionist la nguage model for\npolish. In: Intelligent T ools for Building a Scientiﬁc Info rmation Platform,\npp. 243–250. Springer (2012)\n[5] Grave, E., Joulin, A., Usunier, N.: Improving neural lan guage models with\na continuous cache. arXiv preprint arXiv:1612.04426 (2016 )\n[6] Howard, J., Ruder, S.: Universal language model ﬁne-tun ing for text classi-\nﬁcation. In: Proceedings of the 56th Annual Meeting of the As sociation for\nComputational Linguistics (V olume 1: Long Papers). pp. 328 –339. Associ-\nation for Computational Linguistics (2018)\n[7] Krause, B., Kahembwe, E., Murray , I., Renals, S.: Dynami c evaluation of\nneural sequence models. arXiv preprint arXiv:1709.07432 ( 2017)\n[8] Kudo, T.: Subword regularization: Improving neural net work translation\nmodels with multiple subword candidates. In: Proceedings o f the 56th An-\nnual Meeting of the Association for Computational Linguist ics (V olume 1:\nLong Papers). pp. 66–75. Association for Computational Lin guistics (2018)\n[9] Merity , S., Keskar, N.S., Socher, R.: Regularizing and o ptimizing LSTM\nlanguage models. In: International Conference on Learning Representations\n(2018)\n[10] Mykowiecka, A., Marciniak, M., Rychlik, P .: T esting wo rd embeddings for\npolish. Cognitive Studies (17) (2017)\n[11] Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clar k, C., Lee, K.,\nZettlemoyer, L.: Deep contextualized word representation s. In: Proc. of\nNAACL (2018)\n[12] Pohl, A., Ziółko, B.: Using part of speech n-grams for im proving automatic\nspeech recognition of polish. In: Perner, P . (ed.) Machine L earning and Data\nMining in Pattern Recognition. pp. 492–504. Springer Berli n Heidelberg,\nBerlin, Heidelberg (2013)\n[13] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving lan-\nguage understanding by generative pre-training (2018)\n[14] Rae, J.W., Dyer, C., Dayan, P ., Lillicrap, T.P .: F ast pa rametric learning\nwith activation memorization. CoRR abs/1803.10049 (2018)\n[15] Rogalski, M., Szczepaniak, P .S.: W ord embeddings for t he polish language.\nIn: International Conference on Artiﬁcial Intelligence an d Soft Computing.\npp. 126–135. Springer (2016)\n[16] Sennrich, R., Haddow, B., Birch, A.: Neural machine tra nslation of rare\nwords with subword units. In: Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics, ACL 2016 , August 7-12,\n2016, Berlin, Germany , V olume 1: Long Papers (2016)\n[17] Smith, L.N.: Cyclical learning rates for training neur al networks. In: Ap-\nplications of Computer Vision (W ACV), 2017 IEEE Winter Conf erence on.\npp. 464–472. IEEE (2017)\n[18] Smywiński-Pohl, A., Ziółko, B.: Application of morpho syntactic and class-\nbased language models in automatic speech recognition of po lish. Interna-\ntional Journal on Artiﬁcial Intelligence T ools 25(02) (201 6)\n[19] W ołk, K., W ołk, A., Marasek, K.: Big data language model of contemporary\npolish. In: Computer Science and Information Systems (F edC SIS), 2017\nF ederated Conference on. pp. 389–395. IEEE (2017)\n[20] Y ang, Z., Dai, Z., Salakhutdinov, R., Cohen, W.W.: Brea king the\nsoftmax bottleneck: A high-rank rnn language model. arXiv p reprint\narXiv:1711.03953 (2017)\n[21] Ziółko, B., Skurzok, D.: N-grams model for polish. Spee ch and language\ntechnologies pp. 107–127 (2011)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8352516889572144
    },
    {
      "name": "Lexical analysis",
      "score": 0.828283429145813
    },
    {
      "name": "Inflection",
      "score": 0.7904808521270752
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6630151271820068
    },
    {
      "name": "Natural language processing",
      "score": 0.6517724990844727
    },
    {
      "name": "Language model",
      "score": 0.5589748024940491
    },
    {
      "name": "Task (project management)",
      "score": 0.5499821901321411
    },
    {
      "name": "Word (group theory)",
      "score": 0.5089340209960938
    },
    {
      "name": "Code (set theory)",
      "score": 0.48052486777305603
    },
    {
      "name": "Programming language",
      "score": 0.2823513448238373
    },
    {
      "name": "Linguistics",
      "score": 0.23618078231811523
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 8
}