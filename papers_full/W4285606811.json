{
  "title": "Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention",
  "url": "https://openalex.org/W4285606811",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1976211542",
      "name": "Kai Liu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2137234310",
      "name": "Tianyi Wu",
      "affiliations": [
        "National Engineering Laboratory of Deep Learning Technology and Application",
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2068681617",
      "name": "Cong Liu",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2116438665",
      "name": "Guodong Guo",
      "affiliations": [
        "Baidu (China)",
        "National Engineering Laboratory of Deep Learning Technology and Application"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3174402370",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W6847742374",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W4389977645",
    "https://openalex.org/W4312599212",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3165150763",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W4312349930"
  ],
  "abstract": "Recently, Transformers have shown promising performance in various vision tasks. To reduce the quadratic computation complexity caused by each query attending to all keys/values, various methods have constrained the range of attention within local regions, where each query only attends to keys/values within a hand-crafted window. However, these hand-crafted window partition mechanisms are data-agnostic and ignore their input content, so it is likely that one query maybe attend to irrelevant keys/values. To address this issue, we propose a Dynamic Group Attention (DG-Attention), which dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group. Our DG-Attention can flexibly model more relevant dependencies without any spatial constraint that is used in hand-crafted window based attention. Built on the DG-Attention, we develop a general vision transformer backbone named Dynamic Group Transformer (DGT). Extensive experiments show that our models can outperform the state-of-the-art methods on multiple common vision tasks, including image classification, semantic segmentation, object detection, and instance segmentation.",
  "full_text": "Dynamic Group Transformer: A General Vision Transformer Backbone with\nDynamic Group Attention\nKai Liu1∗†\n, Tianyi Wu2,3 ∗\n, Cong Liu1 ‡\n, Guodong Guo2,3 ‡\n1Sun Yat-sen University, Guangzhou, China\n2Institute of Deep Learning, Baidu Research, Beijing, China\n3National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China\nliuk95@mail2.sysu.edu.cn, liucong3@mail.sysu.edu.cn, {wutianyi01, guoguodong01}@baidu.com\nAbstract\nRecently, Transformers have shown promising per-\nformance in various vision tasks. To reduce\nthe quadratic computation complexity caused by\neach query attending to all keys/values, various\nmethods have constrained the range of attention\nwithin local regions, where each query only at-\ntends to keys/values within a hand-crafted win-\ndow. However, these hand-crafted window par-\ntition mechanisms are data-agnostic and ignore\ntheir input content, so it is likely that one query\nmaybe attends to irrelevant keys/values. To ad-\ndress this issue, we propose a Dynamic Group\nAttention (DG-Attention), which dynamically di-\nvides all queries into multiple groups and selects\nthe most relevant keys/values for each group. Our\nDG-Attention can flexibly model more relevant de-\npendencies without any spatial constraint that is\nused in hand-crafted window based attention. Built\non the DG-Attention, we develop a general vi-\nsion transformer backbone named Dynamic Group\nTransformer (DGT). Extensive experiments show\nthat our models can outperform the state-of-the-\nart methods on multiple common vision tasks, in-\ncluding image classification, semantic segmenta-\ntion, object detection, and instance segmentation.\n1 Introduction\nRecently, Transformer has shown a great potential for various\nvision tasks [Dosovitskiy et al., 2020; Touvron et al., 2020;\nLiu et al., 2021b; Dong et al., 2021]. The pioneer Vision\nTransformer [Dosovitskiy et al., 2020] (ViT) stacked mul-\ntiple Transformer blocks to process non-overlapping image\npatch (i.e., visual token) sequences for image classification.\nHowever, the global self-attention in Transformer makes each\nquery attend to all keys, which has the quadratic complex-\nity to sequence length, and results in expensive computation\ncosts and memory usage, especially for high-resolution im-\nages.\n∗Equal contribution\n†Interns at the Institute of Deep Learning, Baidu Research\n‡Corresponding author\nFigure 1: Illustrate our Dynamic Group Attention in comparison\nwith other attention mechanisms in Transformer backbones. (a)\nGlobal self-attention, each query attends to all keys/values. (b)∼ (e)\nWindow-based attentions, each query attends to keys/values within\na fixed window. (d) Dynamic group attention, all queries are dynam-\nically divided into several groups, and each query attends to relevant\nkeys/values only.\nTo improve the efficiency of the global self-attention, the\nstate-of-the-art methods [Liu et al., 2021b; Huang et al.,\n2021; Dong et al., 2021; Fang et al., 2021] focused on how\nto divide the global image into multiple local regions (or win-\ndows). Each query only attends to a few keys within the man-\nually designed local regions (windows). For example, Swin\nTransformer [Liu et al., 2021b] computed the self-attention\nwithin each local window and employed a shifted mechanism\nto make cross-window connections (Figure 1 (b)). Differ-\nent from Swin Transformer, CSwin [Dong et al., 2021] pro-\nposed cross-shaped window self-attention for computing at-\ntention in the horizontal and vertical stripes in parallel that\nform a cross-shaped window (Figure 1 (c)). Shuffle Trans-\nformer [Huang et al., 2021] presented a spatial shuffle opera-\ntion to make information flow across windows (Figure 1 (d)).\nMSG-Transformer [Fang et al., 2021] proposed to compute\nattention in local regular windows, and used additional MSG\ntokens to get connections between them (Figure 1 (e)). These\nwindow-based methods achieved excellent performances and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1187\nwere superior to the CNN counterparts, however, they rely\non hand-crafted window partition mechanisms. Furthermore,\nthese partition methods are data-agnostic and ignore the input\ncontent, as a result, it is likely that one query maybe attends\nto irrelevant keys/values.\nTo address the issues mentioned above, a good idea is to\ndynamically select relevant keys/values for each query. How-\never, it leads to unreasonably high memory usage and com-\nputation complexity. We propose dynamic group attention\n(DG-Attention), which dynamically divides all queries into\nmultiple groups and selects the most relevant keys/values for\neach group. Specifically, the input visual tokens (or feature\nvectors) are divided adaptively into multiple groups accord-\ning to their similarity to all cluster centroids. Therefore, such\na partition mechanism is adaptive to input images. Then, we\nuse the cluster centroid of each group to select the most rel-\nevant keys/values subset from the whole keys/values set, and\nthe self-attention is conducted within each group. It enables\nour model to focus on relevant keys/values without any spa-\ntial constraint. And also, through the dynamic grouping, our\nDG-Attention does not cause a high memory usage or large\ncomputation cost. For example, as shown in Figure 1 (f),\nthe red point (query) can attend to its relevant region denoted\nwith red solid line boundaries, and the blue point can attend to\nthe regions with blue solid line boundaries. Benefiting from\nthe data-dependent and flexible group mechanism, our DG-\nAttention shows superiority to the other window-based self-\nattention illustrated in Figure 1.\nBased on the proposed DG-Attention, we design a general\nvision transformer backbone for image classification, named\nDynamic Group Transformer (DGT). We scale our approach\nup to get a family of models, including DGT-T (24M), DGT-\nS (52M), and DGT-B (90M). They achieve significantly a\nbetter performance than previous methods. Our DGT-T can\nachieve Top-1 classification accuracy of 83.8% on ImageNet-\n1k, 50.2% mIoU on ADE20K for semantic segmentation,\n47.7% box mAP for object detection, and 43.0% mask mAP\non COCO for instance segmentation, outperforming the state-\nof-the-art methods. Furthermore, our largest variant DGT-\nB is also superior to the previous methods, achieving 85.0%\nTop-1 accuracy on ImageNet-1K, 51.2% mIoU on ADE20K,\n49.1% box mAP, and 44.1% mask mAP on COCO dataset.\n2 Related Work\nThis section briefly reviews related works, including improv-\ning efficiency and enhancing inductive bias for Vision Trans-\nformer.\nImproving Efficiency for Vision Transformer. There are\ntwo main categories of methods to reduce the computation\ndemand for Vision Transformer. (1) Pruning Token. It aims\nto remove redundant tokens and reduce the number of to-\nkens entering into attention modules to save the computa-\ntion demand. For example, DynamicViT [Rao et al., 2021]\npruned tokens in each layer with Gumbel softmax. IA-Red\n[Pan et al., 2021] used reinforcement Learning to achieve a\nsimilar effect. Such methods achieved good performances\non image classification, but they are not friendly enough for\ndownstream dense prediction tasks. (2) Designing efficient\nattention mechanisms. Such methods mainly explored how\nto make each query attend to partial keys/values for reduc-\ning the computational cost. PVT [Wang et al., 2021a] di-\nrectly downsampled the keys and values in each block. Swin\ntransformer [Liu et al., 2021b] divided all queries/keys/values\ninto multiple windows and computed the self-attention within\neach local window. Similarly, CSwin transformer [Dong et\nal., 2021 ] expanded the window into a cross-shaped win-\ndow. MSG-Transformer [Fang et al., 2021] used additional\nMSG tokens to make connections between windows. Differ-\nent from these methods that employed pre-designed, hand-\ncrafted window partition mechanisms, our method dynami-\ncally divides all queries into multiple groups and selects the\nmost relevant keys/values for each group.\nEnhancing Inductive Bias for Vision Transformer. Vi-\nsion transformers have shown successes in various computer\nvision tasks, due to their ability to model long-range depen-\ndencies within an image. However, recent works also showed\nthat inductive bias could be incorporated for vision transform-\ners. CPE [Chu et al., 2021b] used convolution layers to gener-\nate the conditional position encoding. CVT [Wu et al., 2021]\nemployed convolution layers to generate the queries, keys and\nvalues. CMT [Guo et al., 2021] also incorporated the convo-\nlution layers into the FFN.\n3 Method\nIn this section, we first introduce our Dynamic Group atten-\ntion (DG-Attention). Then, we present the composition of\nthe Dynamic Group Transformer Block. Finally, we describe\nthe overall architecture and variant configurations of our Dy-\nnamic Group Transformer (DGT) backbone.\n3.1 Dynamic Group Attention\nTo make each query attend to relevant keys/values, we pro-\npose a Dynamic Group Attention (DG-Attention). It dynam-\nically divides all queries into multiple groups and selects the\nmost relevant keys/values for each group to compute the self-\nattention. As shown in Figure 2(c), given an input feature\nmap X ∈ RH×W×C (C is the channel number, H and\nW denotes the height and width, respectively), we first get\nquery embeddings {Xi\nQ}L\ni=1 , key embeddings {Xi\nK}L\ni=1 ,\nand value embeddings {Xi\nV }L\ni=1, where L = H ×W. For\nsimplicity, we assume there is only one head in the DG-\nAttention. It’s easy to expand to the multi-head situation\nwhere each head has its queries, keys, values, and cluster\ncentroids. Then, we use k-means clustering algorithm to dy-\nnamically divide all queries into G different query groups\n(clusters) XQ = {XQj |XQj ∈RNj×C}G\nj=1, where j is the\ngroup index and Nj is the number of queries in thejth group.\nMeanwhile, we use top-k operations to find the k most rele-\nvant keys and values for each query group, which are denoted\nas XK = {XKj |XKj ∈Rk×C}G\nj=1 and XV = {XVj |XVj ∈\nRk×C}G\nj=1, respectively.\nSpecifically, for the jth query group, we compute the dot\nproduct between its cluster centroidej and all keys{Xi\nK}L\ni=1,\nand then select the k most relevant elements according to the\ndot product sorting, which can be formulated as follow:\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1188\n(a) Overall Architecture of Dynamic Group Transformer (DGT)\n(b) DGT Block\n (c) Dynamic Group Attention (DG-Attention)\nFigure 2: (a) The overall architecture of our Dynamic Group Transformer. (b) The composition of each block. (c) Illustration of our DG-\nAttention. It can dynamically divides all queries into multiple groups and selects the most relevant keys/values for each group.\nidj = Top-k(ej, {Xi\nK}L\ni=1) ∈{1, .., L}k,\nXKj = {Xi\nK|i ∈idj}∈R k×C,\nXVj = {Xi\nV |i ∈idj}∈R k×C,\n(1)\nwhere Top-k is the function that returns the indices of top k\nvalues, and idj is an index vector. Then, the self-attention is\nconducted within each group:\nYj = SA(XQj , XKj , XVj ) ∈RNj×C, (2)\nwhere SA denotes the Self-Attention,Yj is the updated output\nof the jth query group. Finally, {Yj}G\nj=1 are scattered into the\noutput Y ∈RL×C according to their original spatial position\nindexes.\nAs each group has a different number of queries, this algo-\nrithm cannot be implemented using the general matrix multi-\nplication. We implement this algorithm using CUDA, and the\ndetail can be found in the supplementary material.\nTo make the training stable, we update the cluster centroids\nwith exponential moving average after each iteration. Specif-\nically, for the jth cluster centroid, we compute the current\ncluster centroid as follow:\ne′\nj = 1\nNj\nX\ni\nNorm(Xi\nQj ). (3)\nThen, we update the cluster centroid as below:\nej = Norm(τ ×ej + (1 −τ) ×e′\nj), (4)\nwhere τ is a hyper-parameter to control the update speed. We\nempirically set τ to 0.1 ×lr, where lr is the learning rate.\nComputation Complexity Analysis\nWe analyze the computation complexity of our DG-Attention\nand the global self-attention to further reveal the efficiency of\nour method. Here, we only consider the process of computing\nthe attention maps and weighted sums of values for clarity.\nGiven input features of size L ×C, the global self-attention\nhas a computational complexity of\nΩGlobal = 2L2C. (5)\nIn DG-Attention, each query only attends to k keys, so the\nbasic computation complexity of DG-Attention is2kLC. Be-\nsides, grouping queries and selecting the most significant\nk keys require an additional computation cost of 2LGC +\nkGlogL. Therefore, the total computation complexity of our\nDG-Attention is\nΩDG-Attention = 2kLC + 2LGC + kGlogL (6)\nThe ratio of the computation complexity of our DG-\nAttention and Global self-attention is:\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1189\nΩDG-Attention\nΩGlobal\n= 2kLC + 2LGC + kGlogL\n2L2C\n= k\nL + G\nL + kGlogL\n2L2C < 1\n(7)\nwhere L is larger than G and k. For high-resolution inputs,\nthe ratio ΩDG-Attention\nΩGlobal\n<< 1. Typically, for the ImageNet\nclassification task, k is set to 98 for the first three stages, while\nthe corresponding L is 3136, 784, and 196. Thus, the ratio is\n0.05, 0.19, and 0.75 for DGT-T. Besides, k is independent of\nthe shapes of the parameters in our models, so we can adjust\nk to balance the performance and computation efficiency.\n3.2 Dynamic Group Transformer Block\nThe Dynamic Group Transformer block is shown in Figure\n2(b). It first employs the widely-used conditional position\nembeddings (CPE) [Chu et al., 2021b] to generate the posi-\ntional information. Then, DG-Attention is applied to model\nspatial relevant dependencies flexibly and dynamically. Last,\nIRFFN (Inverted Residual Feed-forward Network) [Guo et\nal., 2021] further is employed to capture local dependencies.\nThe forward process of the lth block can be formulated as\nfollows:\n˜Xl = Xl−1 + CPE (Xl−1), (8)\nˆXl = ˜Xl + DG-Attention(LN( ˜Xl)), (9)\nXl = ˆXl + IRFFN (LN( ˆXl)), (10)\nwhere LN(·) denotes Layer Normalization, Xl and Xl−1 are\nthe output of the lth and l-1th block, respectively.\n3.3 Overall Architecture and Variants\nOur Dynamic Group Transformer (DGT) consists of a convo-\nlutional stem, four hierarchical stages, and a classifier head,\nas shown in Figure 2 (a). The stem is designed to extract local\ndependency, similar to [Guo et al., 2021], which consists of\none 3×3 convolution layer with stride = 2 and two 3×3 con-\nvolution layers with stride = 1. After the stem, each stage con-\ntains a patch merging layer and multiple transformer blocks.\nThe first three stages use the DGT block, and the last stage\napplies global self-attention (GSA) block, which is achieved\nby replacing the DG-Attention with global self-attention in\nDGT block. We decrease the number of tokens and double\nthe channel dimension by using a 3× 3 convolutional layer\nwith stride = 2 before each stage to produce a hierarchical\nrepresentation. The final classifier head consists of two linear\nlayers.\nFinally, we design three different variants, including DGT-\nTiny (DGT-T), DGT-Small (DGT-S), and DGT-Base (DGT-\nB), whose detailed configurations are shown in Table 1. For\nall variants, the number of blocks in each stage is fixed with\n[1,2,17,2]. In each DGT block, the expand ratios of IRFFN\nare set to 4, the number of groups G is 48. The number of\nselected keys/values k is 98 for image classification on Ima-\ngeNet [Deng et al., 2009]. The main differences among all\nvariants are the channel dimension and the number of heads\nin DGT blocks. Besides, to train the model stably, we apply\npost LayerNorm and cosine attention [Liu et al., 2021a] in\nDGT-S and DGT-B.\nStage/Stride Layer DGT-T DGT-S DGT-B\nStride=2 Stem 3 ×3, 32, s =\n2\n[3 ×3, 32] ×2\n3 ×3, 48, s =\n2\n[3 ×3, 48] ×2\n3 ×3, 64, s =\n2\n[3 ×3, 64] ×2\nStage 1\nStride=4\nPatch\nMer\nge 3 ×3, 64, s=2 3 ×3, 96, s=2 3 ×3, 128, s=2\nDGT\nBlock\n\n\nH1=2\nG1=48\nk1=98\nR1=4\n\n ×1\n\n\nH1=3\nG1=48\nk1=98\nR1=4\n\n ×1\n\n\nH1=4\nG1=48\nk1=98\nR1=4\n\n ×1\nStage 2\nStride=8\nPatch\nMer\nge 3 ×3, 128, s=2 3 ×3, 192, s=2 3 ×3, 256, s=2\nDGT\nBlock\n\n\nH2=4\nG2=48\nk2=98\nR2=4\n\n ×2\n\n\nH2=6\nG2=48\nk2=98\nR2=4\n\n ×2\n\n\nH2=8\nG2=48\nk2=98\nR2=4\n\n ×2\nStage 3\nStride=16\nPatch\nMer\nge 3 ×3, 256, s=2 3 ×3, 384, s=2 3 ×3, 512, s=2\nDGT\nBlock\n\n\nH3=8\nG3=48\nk3=98\nR3=4\n\n ×17\n\n\nH3=12\nG3=48\nk3=98\nR3=4\n\n ×17\n\n\nH3=16\nG3=48\nk3=98\nR3=4\n\n ×17\nStage 4\nStride=32\nPatch\nMer\nge 3 ×3, 512, s=2 3 ×3, 768, s=2 3 ×3, 1024, s=2\nGSA\nBlock\n\u0014\nH4=16\nR4=4\n\u0015\n×2\n\u0014\nH4=24\nR4=4\n\u0015\n×2\n\u0014\nH4=32\nR4=4\n\u0015\n×2\nFC 1 ×1, 1280\nClassifier 1 ×1, 1000\nParams 24.09 M 51.76 M 90.28 M\nFlops 4.35 G 9.41 G 16.4 G\nTable 1: Detailed configurations of different variants of our DGT.\nHi, Gi and ki represent the number of heads, group, and the se-\nlected key/value in DGT block, respectively. Ri is the expand ratio\nin IRFFN.\n4 Experiments\nWe first compare our Dynamic Group Transformer (DGT)\nwith the state-of-the-art backbones on ImageNet-1K [Deng\net al., 2009] for image classification. To further verify the ef-\nfectiveness and generalization of our backbone, we perform\nexperiments on ADE20K [Zhou et al., 2017] for semantic\nsegmentation, COCO [Lin et al., 2014] for object detection\nand instance segmentation. Finally, we analyze the key de-\nsign of our Dynamic Group Transformer.\n4.1 Image Classification on ImageNet-1K\nWe conduct experiments on ImageNet-1K[Deng et al., 2009]\ndataset, which has 1.28M images in the training set and 50K\nimages in the validation set. Detailed settings are described\nin the supplementary material.\nResults\nTable 2 compares the performance of our DGT with the state-\nof-the-art CNN models and vision transformer backbones\non ImageNet-1K validation set. We can see that our DGT\nvariants outperform the state-of-the-art CNN models and vi-\nsion transformer models when using similar FLOPs. DGT-T\nachieves 83.8% top-1 accuracy with 4.3G FLOPs and out-\nperforms the CNN models Reg-4G and Efficient B4 by 3.8%\nand 0.9%, respectively. Meanwhile, our DGT outperforms\nthe advanced Transformer-based backbones, and is +1.6%\nand +0.8% higher than Swin and CSwin Transformer, re-\nspectively, for all variants under the similar model size and\nFLOPs. For example, our DGT-T can surpass PVT-S, Swin-\nT, and CSwin-S by 4.0%, 2.5%, and 1.1%, respectively. Our\nDGT-B can outperform Swin-B and CSwin-B by 1.7% and\n0.8%, respectively. These results demonstrate the effective-\nness and efficiency of our approach.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1190\nMethod Param.\nFLOPs Top-1\nDeiT-S [T\nouvron et al., 2020] 22M 4.6G 79.8\nPVT-S [W\nang et al., 2021a] 25M 3.8G 79.8\nReg-4G [Radosa\nvovic et al., 2020] 21M 4.0G 80.0\nSwin-T [Liu et al.\n, 2021b] 29M 4.5G 81.3\nCPVT-S [Chu et\nal., 2021b] 23M 4.6G 81.5\nCvT-13 [W\nu et al., 2021] 20M 4.5G 81.6\nViL-S [Zhang et\nal., 2021] 25M 4.9G 82.0\nCSWin-T [Dong et\nal., 2021] 23M 4.3G 82.7\nEff-B4* [T\nan and Le, 2019] 19M 4.2G 82.9\nCMT-S [Guo et\nal., 2021] 25M 4.0G 83.5\nDGT-T (ours) 24M 4.3G 83.8\nPVT-M [W\nang et al., 2021a] 44M 6.7G 81.2\nPVT-L [W\nang et al., 2021a] 61M 9.8G 81.7\nReg-8G [Radosa\nvovic et al., 2020] 39M 8.0G 81.7\nCvT-21 [W\nu et al., 2021] 32M 7.1G 82.5\nSwin-S [Liu et al.\n, 2021b] 50M 8.7G 83.0\nTwins-B [Chu et\nal., 2021a] 56M 8.3G 83.2\nViL-M [Zhang et\nal., 2021] 40M 8.7G 83.3\nCSWin-S [Dong et\nal., 2021] 35M 6.9G 83.6\nEff-B5* [T\nan and Le, 2019] 30M 9.9G 83.6\nDGT-S (ours) 52M 9.4G 84.6\nDeiT-B [T\nouvron et al., 2020] 87M 17.5G 81.8\nCPVT-B [Chu et\nal., 2021b] 88M 17.6G 82.3\nReg-16G [Radosa\nvovic et al., 2020] 84M 16.0G 82.9\nViL-B [Zhang et\nal., 2021] 56M 13.4G 83.2\nSwin-B [Liu et al.\n, 2021b] 88M 15.4G 83.3\nTwins-L [Chu et\nal., 2021a] 99M 14.8G 83.7\nEff-B6* [T\nan and Le, 2019] 43M 19.0G 84.0\nCSWin-B [Dong et\nal., 2021] 78M 15.0G 84.2\nDGT-B (ours) 90M 16.4G 85.0\nTable 2: Comparison with the state-of-the-art models, trained with\n224× 224 on ImageNet-1K Classification.\n4.2 Semantic Segmentation on ADE20K\nTo demonstrate the superiority of our Dynamic Group Trans-\nformer for semantic segmentation. We conduct experiments\non ADE20K with the widely-used UperNet [Xiao et al.,\n2018] framework for fair comparisons to other backbones.\nDetailed implementation can be found in the supplementary\nmaterial.\nResults\nTable 3 shows the comparisons of UperNet [Xiao et al.,\n2018] with various advanced Transformer backbones on\nADE20K validation set. We report both single-scale (SS)\nmIoU and multi-scale (MS) mIoU for a cleaner compari-\nson. Our DGT variants outperforms the state-of-the-art meth-\nods consistently. Specifically, our DGT-T achieves 50.2%\nmIoU with single scale testing, outperforming the Swin-T and\nCrossFormer-S by 5.7% and 2.6%. Our DGT-S outperforms\nSwin-S and CrossFormer-B by 3.2% and 0.9% SS mIoU. Be-\nsides, our DGT-B achieves 51.2%/51.8% SS/MS mIoU, out-\nperforming the Swin-B and CrossFormer-L by 3.1%/2.1%\nand 0.7%/0.4%. These results demonstrate the advantages of\nour Dynamic Group Transformer for semantic segmentation.\nBackbone Prams (M)\nFLOPs (G) mIoU\nSS/MS\nTwinsP-S [Chu et\nal., 2021a] 54.6 919 46.2/47.5\nTwins-S [Chu et\nal., 2021a] 54.4 901 46.2/47.1\nSwin-T [Liu et al.\n, 2021b] 59.9 945 44.5/45.8\nCrossFormer\n-S [Wang et al., 2021b] 62.3 980 47.6/48.4\nDGT-T (ours) 52.5 954 50.2/50.8\nRes101 [He et al.\n, 2016] 86.0 1029 –/44.9\nTwinsP-B [Chu et\nal., 2021a] 74.3 977 47.1/48.4\nTwins-B [Chu et\nal., 2021a] 88.5 1020 47.7/48.9\nSwin-S [Liu et al.\n, 2021b] 81.3 1038 47.6/49.5\nCrossFormer\n-B [Wang et al., 2021b] 83.6 1090 49.7/50.6\nDGT-S (ours) 81.9 1074 50.8/51.6\nTwinsP-L [Chu et\nal., 2021a] 91.5 1041 48.6/49.8\nTwins-L [Chu et\nal., 2021a] 133.0 1164 48.8/50.2\nSwin-B [Liu et al.\n, 2021b] 121.0 1188 48.1/49.7\nCrossFormer\n-L [Wang et al., 2021b] 125.5 1244 50.5/51.4\nDGT-B (ours) 122.2 1234 51.2/51.8\nTable 3: Comparison with different backbones on ADE20K. FLOPs\nare calculated with the resolution of 512×2048.\n4.3 Object Detection and Instance Segmentation\non COCO\nWe further evaluate our DGT backbone on COCO [Lin et\nal., 2014 ] dataset for object detection and instance seg-\nmentation. Following previous works [Liu et al., 2021b;\nDong et al., 2021], we utilize Mask R-CNN [He et al., 2017]\nframework under 1x schedule. More details are provided in\nthe supplementary material.\nResults\nThe results on COCO dataset are shown in Table 4(a). All\nDGT variants outperform the state-of-the-art vision trans-\nformer backbones under similar FLOPs. Specifically, for ob-\nject detection, our DGT-T, DGT-S, and DGT-B can achieve\n47.7%, 48.4% and 49.1% box AP, which surpass Swin by\n5.5%, 3.6%, and 2.2%, respectively. For instance segmenta-\ntion, our DGT-T, DGT-S, and DGT-B are 3.9%, 2.6%, and\n1.8% mask AP higher than the Swin. Besides, DGT-T out-\nperforms CrossFormer-S by 2.3% bos AP on object detection\nand 1.6% mask AP on instance segmentation. DGT-S outper-\nforms CrossFormer-B by 1.2% box AP on object detection\nand 0.8% mask AP on instance segmentation.\n4.4 Ablation Studies\nWe conduct ablation studies for the key designs of our meth-\nods on the image classification task. All experiments are per-\nformed with the Tiny variant under the same training settings.\nEffect of Hyper-parameters G and k\nFirst, we validate the effect of the hyper-parameters G and k.\nG is the number of groups. A small G causes the queries in\na group to be very different, so the selected keys cannot suit\nall queries. k determines how many keys each query attends\nto. There will be too much information loss if k is too small.\nThe default setting of our model on ImageNet is G = 48 and\nk = 98. We compare the default setting with halving G from\n48 to 24 and halving k from 98 to 49.\nThe results are shown in Table 4(b). Decreasing G or k\nleads to a poorer performance. Halving G and k decrease\nthe top-1 accuracy by 0.2% and 0.1%, respectively. We can\nbalance the performance and efficiency by adjustingG and k.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1191\nFigure 3: Visualizations of some query and keys groups. Each dashed box contains a query group and its corresponding keys group. It can\nbe seen that our method can flexibly model relevant dependencies without any spatial constraint.\nBackbone Params\n(M) FLOPS (G) APb APb\n50 APb\n75 APm APm\n50 APm\n75\nRes50 [He et al.\n, 2016] 44 260 38.0 58.6\n41.4 34.4 55.1 36.7\nPVT-S [Wang et al., 2021a] 44 245 40.4 62.9\n43.8 37.8 60.1 40.3\nViL-S [Zhang et al., 2021] 45 218 44.9 67.1\n49.3 41.0 64.2 44.1\nTwinsP-S [Chu et al., 2021a] 44 245 42.9 65.8\n47.1 40.0 62.7 42.9\nTwins-S [Chu et al., 2021a] 44 228 43.4 66.0\n47.3 40.3 63.2 43.4\nSwin-T [Liu et al., 2021b] 48 264 42.2 64.6\n46.2 39.1 61.6 42.0\nCrossFormer-S [Wang et al., 2021b] 50 301 45.4 68.0\n49.7 41.4 64.8 44.6\nDGT-T (ours) 42 272 47.7 69.8\n52.2 43.0 66.9 46.4\nRes101 [He et al.\n, 2016] 63 336 40.4 61.1\n44.2 36.4 57.7 38.8\nX101-32 [Xie et al., 2017] 63 340 41.9 62.5\n45.9 37.5 59.4 40.2\nPVT-M [Wang et al., 2021a] 64 302 42.0 64.4\n45.6 39.0 61.6 42.1\nViL-M [Zhang et al., 2021] 60 261 43.4 —-\n—- 39.7 —- —-\nTwinsP-B [Chu et al., 2021a] 64 302 44.6 66.7\n48.9 40.9 63.8 44.2\nTwins-B [Chu et al., 2021a] 76 340 45.2 67.6\n49.3 41.5 64.5 44.8\nSwin-S [Liu et al., 2021b] 69 354 44.8 66.6\n48.9 40.9 63.4 44.2\nCrossFormer-B [Wang et al., 2021b] 72 408 47.2 69.9\n51.8 42.7 66.6 46.2\nDGT-S (ours) 70 386 48.4 70.7\n53.2 43.5 67.6 47.0\nX101-64 [Xie et al.\n, 2017] 101 493 42.8 63.8\n47.3 38.4 60.6 41.3\nPVT-L [Wang et al., 2021a] 81 364 42.9 65.0\n46.6 39.5 61.9 42.5\nViL-B [Zhang et al., 2021] 76 365 45.1 —-\n—- 41.0 —- —-\nTwinsP-L [Chu et al., 2021a] 81 364 45.4 —-\n—- 41.5 —- —-\nTwins-L [Chu et al., 2021a] 111 474 45.9 —-\n—- 41.6 —- —-\nSwin-B [Liu et al., 2021b] 107 496 46.9 —-\n—- 42.3 —- —-\nDGT-B (ours) 108 540 49.1 70.9\n54.1 44.1 68.1 47.6\n(a) Comparision with different backbones on COCO. Flops are calculated with the resolution\nof 800×1280.\nG k Top-1\n24 98 83.6\n48 49 83.7\n48 98 83.8\n(b) Effect of Hyper-\nparameters G and k.\nBlock Top-1\nSwin block 82.8\nCSwin block 83.0\nCMT block 83.4\nDGT block\n(ours) 83.8\n(c) Comparison with dif-\nferent Vision Transformer\nBlocks.\nTable 4: Experiments on COCO and ablation studies.\nComparison with Related Transformer Blocks\nTo validate the design of our DGT block, which uses convo-\nlution layers to extract local dependency and uses the DG-\nAttention to extract non-local dependency, we replace our\nDGT block with other blocks and compare their performance.\nWe select three blocks: Swin block, CSwin block, and CMT\nblock. Swin block and Cswin block use shifted window-\nbased self-attention and cross-shaped window self-attention,\nrespectively. The results are shown in Figure 4(c). Our DGT\nblock obviously outperforms Swin block, CSwin block and\nCMT block by 1.0%, 0.8% and 0.4%.\n5 Visualization\nWe visualize the query groups and their corresponding se-\nlected keys with an example shown in Figure 3. One can find\n: 1) different queries prefer to attends to different keys ac-\ncording to their content. These three groups mainly contain\nthe queries of the bird, branches, and background, and they\nalso attend to the keys of the bird, branches, and background,\nrespectively. 2) A query may prefer to attend to a long-range\narea rather than a short-range local region. These findings\nshow the advantages of our DG-Attention. More visualiza-\ntion examples can be found in the supplementary material.\n6 Conclusion\nWe have presented an effective dynamic attention mechanism\nnamed Dynamic Group Attention (DG-Attention), which dy-\nnamically divides input queries into multiple groups and se-\nlects relevant keys/values for each group. DG-Attention can\nmodel more relevant context dependencies than the previ-\nous pre-designed window-based local attention mechanism.\nBased on the proposed DG-Attention, we have developed\na general Vision Transformer backbone, Dynamic Group\nTransformer (DGT), which can outperform the state-of-the-\nart on ImageNet-1K for image classification. Furthermore,\nour DGT outperforms the existing Vision Transformer back-\nbones on ADE20K for semantic segmentation, and COCO for\nobject detection and instance segmentation.\nReferences\n[Chu et al., 2021a] Xiangxiang Chu, Zhi Tian, Yuqing\nWang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia\nXia, and Chunhua Shen. Twins: Revisiting spatial at-\ntention design in vision transformers. arXiv preprint\narXiv:2104.13840, 2021.\n[Chu et al., 2021b] Xiangxiang Chu, Zhi Tian, Bo Zhang,\nXinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1192\nShen. Conditional positional encodings for vision trans-\nformers. arXiv preprint arXiv:2102.10882, 2021.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on\ncomputer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[Dong et al., 2021] Xiaoyi Dong, Jianmin Bao, Dongdong\nChen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen,\nand Baining Guo. Cswin transformer: A general vision\ntransformer backbone with cross-shaped windows. arXiv\npreprint arXiv:2107.00652, 2021.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, and Sylvain Gelly. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[Fang et al., 2021] Jiemin Fang, Lingxi Xie, Xinggang\nWang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian.\nMsg-transformer: Exchanging local spatial information\nby manipulating messenger tokens. arXiv preprint\narXiv:2105.15168, 2021.\n[Guo et al., 2021] Jianyuan Guo, Kai Han, Han Wu, Chang\nXu, Yehui Tang, Chunjing Xu, and Yunhe Wang. Cmt:\nConvolutional neural networks meet vision transformers.\narXiv preprint arXiv:2107.06263, 2021.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 770–778, 2016.\n[He et al., 2017] Kaiming He, Georgia Gkioxari, Piotr\nDoll´ar, and Ross Girshick. Mask r-cnn. In Proceedings\nof the IEEE international conference on computer vision,\npages 2961–2969, 2017.\n[Huang et al., 2021] Zilong Huang, Youcheng Ben,\nGuozhong Luo, Pei Cheng, Gang Yu, and Bin Fu.\nShuffle transformer: Rethinking spatial shuffle for vision\ntransformer. arXiv preprint arXiv:2106.03650, 2021.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Be-\nlongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft coco: Com-\nmon objects in context. In European conference on com-\nputer vision, pages 740–755. Springer, 2014.\n[Liu et al., 2021a] Ze Liu, Han Hu, Yutong Lin, Zhuliang\nYao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng\nZhang, Li Dong, et al. Swin transformer v2: Scaling up\ncapacity and resolution. arXiv preprint arXiv:2111.09883,\n2021.\n[Liu et al., 2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. arXiv preprint arXiv:2103.14030, 2021.\n[Pan et al., 2021] Bowen Pan, Rameswar Panda, Yifan Jiang,\nZhangyang Wang, Rogerio Feris, and Aude Oliva. Ia-\nred2: Interpretability-aware redundancy reduction for vi-\nsion transformers. Advances in Neural Information Pro-\ncessing Systems, 34, 2021.\n[Radosavovic et al., 2020] Ilija Radosavovic, Raj Prateek\nKosaraju, Ross Girshick, Kaiming He, and Piotr Doll ´ar.\nDesigning network design spaces. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10428–10436, 2020.\n[Rao et al., 2021] Yongming Rao, Wenliang Zhao, Benlin\nLiu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit:\nEfficient vision transformers with dynamic token sparsifi-\ncation. arXiv preprint arXiv:2106.02034, 2021.\n[Tan and Le, 2019] Mingxing Tan and Quoc Le. Efficient-\nnet: Rethinking model scaling for convolutional neural\nnetworks. In International Conference on Machine Learn-\ning, pages 6105–6114. PMLR, 2019.\n[Touvron et al., 2020] Hugo Touvron, Matthieu Cord,\nMatthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv ´e J ´egou. Training data-efficient image\ntransformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[Wang et al., 2021a] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[Wang et al., 2021b] Wenxiao Wang, Lu Yao, Long Chen,\nBinbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Cross-\nformer: A versatile vision transformer hinging on cross-\nscale attention. arXiv preprint arXiv:2108.00154, 2021.\n[Wu et al., 2021] Haiping Wu, Bin Xiao, Noel Codella,\nMengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv\npreprint arXiv:2103.15808, 2021.\n[Xiao et al., 2018] Tete Xiao, Yingcheng Liu, Bolei Zhou,\nYuning Jiang, and Jian Sun. Unified perceptual parsing\nfor scene understanding. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 418–434,\n2018.\n[Xie et al., 2017] Saining Xie, Ross Girshick, Piotr Doll ´ar,\nZhuowen Tu, and Kaiming He. Aggregated residual trans-\nformations for deep neural networks. InProceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 1492–1500, 2017.\n[Zhang et al., 2021] Pengchuan Zhang, Xiyang Dai, Jian-\nwei Yang, Bin Xiao, Lu Yuan, Lei Zhang, and Jianfeng\nGao. Multi-scale vision longformer: A new vision trans-\nformer for high-resolution image encoding. arXiv preprint\narXiv:2103.15358, 2021.\n[Zhou et al., 2017] Bolei Zhou, Hang Zhao, Xavier Puig,\nSanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. In Proceedings of the\nIEEE conference on computer vision and pattern recog-\nnition, pages 633–641, 2017.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1193",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7562958598136902
    },
    {
      "name": "Transformer",
      "score": 0.6777867078781128
    },
    {
      "name": "Segmentation",
      "score": 0.5535634756088257
    },
    {
      "name": "Computation",
      "score": 0.5510556697845459
    },
    {
      "name": "Sliding window protocol",
      "score": 0.4903002083301544
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4641605019569397
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32519903779029846
    },
    {
      "name": "Algorithm",
      "score": 0.1825409233570099
    },
    {
      "name": "Window (computing)",
      "score": 0.16625472903251648
    },
    {
      "name": "Engineering",
      "score": 0.07869064807891846
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98301712",
      "name": "Baidu (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210129579",
      "name": "National Engineering Laboratory of Deep Learning Technology and Application",
      "country": "CN"
    }
  ]
}