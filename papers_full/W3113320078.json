{
    "title": "Parameter Efficient Multimodal Transformers for Video Representation Learning",
    "url": "https://openalex.org/W3113320078",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1799295894",
            "name": "Lee， Sang-Ho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221412741",
            "name": "Yu, Youngjae",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1502945480",
            "name": "Kim Gun-Hee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2752290379",
            "name": "Breuel, Thomas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2535426689",
            "name": "Kautz, Jan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2906169378",
            "name": "Song, Yale",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962756039",
        "https://openalex.org/W2990503944",
        "https://openalex.org/W2970900903",
        "https://openalex.org/W2152790380",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3010094231",
        "https://openalex.org/W2961193895",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2164587673",
        "https://openalex.org/W2982619606",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2618799552",
        "https://openalex.org/W2184188583",
        "https://openalex.org/W2963928591",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2593116425",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2583815496",
        "https://openalex.org/W2962969419",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2785523195",
        "https://openalex.org/W2970869018",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2962960500",
        "https://openalex.org/W2948242301",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W2619383789",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2981822321",
        "https://openalex.org/W2935908327",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W3118120400",
        "https://openalex.org/W2887997457",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W2968880719",
        "https://openalex.org/W2964345931",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2926645869",
        "https://openalex.org/W2952686080",
        "https://openalex.org/W2963016848",
        "https://openalex.org/W2944779197",
        "https://openalex.org/W2279098554",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3037932933",
        "https://openalex.org/W2914344989",
        "https://openalex.org/W2982624843",
        "https://openalex.org/W3035265375",
        "https://openalex.org/W3020933450",
        "https://openalex.org/W1972567154",
        "https://openalex.org/W3020257313",
        "https://openalex.org/W2999514753",
        "https://openalex.org/W2962865004",
        "https://openalex.org/W2964048159",
        "https://openalex.org/W3029860052",
        "https://openalex.org/W2975357369",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2963814513",
        "https://openalex.org/W2619947201",
        "https://openalex.org/W3034932471",
        "https://openalex.org/W3035635319",
        "https://openalex.org/W3006320872",
        "https://openalex.org/W2145085734",
        "https://openalex.org/W2990408345",
        "https://openalex.org/W3035118106",
        "https://openalex.org/W2052666245",
        "https://openalex.org/W2936774411",
        "https://openalex.org/W3010874390",
        "https://openalex.org/W2300242332",
        "https://openalex.org/W3123318516",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W2963115079",
        "https://openalex.org/W3099206234"
    ],
    "abstract": "The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97$\\%$, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks.",
    "full_text": "Published as a conference paper at ICLR 2021\nPARAMETER EFFICIENT MULTIMODAL TRANSFORM -\nERS FOR VIDEO REPRESENTATION LEARNING\nSangho Lee, Youngjae Yu, Gunhee Kim\nSeoul National University\n{sangho.lee,yj.yu}@vision.snu.ac.kr, gunhee@snu.ac.kr\nThomas Breuel, Jan Kautz\nNVIDIA Research\n{tbreuel,jkautz}@nvidia.com\nYale Song\nMicrosoft Research\nyalesong@microsoft.com\nABSTRACT\nThe recent success of Transformers in the language domain has motivated adapting\nit to a multimodal setting, where a new visual model is trained in tandem with\nan already pretrained language model. However, due to the excessive memory\nrequirements from Transformers, existing work typically ﬁxes the language model\nand train only the vision module, which limits its ability to learn cross-modal infor-\nmation in an end-to-end manner. In this work, we focus on reducing the parameters\nof multimodal Transformers in the context of audio-visual video representation\nlearning. We alleviate the high memory requirement by sharing the parameters of\nTransformers across layers and modalities; we decompose the Transformer into\nmodality-speciﬁc and modality-shared parts so that the model learns the dynamics\nof each modality both individually and together, and propose a novel parameter\nsharing scheme based on low-rank approximation. We show that our approach\nreduces parameters of the Transformers up to 97%, allowing us to train our model\nend-to-end from scratch. We also propose a negative sampling approach based\non an instance similarity measured on the CNN embedding space that our model\nlearns together with the Transformers. To demonstrate our approach, we pretrain\nour model on 30-second clips (480 frames) from Kinetics-700 and transfer it to\naudio-visual classiﬁcation tasks.\n1 I NTRODUCTION\nLearning multimodal representation from unlabeled videos has received considerable attention (Bal-\ntrušaitis et al., 2018). Audio-visual learning is of particular interest due to the abundance of videos\nwith natural audio-visual co-occurrence (Owens & Efros, 2018; Owens et al., 2018; Arandjelovic\n& Zisserman, 2018; Ephrat et al., 2018; Gao & Grauman, 2019; Alwassel et al., 2019). However,\nexisting approaches learn localized representations from short videos (hundreds of milliseconds to\njust under a few seconds), capturing only short-term dependencies in data. While this is useful for\ncertain applications, e.g., source separation (Ephrat et al., 2018) and atomic action recognition (Gu\net al., 2018), learning representation that captures long-term dependencies is equally important, e.g.,\nfor activity recognition (Kay et al., 2017; Carreira et al., 2019; Sigurdsson et al., 2016). Unfortunately,\nprocessing long videos requires large memory resource and capturing long-term dependencies is a\nlong-standing problem (Hochreiter & Schmidhuber, 1997; Cho et al., 2014; Vaswani et al., 2017).\nIn language understanding, strong progress has been made in large-scale learning of contextualized\nlanguage representations using Transformers (Vaswani et al., 2017; Howard & Ruder, 2018; Peters\net al., 2018; Radford et al., 2018; 2019; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). Riding\non the success of Transformers, several recent works have extended it to the multimodal setting\nby adding an additional vision module to the Transformer framework (Sun et al., 2019b; Lu et al.,\n2019). However, these models are typically not end-to-end trained; they rely on a language-pretrained\nBERT (Devlin et al., 2019), which is ﬁxed throughout, and train only the visual components. While\nthe pretrained BERT helps accelerate convergence and brings reliable extra supervision signal to the\n1\narXiv:2012.04124v2  [cs.CV]  22 Sep 2021\nPublished as a conference paper at ICLR 2021\nBOS\n Visual CNN\n Visual CNN\n BOS\n Audio CNN\n Audio CNN\nLayer 1\nLayer 2\nLayer L\n Parameter Sharing in Transformers\n(c) Multimodal Transformer: L layers\n(a) Visual Transformer: L layers (b) Audio Transformer: L layers\nSame Color  =  Shared Parameters\nVisual\nTransformer\nAudio\nTransformer\nMultimodal\nTransformer\nFigure 1: (Left) Our model consists of CNNs encoding short-term dynamics of each modality and\nTransformers encoding long-term dynamics of audio-visual information from videos. (Right) To\nalleviate excessive memory requirements, we propose an efﬁcient parameter sharing scheme based on\nmatrix decomposition with low-rank approximation, which allows us to train our model end-to-end.\nvision component, this partial learning setup can be undesirable if the text data comes from different\ndistributions (of topics, dialects, or foreign languages) or if we want to apply it to different modalities\n(e.g., audio-visual). Unfortunately, end-to-end training of such multimodal Transformer architectures\nis challenging for most existing compute environments due to the excessive memory requirement.\nIn this work, we make three key contributions. First, we propose anend-to-end trainablebidirectional\ntransformer architecture that learns contextualized audio-visual representations of long videos. Our\nmodel, shown in Figure 1, consists of audio/visual CNNs, audio/visual Transformers, and a multi-\nmodal Transformer. The CNNs operate on short (e.g., one second) video clips and are intended to\ncapture short-term dynamics within each modality. The Transformer layers operate on long video se-\nquences (e.g., 30 seconds), capturing long-term dynamics. To enable end-to-end training, we propose\na novel parameter reduction technique that shares parts of weight parameters across Transformers and\nacross layers within each Transformer. We show that this results in up to 97% parameter reduction,\nenabling end-to-end training of our model, with a minimal performance degradation. To the best of\nour knowledge, our work is the ﬁrst to report end-to-end trained multimodal Transformers, and the\nﬁrst to apply Transformers for audio-visual representation learning.\nThe quality of negative samples is crucial in contrastive learning, which is part of our learning\nobjective. As our second contribution, we propose a content-aware negative sampling strategy that\nfavors negatives sufﬁciently similar to a positive instance. Our approach measures the similarity\nby reusing the CNN embeddings obtained during model training, and thus do not introduce extra\nparameters to learn. We show that this improves performance over the standard sampling strategies.\nOur third contribution is a systematic evaluation of different modality fusion strategies. Existing\nworks on multimodal BERT (all using vision-and-language data) typically apply one fusion strategy\nwithout thoroughly comparing with alternatives, e.g., some works perform early fusion (Sun et al.,\n2019b; Su et al., 2020) while others perform mid-level fusion (Lu et al., 2019; Tan & Bansal, 2019).\nAs a result, it is unclear how different fusion methods affect the ﬁnal performance. In this work, we\ncompare three fusion strategies (early, mid, late) and show the superiority of mid-level fusion.\nTo demonstrate our approach, we pretrain our model on long (30-second) video clips from Kinetics-\n700 (Carreira et al., 2019) and ﬁnetune it on various video classiﬁcation tasks. One beneﬁt of the\nmodular design of our architecture is ﬂexibility: once pretrained, we can use any of the subnetworks\nfor downstream tasks depending on the modalities involved (audio-only, visual-only, audio-visual)\nand video lengths (short and long). To show this, we evaluate our model on UCF101 (Soomro\net al., 2012) and ESC-50 (Gemmeke et al., 2017) for short-term visual/audio classiﬁcation, and\nCharades (Sigurdsson et al., 2016) and Kinetics-Sounds (Arandjelovic & Zisserman, 2017) for\nlong-term audio-visual action recognition.\n2 A PPROACH\nFigure 1 shows an overview of the proposed model architecture. The input to our model is a sequence\nof visual clips v1:T and the corresponding sequence of audio streams a1:T. For example, each\n2\nPublished as a conference paper at ICLR 2021\nsequence is a 30 second-long video divided into 30 non-overlapping clips (each clip is one second\nlong). We divide our model into three parts with different characteristics, which are explained below.\nLocal Feature Embedding. We feed each of T video clips to a visual CNN fV(vt) to obtain\nxv\n1:T ∈RT×D, and each audio stream to an audio CNN fA(at) to obtain xa\n1:T ∈RT×D.1 Intuitively,\nthe CNN outputs are temporally localembeddings as they have access to only a short-range temporal\nwindow of the entire video sequence. Thus, they are suitable for representing short-range atomic\nactions (e.g., sit down, raise arms) that constitute long-range events (e.g., gym workout). We use\nthe SlowFast network (Feichtenhofer et al., 2019) with a ResNet-50 backbone (He et al., 2016) as a\nvisual CNN fV, and a ResNet-50 as an audio CNN fA. The weights of both CNNs are randomly\ninitialized and trained end-to-end with the Transformer layers.\nUnimodal Contextualized Embedding.The local feature embeddings capture short-term dynam-\nics but lack long-term contextual information. We use Transformers (Vaswani et al., 2017) to\nenrich the embeddings with sequence-level context. We start by learning unimodal contextualized\nrepresentations using the visual Transformer gV and the audio Transformer gA, respectively.\nThe Transformer consists of L layers, each with two sub-layers: a multi-head attention layer and a\nfeed-forward layer. Given an input sequence of embeddings x ∈RT×D and Aattention heads, the\nj-th head in the attention layer computes the output embedding sequence aj ∈RT×γ,γ = D/Aas\naj = softmax\n(\nQjK⊤\nj\n√γ\n)\nVj, Q j = xWq\nj,Kj = xWk\nj ,Vj = xWv\nj (1)\nwhere Wq\nj,Wk\nj ,Wv\nj ∈RD×γ are weight matrices for computing the (query, key, value) triplet\ngiven the input x. This operation is repeated for each attention head, and the outputs are combined\n(with concatenation followed by one linear layer with weights Wb ∈RD×D), producing a ∈RT×D.\nNext, the feed-forward layer takes this intermediate output and computes o ∈RT×D using a two-\nlayer fully-connected network with weights Wc ∈RD×E and Wd ∈RE×D. The output of each\nsub-layer is computed using a residual function followed by layer normalization (Ba et al., 2016),\ni.e., LayerNorm(x+ Sublayer(x)). In this work, we set the number of layers L= 6, the number of\nattention heads A= 12, the feature dimension D= 768 and the intermediate dimension E = 3072.\nFor simplicity, we use this design for all layers across all three Transformers in our model.\nBefore feeding local embeddings xv and xa to unimodal Transformers, we augment them with\n“positional” embeddings. Speciﬁcally, we append to the beginning of each sequence a special vector\nBOS (beginning of sequence), i.e., xv\n0 for visual and xa\n0 for audio streams; their dimensions are same\nas xv\nt and xa\nt, respectively. We also deﬁne positional embeddingsp0:T encoding time indices (we call\nthis “time” embedding). This is necessary to preserve information about temporal ordering of local\nfeature embeddings, which is otherwise lost in Eqn. 1. We combine them via layer normalization,\nuv\nt = LayerNorm(xv\nt + pv\nt), ua\nt = LayerNorm(xa\nt + pa\nt), ∀t∈[0,T] (2)\nWe initialize {xv\n0,xa\n0,pv\n0:T,pa\n0:T}to the normal distribution and train them with the rest of the model.\nWe feed the augmented visual embeddings into the visual Transformer gV and obtain yv\n0:T =\ngV(uv\n0:T), and similarly obtain ya\n0:T = gA(ua\n0:T). The embeddings at each time step has a direct\naccess to the entire input sequence regardless of their position (it has a one-step signal path during\nforward and backward inference). Multiple layers of such feature transformation thus allow the\nresulting embedding to be deeply contextualized in the time dimension. We denote the output\nembeddings corresponding to the BOS positions by BOSv\ng = yv\n0 and BOSa\ng = ya\n0, and designate them\nas the summary embeddings representing the sequence of each modality.\nMultimodal Contextualized Embedding.The unimodal embeddings capture long-term temporal\ncontext but miss out on cross-modal information. The ﬁnal step in forward inference is to use a\nmultimodal Transformer hAV to obtain embeddings contextualized in the audio-visual space.\nWe ﬁrst augment the embeddings yv\n0:T and ya\n0:T with modality and time embeddings. The modality\nembeddings mv and ma are vectors of the same dimension as yv\nt and ya\nt, respectively. We share mv\n(and ma) across all the unimodal embeddingsyv\n0:T (and ya\n0:T); thus, they add modality-discriminative\ninformation to the Transformer. We also add time embeddings p0:T as before; however, unlike in\n1For notational simplicity, we drop the subscripts to refer to the entire sequence unless distinction is necessary.\n3\nPublished as a conference paper at ICLR 2021\nthe previous step, we share the same p0:T between embeddings from the two modalities to correctly\nindicate the time indices. We augment the modality and time embeddings via layer normalization,\nwv\nt = LayerNorm(yv\nt + pt + mv), wa\nt = LayerNorm(ya\nt + pt + ma), ∀t∈[0,T] (3)\nWe feed the augmented visual embeddings wv\n0:T and audio embeddings wa\n0:T to the multimodal\nTransformer hAV, one after another, and obtain z0:(2T+1) = hAV([wv\n0:T; wa\n0:T]). We again denote\nthe output embeddings corresponding to the BOS positions by BOSv\nh = zv\n0(= z0) and BOSa\nh = za\n0(=\nzT+1), and use them as summary embeddings encoding multimodal context.\nWe emphasize the importance of feeding wv\n0:T and wa\n0:T one after another. An alternative would be\nconcatenating them before feeding them to hAV and obtaining an output z0:T (instead of z0:(2T+1)).\nHowever, this restricts the Transformer to access audio-visual embeddings only from the same\ntime slices, which could be problematic when there is a temporally asynchronous relationship\nbetween the two modalities (e.g., a visual clip matches with sound captured a few times steps\nbefore) (Kazakos et al., 2019; Morgado et al., 2020). By arranging the two sequences one after the\nother, the Transformer can mix-and-match appropriate audio-visual embeddings in an asynchronous\nmanner. Another practical concern with the alternative approach is that it signiﬁcantly increases the\nmodel size; the weight matrices Wq,Wk,Wv grow quadratically with the input feature dimension D.\nSerializing the input resolves both issues.\n2.1 S ELF -SUPERVISED PRETRAINING OBJECTIVES\nTask 1: Masked Embedding Prediction (MEP).BERT (Devlin et al., 2019) is trained using the\nmasked language model (MLM) task, which randomly selects input tokens and replaces them with\na mask token. The model is then trained to predict the original (unmasked) tokens by solving a\nclassiﬁcation task with a cross-entropy loss. However, inputs to our model are real-valued audio-\nvisual signals (rather than discrete tokens),2 so applying the MLM task requires input discretization,\nwhich causes information loss (Lu et al., 2019; Sun et al., 2019a). We instead train our model to\nidentify the correct visual clip or audio stream compared to a set of negative samples in a contrastive\nmanner, which does not require input discretization.\nWe formulate our MEP task using InfoNCE (Oord et al., 2018), which is the softmax version of the\nnoise contrastive estimation (NCE) (Gutmann & Hyvärinen, 2010). Let ˜ot be the t-th output of any\nof the three Transformers obtained by masking the t-th input xt. Our InfoNCE loss is then deﬁned as\nLNCE(x,˜o) = −Ex\n[∑\nt\nlog I(xt,˜ot)\nI(xt,˜ot) + ∑\nj∈neg(t) I(xj,˜ot)\n]\n, (4)\nwhere neg(t) are negative sample indices and the compatibility function I(xt,˜ot) is,\nI(xt,˜ot) = exp\n(\nFFN⊤(˜ot)WIxt\n)\n, (5)\nwhere WI ∈RP×D (P = 256) and FFN is a two-layer feed-forward network. The use of a non-linear\nprediction head has shown to improve the quality of the representations learned in a contrastive\nlearning setup (Chen et al., 2020); following the recent work in Transformers (Devlin et al., 2019; Liu\net al., 2019; Lan et al., 2020), we use a GELU non-linear activation function (Hendrycks & Gimpel,\n2016) in FFN. Optimizing Eqn. 4 enforces I(xt,˜ot) to approximate the density ratio p(xt|˜ot)\np(xt) ; this\ncan be seen as maximizing the mutual information between xt and ˜ot (Oord et al., 2018). Intuitively,\nthis encourages the Transformer to capture the underlying dynamics of x from each modality without\nexplicitly learning a generative model p(xt|˜ot).\nNegative sampling. We ﬁnd that a good negative sampling strategy is essential for the model’s\nconvergence. Existing approaches either use all but xt (positive) within a mini-batch as negative\nsamples or limit it to the current sequence only. However, both these methods ignore the data content\nand thus can miss useful negatives. Oord et al. (2018) showed that leveraging prior knowledge about\ndata can improve the negative sample quality (e.g., by sampling negatives from the same speaker as\nthe positive). Unfortunately, such prior knowledge is often not available in unlabeled videos.\n2In the form of RGB images and log-mel-scaled spectrograms.\n4\nPublished as a conference paper at ICLR 2021\nLayer 1\nLayer 2\nLayer L\n(a) No sharing (b) Cross-layer sharing [37] (c) All sharing (d) Ours\nTransformer 1 Transformer N Transformer 1 Transformer N Transformer 1 Transformer NTransformer 1 Transformer N\nFigure 2: Comparison of parameter sharing schemes. Ours combines (b) and (c) but decomposes\nweights in each layer into private and shared parts so only the latter is shared across Transformers.\nWe propose a content-aware negative sampling strategy that favors negatives sufﬁciently similar\nto a positive instance in the CNN embedding space; we call our approach CANS-Similar. Our\napproach is inspired by Ulyanov et al. (2018) who showed that randomly initialized CNNs provide a\nstrong prior over natural images due to the inductive bias already built into the design of the CNNs.\nThis suggests that our local feature embeddings xv (and xa) can capture the underlying statistical\nregularities in video clips (and audio streams) right from the beginning, which can be sufﬁcient\nto assess the similarity/dissimilarity between clips. Therefore, the distance measured on them can\napproximate content dissimilarity well (and this will improve as the training progresses).\nMotivated by this, we sample the negatives based on local feature embeddingsxv (and xa). Specif-\nically, we compute a pairwise ℓ2 distance between xt (positive) and all other instances within a\nmini-batch, and normalize them to the [0, 1] interval. To remove samples that are either too similar\nor too different from the positive sample, we discard instances that fall outside the 95% conﬁdence\ninterval in the normalized distance space. We then sample the negatives from the remainder using the\nnormalized distance as sampling probability. This makes instances similar to the positive instance\nhave more chance to become negatives. We emphasize the importance of sampling, instead of\ndeterministically taking top most similar samples; the stochasticity allows our model to be robust\nto potentially inaccurate distance estimates because samples with low probabilities will still have a\nchance to be selected as negatives.\nFinally, our MEP loss is the InfoNCE loss computed on all three Transformers,\nLMEP = LNCE(xa,˜ya) + LNCE(xv,˜yv) + LNCE([xa; xv],˜z) (6)\nTask 2: Correct Pair Prediction (CPP).The MEP task encourages our model to learn the underlying\ndynamics within each modality. To help our model learn cross-modal dynamics, we design a task\nthat predicts whether a pair of audio-visual embeddings is from the same video. Speciﬁcally, we\ndeﬁne two binary classiﬁers, one for the two unimodal Transformers and another for the multimodal\nTransformer. Each classiﬁer takes as input either sg = [yv\n0; ya\n0] (and [zv\n0; za\n0]), a pair of audio-visual\n“summary” embeddings corresponding to theBOS positions, or sh = [yv\nt; ya\nt] (or [zv\nt; za\nt]), the output\nembeddings sampled at random positions (we take two random positions t∈[1,T]). The classiﬁer\npredicts p(c|s) indicating whether the pair is from the same video (c= 1) or from different videos\n(c= 0). We train the classiﬁers with a binary cross-entropy loss,\nLCPP = −Ex,y [c·log p(c|sg) + c·log p(c|sh)] (7)\nwhere ·is the inner product. We generate a random derangement of the input mini-batch so that the\nnumber of positive and negative pairs are guaranteed to be the same.\nOverall Pretraining Objective. We train our model end-to-end from scratch by optimizingLMEP +\nαLCPP with a balancing term α. We ﬁnd our model is insensitive to this term, so we set α= 1.0.\n2.2 P ARAMETER REDUCTION\nOptimizing our model is challenging due to the large memory requirement. The most expensive part\nis the Transformers, which take up 82% of model parameters. One could reduce the model size by\nmaking the Transformers shallower, but the depth of Transformers has shown to be crucial to get\ngood performance (Devlin et al., 2019). We propose to reduce the model size by aggressively sharing\nparts of weights across Transformers as well as layers within each Transformer (see Figure 2 (d)).\nSharing across Transformers.We ﬁrst consider sharing weights across Transformers. Each Trans-\nformer encodes data coming from different distributions: gV encodes xv, gA encodes xa, and\n5\nPublished as a conference paper at ICLR 2021\nhAV encodes (yv,ya). These input distributions may each exhibit different dynamics, yet to-\ngether share certain regularities because they all come from the same videos. Motivated by this,\nwe decompose Transformer weights into shared and private parts so that different patterns can be\nlearned in a parameter-efﬁcient manner. Recall that each layer of a Transformer contains weights\n{Wq,Wk,Wv,Wb,Wc,Wd}. We decompose each of these weights into W = UΣV⊤, where\nW ∈RM×N,U ∈RM×O,Σ ∈RO×O,V ∈RN×O. We perform low-rank approximation of W by\nsetting the rank O≪M,N , and share U across Transformers while keeping Σ and V private to each\nTransformer. This helps reduce parameters becauseMO+3(O2 +NO) ≪3MN. We experimented\nwith different matrix ranks Obut the differences were small; we set O= 128 (M,N = 768 or 3072).\nThe decomposition converts a linear projection of input Wx into a series of (unconstrained) linear\nprojections UΣV⊤x. However, this can cause numerical instability during optimization (Nocedal\n& Wright, 2006). We could perform the Singular Value Decomposition (SVD) over W so that it\nperforms rotation ( V⊤), stretch ( Σ), and rotation ( U) with orthogonal basis vectors in U and V.\nUnfortunately, solving the full SVD has a computational complexity of O(max(M,N )2) (Golub\n& Van Loan, 2012). Here, we put an orthogonality constraint only on Σ and perform projection\n(V⊤), rotation (Σ), and projection (U) of input x. In addition, we put V⊤x in a unit sphere (via ℓ2-\nnormalization) before rotating it with Σ. This not only improves numerical stability, but also removes\nmagnitude information in V⊤x and keeps angular information only, which has been shown to provide\nsample discriminative information (Chen et al., 2019a). To impose the orthogonality constraint on Σ,\nwe use the Padé approximation with a scale-squaring trick of Lezcano-Casado & Martínez-Rubio\n(2019). Intuitively, we linearly project x onto a unit sphere ( V⊤x) and rotate it ( ΣV⊤x) in each\nTransformer so that it captures the dynamics of each input distribution independently. We then project\nit to the shared space via U, capturing shared regularities across all three Transformers.\nSharing across Layers.Recently, Bai et al. (2019a) showed that sharing parameters across layers in\ndeep neural networks does not hurt the representational power of the network. Furthermore, (Lan\net al., 2020) demonstrated that cross-layer parameter sharing in the Transformer leads to a lighter\nand faster-to-train model without sacriﬁcing the performance on various language understanding\nbenchmarks. Motivated by this, we let each Transformer share parameters across different layers.\n3 E XPERIMENTS\nWe pretrain our model on Kinetics-700 (Carreira et al., 2019) or AudioSet (Gemmeke et al., 2017) and\nﬁnetune it on various downstream tasks. The ofﬁcial release of Kinetics-700 contains 10-second clips\nonly, so we download 410K original videos from YouTube and take 30-second clips from each video.\nFor fair comparison with prior work, we use 10-second clips from the ofﬁcial release of AudioSet\n(we used 1.8M clips). We pretrain our model on 64 NVIDIA Tesla V100 GPUs with a batch size\nof 256 for 220K iterations. For downstream tasks, we evaluate on short-video/audio classiﬁcation\nusing UCF-101 (Soomro et al., 2012) (13K clips from 101 classes; 7.2 seconds on average) and ESC-\n50 (Gemmeke et al., 2017) (2K clips from 50 classes; 5 seconds), and on long-video classiﬁcation\nusing Kinetics-Sounds (Arandjelovic & Zisserman, 2017) (23K videos from 32 classes; 10 seconds\non average) and Charades (Sigurdsson et al., 2016) (10K videos from 157 classes; 30 seconds on\naverage). We describe various details about experimental setup in Appendix.\n3.1 R ESULTS AND DISCUSSION\nMultimodal Fusion Methods. To evaluate different fusion methods on the quality of learned\nrepresentation, we test the following settings: (i) Early uses a single multimodal Transformer\nwith 2 ×Llayers, (ii) Mid is our approach described in Figure 1, (iii) Late uses two unimodal\nTransformers each with 2 ×Llayers. All the methods are pretrained on audio-visual data using CPP\nand MEP losses, except for (iv) Late-w/o-CPP where we use only the MEP loss. We ﬁnetune\nthe pretrained models on audio-visual, audio-only, and visual-only scenarios. For fair comparisons\nacross different fusion methods, we do not perform parameter sharing in this ablation setting.\nTable 1 (a) shows thatEarly and Mid outperform Late on the audio-visual scenario. This suggests\nthe importance of encoding cross-modal information. Note that Late-w/-CPP gets cross-modal\nself-supervision, which gives marginal performance improvement over Late-w/o-CPP; however,\nboth methods miss the opportunity to encode any cross-modal relationship, leading to inferior results.\n6\nPublished as a conference paper at ICLR 2021\na) Fusion Method Audio-Visual Audio-only Visual-only\nEarly 64.9 / 89.8 - / - - / -\nLate-w/-CPP 61.0 / 88.7 52.3 / 80.8 41.0 / 71.3\nLate-w/o-CPP 60.6 / 87.6 50.5 / 79.9 40.7 / 71.7\nMid† 65.7 / 89.9 53.5 / 82.7 42.5 / 73.2\nb) Sampling Method top-1 top-5\nCurrent-Sequence 64.6 89.8\nCurrent-MiniBatch 65.5 90.8\nCANS-Dissimilar 66.2 91.1\nCANS-Similar† 67.5 92.3\nc) Model X.-L X.-T Params top-1/5\nMulti-2 \u0017 \u0017 7M 60.3 / 88.9\nMulti-6 \u0013 \u0017 21M 65.7 / 89.9\nMulti-6 \u0013 \u0013 (All) 7M 67.1 / 92.3\nMulti-6 \u0013 \u0013 (Part†) 4M 67.5 / 92.3\nd) Model X.-L X.-T Params top-1/5\nVis-2 \u0017 \u0017 14M 41.4 / 71.0\nVis-2 \u0013 \u0017 7M 41.2 / 72.9\nVis-6 \u0017 \u0017 43M 43.8 / 74.2\nVis-6 \u0013 \u0017 7M 43.5 / 73.7\nTable 1: Ablation study on Kinetics-Sounds comparing: (a; top-left)multimodal fusion methods,\n(b; top-right)negative sampling strategies, and (c & d; bottom)parameter sharing schemes. X.-L:\nCross-layer, X.-T: Cross-Transformer sharing. We report top-1 and top-5 accuracy (%). †: Ours.\nWhile both Early and Late perform similarly in the audio-visual scenario, only Late can be used\nin unimodal downstream scenarios (c.f., Early requires the presence of both modalities). This has\npractical implications: Mid and Late can effectively handle missing modalities, i.e., once pretrained\non audio-visual data, we can use it on any of audio-visual, audio-only, and visual-only scenarios. Our\nMid fusion approach enjoys both the advantages, i.e., learning cross-modal relationship and being\nrobust to missing modalities, achieving overall the best performance.\nNegative Sampling Strategies.We compare four strategies: (i) Current-Sequence takes all\nbut the positive instance from the same sequence as negatives, (ii) Current-MiniBatch takes\nall but the positive instance in the mini-batch as negatives; this subsumes Current-Sequence,\n(iii) CANS-Dissimilar stochastically samples negatives using a modiﬁed version of our content-\naware negative sampling (CANS) that favorsdissimilar samples, and (iv) CANS-Similar is our\nproposed CANS approach that favors negatives that are similar to the positive instance.\nTable 1 (b) shows Current-Sequence is the least effective: It makes MEP too difﬁcult because\nnegatives are (sometimes too much) similar to positives. As a result, the training dynamics is\ndominated by CPP, which is relatively easier, leading to inferior performance. We make quite the\ncontrary observations fromCurrent-MiniBatch: the inclusion of negatives from different videos\nmakes MEP easier and thus makes it dominate the training dynamics. Our CANS approach solves\nboth these issues by eliminating negatives that are either almost identical to or trivial to distinguish\nfrom the positives, based on the 95% CI over the CNN embedding distances. It also samples negatives\nin a stochastic manner so a wide variety of samples can be included as negatives. Our proposed\nCANS-Similar can be considered as a “softened” version ofCurrent-Sequence; it samples\nnegatives that are similar to positives with a high probability (this can be considered as online hard\nnegative mining), but it also takes instances from different videos with a lower probability. This\nbalances out hard and easy negatives, making the MEP task effective.\nParameter Sharing Schemes.Our parameter reduction scheme reduces the number of parameters\nfrom 128M to 4M (by 97%) (Table 1 (c)). We reduce the model size by sharing weights across\nTransformers and across layers. We validate these ideas in two sets of experiments. Table 1\n(c) compares cross-Transformer weight sharing schemes. We use Multi-6 that uses all three\nTransformers with 6 layers each, and compare four methods that correspond to Figure 2 (a)-(d).\nNote that No sharing is too large to ﬁt in a Tesla V100 GPU (16GB) even with 2 samples, so we\ndeﬁne Multi-2 that uses three Transformers with 2 layers each, and with the reduced number of\nattention heads Ato 5, the feature dimension Dto 320 and the intermediate dimension Eto 1280.\nWe see that our proposed approach, Part, achieves the best performance with the least number of\nparameters. One might ask how Part leads to a smaller model when All shares all the weights\nacross Transformers: We decompose weights W = UΣV⊤with low-rank approximation and share\nonly U across Transformers, while the ΣV⊤part learns modality-speciﬁc dynamics. Table 1 (d)\ncompares cross-layer weight sharing schemes using the visual Transformer with either 2 (Vis-2) or\n6 (Vis-6) layers. The results show that sharing weights across layers does not hurt the performance,\nconﬁrming the observations by Lan et al. (2020) in the audio-visual setting.\nPretraining Objectives.To evaluate the importance of MEP and CPP tasks, we test two settings:\n(i) Mid-w/o-CPP and (ii) Mid-w/o-MEP. On Kinetics-Sounds, these achieve 65.9% and 64.6%,\nrespectively; ours achieve 67.5% (top-1 accuracy). The result show that the MEP task plays an\nimportant role during pretraining, conﬁrming the ﬁndings from Sun et al. (2019a) that the InfoNCE\n7\nPublished as a conference paper at ICLR 2021\na) Model Net Data UCF\nST-Puzzle 3D-R18 K400 65.8\nClipOrder R(2+1)D UCF 72.4\nDPC 3D-R34 K400 75.7\nCBT S3D K600 79.5\nMultiSens 3D-R18 AS 82.1\nA VTS MC3-18 K400 85.8\nA VTS MC3-18 AS 89.0\nV-CNN† SlowFast K700 85.2\nV-CNN† SlowFast AS 86.1\nb) Model Net Data ESC\nSVM MLP - 39.6\nConvAE CNN-4 39.9\nRF MLP - 44.3\nConvNet CNN-4 - 64.5\nSoundNet CNN-8 FS 74.2\nL3-Net CNN-8 FS 79.3\nDMC VGG-ish FS 79.8\nA VTS VGG-M AS 80.6\nA-CNN† R50 AS 81.5\nc) Model Charades KS\nRandom 5.9 - / -\nATF 18.3 - / -\nATF (OF) 22.4 - / -\nV-CNN 18.7 45.8 / 73.3\nA-CNN 18.9 49.4 / 76.9\nM-CNN 23.1 59.4 / 83.6\nV-BERT 26.0 49.5 / 78.9\nA-BERT 27.4 58.9 / 85.7\nM-BERT† 29.5 75.6 / 94.6\nDatasets. K: Kinetics, AS: AudioSet, FS: Flicker-SoundNet, KS: Kinetics-Sounds. Baselines. ST-Puzzle (Kim et al., 2019), ClipOrder (Xu et al., 2019), DPC (Han\net al., 2019), CBT (Sun et al., 2019a), MultiSens (Owens & Efros, 2018), A VTS (Korbar et al., 2018), AE (Aytar et al., 2016), SVM (Piczak, 2015a), RF (Piczak,\n2015a), ConvNet (Piczak, 2015b), SoundNet (Aytar et al., 2016),L3-Net (Arandjelovic & Zisserman, 2017), DMC (Hu et al., 2019), ATF (Sigurdsson et al., 2017)\nTable 2: (a; left): Short video classiﬁcation results on UCF101 (mean accuracy (%)). (b; cen-\nter): Short audio classiﬁcation results on ESC-50 (mean accuracy (%)). (c; right): Long video\nclassiﬁcation results on Charades (mAP) and Kinetics-Sounds (KS; top-1/5 accuracy (%)). †: Ours.\nloss, as deployed in CBT, is effective in the cross-modal setting. The result also shows that augmenting\nMEP with CPP provides further performance improvement by learning cross-modal correspondence.\nDownstream Evaluation.We pretrain our model with Mid fusion using MEP and CPP tasks (with\nCANS-Similar), and employ Part weight sharing. We use either Kinetics-700 or AudioSet for\nfair comparisons with prior work. Table 2 (a)/(b) shows short-video/audio classiﬁcation results on\nUCF-101/ESC-50. For fair comparisons to the baselines, we use only the visual/audio CNN (no\nTransformers); we ﬁnetune a linear classiﬁer on top of the visual CNN end-to-end for UCF-101, and\ntrain a multi-class one-vs-all linear SVM on top of the ﬁxed audio CNN for ESC-50. Although our\nmodel is pretrained on long video clips with no direct supervision to the CNN layers (gradients must\nﬂow through Transformers), it outperforms most of the baselines (except for A VTS on UCF-101)\nthat received direct supervision from short video clips. We note that, similar to ours, CBT (Su et al.,\n2020) is a multimodal Transformer pretrained on long video clips and thus is the most meaningful\ncomparison to ours; ours outperform CBT on UCF-101 by 5.7%. For sound classiﬁcation, our\napproach outperform all existing published results.\nTable 2 (c) shows long-video classiﬁcation results on Charades and Kinetics-Sounds (KS) when pre-\ntrained on Kinetics-700. We test Visual-only (V), Audio-only (A), and Multimodal\n(M) settings to verify the beneﬁt of multimodal learning. Because there is no published self-\nsupervised learning results on these datasets, we demonstrate long-term representations by comparing\nCNNs (CNN; short-term) to Transformers (BERT; long-term) on KS that contains 10-second clips.\nSince CNNs process 1-second clips, we feed 10 non-overlapping clips to CNNs and average the pre-\ndiction output. In all settings, we add a 2-layer MLP with softmax classiﬁer on top. The results show\nthat Transformers outperform CNNs on Kinetics-Sounds, suggesting the superiority of long-term\nrepresentations. We also see that combining audio-visual information performs the best. We notice\nthat audio representations are generally stronger than visual representations; we believe that learn-\ning discriminative visual representations is generally more challenging, especially when the CNNs\nreceive (self-)supervision signals only indirectlythrough Transformers. We believe that providing\n(self-)supervision directly to CNNs, e.g., by ﬁrst pretraining CNNs on 3D rotation prediction (Jing\net al., 2018) and then jointly training the whole model (as was done in CBT (Sun et al., 2019a)), could\nfurther improve performance. Incorporating contrastive learning (Chen et al., 2020) over the CNN\nembeddings and training the whole model end-to-end is another promising direction for future work.\n4 R ELATED WORK\nMultimodal BERT.Extending BERT (Devlin et al., 2019) to vision-and-language has been actively\nstudied. Existing work typically adopt early fusion (Li et al., 2019; Alberti et al., 2019; Sun et al.,\n2019b; Li et al., 2020; Zhou et al., 2020; Su et al., 2020; Chen et al., 2019b; Zhu & Yang, 2020) or\nmid fusion (Tan & Bansal, 2019; Lu et al., 2019; Sun et al., 2019a; Luo et al., 2020) without thorough\nvalidation, and they train only visual components while relying on a language-pretrained BERT.\nAlthough there have been some efforts to leverage the Transformer architecture (Vaswani et al., 2017)\nfor audio and visual inputs (Boes & Van hamme, 2019; Tian et al., 2020), our approach is the ﬁrst to\ndemonstrate multimodal audio-visual BERT trained from scratch in an end-to-end manner. This is\nenabled by our novel parameter reduction technique, which is one of our main technical contributions.\n8\nPublished as a conference paper at ICLR 2021\nAudio-Visual Learning.Early work in audio-visual learning focused on speech signals, improving\naudio-visual speech recognition than unimodal approaches (Ngiam et al., 2011; Srivastava & Salakhut-\ndinov, 2012). Recent approaches leverage unlabeled videos from speciﬁc domains (Owens et al.,\n2016; Gao & Grauman, 2019; Zhao et al., 2018; Ephrat et al., 2018; Alwassel et al., 2019; Miech et al.,\n2020; Piergiovanni et al., 2020) and often demonstrate on audio-visual source separation, localization,\nand co-segmentation. However, these approaches rely on short-term audio-visual correspondence and\nthus may not generalize to long-term video recognition that requires global context (as was suggested\nin (Hjelm et al., 2019)), which this work focuses on.\nParameter Reduction. Network pruning (Reed, 1993; Caron et al., 2020) trains a large model\nand then reduces its size while maintaining performance. Reducing the size of CNNs for mobile\napplications is an active research area (Rastegari et al., 2016; Howard et al., 2017; 2019; Zhang et al.,\n2018; Iandola et al., 2016). Our work is closely related to the work that shares parameters across layers\nin deep neural networks. Trellis network (Bai et al., 2019b) is a temporal convolutional architecture\nwith weight-tying across time and depth. Similar to ours, Universal Transformer (Dehghani et al.,\n2019), RSNMT (Dabre & Fujita, 2019), DEQ (Bai et al., 2019a), ALBERT (Lan et al., 2020) share\nweights across layers in Transformers. We combine this idea with our novel cross-Transformer weight\nsharing, which decomposes weight matrices with low-rank approximation.\nNegative Sampling. Hard negative mining has been shown to be crucial for contrastive learn-\ning (Arandjelovic & Zisserman, 2017; Owens & Efros, 2018; Korbar et al., 2018; Schroff et al.,\n2015; Zhuang et al., 2019; Morgado et al., 2020; Wu et al., 2020). Korbar et al. (2018) use the time\ndifference between clips to approximate clip similarity (i.e., clips that are further apart are deemed\nmore different). However, such an assumption may not hold for real-world videos, e.g., periodic\nactions such as push-ups. Unlike this line of approaches, we directly use the feature embeddings\nlearned by our model. Several apparoaches adapted a similar idea (Schroff et al., 2015; Zhuang et al.,\n2019; Morgado et al., 2020; Wu et al., 2020). Different from prior work, we bring the stochasticity to\nthe sampling procedure by using the content similarity as the sampling probability; this helps reduce\npotential errors especially during the early stage of training.\n5 C ONCLUSION\nWe introduced a multimodal bidirectional Transformer architecture for self-supervised learning of\ncontextualized audio-visual representation from unlabeled videos. Our main technical contributions\ninclude: (1) we propose a parameter efﬁcient multimodal Transformers based on matrix decomposi-\ntion with low-rank approximation; (2) we propose a novel content-aware negative sampling technique\nfor contrastive learning. We demonstrate a successful end-to-end training of multimodal Transformers\nfor audio-visual learning (which is, to the best of our knowledge, the ﬁrst time in the literature). We\nalso report comprehensive evaluation of various design decisions in multimodal learning.\nAcknowledgements. This work was partially supported by Institute of Information & communi-\ncations Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)\n(No.2017-0-01772, Video Turing Test, No.2019-0-01082, SW StarLab) and the international coopera-\ntion program by the NRF of Korea (NRF-2018K2A9A2A11080927).\nREFERENCES\nChris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of Detected Objects in Text\nfor Visual Question Answering. In EMNLP-IJCNLP, 2019.\nHumam Alwassel, Dhruv Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-\nSupervised Learning by Cross-Modal Audio-Video Clustering. arXiv preprint arXiv:1911.12667,\n2019.\nRelja Arandjelovic and Andrew Zisserman. Look, Listen and Learn. In ICCV, 2017.\nRelja Arandjelovic and Andrew Zisserman. Objects that Sound. In ECCV, 2018.\nYusuf Aytar, Carl V ondrick, and Antonio Torralba. SoundNet: Learning Sound Representations from\nUnlabeled Video. In NeurIPS, 2016.\n9\nPublished as a conference paper at ICLR 2021\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer Normalization. In NeurIPS Deep\nLearning Symposium, 2016.\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep Equilibrium Models. In NeurIPS, 2019a.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis Networks for Sequence Modeling. In ICLR,\n2019b.\nTadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal Machine Learning: A\nSurvey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):\n423–443, 2018.\nWim Boes and Hugo Van hamme. Audiovisual Transformer Architectures for Large-Scale Classiﬁca-\ntion and Synchronization of Weakly Labeled Audio Events. In ACM MM, 2019.\nMathilde Caron, Ari Morcos, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Pruning Convolu-\ntional Neural Networks with Self-Supervision. arXiv preprint arXiv:2001.03554, 2020.\nJoao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A Short Note on the Kinetics-700\nHuman Action Dataset. arXiv preprint arXiv:1907.06987, 2019.\nBeidi Chen, Weiyang Liu Animesh Garg, Zhiding Yu, Anshumali Shrivastava, Jan Kautz, and Anima\nAnandkumar. Angular Visual Hardness. In ICML Workshop on Identifying and Understanding\nDeep Learning Phenomena, 2019a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework for\nContrastive Learning of Visual Representations. arXiv preprint arXiv:2002.05709, 2020.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. arXiv preprint\narXiv:1909.11740, 2019b.\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning Phrase Representations using RNN Encoder–Decoder for\nStatistical Machine Translation. In EMNLP, 2014.\nRaj Dabre and Atsushi Fujita. Recurrent Stacking of Layers for Compact Neural Machine Translation\nModels. In AAAI, 2019.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal\nTransformers. In ICLR, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In NAACL-HLT, 2019.\nAriel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T\nFreeman, and Michael Rubinstein. Looking to Listen at the Cocktail Party: A Speaker-Independent\nAudio-Visual Model for Speech Separation. ACM Transactions on Graphics, 37(4), 2018.\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. SlowFast Networks for Video\nRecognition. In ICCV, 2019.\nRuohan Gao and Kristen Grauman. 2.5D Visual Sound. In CVPR, 2019.\nJort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio Set: An Ontology and Human-Labeled Dataset\nfor Audio Events. In ICASSP, 2017.\nGene H Golub and Charles F Van Loan. Matrix Computations, volume 3. JHU press, 2012.\nChunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Caroline Pantofaru, Yeqing Li, Sudheendra\nVijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. A V A: A Video\nDataset of Spatio-temporally Localized Atomic Visual Actions. In CVPR, 2018.\n10\nPublished as a conference paper at ICLR 2021\nMichael Gutmann and Aapo Hyvärinen. Noise-Contrastive Estimation: A New Estimation Principle\nfor Unnormalized Statistical Models. In AISTATS, 2010.\nTengda Han, Weidi Xie, and Andrew Zisserman. Video Representation Learning by Dense Predictive\nCoding. In ICCV Workshop on Large Scale Holistic Video Understanding, 2019.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image\nRecognition. In CVPR, 2016.\nDan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). arXiv preprint\narXiv:1606.08415, 2016.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. In ICLR, 2019.\nSepp Hochreiter and Jürgen Schmidhuber. Long Short-Term Memory. Neural computation, 9(8):\n1735–1780, 1997.\nAndrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun\nWang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for MobileNetV3. InICCV,\n2019.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. MobileNets: Efﬁcient Convolutional Neural Networks for\nMobile Vision Applications. arXiv preprint arXiv:1704.04861, 2017.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁcation.\nIn ACL, 2018.\nDi Hu, Feiping Nie, and Xuelong Li. Deep Multimodal Clustering for Unsupervised Audiovisual\nLearning. In CVPR, 2019.\nForrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt\nKeutzer. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size.\narXiv preprint arXiv:1602.07360, 2016.\nLonglong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-Supervised Spatiotemporal Feature\nLearning via Video Rotation Prediction. arXiv preprint arXiv:1811.11387, 2018.\nWill Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The Kinetics Human Action Video\nDataset. arXiv preprint arXiv:1705.06950, 2017.\nEvangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. EPIC-Fusion: Audio-\nVisual Temporal Binding for Egocentric Action Recognition. In ICCV, 2019.\nDahun Kim, Donghyeon Cho, and In So Kweon. Self-Supervised Video Representation Learning\nwith Space-Time Cubic Puzzles. In AAAI, 2019.\nBruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative Learning of Audio and Video Models\nfrom Self-Supervised Synchronization. In NeurIPS, 2018.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. In\nICLR, 2020.\nMario Lezcano-Casado and David Martínez-Rubio. Cheap Orthogonal Constraints in Neural Net-\nworks: A Simple Parametrization of the Orthogonal and Unitary Group. In ICML, 2019.\nGen Li, Nan Duan, Yuejian Fang, Min Gong, Daxin Jiang, and Ming Zhou. Unicoder-VL: A Universal\nEncoder for Vision and Language by Cross-modal Pre-training. In AAAI, 2020.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. arXiv preprint arXiv:1908.03557, 2019.\n11\nPublished as a conference paper at ICLR 2021\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visiolin-\nguistic Representations for Vision-and-Language Tasks. In NeurIPS, 2019.\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou.\nUniViLM: A Uniﬁed Video and Language Pre-Training Model for Multimodal Understanding and\nGeneration. arXiv preprint arXiv:2002.06353, 2020.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisser-\nman. End-to-End Learning of Visual Representations from Uncurated Instructional Videos. In\nCVPR, 2020.\nPedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-Visual Instance Discrimination with\nCross-Modal Agreement. arXiv preprint arXiv:2004.12943, 2020.\nJiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multi-\nmodal Deep Learning. In ICML, 2011.\nJorge Nocedal and Stephen Wright. Numerical Optimization. Springer Science & Business Media,\n2006.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive\nPredictive Coding. arXiv preprint arXiv:1807.03748, 2018.\nAndrew Owens and Alexei A Efros. Audio-Visual Scene Analysis with Self-Supervised Multisensory\nFeatures. In ECCV, 2018.\nAndrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H Adelson, and William T\nFreeman. Visually Indicated Sounds. In CVPR, 2016.\nAndrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Learning\nSight from Sound: Ambient Sound Provides Supervision for Visual Learning. International\nJournal of Computer Vision, 126(10):1120–1137, 2018.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V\nLe. SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. In\nINTERSPEECH, 2019.\nMandela Patrick, Yuki M Asano, Ruth Fong, João F Henriques, Geoffrey Zweig, and Andrea\nVedaldi. Multi-modal Self-Supervision from Generalized Data Transformations. arXiv preprint\narXiv:2003.04298, 2020.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep Contextualized Word Representations. In NAACL-HLT, 2018.\nKarol J. Piczak. ESC: Dataset for Environmental Sound Classiﬁcation. In ACM-MM, 2015a.\nKarol J. Piczak. Environmental Sound Classiﬁcation with Convolutional Neural Networks. In MLSP,\n2015b.\nAJ Piergiovanni, Anelia Angelova, and Michael S Ryoo. Evolving Losses for Unsupervised Video\nRepresentation Learning. In CVPR, 2020.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\nstanding by Generative Pre-Training. Techincal Report, OpenAI, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. OpenAI Blog, 1(8), 2019.\n12\nPublished as a conference paper at ICLR 2021\nMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: ImageNet\nClassiﬁcation Using Binary Convolutional Neural Networks. In ECCV, 2016.\nSashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the Convergence of Adam and Beyond. In\nICLR, 2018.\nRussell Reed. Pruning Algorithms-A Survey. IEEE Transactions on Neural Networks, 4(5):740–747,\n1993.\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Uniﬁed Embedding for Face\nRecognition and Clustering. In CVPR, 2015.\nGunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta.\nHollywood in Homes: Crowdsourcing Data Collection for Activity Understanding. In ECCV,\n2016.\nGunnar A Sigurdsson, Santosh Divvala, Ali Farhadi, and Abhinav Gupta. Asynchronous Temporal\nFields for Action Recognition. In CVPR, 2017.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A Dataset of 101 Human\nAction Classes From Videos in The Wild. CRCV-TR-12-01, 2012.\nNitish Srivastava and Russ R Salakhutdinov. Multimodal Learning with Deep Boltzmann Machines.\nIn NeurIPS, 2012.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training\nof Generic Visual-Linguistic Representations. In ICLR, 2020.\nChen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning Video Representations\nusing Contrastive Bidirectional Transformer. arXiv preprint arXiv:1906.05743, 2019a.\nChen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A Joint\nModel for Video and Language Representation Learning. In ICCV, 2019b.\nHao Tan and Mohit Bansal. LXMERT: Learning Cross-Modality Encoder Representations from\nTransformers. In EMNLP-IJCNLP, 2019.\nYapeng Tian, Dingzeyu Li, and Chenliang Xu. Uniﬁed Multisensory Perception: Weakly-Supervised\nAudio-Visual Video Parsing. In ECCV, 2020.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep Image Prior. In CVPR, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention Is All You Need. In NeurIPS, 2017.\nMike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On Mutual\nInformation in Contrastive Learning for Visual Representations. arXiv preprint arXiv:2005.13149,\n2020.\nDejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised Spatiotem-\nporal Learning via Video Clip Order Prediction. In CVPR, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXLNet: Generalized Autoregressive Pretraining for Language Understanding. In NeurIPS, 2019.\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShufﬂeNet: An Extremely Efﬁcient\nConvolutional Neural Network for Mobile Devices. In CVPR, 2018.\nHang Zhao, Chuang Gan, Andrew Rouditchenko, Carl V ondrick, Josh McDermott, and Antonio\nTorralba. The Sound of Pixels. In ECCV, 2018.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Uniﬁed\nVision-Language Pre-Training for Image Captioning and VQA. In AAAI, 2020.\n13\nPublished as a conference paper at ICLR 2021\nLinchao Zhu and Yi Yang. ActBERT: Learning Global-Local Video-Text Representations. InCVPR,\n2020.\nChengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local Aggregation for Unsupervised Learning\nof Visual Embeddings. In ICCV, 2019.\nA I MPLEMENTATION DETAILS\nA.1 A RCHITECTURES OF VISUAL /AUDIO CNN S\nTable 3 shows the architectures of visual and audio CNNs we use for our model. For the visual CNN,\nwe use the SlowFast network (Feichtenhofer et al., 2019) with a ResNet-50 backbone (He et al.,\n2016). We use the speed ratio α= 8 and the channel ratio β = 1/8 for the SlowFast architecture,\nso Tf = 8 ×Ts. We use different values of Ts and Tf for different tasks. During pretraining, we\nset Ts = 4 and Tf = 32. During ﬁnetuning, we use Ts = 8 and Tf = 64 for short-video action\nclassiﬁcation on UCF101 (Soomro et al., 2012) while we use Ts = 4 and Tf = 32 for long-video\naction classiﬁcation on Charades (Sigurdsson et al., 2016) and Kinetics-Sounds (Arandjelovic &\nZisserman, 2017). For the audio CNN, we use a ResNet-50 without the downsampling layer pool1 to\npreserve information along both frequency and time axis in early stages. We use different values of\nTa for different training phases. We set Ta = 220 for one-second clip during pretraining while we\nuse Ta = 440 for two-second clip during ﬁnetuning.\nStage Visual CNN Audio CNNSlow pathway Fast pathway\nraw clip 3 ×Ts ×1122 3 ×Tf ×1122 128 ×Ta\nconv1\n1 ×72,64\nstride 1,22\n5 ×72,8\nstride 1,22\n9 ×9,32\nstride 1,1\npool1\n1 ×32,max\nstride 1,22\n1 ×32,max\nstride 1,22 –\nres2\n\n\n1 ×12,64\n1 ×32,64\n1 ×12,256\n\n×3\n\n\n3 ×12,8\n1 ×32,8\n1 ×12,32\n\n×3\n[1 ×1,32\n3 ×3,32\n1 ×1,128\n]\n×3\nres3\n\n\n1 ×12,128\n1 ×32,128\n1 ×12,512\n\n×4\n\n\n3 ×12,16\n1 ×32,16\n1 ×12,64\n\n×4\n[1 ×1,64\n3 ×3,64\n1 ×1,256\n]\n×4\nres4\n\n\n3 ×12,256\n1 ×32,256\n1 ×12,1024\n\n×6\n\n\n3 ×12,32\n1 ×32,32\n1 ×12,128\n\n×6\n[1 ×1,128\n3 ×3,128\n1 ×1,512\n]\n×6\nres5\n\n\n3 ×12,512\n1 ×32,512\n1 ×12,2048\n\n×3\n\n\n3 ×12,64\n1 ×32,64\n1 ×12,256\n\n×3\n[1 ×1,256\n3 ×3,256\n1 ×1,1024\n]\n×3\nTable 3: The architectures of visual and audio CNNs. For the visual CNN, the input dimensions\nare denoted by {channel size, temporal size, spatial size2}, kernels are denoted by {temporal size,\nspatial size2, channel size}and strides are denoted by {temporal stride, spatial stride2}. For the audio\nCNN, the input dimensions are denoted by {frequency size, temporal size}, kernels are denoted by\n{frequency size, time size, channel size}and strides are denoted by {frequency stride, temporal stride}.\nA.2 D ATA PREPROCESSING\nWe preprocess the data by dividingT-second clips into T non-overlapping parts (T = 30 for Kinetics-\n700 (Carreira et al., 2019) and T = 10 for AudioSet (Gemmeke et al., 2017)) and sampling 16 frames\nfrom each. For audio stream, we take waveform sampled at 44.1 kHz and convert it to log-mel-scaled\nspectrogram. We augment audio data with random frequency/time masking using SpecAugment (Park\net al., 2019), and visual data with color normalization, random resizing, random horizontal ﬂip, and\n14\nPublished as a conference paper at ICLR 2021\n(a-1) MEP + CPP(a-2) Visual MEP(b) Ablation: parameter sharing(a) Ablation: negative samplingMEP + CPP\nFigure 3: Loss curves during pretraining under different ablative settings. (a) compares Content-\nAware Negative Sampling (CANS) that favors negatives that are dissimilar vs. similar to the positive\ninstance. (b) compares different cross-Transformer weight sharing schemes; see the text for details.\nrandom cropping to obtain 112 ×112 pixel frames; for test data, we resize videos to 128 pixels on\nthe shorter side and take three equidistant crops of 128 ×128 pixels to cover the entire region. We\nalso apply audio-visual synchronized temporal jittering (Patrick et al., 2020).\nA.3 D OWNSTREAM EVALUATION\nFor evaluation on UCF101, we follow the test protocol of (Feichtenhofer et al., 2019): We sample\n10 clips from each test video at a uniform time interval, and for each sampled clip, we take three\nequidistant spatial crops, resulting in a total of 30 views. We use each of the 30 views as input to our\nvisual CNN and average the prediction scores from all 30 views to obtain the ﬁnal prediction result.\nFor evaluation on ESC-50 (Piczak, 2015a), we extract 10 equally spaced 2-second clips from each\ntest audio sample. We use each of 10 clips as input to our audio CNN and average the prediction\nscores to obtain the ﬁnal prediction result. For evaluation on Charades and Kinetics-Sounds, we use\nthree audio-visual sequences with different spatial crops from a test video and max-pool/average the\nprediction scores from each sequence, respectively.\nA.4 O PTIMIZATION\nIn all experiments, we use the AMSGrad (Reddi et al., 2018) variant of AdamW (Loshchilov &\nHutter, 2019) optimizer with β1 = 0.9, β2 = 0.98, L2 weight decay of 1e-4. We use a learning rate\nwarm-up for the ﬁrst 6% of iterations followed by a linear decay of learning rate.\nFrom the observations of Lezcano-Casado and Martínez-Rubio (Lezcano-Casado & Martínez-Rubio,\n2019), we have 10 times less learning rate for the orthogonal parameters than that for the non-\northogonal parameters: we use 1e-5 for the former and 1e-4 for the latter.\nWe pretrain our model on Kinetics-700 (Carreira et al., 2019) with a batch size 256 for 220K\niterations and AudioSet (Gemmeke et al., 2017) with a batch size 300 for 220K iterations in the main\nexperiments; for the ablation study, we use a much smaller batch size of 4 and pretrain our model on\nKinetics-700 for 80K iterations.\nFor ﬁnetuning on UCF101, we train our model for 40K iterations with a batch size of 64 and learning\nrate of 0.02. For evaluation on ESC-50, we train a multi-class one-vs-all linear SVM on top of our\nﬁxed audio CNN for 38K iterations with a batch size of 128 and learning rate of 0.003. For ﬁnetuning\non Charades, we train for 40K iterations with a batch size of 8, with learning rate of 0.001 for the\nclassiﬁer and CNN parameters, 1e-5 for the orthogonal parameters and 1e-4 for the rest parameters.\nFor ﬁnetuning on Kinetics-Sounds, we train for 24K iterations with a batch size of 32, with learning\nrate of 0.005 for the classiﬁer and CNN parameters, 1e-4 for the orthogonal parameters and 1e-3 for\nthe rest parameters.\n15\nPublished as a conference paper at ICLR 2021\nB E XTRA RESULTS FROM THE ABLATION STUDY\nB.1 N EGATIVE SAMPLING STRATEGIES\nWe proposed the content-aware negative sampling strategy (CANS) using pairwise l2 distances\nbetween CNN embeddings. We introduced two variants of CANS: CANS-Dissimilar that favors\nnegatives that are dissimilar to the positive instance and CANS-Similar that favors negatives that\nare similar to the positive instance. We chose to use CANS-Similar based on the results from our\nablation study presented in the main paper, Table 1 (b).\nFigure 3 (a) in this appendix provides additional evidence that supports our decision. We see that\nthe loss of CANS-Disimilar initially drops rapidly but starts increasing around iteration 7K\nand continues to increase until around 15K; this is mainly caused by the visual MEP loss shown\nin Figure 3 (a-2). One explanation for this might that CANS-Disimilar is easier to solve than\nCANS-Similar, which causes the loss landscape of CANS-Disimilar to contain too many\nshallow local minima compared to that of CANS-Similar. Recall that we use a learning rate\nwarm-up for the ﬁrst 6% of iterations during pretraining; this roughly equals to the ﬁrst 13K (out\nof 220K) iterations. Given this, we speculate that the model got out of a local minima around\niteration 7K (most likely due to the increasing learning rate), and then eventually settled in another\n(bad) local minima after the warm-up period ended. Compared to this, we observe much milder\nlearning dynamics with CANS-Similar: the loss decreases relatively slowly but steadily, and\neventually leaps around 35K to go below the loss of CANS-Disimilar. We, again, believe that\nthis is because CANS-Similar is more difﬁcult to solve than CANS-Disimilar (as shown by\nthe slower decrease in loss values), which caused the resulting loss landscape to contain steeper local\nminima. Our model eventually found one of those after round 40K of iterations, resulting in a better\nperforming model in the downstream tasks shown in Table 1 (b) of the main paper (the loss kept\nslowly decreasing after iteration 50K).\nB.2 P ARAMETER SHARING SCHEMES\nWe proposed a cross-Transformer weight sharing technique, which decomposes weight matrices with\nlow-rank approximation. Recall that each layer of a Transformer contains the multi-head attention\nlayer weights {Wq,Wk,Wv,Wb}and the feed-forward layer weights{Wc,Wd}. We chose to share\nall six weight matrices across Transformers, though we could have shared any combination of them.\nTo justify this design choice, we empirically compared three variants: (i) Multi-6 that do not share\nparameters across Transformers, (ii) Multi-6-Part_Att that shares only {Wq,Wk,Wv,Wb}\n(but not {Wc,Wd}) and (iii) Multi-6-Part that shares all six weight matrices. Figure 3 (b)\nshows that there is not much difference between all the variants in terms of the loss curves; we chose\nto use Multi-6-Part that requires the least number of parameters. We showed that our approach\noutperforms Multi-6 in the ablation study (Table 1 (c-left) in the main paper).\nB.3 J USTIFICATION FOR THE MEP L OSS FORMULATION\nSince the multimodal Transformer hAV has access to both visual and audio inputs, one might think\nthat the model could “leak” information about visual input into za and information about audio\ninput into zv, which could make MEP trivial to solve. Here we show that this is not the case. By\nconstruction, we mask the same positions in audio and visual streams when designing the MEP task,\nso the model has no access to the masked input even in a cross-modal manner. Empirically, removing\nthe third term in Eq. 6 (LNCE([xa; xv],˜z)) leads to performance degradation in Kinetics-Sounds, i.e.,\ntop-1 accuracy 66.7% vs. ours 67.5% (see Table 1), which suggests that solving the MEP task in the\nmultimodal Transformer is beneﬁcial to our model.\nB.4 J USTIFICATION FOR THE CPP L OSS FORMULATION\nRecall that our CPP loss has two terms; the ﬁrst term uses the summary embeddings sg and the\nsecond term uses output embeddings sh sampled at random positions; see Eq. 7. One could argue\nthat the two terms are redundant as bidirectional Transformers have “one-step” access to all the input\nembeddings, and thus solving CPP only with the summary embeddings (the ﬁrst term) would be\nenough. This is not the case. We encode sh with position-speciﬁc information through the time\n16\nPublished as a conference paper at ICLR 2021\nembeddings pt, which makes every sh different compared to sg. Empirically, we ﬁnd that removing\nthe second term of Eqn. 7 (sh) in our CPP loss leads to an inferior accuracy 66.9% vs. ours 67.5% on\nKinetics-Sounds, suggesting its importance in learning.\nB.5 U SE OF MODALITY EMBEDDINGS IN THE MULTIMODAL TRANSFORMER\nWe use modality embeddings mv and ma as part of input to the multimodal Transformer in order\nto distinguish embeddings coming from visual and audio Transformers. They are learnable weights\ntrained end-to-end with other parameters. Conceptually, incorporating modality-discriminative em-\nbeddings is crucial because of our aggressive weight sharing scheme. Without them, the multimodal\nTransformer will see the output from audio/visual Transformers (ya and yv) as if they are coming\nfrom the same distribution because the two Transformers share a large part of weights. Using modality\nembeddings encourages our model to preserve modality-speciﬁc information in the ﬁnal output, and\nthis empirically leads to performance improvements: ours 67.5% vs. without modality embeddings\n67.1% on Kinetics-Sounds.\nB.6 O N THE IMPORTANCE OF END-TO-END PRETRAINING\nPrevious work in multimodal visual-and-language tasks (Tan & Bansal, 2019; Lu et al., 2019) point\nout that using partially ﬁxed Transformers of different modalities is detrimental to multimodal\nrepresentation learning (c.f., Sun et al. (2019a;b)). We make the same observation in our audio-\nvisual learning scenario. We compare two variants of Multi-6 Part in Table 1 (c), each of\nwhich pretrains only the audio (or visual) CNN/Transformer in the ﬁrst half of pretraining stage and\nthen continues pretraining the remaining weights while ﬁxing the weights of the audio (or visual)\nCNN/Transformer in the second half. This leads to inferior performance (audio-ﬁxed 62.8% and\nvisual-ﬁxed 63.1% vs. ours 67.5%), which is consistent with the results reported in Tan & Bansal\n(2019); Lu et al. (2019).\n17"
}