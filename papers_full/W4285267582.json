{
    "title": "An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers",
    "url": "https://openalex.org/W4285267582",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3021366198",
            "name": "Valentin Hofmann",
            "affiliations": [
                "Ludwig-Maximilians-Universität München",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2111997136",
            "name": "Hinrich Schuetze",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2325354088",
            "name": "Janet Pierrehumbert",
            "affiliations": [
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2890859887",
        "https://openalex.org/W3026401902",
        "https://openalex.org/W3035207248",
        "https://openalex.org/W2798549337",
        "https://openalex.org/W3106416961",
        "https://openalex.org/W3177365697",
        "https://openalex.org/W3213934211",
        "https://openalex.org/W2570821393",
        "https://openalex.org/W2884980639",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3197737114",
        "https://openalex.org/W2461808544",
        "https://openalex.org/W46679369",
        "https://openalex.org/W4302023899",
        "https://openalex.org/W2947415936",
        "https://openalex.org/W3199359833",
        "https://openalex.org/W2954289647",
        "https://openalex.org/W3098265177",
        "https://openalex.org/W3119077874",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2912486864",
        "https://openalex.org/W3037575273",
        "https://openalex.org/W1964965175",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2739688273",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963326795",
        "https://openalex.org/W3103727211",
        "https://openalex.org/W2061551065",
        "https://openalex.org/W4287803080",
        "https://openalex.org/W3016107616",
        "https://openalex.org/W3101140821",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3120157709",
        "https://openalex.org/W2963676641",
        "https://openalex.org/W4309087688",
        "https://openalex.org/W3103671331",
        "https://openalex.org/W3211152275",
        "https://openalex.org/W2561296568",
        "https://openalex.org/W2590654071",
        "https://openalex.org/W3167019521",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W4300466035",
        "https://openalex.org/W2104996750",
        "https://openalex.org/W4245493478",
        "https://openalex.org/W2955002705",
        "https://openalex.org/W2962784628"
    ],
    "abstract": "We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 385 - 393\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nAn Embarrassingly Simple Method to Mitigate und es ira ble\nProperties of Pretrained Language Model Tokenizers\nValentin Hofmann*‡, Hinrich Schütze‡, Janet B. Pierrehumbert†*\n*Faculty of Linguistics, University of Oxford\n†Department of Engineering Science, University of Oxford\n‡Center for Information and Language Processing, LMU Munich\nvalentin.hofmann@ling-phil.ox.ac.uk\nAbstract\nWe introduce FLOTA (Few Longest Token Ap-\nproximation), a simple yet effective method\nto improve the tokenization of pretrained lan-\nguage models (PLMs). FLOTA uses the vo-\ncabulary of a standard tokenizer but tries to\npreserve the morphological structure of words\nduring tokenization. We evaluate FLOTA on\nmorphological gold segmentations as well as\na text classiﬁcation task, using BERT, GPT-2,\nand XLNet as example PLMs. FLOTA leads to\nperformance gains, makes inference more ef-\nﬁcient, and enhances the robustness of PLMs\nwith respect to whitespace noise.\n1 Introduction\nThe ﬁrst step in NLP architectures using pretrained\nlanguage models (PLMs) is to map text to a se-\nquence of tokens corresponding to input embed-\ndings. The tokenizers used to accomplish this have\nbeen shown to exhibit various undesirable prop-\nerties such as generating segmentations that blur\nword meaning (Bostrom and Durrett, 2020; Church,\n2020; Hofmann et al., 2021) and generalizing sub-\noptimally to new domains (Tan et al., 2020; Hong\net al., 2021; Sachidananda et al., 2021).\nIn this paper, we proposeFLOTA(Few Longest\nToken Approximation), a simple yet effective\nmethod to mitigate some shortcomings of PLM\ntokenizers. FLOTA is motivated by the following\nhypothesis: rather than ﬁnding a segmentation that\ncovers all charactersof a word butdestroys its mor-\nphological structure, it can be more beneﬁcial to\nﬁnd a segmentation that does not cover all charac-\nters but preserves key aspects of the morphology.\nWe conﬁrm this hypothesis in this paper.\nOur study investigates three PLMs and corre-\nsponding tokenizers: BERT (base, uncased; Devlin\net al., 2019), which uses WordPiece (Schuster and\nNakajima, 2012; Wu et al., 2016), GPT-2 (base,\ncased; Radford et al., 2019), which uses byte-pair\nencoding (BPE; Gage, 1994; Sennrich et al., 2016),\nand XLNet (base, cased; Yang et al., 2019), which\nuses Unigram (Kudo, 2018). We ﬁnd that FLOTA\nincreases the morphological quality of all tokeniz-\ners as evaluated on human-annotated gold segmen-\ntations as well as the performance of all PLMs on\na text classiﬁcation challenge set.\nContributions. We introduce FLOTA, a simple\nyet effective method to improve the tokenization of\nPLMs during ﬁnetuning. FLOTA uses the vocabu-\nlary of a standard tokenizer but tries to preserve the\nmorphological structure of words during tokeniza-\ntion. We show that FLOTA has three advantages\ncompared to standard tokenization: (i) it can in-\ncrease the performance of PLMs on certain tasks,\nsometimes substantially; (ii) it makes inference\nmore efﬁcient by shortening the processed token\nsequences; (iii) it enhances the robustness of PLMs\nwith respect to certain types of noise in the data.\nAll this is achieved without requiring any addi-\ntional parameters or resourcescompared to vanilla\nPLM ﬁnetuning. We also release a text classiﬁca-\ntion challenge set that can serve as a benchmark for\nfuture studies on PLM tokenizers.1\n2 Few Longest Token Approximation\nLet V be a set of tokens that constitute the vocabu-\nlary of a tokenizer. For the tokenizers discussed in\nthis paper, V contains words, subwords, and char-\nacters. Let φbe a model used by the tokenizer to\nmap text to a sequence of tokens from V.\nFLOTA (Few Longest Token Approximation)\ndiscards φand uses V in a modiﬁed way. Given\na word wnot in V, FLOTA tokenizes it by deter-\nmining the longest substring s∈V of w, returning\ns, and recursing on w \\s, the string(s) remain-\ning when sis removed from w. We stop after k\nrecursive calls or when the residue is null. Fig-\nure 1 provides pseudocode. For the example word\nundesirable and k = 2, FLOTA ﬁrst searches on\n1We make our code and data available at https://\ngithub.com/valentinhofmann/flota.\n385\nMAXSUBWORD SPLIT (w,V )\n1 l= length(w)\n2 for j = ldownto 0\n3 for i = 0to l−j+ 1\n4 s= w[i..i + j]\n5 if s∈V\n6 r= w[0 ..i ] ⊕j w[i+ j..l ]\n7 return s, r, i\nFLOTATOKENIZE (w,k,V )\n1 s,r,i = MAXSUBWORD SPLIT (w,V )\n2 if k== 1 or hyphen(r)\n3 F = {}\n4 F[i] =s\n5 return F\n6 F = FLOTATOKENIZE (r,k −1,V )\n7 F[i] =s\n8 return F\nFigure 1: FLOTA pseudocode. FLOTA is based on a\nrecursive function F LOTATOKENIZE that uses a hash\ntable F to store the longest substring sand its index i\non each recursive call. sand iare found by means of\na second function M AXSUBWORD SPLIT , which also\nreturns a residue r. In practice, to ensure correct in-\ndexing throughout different recursive calls as well as\nprevent using discontinuous substrings for tokeniza-\ntion, we compute rusing an operation ⊕j that concate-\nnates two strings by putting j(length of s) hyphens be-\ntween them. The recursion stops after krecursive calls\nor when r only consists of hyphens (determined by a\nboolean function hyphen). The hash table returned by\nFLOTATOKENIZE is converted to a tokenization using a\nsimple wrapper function that sorts the found substrings\nby their indices (not shown). If M AXSUBWORD SPLIT\ndoes not ﬁnd a substring s ∈V, F LOTATOKENIZE re-\nturns an empty hash table (not shown).\nundesirable and ﬁnds desirable, then searches\non un--------- and ﬁnds un, then stops (since\nk = 2; it would also stop for k > 2 since the\nresidue is null) and returns the tokenization un,\ndesirable. The WordPiece tokenization, on the\nother hand, is und, es, ira, ble.\nFLOTA is guided by the following observations:\nmany words not in V are made up of smaller and\ntypically more frequent elements that determine\ntheir meaning (e.g., they are derivatives such as\nundesirable); many of these elements are in V.2\nBy recursively searching for the longest substrings,\nwe hope to recover the most important meaningful\n2Existing tokenizers have been shown to be able to recover\nthese elements only to a very limited extent (Bostrom and\nDurrett, 2020; Church, 2020; Hofmann et al., 2021).\nModel Tokenization C R M\nBERT FIRST .869 .817 .664\nBERT LONGEST .865 .797 .664\nBERT FLOTA .990 .876 .896\nGPT-2 FIRST .878 .674 .625\nGPT-2 LONGEST .874 .674 .625\nGPT-2 FLOTA .988 .845 .861\nXLNet FIRST .886 .820 .724\nXLNet LONGEST 902 .845 .756\nXLNet FLOTA .992 .900 .922\nTable 1: Morphological quality. C: morphological cov-\nerage (k= 2); R: stem recall; M: full match.\nelements. This is also why it makes sense to stop\nafter krecursions: if FLOTA returns the most im-\nportant meaningful elements as the ﬁrst few tokens,\nwe expect to not lose much by stopping.\n3 Evaluation on Gold Segmentations\nEnglish inﬂection is simple, but the language has\nhighly complex word formation, i.e., derivation and\ncompounding (Cotterell et al., 2017; Pierrehumbert\nand Granell, 2018). To evaluate the morphological\nquality of FLOTA against the standard tokenizers,\nwe thus focus on derivatives and compounds.\nData. Our evaluation uses CELEX (Baayen\net al., 1995) and LADEC (Gagné et al., 2019), two\nlarge datasets of human-annotated gold segmen-\ntations of morphologically complex words. We\nmerge both datasets and extract all words consist-\ning of a preﬁx and a stem (preﬁxed derivatives),\na stem and a sufﬁx (sufﬁxed derivatives), or two\nstems (compounds). We create for each PLM a sub-\nset of words where both morphological elements\n(i.e., stems and afﬁxes) are in the tokenizer vocab-\nulary, but the word itself is not in the tokenizer\nvocabulary. In such cases, a word needs to be seg-\nmented, and it is guaranteed that the gold segmen-\ntation is possible given the tokenizer vocabulary.\nThis procedure results in 11,272, 11,253, 10,848\nwords for BERT, GPT-2, XLNet, respectively.\nExperimental Setup. We deﬁne three metrics\nto analyze how closely FLOTA matches the gold\nsegmentations. We compare against two alterna-\ntive tokenization strategies: representing words\nas the k ﬁrst tokens returned by the standard to-\nkenizer (FIRST) and representing words as the k\nlongest tokens returned by the standard tokenizer\n(LONGEST). Recall that the WordPiece tokeniza-\ntion of the running example undesirable is und,\nes, ira, ble. With k = 3, FIRST is und, es,\nira (i.e., it simply returns the ﬁrst ktokens) and\nLONGEST is und, ira, ble (i.e., it returns the k\n386\nFigure 2: Morphological coverage for varying k.\nlongest tokens in the order in which they occur in\nthe standard tokenization).\nMorphological coverage. We analyze what pro-\nportion of morphological elements is covered by\neach tokenization strategy for varying k, a mea-\nsure that we call morphological coverage, C. For\nundesirable and k = 3, FIRST and LONGEST\ncontain un (C = 0.5) while FLOTA contains both\nun and desirable (C = 1). We compute the mean\nmorphological coverage across all words, C.\nWe ﬁnd that for all three tokenizers, FLOTA\nalready covers about 99% of the morphological el-\nements with just k = 2, a value that FIRST and\nLONGEST only reach with k = 4(Table 1, Fig-\nure 2), indicating that FLOTA needs considerably\nfewer tokens than the standard tokenization to con-\nvey the same amount of semantic and syntactic\ninformation. This can also be seen by examining\nthe average number of tokens needed to fully to-\nkenize a word (i.e., k = ∞), with the values for\nFLOTA (BERT: 2.02; GPT-2: 2.03; XLNet: 2.02)\nbeing lower than the values for the standard tok-\nenization (BERT: 2.30; GPT-2: 2.23; XLNet: 2.26).\nThe pairwise differences are statistically signiﬁcant\n(p< 0.001) as shown by two-tailed t-tests.\nStem recall. Given its relevance for the over-\nall lexical meaning of a word, we are interested\nin how often FLOTA returns the stem at k = 1.\nWe test this using a measure that we call stem\nrecall, R (R = 1 if the token is the stem, 3 oth-\nerwise R = 0), and compute the mean stem re-\ncall Racross all words. We again compare with\nFIRST and LONGEST. Notice the stem according\nto the gold segmentation is longer than the second\nmorphological element in 97% of the examined\ncomplex words, which means that LONGEST pro-\nvides a close estimate of how often the full standard\ntokenization contains the stem (since any other el-\nement in the full standard tokenization is shorter\nand hence very unlikely to be the stem).\n3For compounds: one of the two stems.\nFLOTA returns the stem considerably more often\nthan either FIRST or LONGEST, but there are clear\ndifferences between the models (Table 1): for GPT-\n2, FLOTA increasesRby more than 15% while the\ndifference amounts to 5% for XLNet.\nFull match. Extending the evaluation of stem re-\ncall, we examine whether the tokenization at k= 2\nis identical to the gold segmentation (which al-\nways has two elements) using a measure that we\ncall full match, M (M = 1 if the tokenization\nexactly matches the gold segmentation, otherwise\nM = 0). We again compute the mean value M\nacross all words. Here, the values for both FIRST\nand LONGEST are identical to the performance of\nthe full standard tokenization: for the full standard\ntokenization to exactly match a segmentation of\ntwo elements, it must consist of two tokens, and\nhence it is necessarily equal to both its ﬁrst two\ntokens and its longest two tokens.4 Table 1 shows\nthat FLOTA substantially improvesM.\nThe evaluation on gold segmentations indicates\nthat FLOTA increases the morphological quality\nof PLM tokenizers compared to the standard to-\nkenization and simple alternatives. We also ﬁnd\nunderlying differences in the morphological qual-\nity of the tokenizers, with BPE and Unigram lying\nat the negative and positive extremes, in line with\nprior work (Bostrom and Durrett, 2020). Our anal-\nysis shows that WordPiece lies in between.\n4 Evaluation on Downstream Task\nWe investigate whether the enhanced quality of\nFLOTA tokenizations translates to performance on\ndownstream tasks. We focus on text classiﬁcation\nas one of the most common tasks in NLP.\nData. We create two text classiﬁcation chal-\nlenge sets based on ArXiv,5 each consisting of three\ndatasets. Speciﬁcally, for the subject areas of com-\nputer science, maths, and physics, we extract titles\nfor the 20 most frequent subareas (e.g., Computa-\ntion and Language). We then sample 100/1,000\ntitles per subarea, resulting in three text classiﬁ-\ncation datasets of 2,000/20,000 titles each, which\nwe bundle together as ArXiv-S/L. Our sampling\n4Surprisingly, this does not hold for Unigram, which some-\ntimes creates separate start-of-word tokens; e.g., the Unigram\ntokenization of americanize is _, american, ize, where\n_ is a start-of-word token. Notice that in such cases (with\nk = 2), LONGEST (american, ize) matches the gold seg-\nmentation while FIRST (_, american) does not, explaining\nthe performance difference for XLNet.\n5kaggle.com/Cornell-University/arxiv\n387\nArXiv-S ArXiv-L\nModel Dev Test Dev Test\nBERT .469 .470 .674 .659\n+FLOTA .491 .485 .675 .661\nGPT-2 .329 .324 .526 .507\n+FLOTA .353 .382 .558 .542\nXLNet .435 .454 .660 .641\n+FLOTA .446 .428 .664 .646\nTable 2: Performance. FLOTA leads to gains in av-\neraged F1, particularly for BERT and GPT-2. Perfor-\nmance breakdowns for the individual datasets forming\nArXiv-S/L are provided in Appendix A.3.\nensures that ArXiv-S/L require challenging gen-\neralization from a small number of short training\nexamples with highly complex language. See Ap-\npendix A.1 for more details.\nExperimental Setup. We split the six datasets\nof ArXiv-S and ArXiv-L into 60% train, 20% dev,\nand 20% test. We then train the three PLMs with\nclassiﬁcation heads on the six train splits, once with\nthe standard tokenizers and once with FLOTA. See\nAppendix A.2 for hyperparameters. For FLOTA,\nwe treat kas an additional tunable hyperparameter.\nWe use F1 as the evaluation metric.\nPerformance. The FLOTA models perform bet-\nter than the models with standard tokenization, al-\nbeit to varying degrees for the three PLMs (Table 2).\nThe difference is most pronounced for GPT-2, with\nFLOTA resulting in large performance gains of up\nto 5%. In addition, GPT-2 performs worse than\nthe other two PLMs on all datasets, suggesting\nthat BPE is generally not a good ﬁt for complex\nlanguage. BERT also clearly beneﬁts from using\nFLOTA, particularly on ArXiv-S. Out of the three\nconsidered PLMs, XLNet obtains the smallest per-\nformance gain from using FLOTA, but it still bene-\nﬁts in the majority of cases.\nThe advantage of FLOTA mirrors the differences\nobserved in the morphological analysis, indicating\nthat FLOTA helps close the morphological quality\ngap between standard tokenizations and gold seg-\nmentations. Where the gap is large, gains due to\nFLOTA are large (GPT-2/BPE); where it is small,\ngains due to FLOTA are small (XLNet/Unigram).\nBERT/WordPiece again lies in between.\nImpact of k. To test how the performance varies\nwith k, we focus on BERT and compare the FLOTA\nmodels for k ∈{1,2,3,4}with the two alterna-\ntives FIRST and LONGEST from Section 3. See\nAppendix A.4 for hyperparameters.\nFigure 3 shows that FLOTA only drops slightly\nas we decrease k, with the minimum F1 at k = 1\nFigure 3: FLOTA is less impaired by smaller val-\nues of k (maximum number of tokens per word) than\nFIRST/LONGEST. Results are averaged F1 of BERT\non ArXiv-S (dev/test merged).\nFLOTA\nModel ST k = 1 k = 2 k = 3 k = 4\nBERT 12.9 8.3 10.5 11.4 11.6\nGPT-2 12.9 8.3 10.7 11.5 11.8\nXLNet 13.6 8.3 10.9 11.9 12.2\nTable 3: Average sequence length of titles (ArXiv-L,\nphysics). ST: standard tokenization.\n(43.6%) lying less than 2% below the maximum\nF1 at k = 3 (45.4%). In contrast, FIRST and\nLONGEST drop substantially as we decrease k;\nfor FIRST, the minimum F1 at k= 1(38.2%) lies\nmore than 6% below the maximum F1 at k = 4\n(44.8%). The fact that FLOTA is more effective at\npreserving performance while reducing the number\nof tokens aligns with the observation that it covers\na larger number of morphemes and hence more\nsemantic and syntactic content than FIRST and\nLONGEST for small k(Section 3).\nEfﬁciency. FLOTA allows to reduce the num-\nber of tokens used to tokenize text by varying\nk. Since the attention mechanism scales quadrati-\ncally with sequence length (Peng et al., 2021), this\nhas beneﬁcial effects on the computational cost\ninvolved with employing a model trained using\nFLOTA. We empirically ﬁnd that even for k = 4\n(the largest value used in the experiments), token\nsequences generated by FLOTA are on average\nshorter than the token sequences generated by the\nstandard tokenizations. Table 3 shows for one\ndataset (ArXiv-L, physics) the average sequence\nlength of titles encoded with the standard tokeniza-\ntion versus FLOTA with varying k ∈{1,2,3,4}\nfor the three PLMs.\nRobustness. To examine robustness against\nnoise, a well-known problem for PLMs (Pruthi\net al., 2019), we focus on missing whitespace be-\ntween words (Soni et al., 2019). We randomly drop\nthe whitespace between two adjacent words with\n388\nArXiv-S (N) ArXiv-L (N)\nModel Dev Test Dev Test\nBERT .428 .412 .579 .554\n+FLOTA .486 .447 .652 .632\nGPT-2 .313 .315 .481 .463\n+FLOTA .359 .357 .541 .518\nXLNet .392 .397 .609 .589\n+FLOTA .434 .421 .641 .623\nTable 4: Performance with noise (N). FLOTA clearly\nincreases F1 on ArXiv-S/L for all PLMs when input is\nnoisy. See Appendix A.5 for hyperparameters. Perfor-\nmance breakdowns for the individual datasets forming\nArXiv-S/L are provided in Appendix A.6.\nprobability p= 0.3 in ArXiv-S/L. We useunseen\nnoise, i.e., we only inject noise during evaluation,\nnot training, which is the more realistic and chal-\nlenging scenario (Xue et al., 2021).\nThe results show that synthetic noise increases\nthe performance gap between FLOTA and standard\ntokenization (Table 4). While there is a drop in\nperformance for all models compared to the exper-\niments without noise, the drop is much more pro-\nnounced for standard tokenization; e.g., BERT’s\nperformance on ArXiv-L (test) drops by 3% with\nFLOTA, but by 10% without it.\n5 Limitations\nWhile we ﬁnd FLOTA to work well on text clas-\nsiﬁcation, there are tasks for which FLOTA might\nprove a less suitable tokenization method: e.g., for\nsmall values of k, FLOTA often discards sufﬁxes,\nwhich can be important for tasks with a syntactic\ncomponent such as POS tagging.\nSimilar considerations hold for transfer to lan-\nguages other than English: e.g., in the case of\nlanguages with a non-linear morphology such as\nArabic, FLOTA is expected to inherit the insufﬁ-\nciencies of the underlying tokenizer (Alkaoud and\nSyed, 2020; Antoun et al., 2020).\n6 Related Work\nThe question how PLMs are affected by their\ntokenizer has attracted growing interest recently.\nBostrom and Durrett (2020), Church (2020), Klein\nand Tsarfaty (2020), and Hofmann et al. (2021)\nfocus on the linguistic properties of tokenizers.\nWe contribute to this line of work by conducting\nthe ﬁrst comparative analysis of all three common\nPLM tokenizers and releasing a challenge set as a\nbenchmark for future studies. Another strand of\nresearch has sought to improve PLM tokenizers by\ntraining models from scratch (Clark et al., 2021; Si\net al., 2021; Xue et al., 2021; Zhang et al., 2021) or\nmodifying the tokenizer during ﬁnetuning, mostly\nby adding tokens and corresponding embeddings\n(Chau et al., 2020; Tan et al., 2020; Hong et al.,\n2021; Sachidananda et al., 2021). FLOTA crucially\ndiffers in that it can be used during ﬁnetuning but\ndoes not add any parameters to the PLM. Further-\nmore, there has been work improving tokenization\nby variously exploiting the probabilistic nature of\ntokenizers (Kudo, 2018; Provilkov et al., 2020; Cao\nand Rimell, 2021). By contrast, our method does\nnot need access to the underlying model.\nOur study also relates to computational work on\nderivational morphology (Cotterell et al., 2017; Vy-\nlomova et al., 2017; Cotterell and Schütze, 2018;\nDeutsch et al., 2018; Hofmann et al., 2020a,b,c)\nand word segmentation (Cotterell et al., 2016; Kann\net al., 2016; Ruzsics and Samardži´c, 2017; Mager\net al., 2019, 2020; Seker and Tsarfaty, 2020; Am-\nrhein and Sennrich, 2021). We are the ﬁrst to sys-\ntematically evaluate the segmentations of PLM to-\nkenizers on human-annotated gold data.\nConceptually, the ﬁndings of our study are in\nline with evidence from the cognitive sciences that\nknowledge of a longer (i.e., more detailed and in-\nformative) sequence takes priority over any knowl-\nedge about smaller sequences (Caramazza et al.,\n1988; Laudanna and Burani, 1995; Baayen et al.,\n1997; Needle and Pierrehumbert, 2018).\n7 Conclusion\nWe introduce FLOTA (Few Longest Token Approx-\nimation), a simple yet effective method to improve\nthe tokenization of pretrained language models\n(PLMs). FLOTA uses the vocabulary of a standard\ntokenizer but tries to preserve the morphological\nstructure of words during tokenization. FLOTA\nleads to performance gains, makes inference more\nefﬁcient, and substantially enhances the robustness\nof PLMs with respect to whitespace noise.\nAcknowledgements\nThis work was funded by the European Research\nCouncil (#740516) and the Engineering and Phys-\nical Sciences Research Council (EP/T023333/1).\nThe ﬁrst author was also supported by the German\nAcademic Scholarship Foundation and the Arts\nand Humanities Research Council. We thank the\nreviewers for their helpful comments.\n389\nEthical Considerations\nFLOTA shortens the average length of sequences\nprocessed by PLMs, thus reducing their energy re-\nquirements, a desirable property given their other-\nwise detrimental environmental footprint (Schwartz\net al., 2019; Strubell et al., 2019).\nReferences\nMohamed Alkaoud and Mairaj Syed. 2020. On the im-\nportance of tokenization in Arabic embedding mod-\nels. In Arabic Natural Language Processing Work-\nshop (WANLP) 5.\nChantal Amrhein and Rico Sennrich. 2021. How suit-\nable are subword segmentation strategies for trans-\nlating non-concatenative morphology? In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nAraBERT: Transformer-based model for Arabic lan-\nguage understanding. In Workshop on Open-Source\nArabic Corpora and Processing Tools (OSACT).\nR. Harald Baayen, Ton Dijkstra, and Robert Schreuder.\n1997. Singulars and plurals in Dutch: Evidence for\na parallel dual-route model. Journal of Memory and\nLanguage, 37:94–117.\nR. Harald Baayen, Richard Piepenbrock, and Leon Gu-\nlikers. 1995. The CELEX lexical database (CD-\nROM). Linguistic Data Consortium, Philadelphia,\nPA.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020.\nKris Cao and Laura Rimell. 2021. You should evaluate\nyour language model on marginal likelihood over to-\nkenisations. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP) 2021.\nAlfonso Caramazza, Alessandro Laudanna, and\nCristina Romani. 1988. Lexical access and inﬂec-\ntional morphology. Cognition, 28(297-332).\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank. In Findings of the Association for\nComputational Linguistics: EMNLP 2020.\nKenneth Church. 2020. Emerging trends: Sub-\nwords, seriously? Natural Language Engineering ,\n26(3):375–382.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. CANINE: Pre-training an efﬁcient\ntokenization-free encoder for language representa-\ntion. In arXiv 2103.06874.\nRyan Cotterell and Hinrich Schütze. 2018. Joint se-\nmantic synthesis and morphological analysis of the\nderived word. Transactions of the Association for\nComputational Linguistics, 6:33–48.\nRyan Cotterell, Tim Vieira, and Hinrich Schütze. 2016.\nA joint model of orthography and morphological\nsegmentation. In Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL HLT) 2016.\nRyan Cotterell, Ekaterina Vylomova, Huda Khayral-\nlah, Christo Kirov, and David Yarowsky. 2017.\nParadigm completion for derivational morphology.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2017.\nDavid Crystal. 1997. The Cambridge encyclopedia of\nthe English language. Cambridge University Press,\nCambridge, UK.\nDaniel Deutsch, John Hewitt, and Dan Roth. 2018. A\ndistributional and orthographic aggregation model\nfor English derivational morphology. In Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL) 56.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. In Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL HTL) 2019.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. The C Users Journal, 12(2):23–38.\nChristina L. Gagné, Thomas L. Spalding, and Daniel\nSchmidtke. 2019. LADEC: The large database of\nEnglish compounds. Behavior Research Methods ,\n51(5):2152–2179.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Schütze. 2020a. DagoBERT: Generating deriva-\ntional morphology with a pretrained language model.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) 2020.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Schütze. 2020b. Predicting the growth of mor-\nphological families from social and linguistic factors.\nIn Annual Meeting of the Association for Computa-\ntional Linguistics (ACL) 58.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Schütze. 2021. Superbizarre is not superb: Im-\nproving BERT’s interpretations of complex words\nwith derivational morphology. In Annual Meeting of\nthe Association for Computational Linguistics (ACL)\n59.\nValentin Hofmann, Hinrich Schütze, and Janet B. Pier-\nrehumbert. 2020c. A graph auto-encoder model\nof derivational morphology. In Annual Meeting of\n390\nthe Association for Computational Linguistics (ACL)\n58.\nJimin Hong, Taehee Kim, Hyesu Lim, and Jaegul Choo.\n2021. A V ocaDo: Strategy for adapting vocabulary\nto downstream domain. In Conference on Empirical\nMethods in Natural Language Processing (EMNLP)\n2021.\nKatharina Kann, Ryan Cotterell, and Hinrich Schütze.\n2016. Neural morphological analysis: Encoding-\ndecoding canonical segments. In Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP) 2016.\nDiederik P. Kingma and Jimmy L. Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR) 3.\nStav Klein and Reut Tsarfaty. 2020. Getting the ##life\nout of living: How adequate are word-pieces for\nmodelling complex morphology? In Workshop on\nComputational Research in Phonetics, Phonology,\nand Morphology (SIGMORPHON) 17.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL) 56.\nAlessandro Laudanna and Cristina Burani. 1995. Dis-\ntributional properties of derivational afﬁxes: Impli-\ncations for processing. In Laurie B. Feldman, edi-\ntor, Morphological aspects of language processing ,\npages 345–364. Lawrence Erlbaum, Hillsdale, NJ.\nManuel Mager, Özlem Çetino˘glu, and Katharina Kann.\n2019. Subword-level language identiﬁcation for\nintra-word code-switching. In Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (NAACL HTL) 2019.\nManuel Mager, Özlem Çetino˘glu, and Katharina Kann.\n2020. Tackling the low-resource challenge for\ncanonical segmentation. In Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP) 2020.\nJeremy M. Needle and Janet B. Pierrehumbert. 2018.\nGendered associations of english morphology. Jour-\nnal of the Association for Laboratory Phonology ,\n9(1):119.\nHao Peng, Jungo Kasai, Nikolaos Pappas, Dani\nYogatama, Zhaofeng Wu, Lingpeng Kong, Roy\nSchwartz, and Noah A. Smith. 2021. ABC: At-\ntention with bounded-memory control. In arXiv\n2110.02488.\nJanet B. Pierrehumbert and Ramon Granell. 2018. On\nhapax legomena and morphological productivity. In\nWorkshop on Computational Research in Phonetics,\nPhonology, and Morphology (SIGMORPHON) 15.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Annual Meeting of the Association\nfor Computational Linguistics (ACL) 58.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lip-\nton. 2019. Combating adversarial misspellings with\nrobust word recognition. In Annual Meeting of\nthe Association for Computational Linguistics (ACL)\n57.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nTatyana Ruzsics and Tanja Samardži ´c. 2017. Neu-\nral sequence-to-sequence learning of internal word\nstructure. In Conference on Computational Natural\nLanguage Learning (CoNLL) 21.\nVin Sachidananda, Jason S. Kessler, and Yi-An Lai.\n2021. Efﬁcient domain adaptation of language mod-\nels via adaptive tokenization. In Workshop on Sim-\nple and Efﬁcient Natural Language Processing (Sus-\ntaiNLP) 2.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean voice search. In International Confer-\nence on Acoustics, Speech, and Signal Processing\n(ICASSP) 37.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI. In arXiv 1907.10597.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmenta-\ntion and tagging. In Findings of the Association for\nComputational Linguistics: EMNLP 2020.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Annual Meeting of the Association\nfor Computational Linguistics (ACL) 54.\nChenglei Si, Zhengyan Zhang, Yingfa Chen, Fanchao\nQi, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun.\n2021. SHUOWEN-JIEZI: Linguistically informed\ntokenizers for Chinese language model pretraining.\nIn arXiv 2106.00400.\nSandeep Soni, Lauren F. Klein, and Jacob Eisenstein.\n2019. Correcting whitespace errors in digitized his-\ntorical texts. In Workshop on Computational Lin-\nguistics for Cultural Heritage, Social Sciences, Hu-\nmanities and Literature 3.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Annual Meeting of the As-\nsociation for Computational Linguistics (ACL) 57.\nSamson Tan, Shaﬁq Joty, Lav R. Varshney, and Min-\nYen Kan. 2020. Mind your inﬂections! Improving\nNLP for non-standard Englishes with base-inﬂection\nencoding. In Conference on Empirical Methods in\nNatural Language Processing (EMNLP) 2020.\n391\nEkaterina Vylomova, Ryan Cotterell, Timothy Bald-\nwin, and Trevor Cohn. 2017. Context-aware predic-\ntion of derivational word-forms. In Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics (EACL) 15.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc\nLe V, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. In arXiv 1609.08144.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2021. ByT5: Towards a token-\nfree future with pre-trained byte-to-byte models. In\narXiv 2105.13626.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems (NeurIPS) 33.\nXinsong Zhang, Pengshuai Li, and Hang Li. 2021.\nAMBERT: A pre-trained language model with multi-\ngrained tokenization. In Findings of the Association\nfor Computational Linguistics: ACL 2021.\nA Appendix\nA.1 Preprocessing\nWe exclude texts written in a language other than\nEnglish and lowercase all words. We exclude ti-\ntles with less than three and more than ten words.\nFor each title, we compute the proportion of words\nstarting with a productive preﬁx from the list pro-\nvided by Crystal (1997). During sampling, we then\nweight titles by this proportion in order to make the\nlanguage contained within the datasets as complex\nand challenging as possible.\nA.2 Hyperparameters\nThe vocabulary size is 28,996 for BERT, 50,257\nfor GPT-2, and 32,000 for XLNet. The number\nof trainable parameters is 109,497,620 for BERT,\n124,455,168 for GPT-2, and 117,324,308 for XL-\nNet. The classiﬁcation head for all three models\nuses softmax as the activation function.\nWe use a batch size of 64 and perform\ngrid search for the number of epochs\nn ∈ {1,..., 20} and the learning rate\nl ∈ {1 ×10−5,3 ×10−5,1 ×10−4} (selec-\ntion criterion: F1). We tune lon ArXiv-L (physics)\nand use the best conﬁguration on all datasets.\nFor the FLOTA models, we additionally tune\nk ∈{1,2,3,4}(selection criterion: F1). Models\nare trained with categorical cross-entropy as the\nloss function and Adam (Kingma and Ba, 2015)\nas the optimizer. Experiments are performed on a\nGeForce GTX 1080 Ti GPU (11GB).\nA.3 Performance\nTable 5 provides breakdowns of the performance\nfor the individual datasets forming ArXiv-S/L.\nA.4 Hyperparameters\nAll hyperparameters are as for the main experi-\nment (see Appendix A.2). For the learning rate,\nwe use the best conﬁguration from the main ex-\nperiment. For FIRST and LONGEST, we tune\nk ∈ {1,2,3,4}(selection criterion: F1), identi-\ncally to FLOTA in the main experiment.\nA.5 Hyperparameters\nAll hyperparameters are as for the main experiment\n(see Appendix A.2). For the learning rate, we use\nthe best conﬁguration from the main experiment.\nA.6 Performance\nTable 6 provides breakdowns of the performance\nfor the individual datasets forming ArXiv-S/L.\n392\nArXiv-S ArXiv-L\nDev Test Dev Test\nModel CS MATH PHYS CS MATH PHYS CS MATH PHYS CS MATH PHYS\nBERT .546 .358 .502 .498 .407 .504 .682 .660 .679 .649 .653 .675\n+FLOTA .546 .414 .514 .483 .404 .567 .677 .663 .686 .652 .658 .672\nGPT-2 .354 .281 .353 .316 .261 .395 .493 .506 .578 .465 .498 .559\n+FLOTA .348 .313 .398 .370 .323 .454 .520 .549 .603 .498 .540 .587\nXLNet .473 .357 .476 .489 .358 .515 .654 .643 .684 .627 .642 .655\n+FLOTA .450 .402 .486 .415 .346 .522 .660 .651 .681 .633 .641 .665\nTable 5: Performance (F1). CS: computer science; MATH: mathematics; PHYS: physics.\nArXiv-S (N) ArXiv-L (N)\nDev Test Dev Test\nModel CS MATH PHYS CS MATH PHYS CS MATH PHYS CS MATH PHYS\nBERT .479 .333 .470 .566 .566 .605 .417 .338 .481 .531 .544 .588\n+FLOTA .548 .400 .511 .652 .640 .664 .452 .372 .518 .631 .620 .644\nGPT-2 .336 .261 .342 .452 .461 .530 .326 .252 .366 .423 .454 .511\n+FLOTA .358 .316 .402 .514 .527 .582 .370 .296 .405 .481 .511 .562\nXLNet .431 .311 .433 .607 .594 .625 .470 .300 .421 .587 .576 .605\n+FLOTA .432 .398 .474 .646 .623 .655 .435 .360 .466 .627 .612 .631\nTable 6: Performance (F1) with noise (N). CS: computer science; MATH: mathematics; PHYS: physics.\n393"
}