{
  "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
  "url": "https://openalex.org/W4385572485",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5066595937",
      "name": "Krithika Ramesh",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Birla Institute of Technology and Science, Pilani",
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A5025327860",
      "name": "Arnav Chavan",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Birla Institute of Technology and Science, Pilani",
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A5033179563",
      "name": "Shrey Pandit",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Birla Institute of Technology and Science, Pilani",
        "Indian Institute of Technology Dhanbad"
      ]
    },
    {
      "id": "https://openalex.org/A5005513786",
      "name": "Sunayana Sitaram",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Birla Institute of Technology and Science, Pilani",
        "Indian Institute of Technology Dhanbad"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3198409578",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4285163830",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W3174505797",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4285199616",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W4285294416",
    "https://openalex.org/W4226128078",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W3177189402",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4229026816",
    "https://openalex.org/W2253444830",
    "https://openalex.org/W4226075153",
    "https://openalex.org/W2802105481",
    "https://openalex.org/W4281493312",
    "https://openalex.org/W3095105395",
    "https://openalex.org/W3166961312",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W3032765105",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4287855127",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4299805482",
    "https://openalex.org/W4308672074",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4287887365",
    "https://openalex.org/W2954029489",
    "https://openalex.org/W4287890642",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W3175765954",
    "https://openalex.org/W2785011159",
    "https://openalex.org/W4287891150",
    "https://openalex.org/W4285279512",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3206487987",
    "https://openalex.org/W4223600764",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W4297816198",
    "https://openalex.org/W4226079992",
    "https://openalex.org/W3038073095",
    "https://openalex.org/W3217152367",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3099695344",
    "https://openalex.org/W3035379020",
    "https://openalex.org/W3035390927"
  ],
  "abstract": "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15762–15782\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA Comparative Study on the Impact of Model Compression Techniques on\nFairness in Language Models\nKrithika Ramesh\nMicrosoft Research\nkramesh.tlw@gmail.com\nArnav Chavan∗\nIndian Institute of Technology, Dhanbad\narnavchavan04@gmail.com\nShrey Pandit∗\nBITS, Pilani\npandit.shrey.01@gmail.com\nSunayana Sitaram\nMicrosoft Research\nsunayana.sitaram@microsoft.com\nAbstract\nCompression techniques for deep learning have\nbecome increasingly popular, particularly in\nsettings where latency and memory constraints\nare imposed. Several methods, such as prun-\ning, distillation, and quantization, have been\nadopted for compressing models, each provid-\ning distinct advantages. However, existing lit-\nerature demonstrates that compressing deep\nlearning models could affect their fairness. Our\nanalysis involves a comprehensive evaluation\nof pruned, distilled, and quantized language\nmodels, which we benchmark across a range\nof intrinsic and extrinsic metrics for measur-\ning bias in text classification. We also investi-\ngate the impact of using multilingual models\nand evaluation measures. Our findings high-\nlight the significance of considering both the\npre-trained model and the chosen compression\nstrategy in developing equitable language tech-\nnologies. The results also indicate that com-\npression strategies can have an adverse effect\non fairness measures.\n1 Introduction\nDespite their increasing popularity, machine learn-\ning models have been known to exhibit biases in\ntheir outputs, present privacy risks, and have poten-\ntially negative environmental consequences from\ntheir training and deployment. (Bender et al., 2021;\nTalat et al., 2022). Language models suffer from\nbiases that result in unequal resource distributions\n(allocational harms), in addition to the undesired\ntendency to reproduce biases and stereotypes in\ncontent that is reflective of hegemonic worldviews\n(representational harms). Although measures\nhave been proposed in tasks such as text classi-\nfication (Czarnowska et al., 2021) to investigate the\ndisparate allocational treatment of different classes,\nmuch of the research on fairness in language mod-\nels centers on addressing representational harms\n*Equal contribution\n(Blodgett et al., 2020). The potential of these mod-\nels to further stigmatize marginalized communi-\nties is demonstrated in (Dressel and Farid, 2018),\nwhich illustrates how recidivism prediction systems\nare biased against black defendants, who have a\nhigher baseline risk for repeat offences. Biases\nare also prevalent in computer vision applications\nsuch as facial recognition technologies. Within\nNLP, (Bolukbasi et al., 2016), one of the first forays\nthat studied this phenomenon in language, noted\nthat word embeddings contained stereotypical as-\nsociations with respect to gender. Language mod-\nels can exhibit biases toward different dialects for\ntasks like toxicity and hate speech detection (Garg\net al., 2022; Sap et al., 2019), generate stereotypi-\ncal representations and narratives (Lucy and Bam-\nman, 2021), and are capable of the outright erasure\nof underrepresented identities (Dev et al., 2021).\nCompressed models that are biased may have detri-\nmental consequences in the real world, as they are\ntypically deployed on edge devices, which can fur-\nther disadvantage communities without access to\nother forms of technology. Consequently, these\nissues have compelled a shift towards developing\nmore inclusive systems.\nHooker et al. (2020) demonstrates how compres-\nsion techniques, when applied to models that deal\nwith tabular data, lead to the disparate treatment\nof less-represented classes. However, equivalent\nstudies in NLP (Tal et al., 2022; Ahn et al., 2022;\nSilva et al., 2021) do not provide a conclusive ob-\nservation as to whether compression methods are\neffective for reducing bias in NLP, and are centered\nmainly solely around model distillation being the\ncompression technique of choice. This paper aims\nto resolve the following questions by benchmark-\ning a wide range of metrics and datasets to study\nbias in text classification systems.\n• How does model compression using pruning,\nquantization, or distillation impact bias in lan-\nguage models, and to what extent?\n15762\n• To what extent are these observations influ-\nenced by variables such as the utilization of\ndifferent techniques within a specific compres-\nsion method or a change in model architecture\nor size?\n• How does multilinguality affect these obser-\nvations in compressed models?\n2 Related Work\nCompression techniques such as pruning, distil-\nlation, and quantization have proven effective at\nreducing the size of models while maintaining their\nperformance. Pruning can be done in two ways,\nvia structured and unstructured pruning. While\nstructured pruning involves removing groups of\nneurons, unstructured pruning removes individual\nneurons by zeroing out their values. Structured\npruning methods generally achieve faster infer-\nence speeds, along with a reduction in parame-\nter size. Knowledge distillationtechniques are\nanother alternative that have been demonstrated\nto effectively transfer knowledge from a teacher\nmodel to a smaller student model, using a loss\nfunction designed to minimize the distance be-\ntween the features or the outputs of the student\nand teacher models. We also incorporate a third\nform of model compression - quantization, where\nmodel weights and/or activations are represented\nusing lower-bit precisions. There are two main\napproaches to quantization: post-training quantiza-\ntion, which is applied to a pre-trained model, and\nquantization-aware training (Zafrir et al., 2019a),\nwhich incorporates quantization into the training\nprocess in order to mitigate the loss of accuracy\nthat can occur with post-training quantization. Al-\nthough several techniques for pruning and quanti-\nzation have been developed, we acknowledge that\nour work consists only of models compressed using\npost-training dynamic quantization and the pruning\nmethod proposed in Zafrir et al. (2021).\nWhilst there has been research at the confluence\nof fairness and efficiency in natural language pro-\ncessing (NLP), the results from these studies can be\ninconclusive, limited in their research design, and\nat times, contradict the results from previous analy-\nses. Talat et al. (2022); Orgad and Belinkov (2022);\nField et al. (2021); Blodgett et al. (2020) provide\ncritical insights into the current state of fairness in\nNLP and delve into the details of what research\nstudies must consider when conducting work in\nthis area. The discussion thus far concerning fair-\nness, in general, has mainly been Anglo-centric,\nbut recent forays (Kaneko et al., 2022; Huang et al.,\n2020b; Gonen et al., 2019; Zhao et al., 2020) have\nexplored bias in multilingual spaces and languages\nbeyond English.\nIn the context of model compression, Tal et al.\n(2022) show that while larger models produce\nfewer gendered errors, they produce a greater pro-\nportion of gendered errors in coreference resolution\nwhilst Xu and Hu (2022) suggest that distillation\nand pruning have a regularizing effect that miti-\ngates bias in text classification. On the other hand\nSilva et al. (2021); Ahn et al. (2022); Hessenthaler\net al. (2022) all demonstrate how distillation can\nhave an adverse impact on model fairness.\nHessenthaler et al. (2022) strongly casts doubt on\nthe results from Xu and Hu (2022) by showing that\nknowledge distillation decreases model fairness.\nAdditionally, the findings from Mohammadshahi\net al. (2022) point toward the fact that pruning can\namplify bias in multilingual machine translation\nmodels. It must also be noted that with the excep-\ntion of Hessenthaler et al. (2022); Tal et al. (2022);\nXu and Hu (2022); Mohammadshahi et al. (2022),\nmany of these studies do not validate the fairness\nof these models over downstream tasks. This is\nessential as bias measurements over a model’s pre-\ntrained representations cannot be used as a proxy\nto assess bias in its downstream outputs (Goldfarb-\nTarrant et al., 2021). Lauscher et al. (2021); Gupta\net al. (2022) explore the efficient debiasing of mod-\nels via the use of adapters and an adapted form of\ndistillation, respectively.\nTo our knowledge, our work is the first com-\nprehensive study on fairness in NLP with respect\nto pruning, distillation and quantization, in addi-\ntion to which it addresses both monolingual and\nmultilingual models.\n3 Methodology and Setup\n3.1 Pruning, Quantization, Distillation\nOur pruning approach uses the Prune Once For All\n(Prune OFA) (Zafrir et al., 2021) method on the\nbase models. The Prune OFA method is a state-of-\nthe-art pruning strategy that prunes models during\nthe pre-training phase, eliminating the need for\nadditional pruning on downstream tasks.\nWe employ dynamic quantization (Zafrir et al.,\n2019b) as a post-training quantization method for\nfairness evaluation. This approach converts model\nweights to INT8 format post-training and dynami-\n15763\ncally quantizes activations during runtime based on\nthe range of data. This method has the advantage of\nminimal hyperparameter tuning and additional flex-\nibility in the model, which minimizes any potential\nperformance loss.\nFor knowledge distillation, we consider mod-\nels compressed using the techniques employed in\n(Sanh et al., 2019; Wang et al., 2020a), with the\nprimary difference in these methods being the type\nof feature representations that the student is en-\ncouraged to mimic. We utilize pre-trained distilled\nmodels that are publicly available12 for all of our\nexperiments. The complete list of models we con-\nsidered for these experiments is in the appendix\n(Table 9).\n3.2 Fairness Evaluation in Language Models\nTo examine bias in LMs, we rely on a combina-\ntion of intrinsic and extrinsic measures. Intrinsic\nmeasures primarily evaluate bias in the pre-trained\nrepresentations of language models, such as in the\nstatic and contextualized embedding spaces. On the\nother hand, extrinsic measures estimate bias in the\noutputs produced by the LLM in the downstream\ntask it is fine-tuned for. Extrinsic evaluation mea-\nsures are capable of identifying both allocational\nand representational harms, while intrinsic mea-\nsures only address the latter. The inconsistencies\nand lack of correlation between these two kinds of\nmetrics (Goldfarb-Tarrant et al., 2021; Cao et al.,\n2022) has led to calls for better evaluation prac-\ntices that prioritize extrinsic evaluation. We have\nincluded detailed explanations of the metrics and\ndatasets in the next section and provided a broad\noverview and additional details in the appendix in\nTable 11.\n4 Intrinsic measures\nStereoSet (Nadeem et al., 2021) is an English\ndataset used for analyzing’s a model’s proclivity\nfor stereotypical and anti-stereotypical data across\nthe axes of gender, race, religion, and profession.\nWe consider only the intrasentence samples from\nStereoSet and evaluate the test set split. The ICAT\n(Idealized Context Association Test) score com-\nbines both the language model score (LMS) and\nthe stereotype score (SS) such that it is maximized\nwhen the model is unbiased and simultaneously\n1https://huggingface.co\n2https://github.com/microsoft/unilm/tree/master/minilm\nproficient at language modeling as shown in Equa-\ntion 1.\nICAT = LMS ∗min(SS, 100 −SS)\n50 (1)\nSimilar to StereoSet, CrowS-Pairs (Nangia\net al., 2020) is a crowdsourced dataset that allows\nus to observe bias along the dimensions of gen-\nder, race, and religion. The distance between the\nstereotype and anti-stereotype pairs is kept to a\nminimum, and the metric involves the pseudo-log\nlikelihood scoring mechanism from Salazar et al.\n(2020). However, both StereoSet and CrowS-Pair\nhave been subject to critique for the inconsistencies\nin their datasets (Blodgett et al., 2021).\n5 Extrinsic measures\nFor extrinsic measurement over downstream tasks,\nwe have used multiple datasets with different fair-\nness definitions (details in Table 11 in the ap-\npendix). The Jigsaw dataset is used to evaluate\nbias in toxicity detection systems across multiple\ndemographic identities. We do this by assessing\nthe difference in False Positive Rates (FPR) across\nsubgroups to ensure that text from one group is not\nunfairly flagged as toxic. We report ROC-AUC as\na metric on three specific subsets:\n• Subgroup AUC: The test set is restricted to\nsamples that mention the specific identity sub-\ngroup. A low value suggests that the model\nis ineffective at differentiating between toxic\nand non-toxic remarks that mention the iden-\ntity.\n• BPSN AUC(Background Positive, Subgroup\nNegative) : The test set is restricted to the\nnon-toxic examples that mention the identity\nand the toxic examples that do not mention\nthe identity. A low value suggests that the\nmodel predicts a higher toxicity score than it\nshould for a non-toxic example mentioning\nthe identity.\n• BNSP AUC(Background Negative, Subgroup\nPositive) : The test set is restricted to the\ntoxic examples that mention the identity and\nthe non-toxic examples that do not mention\nthe identity. A low value here indicates that\nthe model predicts lower toxicity scores than\nit should for toxic examples mentioning the\nidentity.\n15764\nThe other monolingual extrinsic measure\nincludes the African American Vernacular\nEnglish-Standard American English (AA VE-\nSAE) dataset (Groenwold et al., 2020a), which con-\nsists of intent-equivalent SAE and AA VE sentence\npairs. Sap et al. (2019) has shown that AA VE lan-\nguage is more likely to be identified as hate speech\ncompared to the standardized form of American\nEnglish. A fair, unbiased model on this data would\nproduce similar sentiment scores for both AA VE\nand SAE. We have also included the results for\nthe Equity Evaluation Corpus (EEC), a template-\nbased dataset that evaluates the emotional intensity\nof sentiment classification systems over four cate-\ngories of data- anger, fear, sadness, and joy, in the\nappendix (Section C.1).\n5.1 Multilingual Datasets\nTo test if these observations are consistent with\nresults across multilingual models, we use a bi-\nnarized hate speech detectiondataset, originally\nsourced from Huang et al. (2020a). It consists of\nonline data accumulated from Twitter along with la-\nbels containing information pertinent to the user’s\nage, gender, country, and race/ethnicity, and the\ndetails regarding the distribution of data and labels\nacross languages are provided in the Appendix in\nTable 17. The fairness evaluation objective for the\nhate speech detection task involves measuring the\nequality differences (ED)metric across each of\nthe groups corresponding to the aforementioned\ndemographic factors. The ED is defined as the\ndifference between the true positive/negative and\nfalse positive/negative rates for each demographic\nfactor. For instance, the ED for false positive rates\n(FPED) is defined below, where d is representative\nof each demographic group within a demographic\nfactor D (for example, gender is a demographic\nfactor, and male is a corresponding representative\ndemographic group).\nFPED =\n∑\ndϵD\n∥FPR d −FPR ∥ (2)\nWe also make use of reviews datasetssourced\nfrom Trustpilot, Yelp, and Amazon, with a rating\n(1-5) for each review (Hovy et al., 2015; Huang\nand Paul, 2019). The data includes user informa-\ntion, such as age, gender, and country (our analysis\nis constrained to gender). For this specific task,\nthe dataset has been transformed into a binary sen-\ntiment analysis classification task, where reviews\nwith a rating above 3 are classified as positive, and\nthose with a rating below 3 are classified as nega-\ntive. Reviews with a rating of 3 are discarded. As\nwith the hate speech dataset, the equality differ-\nence metric is used to evaluate group fairness over\nthis task along a given dimension.\n6 Analysis of Results\n6.1 StereoSet\nThe findings of the StereoSet evaluation are pre-\nsented in Table 1, wherein a higher ICAT score\nimplies a lesser biased model3. According to the re-\nsults, the monolingual models’ distilled and pruned\nversions exhibit more bias than their original coun-\nterparts. However, this trend does not necessarily\napply to the multilingual or quantized versions of\nthese models (Table 13). There is also an indica-\ntion that the extent of pruning is potentially propor-\ntional to the negative impact on fairness in these\nmodels for this metric. Additionally, the MiniLM\nmodels, which employ a different distillation tech-\nnique than the one used for DistilBERT, show a\nsignificant decrease in the ICAT score. However,\nit is worth noting that they are relatively smaller\n(MiniLMv2 being approximately one-third the size\nof DistilBERT). Among the three techniques, quan-\ntization appears to be the rank the lowest in terms\nof bias according to the intrinsic StereoSet mea-\nsure. That said, these results may not accurately\npredict the model’s performance in downstream\ntasks (Goldfarb-Tarrant et al., 2021). Based on\nthe ICAT score measurement, the models distilled\nusing MiniLMv2 exhibit the highest level of bias,\nwhile the quantized models demonstrate the best\nperformance in this metric.\nDistilBERT emerges as the least biased among\nthe distilled models, while the quantized version of\nBERT-base shows the least bias among the quan-\ntized model sets. We highlight that while quantiza-\ntion results in a higher ICAT score for BERT, this is\nnot the case for RoBERTa. Furthermore, although\nwe have aggregated the scores for the dimensions of\ngender, race, and religion, these trends do not per-\nsist uniformly across individual dimensions. This\nobservation is also reflected in our evaluation of\nthe CrowS-Pair dataset.\n3A green arrow indicates that the model is less biased in\ncomparison to the parent model (in bold), while a red arrow\nindicates the opposite.\n15765\nModel Overall ICAT Score\nbert-base-uncased 70.30\ndistilbert-base-uncased 69.52 ↓-0.78\nminiLMv2-L6-H384-uncased 53.94 ↓-16.36\nbert-base-uncased-90%-pruned 69.44 ↓-0.86\nbert-base-uncased-85%-pruned 68.50 ↓-1.8\nbert-base-uncased-quantized 72.06 ↑1.76\nbert-base-multilingual-cased 64.94\ndistilbert-base-multilingual-cased 67.99↑3.05\nxlm-roberta-large 71.29\nmultilingual-MiniLM-L12-H384 52.47↓-18.82\nroberta-base 67.18\ndistilroberta 66.68 ↓-0.5\nroberta-base-quantized 65.81 ↓-1.37\nbert-large-uncased 69.50\nminiLMv2-L6-H384-uncased 49.74 ↓-19.76\nbert-large-uncased-90%-pruned 68.91 ↓-0.59\nbert-large-uncased-quantized 70.20 ↑0.7\nTable 1: We report the overall ICAT score for the model\nevaluations over the StereoSet dataset. The higher the\nICAT score, the less biased the model.\n6.2 CrowS-Pair\nIn Table 2, the results for CrowS-Pair have been\npresented for gender, race, and religion, along with\nthe deviation from the ideal baseline score of 50.\nAccording to this metric, a higher magnitude of\ndeviation indicates more bias in the model. Our\nfindings reveal inconsistent disparities in the scores\nacross different compression methods and their\nbase and large counterparts. For example, while\nthe results suggest that DistilBERT is less biased\nthan BERT-base in terms of gender and race, this\ndoes not hold true for religion. While this may\nalso be in due part to the relatively smaller sam-\nple size of the data for each dimension (Meade\net al., 2022), it would be essential to understand\nif a model demonstrating lower bias in one dimen-\nsion generalizes to other dimensions or data that\nincorporates intersectional identities. However, it\nis important to acknowledge that intrinsic and ex-\ntrinsic measures do not necessarily correlate with\neach other. Additionally, Aribandi et al. (2021)\nhighlights the substantial variance in likelihood-\nbased and representation-based diagnostics during\nempirical evaluations, emphasizing the need for\ncaution when interpreting findings from intrinsic\nmeasures.\n6.3 Jigsaw\nTo evaluate the potential harm caused by these mod-\nels, it is essential to assess bias in the context of\ndownstream tasks. We fine-tuned the models on\nthe Jigsaw dataset and examined how well they\nModel Gender Race Religion\nbert-base-uncased 57.25+7.25 62.33+12.33 62.86+12.86\ndistilbert-base-uncased 56.87+6.87 60.97+10.97 66.67+16.67\nminiLMv2-L6-H384-uncased 50.76+0.76 50.68+0.68 72.38+22.38\nbert-base-uncased-90%-pruned 51.91+1.91 59.61+9.61 60.95+10.95\nbert-base-uncased-85%-pruned 51.91+1.91 53.01+3.01 58.10+8.10\nbert-base-uncased-quantized 57.25+7.25 62.14+12.14 46.67-3.33\nbert-base-multilingual-cased47.71-2.29 44.66-5.34 53.33+3.33\ndistilbert-base-multilingual-cased 50.38+0.38 41.94-8.06 53.33+3.33\nxlm-roberta-large 54.41+4.41 51.65+1.65 69.52+19.52\nmultilingual-MiniLM-L12-H384 39.85-10.15 60.39+10.39 47.62-2.38\nroberta-base 60.15+10.15 63.57+13.57 60.00+10.00\ndistilroberta 52.87 +2.87 60.08+10.08 63.81+13.81\nroberta-base-quantized 53.64+3.64 58.53+8.53 49.52-0.48\nbert-large-uncased 55.73+5.73 60.39+10.39 67.62+17.62\nminiLMv2-L6-H384-uncased 43.13-6.87 50.1+0.1 57.14+7.14\nbert-large-uncased-90%-pruned 54.20+4.20 60.19+10.19 69.52+19.52\nbert-large-uncased-quantized 50.38+0.38 63.11+13.11 55.24+5.24\nTable 2: The results for the CrowS-Pairs metric for\ndifferent model families have been reported, with\nvalues closer to 50 indicating less biased models\naccording to this metric.\nperformed on various forms of protected identity\nmentions. Table 3 presents the aggregated scores\nfor all subgroups across the metrics discussed in\nSection 5.4\nThe overall trend suggests that compression\nmethods can have a negative impact on fairness.\nDistilled models generally appear to demonstrate a\nhigher level of bias compared to their pruned and\nquantized counterparts. In contrast to the findings\nfrom intrinsic measurements, quantization does\nlead to a decrease in performance in these mod-\nels, and this drop is also observed in the multilin-\ngual models. However, the pruned and quantized\nmodels generally exhibit a lower magnitude of bias\ncompared to the distilled models.\nAmong all the compressed models evaluated,\nthe base form of DistilBERT exhibits the highest\ndegree of bias. These findings may vary at different\ntraining stages, and they warrant further probing\nto see if training the models further to improve\nthe performance of these compressed models could\nalso significantly contribute to reducing bias.\n6.4 AA VE-SAE\nGiven the proclivity of hate speech detection sys-\ntems to flag AA VE language as hate speech (Sap\net al., 2019; Groenwold et al., 2020b), we aimed\nto assess whether SST-2 fine-tuned models also\ntend to classify AA VE language as negative. The\nunderlying fairness objective in this context is to\nevaluate the robustness of sentiment analysis mod-\nels to data from diverse dialects. We make use\n4Results for the pruned version of BERT-large excluded\ndue to low performance on Jigsaw and AA VE-SAE.\n15766\nModel Subgroup\nAUC\nBPSN\nAUC\nBNSP\nAUC\nbert-base-uncased 0.918 0.934 0.975\ndistilbert-base-uncased 0.878↓-0.04 0.892↓-0.042 0.972↓-0.003\nminiLM-L12-H384-uncased 0.917↓-0.001 0.943↑0.009 0.970↓-0.005\nbert-base-uncased-90%-pruned 0.915↓-0.003 0.932↓-0.002 0.973↓-0.002\nbert-base-uncased-85%-pruned 0.917↓-0.001 0.933↓-0.001 0.974↓-0.001\nbert-base-uncased-quantized 0.917↓-0.001 0.933↓-0.001 0.974↓-0.001\nbert-base-multilingual-cased0.914 0.936 0.971\ndistilbert-base-multilingual-cased 0.895↓-0.019 0.913↓-0.023 0.969↓-0.002\nxlm-roberta-base 0.914 0.942 0.969\nmultilingual-MiniLM-L12-H384 0.904↓-0.01 0.926↓-0.0160.968↓-0.001\nroberta-base 0.920 0.947 0.971\ndistilroberta 0.901 ↓-0.019 0.921↓-0.026 0.9710\nroberta-base-quantized 0.918↓-0.002 0.943↓-0.004 0.9710\nbert-large-uncased 0.913 0.922 0.975\nbert-large-uncased-quantized 0.909↓-0.004 0.9220 0.971↓-0.004\nTable 3: We report the results for the Jigsaw dataset.\nThe higher the AUC, the less biased the model. The\nscores for the identity subgroups have been aggregated\nand presented in this table.\nof well-optimized, pre-trained models that were\nfine-tuned on the Stanford Sentiment Bank (SST-\n2) dataset (Socher et al., 2013), and we fine-tuned\nthe pruned pre-trained models over SST-2. Ad-\nditionally, we applied quantization techniques to\nthe existing models and compared the outcomes\nof dynamically quantized models with other com-\npressed variations. We examined the change in\npredictions when considering the AA VE intent-\nequivalent counterpart of the SAE language. We\nterm the contradictory predictions of the classifier\non AA VE-SAE sentence pairs asnon-concurrent\npredictions, and our results are presented in Table\n4.\nA consistent pattern is observed where distilled\nmodels demonstrate a significantly higher degree\nof bias in this particular task than their base models.\nWhile the BERT-base pruned models also show a\ndecline in performance, the 90% pruned version\nappears to be more robust than the 85% pruned ver-\nsion. Across all cases, except for the dynamically\nquantized form of RoBERTA-base, the quantized\nmodels show an increase in these non-concurrent\npredictions. Another interesting point of note is\nthat several of these models seem to record posi-\ntive to negative non-concurrent predictions when\nconsidering AA VE language instead of its SAE\nintent-equivalent counterpart.\n7 Multilingual Datasets\nTo investigate whether the observed trends in a\nmonolingual setting extend to a multilingual sce-\nnario, we conducted experiments using a separate\nset of models, with information about their size\nModel Negative\nto Positive\nPositive\nto Negative\nTotal\nChanges\nbert-base-uncased 238 89 327\ndistilbert-base-uncased 326 ↑88 76↓-13 402↑75\nbert-base-uncased-90%-pruned 205↓-33 128↑39 333↑6\nbert-base-uncased-85%-pruned 340↑102 147↑58 487↑160\nbert-base-uncased-quantized 281↑43 93↑4 374↑47\nxlm-roberta-base 247 56 303\nmultilingual-MiniLM-L12-H384 294↑47 73↑17 367↑64\nroberta-base 241 102 343\ndistilroberta 238 ↓-3 108↑6 346↑3\nroberta-base-quantized 207 ↓-34 115↑13 322↓-21\nroberta-large 178 110 288\nminiLM-L12-H384-uncased 265↑87 64↑-46 329↑41\nbert-large-uncased 230 72 302\nbert-large-uncased-quantized 175↓-55 156↑84 331↑29\nTable 4: The results depict the count of non-concurrent\npredictions for the SST-2 fine-tuned models tested over\nthe AA VE-SAE dataset.\nprovided in Table 10 in the appendix. For these\nexperiments, we employed the same techniques of\npruning, distillation, and quantization as used in\nthe monolingual experiments.\n7.1 Hate Speech Detection\nThe hate speech dataset evaluation results are pre-\nsented in Table 5 and Table 7. In contrast to the\ntrends observed in the monolingual evaluations con-\nducted for English, the impact on fairness, as mea-\nsured by the equality differences ( ED) metric, is\nnot as consistently evident among the compressed\nmodels in the multilingual setup. In the quantized\nand distilled models, the trends with respect to En-\nglish remain consistently negative.\nThe training for all these models was constrained\nto 5 epochs, and the F1 and AUC scores for the base\nmodels are lower than their compressed counter-\nparts. The compressed models demonstrate greater\nperformance gains within the same training du-\nration as compared to their base forms, and this\nobserved improvement in performance could con-\ntribute to enhanced fairness outcomes as well.\nFurthermore, it is worth considering that in previ-\nous monolingual tasks and even in the multilingual\nevaluation of Trustpilot reviews (Table 8), the com-\npressed models were more likely to experience a\ndrop in the ED metric. However, it is essential\nto highlight that the magnitude of this drop ob-\nserved in the current results is considerably less\npronounced. Additionally, the F1 and AUC per-\nformance of these models over these datasets is\nsignificantly higher.\nAcross nearly all the experiments conducted\nand languages documented in Tables 5, 7, and 8,\n15767\nModel Language AUC F1-macro Age Gender\nbert-base-\nmultilingual-cased\nEnglish 0.743 0.645 0.110 0.043\nItalian 0.662 0.509 0.064 0.070\nPolish 0.735 0.648 0.302 0.266\nPortuguese 0.616 0.539 0.194 0.181\nSpanish 0.676 0.618 0.177 0.179\ndistilbert-base-\nmultilingual-cased\nEnglish 0.790 0.702 0.199 ↑+0.089 0.084↑+0.041\nItalian 0.673 0.551 0.123 ↑+0.059 0.102↑+0.032\nPolish 0.706 0.638 0.264 ↓-0.038 0.249↓-0.017\nPortugese 0.651 0.513 0.031 ↓-0.163 0.173↓-0.008\nSpanish 0.695 0.617 0.134 ↓-0.043 0.135↓-0.044\nbert-base-multilingual\n-cased-quantized\nEnglish 0.750 0.641 0.141 ↑+0.031 0.080↑+0.037\nItalian 0.675 0.509 0.089 ↑+0.025 0.078↑+0.008\nPolish 0.735 0.628 0.314 ↑+0.012 0.242↓-0.024\nPortuguese 0.602 0.493 0.191 ↓-0.003 0.026↓-0.155\nSpanish 0.670 0.613 0.217 ↑+0.040 0.173↓-0.006\nbert-base-multilingual-\ncased-90%-pruned\nEnglish 0.813 0.708 0.135 ↑+0.025 0.075↑+0.032\nItalian 0.666 0.537 0.150 ↑+0.086 0.238↑+0.168\nPolish 0.698 0.580 0.221 ↓-0.081 0.230↓-0.036\nPortuguese 0.697 0.540 0.209 ↑+0.015 0.054↓-0.127\nSpanish 0.659 0.616 0.185 ↑+0.008 0.150↓-0.029\nbert-base-multilingual-\ncased-50%-pruned\nEnglish 0.764 0.657 0.078 ↓-0.032 0.048↑+0.005\nItalian 0.648 0.553 0.168 ↑+0.104 0.178↑+0.108\nPolish 0.711 0.622 0.245 ↓-0.057 0.233↓-0.033\nPortuguese 0.644 0.505 0.115 ↓-0.079 0.108↓-0.073\nSpanish 0.684 0.625 0.246 ↑+0.069 0.085↓-0.094\nbert-base-multilingual-\ncased-10%-pruned\nEnglish 0.745 0.644 0.089 ↓-0.021 0.051↑+0.008\nItalian 0.670 0.565 0.210 ↑+0.146 0.260↑+0.190\nPolish 0.670 0.597 0.160 ↓-0.142 0.167↓-0.099\nPortuguese 0.590 0.480 0.142 ↓-0.052 0.048↓-0.133\nSpanish 0.681 0.620 0.347 ↑+0.170 0.188↑+0.009\nxlm-roberta-large\nEnglish 0.529 0.218 0.005 0.004\nItalian 0.629 0.549 0.246 0.119\nPolish 0.580 0.520 0.080 0.067\nPortuguese 0.447 0.398 0.126 0.045\nSpanish 0.590 0.556 0.251 0.088\nmultilingual-MiniLM-L12-H384\nEnglish 0.701 0.605 0.060 ↑+0.055 0.032↑+0.028\nItalian 0.622 0.571 0.337 ↑+0.091 0.191↑+0.072\nPolish 0.643 0.587 0.138 ↑+0.058 0.098↑+0.031\nPortuguese 0.606 0.559 0.336 ↑+0.210 0.237↑+0.192\nSpanish 0.624 0.570 0.270 ↑+0.019 0.096↑+0.008\nTable 5: The results for the age and gender categories of the Hate Speech dataset. The lower the ED, the less biased\nthe model.\nLanguage Race Country Age Gender\nEnglish distilbert-base-multilingual-cased distilbert-base-multilingual-cased distilbert-base-multilingual-cased distilbert-base-multilingual-casedItalian - - bert-base-multilingual-cased-10%-pruned bert-base-multilingual-cased-10%-prunedSpanish multilingual-MiniLM-L12-H384 bert-base-multilingual-cased-90%-pruned bert-base-multilingual-cased-10%-pruned bert-base-multilingual-cased-10%-prunedPortuguese multilingual-MiniLM-L12-H384 multilingual-MiniLM-L12-H384 multilingual-MiniLM-L12-H384 multilingual-MiniLM-L12-H384Polish - - multilingual-MiniLM-L12-H384 multilingual-MiniLM-L12-H384\nTable 6: The list of compressed models which demonstrate the sharpest increase in the ED metric relative to their\nbase model.\n15768\nModel Language AUC F1-macro Race Country\nbert-base-\nmultilingual-cased\nEnglish 0.743 0.645 0.059 0.031\nPortuguese 0.616 0.539 0.200 0.109\nSpanish 0.676 0.618 0.087 0.130\ndistilbert-base-\nmultilingual-cased\nEnglish 0.790 0.702 0.086 ↑+0.027 0.077↑+0.046\nPortuguese 0.651 0.513 0.105 ↓-0.095 0.089↓-0.020\nSpanish 0.695 0.617 0.089 ↑+0.002 0.127↓-0.003\nbert-base-multilingual\n-cased-quantized\nEnglish 0.750 0.641 0.066 ↑+0.007 0.043↑+0.012\nPortuguese 0.602 0.493 0.069 ↓-0.131 0.037↓-0.072\nSpanish 0.670 0.613 0.039 ↓-0.048 0.149↑+0.019\nbert-base-multilingual-\ncased-90%-pruned\nEnglish 0.813 0.708 0.041 ↓-0.018 0.026↓-0.005\nPortuguese 0.697 0.540 0.151 ↓-0.049 0.106↓-0.003\nSpanish 0.659 0.616 0.033 ↓-0.054 0.289↑+0.159\nbert-base-multilingual-\ncased-50%-pruned\nEnglish 0.764 0.657 0.038 ↓-0.019 0.020↓-0.011\nPortugese 0.644 0.505 0.086 ↓-0.114 0.118↑+0.009\nSpanish 0.684 0.625 0.092 ↑+0.005 0.217↑+0.087\nbert-base-multilingual-\ncased-10%-pruned\nEnglish 0.745 0.644 0.024 ↓-0.025 0.009↓-0.022\nPortuguese 0.590 0.480 0.193 ↓-0.007 0.024↓-0.085\nSpanish 0.681 0.620 0.130 ↑+0.043 0.249↑+0.119\nxlm-roberta-large\nEnglish 0.529 0.218 0.005 0.003\nPortuguese 0.447 0.398 0.121 0.175\nSpanish 0.590 0.556 0.030 0.376\nmultilingual-MiniLM-L12-H384\nEnglish 0.701 0.605 0.011 ↑+0.006 0.027↑+0.024\nPortuguese 0.606 0.559 0.263 ↑+0.142 0.232↑+0.057\nSpanish 0.624 0.570 0.097 ↑+0.067 0.383↑+0.007\nTable 7: The results for the race/ethnicity and country categories of the Hate Speech dataset. The lower the ED, the\nless biased the model.\nthe MiniLM model distilled from XLM-R Large\ndemonstrates higher levels of bias compared to the\nbase model. These results also exhibit variations\nacross languages and dimensions under consider-\nation. A model may produce fairer outcomes for\ndata in one language but not necessarily generalize\nto another language or dimension. Additionally,\nthe trends observed in the ED values for pruning\nthe multilingual BERT-base model are not consis-\ntently monotonic. We have included the results for\nthe most significant decrease in magnitude across\neach dimension and language for these experiments\nin Table 6. Our benchmarking of these compressed\nmodels indicates that various elements in the exper-\nimental setup, such as the selection of techniques\nwithin a given compression method or the choice\nof pre-trained model architecture, are likely to have\nconsequences in the measurements we observe.\n7.2 Trustpilot Reviews Dataset\nWe also fine-tuned these models using a dataset\ncomprising Trustpilot reviews from four different\nlanguages. The results for the equality difference\n(ED) for gender are presented in Table 8. Although\nthe compressed models generally exhibit poorer\nperformance in terms of their overall equality dif-\nference, the magnitude of the difference in ED\nbetween the compressed models and their base\nforms is considerably smaller compared to the val-\nues observed in the previous task. However, it is\nworth noting that the results for the English reviews\ndataset (Table 12 in the appendix) contradict this\npattern. In that case, the compressed versions of\nBERT demonstrate less bias, whereas the opposite\nis true for XLM-R Large.\n8 How Does Model Compression Affect\nFairness?\n8.1 Distillation, Pruning and Quantization\nThe claim that distillation tends to amplify biases\nin models aligns with our findings in monolingual\nevaluation experiments. However, the impact on\nfairness metrics can vary, and this pattern does not\nnecessarily hold true in multilingual settings, as ev-\nidenced by our evaluation of multilingual fairness\ndatasets. Similar observations can be made regard-\ning pruned models, although further investigation\nis warranted to understand how different pruning\nstrategies and levels of pruning may influence these\neffects.\nIn contrast, our approach of post-training quanti-\nzation has yielded more diverse outcomes. While\nits impact on fairness may be relatively less pro-\nnounced, it can sometimes lead to impractical mod-\nels for downstream tasks due to their low perfor-\n15769\nModel Language F1-W Avg AUC-W Avg Total ED\nbert-base-multilingual-cased\nEnglish 0.981 0.987 0.026French 0.976 0.990 0.022German 0.979 0.985 0.014Danish 0.971 0.992 0.015\ndistilbert-base-multilingual-cased\nEnglish 0.975 0.987 0.0260.000\nFrench 0.971 0.984 0.037↑+0.015\nGerman 0.976 0.977 0.043↑+0.029\nDanish 0.964 0.992 0.020↑+0.005\nbert-base-multilingual-cased-quantized\nEnglish 0.978 0.984 0.047↑+0.021\nFrench 0.969 0.984 0.048↑+0.026\nGerman 0.976 0.980 0.005↓-0.009\nDanish 0.970 0.991 0.021↑+0.006\nbert-base-multilingual-cased-90%-pruned\nEnglish 0.976 0.988 0.029↑+0.003\nFrench 0.973 0.986 0.036↑+0.014\nGerman 0.975 0.982 0.025↑+0.011\nDanish 0.963 0.991 0.024↑+0.009\nbert-base-multilingual-cased-50%-pruned\nEnglish 0.980 0.989 0.020↓-0.006\nFrench 0.975 0.989 0.038↑+0.016\nGerman 0.977 0.988 0.025↑+0.011\nDanish 0.970 0.991 0.019↑+0.004\nbert-base-multilingual-cased-10%-pruned\nEnglish 0.979 0.988 0.0260.000\nFrench 0.976 0.988 0.028↑+0.006\nGerman 0.976 0.981 0.017↑+0.003\nDanish 0.969 0.993 0.017↑+0.002\nxlm-roberta-large\nEnglish 0.987 0.993 0.018French 0.984 0.991 0.024German 0.985 0.992 0.031Danish 0.985 0.994 0.008\nmultilingual-MiniLM-L12-H384\nEnglish 0.976 0.991 0.042↑+0.024\nFrench 0.972 0.989 0.041↑+0.017\nGerman 0.975 0.986 0.023↓-0.008\nDanish 0.970 0.993 0.017↑+0.009\nTable 8: The results for the gender category of the Trustpilot Reviews dataset. The lower the ED, the less biased the\nmodel.\nmance. Therefore, careful consideration is required\nwhen employing post-training quantization to strike\na balance between fairness and task effectiveness.\n8.2 Multilingual vs Monolingual Models\nWhile monolingual evaluation generally negatively\nimpacts fairness, the same cannot be said for multi-\nlingual evaluation, which varies across languages\nand dimensions. It would be valuable to investigate\nthe underlying causes for the decrease in fairness\nduring compression and explore its relationship\nwith the multilingual and monolingual aspects of\nthe model. It also remains to be seen whether well-\noptimized models for a specific task are more prone\nto demonstrating increased bias in their compressed\nversions, thereby possibly relying on unfair associ-\nations to make predictions.\n8.3 Additional Considerations\nThere are still lingering questions regarding the in-\nfluence of various elements, such as model size,\narchitecture choices, different variants of compres-\nsion techniques, and their impact on our evalua-\ntions. While our results seem to indicate other-\nwise for some of these parameters (such as size), it\nis essential to explore whether these observations\ntranslate across different tasks. As evinced by Tal\net al. (2022), the size of a model does not neces-\nsarily correlate with reduced biases, a notion that\nis further supported by our own findings. It would\nbe worthwhile to extensively examine how these\nmodels are affected when different compression\nmethods are combined or constrained to the same\nparameter count.\n9 Conclusion\nIn this work, we conduct a comprehensive evalu-\nation of fairness in compressed language models,\ncovering multiple base models, compression tech-\nniques, and various fairness metrics. While prior\nstudies have evaluated the fairness of compressed\nmodels, the results have not always been conclu-\nsive. In contrast, our extensive benchmarking pro-\nvides evidence that challenges recent research sug-\ngesting that model compression can effectively re-\nduce bias through regularization, and we demon-\nstrate that this is the case for both multilingual and\nmonolingual models across different datasets.\nThe compression of language models through\ndistillation, quantization, and pruning is crucial for\nthe practical use of language technologies in var-\nious real-world applications. While it is essential\nto preserve performance during compression, it is\nequally imperative that the compressed versions of\nlanguage models maintain or even enhance fairness\nmeasures to avoid potential harm.\n15770\n10 Ethics Statement\nOur results indicate that compression does harm\nfairness, particularly in the monolingual setting.\nThe potential harm that the system may cause and\nthe application it will be used for should be con-\nsidered when selecting a model compression tech-\nnique, in addition to factors like accuracy, latency,\nand size. Although we have not observed absolute\ntrends across models, datasets, and compression\ntechniques, it is especially crucial to evaluate com-\npressed models for fairness and accuracy before\ndeployment and, on a broader note, to understand\nwhy compressed models might exhibit issues with\nrespect to fairness.\nIn our paper, we conducted evaluations of multi-\nlingual language models using fairness metrics for\nvarious languages, including English. We observed\nvarying trends regarding their performance on fair-\nness metrics across different languages. However,\nit is vital to consider the potential influence of the\nlack of well-optimized models for these specific\ntasks, which may mitigate some of these issues.\nAdditionally, evaluation datasets are scarce for as-\nsessing bias in languages other than English and for\ndifferent fairness definitions. We also acknowledge\nthat fairness trends identified in English evaluations\nmay not necessarily be true for all languages.\nWhile our benchmarking encompassed multiple\nintrinsic and extrinsic metrics, it is important to\nacknowledge their limitations in capturing all di-\nmensions of fairness. Further research is needed\nto develop comprehensive extrinsic metrics across\ndiverse tasks. Although our work has been cen-\ntered around fairness in allocation-based (classifi-\ncation) applications, addressing fairness concerns\nin other types of language models, such as natural\nlanguage generation models, is necessary. In gen-\nerative tasks, the measurement of unfair outcomes\nwould be distinct from the methods we have used.\nAnother area of potential future work could involve\nbenchmarking debiasing methods for compressed\nmodels and developing new compression-aware\nmethods.\nLimitations\nThe primary motivation behind this paper was to\nprovide a comprehensive benchmarking study that\nexplores the impact of model compression tech-\nniques on bias in large language models. While our\nwork is among the first efforts to address fairness in\ncompressed language models across multiple com-\npression methods, including exploring multilingual\nsettings, we are aware of the inherent limitations\nassociated with our benchmarking study. Some of\nthe limitations and potential directions for future\nwork that builds on our study include the following:\n• Our study primarily focused on benchmark-\ning pre-trained models and evaluating their\nperformance in the downstream text classifi-\ncation task. Expanding our investigation to\nencompass other tasks, particularly those in-\nvolving generative models or large language\nmodels (LLMs), would be a valuable contri-\nbution to the research community. Examining\nthe impact of model compression techniques\non fairness in these domains would provide\nfurther insights and contribute to a more com-\nprehensive understanding of bias in different\ntypes of language models.\n• While our work includes a multilingual evalu-\nation component, we acknowledge that there\nis room for further improvement and compre-\nhensiveness in our benchmarking study, par-\nticularly with regard to quantization and prun-\ning techniques. Apart from this, we did not\nprovide a comparative analysis of monolin-\ngual and multilingual models using the same\nextrinsic data, which could provide valuable\ninsights into the disparate impact of compres-\nsion on the bias across languages. These are\npotential areas for future research that could\ncontribute to a more thorough understanding\nof bias in compressed language models.\n• Despite showing results for state-of-the-art\npruning methods, further benchmarking is\nnecessary to observe how bias varies across\ndifferent pruning techniques. Similarly, whilst\nour method serves as a proxy to estimate\nbias trends in quantized models, a thorough\nquantization-specific study is needed.\n• Different compression strategies yield var-\nied benefits in terms of latency, memory,\nand so forth. Investigating the tradeoffs be-\ntween these elements and fairness and accu-\nracy would yield valuable insights for obtain-\ning realistic estimations in real-world scenar-\nios. Additionally, conducting case-study anal-\nyses would give practitioners in the field a\ndeeper understanding of the potential harm\nthese methods may introduce.\n15771\nReferences\nJaimeen Ahn, Hwaran Lee, Jinhwa Kim, and Alice Oh.\n2022. Why knowledge distillation amplifies gender\nbias and how to mitigate from the perspective of Dis-\ntilBERT. In Proceedings of the 4th Workshop on Gen-\nder Bias in Natural Language Processing (GeBNLP),\npages 266–272, Seattle, Washington. Association for\nComputational Linguistics.\nSarah Alnegheimish, Alicia Guo, and Yi Sun. 2022.\nUsing natural sentence prompts for understanding bi-\nases in language models. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2824–2830, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nVamsi Aribandi, Yi Tay, and Donald Metzler. 2021.\nHow reliable are model diagnostics? In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 1778–1785, Online. Association\nfor Computational Linguistics.\nMarion Bartl, Malvina Nissim, and Albert Gatt. 2020.\nUnmasking contextual stereotypes: Measuring and\nmitigating BERT’s gender bias. In Proceedings of\nthe Second Workshop on Gender Bias in Natural\nLanguage Processing, pages 1–16, Barcelona, Spain\n(Online). Association for Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Zou,\nVenkatesh Saligrama, and Adam Kalai. 2016. Man is\nto computer programmer as woman is to homemaker?\ndebiasing word embeddings.\nAntónio Câmara, Nina Taneja, Tamjeed Azad, Emily\nAllaway, and Richard Zemel. 2022. Mapping the\nmultilingual margins: Intersectional biases of sen-\ntiment analysis systems in English, Spanish, and\nArabic. In Proceedings of the Second Workshop on\nLanguage Technology for Equality, Diversity and In-\nclusion, pages 90–106, Dublin, Ireland. Association\nfor Computational Linguistics.\nYang Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul\nGupta, Varun Kumar, Jwala Dhamala, and Aram Gal-\nstyan. 2022. On the intrinsic and extrinsic fairness\nevaluation metrics for contextualized language repre-\nsentations. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 561–570, Dublin,\nIreland. Association for Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. CoRR,\nabs/1911.02116.\nPaula Czarnowska, Yogarshi Vyas, and Kashif Shah.\n2021. Quantifying Social Biases in NLP: A Gen-\neralization and Empirical Comparison of Extrinsic\nFairness Metrics. Transactions of the Association for\nComputational Linguistics, 9:1249–1267.\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar-\njun Subramonian, Jeff Phillips, and Kai-Wei Chang.\n2021. Harms of gender exclusivity and challenges in\nnon-binary representation in language technologies.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1968–1994, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nJulia Dressel and Hany Farid. 2018. The accuracy, fair-\nness, and limits of predicting recidivism. Science\nAdvances, 4:eaao5580.\nAnjalie Field, Su Lin Blodgett, Zeerak Waseem, and\nYulia Tsvetkov. 2021. A survey of race, racism, and\nanti-racism in NLP. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1905–1925, Online. Association\nfor Computational Linguistics.\nTanmay Garg, Sarah Masud, Tharun Suresh, and Tan-\nmoy Chakraborty. 2022. Handling bias in toxic\nspeech detection: A survey.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\n15772\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1926–1940, Online. Association\nfor Computational Linguistics.\nHila Gonen, Yova Kementchedjhieva, and Yoav Gold-\nberg. 2019. How does grammatical gender affect\nnoun representations in gender-marking languages?\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n463–471, Hong Kong, China. Association for Com-\nputational Linguistics.\nSophie Groenwold, Lily Ou, Aesha Parekh, Samhita\nHonnavalli, Sharon Levy, Diba Mirza, and\nWilliam Yang Wang. 2020a. Investigating african-\namerican vernacular english in transformer-based\ntext generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5877–5883.\nSophie Groenwold, Lily Ou, Aesha Parekh, Samhita\nHonnavalli, Sharon Levy, Diba Mirza, and\nWilliam Yang Wang. 2020b. Investigating African-\nAmerican Vernacular English in transformer-based\ntext generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5877–5883, Online. As-\nsociation for Computational Linguistics.\nUmang Gupta, Jwala Dhamala, Varun Kumar, Apurv\nVerma, Yada Pruksachatkun, Satyapriya Krishna,\nRahul Gupta, Kai-Wei Chang, Greg Ver Steeg, and\nAram Galstyan. 2022. Mitigating gender bias in dis-\ntilled language models via counterfactual role rever-\nsal. In Findings of the Association for Computational\nLinguistics: ACL 2022, pages 658–678, Dublin, Ire-\nland. Association for Computational Linguistics.\nMarius Hessenthaler, Emma Strubell, Dirk Hovy, and\nAnne Lauscher. 2022. Bridging fairness and environ-\nmental sustainability in natural language processing.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nSara Hooker, Nyalleng Moorosi, Gregory Clark, Samy\nBengio, and Emily Denton. 2020. Characterising\nbias in compressed models.\nDirk Hovy, Anders Johannsen, and Anders Søgaard.\n2015. User review sites as a resource for large-scale\nsociolinguistic studies. In Proceedings of the 24th\nInternational Conference on World Wide Web, WWW\n’15, page 452–461, Republic and Canton of Geneva,\nCHE. International World Wide Web Conferences\nSteering Committee.\nXiaolei Huang. 2022. Easy adaptation to mitigate\ngender bias in multilingual text classification. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 717–723, Seattle, United States. Association\nfor Computational Linguistics.\nXiaolei Huang, Xing Linzi, Franck Dernoncourt, and\nMichael J. Paul. 2020a. Multilingual twitter corpus\nand baselines for evaluating demographic bias in hate\nspeech recognition. In Proceedings of the Twelveth\nInternational Conference on Language Resources\nand Evaluation (LREC 2020), Marseille, France. Eu-\nropean Language Resources Association (ELRA).\nXiaolei Huang and Michael J. Paul. 2019. Neural user\nfactor adaptation for text classification: Learning\nto generalize across author demographics. In Pro-\nceedings of the Eighth Joint Conference on Lexical\nand Computational Semantics (*SEM 2019), pages\n136–146, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXiaolei Huang, Linzi Xing, Franck Dernoncourt, and\nMichael J. Paul. 2020b. Multilingual Twitter cor-\npus and baselines for evaluating demographic bias\nin hate speech recognition. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 1440–1448, Marseille, France. European\nLanguage Resources Association.\nJigsaw.\nMasahiro Kaneko, Aizhan Imankulova, Danushka Bol-\nlegala, and Naoaki Okazaki. 2022. Gender bias in\nmasked language models for multiple languages. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2740–2750, Seattle, United States. Association\nfor Computational Linguistics.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert: Integer-\nonly bert quantization. In International conference\non machine learning, pages 5506–5518. PMLR.\nSvetlana Kiritchenko and Saif Mohammad. 2018. Ex-\namining gender and race bias in two hundred senti-\nment analysis systems. In Proceedings of the Sev-\nenth Joint Conference on Lexical and Computational\nSemantics, pages 43–53, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander M Rush. 2021. Block pruning for faster\ntransformers. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 10619–10629.\nAnne Lauscher, Tobias Lueken, and Goran Glavaš. 2021.\nSustainable modular debiasing of language models.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 4782–4797, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\n15773\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nLi Lucy and David Bamman. 2021. Gender and rep-\nresentation bias in GPT-3 generated stories. In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nNicholas Meade, Elinor Poole-Dayan, and Siva Reddy.\n2022. An empirical survey of the effectiveness of\ndebiasing techniques for pre-trained language models.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1878–1898, Dublin, Ireland.\nAssociation for Computational Linguistics.\nAlireza Mohammadshahi, Vassilina Nikoulina, Alexan-\ndre Berard, Caroline Brun, James Henderson, and\nLaurent Besacier. 2022. What do compressed multi-\nlingual machine translation models forget?\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nHadas Orgad and Yonatan Belinkov. 2022. Choose your\nlenses: Flaws in gender bias evaluation. In Proceed-\nings of the 4th Workshop on Gender Bias in Natu-\nral Language Processing (GeBNLP), pages 151–167,\nSeattle, Washington. Association for Computational\nLinguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nVictor Sanh, Thomas Wolf, and Alexander Rush. 2020.\nMovement pruning: Adaptive sparsity by fine-tuning.\nAdvances in Neural Information Processing Systems,\n33:20378–20389.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1668–1678, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAndrew Silva, Pradyumna Tambwekar, and Matthew\nGombolay. 2021. Towards a comprehensive under-\nstanding and accurate evaluation of societal biases in\npre-trained transformers. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2383–2389, Online.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices.\nYarden Tal, Inbal Magar, and Roy Schwartz. 2022.\nFewer errors, but more stereotypes? the effect of\nmodel size on gender bias. In Proceedings of the 4th\nWorkshop on Gender Bias in Natural Language Pro-\ncessing (GeBNLP), pages 112–120, Seattle, Wash-\nington. Association for Computational Linguistics.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna\nClinciu, Manan Dey, Shayne Longpre, Sasha Luc-\ncioni, Maraim Masoud, Margaret Mitchell, Dragomir\nRadev, Shanya Sharma, Arjun Subramonian, Jaesung\nTae, Samson Tan, Deepak Tunuguntla, and Oskar Van\nDer Wal. 2022. You reap what you sow: On the chal-\nlenges of bias evaluation under multilingual settings.\nIn Proceedings of BigScience Episode #5 – Workshop\non Challenges & Perspectives in Creating Large Lan-\nguage Models, pages 26–41, virtual+Dublin. Associ-\nation for Computational Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020a. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers.\nZiheng Wang, Jeremy Wohlwend, and Tao Lei. 2020b.\nStructured pruning of large language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6151–6162.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel,\nEmily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and\nSlav Petrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models.\n15774\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nGuangxuan Xu and Qingyuan Hu. 2022. Can\nmodel compression improve nlp fairness. ArXiv,\nabs/2201.08542.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019a. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019b. Q8BERT: quantized 8bit BERT.\nCoRR, abs/1910.06188.\nOfir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen,\nand Moshe Wasserblat. 2021. Prune once for all:\nSparse pre-trained language models. arXiv preprint\narXiv:2111.05754.\nJieyu Zhao, Subhabrata Mukherjee, Saghar Hosseini,\nKai-Wei Chang, and Ahmed Hassan Awadallah. 2020.\nGender bias in multilingual embeddings and cross-\nlingual transfer. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 2896–2907, Online. Association for\nComputational Linguistics.\nA Appendix\nA.1 Methodology and Setup\nA.1.1 Pruning\nWe adopt the Prune Once For Allor Prune OFA\nmethod (Zafrir et al., 2021) as our central pruning\nstrategy. Prune OFA has demonstrated state-of-\nthe-art performance in terms of compression-to-\naccuracy ratio for BERT-based models, and it also\neliminates the need to conduct task-specific prun-\ning, as the sparse pre-trained language model can\nbe directly fine-tuned on the target task. This sim-\nplifies our comparisons, as the same pruned model\ncan be fine-tuned on different datasets.\nA.1.2 Distillation\nWe use the pre-trained distilled variants of base\nmodels such as BERT (Devlin et al., 2018),\nRoBERTa (Liu et al., 2019), and XLM-R (Con-\nneau et al., 2019), namely DistilBERT (Sanh et al.,\n2019), DistilRoBERTa, and multilingual MiniLM\n(Wang et al., 2020a), which are publicly available\nthrough the HuggingFace API (Wolf et al., 2020)\nfor our experiments. DistilBERT selects one layer\nfrom each pair of alternate layers in the teacher\narchitecture (BERT-base), lowering the number of\nlayers in the distilled model by half. MiniLM is\ndistilled from the final attention layer of the teacher\nmodel, thus making this knowledge distillation\nmethod task-independent. In addition to evaluat-\ning bias in these pre-trained models using intrinsic\nmetrics, we fine-tuned some distilled models on the\nSAE-AA VE, Jigsaw, and Equity Evaluation Cor-\npus (EEC) datasets for evaluation using extrinsic\nmetrics.\nA.1.3 Quantization\nDynamic quantization is particularly effective\nwhen the time required to execute a model is domi-\nnated by loading weights from memory rather than\ncomputing matrix multiplications, as with trans-\nformer models. Therefore, we adopt dynamic quan-\ntization in all of our experiments. With this ap-\nproach, model parameters are converted to INT-8\nformat post-training, and the scale factor for activa-\ntions is dynamically determined based on the range\nof the data observed at runtime, which helps to\nmaintain flexibility in the model and minimize any\nloss in performance. Additionally, dynamic quanti-\nzation requires minimal hyperparameter tuning and\nis easy to deploy in production.\nA.2 Further Details on Pruning, Quantization\nand Distillation\nA.2.1 Pruning\nNeural architecture pruning aims at eliminating\nredundant parts of neural networks while main-\ntaining model performance. Unstructured pruning\nremoves individual neurons by setting the value\nof these parameters to zero, whereas structured\npruning removes groups of neurons such as lay-\ners, attention heads, and so forth. (Sanh et al.,\n2020) presents a form of unstructured weight prun-\ning in which individual weights can be eliminated\nto create a sparse network. Although massive re-\nductions in the parameter count are observed, the\ninference speeds show no such improvement. On\nthe other hand, structured pruning methods (Wang\net al., 2020b) achieve faster inference speeds along\nwith a reduction in parameter size. (Lagunas et al.,\n2021) extend the work of movement pruning to\nthe structured and semi-structured domains. Re-\n15775\ncently, (Zafrir et al., 2021) showed that integrating\npruning during the pre-training of language models\ngives high-performing sparse pre-trained models,\nthus removing the burden of pruning for a specific\ndownstream task.\nA.2.2 Distillation\nKnowledge distillation (KD) (Hinton et al., 2015)\nhas been shown to effectively transfer knowledge\nfrom a teacher model to a smaller student model,\nwith a loss function designed to minimize the dis-\ntance between the features or the outputs of the\nstudent and teacher models. Numerous alterations\ncan be made to the KD setup, such as choosing\nintermediate layers of the teacher model for initial-\nizing the student architecture (Sanh et al., 2019),\ndistilling the final attention layer of the teacher\ntransformer architecture (Wang et al., 2020a), in-\ntroducing bottlenecks for distillation (Sun et al.,\n2020). However, biases in the teacher model could\npotentially propagate into the distilled models mak-\ning it more biased compared to the original teacher\nmodel (Silva et al., 2021).\nA.2.3 Quantization\nQuantization compresses models by representing\nmodel weights and/or activations with lower bit\nprecisions. It can also make it possible to carry out\ninference using integer-only operations, as demon-\nstrated by Kim et al. (2021). There are two main\napproaches to quantization: post-training quantiza-\ntion, which is applied to a pre-trained model, and\nquantization-aware training (Zafrir et al., 2019a),\nwhich incorporates quantization into the training\nprocess in order to mitigate the loss of accuracy\nthat can occur with post-training quantization.\n15776\nB Additional Results\nWe have included the results and a brief description\nfor certain monolingual and multilingual measures\nbelow. Our decision to include the Equity Evalua-\ntion Corpus (EEC) and Log Probability Bias Score\n(LPBS) metric measures in the appendix is moti-\nvated by the fact that both these metrics consist\nof template-based data lacking concrete fairness\nobjectives, and are therefore not a reflection of\nharms that can be caused in real-world applications.\nRecent research (Alnegheimish et al., 2022) has\neffectively highlighted the sensitivity of template-\nbased evaluations to the selection and design of\ntemplates, which can bias the results. Furthermore,\nthe LPBS is an intrinsic measure, and Aribandi et al.\n(2021) addresses the instability of likelihood and\nrepresentation-based model diagnostic measures.\nTherefore, we advise readers to exercise caution\nwhen drawing conclusions from these results.\nB.1 Multilingual Datasets\nThe findings of our multilingual evaluation on the\nEnglish reviews dataset, comprising reviews ob-\ntained from platforms such as Amazon and Yelp,\nhave been presented in Table 12. Additionally, we\nhave included the results of the evaluation of multi-\nlingual models on the English version of StereoSet\nin Table 13, as well as the evaluation of these mod-\nels for the Crows-Pair dataset for English in Table\n14. Ideally, we intended to study the performance\nof models on datasets with comparable fairness\nnotions or objectives in both monolingual and mul-\ntilingual contexts. Unfortunately, we encountered\nlimitations in sourcing such datasets, and therefore,\nwe leave this as an avenue for future research.\nC Fine-tuning Setup\nFor the extrinsic measures, the models are fine-\ntuned over a specific training dataset before the\nfairness evaluation is carried out over the test set.\nMost of our fine-tuning setups have been derived\nfrom previous work (Huang et al., 2020a; Huang,\n2022; Câmara et al., 2022). The intrinsic measures\ndo not require a hyperparameter search, as they are\nevaluated over pre-trained model representations.\nFor the extrinsic measures, we relied on pre-trained\nSST-2 fine-tuned models available on HuggingFace.\nWe performed fine-tuning solely for all the mod-\nels trained on the Jigsaw Toxicity Classification\ndataset, the final details of which are as follows:\n• Batch Size : 16\n• Learning Rate : 1e-4\n• Weight decay: 0.01\n• Warmup Ratio: 0.06\n• Epochs: 5\n• Optimizer: AdamW\nC.1 Equity Evaluation Corpus\nThe Equity Evaluation Corpus (Kiritchenko and\nMohammad, 2018) is a template-based corpus for\nevaluating sentiment analysis systems for emo-\ntional intensity across four categories (joy, sadness,\nanger and joy). In this particular task, we mea-\nsure the Pearson Correlation Coefficient (PCC) of\nthe predictions of these models against the gold\nlabel. the It must be noted that previous research\n(Alnegheimish et al., 2022) indicates that bias eval-\nuation is sensitive to design choices in template-\nbased data, and that evaluating our models over\nnatural sentence-based datasets would be a better\nalternative to gauge the impact these models can\nhave. The fairness objective here looks to address\nthe disparity in terms of the PCC across all the\nmodels across the different categories of template\ndata. The results have been reported in Table 15.\nC.2 Log Probability Bias Score\nThe Log Probability Bias Score (LPBS) (Kurita\net al., 2019) was proposed as a modification to the\nDisCo metric (Webster et al., 2020). LPBS operates\nsimilarly to WEAT, using template sentences (e.g.,\n‘[TGT] likes to [ATT]’) in which TGT represents\na list of target words and ATT represents a list\nof attributes for which we aim to measure biased\nassociations. The test also accounts for the prior\nprobability of the target attribute, allowing us to\nevaluate bias solely based on the attributes without\nbeing influenced by the prior probability of the\ntarget token. The attribute categories that we have\ntaken into consideration are a list of professions,\npositive words, and negative words (Bartl et al.,\n2020) (Kurita et al., 2019). The results have been\nreported in Table 16.\n15777\nModel Name Parameter # Jigsaw EEC AA VE-SAE StereoSet CrowS-Pair LPBS\nbert-base-uncased 110 M ✓ ✓ ✓ ✓ ✓ ✓\ndistilbert-base-uncased 66 M ✓ ✓ ✓ ✓ ✓ ✓\nminiLM-L12-H384-uncased 33 M ✓ ✓ ✓ ✗ ✗ ✗\nbert-base-uncased-85%-pruned 16.5 M ✓ ✓ ✓ ✓ ✓ ✓\nbert-base-uncased-90%-pruned 11 M ✓ ✓ ✓ ✓ ✓ ✓\nbert-base-uncased-quantized 110 M ✓ ✓ ✓ ✓ ✓ ✓\nbert-large-uncased 340 M ✓ ✓ ✓ ✓ ✓ ✓\nbert-large-uncased-90%-pruned 34 M ✗ ✓ ✗ ✓ ✓ ✓\nbert-large-uncased-quantized 340 M ✓ ✓ ✓ ✓ ✓ ✓\nbert-base-multilingual-cased 178 M ✓ ✓ ✗ ✓ ✓ ✓\ndistilbert-base-multilingual-cased 135 M ✓ ✓ ✗ ✓ ✓ ✓\nxlm-roberta-large 560 M ✗ ✗ ✗ ✓ ✓ ✓\nmultilingual-MiniLM-L12-H384 [xlm-roberta-large] 117 M✗ ✗ ✗ ✓ ✓ ✓\nxlm-roberta-base 278 M ✓ ✓ ✓ ✗ ✗ ✗\nmultilingual-MiniLM-L12-H384 [xlm-roberta-base] 117 M✓ ✓ ✓ ✗ ✗ ✗\nroberta-base 125 M ✓ ✓ ✓ ✓ ✓ ✓\ndistilroberta 82 M ✓ ✓ ✓ ✓ ✓ ✓\nroberta-base-quantized 125 M ✓ ✓ ✓ ✓ ✓ ✓\nTable 9: Details about the models and which metrics they were evaluated for in the monolingual fairness\nexperiments. The parameter counts for the pruned models indicates the total number of non-sparse parameters.\nSome of the models could not be evaluated for the intrinsic measures due to their architectural setup.\nModel Name Parameter #\nbert-base-multilingual-cased 178 M\ndistilbert-base-multilingual-cased 135 M\nbert-base-multilingual-cased-10%-pruned 160 M\nbert-base-multilingual-cased-50%-pruned 89 M\nbert-base-multilingual-cased-90%-pruned 17 M\nbert-base-multilingual-cased-quantized 178 M\nxlm-roberta-large 560 M\nmultilingual-MiniLM-L12-H384 117 M\nTable 10: Parameter count for all the models used for the multilingual fairness evaluation experiments. The\nparameter counts for the pruned models indicates the total number of non-sparse parameters. These models have\nbeen used uniformly for all the multilingual datasets.\nMetric Type of Metric Downstream Task Template-Based Fairness Objective Dimensions\nMonolingual\nJigsaw ToxicityUnintended BiasExtrinsic ToxicityDetection No Increased likelihood of being classifyingcomment as toxic based on identity group mentions\nMultiple [Gender, Religion,Race/Ethnicity, Sexual Orientation,Disability, etc]\nAA VE-SAE ExtrinsicSentimentClassificationNo Increased likelihood of being classifying commentas negative based on dialect usedDialect\nEEC Extrinsic SentimentClassificationYes Difference in emotion categories for emotionalintensity prediction Emotional Intensity\nStereoSet Intrinsic N/A No Evaluation of model preference for stereotypicalsentences Gender, Race/Ethnicity, Religion,Profession\nCrowS-Pair Intrinsic N/A No Evaluation of model preference for stereotypicalsentences Gender, Race/Ethnicity, Religion\nLPBS Intrinsic N/A Yes Evaluation of model preference for stereotypicalassociations Gender\nMultilingual\nHate Speech Extrinsic Hate SpeechDetection No Measuring performance across data based on thedemographic groups they are sourced fromAge, Gender, Country, Race/Ethnicity\nReviews Dataset ExtrinsicSentimentClassificationNo Measuring performance across data based on thedemographic groups they are sourced fromGender\nTable 11: List of all the details pertaining to the fairness metrics used.\n15778\nModel F1-W Avg AUC-W Avg Total ED\nbert-base-multilingual-cased 0.872 0.916 0.499\ndistilbert-base-multilingual-cased 0.868 0.914 0.350 ↑-0.149\nbert-base-multilingual-cased-quantized 0.854 0.892 0.317 ↑-0.182\nbert-base-multilingual-cased-10%-pruned 0.869 0.921 0.258 ↑-0.241\nbert-base-multilingual-cased-50%-pruned 0.865 0.918 0.313 ↑-0.186\nbert-base-multilingual-cased-90%-pruned 0.862 0.910 0.442 ↑-0.057\nxlm-roberta-large 0.908 0.947 0.290\nmultilingual-MiniLM-L12-H384-distilled-XLMR-Large 0.839 0.898 0.402 ↓+0.112\nxlm-roberta-large-quantized 0.865 0.928 0.474 ↓+0.184\nxlm-roberta-base 0.787 0.900 0.349 ↓+0.059\nTable 12: We report the performance of multilingual models and the ED (equality differences) fairness estimate\nover a set of English reviews sourced from websites such as Amazon, Yelp, etc. The higher the ED, the less fair the\nmodel.\nModel Overall ICAT Score\nbert-base-multilingual-cased 64.94\ndistilbert-base-multilingual-cased 67.99 ↑+3.05\nbert-base-multilingual-cased-quantized 64.78↓-0.16\nbert-base-multilingual-cased-10%-pruned 67.82↑+2.88\nbert-base-multilingual-cased-50%-pruned 66.67↑+1.73\nbert-base-multilingual-cased-90%-pruned 67.00↑+2.06\nxlm-roberta-large 71.29\nmultilingual-MiniLM-L12-H384 52.47 ↓-18.82\nxlm-roberta-large-quantized 69.63 ↓-1.66\nTable 13: The overall ICAT score for the multilingual models for the StereoSet (English) dataset. The higher the\nICAT score, the less biased the model.\nModel Gender Race Religion\nbert-base-multilingual-cased 47.71 -2.29 44.66 -5.34 53.33 +3.33\ndistilbert-base-multilingual-cased 50.38 +0.38 41.94 -8.06 53.33 +3.33\nbert-base-multilingual-cased-quantized 52.29 +2.29 42.72 -7.28 52.38 +2.38\nbert-base-multilingual-cased-10%-pruned 47.71 -2.29 47.57 -2.43 58.1 +8.1\nbert-base-multilingual-cased-50%-pruned 49.24 -0.76 48.54 -1.46 56.19 +6.19\nbert-base-multilingual-cased-90%-pruned 50.0 0 57.48 +7.48 53.33 +3.33\nxlm-roberta-large 54.41 +4.41 51.65 +1.65 69.52 +19.52\nmultilingual-MiniLM-L12-H384 39.85 -10.15 60.39 +10.39 47.62 -2.38\nxlm-roberta-large-quantized 52.87 2.87 57.28 +7.28 71.43 +21.43\nTable 14: The results for the CrowS-Pairs metric for multilingual models have been reported, with values closer to\n50 indicating less biased models according to this metric.\n15779\nModel Joy Sadness Anger Fear\nbert-base-uncased 0.600 0.533 0.557 0.552distilbert-base-uncased 0.623 0.587 0.623 0.565distilbert-base-uncased-60%-pruned 0.586 0.551 0.585 0.540miniLM-L12-H384-uncased 0.352 0.195 0.230 0.245miniLM-L12-H384-uncased-70%-pruned 0.600 0.539 0.573 0.547bert-base-uncased-85%-pruned 0.550 0.432 0.464 0.478bert-base-uncased-90%-pruned 0.523 0.418 0.450 0.472bert-base-uncased-quantized 0.455 0.382 0.383 0.410\nbert-base-multilingual-cased0.506 0.386 0.364 0.408distilbert-base-multilingual-cased 0.478 0.380 0.328 0.410\nxlm-roberta-base 0.491 0.476 0.039 0.354multilingual-miniLM-L12-H384 0.336 0.012 0.019 0.046\nroberta-base 0.495 0.305 0.393 0.450distilroberta-base 0.540 0.503 0.508 0.557roberta-base-quantized 0.177 0.230 0.360 0.108\nbert-large-uncased 0.545 0.450 0.549 0.503bert-large-uncased-90%-pruned 0.614 0.476 0.519 0.547bert-large-uncased-quantized 0.375 0.314 0.356 0.364\nTable 15: The results for the emotionality intensity regression task over the EEC corpus. The results represent the\nPearson Correlation Coefficient of the model for each emotion and indicates how the model performs on that\nparticular category’s template data.\nProfession Positive Negative\nbert-base-uncased 0.694 0.040 0.111\ndistilbert-base-uncased 1.113 0.279 0.218\ndistilbert-base-uncased-60% 0.206 0.422 0.361\nbert-base-uncased-85%-pruned 1.393 0.090 0.135\nbert-base-uncased-90%-pruned 1.943 0.070 0.048\nbert-base-uncased-quantized 1.116 0.102 0.006\nbert-base-multilingual-cased 1.326 0.322 0.052\ndistilbert-base-multilingual-cased 0.660 0.005 0.053\nxlm-roberta-large 2.007 0.031 0.073\nmultilingual-MiniLM-L12-H384 0.667 1.739 0.028\nroberta-base 4.704 0.016 0.014\ndistilroberta 6.218 0.287 0.271\nroberta-base-quantized 3.657 0.019 0.014\nbert-large-uncased 0.155 0.359 0.343\nbert-large-uncased-90%-pruned 0.899 0.293 0.269\nbert-large-uncased-quantized 1.861 0.115 0.082\nTable 16: The results for the effect size from the LPBS metric. The higher the effect size (calculated using Cohen’s\nd), the higher the magnitude of bias in the model.\nDataset Languages\nStereoSet en\nCrows-Pair en\nReviews Dataset en, fr, de, dk\nHate Speech Detection en, pt, es, it, po\nTable 17: List of the multilingual datasets and their corresponding languages.\n15780\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nYes, in the ﬁnal section of the paper.\n□\u0017 A2. Did you discuss any potential risks of your work?\nNo, as our work deals with pointing out the potential risks of certain strategies used to optimize for\nefﬁciency that could inadvertently cause models to make more biased decision. We have extensively\ndiscussed the limitations and other potential implications of our ﬁndings, however.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nYes, Section 1. Section 8 does this as well.\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nYes, Grammarly, for ensuring that the content was grammatically correct and coherent.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nYes, we cited the relevant papers/authors that developed these datasets in whichever sections they\nwere brought up.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe datasets we used are open access and not proprietary.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe use the datasets for their intended use i.e to evaluate models.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThe details for the datasets are listed in tables in the Appendix.\n□\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15781\nC □\u0013 Did you run computational experiments?\nSection 6 onwards\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nYes, Section 4 onward. We utilized hyperparameters that have been used in previous related research\nand did not speciﬁcally perform a hyperparameter search to ﬁnd the most optimized model, due to the\ncomputational expense required given the extent of our benchmarking. The point of this paper was to\nevaluate and benchmark fairness across a series of compression methods to provide a conclusive\nanswer as to how compression may affect fairness, and to what extent.\n□\u0017 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo, our experiments are based on fairness evaluation and do not require multiple reruns.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe have referenced previous research that proposes evaluation metrics using which we have carried\nout these evaluations.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n15782",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8240392208099365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5451173186302185
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.5273944735527039
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5143598318099976
    },
    {
      "name": "Machine learning",
      "score": 0.5119117498397827
    },
    {
      "name": "Language model",
      "score": 0.502936065196991
    },
    {
      "name": "Compression (physics)",
      "score": 0.4821281433105469
    },
    {
      "name": "Deep learning",
      "score": 0.42749661207199097
    },
    {
      "name": "Natural language processing",
      "score": 0.36135047674179077
    },
    {
      "name": "Algorithm",
      "score": 0.11303991079330444
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I74796645",
      "name": "Birla Institute of Technology and Science, Pilani",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I189109744",
      "name": "Indian Institute of Technology Dhanbad",
      "country": "IN"
    }
  ],
  "cited_by": 9
}