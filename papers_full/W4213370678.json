{
  "title": "MisRoBÆRTa: Transformers versus Misinformation",
  "url": "https://openalex.org/W4213370678",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Truic\\u{a}, Ciprian-Octavian",
      "affiliations": [
        "Uppsala University",
        "Universitatea Națională de Știință și Tehnologie Politehnica București"
      ]
    },
    {
      "id": "https://openalex.org/A4298730867",
      "name": "Apostol, Elena-Simona",
      "affiliations": [
        "Universitatea Națională de Știință și Tehnologie Politehnica București"
      ]
    },
    {
      "id": "https://openalex.org/A4296125875",
      "name": "Truică, Ciprian-Octavian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2913578037",
    "https://openalex.org/W2907225248",
    "https://openalex.org/W4206512241",
    "https://openalex.org/W4288281505",
    "https://openalex.org/W2792307011",
    "https://openalex.org/W3124916197",
    "https://openalex.org/W3119467012",
    "https://openalex.org/W2410465342",
    "https://openalex.org/W2924988155",
    "https://openalex.org/W3001895040",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2967982882",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2977130232",
    "https://openalex.org/W3168667219",
    "https://openalex.org/W2969762015",
    "https://openalex.org/W3037029545",
    "https://openalex.org/W2977526300",
    "https://openalex.org/W3028908967",
    "https://openalex.org/W3034195663",
    "https://openalex.org/W3080612337",
    "https://openalex.org/W3158012322",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3021435352",
    "https://openalex.org/W6792624057",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W6767737316",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2160306971",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963477629",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W3102641573",
    "https://openalex.org/W2945214158",
    "https://openalex.org/W2128521126",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3115903740",
    "https://openalex.org/W2810806782",
    "https://openalex.org/W3108788912",
    "https://openalex.org/W3211064917",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W4392798193",
    "https://openalex.org/W3105343401",
    "https://openalex.org/W3138953784",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3104758113",
    "https://openalex.org/W3036332725",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Misinformation is considered a threat to our democratic values and principles. The spread of such content on social media polarizes society and undermines public discourse by distorting public perceptions and generating social unrest while lacking the rigor of traditional journalism. Transformers and transfer learning proved to be state-of-the-art methods for multiple well-known natural language processing tasks. In this paper, we propose MisRoBÆRTa, a novel transformer-based deep neural ensemble architecture for misinformation detection. MisRoBÆRTa takes advantage of two state-of-the art transformers, i.e., BART and RoBERTa, to improve the performance of discriminating between real news and different types of fake news. We also benchmarked and evaluated the performances of multiple transformers on the task of misinformation detection. For training and testing, we used a large real-world news articles dataset (i.e., 100,000 records) labeled with 10 classes, thus addressing two shortcomings in the current research: (1) increasing the size of the dataset from small to large, and (2) moving the focus of fake news detection from binary classification to multi-class classification. For this dataset, we manually verified the content of the news articles to ensure that they were correctly labeled. The experimental results show that the accuracy of transformers on the misinformation detection problem was significantly influenced by the method employed to learn the context, dataset size, and vocabulary dimension. We observe empirically that the best accuracy performance among the classification models that use only one transformer is obtained by BART, while DistilRoBERTa obtains the best accuracy in the least amount of time required for fine-tuning and training. However, the proposed MisRoBÆRTa outperforms the other transformer models in the task of misinformation detection. To arrive at this conclusion, we performed ample ablation and sensitivity testing with MisRoBÆRTa on two datasets.",
  "full_text": null,
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.7921980619430542
    },
    {
      "name": "Transformer",
      "score": 0.5166659355163574
    },
    {
      "name": "Psychology",
      "score": 0.34346818923950195
    },
    {
      "name": "Computer science",
      "score": 0.3226870894432068
    },
    {
      "name": "Engineering",
      "score": 0.2126063108444214
    },
    {
      "name": "Electrical engineering",
      "score": 0.19922402501106262
    },
    {
      "name": "Computer security",
      "score": 0.17520803213119507
    },
    {
      "name": "Voltage",
      "score": 0.04263567924499512
    }
  ]
}