{
    "title": "Protein language models trained on multiple sequence alignments learn phylogenetic relationships",
    "url": "https://openalex.org/W4307068738",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1980321851",
            "name": "Umberto Lupo",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2911972329",
            "name": "Damiano Sgarbossa",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A4228007736",
            "name": "Anne-Florence Bitbol",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A1980321851",
            "name": "Umberto Lupo",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2911972329",
            "name": "Damiano Sgarbossa",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A4228007736",
            "name": "Anne-Florence Bitbol",
            "affiliations": [
                "SIB Swiss Institute of Bioinformatics",
                "École Polytechnique Fédérale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2065921821",
        "https://openalex.org/W2593641128",
        "https://openalex.org/W4297734170",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3040739508",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3111174583",
        "https://openalex.org/W4226486988",
        "https://openalex.org/W6783944145",
        "https://openalex.org/W3010387158",
        "https://openalex.org/W3186612807",
        "https://openalex.org/W3216580380",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W3186179742",
        "https://openalex.org/W3191761521",
        "https://openalex.org/W1995924392",
        "https://openalex.org/W2053671774",
        "https://openalex.org/W2146891463",
        "https://openalex.org/W2151457629",
        "https://openalex.org/W1514077080",
        "https://openalex.org/W1979762151",
        "https://openalex.org/W2061042699",
        "https://openalex.org/W2008545402",
        "https://openalex.org/W1965839094",
        "https://openalex.org/W2137566700",
        "https://openalex.org/W2048310584",
        "https://openalex.org/W2783644078",
        "https://openalex.org/W2769882797",
        "https://openalex.org/W6791955017",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W4210494137",
        "https://openalex.org/W74946117",
        "https://openalex.org/W2031611770",
        "https://openalex.org/W3044778276",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3037888463",
        "https://openalex.org/W3146384714",
        "https://openalex.org/W2890223884",
        "https://openalex.org/W2995361046",
        "https://openalex.org/W3208082951",
        "https://openalex.org/W2772248591",
        "https://openalex.org/W2951716718",
        "https://openalex.org/W2988868846",
        "https://openalex.org/W3165376129",
        "https://openalex.org/W2137991504",
        "https://openalex.org/W2979846625",
        "https://openalex.org/W2983468544",
        "https://openalex.org/W4206510828",
        "https://openalex.org/W4210672654",
        "https://openalex.org/W4223950755",
        "https://openalex.org/W3095583226",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W3122896559",
        "https://openalex.org/W2181523240",
        "https://openalex.org/W4206925492",
        "https://openalex.org/W4319061788",
        "https://openalex.org/W3100163799",
        "https://openalex.org/W3211728297",
        "https://openalex.org/W2913668833"
    ],
    "abstract": "Abstract Self-supervised neural language models with attention have recently been applied to biological sequence data, advancing structure, function and mutational effect prediction. Some protein language models, including MSA Transformer and AlphaFold’s EvoFormer, take multiple sequence alignments (MSAs) of evolutionarily related proteins as inputs. Simple combinations of MSA Transformer’s row attentions have led to state-of-the-art unsupervised structural contact prediction. We demonstrate that similarly simple, and universal, combinations of MSA Transformer’s column attentions strongly correlate with Hamming distances between sequences in MSAs. Therefore, MSA-based language models encode detailed phylogenetic relationships. We further show that these models can separate coevolutionary signals encoding functional and structural constraints from phylogenetic correlations reflecting historical contingency. To assess this, we generate synthetic MSAs, either without or with phylogeny, from Potts models trained on natural MSAs. We find that unsupervised contact prediction is substantially more resilient to phylogenetic noise when using MSA Transformer versus inferred Potts models.",
    "full_text": "Article https://doi.org/10.1038/s41467-022-34032-y\nProtein language models trained on multiple\nsequence alignments learn phylogenetic\nrelationships\nUmberto Lupo 1,2 , Damiano Sgarbossa1,2 & Anne-Florence Bitbol1,2\nSelf-supervised neural language models with attention have recently been\napplied to biological sequence data, advancing structure, function and\nmutational effect prediction. Some protein language models, including MSA\nTransformer and AlphaFold’sE v o F o r m e r ,t a k em u l t i p l es e q u e n c ea l i g n m e n t s\n(MSAs) of evolutionarily related proteins as inputs. Simple combinations of\nMSA Transformer’s row attentions have led to state-of-the-art unsupervised\nstructural contact prediction. We demonstrate that similarly simple, and uni-\nversal, combinations of MSA Transformer’s column attentions strongly cor-\nrelate with Hamming distances between sequences in MSAs. Therefore,\nMSA-based language models encode detailed phylogenetic relationships.\nWe further show that these models can separate coevolutionary signals\nencoding functional and structural constraints from phylogenetic correlations\nreﬂecting historical contingency. To assess this, we generate synthetic MSAs,\neither without or with phylogeny, from Potts models trained on natural MSAs.\nWe ﬁnd that unsupervised contact prediction is substantially more resilient\nto phylogenetic noise when using MSA Transformer versus inferred\nPotts models.\nThe explosion of available biological sequence data has led to\nmultiple computational approaches aiming to infer three-\ndimensional structure, biological function, ﬁtness, and evolu-\ntionary history of proteins from sequence data\n1,2. Recently, self-\nsupervised deep learning models based on natural language pro-\ncessing methods, especially attention\n3 and transformers4,h a v e\nbeen trained on large ensembles of protein sequences by means of\nthe masked language modeling objective ofﬁlling in masked amino\nacids in a sequence, given the surrounding ones\n5–10. These models,\nwhich capture long-range dependencies, learn rich representations\nof protein sequences, and can be employed for multiple tasks. In\nparticular, they can predict structural contacts from single\nsequences in an unsupervised way\n7, presumably by transferring\nknowledge from their large training set11. Neural network archi-\ntectures based on attention are also employed in the Evoformer\nblocks in AlphaFold12,a sw e l la si nR o s e T T A F o l d13 and RGN214,a n d\nthey contributed to the recent breakthrough in the supervised\nprediction of protein structure.\nProtein sequences can be classiﬁed in families of homologous\nproteins, that descend from an ancestral protein and share a similar\nstructure and function. Analyzing multiple sequence alignments\n(MSAs) of homologous proteins thus provides substantial information\nabout functional and structural constraints\n1. The statistics of MSA\ncolumns, representing amino-acid sites, allow to identify functional\nresidues that are conserved during evolution, and correlations of\namino-acid usage between columns contain key information about\nfunctional sectors and structural contacts\n15–18. Indeed, through the\ncourse of evolution, contacting amino acids need to maintain their\nphysico-chemical complementarity, which leads to correlated amino-\nacid usages at these sites: this is known as coevolution. Potts models,\nalso known as Direct Coupling Analysis (DCA), are pairwise maximum\nentropy models trained to match the empirical one- and two-body\nReceived: 8 April 2022\nAccepted: 7 October 2022\nCheck for updates\n1Institute of Bioengineering, School of Life Sciences, École Polytechnique Fédérale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland.2SIB Swiss Institute of\nBioinformatics, CH-1015 Lausanne, Switzerland.e-mail: umberto.lupo@epﬂ.ch; anne-ﬂorence.bitbol@epﬂ.ch\nNature Communications|         (2022) 13:6298 1\n1234567890():,;\n1234567890():,;\nfrequencies of amino acids observed in the columns of an MSA of\nhomologous proteins2,19–26. They capture the coevolution of contacting\namino acids, and provided state-of-the-art unsupervised predictions of\nstructural contacts before the advent of protein language models.\nNote that coevolutionary signal also aids supervised contact\nprediction\n27.\nWhile most protein language neural networks take individual\namino-acid sequences as inputs, some others have been trained to\nperform inference from MSAs of evolutionarily related sequences. This\nsecond class of networks includes MSA Transformer\n28 and the Evo-\nformer blocks in AlphaFold12, both of which interleave row (i.e. per-\nsequence) attention with column (i.e. per-site) attention. Such an\narchitecture is conceptually extremely attractive because it can\nincorporate coevolution in the framework of deep learning models\nusing attention. In the case of MSA Transformer, simple combinations\nof the model’s row attention heads have led to state-of-the-art unsu-\npervised structural contact prediction, outperforming both language\nmodels trained on individual sequences and Potts models\n28.B e y o n d\nstructure prediction, MSA Transformer is also able to predict muta-\ntional effects\n29,30 and to captureﬁtness landscapes31. In addition to\ncoevolutionary signal caused by structural and functional constraints,\nMSAs feature correlations that directly stem from the common\nancestry of homologous proteins, i.e. from phylogeny. Does MSA\nTransformer learn to identify phylogenetic relationships between\nsequences, which are a key aspect of the MSA data structure?\nHere, we show that simple, and universal, combinations of MSA\nTransformer’sc o l u m na t t e n t i o nh e a d s ,c o m p u t e do nag i v e nM S A ,\nstrongly correlate with the Hamming distances between sequences in\nthat MSA. This demonstrates that MSA Transformer encodes detailed\nphylogenetic relationships. Is MSA Transformer able to separate coe-\nvolutionary signals encoding functional and structural constraints\nfrom phylogenetic correlations arising from historical contingency?\nTo address this question, we generate controlled synthetic MSAs from\nPotts models trained on natural MSAs, either without or with phylo-\ngeny. For this, we perform Metropolis Monte Carlo sampling under the\nPotts Hamiltonians, either at equilibrium or along phylogenetic trees\ninferred from the natural MSAs. Using the top Potts model couplings as\nproxies for structural contacts, we demonstrate that unsupervised\ncontact prediction via MSA Transformer is substantially more resilient\nto phylogenetic noise than contact prediction using inferred Potts\nmodels.\nResults\nColumn attention heads capture Hamming distances in sepa-\nrate MSAs\nWe ﬁrst considered separately each of 15 different Pfam seed MSAs\n(see “Methods– Datasets”and Supplementary Table 1), corresponding\nto distinct protein families, and asked whether MSA Transformer has\nlearned to encode phylogenetic relationships between sequences in its\nattention layers. To test this, we split each MSA randomly into a\ntraining and a test set, and train a logistic model [Eqs. (5)a n d(6)] based\non the column-wise means of MSA Transformer’s column attention\nheads on all pairwise Hamming distances in the training set— see Fig.1\nfor a schematic, and“Methods – Supervised prediction of Hamming\ndistances” for details. Figure2 and Table1 show the results ofﬁtting\nthese specialized logistic models.\nFor all alignments considered, large regression coefﬁcients con-\ncentrate in early layers in the network, and single out some speciﬁc\nheads consistently across different MSAs— see Fig.2, ﬁrst and second\ncolumns, for results on four example MSAs. These logistic models\nreproduce the Hamming distances in the training set very well, and\nsuccessfully predict those in the test set— see Fig.2, third and fourth\ncolumns, for results on four example MSAs. Note that the block\nstructures visible in the Hamming distance matrices, and well repro-\nduced by our models, come from the phylogenetic ordering of\nsequences in our seed MSAs, see“Methods– Datasets”.Q u a n t i t a t i v e l y ,\nin all the MSAs studied, the coefﬁcients of determination (R\n2) com-\nputed on the test sets are above 0.84 in all our MSAs— see Table1.\nA striking result from our analysis is that the regression coefﬁ-\ncients appear to be similar across MSAs— see Fig.2, ﬁrst column. To\nquantify this, we computed the Pearson correlations between the\nregression coefﬁcients learnt on the larger seed MSAs. Figure 3\ndemonstrates that regression coefﬁcients are indeed highly correlated\nacross these MSAs.\nFig. 1 | MSA Transformer: column attentions and Hamming distances. aMSA\nTransformer is trained using the masked language modeling objective ofﬁlling in\nrandomly masked residue positions in MSAs. For each residue position in an input\nMSA, it assigns attention scores to all residue positions in the same row (sequence)\nand column (site) in the MSA. These computations are performed by 12 indepen-\ndent row/column attention heads in each of 12 successive layers of the network.\nb Our approach for Hamming distance matrix prediction from the column atten-\ntions computed by the trained MSA Transformer model, using a natural MSA as\ninput. For eachi =1 ,… , M, j =0 ,… , L and l =1 ,… ,1 2 ,t h ee m b e d d i n gv e c t o rx\nðlÞ\nij is the\ni-th row of the matrixXðlÞ\nj deﬁned in“Methods – MSA Transformer and column\nattention”, and the column attentions are computed according to Eqs. (2)a n d(3).\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 2\nMSA Transformer learns a universal representation of Hamming\ndistances\nGiven the substantial similarities between our models trained sepa-\nrately on different MSAs, we next asked whether a common model\nacross MSAs could capture Hamming distances within generic MSAs.\nTo address this question, we trained a single logistic model, based on\nthe column-wise means of MSA Transformer’s column attention heads,\non all pairwise distances within each of theﬁrst 12 of our seed MSAs.\nWe assessed its ability to predict Hamming distances in the remaining\n3 seed MSAs, which thus correspond to entirely different Pfam families\nfrom those in the training set. Figure4 shows the coefﬁcients of this\nregression (ﬁrst and second panels), as well as comparisons between\npredictions and ground truth values for the Hamming distances within\nthe three test MSAs (last three panels). We observe that large regres-\nsion coefﬁcients again concentrate in the early layers of the model, but\nsomewhat less than in individual models. Furthermore, the common\nmodel captures well the main features of the Hamming distance\nmatrices in test MSAs.\nIn Supplementary Table 2, we quantify the quality ofﬁtf o rt h i s\nmodel on all our MSAs. In all cases, weﬁnd very high Pearson corre-\nlation between the predicted distances and the ground truth Hamming\ndistances. Furthermore, the median value of theR\n2 coefﬁcient of\ndetermination is 0.6, conﬁrming the good quality ofﬁt. In the three\nFig. 2 | Fitting logistic models to predict Hamming distances separately in each\nMSA. The column-wise means of MSA Transformer’s column attention heads are\nused to predict normalised Hamming distances as probabilities in a logistic model.\nEach MSA is randomly split into a training set comprising 70% of its sequences and\na test set composed of the remaining sequences. For each MSA, a logistic model is\ntrained on all pairwise distances in the training set. Regression coefﬁcients are\nshown for each layer and attention head (ﬁrst column), as well as their absolute\nvalues averaged over heads for each layer (second column). For four example\nMSAs, ground truth Hamming distances are shown in the upper triangle (blue) and\npredicted Hamming distances in the lower triangle and diagonal (green), for the\ntraining and test sets (third and fourth columns). Darker shades correspond to\nlarger Hamming distances.\nTable 1 | Quality ofﬁt for logistic models trained to predict\nHamming distances separately in each MSA\nFamily R2\nPF00004 0.97\nPF00005 0.99\nPF00041 0.98\nPF00072 0.99\nPF00076 0.98\nPF00096 0.94\nPF00153 0.95\nPF00271 0.94\nPF00397 0.84\nPF00512 0.94\nPF00595 0.98\nPF01535 0.86\nPF02518 0.92\nPF07679 0.99\nPF13354 0.99\nR2 coefﬁcients of determination are shown for the predictions by eachﬁtted model on the\nassociated test set, see Fig.2.\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 3\nshortest and the two shallowest MSAs, the model performs below this\nmedian, while all MSAs for whichR2 is above median have depthM ≥ 52\nand lengthL ≥ 67. We also compute, for each MSA, the slope of the\nlinear ﬁt when regressing the ground truth Hamming distances on the\ndistances predicted by the model. MSA depth is highly correlated with\nthe value of this slope (Pearsonr ≈ 0.95). This bias may be explained by\nthe under-representation in the training set of Hamming distances and\nattention values from shallower MSAs, as their number is quadratic in\nMSA depth.\nRef. 28 showed that some column attention matrices, summed\nalong one of their dimensions, correlate with phylogenetic sequence\nweights (see “Methods – Supervised prediction of Hamming dis-\ntances”). This indicates that the model is, in part, attending to\nmaximally diverse sequences. Our study demonstrates that MSA\nTransformer actually learns pairwise phylogenetic relationships\nbetween sequences, beyond these aggregate phylogenetic sequence\nweights. It also suggests an additional mechanism by which the\nmodel may be attending to these relationships, focusing on similarity\ninstead of diversity. Indeed, while our regression coefﬁcients with\npositive sign in Fig.4 are associated with (average) attentions that are\npositively correlated with the Hamming distances, we alsoﬁnd sev-\neral coefﬁcients with large negative values. They indicate the exis-\ntence of important negative correlations: in those heads, the model is\nactually attending to pairs of similar sequences. Besides, comparing\nour Figs.2, 4 with Fig. 5 in ref.28 shows that different attention heads\nare important in our study versus in the analysis of ref.28 (Sec. 5.1).\nSpeciﬁcally, here weﬁnd that theﬁfth attention head in theﬁrst layer\nin the network is associated with the largest positive regression\ncoefﬁcient, while the sixth one was most important there. Moreover,\nstill focusing on theﬁrst layer of the network, the other most pro-\nminent heads here were not signiﬁcant there. MSA Transformer’s\nability to focus on similarity may also explain why its performance at\npredicting mutational effects can decrease signiﬁcantly when using\nMSAs which include a duplicate of the query sequence (see ref.29,\nFig. 3 | Pearson correlations between regression coefﬁcients in larger MSAs.\nSufﬁciently deep (≥ 100 sequences) and long (≥ 30 residues) MSAs are considered\n(mean/min/max Pearson correlations: 0.80/0.69/0.87).\nFig. 5 | Correlations from coevolution and from phylogeny in MSAs. aNatural\nselection on structure and function leads to correlations between residue positions\nin MSAs (coevolution).b Potts models, also known as DCA, aim to capture these\ncorrelations in their pairwise couplings.c Historical contingency can lead to cor-\nrelations even in the absence of structural or functional constraints.\nFig. 4 | Fitting a single logistic model to predict Hamming distances.Our col-\nlection of 15 MSAs is split into a training set comprising 12 of them and a test set\ncomposed of the remaining 3. A logistic regression is trained on all pairwise dis-\ntances within each MSA in the training set. Regression coefﬁcients (ﬁrst panel) and\ntheir absolute values averaged over heads for each layer (second panel) are shown\nas in Fig.2. For the three test MSAs, ground truth Hamming distances are shown in\nthe upper triangle (blue) and predicted Hamming distances in the lower triangle\nand diagonal (green), also as in Fig.2 (last three panels). We further report theR2\ncoefﬁcients of determination for the regressions on these test MSAs— see also\nSupplementary Table 2.\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 4\nSupplementary Fig. 9 and Table 10): in these cases, the model pre-\ndicts masked tokens with very high conﬁdence using information\nfrom the duplicate sequence.\nHow much does the ability of MSA Transformer to capture\nphylogenetic relationships arise from its training? To address this\nquestion, we trained a common logistic model as above to predict\nHamming distances, but using column attention values computed\nfrom a randomly re-initialized version of the MSA Transformer net-\nwork. We used the same protocol as in MSA Transformer’s original\npre-training to randomly initialize the entries of the network’sr o w -\nand column-attention weight matricesW\nðl,hÞ\nQ , Wðl,hÞ\nK and Wðl,hÞ\nV (see\n“Methods – MSA Transformer and column attention”), as well as\nthe entries of the matrix used to embed input tokens, the weights in\nthe feed-forward layers, and the positional encodings. Speciﬁcally,\nwe sampled these entries (with the exception of bias terms and of the\nembedding vector for the padding token, which were set to zero)\nfrom a Gaussian distribution with mean 0 and standard deviation\n0.02. The results obtained in this case for our regression task are\nreported in Supplementary Table 3. They demonstrate that,\nalthough random initialization can yield better performance than\nrandom guessing (which may partly be explained by Gordon ’s\nTheorem\n32), the trained MSA Transformer gives vastly superior\nresults. This conﬁrms that the masked language modeling pre-\ntraining has driven it towards precisely encoding distances between\nsequences.\nFor each layer and attention head in the network, MSA Transfor-\nmer computes one matrix of column attention values per site— see Eq.\n(4). This is in contrast with row attention, which is tied (see“Methods–\nMSA Transformer and column attention”). Our results are more sur-\nprising that they would be if the model’sc o l u m na t t e n t i o n sw e r ea l s o\ntied. Indeed, during pre-training, by tuning its row-attention weight\nmatrices to achieve optimal tied attention, MSA Transformer discovers\ncovariance between MSA sites in early layers, and covariance between\nMSA sequences is related to Hamming distance.\nFinally, to explore the contribution of each column to perfor-\nmance in our regression task, we employed our common logistic\nmodel (trained on the means of column attention matrices) to predict\nHamming distances using column attentions from individual sites. We\nﬁnd that the most highly conserved sites (corresponding to columns\nwith low entropy) lead to predictions whose errors have among the\nsmallest standard deviations— see Supplementary Table 4. Note that\nwe focused on standard deviations to mitigate the biases of the com-\nmon logistic model (see above). This indicates that highly conserved\nsites lead to more stable predictions.\nMSA Transformer efﬁciently disentangles correlations from\ncontacts and phylogeny\nMSA Transformer is known to capture three-dimensional contacts\nthrough its (tied) row attention heads28, and we have shown that it also\ncaptures Hamming distances, and thus phylogeny, through its column\nattention heads. Correlations observed between the columns of an\nMSA can arise both from coevolution due to functional constraints and\nfrom phylogeny (see Fig.5). How efﬁciently does MSA Transformer\ndisentangle correlations from contacts and phylogeny? We address\nthis question in the concrete case of structure prediction. Because\ncorrelations from contacts and phylogeny are always both present in\nnatural data, we constructed controlled synthetic data by sampling\nfrom Potts models (Fig.5b), either independently at equilibrium, or\nalong a phylogenetic tree inferred from the natural MSA using\nFastTree\n33. The Potts models we used were trained on each of 15 full\nnatural MSAs (see“Methods – Datasets” and Supplementary Table 1)\nusing the generative method bmDCA26,34— see “Methods – Synthetic\nMSA generation via Potts model sampling along inferred phylogenies”.\nThis setup allows us to compare data where all correlations come from\ncouplings (pure Potts model) to data that comprises phylogenetic\ncorrelations on top of these couplings. For simplicity, let us call\n“contacts” the top scoring pairs of amino-acid sites according to the\nbmDCA models used to generate our MSAs, and refer to the task of\ninferring these top scoring pairs as“contact prediction”.\nContact maps inferred by plmDCA\n24,25 and by MSA Transformer\nfor our synthetic datasets are shown in Supplementary Fig. 4. For\ndatasets generated with phylogeny, more false positives, scattered\nacross the whole contact maps, appear in the inference by plmDCA\nthan in that by MSA Transformer. This is shown quantitatively in\nTable 2, which reports the area under the receiver operating char-\nacteristic curve (ROC-AUC) for contact prediction for two different\ncutoffs on the number of contacts. We also quantify the degradation\nin performance caused by phylogeny by computing the relative drop\nΔ in ROC-AUC due to the injection of phylogeny in our generative\nprocess, for each Pfam family and for both plmDCA and MSA\nTransformer. On average,Δ is twice or three times (depending on the\ncutoff) higher for plmDCA than for MSA Transformer. We checked\nthat these outcomes are robust to changes in the strategy used to\ncompute plmDCA scores. In particular, the averageΔ for plmDCA\nbecomes even larger when we average scores coming from inde-\npendent models ﬁtted on the 10 subsampled MSAs used for MSA\nTransformer— thus using the exact same method as for predicting\ncontacts with MSA Transformer (see “Methods – Generating\nsequences along an inferred phylogeny under a Potts model”). The\nconclusion is the same if 10 (or 6, for Pfam family PF13354) twice-\ndeeper subsampled MSAs are employed.\nThese results demonstrate that contact inference by MSA Trans-\nformer is less deteriorated by phylogenetic correlations than contact\ninference by DCA. This resilience might explain the remarkable result\nthat structural contacts are predicted more accurately by MSA\nTransformer than by Potts models even when MSA Transformer’s pre-\ntraining dataset minimizes diversity (see ref.28,S e c .5 . 1 ) .\nTable 2 also shows that plmDCA performs better than MSA\nTransformer on the synthetic MSAs generated without phylogeny.\nBecause these sequences are sampled independently and at equili-\nbrium from Potts models inferred from the natural MSAs, they are by\ndeﬁnition well-described by Potts models. However, these sequences\nincorporate the imperfections of the inferred Potts models (see the\ninferred contact maps versus the experimental ones in Supplemen-\ntary Fig. 2), in addition to lacking the phylogenetic relationships that\nexist in natural MSAs. These differences with the natural MSAs that\nwere used to train MSA Transformer might explain why it performs\nless well than plmDCA on these synthetic MSAs, while the opposite\nholds for natural MSAs (see ref.28 and Supplementary Figs. 2 and 3).\nNote that directly comparing the performance of inference between\nnatural and synthetic data is difﬁcult because the ground-truth\ncontacts are not the same and because synthetic data relies on\ninferred Potts models and inferred phylogenetic trees with their\nimperfections. However, this does not impair our comparisons of the\nsynthetic datasets generated without and with phylogeny, or of\nplmDCA and MSA Transformer on the same datasets. Furthermore,\nan interesting feature that can be observed in Supplementary Fig. 4,\nand is quantiﬁed in Supplementary Table 5, is that MSA Transformer\ntends to recover the experimental contact maps from our synthetic\ndata generated by bmDCA. Speciﬁcally, some secondary structure\nfeatures that were partially lost in the bmDCA inference and gen-\neration process (see the experimental contact maps in Supplemen-\ntary Fig. 2) become better deﬁned again upon contact inference by\nMSA Transformer. This could be because MSA Transformer has\nlearnt the structure of contact maps, including the spatial com-\npactness and shapes of secondary structures.\nDiscussion\nMSA Transformer is known to capture structural contacts through its\n(tied) row attention heads28. Here, we showed that it also captures\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 5\nHamming distances, and thus phylogenetic information, through its\ncolumn attention heads. This separation of the two signals in the\nrepresentation of MSAs built by MSA Transformer comes directly from\nits architecture with interleaved row and column attention heads. It\nmakes sense, given that some correlations between columns (i.e.\namino-acid sites) of an MSA are associated to contacts between sites,\nwhile similarities between rows (i.e. sequences) arise from relatedness\nbetween sequences\n15.S p e c iﬁcally, we found that simple combinations\nof column attention heads, tuned to individual MSAs, can predict\npairwise Hamming distances between held-out sequences with very\nhigh accuracy. The larger coefﬁcients in these combinations are found\nin early layers in the network. More generally, this study demonstrated\nthat the regressions trained on different MSAs had major similarities.\nThis motivated us to train a single model across a heterogeneous\ncollection of MSAs, and this general model was still found to accurately\npredict pairwise distances in test MSAs from entirely distinct\nPfam families. This result hints at a universal representation of phy-\nlogenetic relationships in MSA Transformer. Furthermore, our results\nsuggest that the network has learned to quantify phylogenetic relat-\nedness by attending not only to dissimilarity\n28, but also to similarity\nrelationships.\nNext, to test the ability of MSA Transformer to disentangle phy-\nlogenetic correlations from functional and structural ones, we focused\non unsupervised contact prediction tasks. Using controlled synthetic\ndata, we showed that unsupervised contact prediction is more robust\nto phylogeny when performed by MSA Transformer than by inferred\nPotts models.\nLanguage models often capture important properties of the\ntraining data in their internal representations\n35. For instance, those\ntrained on single protein sequences learn structure and binding sites36,\nand those trained on chemical reactions learn how atoms rearrange37.\nOur ﬁnding that detailed phylogenetic relationships between\nsequences are learnt by MSA Transformer, in addition to structural\ncontacts, and in an orthogonal way, demonstrates how precisely this\nmodel represents the MSA data structure. We note that, without lan-\nguage models, analyzing the correlations in MSAs can reveal\nevolutionary relatedness and sub-families\n15,a sw e l la sc o l l e c t i v em o d e s\nof correlation, some of which are phylogenetic and some functional18.\nFurthermore, Potts models capture the clustered organization of\nprotein families in sequence space\n26, and the latent space of variational\nautoencoder models trained on sequences38–40 qualitatively captures\nphylogeny39. Here, we demonstrated the stronger result that detailed\npairwise phylogenetic relationships between sequences are quantita-\ntively learnt by MSA Transformer.\nSeparating coevolutionary signals encoding functional and\nstructural constraints from phylogenetic correlations arising from\nhistorical contingency constitutes a key problem in analyzing the\nsequence-to-function mapping in proteins\n15,18. Phylogenetic correla-\ntions are known to obscure the identiﬁcation of structural contacts by\ntraditional coevolution methods, in particular by inferred Potts\nmodels\n20,21,41–44, motivating various corrections17,21,22,24,45–48.F r o ma\ntheoretical point of view, disentangling these two types of signals is a\nfundamentally hard problem\n49. In this context, the fact that protein\nlanguage models such as MSA Transformer learn both signals in\northogonal representations, and separate them better than Potts\nmodel, is remarkable.\nHere, we have focused on Hamming distances as a simple measure\nof phylogenetic relatedness between sequences. It would be very\ninteresting to extend our study to other, more detailed, measures of\nphylogeny. One may ask whether they are encoded in deeper layers in\nthe network than those most involved in our study. Besides, we have\nmainly considered attentions averaged over columns, but exploring in\nmore detail the role of individual columns would be valuable, espe-\ncially given the impact we found for column entropies. More generally,\nour results suggest that the performance of protein language models\ntrained on MSAs could be assessed by evaluating not only how well\nthey capture structural contacts, but also how well they capture phy-\nlogenetic relationships. In addition, the ability of protein language\nmodels to learn phylogeny could make them particularly well-suited at\ngenerating synthetic MSAs capturing the data distribution of natural\nones\n50. It also raises the question of their possible usefulness to infer\nphylogenies and evolutionary histories.\nTable 2 | Impact of phylogeny on contact prediction by plmDCA and MSA Transformer\nROC-AUC forN contacts ROC-AUC for 2 L contacts\nplmDCA MSA Trans. plmDCA MSA Trans.\nPfam ID Eq. Tree Δ Eq. Tree Δ Eq. Tree Δ Eq. Tree Δ\nPF00004 0.87 0.58 0.33 0.70 0.67 0.04 0.93 0.61 0.34 0.80 0.71 0.11\nPF00005 0.93 0.67 0.28 0.79 0.76 0.03 0.96 0.74 0.23 0.81 0.82 −0.01\nPF00041 0.86 0.64 0.25 0.69 0.62 0.10 0.94 0.73 0.22 0.87 0.79 0.09\nPF00072 0.94 0.73 0.23 0.86 0.77 0.10 0.99 0.85 0.14 0.94 0.87 0.08\nPF00076 0.92 0.69 0.25 0.81 0.76 0.05 0.97 0.72 0.25 0.88 0.83 0.05\nPF00096 0.88 0.54 0.39 0.68 0.54 0.21 0.92 0.54 0.41 0.78 0.54 0.30\nPF00153 0.95 0.71 0.26 0.83 0.63 0.24 0.98 0.77 0.21 0.90 0.65 0.28\nPF00271 0.91 0.62 0.32 0.78 0.72 0.07 0.95 0.67 0.29 0.85 0.77 0.10\nPF00397 0.85 0.58 0.33 0.69 0.58 0.15 0.93 0.61 0.34 0.76 0.59 0.22\nPF00512 0.94 0.74 0.21 0.84 0.77 0.08 0.97 0.78 0.20 0.88 0.81 0.08\nPF00595 0.91 0.61 0.33 0.72 0.62 0.14 0.96 0.64 0.33 0.83 0.68 0.18\nPF01535 0.85 0.66 0.23 0.66 0.63 0.05 0.88 0.72 0.18 0.73 0.72 0.01\nPF02518 0.93 0.69 0.27 0.82 0.75 0.09 0.98 0.78 0.20 0.90 0.79 0.12\nPF07679 0.85 0.63 0.26 0.68 0.64 0.05 0.95 0.77 0.19 0.85 0.80 0.05\nPF13354 0.68 0.56 0.18 0.76 0.65 0.14 0.82 0.65 0.21 0.91 0.74 0.19\nAverage 0.88 0.64 0.27 0.75 0.68 0.10 0.94 0.71 0.25 0.85 0.74 0.12\nWe consider synthetic MSAs generated by sampling Potts models either at equilibrium (Eq.) or along inferred phylogenies (Tree). We report the ROC-AUCs for contact prediction, computed by\ncomparing couplings inferred from our synthetic MSAs using plmDCA and MSA Transformer, with ground-truth proxy contacts consisting of either theN or the 2L pairs with top coupling scores\naccording to the Potts models that generated the data (see“Methods – Synthetic MSA generation via Potts model sampling along inferred phylogenies”). Here,N denotes the number of pairs of\nresidues that have an all-atom minimal distance smaller than 8Åin the experimental structure in Supplementary Table 1, excluding pairs at positionsi, j with∣i − j ∣ ≤ 4 (in all cases,N >2 L). To assess the\nimpact of phylogenetic noise, we computeΔ ≔ (Aeq − Atree)/ Aeq,w h e r eAeq is the ROC-AUC obtained from the equilibrium MSA andAtree is the ROC-AUC obtained from the MSA with phylogeny.\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 6\nMethods\nDatasets\nThe Pfam database51 contains a large collection of related protein\nregions (families), typically associated to functional units called\ndomains that can be found in multiple protein contexts. For each of its\nfamilies, Pfam provides an expert-curated seed alignment that con-\ntains a representative set of sequences. In addition, Pfam provides\ndeeper “full” alignments, that are automatically built by searching\nagainst a large sequence database using a proﬁle hidden Markov\nmodel (HMM) built from the seed alignments.\nFor this work, we considered 15 Pfam families, and for each we\nconstructed (or retrieved, see below) one MSA from its seed alignment\n— henceforth referred to as the“seed MSA”—and one from its full\nalignment— henceforth referred to as the full MSA. The seed MSAs\nwere created byﬁrst aligning Pfam seed alignments (Pfam version 35.0,\nNov. 2021) to their HMMs using thehmmalign command from the\nHMMER suite (http://hmmer.org, version 3.3.2), and then removing\ncolumns containing only insertions or gaps. We retained the original\nPfam tree ordering, with sequences ordered according to phylogeny\ninferred by FastTree\n33. In the case of family PF02518, out of the initial\n658 sequences, we kept only theﬁrst 500 in order to limit the memory\nrequirements of our computational experiments to less than 64 GB. Of\nthe full MSAs, six (PF00153, PF00397, PF00512, PF01535, PF13354)\nwere created from Pfam full alignments (Pfam version 34.0, Mar. 2021),\nremoving columns containing only insertions or gaps, andﬁnally\nremoving sequences where 10% or more characters were gaps. The\nremaining nine full MSAs were retrieved from the online repository\nhttps://github.com/matteoﬁgliuzzi/bmDCA (publication date: Dec.\n2017) and were previously considered in ref.26.T h e s ea l i g n m e n t s\nwere constructed from full Pfam alignments from an earlier release\nof Pfam.\nAn MSA is a matrixM with L columns, representing the different\namino-acid sites, andM rows. Each rowi, denoted byx\n(i), represents\none sequence of the alignment. We will refer toL as the MSA length,\nand to M as its depth. For all but one (PF13354) of our full MSAs,\nM > 36000. Despite their depth, however, our full MSAs include some\nhighly similar sequences due to phylogenetic relatedness, a usual\nfeature of large alignments of homologous proteins. We computed the\neffective depth20 of each MSAM as\nMðδÞ\neff : =\nXM\ni =1\nwi,w i t hwi : = ∣fi0 : dHðxðiÞ,xði0 ÞÞ < δg∣/C0 1, ð1Þ\nwhere dH(x, y) is the (normalized) Hamming distance between two\nsequences x and y, i.e. the fraction of sites where the amino acids\ndiffer, and we setδ =0 . 2 .W h i l eMð0:2Þ\neff =M can be as low as 0.06 for our\nfull MSAs, this ratio is close to 1 for all seed MSAs: it is almost 0.83 for\nPF00004, and larger than 0.97 for all other families.\nFinally, for each Pfam domain considered, we retrieved one\nexperimental three-dimensional protein structure, corresponding to a\nsequence present in the full MSA, from the PDB (https://www.rcsb.\norg). All these structures were obtained by X-ray crystallography and\nhave R-free values between 0.13 and 0.29. Information about our MSAs\nis summarized in Supplementary Table 1.\nAll these families have been previously considered in the literature\nand shown to contain coevolutionary signal detectable by DCA\nmethods\n26, making our experiments on contact prediction readily\ncomparable with previous results. While the precise choice of Pfam\nfamilies is likely immaterial for our investigation of the column\nattention heads computed by MSA Transformer, our domains’short\nlengths are convenient in view of MSA Transformer’s large memory\nfootprint— which isO(LM\n2)+ O(L2).\nMSA Transformer and column attention\nWe used the pre-trained MSA Transformer model introduced in ref.28,\nretrieved from the Python Package Index asfair-esm 0.4.0.W e\nbrieﬂy recall that this model was trained, with a variant of the masked\nlanguage modeling (MLM) objective52,o n2 6m i l l i o nM S A sc o n s t r u c t e d\nfrom UniRef50 clusters (March 2018 release), and contains 100 million\ntrained parameters. The input to the model is an MSA withL columns\nand M rows. First, the model pre-pends a special beginning-of-\nsentence token to each row in the input MSA (this is common in lan-\nguage models inspired by the BERT architecture\n52). Then, each residue\n(or token) is embedded independently, via a learned mapping from the\nset of possible amino-acid/gap symbols intoR\nd (d =7 6 8 ) . T o t h e s e\nobtained embeddings, the model adds two kinds of learned6 scalar\npositional encodings53, designed to allow the model to distinguish\nbetween (a) different aligned positions (columns), and (b) between\ndifferent sequence positions (rows). (Note that removing the latter\nkind was shown in ref.28 to have only limited impact.) The resulting\ncollection ofM ×( L +1 )d-dimensional vectors, viewed as anM ×( L +\n1) ×d array, is then processed by a neural architecture consisting of 12\nlayers. Each layer is a variant of the axial attention54 architecture,\nconsisting of a multi-headed (12 heads) tied row attention block, fol-\nlowed by a multi-headed (12 heads) column attention block, andﬁnally\nby a feed-forward network. (Note that both attention blocks, and the\nfeed-forward network, are in fact preceded by layer normalization\n55).\nThe roles of row and column attention in the context of the MLM\ntraining objective are illustrated in Fig.1a. Tied row attention incor-\nporates the expectation that 3D structure should be conserved\namongst sequences in an MSA; we refer the reader to ref.28 for\ntechnical details. Column attention works as follows: letX\nðlÞ\nj be the\nM × d matrix corresponding to columnj in the M ×( L +1 )×d array\noutput by the row attention block in layerl with l =1 ,… , 12. At each\nlayer l and each headh =1 ,… , 12, the model learns threed × d matrices\nWðl,hÞ\nQ , Wðl,hÞ\nK and Wðl,hÞ\nV (note that these matrices,mutatis mutandis,\ncould be of dimensiond × d0 with d0≠d), used to obtain threeM × d\nmatrices\nQðl,hÞ\nj = XðlÞ\nj Wðl,hÞ\nQ , Kðl,hÞ\nj = XðlÞ\nj Wðl,hÞ\nK , Vðl,hÞ\nj = XðlÞ\nj Wðl,hÞ\nV , ð2Þ\nwhose rows are referred to as“query”, “key”,a n d“value” vectors\nrespectively. The column attention from MSA columnj ∈ {0, … , L}\n(where j = 0 corresponds to the beginning-of-sentence token), at layer\nl,a n df r o mh e a dh,i st h e nt h eM × M matrix\nAðl,hÞ\nj : =s o f t m a xrow\nQðl,hÞ\nj Kðl,hÞ\nj\nT\nﬃﬃﬃ\nd\np\n0\n@\n1\nA, ð3Þ\nwhere we denote by softmax row the application of\nsoftmaxðξ1, ... ξdÞ = ðeξ1 , ... ,eξd Þ= Pd\nk =1 eξk to each row of a matrix\nindependently, and by (⋅)T matrix transposition. As in the standard\nTransformer architecture4, these attention matrices are then used to\ncompute M × d matrices Zðl,hÞ\nj = Aðl,hÞ\nj Vðl,hÞ\nj , one for each MSA columnj\nand headh. Projecting the concatenationZðl,1Þ\nj ∣ /C1/C1/C1 ∣Zðl,12Þ\nj , a singleM × d\nmatrix ZðlÞ\nj is ﬁnally obtained at layerl.T h ec o l l e c t i o nðZðlÞ\nj Þj =1 ,...,L,\nthought of as anM ×( L +1 )×d array, is then passed along to the feed-\nforward layer.\nSupervised prediction of Hamming distances\nRow i of the column attention matricesAðl,hÞ\nj in Eq. (3) consists ofM\npositive weights summing to one— one weight per row indexi0 in the\noriginal MSA. According to the usual interpretation of the attention\nmechanism\n3,4, the role of these weights may be described as follows:\nWhen constructing a new internal representation (at layerl)f o rt h e\nrow-i,c o l u m n -j residue position, the network distributes its focus,\naccording to these weights, among theM available representation\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 7\nvectors associated with each MSA row-i0,c o l u m n -j residue position\n(includingi0 = i). Since row attention precedes column attention in the\nMSA Transformer architecture, we remark that, even at theﬁrst layer,\nthe row-i0, column-j representation vectors that are processed by that\nlayer’s column attention block can encode information about the\nentire rowi0 in the MSA.\nIn ref.28 (Sec. 5.1), it was shown that, for some layersl and headsh,\naveraging theM × M column attention matricesAðl,hÞ\nj in Equation (3)\nfrom all MSA columnsj, and then averaging the result along theﬁrst\ndimension, yieldsM-dimensional vectors whose entries correlate rea-\nsonably well with the phylogenetic sequence weightswi deﬁned in\nEquation (1). Larger weights are, by deﬁnition, associated with less\nredundant sequences, and MSA diversity is known to be important for\ncoevolution-based methods— particularly in structure prediction tasks.\nThus, these correlations can be interpreted as suggesting that the\nmodel is, in part, explicitly attending to a maximally diverse set of\nsequences.\nBeyond this, we hypothesize that MSA Transformer may have\nlearned to quantify and exploit phylogenetic correlations in order to\noptimize its performance in the MLM training objective ofﬁlling in\nrandomly masked residue positions. To investigate this, we set up\nregression tasks in which, to predict the Hamming distancey between\nthe i-th and thei\n0-th sequence in an MSAM of lengthL,w eu s e dt h e\nentriesaðl,hÞ\ni,i0 at positionði, i0Þ (hencefortha(l, h) for brevity) from the 144\nmatrices\nAðl,hÞ : = 1\n2ðL +1 Þ\nXL\nj =0\nAðl,hÞ\nj + Aðl,hÞ\nj\nT\n/C18/C19\n,w i t h1≤ l ≤ 12 and 1≤ h ≤ 12: ð4Þ\nThese matrices are obtained by averaging, across all columns\nj =0 ,… , L, the symmetrised column attention mapsAðl,hÞ\nj computed by\nMSA Transformer, when takingM as input. We highlight that column\nj = 0, corresponding to the beginning-of-sentence token, is included in\nthe average deﬁning A(l, h).\nWe ﬁt fractional logit models via quasi-maximum likelihood\nestimation56 using the statsmodels package (version 0.13.2)57.\nNamely, we model the relationship between the Hamming distancey\nand the aforementioned symmetrised, and averaged, attention values\na =( a\n(1, 1), … , a(12, 12)), as\nE½y ∣ a/C138 = Gβ0,βðaÞ,w i t hGβ0,βðaÞ : = σβ 0 + aβT\n/C16/C17\n, ð5Þ\nwhere E½/C1 ∣ /C1/C138 denotes conditional expectation,σðxÞ = ð1+ e/C0 xÞ/C0 1 is the\nstandard logistic function, and the coefﬁcients β0 and β =( β1, … , β144)\nare determined by maximising the sum of Bernoulli log-likelihoods\n‘ðβ0, β ∣ a,yÞ = y log½Gβ0, βðaÞ/C138 + ð1 /C0 yÞ log½1 /C0 Gβ0, βðaÞ/C138 , ð6Þ\nevaluated over a training set of observations ofy and a. Note that this\nsetup is similar to logistic regression, but allows for the dependent\nvariable to take real values between 0 and 1 (it can be equivalently\ndescribed as a generalized linear model with binomial family and logit\nlink). For simplicity, we refer to these fractional logit models simply as\n“logistic models”. Our general approach to predict Hamming distances\nis illustrated in Fig.1b.\nUsing data from our seed MSAs (cf. Supplementary Table 1), we\nperformed two types of regression tasks. In theﬁrst one, we randomly\npartitioned the set of row indices in each separate MSAM into two\nsubsets I\nM,train and IM,test,w i t hIM,train containing 70% of the indices.\nWe then trained and evaluated one model for eachM, using as training\ndata the Hamming distances, and column attentions, coming from\n(unordered) pairs of indices inI\nM,train, and as test data the Hamming\ndistances, and column attentions, coming from pairs of indices in\nIM, test. The second type of regression task was a single modelﬁto v e ra\ntraining dataset consisting of all pairwise Hamming distances, and\ncolumn attentions, from theﬁrst 12 of our 15 MSAs. We then evaluated\nthis second model over a test set constructed in an analogous way\nfrom the remaining 3 MSAs.\nSynthetic MSA generation via Potts model sampling along\ninferred phylogenies\nTo assess the performance of MSA Transformer at disentangling sig-\nnals encoding functional and structural (i.e.ﬁtness) constraints from\nphylogenetic correlations arising from historical contingency, we\ngenerated and studied controlled synthetic data. Indeed, disen-\ntangling ﬁtness landscapes from phylogenetic history in natural data\nposes a fundamental challenge\n49— see Fig.5 for a schematic illustration.\nThis makes it very difﬁcult to assess the performance of a method at\nthis task directly on natural data, because gold standards where the\ntwo signals are well-separated are lacking. We resolved this con-\nundrum by generating synthetic MSAs according to well-deﬁned\ndynamics such that the presence of phylogeny can be controlled.\nFirst, we inferred unrooted phylogenetic trees from our full MSAs\n(see “Methods– Datasets”), using FastTree version 2.1\n33 with its default\nsettings. Our use of FastTree is motivated by the depth of the full\nMSAs, which makes it computationally prohibitive to employ more\nprecise inference methods. Deep MSAs are needed for the analysis\ndescribed below, since it relies on accuratelyﬁtting Potts models.\nThen, we ﬁtted Potts models on each of these MSAs using\nbmDCA\n26 (https://github.com/ranganathanlab/bmDCA, version 0.8.12)\nwith its default hyperparameters. These include, in particular, reg-\nularization strengths for the Potts modelﬁelds and couplings, both set\nat λ =1 0−2. With the exception of family PF13354, we trained all models\nfor 2000 iterations and stored theﬁelds and couplings at the last\niteration; in the case of PF13354, we terminated training after 1480\niterations. In all cases, we veriﬁed that, during training, the model’s loss\nhad converged. The choice of bmDCA is motivated by the fact that, as\nhas been shown in refs.26, 34,m o d e lﬁtting on natural MSAs using\nBoltzmann machine learning yields Potts models with good generative\npower. This sets it apart from other DCA inference methods, especially\npseudo-likelihood DCA (plmDCA)\n24,25, which is the DCA standard for\ncontact prediction, but cannot faithfully reproduce empirical one- and\ntwo-body marginals, making it a poor choice of a generative model26.\nUsing the phylogenetic trees and Potts models inferred from each\nfull MSA, we generated synthetic MSAs without or with phylogeny, as\nwe now explain. In the remainder of this subsection, letM denote an\narbitrary MSA from our set of full MSAs,L its length, andM its depth.\nConsider a sequence ofL amino-acid sites. We denote byx\ni ∈ {1,\n… , q} the state of sitei ∈ {1, … , L}, whereq = 21 is the number of pos-\nsible states, namely the 20 natural amino acids and the alignment gap.\nA general Potts model Hamiltonian applied to a sequencex =( x\n1, … , xL)\nreads\nHðxÞ = /C0\nXL\ni =1\nhiðxiÞ/C0\nXL\nj =1\nXj/C0 1\ni =1\neijðxi,xjÞ, ð7Þ\nwhere theﬁelds hi(xi) and couplingseij(xi, xj) are parameters that can\nbe inferred from data by DCA methods2,20. In our case, they are inferred\nfrom M by bmDCA26,34. The Potts model probability distribution is\nthen given by the Boltzmann distribution associated to the Hamilto-\nnian H in Equation (7):\nPðxÞ =\ne/C0 HðxÞ\nZ , ð8Þ\nwhere Z is a constant ensuring normalization. In this context, we\nimplement a Metropolis–Hastings algorithm for Markov Chain Monte\nCarlo (MCMC) sampling fromP, where an iteration step consists of a\nproposed move (mutation) in which a sitei is chosen uniformly at\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 8\nrandom, and its statexi may be changed into another state chosen\nuniformly at random. Each of these attempted mutations is accepted\nor rejected according to the Metropolis criterion, i.e. with probability\np = min 1, exp/C0 ΔHðÞ½/C138 , ð9Þ\nwhere ΔH is the difference in the value ofH after and before the\nmutation.\nGenerating independent equilibrium sequences under a Potts\nmodel. To generate a synthetic MSAs without phylogeny from each\nM, we performed equilibrium MCMC sampling from the Potts model\nwith HamiltonianH in Eq. (7), using the Metropolis–Hastings algo-\nrithm. Namely, we started from a set ofM randomly and independently\ninitialized sequences, and proposed a total numberN of mutations on\neach sequence. Suitable values forN are estimated by bmDCA during\nits training, to ensure that Metropolis–Hastings sampling reaches\nthermal equilibrium afterN steps when starting from a randomly\ninitialized sequence\n26. We thus used the value ofN estimated by\nbmDCA at the end of training. This yielded a synthetic MSA of the same\ndepth M as the original full MSAM, composed of independent equi-\nlibrium sequences.\nGenerating sequences along an inferred phylogeny under a Potts\nmodel. We also generated synthetic data using MCMC sampling along\nour inferred phylogenetic trees42, using an open-source implementa-\ntion available at https://github.com/Bitbol-Lab/Phylogeny-\nPartners (version 2.0). We started from an equilibrium ancestor\nsequence sampled as explained above, and placed it at the root (note\nthat, while FastTree roots its trees arbitrarily, root placement does not\nmatter; see below). Then, this sequence was evolved by successive\nduplication (at each branching of the tree) and mutation events (along\neach branch). Mutations were again modeled using for acceptance the\nMetropolis criterion in Eq. (9) with the Hamiltonian in Eq. (7). As the\nlength b of a branch gives the estimated number of substitutions that\noccurred per site along it\n33, we generate data by making a number of\naccepted mutations on this branch equal to the integer closest tobL.\nS i n c ew et r a v e r s e dt h ee n t i r ei n f e r r e dt r e ei nt h i sm a n n e r ,t h er e s u l t i n g\nsequences at the leaves of the tree yield a synthetic MSA of the same\ndepth as the original full MSAM.F i n a l l y ,w ev e r iﬁed that the Hamming\ndistances between sequences in these synthetic MSAs were reasonably\ncorrelated with those between corresponding sequences in the natural\nMSAs— see Supplementary Fig. 1.\nBecause we start from an ancestral equilibrium sequence, and\nthen employ the Metropolis criterion, all sequences in the phylogeny\nare equilibrium sequences. Thus, some of the correlations between the\nsequences at the leaves of the tree can be ascribed to the couplings in\nthe Potts model, as in the case of independent equilibrium sequences\ndescribed above. However, their relatedness adds extra correlations,\narising from the historical contingency in their phylogeny. Note that\nseparating these ingredients is extremely tricky in natural data\n49,w h i c h\nmotivates our study of synthetic data.\nOur procedure for generating MSAs along a phylogeny is inde-\npendent of the placement of the tree’sr o o t .I n d e e d ,i n f o r m a l l y ,at r e e’s\nroot placement determines the direction of evolution; hence, root\nplacement should not matter when evolution is a time-reversible\nprocess. That evolution via our mutations and duplications is a time-\nreversible process is a consequence of the fact that we begin with\nequilibrium sequences at the (arbitrarily chosen) root. More formally,\nfor an irreducible Markov chain with transition matrixP and state\nspace Ω,a n df o ra n yn ≥ 1, let Markov\nnðπ,PÞ denote the probability\nspace of chainsðXkÞ0 ≤ k ≤ n with initial distributionπ on Ω.I fπ is the\nchain’s stationary distribution andπ satisﬁes detailed balance, then,\nfor any number of stepsn ≥ 1, any chainðXkÞ0 ≤ k ≤ n 2 Markovnðπ, PÞ is\nreversible in the sense thatðXn/C0 kÞ0 ≤ k ≤ n 2 Markovnðπ, PÞ. In our case,\nsince the Metropolis–Hastings algorithm constructs an irreducible\nMarkov chain whose stationary distribution satisﬁes detailed balance,\nand since duplication events are also time-reversible constraints\nimposed at each branching node, all ensemble observables are inde-\npendent of root placement as long as the root sequences are sampled\nfrom the stationary distribution.\nAssessing performance degradation due to phylogeny in coupling\ninference. DCA methods and MSA Transformer both offer ways to\nperform unsupervised inference of structural contacts from MSAs of\nnatural proteins. In the case of DCA, the established methodology\n24–26 is\nto (1) learnﬁelds and couplings [see Eq. (7)] byﬁtting the Potts model,\n(2) change the gauge to the zero-sum gauge, (3) compute the Frobenius\nnorms, for all pairs of sites (i, j), of the coupling matricesðe\nijðx, yÞÞx,y,\nand ﬁnally (4) apply the average product correction (APC)17, yielding a\ncoupling scoreEij. Top scoring pairs of sites are then predicted as being\ncontacts. In the case of MSA Transformer28, a single logistic regression\n(shared across all possible input MSAs) was trained to regress contact\nmaps from a sparse linear combination of the symmetrized and APC-\ncorrected row attention heads (see“Methods – MSA Transformer and\ncolumn attention”).\nWe applied these inference techniques, normally used to predict\nstructural contacts, on our synthetic MSAs generated without and with\nphylogeny (see above). As proxies for structural contacts, we used the\npairs of sites with top coupling scores in the Potts models used to\ngenerate the MSAs. Indeed, when presented with our synthetic MSAs\ngenerated at equilibrium, DCA methods forﬁtting Potts models should\nrecover the ranks of these coupling scores well. Hence, their perfor-\nmance in this task provide a meaningful baseline against which per-\nformance when a phylogeny was used to generate the data, as well as\nMSA Transformer’s performance, can be measured.\nAs a DCA method to infer these coupling scores, we used\nplmDCA\n24,25 as implemented in thePlmDCA Julia package (https://\ngithub.com/pagnani/PlmDCA, version 0.4.1), which is the state-of-the-\nart DCA method for contact inference. Weﬁtted one plmDCA model\nper synthetic MSA, using default hyperparameters throughout; these\ninclude, in particular, regularization strengths set atλ =1 0\n−2 for both\nﬁelds and couplings, and automatic estimation of the phylogenetic\ncutoffδ in Eq. (1). We veriﬁed that these settings led to good inference\nof structural contacts on the original full MSAs by comparing them to\nthe PDB structures in Supplementary Table 1— see Supplementary\nFig. 2. For each synthetic MSA, we computed coupling scoresE\nij for all\npairs of sites.\nWhile Potts models need to beﬁt t e do nd e e pM S A st oa c h i e v e\ngood contact prediction, MSA Transformer’s memory requirements\nare considerable even at inference time, and the average depth of the\nMSAs used to train MSA Transformer was 1192\n28.C o n c o r d a n t l y ,w e\ncould not run MSA Transformer on any of the synthetic MSAs in their\nentirety. Instead, we subsampled each synthetic MSA 10 times, by\nselecting each time a numberMsub of row indices uniformly at random,\nwithout replacement. We usedMsub ≈ 380 for family PF13354 due to its\ngreater length, andMsub ≈ 500 for all other families. Then, we com-\nputed for each subsample a matrix of coupling scores using MSA\nTransformer’s row attention heads and the estimated contact prob-\nabilities from the aforementioned logistic regression. Finally, we\naveraged the resulting 10 matrices to obtain a single matrix of coupling\nscores. We used a similar strategy (and the same randomly sampled\nrow indices) to infer structural contact scores from the natural MSAs—\nsee Supplementary Fig. 3. Consistently withﬁndings in ref.28,M S A\nTransformer generally performs better than plmDCA (Supplementary\nFig. 2) at contact inference.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 9\nData availability\nAll sequence data used or generated in our work has been deposited in\nhttps://zenodo.org/record/7096792.W em a d eu s eo ft h ef o l l o w i n g\nPDB structures: 4D81 [https://doi.org/10.2210/pdb4D81/pdb], 1L7V\n[https://doi.org/10.2210/pdb1L7V/pdb], 3UP1 [https://doi.org/10.2210/\npdb3UP1/pdb], 3ILH [ https://doi.org/10.2210/pdb3ILH/pdb], 3NNH\n[https://doi.org/10.2210/pdb3NNH/pdb], 4R2A [ https://doi.org/10.\n2210/pdb4R2A/pdb], 1OCK [ https://doi.org/10.2210/pdb1OCK/pdb],\n3EX7 [https://doi.org/10.2210/pdb3EX7/pdb], 4REX [https://doi.org/\n10.2210/pdb4REX/pdb], 3DGE [ https://doi.org/10.2210/pdb3DGE/\npdb], 1BE9 [https://doi.org/10.2210/pdb1BE9/pdb], 4M57 [https://doi.\norg/10.2210/pdb4M57/pdb], 3G7E [https://doi.org/10.2210/pdb3G7E/\npdb], 1FHG [https://doi.org/10.2210/pdb1FHG/pdb], 6QW8 [https://\ndoi.org/10.2210/pdb6QW8/pdb].\nCode availability\nOur code is available athttps://zenodo.org/record/7096792.\nReferences\n1. de Juan, D., Pazos, F. & Valencia, A. Emerging methods in protein\nco-evolution.Nat. Rev. Genet.14,2 4 9–261 (2013).\n2. Cocco, S., Feinauer, C., Figliuzzi, M., Monasson, R. & Weigt, M.\nInverse statistical physics of protein sequences: a key issues review.\nRep. Prog. Phys.81,0 3 2 6 0 1( 2 0 1 8 ) .\n3. Bahdanau, D., Cho, K. & Bengio, Y. Neural machine translation by\nj o i n t l yl e a r n i n gt oa l i g na n dt r a n s l a t e( I C L R2 0 1 5 ) .arXiv https://doi.\norg/10.48550/arXiv.1409.0473(2014).\n4. Vaswani, A. et al. Attention is all you need.Adv. Neural Inf. Process.\nSyst. 30, 5998–6008 (2017).\n5. Elnaggar, A. et al. ProtTrans: towards cracking the language of life’s\ncode through self-supervised learning.bioRxiv https://doi.org/10.\n1101/2020.07.12.199554(2020).\n6. Rives, A. et al. Biological structure and function emerge from\nscaling unsupervised learning to 250 million protein sequences.\nProc. Natl. Acad. Sci. USA118 https://www.pnas.org/content/118/\n15/e2016239118(2021).\n7. Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. & Rives, A. Transformer\nprotein language models are unsupervised structure learners. In\nInternational Conference on Learning Representationshttps://\nopenreview.net/forum?id=fylclEqgvgd(2021).\n8. Choromanski, K. et al. Rethinking attention with Performers. In\nInternational Conference on Learning Representations. https://\nopenreview.net/forum?id=Ua6zuk0WRH(2021).\n9. Madani, A. et al. ProGen: Language modeling for protein genera-\ntion. bioRxiv https://doi.org/10.1101/2020.03.07.982272\n(2020).\n10. Madani, A. et al. Deep neural language modeling enables functional\nprotein generation across families.bioRxiv https://doi.org/10.1101/\n2021.07.18.452833(2021).\n11. Bhattacharya, N. et al. Interpreting potts and transformer protein\nmodels through the lens of simpliﬁed attention.Pac. Symp. Bio-\ncomput. 27,3 4–45 (2022).\n12. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3–589 (2021).\n13. Baek, M. et al. Accurate prediction of protein structures and inter-\nactions using a three-track neural network.Science 373,\n871–876 (2021).\n14. Chowdhury, R. et al. Single-sequence protein structure prediction\nusing language models from deep learning.bioRxivhttps://doi.org/\n10.1101/2021.08.02.454840(2021).\n15. Casari, G., Sander, C. & Valencia, A. A method to predict functional\nresidues in proteins.Nat. Struct. Biol.2,1 7 1–178 (1995).\n16. Socolich, M. et al. Evolutionary information for specifying a protein\nfold. Nature 437,5 1 2–518 (2005).\n17. Dunn, S. D., Wahl, L. M. & Gloor, G. B. Mutual information without\nthe inﬂuence of phylogeny or entropy dramatically improves resi-\ndue contact prediction.Bioinformatics24, 333–340 (2008).\n1 8 . H a l a b i ,N . ,R i v o i r e ,O . ,L e i b l e r ,S .&R a n g a n a t h a n ,R .P r o t e i ns e c t o r s :\nevolutionary units of three-dimensional structure.Cell 138,\n774–786 (2009).\n19. Lapedes, A. S., Giraud, B. G., Liu, L. & Stormo, G. D. Correlated\nmutations in models of protein sequences: phylogenetic and\nstructural effects. InStatistics in molecular biology and genetics–\nIMS Lecture Notes– Monograph Series,v o l .3 3 ,2 3 6–256 (Institute of\nMathematical Statistics, 1999).https://doi.org/10.1214/lnms/\n1215455556.\n2 0 . W e i g t ,M . ,W h i t e ,R .A . ,S z u r m a n t ,H . ,H o c h ,J .A .&H w a ,T .I d e n t i -\nﬁcation of direct residue contacts inprotein-protein interaction by\nmessage passing.P r o c .N a t l .A c a d .S c i .U S A106,6 7–72 (2009).\n21. Marks, D. S. et al. Protein 3D structure computed from evolutionary\nsequence variation.PLoS ONE6, e28766 (2011).\n22. Morcos, F. et al. Direct-coupling analysis of residue coevolution\ncaptures native contacts across many protein families.Proc. Natl.\nA c a d .S c i .U S A108,E 1 2 9 3–1301 (2011).\n23. Sułk o w s k a ,J .I . ,M o r c o s ,F . ,W e i g t ,M . ,H w a ,T .&O n u c h i c ,J .N .\nGenomics-aided structure prediction.P r o c .N a t l .A c a d .S c i .U S A\n109,1 0 3 4 0–10345 (2012).\n2 4 . E k e b e r g ,M . ,L o v k v i s t ,C . ,L a n ,Y . ,W e i g t ,M .&A u r e l l ,E .I m p r o v e d\nc o n t a c tp r e d i c t i o ni np r o t e i n s :using pseudolikelihoods to infer\nPotts models.Phys. Rev. E87,0 1 2 7 0 7( 2 0 1 3 ) .\n25. Ekeberg, M., Hartonen, T. & Aurell, E. Fast pseudolikelihood max-\nimization for direct-coupling analysis of protein structure from\nmany homologous amino-acid sequences.J. Comput. Phys.276,\n341–356 (2014).\n26. Figliuzzi, M., Barrat-Charlaix, P. & Weigt, M. How pairwise coevolu-\ntionary models capture the collective residue variability in proteins?\nMol. Biol. Evol.35,1 0 1 8–1027 (2018).\n27. Abriata, L. A., Tamó, G. E., Monastyrskyy, B., Kryshtafovych, A. & Dal\nPeraro, M. Assessment of hard target modeling in CASP12 reveals\nan emerging role of alignment-based contact prediction methods.\nProteins86,9 7–112 (2018).\n2 8 . R a o ,R .M .e ta l .M S AT r a n s f o r m e r .Proceedings of the 38th Inter-\nnational Conferenceon Machine Learning139,8 8 4 4–8856 (2021).\n29. Meier, J. et al. Language models enable zero-shot prediction of the\neffects of mutations on protein function. In Beygelzimer, A., Dau-\nphin, Y., Liang, P. & Vaughan, J. W. (eds.)Advances in Neural Infor-\nmation Processing Systems(2021). https://openreview.net/forum?\nid=uXc42E9ZPFs.\n30. Hie, B. L., Yang, K. K. & Kim, P. S. Evolutionary velocity with protein\nlanguage models predicts evolutionary dynamics of diverse pro-\nteins. Cell Systems13,2 7 4–285.e6 (2022).\n31. Hawkins-Hooker, A., Jones, D. T. & Paige, B. MSA-conditioned\ngenerative protein language models forﬁtness landscape model-\nling and design. InMachine Learning for Structural Biology Work-\nshop, NeurIPS(2021). https://www.mlsb.io/papers_2021/\nMLSB2021_MSA-Conditioned_Generative_Protein_Language.pdf.\n32. Gordon, Y. On Milman’s inequality and random subspaces which\nescape through a mesh inRn. In Lindenstrauss, J. & Milman, V. D.\n(eds.) Geometric Aspects of Functional Analysis,8 4–106 (Springer,\nBerlin, Heidelberg, 1988).https://doi.org/10.1007/BFb0081737.\n33. Price, M. N., Dehal, P. S. & Arkin, A. P. FastTree 2 - approximately\nmaximum-likelihood trees for large alignments.PLOS ONE5,\n1–10 (2010).\n34. Russ, W. P. et al. An evolution-based model for designing chor-\nismate mutase enzymes.Science 369, 440–445 (2020).\n35. Rogers, A., Kovaleva, O. & Rumshisky, A. A primer in BERTology:\nwhat we know about how BERT works.Transactions of the Asso-\nciation for Computational Linguistics8,8 4 2–866 (2020).\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 10\n36. Vig, J. et al. BERTology meets biology: Interpreting attention in\nprotein language models. InInternational Conference on Learning\nRepresentations(2021). https://openreview.net/forum?id=\nYWtLZvLmud7.\n37. Schwaller, P., Hoover, B., Reymond, J. L., Strobelt, H. & Laino, T.\nExtraction of organic chemistry grammar from unsupervised\nlearning of chemical reactions. Sci. Adv.7, https://doi.org/10.1126/\nsciadv.abe4166(2021).\n38. Riesselman, A. J., Ingraham, J. B. & Marks, D. S. Deep generative\nmodels of genetic variation capture the effects of mutations.Nat.\nMethods 15,8 1 6–822 (2018).\n39. Ding, X., Zou, Z. & Brooks III, C. L. Deciphering protein evolution and\nﬁtness landscapes with latent space models.Nat. Commun.10,\n5644 (2019).\n4 0 . M c G e e ,F .e ta l .T h eg e n e r a t i v ec a p a c i t yo fp r o b a b i l i s t i cp r o t e i n\nsequence models.Nat. Commun.12, 6302 (2021).\n41. Qin, C. & Colwell, L. J. Power law tails in phylogenetic systems.Proc.\nNatl. Acad. Sci. USA115, 690–695 (2018).\n4 2 . V o r b e r g ,S . ,S e e m a y e r ,S .&S ö d i n g ,J .S y n t h e t i cp r o t e i na l i g n m e n t s\nby CCMgen quantify noise in residue-residue contact prediction.\nPLoS Comput. Biol.14,1 –25 (2018).\n43. Rodriguez Horta, E., Barrat-Charlaix, P. & Weigt, M. Toward inferring\nPotts models for phylogenetically correlated sequence data.\nEntropy 21 https://www.mdpi.com/1099-4300/21/11/1090(2019).\n44. Rodriguez Horta, E. & Weigt, M. On the effect of phylogenetic\ncorrelations in coevolution-based contact prediction in proteins.\nPLoS Comput. Biol.17 https://doi.org/10.1371/journal.pcbi.\n1008957 (2021).\n45. Lichtarge, O., Bourne, H. R. & Cohen, F. E. An evolutionary trace\nmethod deﬁnes binding surfaces common to protein families.J.\nMol. Biol.257,3 4 2–358 (1996).\n46. Hockenberry, A. J. & Wilke, C. O. Phylogenetic weighting does little\nto improve the accuracy of evolutionary coupling analyses.Entropy\n21, https://doi.org/10.3390/e21101000(2019).\n47. Malinverni, D. & Barducci, A. Coevolutionary analysis of protein\nsubfamilies by sequence reweighting.Entropy 21, 1127 (2020).\n48. Colavin, A., Atolia, E., Bitbol, A.-F. & Huang, K. C. Extracting phylo-\ngenetic dimensions of coevolution reveals hidden functional sig-\nnals. Sci. Rep.12, 820 (2022).\n4 9 . W e i n s t e i n ,E .N . ,A m i n ,A .N . ,F r a z e r ,J .&M a r k s ,D .S .N o n -\nidentiﬁability and the blessings of misspeciﬁ\ncation in models of\nmolecularﬁtness and phylogeny.bioRxiv https://doi.org/10.1101/\n2022.01.29.478324(2022).\n50. Sgarbossa, D., Lupo, U. & Bitbol, A.-F. Generative power of a protein\nlanguage model trained on multiple sequence alignments.bioRxiv\nhttps://doi.org/10.1101/2022.04.14.488405(2022).\n51. Mistry, J. et al. Pfam: The protein families database in 2021.Nucleic\nAcids Res.49,D 4 1 2–D419 (2020).\n52. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 4171-4186 (Asso-\nciation for Computational Linguistics, Minneapolis, Minnesota,\n2019). https://aclanthology.org/N19-1423.\n53. Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y. N. Con-\nvolutional sequence to sequence learning. In Precup, D. & Teh, Y. W.\n(eds.)Proceedings of the 34th International Conference on Machine\nLearning,v o l .7 0o fProceedings of Machine Learning Research,\n1243–1252 (PMLR, 2017).https://proceedings.mlr.press/v70/\ngehring17a.html.\n54. Ho, J., Kalchbrenner, N., Weissenborn, D. & Salimans, T. Axial\nattention in multidimensional transformers.arXiv https://doi.org/\n10.48550/arXiv.1912.12180(2019).\n55. Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer normalization.arXivhttps://\ndoi.org/10.48550/arXiv.1607.06450(2016).\n56. Papke, L. E. & Wooldridge, J. M. Econometric methods for fractional\nresponse variables with an application to 401(k) plan participation\nrates. J. Appl. Econ.11,6 1 9–632 (1996).\n57. Seabold, S. & Perktold, J. Statsmodels: Econometric and statistical\nmodeling with Python. In9th Python in Science Conference(2010).\nhttps://doi.org/10.25080/Majora-92bf1922-011.\nAcknowledgements\nThe authors thank Mohammed AlQuraishi for inspiring discussions,\nSergey Ovchinnikov for valuable feedback on theﬁrst version of this\nmanuscript, Tom Sercu for clarifying some aspects of MSA Transfor-\nmer’s pre-training, and Alexander Rives for interesting conversations.\nCode by Andonis Gerardos was used to generate synthetic MSAs along\nprescribed phylogenies. This project has received funding from the\nEuropean Research Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (grant agreement No. 851173,\nto A.-F.B.).\nAuthor contributions\nAll authors participated in the design of the project. U.L. and D.S. wrote\nthe software. U.L. built the datasets and performed the bmDCA analysis.\nU.L. and D.S. performed the analysis of MSA Transformer column\nattention. All authors interpreted the results. A.-F.B. supervised the\nproject. U.L. and A.-F.B. wrote the manuscript. All authors reviewed and\nedited the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-022-34032-y.\nCorrespondenceand requests for materials should be addressed to\nUmberto Lupo or Anne-Florence Bitbol.\nPeer review informationNature Communicationsthanks Arne Elofsson\nand the other, anonymous, reviewer(s)for their contribution to the peer\nreview of this work.\nReprints and permission informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nArticle https://doi.org/10.1038/s41467-022-34032-y\nNature Communications|         (2022) 13:6298 11"
}