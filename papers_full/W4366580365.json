{
  "title": "Faithful AI in Medicine: A Systematic Review with Large Language Models and Beyond",
  "url": "https://openalex.org/W4366580365",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2126157100",
      "name": "Qianqian Xie",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2548939160",
      "name": "Edward J Schenck",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2348087827",
      "name": "He S Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104208150",
      "name": "Yong Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099815532",
      "name": "Yifan Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2126157100",
      "name": "Qianqian Xie",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2895763047",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3205235328",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W133394232",
    "https://openalex.org/W4296027312",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3197043999",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4221148722",
    "https://openalex.org/W3150212014",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3034863243",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3166845084",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2951675429",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3035214886",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W4313197536",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W1989335536",
    "https://openalex.org/W4367046742",
    "https://openalex.org/W4285807172",
    "https://openalex.org/W2891022667",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W4309302776",
    "https://openalex.org/W2532137099",
    "https://openalex.org/W4327810118",
    "https://openalex.org/W4206445734",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3080464152",
    "https://openalex.org/W4312711992",
    "https://openalex.org/W3177314227",
    "https://openalex.org/W3169068430",
    "https://openalex.org/W6986955575",
    "https://openalex.org/W3177477686",
    "https://openalex.org/W4285249816",
    "https://openalex.org/W4385574015",
    "https://openalex.org/W4323651065",
    "https://openalex.org/W3213990450",
    "https://openalex.org/W2111722073",
    "https://openalex.org/W4385567129",
    "https://openalex.org/W4320835282",
    "https://openalex.org/W4313447794",
    "https://openalex.org/W4327810494",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W4285265395",
    "https://openalex.org/W3103733301",
    "https://openalex.org/W4307309335",
    "https://openalex.org/W4385574195",
    "https://openalex.org/W6760721734",
    "https://openalex.org/W4293918660",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4323571945",
    "https://openalex.org/W3101757358",
    "https://openalex.org/W3099977667",
    "https://openalex.org/W3108639220",
    "https://openalex.org/W4205621476",
    "https://openalex.org/W3176640961",
    "https://openalex.org/W4224981510",
    "https://openalex.org/W4306178243",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W4287854917",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W3166664235",
    "https://openalex.org/W4385546024",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4377010595",
    "https://openalex.org/W3098800931",
    "https://openalex.org/W4387356888",
    "https://openalex.org/W4287017694"
  ],
  "abstract": "ABSTRACT Artificial intelligence (AI), especially the most recent large language models (LLMs), holds great promise in healthcare and medicine, with applications spanning from biological scientific discovery and clinical patient care to public health policymaking. However, AI methods have the critical concern for generating factually incorrect or unfaithful information, posing potential long-term risks, ethical issues, and other serious consequences. This review aims to provide a comprehensive overview of the faithfulness problem in existing research on AI in healthcare and medicine, with a focus on the analysis of the causes of unfaithful results, evaluation metrics, and mitigation methods. We systematically reviewed the recent progress in optimizing the factuality across various generative medical AI methods, including knowledge-grounded LLMs, text-to-text generation, multimodality-to-text generation, and automatic medical fact-checking tasks. We further discussed the challenges and opportunities of ensuring the faithfulness of AI-generated information in these applications. We expect that this review will assist researchers and practitioners in understanding the faithfulness problem in AI-generated information in healthcare and medicine, as well as the recent progress and challenges in related research. Our review can also serve as a guide for researchers and practitioners who are interested in applying AI in medicine and healthcare.",
  "full_text": "Faithful AI in Healthcare and Medicine\nQianqian Xie1 and Fei Wang1\n1Department of Population Health Science, Weill Cornell Medicine, Cornell University\nABSTRACT\nArtificial intelligence (AI) holds great promise in healthcare and medicine on being able to help with many aspects, from\nbiological scientific discovery, to clinical patient care, to public health policy making. However, the potential risk of AI methods for\ngenerating factually incorrect or unfaithful information is a big concern, which could result in serious consequences. This review\naims to provide a comprehensive overview of the faithfulness problem in existing research on AI in healthcare and medicine,\nincluding analysis of the cause of unfaithful results, evaluation metrics, and mitigation methods. We will systematically review\nthe recent progress in optimizing the factuality in various generative medical AI methods, including knowledge grounded large\nlanguage models, text-to-text generation tasks such as medical text summarization, medical text simplification, multimodality-to-\ntext generation tasks such as radiology report generation, and automatic medical fact-checking. The challenges and limitations\nof ensuring the faithfulness of AI-generated information in these applications, as well as forthcoming opportunities will be\ndiscussed. We expect this review to help researchers and practitioners understand the faithfulness problem in AI-generated\ninformation in healthcare and medicine, as well as the recent progress and challenges on related research. Our review can\nalso serve as a guide for researchers and practitioners who are interested in applying AI in medicine and healthcare.\n1 Introduction\nArtificial intelligence (AI) has been gradually applied in different aspects of healthcare and medicine1–3. It has demonstrated\na positive impact on a wide range of areas including accelerating medical research, aiding disease detection and diagnosis,\nproviding personalized health recommendations, and so on (as shown in Figure 2). The success of Medical AI has been closely\ntied to the development of fundamental AI algorithms and the availability of various biomedical data. For example, deep\nlearning 4 has shown great performance on various medical image analysis tasks5.\nRecently, pre-trained language models (PLMs)6, 7, which are pre-trained on a huge amount of unlabelled language texts in\na self-supervised learning manner (typically based on the new backbone neural network architecture called Transformer 8),\nhas become the new paradigm of natural language processing (NLP) and demonstrated strong potential in Medical AI. PLMs\nhave shown high accuracy and strong generalization ability in few-shot learning and even zero-shot learning scenarios in\nvarious medical tasks9, 10, which went beyond NLP. For example, AlphaFold11, a deep learning computational method based\non Transformer developed by DeepMind, has been shown to be able to predict the protein structure with near experimental\naccuracy. More recently, large generative language models12, such as ChatGPT13 and GPT-414 developed by OpenAI, have\nexhibited strong capabilities of understanding complex input in natural language and produce near human-writing text contents.\nChatGPT has been reported to be close to pass the threshold of the United States Medical Licensing Exam (USMLE) and\nshowed great potentials on assisting clinical decision-making15.\nPlease summarize the paper:\nAggarwal R, Sounderajah V,\nMartin G, et al. Diagnostic\naccuracy of deep learning in\nmedical imaging: a systematic\nreview and meta-analysis[J]. NPJ\ndigital medicine, 2021, 4(1): 65.\nPrompt ChatGPT\nThe paper provides a systematic review and meta-analysis of studies that\nevaluated the diagnostic accuracy of deep learning models in medical\nimaging. The authors conducted a comprehensive search of literature\ndatabases and identified 82 studies that met the inclusion criteria..........For\nexample, the diagnostic accuracy of deep learning models was higher in\nCT and MRI than in radiography and ultrasound, and it was higher in the\ndiagnosis of cancer than in the diagnosis of other diseases.\nOur aim was to evaluate the diagnostic accuracy of DL algorithms to identify pathology in medical imaging. Searches were conducted in\nMedline and EMBASE up to January 2020. We identified 11,921 studies, of which 503 were included in the systematic review.  Eighty-\ntwo studies in ophthalmology, 82 in breast disease and 115 in respiratory disease were included for meta-analysis. ......... Heterogeneity\nwas high between studies and extensive variation in methodology, terminology and outcome measures was noted. This can lead to an\noverestimation of the diagnostic accuracy of DL algorithms on medical imaging. There is an immediate need for the development of\nartificial intelligence-specific EQUATOR guidelines, particularly STARD, in order to provide guidance around key issues in this field.\nGenerated  summary\nGround truth summary\nFigure 1. The example of the generated content with factual errors from ChatGPT.\n1\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nFigure 2. The example of medical data and realistic medical applications with AI.\nDespite the promises of medical AI16, one major concern that has attracted a lot of attention is its potential risk of generating\nnon-factual or unfaithful information17, which is usually referred to as the faithfulness problem18. Specifically, generative AI\nmethods can generate contents that are factually inaccurate or biased. For example, in Figure 1, we input the natural language\ntexts to ask ChatGPT to summarize a systematic review paper named \"Diagnostic accuracy of deep learning in medical imaging:\na systematic review and meta-analysis\" published in npj Digital Medicine 202119. ChatGPT generates the summary with the\nintrinsic factual error that contradicts the content of the review, such as it showing 82 studies met the inclusion criteria while the\nreview is actually based on 503 studies. The generated summary also has the extrinsic factual error that can’t be supported by\nthe review, such as \"the diagnostic accuracy of deep learning models was higher in computed tomography (CT) and magnetic\nresonance imaging (MRI) than in radiography and ultrasound, and it was higher in the diagnosis of cancer than in the diagnosis\nof other diseases\". This could mislead researchers and practitioners and lead to unintended consequences20.\nIn this review, we provide an overview of the research on faithfulness problem in existing medical AI studies, including\ncause analysis, evaluation metrics, and mitigation methods. We comprehensively summarize the recent progress of maintaining\nfactual correctness in various generative medical AI models including knowledge grounded large language models, text-to-\ntext generation tasks such as medical text summarization and simplification, multimodality-to-text generation tasks such as\nradiology report generation, and automatic medical fact-checking. We discuss the challenges of maintaining the faithfulness\nof AI-generated information in the medical domain, as well as the forthcoming opportunities. The goal of this review is to\nprovide researchers and practitioners with a blueprint of the research progress on the faithfulness problem in medical AI, help\nthem understand the importance and challenges of the faithfulness problem, and offer them guidance for using AI methods in\nmedical practice and future research.\n2/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nRecords identified from:PubMed (n = 472)Scopus (n=381)IEEE Xplore (n=468)ACM Digital Library (n=715)Google Scholar (n=1403)\nRecords removed before screening:Remove duplicate records (n= 2574) \nRecords excluded (n = 816)Exclusion criteria:Review papersNon-medical domainNon-EnglishResearch not for factuality problem\nIdentification\nRecords screened(n = 865)\nScreeningReports assessed for eligibility(n = 49) Reports excluded (n = 12)Reason: low quality \nArticles included in review(n = 37)Included\nFigure 3. The process of article selection.\n2 Search Methodology\n2.1 Search Strategy\nWe conducted a comprehensive search for articles published between January 2018 to March 2023 from multiple databases\nincluding PubMed, Scopus, IEEE Xplore, ACM Digital Library, and Google Scholar. We used two groups of search queries:\n1) faithful biomedical language models: factuality/faithfulness/hallucination, biomedical/medical/clinical language models,\nbiomedical/medical/clinical knowledge, 2) mitigation methods and evaluation metrics: factuality/faithfulness/hallucination,\nevaluation metrics, biomedical/medical/clinical summarization, biomedical/medical/clinical text simplification; radiology report\nsummarization, radiology report generation, medical fact-checking.\n2.2 Filtering Strategy\nA total of 3,439 records were retrieved from five databases. 865 articles were kept after removing duplicate records, which were\nfurther screened based on title and abstract. We applied the exclusion criteria during the filtering process (also shown in Figure\n3): 1) the article is a review paper, 2) the main content of the article is not in English, 3) the article is not related to the medical\ndomain, 4) the article is not relevant to the factuality problem. As a result, 49 articles were retained after the screening process.\nThen, we conducted a full-text review of these articles and further excluded 12 articles due to the low quality of the content.\n3 What is Faithfulness Problem ?\n3.1 Definition and Categorization\nFaithfulness generally means being loyal to something or some person. In the context of AI or NLP, faithful AI means\nthe algorithm can produce contents that are factually correct, namely staying faithful to facts 18, 21. Generative medical AI\nsystems11, 22, 23 learn to map from various types of medical data such as electronic health records (EHRs), medical images\nor protein sequences, to desired output such as the summarization or explanation of medical scans, radiology reports, and\n3/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nTable 1. The reference of different exemplar generative medical tasks for evaluating the factual consistency of the generated\noutput by AI systems.\nTask Input Output Reference\nmedical text summarization medical texts short summarization input\nradiology report summarization radiology report impression impression from radiologist\nmedical text simplification medical texts simplified texts input\nmedical dialogue generation dialogue from patients response dialogue history\nmedical question answering medical question answering clinician-generated answer\nradiology report generation Chest X-ray image radiology report radiology report from radiologist\nthree-dimensional (3D) protein structures. We refer a medical AI systems to be unfaithful or has a factual inconsistency issue\n(which is also called \"hallucination\" in some studies 21) if it produces content that is not supported by existing knowledge,\nreference or data. Similar to the general generative AI methods, the factual inconsistency issue in generative medical AI\ngenerally can also be categorized into the following two major types24:\n• Intrinsic Error:the generated output contradicts with existing knowledge, reference or data. For example, if there is a\nsentence in the radiology report summary generated by the AI system \"The left-sided pleural effusion has increased in\nsize\", but the actual sentence in the radiology report is \"There is no left pleural effusion\". Then the summary contradicts\nfacts contained in the input data.\n• Extrinsic Error:the generated output cannot be confirmed (either supported or contradicted) by existing knowledge,\nreference or data. For example, if the content of the radiology report includes: \"There is associated right basilar\natelectasis/scarring, also stable. Healed right rib fractures are noted. On the left, there is persistent apical pleural\nthickening and apical scarring. Linear opacities projecting over the lower lobe are also compatible with scarring,\nunchanged. There is no left pleural effusion\", and the generated summary includes: \"Large right pleural effusion is\nunchanged in size\". Then this piece of information cannot be verified by the given radiology report, since the information\nabout \"right pleural effusion\" is not mentioned there.\nDifferent from general applications, the tolerance of factual incorrectness should be relatively low for medical and healthcare\ntasks to due to their high-stake nature. Different medical tasks have different references on accessing the factuality of the\ngenerated output. For the AI based text summarization 22 or simplification systems25 that summarize or simplify the input\nmedical texts such as study protocols, biomedical literature, or clinical notes, the reference is usually the input medical\ndocument itself, which ensures the AI generated content is faithful to its original information. For AI systems that automatically\ngenerate radiology reports with the input Chest X-ray image26, the reference is the radiology report of the input Chest X-ray\nimage written by the radiologist. In table 4, we summarize the reference of different exemplar generative medical tasks for\nevaluating the factual consistency of the generated output by AI systems.\n3.2 Why There is the Faithfulness Problem?\nThe factual inconsistency of medical AI systems can be attributed to a wide range of reasons.\n3.2.1 Limitation of backbone language model\nPre-trained language models (PLMs), which have been the dominant backbone model for representing medical data in various\ntypes and modalities, have limitations on generalization and representation abilities for medical data. It has been shown that the\nlarge language models (LLMs) such as GPT-312 has poor performance of few-shot learning on information extraction (IE) tasks\nof biomedical texts27, 28. This is because these LLMs were not pre-trained on medical data, thus they lack the representability\nthere and cannot capture medical knowledge effectively. There have been domain-specific language models trained with\nbiomedical data such as BioBERT29, PubMedBERT30, BioGPT23 et al. either with fine-tuning or from scratch, but the scale\nof these data are much smaller than those used for training PLMs in general domains. For example, it has been shown that\ndomain-specific language models such BioBERT27 has better performance than GPT-312 on the few-shot learning of IE tasks,\nbut it still has a large gap with the performance of fine-tuning based methods. In addition, most biomedical texts such as EHRs,\nand radiology report texts have limited availability due to privacy concerns. As a result, existing domain-specific language\nmodels were mainly pre-trained with biomedical literature texts, that are easy to be accessed on large scale. Therefore, it\nis challenging to pre-train LLMs with biomedical domain specific data that can generalize well and encode comprehensive\nmedical knowledge. Overall, both existing biomedical PLMs and powerful LLMs in the general domain including ChatGPT\nand GPT-4 are not effective enough in encoding biomedical knowledge31. This inevitably leads to factual inconsistencies in\nvarious medical AI systems that uses these PLMs and LLMs as the backbone model.\n4/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \n3.2.2 Lacking of data sources\nFine-tuning has been the main paradigm for adapting PLMs to various biomedical tasks7. It transfers the semantic information\nand knowledge encoded in PLMs and to the targeted tasks via supervised learning with labeled data. Similar to conventional\ndeep learning methods4, it relies on a sufficient amount of annotated medical data to achieve desirable performance. However,\nhigh-quality annotated medical data are both expensive and time consuming to obtain. This is another reason why medical AI\nsystems can be vulnerable to factual errors.\n3.2.3 Data discrepancy\nThe factual inconsistency problem also arises from the data discrepancy between the ground truth output used for training and\nthe reference in most medical tasks. For example, in the task of generating lay summaries for technical biomedical scientific\npapers, the ground truth outputs namely lay summaries usually include additional information for ease the understanding of lay\npeople, which is not mentioned in the reference namely the original biomedical scientific papers. For the medical dialogue\ngeneration tasks, the ground truth output responses don’t always faithful to the references namely dialogue histories, since the\ntopic of responses can be transferred and diverse according to input dialogues from patients. Thus, the medical AI systems that\nare trained by generating the output with minimized divergence with the ground truth output, can produce an output that is not\nfaithful with the reference.\n3.2.4 Limitations of decoding\nAnother cause for the factual inconsistency problem is the limitation of the decoding strategy used by medical AI systems for\ntext generation. There exists the exposure bias problem32, 33, which is the discrepancy between training and inference on the\ndecoding process based on the decoder. Specifically, during training, the decoder is encouraged to generate the next token on\nthe conditional of the previous ground truth tokens. While during inference, the ground truth tokens are unobservable, and thus\nthe decoder can only be conditioned on the previously generated tokens by itself to predict the next token. Such discrepancy\nbetween training and inference has been shown to potentially result in factual errors of outputs24. Moreover, existing methods\nusually use the sampling-based decoding strategy such as beam search and greedy search to improve the diversity of outputs,\nand they have not considered the factual consistency when selecting tokens and candidate outputs. The randomness of selecting\ntokens and candidate outputs can increase the probability of generating outputs with factual errors34.\n4 Evaluating and Optimizing Faithfulness\nTo improve the faithfulness of LLMs, many efforts have been focusing on improving the backbone model with medical\nknowledge, optimizing the factual correctness of AI medical systems for various generative medical tasks, and developing\nfact-checking methods, which will be introduced in the following subsections.\n4.1 Faithfulness in Large Language Models (LLMs)\nAlthough domain-specific language models such as BioBERT, PubMedBERT, and BioGPT et al, have shown the effectiveness in\nencoding medical texts through pre-training with unlabeled medical texts, their ability in understanding medical knowledge and\ntexts is challenging for them to generate factually correct contents. Some efforts have focused on explicitly incorporating extra\nmedical knowledge to address this challenge. Yuan et al35 proposed to train knowledge-aware language model by infusing entity\nrepresentations, as well as the entity detection and entity linking pre-training tasks based on the Unified Medical Language\nSystem (UMLS) knowledge base. The proposed method improves the performance of a series of biomedical language models\nsuch as BioBERT and PubMedBERT, on the named entity recognition and relation extraction tasks. Jha et al36 proposed to prob\ndiverse medical knowledge bases into language models with the continual knowledge infusion mechanism to avoid forgetting\nencoded knowledge previously when injecting multiple knowledge bases, which improves several biomedical language models\nsuch as BioBERT and PubMedBERT in medical question answering, named entity recognition and relation extraction. Singhal\net al37 investigated the ability of LLMs on capturing clinical knowledge, based on a 540-billion parameter LLM PaLM38 and\nits instruction-tuned variant Flan-PaLM39, on the medical question answering (Q&A) task. Flan-PaLM achieves state-of-the-art\n(SOTA) performance on medical Q&A benchmark datasets including MedQA40, MedMCQA41, PubMedQA42, and MMLU\nclinical topics43, by using the few-shot prompting, chain-of-thought prompting, and self-consistency prompting techniques and\noutperforms other domain-specific language models such as BioGPT with the 355 million parameters, PubMedGPT44 with\nthe 2.7 billion parameters and Galactica45 with the 120 billion parameters. They further proposed the Med-PaLM that aligns\nPaLM into the medical domain by instruction prompt tuning, which greatly alleviates errors of PaLM on scientific grounding,\nharm, and bias from low-quality feedback. Their experiments show that current LLMs such as Flan-PaLM are not ready to\nbe used in areas such as medicine where satefy is the first priority, and the instruction prompt tuning used in Med-PaLM is\nindeed useful in improving its factuality, consistency, and safety. Zakka et al46 proposed the knowledge-grounded language\nmodel Almanac, which uses the LLMs as the clinical knowledge base to retrieve and distill information from medical databases\nfor replying to clinical queries, rather than directly generate content with LLMs. Almanac has shown better performance\n5/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nthan ChatGPT on medical Q&A based on the 20 questions derived from their ClinicalQA dataset and mitigates the factual\ninconsistency problem by grounding LLMs with factually correct information retrieved from predefined knowledge repositories.\nNori et al47 conducted a comprehensive study on the ability of GPT-4 on medical competency examinations and medical\nQ&A benchmarks recently37. They evaluated the performance of GPT-4 on steps 1-3 of the United States Medical Licensing\nExamination (USMLE). On the USMLE self-assessment and sample exam, GPT-4 achieves the average accuracy score of\n86.65% and 86.70% and outperforms GPT-3.5 by around 30%. On the multiple-choice medical Q&A benchmark with four\ndatasets, GPT-4 also significantly outperforms GPT-3.5 and the Flan-PaLM 540B39 as introduced before by a large margin. On\nthe USMLE self-assessment and sample exam, they further evaluated the calibration of GPT-4, which measures the alignment\nof the predicted probability of the answer’s correctness and the true probability. GPT-4 shows much better calibration than\nGPT-3.5. More concretely, on the multiple-choice question-answering task, the data-points with predicted probability of 0.96\nassigned by GPT-4 can be correct at 93% of the time, while only 55% correctness of data-points with the similar predicted\nprobability from GPT-3.5. However, they suggested GPT-4 still has a large gap on safe adoption in the medical domain like\nprior LLMs.\nDiscussion. Although the above studies have shown that the incorporation of medical knowledge into LLMs can potentially\nhelp improve their faithfulness. They showed that even SOTA LLMs such as Med-PaLM and GPT-4 cannot satisfy the need\nfor safe use in medicine and healthcare. To bridge the gap, there are several limitations to be addressed and promising future\ndirections.\n• Systematic evaluation benchmark: existing methods only assess the factuality of LLMs in limited tasks such as question\nanswering and radiology report summarization, there is no systematic evaluation of LLMs in many other critical\ngenerative tasks such as medical text generation. We believe future efforts should be spent on evaluating and improving\nLLMs in diverse medical tasks to fill the gap with domain experts.\n• Multimodal and multilingual: most existing methods can only process the medical texts and the language in English.\nFuture efforts are encouraged to build LLMs in the medical domain with the ability to tackle inputs with multi-modalities\nand multiple languages, for example adapt the recently released multi-modal large language models such as GPT-4 and\nKosmos48 into the medical domain.\n• Unified automatic evaluation method: evaluating the performance of LLMs in factuality is especially challenging in\nthe medical domain and existing methods rely on human evaluation, which is expensive and hard to be on large scale.\nThe unified automatic factuality evaluation method should be proposed for supporting the effective evaluate the factual\ncorrectness of LLMs on various medical tasks.\nThe above methods only take the initial step, and we consider more efforts should be proposed in the future to make AI methods\ncloser to real-world medical applications.\n4.2 Faithfulness of AI Models in Different Medical Tasks\nMany efforts have been devoted to optimize the factuality of generative methods in medicine and healthcare, as well as their\nfactuality evaluation for a specific task with various techniques such as incorporating medical knowledge, reinforcement\nlearning and prompt learning.\n4.2.1 Medical Text Summarization\nMedical text summarization49 is an important generative medical task, with the goal of condensing medical texts such as\nscientific articles, clinical notes, or radiology reports into short summaries. Medical text summarization supports many\napplications in medicine and healthcare, such as assisting researchers and clinicians to quickly access important information\nfrom a large amount of medical literature and patient records and identify key medical evidence for clinical decisions. It\nis important to ensure the generated summaries by AI methods are factually consistent with the input or reference medical\ntexts. In the following we briefly overview the recent efforts on studying the factual inconsistency problem in medical text\nsummarization.\nOptimization Methods The factual inconsistency problem was first explored in the radiology report summarization. Specifi-\ncally, Zhang et al50 found that nearly 30% of radiology report summaries generated from the neural sequence-to-sequence\nmodels contained factual errors. To deal with the problem, Zhang et al22 proposed to optimize the factual correctness of the\nradiology report summarization methods with reinforcement learning. They evaluated the factual correctness of the generated\nsummary with the CheXpert F1 score51, which calculates the overlap of 14 clinical observations between the generated summary\nand the reference summary. They optimized such factual correctness with policy learning, by taking the factual correctness\nscore of the generated summary as the reward and the summarizer as the agent. They demonstrated that such training strategy\ncould improve the CheXpert F1 score by 10% when compared with the baseline method. However, this evaluation metric was\n6/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nonly limited to chest X-rays. Delbrouck et al52 further released the new dataset based on MIMIC-III53 with new modalities\nincluding MRI and CT, as well as anatomies including chest, head, neck, sinus, spine, abdomen, and pelvis. They proposed the\nnew factual correctness evaluation metric RadGraph score that could be used for various modalities and anatomies and designed\na summarization method that optimized the RadGraph score-based reward with reinforcement learning. Their experiment\nresults showed that optimizing the RadGraph score as the reward could consistently improve the factual correctness and quality\nof the generated summary from the summarizer, where the RadGraph score, F1CheXbert, ROUGE-L54 (a commonly used\nmetric in text generation, that calculates the longest common sub-sequence between the generated summary and reference\nsummary) are improved by 2.28%-4.96%, 3.61%-5.1%, and 0.28%-0.5%. Xie et al55 proposed the two-stage summarization\nmethod FactReranker, which aims to select the best summary from all candidates based on their factual correctness scores. They\nproposed to incorporate the medical factual knowledge based on the RadGraph Schema to guide the selection of FactReranker.\nFactReranker achieves the new SOTA on the MIMIC-CXR dataset and improves the RadGraph score, F1CheXbert, and\nROUGE-L by 4.84%, 4.75%, and 1.5%.\nThere are also efforts investigating the factual inconsistency problem in the automatic summarization of other medical texts\nsuch as biomedical literature, medical Q&A, and medical dialogues. Deyoung et al 56 found that the summarizer based on\nlanguage models such as BART57 could produce fluent summaries for medical studies, but the faithfulness problem remains\nan outstanding challenge. For example, the generated summary from the summarizer only had 54% agree on the direction of\nthe intervention’s effect with that of the input systematic review. Wallace et al58 proposed the decoration and sorting strategy\nthat explicitly informed the model of the position of inputs conveying key findings, to improve the factual correctness of the\ngenerated summary from the BART-based summarizer for published reports of randomized controlled trials (RCTs). Alambo59\nstudied the factual inconsistency problem of a transformer-based encoder decoder summarization method. They proposed to\nintegrate the biomedical named entities detected in input articles and medical facts retrieved from the biomedical knowledge\nbase to improve the model faithfulness. Yadav et al60 proposed to improve the factual correctness of the generated summary for\nmedical questions, via maximizing the question type identification reward and question focus recognition reward with the policy\ngradient approach. Chintagunta et al61 investigated using GPT-3 as the labeled data generator, and incorporated the medical\nconcepts identified from the input medical dialogues as the guidance for generating labeled data with higher quality, which has\nproven to be able to train the summarizer with better factual correctness when compared with the human-labeled training data\nthat is thirty times larger. Liu et al62 proposed the task of automatic generating discharge instruction based on the patients’\nelectronic health records and the Re3Writer method for the task, which retrieved related information from discharge instructions\nof previous patients and medical knowledge to generate faithful patient instructions. The human evaluation results showed the\npatient instructions generated by proposed method had better faithfulness and comprehensiveness than those generated from\nbaseline sequence-to-sequence methods.\nEvaluation Metrics Derivation of effective automatic factual correctness evaluation metrics is critical for evaluating the\nquality of the generated summary and summarization method development. Existing commonly used evaluation metrics in\ntext summarization such as ROUGE54 and BERTScore63 have been proven to be ineffective in evaluating factual correctness,\nespecially in medical domain. Existing evaluation metrics for factual consistency such as FactCC64 and BARTScore65, are\nalso designed for the general domain. Recently, efforts have been conducted to develop new metrics for evaluating the factual\nconsistency of the summarizer for different medical texts.\nFor radiology report summarization, Zhang et al22 proposed the CheXpert F1 score that calculates the overlap of 14 clinical\nobservations such as \"enlarged cardiom\" and \"cardiomegaly\", between the generated summary and the reference summary.\nHowever, it is limited to the reports of chest X-rays. Delbrouck et al52 further proposed RadGraph score that calculates the\noverlap of medical entities and relations based on the RadGraph66 (a dataset with annotations of medical entities and relations\nfor radiology reports) between the generated summary and the gold summary, and can be used for various modalities and\nanatomies. For the biomedical literature summarization, Wallace et al\n58 proposed findings-Jensen-Shannon Distance (JSD)\ncalculating the agreement of evidence directions (including significant difference, or no significant difference) of the generated\nsummary and reference summary of the systematic review, according to JSD. Based on findings-JSD, Deyoung et al56 further\nproposed the improved metric ∆EI that calculates the agreement of the intervention, outcome and evidence direction based on\nthe Jensen-Shannon Distance, between the generated summary and the input medical studies. Otmakhova et al67 proposed the\nhuman evaluation approach for medical study summarization, where they defined several quality dimensions including PICO\ncorrectness, evidence direction correctness, and modality to evaluate the factuality of the generated summary. Based on the\nhuman evaluation protocol, Otmakhova et al68 further developed the ∆loss to evaluate the factual correctness of the generated\nsummary on different aspects such as strong claim, no evidence, no claim, etc., and evidence directions. It calculates the\ndifference of negative log-likelihood loss with the summarization model, between the generated summary and the counterfactual\nsummary (the corrupted target summary that has different modality and polarity with the target summary). Adams et al 69\ndid a meta-evaluation on existing automatic evaluation metrics including BARTScore, BERTScore, CTC? and SummaC70\n(two SOTA metrics based on natural language inference) on assessing long-form hospital-course summarization. They first\n7/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \ncreated a human-annotated dataset with the fine-grained faithfulness annotations of generated hospital-course summaries with\nLongformer Encoder-Decoder (LED)71, and then measure the correlation between the assessment of automatic evaluation\nmetrics and human annotations. Although without aligning to the biomedical domain, they found the evaluation results of these\nautomatic evaluation metrics have a good correlation with that of human annotations. However, the calculated correlation is\nfound to has bias and not correct enough, since the generated summaries from LED tend to have a high overlap with original\nsentences of original inputs rather than regenerate new sentences, which is not applicable in a realistic situation. Moreover, these\nmetrics have limitations on assessing factual errors which require deep clinical knowledge such as missingness, incorrectness\nand not in notes.\nDiscussion To understand how existing methods perform on the factuality of medical text summarization, in Table2 and\nTable 3, we show the performance of SOTA methods on medical study summarization and radiology report summarization\nwith human evaluation and automatic evaluation metrics introduced above. We can see that existing SOTA methods have\nrelatively good performance on the grammar and lexical of generated summaries for medical studies. Only 8%-9% of the\ngenerated summaries are completely factually correct. Less than 50% of the generated summaries are with correct PICO 72\n(Patient, Intervention, Comparison, Outcome) and modality indicating the certainty level of evidence claim (such as strong\nclaim, moderate claim, and weak claim). For radiology report summarization, we can find that although most methods22, 52\nuse reinforcement learning for optimizing the factuality, the method proposed in Xie et al55 incorporating medical knowledge\nachieves the best performance, which shows the importance of using medical knowledge to improve factuality. Similarly, the\nfactuality performance of SOTA methods was not high either. For example, the RadGraph F1 scores of SOTA methods were\nonly around 50%.\nTable 2. The performance of human evaluation of SOTA methods on medical study summarization in MS2 dataset56 from the\npaper67, which evaluates generated summaries from the factual correctness of PICO, evidence direction, and modality.\nFactually correct means the composition performance of PICO, evidence direction, and modality.\nPICO Direction Modality Factually correct Grammar Lexical\nBART56 45% 77% 45% 9% 75% 69%\nLED56 40% 75% 44% 8% 73% 73%\nTable 3. The performance of SOTA methods for radiology reports summarization on MIMIC-CXR dataset.\nROUGE-1 ROUGE-2 ROUGE-L F1CheXbert RadGraph\nL4-RL52 51.96 35.65 47.10 74.86 48.23\nFactReranker55 55.94 40.63 51.85 76.36 53.17\nFor future work, we believe the following aspects are important to focus on.\n• Creation of benchmark data sets. There are limited open source datasets for medical text summarization, due to the high\ncost of expert annotation and other issues such as privacy. In fact, there are only two public datasets for medical study\nand radiology report summarization, which covers limited modalities, anatomies and languages. Moreover, the sizes of\nthese datasets are much smaller when compared with datasets in the general domain. High-quality large-scale benchmark\ndatasets should be created in the future for facilitating the development of medical summarization methods.\n• Derivation of unified automatic evaluation metrics. Existing evaluation metrics for assessing the factuality of methods in\ndifferent medical texts are specific, and there are even no such metrics for methods on certain types of medical texts such\nas medical dialogue and medical question summarization. It is important to develop unified automatic evaluation metrics\nfor supporting the assessment of summarization methods across different types of medical text.\n• Development of better optimization methods. Most existing methods employ reinforcement learning to improve the\nfactuality of generated summaries, while little attention has been paid to incorporating medical knowledge. As mentioned\nbefore, the factual correctness of SOTA methods is not good enough for reliable use in realistic medicine and healthcare.\nTherefore, more advanced methods are needed, for example, by using more powerful backbone language models and\ndesigning a more effective decoding strategy guided by medical facts.\n4.2.2 Medical Text Simplification\nMedical text simplification aims to simplify highly technical medical texts to plain texts that are easier to understand by\nnon-experts such as patients. It can greatly improve the accessibility of medical information.\n8/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nOptimization Methods Most existing work focused on creating data resources for supporting the development and evaluation\nof medical text simplification. For example, Trienes et al73 created a dataset with 851 pathology reports for document-level\nsimplification, Devaraj et al25 constructed a dataset with the biomedical systematic reviews, including both their technical\nsummaries and plain language summaries, to support the paragraph-level simplification. Lu et al74 proposed the summarize-\nthen-simplify method for paragraph-level medical text simplification, where they designed the narrative prompt with key phrases\nto encourage the factual consistency between the input and the output. The human evaluation showed that their proposed\nmethod significantly outperforms the BART-based simplification method proposed in25 by 0.49 in the 5-point scale on the\nfactuality of outputs. Jeblick et al75 utilized ChatGPT to simplify 45 radiology reports and asked 15 radiologists to evaluate\noutputs from factual correctness, completeness, and potential harm. They found the radiologists agreed that most of the outputs\nwere factually correct, but there were still some errors such as misinterpretation of medical terms, imprecise language, and\nextrinsic factual errors. Lyu et al 76 also evaluated the performance of ChatGPT on simplifying radiology reports to plain\nlanguage. The evaluation dataset consisted of 62 chest CT screening reports and 76 brain MRI screening reports. ChatGPT\nshowed good performance with an overall score of 4.268 in the five-point system assessed by radiologists. It has an averaged of\n0.08 and 0.07 places of information missing (assessing the number of places with information lost) and inaccurate information\n(assessing the number of places with inaccurate information). The suggestions for patients and healthcare provider generated\nby ChatGPT tended to be general, such as closely observing any symptoms, and only 37% provides specific suggestions based\non the findings of reports. ChatGPT also showed instability in generating over-simplified reports, which can be alleviated by\ndesigning better prompts. Overall, the factuality problem in automatic medical text simplification methods has rarely been\nexplored and we believe more efforts should be devoted in this area.\nEvaluation Metrics Similar to the medical text summarization, automatic similarity-based evaluation metrics such as ROUGE\nand BERTScore were also used for evaluating the semantic similarity between outputs and references in the medical text\nsimplification. Moreover, other important aspects in the evaluation are the readability and simplicity of outputs, for which\ncommonly used metrics include the Flesch-Kincaid grade level (FKGL)77, the automated readability index (ARI)78, and SARI79.\nIntuitively, the language model pre-trained on the technical corpus can assign higher likelihoods to the technical terms than\nthe language models pre-trained on the general corpus. Based on this intuition, Devaraj et al.25 proposed a new readability\nevaluation metric calculating the likelihood scores of input texts with a masked language model trained on the technical corpus.\nDevaraj et al80 further proposed a method for evaluating factuality in text simplification, in which they trained a RoBERTa-based\nclassification model to classify the factuality level according to different types of factual errors including insertion, deletion,\nand substitution errors based on human-annotated data. They found that existing metrics such as ROUGE and SARI cannot\neffectively capture factual errors, and it is also challenging to evaluate the factual errors for their proposed method, which\nis even trained with extra training data generated by data augmentation. As introduced above, only one automatic factuality\nevaluation metric is proposed by Devaraj et al80, which is however not effective in capturing factual errors. We believe more\nefforts on factuality evaluation metrics should be developed in this area.\nDiscussion We can see that there is limited research studying the factuality problem in medical text simplification. Moreover,\nthe assessment of the factuality of existing methods have been largely relying on human evaluation. Therefore, it is important\nto have more efforts to study the factuality problem in medical text simplification by developing better optimization methods,\nautomatic evaluation metrics, and creating more data resources in the future.\n4.2.3 Radiology Report Generation\nRadiology report generation aims to automatically generate radiology reports illustrating clinical observations and findings with\nthe input medical images such as chest X-rays and MRI scans. It can help to reduce the workload of radiologists and improve\nthe quality of healthcare.\nOptimization Methods Most existing efforts adopted reinforcement learning (RL) to optimize the factual correctness of\nradiology report generation methods. Nishino et al81 proposed the RL-based method by optimizing the clinical reconstruction\nscore calculating the correctness of predicting finding labels with the generated report, to improve the factual correctness of\ngenerated reports. Their experiments showed that the model improved the performance on the F1 score of the factuality metric\nCheXpert (it assesses the clinical correctness of the generated report that calculates the overlap of finding labels between\ngenerated and reference report) by 5.4% compared with the model without the RL-based optimization. Miura 26 proposed\nto use reinforcement learning to optimize the entailing entity match reward assessing the inferentially consistent (such as\nentailment, neutral, contradiction) between entities of the generated report and the reference report, and exact entity match\nreward evaluating the consistent of disease and anatomical entities between the generated report and reference report, which\nencourages the model to generate key medical entities that are consistent with references. The proposed method greatly\nimproves the F1 CheXpert score by 22.1% compared with the baselines. However, it relied on the named entity recognition\nmethods that are not trained with annotated data of the Chest X-ray domain. Delbrouck et al82 further designed the RadGraph\n9/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nreward calculating the overlap of entities and relations between the generated report and the reference, based on the RadGraph\ndataset including annotated entities and relations of the Chest X-ray reports. The proposed method improves the factuality\nevaluation metric F1 RadGraph score (calculating the overlap of entities and relations between the generated report and the\nreference) by 5.5% on the MIMIC-CXR dataset when compared with baselines including26. Nishino et al83 further proposed\nan RL-based method Coordinated Planning (CoPlan) with the fact-based evaluator and description-order-based evaluator, to\nencourage the model to generate radiology reports that are not only factually consistent but also chronologically consistent with\nreference reports. Their method outperforms the baseline T5 model on clinical factual accuracy by 9.1%.\nEvaluation Metrics Similar to medical text summarization, to evaluate the clinical correctness of the generated radiology\nreports, some efforts26, 81, 84 proposed to use the CheXpert-based metric to evaluate the overlap of 14 clinical observations\nbetween generated reports and references annotated by the CheXpert, for which they calculated the precision, recall, and F1\nscore. Delbrouck et al 82 proposed the RadGraph-based metric to calculate the overlap of the clinical entities and relations\nbetween generated reports and references annotated by the RadGraph schema. Recently, Yu et al85 examined the correlation\nbetween existing automatic evaluation metrics including BLEU86, BERTScore, F1 CheXpert, and RadGraph F1, and the score\ngiven by radiologists on evaluating the factuality of the generated reports. They found that the evaluation results of F1 CheXpert\nand BLEU were not aligned with that of radiologists, and BERTScore and RadGraph F1 were more reliable. BERTScore and\nRadGraph F1 outperformed BLEU in evaluating the false prediction of finding, while F1 CheXpert has worse performance than\nBLEU in evaluating the incorrect location of finding. The authors further proposed a new evaluation metric RadCliQ, which\ntrained an regression model to predict the number of errors in generated radiology reports assigned by radiologists based on the\ncombination score with BLEU and RadGraph F1. RadCliQ is thus the weighted sum of the score from BLEU and RadGraph F1\nbased on their optimized coefficients. It showed better alignment with the evaluation of radiologists than the above four metrics\nand had the Kendall-tau b correlation coefficient of 0.522 for the number of errors annotated by radiologists.\nDiscussion Existing methods have demonstrated the effectiveness of using RL and medical knowledge in improving the\nfactuality of generated radiology reports. However, the medical knowledge are typically incorporated in implicit ways based on\nRL, we consider that future efforts should pay more attention to explicitly incorporating medical knowledge on improving\nthe encoder and decoder of the LLMs. For example, investigating the use knowledge grounded backbone language models\nas encoder37, and developing decoding strategy guided by medical facts55, 87. Moreover, the radiology report generation task\nrequires the combination of information both radiology images and the associated text reports, we believe cross-modality\nvision-language foundation models88 should be explored to improve the faithfulness of radiology report generation methods in\nthe future. For the evaluation metrics, there is only one paper85 as we described above on analyzing the correlation between\nautomatic factuality evaluation metrics and scores of experts based on human annotation. It is necessary to have more efforts on\ndeveloping automatic factuality evaluation metrics, and creating public benchmark datasets to help the meta-evaluation of these\nmetrics.\n4.3 Medical Fact Checking\nAutomatic medical fact-checking aims to detect whether the claim made in certain medical text is true, which is a promising\nmethod for assisting in detecting factual errors and improving the factuality of medical generative methods. Many existing\nefforts have contributed to the creation of medical data resources to facilitate the development of automatic medical fact-\nchecking methods and their evaluation. Kotonya et al.\n89 built the PUBHEALTH dataset with 11.8K public health-related\nclaims with gold-standard fact-checking explanations by journalists. Wadden et al.90 created the SCI-FACT dataset with 1.4K\nclinical medicine-related scientific claims paired with abstracts including their corresponding evidence, and the annotated labels\n(including supports, refutes, and neutral) as well as rationales. Poliak et al91 collected over 2.1K verified question-answer pairs\nrelated to COVID-19 from over 40 trusted websites. Sarrouti et al92 developed HEALTHVER for evidence-based fact-checking\nof health related claims, with 14,330 claim-evidence pairs from online questions on COVID-19, and their label including\nsupports, refutes, and neutral. On a relevant effort, Saakyan et al93 created COVID-Fact with 4,086 real-world claims related to\nthe COVID-19 pandemic, sentence-level evidence for these claims, and their counter claims. Mohr et al94 built a fact-checked\ncorpus with 300 COVID-19-related tweets annotated with their verdict label, biomedical named entities, and supporting\nevidence. Srba et al95 developed a new medical misinformation dataset with 573 manually and more than 51k automatically\nannotated relations between verified claims and 317k news articles, including the claim presence label indicating whether a\nclaim is included in the article and article stance label indicating whether a claim is supported by the article such as supports,\nrefutes, and neutral. Wadhwa et al96 constructed the RedHOT corpus with 22,000 health-related social media posts from Reddit\nannotated with claims, questions, and personal experiences, which can support for identifying health claims and retrieving\nrelated medical literature.\nThere were also efforts on developing automatic medical fact-checking methods with these data resources. Specifically,\nKotonya et al89 proposed an explainable automatic fact-checking method with a classifier for predicting label of the given\n10/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nTable 4. Summary of datasets.\nData Domain Claim Source Negation Method Size\nPUBHEALTH89 Publich health Fact checking and News websites Natural 11,832\nHEALTHVER92 COVID TREC-COVID 97 Natural 14,330\nSCI-FACT90 Biomed S2ORC 98 Human 1,409\nCOVID-Fact93 COVID Reddit Automatic 4,086\nMisinformation95 Medical Fact checking and News websites Human 573\nCovert94 COVID Twitter Human 300\nRedHOT96 Health Reddit Human 22,000\nclain based on pre-trained language models such as BERT, SciBERT 99, BioBERT, and a summarization model based on\nthe BERTSum summarizer100 for generating fact-checking explanations. On the PUBHEALTH dataset, the SciBERT-based\nprediction method achieved the highest macro F1, precision, and accuracy scores, and fact-checking explanation model\nfine-tuned on the PUBHEALTH dataset achieved promising performance. Wadden et al90 proposed the automatic fact-checking\npipeline with the SCI-FACT dataset that retrieves abstracts based on input claims according to the TD-IDF similarity, selects\nrationale sentences and then predicts the labels (SUPPORTS, REFUTES, or NOINFO) of abstracts regarding the given\nclaims with BERT based related language models. They investigated using SciBERT, BioMedRoBERTa, RoBERTa-base, and\nRoBERTa-large as the sentence encoder, where the RoBERTa-large achieves the best performance on label prediction. Wadden\net al101 proposed MULTIVERS for predicting the fact-checking label for a claim and the given evidence abstract, which uses\nLong-former encoder71 to encode the long sequence from the claim and the evidence abstract, and predicts the abstract-level\nfact-checking label aligned to the sentence-level label by multi-task learning. On three medical fact-checking datasets including\nHEALTHVER, COVID-Fact, and SCI-FACT, MULTIVERS showed better performance on the zero-shot and few-shot settings\ncompared with existing methods, due to the weak supervision by the multi-task learning.\nDiscussion Although fact-checking is a promising way for detecting and mitigating the hallucination problem of AI methods,\nwe can see that existing fact-checked datasets only covered limited medical facts in specific domains such as COVID and public\nhealth. Therefore, future efforts can also be spent on developing effective automatic fact-checking methods based on other\nresources with rich medical facts such as medical knowledge bases and plain medical texts. Moreover, there are no methods\nexploring fact-checking on LLMs, which should be a research focus as well given the popularity of LLMs.\n5 Limitations and Future Directions\nWith all the reviews and discussions of existing works on faithful AI in healthcare and medicine above, we will summarize the\noverall limitations of existing studies and discuss future research directions.\n5.1 Limitations\n5.1.1 Datasets\nUnlabeled Data for Self-supervised Learning Many large AI models including LLMs are typically trained with self-\nsupervised learning, which relies on the availability of large-scale unlabeled medical data. However, it is challenging to collect\nlarge amounts of biomedical data in many scenarios due to the various considerations such as privacy and cost, which makes\nthe volume of publicly available biomedical to be much smaller than that of unlabeled data in other application domains such as\ncomputer vision and natural language processing. For example, the unlabeled clinical data used to train the clinical language\nmodels such as ClinicalBERT102 is 3.7GB, while the data size of unlabeled data used to train the large language models in the\ngeneral domain can be up to 45TB.\nAnnotated Data for Supervised Learning and Evaluation Developing faithful medical AI methods and evaluating their\nfactuality also relies on high-quality annotated medical data. However, collecting large-scale high-quality annotated medical\ndata is even more challenging, due to the high cost on both time and expertise. The sizes of existing annotated datasets are\nmostly small (e.g., the sample sizes of existing datasets for medical question summarization are around 1,000103). In addition,\nthere is no publicly available annotated data for supporting the meta-evaluation of automatic evaluation metrics in most medical\ntasks, which makes it challenging to verify the reliability of different automatic evaluation metrics.\nData in Multimodal and Multilingual Many existing medical datasets consist of only a single data modality such as the UK\nBiobank104. There are few multimodal datasets with texts and images, such as MIMIC-CXR consisting of medical radiographs\nand text reports. However, there are many other data modalities such as biosensors, genetic, epigenetic, proteomic, and\nmicrobiome, that have been rarely considered in existing datasets. Moreover, most existing datasets are limited to a single\n11/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nlanguage, where English is the predominantly used language. This can hinder the development of faithful medical AI methods\nin low-resources and rural areas.\n5.1.2 Backbone Models\nBiomedical Domain-Specific Language Models Many existing medical AI methods use biomedical domain-specific\nlanguage models as the backbone and fine-tune them with task-specific datasets for various downstream tasks. However, these\nmodels are typically pre-trained with the biomedical literature texts and a few other types of medical texts such as EHRs\nand radiology reports. Therefore, they can only capture limited medical knowledge and cannot be effectively generalized to\ntasks involving other types of medical data. Moreover, these domain-specific language models are typically small in their\nsizes (measured by the number of parameters, usually less than 1B parameters), which further impacts their performance\nand generalization abilities. Up to now, the largest generative language model in the medical domain PubMedGPT has 2.7B\nparameters, which is far smaller than the scale of large language models in the general domain such as GPT-3 with 175B\nparameters, PaLM with 540B parameters, and the most recent GPT-4 with 100T parameters.\nLarge Generative Language Models Recently, large generative language models in the general domain such as GPT-3,\nPaLM, ChatGPT, GPT-4 et al, have shown amazing abilities in natural language understanding and generation. However, they\nare not trained with data in medical domain, which limits their ability in understanding medical data. Moreover, none of them\nare made publicly available, which hinders the research in using these LLMs for developing faithful medical AI methods. There\nis a recent work37 that aligns the LLM PaLM with the medical domain. Unfortunately, it is still not publicly available.\n5.1.3 Faithful Medical AI Methodologies\nMitigation Methods Although factuality is a critical issue in existing medical AI methods, little efforts has been devoted\nto the improvement of the faithfulness of backbone language models and medical AI methods for downstream tasks. There\nhas been no research investigating the factuality in medical tasks including medical dialogue generation, medical question\nanswering and drug discovery et al. It is clear the factuality problem should attract more attention in future efforts.\nIncorporating Medical Knowledge Existing efforts have proven the effectiveness and importance of improving the factuality\nin both backbone language models and medical AI methods for specific tasks by incorporating medical knowledge. Most\nof them focused on extracting medical knowledge from external biomedical knowledge bases. Recently, there is an effort37\ninvestigating the efficiency of instruction prompt tuning on injecting medical knowledge into LLMs, which relies on human\nfeedback and thus can be expensive and time consuming. Effective incorporation of medical knowledge in efficient and scalable\nways remains a critical challenge.\n5.1.4 Evaluations\nAutomatic Evaluation Metrics Existing automatic evaluation metrics usually calculate the overlap of medical facts such as\nmedical entities and relations between outputs generated by the algorithm and references. They cannot assess and distinguish\ndifferent types of factual errors such as intrinsic and extrinsic errors as introduced in the Section 3.1, which is essential to\nanalyze the cause of factual errors and mitigate them. In addition, the assessment of factuality relies on human evaluations for\nmany tasks such as medical question answering and dialogue generation, where there are no automatic factuality evaluation\nmetrics.\nMeta-evaluation Assessing the effectiveness of automatic evaluation metrics is critical for correctly evaluating the factuality\nof methods. Otherwise, the ineffective automatic evaluation metrics can misguide the optimization and evaluation of methods.\nThere is rare work 85 investigating the meta-evaluation of automatic factuality metrics used in various medical tasks, and\nanalyzing their alignment with domain experts.\n5.2 Future Directions\nWith the above limitations we summarized, in the following We outline promising directions for future research to develop\nfaithful medical AI methods.\n5.2.1 Datasets\nLarge Scale Datasets for Pre-training Large scale public multimodal and multilingual medical datasets should be developed\nin the future for facilitating the pre-training of reliable biomedical language models. We believe it is important to improve the\ngeneralization ability and faithfulness of language models in the medical domain, by pre-training them with various languages\nand modalities of medical data such as text, image, clinical, genetic et al.\n12/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nBenchmark Datasets for Factuality Evaluation More public benchmark datasets for various medical tasks should be\nconstructed to support the development of reliable medical AI methods. Moreover, future efforts should pay attention to\ndeveloping benchmark factuality evaluation datasets in the medical domain to assess the factuality of LLMs, like the TruthfulQA\ndataset105 for evaluating the factuality of GPT-4 in the general domain.\nAnnotated Datasets for Meta-evaluation It is urgent to build domain expert annotated datasets of various medical tasks for\nsupporting analysis of the alignment between automatic factuality evaluation metrics and preferences of domain experts. They\nare critical for future research of developing reliable automatic evaluation metrics and developing effective mitigation methods.\n5.2.2 Backbone Models\nFaithful Biomedical Language Models Reliable backbone models, mainly the biomedical language models, are crucial\nto enhance the factuality of medical AI methods. Different strategies can be explored in the future, such as training with\nmultimodal data, incorporating medical knowledge, and prompt-based pre-training.\nAligning Large Language Models to Medical Domain Large language models have shown predominantly superior perfor-\nmance than pre-trained language models with smaller scales, and have been the new backbone model in the general domain. It\nis crucial to adapt them to the medical domain for their reliable use. Several adaptation strategies can be explored, such as\nfine-tuning with domain-specific data and instruction prompt tuning with human feedback. Med-PaLM37 is a pioneer work in\nthis direction, however it is not publicly accessible.\nEvaluating the Faithfulness of Backbone Models Future efforts should also focus on evaluating and analyzing the\nperformance of LLMs in factuality, which can provide crucial insights on how to improve their faithfulness. To support the\nresearch in this direction, it is important to build a standardized benchmark that can cover a broad variety of critical medical\ntasks and datasets.\n5.2.3 Methodologies\nMedical Knowledge Medical knowledge is critical for alleviating the hallucination of medical AI methods for various tasks.\nFuture efforts should explore effective ways of injecting medical knowledge into medical AI methods, such as encoding medical\nknowledge based on prompt learning106, and explicitly leveraging the medical knowledge to guide the text generation87.\nReinforcement Learning with Human Feedback Reinforcement learning with human feedback (RLHF) has exhibited a\nstrong potential on improving the factuality of LLMs, and GPT-4 has greatly improved its safety and faithfulness by the RLHF\npost-training, when compared to GPT-3.5. More efforts should be devoted to explore the mitigation methods with RLHF in the\nmedical domain. One potential challenge of using RLHF is its reliance on human feedback, which comes with high cost in\nthe medical domain. Therefore, it is an open question on how to apply RLHF in a cost-effective and low-resource manner in\nmedicine and healthcare.\nModel Explainability It is also important to develop explainable medical AI methods, especially LLMs. The explanation can\nplay an important role in alleviating the hallucination problem since it helps to understand and trace causes of factual errors,\nand also makes it easier to assess factual errors and enhance the medical AI faithfulness.\n5.2.4 Evaluation Methods\nEvaluation Guideline It has been observed that the criteria for factuality evaluation with different automatic evaluation\nmetrics and human evaluation used in different research often differ. For example, in radiology report summarization and\ngeneration, two automatic metrics the F1 ChexPert and RadGraph Score were used. F1 ChexPert calculates the overlap of\n14 clinical observations such as \"enlarged cardiom\" and \"cardiomegaly\", between the generated summary and the reference\nsummary, while F1 RadGraph calculates their overlap on medical entities and relations. They focused more on evaluating the\nalignment of different medical facts between outputs and references. It is necessary to have a unified evaluation guideline to\ndefine the detailed standardized evaluation criteria for various medical tasks, which can support the development of unified\nautomatic evaluation metrics and fair comparison of different research studies.\nFine-grained Automatic Evaluation Metrics It is expected that the fine-grained automatic evaluation metrics that can clarify\nand assess different medical factual errors can be explored in future efforts. Assessing the fine-grained factual errors such as\nintrinsic and extrinsic errors of medical AI methods plays an important role in understanding and alleviating their factuality\nproblem. One promising direction is to explore developing fine-grained evaluation metrics with the RLHF.\n6 Conclusions\nThe progress of fundamental AI methods, especially the most recent large language models, provides great opportunities for\nmedical AI, but there is a severe concern about the reliability, safety, and factuality of generated content by medical AI methods.\n13/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \nIn this review, we provide the first comprehensive overview of the faithfulness problem in medical AI, analyzing causes,\nsummarizing mitigation methods and evaluation metrics, discussing challenges and limitations, and outlooking future directions.\nExisting research on investigating the factuality problem in medical AI remains in the initial phase, and there are several\nsignificant challenges to data resources, backbone models, mitigation methods, and evaluation metrics in this research direction.\nIt is clear more future research efforts should be conducted and there are significant opportunities for novel faithful medical\nAI research involving adapting large language models, prompt learning, and reinforcement learning from human feedback et\nal. We hope this review can inspire further research efforts in this direction, as well as serve as a guide for researchers and\npractitioners on the safe use of AI methods in realistic medical practice.\nCompeting Interests\nThe authors declare no competing interests.\nReferences\n1. Yu, K.-H., Beam, A. L. & Kohane, I. S. Artificial intelligence in healthcare. Nat. biomedical engineering 2, 719–731\n(2018).\n2. Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. medicine 25, 44–56\n(2019).\n3. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. Ai in health and medicine. Nat. medicine 28, 31–38 (2022).\n4. LeCun, Y ., Bengio, Y . & Hinton, G. Deep learning.nature 521, 436–444 (2015).\n5. Shen, D., Wu, G. & Suk, H.-I. Deep learning in medical image analysis. Annu. review biomedical engineering 19,\n221–248 (2017).\n6. Kenton, J. D. M.-W. C. & Toutanova, L. K. Bert: Pre-training of deep bidirectional transformers for language understand-\ning. In Proceedings of NAACL-HLT, 4171–4186 (2019).\n7. Wang, B.et al. Pre-trained language models in biomedical domain: A systematic survey.arXiv preprint arXiv:2110.05006\n(2021).\n8. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017).\n9. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y . & Sontag, D. Large language models are zero-shot clinical information\nextractors. arXiv preprint arXiv:2205.12689 (2022).\n10. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning. Nat.\nBiomed. Eng. 1–8 (2022).\n11. Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589 (2021).\n12. Brown, T. et al. Language models are few-shot learners. Adv. neural information processing systems 33, 1877–1901\n(2020).\n13. OpenAI. Chatgpt. https://openai.com/blog/chatgpt (2022).\n14. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n15. Kung, T. H. et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language\nmodels. PLOS Digit. Heal. 2, e0000198 (2023).\n16. Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259–265 (2023).\n17. van Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R. & Bockting, C. L. Chatgpt: five priorities for research. Nature 614,\n224–226 (2023).\n18. Li, W. et al. Faithfulness in natural language generation: A systematic survey of analysis, evaluation and optimization\nmethods. arXiv preprint arXiv:2203.05227 (2022).\n19. Aggarwal, R. et al. Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis.\nNPJ digital medicine 4, 65 (2021).\n20. Weidinger, L. et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359 (2021).\n21. Ji, Z. et al. Survey of hallucination in natural language generation. ACM Comput. Surv. (2022).\n14/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \n22. Zhang, Y ., Merck, D., Tsai, E., Manning, C. D. & Langlotz, C. Optimizing the factual correctness of a summary: A study\nof summarizing radiology reports. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 5108–5120 (2020).\n23. Luo, R. et al. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings Bioinforma.\n23 (2022).\n24. Maynez, J., Narayan, S., Bohnet, B. & McDonald, R. On faithfulness and factuality in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 1906–1919 (2020).\n25. Devaraj, A., Wallace, B. C., Marshall, I. J. & Li, J. J. Paragraph-level simplification of medical texts. In Proceedings\nof the conference. Association for Computational Linguistics. North American Chapter. Meeting, vol. 2021, 4972 (NIH\nPublic Access, 2021).\n26. Miura, Y ., Zhang, Y ., Tsai, E., Langlotz, C. & Jurafsky, D. Improving factual completeness and consistency of image-to-\ntext radiology report generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, 5288–5304 (2021).\n27. Moradi, M., Blagec, K., Haberl, F. & Samwald, M. Gpt-3 models are poor few-shot learners in the biomedical domain.\narXiv preprint arXiv:2109.02555 (2021).\n28. Gutiérrez, B. J. et al. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint\narXiv:2203.08410 (2022).\n29. Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics\n36, 1234–1240 (2020).\n30. Gu, Y .et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions\non Comput. for Healthc. (HEALTH) 3, 1–23 (2021).\n31. Antaki, F., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating the performance of chatgpt in ophthalmology: An\nanalysis of its successes and shortcomings. medRxiv 2023–01 (2023).\n32. Bengio, S., Vinyals, O., Jaitly, N. & Shazeer, N. Scheduled sampling for sequence prediction with recurrent neural\nnetworks. Adv. neural information processing systems 28 (2015).\n33. Wang, C. & Sennrich, R. On exposure bias, hallucination and domain shift in neural machine translation. In 2020 Annual\nConference of the Association for Computational Linguistics, 3544–3552 (Association for Computational Linguistics\n(ACL), 2020).\n34. Lee, N. et al. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information\nProcessing Systems.\n35. Yuan, Z., Liu, Y ., Tan, C., Huang, S. & Huang, F. Improving biomedical pretrained language models with knowledge. In\nProceedings of the 20th Workshop on Biomedical Language Processing, 180–190 (2021).\n36. Jha, K. & Zhang, A. Continual knowledge infusion into pre-trained biomedical language models. Bioinformatics 38,\n494–502 (2022).\n37. Singhal, K. et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138 (2022).\n38. Chowdhery, A. et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n39. Chung, H. W. et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n40. Jin, D. et al. What disease does this patient have? a large-scale open domain question answering dataset from medical\nexams. Appl. Sci. 11, 6421 (2021).\n41. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Medmcqa: A large-scale multi-subject multi-choice dataset for medical\ndomain question answering. In Conference on Health, Inference, and Learning, 248–260 (PMLR, 2022).\n42. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. & Lu, X. Pubmedqa: A dataset for biomedical research question answering. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), 2567–2577 (2019).\n43. Hendrycks, D. et al. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020).\n44. Bolton, E. et al. Pubmedgpt 2.7b. https://github.com/stanford-crfm/BioMedLM (2022).\n45. Taylor, R. et al. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 (2022).\n15/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \n46. Zakka, C., Chaurasia, A., Shad, R. & Hiesinger, W. Almanac: Knowledge-grounded language models for clinical\nmedicine. arXiv preprint arXiv:2303.01229 (2023).\n47. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical challenge problems.\narXiv preprint arXiv:2303.13375 (2023).\n48. Huang, S. et al. Language is not all you need: Aligning perception with language models.arXiv preprint arXiv:2302.14045\n(2023).\n49. Afantenos, S., Karkaletsis, V . & Stamatopoulos, P. Summarization from medical documents: a survey.Artif. intelligence\nmedicine 33, 157–177 (2005).\n50. Zhang, Y ., Ding, D. Y ., Qian, T., Manning, C. D. & Langlotz, C. P. Learning to summarize radiology findings. In\nProceedings of the Ninth International Workshop on Health Text Mining and Information Analysis, 204–213 (2018).\n51. Irvin, J. et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings\nof the AAAI conference on artificial intelligence, vol. 33, 590–597 (2019).\n52. Delbrouck, J.-B., Varma, M. & Langlotz, C. P. Toward expanding the scope of radiology report summarization to multiple\nanatomies and modalities. arXiv preprint arXiv:2211.08584 (2022).\n53. Johnson, A. E. et al. Mimic-iii, a freely accessible critical care database. Sci. data 3, 1–9 (2016).\n54. Lin, C.-Y . Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, 74–81 (2004).\n55. Xie, Q., Zhou, J., Peng, Y . & Wang, F. Factreranker: Fact-guided reranker for faithful radiology report summarization.\narXiv preprint arXiv:2303.08335 (2023).\n56. DeYoung, J., Beltagy, I., van Zuylen, M., Kuehl, B. & Wang, L. Ms^2: Multi-document summarization of medical studies.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 7494–7513 (2021).\n57. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and\ncomprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7871–7880\n(2020).\n58. Wallace, B. C., Saha, S., Soboczenski, F. & Marshall, I. J. Generating (factual?) narrative summaries of rcts: Experiments\nwith neural multi-document summarization. AMIA Summits on Transl. Sci. Proc. 2021, 605 (2021).\n59. Alambo, A., Banerjee, T., Thirunarayan, K. & Raymer, M. Entity-driven fact-aware abstractive summarization of\nbiomedical literature. In 2022 26th International Conference on Pattern Recognition (ICPR), 613–620 (IEEE, 2022).\n60. Yadav, S., Gupta, D., Abacha, A. B. & Demner-Fushman, D. Reinforcement learning for abstractive question sum-\nmarization with question-aware semantic rewards. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2:\nShort Papers), 249–255 (2021).\n61. Chintagunta, B., Katariya, N., Amatriain, X. & Kannan, A. Medically aware gpt-3 as a data generator for medical\ndialogue summarization. In Machine Learning for Healthcare Conference, 354–372 (PMLR, 2021).\n62. Liu, F. et al. Retrieve, reason, and refine: Generating accurate and faithful patient instructions. In Advances in Neural\nInformation Processing Systems.\n63. Zhang, T., Kishore, V ., Wu, F., Weinberger, K. Q. & Artzi, Y . Bertscore: Evaluating text generation with bert. In\nInternational Conference on Learning Representations.\n64. Kry´sci´nski, W., McCann, B., Xiong, C. & Socher, R. Evaluating the factual consistency of abstractive text summarization.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 9332–9346\n(2020).\n65. Yuan, W., Neubig, G. & Liu, P. Bartscore: Evaluating generated text as text generation.Adv. Neural Inf. Process. Syst. 34,\n27263–27277 (2021).\n66. Jain, S. et al. Radgraph: Extracting clinical entities and relations from radiology reports. In Thirty-fifth Conference on\nNeural Information Processing Systems Datasets and Benchmarks Track (Round 1).\n67. Otmakhova, J., Verspoor, K., Baldwin, T. & Lau, J. H. The patient is more dead than alive: exploring the current state\nof the multi-document summarisation of the biomedical literature. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), 5098–5111 (2022).\n16/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \n68. Otmakhova, J., Verspoor, K., Baldwin, T., Yepes, A. J. & Lau, J. H. M3: Multi-level dataset for multi-document\nsummarisation of medical studies. In Findings of the Association for Computational Linguistics: EMNLP 2022, 3887–\n3901 (2022).\n69. Adams, G., Zucker, J. & Elhadad, N. A meta-evaluation of faithfulness metrics for long-form hospital-course summariza-\ntion. arXiv preprint arXiv:2303.03948 (2023).\n70. Laban, P., Schnabel, T., Bennett, P. N. & Hearst, M. A. Summac: Re-visiting nli-based models for inconsistency detection\nin summarization. Transactions Assoc. for Comput. Linguist. 10, 163–177 (2022).\n71. Beltagy, I., Peters, M. E. & Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150\n(2020).\n72. Schardt, C., Adams, M. B., Owens, T., Keitz, S. & Fontelo, P. Utilization of the pico framework to improve searching\npubmed for clinical questions. BMC medical informatics decision making 7, 1–6 (2007).\n73. Trienes, J., Schlötterer, J., Schildhaus, H.-U. & Seifert, C. Patient-friendly clinical notes: Towards a new text simplification\ndataset. In Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), 19–27 (2022).\n74. Lu, J., Li, J., Wallace, B. C., He, Y . & Pergola, G. Napss: Paragraph-level medical text simplification via narrative\nprompting and sentence-matching summarization. arXiv preprint arXiv:2302.05574 (2023).\n75. Jeblick, K. et al. Chatgpt makes medicine easy to swallow: An exploratory case study on simplified radiology reports.\narXiv preprint arXiv:2212.14882 (2022).\n76. Lyu, Q. et al. Translating radiology reports into plain language using chatgpt and gpt-4 with prompt learning: Promising\nresults, limitations, and potential. arXiv preprint arXiv:2303.09038 (2023).\n77. Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L. & Chissom, B. S. Derivation of new readability formulas (automated\nreadability index, fog count and flesch reading ease formula) for navy enlisted personnel. Tech. Rep., Naval Technical\nTraining Command Millington TN Research Branch (1975).\n78. Senter, R. & Smith, E. A. Automated readability index. Tech. Rep., Cincinnati Univ OH (1967).\n79. Xu, W., Napoles, C., Pavlick, E., Chen, Q. & Callison-Burch, C. Optimizing statistical machine translation for text\nsimplification. Transactions Assoc. for Comput. Linguist. 4, 401–415 (2016).\n80. Devaraj, A., Sheffield, W., Wallace, B. C. & Li, J. J. Evaluating factuality in text simplification. InProceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 7331–7345 (2022).\n81. Nishino, T. et al. Reinforcement learning with imbalanced dataset for data-to-text medical report generation. In Findings\nof the Association for Computational Linguistics: EMNLP 2020, 2223–2236 (2020).\n82. Delbrouck, J.-B. et al. Improving the factual correctness of radiology report generation with semantic rewards. In\nFindings of the Association for Computational Linguistics: EMNLP 2022, 4348–4360 (2022).\n83. Nishino, T. et al. Factual accuracy is not enough: Planning consistent description order for radiology report generation.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 7123–7138 (2022).\n84. Liu, G. et al. Clinically accurate chest x-ray report generation. In Machine Learning for Healthcare Conference, 249–269\n(PMLR, 2019).\n85. Yu, F.et al. Evaluating progress in automatic chest x-ray radiology report generation. medRxiv 2022–08 (2022).\n86. Papineni, K., Roukos, S., Ward, T. & Zhu, W.-J. Bleu: a method for automatic evaluation of machine translation. In\nProceedings of the 40th annual meeting of the Association for Computational Linguistics, 311–318 (2002).\n87. Wan, D., Liu, M., McKeown, K., Dreyer, M. & Bansal, M. Faithfulness-aware decoding strategies for abstractive\nsummarization. arXiv preprint arXiv:2303.03278 (2023).\n88. Radford, A., Sutskever, I., Kim, J. W., Krueger, G. & Agarwal, S. Clip: Learning an image classifier from language and\npixels. https://openai.com/research/clip (2021).\n89. Kotonya, N. & Toni, F. Explainable automated fact-checking for public health claims. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), 7740–7754 (2020).\n90. Wadden, D. et al. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 7534–7550 (2020).\n91. Poliak, A. et al. Collecting verified covid-19 question answer pairs. In Proceedings of the 1st Workshop on NLP for\nCOVID-19 (Part 2) at EMNLP 2020(2020).\n17/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint \n92. Sarrouti, M., Abacha, A. B., M’rabet, Y . & Demner-Fushman, D. Evidence-based fact-checking of health-related claims.\nIn Findings of the Association for Computational Linguistics: EMNLP 2021, 3499–3512 (2021).\n93. Saakyan, A., Chakrabarty, T. & Muresan, S. Covid-fact: Fact extraction and verification of real-world claims on covid-19\npandemic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2116–2129 (2021).\n94. Mohr, I., Wührl, A. & Klinger, R. Covert: A corpus of fact-checked biomedical covid-19 tweets. In Proceedings of the\nThirteenth Language Resources and Evaluation Conference, 244–257 (2022).\n95. Srba, I. et al. Monant medical misinformation dataset: Mapping articles to fact-checked claims. In Proceedings of the\n45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2949–2959 (2022).\n96. Wadhwa, S., Khetan, V ., Amir, S. & Wallace, B. Redhot: A corpus of annotated medical questions, experiences, and\nclaims on social media. arXiv preprint arXiv:2210.06331 (2022).\n97. V oorhees, E.et al. Trec-covid: constructing a pandemic information retrieval test collection. In ACM SIGIR Forum,\nvol. 54, 1–12 (ACM New York, NY , USA, 2021).\n98. Lo, K., Wang, L. L., Neumann, M., Kinney, R. & Weld, D. S. S2orc: The semantic scholar open research corpus. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 4969–4983 (2020).\n99. Beltagy, I., Lo, K. & Cohan, A. Scibert: A pretrained language model for scientific text. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 3615–3620 (2019).\n100. Liu, Y . & Lapata, M. Text summarization with pretrained encoders. InProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3730–3740 (2019).\n101. Wadden, D. et al. Multivers: Improving scientific claim verification with weak supervision and full-document context. In\nFindings of the Association for Computational Linguistics: NAACL 2022, 61–76 (2022).\n102. Alsentzer, E. et al. Publicly available clinical bert embeddings. In Proceedings of the 2nd Clinical Natural Language\nProcessing Workshop, 72–78 (2019).\n103. Abacha, A. B. & Demner-Fushman, D. On the summarization of consumer health questions. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, 2228–2234 (2019).\n104. Sudlow, C. et al. Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of\nmiddle and old age. PLoS medicine 12, e1001779 (2015).\n105. Lin, S., Hilton, J. & Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 3214–3252 (2022).\n106. Wang, J. et al. Knowledge prompting in pre-trained language model for natural language understanding. InProceedings of\nthe 2022 Conference on Empirical Methods in Natural Language Processing, 3164–3177 (Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 2022).\n18/18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 19, 2023. ; https://doi.org/10.1101/2023.04.18.23288752doi: medRxiv preprint ",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6463651657104492
    },
    {
      "name": "Generative grammar",
      "score": 0.595320463180542
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46718576550483704
    },
    {
      "name": "Multimodality",
      "score": 0.4258050322532654
    },
    {
      "name": "Computer science",
      "score": 0.4181436598300934
    },
    {
      "name": "Engineering ethics",
      "score": 0.38443809747695923
    },
    {
      "name": "Data science",
      "score": 0.35447758436203003
    },
    {
      "name": "Management science",
      "score": 0.3275216817855835
    },
    {
      "name": "Political science",
      "score": 0.205532968044281
    },
    {
      "name": "Engineering",
      "score": 0.12350419163703918
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ],
  "cited_by": 28
}