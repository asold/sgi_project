{
    "title": "Hyperspectral Image Classification via Spectral Pooling and Hybrid Transformer",
    "url": "https://openalex.org/W4296830425",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2128819664",
            "name": "Chen Ma",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2108913913",
            "name": "Junjun Jiang",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100281165",
            "name": "Huayi Li",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2112257469",
            "name": "Xiaoguang Mei",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2222046485",
            "name": "Chengchao Bai",
            "affiliations": [
                "Harbin Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2942454403",
        "https://openalex.org/W4285187901",
        "https://openalex.org/W2767805377",
        "https://openalex.org/W2008847349",
        "https://openalex.org/W2136251662",
        "https://openalex.org/W2058795991",
        "https://openalex.org/W2158400785",
        "https://openalex.org/W2500751094",
        "https://openalex.org/W2799390666",
        "https://openalex.org/W2809113079",
        "https://openalex.org/W1521436688",
        "https://openalex.org/W2614256707",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2888119354",
        "https://openalex.org/W2764276316",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2971432438",
        "https://openalex.org/W3171853541",
        "https://openalex.org/W3128776197",
        "https://openalex.org/W4313229413",
        "https://openalex.org/W2004104348",
        "https://openalex.org/W6840360099",
        "https://openalex.org/W4214588794",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W4312950730",
        "https://openalex.org/W4289522819",
        "https://openalex.org/W4292553536",
        "https://openalex.org/W4281934405",
        "https://openalex.org/W4281256697",
        "https://openalex.org/W4225829036",
        "https://openalex.org/W3186033197",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W2101711129",
        "https://openalex.org/W4285124290",
        "https://openalex.org/W3103753223"
    ],
    "abstract": "Hyperspectral images (HSIs) contain spatially structured information and pixel-level sequential spectral attributes. The continuous spectral features contain hundreds of wavelength bands and the differences between spectra are essential for achieving fine-grained classification. Due to the limited receptive field of backbone networks, convolutional neural networks (CNNs)-based HSI classification methods show limitations in modeling spectral-wise long-range dependencies with fixed kernel size and a limited number of layers. Recently, the self-attention mechanism of transformer framework is introduced to compensate for the limitations of CNNs and to mine the long-term dependencies of spectral signatures. Therefore, many joint CNN and Transformer architectures for HSI classification have been proposed to obtain the merits of both networks. However, these architectures make it difficult to capture spatial–spectral correlation and CNNs distort the continuous nature of the spectral signature because of the over-focus on spatial information, which means that the transformer can easily encounter bottlenecks in modeling spectral-wise similarity and long-range dependencies. To address this problem, we propose a neighborhood enhancement hybrid transformer (NEHT) network. In particular, a simple 2D convolution module is adopted to achieve dimensionality reduction while minimizing the distortion of the original spectral distribution by stacked CNNs. Then, we extract group-wise spatial–spectral features in a parallel design to enhance the representation capability of each token. Furthermore, a feature fusion strategy is introduced to increase subtle discrepancies of spectra. Finally, the self-attention of transformer is employed to mine the long-term dependencies between the enhanced feature sequences. Extensive experiments are performed on three well-known datasets and the proposed NEHT network shows superiority over state-of-the-art (SOTA) methods. Specifically, our proposed method outperforms the SOTA method by 0.46%, 1.05% and 0.75% on average in overall accuracy, average accuracy and kappa coefficient metrics.",
    "full_text": null
}