{
  "title": "Efficient GPT-4V level multimodal large language model for deployment on edge devices",
  "url": "https://openalex.org/W4411892026",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2062353949",
      "name": "Yuan Yao",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Tsinghua University",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2142730550",
      "name": "Tianyu Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2004093727",
      "name": "Ao Zhang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A3029710514",
      "name": "Chongyi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2274079271",
      "name": "Junbo Cui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137607705",
      "name": "Hongji Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103508815",
      "name": "Tianchi Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005825116",
      "name": "Chi Chen",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2107442531",
      "name": "Haoyu Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2098285428",
      "name": "Zhao Weilin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2111318257",
      "name": "Zhihui He",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2150604126",
      "name": "Qianyu Chen",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2099238249",
      "name": "Rong-Hua Zhou",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Zhensheng Zou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095846531",
      "name": "Hao-Ye Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2998424002",
      "name": "Shengding Hu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2008445811",
      "name": "Zhi Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084208236",
      "name": "Jie Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2986593302",
      "name": "Guoyang Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123001993",
      "name": "Dahai Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2062353949",
      "name": "Yuan Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142730550",
      "name": "Tianyu Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2004093727",
      "name": "Ao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3029710514",
      "name": "Chongyi Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2274079271",
      "name": "Junbo Cui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137607705",
      "name": "Hongji Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103508815",
      "name": "Tianchi Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005825116",
      "name": "Chi Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107442531",
      "name": "Haoyu Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098285428",
      "name": "Zhao Weilin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111318257",
      "name": "Zhihui He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150604126",
      "name": "Qianyu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099238249",
      "name": "Rong-Hua Zhou",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Zhensheng Zou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095846531",
      "name": "Hao-Ye Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2998424002",
      "name": "Shengding Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2008445811",
      "name": "Zhi Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2084208236",
      "name": "Jie Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2986593302",
      "name": "Guoyang Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123001993",
      "name": "Dahai Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4392678661",
    "https://openalex.org/W6858268588",
    "https://openalex.org/W4395687490",
    "https://openalex.org/W4405595839",
    "https://openalex.org/W4386148428",
    "https://openalex.org/W4392873997",
    "https://openalex.org/W4404002404",
    "https://openalex.org/W6870687016",
    "https://openalex.org/W4395474395",
    "https://openalex.org/W4393300611",
    "https://openalex.org/W4392970322",
    "https://openalex.org/W4399114975",
    "https://openalex.org/W4389362530",
    "https://openalex.org/W4402727405",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W4382142077",
    "https://openalex.org/W4389156660",
    "https://openalex.org/W4402716477",
    "https://openalex.org/W4405399726",
    "https://openalex.org/W2979382951",
    "https://openalex.org/W6761041305",
    "https://openalex.org/W3120043490",
    "https://openalex.org/W2962735233",
    "https://openalex.org/W4392627835",
    "https://openalex.org/W4392011716",
    "https://openalex.org/W4400024950",
    "https://openalex.org/W6917636526",
    "https://openalex.org/W4402727764",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4394709684",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W6966618176",
    "https://openalex.org/W4283218507",
    "https://openalex.org/W3156892778",
    "https://openalex.org/W4320482280",
    "https://openalex.org/W2343052201",
    "https://openalex.org/W6840287690",
    "https://openalex.org/W4392426159",
    "https://openalex.org/W4402671151",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2189070436",
    "https://openalex.org/W3212452301",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W6640773114",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W2788643321",
    "https://openalex.org/W2963622213",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2947312908",
    "https://openalex.org/W4312846625",
    "https://openalex.org/W2966317026",
    "https://openalex.org/W4296605665",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W4388444960",
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2741631785",
    "https://openalex.org/W4382323090",
    "https://openalex.org/W3169530364",
    "https://openalex.org/W3176186248",
    "https://openalex.org/W4386076283",
    "https://openalex.org/W3004268082",
    "https://openalex.org/W2988326850",
    "https://openalex.org/W6762955845",
    "https://openalex.org/W3173585224",
    "https://openalex.org/W2963420691",
    "https://openalex.org/W2766732270",
    "https://openalex.org/W4285255856",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W4213213306",
    "https://openalex.org/W6809008970",
    "https://openalex.org/W3160293046",
    "https://openalex.org/W3201693581",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W4385990960",
    "https://openalex.org/W2307512708",
    "https://openalex.org/W6698632916",
    "https://openalex.org/W4249013746",
    "https://openalex.org/W6745764446",
    "https://openalex.org/W4382766522",
    "https://openalex.org/W4387323774",
    "https://openalex.org/W4388962905",
    "https://openalex.org/W4404575065",
    "https://openalex.org/W2948672349",
    "https://openalex.org/W4391987298",
    "https://openalex.org/W4378446837",
    "https://openalex.org/W4389519291",
    "https://openalex.org/W6852874933"
  ],
  "abstract": "Abstract Multimodal large language models have revolutionized AI research and industry, paving the way toward the next milestone. However, their large sizes and high computational costs restrict deployment to cloud servers, limiting use in mobile, offline, energy-sensitive, or privacy-critical scenarios. We present MiniCPM-V, efficient models for edge devices that integrate advancements in architecture, training, and data. The 8B model outperforms GPT-4V, Gemini Pro, and Claude 3 across 11 public benchmarks, processes high-resolution images at any aspect ratio, achieves robust optical character recognition, exhibits low hallucination rates, and supports over 30 languages while running efficiently on mobile phones. This progress reflects a broader trend: The sizes for high-performing models are rapidly decreasing alongside growing edge computation capacity, enabling advanced multimodal models to operate locally on consumer hardware. Such developments unlock applications across diverse real-world scenarios, from enhanced mobile AI to privacy-preserving solutions, marking a critical step toward democratizing powerful multimodal intelligence.",
  "full_text": "Article https://doi.org/10.1038/s41467-025-61040-5\nEfﬁcient GPT-4V level multimodal large\nlanguage model for deployment on edge\ndevices\nYuan Yao1,2,3,T i a n y uY u1, Ao Zhang3, Chongyi Wang4,J u n b oC u i4, Hongji Zhu4,\nTianchi Cai4,C h iC h e n1,H a o y uL i1, Weilin Zhao1, Zhihui He1, Qianyu Chen5,\nRonghua Zhou4, Zhensheng Zou4, Haoye Zhang1, Shengding Hu1, Zhi Zheng4,\nJie Zhou4,J i eC a i4,X uH a n1, Guoyang Zeng4,D a h a iL i4, Zhiyuan Liu 1 &\nMaosong Sun 1\nMultimodal large language models have revolutionized AI research and\nindustry, paving the way toward the nextmilestone. However, their large sizes\nand high computational costs restrict deployment to cloud servers, limiting\nuse in mobile, ofﬂine, energy-sensitive, or privacy-critical scenarios. We pre-\nsent MiniCPM-V, efﬁcient models for edge devices that integrate advance-\nments in architecture, training, and data. The 8B model outperforms GPT-4V,\nGemini Pro, and Claude 3 across 11 public benchmarks, processes high-\nresolution images at any aspect ratio, achieves robust optical character\nrecognition, exhibits low hallucination rates, and supports over 30 languages\nwhile running efﬁciently on mobile phones. This progress reﬂects a broader\ntrend: The sizes for high-performing models are rapidly decreasing alongside\ngrowing edge computation capacity, enabling advanced multimodal models\nto operate locally on consumer hardware. Such developments unlock appli-\ncations across diverse real-world scenarios, from enhanced mobile AI to\nprivacy-preserving solutions, markinga critical step toward democratizing\npowerful multimodal intelligence.\nThe rapid development of Multimodal Large Language Models\n(MLLMs)1– 11 has brought an impressive surge in multimodal capabilities\nin understanding, reasoning and interaction. This has not only funda-\nmentally reshaped the landscape of AI research and industry, but also\nshed light on a promising path towards the next AI milestone. How-\never, current MLLMs are still far from being practical in real-world\napplications. One of the most predominant challenges is the heavy\ncomputational burdens imposed by the massive number of para-\nmeters of MLLMs. As a result, most MLLMs can only be deployed on\nhigh-performing cloud servers, leading to signiﬁcant energy con-\nsumption and carbon emissions. This limitation signiﬁcantly con-\nstrains potential application scopes such as on mobile devices, energy-\nsensitive scenarios, of ﬂine settings without stable network\nconnections, and privacy/security protective scenarios for both per-\nsonal and industrial users.\nIn light of these limitations, there is a growing interest in exploring\nmore efﬁcient lightweight MLLMs\n1,3,11,12 that can run on edge devices.\nEdge scenarios encompass a broader scope of equipment, including\nmobile phones, personal computers, vehicles and robotics, etc., which\nare ubiquitous in users’ daily lives and are experiencing rapid\nadvancements in computation capacities. On-device MLLMs provide a\npromising solution towards more practical applications due to their\nbroader usage scope, better computation efﬁciency, more robust\nofﬂine behaviors, and better privacy/security protection.\nHowever, developing capable on-device MLLMs is challenging\ndue to signiﬁcantly constrained parameter and inference computation\nReceived: 14 January 2025\nAccepted: 11 June 2025\nCheck for updates\n1Tsinghua University, Beijing, China.2Shanghai Qi Zhi Institute, Shanghai, China.3National University of Singapore, Singapore, Singapore.4ModelBest Inc.,\nBeijing, China.5The Chinese University of Hong Kong, Hong Kong, China.e-mail: liuzy@tsinghua.edu.cn; sms@tsinghua.edu.cn\nNature Communications|         (2025) 16:5509 1\n1234567890():,;\n1234567890():,;\nbudgets. As a result, more careful architecture designs and training\nrecipes are required to fully unleash the potential of on-device MLLMs.\nIn this work, we present MiniCPM-V, a series of efﬁcient MLLMs\ndeployable on edge devices. The philosophy of MiniCPM-V is to\nachieve a good balance between performance and efﬁciency, a more\nimportant objective in real-world applications. From February to May\nin 2024, we unveiled three models: (1) In February, we launched\nMiniCPM-V 1.0 2B, an early prototype of on-device MLLMs. (2) In April,\nwe released MiniCPM-V 2.0 2B, which outperforms strong larger\nMLLMs such as Qwen-VL 9B\n7,C o g V L M1 7 B5,a n dY i - V L3 4 B13.T h i s\niteration also introduces support for high-resolution image perception\nand exhibits promising OCR capabilities. (3) Most recently in May, we\nintroduced MiniCPM-Llama3-V 2.5 8B, which outperforms proprietary\nGPT-4V-1106, Gemini Pro and Claude 3 on the OpenCompass evalua-\ntion. Noteworthy features of this model include strong OCR capability,\nhigh-resolution image perception, trustworthy behavior, multilingual\nsupport, and efﬁcient edge deployment optimization. The capabilities\nof on-device MLLMs have been growing even stronger in our later\nreleases since May 2024.\nMore importantly, MiniCPM-V can be viewed as a representative\nexample of a promising miniaturization trend of MLLMs. Figure1\nsummarizes the recent development of MLLMs\n3,12,14 in terms of per-\nformance, parameters and release time. We observe an interesting\ntrend akin to Moore’sL a w\n15 indicated by the red line: the sizes of\nmodels reaching GPT-4V level performance are rapidly decreasing\nover time. This phenomenon could perhaps be called Moore’s Law of\nMLLMs. Simultaneously, the computational capacity of edge devices\nsuch as phones and personal computers is steadily increasing (qua-\nlitatively depicted by the blue line). The convergence of these two\ntrends indicates usable (e.g., GPT-4V level) MLLMs deployable on\nedge devices are soon within reach, opening up broader possibilities\nand beneﬁting more application scenarios in the near future. From a\nhistorical perspective of human technology development, this trend\ncan also be viewed as the human pursuit of miniaturization of state-\nof-the-art technologies, which has been repeatedly witnessed in\nother science and technologyﬁelds. For example, in aerospace, the\nlatest SpaceX Raptor 2 rocket engine can achieve a strong thrust of\n2,256 kN with a mass of 1.6 tons, whereas 20 years ago, the RD-0750\nrocket engine could only achieve a thrust of 1,413 kN with a mass\nexceeding 4 tons\n16.\nMiniCPM-V Series Techniques\nIn this paper, we will take MiniCPM-Llama3-V 2.5 as an example, and\nsystematically introduce the notable features of MiniCPM-V series and\nthe key techniques behind them:\n Leading Performance. MiniCPM-Llama3-V 2.5 achieves better\nperformance than GPT-4V-1106, Gemini Pro and Claude 3 on\nOpenCompass collection, a comprehensive evaluation over 11\npopular benchmarks. This is jointly contributed by its careful\ndesign in architecture, data and training recipes, which we will\ndetail in the following.\n Strong OCR Capability. MiniCPM-Llama3-V 2.5 outperforms GPT-\n4V, Gemini Pro and Qwen-VL-Max on OCRBench. It also supports\nhigh-utility functions such as table-to-markdown conversion and\nfull OCR content transcription. These are largely attributed to the\n1.8M pixel high-resolution (e.g., 1344 × 1344) image perception\ntechnique across any aspect ratios\n17 of MiniCPM-Llama3-V 2.5.\n Trustworthy Behavior. Based on the RLAIF-V 18 and RLHF-V19\ntechniques that align MLLM behaviors from high-quality AI/\nhuman feedback, MiniCPM-Llama3-V 2.5 exhibits more trust-\nworthy behaviors, achieving lower hallucination rates than GPT-\n4V-1106 on Object HalBench.\nFig. 1 | Moore’s Law for MLLM? Trends of MLLM development in terms of time\n(x-axis), model size (y-axis), and performance (color).The red line shows the\ndecreasing model sizes for achieving GPT-4V level performance, while the blue line\nrepresents the growing edge device computation capacity. This jointly shows that\nGPT-4V level MLLMs deployed on edge devices are becoming increasingly possible,\nunlocking a wider spectrum of real-world AI applications in the near future.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 2\n Multilingual Support.I n s p i r e db yt h eﬁndings from VisCPM8,t h e\nintegration of multilingual LLM signiﬁcantly alleviates the heavy\nreliance on multimodal training data in low-resource languages.\nBased on the foundation, a lightweight multilingual multimodal\ninstruction tuning helps MiniCPM-Llama3-V 2.5 generalize its\nmultimodal capabilities to more than 30 languages.\n Efﬁcient Edge Deployment. We systematically integrate a suite of\non-device optimization techniques, encompassing quantization,\nmemory optimization, compilation optimization and NPU accel-\neration, enabling efﬁcient deployment on edge devices.\nWe hope MiniCPM-V series can serve as an example for unveiling\nthe potential of on-device MLLMs, and help draw more attention to\npromote the research in this direction. Following Moore’sL a wf o r\nMLLM, we believe there will be increasingly powerful on-device MLLMs\nwith reduced sizes, bringing efﬁcient, safe, and trustworthy AI services\non devices soon.\nResults\nOverview of MiniCPM-V\nAs shown in Fig.2b, MiniCPM-V comprises three key modules: the\nvisual encoder, compression layer, and LLM. The input image isﬁrst\nencoded by a visual encoder, utilizing the adaptive visual encoding\napproach. The visual tokens are then compressed by the compression\nlayer, which adopts a perceiver resampler structure with one layer\ncross-attention. Finally, the compressed visual tokens, along with the\ntext input, are fed into the LLM for conditional text generation.\nEncoding high-resolution images poses two major challenges. In\nterms of efﬁciency, directly encoding high-resolution images results in\nan excessive number of visual tokens, rendering it computationally\nFig. 2 | Overview of MiniCPM-V. aConventional visual encoding requires a large\nnumber of tokens when encoding high-resolution images. Our proposed adaptive\nvisual encoding strategy employs adaptive image partitioning and compressed\nencoding, signiﬁcantly reducing computational costs when processing high-\nresolution images.b Overall structure presents the architecture of the model\nincluding the visual encoder, shared compression layer, and LLM.c The progressive\nmultimodal learning strategy is applied to train MiniCPM-V, encompassing three\nphases: the pre-training phase, supervisedﬁne-tuning phase, and alignment phase.\nd RLAIF-V framework for hallucination reduction. (1) Response generation pro-\nduces multiple responses for an instruction using the policy model. (2) Feedback\ncollection evaluates the correctness of each response in a divide-and-conquer\nfashion. (3) DPO optimizes the model on the preference dataset.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 3\nprohibitive for edge devices. In terms of effectiveness, the consider-\nable discrepancy between the image resolution and the resolution\nemployed during ViT pre-training can lead to out-of-distribution pro-\nblems and therefore substantially degrade encoding performance. To\naddress the challenges, we take advantage of the adaptive visual\nencoding strategy\n17 as shown in Fig.2a. To handle the high-resolution\nimages with different aspect ratios, we divide images into slices, where\neach slice better matches ViT’s pre-training setting in terms of reso-\nlution and aspect ratio. Each image is divided into a maximum of 10\nslices, supporting 1.8 million pixels (e.g., 1344 × 1344 resolution) at\nmost in total during encoding, which covers most real-world applica-\ntion scenarios. Then we adjust each slice by resizing it proportionally\nso that the resultant area size matches ViT pre-training area size, and\ninterpolate the ViT’s position embeddings to adapt to the slice’s aspect\nratio. After visual encoding, each slice is encoded into 1,024 tokens,\nwhere 10 slices can yield over 10k tokens collectively. To manage this\nhigh token count, we employ a compression module comprising of\none-layer cross-attention and a moderate number of queries, with 2D\npositions informed\n7. In practice, the visual tokens of each slice are\ncompressed into 64 queries for MiniCPM V1&2 and 96 tokens for\nMiniCPM-Llama3-V 2.5 through this layer. Compared with other\nMLLMs with competitive performance, the signiﬁcantly smaller num-\nber of visual tokens in MiniCPM-V series enables superior efﬁciency in\nterms of GPU memory consumption, inference speed,ﬁrst-token\nlatency and power consumption, making it more friendly to wider\napplication scopes and communities. Finally, we introduce a spatial\nschema inspired by\n20 to indicate each slice’s position relative to the\nwhole image. Weﬁrst wrap tokens of each slice by two special tokens\n“<slice>” and “<\\slice>”, and then employ a special token“\\n” to\nseparate slices from different rows.\nWe adopt a three-phase progressive multimodal learning strategy\nas shown in Fig.2c, which consists of the pre-training phase, super-\nvised ﬁne-tuning phase, and alignment phase. In theﬁrst phase, we\nutilize large-scale image-text pairs for MLLM pre-training to align the\nvisual modules (i.e., visual encoder and compression layer) with the\ninput space of LLM and to acquire foundational multimodal knowl-\nedge. The pre-training phase can be further divided into three stages.\nIn the ﬁrst stage, the compression layer is warmed up. The second\nstage involves extending the input resolution of the pre-trained visual\nencoder. Finally, in the third stage, the visual modules are trained using\nan adaptive visual encoding strategy, allowing them to effectively\nhandle high-resolution inputs with any aspect ratios. In the second\nphase, we perform Supervised Fine-Tuning (SFT) on high-quality visual\nquestion answering datasets to further learn knowledge and interac-\ntion capability from human annotations. We unlock all model para-\nmeters to better exploit the data and learn rich knowledge during the\nSFT phase. We also conduct a lightweight yet high-quality SFT process\nas in VisCPM\n8 to enhance alignment with languages beyond English\nand Chinese, achieving strong multimodal performance across more\nthan 30 languages. In the alignment phase, we employ the recent\nRLAIF-V\n18 approach to address the hallucination problem (Fig.2d),\nwhere the MLLM generates responses that are not factually grounded\nin the input image\n19.T h eﬁrst step of RLAIF-V is to generate multiple\nresponses for a given instruction using the policy model. Then a divide-\nand-conquer strategy is applied for response scoring. After collecting\nthe high-quality AI feedback, we perform preference learning via DPO\n21\nmethod.\nThis paper introduces theﬁrst 3 models in the MiniCPM-V series,\nincluding MiniCPM-V 1.0, MiniCPM-V 2.0, and MiniCPM-Llama3-V 2.5.\nMiniCPM-V 1.0 is trained with the pre-training stage1&2 and SFT\nwithout using the adaptive visual encoding and RLAIF-V. For MiniCPM-\nV 2.0, we include all training stages and the adaptive visual encoding\nstrategy to further improve performance. MiniCPM-Llama3-V 2.5\nadopts Llama3-Instruct 8B as its base LLM, showcasing strong multi-\nmodal understanding capabilities, as illustrated in Fig.3.\nEvaluation across diverse multimodal understanding\nbenchmarks\nWe perform a comprehensive evaluation on popular benchmarks\ncovering visual question answering, multimodal conversation, knowl-\nedge and reasoning, OCR, and hallucination. (1)General benchmarks.\nWe adopt OpenCompass22 as the general evaluation indicator, which is\na comprehensive collection over 11 popular multimodal benchmarks,\nincluding MME\n23, MMBench24, MMMU25,M a t h V i s t a26, LLaVA Bench4,\netc. We also report the results on RealWorldQA for real-world spatial\nunderstanding capabilities. (2) OCR benchmarks.W ea d o p tt h r e e\nwidely used benchmarks for OCR capability evaluation, including\nincluding OCRBench\n27,T e x t V Q A28 and DocVQA29.( 3 )Hallucination\nbenchmarks. We also include Object HalBench19,30 to evaluate the\ntrustworthiness of the models.\nWe compare with strong baselines in different series: For open-\nsource models, we compare with strong models including Yi-VL-6B/\n34B13,Q w e n - V L - C h a t - 9 B7, DeepSeek-VL-7B3, TextMonkey31,C o g V L M -\nChat-17B5, CogVLM2-Llama3-19B5,I d eﬁcs2-8B9, Bunny-Llama-3-8B32,\nXTuner-Llama-3-8B-v1.133, LLaVA-NeXT-Llama-3-8B34, Cambrian-8B/\n34B35, LLaVA-NeXT-Yi-34B36, DeepSeek-VL-1.3B3,M o b i l e V L MV 237,M i n i -\nGemini14 and Phi-3-Vision-128k-instruct12. For proprietary models, we\ncompare with GPT-4V-11062,G e m i n i - P r o1 and Claude 3 Opus38.\nFrom the experimental results in Fig.4, we have the following\nobservations: (1) MiniCPM-Llama3-V 2.5 outperforms strong open-\nsource models by a notable margin. For instance, MiniCPM-Llama3-V\n2.5 surpasses the recent strong Ideﬁcs2-8B by 7.9 points on the\nOpenCompass benchmark, with similar model sizes. It also achieves\nbetter results than signiﬁcantly larger models such as Cambrian-34B,\nLLaVA-NeXT-Yi-34B, Yi-VL-34B and CogVLM2-Llama3-19B. (2) Com-\npared with powerful proprietary models, such as GPT-4V-1106 and\nGemini Pro, MiniCPM-Llama3-V 2.5 achieves better performance on\nthe OpenCompass benchmark with signiﬁcantly fewer parameters. In\naddition, MiniCPM-Llama3-V 2.5 also achieves lower hallucination rates\nthan GPT-4V-1106 on Object HalBench, indicating its trustworthiness\nfor real-world applications. (3) The smaller MiniCPM-V 2.0 with 2B\nparameters achieves signiﬁcantly better performance compared with\nother 2B ~ 4B models, and is even comparable with 8B MLLMs such as\nBunny-Llama-3-8B. In summary, the results show that MiniCPM-V ser-\nies achieves a good balance between performance and efﬁciency,\nmaking it more friendly for broader communities and applications.\nMiniCPM-V models also show strong OCR capabilities, including\nscene-text, document and screenshot understanding. As shown in\nFig. 5a, MiniCPM-Llama3-V 2.5 outperforms open-source MLLMs ran-\nging 1.7B– 34B on OCRBench, TextVQA, and DocVQA. The perfor-\nmance on these datasets is even comparable to proprietary models like\nGPT-4V-1106 and Gemini Pro. MiniCPM-V 2.0 also achieves signiﬁcantly\nbetter performance among models in the 2B – 4B parameter\nrange (Fig.5b).\nBased on the multilingual multimodal generalization approach\nfrom VisCPM, MiniCPM-Llama3-V 2.5 extends its multimodal capability\nto over 30 languages. As shown in Fig.5c, MiniCPM-Llama3-V 2.5 can\noutperform Yi-VL 34B and Phi-3-vision-128k-instruct on the multi-\nlingual LLaVA Bench. The promising multilingual multimodal cap-\nability makes MiniCPM-Llama3-V 2.5 useful in serving larger linguistic\ngroups.\nTo investigate the effectiveness of key components, we perform\nan ablation study on high-resolution perception and multi-stage\ntraining pipeline. To ablate high-resolution perception, we follow the\nstandard method in LLaVA-1.5\n39 to downsample high-resolution images\ninto low-resolution versions (i.e., 448 × 448) in both training and\ninference. To ablate multi-stage training pipeline, we follow the stan-\ndard two-stage training (i.e., pretraining and instruction tuning) in\nLLaVA-1.5\n39. Due to the high computational costs of model training on\nfull data, we perform ablation on a subset of full training data by ran-\ndomly sampling 10% data from each dataset, resulting in 70M data in\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 4\ntotal, which is sufﬁcient to validate a frontier MLLM. From the\nexperimental results in Table1, we can see that both high-resolution\nperception and multi-stage training pipeline contribute to theﬁnal\nperformance. The reason is that high-resolution perception is crucial\nfor MLLMs to perceiveﬁne-grained visual details, especially for OCR-\nrelated tasks, and multi-stage training can betterﬁta n de x p l o i tt r a i n -\ning data of different forms and qualities.\nMoreover, it is worth noting that MiniCPM-Llama3-V 2.5 requires\nsigniﬁcantly less inference computation. For example, the visual token\nnumber range of MiniCPM-Llama3-V 2.5 is (96, 960), which is lower\nthan LLaVA-NeXT-Llama-3-8B’s (1728, 2880). This can be important\nespecially for real-world on-device applications in terms of inference\nspeed, ﬁrst-token latency, memory usage, and power consumption.\nSpeciﬁcally, we provide a comparison of the computational costs\nbetween the adaptive visual encoding method and the vanilla visual\nencoding method (i.e., visual features of the original image from ViT\nare directly projected and input into the LLM, as in LLaVA-1.5\n39). From\nthe results in Fig.2a, we can see that compared with the standard\nmethod, adaptive visual encoding largely reduces both FLOPs and GPU\nmemory usage in both ViT and LLM for high-resolution images. The\nreason is that, compared with the standard method, image slicing\nprevents the quadratic computation growth of ViT, and the com-\npression layer largely reduces the number of visual tokens to LLMs.\nEfﬁcient Deployment of MiniCPM-V on Edge Devices\nIn this section, we investigate the deployment of MiniCPM-V on edge\ndevices. Edge devices such as smartphones and computers often face\nresource constraints due to factors like heat dissipation, size limita-\ntions, and power consumption. When deploying models, the two most\ncritical limitations are memory capacity and CPU/GPU processing\nspeed. High-performance servers typically boast extensive memory\ncapacities, often exceeding 100GB or even 1TB. In contrast, the\nmemory available on mobile phones typically ranges from 12GB to\n16GB, which can be insufﬁcient for MLLM deployment. On the other\nhand, The overall processing speeds of CPUs in smartphones are\nnotably slower. For instance, the Snapdragon 8 Gen3 features 8 CPU\ncores, whereas high-performance server like Intel Xeon Platinum 8580\nhas 60 CPU cores. Similarly, mobile phone GPUs are not as powerful as\nserver GPUs. For example, Qualcomm Adreno 750 only has 6 TFLOPS,\nw h i l eN V I D I A4 0 9 0c a nr e a c h8 3T F L O P S .\nTo deploy the MLLM on edge devices, weﬁrst employ quantiza-\ntion for reduced memory cost. For MiniCPM-Llama3-V 2.5, the fp16\nFig. 3 | Qualitative results of MiniCPM-Llama3-V 2.5.MiniCPM-Llama3-V 2.5 demonstrates strong performance in a variety of tasks, including text recognition in images,\ntable-to-markdown conversion, complex reasoning, and multilingual interactions.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 5\nversion model typically demands 16– 17G memory. We opt for the\nQ4_K_M mode 4-bit quantization strategy within GGML framework.\nThis reduces the memory requirement to around 5G, which is friendly\nto mobile phone usage. We then empirically investigate the deploy-\nment results on different frameworks. Several frameworks have been\nproposed for on-device deployment. Illustrated in Fig.6,w em a k ea\nthorough investigation of different frameworks for different chip\ntypes including CPU, GPU, and NPU. Given the ubiquity of CPU usage\nacross devices, we prioritize this chip type and opt for the llama.cpp\n40\nframework. Combining quantization and llama.cpp on Xiaomi 14 Pro\n(Snapdragon 8 Gen 3), the model achieves a text encoding latency of\n64.2s and a text decoding speed of 1.3 tokens/s (as depicted in Fig.6f),\nwhich is still far from acceptable for users.\nTo achieve better acceleration, we further investigate a series of\nadvanced techniques including memory usage optimization, compi-\nlation optimization, conﬁguration optimization, and NPU acceleration,\nFig. 4 | Performance comparison of proprietary MLLMs, open-source MLLMs\non general multimodal benchmarks.MiniCPM-Llama-V 2.5, with only 8 billion\nparameters, outperforms leading open-source MLLMs and achieves superior\nresults on the OpenCompass benchmark compared to proprietary models like GPT-\n4V-1106 and Gemini Pro. In addition, MiniCPM-V 2.0, with 2 billion parameters,\nsigniﬁcantly outperforms other MLLMs with fewer than 4 billion parameters.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 6\nas shown in Fig.6a– d. We ﬁrst explore memory usage optimization\nstrategies to address the image processing bottleneck of the inference\nspeed due to limited memory resources on mobile phones. Instead of\nloading both ViT and LLM simultaneously into memory, we adopt a\nsequential loading approach. Speciﬁcally, weﬁrst load ViT for visual\nencoding, followed by the LLM for visual and text token encoding. By\nreleasing the large amount of memory occupied by LLM, we can pre-\nvent frequent paging (swapping in and out) during ViT encoding,\nthereby improving the program efﬁciency. This optimization techni-\nque, as illustrated in Fig.6e, results in a notable reduction of image\nprocessing time from 45.2s to 31.5s. We alsoﬁnd that directly com-\npiling the models on the target devices can signiﬁcantly improve the\nencoding latency and the decoding throughput. This can be attributed\nto better consistency between the compilation and target device\ninstruction set architecture. As depicted in Fig.6e, this optimization\nendeavor yields promising results. Encoding latency shows a notable\nreduction from 50.5s to 17.0s, while decoding throughput experiences\nas i g n iﬁcant boost from 1.3 tokens/s to 3.2 tokens/s. Next, instead of\nrelying on a single default conﬁguration of the llama.cpp framework,\nwe propose an automated parameter search algorithm that dynami-\ncally identiﬁes the optimal conﬁgurations (e.g., computational dis-\ntribution across CPU cores) tailored to various edge devices. Through\nconﬁguration optimization, we can achieve good improvements.\nSpeciﬁcally, decoding throughput surged from 3.2 tokens/s to an\nimpressive 8.2 tokens/s, surpassing the typical human reading speed.\nFinally, we leverage Neural Processing Units (NPUs), a class of specia-\nlized hardware designed to accelerate AI applications, available in\ncertain smartphones. Recognized for their ability to address compu-\ntational bottlenecks, NPUs enable accelerated visual encoding. Speci-\nﬁcally, we replace the backend framework of ViT with QNN while\nretaining the llama.cpp backend for the language model component.\nOn mobile devices equipped with Qualcomm NPUs, this optimization\nyields a notable reduction in visual encoding time, decreasing from 3.7\nto 1.3 s.\nFor a comprehensive assessment of MiniCPM-Llama3-V 2.5’sp e r -\nformance across various edge devices, we present test results on\nXiaomi 14 Pro (Snapdragon 8 Gen 3), vivo X00 Pro (Mediatek Dimen-\nsity 9300), Macbook Pro (M1), and Jetson AGX Orin 32G in Fig.6f.\nThanks to the deployment optimization techniques, MiniCPM-Llama3-\nV 2.5 can operate efﬁciently on both mobile phones and personal\ncomputers, delivering acceptable latency and throughput. For\ninstance, leveraging NPU on Xiaomi 14 Pro enables it to achieve a\nsimilar encoding speed as the Mac M1. Furthermore, nearly all devices\nexhibit comparable or higher throughput compared with human\nreading speed. Upon analyzing the results, it becomes evident that the\ncurrent computation bottleneck primarily stems from LLM preﬁlling,\nwhich mainly involves encoding image and text tokens for LLM infer-\nence. Promising research directions involve developing more efﬁcient\nFig. 5 | Performance on OCR and multilingual benchmarks. a, b Results on OCR benchmarks including OCRBench, the DocVQA test set and the TextVQA val set for (a)\nopen-source MLLMs (>4B) and (b) MLLMs (<4B).c, Multilingual multimodal capabilities on the multilingual LLaVABench.\nTable 1 | Multi-stage training and high-resolution perception ablation results on a subset (10%) of full training data\nModel OpenCompass 2.0 MMMU MMVet MMBv1.1 EN OCRBench ChartQA MME TextVQA DocVQA\n(val) (val) (test) (val) (test)\nMiniCPM-Llama3-V 2.5 54.1 43.3 46.7 69.6 697 65.4 1922 72.5 79.5\nw/o multi-stage training 50.1 42.9 44.9 67.5 549 57.6 1544 70.1 64.7\nw/o high-resolution perception 48.5 37.9 37.8 64.2 580 51.2 1701 65.1 58.4\nBold font indicates the best result on each test set.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 7\nFig. 6 | Deploying MiniCPM-V on edge devices. a– d Advanced techniques used in\nthe deployment of MiniCPM-V on edge devices, including (a)m e m o r yu s a g e\noptimization, (b) compilation optimization, (c)c o nﬁguration optimization, and\nd NPU acceleration.e The inﬂuence of these techniques on the encoding latency\nand decoding throughput. The results are tested on the Xiaomi 14 Pro (Snapdragon\n8 Gen 3). No opt.: non-optimized, mem. opt.: memory usage optimization, comp.\nopt.: compilation optimization, conﬁg. opt.: conﬁguration optimization, NPU: NPU\nacceleration.f Results on different edge devices. We show the encoding latency and\ndecoding throughput across different device types. Xiaomi 14 Pro is the only device\nwith NPU.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 8\nvisual encoding methods with fewer visual tokens, and better lever-\naging GPU/NPU acceleration for LLM encoding. With increasing\nattention to on-device MLLMs and the rapid advancement of GPU/NPU\nacceleration techniques, we believe that real-time interaction with on-\ndevice MLLMs can be reached soon.\nDiscussion\nThe MiniCPM-V series models are a primary exploration into powerful\non-device MLLMs. Thanks to techniques such as adaptive visual\nencoding, multilingual generalization, and the RLAIF-V method,\nMiniCPM-Llama3-V 2.5 can achieve GPT-4V level performance with\nsigniﬁcantly fewer parameters. Leveraging diverse optimization tech-\nniques for edge deployment, this model ensures an acceptable user\nexperience on mobile phones.\nDespite promising performance, there remain several limitations\nwith the current MiniCPM-V models. (1) Capability Depth. There is still\nplenty of room for improvement in enhancing multimodal under-\nstanding capability and inference efﬁciency. (2) Capability Width. In\naddition to image modality, it’s promising to expand MLLM cap-\nabilities to encompass other modalities, such as video and audio, etc.,\nwhere GPT-4o\n41 and Google Astra42 have given good examples.\nIn addition to MLLM capabilities, edge deployment also presents\nunique challenges. The inference speed and latency are still far from\ngood enough and the model service can be limited by the battery\ncapacity. In addition, previous efforts on chips and deployment fra-\nmeworks mainly target CNNs and LSTMs, which can be sub-optimal for\nMLLMs. Tailored efforts to MLLMs can bring ample room for\nimprovement.\nConsidering the current limitations and the promising future of\non-device MLLMs, we anticipate increasing efforts from both academia\nand industry in enhancing model capabilities in terms of depth and\nwidth, and improving smartphone chips and deployment frameworks.\nWe believe that simultaneous advancements in model capability and\nedge device capacity will lead to on-device applications providing a\nsatisfying user experience in the near future.\nMethods\nAdaptive visual encoding\nImage partition. To process high-resolution images with varying\naspect ratios, we divide them into slices17. Each slice is adjusted to\nmore closely align with ViT’s pre-training settings in terms of both\nresolution and aspect ratio. Speciﬁcally, we ﬁrst calculate the ideal\nnumber of slices based on the input image size. Given an image with\nresolution (W\nI, HI) and a ViT pre-trained on images with resolution\n(Wv, Hv), we calculate the ideal slice numberN = dWI × HI\nWv × Hv\ne. Then, we\nchoose the combination of rowsn and columns m from the set\nCN = fðm, nÞjm × n = N, m 2 N, n 2 Ng. A good partition (m, n)s h o u l d\nresult in slices that match well with ViT’s pre-training setting. To\nachieve this, we use a score function to evaluate each potential parti-\ntion:\nSðm, nÞ = /C0 log WI =m\nHI =n /C0 log Wv\nHv\n/C12/C12\n/C12/C12\n/C12/C12\n/C12/C12: ð1Þ\nWe select the partition with the highest score from all possible\ncandidates:\nm\n*, n* = arg max\nðm, nÞ2 /C22C\nSðm, nÞ, ð2Þ\nwhere /C22C is the possible (m, n) combinations with the productN.\nHowever, whenN is a prime number, the feasible solutions can be\nlimited to (N,1 )a n d( 1 ,N). Therefore, we additionally introduceCN/C0 1\nand CN +1 ,a n ds e t/C22C= CN/C0 1 ∪ CN ∪ CN +1 . In practice, we setN <1 0 ,\nsupporting 1.8 million pixels (e.g., 1344 × 1344 resolution) at most\nduring encoding. Although we can encompass more image slices for\nhigher resolutions, we purposely impose this resolution upper-bound,\nsince it already well covers most real-world application scenarios, and\nthe beneﬁt of further increasing encoding resolution is marginal\nconsidering the performance and overhead.\nSlice encoding. Although image partitioning can ensure a good match\nbetween the slices and the ViT pre-training setting, each slice’ss i z ei s\nnot precisely equal to (W\nv, Hv). To feed the slices into ViT, weﬁrst\nadjust each slice by resizing it proportionally so that the resultant area\nsize matches ViT pre-training area sizeW\nv × Hv. This adjustment helps\nprevent a signiﬁcant gap between the number of encoded patches and\nthe ViT’s pre-training setting. Subsequently, we interpolate the ViT’s\nposition embeddings to adapt to the slice’sr a t i o .T h i si n v o l v e s\nreshaping the ViT’s1 De m b e d d i n gP1 2 RQ × l back to its 2D format\nP2 2 Rq × q × l, where the number of position embeddingsQ = q × q.\nThen, we interpolateP2 to ﬁt the size of each slice via 2D interpolation.\nWe also include the original image as an additional slice to provide\nholistic information about the entire image.\nToken compression. After visual encoding, each slice is encoded into\n1,024 tokens, where 10 slices can yield over 10k tokens collectively. To\nmanage this high token count, we employ a compression module\ncomprising of one-layer cross-attention and a moderate number of\nqueries, with 2D positions. In practice, the visual tokens of each slice\nare compressed into 64 queries for MiniCPM V1&2 and 96 tokens for\nMiniCPM-Llama3-V 2.5 through this layer. Compared with other\nMLLMs with competitive performance, the signiﬁcantly smaller num-\nber of visual tokens in MiniCPM-V series enables superior efﬁciency in\nterms of GPU memory consumption, inference speed,ﬁrst-token\nlatency and power consumption, making it more friendly to wider\napplication scopes and communities.\nSpatial schema.T oi n d i c a t ee a c hs l i c e’s position relative to the whole\nimage, inspired by\n20, we introduce a spatial schema. Weﬁrst wrap\ntokens of each slice by two special tokens“<slice>” and “<\\slice>”,a n d\nthen employ a special token“\\n” to separate slices from different rows.\nPre-training\nIn this phase, we utilize large-scale image-text pairs for MLLM pre-\ntraining. The primary goal of this phase is to align the visual modules\n(i.e., visual encoder and compression layer) with the input space of the\nLLM and learn foundational multimodal knowledge. We show the pre-\ntraining data composition in Table2. The pre-training phase is further\ndivided into 3 stages.\nStage-1. The role of stage-1 is to warm up the compression layer, pri-\nmarily connecting the visual encoder and LLMs. (1) Trainable Modules.\nWe randomly initialize the compression layer and train this module in\nstage-1, keeping other parameters frozen. The visual encoder’sr e s o -\nlution is set to 224 × 224, which is the same as the visual encoder’s pre-\ntraining setting. (2) Data. To warm up the compression layer, we ran-\ndomly select 200M data from the Image Captioning data in Table2.\nData cleaning is performed to remove image-text pairs with poor\ncorrelation and ill-formatted text data, ensuring the data quality.\nStage-2. After the warm-up training of the compression layer, the role\nof stage-2 is to extend the input resolution of the pre-trained visual\nencoder. (1) Trainable Modules. In stage-2, we extend the image\nresolution from 224 × 224 to 448 × 448. The whole visual encoder is\ntrained, leaving other parameters frozen. (2) Data. To extend the pre-\ntrained resolution, we additionally select 200M data from the Image\nCaptioning data in Table2.\nStage-3. After extending the primary input resolution of the visual\nencoder, weﬁnally train the visual modules using the adaptive visual\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 9\nencoding strategy, which can further accommodate high-resolution\ninputs with any aspect ratio. (1) Trainable Modules. During the stage-3\ntraining, both the compression layer and the visual encoder are trained\nto adapt to the language model embedding space. The LLM is kept\nfrozen to avoid disruption from the relatively low-quality pre-training\ndata. (2) Data. Different from the previous stages with only image\ncaptioning data, during the high-resolution pre-training stage, we\nadditionally introduce OCR data to enhance the visual encoders’OCR\ncapability.\nCaption rewriting. Image-text pairs sourced from the Web\n43,44 can\nsuffer from quality issues in the caption data, including non-ﬂuent\ncontent, grammatical errors, and duplicated words. Such low-quality\ndata can lead to unstable training dynamics. To address the issue, we\nintroduce an auxiliary model for low-quality caption rewriting. The\nrewriting model takes the raw caption as input and is asked to convert\nit into a question-answer pair. The answer from this process is adopted\nas the updated caption. In practice, we leverage GPT-4\n45 to annotate a\nsmall number of seed samples, which are then used toﬁne-tune an LLM\nfor the rewriting task.\nData packing. Samples from different data sources usually have dif-\nferent lengths. The high variance of sample lengths across batches will\nlead to inefﬁciency in memory usage and the risk of out-of-memory\nproblem. To address the issue, we pack multiple samples into a single\nsequence with aﬁxed length. By truncating the last sample in the\nsequence, we ensure uniformity in sequence lengths, facilitating more\nconsistent memory consumption and computational ef ﬁciency.\nMeanwhile, we modify the position ids and attention masks to avoid\ninterference between different samples. In our experiments, the data\npacking strategy can bring 2 ~ 3 times acceleration in the pre-\ntraining phase.\nMultilingual generalization. Multimodal capability across multiple\nlanguages is essential for serving users from broader communities.\nTraditional solutions involve extensive multimodal data collection and\ncleaning, and training for the target languages. Fortunately, recent\nﬁndings\n8 have shown that the multimodal capabilities can be efﬁciently\ngeneralized across languages via a strong multilingual LLM pivot, lar-\ngely alleviating the heavy reliance on multimodal data in low-resource\nlanguages. In practice, we only pre-train our model on English and\nChinese multimodal data, and then perform a lightweight but high-\nquality multilingual supervisedﬁne-tuning to align to the target lan-\nguages. Despite its simplicity, weﬁnd the resultant MiniCPM-Llama3-V\n2.5 can achieve good performance in over 30 languages as compared\nwith signiﬁcantly larger MLLMs.\nTable 2 | Pre-training data\nCategory Sources Size\nImage Captioning English COCO 47,V G48,C C 3 M49, CC12M50 410M\nLAION-COCO43, COYO44,L A I O N - 2 B43\nChinese AIC 51, LAION-2B-Chinese43,W u K o n g52 110M\nZero-Chinese53, etc.\nOCR+Knowledge English WIT 54,I D L55, SynthText56, SynthDoG-en57 39M\nSynthDoG-zh57, ArxivCap58,e t c .\nChinese WIT 54,L A I O N - 2 B - O C R 1 1 M\nThe pre-training data consists of image captioning and OCR data in English and Chinese. LAION-2B-OCR is generated by applying OCR tools to LAION-2B images.\nTable 3 | SFT data for MiniCPM-V series\nCategory Sources Size\nPart-1 Short Caption Flickr-30K 59, COCO47 560K\nVQA FM-IQA 60,V G Q A48, IconQA61,G Q A62,V Q A v 263 1.4M\nCLEVR64,V i z W i z65,V i s u a l 7 W66,C O C O - Q A67\nKnowledge OKVQA 68, A-OKVQA69,K V Q A70, ScienceQA71 60K\nGrounding RefCOCO 72 570K\nReasoning COMVINT 73,V C R74,N L V R75,L R V76 135K\nMath GeoQA 77, SMART-10178 125K\nOCR DocVQA 29, TextVQA28,O C R - V Q A79,S T - V Q A80, VisualMRC81,D V Q A82 1.7M\nFigureQA83,C h a r t Q A84, DeepForm85,T a b F a c t86, InfographicsVQA87\nKleister Charity88, WikiTableQuestions89, Real-CQA90,A I 2 D91, etc.\nChat FSVQA 92, Visual-Dialog93 780K\nPart-2 Part-1 sample from Part-1 data 400K\nOCR DocVQA, TextVQA, OCR-VQA, VisualMRC, ChartQA, AI2D 690K\nArxivQA58, LLaVAR94,T e x t O C R - G P T 4 V95,e t c .\nInstruct SVIT 96, LLaVA-Instruct-150K4, UniMM-Chat97, ShareGPT4V98 1.9M\nLVIS99,A L L a V A100\nText-Only UltraChat 101, Alpaca102, ShareGPT103, BELLE104 -\nOpenOrca105, OpenHermes106, In-House-MiniCPM-SFT\nPart-1&2 data are concatenated sequentially in the SFT phase. Part-1 focuses on bolstering basic recognition capabilities, while part-2 aims to enhance advanced capabilities in generating detailed\nresponses and following human instructions.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 10\nSupervisedﬁne-tuning\nAfter learning foundational capabilities from pre-training, we perform\nsupervisedﬁne-tuning (SFT) on high-quality visual question answering\ndatasets to further learn knowledge and interaction capability from\nhuman annotations.\nTrainable Modules. Compared with the pre-training phase which\nmainly uses crawled data from the Web, the SFT phase mainly utilizes\nhigh-quality datasets annotated by either human lablers or strong\nmodels such as GPT-4. Therefore, we unlock all model parameters to\nbetter exploit the data and learn rich knowledge during SFT phase.\nData.R e c e n tw o r k s\n1,46 show that data near the end of training plays a\nmore important role in shaping the models’ capabilities and response\nstyles. We categorize the SFT data into two parts, as shown in Table3.\nPart-1 focuses on bolstering the models’basic recognition capabilities,\nwhile part-2 is tailored to enhance their capabilities in generating\ndetailed responses and following human instructions. Speciﬁcally, part-1\ndata consists of the traditional question answering and captioning\ndatasets with relativelyshort response lengths, which helps enhance the\nmodel’s basic recognition capabilities. In comparison, part-2 encom-\npasses datasets featuring long responses with complex interactions,\neither in text or multimodal context. During SFT, these two parts of data\nare concatenated and sequentially fed into the model. For MiniCPM-\nLlama3-V 2.5, we integrate 2M data from the recent Cauldron dataset\n9\nfor multimodal knowledge augmentation, and 90K multilingual data\nover 36 languages for boosting the multilingual conversation capability.\nAlignment\nMLLMs are typically prone to hallucination problems, generating\nresponses that are not factually grounded in the input image19.T h e\nissue greatly limits the wide application of MLLMs, especially in high-\nstakes scenarios, such as autonomous driving and assistance for\nvisually impaired groups. To address the hallucination problem, we\nemploy the recent RLAIF-V\n18 approach, where the key is to obtain\nscalable high-quality feedback from open-source models for pre-\nference learning\n21.\nResponse generation.T h eﬁrst step of RLAIF-V is to generate multiple\nresponses for a given instruction using the policy model. Speciﬁcally,\ngiven a model M waiting for alignment, we sample 10 responses\nY ={ y1, y2, ⋯ , yn}f r o mM u s i n gs a m p l i n gd e c o d i n gw i t hh i g ht e m -\nperatures. There are several beneﬁts of using the policy modelM for\nresponse generation: (1) Feedback collection and learning can better\nfocus on trustworthiness, since different text styles from multiple\nMLLMs are avoided. (2) Feedback learning is more efﬁcient since\npreference is directly collected on the distribution of the policy model.\nFeedback collection. Collecting high-quality feedback from open-\nsource MLLMs can be challenging due to their typically weaker\ncapabilities compared with proprietary models. To address the issue,\nRLAIF-V uses a divide-and-conquer strategy for response scoring.\nSpeciﬁcally, each response y\ni is divided into atomic claims\nCi ={ c1, c2,… , cm} using Llama-3 8B, where the correctness of atomic\nclaims is much easier to evaluate. Then, we verify the claims by\nconverting each claim to a yes/no question and employing an open-\nsource MLLM to score each claim. In practice, we adopt OmniLMM\n12B for MiniCPM-V 2.0 scoring and LLaVA-NeXT-Yi 34B for MiniCPM-\nLlama3-V 2.5 scoring. Theﬁnal scores\ni of the responseyi is given by\n− nrej, wherenrej is the number of invalid atomic claims.\nDirect preference optimization. After collecting the high-quality AI\nfeedback, we perform preference learning via DPO method. The DPO\nalgorithm requires training on preference pairs, where one sampley\nw\nis preferred to the other oneyl. To compose the preference dataset, we\nrandomly sample pairs from each response setY ={ y1, y2,… ,yn}, and\ndetermine (yw, yl) based on their relative scores. Finally, we construct a\npreference dataset consisting of 6K preference pairs from 3K unique\nimages for preference learning.\nData availability\nThe datasets used for training are described in detail in the Methods\nsection. While most of the training data are publicly available, a small\nportion originates from proprietary datasets licensed from a com-\nmercial provider and cannot be shared due to legal and contractual\nrestrictions. These restricted datasets were used exclusively for model\ntraining and do not affect the reproducibility of the coreﬁndings\npresented in this study. Source data are provided with this paper.\nCode availability\nCode for training and evaluating our model is publicly available on\nGitHub at https://github.com/OpenBMB/MiniCPM-o,a n dh a sb e e n\narchived athttps://doi.org/10.5281/zenodo.15525638.\nReferences\n1. Reid, M. et al. Gemini 1.5: Unlocking multimodal understanding\nacross millions of tokens of context. Preprint athttps://arxiv.org/\nabs/2403.05530(2024).\n2. Achiam, J. et al. GPT-4 Technical Report. Preprint athttps://arxiv.\norg/abs/2303.08774(2023).\n3. Lu, H. et al. DeepSeek-VL: Towards real-world vision-language\nunderstanding. Preprint athttps://arxiv.org/abs/2403.\n05525 (2024).\n4. Liu, H., Li, C., Wu, Q. & Lee, Y. J. Visual instruction tuning.NeurIPS\n36 (2024).\n5. Wang, W. et al. CogVLM: Visual expert for pretrained language\nmodels. In:Proc. NeurIPS(2023).\n6 . C h e n ,Z .e ta l .H o wf a ra r ew et oG P T - 4 V ?C l o s i n gt h eg a pt o\ncommercial multimodal models with open-source suites. InSci-\nence China Information Sciences(2024).\n7. Bai, J. et al. Qwen-VL: a frontier large vision-language model with\nversatile abilities. Preprint athttps://arxiv.org/abs/2308.\n12966 (2023).\n8. Hu, J. et al. Large multilingual models pivot zero-shot multimodal\nlearning across languages. InProc. ICLR(2024).\n9. Laurençon, H., Tronchon, L., Cord, M. & Sanh, V. What matters\nwhen building vision-language models? InProc. NeurIPS(2024).\n10. McKinzie, B. et al. MM1: methods, analysis & insights from multi-\nmodal LLM pre-training. InProc. ECCV(2024).\n11. Beyer, L. et al. PaliGemma: a versatile 3b vlm for transfer. Preprint\nat https://arxiv.org/abs/2407.07726(2024).\n12. Abdin, M. et al. Phi-3 technical report: a highly capable language\nmodel locally on your phone. Preprint athttps://arxiv.org/abs/\n2404.14219(2024).\n13. Young, A. et al. Yi: open foundation models by 01.AI. Preprint at\nhttps://arxiv.org/abs/2403.04652(2024).\n14. Li, Y. et al. Mini-Gemini: mining the potential of multi-modality\nvision language models. Preprint athttps://arxiv.org/abs/2403.\n18814 (2024).\n15. Wikipedia. Moore ’s law. In Wikipedia, the Free Encyclope-\ndia (2001).\n16. Wikipedia. Thrust ‑to‑weight ratio. In Wikipedia, the Free Encyclo-\npedia (2024).\n17. Xu, R. et al. LLaVA-UHD: an LMM perceiving any aspect ratio and\nhigh-resolution images. InProc. ECCV(2024).\n18. Yu, T. et al. RLAIF-V: aligning MLLMs through open-source AI\nfeedback for super GPT-4V trustworthiness. InProc. CVPR(2025).\n19. Yu, T. et al. RLHF-V: towards trustworthy MLLMs via behavior\nalignment fromﬁne-grained correctional human feedback. In:\nProc. CVPR(2024).\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 11\n20. Bavishi, R. et al. Introducing our multimodal models. InAdept\nBlog (2023).\n21. Rafailov, R. et al. Direct preference optimization: your language\nmodel is secretly a reward model.NeurIPS. 36 (2024).\n22. OpenCompass Contributors. OpenCompass: a universal eva-\nluation platform for foundation models. InGitHub repository\n(2023).\n23. Fu, C. et al. MME: a comprehensive evaluation benchmark for\nmultimodal large language models. Preprint athttps://arxiv.org/\nabs/2306.13394(2023).\n24. Liu, Y. et al. MMBench: Is your multi-modal model an all-around\nplayer? Preprint athttps://arxiv.org/abs/2307.06281(2023).\n25. Yue, X. et al. MMMU: A massive multi-discipline multimodal\nunderstanding and reasoningbenchmark for expert AGI. InProc.\nCVPR (2024).\n26. Lu, P. et al. MathVista: Evaluating mathematical reasoning of\nfoundation models in visual contexts. InProc. ICLR(2024).\n27. Liu, Y. et al. OCRBench: on the hidden mystery of OCR in large\nmultimodal models. InScience China Information Sciences(2023).\n28. Singh, A. et al. Towards VQA models that can read. In:Proc.\nCVPR (2019).\n29. Mathew, M., Karatzas, D. & Jawahar, C. DocVQA: a dataset for VQA\non document images. In:Proc. WACV(2021).\n30. Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T. & Saenko, K.\nObject hallucination in image captioning. InProc. EMNLP(2018).\n31. Liu, Y. et al. TextMonkey: An OCR-free large multimodal model for\nunderstanding document. Preprint athttps://arxiv.org/abs/2403.\n04473 (2024).\n32. He, M. et al. Efﬁcient multimodal learning from data-centric per-\nspective. Preprint athttps://arxiv.org/abs/2402.11530(2024).\n33. XTuner Contributors. XTuner: a toolkit for efﬁciently ﬁne‑tuning\nLLM. InGitHub repository(2023).\n34. Li, B. et al. LLaVA‑NeXT: Stronger LLMs Supercharge Multimodal\nCapabilities in the Wild. InLLaVA blog(2024).\n35. Tong, S. et al. Cambrian-1: A fully open, vision-centric exploration\nof multimodal LLMs. InProc. NeurIPS(2024).\n36. Liu, H. et al. LLaVA‑NeXT: Improved reasoning, OCR, and world\nknowledge. InLLaVA blog(January 30, 2024).\n37. Chu, X. et al. MobileVLM: a fast, reproducible and strong vision\nlanguage assistant for mobile devices. Preprint athttps://arxiv.\norg/abs/2312.16886(2023).\n38. Anthropic. Introducing the next generation of Claude. InClaude 3\nblog (2024).\n39. Liu, H., Li, C., Li, Y. & Lee, Y. J. Improved baselines with visual\ninstruction tuning. In:Proc. CVPR(2024).\n40. Georgi Gerganov and the llama.cpp Group. llama.cpp: LLM\ninference in C/C++ with cross‑platform support and hardware-\noptimized quantization. InGitHub repository(2023).\n41. OpenAI. Hello GPT‑4o: our newﬂagship, multimodal model for\nreal-time reasoning across text, vision, and audio. InOpenAI blog\n(May 13, 2024).\n42. DeepMind. Project Astra: a research prototype exploring cap-\nabilities toward a universal AI assistant. InGoogle DeepMind\nModels (2024).\n43. Schuhmann, C. et al. LAION-5B:An open large-scale dataset for\ntraining next generation image-text models.NeurIPS 35,\n25278–25294 (2022).\n44. Byeon, M. et al. COYO-700M: Image-text pair dataset. InGitHub\nrepository(2022).\n45. Bubeck, S. et al. Sparks of artiﬁcial general intelligence: early\nexperiments with GPT-4. Preprint athttps://arxiv.org/abs/2303.\n12712 (2023).\n46. Hu, S. et al. MiniCPM: Unveiling the potential of small language\nmodels with scalable training strategies. InProc. COLM(2024).\n47. Lin, T.-Y. et al. Microsoft COCO: Common objects in context. In:\nECCV (Eds. Fleet, D., Pajdla, T., Schiele, B. & Tuytelaars, T.)\n740–755 (Springer, 2014).\n48. Krishna, R. et al. Visual Genome: Connecting language and vision\nusing crowdsourced dense image annotations”. IJCV 123,\n32–73 (2017).\n49. Sharma, P., Ding, N., Goodman, S. & Soricut, R. Conceptual cap-\ntions: a cleaned, hypernymed, image alt-text dataset for auto-\nmatic image captioning. In:ACL (2018).\n50. Changpinyo, S., Sharma, P., Ding, N. & Soricut, R. Conceptual 12M:\npushing web-scale image-text pre-training to recognize long-tail\nvisual concepts. In:CVPR (2021).\n51. Wu, J. et al. AI Challenger: a large-scale dataset for going deeper\nin image understanding. Preprint athttps://arxiv.org/abs/1711.\n06475 (2017).\n52. Gu, J. et al. Wukong: a 100 million large-scale Chinese cross-\nmodal pre-training benchmark.NeurIPS 35,2 6 4 1 8–26431 (2022).\n5 3 . X i e ,C . ,L i ,J .&Z h a n g ,B .C C M B :AL a r g e - s c a l eC h i n e s eC r o s s -\nmodal Benchmark. Preprint athttps://arxiv.org/abs/2205.\n03860 (2022).\n5 4 . S r i n i v a s a n ,K . ,R a m a n ,K . ,C h e n ,J . ,B e n d e r s k y ,M .&N a j o r k ,M .W I T :\nWikipedia-based image text dataset for multimodal multilingual\nmachine learning. In:SIGIR 2443–2449 (2021).\n5 5 . B i t e n ,A .F . ,T i t o ,R . ,G o m e z ,L . ,V a l v e n y ,E . &K a r a t z a s ,D .O C R - I D L :\nOCR annotations for industry document library dataset. In:ECCV\n( E d s .A v i d a n ,S . ,B r o s t o w ,G . ,C i s s é ,M . ,F a r i n e l l a ,G .M .&H a s s n e r ,\nT.). 241–252 (Springer, 2022)..\n56. Gupta, A., Vedaldi, A. & Zisserman, A. Synthetic data for text\nlocalisation in natural images. In:CVPR 2315–2324 (2016).\n57. Kim, G. et al. OCR-free document understanding transformer. In:\nECCV ( E d s .A v i d a n ,S . ,B r o s t o w ,G . ,C i s s é ,M . ,F a r i n e l l a ,G .M .&\nHassner, T.) (2022).\n58. Li, L. et al. Multimodal ArXiv: a dataset for improving scientiﬁc\ncomprehension of large vision-language models. InProc.\nACL (2024).\n59. Plummer, B. A. et al. Flickr30k entities: collecting region-to-phrase\ncorrespondences for richer image-to-sentence models. In:ICCV\n2641–2649 2015.\n60. Gao, H. et al. Are you talking to a machine? dataset and methods\nfor multilingual image question.NeurIPS 28 (2015).\n61. Lu, P. et al. IconQA: a new benchmark for abstract diagram\nunderstanding and visual language reasoning. InProc. NeurIPS\nDatasets and Benchmarks Track(2021).\n62. Hudson, D. A. & Manning, C. D. GQA: a new dataset for real-world\nvisual reasoning and compositional question answering. In:CVPR\n6700–6709 (2019).\n63. Antol, S. et al. VQA: Visual question answering. In:ICCV\n2425–2433 (2015).\n64. Johnson, J. et al. CLEVR: a diagnostic dataset for compositional\nlanguage and elementary visual reasoning. In:CVPR\n2901–2910 (2017).\n65. Gurari, D. et al. VizWiz Grand Challenge: answering visual ques-\ntions from blind people. In:CVPR 3608–3617 (2018).\n66. Zhu, Y., Groth, O., Bernstein, M. & Fei-Fei, L. Visual7W: Grounded\nquestion answering in images. In:CVPR (2016).\n67. Ren, M., Kiros, R. & Zemel, R. Exploring models and data for image\nquestion answering.NeurIPS 28 (2015).\n68. Marino, K., Rastegari, M., Farhadi, A. & Mottaghi, R. OK-VQA: A\nvisual question answering benchmark requiring external knowl-\nedge. In:CVPR 3195–3204 (2019).\n6 9 . S c h w e n k ,D . ,K h a n d e l w a l ,A . ,C l a r k ,C . ,M a r i n o ,K .&M o t t a g h i ,R .A -\nOKVQA: A benchmark for visual question answering using world\nknowledge. In:ECCV (Eds. Avidan, S., Brostow, G., Cissé, M.,\nFarinella, G. M. & Hassner, T.) 146–162 (Springer, 2022)..\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 12\n70. Shah, S., Mishra, A., Yadati, N. & Talukdar, P. P. KVQA: knowledge-\naware visual question answering.AAAI 33,8 8 7 6–8884 (2019).\n71. Lu, P. et al. Learn to explain: multimodal reasoning via thought\nchains for science question answering.NeurIPS 35,\n2507–2521 (2022).\n7 2 . Y u ,L . ,P o i r s o n ,P . ,Y a n g ,S . ,B e r g ,A .C .&B e r g ,T .L .M o d e l i n g\ncontext in referring expressions. In:ECCV. (Eds. Leibe, B., Matas,\nJ., Sebe, N. & Welling, M.). 69–85 (Springer, 2016).\n73. Du, Y. et al. What makes for good visual instructions? Synthesizing\ncomplex visual reasoning instructions for visual instruction tun-\ning. InProc. COLING(2025).\n74. Zellers, R., Bisk, Y., Farhadi, A. & Choi, Y. From recognition to\ncognition: visual commonsense reasoning. In:CVPR (2019).\n75. Suhr, A., Lewis, M., Yeh, J. & Artzi, Y. A corpus of natural language\nfor visual reasoning. In:ACL 217–223 (2017).\n76. Liu, F. et al. Mitigating hallucination in large multi-modal models\nvia robust instruction tuning. InProc. ICLR(2024).\n77. Chen, J. et al. GeoQA: a geometric question answering benchmark\ntowards multimodal numerical reasoning. InProc. ACL Find-\nings (2021).\n7 8 . C h e r i a n ,A . ,P e n g ,K . - C . ,L o h i t ,S . ,S m i t h ,K .A .&T e n e n b a u m ,J .B .\nAre deep neural networks smarter than second graders? In:CVPR\n10834–10844 (2023).\n79. Mishra, A., Shekhar, S., Singh, A. K. & Chakraborty, A. OCR-VQA:\nVisual question answering by reading text in images. In:ICDAR.\n(2019).\n80. Biten, A. F. et al. Scene text visual question answering. In:CVPR\n4291–4301 (2019).\n81. Tanaka, R., Nishida, K. & Yoshida, S. VisualMRC: Machine reading\ncomprehension on document image.AAAI 35,\n13878–13888 (2021).\n82. Ka ﬂe, K., Price, B., Cohen, S. & Kanan, C. DVQA: understanding\ndata visualizations via question answering. In:CVPR 5648–5656\n(2018).\n83. Kahou, S. E. et al. FigureQA: an annotatedﬁgure dataset for visual\nreasoning. InProc. ICLR(2018).\n84. Masry, A., Long, D. X., Tan, J. Q., Joty, S. & Hoque, E. ChartQA: a\nbenchmark for question answering about charts with visual and\nlogical reasoning. InProc. ACL Findings(2022).\n85. Svetlichnaya, S. DeepForm: understand structured documents at\nscale. InWeights & Biases report(2020).\n86. Chen, W. et al. TabFact: a large-scale dataset for table-based fact\nveriﬁcation. InProc. ICLR(2020).\n87. Mathew, M. et al. InfographicVQA. In:WACV 1697–1706\n(2022).\n88. Stanis ławek, T. et al. Kleister: Key information extraction datasets\ninvolving long documents with complex layouts. In:ICDAR (Eds.\nLladós, J., Lopresti, D. & Uchida, S.). 564–579 (Springer, 2021).\n89. Pasupat, P. & Liang, P. Compositional semantic parsing on semi-\nstructured tables. InProc. ACL(2015).\n90. Ahmed, S., Jawade, B., Pandey, S., Setlur, S. & Govindaraju, V.\nRealCQA: Scientiﬁc chart question answering as a test-bed for\nﬁrst-order logic. In:ICDAR (Eds. Fink, G. A., Jain, R., Kise, K. &\nZanibbi, R.). 66–83 (Springer, 2023).\n91. Kembhavi, A. et al. A diagram is worth a dozen images. In:ECCV\n(Eds. Leibe, B., Matas, J., Sebe, N. & Welling, M.). 235–251\n(Springer, 2016).\n92. Shin, A., Ushiku, Y. & Harada, T. The color of the cat is gray: 1\nmillion full-sentences visual question answering (FSVQA). Preprint\nat https://arxiv.org/abs/1609.06657(2016).\n93. Das, A. et al. Visual Dialog. In:CVPR 326–335 (2017).\n94. Zhang, Y. et al. LLaVAR: enhancedvisual instruction tuning for\ntext-rich image understanding. Preprint athttps://arxiv.org/abs/\n2306.17107(2023).\n95. Carter, J. TextOCR‑GPT4V. In Hugging Face dataset reposi-\ntory (2024).\n96. Zhao, B., Wu, B. & Huang, T. SVIT: scaling up visual instruction\ntuning. Preprint athttps://arxiv.org/abs/2307.04087(2023).\n97. Yu, T. et al. Reformulating vision-language foundation models and\ndatasets towards universal multimodal assistants. Preprint at\nhttps://arxiv.org/abs/2310.00653(2023).\n98. Chen, L. et al. ShareGPT4V: Improving large multi-modal models\nwith better captions. InProc. ECCV(2024).\n9 9 . G u p t a ,A . ,D o l l a r ,P . &G i r s h i c k ,R .L V I S :ad a t a s e tf o rl a r g ev o c a -\nbulary instance segmentation. In:CVPR 5356–5364 (2019).\n100. Chen, G. H. et al. ALLaVA: harnessing GPT4V-synthesized data for\na lite vision-language model. Preprint athttps://arxiv.org/abs/\n2402.11684(2024).\n101. Ding, N. et al. Enhancing chat language models by scaling high-\nquality instructional conversations. InProc. EMNLP(2023).\n102. Taori, R. et al. Stanford Alpaca: An instruction‑following LLaMA\nmodel. InGitHub repository(2023).\n103. Zheng, L. et al. Judging LLM-as-a-judge with MT-Bench and\nChatbot Arena.NeurIPS 36 (2024).\n104. BELLEGroup. BELLE: Be Everyone’s Large Language model\nEngine. InGitHub repository(2023).\n105. Lian, W. et al. OpenOrca: an open dataset of GPT‑augmented\nFLAN reasoning traces. InHugging Face dataset repository\n(2023).\n106. Teknium. OpenHermes 2.5: An open dataset of synthetic data for\ngeneralist LLM assistants. InHugging Face dataset reposi-\ntory (2023).\nAcknowledgements\nZ.L. and M.S. are supported by the Research Project 2025QGS16007.\nY.Y. is supported by the Shanghai Qi Zhi Institute Innovation Program\nSQZ202410.\nAuthor contributions\nYuan Yao initiated and led the research project, and designed the\nmodels and experiments. Tianyu Yu, Chongyi Wang, Junbo Cui, Hongji\nZhu, Haoyu Li, Zhihui He, Haoye Zhang, Zhi Zheng, Jie Zhou and Jie Cai\ncontributed to the experiments. Chi Chen, Ao Zhang and Yuan Yao wrote\nthe paper. Tianchi Cai, Weilin Zhao, Qianyu Chen, Ronghua Zhou and\nZhensheng Zou contributed to the open-source work. Shengding Hu, Xu\nHan, Guoyang Zeng, Dahai Li, Zhiyuan Liu and Maosong Sun provided\nvaluable suggestions and proofread the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-61040-5.\nCorrespondenceand requests for materials should be addressed to\nZhiyuan Liu or Maosong Sun.\nPeer review information: Nature Communicationsthanks the anon-\nymous, reviewer(s) for their contribution to the peer review of this work.\nAp e e rr e v i e wﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 13\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article's Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article's CreativeCommons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2025\nArticle https://doi.org/10.1038/s41467-025-61040-5\nNature Communications|         (2025) 16:5509 14",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.786225438117981
    },
    {
      "name": "Software deployment",
      "score": 0.7182544469833374
    },
    {
      "name": "Cloud computing",
      "score": 0.6358795762062073
    },
    {
      "name": "Mobile device",
      "score": 0.5725176334381104
    },
    {
      "name": "Limiting",
      "score": 0.5659995675086975
    },
    {
      "name": "Enhanced Data Rates for GSM Evolution",
      "score": 0.5651810765266418
    },
    {
      "name": "Server",
      "score": 0.501488208770752
    },
    {
      "name": "Edge device",
      "score": 0.45740818977355957
    },
    {
      "name": "Distributed computing",
      "score": 0.4496793746948242
    },
    {
      "name": "Edge computing",
      "score": 0.42963337898254395
    },
    {
      "name": "Language model",
      "score": 0.4193386137485504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40363389253616333
    },
    {
      "name": "Data science",
      "score": 0.38374415040016174
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3479152321815491
    },
    {
      "name": "Machine learning",
      "score": 0.33968278765678406
    },
    {
      "name": "Software engineering",
      "score": 0.21706074476242065
    },
    {
      "name": "World Wide Web",
      "score": 0.16711992025375366
    },
    {
      "name": "Operating system",
      "score": 0.16284751892089844
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}