{
  "title": "Appearance Prompt Vision Transformer for Connectome Reconstruction",
  "url": "https://openalex.org/W4385767927",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108192835",
      "name": "Rui Sun",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A5046327635",
      "name": "Naisong Luo",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2120395321",
      "name": "Yu-Wen Pan",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2128945953",
      "name": "Hua-yu Mai",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2134312029",
      "name": "Tianzhu Zhang",
      "affiliations": [
        "National Science Center",
        "Institute of Art",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2106043710",
      "name": "Zhiwei Xiong",
      "affiliations": [
        "Institute of Art",
        "University of Science and Technology of China",
        "National Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2097876706",
      "name": "Feng Wu",
      "affiliations": [
        "National Science Center",
        "University of Science and Technology of China",
        "Institute of Art"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2584383026",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2963833670",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386065545",
    "https://openalex.org/W4386075526",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4226228454",
    "https://openalex.org/W2621042378",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W2560374178",
    "https://openalex.org/W3167044851",
    "https://openalex.org/W4386076024",
    "https://openalex.org/W1898703532",
    "https://openalex.org/W4286527741",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2803790834",
    "https://openalex.org/W4283822559",
    "https://openalex.org/W4304014405",
    "https://openalex.org/W1756731880",
    "https://openalex.org/W2061628168",
    "https://openalex.org/W2895933766",
    "https://openalex.org/W2949124352",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2110232607",
    "https://openalex.org/W4312635568"
  ],
  "abstract": "Neural connectivity reconstruction aims to understand the function of biological reconstruction and promote basic scientific research. The intricate morphology and densely intertwined branches make it an extremely challenging task. Most previous best-performing methods adopt affinity learning or metric learning. Nevertheless, they either neglect to model explicit voxel semantics caused by implicit optimization or are hysteresis to spatial information. Furthermore, the inherent locality of 3D CNNs limits modeling long-range dependencies, leading to sub-optimal results. In this work, we propose a coherent and unified Appearance Prompt Vision Transformer (APViT) to integrate affinity and metric learning to exploit the complementarity by learning long-range spatial dependencies. The proposed APViT enjoys several merits. First, the extension continuity-aware attention module aims at constructing hierarchical attention customized for neuron extensibility and slice continuity to learn instance voxel semantic context from a global perspective and utilize continuity priors to enhance voxel spatial awareness. Second, the appearance prompt modulator is responsible for leveraging voxel-adaptive appearance knowledge conditioned on affinity rich in spatial information to instruct instance voxel semantics, exploiting the potential of affinity learning to complement metric learning. Extensive experimental results on multiple challenging benchmarks demonstrate that our APViT achieves consistent improvements with huge flexibility under the same post-processing strategy.",
  "full_text": "Appearance Prompt Vision Transformer for Connectome Reconstruction\nRui Sun1 , Naisong Luo1 , Yuwen Pan1 , Huayu Mai1 ,\nTianzhu Zhang1,2,3‚àó , Zhiwei Xiong1,2 and Feng Wu1,2\n1University of Science and Technology of China\n2Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3Deep Space Exploration Lab\n{issunrui, lns6, panyw, mai556}@mail.ustc.edu.cn, {tzzhang, zwxiong, fengwu}@ustc.edu.cn\nAbstract\nNeural connectivity reconstruction aims to un-\nderstand the function of biological reconstruction\nand promote basic scientific research. The intri-\ncate morphology and densely intertwined branches\nmakes it an extremely challenging task. Most pre-\nvious best-performing methods adopt affinity learn-\ning or metric learning. Nevertheless, they either\nneglect to model explicit voxel semantics caused\nby implicit optimization or are hysteresis to spa-\ntial information. Furthermore, the inherent local-\nity of 3D CNNs limits modeling long-range de-\npendencies, leading to sub-optimal results. In this\nwork, we propose a coherent and unified Appear-\nance Prompt Vision Transformer (APViT) to in-\ntegrate affinity and metric learning to exploit the\ncomplementarity by learning long-range spatial de-\npendencies. The proposed APViT enjoys several\nmerits. First, the extension continuity-aware at-\ntention module aims at constructing hierarchical\nattention customized for neuron extensibility and\nslice continuity to learn instance voxel semantic\ncontext from a global perspective and utilize con-\ntinuity priors to enhance voxel spatial awareness.\nSecond, the appearance prompt modulator is re-\nsponsible for leveraging voxel-adaptive appearance\nknowledge conditioned on affinity rich in spatial in-\nformation to instruct instance voxel semantics, ex-\nploiting the potential of affinity learning to comple-\nment metric learning. Extensive experimental re-\nsults on multiple challenging benchmarks demon-\nstrate that our APViT achieves consistent improve-\nments with huge flexibility under the same post-\nprocessing strategy.\n1 Introduction\nNeural connectivity reconstruction is a fundamental task to\nunderstand the function of biological reconstruction, which\ncan widely promote basic scientific research including elec-\ntrophysiology [Ascoli, 2002], cellular physiology [Donohue\nand Ascoli, 2011 ], genetics [Livet and Weissman, 2007 ],\n‚àóCorresponding author\nImage Stack\n3D CNN\nV oxel Embedding\nManual threshold \nbased optimization\nAffinity \nPredictor\nImage Stack Affinity Map\nImplict semantic \noptimization\nImage Stack\n(a) Metric learning methods (b) Affinity learning methods\nMetric\nLearning\nAffinity \nLearning\nV oxel Embedding\nContrastive \nLearning\nTransformer-based Model √óùêø Layers\n(c) APViT (ours)\nPrompt\nKnowledge\nTransfer\n3D CNN\nFigure 1: Different learning formulation for neural connectivity re-\nconstruction. (a) Metric learning methods with manual threshold\nbased optimization. (b) Affinity learning methods with an intuitive\nyet implicit optimization. (c) Our proposed APViT absorbs the mer-\nits of both metric learning and affinity learning methods by learning\nlong-range spatial dependencies to model spatially-aware voxel se-\nmantics in an explicit and flexible optimization strategy.\netc. 3D electron microscopy (EM) is the only available\nimaging instrument with the sufficient resolution to visual-\nize and reconstruct dense neural morphology without ambi-\nguity. However, at this resolution, even moderately small\nneural circuits yield numerous neuron numbers (e.g., typi-\ncally hundreds of neuron instances in a single megapixel im-\nage [Meirovitch et al., 2019a]) that are prohibitively labori-\nous for human manual annotation (e.g., normally the human\nlabor required to reconstruct a 100 3 ¬µm3 volume is at more\nthan 100,000 hours [Berning et al., 2015]). Recently, consid-\nerable works [Funke et al., 2018; Januszewski et al., 2018;\nMeirovitch et al., 2019b] have turned their attention to deep\nneural networks in the pursuit of automatic neural connec-\ntivity reconstruction. Since all neuron instances are of the\nsame type (i.e., biological cells), with intricate morphology\nand densely intertwined branches, how to fully probe discrim-\ninative information to perform accurate neuron reconstruction\nis thus extremely challenging.\nTo tackle the neural connectivity reconstruction problem,\nexisting methods can be roughly categorized as object track-\ning based and boundary detection based paradigms. In the\nobject tracking based paradigm [Januszewski et al., 2018;\nMeirovitch et al., 2016], 3D recurrent convolutional neural\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1423\nnetworks (CNNs) are trained to iteratively extend one neu-\nron object at a time, but this process is time-consuming and\ninappropriate for large-scale applications. The boundary de-\ntection based paradigm [Funke et al., 2018; Leeet al., 2017]\ntends to adopt the CNN in pursuit of the relationship between\nvoxel pairs for perceiving neuron boundaries, which are post-\nprocessed to yield neuron segmentation in a gradual agglom-\neration manner (e.g., mutex watershed [Wolf et al., 2018]).\nOur work follows the boundary detection based paradigm\ncredited to achieving competitive performance (e.g., top tier\nin SNEMI3D [Lee et al., 2017] neuron segmentation chal-\nlenge) while sustaining an efficient pipeline for numerous\nneuron objects.\nIn the boundary detection based paradigm, two represen-\ntative methods are affinity learning and metric learning con-\nditioned on the network optimization strategy [Huang et al.,\n2022]. On one hand, affinity learning methods [Funke et\nal., 2018; Lee et al., 2017; Beier et al., 2017] transform the\nground truth into affinity to constrain the network to learn\nthe relationship between voxel pairs in an intuitive yet im-\nplicit optimization manner, seeking perceptibility to spatial\nposition and discrimination to adjacent neuron instances with\nsimilar appearance (see Figure 1b). However, these methods\ndirectly output multi-channel maps as affinities, which tend\nto suffer from the absence of explicit voxel semantics, lead-\ning to confusion about long-range voxel correlation (affin-\nity). On the other hand, the work [Lee et al., 2021] takes\nadvantage of metric learning to pull voxels belonging to the\nsame neuron instance together and push those of unrelated in-\nstances away on top of manual threshold based optimization,\nperforming well in preserving instance semantic information\nwell, which is crucial for further improvements in accuracy\n(see Figure 1a). However, hand-crafted threshold is fragile\nin flexibility and robustness for different datasets. Moreover,\nrelying solely on the optimization of voxel embeddings in-\nevitably compromises the spatial information, and this nega-\ntive impact is inevitably amplified by inbuilt localized recep-\ntive fields of 3D CNNs. Overall, the above analysis indicates\nthat affinity learning methods and metric learning methods\nare naturally complementary. The former preserves the spa-\ntial information well and possesses a more flexible yet im-\nplicit optimization, but suffers from the absence of explicit\nvoxel semantics, while the latter is just the other way around.\nBesides, the inherent locality of 3D CNNs limits both formu-\nlations to modeling long-range dependencies and capturing\nglobal voxel context, leading to sub-optimal results. There-\nfore, it is more desirable to integrate these two formulations to\nexploit their complementary potential by learning long-range\nspatial dependencies to model spatial-aware voxel semantics\nin an explicit and flexible optimization strategy.\nMotivated by the above discussions, we propose a coherent\nand unified Appearance Prompt Vision Transformer (APViT)\nto enable appearance knowledge conditioned on affinity to\ninstruct voxels with explicit semantics from a global per-\nspective based on metric learning (Figure 1c), including an\nextension continuity-aware attention module and an appear-\nance prompt modulator. In the extension continuity-aware\nattention module, we construct hierarchical attention cus-\ntomized for neuron extensibility and slice continuity to learn\ninstance voxel semantic context from a global perspective and\nutilize continuity priors to enhance voxel spatial awareness.\n(a) Neuron extensibility. Considering that interleaved dif-\nferent neuron instances contain intricate morphology, which\ntends to extend from one end of the input 3D volume to an-\nother. Therefore, to endow voxel semantics with long-range\ndependencies, we take advantage of the extension-aware at-\ntention mechanism to aggregate global context to each voxel\nposition to obtain robust context-aware voxel embeddings\nthat can adapt to extended neuronal morphology. (b) Slice\ncontinuity. Intuitively, the neural deformation across several\ncontiguous slices is always smooth and continuous. Thus, we\nemploy the continuity-aware attention mechanism for aggre-\ngating information in a corresponding 3D spatial neighbor-\nhood for each voxel location, aiming to empower the discrim-\nination to neighboring neuron instances. In the appearance\nprompt modulator, we draw inspiration from the prompt-\nbased learning [Jia et al., 2022], which provides a general\nparadigm for specific knowledge learning from offline train-\ning, and leverage voxel-adaptive appearance knowledge con-\nditioned on affinity rich in spatial information to instruct\ninstance voxel semantics, exploiting the potential of affin-\nity learning to complement metric learning. In specific, we\nprepend a set of appearance prompts encapsulated in prompt\nbase to interact with voxel features by cross-attention mecha-\nnism to obtain voxel-adaptive appearance prompts, achieving\nappearance knowledge extraction. Then the resultant voxel-\nadaptive appearance prompts are leveraged to modulate the\nvoxel feature rich in semantic pattern conditioned on the cal-\nculated affinity to enhance spatial awareness. Besides, we\nimpose the diversity loss to expand the discrepancy among\nprompts, enabling prompts to carry diverse and comprehen-\nsive knowledge for voxel appearance. For training, we op-\ntimize the model with centroid-anchored contrastive learn-\ning to well structure the voxel embedding space against the\ncoarseness of manual threshold.\nAn attention-based task information modeling algorithm is\nproposed. To solve the problem that the average pooling oper-\nation in traditional task embedding generation methods is too\ncoarse, this algorithm introduces the attention mechanism to\ncapture the important difference of different samples, so as\nto extract more accurate task information. The algorithm uti-\nlizes learnable task vectors to store task information and uses\nan attention mechanism to identify and assign high weights\nto critical samples, then aggregate sample features to model\ntask information. To verify the effectiveness of the proposed\nmethod, extensive experiments are carried out on several stan-\ndard few-shot image classification datasets. Experimental re-\nsults show that the proposed attention based task information\nmodeling algorithm achieves better performance compared\nwith the existing methods. Furthermore, our APViT can adapt\nto multiple post-processing operations (e.g., waterz[Funke et\nal., 2018]), in other words, it could be possible to enjoy the\nflexibility with a single trained model via adaptive modula-\ntion of the post-processing configuration at the test time.\nThe contributions of our method could be summarized as\nfollows: (1) We propose an appearance prompt vision trans-\nformer tailored for the connectome reconstruction in a co-\nherent and unified framework. Specifically, we design the\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1424\nextension continuity-aware attention module to construct hi-\nerarchical attention customized for neuron extensibility and\nslice continuity, the appearance prompt modulator to exploit\nthe potential of affinity learning to complement metric learn-\ning. (2) To the best of our knowledge, this is the first work to\nabsorb the merits of both affinity learning and metric learn-\ning formulation by learning long-range spatial dependencies\nto model spatially-aware voxel semantics in an explicit and\nflexible optimization strategy. (3) Extensive experimental re-\nsults on multiple challenging benchmarks demonstrate that\nour APViT achieves consistent improvements with huge flex-\nibility under the same post-processing strategy.\n2 Related Work\n2.1 Connectome Reconstruction\nThe reconstruction of connectomes has tremendous biolog-\nical significance for studying neuronal morphology and ac-\ntivity. Deep learning-based methods have paved the way\nfor research, which can be roughly divided into two cate-\ngories: object tracking based methods and boundary detec-\ntion based methods. Among them, the object tracking based\nmethods [Januszewski et al., 2018; Meirovitch et al., 2016]\nonly reconstruct a single neuron at a time, which is inefficient\nand time-consuming. In contrast, boundary detection based\nmethods exhibit superior performance. Among them, [Funke\net al., 2018; Leeet al., 2017] take advantage of affinity learn-\ning to separate connectomes with similar appearances. [Lee\net al., 2021] aggregates voxels belonging to the same con-\nnectome with the help of metric learning and utilizes hand-\ncrafted threshold to optimize the reconstruction result. How-\never, the affinity learning paradigm tends to suffer from the\nabsence of explicit voxel semantics, while the metrics learn-\ning one inevitably drops the spatial information, and it is\ngreatly affected by the artificial threshold. [Huang et al.,\n2022] absorbs the merits of both affinity learning and met-\nric learning methods but fails to model long-range dependen-\ncies due to the locality of 3D CNNs. Different from those\nmethods, we propose to integrate affinity learning and metric\nlearning via a unified Appearance Prompt Vision Transformer\nto alleviate the above problems and accomplish the task of\nconnectome reconstruction in an explicit yet flexible manner.\n2.2 Vision Transformer and Prompt Learning\nVision Transformer.Transformer was originally introduced\nin [Vaswani et al., 2017] for machine translation. Many ef-\nforts [Sun et al., 2021; Wang et al., 2022; Mai et al., 2023;\nLuo et al., 2023; Wang et al., 2023; Chen and Lian, 2022 ]\nhave also been made to apply it to vision tasks, including ob-\nject detection, image classification and image segmentation.\nViT [Dosovitskiy and Beyer, 2020] applies a transformer ar-\nchitecture on sequences of image patches to capture global\ncues for image classification tasks, building a new foundation\nfor numerous vision tasks. Besides, emerging from trans-\nformer, prompt-based learning has been proven effective in\nNLP tasks by importing language instruction (prompt) to the\ninput text so that the language model can perform well for\nthe downstream tasks. For example, VPT [Jia et al., 2022]\ndynamically learns a set of trainable prompts to acquire task-\nspecific information. Nonetheless, the remaining problem is\nthat it is not suitable to learn generic prompts for scenario\nadaptation. Hence we design a prompt-aware transformer to\nmodel adaptive prompts for different connectomes.\nPrompt Learning. Prompting [Liu et al., 2021] originally\nrefers to inserting a few instructions to the input sentences\nin the NLP tasks [Gao et al., 2021]. Many recent works [Li\nand Liang, ; Gu et al., 2022] propose to exploit the prompt-\ning techniques to deal with different downstream tasks or do-\nmains with the combination of transformers without optimiz-\ning all of the parameters. In this paper, we prepend a set\nof appearance prompts to modulate the voxel embedding for\nbetter instructing instance voxel semantics.\n3 Method\nIn this section, we first present the overall architecture of the\nAppearance Prompt Vision Transformer (APViT) as shown\nin Figure 2, and then describe each component in detail.\n3.1 Overview\nAPViT enables appearance knowledge conditioned on affin-\nity to instruct voxels from a global perspective based on met-\nric learning, that is, exploits the complementary potential of\nmetric learning and affinity learning, and has four stages (in-\ndexed with i). Each stage of APViT encapsulates a patch\nembedding, Li extension continuity-aware attention module\n(ECAM, Section 3.2), and an appearance prompt modulator\n(APM, Section 3.3). For training, we optimize the model with\ncentroid-anchored contrastive learning (Section 3.4) to well\nstructure the voxel embedding space against the coarseness\nof manual threshold in previous work.\nIn specific, given the input neuron volume I ‚àà RD√óH√óW ,\nwhere D, H, and W denote depth, height and width, respec-\ntively. In the first stage, we first divide I into D √ó H\n2 √ó W\n2\npatches, each of size 1 √ó 2 √ó 2. Then, we feed the flattened\npatches to a linear projection and obtain embedded patches of\nsize DHW\n22 √óC1. After that, the embedded patches are passed\nthrough a APViT block, and the output is reshaped to a fea-\nture map F1 of size D √ó H\n2 √ó W\n2 √ó C1. In the same way, at\nthe beginning of each stage i, using the feature map from the\nprevious stage as input, we obtain the feature map Fi of size\nD √ó H\nPi\n√ó W\nPi\n√ó Ci, where Pi = 2i, and i = {1, 2, 3, 4}.\n3.2 Extension Continuity-aware Attention Module\nConsidering that all previous methods based on metric learn-\ning and affinity learning employ 3D convolutions, however,\nthe inherent locality of 3D CNNs limits both formulations\nto modeling long-range dependencies and capturing global\nvoxel context, leading to sub-optimal results. Therefore, we\ndevelop an extension continuity-aware attention module to\nconstruct hierarchical attention customized for neuron exten-\nsibility and slice continuity to learn instance voxel semantic\ncontext from a global perspective and utilize continuity priors\nto enhance voxel spatial awareness.\nExtension-aware Attention. Considering that interleaved\ndifferent neuron instances contain intricate morphology,\nwhich tends to extend from one end of the input 3D volume\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1425\nPatch Embedding\nLayer Norm\nExtension-aware\nAttention\nLayer Norm\nFeed Forward\nAppearance Prompt \nModulator\nùë´√óùëØ\nùüê √óùëæ\nùüê √óùë™ùüè\n√óùë≥ùüè\nStage 1\nStage  2\nUpsampling  Module\nRaw Images\nStage  3\nùë´√óùëØ\nùüê √óùëæ\nùüê √óùë™ùüè\nùë´√óùëØ\nùüí √óùëæ\nùüí √óùë™ùüê\nùë´√óùëØ\nùüñ √óùëæ\nùüñ √óùë™ùüë\nStage 4\nùë´√ó ùëØ\nùüèùüî√ó ùëæ\nùüèùüî√óùë™ùüí\nPost-processing\nVoxel Embedding\nReconstruction\nExtension Continuity-aware \nAttention Module\nPrompt Base\n1\nRetrieve\nContinuity-aware\nAttention\nFigure 2: The overview of the APViT framework.Raw images are processed in four consecutive stages, each stage of APViT encapsulates\na patch embedding, Li extension continuity-aware attention module (ECAM, Section 3.2) to extract hierarchical features and an appearance\nprompt modulator (APM, Section 3.3) to learn voxel-adaptive appearance knowledge. After the upsampling module aggregating features\nfrom different stages, the final reconstruction result can be obtained through a post-processing step.\nto another, we specially design an Extension-aware Attention\nModule (AttE) to obtain semantic-rich voxel embedding with\nlong-range dependencies. Specifically, given the feature map\nF ‚àà RD√óH\nP √óW\nP √óC (omit the subscript for brevity), we first\nflatten the spatial dimensions to produce a feature sequence\neF = R\nDHW\nP2 √óC. Queries, keys and values arise from the fea-\nture sequence as follow:\nQ = eFWQ, K = eFWK, V = eFWV , (1)\nwhere WQ ‚àà RC√óCk , WK ‚àà RC√óCk , WV ‚àà RC√óCv\nare linear projections. Then we can calculate the extension-\nattention weight matrix S ‚àà R\nDHW\nP2 √óDHW\nP2 with the scaled\ndot-product and the output are computed byS-weighted sum-\nmation on value V:\nAttE(F) =SV = Softmax(QK‚ä§\n‚àö\nC\n)V, (2)\nwhere\n‚àö\nC is a scaling factor for stabilizing the training and\nthe ‚ä§ denotes the transpose operation. Following the standard\ntransformer[Vaswani et al., 2017], the Eq. 2 is implemented\nwith the multi-head mechanism and the feed-forward network\n(FFN) is further applied to obtain the final output.\nContinuity-aware Attention. Intuitively, the neural defor-\nmation across several contiguous slices is always smooth and\ncontinuous. Thus, we employ the Continuity-aware Atten-\ntion mechanism (AttC) for aggregating information in a cor-\nresponding 3D spatial neighborhood for each voxel location,\naiming to empower the discrimination to neighboring neu-\nron instances. Predefining a spatial window œÉ(Œª) centered at\nvoxel Œª with size of z √ó p √ó p, we denotes FœÉ(Œª) ‚àà Rzp2√óC\nas the features of neighbourhood voxels andfŒª as the feature\nof voxel Œª. Then, denoting fŒª as query, FœÉ(Œª) as keys and\nvalues, we can get q, K, V by:\nq = fŒªWQ, K = FœÉ(Œª)WK, V = FœÉ(Œª)WV . (3)\nd\na\n b\nc\nPrompt Base\nRetrieve\nPrompt\nModulator\n a‚Äô\nUpdate\n d‚Äô\na\nc‚Äô\nb‚Äô\nb‚Äô\nRaw voxel \nfeature\nAppearance \nPrompts\nPrompt-refined\nfeature\nPositive/Negative\nAffinity\nVoxel Features\nReferred \nVoxel\nVoxel-adaptive \nAppearance \nPrompts\nFigure 3: Illustration of the appearance prompt modulator. After\ncalculating the affinity of each voxel in three directions, the trans-\nferred prompts are then adaptively retrieved from the prompt base to\ninteract with the prompt modulator to update the voxel features.\nSimilar to Eq. 2, the output of AttC can be calculated by:\nAttC(fŒª) = Softmax(qK‚ä§\n‚àö\nC\n)V. (4)\nTreating D √ó H\nP √ó H\nW voxels for F in the same way, we can\nget AttC(F). The above two attention layers work in parallel\nand the output of them are added and fed into next extension\ncontinuity-aware attention module.\n3.3 Appearance Prompt Modulator\nIn order to exploit the potential of affinity learning to comple-\nment metric learning, we propose an appearance prompt mod-\nulator, leveraging voxel-adaptive appearance knowledge con-\nditioned on affinity to instruct instance voxel semantics. We\nintroduce a prompt base Pi = {pi\nn}Ni\nn=1, where Ni refers to\nthe number of appearance prompts for stage i ‚àà {1, 2, 3, 4}.\nThe prompts will be leveraged to extract appearance knowl-\nedge by interaction with voxel features, then modulate the\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1426\nvoxel feature conditioned on the calculated affinity, which\nhighly represents spatial association between voxels. In spe-\ncific, as shown in Figure 3, for an arbitrary voxel a with fea-\nture fi\na, we calculate its affinity with adjacent voxel b, c, dat\n3 directions, respectively. For simplicity, take b as example,\nthe affinity can be formulated as\nAa,b = max\n \n0, (fi\na)‚ä§fi\nb\n‚à•fia‚à•2\n\r\rfi\nb\n\r\n\r\n2\n!\n. (5)\nThe affinity describes the similarity between two spatially ad-\njacent voxels. If the affinity is close to 1, it indicates that the\ntwo voxels have a high probability of belonging to the same\ninstance; otherwise, 0 indicates different instances. We use\nthe voxel feature fi\na to retrieve the appearance prompts from\nprompt base Pi and obtain voxel-adaptive prompts, as\nÀÜpi\na = fi\na + Softmax\n\u0012fi\na(Pi)‚ä§\n‚àö\nCi\n\u0013\nPi. (6)\nThe voxel-adaptive appearance prompt ÀÜpi\na is the linear com-\nbination of the prompts conditioned on voxel a, and is utiliz-\ning to modulate the feature of voxel b as follow,\nÀÜfi\nb = fi\nb + Aa,b ¬∑ ÀÜpi\na. (7)\nIn order to enable prompts to carry diverse and comprehen-\nsive knowledge for voxel appearance, we impose the diversity\nloss on the prompt base P. Formally,\nLdiv =\n4X\ni=1\nNiX\nm=1\nNiX\nn=1,mÃ∏=n\ncos(pi\nm, pi\nn), (8)\nwhere the cos(¬∑, ¬∑) denotes cosine similarity.\n3.4 Training Objectives\nAfter consecutive four stages process, the output of each stage\nwill be fed into an upsampling module [Hatamizadeh et al.,\n2022] to restore the original size and get the final voxel em-\nbedding E ‚àà RD√óH√óW√óC. To well structure the voxel em-\nbedding space, we design two centroid-anchored contrastive\nloss. Firstly, we calculate the centroid\nei for each instance\ni, by averaging the embedding of the voxels belonging to in-\nstance i based on the ground truth. With the set of centroid\n{\nei}N\ni=1 (N denotes the number of instances), we can get:\nLc1 =\nNX\ni=1\nX\ne‚ààEi\n‚àílog exp(e‚ä§ei/Œµ)\nexp(e‚ä§ei/Œµ)+P\ne‚àí‚ààE‚àíexp(e‚ä§e‚àí/Œµ),\n(9)\nwhere Ei denotes the set of voxel embeddings belonging to\ninstance i, E‚àí= {ej}N\nj=1\n\u000e\nei, and the temperature Œµ controls\nthe concentration level of representations. Intuitively, Eq. 9\nenforces each voxel embeddinge to be similar with its ground\ntruth (‚Äòpositive‚Äô) centroid and dissimilar with other irrelevant\n(‚Äònegative‚Äô) centroids. Another contrastive loss is proposed\nfor compactness by directly minimizing the distance between\neach embedded voxel and its ground truth centroid:\nLc2 =\nX\ne‚ààEi\n(1 ‚àí e‚ä§\nei)2. (10)\nNote that both e and ei are ‚Ñì2-normalized. As a result, our\noverall training objective is formulated as:\nL = Lc1 + Lc2 + Œªdiv √ó Ldiv, (11)\nwhere Œªdiv is the trade-off weight.\n4 Experiments\n4.1 Experimental Setup\nDatasets. Two commonly used neuron datasets, named\nCREMI [Funke et al.,] and AC3/AC4 [Arganda-Carreras et\nal., 2015], are used for the evaluation of our method. CREMI\ndataset is divided into three sub-datasets, each consisting of\ntwo volumes of size 125 √ó 1250 √ó 1250 for training and\ntesting, respectively. We use the volume with public ground\ntruth as the training and testing set, which consists 100 and\n25 slices, respectively. The AC3/AC4 is used for SNEMI3D\nchallenge, where size of AC3 is 256 √ó 1024 √ó 1024 and\nAC4 consists of 100 √ó 1024 √ó 1024 voxels. Following the\nSNEMI3D challenge, We use the top 80 slices of AC4 as\ntraining set and the rest of AC4 as validation set. And the\ntop 100 slices of AC3 are testing set.\nImplementation Details.In our APViT, the number of layers\nis {1, 2, 4, 2}. The volume size of the input is anisotropic (18,\n160, 160), and the patch size is (1, 2, 2) at each stage. During\ntraining, our model is trained with batch size of 2, using the\nAdam optimizer with an initial learning rate of 0.0001 for\n200,000 iterations. And we constrain the output at different\nresolutions for each stage with GT as an auxiliary loss where\nwe set Œªdiv = 0.1.\nEvaluation Metrics.Following the conventions, V OI(vari-\nation of information) is deemed as the main evaluation metric.\nWe also report ARAND (adapted Rand error) to assess the\nreconstruction results. Smaller values of these two metrics\nindicate better segmentation performance.\n4.2 Comparison with State-of-the-art Methods\nTables 1 and 2 report the neuron reconstruction performance\ncomparison of our method and several state-of-the-art meth-\nods on AC3/AC4 and CREMI datasets, respectively. As\nshown in Table 1, we replace the feature extractors of several\nmethods by our APViT. The insertion of our APViT can bring\nsignificant performance improvements to different baseline\nmethods. For example, when we adopt MALA [Funke et al.,\n2018] as the baseline method and LMC as the post-processing\napproach, the V OIand the ARAND metrics improve from\n1.3857 to 1.0235 and from 0.1143 to 0.0898, respectively.\nFrom Table 2, we can observe that our proposed APViT\noutperforms all previous methods by a substantial margin.\nSpecifically, APViT surpasses the second-best method (PEA)\non the VOI metric by 12.01%, 13.56%, 11.81% on Cremi-A,\nCremi-B, Cremi-C, respectively.\n4.3 Ablation Study and Analysis\nTo look deeper into our method, we perform a series of\nablation studies on AC3/AC4 dataset with waterz as post-\nprocessing to validate the effectiveness of APViT, including\nthe extension continuity-aware attention module (ECAM),\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1427\nMethod Waterz LMC MWS\nV O\nIsplit V OImerge V OI ARAND V O\nIsplit V OImerge V OI ARAND V O\nIsplit V OImerge V OI ARAND\nML-De - -\n- - - -\n- - 1.5752 0.6151\n2.1903 0.1964\nSuperHuman 1.0910 0.3418\n1.4328 0.1685 1.1443 0.2630\n1.4073 0.1221 - -\n- -\nOurs 0.9222 0.3305 1.2527 0.1228 0.9001 0.2208 1.1209 0.0942 0.7257 0.5017 1.2274 0.1364\nMALA 1.0988 0.2446\n1.3434 0.1089 1.1457 0.2400\n1.3857 0.1143 - -\n- -\nOurs 0.8358 0.1945 1.0303 0.0840 0.8259 0.1976 1.0235 0.0898 0.8417 0.3399 1.1815 0.0912\nPEA 0.9116 0.2934\n1.2050 0.1212 0.8999 0.2506\n1.1505 0.1069 0.8522 0.2322 1.0844 0.0938\nOurs 0.7671 0.2093 0.9764 0.0775 0.8231 0.2054 1.0285 0.0940 0.5943 0.3842 0.9785 0.0865\nTable 1: Comparisons of different methods on AC3/AC4 dataset.\nMethod Post-processing Cremi-A Cremi-B Cremi-C\nV O\nIsplit V OImerge V OI ARAND V O\nIsplit V OImerge V OI ARAND V O\nIsplit V OImerge V OI ARAND\nSuperHuman Waterz 1.0581 0.3884\n1.4465 0.2167 0.8095 0.1469 0.9564 0.0443 0.9791 0.3992\n1.3782 0.1563\nLMC 1.0883 0.4232\n1.5114 0.2438 0.8281 0.1867\n1.0148 0.0468 1.0017 0.2742\n1.2760 0.1202\nMALA Waterz 0.5508 0.2371 0.7879 0.1251 0.8810 0.1685\n1.0496 0.0482 1.1493 0.1963 1.3456 0.1308\nLMC 0.5263 0.2596\n0.7859 0.1177 0.9688 0.2005\n1.1694 0.0612 1.2016 0.2371\n1.4387 0.1365\nPEA Waterz 0.4892 0.3001\n0.7892 0.1546 0.6887 0.1978\n0.8865 0.0370 1.0247 0.2255\n1.2502 0.1128\nLMC 0.4774 0.2917\n0.7691 0.1425 0.6648 0.2183\n0.8831 0.0393 0.9983 0.2490\n1.2472 0.1146\nWaterz 0.4447 0.2595 0.7041 0.1169 0.5793 0.2014 0.7807 0.0319 0.8839 0.2341 1.1181 0.1102Ours LMC 0.4336 0.2914 0.7249 0.1304 0.5777 0.2162 0.7939 0.0340 0.8719 0.2527 1.1247 0.1116\nTable 2: Comparisons of different methods on CREMI dataset.\nMALA APViT (ours)\n3D  Reconstruction2D  Demonstration\nRaw Image\nGround Truth\nAffinity Affinity\n2D Reconstruction 2D Reconstruction\nSplit Error\nMALA APViT (ours)\nFigure 4: Comparison between MALA and APViT (ours) on\nAC3/AC4 datatset.\nthe appearance prompt modulator (APM), and the centroid-\nanchored contrastive learning. Note that we remove all pro-\nposed modules and only maintain the bald vision transformer,\nand take manual threshold based optimization following[Lee\net al., 2021] as our baseline.\nEffectiveness of Main Components. Table 4 summarizes\nthe results of module ablation studies under different con-\nModel #Params V\nOI ARAND\nCNN-based model 36.78M 1.1989\n0.1685\nTransformer-based model 35.43M 1.4073\n0.1321\nOurs 37.25M 0.9764 0.0775\nTable 3: Illustration of advantages of APViT on AC3/AC4 dataset.\nECAM APM\nContrastive learning V O\nI ARAND\n1.1989 0.1932\n‚úì 1.0998 0.1095\n‚úì 1.1698 0.1313\n‚úì\n‚úì 1.0282 0.0925\n‚úì\n‚úì ‚úì 0.9764 0.0775\nTable 4: Ablation on main components on AC3/AC4 dataset.\nfigurations. (1) We ablate the ECAM to study the impor-\ntance of hierarchical attention. As deteriorated results indi-\ncate, customized for neuron extensibility and slice continuity\nis crucial to learn voxel semantic context from a global per-\nspective and utilize continuity priors to enhance voxel spa-\ntial awareness. (2) Then we investigate the impact of intro-\nducing APM, and observe a absolute performance lift (from\n0.1932 to 0.1313 in ARAND). The improvements can be\nmainly ascribed to the strong ability of the APM to leverage\nvoxel-adaptive appearance knowledge conditioned on affinity\nto instruct voxel semantics, exploiting the potential of affinity\nlearning to complement metric learning. (3) We also explore\nthe centroid-anchored contrastive learning. When we replace\nthe optimization strategy with manual threshold based opti-\nmization [Lee et al., 2021], the performance of the model\nis clearly degraded. This proves the necessity of contrastive\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1428\nExtension-aw\nare attention Continuity-aware attention V O\nI ARAND\n1.1698 0.1313\n‚úì 1.1310 0.1083\n‚úì 1.0560 0.0899\n‚úì ‚úì 0.9764 0.0775\nTable 5: Ablation on different attention mechanisms on AC3/AC4.\nWindo\nw size at Stage 4 (z √ó x √ó y) VOI\nARAND\n1 √ó 7 √ó 7 1.1084 0.1415\n3 √ó 5 √ó 5 1.0285 0.0845\n5 √ó 3 √ó 3 1.0169 0.0795\n7 √ó 1 √ó 1 0.9764 0.0775\nTable 6: Ablation on window sizes in continuity-aware attention.\nlearning to well structure the voxel embedding space against\nthe coarseness and sensitivity of manual threshold. Without\nall the proposed methods, the model has degenerated into the\nbaseline. The performance improvement of our final model\nover the baselines is significant.\nAdvantages of Our Framework. To validate the advan-\ntage of our framework tailored for connectome reconstruc-\ntion, we perform an ablation study to investigate the impact\nof 3D CNNs (3D ResUNet), pure vision transformer (UN-\nETR [Hatamizadeh et al., 2022]), and our APViT with the\nsame parameters, as tabulated in Table 3. We observe that\ntransformer-based method outperforms CNN-based methods\ndue to long-range dependency modeling capabilities. Fur-\nthermore, our APViT achieves a significant lead, which in-\ndicates that instead of simply using the vision transformer,\nAPViT absorbs the merits of both affinity learning and metric\nlearning formulation to model spatially-aware voxel seman-\ntics in an explicit and flexible optimization strategy. More\nimportantly, it could be possible to enjoy the flexibility with\na single trained model via adaptive modulation of the post-\nprocessing configuration at the test time.\nEffectiveness of Extension Contiguity-aware Attention.\nTo analyze the ECAM in depth, we ablate extension-aware\nattention and continuity-aware attention separately, as de-\nscribed in Table 5. Adding either of the two attention mech-\nanisms contributes to a remarkable performance gain. Fur-\nthermore, these two mechanisms work in conjunction enables\nfurther performance gain, benefiting from learning instance\nvoxel semantic context from a global perspective and utiliz-\ning continuity priors to enhance voxel spatial awareness.\nLocal Window Size. In Table 6, we observe that the local\nwindow size in continuity-aware attention has a large im-\npact on reconstruction performance. Experiments show that\nAPViT achieves the best result at the window size of7√ó1√ó1.\nWe conjecture that voxels in the z-stereoscopic direction con-\ntain more connectome information, including connections be-\ntween slices, thus larger size along the z-axis of the local win-\ndow is more beneficial for feature extraction.\nPrompt Strategy.As shown in Table 7, we ablate the com-\nponents inside the prompt assignment module. In fact, the\nintroduction of a prompt can be deemed as a guidepost in ar-\ntificially solving incidents, i.e., guiding information, which\ncan strongly correct voxel features based on ECAM and en-\nNumbers of\nprompt at each stage (from 1 to 4) VOI\nARAND\n(3, 3,\n3, 3) 1.0125 0.0841\n(6,\n6, 6, 6) 1.0804 0.0956\n(9,\n9, 9, 9) 1.0544 0.0932\n(12,\n12, 12, 12) 1.1329 0.1510\n(12,\n9, 6, 3) 0.9764 0.0775\nTable 7: Ablation on different prompt numbers at each stage.\nRaw Image GT affinity Embedding\n(MALA)\nAffinity\n(MALA)\nEmbedding\n(ours)\nAffinity\n(ours)\nFigure 5: Visualization of the affinity and embedding.\nhance different instance clustering to avoid foreground and\nbackground confusion issues better, thus reducing splitting\nand fusion errors. Therefore, the number of prompts selected\nalso plays an important role in the prompt assignment mod-\nule, directly affecting the modeling ability of prompts. Com-\npared with using a fixed number of prompts at each stage, the\nflexible, prompt allocation strategy is obviously more advan-\ntageous for the diverse neuron reconstruction task, yielding\nsignificant improvements. The principal reason for this phe-\nnomenon is that the feature resolution processed by each en-\ncoder stage is different, making it necessary to customize the\nnumber of prompts according to different stages.\n4.4 Explainable Visualization Study\nWe visualize the reconstruction results of various methods on\nthe test set of AC3/AC4. It can be seen that each method per-\nforms well when reconstructing relatively simple and clear\nneurons, however, when faced with entangled adjacent neu-\nron individuals, the previous methods generally perform un-\nsatisfactorily. In Figure 4, MALA produces more over-\nsegmentation results (e.g., blue and red connectomes), sepa-\nrating neurons that should belong to the same label, result-\ning in split errors, while APViT is able to reconstruct the\ncorrect neuron. The underlying reason is that the extension\ncontinuity-aware attention module takes into account a wide\nrange of each neuron. Meanwhile, the continuity-aware atten-\ntion mechanism takes advantage of the smoothness and conti-\nnuity between slices, leading to less split error in ambiguous\nareas. In Figure 5, it can be observed that the embedding is\nthe clear cluster corresponding to each neuron instance. And\nthe affinity map obtained from our voxel embeddings does\nnot misjudge the confusing boundaries. It indicates that the\nappearance prompt modulator aggregates rich spatial infor-\nmation via the voxel-adaptive appearance prompt.\n5 Conclusion\nIn this paper, we propose we propose a coherent and uni-\nfied Appearance Prompt Vision Transformer (APViT) to en-\nable appearance knowledge conditioned on affinity to instruct\nvoxels with explicit semantics based on metric learning, in-\ncluding an extension continuity-aware attention module and\nan appearance prompt modulator. Extensive experimental re-\nsults on challenging benchmarks show effectiveness.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1429\nContribution Statement\nRui Sun, Naisong Luo and Yuwen Pan contributed equally to\nthis paper.\nAcknowledgments\nThis work was partially supported by the National Nature Sci-\nence Foundation of China (Grant 62022078, 62021001).\nReferences\n[Arganda-Carreras et al., 2015] Ignacio Arganda-Carreras,\nSrinivas C Turaga, and Berger. Crowdsourcing the cre-\nation of image segmentation algorithms for connectomics.\nFrontiers in neuroanatomy, page 142, 2015.\n[Ascoli, 2002] Giorgio A Ascoli. Computational neu-\nroanatomy: Principles and methods. Springer Science &\nBusiness Media, 2002.\n[Beier et al., 2017] Thorsten Beier, Constantin Pape, Nasim\nRahaman, Timo Prange, Stuart Berg, Davi D Bock, Albert\nCardona, Graham W Knott, Stephen M Plaza, Louis K\nScheffer, et al. Multicut brings automated neurite seg-\nmentation closer to human performance. Nature methods,\n14(2):101‚Äì102, 2017.\n[Berning et al., 2015] Manuel Berning, Kevin M Boergens,\nand Moritz Helmstaedter. Segem: efficient image analysis\nfor high-resolution connectomics. Neuron, 87(6):1193‚Äì\n1206, 2015.\n[Chen and Lian, 2022] Zenggui Chen and Zhouhui Lian.\nSemi-supervised semantic segmentation via prototypical\ncontrastive learning. In Proceedings of the 30th ACM In-\nternational Conference on Multimedia, pages 6696‚Äì6705,\n2022.\n[Donohue and Ascoli, 2011] Duncan E Donohue and Gior-\ngio A Ascoli. Automated reconstruction of neuronal mor-\nphology: an overview. Brain research reviews, 67(1-\n2):94‚Äì102, 2011.\n[Dosovitskiy and Beyer, 2020] Alexey Dosovitskiy and\nBeyer. An image is worth 16x16 words: Transform-\ners for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[Funke et al.,] Jan Funke, Eric Perlman, Srini Turaga, Davi\nBock, and Stephan Saalfeld. Cremi challenge leaderboard,\nas of 2017/22/09.\n[Funke et al., 2018] Jan Funke, Fabian Tschopp, William\nGrisaitis, Arlo Sheridan, Chandan Singh, Stephan\nSaalfeld, and Srinivas C Turaga. Large scale image seg-\nmentation with structured loss based deep learning for\nconnectome reconstruction. TPAMI, 41(7):1669‚Äì1680,\n2018.\n[Gao et al., 2021] Tianyu Gao, Adam Fisch, and Danqi\nChen. Making pre-trained language models better few-\nshot learners. In Chengqing Zong, Fei Xia, Wenjie Li, and\nRoberto Navigli, editors, ACL, pages 3816‚Äì3830. ACL,\n2021.\n[Gu et al., 2022] Yuxian Gu, Xu Han, Zhiyuan Liu, and Min-\nlie Huang. PPT: pre-trained prompt tuning for few-shot\nlearning. In ACL, pages 8410‚Äì8423. Association for Com-\nputational Linguistics, 2022.\n[Hatamizadeh et al., 2022] Ali Hatamizadeh, Yucheng Tang,\nVishwesh Nath, Dong Yang, Andriy Myronenko, Bennett\nLandman, Holger R Roth, and Daguang Xu. Unetr: Trans-\nformers for 3d medical image segmentation. In Proceed-\nings of the IEEE/CVF winter conference on applications\nof computer vision, pages 574‚Äì584, 2022.\n[Huang et al., 2022] Wei Huang, Shiyu Deng, Chang Chen,\nXueyang Fu, and Zhiwei Xiong. Learning to model pixel-\nembedded affinity for homogeneous instance segmenta-\ntion. In AAAI, 2022.\n[Januszewski et al., 2018] Micha≈Ç Januszewski, J ¬®orgen Ko-\nrnfeld, Peter H Li, Art Pope, Tim Blakely, Larry Lind-\nsey, Jeremy Maitin-Shepard, Mike Tyka, Winfried Denk,\nand Viren Jain. High-precision automated reconstruction\nof neurons with flood-filling networks. Nature methods,\n15(8):605‚Äì610, 2018.\n[Jia et al., 2022] Menglin Jia, Luming Tang, Bor-Chun\nChen, Claire Cardie, Serge Belongie, Bharath Hariharan,\nand Ser-Nam Lim. Visual prompt tuning. arXiv preprint\narXiv:2203.12119, 2022.\n[Lee et al., 2017] Kisuk Lee, Jonathan Zung, Peter Li, Viren\nJain, and H Sebastian Seung. Superhuman accuracy\non the snemi3d connectomics challenge. arXiv preprint\narXiv:1706.00120, 2017.\n[Lee et al., 2021] Kisuk Lee, Ran Lu, Kyle Luther, and H Se-\nbastian Seung. Learning and segmenting dense voxel em-\nbeddings for 3d neuron reconstruction.TMI, 40(12):3801‚Äì\n3811, 2021.\n[Li and Liang, ] Xiang Lisa Li and Percy Liang. Prefix-\ntuning: Optimizing continuous prompts for generation. In\nChengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli,\neditors, ACL.\n[Liu et al., 2021] Pengfei Liu, Weizhe Yuan, Jinlan Fu,\nZhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.CoRR,\nabs/2107.13586, 2021.\n[Livet and Weissman, 2007] Jean Livet and Tamily Weiss-\nman. Transgenic strategies for combinatorial expression\nof fluorescent proteins in the nervous system. Nature,\n450(7166):56‚Äì62, 2007.\n[Luo et al., 2023] Naisong Luo, Yuwen Pan, Rui Sun,\nTianzhu Zhang, Zhiwei Xiong, and Feng Wu. Camou-\nflaged instance segmentation via explicit de-camouflaging.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023.\n[Mai et al., 2023] Huayu Mai, Rui Sun, Tianzhu Zhang, Zhi-\nwei Xiong, and Feng Wu. Dualrel: semi-supervised mito-\nchondria segmentation from a prototype perspective. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1430\n[Meirovitch et al., 2016] Yaron Meirovitch, Alexander\nMatveev, Hayk Saribekyan, David Budden, David\nRolnick, Gergely Odor, Seymour Knowles-Barley,\nThouis Raymond Jones, Hanspeter Pfister, Jeff William\nLichtman, et al. A multi-pass approach to large-scale\nconnectomics. arXiv preprint arXiv:1612.02120, 2016.\n[Meirovitch et al., 2019a] Yaron Meirovitch, Lu Mi, Hayk\nSaribekyan, Alexander Matveev, David Rolnick, and Nir\nShavit. Cross-classification clustering: an efficient multi-\nobject tracking technique for 3-d instance segmentation in\nconnectomics. In CVPR, pages 8425‚Äì8435, 2019.\n[Meirovitch et al., 2019b] Yaron Meirovitch, Lu Mi, Hayk\nSaribekyan, Alexander Matveev, David Rolnick, and Nir\nShavit. Cross-classification clustering: an efficient multi-\nobject tracking technique for 3-d instance segmentation in\nconnectomics. In CVPR, pages 8425‚Äì8435, 2019.\n[Sun et al., 2021] Rui Sun, Yihao Li, Tianzhu Zhang, Zhen-\ndong Mao, Feng Wu, and Yongdong Zhang. Lesion-aware\ntransformers for diabetic retinopathy grading. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10938‚Äì10947, 2021.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. NIPS, 30, 2017.\n[Wang et al., 2022] Yuan Wang, Rui Sun, Zhe Zhang, and\nTianzhu Zhang. Adaptive agent transformer for few-shot\nsegmentation. In Computer Vision‚ÄìECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23‚Äì27, 2022,\nProceedings, Part XXIX, pages 36‚Äì52. Springer, 2022.\n[Wang et al., 2023] Yuan Wang, Rui Sun, and Tianzhu\nZhang. Rethinking the correlation in few-shot segmen-\ntation: a buoys view. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2023.\n[Wolf et al., 2018] Steffen Wolf, Constantin Pape, Alberto\nBailoni, Nasim Rahaman, Anna Kreshuk, Ullrich Kothe,\nand FredA Hamprecht. The mutex watershed: efficient,\nparameter-free image partitioning. In ECCV, pages 546‚Äì\n562, 2018.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1431",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7863552570343018
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6255149245262146
    },
    {
      "name": "Spatial contextual awareness",
      "score": 0.5512531399726868
    },
    {
      "name": "Voxel",
      "score": 0.5412137508392334
    },
    {
      "name": "Locality",
      "score": 0.4859241545200348
    },
    {
      "name": "Exploit",
      "score": 0.44054800271987915
    },
    {
      "name": "Machine learning",
      "score": 0.4313699007034302
    },
    {
      "name": "Theoretical computer science",
      "score": 0.36901792883872986
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210105595",
      "name": "Institute of Art",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210137491",
      "name": "National Science Centre",
      "country": "PL"
    }
  ]
}