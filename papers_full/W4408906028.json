{
  "title": "Grading explanations of problem-solving process and generating feedback using large language models at human-level accuracy",
  "url": "https://openalex.org/W4408906028",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2109955818",
      "name": "Zhongzhou Chen",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2134601230",
      "name": "Tong Wan",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4403637324",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W3038916389",
    "https://openalex.org/W4400350333",
    "https://openalex.org/W4391071215",
    "https://openalex.org/W4400329724",
    "https://openalex.org/W4392202731",
    "https://openalex.org/W4404280721",
    "https://openalex.org/W4402471009",
    "https://openalex.org/W4399627833",
    "https://openalex.org/W4389132588",
    "https://openalex.org/W4401403953",
    "https://openalex.org/W4382397550",
    "https://openalex.org/W2990427812",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W2727426218",
    "https://openalex.org/W2229908198",
    "https://openalex.org/W2914304175"
  ],
  "abstract": "[This paper is part of the Focused Collection in Artificial Intelligence Tools in Physics Teaching and Physics Education Research.] This study examines the feasibility and potential advantages of using large language models, in particular GPT-4o, to perform partial credit grading of large numbers of student written responses to introductory level physics problems. Students were instructed to write down verbal explanations of their reasoning process when solving one conceptual and two numerical calculation problems on two exams. The explanations were then graded according to a three-item rubric with each item graded as binary (1 or 0). We first demonstrate that machine grading using GPT-4o with no examples or reference answers can reliably agree with human graders in 70%–80% of all cases, which is equal to or higher than the level at which two human graders agree with each other. Two methods are essential for achieving this level of accuracy: (i) Adding explanation language to each rubric item that targets the errors of initial machine grading. (ii) Running the grading process 5 times and taking the most frequent outcome. Next, we show that the variation in outcomes across five machine grading attempts can serve as a grading confidence index. The index allows a human expert to identify <a:math xmlns:a=\"http://www.w3.org/1998/Math/MathML\" display=\"inline\"><a:mrow><a:mo>∼</a:mo><a:mn>40</a:mn><a:mi>%</a:mi></a:mrow></a:math> of all potentially incorrect gradings by reviewing just 10%–15% of all responses with the highest variation. Finally, we show that it is straightforward to use GPT-4o to write a clear and detailed explanation of the partial credit grading outcome. Those explanations can be used as feedback for students, which will allow students to understand their grades and raise different opinions when necessary. Almost all feedback messages generated were rated three or above on a five-point scale by two instructors who had taught the course multiple times. The entire grading and feedback generating process costs roughly $5 per 100 student answers, which shows immense promise for automating labor-intensive grading process through a combination of machine grading with human input and supervision.",
  "full_text": null,
  "topic": "Grading (engineering)",
  "concepts": [
    {
      "name": "Grading (engineering)",
      "score": 0.6950690746307373
    },
    {
      "name": "Computer science",
      "score": 0.6440075635910034
    },
    {
      "name": "Process (computing)",
      "score": 0.5266354084014893
    },
    {
      "name": "Natural language processing",
      "score": 0.41925594210624695
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35009825229644775
    },
    {
      "name": "Programming language",
      "score": 0.22246530652046204
    },
    {
      "name": "Engineering",
      "score": 0.1147531270980835
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    }
  ]
}