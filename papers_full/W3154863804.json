{
  "title": "Generating Datasets with Pretrained Language Models",
  "url": "https://openalex.org/W3154863804",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4222281043",
      "name": "Schick, Timo",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": null,
      "name": "Sch\\\"utze, Hinrich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2764041618",
      "name": "Schütze, Hinrich",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093713068",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3134354193",
    "https://openalex.org/W2963283805",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W3034198728",
    "https://openalex.org/W3100652389",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W3115295967",
    "https://openalex.org/W3033406728",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W3120948048",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3021526287",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W2995335514",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W3093940687",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W1614298861"
  ],
  "abstract": "To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943–6951\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n6943\nGenerating Datasets with Pretrained Language Models\nTimo Schick and Hinrich Schütze\nCenter for Information and Language Processing\nLMU Munich, Germany\nschickt@cis.lmu.de\nAbstract\nTo obtain high-quality sentence embeddings\nfrom pretrained language models (PLMs), they\nmust either be augmented with additional pre-\ntraining objectives or finetuned on a large set\nof labeled text pairs. While the latter approach\ntypically outperforms the former, it requires\ngreat human effort to generate suitable datasets\nof sufficient size. In this paper, we show how\nPLMs can be leveraged to obtain high-quality\nsentence embeddings without the need for la-\nbeled data, finetuning or modifications to the\npretraining objective: We utilize the generative\nabilities of large and high-performing PLMs\nto generate entire datasets of labeled text pairs\nfrom scratch, which we then use for finetun-\ning much smaller and more efficient models.\nOur fully unsupervised approach outperforms\nstrong baselines on several semantic textual\nsimilarity datasets.1\n1 Introduction\nWhile pretrained language models (PLMs) achieve\nstrong results for many NLP tasks (Peters et al.,\n2018; Radford et al., 2018; Devlin et al., 2019),\nthey do not produce good sentence embeddings out\nof the box (Reimers and Gurevych, 2019). Recent\napproaches address this by augmenting or replacing\nthe language modeling objective with likewise un-\nsupervised sentence-level objectives (e.g., Zhang\net al., 2020; Li et al., 2020), but they typically\nlag behind their supervised counterparts trained on\nhuman-annotated sentence pairs. Unfortunately,\nobtaining large amounts of high-quality training\ndata can be both difficult and prohibitively expen-\nsive (Bowman et al., 2015; Agirre et al., 2016).\nFurthermore, with larger and larger model sizes\n(Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Fedus et al., 2021), it becomes increas-\ningly challenging to finetune PLMs.\n1Our code and datasets are publicly available at https:\n//github.com/timoschick/dino.\nTask: Write two sentences that mean the same thing.\nSentence 1: “A man is playing a flute.”\nSentence 2: “He’s playing a flute.”\nTask: Write two sentences that are somewhat similar.\nSentence 1: “A man is playing a flute.”\nSentence 2: “A woman has been playing the violin.”\nTask: Write two sentences that are on completely\ndifferent topics.\nSentence 1: “A man is playing a flute.”\nSentence 2: “A woman is walking down the street.”\nFigure 1: Continuations generated by GPT2-XL with\nDINO for three different task descriptions. We investi-\ngate two different unsupervised approaches to generat-\ning sentence-similarity datasets: (i) The input sentence\nis given and only the continuation is generated. This\nrequires that an (unlabeled) set of sentences is available.\n(ii) Both input sentence and continuation are generated.\nThis does not rely on the availability of any resources.\nTo alleviate both problems, we explore a novel\napproach to obtaining high-quality sentence em-\nbeddings: We mimic the creation of NLI datasets\nby human crowdworkers (Bowman et al., 2015;\nWilliams et al., 2018), but replace human annota-\ntors with large PLMs. This allows us to automat-\nically create entire datasets from scratch that can\nbe used for supervised training of much smaller\nmodels. Not only does this solve the problem of\nlimited training data, it also provides a viable path\nto leverage big models like GPT-3 (Brown et al.,\n2020) without requiring any updates to their param-\neters. As illustrated in Figure 1, our approach is\nbased on recent methods for providing instructions\nto PLMs (e.g., Radford et al., 2019; Brown et al.,\n2020; Schick and Schütze, 2020, 2021a). We use\nthe self-debiasing approach of Schick et al. (2021)\nto ensure that each generated text pair is not only a\n6944\ngood fit for a given similarity label, but also not a\ngood fit for other labels. We refer to our method as\nDatasets from Instructions (DINO ).\nIn summary, our contributions are as follows:\n• We introduce DINO , a method for automati-\ncally generating labeled datasets of arbitrary\nsize by providing PLMs with instructions.\n• We release STS-\n (read as “STS-Dino”), the\nfirst textual similarity dataset generated com-\npletely automatically, without any human an-\nnotation effort.\n• We show that Sentence-RoBERTa (Reimers\nand Gurevych, 2019) trained on STS-\n out-\nperforms strong baselines on several semantic\ntextual similarity datasets.\n2 Related Work\nThere are many unsupervised approaches to ob-\ntaining sentence embeddings, for example by av-\neraging word embeddings (Mikolov et al., 2013;\nPennington et al., 2014; Bojanowski et al., 2017) or\nwith carefully designed sentence-level objectives\n(Le and Mikolov, 2014; Kiros et al., 2015). Ensem-\nbling several methods improves results (Pörner and\nSchütze, 2019; Pörner et al., 2020). Recent work\nobtains sentence representations by supplementing\nBERT (Devlin et al., 2019) or other PLMs with\nadditional unsupervised objectives (Zhang et al.,\n2020; Li et al., 2020; Wu et al., 2020; Giorgi et al.,\n2020). Often, labeled datasets such as paraphrase\ndatabases (Wieting and Gimpel, 2018) or natural\nlanguage inference datasets (Conneau et al., 2017;\nCer et al., 2018; Reimers and Gurevych, 2019) are\nused for supervised learning.\nSome approaches augment existing datasets with\nautomatically generated examples (Anaby-Tavor\net al., 2020; Papanikolaou and Pierleoni, 2020;\nYang et al., 2020; Mohapatra et al., 2020; Kumar\net al., 2021), but in contrast to our work, all of\nthese approaches require that there already exists a\nlabeled dataset for finetuning the generator. Provid-\ning PLMs with task descriptions for zero- or few-\nshot learning has been studied extensively (e.g.,\nRadford et al., 2019; Puri and Catanzaro, 2019;\nBrown et al., 2020; Schick and Schütze, 2020,\n2021b,a; Weller et al., 2020; Gao et al., 2021; Tam\net al., 2021). However, none of these approaches is\nsuitable for generating sentence embeddings.\nClosely related to our work, Efrat and Levy\n(2020) examine the ability of PLMs to follow natu-\nTask: Write two sentences thatiy.\nSentence 1: “x1”\nSentence 2: “\nFigure 2: Instruction template Iy(x1) for similarity la-\nbel y and input sentence x1; iy is described in Section 3.\nSee Figure 1 for three instantiations of the template.\nral language instructions for generating examples\nin place of human crowdworkers, but find that their\napproach performs poorly.\n3 Datasets from Instructions\nLet M be a PLM with vocabulary V , X = V ∗\nthe set of all token sequences and Y a finite set of\nsemantic similarity labels. Our aim is to generate a\ndataset Z ⊂ X ×X ×Y of text pairs (x1, x2) with\ncorresponding similarity labels y. For x ∈ V and\nx ∈ X, we denote with pM (x |x) the probability\nthat M assigns to x as a continuation of x.\nWe first assume that we already have access\nto a setX1 ⊂ X of texts(e.g., a set of sentences\nthat are typical of the domain of interest). This is a\nrealistic setting for many real-world applications,\nwhere large amounts of unlabeled text are abundant,\nbut it is difficult to obtain interesting and (for our\ntask) useful text pairs and labels. DINO requires a\nset of instructions I = {Iy | y ∈ Y } where each\nIy ∈ Iis a function that, given an input x1 ∈ X1,\nprompts its recipient to generate an appropriate\nsecond text x2. We use the instruction template\nin Figure 2 and consider three levels of similarity\n(Y = {0, 0.5, 1}), where\niy =\n\n\n\nmean the same thing if y = 1\nare somewhat similar if y = 0.5\nare on completely different topics if y = 0\nis loosely based on Cer et al. (2017)’s five-level\nsimilarity scheme. Note that for all y, Iy ends with\nan opening quotation mark, which allows us to treat\nthe first quotation mark generated by the PLM as a\nsign that it is done.\nFor a given x1 ∈ X1 and y ∈ Y , we could\ndirectly use the instructions Iy to obtain x2 by con-\ntinuously sampling tokens\nxk ∼ pM (xk |Iy(x1), x1, . . . , xk−1)\nstarting from k = 1until xk is a quotation mark\nand setting x2 = x1, . . . , xk−1. However, we may\n6945\nwant the PLM to generate a text x2 that is not only\na good fit for instruction Iy(x1), but also not a\ngood fit for some other instruction Iy′ (x1). We\nrefer to y′ as a counterlabel for y and denote the\nset of y’s counterlabels as CL(y). For example,\n1 ∈ CL(0.5) means that for y = 0.5, we want\nM to generate a sentence x2 that is similar to\n(y = 0.5), but at the same time does not have the\nsame meaning as (y = 1) sentence x1. We achieve\nthis using Schick et al. (2021)’s self-debiasing algo-\nrithm: When sampling the token xk, we consider\nnot just py = pM (xk |Iy(x1), x1, . . . , xk−1) [xk’s\nprobability given Iy(x1)], but also py′ [xk’s prob-\nability given Iy′ (x1)], for all y′ ∈ CL(y). We\npenalize each token xk for which py is lower than\nany py′ by multiplying its probability with a factor\nα = exp(λ · δy) where\nδy = py − max\ny′∈CL(y)\npy′\nis the difference between xk’s probability given\nIy(x1) and its maximum probability given Iy′ (x1)\nfor any y′ ∈ CL(y), and the decay constant λ is a\nhyperparameter.\nFor settings where no set of unlabeled textsX1\nis available, a straightforward approach would be\nto use the phrase shown in Figure 2 up to and in-\ncluding the first quotation mark as an instruction\nto let the PLM generate both x1 and x2. However,\nthis approach has at least two issues: First, gener-\nated texts may not match the required schema (e.g.,\nthe model may never produce the string “Sentence\n2:”). Second, the set of texts x1 should ideally be\nhighly diverse, whereas we want to give the model\nless leeway when generating x2, so we may want\nto use different sampling strategies for x1 and x2.\nWe solve both problems as follows: We first use\nIy (Figure 2) up to and including the first quotation\nmark (the one right after “Sentence 1:”) to generate\nx1; we stop as soon as the model produces a quota-\ntion mark. We run this procedure repeatedly until\nwe have a sufficient number of sentences. These\nare gathered into a set X1 and then we proceed\nexactly as in the case where X1 is already given.\n4 Experiments\nWe evaluate DINO on several English semantic tex-\ntual similarity datasets: the STS tasks 2012–2016\n(Agirre et al., 2012, 2013, 2014, 2015, 2016), the\nSTS benchmark (STSb) (Cer et al., 2017), and the\nSICK-Relatedness dataset (SICK) (Marelli et al.,\n2014). For all tasks, we adopt the unsupervised\nsetting without task-specific training examples.\nWe useDINO to generate STS-\n ⊂X×X×Y , a\ndataset of text pairs with semantic similarity labels.\nWe generate two variants:\n• STS-\n -x2, for which we make use of STSb\nto obtain a set of texts X1;\n• STS-\n -x1x2, where the set of sentences X1\nis generated from scratch.\nWe use GPT2-XL as PLM with a decay constant\nof λ = 100and the set of counterlabels CL(y) =\n{y′ ∈ Y | y′ > y}. That is, we do not restrict\nthe PLM when generating texts for y = 1, but for\ny = 0.5 (y = 0) we encourage it not to generate\ntexts x2 that mean the same thing as (are somewhat\nsimilar to) x1. We apply top- p (Holtzman et al.,\n2020) and top-k (Fan et al., 2018; Holtzman et al.,\n2018) sampling with p = 0.9, k = 5and generate\nup to 40 output tokens. For each x1 ∈ X1 and y ∈\nY , we generate up to two corresponding x2’s.2 For\nSTS-\n -x1x2, we obtain X1 by generating 15,000\nsentences using only top- p sampling (again with\np = 0.9) and no top- k sampling to ensure more\ndiversity in the generated output. We remove all\nexamples where x1 = x2 (as those provide no\ntraining signal to the model) and split the datasets\n90/10 into training and validation.\nTo assess the quality of the generated datasets,\nwe use them to train Sentence-RoBERTa (Reimers\nand Gurevych, 2019), a biencoder architecture\nbased on RoBERTa (base) (Liu et al., 2019) that\nmeasures the similarity of two texts by comput-\ning the cosine similarity of their embeddings. As\nour datasets contain many noisy examples, we use\na technique similar to label smoothing (Szegedy\net al., 2016) and replace similarity scores of 0 and\n1 with 0.1 and 0.9, respectively. Additionally, for\neach x1, we sample two x2’s from other dataset\nentries and augment the dataset with (x1, x2, 0).\nWe use the default parameters of Reimers and\nGurevych (2019) with a batch size of 32 and train\nfor at most one epoch; the exact number of train-\ning steps is determined based on Spearman’s rank\ncorrelation on the STS-\n validation set.\nResults We compare S-RoBERTa (base) trained\non datasets generated with DINO to S-BERT and\nS-RoBERTa finetuned on NLI data as well as Uni-\nversal Sentence Encoder (USE) (Cer et al., 2018)\n2As the PLM may not generate a quotation mark in the\nfirst 40 tokens, we use up to 5 tries to generate the two x2’s.\n6946\nModel UD STS12 STS13 STS14 STS15 STS16 STSb SICK Avg.\nsup.\nInferSent, Glove – 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01\nUSE – 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22\nS-BERT (base) – 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89\nS-RoBERTa (base) – 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21\nunsup.\nAvg. GloVe – 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32\nAvg. BERT – 38.78 57.98 57.98 63.15 61.06 46.35 58.40 54.81\nBERT CLS – 20.16 30.01 20.09 36.88 38.08 16.50 42.63 29.19\nZhang et al. (2020) NLI 56.77 69.24 61.21 75.23 70.16 69.21 64.25 66.58\nLi et al. (2020) NLI 59.54 64.69 64.66 72.92 71.84 58.56 65.44 65.38\nLi et al. (2020) STS 63.48 72.14 68.42 73.77 75.37 70.72 63.11 69.57\nDINO (STS-\n -x1x2) – 64.87 78.30 66.38 79.60 76.47 76.51 74.26 73.77\nDINO (STS-\n -x2) STS 70.27 81.26 71.25 80.49 77.18 77.82 68.09 75.20\nTable 1: Spearman’s rank correlation on STS12–16, STSb and SICK without finetuning on task-specific examples\nfor models with NLI supervision (“sup.”) and fully unsupervised (“unsup.”) models using the same evaluation setup\nas Reimers and Gurevych (2019). The second column shows which unlabeled data (“UD”) is used by unsupervised\napproaches in addition to original pretraining data; the final column shows average performance. Results for\nall baselines except Zhang et al. (2020) and Li et al. (2020) are from Reimers and Gurevych (2019). The best\nunsupervised result is shown in bold, the best overall result is underlined. DINO outperforms all unsupervised\napproaches and, surprisingly, also supervised approaches on four out of six STS datasets.\nModel STS12-16 STSb SICK\nDINO (STS-\n -x2) 76.09 77.82 68.09\ndecay constant λ = 0 65.50 70.71 67.60\ndecay constant λ = 200 75.40 77.49 66.83\nno label smoothing 74.50 76.26 66.23\nno augmentation 70.90 73.81 63.98\nTable 2: Effect of removing self-debiasing ( λ = 0)\nor increasing the decay constant ( λ = 200), using no\nlabel smoothing and performing no data augmentation\n(sampling random x2’s for each x1) on the performance\nof DINO on STS12-16 (avg), STSb and SICK\nand InferSent (Conneau et al., 2017), all of which\nare trained on hundreds of thousands of labeled text\npairs from SNLI (Bowman et al., 2015) and MNLI\n(Williams et al., 2018). We additionally compare\nto the following fully unsupervised approaches: av-\neraging word-level GloVe (Pennington et al., 2014)\nor BERT (Devlin et al., 2019) embeddings, using\nBERT’s CLS token, and recent methods by Zhang\net al. (2020) and Li et al. (2020) based on pretrained\nBERT models. We do not compare to approaches\ntrained with direct supervision as our focus is on\nobtaining sentence representations without task-\nspecific labeled examples. As shown in Table 1,\ntraining on datasets generated with DINO clearly\noutperforms the fully unsupervised baselines; on\naverage, training on STS-\n -x2 even outperforms\nall approaches with NLI supervision. STS -\n -x2\ngives better results than STS-\n -x1x2 on all STS\ndatasets as its examples are – by design – very sim-\nilar to examples found in these datasets, while the\nlatter gives better results on SICK.\nWe investigate the importance of self-debiasing\n(Schick et al., 2021) in Table 2 (top); as can be\nseen, removing self-debiasing (λ = 0) dramatically\nhurts performance. Increasing the decay constant\n(λ = 200) leads to slightly worse performance\nas the overall quality of generated sentences de-\ncreases (Schick et al., 2021). Table 2 (bottom)\nshows that training on STS-\n requires measures\nto limit the effect of noisy labels: removing label\nsmoothing and performing no data augmentation\n(i.e., not generating additional pairs (x1, x2, 0) by\nsampling random x2’s for each x1) clearly hurts\nperformance.\nTo further assess the quality of datasets gener-\nated with DINO , we additionally perform a small-\nscale human evaluation. To this end, we consider\nthe exact version of STS -\n -x2 used for training\nS-RoBERTa; that is, we perform label smoothing,\naugmentation with randomly sampled text pairs,\nand removal of trivial examples where x1 = x2.\nFrom the resulting dataset, we randomly select\n100 text pairs (x1, x2) and annotate them ourselves\nwith similarity scores y ∈ {0, 0.1, 0.5, 0.9}, where\nwe assign a score of 0.9 when x1 and x2 mean\n(almost) the same thing and a score of 0.1 when\nthey are on different topics, but still show a weak\nsimilarity in some aspect.\nIn Table 3, human annotations are compared\nto originally assigned scores, yielding some inter-\nesting insights. For one, it becomes clear why\naugmentation with randomly sampled text pairs is\nimportant for good downstream task performance:\nOf the examples generated by DINO that are sup-\n6947\nDINO Labels → 0.0 0 .1 0 .5 0 .9\nHuman Labels\n0.0 95% 15% 0% 0%\n0.1 0% 44% 11% 12%\n0.5 5% 41% 60% 41%\n0.9 0% 0% 29% 47%\nTable 3: Comparison of similarity scores in STS-\n -x2\nto human judgments for 100 examples. Examples are\nchosen randomly from the version of STS-\n -x2 used\nfor training (including label smoothing, augmentation\nwith random pairs and removal of examples where\nx1 = x2). For column i and row j, the value shown is\nthe percentage of examples generated by DINO for sim-\nilarity score i that were assigned score j in our human\nevaluation.\nposed to be on completely different topics, many\n(41%) still have a certain similarity according to\nhuman judgment. In contrast, randomly sampled\npairs are indeed on completely different topics in\nalmost all cases. Moreover, we can see that GPT2-\nXL has particular difficulty in generating pairs of\nnon-identical sentences that really mean the same\nthing: Only 47% of all examples that should have\nthe same meaning do actually mean (almost) the\nsame thing. However, the strong performance of\nS-RoBERTa trained on STS-\n -x2 suggests that,\ndespite this noise, there is sufficient signal in this\ndataset for successful training.\nWe finally take a qualitative look at both positive\nexamples where DINO is able to create high-quality\ntext pairs and at some typical errors found in many\nof the generated examples. As shown in Table 4, for\ny = 1the PLM sometimes comes up with decent\nparaphrases (e.g. “notches a victory” 7→ “wins”) or\nsubstitutes with very similar meaning (“cutting” 7→\n“slicing”), but more often it generates sentences that\neither omit or mix up important information, and\nsometimes it produces sentences with an entirely\ndifferent meaning. Whereas sentences generated\nfor y = 0.5 by and large look reasonable, fory = 0\nthe PLM often simply flips words (“closed” 7→\n“open”, “large” 7→ “small”) instead of producing\nsentences on completely different topics.\n5 Conclusion\nWe have introducedDINO , a method for using large\nPLMs to generate entire datasets of labeled sen-\ntence pairs from scratch, requiring no labeled data\nand no parameter updates. This is achieved by\nproviding instructions in natural language, com-\nbined with the self-debiasing method of Schick\ny = 1\nx1 = Rick Santorum notches a victory in Kansas caucuses.✓x2 = Rick Santorum wins Kansas caucuses.\nx1 = A man is cutting cucumbers. ✓x2 = A man is slicing cucumbers.\nx1 = US closes embassy in Syria ✗x2 = US Embassy in Syria\nx1 = A man is playing the cello. ✗x2 = The cello is playing the man.\nx1 = A plane is taking off. ✗x2 = I want to be a pilot.\ny = 0.5 x1 = A woman is seasoning a piece of meat. ✓x2 = A man is cooking the meat and adding spices [...]\nx1 = Second day of Egyptian presidential election ✓x2 = The first night of the election.\ny = 0\nx1 = A white bus with the word Julia is near water [...]✓x2 = There is an open beach in my hometown.\nx1 = Strong earthquake in Mexico ✓x2 = It’s the best time to get a job\nx1 = Closed roads in Armenia ✗x2 = Open roads in Azerbaijan\nx1 = The man is playing the guitar. ✗x2 = I’m not a guitar player.\nx1 = A man is playing a large flute. ✗x2 = A man is listening to a small flute.\nTable 4: A selection of high-quality (✓) and low-quality\n(✗) examples in STS -\n -x2. Many sentence pairs for\ny = 1are not similar and have quite different meanings.\nSome sentence pairs for y = 0are not on completely\ndifferent topics.\net al. (2021). With appropriate measures for han-\ndling noisy data, models trained on datasets gener-\nated with DINO achieve strong results on several\nsemantic textual similarity datasets.\nFor future work, it would be interesting to see\nwhether the noise in datasets generated with DINO\ncan further be reduced, e.g., by using different\nsets of instructions (Jiang et al., 2020; Schick and\nSchütze, 2021a) or by supplementing our pipeline\nwith some additional filtering steps.\nAcknowledgments This work was funded by the\nEuropean Research Council (ERC #740516). We\nthank the anonymous reviewers for their helpful\ncomments.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Iñigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\n6948\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015),\npages 252–263, Denver, Colorado. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,\nMona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,\nRada Mihalcea, German Rigau, and Janyce Wiebe.\n2014. SemEval-2014 task 10: Multilingual semantic\ntextual similarity. In Proceedings of the 8th Interna-\ntional Workshop on Semantic Evaluation (SemEval\n2014), pages 81–91, Dublin, Ireland. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evaluation\n(SemEval-2016), pages 497–511, San Diego, Califor-\nnia. Association for Computational Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Semantics\n(*SEM), Volume 1: Proceedings of the Main Confer-\nence and the Shared Task: Semantic Textual Similar-\nity, pages 32–43, Atlanta, Georgia, USA. Association\nfor Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A\npilot on semantic textual similarity. In Proceedings\nof the First Joint Conference on Lexical and Compu-\ntational Semantics - Volume 1: Proceedings of the\nMain Conference and the Shared Task, and Volume 2:\nProceedings of the Sixth International Workshop on\nSemantic Evaluation, SemEval ’12, page 385–393,\nUSA. Association for Computational Linguistics.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? Deep learning to the rescue! Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):7383–7390.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nDaniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670–680, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAvia Efrat and Omer Levy. 2020. The turking test: Can\nlanguage models understand instructions? Comput-\ning Research Repository, arXiv:2010.11982.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Computing\nResearch Repository, arXiv:2101.03961.\n6949\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nJohn M. Giorgi, Osvald Nitski, Gary D. Bader, and\nBo Wang. 2020. DeCLUTR: Deep contrastive learn-\ning for unsupervised textual representations. Com-\nputing Research Repository, arXiv:2006.03659.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018. Learn-\ning to write with cooperative discriminators. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1638–1649, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard\nZemel, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Skip-thought vectors. In Advances in\nNeural Information Processing Systems, volume 28.\nCurran Associates, Inc.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2021. Data augmentation using pre-trained trans-\nformer models. Computing Research Repository ,\narXiv:2003.02245.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Pro-\nceedings of the 31st International Conference on Ma-\nchine Learning, volume 32 of Proceedings of Ma-\nchine Learning Research, pages 1188–1196, Bejing,\nChina. PMLR.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretrain-\ning approach. Computing Research Repository ,\narXiv:1907.11692.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014. A SICK cure for the evaluation of\ncompositional distributional semantic models. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 216–223, Reykjavik, Iceland. European Lan-\nguage Resources Association (ELRA).\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efficient estimation of word representa-\ntions in vector space. Computing Research Reposi-\ntory, arXiv:1301.3781.\nBiswesh Mohapatra, Gaurav Pandey, Danish Contractor,\nand Sachindra Joshi. 2020. Simulated chats for task-\noriented dialog: Learning to generate conversations\nfrom instructions. Computing Research Repository,\narXiv:2010.10216.\nYannis Papanikolaou and Andrea Pierleoni. 2020.\nDARE: Data augmented relation extraction\nwith GPT-2. Computing Research Repository ,\narXiv:2004.13845.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin,\nAlban Desmaison, Luca Antiga, and Adam Lerer.\n2017. Automatic differentiation in PyTorch. In NIPS\nAutodiff Workshop.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nNina Pörner and Hinrich Schütze. 2019. Multi-view do-\nmain adapted sentence embeddings for low-resource\nunsupervised duplicate question detection. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019, pages 1630–1641. As-\nsociation for Computational Linguistics.\nNina Pörner, Ulli Waltinger, and Hinrich Schütze. 2020.\nSentence meta-embeddings for unsupervised seman-\ntic textual similarity. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, ACL 2020, Online, July 5-10, 2020,\npages 7027–7034. Association for Computational\nLinguistics.\n6950\nRaul Puri and Bryan Catanzaro. 2019. Zero-shot text\nclassification with generative language models. Com-\nputing Research Repository, arXiv:1912.10165.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nTimo Schick and Hinrich Schütze. 2020. Few-shot text\ngeneration with pattern-exploiting training. Comput-\ning Research Repository, arXiv:2012.11926.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze questions for few shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics, Kyiv, Ukraine\n(Online). International Committee on Computational\nLinguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for\nreducing corpus-based bias in NLP. Transactions of\nthe Association for Computational Linguistics.\nC. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and\nZ. Wojna. 2016. Rethinking the inception architec-\nture for computer vision. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 2818–2826.\nDerek Tam, Rakesh R Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. Computing\nResearch Repository, arXiv:2103.11955.\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew Peters. 2020. Learning from task descrip-\ntions. Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-50M:\nPushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 451–462, Melbourne, Australia. As-\nsociation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nZhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa,\nFei Sun, and Hao Ma. 2020. CLEAR: Contrastive\nlearning for sentence representation. Computing Re-\nsearch Repository, arXiv:2012.15466.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1008–1025, Online. Association for Computational\nLinguistics.\nYan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,\nand Lidong Bing. 2020. An unsupervised sentence\nembedding method by mutual information maximiza-\ntion. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1601–1610, Online. Association for\nComputational Linguistics.\n6951\nA Experimental Setup\nOur implementation is based on the Transformers\nlibrary (Wolf et al., 2020) and PyTorch (Paszke\net al., 2017). All our experiments were con-\nducted using two GPUs with 11GB RAM (NVIDIA\nGeForce GTX 1080 Ti). Generating STS-\n -x1x2\nand STS -\n -x2 using both GPUs took approxi-\nmately 48 hours per dataset. Training a Sentence\nTransformer on these datasets took less than 2\nhours on average.\nB Datasets\nBoth datasets generated with DINO (STS-\n -x1x2\nand STS-\n -x2) are publicly available at https:\n//github.com/timoschick/dino. After\nfiltering out examples where the language model\ndid not produce a quotation mark, STS-\n -x2 con-\ntains 121,275 examples and STS-\n -x1x2 contains\n143,968 examples.\nC Additional Results\nOur main results do not include scores for De-\nCLUTR (Giorgi et al., 2020) and CLEAR (Wu\net al., 2020) – two recent approaches using con-\ntrastive learning – as their evaluation setup dif-\nfers from that described in Reimers and Gurevych\n(2019) (and used by all other baselines) in the fol-\nlowing respects:\n• Both Giorgi et al. (2020) and Wu et al. (2020)\ntreat SICK and STSb as supervised tasks, i.e.,\nthey use the provided task-specific training\nsets to perform regular supervised training.\n• The STS12–16 datasets each consist of sev-\neral subsets. Giorgi et al. (2020) and Wu et al.\n(2020) compute Spearman’s correlation co-\nefficient separately for each of these subsets\nand report the mean score across all subsets.\nIn contrast, for our main results we follow\nReimers and Gurevych (2019) and concate-\nnate all subsets to form one large set on which\nSpearman’s correlation is computed just once.\nAs the implementations of both methods are not\npublicly available as of this writing, we are unable\nto compute scores for DeCLUTR and CLEAR us-\ning the evaluation setup of Reimers and Gurevych\n(2019) ourselves. Instead, we recompute scores for\nDINO (both with STS-\n -x2 and STS-\n -x1x2) us-\ning the evaluation setup of Giorgi et al. (2020) and\nWu et al. (2020) on STS12–16; results are shown\nin Table 5.\nModel STS12 STS13 STS14 STS15 STS16 Avg.\nCLEAR 49.0 48.9 57.4 63.6 65.6 56.9\nDeCLUTR 64.2 70.4 70.0 77.5 75.4 71.5\nSTS-\n -x1x2 65.1 69.9 68.6 76.3 76.6 71.3\nSTS-\n -x2 65.3 71.8 72.7 75.9 76.9 72.5\nTable 5: Results for CLEAR (Wu et al., 2020), DeCLUTR\n(Giorgi et al., 2020) and Sentence-RoBERTa (base) trained\non STS-\n -x1x2 and STS-\n -x2 using the evaluation setup\nof Wu et al. (2020) and Giorgi et al. (2020): For each task,\nwe report the mean Spearman correlation of all subtasks in\na fully unsupervised setting.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8637270927429199
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6377209424972534
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.6304247379302979
    },
    {
      "name": "Sentence",
      "score": 0.5835612416267395
    },
    {
      "name": "Natural language processing",
      "score": 0.5776165723800659
    },
    {
      "name": "Generative grammar",
      "score": 0.543819010257721
    },
    {
      "name": "Language model",
      "score": 0.5135903358459473
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4819302558898926
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4614037871360779
    },
    {
      "name": "Machine learning",
      "score": 0.42639437317848206
    },
    {
      "name": "Image (mathematics)",
      "score": 0.11403700709342957
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}