{
  "title": "GrapeLeafNet: A Dual-Track Feature Fusion Network With Inception-ResNet and Shuffle-Transformer for Accurate Grape Leaf Disease Identification",
  "url": "https://openalex.org/W4391428187",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2109846087",
      "name": "R Karthik",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A1231228824",
      "name": "R. Menaka",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A5092693947",
      "name": "S. Ompirakash",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2104578828",
      "name": "P. Bala Murugan",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A5093837573",
      "name": "M. Meenakashi",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2801776074",
      "name": "Sindhia Lingaswamy",
      "affiliations": [
        "National Institute of Technology Tiruchirappalli"
      ]
    },
    {
      "id": "https://openalex.org/A2792229169",
      "name": "Daehan Won",
      "affiliations": [
        "Binghamton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4249301112",
    "https://openalex.org/W2236288779",
    "https://openalex.org/W2987692263",
    "https://openalex.org/W4210803908",
    "https://openalex.org/W3206884405",
    "https://openalex.org/W2907194960",
    "https://openalex.org/W4206990632",
    "https://openalex.org/W2000083963",
    "https://openalex.org/W4210837323",
    "https://openalex.org/W4310154428",
    "https://openalex.org/W3119451451",
    "https://openalex.org/W4210764165",
    "https://openalex.org/W2103959917",
    "https://openalex.org/W2808247525",
    "https://openalex.org/W2980069994",
    "https://openalex.org/W4281795648",
    "https://openalex.org/W2969791356",
    "https://openalex.org/W4223560697",
    "https://openalex.org/W3107829143",
    "https://openalex.org/W6841314170",
    "https://openalex.org/W3115279044",
    "https://openalex.org/W3208028783",
    "https://openalex.org/W2981513382",
    "https://openalex.org/W4295856619",
    "https://openalex.org/W4297151160",
    "https://openalex.org/W3033864984",
    "https://openalex.org/W4283015487",
    "https://openalex.org/W4200414358",
    "https://openalex.org/W3084726437",
    "https://openalex.org/W3114564184",
    "https://openalex.org/W4224020090",
    "https://openalex.org/W3193250208",
    "https://openalex.org/W3033097971",
    "https://openalex.org/W6694260854",
    "https://openalex.org/W3192289469",
    "https://openalex.org/W6796237581",
    "https://openalex.org/W3198155834",
    "https://openalex.org/W4361218351",
    "https://openalex.org/W4290717061",
    "https://openalex.org/W4220719423",
    "https://openalex.org/W4377079862",
    "https://openalex.org/W4224996007",
    "https://openalex.org/W3112181535",
    "https://openalex.org/W3201835768",
    "https://openalex.org/W3168101492",
    "https://openalex.org/W4292217765",
    "https://openalex.org/W2473156356",
    "https://openalex.org/W4248903096",
    "https://openalex.org/W4226278842",
    "https://openalex.org/W2964350391"
  ],
  "abstract": "Grapes are a widely cultivated crop in the horticultural industry, renowned for their unique flavor and nutritional benefits. However, this crop is highly susceptible to various diseases that can cause significant reductions in yield and quality, resulting in considerable financial losses. Therefore, it is imperative to identify these diseases to effectively manage their spread. Traditionally, the identification of grape leaf diseases has relied on scientific expertise and observational skills. However, with the advent of deep learning methods, it is now feasible to recognize disease patterns from images of infected leaves. In this research, we propose a novel dual-track feature fusion network titled &#x2018;GrapeLeafNet&#x2019; for detecting grape leaf disease. It employs a dual-track feature fusion approach, combining Inception-ResNet blocks with CBAM for local feature extraction and Shuffle-Transformer for global feature extraction. The first track uses Inception-ResNet blocks to represent features at multiple scales and map significant features, and CBAM captures significant spatial and channel dependencies. The second track employs Shuffle-Transformer to extract long-term dependencies and complex global features in images. The extracted features are then fused using Coordinate attention, enabling the network to capture both local and global contextual information. Experimental results on the Grape leaf disease dataset from Plant Village demonstrate the effectiveness of the proposed network, achieving an accuracy of 99.56&#x0025;.",
  "full_text": "Dr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000. \nDigital Object Identifier 10.1109/ACCESS.2022.Doi Number \nGrapeLeafNet: A Dual -Track Feature Fusion \nNetwork with Inception -ResNet and Shuffle -\nTransformer for Accurate Grape Leaf Disease \nIdentification \nKarthik R1, Menaka R1,  Ompirakash S2, Bala Murugan P2, Meenakashi M2, Sindhia \nLingaswamy3 and Daehan Won4 \n \n1Centre for Cyber Physical Systems (CCPS), Vellore Institute of Technology, Chennai, India. \n2School of Electronics Engineering, Vellore Institute of Technology, Chennai, India. \n3Department of Computer Applications, National Institute of Technology, Tiruchirappalli, India.  \n4System Sciences and Industrial Engineering, Binghamton University, USA.  \n \nCorresponding author: Karthik R (e-mail: r.karthik@vit.ac.in). \n \nABSTRACT  Grapes are a widely cultivated crop in the horticultural industry, renowned for their unique \nflavor and nutritional benefits. However, this crop is highly susceptible to various diseases that can cause \nsignificant reductions in yield and quality, resulting in considerable financial losses. Therefore, it is \nimperative to identify these diseases to effectively manage their spread. Traditionally, the identification of \ngrape leaf diseases has relied on scien tific expertise and observational skills. However, with the advent of \ndeep learning methods, it is now feasible to recognize disease patterns from images of infected leaves. In this \nresearch, we propose a novel dual-track feature fusion network titled ‚ÄòGrapeLeafNet‚Äô for detecting grape leaf \ndisease. It employs a dual -track feature fusion approach, combining Inception -ResNet blocks with CBAM \nfor local feature extraction and Shuffle -Transformer for global feature extraction.  The first track uses \nInception-ResNet blocks to represent features at multiple scales and map significant features, and CBAM \ncaptures significant spatial and channel dependencies. The second track employs Shuffle -Transformer to \nextract long-term dependencies and complex global features in images. The extracted features are then fused \nusing Coordinate attention, enabling the network to capture both local and global contextual information. \nExperimental results on the Grape leaf disease dataset from Plant Village demonstrate the effectiveness of \nthe proposed network, achieving an accuracy of 99.56%. \nINDEX TERMS  Grape leaf disease, deep learning, transformer, CNN, attention\nI. INTRODUCTION \nGrapes constitute a significant crop to produce wine, with \nan estimated global production of approximately 77.4 \nmillion metric tonnes [1]. The leading grape -producing \nnations comprise China, Italy, the United States of \nAmerica, France, and Spain [2]. Grape s are the third most \nvaluable horticultural crop grown widely in the \nMediterranean region [3]. Despite their global popularity, \ngrapes are prone to diseases caused by fungi and bacteria, \nwhich can result in up to the maximum economic loss of \n80% [4]. Out o f the different diseases, Grape Black Rot, \nGrape Black Measles, and Isariopsis Leaf Spot are the most \ncommon diseases that significantly affect grape leaves \ncurrently [5,6].  \nGrape Black Measles is caused by fungi namely \nPhaeomoniella  spp. The typical symptoms of this disease \nare rounded, irregular spots that spread across the veins of \nthe leaves. A few leaves affected by this disease can result \nin a severe fungal infection throughout the entire plant, \nemphasizing its severity [7]. Isa riopsis leaf spot, or Grape \nLeaf Blight, is caused by Pseudocerospora Vitis and \ndevelops on leaves. This disease exhibits light brown \nsymptoms initially and dark brown symptoms later, \ndepending on the season [8]. Grape Black Rot is caused by \nGuignardia Bid wellii and grows rapidly during humid and \nearly spring conditions. The disease initially appears as \nblack spots throughout the veins of the leaves and alters the \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n2                                                                                                                                                                                                         Volume xxxx \nchemical composition by increasing the sugar \nconcentration [9,10].  \nConventional disease detection methods rely on \nmanual inspection and subjective symptom assessment, \nwhich can lead to errors and delays in diagnosis [11]. DNA-\nbased serological methods offer improved pathogen \nidentification but are time -consuming and requi re \nspecialized expertise [11]. As the morphological \ncharacteristics of diseased spots can vary considerably, \nthere is a need for more efficient and accurate disease \ndetection methods [12]. Computer vision techniques, \ncoupled with high -resolution sensors, s martphone \nintegration, and deep learning algorithms, have emerged as \na promising approach for disease diagnosis [13,14]. The \nability of Deep learning to extract and analyze complex \npatterns from images makes it well -suited for automated \ndisease detection. This research proposes a deep learning -\nbased method for accurately diagnosing and classifying \ngrape leaf diseases.  \n \nII. RELATED WORKS \n \n         In recent years, various methods have been proposed \nfor detecting diseases in grape leaves. These methods can be \nclassified into two categories: Traditional Machine Learning \nmethods and Deep Learning. \nNitesh et al. presented an approach for grape leaf \ndisease detection using Gray -Level Occurrence Matrix -\nbased features and Support Vector Machine (SVM). This \nmethod transformed the input images into HSV color space \nfor precise feature extraction [15]. Sajj ad et al. proposed \nanother method for grape leaf disease detection using Gray -\nLevel Co -occurrence Matrix (GLCM), Gray Level Run \nLength Matrix (GLRLM), and Local Binary Pattern (LBP) \nbased features [16]. A few methods were proposed based on \nsegmenting the d iseased portions of the leaves and then \nprocessing them further for classification. S.M. Jaisakthi et \nal. presented a Grabcut segmentation approach for grape leaf \ndisease detection [17]. GLCM-based features were extracted \nfrom the segmented region for furt her classification. Seyed \net al. employed k-means clustering to segment the region of \ninterest for grape leaf disease detection [18]. Then, feature \nextraction is performed in different color spaces namely \nRGB, HSV, and LAB. SVM was used for classifying the  \ntype of disease. Alishba et al. applied a local contrast haze \nreduction technique to enhance the input grape leaf images \nfor effective disease detection [19]. Color, geometric, and \nLocal Binary Pattern based features were considered for \nclassification using SVM. Another method was proposed by \nShanthakumari et al. for grape leaf disease detection using \nhistogram gradient features and Improvised K -Nearest \nNeighbor (KNN) [20]. Zamani et al. used Principal \nComponent Analysis (PCA) to extract significant features \nfor plant leaf disease detection and these extracted features \nare classified using Random Forest [44].  \nResearchers have increasingly adopted deep learning \n(DL) methods over machine learning (ML) methods based \non manual feature extraction  for grape leaf disease \nclassification, as the latter is limited by small dataset sizes \nand requires more data for accurate classification.  Despite \nthe limited dataset size challenge in DL, data augmentation \ntechniques are available to address this issue. Unlike ML, DL \nautomatically extracts features, eliminating the need for \ndomain expertise in feature selection.  \nMost of the recent works for grape leaf disease \ndetection focus on implementing deep learning techniques, \nincluding pre -trained architectures and transfer learning. \nSome works use deep learning networks for feature \nextraction and traditional machine learning for classification. \nKhaing et al. proposed the use of the VGG16 architecture as \nthe base model for feature extraction in their grape leaf \ndisease classification model [21]. The VGG16 model has a \ndeep network structure, consisting of 16 convolutional layers \nand 3 fully connected layers, which can capture and extract \ncomplex features from the input images. Subsequently, SVM \nwas applied to classify the extracted features into specific \ndisease classes. Similarly, Bhavya et al. also adopted the \nVGG16 architec ture to extract features from grape leaf \nimages [22]. The extracted features were then trained with a \nRandom Forest classifier that can handle multi -class \nclassification tasks with high accuracy.  \nZhaohua et al. presented an ensemble model for grape \nleaf disease using VGG16, MobileNet, and AlexNet [23]. \nPeng et al. proposed another hybrid method for grape leaf \ndisease detection with ResNet50 and ResNet101 as feature \nextractors and SVM for classifica tion [24]. Many other \nstudies have also utilized pre -trained deep learning -based \narchitectures to classify grape leaf diseases. Miaomiao et al. \nproposed a united model using GoogLeNet and ResNet \narchitectures for grape leaf disease detection [25]. The \nUnited Model was employed to classify grape leaves as \nhealthy or diseased, specifically identifying common \ndiseases such as black rot, esca, and isariopsis leaf spot. \nAshokkumar et al. proposed an approach for grape leaf \ndisease detection using Faster Region -based Convolutional \nNeural Network [26]. This approach combined attention -\nbased multilayer convolutional feature creation, object \nidentification, & categorization. Fraiwan et al. presented a \ncomprehensive study for grape leaf disease detection with 11 \ndifferent networks comprising Googlenet, Inception, \nMobileNet, ResNet architectures, etc [27]. The study \nreported DarkNet architecture yielding the maximum \naccuracy when compared to other networks. Atul Kumar et \nal. presented a transfer learning -based approach for grape \nleaf disease detection based on EfficientNet architecture \n[29]. Khaing et al. presented an approach for grape leaf \ndisease detection using the VGG16 network [30].  \nGenerative Adversarial Networks (GAN) were applied \nto generate synthetic images to combat the data imbalance \nissues. Bin et al. presented a GAN -based approach to \ngenerate images of four different grape leaf diseases for \ntraining identification models [28]. Zhe Tang et al. presented \na lightweight approach for grape leaf disease detection using \na channel-wise attention mechanism [31]. The network was \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         3 \nproposed to diagnose grape diseases, including black rot, \nblack measles, and leaf blight. Sandy et al. presented a \ntransfer learning approach with Alexnet based on Regions \nwith Convolutional Neural Networks [32]. Sri et al. analyzed \nthe impact of the transfer learning approach with VGG16 and \nVGG19 networks for grape leaf disease detection [33]. \nAnother work was proposed based for grape leaf disease \ndetection based on the VGG network [34]. Xiaoyue et al. \npresented a deep learning -based approach for grape lea f \ndisease detection using the Inception -v1 module, Inception-\nResNet-v2 module, and SE -blocks [35]. Guo et.al \nimplemented data augmentation using both rigid \ntransformation and deep convolutional generative \nadversarial networks for feature extraction .  Modified \nResNet and GoogleNet are employed for classification [45]. \nIn the recent years, Customized CNN are employed for \nplant's environmental concerns by incorporating attention \nnetworks and cross-layer extraction structures [40-43,46]. \nAlthough deep learning models have shown promising \nresults in detecting grape leaf diseases , this research has \nhighlighted some limitations that need to be addressed in the \ncontext of grape leaf disease detection which are tabulated in \nTable 1 . These limitations include class imbalance, \ninadequate class-wise adaptability, and insufficient attention \nto critical leaf features. The proposed work will address these \nresearch gaps, and the following subsection will discuss the \nsteps taken to address these limitations. \n \nA. RESEARCH GAPS AND MOTIVATION \n \nThe following are the research gaps that were observed \nin the existing works related to grape leaf disease detection.  \nPrevious studies have mainly focused on either local or \nglobal feature extraction, which can lead to suboptimal \nperformance in detecting grape leaf diseases. To overcome \nthis issue, it is recommended to use feature fusion \ntechniques. \nThe model architectures proposed in existing research \nare often large and complex, making it difficult to deploy \nthem on low -resource devices. The linear arrangement of \nconvolutional layers in these models can result in intricate \ncomputations and a higher number of trainable parameters, \nlimiting their practical usefulness in real-world applications. \nCNNs are commonly utilized in previous studies due to \ntheir high accuracy in detecting and classifying various plant \ndiseases. However, CNNs may not be suitable for capturing \nglobal contextual information and handling long -range \ndependencies. This limitati on can be particularly \nproblematic in cases where the disease symptoms are subtle \nand require a more comprehensive understanding of the \ncontext. \n \nB. RESEARCH CONTRIBUTIONS \n \nThe following are the research contributions towards the \nproposed work. \nThe proposed network aims to improve the \nclassification of grape leaf diseases by combining features \nfrom two different tracks to extract both global and local \nfeatures. The first track uses a CNN to extract local features \nfrom the input image, while the second track utilizes a \nShuffle Transformer to extract global features. \nThe proposed network incorporates Inception -ResNet \nfor better feature extraction, resulting in reduced \ncomputation. This is achieved by performing feature fusion \nof different feature scale convolutions in parallel with input \nfeatures. \nThe proposed network applies both CBAM and \nCoordinate Attention to capture both spatial and positional \ninformation in the image. Specifically, the CBAM module \ncan capture interdependencies between different feature \nchannels, while the Coordinate Attention can capture \npositional relationships between different features. \nThe Shuffle Transformer track used in the proposed \nnetwork is effective in capturing long-range dependencies in \nimages. By incorporating channel shuffling techniques and \nutilizing both global and local information, it can capture \ncomplex features that may be important for identifying \nsubtle symptoms of grape leaf diseases. \n \nIII. PROPOSED SYSTEM \nThis section presents an overview of the different \nmodules employed in the proposed network. \n \nA. ARCHITECTURE OVERVIEW \n \nThis section presents an overview of the different \nmodules employed in the proposed network. \n \nThe proposed network ‚ÄòGrapeLeafNet‚Äô has two distinct \nfeature extraction tracks for precise classification of grape \nleaf disease detection. The first track consists of a linear  \nCNN integrated with modules like Inception -ResNet block \nand Convolutional Block Attention Mo dule (CBAM). This \ntrack uses Inception -ResNet blocks to achieve an optimal \nbalance between performance and computational efficiency \nby employing filters of various sizes, such as 1x1, and 3x3, \nin parallel within the same layers. CBAM integrates the \nChannel Attention and Spatial Attention mechanisms, \nenabling the network to selectively focus on informative \nchannels and spatial regions of the input feature maps, which \nin turn improves the network's feature representation \ncapability. The second track involves the use of a Shuffle \ntransformer to effectively extract the long -range \ndependencies. The features from the two different tracks \nwere concatenated and subjected to the Coordinate Attention \nmodule to further enhance the feature representation for \nprecise cla ssification. The architecture of the proposed \nnetwork is presented in Figure 1. \n \nB. INCEPTION-RESNET BLOCK \n \nConvolutional Neural Networks (CNNs) generally \nrequire larger convolutional layers to improve feature \nextraction and classification performance. However, \nincreasing the number of layers in the network can lead to \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n4                                                                                                                                                                                                         Volume xxxx \nlonger computation times. Conversely, a model designed for \nthe low computational cost may not achieve satisfactory \nclassification accuracy. To address these challenges, the \nInception-ResNet block has been incorporated into the \nbaseline CNN module [36]. The  Inception-ResNet block \nemploys a series of parallel convolutional layers with \ndifferent kernel sizes to extract features at multiple scales \nfrom the input tensor. The outputs of these parallel layers are \nconcatenated along the channel dimension, generatin g a \nhigh-dimensional feature map. Next, a bottleneck layer \nconsisting of 1x1 convolutions reduces the feature map's \ndimensionality. Figure 2 illustrates the architecture of the \nInception-ResNet block. \n \nC. CONVOLUTIONAL BLOCK ATTENTION MODULE \n(CBAM) \n \nThe CBAM module is a neural network component that \nenhances the performance of convolutional neural networks \n(CNNs) by focusing on relevant image features [37]. It \n \n \nFigure 1. Architecture of the proposed network\n \n \n \n  \n \n \n \ncomprises two modules: the channel attention module and \nthe spatial attention module. The channel attention module \nuses average pooling and max pooling to obtain the global \ninformation of each channel. This information is utilized to \n  Figure 2. Schematic Overview of Inception-Resnet Figure 3. Architecture of the CBAM Block \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         5 \ncalculate channel -wise attention weights by concatenating \nthe average and maximum pooling values and passing them \nthrough several fully connected layers. These weights are \nthen employed to highlight the essential channels and \nsuppress the less significant ones by rescaling the channel \nfeatures. \nThe spatial attention module of the CBAM module \nfollows a similar approach to the channel attention module, \nbut it rescales the spatial features rather than the channel \nfeatures. The spatial attention module applies average \npooling and max pooling to the feature maps of each channel \nto determine the spatial attention weights. These weights are \nthen used to rescale the spatial features by emphasizing the \nsignificant spatial locations and reducing the unimportant \nones. By integ rating the channel and spatial a ttention \nmodules, the CBAM module can selectively focus on \nimportant features, resulting in improved performance in \ntasks such as image classification. Figure 3 illustrates the \narchitecture of the CBAM.\n \n \nFigure 4. Architecture of the Shuffle-Transformer block \n \nD. SHUFFLE TRANSFORMER \n \nThe Shuffle Transformer is a convolutional neural \nnetwork (CNN) that employs three modules to learn global \nand local relationships between feature maps in an image \n[38]. The first module is the window -based multi-head self-\nattention module (WMSA), which converts the feature maps \ninto a 2D grid and uses self -attention over the rows and \ncolumns to capture long -range dependencies between \ndifferent parts of the image. The second module is the \nneighbor window connection module (NWC), which \nconnects the feature m aps of neighboring pixels within a \nsmall window to capture local relationships and improve \nperformance while reducing computational complexity. \nFinally, the shuffle window -based multi-head self-attention \nmodule (shuffle-WMSA) is used to shuffle the channels after \napplying self-attention to capture diverse information from \nthe feature maps and improve the network's ability to detect \ncomplex features.  \nThe combination of these three modules allows the \nShuffle Transformer to achieve state-of-the-art performance \nin image classification. By using a combination of global and \nlocal information and leveraging channel shuffling \ntechniques, the Shuffle Transform er is capable of capturing \ncomplex features in images efficiently and accurately. Figure \n4 illustrates the architecture of the Shuffle-Transformer. \n \nE. COORDINATE ATTENTION MECHANISM BLOCK \n \nThe Coordinate Attention Block is a convolutional \nneural network module that attends to features selectively, \nbased on their spatial locations in an image [39]. This is \nachieved by separately applying global pooling along the x \nand y axes to capture the me an and variance of the feature \nmaps along each axis. Specifically, the module computes the \nmean of the feature maps at each position along the x -axis \nand y-axis through global average pooling. These two mean \nvalues are then concatenated and fed into a feed forward \nnetwork to calculate attention weights for each spatial \nlocation. The attention weights are then utilized to rescale the \noriginal feature maps, highlighting the significant locations \nand de-emphasizing the less important ones. The Coordinate \nAttention Block selectively focuses on critical spatial \nlocations in an image, leading to improved performance of \nCNNs in various computer vision tasks, such as object \ndetection, image classification, and semantic segmentation. \nAdditionally, the module requires fewer parameters than \nother attention mechanisms, resulting in computational \nefficiency and ease of implementation in CNN architectures. \nThe following architecture is represented in Figure 5. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n6                                                                                                                                                                                                         Volume xxxx \n \n \n \n \nIV. RESULTS AND DISCUSSION \n \nThis section presents an overview of the dataset \ndescription, data augmentation techniques, experimental \nsetup, model training, and validation. The final subsection \nevaluates the performance of the model using several \nperformance metrics for each ablation experiment \nperformed. \n \nA. DATASET DESCRIPTION \n \nThis study utilized the grape leaf dataset, which \nincludes 4062 labeled images with dimensions of 256 by 256 \npixels, \n.Figure 5. Architecture of the Coordinate Attention Block \n \nTABLE 1. Sample images for each class from the plant village dataset. \n \nInput class Image samples \nHealthy \n                 \nBlack Measles \n                 \n \nBlack Rot \n                 \n \nIsariopsis leaf spot \n                 \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         7 \nsourced from the plant village dataset. The grape leaf dataset \nis separated into two subsets, healthy and diseased, and \nfurther categorized into three distinct diseases: Black Rot, \nBlack Measles, and Isariopsis leaf spot. Table 1 presents the \nsample images for each class. \n \nB. DATA AUGMENTATION \n \nData augmentation is a technique used to increase \ndataset diversity for leaf disease detection. It creates new \ndata points by applying transformations such as rotations, \nflips, scaling, and changes in brightness and contrast. This \ntechnique can help to balance an imbalanced dataset and \nimprove classifier accuracy. It can also improve the \ngeneralization of the classifier by exposing it to a wider range \nof variations in the appearance of diseased leaves. Overall, \ndata augmentation is useful for improving the accuracy and \nrobustness of the classifier in limited or imbalanced datasets. \nIt can help distinguish between healthy and disea sed leaves \nin real -world scenarios. Data augmentation is done in this \nwork using various techniques to create new data points from \nexisting ones. These techniques include: (1) rotating the \nimage randomly between 0 to 40 degrees, (2) shearing the \nimage randomly between 0 to 0.2, (3) shifting the width of \nthe image randomly between 0 to 0.2, (4) flipping the image \nrandomly horizontally and vertically, (5) adjusting the \nbrightness randomly between 0.5 to 1.5, and (6) applying a \nrandom gaussian blur with sigma values between 0 to 0.8. \nThe dataset was divided into three distinct sets, with the \ntraining set containing 60% of the data, the validation set \ncontaining 20%, and the remaining 20% comprising the test \nset. After the split, augmentation was carried out on both the \ntraining and validation sets. \n \nC. ENVIRONMENTAL SETUP \n \nThe proposed network was developed utilizing \nPyTorch, which is an open-source deep-learning framework. \nThe model employed the Adam optimization technique, with \na learning rate of 0.0001 and weight decay of 0.0001, to \nminimize the loss. The training process was carried out on an \nAzure virtual machine that was equipped with an NVIDIA \nTesla P40 GPU, specifically designed for deep learning \ntasks. \n \nD. EVALUATION METRICS \n \nThe following metrics are evaluated in this context: \nAccuracy, Precision, Recall, F1 Score, Specificity, and Area \nUnder Curve (AUC). These parameters are used to determine \nthe quality of a model. Accuracy is a statistical metric that \nmeasures how accurately a predicted value matches its actual \nvalue. Precision is the ratio of true positives to the total \nnumber of positive predictions made by the model. \nSpecificity measures the model's ability to correctly identify \ntrue negative values, and it is calculated as the ratio of true \nnegatives to the total number of negatives. Recall is the ratio \nof true positives to the total number of positive samples in \nthe dataset. The F1 score is a combination of precision and \nrecall and measures the model's ability to identify positive \nsamples. The evaluation metrics are determined using the \nTrue Positive (TP), False Positive (FP), True Negative (TN), \nand False Negative (FN) values, which are expressed in (1 -\n5) below. Overall, these metrics help in evaluating the \nperformance o f a model and identifying areas for \nimprovement. The area under Receiver Operating \nCharacteristic Curve (ROC) is used to describe the \nprobabilistic variation between recall and False Positive. \n \nAccuracy = \nùëáùëù+ùëáùëõ\nùëáùëù+ùëáùëõ+ùêπùëù+ùêπùëõ\n                                                     (1) \n \nPrecision = \nùëáùëù\nùëáùëù+ùêπùëù\n                                                                (2) \n \nRecall = \nùëáùëù\nùëáùëù+ùêπùëõ\n                                                                    (3)  \n \nF1 Score = \n2‚àóùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ‚àóùëÖùëíùëêùëéùëôùëô\nùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëÖùëíùëêùëéùëôùëô                                                (4) \n \nSpecificity = \nùëáùëõ\nùëáùëõ+ùêπùëù\n                                                             (5) \n \nE. ABLATION STUDIES \n \nAblation studies are used in CNN to analyze the \nimportance of different components by removing or \ndisabling them. This helps to understand their individual \ncontributions to the model's overall performance and identify \nareas for improvement. The proposed model comprises of a \n7-layer CNN as the base model, integrated with several other \nblocks including Inception -ResNet, Convolutional Block \nAttention Module (CBAM), Shuffle Transformer block, and \ncoordinate attention block. Sequential analysis was \nperformed to understand the impact of integrating these \nblocks into the bas eline CNN. Ablation studies were \nconducted on each model to assess the effect of each block \non the network's performance. An experiment was then \nconducted to measure the accuracy, precision, recall, F1 \nscore, and area under the ROC curve for each model \nsequentially, and the results were presented in Table 2. \n \n1) Analysis of the 7-layer linear CNN \n \nThe performance of the baseline 7-layer CNN network \nwas evaluated in this experiment. The linear layered CNN \nstructure, without additional modules, was trained for 120 \nepochs. The proposed network architecture consists of \nconvolutional blocks followed by b atch normalization and \nRectified Linear Unit (ReLU) activation functions. Three out \nof the seven convolutional blocks are accompanied by max \npooling and dropout layers to enhance the model's \ngeneralization ability. Fig. 6 illustrates the performance and \nROC curve of the baseline 7 -layer CNN is presented. The \nevaluation of the trained baseline model on a separate dataset \nyielded an accuracy of 92.05%. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n8                                                                                                                                                                                                         Volume xxxx \n \n2) Effectiveness of the Inception-ResNet \n \nThis study investigates the impact of incorporating \nInception-ResNet blocks into the linear CNN model used in \nthe previous experiment. By utilizing parallel convolution \nwith different kernel sizes, Inception -ResNet blocks lower \nthe network's computational expense and enhance feature \nextraction by combining the input image with multiple \nparallel convoluted images. The trained model achieves a \ntest set accuracy of 98.50%, with additional metric results \nprovided in Table 2. Figure 7 displays the model's \nperformance and ROC curve. \n \n3) Effectiveness of the CBAM Block \n \nIn this study, the impact of incorporating a CBAM \nblock into the network used in the previous experiment is \nanalyzed. CBAM enhances spatial and channel features, \nleading to improved accuracy and generalization for \nsignificant feature extraction. After training the model with \nCBAM, it achieved an enhanced accuracy of 98.75% on a \nnew testing dataset, which is 0.25% higher than the previous \nablation study. Figure 8 presents the accuracy, loss, and ROC \ncurve of this proposed model, while the other parameters are \npresented in Table 2. \n \n4) Analysis of the Shuffle Transformer Block \n \nThis study examines how incorporating the shuffle \ntransformer track affects the final classification. By utilizing \nglobal and local information and incorporating channel \nshuffling techniques, the Shuffle Transformer can effectively \nand accurately capture c omplex features in images. The \nnetwork was trained for 120 epochs and achieved a test set \naccuracy of 97.44%. Figure 9 displays the accuracy, loss, and \nROC curve of this proposed model, while the other \nparameters are listed in Table 2. \n \n(a)                                                              (b)                                                        (c) \n \nFigure 6. Analysis of the baseline CNN network (a) Accuracy (b) Loss (c) ROC curve. \n \n \n(a)                                                              (b)                                                         (c)                     \n \nFigure 7. Analysis of the baseline CNN network + Inception-Resnet Block (a) Accuracy (b) Loss (c) ROC curve. \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         9 \n \n(a)                                                              (b)                                                         (c) \n \nFigure 8. Analysis of the baseline CNN + Inception-Resnet + CBAM module Block (a) Accuracy (b) Loss (c) ROC curve. \n \n5) Analysis of the proposed network without CA Block \n \nThis study investigates the impact of concatenating the \nfeatures extracted from two distinct branches, namely the \nCNN track (Section 4.5.3) and the shuffle transformer track \n(Section 4.5.4). The features were combined and trained \nwithout utilizing Coordina te attention. The resulting \naccuracy for the test set was 99.19%, which represents an \nimprovement of almost 0.44% over all previous ablation \nstudies. Figure 10 illustrates the accuracy, loss, and ROC \ncurve of this proposed model, and Table 2 lists the othe r \nparameters. \n \n6) Analysis of the proposed network \n \nThe network used in the previous experiment was \nmodified by adding coordinate attention and then trained for \n120 epochs. This architecture comprises a linear CNN as the \nbase \n \n(a)                                                           (b)                                                          (c) \n \nFigure 9. Analysis of the shuffle Transformer track (a) Accuracy (b) Loss (c) ROC curve. \n \n \n(a)                                                           (b)                                                         (c) \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n10                                                                                                                                                                                                         Volume xxxx \n \nFigure 10. Analysis of the Proposed Network without CA Block (a) Accuracy (b) Loss (c) ROC curve. \n \n \n(a)                                                           (b)                                                         (c) \n \nFigure 11. Analysis of the Proposed model (a) Accuracy (b) Loss (c) ROC curve. \n \nmodel, Inception-ResNet to reduce the computational cost, \nCBAM to enhance generalization by improving spatial and \nchannel features, Shuffle Transformer to extract long -range \ndependency features, and coordinate attention to focus on the \nx-axis and y -axis us ing global average pooling (GAP) to \nprovide precise feature weighting. The proposed model \nachieved an improved accuracy of 99.59% on the test set, \nwhich is a 0.40% increase compared to the previous model \nwithout coordinate attention. Figure 11 shows the ac curacy, \nloss, and ROC for the proposed model, and additional \nmetrics are presented in Table 2. \n \n \n \nTABLE 2. Sample images for each class from the plant village dataset. \n \nModel Accuracy Precision Recall F1-Score Specificity AUC \nBaseline CNN 0.9206 0.9217 0.9206 0.9208 0.9225 0.9201 \nBaseline CNN + Inception-\nResnet 0.9850 0.9856 0.9850 0.9851 0.9750 0.9904 \nBaseline CNN + Inception-\nResnet + CBAM 0.9875 0.9877 0.9875 0.9875 0.9725 0.9925 \nShuffle Transformer 0.9744 0.9759 0.9744 0.9745 0.9725 0.9850 \nBaseline CNN + Inception-\nResnet + CBAM + Shuffle \nTransformer \n0.9919 0.9921 0.9919 0.9919 0.9750 0.9925 \nProposed Network 0.9956 0.9956 0.9956 0.9956 0.9975 0.9975 \n \n \nF. PERFORMANCE ANALYSIS \n \nTable 3 presents the performance evaluation between the \nexisting works and the proposed model. Existing works \npredominantly concentrated on pre -trained architectures, \nwhich performed modestly on a limited amount of data. The \nproposed model integrates various compo nents, such as \nInception-ResNet, Convolutional Block Attention Module \n(CBAM), Shuffle Transformer, and Coordinate Attention \nmechanism. Transfer learning -based approaches were quite \nsuccessful in disease detection and these methods have an \naccuracy in the r ange of 95 to 98%. Although transfer \nlearning can be an effective method to achieve high accuracy \nrates, it is crucial to acknowledge that pre -trained \narchitectures may not always be suitable for every task. In \ncontrast to prior studies that did not incorp orate attention or \ncontext mechanisms, the proposed customized CNN model \nwas more successful in identifying grape leaf diseases. The \nproposed system leverages the connections among features \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         11 \nand contextual information, allowing it to acquire features \nwith greater accuracy and attain higher levels of precision. \nAdditionally, Table 4 illustrates a comparison between the \nnumber of trainable parameters in the proposed network and \nthose in state-of-the-art architectures. The analysis indicates \nthat the proposed network outperforms its counterparts while \nusing fewer trainable parameters. \n \nTABLE 3. Comparative analysis of the performance of the \nproposed work with that of other existing works \n \n \nSl.no Source Methodology Accuracy \n(in %) \n1 \nYohan \nRayhan et al. \n[30] \nVGG16 87.5 \n2 Sajjad Nasari \net al. [16] GLCM with SVM 89.93 \n3 Alishba Adeel \net al. [19] \nLow contrast haze \nreduction-\nneighbourhood \n91.34 \ncomponent analysis \nwith SVM \n4 Arie Hasan et \nal. [34] VGG16 95 \n5 Adi  et al. [33] VGG6 and VGG19 98 \n6 Khaing  et al. \n[21] VGG16 98.4 \n7 Miaomiao  et \nal. [25] \nGoogleNet & \nResNet50 98.57 \n8 Zhe et al. [31] AlexNet \n 99.01 \n9 Xiaoyue et \nal. [35] \nInception-Squeeze \nand Excitation-Resnet \nModel \n99.47 \n10 Proposed \nnetwork \nDual track deep \nfeature network 99.59 \n \n \n \n \n \n \nTABLE 4. Parameter analysis of the proposed model. \n \nModel  No. of \nParameters  \nAccuracy Precision F1-Score Recall \nVGG16 134,276,932 0.9444 0.9444 0.9472 0.9439 \nAlexNet 56,337,156 0.9500 0.9500 0.9528 0.9504 \nInception-ResNetV3 54,342,884 0.9844 0.9844 0.9849 0.9844 \nResNet50 23,595,908 0.9550 0.9550 0.9567 0.9548 \nInceptionV3 21,810,980 0.9675 0.9675 0.9696 0.9671 \nXception 20,869,676 0.9781 0.9781 0.9794 0.9781 \nProposed network  8,836,697 0.9956 0.9956 0.9956 0.9956 \n \nV. CONCLUSION \n \nGrapes offer numerous nutritional benefits, including \nhigh levels of antioxidants, vitamins, and minerals. \nHowever, diverse climatic conditions during grape \ncultivation make them susceptible to different diseases. \nCurrent disease detection methods rely on manual visual \nrecognition of symptoms, leading to delayed treatment and \npotential crop loss. Advanced computer vision techniques \ncan address this challenge by providing automatic and \naccurate disease classification. This research proposes a \nnovel network for grap e leaf disease detection with two \nfeature extraction tracks for precise classification. The first \ntrack utilizes Inception-ResNet blocks and CBAM to achieve \noptimal balance and enhance feature representation. CBAM \nintegrates Channel and Spatial Attention mechanisms to \nfocus on informative channels and spatial regions. The \nsecond track employs a Shuffle transformer to effectively \nextract long -range dependencies . The features from both \ntracks are combined and subjected to the Coordinate \nattention module to fu rther refine feature representation for \nprecise classification. The proposed model, trained on the \nGrape leaf disease dataset from Plant Village, achieves an \naccuracy of 99.56%. The proposed network is limited to \ndetect disease patterns  acquired from constrained leaf \nportions under experimental setup. Though it performs well \nfor these constrained setup, as an extension, further research \nis required to detect diseases from large scale landscape \nimages acquired through drones for real -time deployment. \nOther p otential future research directions include refining \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n12                                                                                                                                                                                                         Volume xxxx \nsegmentation algorithms to improve the accuracy of \noutlining disease markers in grape leaf images and extending \nthe proposed framework to other plant types, enabling \ncomputer-aided disease classification across various crops. \n \nREFERENCE \n \n[1] C. Venkitasamy, L. Zhao, R. Zhang, and Z. Pan, ‚ÄúGrapes,‚Äù Integrated \nProcessing Technologies for Food and Agricultural By -Products. Elsevier, \npp. 133‚Äì163, 2019. doi: 10.1016/b978-0-12-814138-0.00006-x. \n[2] A. Seccia, F. G. Santeramo, and G. Nardone, ‚ÄúTrade Competitiveness in \nTable Grapes,‚Äù Outlook on Agriculture, vol. 44, no. 2. SAGE Publications, \npp. 127‚Äì134, Jun. 2015. doi: 10.5367/oa.2015.0205. \n \n[3] J. M. Alston and O. Sambucci, ‚ÄúGrapes in the World Economy,‚Äù \nCompendium of Plant Genomes. Springer International Publishing, pp. 1 ‚Äì\n24, 2019. doi: 10.1007/978-3-030-18601-2_1. \n \n[4] S. G. Crandall, J. Spychalla, U. T. Crouch, F. E. Acevedo, R. P. Naegele, \nand T. D. Miles, ‚ÄúRotting Grapes Don‚Äôt Improve with Age: Cluster Rot \nDisease Complexes, Management, and Future Prospects,‚Äù Plant Disease, \nvol. 106, no. 8. Scientific Societies, pp. 2 013‚Äì2025, Aug. 01, 2022. doi: \n10.1094/pdis-04-21-0695-fe. \n \n[5] K. Sanghavi, M. Sanghavi, and A. M. Rajurkar, ‚ÄúEarly stage detection \nof Downey and Powdery Mildew grape disease using atmospheric \nparameters through sensor nodes,‚Äù Artificial Intelligence in Agriculture, \nvol. 5. Elsevier BV, pp. 223‚Äì232, 2021. doi: 10.1016/j.aiia.2021.10.001. \n \n[6] J. Zhu, A. Wu, X. Wang, and H. Zhang, ‚ÄúIdentification of grape diseases \nusing image analysis and BP neural networks,‚Äù Multimedia Tools and \nApplications, vol. 79, no. 21 ‚Äì22. Springer Science and Business Media \nLLC, pp. 14539‚Äì14551, Jan. 02, 2019. doi: 10.1007/s11042-018-7092-0. \n \n[7] M. Ji and Z. Wu, ‚ÄúAutomatic detection and severity analysis of grape \nblack measles disease based on deep learning and fuzzy logic,‚Äù Computers \nand Electronics in Agriculture, vol. 193. Elsevier BV, p. 106718, Feb. 2022. \ndoi: 10.1016/j.compag.2022.106718. \n \n[8] D. Molitor and B. Berkelmann-Loehnertz, ‚ÄúSimulating the susceptibility \nof clusters to grape black rot infections depending on their phenological \ndevelopment,‚Äù Crop Protection, vol. 30, no. 12. Elsevier BV, pp. 1649 ‚Äì\n1654, Dec. 2011. doi: 10.1016/j.cropro.2011.07.020. \n \n[9] N. Kellner, E. Antal, A. Szab√≥, and R. Matolcsi, ‚ÄúThe effect of black rot \non grape berry composition,‚Äù Acta Alimentaria, vol. 51, no. 1. Akademiai \nKiado Zrt., pp. 126‚Äì133, Feb. 28, 2022. doi: 10.1556/066.2021.00195. \n \n[10] L. W. Kuswidiyanto, H.-H. Noh, and X. Han, ‚ÄúPlant Disease Diagnosis \nUsing Deep Learning Based on Aerial Hyperspectral Images: A Review,‚Äù \nRemote Sensing, vol. 14, no. 23. MDPI AG, p. 6031, Nov. 28, 2022. doi: \n10.3390/rs14236031. \n \n[11] A. Sinha and R. Singh Shekhawat, ‚ÄúA novel image classification \ntechnique for spot and blight diseases in plant leaves,‚Äù The Imaging Science \nJournal, vol. 68, no. 4. Informa UK Limited, pp. 225 ‚Äì239, May 18, 2020. \ndoi: 10.1080/13682199.2020.1865652. \n \n[12] A. Ali, S. Ali, M. Husnain, M. M. Saad Missen, A. Samad, and M. \nKhan, ‚ÄúDetection of Deficiency of Nutrients in Grape Leaves Using Deep \nNetwork,‚Äù Mathematical Problems in Engineering, vol. 2022. Hindawi \nLimited, pp. 1‚Äì12, Jan. 31, 2022. doi: 10.1155/2022/3114525. \n \n[13] A.-K. Mahlein, ‚ÄúPlant Disease Detection by Imaging Sensors ‚Äì \nParallels and Specific Demands for Precision Agriculture and Plant \nPhenotyping,‚Äù Plant Disease, vol. 100, no. 2. Scientific Societies, pp. 241 ‚Äì\n251, Feb. 2016. doi: 10.1094/pdis-03-15-0340-fe. \n \n[14] S. P. Mohanty, D. P. Hughes, and M. Salath√©, ‚ÄúUsing Deep Learning \nfor Image-Based Plant Disease Detection,‚Äù Frontiers in Plant Science, vol. \n7. Frontiers Media SA, Sep. 22, 2016. doi: 10.3389/fpls.2016.01419. \n \n[15] N. Agrawal, J. Singhai, and D. K. Agarwal, ‚ÄúGrape leaf disease \ndetection and classification using multi-class support vector machine,‚Äù 2017 \nInternational Conference on Recent Innovations in Signal processing and \nEmbedded Systems (RISE). IEEE, Oct. 2017. do i: \n10.1109/rise.2017.8378160. \n \n[16] Nasiri S and Zhart Nejand M, ‚ÄúA method based on image processing \nfor the automatic diagnosis of grapevine leaf disease. ‚Äù, Biosystem \nEngineering of Iran, vol.53, no.1,  Apr.2022, doi: \n10.22059/ijbse.2022.327192.665432. \n \n[17] S. M. Jaisakthi, P. Mirunalini, D. Thenmozhi, and Vatsala, ‚ÄúGrape Leaf \nDisease Identification using Machine Learning Techniques,‚Äù 2019 \nInternational Conference on Computational Intelligence in Data Science \n(ICCIDS). IEEE, Feb. 2019. doi: 10.1109/iccids.2019.8862084. \n \n[18] S. M. Javidan, A. Banakar, K. A. Vakilian, and Y. Ampatzidis, \n‚ÄúDiagnosis of grape leaf diseases using automatic K -means clustering and \nmachine learning,‚Äù Smart Agricultural Technology, vol. 3. Elsevier BV, p. \n100081, Feb. 2023. doi: 10.1016/j.atech.2022.100081. \n \n[19] A. Adeel et al., ‚ÄúDiagnosis and recognition of grape leaf diseases: An \nautomated system based on a novel saliency approach and canonical \ncorrelation analysis based multiple features fusion,‚Äù Sustainable \nComputing: Informatics and Systems, vol. 24. Elsevier BV, p. 100349, Dec. \n2019. doi: 10.1016/j.suscom.2019.08.002. \n \n[20] M. Shantkumari and S. V. Uma, ‚ÄúGrape leaf image classification based \non machine learning technique for accurate leaf disease detection,‚Äù \nMultimedia Tools and Applications, vol. 82, no. 1. Springer Science and \nBusiness Media LLC, pp. 1477‚Äì1487, Apr. 11, 2022. doi: 10.1007/s11042-\n022-12976-z. \n \n[21] K. Z. Thet, K. K. Htwe, and M. M. Thein, ‚ÄúGrape Leaf Diseases \nClassification using Convolutional Neural Network,‚Äù 2020 International \nConference on Advanced Information Technologies (ICAIT). IEEE, Nov. \n04, 2020. doi: 10.1109/icait51105.2020.9261801. \n \n[22] B. Jain and S. Periyasamy, ‚ÄúGrapes disease detection using transfer \nlearning.‚Äù arXiv, 2022. doi: 10.48550/ARXIV.2208.07647. \n \n[23] Z. Huang, A. Qin, J. Lu, A. Menon, and J. Gao, ‚ÄúGrape Leaf Disease \nDetection and Classification Using Machine Learning,‚Äù 2020 International \nConferences on Internet of Things (iThings) and IEEE Green Computing \nand Communications (GreenCom) and IEEE Cyber, P hysical and Social \nComputing (CPSCom) and IEEE Smart Data (SmartData) and IEEE \nCongress on Cybermatics (Cybermatics). IEEE, Nov. 2020. doi: \n10.1109/ithings-greencom-cpscom-smartdata-\ncybermatics50389.2020.00150. \n \n[24] Y. Peng, S. Zhao, and J. Liu, ‚ÄúFused-Deep-Features Based Grape Leaf \nDisease Diagnosis,‚Äù Agronomy, vol. 11, no. 11. MDPI AG, p. 2234, Nov. \n04, 2021. doi: 10.3390/agronomy11112234. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDr. Karthik R et. al                                                                                                                                                          \nVolume xxxx                                                                                                                                                                                                         13 \n \n[25] M. Ji, L. Zhang, and Q. Wu, ‚ÄúAutomatic grape leaf diseases \nidentification via UnitedModel based on multiple convolutional neural \nnetworks,‚Äù Information Processing in Agriculture, vol. 7, no. 3. Elsevier \nBV, pp. 418‚Äì426, Sep. 2020. doi: 10.1016/j.inpa.2019.10.003. \n \n[26] K. Ashokkumar, S. Parthasarathy, S. Nandhini, and K. Ananthajothi , \n‚ÄúPrediction of grape leaf through digital image using FRCNN,‚Äù \nMeasurement: Sensors, vol. 24. Elsevier BV, p. 100447, Dec. 2022. doi: \n10.1016/j.measen.2022.100447. \n \n[27] M. Fraiwan, E. Faouri, and N. Khasawneh , ‚ÄúMulticlass Classification \nof Grape Diseases Using Deep Artificial Intelligence,‚Äù Agriculture, vol. 12, \nno. 10. MDPI AG, p. 1542, Sep. 24, 2022. doi: \n10.3390/agriculture12101542. \n \n[28] B. Liu, C. Tan, S. Li, J. He, and H. Wang, ‚ÄúA Data Augmentation \nMethod Based on Generative Adversarial Networks for Grape Leaf Disease \nIdentification,‚Äù IEEE Access, vol. 8. Institute of Electrical and Electronics \nEngineers (IEEE), pp. 102188 ‚Äì102198, 2020. doi: \n10.1109/access.2020.2998839. \n \n[29] A. K. Uttam, ‚ÄúGrape Leaf Disease Prediction Using Deep Learning,‚Äù \n2022 International Conference on Applied Artificial Intelligence and \nComputing (ICAAIC). IEEE, May 09, 2022. doi: \n10.1109/icaaic53929.2022.9792739. \n \n[30] Y. Rayhan and D. B. Setyohadi, ‚ÄúClassification of Grape Leaf Disease \nUsing Convolutional Neural Network (CNN) with Pre -Trained Model \nVGG16,‚Äù 2021 International Conference on Smart Generation Computing, \nCommunication and Networking (SMART GENCON). IEEE, Oct . 29, \n2021. doi: 10.1109/smartgencon51891.2021.9645862. \n \n[31] Z. Tang, J. Yang, Z. Li, and F. Qi, ‚ÄúGrape disease image classification \nbased on lightweight convolution neural networks and channelwise \nattention,‚Äù Computers and Electronics in Agriculture, vol. 178. Elsevier BV, \np. 105735, Nov. 2020. doi: 10.1016/j.compag.2020.105735. \n \n[32] S. Lauguico, R. Concepcion, R. Ruzcko Tobias, A. Bandala, R. Rhay \nVicerra, and E. Dadios, ‚ÄúGrape Leaf Multi -disease Detection with \nConfidence Value Using Transfer Learning Integrated to Regions with \nConvolutional Neural Networks,‚Äù 2020 IEEE REGION 10 CONFERENCE \n(TENCON).IEEE,Nov. 16, 2020. doi: 10.1109/tencon50793.2020.9293866. \n \n[33] S. A. P. N. Kavala and R. Pothuraju, ‚ÄúDetection Of Grape Leaf Disease \nUsing Transfer Learning Methods: VGG16 &amp; VGG19,‚Äù 2022 6th \nInternational Conference on Computing Methodologies and \nCommunication (ICCMC). IEEE, Mar. 29, 2022. doi: \n10.1109/iccmc53470.2022.9753773. \n \n[34] Moh. A. Hasan, Y. Riyanto, and D. Riana, ‚ÄúGrape leaf image disease \nclassification using CNN -VGG16 model,‚Äù Jurnal Teknologi dan Sistem \nKomputer, vol. 9, no. 4. Institute of Research and Community Services \nDiponegoro University (LPPM UNDIP), pp. 218 ‚Äì223, Jul. 05, 2021. doi: \n10.14710/jtsiskom.2021.14013. \n \n[35] X. Xie, Y. Ma, B. Liu, J. He, S. Li, and H. Wang, ‚ÄúA Deep -Learning-\nBased Real -Time Detector for Grape Leaf Diseases Using Improved \nConvolutional Neural Networks,‚Äù Frontiers in Plant Science, vol. 11. \nFrontiers Media SA, Jun. 03, 2020. doi: 10.3389/fpls.2020.00751. \n \n[36] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi, ‚ÄúInception -v4, \nInception-ResNet and the Impact of Residual Connections on Learning.‚Äù \narXiv, 2016. doi: 10.48550/ARXIV.1602.07261. \n \n[37] M. Canayaz, ‚ÄúC+EffxNet: A novel hybrid approach for COVID -19 \ndiagnosis on CT images based on CBAM and EfficientNet,‚Äù Chaos, Solitons \n&amp; Fractals, vol. 151. Elsevier BV, p. 111310, Oct. 2021. doi: \n10.1016/j.chaos.2021.111310. \n \n[38] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, ‚ÄúShuffle \nTransformer: Rethinking Spatial Shuffle for Vision Transformer.‚Äù arXiv, \n2021. doi: 10.48550/ARXIV.2106.03650. \n \n[39] S. Cheng, L. Wang, and A. Du, ‚ÄúAsymmetric coordinate attention \nspectral-spatial feature fusion network for hyperspectral image \nclassification,‚Äù Scientific Reports, vol. 11, no. 1. Springer Science and \nBusiness Media LLC, Aug. 31, 2021. doi: 10.1038/s41598-021-97029-5. \n \n[40] Z. Tang et al., ‚ÄúA Precise Image-Based Tomato Leaf Disease Detection \nApproach Using PLPNet,‚Äù Plant Phenomics, vol. 5. American Association \nfor the Advancement of Science (AAAS), Jan. 2023. doi: \n10.34133/plantphenomics.0042.  \n \n[41] J. Li, G. Zhou, A. Chen, C. Lu, and L. Li, ‚ÄúBCMNet: Cross -Layer \nExtraction Structure and Multiscale Downsampling Network With \nBidirectional Transpose FPN for Fast Detection of Wildfire Smoke,‚Äù IEEE \nSystems Journal, vol. 17, no. 1. Institute of Electri cal and Electronics \nEngineers (IEEE), pp. 1235 ‚Äì1246, Mar. 2023. doi: \n10.1109/jsyst.2022.3193951.  \n \n[42]  J. Zhan, Y. Hu, G. Zhou, Y. Wang, W. Cai, and L. Li, ‚ÄúA high -\nprecision forest fire smoke detection approach based on ARGNet,‚Äù \nComputers and Electronics in Agriculture, vol. 196. Elsevier BV, p. 106874, \nMay 2022. doi: 10.1016/j.compag.2022.106874.  \n \n[43] L. Zhang, C. Lu, H. Xu, A. Chen, L. Li, and G. Zhou, ‚ÄúMMFNet: Forest \nFire Smoke Detection Using Multiscale Convergence Coordinated Pyramid \nNetwork With Mixed Attention and Fast -Robust NMS,‚Äù IEEE Internet of \nThings Journal, vol. 10, no. 20. Institute o f Electrical and Electronics \nEngineers (IEEE), pp. 18168 ‚Äì18180, Oct. 15, 2023. doi: \n10.1109/jiot.2023.3277511.   \n \n[44] A. S. Zamani et al., ‚ÄúPerformance of Machine Learning and Image \nProcessing in Plant Leaf Disease Detection,‚Äù Journal of Food Quality, vol. \n2022. Hindawi Limited, pp. 1 ‚Äì7, Apr. 26, 2022. doi: \n10.1155/2022/1598796.  \n \n[45] Z. Guo et al., ‚ÄúQuality grading of jujubes using composite \nconvolutional neural networks in combination with RGB color space \nsegmentation and deep convolutional generative adversarial networks,‚Äù \nJournal of Food Process Engineering, vol. 44, no. 2. Wil ey, Dec. 06, 2020. \ndoi: 10.1111/jfpe.13620.  \n \n[46] V. Meshram, K. Patil, V. Meshram, D. Hanchate, and S. D. Ramkteke, \n‚ÄúMachine learning in agriculture domain: A state -of-art survey,‚Äù Artificial \nIntelligence in the Life Sciences, vol. 1. Elsevier BV, p. 100010, Dec. 2021. \ndoi: 10.1016/j.ailsci.2021.100010.  \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n                                                                                                                                                         Dr. Karthik R et. al \n14                                                                                                                                                                                                         Volume xxxx \n \nDr. R. Karthik obtained his Doctoral degree \nfrom Vellore Institute of Technology, India and \nMaster‚Äôs degree from Anna University, India. \nCurrently, He serves as Associate Professor in \nthe Research Centre for Cyber Physical Systems, \nVellore Institute of Technology, Chennai. His \nresearch interests include Deep Learning, \nComputer Vision, Digital Image Processing, and \nMedical Image Analysis. He has published \naround 75 papers in peer reviewed journals and conferences. He is an active \nreviewer for journals published by Elsevier, IEEE Springer and Nature. \n \n \nDr. R. Menaka completed her Masters in \nApplied Electronics from Anna University, \nChennai, India. She received her Doctoral \ndegree from Anna University. She is currently \nserving as Professor in the Center for Cyber \nPhysical Systems, Vellore Institute of \nTechnology, Chennai. Her areas of interest are \nbiomedical image and signal processing, neural \nnetworks and fuzzy logic. She has published \naround 80 papers in peer reviewed journals and conferences. \n \nOmpirakash S completed his bachelor‚Äôs degree \nin Electronics and Communication Engineering \nfrom Vellore Institute of Technology, Chennai. \nHis research work interest includes deep learning, \nmachine learning in the agricultural field, and \nchatbot for customer service . Recently, he has \npublished research work on chatbots. \n \n \n \nBalamurugan P  is currently enrolled in the \nUniversity of Liverpool, pursuing a Master's \ndegree in Data Science and Artificial \nIntelligence. He earned his Bachelor of \nTechnology degree in Electronics and \nCommunication Engineering from VIT Chennai. \nHis interest in research include areas like Deep \nLearning, Image Recognition, Data Science, and \nMachine Learning. \n \n \n \n Meenakshi M , an  electronics and \ncommunication engineer passionate about \nbringing innovative solutions for the real -world \nproblem in a more efficient way.  Her Interest in \nresearch work is currently about bringing an \noptimizing solution using deep learning \ntechnologies for agriculture-based problems. \n \n \n \nDr. Sindhia Lingaswamy  is an Assistant \nprofessor with the Department of Computer \nApplications at National Institute of Technology, \nTiruchirappalli. Her areas of interest are \nComputer Vision, Image Processing, Artificial \nIntelligence, and Machine Learning. She has \npublished arou nd 10 papers in peer reviewed \njournals and conferences. \n \n \n \nDr. Daehan Won is an Associate Professor with \nthe Department of Systems Science and Industrial \nEngineering, State University of New York at \nBinghamton, Binghamton, NY, USA. His \nresearch interests include large scale \nmathematical programming, data \nanalytics/mining in hea lthcare, and designing \nsmart manufacturing systems to advance industry \n4.0. \n \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3361044\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Residual neural network",
  "concepts": [
    {
      "name": "Residual neural network",
      "score": 0.6940032243728638
    },
    {
      "name": "Computer science",
      "score": 0.6779881119728088
    },
    {
      "name": "Identification (biology)",
      "score": 0.5205710530281067
    },
    {
      "name": "Transformer",
      "score": 0.4819178879261017
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.47205811738967896
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42261192202568054
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34790441393852234
    },
    {
      "name": "Artificial neural network",
      "score": 0.26250529289245605
    },
    {
      "name": "Engineering",
      "score": 0.13394442200660706
    },
    {
      "name": "Electrical engineering",
      "score": 0.08463379740715027
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}