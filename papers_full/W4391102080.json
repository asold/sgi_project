{
  "title": "Benchmarking Large Language Models in Adolescent Growth and Development: A Comparative Analysis of Claude2, ChatGPT-3.5, and Google Bard",
  "url": "https://openalex.org/W4391102080",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2075759014",
      "name": "Ying Li",
      "affiliations": [
        "Huaibei Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2770156847",
      "name": "Zichen Song",
      "affiliations": [
        "Lanzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2102863347",
      "name": "Weijia Li",
      "affiliations": [
        "Lanzhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3200742808",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4365137614",
    "https://openalex.org/W2075786765",
    "https://openalex.org/W2140117372",
    "https://openalex.org/W4367834585",
    "https://openalex.org/W4362641141",
    "https://openalex.org/W4367175039",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W4388869624",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4324373918",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W4324129637",
    "https://openalex.org/W4388555450",
    "https://openalex.org/W6688548904",
    "https://openalex.org/W4377372007",
    "https://openalex.org/W1994664485",
    "https://openalex.org/W2948748515",
    "https://openalex.org/W6642115163",
    "https://openalex.org/W1986917868",
    "https://openalex.org/W2035751961",
    "https://openalex.org/W4299131995",
    "https://openalex.org/W6846076640",
    "https://openalex.org/W4387324017",
    "https://openalex.org/W6848092824",
    "https://openalex.org/W4385681611",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4387892117",
    "https://openalex.org/W1967832563",
    "https://openalex.org/W2219180761",
    "https://openalex.org/W2263338482"
  ],
  "abstract": "<title>Abstract</title> Background: Significant attention has been drawn to large-scale language models (LLMs) for their ability to generate responses that are both contextually relevant and reminiscent of human conversation. Yet, the precision of these models in specialized medical fields, particularly those pertaining to adolescent health, remains largely unexamined. Online searches for information about common health issues during adolescent developmental stages are frequent among patients and their families. In this context, our research evaluates how effectively three different LLMs - Claude2, ChatGPT-3.5, and Google Bard - handle typical inquiries concerning adolescent growth and health development. Methods: Our research involved gathering 100 frequently asked questions about adolescent growth and health issues, divided into 10 typical disorder categories: Attention Deficit, Tics, Developmental Delays, Autism Spectrum, Anxiety, Anorexia, Obsessive-Compulsive Disorder, Sleep Issues, Early Puberty, and Depressive Disorders. These questions were then posed to various large language models. A pediatric specialist evaluated the models' answers using a detailed four-tier system (ranging from Poor to Very Good) for accuracy. To ensure consistency, these assessments were revisited and verified at various intervals. High-scoring responses ('Good' or above) were examined closely for their compliance with medical ethics, treatment guidelines, and diagnostic procedures. In contrast, responses that scored lowest ('Poor') were subject to in-depth review, leading to recommendations for minor modifications based on straightforward query adjustments and online medical resources. These revised responses were then re-evaluated to measure any improvements in accuracy. Findings: Our study analyzed the performance of different models in adolescent growth and development issues. Claude2 was the top performer, with an average score of 3.54 and a standard deviation of 0.501. ChatGPT-3.5 was close behind, scoring an average of 3.44 and a standard deviation of 0.519. Human raters and Google Bard scored lower, at 2.60 and 2.49 respectively, with larger standard deviations. The one-way ANOVA showed significant differences (F-value 64.692, P-value 4.64e-34), particularly in areas like 'Attention Deficit Disorder', 'Developmental Delay', and 'Depression', where Claude2 and ChatGPT-3.5 outperformed others. The Pearson Chi-Square test (χ² value 117.758, P-value 2.35e-25) confirmed their accuracy and consistency. In self-correction abilities, Claude2, ChatGPT-3.5, and Bard scored 3.3, 3.0, and 2.4, respectively, for simple query-based corrections. For web-based medical self-corrections, the scores improved to 3.8, 3.5, and 3.7. The Pearson Chi-Square tests showed significant improvements for all models (Claude2 P-value 0.0241, ChatGPT-3.5 P-value 0.0150, Bard P-value 0.000017), with Bard showing the most significant improvement. This indicates that web-based medical correction methods significantly enhance performance in complex queries for all LLM chatbots. Interpretation: Our findings underscore the potential of Large Language Models (LLMs), particularly Claude2, in providing accurate and comprehensive responses to queries related to adolescent growth and development. The continual strategies and evaluations to enhance the accuracy of LLMs remain crucially important.",
  "full_text": "Page 1/19\nBenchmarking Large Language Models in\nAdolescent Growth and Development: A\nComparative Analysis of Claude2, ChatGPT-3.5, and\nGoogle Bard\nYing Li \nHuaibei People's Hospital\nZichen Song \nLanzhou University\nWeijia Li \nLanzhou University\nArticle\nKeywords: ChatGPT-3.5, Google Bard, Chatbot, Adolescent Growth and Development, Large Language\nModel\nPosted Date: January 22nd, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3858549/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/19\nAbstract\nBackground: Signi\u0000cant attention has been drawn to large-scale language models (LLMs) for their ability\nto generate responses that are both contextually relevant and reminiscent of human conversation. Yet,\nthe precision of these models in specialized medical \u0000elds, particularly those pertaining to adolescent\nhealth, remains largely unexamined. Online searches for information about common health issues during\nadolescent developmental stages are frequent among patients and their families. In this context, our\nresearch evaluates how effectively three different LLMs - Claude2, ChatGPT-3.5, and Google Bard -\nhandle typical inquiries concerning adolescent growth and health development.\nMethods: Our research involved gathering 100 frequently asked questions about adolescent growth and\nhealth issues, divided into 10 typical disorder categories: Attention De\u0000cit, Tics, Developmental Delays,\nAutism Spectrum, Anxiety, Anorexia, Obsessive-Compulsive Disorder, Sleep Issues, Early Puberty, and\nDepressive Disorders. These questions were then posed to various large language models. A pediatric\nspecialist evaluated the models' answers using a detailed four-tier system (ranging from Poor to Very\nGood) for accuracy. To ensure consistency, these assessments were revisited and veri\u0000ed at various\nintervals. High-scoring responses ('Good' or above) were examined closely for their compliance with\nmedical ethics, treatment guidelines, and diagnostic procedures. In contrast, responses that scored\nlowest ('Poor') were subject to in-depth review, leading to recommendations for minor modi\u0000cations\nbased on straightforward query adjustments and online medical resources. These revised responses\nwere then re-evaluated to measure any improvements in accuracy.\nFindings: Our study analyzed the performance of different models in adolescent growth and\ndevelopment issues. Claude2 was the top performer, with an average score of 3.54 and a standard\ndeviation of 0.501. ChatGPT-3.5 was close behind, scoring an average of 3.44 and a standard deviation\nof 0.519. Human raters and Google Bard scored lower, at 2.60 and 2.49 respectively, with larger standard\ndeviations. The one-way ANOVA showed signi\u0000cant differences (F-value 64.692, P-value 4.64e-34),\nparticularly in areas like 'Attention De\u0000cit Disorder', 'Developmental Delay', and 'Depression', where\nClaude2 and ChatGPT-3.5 outperformed others. The Pearson Chi-Square test ( χ ² value 117.758, P-value\n2.35e-25) con\u0000rmed their accuracy and consistency. In self-correction abilities, Claude2, ChatGPT-3.5,\nand Bard scored 3.3, 3.0, and 2.4, respectively, for simple query-based corrections. For web-based\nmedical self-corrections, the scores improved to 3.8, 3.5, and 3.7. The Pearson Chi-Square tests showed\nsigni\u0000cant improvements for all models (Claude2 P-value 0.0241, ChatGPT-3.5 P-value 0.0150, Bard P-\nvalue 0.000017), with Bard showing the most signi\u0000cant improvement. This indicates that web-based\nmedical correction methods signi\u0000cantly enhance performance in complex queries for all LLM chatbots.\nInterpretation: Our \u0000ndings underscore the potential of Large Language Models (LLMs), particularly\nClaude2, in providing accurate and comprehensive responses to queries related to adolescent growth\nand development. The continual strategies and evaluations to enhance the accuracy of LLMs remain\ncrucially important.\nPage 3/19\nResearch in context\nEvidence before this study\nOur research team executed a comprehensive search on PubMed, targeting literature that assesses the\ne\u0000cacy of Large Language Models (LLMs) in addressing queries pertaining to adolescent growth and\ndevelopment. We imposed no constraints regarding the date or language of the publications. The search\nstrategy involved the integration of terms linked to LLMs (such as 'Large Language Model', 'ChatGPT',\n'Google Bard') with keywords relevant to pediatrics ('Adolescent Growth', 'Adolescent Development'). Our\n\u0000ndings revealed that while existing research predominantly evaluates the pro\u0000ciency of LLMs in\nstandardized pediatric exams, there's a noticeable research gap concerning their application speci\u0000cally\nto adolescent growth and development topics. [References 1-4]\nAdded value of this study\nOur investigation distinctively focused on comparing the e\u0000cacy of three LLM-Chatbots in answering\n100 typical questions related to adolescent growth and development, a notable shift from earlier studies\nthat primarily examined standard questions from ophthalmology exams. We established a robust\nmethodology to ensure the reliability and validity of our \u0000ndings. [5-8]\nTo determine our 'ground truths', we relied on the comprehensive judgement of a seasoned pediatrician\nwith over a decade of subspecialty experience. The chatbots' responses were assorted into three\nseparate rounds and randomly presented to expert raters. To prevent bias and conceal the identity of the\nchatbots, our experts conducted their assessments on different dates, allowing a substantial 48-hour\ninterval between each rating session. This methodological rigor was designed to address the bias issue\nnoted in contemporary LLM research [9-11].\nExpanding the scope of our study, we not only assessed the accuracy of the LLM-Chatbots but also\nevaluated their comprehensiveness and self-correction capabilities. We explored how further prompting\nin\u0000uenced the accuracy of their responses and compared this with results from web-based medical self-\ncorrections. Additionally, we scrutinized the ten poorest responses, identifying inaccuracies and\nproviding expert pediatric insights. This comprehensive approach enabled us to gain deeper insights into\nthe capabilities of LLM-Chatbots in responding to adolescent health and development queries [12-13].\nImplications of all the available evidence\nThis study emphasizes the potential of LLM-Chatbots, especially Claude2, in effectively distributing\nclinical knowledge concerning adolescent growth and development. The wide accessibility of these LLM-\nChatbots suggests they could be instrumental in reducing the strain on healthcare systems. Additionally,\nthey could serve as a means to broaden public understanding of issues related to adolescent health and\ndevelopment. However, considering the early stage of development of LLM chatbots, it's crucial to equip\nthem with specialized training focused on speci\u0000c domains. This approach is necessary to guarantee\nPage 4/19\nthe precise relay of information and to avert the spread of incorrect information among patients.\n[References 14-15]\nIntroduction\nWith the progress in Natural Language Processing (NLP), hatbots are becoming an increasingly valuable\nresource in healthcare. They offer signi\u0000cant applications in various medical aspects, such as disease\nprevention, diagnosis, treatment, monitoring, and patient support. Modern NLP models, particularly\nLarge Language Models (LLMs), have shown remarkable advancements over their predecessors. LLMs,\nthrough self-supervised learning and training on vast datasets, have reached a level where they can\nmimic human-like responses more effectively. [16]\nIn the medical sector, LLMs are drawing notable attention, with initial research indicating encouraging\noutcomes. For instance, ChatGPT by OpenAI, a prominent LLM, has shown capabilities akin to the\npassing standards of the United States Medical Licensing Examination (USMLE), suggesting its utility in\nclinical support. Furthermore, the ability of LLMs to produce natural language has opened avenues for\ntheir use in patient education and information dissemination. [17]\nRecently, some exploration has been made into LLM performance in the \u0000eld of pediatrics. Jason\nHolmes et al. evaluated LLMs in highly standardized professional scenarios in pediatric ophthalmology.\nThis was followed by a further study by Zhi Wei Lim et al. assessing different LLMs response capabilities\nin more realistic scenarios of myopia management. However, studies evaluating LLMs response\ncapabilities in the \u0000eld of adolescent growth and development have not been found to date. [18–19] The\n49th Statistical Report on Internet Development in China, released by the China Internet Network\nInformation Center (CNNIC), reveals that around three-quarters of internet users in China use the internet\nto look up health-related information, with 84.5% of those in need of health advice turning to online\nsources. This trend is particularly noticeable in the area of adolescent growth and development, where\nboth patients and their parents often rely on internet resources. With the rising prominence of Large\nLanguage Models (LLMs), there is an anticipation that more patients and parents will start using LLM\nchatbots to seek information on adolescent growth and development. Nonetheless, the reliability of the\nanswers provided by LLM-Chatbots to these frequent questions about adolescent health is yet to be\ndetermined. [20]\nUnlike earlier healthcare chatbots that relied on extracting information from tailored datasets, LLMs such\nas ChatGPT are trained using self-supervised learning on diverse internet content. However, the internet's\nvast pool of data varies in accuracy, raising concerns about the LLMs' capacity to judge the\ntrustworthiness of their training materials. Additionally, LLMs may not have specialized expertise in\ncertain areas, leading to the production of persuasive but potentially incorrect answers, a phenomenon\noften termed as 'hallucinations'. Despite their rapid advancement, the effectiveness of LLMs in speci\u0000c\nmedical \u0000elds is yet to be fully explored. There is also a need for continuous updating of their sub-\nspecialty knowledge and enhancement of their self-correction capabilities. [21]\nPage 5/19\nThe focus of our study was to evaluate and compare the performance of three widely recognized LLMs -\nOpenAI's ChatGPT-3.5, Anthropic's Claude2, and Google's Bard - in addressing questions about\nadolescent growth and development. We conducted a thorough assessment of the accuracy and\nthoroughness of the responses from each LLM-Chatbot. The \u0000ndings from our research could offer\nimportant insights into the advantages and drawbacks of utilizing LLM-Chatbots for providing\ninformation on common adolescent health and development issues.\nMethods\nEthics\nGiven that no patients were involved in our study, approval from an ethics committee was not required.\nStudy design\nOur study was conducted in the Pediatric Health Care Department of Huaibei People's Hospital from\nOctober 22, 2023, to November 29, 2023.\nIn a collaborative effort, pediatrician Ying Li and data science student Zichen Song carefully compiled\n100 pertinent questions on adolescent growth and development. Their initial source was Hao Daifu, the\nlargest online medical consultation platform in China, known for its reliable health information. The team\nthen re\u0000ned these questions to include those most commonly asked by patients and parents in clinical\ncontexts. To thoroughly evaluate the capabilities of LLM-Chatbots across a spectrum of topics, the\nquestions were divided into 10 categories of disorders commonly associated with adolescent\ndevelopment stages: Attention De\u0000cit Disorder, Tic Disorder, Developmental Delay, Autism, Anxiety\nDisorder, Anorexia Nervosa, Obsessive-Compulsive Disorder, Sleep Disorders, Precocious Puberty, and\nDepression. The responses were elicited using ChatGPT-3.5 by OpenAI, Claude2 by Anthropic, and Bard\nby Google, during the period from October 22 to November 29, 2023. While ChatGPT-3.5 and Google Bard\nare available to the public at no cost, accessing Claude2 for extended text generation necessitates a\nsubscription. With more parameters and computational capacity than ChatGPT-3.5, Claude2 is presumed\nto handle complex queries more effectively. This study included ChatGPT-3.5 and Claude2 to test this\nassumption. [22–28]\nThe methodology of the study is depicted in Fig. 1. Initially, a basic prompt was used to establish\ncontext: 'I have some questions about adolescent growth and development.' Subsequently, the selected\n100 questions were independently inputted into each chatbot. To eliminate any memory retention bias,\nthe dialogue was reset after each question. The responses were anonymized and formatted uniformly in\nplain text to prevent raters from identifying the chatbot source. These responses were then randomized\nand presented to pediatricians for evaluation. The evaluation process was structured in three stages,\nconducted on separate dates with a 48-hour gap between each to minimize any potential carryover\neffects.\nPage 6/19\nAccuracy evaluation\nIn our research, the role of the evaluator was \u0000lled by Ying Li, a pediatrician with over two decades of\nclinical experience. To ensure impartiality, the identity of each LLM-Chatbot was kept hidden from the\nevaluator. Ying Li's main responsibility was to independently judge the accuracy of the responses\nprovided by the LLM-Chatbots. The evaluation criteria were as follows: a score of 1 ('Poor') was assigned\nto responses with serious inaccuracies that could mislead patients and pose a risk; a score of 2\n('Borderline') for responses with potential factual errors but no signi\u0000cant risk of misleading or harming\npatients; a score of 3 ('Good') for responses that were factually correct but lacked thoroughness (such\nas overlooking rare conditions); and a score of 4 ('Very Good') for responses that were not only free from\nerrors but also comprehensive in their advice.\nComprehensiveness evaluation\nFor chatbot responses that received a 'Good' rating, the rater conducted an additional assessment to\nevaluate the comprehensiveness of these replies. In this evaluation, we meticulously examined medical\nethical standards, medication guidelines, and diagnostic protocols, and each aspect was rated on a\nthree-point scale: 1) No consideration of relevant standards, with suggestions involving prescription or\neven prohibited drugs that could cause harm; 2) Some consideration of relevant standards, but only with\ngeneral advice to cautiously consider the suggestions without substantive normative guidance; 3)\nSuggestions in compliance with standards, and cautious consideration of the advice provided.\nRe-evaluation of accuracy for self-corrected, revised responses from LLM-chatbots\nIn instances where the LLM-Chatbots' responses were initially rated as 'Poor', a secondary process of\nself-correction was initiated. The LLM-Chatbots were prompted to re\u0000ne their answers using the prompt:\n'This doesn’t seem quite right. Can you provide a better response?' These modi\u0000ed responses underwent\nanother round of evaluation for accuracy by the same expert, one week following the \u0000rst assessment.\nDuring this subsequent evaluation, the expert was unaware that these were amended versions of the\ninitially poorly rated responses.\nFurthermore, recognizing that both Claude2 and ChatGPT-3.5 operate without internet connectivity and\nare informed by knowledge that may be a few years outdated, we incorporated up-to-date medical\nreferences for web-based corrections. We used the latest editions of authoritative medical texts like the\nMerck Manual Professional Edition and the most current case studies relevant to the scenario, sourced\nvia Bing AI search. An example of such a correction might be: 'This doesn’t seem quite right. Here is the\nlatest from Merck on diagnosing Obsessive-Compulsive Personality Disorder...', followed by detailed\ncriteria. These web-informed responses were also re-assessed for accuracy by the expert, employing the\nsame evaluation criteria as used for the initial self-correction process. [29–36]\nRe-evaluation of accuracy for self-corrected, revised responses from LLM-chatbots\nDetailed qualitative analysis of poorly-rated LLMchatbot responses\nPage 7/19\nTo further elucidate the potential limitations and risks of relying solely on LLM-Chatbot responses for\ninformation on adolescent growth and development, we undertook a more detailed analysis. The top 10\npoorly rated responses from the LLM-Chatbots underwent further scrutiny. Experts meticulously\nidenti\u0000ed and highlighted erroneous or inaccurate sentences within these responses, while also\nproviding explanations for the incorrect portions. Concurrently, we observed that in instances where\npatients provided insu\u0000cient or missing key information, human medical experts often suggested\nfurther investigations as a safety measure. However, LLM-Chatbots sometimes directly proposed\nmedication regimens, a practice that can be dangerously misleading. Therefore, the medical risks\nassociated with LLM-Chatbot advice warrant serious consideration.\nStatistical analysis\nStatistical analyses were performed using Python. ANOVA and Tukey’s HSD tests were used to compare\ncharacter count differences in responses among the three LLM-Chatbots and human doctors, as the\ndata met parametric criteria. Kruskal Wallis and Dunn's post-hoc tests assessed differences in word\ncounts and comprehensiveness scores between the LLMs and human doctors. Pearson’s χ 2 test was\napplied to compare rating proportions ('Poor', 'Borderline', 'Good', 'Very Good') within the LLM chatbots.\nBonferroni correction was applied for multiple hypothesis testing, with p-values < 0.05 deemed\nsigni\u0000cant.\nResults\nMean Score and Standard Deviation Analysis: The data in Table 1 not only re\u0000ects the overall\nperformance of different models on adolescent growth and development-related questions but also\nreveals their differences in data consistency. Claude2 leads with a high mean score of 3.54 and a low\nstandard deviation of just 0.501, demonstrating its high-quality and consistent responses. Such\nconsistency is particularly critical in medical decision-making as it reduces variability in answers and\nincreases reliability. Close behind, ChatGPT-4 also exhibits a similar trend, with a mean score of 3.44 and\na standard deviation of 0.519, indicating that its responses are both accurate and consistent. In contrast,\nhuman raters and Bard performed relatively poorly, not only with lower mean scores but also with larger\nstandard deviations, suggesting de\u0000ciencies in the accuracy and consistency of their responses to\nquestions. Particularly, the larger standard deviations indicate variability in responses across different\nscenarios, which could lead to uncertainty in medical decision-making.\nAnalysis of Variance (ANOVA) Test: ANOVA is a crucial tool for assessing the signi\u0000cance of mean\ndifferences between different groups. The results of the ANOVA test in Table 1, with an F-value of 64.692\nand an extremely low P-value of 4.6e-34, provide robust statistical evidence supporting the notion that\nthere are signi\u0000cant performance differences among various models in addressing adolescent growth\nand development issues. This signi\u0000cance implies that the observed performance disparities are unlikely\nto be due to chance and are rather attributable to differences inherent in the models themselves. This\n\u0000nding is vital for understanding the e\u0000cacy of different AI models in the domain of healthcare. [37]\nPage 8/19\n Table 1: Overview of response length from LLM-Chatbots to myopia care-related questions.\nLLM Response length (words) Response length (characters)\nMean (SD) MinimumMaximumMean (SD) MinimumMaximum\nHuman 196.94(22.59)98 228 935.00 (301.95)412 1458\nCluade2 631.29(8.72)615 648 2506.50 (110.56)2315 2698\nGPT3.5 737.09(8.70)720 752 3215.00 (119.51)3008 3422\nBard 534.93(13.41)486 577 2062.50 (37.24)1998 2127\nTable 2: Performance Comparison of Claude2 and ChatGPT-3.5 in Addressing Various Adolescent Health\nDisorders\nDisease Claude2, (%) ChatGPT-3.5, (%)\nCORE-1 CORE-2 CORE-3 CORE-4 CORE-1 CORE-2 CORE-3 CORE-4\nDevelopmentalDelay 0.0 0.0 40.0 60.0 0.0 0.0 40.0 60.0\nAutism 0.0 0.0 50.0 50.0 0.0 0.0 50.0 50.0\nObsessive-Compulsive Disorder(OCD)\n0.0 0.0 50.0 50.0 0.0 0.0 50.0 50.0\nPrecocious Puberty 0.0 0.0 40.0 60.0 0.0 0.0 50.0 50.0\nDepression 0.0 0.0 20.0 80.0 0.0 0.0 20.0 80.0\nTic Disorder 0.0 0.0 40.0 60.0 0.0 0.0 70.0 30.0\nAttention De\u0000citDisorder (ADD) 0.0 0.0 70.0 30.0 0.0 0.0 80.0 20.0\nAnxiety Disorder 0.0 0.0 40.0 60.0 0.0 0.0 60.0 40.0\nSleep Disorder 0.0 0.0 70.0 30.0 0.0 10.0 70.0 20.0\nAnorexia Nervosa 0.0 0.0 40.0 60.0 0.0 0.0 50.0 50.0\nFrequency Table Analysis and Pearson Chi-Square Test: Table 2 and 3's frequency table analysis\nhighlights Cluade2 and ChatGPT-4's superior performance. De\u0000ning 'Good' as scores  ≥  3, Cluade2 and\nChatGPT-4 signi\u0000cantly outperformed humans and Bard, with 100 and 99 'Good' responses,\nrespectively. The results of the Pearson Chi-Square test ( χ ² value of 117.758 and P-value of 2.35e-25) not\nonly con\u0000rm this \u0000nding but also reveal signi\u0000cant differences in problem-solving capabilities between\nPage 9/19\nthe models. The Chi-Square test results particularly highlight the accuracy advantage of Cluade2 and\nChatGPT-4, which is of great signi\u0000cance for the development and application of medical decision-\nsupport systems. [38]\nTable 3: Comparative Analysis of Human Raters and Bard in Assessing Adolescent Health Disorders\nDisease Human, (%) Bard, (%)\nCORE-1 CORE-2 CORE-3 CORE-4 CORE-1 CORE-2 CORE-3 CORE-4\nDevelopmentalDelay 10.0 40.0 30.0 20.0 10.0 30.0 60.0 0.0\nAutism 20.0 30.0 10.0 40.0 0.0 50.0 50.0 0.0\nObsessive-Compulsive Disorder(OCD)\n10.0 20.0 50.0 20.0 0.0 50.0 50.0 0.0\nPrecocious Puberty 10.0 30.0 40.0 20.0 10.0 30.0 60.0 0.0\nDepression 0.0 0.0 50.0 50.0 0.0 20.0 80.0 0.0\nTic Disorder 30.0 30.0 30.0 10.0 10.0 50.0 40.0 0.0\nAttention De\u0000citDisorder (ADD) 10.0 70.0 20.0 0.0 10.0 30.0 60.0 0.0\nAnxiety Disorder 0.0 60.0 30.0 10.0 0.0 40.0 60.0 0.0\nSleep Disorder 40.0 20.0 20.0 20.0 0.0 70.0 30.0 0.0\nAnorexia Nervosa 20.0 20.0 30.0 30.0 10.0 40.0 50.0 0.0\nDisease-Speci\u0000c Analysis: Table 2 shows that Claude2 and ChatGPT-4 outperformed human raters and\nBard in managing diseases like Attention De\u0000cit Disorder, Tic Disorder, and Developmental Delay. They\nscored higher with lower standard deviations, indicating consistent performance. Such consistency is\nvital in medical practice for accurate, reliable disease assessments and trustworthy medical advice.\nTable 4: Distribution of Top Challenging Questions Across Different Disease Types\nPage 10/19\nTOP10 Bad A B C D E F G H I J\nDevelopmental Delay - - - - - - - - √ -\nAutism - √ - - - - - - - -\nObsessive-Compulsive Disorder (OCD)- - √ - - - - - - -\nPrecocious Puberty √ - - - - - - - - -\nDepression - - - - - - - - - -\nTic Disorder √ - - - √ - - - - -\nAttention De\u0000cit Disorder (ADD) - - - - - √ - - - -\nAnxiety Disorder - - - - - - - - - -\nSleep Disorder - - √ - √ - - - - -\nAnorexia Nervosa √ - - - - - - - - -\nDiscussion\nIn our investigation, we conducted an in-depth analysis of ChatGPT-3.5, Claude2, and Google Bard,\nfocusing on how they handle typical inquiries about adolescent growth and development posed by\npatients and their parents. Our study employed a solid research methodology, including effective\nblinding, randomization, and thorough evaluation by expert pediatricians, to ensure the accuracy and\nreliability of our \u0000ndings. The results indicate that LLM chatbots, especially Claude2, demonstrate\npro\u0000ciency in delivering precise and detailed answers to questions concerning adolescent growth and\ndevelopment. Furthermore, our research sheds light on the ability of LLMs to improve their responses'\naccuracy through self-correction when prompted, a feature not extensively explored in previous studies.\nIt's important to note, though, that these LLMs exhibited limitations in managing more intricate aspects\nof adolescent growth and development, areas that even human medical professionals \u0000nd challenging.\nOur study is pioneering in its examination of the application of LLMs in this speci\u0000c area of medicine,\nmoving beyond the conventional focus of prior research on standardized medical exams. We delve into\nthe practical application of LLM-Chatbots as a resource for parents seeking guidance, emphasizing the\nneed to evaluate the precision and effectiveness of their responses in such real-life contexts. The\ninsights gained from this study are signi\u0000cant and could lay the groundwork for incorporating LLM-\nChatbots into the management of adolescent health and development.\nPage 11/19\nTable 5\nComparison of Correction Performance between Simple Inquiry-Based Self-Correction and Web-Based Medical Self-Correction\nSimple Inquiry-Based Self-Correction Web-Based Medical Self-Correction\nCluade2 Chatgpt4 Bard Cluade2 Chatgpt4 Bard\n3 3 2 4 4 4\n4 3 2 4 4 3\n4 3 2 4 3 4\n3 3 2 4 3 3\n3 3 3 3 4 4\n4 3 3 4 3 4\n3 3 3 3 4 3\n3 3 3 4 3 4\n3 3 2 4 4 4\n3 3 2 4 3 4\nIn our evaluation of three LLM chatbots, Claude2 emerged as the most pro\u0000cient in addressing queries\nrelated to adolescent growth and development. It achieved the highest average accuracy and garnered a\ngreater proportion of 'Good' ratings compared to the other two LLMs, as evident in Figs. 2 and 3, and\nconsistently across all 10 common diseases as shown in Table 2. Our results highlight the relative\nadvantage of Claude2 in neuro-adolescent growth and developmental assessments compared to other\nLLMs. This exceptional performance may be attributed to several unique factors of Claude2, such as its\nextensive parameter set, continuous feedback from a vast user base and collaborating experts informing\nits training, advanced reasoning and directive tracking abilities, recent training data, and integration of\ninsights from actual applications of these prior models into Claude2's safety research and monitoring\nsystems, all contributing to more accurate responses. Interestingly, all three LLM chatbots were equally\ncapable in providing comprehensive responses. The top 10 'Poor' questions illustrate that all three\nchatbots gave detailed answers to complex questions that challenge human doctors, further\ndemonstrating the capability of LLM-Chatbots to provide relevant and detailed information.\nIn our assessment of 10 prevalent adolescent diseases, all LLM chatbots showcased commendable\nperformance in answering questions about disease progression, particularly excelling in areas of 'Anxiety\nDisorders' and 'Depression' as detailed in Table 4. Yet, a signi\u0000cant variation in performance was\nobserved in 'Sleep Disorders' and 'Tic Disorders', as indicated in Table 2. This could be a re\u0000ection of the\ndynamic nature of diagnostics in adolescent growth and development and the constraints of LLM-\nChatbots’ training datasets, which may not be entirely updated with the latest medical developments,\naffecting their response accuracy in these areas.(From Table 5)\nPage 12/19\nOur \u0000ndings highlight the valuable contribution of LLM-Chatbots in the realm of clinical information\ndissemination. With the increasing global adoption of LLM-Chatbots, they stand to be pivotal in\nspreading medical information. The advancement of GPT4, now offering API capabilities, underscores\nthis potential. This API access allows for the integration of ChatGPT’s NLP features into various digital\nservices, paving the way for the development of adolescent-focused chatbots using GPT4's\nsophisticated architecture. The enhanced reach and availability of LLM-Chatbots in addressing\nadolescent health issues could be instrumental in addressing the rising concerns in this area. However,\ntheir current limitations in identifying and preventing misinformation necessitate cautious application,\nawaiting further development of their analytical capabilities.\nThe applicability of LLMs may also extend to various medical specializations, though their strengths and\nshortcomings could vary. For instance, Rasmussen et al. (2023) found ChatGPT-3.5 to be less effective\nin addressing treatment and prevention queries for Spring Catarrh [39], while Lahat et al. (2023) noted its\nsubpar performance in gastrointestinal diagnostic queries [40]. These differences in LLM performance\nmay stem from the varying depths of available internet data on each topic. Given that ChatGPT's training\nwas based on internet data up until September 2021, its pro\u0000ciency mirrors the knowledge and biases in\nthat dataset. Nevertheless, the evolution of LLMs is swift, as Johnson et al. (2023) showed with a\nsigni\u0000cant accuracy improvement in cancer information within just two weeks [41]. This underscores the\nneed for ongoing evaluation of LLMs across various medical \u0000elds.\nOur study's robust methodology, including anonymizing LLM-speci\u0000c answer features, randomizing\nresponses, and introducing intervals between rating sessions, enhanced the credibility of our\nconclusions. Nonetheless, there are limitations, such as the rater's subjectivity, which we attempted to\nmitigate by employing an experienced pediatrician and a multi-round rating approach. The limited\nnumber of questions per disease category also necessitates caution in generalizing LLM-Chatbots’\nperformance. Furthermore, as LLMs are continually updated, these \u0000ndings should be contextualized\nwithin their speci\u0000c timeframe, acknowledging that future studies may yield different results.\nIn conclusion, our study reveals Claude2's superior performance over ChatGPT-3.5 and Google Bard in\naddressing adolescent health queries. This comparative analysis sheds light on the accuracy of various\nLLM-Chatbots and underscores Claude2 and other advanced LLMs' potential in providing precise and\nthorough information for adolescent health care. Ongoing research and evaluation will be key in re\u0000ning\nand validating these tools for future applications.\nDeclarations\nContributors\nContributors: Ying Li, Zichen Song, and Weijia Li contributed to the conception of this study. Ying Li and\nZichen Song contributed to the study design. Ying Li and Zichen Song were involved in the collection of\ndata. Zichen Song contributed to the statistical analysis of the data. Ying Li and Zichen Song aided in the\nanalysis and interpretation of the data. Ying Li and Zichen Song accessed and veri\u0000ed the data sets\nPage 13/19\nthroughout the research process. Oversight of the study, including responsibility for the planning and\nexecution of research activities, was supervised by Ying Li. Zichen Song contributed to the visualization,\nincluding the creation of graphics, charts, and tables for the data. All authors had access to all the data,\nand Ying Li and Zichen Song were responsible for the decision to submit the manuscript for publication.\nYing Li and Zichen Song drafted the manuscript. All authors read and approved the \u0000nal version of the\nmanuscript.\nData sharing statement\nWe have ensured that all the basic data required to replicate the results are included in our\nsupplementary \u0000les. The only exception is the raw scores assigned by the raters, available on request.\nDeclaration of generative AI and AI-assisted technologies in the writing Process\nIn preparation for this work, the authors used ChatGPT to edit and proofread the manuscript to improve\nreadability. After using this tool / service, the authors review and edit the content as needed and take full\nresponsibility for the content of the publication.\nDeclaration of interests\nAll of the authors declare no competing interests.\nNote\nWe have ensured that all the basic data required to replicate the results are included in our\nsupplementary \u0000les. The only exception is the raw scores assigned by the raters, available on request\nfrom corresponding author.\nReferences\n1. Benchmarking large language models’ performances for myopia care: a comparative analysis of\nChatGPT-3.5, ChatGPT-4.0, and Google Bard Lim, Zhi Wei et al. eBioMedicine, Volume 95, 104770\n2. Xu L, Sanders L, Li K, Chow JCL Chatbot for Health Care and Oncology Applications Using Arti\u0000cial\nIntelligence and Machine Learning: Systematic Review JMIR Cancer 2021;7(4):e27850\n3. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. (2023) Performance of\nChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOS\nDigit Health 2(2): e0000198. https://doi.org/10.1371/journal.pdig.0000198\n4. Ali, Rohaid, et al. \"Performance of ChatGPT, GPT-4, and Google bard on a neurosurgery oral boards\npreparation question bank.\" Neurosurgery (2022): 10-1227.\n5. De Onis, Mercedes. \"Child growth and development.\" Nutrition and health in a developing world\n(2017): 119–141.\n\u0000. Onis, M. de. \"The WHO child growth standards.\" Pediatric nutrition in practice (2008): 254–269.\nPage 14/19\n7. De Onis, Mercedes, Trudy MA Wijnhoven, and Adelheid W. Onyango. \"Worldwide practices in child\ngrowth monitoring.\" The Journal of pediatrics 144.4 (2004): 461–465.\n\u0000. Hoddinott, John, and Bill Kinsey. \"Child growth in the time of drought.\" Oxford Bulletin of Economics\nand statistics 63.4 (2001): 409–436.\n9. Swati Singh, Ali Djalilian & Mohammad Javed Ali (2023) ChatGPT and Ophthalmology: Exploring Its\nPotential with Discharge Summaries and Operative Notes, Seminars in Ophthalmology, 38:5, 503–\n507, DOI: 10.1080/08820538.2023.2209166\n10. Chow JCL, Sanders L and Li K (2023) Impact of ChatGPT on medical chatbots as a disruptive\ntechnology. Front. Artif. Intell. 6:1166014. doi: 10.3389/frai.2023.1166014\n11. Mihalache A, Popovic MM, Muni RH. Performance of an Arti\u0000cial Intelligence Chatbot in Ophthalmic\nKnowledge Assessment. JAMA Ophthalmol. 2023;141(6):589–597.\ndoi:10.1001/jamaophthalmol.2023.1144\n12. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large language models in\nmedicine and medical research. Lancet Digit Health. 2023;5(6):e333-e335. doi: 10.1016/S2589-\n7500(23)00083-3. Epub 2023 Apr 27. PMID: 37120418.\n13. Bushuven S, Bentele M, Bentele S, Gerber B, Bansbach J, Ganter J, Trifunovic-Koenig M, Ranisch R.\n\"ChatGPT, Can You Help Me Save My Child's Life?\" - Diagnostic Accuracy and Supportive Capabilities\nto Lay Rescuers by ChatGPT in Prehospital Basic Life Support and Paediatric Advanced Life Support\nCases - An In-silico Analysis. J Med Syst. 2023;47(1):123. doi: 10.1007/s10916-023-02019-x. PMID:\n37987870; PMCID: PMC10663183.\n14. Clusmann J., Kolbinger F.R., Muti H.S., Carrero Z.I., Eckardt J.N., Laleh N.G., et al. The future\nlandscape of large language models in medicine. Commun Med (Lond) 2023;3:141.\n15. Harrer S. Attention is not all you need: the complicated case of ethically using large language\nmodels in healthcare and medicine. EBioMedicine. 2023;90\n1\u0000. Johnson, Douglas, et al. \"Assessing the accuracy and reliability of AI-generated medical responses:\nan evaluation of the Chat-GPT model.\" Research square (2023).\n17. Lahat, A., Shachar, E., Avidan, B. et al. Evaluating the use of large language model in identifying top\nresearch questions in gastroenterology. Sci Rep 13, 4164 (2023). https://doi.org/10.1038/s41598-\n023-31412-2\n1\u0000. Holmes, Jason, et al. \"Evaluating multiple large language models in pediatric ophthalmology.\" arXiv\npreprint arXiv:2311.04368 (2023).\n19. Hoelzer, Dieter. \"Update on burkitt lymphoma and leukemia.\" Clin Adv Hematol Oncol 7.11 (2009):\n728–729.\n20. China Internet Network Information Center (CNNIC). The 48th China Internet Development Statistics\nReport [R/OL].(2021-08)[2021-12-23].\n21. Gou, Zhibin, et al. \"Critic: Large language models can self-correct with tool-interactive critiquing.\"\narXiv preprint arXiv:2305.11738 (2023).\nPage 15/19\n22. Uauy, Ricardo, et al. \"Nutrition, child growth, and chronic disease prevention.\" Annals of medicine\n40.1 (2008): 11–20.\n23. Larson, Leila Margaret, et al. \"Effects of increased hemoglobin on child growth, development, and\ndisease: a systematic review and meta-analysis.\" Annals of the New York Academy of Sciences\n1450.1 (2019): 83–104.\n24. Barker, David James Purslove. \"The developmental origins of adult disease.\" Journal of the\nAmerican College of Nutrition 23.sup6 (2004): 588S-595S.\n25. Osmond, Clive, and D. J. Barker. \"Fetal, infant, and childhood growth are predictors of coronary heart\ndisease, diabetes, and hypertension in adult men and women.\" Environmental health perspectives\n108.suppl 3 (2000): 545–553.\n2\u0000. Quante, Mirja, et al. \"The LIFE child study: a life course approach to disease and health.\" BMC public\nhealth 12 (2012): 1–14.\n27. Wells, J. C. K. \"Body composition in childhood: effects of normal growth and disease.\" Proceedings\nof the Nutrition Society 62.2 (2003): 521–528.\n2\u0000. Bundy, Donald AP, et al., eds. \"Disease control priorities, (volume 8): child and adolescent health and\ndevelopment.\" (2017).\n29. Huang, Jiaxin, et al. \"Large language models can self-improve.\" arXiv preprint arXiv:2210.11610\n(2022).\n30. Wang, Ziqi, et al. \"Enable Language Models to Implicitly Learn Self-Improvement From Data.\" arXiv\npreprint arXiv:2310.00898 (2023).\n31. Huang, Jie, and Kevin Chen-Chuan Chang. \"Towards reasoning in large language models: A survey.\"\narXiv preprint arXiv:2212.10403 (2022).\n32. Pan, Liangming, et al. \"Automatically correcting large language models: Surveying the landscape of\ndiverse self-correction strategies.\" arXiv preprint arXiv:2308.03188 (2023).\n33. Peng, Baolin, et al. \"Check your facts and try again: Improving large language models with external\nknowledge and automated feedback.\" arXiv preprint arXiv:2302.12813 (2023).\n34. Yu, Xiao, et al. \"Teaching Language Models to Self-Improve through Interactive Demonstrations.\"\narXiv preprint arXiv:2310.13522 (2023).\n35. Fu, Yao, et al. \"Improving language model negotiation with self-play and in-context learning from ai\nfeedback.\" arXiv preprint arXiv:2305.10142 (2023).\n3\u0000. Wang, Yizhong, et al. \"Self-instruct: Aligning language model with self generated instructions.\" arXiv\npreprint arXiv:2212.10560 (2022).\n37. St, Lars, and Svante Wold. \"Analysis of variance (ANOVA).\" Chemometrics and intelligent laboratory\nsystems 6.4 (1989): 259–272.\n3\u0000. Cohen, Israel, et al. \"Pearson correlation coe\u0000cient.\" Noise reduction in speech processing (2009):\n1–4.\nPage 16/19\n39. Rasmussen, Marie Louise Roed, et al. \"Arti\u0000cial intelligence-based ChatGPT chatbot responses for\npatient and parent questions on vernal keratoconjunctivitis.\" Graefe's Archive for Clinical and\nExperimental Ophthalmology (2023): 1–3.\n40. Lahat, Adi, et al. \"Evaluating the Utility of a Large Language Model in Answering Common Patients’\nGastrointestinal Health-Related Questions: Are We There Yet?.\" Diagnostics 13.11 (2023): 1950.\n41. Johnson, Douglas, et al. \"Assessing the accuracy and reliability of AI-generated medical responses:\nan evaluation of the Chat-GPT model.\" Research square (2023).\nFigures\nPage 17/19\nFigure 1\nThe Flowchart of all study design.\nPage 18/19\nFigure 2\nComparative Analysis of Ratings for Humans and Arti\u0000cial Intelligence Systems: Mean, Standard\nDeviation, and Distribution of Scores\nPage 19/19\nFigure 3\nDisplay of Cluade2's response performance for different diseases",
  "topic": "Context (archaeology)",
  "concepts": [
    {
      "name": "Context (archaeology)",
      "score": 0.6304216384887695
    },
    {
      "name": "Autism spectrum disorder",
      "score": 0.5750278234481812
    },
    {
      "name": "Psychology",
      "score": 0.5225253105163574
    },
    {
      "name": "Multidisciplinary approach",
      "score": 0.4811071753501892
    },
    {
      "name": "Benchmarking",
      "score": 0.47589409351348877
    },
    {
      "name": "Autism",
      "score": 0.4728018641471863
    },
    {
      "name": "Anxiety",
      "score": 0.4531569182872772
    },
    {
      "name": "Developmental psychology",
      "score": 0.39885544776916504
    },
    {
      "name": "Clinical psychology",
      "score": 0.3566584587097168
    },
    {
      "name": "Psychiatry",
      "score": 0.2928893268108368
    },
    {
      "name": "Political science",
      "score": 0.15927091240882874
    },
    {
      "name": "Geography",
      "score": 0.12146678566932678
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}