{
  "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
  "url": "https://openalex.org/W3217112505",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2127850563",
      "name": "Cheng Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221790083",
      "name": "Misra, Ishan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225672502",
      "name": "Schwing, Alexander G.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799384312",
      "name": "Kirillov, Alexander",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3123360993",
      "name": "Girdhar, Rohit",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3201717975",
    "https://openalex.org/W3196107618",
    "https://openalex.org/W2088049833",
    "https://openalex.org/W3106546328",
    "https://openalex.org/W3204057394",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3177269789",
    "https://openalex.org/W3211671711",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W3047032303",
    "https://openalex.org/W3202163745",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3203892946",
    "https://openalex.org/W2954469458",
    "https://openalex.org/W3176659256",
    "https://openalex.org/W3194403109",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2969825080",
    "https://openalex.org/W2781228439",
    "https://openalex.org/W3165745140",
    "https://openalex.org/W3213165621",
    "https://openalex.org/W1991367009",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3034355852",
    "https://openalex.org/W2912662889",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W3035358681",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2953139137",
    "https://openalex.org/W3149484991",
    "https://openalex.org/W3028392891",
    "https://openalex.org/W3107443479",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2037227137",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2558156561"
  ],
  "abstract": "Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).",
  "full_text": "Masked-attention Mask Transformer for Universal Image Segmentation\nBowen Cheng1,2* Ishan Misra1 Alexander G. Schwing2 Alexander Kirillov1 Rohit Girdhar1\n1Facebook AI Research (FAIR) 2University of Illinois at Urbana-Champaign (UIUC)\nhttps://bowenc0221.github.io/mask2former\nAbstract\nImage segmentation groups pixels with different seman-\ntics, e.g., category or instance membership. Each choice\nof semantics deÔ¨Ånes a task. While only the semantics of\neach task differ, current research focuses on designing spe-\ncialized architectures for each task. We present Masked-\nattention Mask Transformer (Mask2Former), a new archi-\ntecture capable of addressing any image segmentation task\n(panoptic, instance or semantic). Its key components in-\nclude masked attention, which extracts localized features by\nconstraining cross-attention within predicted mask regions.\nIn addition to reducing the research effort by at least three\ntimes, it outperforms the best specialized architectures by\na signiÔ¨Åcant margin on four popular datasets. Most no-\ntably, Mask2Former sets a new state-of-the-art for panoptic\nsegmentation (57.8 PQ on COCO), instance segmentation\n(50.1 AP on COCO) and semantic segmentation (57.7 mIoU\non ADE20K).\n1. Introduction\nImage segmentation studies the problem of grouping\npixels. Different semantics for grouping pixels, e.g., cat-\negory or instance membership, have led to different types\nof segmentation tasks, such as panoptic, instance or seman-\ntic segmentation. While these tasks differ only in semantics,\ncurrent methods develop specialized architectures for each\ntask. Per-pixel classiÔ¨Åcation architectures based on Fully\nConvolutional Networks (FCNs) [37] are used for semantic\nsegmentation, while mask classiÔ¨Åcation architectures [5,24]\nthat predict a set of binary masks each associated with a\nsingle category, dominate instance-level segmentation. Al-\nthough such specialized architectures [6, 10, 24, 37] have\nadvanced each individual task, they lack the Ô¨Çexibility to\ngeneralize to the other tasks. For example, FCN-based ar-\nchitectures struggle at instance segmentation, leading to the\nevolution of different architectures for instance segmenta-\ntion compared to semantic segmentation. Thus, duplicate\nresearch and (hardware) optimization effort is spent on each\n*Work done during an internship at Facebook AI Research.\npanoptic\n instance\n semantic\n57.8\n52.751.1\nUniversal architectures:\nMask2Former (ours) MaskFormer\nSOTA specialized architectures:\nMax-DeepLab Swin-HTC++ BEiT\n50.1\n40.1\n49.5 57.755.657.0\nFigure 1. State-of-the-art segmentation architectures are typically\nspecialized for each image segmentation task. Although recent\nwork has proposed universal architectures that attempt all tasks\nand are competitive on semantic and panoptic segmentation, they\nstruggle with segmenting instances. We propose Mask2Former,\nwhich, for the Ô¨Årst time, outperforms the best specialized architec-\ntures on three studied segmentation tasks on multiple datasets.\nspecialized architecture for every task.\nTo address this fragmentation, recent work [14, 62] has\nattempted to designuniversal architectures, that are capable\nof addressing all segmentation tasks with the same archi-\ntecture (i.e., universal image segmentation). These archi-\ntectures are typically based on an end-to-end set prediction\nobjective (e.g., DETR [5]), and successfully tackle multiple\ntasks without modifying the architecture, loss, or the train-\ning procedure. Note, universal architectures are still trained\nseparately for different tasks and datasets, albeit having the\nsame architecture. In addition to being Ô¨Çexible, universal\narchitectures have recently shown state-of-the-art results on\nsemantic and panoptic segmentation [14]. However, re-\ncent work still focuses on advancing specialized architec-\ntures [20, 39, 45], which raises the question: why haven‚Äôt\nuniversal architectures replaced specialized ones?\nAlthough existing universal architectures are Ô¨Çexible\nenough to tackle any segmentation task, as shown in Fig-\nure 1, in practice their performance lags behind the best\nspecialized architectures. For instance, the best reported\n1\narXiv:2112.01527v3  [cs.CV]  15 Jun 2022\nperformance of universal architectures [14, 62], is currently\nlower ( > 9 AP) than the SOTA specialized architecture\nfor instance segmentation [6]. Beyond the inferior per-\nformance, universal architectures are also harder to train.\nThey typically require more advanced hardware and a much\nlonger training schedule. For example, training Mask-\nFormer [14] takes 300 epochs to reach 40.1 AP and it can\nonly Ô¨Åt a single image in a GPU with 32G memory. In con-\ntrast, the specialized Swin-HTC++ [6] obtains better perfor-\nmance in only 72 epochs. Both the performance and train-\ning efÔ¨Åciency issues hamper the deployment of universal\narchitectures.\nIn this work, we propose a universal image segmen-\ntation architecture named Masked-attention Mask Trans-\nformer ( Mask2Former) that outperforms specialized ar-\nchitectures across different segmentation tasks, while still\nbeing easy to train on every task. We build upon a sim-\nple meta architecture [14] consisting of a backbone fea-\nture extractor [25, 36], a pixel decoder [33] and a Trans-\nformer decoder [51]. We propose key improvements that\nenable better results and efÔ¨Åcient training. First, we use\nmasked attention in the Transformer decoder which restricts\nthe attention to localized features centered around predicted\nsegments, which can be either objects or regions depend-\ning on the speciÔ¨Åc semantic for grouping. Compared to\nthe cross-attention used in a standard Transformer decoder\nwhich attends to all locations in an image, our masked atten-\ntion leads to faster convergence and improved performance.\nSecond, we use multi-scale high-resolution features which\nhelp the model to segment small objects/regions. Third,\nwe propose optimization improvements such as switching\nthe order of self and cross-attention, making query features\nlearnable, and removing dropout; all of which improve per-\nformance without additional compute. Finally, we save 3√ó\ntraining memory without affecting the performance by cal-\nculating mask loss on few randomly sampled points. These\nimprovements not only boost the model performance, but\nalso make training signiÔ¨Åcantly easier, making universal ar-\nchitectures more accessible to users with limited compute.\nWe evaluate Mask2Former on three image segmenta-\ntion tasks (panoptic, instance and semantic segmentation)\nusing four popular datasets (COCO [35], Cityscapes [16],\nADE20K [65] and Mapillary Vistas [42]). For the Ô¨Årst\ntime, on all these benchmarks, our single architecture\nperforms on par or better than specialized architectures.\nMask2Former sets the new state-of-the-art of 57.8 PQ on\nCOCO panoptic segmentation [28], 50.1 AP on COCO in-\nstance segmentation [35] and 57.7 mIoU on ADE20K se-\nmantic segmentation [65] using the exact same architecture.\n2. Related Work\nSpecialized semantic segmentation architectures typi-\ncally treat the task as a per-pixel classiÔ¨Åcation problem.\nFCN-based architectures [37] independently predict a cat-\negory label for every pixel. Follow-up methods Ô¨Ånd con-\ntext to play an important role for precise per-pixel classi-\nÔ¨Åcation and focus on designing customized context mod-\nules [7,8,63] or self-attention variants [21,26,45,55,61,64].\nSpecialized instance segmentation architectures are typ-\nically based upon ‚Äúmask classiÔ¨Åcation.‚Äù They predict a set\nof binary masks each associated with a single class label.\nThe pioneering work, Mask R-CNN [24], generates masks\nfrom detected bounding boxes. Follow-up methods either\nfocus on detecting more precise bounding boxes [4, 6], or\nÔ¨Ånding new ways to generate a dynamic number of masks,\ne.g., using dynamic kernels [3, 49, 56] or clustering algo-\nrithms [11, 29]. Although the performance has been ad-\nvanced in each task, these specialized innovations lack the\nÔ¨Çexibility to generalize from one to the other, leading to\nduplicated research effort. For instance, although multiple\napproaches have been proposed for building feature pyra-\nmid representations [33], as we show in our experiments,\nBiFPN [47] performs better for instance segmentation while\nFaPN [39] performs better for semantic segmentation.\nPanoptic segmentation has been proposed to unify both se-\nmantic and instance segmentation tasks [28]. Architectures\nfor panoptic segmentation either combine the best of spe-\ncialized semantic and instance segmentation architectures\ninto a single framework [11, 27, 31, 60] or design novel ob-\njectives that equally treat semantic regions and instance ob-\njects [5, 52]. Despite those new architectures, researchers\ncontinue to develop specialized architectures for different\nimage segmentation tasks [20, 45]. We Ô¨Ånd panoptic archi-\ntectures usually only report performance on a single panop-\ntic segmentation task [52], which does not guarantee good\nperformance on other tasks (Figure 1). For example, panop-\ntic segmentation does not measure architectures‚Äô abilities to\nrank predictions as instance segmentations. Thus, we re-\nfrain from referring to architectures that are only evaluated\nfor panoptic segmentation as universal architectures. In-\nstead, here, we evaluate our Mask2Former on all studied\ntasks to guarantee generalizability.\nUniversal architectures have emerged with DETR [5] and\nshow that mask classiÔ¨Åcation architectures with an end-to-\nend set prediction objective are general enough for any im-\nage segmentation task. MaskFormer [14] shows that mask\nclassiÔ¨Åcation based on DETR not only performs well on\npanoptic segmentation but also achieves state-of-the-art on\nsemantic segmentation. K-Net [62] further extends set pre-\ndiction to instance segmentation. Unfortunately, these ar-\nchitectures fail to replace specialized models as their perfor-\nmance on particular tasks or datasets is still worse than the\nbest specialized architecture (e.g., MaskFormer [14] cannot\nsegment instances well). To our knowledge, Mask2Former\nis the Ô¨Årst architecture that outperforms state-of-the-art spe-\ncialized architectures on all considered tasks and datasets.\n2\n3. Masked-attention Mask Transformer\nWe now present Mask2Former. We Ô¨Årst review a meta\narchitecture for mask classiÔ¨Åcation that Mask2Former is\nbuilt upon. Then, we introduce our new Transformer de-\ncoder with masked attention which is the key to better con-\nvergence and results. Lastly, we propose training improve-\nments that make Mask2Former efÔ¨Åcient and accessible.\n3.1. Mask classiÔ¨Åcation preliminaries\nMask classiÔ¨Åcation architectures group pixels into N\nsegments by predicting N binary masks, along with N cor-\nresponding category labels. Mask classiÔ¨Åcation is sufÔ¨Å-\nciently general to address any segmentation task by assign-\ning different semantics, e.g., categories or instances, to dif-\nferent segments. However, the challenge is to Ô¨Ånd good\nrepresentations for each segment. For example, Mask R-\nCNN [24] uses bounding boxes as the representation which\nlimits its application to semantic segmentation. Inspired by\nDETR [5], each segment in an image can be represented as\na C-dimensional feature vector (‚Äúobject query‚Äù) and can be\nprocessed by a Transformer decoder, trained with a set pre-\ndiction objective. A simple meta architecture would con-\nsist of three components. A backbone that extracts low-\nresolution features from an image. A pixel decoder that\ngradually upsamples low-resolution features from the out-\nput of the backbone to generate high-resolution per-pixel\nembeddings. And Ô¨Ånally a Transformer decoder that oper-\nates on image features to process object queries. The Ô¨Ånal\nbinary mask predictions are decoded from per-pixel embed-\ndings with object queries. One successful instantiation of\nsuch a meta architecture is MaskFormer [14], and we refer\nreaders to [14] for more details.\n3.2. Transformer decoder with masked attention\nMask2Former adopts the aforementioned meta archi-\ntecture, with our proposed Transformer decoder (Figure 2\nright) replacing the standard one. The key components of\nour Transformer decoder include a masked attention opera-\ntor, which extracts localized features by constraining cross-\nattention to within the foreground region of the predicted\nmask for each query, instead of attending to the full fea-\nture map. To handle small objects, we propose an efÔ¨Åcient\nmulti-scale strategy to utilize high-resolution features. It\nfeeds successive feature maps from the pixel decoder‚Äôs fea-\nture pyramid into successive Transformer decoder layers in\na round robin fashion. Finally, we incorporate optimiza-\ntion improvements that boost model performance without\nintroducing additional computation. We now discuss these\nimprovements in detail.\nBackbonePixel Decoder\nTransformerDecoder\nùêø√ó maskclass\nmaskedattentionadd & normself-attentionadd & norm\nadd & normFFN\nVKQ\nVKQ\nmaskqueryfeaturesimagefeatures\nFigure 2. Mask2Former overview. Mask2Former adopts the\nsame meta architecture as MaskFormer [14] with a backbone, a\npixel decoder and a Transformer decoder. We propose a new\nTransformer decoder withmasked attention instead of the standard\ncross-attention (Section 3.2.1). To deal with small objects, we pro-\npose an efÔ¨Åcient way of utilizing high-resolution features from a\npixel decoder by feeding one scale of the multi-scale feature to one\nTransformer decoder layer at a time (Section 3.2.2). In addition,\nwe switch the order of self and cross-attention ( i.e., our masked\nattention), make query features learnable, and remove dropout to\nmake computation more effective (Section 3.2.3). Note that posi-\ntional embeddings and predictions from intermediate Transformer\ndecoder layers are omitted in this Ô¨Ågure for readability.\n3.2.1 Masked attention\nContext features have been shown to be important for im-\nage segmentation [7,8,63]. However, recent studies [22,46]\nsuggest that the slow convergence of Transformer-based\nmodels is due to global context in the cross-attention layer,\nas it takes many training epochs for cross-attention to learn\nto attend to localized object regions [46]. We hypothesize\nthat local features are enough to update query features and\ncontext information can be gathered through self-attention.\nFor this we propose masked attention, a variant of cross-\nattention that only attends within the foreground region of\nthe predicted mask for each query.\nStandard cross-attention (with residual path) computes\nXl = softmax(QlKT\nl)Vl + Xl‚àí1. (1)\nHere, l is the layer index, Xl ‚àà RN√óC refers to N\nC-dimensional query features at the lth layer and Ql =\nfQ(Xl‚àí1) ‚ààRN√óC. X0 denotes input query features to\nthe Transformer decoder. Kl,Vl ‚ààRHlWl√óC are the im-\nage features under transformation fK(¬∑) and fV(¬∑) respec-\ntively, and Hl and Wl are the spatial resolution of image\nfeatures that we will introduce next in Section 3.2.2. fQ,\nfK and fV are linear transformations.\n3\nOur masked attention modulates the attention matrix via\nXl = softmax(Ml‚àí1 + QlKT\nl)Vl + Xl‚àí1. (2)\nMoreover, the attention mask Ml‚àí1 at feature location\n(x,y) is\nMl‚àí1(x,y) =\n{ 0 if Ml‚àí1(x,y) = 1\n‚àí‚àû otherwise . (3)\nHere, Ml‚àí1 ‚àà {0,1}N√óHlWl is the binarized output\n(thresholded at 0.5) of the resized mask prediction of the\nprevious (l‚àí1)-th Transformer decoder layer. It is resized\nto the same resolution ofKl. M0 is the binary mask predic-\ntion obtained from X0, i.e., before feeding query features\ninto the Transformer decoder.\n3.2.2 High-resolution features\nHigh-resolution features improve model performance, espe-\ncially for small objects [5]. However, this is computation-\nally demanding. Thus, we propose an efÔ¨Åcient multi-scale\nstrategy to introduce high-resolution features while control-\nling the increase in computation. Instead of always using\nthe high-resolution feature map, we utilize a feature pyra-\nmid which consists of both low- and high-resolution fea-\ntures and feed one resolution of the multi-scale feature to\none Transformer decoder layer at a time.\nSpeciÔ¨Åcally, we use the feature pyramid produced by\nthe pixel decoder with resolution 1/32, 1/16 and 1/8 of\nthe original image. For each resolution, we add both a si-\nnusoidal positional embedding epos ‚àà RHlWl√óC, follow-\ning [5], and a learnable scale-level embeddingelvl ‚ààR1√óC,\nfollowing [66]. We use those, from lowest-resolution to\nhighest-resolution for the corresponding Transformer de-\ncoder layer as shown in Figure 2 left. We repeat this 3-layer\nTransformer decoder L times. Our Ô¨Ånal Transformer de-\ncoder hence has 3Llayers. More speciÔ¨Åcally, the Ô¨Årst three\nlayers receive a feature map of resolution H1 = H/32,\nH2 = H/16, H3 = H/8 and W1 = W/32, W2 = W/16,\nW3 = W/8, where H and W are the original image reso-\nlution. This pattern is repeated in a round robin fashion for\nall following layers.\n3.2.3 Optimization improvements\nA standard Transformer decoder layer [51] consists of three\nmodules to process query features in the following order: a\nself-attention module, a cross-attention and a feed-forward\nnetwork (FFN). Moreover, query features (X0) are zero ini-\ntialized before being fed into the Transformer decoder and\nare associated with learnable positional embeddings. Fur-\nthermore, dropout is applied to both residual connections\nand attention maps.\nTo optimize the Transformer decoder design, we make\nthe following three improvements. First, we switch the\norder of self- and cross-attention (our new ‚Äúmasked atten-\ntion‚Äù) to make computation more effective: query features\nto the Ô¨Årst self-attention layer are image-independent and\ndo not have signals from the image, thus applying self-\nattention is unlikely to enrich information. Second, we\nmake query features ( X0) learnable as well (we still keep\nthe learnable query positional embeddings), and learnable\nquery features are directly supervised before being used in\nthe Transformer decoder to predict masks ( M0). We Ô¨Ånd\nthese learnable query features function like a region pro-\nposal network [43] and have the ability to generate mask\nproposals. Finally, we Ô¨Ånd dropout is not necessary and\nusually decreases performance. We thus completely remove\ndropout in our decoder.\n3.3. Improving training efÔ¨Åciency\nOne limitation of training universal architectures is the\nlarge memory consumption due to high-resolution mask\nprediction, making them less accessible than the more\nmemory-friendly specialized architectures [6, 24]. For ex-\nample, MaskFormer [14] can only Ô¨Åt a single image in a\nGPU with 32G memory. Motivated by PointRend [30] and\nImplicit PointRend [13], which show a segmentation model\ncan be trained with its mask loss calculated on Krandomly\nsampled points instead of the whole mask, we calculate the\nmask loss with sampled points in both the matching and\nthe Ô¨Ånal loss calculation. More speciÔ¨Åcally, in the match-\ning loss that constructs the cost matrix for bipartite match-\ning, we uniformly sample the same set of K points for all\nprediction and ground truth masks. In the Ô¨Ånal loss be-\ntween predictions and their matched ground truths, we sam-\nple different sets of K points for different pairs of predic-\ntion and ground truth using importance sampling [30]. We\nset K = 12544, i.e., 112 √ó112 points. This new training\nstrategy effectively reduces training memory by 3√ó, from\n18GB to 6GB per image, making Mask2Former more ac-\ncessible to users with limited computational resources.\n4. Experiments\nWe demonstrate Mask2Former is an effective architec-\nture for universal image segmentation through compar-\nisons with specialized state-of-the-art architectures on stan-\ndard benchmarks. We evaluate our proposed design de-\ncisions through ablations on all three tasks. Finally we\nshow Mask2Former generalizes beyond the standard bench-\nmarks, obtaining state-of-the-art results on four datasets.\nDatasets. We study Mask2Former using four widely used\nimage segmentation datasets that support semantic, instance\nand panoptic segmentation: COCO [35] (80 ‚Äúthings‚Äù and\n53 ‚Äústuff‚Äù categories), ADE20K [65] (100 ‚Äúthings‚Äù and\n50 ‚Äústuff‚Äù categories), Cityscapes [16] (8 ‚Äúthings‚Äù and 11\n‚Äústuff‚Äù categories) and Mapillary Vistas [42] (37 ‚Äúthings‚Äù\nand 28 ‚Äústuff‚Äù categories). Panoptic and semantic seg-\n4\nmethod backbone query type epochs PQ PQ Th PQSt APTh\npan mIoUpan #params. FLOPs fps\nDETR [5] R50 100 queries 500+25 43.4 48.2 36.3 31.1 - - - -\nMaskFormer [14] R50 100 queries 300 46.5 51.0 39.8 33.0 57.8 45M 181G 17.6\nMask2Former (ours) R50 100 queries 50 51.9 57.7 43.0 41.7 61.7 44M 226G 8.6\nDETR [5] R101 100 queries 500+25 45.1 50.5 37.0 33.0 - - - -\nMaskFormer [14] R101 100 queries 300 47.6 52.5 40.3 34.1 59.3 64M 248G 14.0\nMask2Former (ours) R101 100 queries 50 52.6 58.5 43.7 42.6 62.4 63M 293G 7.2\nMax-DeepLab [52] Max-L 128 queries 216 51.1 57.0 42.2 - - 451M 3692G -\nMaskFormer [14] Swin-L‚Ä† 100 queries 300 52.7 58.5 44.0 40.1 64.8 212M 792G 5.2\nK-Net [62] Swin-L‚Ä† 100 queries 36 54.6 60.2 46.0 - - - - -\nMask2Former (ours) Swin-L‚Ä† 200 queries 100 57.8 64.2 48.1 48.6 67.4 216M 868G 4.0\nTable 1. Panoptic segmentation on COCO panoptic val2017 with 133 categories. Mask2Former consistently outperforms Mask-\nFormer [14] by a large margin with different backbones on all metrics. Our best model outperforms prior state-of-the-art MaskFormer by\n5.1 PQ and K-Net [62] by 3.2 PQ. Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.\nmentation tasks are evaluated on the union of ‚Äúthings‚Äù and\n‚Äústuff‚Äù categories while instance segmentation is only eval-\nuated on the ‚Äúthings‚Äù categories.\nEvaluation metrics. For panoptic segmentation, we use\nthe standard PQ (panoptic quality) metric [28]. We fur-\nther report APTh\npan, which is the AP evaluated on the ‚Äúthing‚Äù\ncategories using instance segmentation annotations, and\nmIoUpan, which is the mIoU for semantic segmentation\nby merging instance masks from the same category, of the\nsame model trained only with panoptic segmentation anno-\ntations. For instance segmentation, we use the standard AP\n(average precision) metric [35]. Forsemantic segmentation,\nwe use mIoU (mean Intersection-over-Union) [19].\n4.1. Implementation details\nWe adopt settings from [14] with the following differences:\nPixel decoder. Mask2Former is compatible with any exist-\ning pixel decoder module. In MaskFormer [14], FPN [33]\nis chosen as the default for its simplicity. Since our goal\nis to demonstrate strong performance across different seg-\nmentation tasks, we use the more advanced multi-scale de-\nformable attention Transformer (MSDeformAttn) [66] as\nour default pixel decoder. SpeciÔ¨Åcally, we use 6 MSDefor-\nmAttn layers applied to feature maps with resolution 1/8,\n1/16 and 1/32, and use a simple upsampling layer with lat-\neral connection on the Ô¨Ånal 1/8 feature map to generate the\nfeature map of resolution 1/4 as the per-pixel embedding.\nIn our ablation study, we show that this pixel decoder pro-\nvides best results across different segmentation tasks.\nTransformer decoder. We use our Transformer decoder\nproposed in Section 3.2 with L= 3(i.e., 9 layers total) and\n100 queries by default. An auxiliary loss is added to every\nintermediate Transformer decoder layer and to the learnable\nquery features before the Transformer decoder.\nLoss weights. We use the binary cross-entropy loss (instead\nof focal loss [34] in [14]) and the dice loss [41] for our mask\nloss: Lmask = ŒªceLce + ŒªdiceLdice. We set Œªce = 5.0 and\nŒªdice = 5.0. The Ô¨Ånal loss is a combination of mask loss and\nclassiÔ¨Åcation loss: Lmask +ŒªclsLcls and we set Œªcls = 2.0 for\npredictions matched with a ground truth and 0.1 for the ‚Äúno\nobject,‚Äù i.e., predictions that have not been matched with\nany ground truth.\nPost-processing. We use the exact same post-processing\nas [14] to acquire the expected output format for panoptic\nand semantic segmentation from pairs of binary masks and\nclass predictions. Instance segmentation requires additional\nconÔ¨Ådence scores for each prediction. We multiply class\nconÔ¨Ådence and mask conÔ¨Ådence ( i.e., averaged foreground\nper-pixel binary mask probability) for a Ô¨Ånal conÔ¨Ådence.\n4.2. Training settings\nPanoptic and instance segmentation. We use Detec-\ntron2 [57] and follow the updated Mask R-CNN [24] base-\nline settings1 for the COCO dataset. More speciÔ¨Åcally, we\nuse AdamW [38] optimizer and the step learning rate sched-\nule. We use an initial learning rate of 0.0001 and a weight\ndecay of 0.05 for all backbones. A learning rate multiplier\nof 0.1 is applied to the backbone and we decay the learning\nrate at 0.9 and 0.95 fractions of the total number of training\nsteps by a factor of 10. If not stated otherwise, we train our\nmodels for 50 epochs with a batch size of 16. For data aug-\nmentation, we use the large-scale jittering (LSJ) augmenta-\ntion [18,23] with a random scale sampled from range 0.1 to\n2.0 followed by a Ô¨Åxed size crop to1024√ó1024. We use the\nstandard Mask R-CNN inference setting where we resize an\nimage with shorter side to 800 and longer side up-to 1333.\nWe also report FLOPs and fps. FLOPs are averaged over\n100 validation images (COCO images have varying sizes).\nFrames-per-second (fps) is measured on a V100 GPU with\na batch size of 1 by taking the average runtime on the entire\nvalidation set including post-processing time.\nSemantic segmentation. We follow the same settings\nas [14] to train our models, except: 1) a learning rate multi-\nplier of 0.1 is applied to both CNN and Transformer back-\nbones instead of only applying it to CNN backbones in [14],\n2) both ResNet and Swin backbones use an initial learning\nrate of 0.0001 and a weight decay of 0.05, instead of using\n1https://github.com/facebookresearch/detectron2/blob/\nmain / MODEL _ ZOO . md # new - baselines - using - large - scale -\njitter-and-longer-training-schedule\n5\nmethod backbone query type epochs AP AP S APM APL APboundary #params. FLOPs fps\nMaskFormer [14] R50 100 queries 300 34.0 16.4 37.8 54.2 23.0 45M 181G 19.2\nMask R-CNN [24] R50 dense anchors 36 37.2 18.6 39.5 53.3 23.1 44M 201G 15.2\nMask R-CNN [18, 23, 24] R50 dense anchors 400 42.5 23.8 45.0 60.0 28.0 46M 358G 10.3\nMask2Former (ours) R50 100 queries 50 43.7 23.4 47.2 64.8 30.6 44M 226G 9.7\nMask R-CNN [24] R101 dense anchors 36 38.6 19.5 41.3 55.3 24.5 63M 266G 10.8\nMask R-CNN [18, 23, 24] R101 dense anchors 400 43.7 24.6 46.4 61.8 29.1 65M 423G 8.6\nMask2Former (ours) R101 100 queries 50 44.2 23.8 47.7 66.7 31.1 63M 293G 7.8\nQueryInst [20] Swin-L‚Ä† 300 queries 50 48.9 30.8 52.6 68.3 33.5 - - 3.3\nSwin-HTC++ [6, 36] Swin-L‚Ä† dense anchors 72 49.5 31.0 52.4 67.2 34.1 284M 1470G -\nMask2Former (ours) Swin-L‚Ä† 200 queries 100 50.1 29.9 53.9 72.1 36.2 216M 868G 4.0\nTable 2. Instance segmentation on COCOval2017 with 80 categories. Mask2Former outperforms strong Mask R-CNN [24] baselines\nfor both AP and AP boundary [12] metrics when training with 8√ó fewer epochs. Our best model is also competitive to the state-of-the-art\nspecialized instance segmentation model on COCO and has higher boundary quality. For a fair comparison, we only consider single-scale\ninference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.\ndifferent learning rates in [14].\n4.3. Main results\nPanoptic segmentation. We compare Mask2Former with\nstate-of-the-art models for panoptic segmentation on the\nCOCO panoptic [28] dataset in Table 1. Mask2Former\nconsistently outperforms MaskFormer by more than 5 PQ\nacross different backbones while converging 6√ó faster.\nWith Swin-L backbone, our Mask2Former sets a new state-\nof-the-art of 57.8 PQ, outperforming existing state-of-the-\nart [14] by 5.1 PQ and concurrent work, K-Net [62], by\n3.2 PQ. Mask2Former even outperforms the best ensemble\nmodels with extra training data in the COCO challenge (see\nAppendix A.1 for test set results).\nBeyond the PQ metric, our Mask2Former also achieves\nhigher performance on two other metrics compared to\nDETR [5] and MaskFormer: AP Th\npan, which is the AP eval-\nuated on the 80 ‚Äúthing‚Äù categories using instance segmen-\ntation annotation, and mIoU pan, which is the mIoU evalu-\nated on the 133 categories for semantic segmentation con-\nverted from panoptic segmentation annotation. This shows\nMask2Former‚Äôs universality: trained only with panoptic\nsegmentation annotations, it can be used for instance and\nsemantic segmentation.\nInstance segmentation. We compare Mask2Former with\nstate-of-the-art models on the COCO [35] dataset in Ta-\nble 2. With ResNet [25] backbone, Mask2Former outper-\nforms a strong Mask R-CNN [24] baseline using large-\nscale jittering (LSJ) augmentation [18, 23] while requir-\ning 8√ófewer training iterations. With Swin-L backbone,\nMask2Former outperforms the state-of-the-art HTC++ [6].\nAlthough we only observe +0.6 AP improvement over\nHTC++, the Boundary AP [12] improves by2.1, suggesting\nthat our predictions have a better boundary quality thanks to\nthe high-resolution mask predictions. Note that for a fair\ncomparison, we only consider single-scale inference and\nmodels trained with only COCO train2017 set data.\nWith a ResNet-50 backbone Mask2Former improves\nover MaskFormer on small objects by 7.0 APS, while over-\nmethod backbone crop size mIoU (s.s.) mIoU (m.s.)\nMaskFormer [14] R50 512 44.5 46.7\nMask2Former (ours) R50 512 47.2 49.2\nSwin-UperNet [36, 58] Swin-T 512 - 46.1\nMaskFormer [14] Swin-T 512 46.7 48.8\nMask2Former (ours) Swin-T 512 47.7 49.6\nMaskFormer [14] Swin-L‚Ä† 640 54.1 55.6\nFaPN-MaskFormer [14, 39] Swin-L-FaPN‚Ä† 640 55.2 56.7\nBEiT-UperNet [2, 58] BEiT-L‚Ä† 640 - 57.0\nMask2Former (ours) Swin-L‚Ä† 640 56.1 57.3\nSwin-L-FaPN‚Ä† 640 56.4 57.7\nTable 3. Semantic segmentation on ADE20K val with\n150 categories. Mask2Former consistently outperforms Mask-\nFormer [14] by a large margin with different backbones (all\nMask2Former models use MSDeformAttn [66] as pixel decoder,\nexcept Swin-L-FaPN uses FaPN [39]). Our best model outper-\nforms the best specialized model, BEiT [2]. We report both single-\nscale (s.s.) and multi-scale (m.s.) inference results. Backbones\npre-trained on ImageNet-22K are marked with ‚Ä†.\nall the highest gains come from large objects (+10.6 APL).\nThe performance on APS still lags behind other state-of-the-\nart models. Hence there still remains room for improvement\non small objects, e.g., by using dilated backbones like in\nDETR [5], which we leave for future work.\nSemantic segmentation. We compare Mask2Former with\nstate-of-the-art models for semantic segmentation on the\nADE20K [65] dataset in Table 3. Mask2Former outper-\nforms MaskFormer [14] across different backbones, sug-\ngesting that the proposed improvements even boost seman-\ntic segmentation results where [14] was already state-of-\nthe-art. With Swin-L as backbone and FaPN [39] as pixel\ndecoder, Mask2Former sets a new state-of-the-art of 57.7\nmIoU. We also report the test set results in Appendix A.3.\n4.4. Ablation studies\nWe now analyze Mask2Former through a series of abla-\ntion studies using a ResNet-50 backbone [25]. To test the\ngenerality of the proposed components for universal image\nsegmentation, all ablations are performed on three tasks.\n6\nAP PQ mIoU FLOPs\nMask2Former (ours) 43.7 51.9 47.2 226G\n‚àímasked attention 37.8 (-5.9) 47.1 (-4.8) 45.5 (-1.7) 213G\n‚àíhigh-resolution features 41.5 (-2.2) 50.2 (-1.7) 46.1 (-1.1) 218G\n(a) Masked attention and high-resolution features (from efÔ¨Åcient multi-scale\nstrategy) lead to the most gains. More detailed ablations are in Table 4c and\nTable 4d. We remove one component at a time.\nAP PQ mIoU FLOPs\nMask2Former (ours) 43.7 51.9 47.2 226G\n‚àílearnable query features 42.9 (-0.8) 51.2 (-0.7) 45.4 (-1.8) 226G\n‚àícross-attention Ô¨Årst 43.2 (-0.5) 51.6 (-0.3) 46.3 (-0.9) 226G\n‚àíremove dropout 43.0 (-0.7) 51.3 (-0.6) 47.2 (-0.0) 226G\n‚àíall 3 components above 42.3 (-1.4) 50.8 (-1.1) 46.3 (-0.9) 226G\n(b) Optimization improvements increase the performance without introduc-\ning extra compute. Following DETR [5], query features are zero-initialized\nwhen not learnable. We remove one component at a time.\nAP PQ mIoU FLOPs\ncross-attention 37.8 47.1 45.5 213G\nSMCA [22] 37.9 47.2 46.6 213G\nmask pooling [62] 43.1 51.5 46.0 217G\nmasked attention 43.7 51.9 47.2 226G\n(c) Masked attention. Our masked attention\nperforms better than other variants of cross-\nattention across all tasks.\nAP PQ mIoU FLOPs\nsingle scale (1/32) 41.5 50.2 46.1 218G\nsingle scale (1/16) 43.0 51.5 46.5 222G\nsingle scale (1/8) 44.0 51.8 47.4 239G\nna¬®ƒ±ve m.s. (3 scales) 44.0 51.9 46.3 247G\nefÔ¨Åcient m.s. (3 scales) 43.7 51.9 47.2 226G\n(d) Feature resolution. High-resolution features (sin-\ngle scale 1/8) are important. Our efÔ¨Åcient multi-scale\n(efÔ¨Åcient m.s.) strategy effectively reduces the FLOPs.\nAP PQ mIoU FLOPs\nFPN [33] 41.5 50.7 45.6 195G\nSemantic FPN [27] 42.1 51.2 46.2 258G\nFaPN [39] 42.4 51.8 46.8 -\nBiFPN [47] 43.5 51.8 45.6 204G\nMSDeformAttn [66] 43.7 51.9 47.2 226G\n(e) Pixel decoder. MSDeformAttn [66] consis-\ntently performs the best across all tasks.\nTable 4. Mask2Former ablations. We perform ablations on three tasks: instance (AP on COCO val2017), panoptic (PQ on COCO\npanoptic val2017) and semantic (mIoU on ADE20K val) segmentation. FLOPs are measured on COCO instance segmentation.\nTransformer decoder. We validate the importance of each\ncomponent by removing them one at a time. As shown in\nTable 4a, masked attention leads to the biggest improve-\nment across all tasks. The improvement is larger for in-\nstance and panoptic segmentation than for semantic seg-\nmentation. Moreover, using high-resolution features from\nthe efÔ¨Åcient multi-scale strategy is also important. Table 4b\nshows additional optimization improvements further im-\nprove the performance without extra computation.\nMasked attention. Concurrent work has proposed other\nvariants of cross-attention [22, 40] that aim to improve the\nconvergence and performance of DETR [5] for object de-\ntection. Most recently, K-Net [62] replaced cross-attention\nwith a mask pooling operation that averages features within\nmask regions. We validate the importance of our masked\nattention in Table 4c. While existing cross-attention vari-\nants may improve on a speciÔ¨Åc task, our masked attention\nperforms the best on all three tasks.\nFeature resolution. Table 4d shows that Mask2Former\nbeneÔ¨Åts from using high-resolution features ( e.g., a single\nscale of 1/8) in the Transformer decoder. However, this in-\ntroduces additional computation. Our efÔ¨Åcient multi-scale\n(efÔ¨Åcient m.s.) strategy effectively reduces the FLOPs with-\nout affecting the performance. Note that, naively concate-\nnating multi-scale features as input to every Transformer\ndecoder layer (na¬®ƒ±ve m.s.) does not yield additional gains.\nPixel decoder. As shown in Table 4e, Mask2Former is com-\npatible with any existing pixel decoder. However, we ob-\nserve different pixel decoders specialize in different tasks:\nwhile BiFPN [47] performs better on instance-level seg-\nmentation, FaPN [39] works better for semantic segmen-\ntation. Among all studied pixel decoders, the MSDefor-\nmaAttn [66] consistently performs the best across all tasks\nand thus is selected as our default. This set of ablations also\nmatching loss training loss\nAP\n(COCO)\nPQ\n(COCO)\nmIoU\n(ADE20K)\nmemory\n(COCO)\nmask mask 41.0 50.3 45.9 18G\npoint 41.0 50.8 45.9 6G\npoint (ours) mask 43.1 51.4 47.3 18G\npoint (ours) 43.7 51.9 47.2 6G\nTable 5. Calculating loss with points vs. masks. Training with\npoint loss reduces training memory without inÔ¨Çuencing the perfor-\nmance. Matching with point loss further improves performance.\nsuggests that designing a module like a pixel decoder for a\nspeciÔ¨Åc task does not guarantee generalization across seg-\nmentation tasks. Mask2Former, as a universal model, could\nserve as a testbed for a generalizable module design.\nCalculating loss with points vs. masks. In Table 5 we\nstudy the performance and memory implications when cal-\nculating the loss based on either mask or sampled points.\nCalculating the Ô¨Ånal training loss with sampled points re-\nduces training memory by 3√ówithout affecting the per-\nformance. Additionally, calculating the matching loss with\nsampled points improves performance across all three tasks.\nLearnable queries as region proposals. Region propos-\nals [1, 50], either in the form of boxes or masks, are re-\ngions that are likely to be ‚Äúobjects.‚Äù With learnable queries\nbeing supervised by the mask loss, predictions from learn-\nable queries can serve as mask proposals. In Figure 3 top,\nwe visualize mask predictions of selected learnable queries\nbefore feeding them into the Transformer decoder (the pro-\nposal generation process is shown in Figure 3 bottom right).\nIn Figure 3 bottom left, we further perform a quantita-\ntive analysis on the quality of these proposals by calculat-\ning the class-agnostic average recall with 100 predictions\n(AR@100) on COCO val2017. We Ô¨Ånd these learnable\nqueries already achieve good AR@100 compared to the Ô¨Å-\n7\n50.3\nlearnable\nqueries\n56.8\nlayer 3 57.4\nlayer 6 57.7\nlayer 9\nAR@100 on COCO val2017\nBackbone\nmask\nLearnable queries\nPixel Decoder\nFigure 3. Learnable queries as ‚Äúregion proposals‚Äù. Top: We\nvisualize mask predictions of four selected learnable queries be-\nfore feeding them into the Transformer decoder (using R50 back-\nbone). Bottom left: We calculate the class-agnostic average recall\nwith 100 proposals (AR@100) and observe that these learnable\nqueries provide good proposals compared to the Ô¨Ånal predictions\nof Mask2Former after the Transformer decoder layers (layer 9).\nBottom right: Illustration of proposal generation process.\npanoptic model semantic model\nmethod backbone PQ AP Th\npan mIoUpan mIoU (s.s.) (m.s.)\nPanoptic FCN [31] Swin-L‚Ä† 65.9 - - - -\nPanoptic-DeepLab [11] SWideRNet [9] 66.4 40.1 82.2 - -\nPanoptic-DeepLab [11] SWideRNet [9] 67.5‚àó 43.9‚àó 82.9‚àó - -\nSETR [64] ViT-L‚Ä† [17] - - - - 82.2\nSegFormer [59] MiT-B5 [59] - - - - 84.0\nMask2Former (ours)\nR50 62.1 37.3 77.5 79.4 82.2\nSwin-B‚Ä† 66.1 42.8 82.7 83.3 84.5\nSwin-L‚Ä† 66.6 43.6 82.9 83.3 84.3\nTable 6. Cityscapes val. Mask2Former is competitive to spe-\ncialized models on Cityscapes. Panoptic segmentation models use\nsingle-scale inference by default, multi-scale numbers are marked\nwith ‚àó. For semantic segmentation, we report both single-scale\n(s.s.) and multi-scale (m.s.) inference results. Backbones pre-\ntrained on ImageNet-22K are marked with ‚Ä†.\nnal predictions of Mask2Former after the Transformer de-\ncoder layers, i.e., layer 9, and AR@100 consistently im-\nproves with more decoder layers.\n4.5. Generalization to other datasets\nTo show our Mask2Former can generalize beyond the\nCOCO dataset, we further perform experiments on other\npopular image segmentation datasets. In Table 6, we show\nresults on Cityscapes [16]. Please see Appendix B for de-\ntailed training settings on each dataset as well as more re-\nsults on ADE20K [65] and Mapillary Vistas [42].\nPQ AP mIoU\npanoptic 51.9 41.7 61.7\ninstance - 43.7 -\nsemantic - - 61.5\n(a) COCO\nPQ AP mIoU\n39.7 26.5 46.1\n- 26.4 -\n- - 47.2\n(b) ADE20K\nPQ AP mIoU\n62.1 37.3 77.5\n- 37.4 -\n- - 79.4\n(c) Cityscapes\nTable 7. Limitations of Mask2Former. Although a single\nMask2Former can address any segmentation task, we still need\nto train it on different tasks. Across three datasets we Ô¨Ånd\nMask2Former trained with panoptic annotations performs slightly\nworse than the exact same model trained speciÔ¨Åcally for instance\nand semantic segmentation tasks with the corresponding data.\nWe observe that our Mask2Former is competitive to\nstate-of-the-art methods on these datasets as well. It sug-\ngests Mask2Former can serve as a universal image segmen-\ntation model and results generalize across datasets.\n4.6. Limitations\nOur ultimate goal is to train a single model for all im-\nage segmentation tasks. In Table 7, we Ô¨Ånd Mask2Former\ntrained on panoptic segmentation only performs slightly\nworse than the exact same model trained with the corre-\nsponding annotations for instance and semantic segmenta-\ntion tasks across three datasets. This suggests that even\nthough Mask2Former can generalize to different tasks, it\nstill needs to be trained for those speciÔ¨Åc tasks. In the fu-\nture, we hope to develop a model that can be trained only\nonce for multiple tasks and even for multiple datasets.\nFurthermore, as seen in Tables 2 and 4d, even though it\nimproves over baselines, Mask2Former struggles with seg-\nmenting small objects and is unable to fully leverage multi-\nscale features. We believe better utilization of the feature\npyramid and designing losses for small objects are critical.\n5. Conclusion\nWe present Mask2Former for universal image segmen-\ntation. Built upon a simple meta framework [14] with a\nnew Transformer decoder using the proposed masked atten-\ntion, Mask2Former obtains top results in all three major im-\nage segmentation tasks (panoptic, instance and semantic) on\nfour popular datasets, outperforming even the best special-\nized models designed for each benchmark while remaining\neasy to train. Mask2Former saves 3√óresearch effort com-\npared to designing specialized models for each task, and it\nis accessible to users with limited computational resources.\nWe hope to attract interest in universal model design.\nEthical considerations: While our technical innovations do not appear to\nhave any inherent biases, the models trained with our approach on real-\nworld datasets should undergo ethical review to ensure the predictions do\nnot propagate problematic stereotypes, and the approach is not used for\napplications including but not limited to illegal surveillance.\nAcknowledgments: Thanks to Nicolas Carion and Xingyi Zhou for help-\nful feedback. BC and AS are supported in part by NSF #1718221,\n2008387, 2045586, 2106825, MRI #1725729, NIFA 2020-67021-32799\nand Cisco Systems Inc. (CG 1377144 - thanks for access to Arcetri).\n8\nReferences\n[1] Pablo Arbel ¬¥aez, Jordi Pont-Tuset, Jonathan T Barron, Fer-\nran Marques, and Jitendra Malik. Multiscale combinatorial\ngrouping. In CVPR, 2014.\n[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-\ntraining of image transformers. arXiv, 2021.\n[3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\nYOLACT++: Better real-time instance segmentation, 2019.\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-\ning into high quality object detection. In CVPR, 2018.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[6] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-\niao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi,\nWanli Ouyang, et al. Hybrid task cascade for instance seg-\nmentation. In CVPR, 2019.\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected CRFs. PAMI, 2018.\n[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv:1706.05587, 2017.\n[9] Liang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scal-\ning wide residual networks for panoptic segmentation.\narXiv:2011.11675, 2020.\n[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018.\n[11] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-DeepLab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR, 2020.\n[12] Bowen Cheng, Ross Girshick, Piotr Doll ¬¥ar, Alexander C\nBerg, and Alexander Kirillov. Boundary iou: Improving\nobject-centric image segmentation evaluation. In CVPR,\n2021.\n[13] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov.\nPointly-supervised instance segmentation. arXiv, 2021.\n[14] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-\nillov. Per-pixel classiÔ¨Åcation is not all you need for semantic\nsegmentation. In NeurIPS, 2021.\n[15] Franc ¬∏ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017.\n[16] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The Cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021.\n[18] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\nLin. Simple training strategies and model scaling for object\ndetection. arXiv preprint arXiv:2107.00057, 2021.\n[19] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-\npher KI Williams, John Winn, and Andrew Zisserman. The\nPASCAL visual object classes challenge: A retrospective.\nIJCV, 2015.\n[20] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen\nFang, Ying Shan, Bin Feng, and Wenyu Liu. Instances as\nqueries. In ICCV, 2021.\n[21] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene\nsegmentation. In CVPR, 2019.\n[22] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai,\nand Hongsheng Li. Fast convergence of detr with spatially\nmodulated co-attention. In ICCV, 2021.\n[23] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. In CVPR, 2021.\n[24] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016.\n[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. CCNet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019.\n[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDoll¬¥ar. Panoptic feature pyramid networks. In CVPR, 2019.\n[28] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll ¬¥ar. Panoptic segmentation. In CVPR,\n2019.\n[29] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bog-\ndan Savchynskyy, and Carsten Rother. InstanceCut: from\nedges to instances with multicut. In CVPR, 2017.\n[30] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-\nshick. PointRend: Image segmentation as rendering. In\nCVPR, 2020.\n[31] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Yukang Chen,\nLu Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia.\nFully convolutional networks for panoptic segmentation with\npoint-based supervision. arXiv preprint arXiv:2108.07682,\n2021.\n[32] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima\nAnandkumar, Jose M Alvarez, Tong Lu, and Ping Luo.\nPanoptic segformer. arXiv preprint arXiv:2109.03814, 2021.\n[33] Tsung-Yi Lin, Piotr Doll ¬¥ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017.\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll¬¥ar. Focal loss for dense object detection. In ICCV,\n2017.\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014.\n9\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv:2103.14030, 2021.\n[37] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR, 2015.\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019.\n[39] Shihua Huang Zhichao Lu, Ran Cheng, and Cheng He. Fapn:\nFeature-aligned pyramid network for dense image predic-\ntion. arXiv, 2021.\n[40] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng,\nHouqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang.\nConditional detr for fast training convergence. In ICCV,\n2021.\n[41] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-Net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 3DV, 2016.\n[42] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bul `o, and\nPeter Kontschieder. The mapillary vistas dataset for semantic\nunderstanding of street scenes. In CVPR, 2017.\n[43] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In NeurIPS, 2015.\n[44] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\nlenge. IJCV, 2015.\n[45] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmenta-\ntion. In ICCV, 2021.\n[46] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris M Ki-\ntani. Rethinking transformer-based set prediction for object\ndetection. In ICCV, 2021.\n[47] Mingxing Tan, Ruoming Pang, and Quoc V Le. EfÔ¨Åcientdet:\nScalable and efÔ¨Åcient object detection. In CVPR, 2020.\n[48] Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hi-\nerarchical multi-scale attention for semantic segmentation.\narXiv:2005.10821, 2020.\n[49] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convo-\nlutions for instance segmentation. In ECCV, 2020.\n[50] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-\ners, and Arnold WM Smeulders. Selective search for object\nrecognition. IJCV, 2013.\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[52] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. MaX-DeepLab: End-to-end panoptic\nsegmentation with mask transformers. In CVPR, 2021.\n[53] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep\nhigh-resolution representation learning for visual recogni-\ntion. PAMI, 2019.\n[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPvtv2: Improved baselines with pyramid vision transformer.\narXiv preprint arXiv:2106.13797, 2021.\n[55] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018.\n[56] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-\nhua Shen. SOLOv2: Dynamic and fast instance segmenta-\ntion. NeurIPS, 2020.\n[57] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019.\n[58] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. UniÔ¨Åed perceptual parsing for scene understand-\ning. In ECCV, 2018.\n[59] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. Segformer: Simple and ef-\nÔ¨Åcient design for semantic segmentation with transformers.\nIn NeurIPS, 2021.\n[60] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min\nBai, Ersin Yumer, and Raquel Urtasun. Upsnet: A uniÔ¨Åed\npanoptic segmentation network. In CVPR, 2019.\n[61] Yuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin\nChen, and Jingdong Wang. OCNet: Object context for se-\nmantic segmentation. IJCV, 2021.\n[62] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and\nChen Change Loy. K-net: Towards uniÔ¨Åed image seg-\nmentation. In NeurIPS, 2021.\n[63] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017.\n[64] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In CVPR, 2021.\n[65] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nADE20K dataset. In CVPR, 2017.\n[66] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. In ICLR, 2021.\n10\nAppendix\nWe Ô¨Årst provide more results for Mask2Former with differ-\nent backbones as well as test-set performance on standard\nbenchmarks (Appendix A): We use COCO panoptic [28]\nfor panoptic, COCO [35] for instance, and ADE20K [65]\nfor semantic segmentation. Then, we provide more detailed\nresults on additional datasets (Appendix B). Finally, we pro-\nvide additional ablation studies (Appendix C) and visualiza-\ntion of Mask2Former predictions for all three segmentation\ntasks (Appendix D).\nA. Additional results\nHere, we provide more results of Mask2Former with\ndifferent backbones on COCO panoptic [28] for panoptic\nsegmentation, COCO [35] for instance segmentation and\nADE20K [65] for semantic segmentation. More speciÔ¨Å-\ncally, for each benckmark, we evaluate Mask2Former with\nResNet [25] with 50 and 101 layers, as well as Swin [36]\nTiny, Small, Base and Large variants as backbones. We use\nImageNet [44] pre-trained checkpoints to initialize back-\nbones.\nA.1. Panoptic segmentation.\nIn Table I, we report Mask2Former with various back-\nbones on COCO panoptic val2017. Mask2Former out-\nperforms all existing panoptic segmentation models with\nvarious backbones. Our best model sets a new state-of-the-\nart of 57.8 PQ.\nIn Table II, we further report the best Mask2Former\nmodel on the test-dev set. Note that Mask2Former\ntrained only with the standard train2017 data,\nachieves the absolute new state-of-the-art performance on\nboth validation and test set. Mask2Former even outper-\nforms the best COCO competition entry which uses extra\ntraining data and test-time augmentation.\nA.2. Instance segmentation.\nIn Table III, we report Mask2Former results ob-\ntained with various backbones on COCO val2017.\nMask2Former outperforms the best single-scale model,\nHTC++ [6, 36]. Note that it is non-trivial to do multi-scale\ninference for instance-level segmentation tasks without in-\ntroducing complex post-processing like non-maximum sup-\npression. Thus, we only compare Mask2Former with other\nsingle-scale inference models. We believe multi-scale infer-\nence can further improve Mask2Former performance and it\nremains an interesting future work.\nIn Table IV, we further report the best Mask2Former\nmodel on the test-dev set. Mask2Former achieves the\nabsolute new state-of-the-art performance on both valida-\ntion and test set. On the one hand, Mask2Former is ex-\ntremely good at segmenting large objects: we can even\noutperform the challenge winner (which uses extra train-\ning data, model ensemble, etc.) on AP L by a large margin\nwithout any bells-and-whistles. On the other hand, the poor\nperformance on small objects leaves room for further im-\nprovement in the future.\nA.3. Semantic segmentation.\nIn Table V, we report Mask2Former results obtained\nwith various backbones on ADE20K val. Mask2Former\noutperforms all existing semantic segmentation models\nwith various backbones. Our best model sets a new state-\nof-the-art of 57.7 mIoU.\nIn Table VI, we further report the best Mask2Former\nmodel on the test set. Following [14], we train\nMask2Former on the union of ADE20K train and val\nset with ImageNet-22K pre-trained checkpoint and use\nmulti-scale inference. Mask2Former is able to outperform\nprevious state-of-the-art methods on all metrics.\nB. Additional datasets\nWe study Mask2Former on three image segmentation\ntasks (panoptic, instance and semantic segmentation) us-\ning four datasets. Here we report additional results on\nCityscapes [16], ADE20K [65] and Mapillary Vistas [42]\nas well as more detailed training settings.\nB.1. Cityscapes\nCityscapes is an urban egocentric street-view dataset\nwith high-resolution images ( 1024 √ó2048 pixels). It con-\ntains 2975 images for training, 500 images for validation\nand 1525 images for testing with a total of 19 classes.\nTraining settings. For all three segmentation tasks: we use\na crop size of 512 √ó1024, a batch size of 16 and train\nall models for 90k iterations. During inference, we oper-\nate on the whole image ( 1024 √ó2048). Other implemen-\ntation details largely follow Section 4.1 (panoptic and in-\nstance segmentation follow semantic segmentation training\nsettings), except that we use 200 queries for panoptic and\ninstance segmentation models with Swin-L backbone. All\nother backbones or semantic segmentation models use 100\nqueries.\nResults. In Table VII, we report Mask2Former results ob-\ntained with various backbones on Cityscapes for three seg-\nmentation tasks and compare it with other state-of-the-art\nmethods without using extra data . For panoptic segmen-\ntation, Mask2Former with Swin-L backbone outperforms\nthe state-of-the-art Panoptic-DeepLab [11] with SWideR-\nnet [9] using single-scale inference. For semantic segmen-\ntation, Mask2Former with Swin-B backbone outperforms\nthe state-of-the-art SegFormer [59].\n11\nmethod backbone search space epochs PQ PQ Th PQSt APTh\npan mIoUpan #params. FLOPs\nCNN backbones\nDETR [5] R50 100 queries 500+25 43.4 48.2 36.3 31.1 - - -\nR101 100 queries 500+25 45.1 50.5 37.0 33.0 - - -\nK-Net [62] R50 100 queries 36 47.1 51.7 40.3 - - - -\nPanoptic SegFormer [32] R50 400 queries 50 50.0 56.1 40.8 - - 47M 246G\nMaskFormer [14] R50 100 queries 300 46.5 51.0 39.8 33.0 57.8 45M 181G\nR101 100 queries 300 47.6 52.5 40.3 34.1 59.3 64M 248G\nMask2Former (ours) R50 100 queries 50 51.9 57.7 43.0 41.7 61.7 44M 226G\nR101 100 queries 50 52.6 58.5 43.7 42.6 62.4 63M 293G\nTransformer backbones\nMax-DeepLab [52] Max-S 128 queries 216 48.4 53.0 41.5 - - 62M 324G\nMax-L 128 queries 216 51.1 57.0 42.2 - - 451M 3692G\nPanoptic SegFormer [32] PVTv2-B5 [54] 400 queries 50 54.1 60.4 44.6 - - 101M 391G\nK-Net [62] Swin-L‚Ä† 100 queries 36 54.6 60.2 46.0 - - - -\nMaskFormer [14]\nSwin-T 100 queries 300 47.7 51.7 41.7 33.6 60.4 42M 179G\nSwin-S 100 queries 300 49.7 54.4 42.6 36.1 61.3 63M 259G\nSwin-B 100 queries 300 51.1 56.3 43.2 37.8 62.6 102M 411G\nSwin-B‚Ä† 100 queries 300 51.8 56.9 44.1 38.5 63.6 102M 411G\nSwin-L‚Ä† 100 queries 300 52.7 58.5 44.0 40.1 64.8 212M 792G\nMask2Former (ours)\nSwin-T 100 queries 50 53.2 59.3 44.0 43.3 63.2 47M 232G\nSwin-S 100 queries 50 54.6 60.6 45.7 44.7 64.2 69M 313G\nSwin-B 100 queries 50 55.1 61.0 46.1 45.2 65.1 107M 466G\nSwin-B‚Ä† 100 queries 50 56.4 62.4 47.3 46.3 67.1 107M 466G\nSwin-L‚Ä† 200 queries 100 57.8 64.2 48.1 48.6 67.4 216M 868G\nTable I. Panoptic segmentation on COCO panoptic val2017 with 133 categories. Mask2Former outperforms all existing panoptic\nsegmentation models by a large margin with different backbones on all metrics. Our best model sets a new state-of-the-art of 57.8 PQ.\nBesides PQ for panoptic segmentation, we also report AP Th\npan (the AP evaluated on the 80 ‚Äúthing‚Äù categories using instance segmentation\nannotation) and mIoU pan (the mIoU evaluated on the 133 categories for semantic segmentation converted from panoptic segmentation\nannotation) of the same model trained for panoptic segmentation (note: we train all our models with panoptic segmentation annotation\nonly). Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.\nmethod backbone PQ PQ Th PQSt SQ RQ\nMax-DeepLab [52] Max-L 51.3 57.2 42.4 82.5 61.3\nPanoptic FCN [31] Swin-L 52.7 59.4 42.5 - -\nMaskFormer [14] Swin-L 53.3 59.1 44.5 82.0 64.1\nPanoptic SegFormer [32] PVTv2-B5 [54] 54.4 61.1 44.3 83.3 64.6\nK-Net [62] Swin-L 55.2 61.2 46.2 - -\nMegvii (challenge winner) - 54.7 64.6 39.8 83.6 64.3\nMask2Former (ours) Swin-L 58.3 65.1 48.1 84.1 68.6\nTable II. Panoptic segmentation on COCO panoptic test-dev with 133 categories. Mask2Former, without any bells-and-whistles,\noutperforms the challenge winner (which uses extra training data, model ensemble, etc.) on the test-dev set. We only train our model\non the COCO train2017 set with ImageNet-22K pre-trained checkpoint.\nB.2. ADE20K\nTraining settings. For panoptic and instance segmentation,\nwe use the exact same training parameters as we used for\nsemantic segmentation, except that we always use a crop\nsize of 640 √ó640 for all backbones. Other implementa-\ntion details largely follow Section 4.1 , except that we use\n200 queries for panoptic and instance segmentation models\nwith Swin-L backbone. All other backbones or semantic\nsegmentation models use 100 queries.\nResults. In Table VIII, we report the results of\nMask2Former obtained with various backbones on\nADE20K for three segmentation tasks and compare it\nwith other state-of-the-art methods. Mask2Former with\nSwin-L backbone sets a new state-of-the-art performance\non ADE20K for panoptic segmentation. As there are\nfew papers reporting results on ADE20K, we hope this\nexperiment could set up a useful benchmark for future\nresearch.\nB.3. Mapillary Vistas\nMapillary Vistas is a large-scale urban street-view\ndataset with 18k, 2k and 5k images for training, validation\nand testing. It contains images with a variety of resolutions,\nranging from 1024 √ó768 to 4000 √ó6000. We only report\npanoptic and semantic segmentation results for this dataset.\nTraining settings. For both panoptic and semantic segmen-\ntation, we follow the same data augmentation of [14]: stan-\ndard random scale jittering between 0.5 and 2.0, random\nhorizontal Ô¨Çipping, random cropping with a crop size of\n1024 √ó1024 as well as random color jittering. We train\n12\nmethod backbone search space epochs AP AP S APM APL APboundary #params. FLOPs\nCNN backbones\nMask R-CNN [24]\nR50 dense anchors 36 37.2 18.6 39.5 53.3 23.1 44M 201G\nR50 dense anchors 400 42.5 23.8 45.0 60.0 28.0 46M 358G\nR101 dense anchors 36 38.6 19.5 41.3 55.3 24.5 63M 266G\nR101 dense anchors 400 43.7 24.6 46.4 61.8 29.1 65M 423G\nMask2Former (ours) R50 100 queries 50 43.7 23.4 47.2 64.8 30.6 44M 226G\nR101 100 queries 50 44.2 23.8 47.7 66.7 31.1 63M 293G\nTransformer backbones\nQueryInst [20] Swin-L‚Ä† 300 queries 50 48.9 30.8 52.6 68.3 33.5 - -\nSwin-HTC++ [6, 36] Swin-B‚Ä† dense anchors 36 49.1 - - - - 160M 1043G\nSwin-L‚Ä† dense anchors 72 49.5 31.0 52.4 67.2 34.1 284M 1470G\nMask2Former (ours)\nSwin-T 100 queries 50 45.0 24.5 48.3 67.4 31.8 47M 232G\nSwin-S 100 queries 50 46.3 25.3 50.3 68.4 32.9 69M 313G\nSwin-B 100 queries 50 46.7 26.1 50.5 68.8 33.2 107M 466G\nSwin-B‚Ä† 100 queries 50 48.1 27.8 52.0 71.1 34.4 107M 466G\nSwin-L‚Ä† 200 queries 100 50.1 29.9 53.9 72.1 36.2 216M 868G\nTable III. Instance segmentation on COCO val2017 with 80 categories. Mask2Former outperforms strong Mask R-CNN [24] base-\nlines with 8√ó fewer training epochs for both AP and AP boundary [12] metrics. Our best model is also competitive to the state-of-the-art\nspecialized instance segmentation model on COCO and has higher boundary quality. For a fair comparison, we only consider single-scale\ninference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.\nmethod backbone AP AP50 AP75 APS APM APL\nQueryInst [20] Swin-L 49.1 74.2 53.8 31.5 51.8 63.2\nSwin-HTC++ [6, 36] Swin-L 50.2 - - - - -\nSwin-HTC++ [6, 36] (multi-scale) Swin-L 51.1 - - - - -\nMegvii (challenge winner) - 53.1 76.8 58.6 36.6 56.5 67.7\nMask2Former (ours) Swin-L 50.5 74.9 54.9 29.1 53.8 71.2\nTable IV.Instance segmentation on COCOtest-dev with 80 categories. Mask2Former is extremely good at segmenting large objects:\nwe can even outperform the challenge winner (which uses extra training data, model ensemble,etc.) on APL by a large margin without any\nbells-and-whistles. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint.\nour model for 300k iterations with a batch size of 16 using\nthe ‚Äúpoly‚Äù learning rate schedule [7]. During inference, we\nresize the longer side to 2048 pixels. Our panoptic segmen-\ntation model with a Swin-L backbone uses 200 queries. All\nother backbones or semantic segmentation models use 100\nqueries.\nResults. In Table IX, we report Mask2Former results\nobtained with various backbones on Mapillary Vistas for\npanoptic and semantic segmentation tasks and compare it\nwith other state-of-the-art methods. Our Mask2Former is\nvery competitive compared to state-of-the art specialized\nmodels even if it is not designed for Mapillary Vistas.\nC. Additional ablation studies\nWe perform additional ablation studies of Mask2Former\nusing the same settings that we used in the main paper: a\nsingle ResNet-50 backbone [25].\nC.1. Convergence analysis\nWe train Mask2Former with 12, 25, 50 and 100\nepochs with either standard scale augmentation (Standard\nAug.) [57] or the more recent large-scale jittering aug-\nmentation (LSJ Aug.) [18, 23]. As shown in Figure IV,\nMask2Former converges in 25 epochs using standard aug-\nmentation and almost converges in 50 epochs using large-\n(a) Visualization of cross-attention (top) and masked attention (bottom) for\ndifferent resolutions.\n1/32 1/16 1/8 average\nfg bg fg bg fg bg fg bg\ncross-attention 0.23 0.77 0.23 0.77 0.15 0.85 0.20 0.80\nmasked attention 0.53 0.47 0.61 0.39 0.64 0.36 0.59 0.41\n(b) Cumulative attention weights on foreground (fg) and background (bg)\nregions for different resolutions.\nFigure I. Masked attention analysis.\nscale jittering augmentation. This shows that Mask2Former\nwith our proposed Transformer decoder converges faster\nthan models using the standard Transformer decoder: e.g.,\nDETR [5] and MaskFormer [14] require 500 epochs and\n300 epochs respectively.\nC.2. Masked attention analysis\nWe quantitatively and qualitatively analyzed the COCO\npanoptic model with the R50 backbone. First, we visual-\n13\nmethod backbone crop size mIoU (s.s.) mIoU (m.s.) #params. FLOPs\nCNN\nMaskFormer [14] R50 512 √ó512 44.5 46.7 41M 53G\nR101 512 √ó512 45.5 47.2 60M 73G\nMask2Former (ours) R50 512 √ó512 47.2 49.2 44M 71G\nR101 512 √ó512 47.8 50.1 63M 90G\nTransformer backbones\nSwin-UperNet [36, 58] Swin-L‚Ä† 640 √ó640 - 53.5 234M 647G\nFaPN-MaskFormer [14, 39] Swin-L‚Ä† 640 √ó640 55.2 56.7 - -\nBEiT-UperNet [2, 58] BEiT-L‚Ä† 640 √ó640 - 57.0 502M -\nMaskFormer [14]\nSwin-T 512 √ó512 46.7 48.8 42M 55G\nSwin-S 512 √ó512 49.8 51.0 63M 79G\nSwin-B 640 √ó640 51.1 52.3 102M 195G\nSwin-B‚Ä† 640 √ó640 52.7 53.9 102M 195G\nSwin-L‚Ä† 640 √ó640 54.1 55.6 212M 375G\nMask2Former (ours)\nSwin-T 512 √ó512 47.7 49.6 47M 74G\nSwin-S 512 √ó512 51.3 52.4 69M 98G\nSwin-B 640 √ó640 52.4 53.7 107M 223G\nSwin-B‚Ä† 640 √ó640 53.9 55.1 107M 223G\nSwin-L‚Ä† 640 √ó640 56.1 57.3 215M 403G\nSwin-L-FaPN‚Ä† 640 √ó640 56.4 57.7 217M -\nTable V. Semantic segmentation on ADE20K val with 150 categories. Mask2Former consistently outperforms MaskFormer [14] by\na large margin with different backbones (all Mask2Former models use MSDeformAttn [66] as pixel decoder, except Swin-L-FaPN uses\nFaPN [39]). Our best model outperforms the best specialized model, BEiT [2], with less than half of the parameters. We report both\nsingle-scale (s.s.) and multi-scale (m.s.) inference results. Backbones pre-trained on ImageNet-22K are marked with ‚Ä†.\nmethod backbone P.A. mIoU score\nSETR [64] ViT-L 78.35 45.03 61.69\nSwin-UperNet [36, 58] Swin-L 78.42 47.07 62.75\nMaskFormer [14] Swin-L 79.36 49.67 64.51\nMask2Former (ours) Swin-L-FaPN 79.80 49.72 64.76\nTable VI. Semantic segmentation on ADE20K test with 150 categories. Mask2Former outperforms previous state-of-the-art methods\non all three metrics: pixel accuracy (P.A.), mIoU, as well as the Ô¨Ånal test score (average of P.A. and mIoU). We train our model on the\nunion of ADE20K train and val set with ImageNet-22K pre-trained checkpoint following [14] and use multi-scale inference.\nize the last three attention maps of our model using cross-\nattention (Figure Ia top) and masked attention (Figure Ia\nbottom) of a single query that predicts the ‚Äúcat.‚Äù With\ncross-attention, the attention map spreads over the entire\nimage and the region with highest response is outside the\nobject of interest. We believe this is because the softmax\nused in cross-attention never attains zero, and small atten-\ntion weights on large background regions start to dominate.\nInstead, masked attention limits the attention weights to fo-\ncus on the object. We validate this hypothesis in Table Ib:\nwe compute the cumulative attention weights on foreground\n(deÔ¨Åned by the matching ground truth to each prediction)\nand background for all queries on the entire COCO val\nset. On average, only 20% of the attention weights in cross-\nattention focus on the foreground while masked attention\nincreases this ratio to almost 60%. Second, we plot the\npanoptic segmentation performance using output from each\nTransformer decoder layer (Figure II). We Ô¨Ånd masked at-\ntention with a single Transformer decoder layer already out-\nperforms cross-attention with 9 layers. We hope the effec-\ntiveness of masked attention, together with this analysis,\nleads to better attention design.\n1 2 3 4 5 6 7 8 9\nTransformer decoder layer\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nPQ\ncross-attention\nmasked attention\nFigure II. Panoptic segmentation performance of each Transformer\ndecoder layer.\nC.3. Object query analysis\nObject queries play an important role in Mask2Former.\nWe ablate different design choices of object queries includ-\ning the number of queries and making queries learnable.\nNumber of queries. We study the effect of different num-\n14\npanoptic model instance model semantic model\nmethod backbone PQ (s.s.) PQ (m.s.) AP Th\npan mIoUpan AP AP50 mIoU (s.s.) mIoU (m.s.)\nPanoptic-DeepLab [11]\nR50 60.3 - 32.1 78.7 - - - -\nX71 [15] 63.0 64.1 35.3 80.5 - - - -\nSWideRNet [9] 66.4 67.5 40.1 82.2 - - - -\nPanoptic FCN [31] Swin-L‚Ä† 65.9 - - - - - - -\nSegmenter [45] ViT-L‚Ä† - - - - - - - 81.3\nSETR [64] ViT-L‚Ä† - - - - - - - 82.2\nSegFormer [59] MiT-B5 - - - - - - - 84.0\nMask2Former (ours)\nR50 62.1 - 37.3 77.5 37.4 61.9 79.4 82.2\nR101 62.4 - 37.7 78.6 38.5 63.9 80.1 81.9\nSwin-T 63.9 - 39.1 80.5 39.7 66.9 82.1 83.0\nSwin-S 64.8 - 40.7 81.8 41.8 70.4 82.6 83.6\nSwin-B‚Ä† 66.1 - 42.8 82.7 42.0 68.8 83.3 84.5\nSwin-L‚Ä† 66.6 - 43.6 82.9 43.7 71.4 83.3 84.3\nTable VII. Image segmentation results on Cityscapes val. We report both single-scale (s.s.) and multi-scale (m.s.) inference results\nfor PQ and mIoU. All other metrics are evaluated with single-scale inference. Since Mask2Former is an end-to-end model, we only use\nsingle-scale inference for instance-level segmentation tasks to avoid the need for further post-processing (e.g., NMS).\npanoptic model instance model semantic model\nmethod backbone PQ AP Th\npan mIoUpan AP AP S APM APL mIoU (s.s.) mIoU (m.s.)\nMaskFormer [14] R50 34.7 - - - - - - - -\nPanoptic-DeepLab [11] SWideRNet [9] 37.9‚àó - 50.0 ‚àó - - - - - -\nSwin-UperNet [36, 58] Swin-L‚Ä† - - - - - - - - 53.5\nMaskFormer [14] Swin-L‚Ä† - - - - - - - 54.1 55.6\nFaPN-MaskFormer [14, 39] Swin-L‚Ä† - - - - - - - 55.2 56.7\nBEiT-UperNet [2, 58] BEiT-L‚Ä† - - - - - - - - 57.0\nMask2Former (ours)\nR50 39.7 26.5 46.1 26.4 10.4 28.9 43.1 47.2 49.2\nSwin-L‚Ä† 48.1 34.2 54.5 34.9 16.3 40.0 54.7 56.1 57.3\nSwin-L-FaPN‚Ä† 46.2 33.2 55.4 33.4 14.6 37.6 54.6 56.4 57.7\nTable VIII. Image segmentation results on ADE20K val. Mask2Former is competitive to specialized models on ADE20K. Panoptic\nsegmentation models use single-scale inference by default, multi-scale numbers are marked with ‚àó. For semantic segmentation, we report\nboth single-scale (s.s.) and multi-scale (m.s.) inference results.\nber of queries for three image segmentation tasks in Ta-\nble Xa. For instance and semantic segmentation, using\n100 queries achieves the best performance, while using 200\nqueries can further improve panoptic segmentation results.\nAs panoptic segmentation is a combination of instance and\nsemantic segmentation, it has more segments per image\nthan the other two tasks. This ablation suggests that pick-\ning the number of queries for Mask2Former may depend on\nthe number of segments per image for a particular task or\ndataset.\nLearnable queries. An object query consists of two parts:\nobject query features and object query positional embed-\ndings. Object query features are only used as the initial\ninput to the Transformer decoder and are updated through\ndecoder layers; whereas query positional embeddings are\nadded to query features in every Transformer decoder layer\nwhen computing the attention weights. In DETR [5], query\nfeatures are zero-initialized and query positional embed-\ndings are learnable. Furthermore, there is no direct su-\npervision on these query features before feeding them into\nthe Transformer (since they are zero vectors). In our\nMask2Former, we still make query positional embeddings\nlearnable. In addition, we make query features learnable\nas well and directly apply losses on these learnable query\nfeatures before feeding them into the Transformer decoder.\nIn Table Xb, we compare our learnable query features\nwith zero-initialized query features in DETR. We Ô¨Ånd it\nis important to directly supervise object queries even be-\nfore feeding them into the Transformer decoder. Learnable\nqueries without supervision perform similarly well as zero-\ninitialized queries in DETR.\nC.4. MaskFormer vs. Mask2Former\nMask2Former builds upon the same meta architecture\nas MaskFormer [14] with two major differences: 1) We\nuse more advanced training parameters summarized in Ta-\nble XIa; and 2) we propose a new Transformer decoder with\nmasked attention, instead of using the standard Transformer\ndecoder, as well as some optimization improvements sum-\nmarized in Table XIb. To better understand Mask2Former‚Äôs\nimprovements over MaskFormer, we perform ablation stud-\nies on training parameter improvements and Transformer\ndecoder improvements in isolation.\nIn Table XIc, we study our new training parameters. We\n15\npanoptic model semantic model\nmethod backbone PQ mIoU pan mIoU (s.s.) mIoU (m.s.)\nPanoptic-DeepLab [11]\nensemble 42.2‚àó 58.7‚àó - -\nSWideRNet [9] 43.7 59.4 - -\nSWideRNet [9] 44.8‚àó 60.0‚àó - -\nPanoptic FCN [31] Swin-L‚Ä† 45.7 - - -\nMaskFormer [14] R50 - - 53.1 55.4\nHMSANet [48] HRNet [53] - - - 61.1\nMask2Former (ours) R50 36.3 50.7 57.4 59.0\nSwin-L‚Ä† 45.5 60.8 63.2 64.7\nTable IX.Image segmentation results on Mapillary Vistasval. Mask2Former is competitive to specialized models on Mapillary Vistas.\nPanoptic segmentation models use single-scale inference by default, multi-scale numbers are marked with ‚àó. For semantic segmentation,\nwe report both single-scale (s.s.) and multi-scale (m.s.) inference results.\n200 400 600 800\nGFLOPs\n48\n50\n52\n54\n56\n58PQ\nMaskFormer\nMask2Former (ours)\nFigure III. MaskFormer [14] vs. Mask2Former (ours) with differ-\nent Swin Transformer backbones.\ntrain the MaskFormer model with either its original train-\ning parameters in [14] or our new training parameters. We\nobserve signiÔ¨Åcant improvements of using our new training\nparameters for MaskFormer as well. This shows the new\ntraining parameters are also generally applicable to other\nmodels.\nIn Table XId, we study our new Transformer decoder.\nWe train a MaskFormer model and a Mask2Former model\nwith the exact same backbone, i.e., a ResNet-50; pixel de-\ncoder, i.e., a FPN; and training parameters. That is, the only\ndifference is in the Transformer decoder, summarized in Ta-\nble XIb. We observe improvements for all three tasks, sug-\ngesting that the new Transformer decoder itself is indeed\nbetter than the standard Transformer decoder.\nWhile computational efÔ¨Åciency was not our primary\ngoal, we Ô¨Ånd that Mask2Former actually has a better\ncompute-performance trade-off compared to MaskFormer\n(Figure III). Even the lightest instantiation of Mask2Former\noutperforms the heaviest MaskFormer instantiation, using\n1\n4\nth\nthe FLOPs.\nD. Visualization\nWe visualize sample predictions of the Mask2Former\nmodel with Swin-L [36] backbone on three tasks: COCO\npanoptic val2017 set for panoptic segmentation (57.8 PQ)\nin Figure V, COCO val2017 set for instance segmenta-\ntion (50.1 AP) in Figure VI and ADE20K validation set for\nsemantic segmentation (57.7 mIoU, multi-scale inference)\nin Figure VII.\n16\n12 25 50 100\nEpochs (log-scale)\n38\n40\n42\n44Mask AP\n Standard Aug.\nLSJ Aug.\nFigure IV. Convergence analysis. We train Mask2Former with different epochs using either standard scale augmentation (Standard\nAug.) [57] or the more recent large-scale jittering augmentation (LSJ Aug.) [18, 23]. Mask2Former converges in 25 epochs using standard\naugmentation and almost converges in 50 epochs using large-scale jittering augmentation. Using LSJ also improves performance with\nlonger training epochs (i.e., with more than 25 epochs).\nAP\n(COCO)\nPQ\n(COCO)\nmIoU\n(ADE20K)\nFLOPs\n(COCO)\n50 42.4 50.5 46.2 217G\n100 43.7 51.9 47.2 226G\n200 43.5 52.2 47.0 246G\n300 43.5 52.1 46.5 265G\n1000 40.3 50.7 44.8 405G\n(a) Number of queries ablation. For instance and semantic segmentation,\nusing 100 queries achieves the best performance while using 200 queries can\nfurther improve panoptic segmentation results.\nAP\n(COCO)\nPQ\n(COCO)\nmIoU\n(ADE20K)\nFLOPs\n(COCO)\nzero-initialized (DETR [5]) 42.9 51.2 45.5 226G\nlearnable w/o supervision 42.9 51.2 47.0 226G\nlearnable w/ supervision 43.7 51.9 47.2 226G\n(b) Learnable queries ablation. It is important to supervise object queries\nbefore feeding them into the Transformer decoder. Learnable querieswithout\nsupervision perform similarly well as zero-initialized queries in DETR.\nTable X. Analysis of object queries. Table Xa: ablation on number of queries. Table Xb: ablation on using learnable queries.\ntraining parameters MaskFormer Mask2Former (ours)\nlearning rate 0.0001 0.0001\nweight decay 0.0001 0.05\nbatch size 16‚àó 16\nepochs 75‚àó 50\ndata augmentation standard scale aug. w/ crop LSJ aug.\nŒªcls 1.0 2.0\nŒªfocal / Œªce 20.0 / - - / 5.0\nŒªdice 1.0 5.0\nmask loss mask 12544 sampled points\n(a) Comparison of training parameters for MaskFormer [14] and our Mask2Former on the COCO dataset. ‚àó: in the original MaskFormer implementation,\nthe model is trained with a batch size of 64 for 300 epochs. We Ô¨Ånd MaskFormer achieves similar performance when trained with a batch size of 16 for 75\nepochs, i.e., the same number of iterations with a smaller batch size.\nTransformer decoder MaskFormer Mask2Former (ours)\n# of layers 6 9\nsingle layer SA-CA-FFN MA-SA-FFN\ndropout 0.1 0.0\nfeature resolution {1/32}√ó6 {1/32,1/16,1/8}√ó3\ninput query features zero init. learnable\nquery p.e. learnable learnable\n(b) Comparison of Transformer decoder in MaskFormer [14] and our Mask2Former. SA: self-attention, CA: cross-attention, FFN: feed-forward network,\nMA: masked attention, p.e.: positional embedding.\nmodel training params.\nAP\n(COCO)\nPQ\n(COCO)\nmIoU\n(ADE20K)\nMaskFormer MaskFormer 34.0 46.5 44.5\nMaskFormer Mask2Former 37.8 (+3.8) 48.2 (+1.7) 45.3 (+0.8)\n(c) Improvements from better training parameters.\nTransformer decoder pixel decoder\nAP\n(COCO)\nPQ\n(COCO)\nmIoU\n(ADE20K)\nMaskFormer FPN 37.8 48.2 45.3\nMask2Former FPN 41.5 (+3.7) 50.7 (+2.5) 45.6 (+0.3)\n(d) Improvements from better Transformer decoder.\nTable XI. MaskFormer vs. Mask2Former. Table XIa and Table XIb provide an in-depth comparison between MaskFormer and our\nMask2Former settings. Table XIc: MaskFormer beneÔ¨Åts from our new training parameters as well. Table XId: Comparison between\nMaskFormer and our Mask2Former with the exact same backbone, pixel decoder and training parameters. The improvements solely come\nfrom a better Transformer decoder.\n17\nFigure V. Visualization ofpanoptic segmentation predictions on the COCO panoptic dataset: Mask2Former with Swin-L backbone which\nachieves 57.8 PQ on the validation set. First and third columns: ground truth. Second and fourth columns: prediction. Last row shows\nfailure cases.\n18\nFigure VI. Visualization ofinstance segmentation predictions on the COCO dataset: Mask2Former with Swin-L backbone which achieves\n50.1 AP on the validation set. First and third columns: ground truth. Second and fourth columns: prediction. Last row shows failure\ncases. We show predictions with conÔ¨Ådence scores greater than 0.5.\n19\nFigure VII. Visualization of semantic segmentation predictions on the ADE20K dataset: Mask2Former with Swin-L backbone which\nachieves 57.7 mIoU (multi-scale) on the validation set. First and third columns: ground truth. Second and fourth columns: prediction.\nLast row shows failure cases.\n20",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7996981143951416
    },
    {
      "name": "Computer science",
      "score": 0.7117925882339478
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.7106553316116333
    },
    {
      "name": "Transformer",
      "score": 0.6328436136245728
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6147139072418213
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.5966380834579468
    },
    {
      "name": "Image segmentation",
      "score": 0.4815114438533783
    },
    {
      "name": "Task (project management)",
      "score": 0.4353540539741516
    },
    {
      "name": "Pixel",
      "score": 0.4264739155769348
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3585442900657654
    },
    {
      "name": "Natural language processing",
      "score": 0.3485080599784851
    },
    {
      "name": "Computer vision",
      "score": 0.34547221660614014
    },
    {
      "name": "Machine learning",
      "score": 0.1858566701412201
    },
    {
      "name": "Engineering",
      "score": 0.07547160983085632
    },
    {
      "name": "Programming language",
      "score": 0.05932527780532837
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}