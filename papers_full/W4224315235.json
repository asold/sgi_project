{
  "title": "Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations",
  "url": "https://openalex.org/W4224315235",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103641016",
      "name": "Meng Yu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A1901863177",
      "name": "Zhang, Yunyi",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2054901068",
      "name": "Huang Jia-Xin",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2102355683",
      "name": "Zhang Yu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2125747881",
      "name": "Han, Jiawei",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964117810",
    "https://openalex.org/W3176380929",
    "https://openalex.org/W2128925311",
    "https://openalex.org/W2072644219",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2807056933",
    "https://openalex.org/W3104613320",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3170611235",
    "https://openalex.org/W3100474067",
    "https://openalex.org/W3081051539",
    "https://openalex.org/W2106490775",
    "https://openalex.org/W2238728730",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W3004119480",
    "https://openalex.org/W2890931111",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W3042602466",
    "https://openalex.org/W2963631950",
    "https://openalex.org/W1969486090",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2167232041",
    "https://openalex.org/W3035332461",
    "https://openalex.org/W2169606435",
    "https://openalex.org/W3214566704",
    "https://openalex.org/W3101606352",
    "https://openalex.org/W3104217184",
    "https://openalex.org/W3105538385",
    "https://openalex.org/W3099045991"
  ],
  "abstract": "Topic models have been the prominent tools for automatic topic discovery from\\ntext corpora. Despite their effectiveness, topic models suffer from several\\nlimitations including the inability of modeling word ordering information in\\ndocuments, the difficulty of incorporating external linguistic knowledge, and\\nthe lack of both accurate and efficient inference methods for approximating the\\nintractable posterior. Recently, pretrained language models (PLMs) have brought\\nastonishing performance improvements to a wide variety of tasks due to their\\nsuperior representations of text. Interestingly, there have not been standard\\napproaches to deploy PLMs for topic discovery as better alternatives to topic\\nmodels. In this paper, we begin by analyzing the challenges of using PLM\\nrepresentations for topic discovery, and then propose a joint latent space\\nlearning and clustering framework built upon PLM embeddings. In the latent\\nspace, topic-word and document-topic distributions are jointly modeled so that\\nthe discovered topics can be interpreted by coherent and distinctive terms and\\nmeanwhile serve as meaningful summaries of the documents. Our model effectively\\nleverages the strong representation power and superb linguistic features\\nbrought by PLMs for topic discovery, and is conceptually simpler than topic\\nmodels. On two benchmark datasets in different domains, our model generates\\nsignificantly more coherent and diverse topics than strong topic models, and\\noffers better topic-wise document representations, based on both automatic and\\nhuman evaluations.\\n",
  "full_text": "Topic Discovery via Latent Space Clustering of Pretrained\nLanguage Model Representations\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Jiawei Han\nDepartment of Computer Science, University of Illinois at Urbana-Champaign, IL, USA\n{yumeng5, yzhan238, jiaxinh3, yuz9, hanj}@illinois.edu\nABSTRACT\nTopic models have been the prominent tools for automatic topic\ndiscovery from text corpora. Despite their effectiveness, topic mod-\nels suffer from several limitations including the inability of mod-\neling word ordering information in documents, the difficulty of\nincorporating external linguistic knowledge, and the lack of both\naccurate and efficient inference methods for approximating the\nintractable posterior. Recently, pretrained language models (PLMs)\nhave brought astonishing performance improvements to a wide\nvariety of tasks due to their superior representations of text. Inter-\nestingly, there have not been standard approaches to deploy PLMs\nfor topic discovery as better alternatives to topic models. In this\npaper, we begin by analyzing the challenges of using PLM repre-\nsentations for topic discovery, and then propose a joint latent space\nlearning and clustering framework built upon PLM embeddings. In\nthe latent space, topic-word and document-topic distributions are\njointly modeled so that the discovered topics can be interpreted by\ncoherent and distinctive terms and meanwhile serve as meaning-\nful summaries of the documents. Our model effectively leverages\nthe strong representation power and superb linguistic features\nbrought by PLMs for topic discovery, and is conceptually simpler\nthan topic models. On two benchmark datasets in different domains,\nour model generates significantly more coherent and diverse topics\nthan strong topic models, and offers better topic-wise document\nrepresentations, based on both automatic and human evaluations.1\nCCS CONCEPTS\n‚Ä¢ Information systems ‚ÜíClustering; Document topic mod-\nels; ‚Ä¢ Computing methodologies ‚ÜíNatural language pro-\ncessing.\nKEYWORDS\nTopic Discovery, Pretrained Language Models, Clustering\nACM Reference Format:\nYu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Jiawei Han. 2022. Topic\nDiscovery via Latent Space Clustering of Pretrained Language Model Rep-\nresentations. In Proceedings of the ACM Web Conference 2022 (WWW ‚Äô22),\nApril 25‚Äì29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3485447.3512034\n1Code and data can be found at https://github.com/yumeng5/TopClus.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWWW ‚Äô22, April 25‚Äì29, 2022, Virtual Event, Lyon, France\n¬© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9096-5/22/04. . . $15.00\nhttps://doi.org/10.1145/3485447.3512034\n1 INTRODUCTION\nAutomatically discovering coherent and meaningful topics from\ntext corpora is intuitively appealing for web-scale content anal-\nyses, as it facilitates many web applications including document\nanalysis [9], text summarization [63] and ad-hoc information re-\ntrieval [65]. Decades of research efforts have been dedicated to the\ndevelopment of such algorithms, among which topic models [11, 26]\nare the most prominent methods. The success of topic models can\nbe largely credited to their proposed generative process: By max-\nimizing the likelihood of a probabilistic process that models how\ndocuments are generated conditioned on the hidden topics, topic\nmodels are able to uncover the latent topic structures in the corpus.\nDespite the success of topic models, the generative process in-\ncurs several notable limitations: (1) The ‚Äúbag-of-words‚Äù generative\nassumption completely ignores word ordering information in text,\nwhich is essential for defining word meanings [18]. (2) The gener-\native process cannot leverage external knowledge to learn word\nsemantics, which may miss important topic-indicating words if they\nare not sufficiently reflected by the co-occurrence statistics of the\ngiven corpus, as is likely the case for small-scale/short-text corpora.\n(3) The generative process induces an intractable posterior that re-\nquires approximation algorithms like Monte Carlo simulation [50]\nor variational inference [1]. Unfortunately, there is always a trade-\noff between accuracy and efficiency with these approximations\nsince they can only be asymptotically exact [ 57]. Later variants\nof topic models attempt to overcome some of these limitations by\neither replacing the analytic approximation of the posterior with\ndeep neural networks [47, 60, 64] to improve the effectiveness and\nefficiency of the inference process, or incorporating word embed-\ndings [15, 17, 52] to make up for the representation deficiency of\nthe ‚Äúbag-of-words‚Äù generative assumption. Nevertheless, without\nfundamental changes of the topic modeling framework, none of\nthese approaches address the limitations of topic models all at once.\nAlong another line of text representation learning research, text\nembeddings have achieved enormous success in a wide spectrum of\ndownstream tasks. The effectiveness of text embeddings stems from\nthe learning of distributed representations of words and documents\nfrom contexts. Early models like Word2Vec [48] learn context-free\nword semantics based on a local context window of the center\nword. Recently, pretrained language models (PLMs) like BERT [16],\nRoBERTa [36] and XLNet [ 67] have revolutionized text process-\ning via learning contextualized word embeddings. They employ\nTransformer [62] as the backbone architecture for capturing the\nlong-range, high-order semantic dependency in text sequences,\nyielding superior representations to previous context-free embed-\ndings. Since these PLMs are pretrained on large-scale text corpora\nlike Wikipedia, they carry superb linguistic features that can be\ngeneralized to almost any text-related applications.\nMotivated by the strong representation power of the contextu-\nalized embeddings that accurately capture word semantics, a few\narXiv:2202.04582v1  [cs.CL]  9 Feb 2022\nrecent studies have attempted to utilize PLMs for topic discovery.\nSia et al. [59] directly cluster averaged BERT word embeddings to\nobtain word clusters as topics. The resulting topic quality relies\nsignificantly on heuristic tricks like frequency-based weighting/re-\nranking and barely reaches the performance of LDA, the most basic\ntopic model. Instead of clustering word embeddings, BERTopic [23]\nclusters document embeddings and then uses TF-IDF metrics to\nextract representative terms from each notable document cluster\nas topics. However, as the document embeddings in BERTopic are\nobtained from Sentence-BERT [ 56], which is trained on natural\nlanguage inference datasets with manually annotated sentence la-\nbels, the performance of BERTopic may suffer from domain shift\nwhen the target corpus is semantically different from the Sentence-\nBERT training set, and when manually annotated labels for re-\ntraining the sentence embeddings are absent. Moreover, BERTopic\nconstructs topics via TF-IDF metrics and fails to take advantage of\nthe distributed representations of PLMs, which are known to better\ncapture word semantics than frequency-based statistics.\nIn this work, we study topic discovery with PLM embeddings as a\npotential alternative to topic models. We first analyze the challenges\nof directly operating on the PLM embedding space by investigating\nits structure. Motivated by the challenges, we propose TopClus, a\njoint latent space learning and clustering approach that derives\na lower-dimensional, spherical latent embedding space with topic\nstructures. Such latent space mitigates the ‚Äúcurse of dimensionality‚Äù\nissue and uses angular similarity to model semantic correlations\namong words, documents and topics, thus is better suited for cluster-\ning than the high-dimensional Euclidean embedding space of PLMs.\nUnlike traditional clustering algorithms that work with fixed data\nrepresentations, TopClus jointly adjusts the latent space represen-\ntations and performs clustering. Topic-word and document-topic\ndistributions are jointly modeled in the latent space to derive topics\nthat (1) are interpretable by coherent and distinctive words and (2)\nserve as meaningful summaries of documents.\nTopClus enjoys the following advantages over topic models: (1)\nTopClus works with PLM contextualized embeddings obtained by\nmodeling the entire text sequences with positional information,\nwhich are expected to provide better representations than the ‚Äúbag-\nof-words‚Äù assumption of topic models. (2) TopClus employs PLMs\nto bring in general linguistic knowledge which helps generate more\naccurate and stable word representations on the target corpus than\ntraining topic models from scratch on it. (3) The training algorithm\nof TopClus does not involve any probabilistic approximations, and\nis computationally and conceptually simpler than variational infer-\nence in topic models. With these advantageous properties, TopClus\nsimultaneously addresses the major limitations of topic models.\nOur contributions are summarized as follows:\n(1) We explore using PLM embeddings for topic discovery. We first\nidentify the challenges with an in-depth analysis of the original\nPLM embedding space‚Äôs structure.\n(2) We propose a new framework TopClus which jointly learns a\nlower-dimensional, spherical latent space with cluster struc-\ntures based on word and document embeddings from PLMs.\nHigh-quality topic clusters are derived by simultaneously mod-\neling topic-word and document-topic distributions.TopCluscan\nbe integrated with any PLMs for unsupervised topic discovery.\n(3) We propose three objectives for training TopClus to induce\ndistinctive and balanced cluster structures in the latent space\nwhich result in diverse and coherent topics.\n‚ñ°100 0 100\n‚ñ°100\n0\n100\n(a) New York Times .\n‚ñ°100 0 100\n‚ñ°100\n0\n100 (b) Yelp Review.\nFigure 1: Visualization using t-SNE of 3,000 randomly sam-\npled contextualized word embeddings of BERT on (a) NYT\nand (b) Yelp datasets, respectively. The embedding spaces do\nnot have clearly separated clusters.\n(4) We evaluate TopClus on two benchmark datasets in different\ndomains. TopClus significantly outperforms strong topic dis-\ncovery methods by generating more coherent and diverse topics\nand providing better document topic representations judged\nfrom both automatic and human evaluations.\n2 CHALLENGES OF TOPIC DISCOVERY\nWITH PRETRAINED LANGUAGE MODELS\nWe first identify three major challenges of using PLM embeddings\nfor topic discovery, which motivate our proposed model in Section 3.\nUnsuitability of PLM Embedding Space for Clustering. One\nstraightforward way of obtaining ùêæ topics with PLM embeddings\n(e.g., from BERT [16]) is to simply apply clustering algorithms like\nùêæ-means [37] to group correlated terms that form topics. To pro-\nvide empirical evidence that such direct clustering may not work\nwell, we visualize 3,000 randomly sampled contextualized word\nembeddings obtained by running BERT on the New York Times and\nYelp Review datasets in Figure 1. The embedding spaces do not ex-\nhibit clearly separated clusters, and applying clustering algorithms\nlike ùêæ-means with a typical ùêæ (e.g., ùêæ = 100) to these spaces leads\nto low-quality and unstable clusters. We show theoretically that\nsuch a phenomenon is due to too many clusters in the embedding\nspace. Below, we study the effect of the Masked Language Modeling\n(MLM) pretraining objective of BERT on the embedding space.\nTheorem 2.1. The MLM pretraining objective of BERT assumes\nthat the learned contextualized embeddings are generated from a\nGaussian Mixture Model (GMM) with |ùëâ|mixture components where\n|ùëâ|is the vocabulary size of BERT.\nProof. See Appendix B. ‚ñ°\nTheorem 2.1 applies to many PLMs (e.g., BERT [16], RoBERTa [35],\nXLNet [67]) that use MLM-like pretraining objectives. It reveals that\nthe optimal number of cluster ùêæ to apply ùêæ-means like algorithm\nis |ùëâ|(|ùëâ|‚âà 30,000 in the BERT base model). In other words, the\nPLM embedding space is partitioned into extremely fine-grained\nclusters and lacks topic structures inherently. If a typicalùêæfor topic\ndiscovery is used (ùêæ ‚â™|ùëâ|), the partition will not fit the original\ndata well, resulting in unstable and low-quality clusters. If a very\nbig ùêæ is used (ùêæ ‚âà|ùëâ|), most clusters will contain only one unique\nterm, which is meaningless for topic discovery.\nCurse of Dimensionality. PLM embeddings are usually high-\ndimensional (e.g., number of dimensions ùëü = 768 in the BERT\nbase model), while distance functions can become meaningless and\nunreliable in high-dimensional spaces [5], rendering Euclidean dis-\ntance based clustering algorithms ineffective for high-dimensional\ncases, known as the ‚Äúcurse of dimensionality‚Äù. From another per-\nspective, the high-dimensional PLM embeddings encode linguistic\ninformation of multiple aspects for the generic language modeling\npurpose, but some features are not necessary for or may even in-\nterfere with topic discovery. For example, some syntactic features\nin the PLM embeddings should not be considered when grouping\nsemantically similar concepts ( e.g., ‚Äúplay‚Äù, ‚Äúplays‚Äù and ‚Äúplaying‚Äù\nshould not represent different topics).\nLack of Good Document Representations from PLMs. Topic\ndiscovery usually requires jointly modeling documents with words\nto derive latent topics. Although PLMs are famous for their supe-\nrior contextualized word representations, obtaining quality docu-\nment embeddings from PLMs has been a big challenge. Sentence-\nBERT [56] reports that the inherent BERT sequence embeddings (i.e.,\nobtained from the [CLS] token) are of rather bad quality without\nfine-tuning, even worse than averaged GloVe context-free embed-\ndings. To obtain meaningful sentence embeddings, Sentence-BERT\nfine-tunes pretrained BERT model on natural language inference\n(NLI) tasks with manually annotated sentences. However, using\nSentence-BERT for topic discovery raises two concerns: (1) When\nthe given corpus has a big domain shift from the Sentence-BERT\ntraining set (e.g., the documents are much longer than the sentences\nin NLI, or are very different semantically from the NLI dataset), the\ndocument embeddings need to be re-trained from target corpus doc-\nument labels, which contradicts the unsupervised nature of topic\ndiscovery. (2) The sentence embeddings are in a different space\nfrom word embeddings as they are not jointly trained, and cannot\nbe simultaneously used to model both words and documents. This\nis why BERTopic [23] relies on TF-IDF for topic word selection.\n3 METHOD\nWe first introduce the two major components in ourTopClus model:\n(1) attention-based document embedding learning module and (2)\nlatent space generative module, and then we introduce three train-\ning objectives for model learning. Figure 2 provides an overview of\nTopClus. We assume BERT is used as the PLM, but TopClus can be\nseamlessly integrated with any other PLMs.\n3.1 Attention-Based Document Embeddings\nAs the prerequisite of topic discovery is the joint modeling of words\nand documents, we first propose a simple attention mechanism to\nlearn document embeddings. Previous studies [33] show that a sim-\nple average of word embeddings from PLMs can serve as decent\ngeneric sequence representations. In this work, we assume that\nnot all words in a document are equally topic-indicative, so we\nlearn attention weights of each token to derive document embed-\ndings as a weighted average of contextualized word embeddings\nwhich are expected to be better tailored for topic discovery than\nan unweighted average of word embeddings. This also allows the\nlearned document embeddings to share the same space with word\nembeddings which enables joint modeling of words and documents.\nFor each text document ùíÖ = [ùë§1,ùë§2,...,ùë§ ùëõ], we obtain the\nBERT contextualized word representations [ùíâ(ùë§)\n1 ,ùíâ(ùë§)\n2 ,..., ùíâ(ùë§)\nùëõ ]\nwhere ùíâ(ùë§)\nùëñ ‚ààRùëü (ùëü = 768 in the BERT base model). The attention\nweights ùú∂ = [ùõº1,ùõº2,...,ùõº ùëõ]are learned for each token as follows:\nùíçùëñ = tanh\n\u0010\nùëæùíâ (ùë§)\nùëñ +ùíÉ\n\u0011\n, ùõº ùëñ =\nexp(ùíç‚ä§\nùëñ ùíó)\n√çùëõ\nùëó=1 exp(ùíç‚ä§\nùëó ùíó),\nwhere ùëæ and ùíÉ are learnable parameters of a linear layer with the\ntanh(¬∑)activation. Each word embedding ùíâ(ùë§)\nùëñ is transformed to a\nnew representation ùíçùëñ whose dot product with another learnable\nvector ùíó reflects how topic-indicative the token is. Finally, the doc-\nument embedding ùíâ(ùëë)is obtained as the combination of all word\nembeddings in the document weighted by the attention values:\nùíâ(ùëë)=\nùëõ‚àëÔ∏Å\nùëñ=1\nùõºùëñùíâ(ùë§)\nùëñ .\nWe note that the contextualized word embeddings from BERT\n{ùíâ(ùë§)\nùëñ }ùëõ\nùëñ=1 are not updated during topic discovery since they al-\nready capture word semantics reliably and accurately through pre-\ntraining. The learnable parameters associated with the attention\nmechanism ùë® = {ùëæ,ùíÉ,ùíó}are randomly initialized and trained via\nthe unsupervised objectives to be introduced in Section 3.3.\n3.2 The Latent Space Generative Model\nMotivation and Assumptions. As we have shown in Section 2,\nthe original embedding spaceùëØ of PLMs is unsuitable for direct clus-\ntering to generate topic clusters. To address the challenges, we pro-\npose to project the original embedding spaceùëØ into a latent spaceùíÅ\nwith ùêæsoft clusters of words corresponding toùêælatent topics. We as-\nsume that ùíÅ is spherical (i.e., ùíÅ ‚äÇSùëü‚Ä≤‚àí1; Sùëü‚Ä≤‚àí1 = {ùíõ ‚ààRùëü‚Ä≤\n: ‚à•ùíõ‚à•= 1}\nis the unit ùëü‚Ä≤‚àí1 sphere) and lower-dimensional (i.e., ùëü‚Ä≤< ùëü). Such a\nlatent space has the following preferable properties: (1) In the spher-\nical latent space, angular similarity (i.e., without considering vector\nnorms) between vectors is employed to capture word semantic cor-\nrelations, which works better than Euclidean metrics (e.g., cosine\nsimilarity between embeddings is more effective for measuring\nword similarity [40, 46]). (2) The lower-dimensional space mitigates\nthe ‚Äúcurse of dimensionality‚Äù of the original high-dimensional space\nand better suits the clustering task. (3) Projecting high-dimensional\nembeddings to the lower-dimensional space forces the model to\ndiscard the information that does not help form topic clusters (e.g.,\nsyntactic features).\nWhy Not Naive Approach? A straightforward way is to first\napply a dimensionality reduction technique to the original embed-\nding space ùëØ to obtain the aforementioned latent space ùíÅ, and\nsubsequently apply clustering algorithms to ùíÅ for obtaining the\nlatent space clusters representing topics. However, such a naive ap-\nproach cannot guarantee that the reduced-dimension embeddings\nwill be naturally suited for clustering, given that no clustering-\npromoting objective is incorporated in the dimensionality reduc-\ntion step. Therefore, we propose to jointly learn the latent space\nprojection and cluster in the latent space instead of conducting\nthem one after another, so that the latent representation learning is\nguided by the clustering objective, and the cluster quality benefits\nfrom the well-separated structure of the latent space, achieving a\nmutually-enhanced effect. Such joint learning is realized by training\na generative model that connects the latent topic structure with\nthe original space representations.\nOur Generative Model. We introduce our latent space generative\nmodel as follows. With the number of topics ùêæ as the input to the\nmodel, we assume that there exists a latent space ùíÅ ‚äÇSùëü‚Ä≤‚àí1 with\nùêæ topics reflecting the latent structure of the original embedding\nspace ùëØ. Each topic is associated with a spherical distribution called\nthe von Mises-Fisher (vMF) distribution [2, 21] that characterizes\nthe topic-word and document-topic distributions in the latent space.\n<latexit sha1_base64=\"LOO5MZ4zcVOH6H28v7ADoSbJEyc=\">AAACDHicbVDLSsNAFJ3UV61Vqy4FGSyCq5IUUXFVcNNlC/aBTSiTyaQdOsmEmYlQQpYu3PgrblxUxK0f4M5v8CectF3U1gvDHM65l3vPcSNGpTLNbyO3tr6xuZXfLuwUd/f2SweHbcljgUkLc8ZF10WSMBqSlqKKkW4kCApcRjru6DbTOw9ESMrDOzWOiBOgQUh9ipHSVL9U9m9slzNPjgP9JfUU2orDReo+1V1mxZwWXAXWHJRrxUnz5/Fk0uiXvmyP4zggocIMSdmzzEg5CRKKYkbSgh1LEiE8QgPS0zBEAZFOMjWTwjPNeNDnQr9QwSm7OJGgQGan6c4AqaFc1jLyP60XK//aSWgYxYqEeLbIjxnUfrNkoEcFwYqNNUBYUH0rxEMkEFY6v4IOwVq2vAra1Yp1Wblo6jSqYFZ5cAxOwTmwwBWogTpogBbA4Am8gAl4M56NV+Pd+Ji15oz5zBH4U8bnL0xkn3A=</latexit>\nf : H ! Z\n<latexit sha1_base64=\"LOO5MZ4zcVOH6H28v7ADoSbJEyc=\">AAACDHicbVDLSsNAFJ3UV61Vqy4FGSyCq5IUUXFVcNNlC/aBTSiTyaQdOsmEmYlQQpYu3PgrblxUxK0f4M5v8CectF3U1gvDHM65l3vPcSNGpTLNbyO3tr6xuZXfLuwUd/f2SweHbcljgUkLc8ZF10WSMBqSlqKKkW4kCApcRjru6DbTOw9ESMrDOzWOiBOgQUh9ipHSVL9U9m9slzNPjgP9JfUU2orDReo+1V1mxZwWXAXWHJRrxUnz5/Fk0uiXvmyP4zggocIMSdmzzEg5CRKKYkbSgh1LEiE8QgPS0zBEAZFOMjWTwjPNeNDnQr9QwSm7OJGgQGan6c4AqaFc1jLyP60XK//aSWgYxYqEeLbIjxnUfrNkoEcFwYqNNUBYUH0rxEMkEFY6v4IOwVq2vAra1Yp1Wblo6jSqYFZ5cAxOwTmwwBWogTpogBbA4Am8gAl4M56NV+Pd+Ji15oz5zBH4U8bnL0xkn3A=</latexit>\nf : H ! Z\nWord & Document\nEmbeddings\nAttention-Based \nDocument Embeddings\nOriginal Space \n<latexit sha1_base64=\"qhtvaEHdSwdVQVBHbs2UCuFlquY=\">AAAB9XicbVDNSgMxGPy2/tX6V/XoJVgET2VXKnoseOmxgm2Fdi3ZbLYNzSZLklXK0vfw4kERr76LN9/GbLsHbR0IGWa+j0wmSDjTxnW/ndLa+sbmVnm7srO7t39QPTzqapkqQjtEcqnuA6wpZ4J2DDOc3ieK4jjgtBdMbnK/90iVZlLcmWlC/RiPBIsYwcZKD4NA8lBPY3tlrdmwWnPr7hxolXgFqUGB9rD6NQglSWMqDOFY677nJsbPsDKMcDqrDFJNE0wmeET7lgocU+1n89QzdGaVEEVS2SMMmqu/NzIc6zyanYyxGetlLxf/8/qpia79jIkkNVSQxUNRypGRKK8AhUxRYvjUEkwUs1kRGWOFibFFVWwJ3vKXV0n3ou5d1t3bRq3ZKOoowwmcwjl4cAVNaEEbOkBAwTO8wpvz5Lw4787HYrTkFDvH8AfO5w8HeJLP</latexit>\nH\n<latexit sha1_base64=\"qhtvaEHdSwdVQVBHbs2UCuFlquY=\">AAAB9XicbVDNSgMxGPy2/tX6V/XoJVgET2VXKnoseOmxgm2Fdi3ZbLYNzSZLklXK0vfw4kERr76LN9/GbLsHbR0IGWa+j0wmSDjTxnW/ndLa+sbmVnm7srO7t39QPTzqapkqQjtEcqnuA6wpZ4J2DDOc3ieK4jjgtBdMbnK/90iVZlLcmWlC/RiPBIsYwcZKD4NA8lBPY3tlrdmwWnPr7hxolXgFqUGB9rD6NQglSWMqDOFY677nJsbPsDKMcDqrDFJNE0wmeET7lgocU+1n89QzdGaVEEVS2SMMmqu/NzIc6zyanYyxGetlLxf/8/qpia79jIkkNVSQxUNRypGRKK8AhUxRYvjUEkwUs1kRGWOFibFFVWwJ3vKXV0n3ou5d1t3bRq3ZKOoowwmcwjl4cAVNaEEbOkBAwTO8wpvz5Lw4787HYrTkFDvH8AfO5w8HeJLP</latexit>\nH\n<latexit sha1_base64=\"qgwquz0JqI+6ceV9z1rc67fxhwM=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwVNLSzu428OJxgpuDrZQ0y7aw9IMkHYyy/8SLB0W8+p94878x3Sao6IOQx3u/H3l5UcaZVAh9GJWNza3tnepubW//4PDIPD7pyjQXhHZIylPRi7CknCW0o5jitJcJiuOI0/toel369zMqJEuTOzXPaBDjccJGjGClpdA0B1HKh3Ie66tQi9ANzTqymn7D9j2ILOQ7yHU08exG03OgbaEl6mCNdmi+D4YpyWOaKMKxlH0bZSoosFCMcLqoDXJJM0ymeEz7miY4pjIolskX8EIrQzhKhT6Jgkv1+0aBY1mG05MxVhP52yvFv7x+rkZ+ULAkyxVNyOqhUc6hSmFZAxwyQYnic00wEUxnhWSCBSZKl1XTJXz9FP5Puo5lexa6destd11HFZyBc3AJbHAFWuAGtEEHEDADD+AJPBuF8Wi8GK+r0Yqx3jkFP2C8fQJtB5Qk</latexit>\nt 4\n<latexit sha1_base64=\"VKk51risQeMi2DL6t2q9H8TffqU=\">AAAB+XicdVBPS8MwHE3nvzn/VT16CQ7BU0nnOudt4MXjBOcGWylpmm1haVqSdDDKvokXD4p49Zt489uYbhNU9EHI473fj7y8MOVMaYQ+rNLa+sbmVnm7srO7t39gHx7dqySThHZIwhPZC7GinAna0Uxz2kslxXHIaTecXBd+d0qlYom407OU+jEeCTZkBGsjBbY9CBMeqVlsrlzPAzewq8hBjQa6qEPkeKjmXSFDEGo0vRp0DSlQBSu0A/t9ECUki6nQhGOl+i5KtZ9jqRnhdF4ZZIqmmEzwiPYNFTimys8XyefwzCgRHCbSHKHhQv2+keNYFeHMZIz1WP32CvEvr5/pYdPPmUgzTQVZPjTMONQJLGqAEZOUaD4zBBPJTFZIxlhiok1ZFVPC10/h/+S+5rieg27r1VZ9VUcZnIBTcA5ccAla4Aa0QQcQMAUP4Ak8W7n1aL1Yr8vRkrXaOQY/YL19AksFlA0=</latexit>\nt 1\n<latexit sha1_base64=\"PxTpiLA0oomDfPnADLr7ABmN7qc=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwNNoxdbsNvHic4D5gKyVN0y0sTUqSDkbZf+LFgyJe/U+8+d+YbhWm6IOQx3u/H3l5QcKo0o7zaZU2Nre2d8q7lb39g8Mj+/ikp0QqMeliwYQcBEgRRjnpaqoZGSSSoDhgpB9Mb3O/PyNSUcEf9DwhXozGnEYUI20k37ZHgWChmsfmyvTCr/t21ak5S8A10mo1nes6dAulCgp0fPtjFAqcxoRrzJBSQ9dJtJchqSlmZFEZpYokCE/RmAwN5SgmysuWyRfwwighjIQ0h2u4VNc3MhSrPJyZjJGeqN9eLv7lDVMdNb2M8iTVhOPVQ1HKoBYwrwGGVBKs2dwQhCU1WSGeIImwNmVVTAnfP4X/k1695l7VnPtGtd0o6iiDM3AOLoELbkAb3IEO6AIMZuARPIMXK7OerFfrbTVasoqdU/AD1vsXJIKT8w==</latexit>\nt 2\n<latexit sha1_base64=\"oeONZ69isdrDXjrJXdmkSogD8tQ=\">AAAB+XicdVBPS8MwHE3nvzn/VT16CQ7BU2ndpttt4MXjBOcGWylpmm5haVKSdDDKvokXD4p49Zt489uYbhNU9EHI473fj7y8MGVUadf9sEpr6xubW+Xtys7u3v6BfXh0r0QmMeliwYTsh0gRRjnpaqoZ6aeSoCRkpBdOrgu/NyVSUcHv9CwlfoJGnMYUI22kwLaHoWCRmiXmyvU8qAV21XUarkENuo67gCGtVrNWv4TeSqmCFTqB/T6MBM4SwjVmSKmB56baz5HUFDMyrwwzRVKEJ2hEBoZylBDl54vkc3hmlAjGQprDNVyo3zdylKginJlMkB6r314h/uUNMh03/ZzyNNOE4+VDccagFrCoAUZUEqzZzBCEJTVZIR4jibA2ZVVMCV8/hf+T+wvHazjubb3arq/qKIMTcArOgQeuQBvcgA7oAgym4AE8gWcrtx6tF+t1OVqyVjvH4Aest0847pQB</latexit>\nt 3\n<latexit sha1_base64=\"X75KeoG9I341wUuDGB4IcBN9d1I=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwVNJ9MHcbePE4wc3BVkqapltYmpYkHYyy/8SLB0W8+p94878x3Sao6IOQx3u/H3l5QcqZ0gh9WKWNza3tnfJuZW//4PDIPj7pqySThPZIwhM5CLCinAna00xzOkglxXHA6X0wvS78+xmViiXiTs9T6sV4LFjECNZG8m17FCQ8VPPYXLle+E3friKn3UYN1ILIaSLk1tuGoHrdMOg6aIkqWKPr2++jMCFZTIUmHCs1dFGqvRxLzQini8ooUzTFZIrHdGiowDFVXr5MvoAXRglhlEhzhIZL9ftGjmNVhDOTMdYT9dsrxL+8YaajKy9nIs00FWT1UJRxqBNY1ABDJinRfG4IJpKZrJBMsMREm7IqpoSvn8L/Sb/muE0H3Taqndq6jjI4A+fgErigBTrgBnRBDxAwAw/gCTxbufVovVivq9GStd45BT9gvX0CS3+UCw==</latexit>\nt 5\nLatent Spherical Space \n<latexit sha1_base64=\"rrfytvYVGyoononeapkrmDy7P3Y=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIosuCG5cV7APbsWQymTY0kwxJRilD/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Ovc7j1RpJsWdmSTUj/FQsIgRbKz00A8kD/Uktld2Px1Ua27dnQEtE68gNSjQHFS/+qEkaUyFIRxr3fPcxPgZVoYRTqeVfqppgskYD2nPUoFjqv1slnqKTqwSokgqe4RBM/X3RoZjnUezkzE2I73o5eJ/Xi810ZWfMZGkhgoyfyhKOTIS5RWgkClKDJ9YgoliNisiI6wwMbaoii3BW/zyMmmf1b2Lunt7XmucF3WU4QiO4RQ8uIQG3EATWkBAwTO8wpvz5Lw4787HfLTkFDuH8AfO5w8i0pLh</latexit>\nZ\n<latexit sha1_base64=\"rrfytvYVGyoononeapkrmDy7P3Y=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIosuCG5cV7APbsWQymTY0kwxJRilD/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Ovc7j1RpJsWdmSTUj/FQsIgRbKz00A8kD/Uktld2Px1Ua27dnQEtE68gNSjQHFS/+qEkaUyFIRxr3fPcxPgZVoYRTqeVfqppgskYD2nPUoFjqv1slnqKTqwSokgqe4RBM/X3RoZjnUezkzE2I73o5eJ/Xi810ZWfMZGkhgoyfyhKOTIS5RWgkClKDJ9YgoliNisiI6wwMbaoii3BW/zyMmmf1b2Lunt7XmucF3WU4QiO4RQ8uIQG3EATWkBAwTO8wpvz5Lw4787HfLTkFDuH8AfO5w8i0pLh</latexit>\nZ  with \n<latexit sha1_base64=\"/v1yl+2T2s+qpV0nAswg02/gQKI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseCF8FLC7YW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0PBh7vzTAzL0gE18Z1v53C2vrG5lZxu7Szu7d/UD48aus4VQxbLBax6gRUo+ASW4YbgZ1EIY0CgQ/B+GbmPzyh0jyW92aSoB/RoeQhZ9RYqXnXL1fcqjsHWSVeTiqQo9Evf/UGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa6mkEWo/mx86JWdWGZAwVrakIXP190RGI60nUWA7I2pGetmbif953dSE137GZZIalGyxKEwFMTGZfU0GXCEzYmIJZYrbWwkbUUWZsdmUbAje8surpH1R9S6rbrNWqdfyOIpwAqdwDh5cQR1uoQEtYIDwDK/w5jw6L86787FoLTj5zDH8gfP5A58hjMU=</latexit>\nK\n<latexit sha1_base64=\"/v1yl+2T2s+qpV0nAswg02/gQKI=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseCF8FLC7YW2lA220m7drMJuxuhhP4CLx4U8epP8ua/cdvmoK0PBh7vzTAzL0gE18Z1v53C2vrG5lZxu7Szu7d/UD48aus4VQxbLBax6gRUo+ASW4YbgZ1EIY0CgQ/B+GbmPzyh0jyW92aSoB/RoeQhZ9RYqXnXL1fcqjsHWSVeTiqQo9Evf/UGMUsjlIYJqnXXcxPjZ1QZzgROS71UY0LZmA6xa6mkEWo/mx86JWdWGZAwVrakIXP190RGI60nUWA7I2pGetmbif953dSE137GZZIalGyxKEwFMTGZfU0GXCEzYmIJZYrbWwkbUUWZsdmUbAje8surpH1R9S6rbrNWqdfyOIpwAqdwDh5cQR1uoQEtYIDwDK/w5jw6L86787FoLTj5zDH8gfP5A58hjMU=</latexit>\nK  Clusters   \nRecovered Space\n \n<latexit sha1_base64=\"QcSvc7r77J5EZiDr8QAbFrNX/d4=\">AAAB/XicbVBLSwMxGMzWV62v9XHzEiyCp7IrRb1Z8NJjBfuAbinZbLYNzSZLkhXqsvg79OTFgyJexb/hzX9jtu1BWwdChpnvI5PxY0aVdpxvq7C0vLK6VlwvbWxube/Yu3stJRKJSRMLJmTHR4owyklTU81IJ5YERT4jbX90lfvtWyIVFfxGj2PSi9CA05BipI3Utw+8IdKp5wsWqHFkrrSeZX277FScCeAicWekfPn5kOOx0be/vEDgJCJcY4aU6rpOrHspkppiRrKSlygSIzxCA9I1lKOIqF46SZ/BY6MEMBTSHK7hRP29kaJI5dnMZIT0UM17ufif1010eNFLKY8TTTiePhQmDGoB8ypgQCXBmo0NQVhSkxXiIZIIa1NYyZTgzn95kbROK+5ZpXrtlGtVMEURHIIjcAJccA5qoA4aoAkwuANP4AW8WvfWs/VmvU9HC9ZsZx/8gfXxA4z9ml0=</latexit>\nÀÜH\n<latexit sha1_base64=\"QcSvc7r77J5EZiDr8QAbFrNX/d4=\">AAAB/XicbVBLSwMxGMzWV62v9XHzEiyCp7IrRb1Z8NJjBfuAbinZbLYNzSZLkhXqsvg79OTFgyJexb/hzX9jtu1BWwdChpnvI5PxY0aVdpxvq7C0vLK6VlwvbWxube/Yu3stJRKJSRMLJmTHR4owyklTU81IJ5YERT4jbX90lfvtWyIVFfxGj2PSi9CA05BipI3Utw+8IdKp5wsWqHFkrrSeZX277FScCeAicWekfPn5kOOx0be/vEDgJCJcY4aU6rpOrHspkppiRrKSlygSIzxCA9I1lKOIqF46SZ/BY6MEMBTSHK7hRP29kaJI5dnMZIT0UM17ufif1010eNFLKY8TTTiePhQmDGoB8ypgQCXBmo0NQVhSkxXiIZIIa1NYyZTgzn95kbROK+5ZpXrtlGtVMEURHIIjcAJccA5qoA4aoAkwuANP4AW8WvfWs/VmvU9HC9ZsZx/8gfXxA4z9ml0=</latexit>\nÀÜH\n<latexit sha1_base64=\"BatYsbvA8tAkkDJUz7ieyCf/V3E=\">AAACDHicbVDLSgMxFM3UV62vqks3wSK4KjNSUVwV3HRZwT6wM5RMJtOGZpIhyQhlmA9w46+4caGIWz/AnX9jpp1Fbb0QcjjnXu49x48ZVdq2f6zS2vrG5lZ5u7Kzu7d/UD086iqRSEw6WDAh+z5ShFFOOppqRvqxJCjyGen5k9tc7z0Sqajg93oaEy9CI05DipE21LBaG924vmCBmkbmSx8y6GoBF6lWZrrsuj0ruAqcAtRAUe1h9dsNBE4iwjVmSKmBY8faS5HUFDOSVdxEkRjhCRqRgYEcRUR56cxMBs8ME8BQSPO4hjN2cSJFkcpPM50R0mO1rOXkf9og0eG1l1IeJ5pwPF8UJgwav3kyMKCSYM2mBiAsqbkV4jGSCGuTX8WE4CxbXgXdi7pzWbfvGrVmo4ijDE7AKTgHDrgCTdACbdABGDyBF/AG3q1n69X6sD7nrSWrmDkGf8r6+gV3D5vX</latexit>\ng : Z ! H\n<latexit sha1_base64=\"BatYsbvA8tAkkDJUz7ieyCf/V3E=\">AAACDHicbVDLSgMxFM3UV62vqks3wSK4KjNSUVwV3HRZwT6wM5RMJtOGZpIhyQhlmA9w46+4caGIWz/AnX9jpp1Fbb0QcjjnXu49x48ZVdq2f6zS2vrG5lZ5u7Kzu7d/UD086iqRSEw6WDAh+z5ShFFOOppqRvqxJCjyGen5k9tc7z0Sqajg93oaEy9CI05DipE21LBaG924vmCBmkbmSx8y6GoBF6lWZrrsuj0ruAqcAtRAUe1h9dsNBE4iwjVmSKmBY8faS5HUFDOSVdxEkRjhCRqRgYEcRUR56cxMBs8ME8BQSPO4hjN2cSJFkcpPM50R0mO1rOXkf9og0eG1l1IeJ5pwPF8UJgwav3kyMKCSYM2mBiAsqbkV4jGSCGuTX8WE4CxbXgXdi7pzWbfvGrVmo4ijDE7AKTgHDrgCTdACbdABGDyBF/AG3q1n69X6sD7nrSWrmDkGf8r6+gV3D5vX</latexit>\ng : Z ! H\n<latexit sha1_base64=\"cfLucvCFP1XE6FFW6F3QWTzbOzY=\">AAAB/3icbVDNS8MwHE39nPOrKnjxEhzCvIxWhnocePE4wX3AVkuaZltYmpQkVUbtwX/FiwdFvPpvePO/Md160M0HIY/3fj/y8oKYUaUd59taWl5ZXVsvbZQ3t7Z3du29/bYSicSkhQUTshsgRRjlpKWpZqQbS4KigJFOML7K/c49kYoKfqsnMfEiNOR0QDHSRvLtw34gWKgmkbnSUea7d2n14TTz7YpTc6aAi8QtSAUUaPr2Vz8UOIkI15ghpXquE2svRVJTzEhW7ieKxAiP0ZD0DOUoIspLp/kzeGKUEA6ENIdrOFV/b6QoUnlEMxkhPVLzXi7+5/USPbj0UsrjRBOOZw8NEga1gHkZMKSSYM0mhiAsqckK8QhJhLWprGxKcOe/vEjaZzX3vFa/qVcaTlFHCRyBY1AFLrgADXANmqAFMHgEz+AVvFlP1ov1bn3MRpesYucA/IH1+QMh8JYf</latexit>\nh\n( w )\n1\nBERT\n(no Ô¨Åne-tuning)\n¬∑¬∑¬∑\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n<latexit sha1_base64=\"W4CHVkKewjyOR8AmK0awMB4/Cr4=\">AAAB6nicbVDLSgNBEOw1PmJ8RcWTl8EgeAq7Iuox4MVjRPOAJITZSW8yZHZ2mZlVwpJP8OJBEa/+iL/gQfDkp+jkcdDEgoaiqpvuLj8WXBvX/XQWMotLyyvZ1dza+sbmVn57p6qjRDGssEhEqu5TjYJLrBhuBNZjhTT0Bdb8/sXIr92i0jySN2YQYyukXckDzqix0vVd22vnC27RHYPME29KCqXMx/fb3heW2/n3ZidiSYjSMEG1bnhubFopVYYzgcNcM9EYU9anXWxYKmmIupWOTx2SQ6t0SBApW9KQsfp7IqWh1oPQt50hNT09643E/7xGYoLzVsplnBiUbLIoSAQxERn9TTpcITNiYAllittbCetRRZmx6eRsCN7sy/Okelz0TosnVzYNFybIwj4cwBF4cAYluIQyVIBBF+7hEZ4c4Tw4z87LpHXBmc7swh84rz+qm5HE</latexit>\nw 1\n<latexit sha1_base64=\"W4CHVkKewjyOR8AmK0awMB4/Cr4=\">AAAB6nicbVDLSgNBEOw1PmJ8RcWTl8EgeAq7Iuox4MVjRPOAJITZSW8yZHZ2mZlVwpJP8OJBEa/+iL/gQfDkp+jkcdDEgoaiqpvuLj8WXBvX/XQWMotLyyvZ1dza+sbmVn57p6qjRDGssEhEqu5TjYJLrBhuBNZjhTT0Bdb8/sXIr92i0jySN2YQYyukXckDzqix0vVd22vnC27RHYPME29KCqXMx/fb3heW2/n3ZidiSYjSMEG1bnhubFopVYYzgcNcM9EYU9anXWxYKmmIupWOTx2SQ6t0SBApW9KQsfp7IqWh1oPQt50hNT09643E/7xGYoLzVsplnBiUbLIoSAQxERn9TTpcITNiYAllittbCetRRZmx6eRsCN7sy/Okelz0TosnVzYNFybIwj4cwBF4cAYluIQyVIBBF+7hEZ4c4Tw4z87LpHXBmc7swh84rz+qm5HE</latexit>\nw 1\n<latexit sha1_base64=\"0lymhe+SoahmHM5UE3GDGYX422E=\">AAAB6nicbVDLSgNBEOxNfMT4ioonL4tB8BR2g6jHgBePEc0DkiXMTnqTIbOzy8ysEpZ8ghcPinj1R/wFD4InP0Unj4MmFjQUVd10d/kxZ0o7zqeVyS4tr6zm1vLrG5tb24Wd3bqKEkmxRiMeyaZPFHImsKaZ5tiMJZLQ59jwBxdjv3GLUrFI3OhhjF5IeoIFjBJtpOu7TrlTKDolZwJ7kbgzUqxkP77f9r+w2im8t7sRTUIUmnKiVMt1Yu2lRGpGOY7y7URhTOiA9LBlqCAhKi+dnDqyj4zStYNImhLanqi/J1ISKjUMfdMZEt1X895Y/M9rJTo491Im4kSjoNNFQcJtHdnjv+0uk0g1HxpCqGTmVpv2iSRUm3TyJgR3/uVFUi+X3NPSyZVJw4EpcnAAh3AMLpxBBS6hCjWg0IN7eIQni1sP1rP1Mm3NWLOZPfgD6/UHrB+RxQ==</latexit>\nw 2\n<latexit sha1_base64=\"0lymhe+SoahmHM5UE3GDGYX422E=\">AAAB6nicbVDLSgNBEOxNfMT4ioonL4tB8BR2g6jHgBePEc0DkiXMTnqTIbOzy8ysEpZ8ghcPinj1R/wFD4InP0Unj4MmFjQUVd10d/kxZ0o7zqeVyS4tr6zm1vLrG5tb24Wd3bqKEkmxRiMeyaZPFHImsKaZ5tiMJZLQ59jwBxdjv3GLUrFI3OhhjF5IeoIFjBJtpOu7TrlTKDolZwJ7kbgzUqxkP77f9r+w2im8t7sRTUIUmnKiVMt1Yu2lRGpGOY7y7URhTOiA9LBlqCAhKi+dnDqyj4zStYNImhLanqi/J1ISKjUMfdMZEt1X895Y/M9rJTo491Im4kSjoNNFQcJtHdnjv+0uk0g1HxpCqGTmVpv2iSRUm3TyJgR3/uVFUi+X3NPSyZVJw4EpcnAAh3AMLpxBBS6hCjWg0IN7eIQni1sP1rP1Mm3NWLOZPfgD6/UHrB+RxQ==</latexit>\nw 2\n<latexit sha1_base64=\"j3YI7AW0+PSVk8L64t3vnace3SI=\">AAAB6nicbVDLSgNBEOw1PmJ8RcWTl8EgeAq7Iuox4MVjRPOAJITZSW8yZHZ2mZlVwpJP8OJBEa/+iL/gQfDkp+jkcdDEgoaiqpvuLj8WXBvX/XQWMotLyyvZ1dza+sbmVn57p6qjRDGssEhEqu5TjYJLrBhuBNZjhTT0Bdb8/sXIr92i0jySN2YQYyukXckDzqix0vVdW7bzBbfojkHmiTclhVLm4/tt7wvL7fx7sxOxJERpmKBaNzw3Nq2UKsOZwGGumWiMKevTLjYslTRE3UrHpw7JoVU6JIiULWnIWP09kdJQ60Ho286Qmp6e9Ubif14jMcF5K+UyTgxKNlkUJIKYiIz+Jh2ukBkxsIQyxe2thPWooszYdHI2BG/25XlSPS56p8WTK5uGCxNkYR8O4Ag8OIMSXEIZKsCgC/fwCE+OcB6cZ+dl0rrgTGd24Q+c1x8HHpIB</latexit>\nw n\n<latexit sha1_base64=\"j3YI7AW0+PSVk8L64t3vnace3SI=\">AAAB6nicbVDLSgNBEOw1PmJ8RcWTl8EgeAq7Iuox4MVjRPOAJITZSW8yZHZ2mZlVwpJP8OJBEa/+iL/gQfDkp+jkcdDEgoaiqpvuLj8WXBvX/XQWMotLyyvZ1dza+sbmVn57p6qjRDGssEhEqu5TjYJLrBhuBNZjhTT0Bdb8/sXIr92i0jySN2YQYyukXckDzqix0vVdW7bzBbfojkHmiTclhVLm4/tt7wvL7fx7sxOxJERpmKBaNzw3Nq2UKsOZwGGumWiMKevTLjYslTRE3UrHpw7JoVU6JIiULWnIWP09kdJQ60Ho286Qmp6e9Ubif14jMcF5K+UyTgxKNlkUJIKYiIz+Jh2ukBkxsIQyxe2thPWooszYdHI2BG/25XlSPS56p8WTK5uGCxNkYR8O4Ag8OIMSXEIZKsCgC/fwCE+OcB6cZ+dl0rrgTGd24Q+c1x8HHpIB</latexit>\nw n\n¬∑¬∑¬∑\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n<latexit sha1_base64=\"FNS2fi95VyFAnt2cvRvS3ldvzk0=\">AAAB/3icbVDNS8MwHE3n15xfVcGLl+IQ5mW0Y6jHgRePE9wHbLWkabqFpUlJUmXUHvxXvHhQxKv/hjf/G9OtB918EPJ47/cjL8+PKZHKtr+N0srq2vpGebOytb2zu2fuH3QlTwTCHcQpF30fSkwJwx1FFMX9WGAY+RT3/MlV7vfusZCEs1s1jbEbwREjIUFQackzj4Y+p4GcRvpKx5nXuEtrD2eZZ1btuj2DtUycglRBgbZnfg0DjpIIM4UolHLg2LFyUygUQRRnlWEicQzRBI7wQFMGIyzddJY/s061ElghF/owZc3U3xspjGQeUU9GUI3lopeL/3mDRIWXbkpYnCjM0PyhMKGW4lZehhUQgZGiU00gEkRntdAYCoiUrqyiS3AWv7xMuo26c15v3jSrLbuoowyOwQmoAQdcgBa4Bm3QAQg8gmfwCt6MJ+PFeDc+5qMlo9g5BH9gfP4AI3qWIA==</latexit>\nh\n( w )\n2\n<latexit sha1_base64=\"WDUlFY/IMlqCnrnFMpokfKS6FYQ=\">AAAB/3icbVDNS8MwHE39nPOrKnjxEhzCvIxWhnocePE4wX3AVkuaZltYmpQkVUbtwX/FiwdFvPpvePO/Md160M0HIY/3fj/y8oKYUaUd59taWl5ZXVsvbZQ3t7Z3du29/bYSicSkhQUTshsgRRjlpKWpZqQbS4KigJFOML7K/c49kYoKfqsnMfEiNOR0QDHSRvLtw34gWKgmkbnSUebzu7T6cJr5dsWpOVPAReIWpAIKNH37qx8KnESEa8yQUj3XibWXIqkpZiQr9xNFYoTHaEh6hnIUEeWl0/wZPDFKCAdCmsM1nKq/N1IUqTyimYyQHql5Lxf/83qJHlx6KeVxognHs4cGCYNawLwMGFJJsGYTQxCW1GSFeIQkwtpUVjYluPNfXiTts5p7Xqvf1CsNp6ijBI7AMagCF1yABrgGTdACGDyCZ/AK3qwn68V6tz5mo0tWsXMA/sD6/AF/0pZc</latexit>\nh\n( w )\nn\n<latexit sha1_base64=\"znYY9scH54y4Fvc9fmTbd22GHnE=\">AAAB/XicdVBJSwMxGM241rqNy81LsAj1UpJSutwKXjxWsAu0Y8lk0jY0s5BkhDoM/hUvHhTx6v/w5r8x01ZQ0Qchj/e+j7w8NxJcaYQ+rJXVtfWNzdxWfntnd2/fPjjsqDCWlLVpKELZc4liggesrbkWrBdJRnxXsK47vcj87i2TiofBtZ5FzPHJOOAjTok20tA+Hrih8NTMN1cySW+SoneeDu0CKiGEMMYwI7hWRYY0GvUyrkOcWQYFsERraL8PvJDGPgs0FUSpPkaRdhIiNaeCpflBrFhE6JSMWd/QgPhMOck8fQrPjOLBUSjNCTScq983EuKrLKCZ9ImeqN9eJv7l9WM9qjsJD6JYs4AuHhrFAuoQZlVAj0tGtZgZQqjkJiukEyIJ1aawvCnh66fwf9Ipl3C1VLmqFJpoWUcOnIBTUAQY1EATXIIWaAMK7sADeALP1r31aL1Yr4vRFWu5cwR+wHr7BB2slZ4=</latexit>\nh\n( d )\n<latexit sha1_base64=\"p5z4Wvu1h356foKLdoXSQMiF974=\">AAAB73icdVDLSsNAFL2pr1pfVZduBovgKiS1tsmu4MZlBfuANpTJdNIOnTycmQgl9CfcuFDErb/jzr9x0lZQ0QMXDufcy733+AlnUlnWh1FYW9/Y3Cpul3Z29/YPyodHHRmngtA2iXksej6WlLOIthVTnPYSQXHoc9r1p1e5372nQrI4ulWzhHohHkcsYAQrLfUGmCcTPLSH5YplXjquU28gy6xXXesiJ67rNBwH2aa1QAVWaA3L74NRTNKQRopwLGXfthLlZVgoRjidlwappAkmUzymfU0jHFLpZYt75+hMKyMUxEJXpNBC/T6R4VDKWejrzhCrifzt5eJfXj9VgeNlLEpSRSOyXBSkHKkY5c+jEROUKD7TBBPB9K2ITLDAROmISjqEr0/R/6RTNe26WbupVZq1VRxFOIFTOAcbGtCEa2hBGwhweIAneDbujEfjxXhdthaM1cwx/IDx9glfn5Au</latexit>\n‚Üµ 1\n<latexit sha1_base64=\"z87OMXG+8K1Eoz/8hkmXY0KcihA=\">AAAB73icdVDLSgNBEJyNrxhfUY9eBoPgKeyEkMct4MVjBPOAZAm9k9lkyOzsOjMrhCU/4cWDIl79HW/+jbNJBBUtaCiquunu8mPBtXHdDye3sbm1vZPfLeztHxweFY9PujpKFGUdGolI9X3QTHDJOoYbwfqxYhD6gvX82VXm9+6Z0jySt2YeMy+EieQBp2Cs1B+CiKcwqoyKJbfsui4hBGeE1GuuJc1mo0IamGSWRQmt0R4V34fjiCYhk4YK0HpA3Nh4KSjDqWCLwjDRLAY6gwkbWCohZNpLl/cu8IVVxjiIlC1p8FL9PpFCqPU89G1nCGaqf3uZ+Jc3SEzQ8FIu48QwSVeLgkRgE+HseTzmilEj5pYAVdzeiukUFFBjIyrYEL4+xf+TbqVMauXqTbXUqq7jyKMzdI4uEUF11ELXqI06iCKBHtATenbunEfnxXldteac9cwp+gHn7RMCEo/u</latexit>\n‚Üµ 2\n<latexit sha1_base64=\"W4q3Yw/D/N6hYKuaIShn+7iPAqI=\">AAAB73icdVDLSgNBEOyNrxhfUY9eBoPgKeyEkMct4MVjBPOAZAmzk9lkyOzsOjMrhCU/4cWDIl79HW/+jbNJBBUtaCiquunu8mPBtXHdDye3sbm1vZPfLeztHxweFY9PujpKFGUdGolI9X2imeCSdQw3gvVjxUjoC9bzZ1eZ37tnSvNI3pp5zLyQTCQPOCXGSv0hEfGUjOSoWHLLrutijFFGcL3mWtJsNiq4gXBmWZRgjfao+D4cRzQJmTRUEK0H2I2NlxJlOBVsURgmmsWEzsiEDSyVJGTaS5f3LtCFVcYoiJQtadBS/T6RklDreejbzpCYqf7tZeJf3iAxQcNLuYwTwyRdLQoSgUyEsufRmCtGjZhbQqji9lZEp0QRamxEBRvC16fof9KtlHGtXL2pllrVdRx5OINzuAQMdWjBNbShAxQEPMATPDt3zqPz4ryuWnPOeuYUfsB5+wRdApAq</latexit>\n‚Üµ n\n<latexit sha1_base64=\"b7r+TzF7s1zdbmlZ12hX1q/gbDg=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipaQblilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDWz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6reddVt1ir1Wh5HEc7gHC7Bgxuowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A3UWM7g==</latexit>\nt\n<latexit sha1_base64=\"b7r+TzF7s1zdbmlZ12hX1q/gbDg=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipaQblilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDWz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6reddVt1ir1Wh5HEc7gHC7Bgxuowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A3UWM7g==</latexit>\nt\nsharpened\nDistinctive Topic Clustering\n<latexit sha1_base64=\"LyMuTmUggxvigwqoJY0Wlq21c7Y=\">AAACAnicbVC7TsMwFHXKq5RXgAmxWFRI7VIlqALGSiyMRaIPqQ2V4zitVSeObAdUQsTCr7AwgBArX8HG3+C0GaDlSJaPzrlX997jRoxKZVnfRmFpeWV1rbhe2tjc2t4xd/fakscCkxbmjIuuiyRhNCQtRRUj3UgQFLiMdNzxReZ3bomQlIfXahIRJ0DDkPoUI6WlgXkQVdRD3+XMk5NAf8l9epNU7qppdWCWrZo1BVwkdk7KIEdzYH71PY7jgIQKMyRlz7Yi5SRIKIoZSUv9WJII4TEakp6mIQqIdJLpCSk81ooHfS70CxWcqr87EhTIbENdGSA1kvNeJv7n9WLlnzsJDaNYkRDPBvkxg4rDLA/oUUGwYhNNEBZU7wrxCAmElU6tpEOw509eJO2Tmn1aq1/Vy416HkcRHIIjUAE2OAMNcAmaoAUweATP4BW8GU/Gi/FufMxKC0besw/+wPj8AX9Tl3Q=</latexit>\np ( t | z\n( w )\n)\n<latexit sha1_base64=\"IkMPfsFb6llmr3k3mMyXCJo5bDQ=\">AAACAnicbVC7TsMwFHV4lvIKMCEWiwqpXaoEVcBYiYWxSPQhtaFyHKe16jjBdkAlRCz8CgsDCLHyFWz8DU6bAVqOZPnonHt17z1uxKhUlvVtLCwuLa+sFtaK6xubW9vmzm5LhrHApIlDFoqOiyRhlJOmooqRTiQIClxG2u7oPPPbt0RIGvIrNY6IE6ABpz7FSGmpb+7flNVDzw2ZJ8eB/pL79Dop31XSSt8sWVVrAjhP7JyUQI5G3/zqeSGOA8IVZkjKrm1FykmQUBQzkhZ7sSQRwiM0IF1NOQqIdJLJCSk80ooH/VDoxxWcqL87EhTIbENdGSA1lLNeJv7ndWPlnzkJ5VGsCMfTQX7MoAphlgf0qCBYsbEmCAuqd4V4iATCSqdW1CHYsyfPk9Zx1T6p1i5rpXotj6MADsAhKAMbnII6uAAN0AQYPIJn8ArejCfjxXg3PqalC0beswf+wPj8AYDvl3U=</latexit>\nq ( t | z\n( w )\n)\nTopical Reconstruction of Documents\n<latexit sha1_base64=\"b7r+TzF7s1zdbmlZ12hX1q/gbDg=\">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipaQblilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDWz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6reddVt1ir1Wh5HEc7gHC7Bgxuowz00oAUMEJ7hFd6cR+fFeXc+lq0FJ585hT9wPn8A3UWM7g==</latexit>\nt\n<latexit sha1_base64=\"/pTTfY2g+ZmqiowuIo3COH5p3pE=\">AAACAnicbVDLSgMxFM34rPU16krcBIvQbsqMFHVZcOOygn1AW0smk2lDM8mQZIQ6Dm78FTcuFHHrV7jzb8y0s9DWAyGHc+7l3nu8iFGlHefbWlpeWV1bL2wUN7e2d3btvf2WErHEpIkFE7LjIUUY5aSpqWakE0mCQo+Rtje+zPz2HZGKCn6jJxHph2jIaUAx0kYa2IdRWT/0PMF8NQnNl9ynt0nZr6SVgV1yqs4UcJG4OSmBHI2B/dXzBY5DwjVmSKmu60S6nyCpKWYkLfZiRSKEx2hIuoZyFBLVT6YnpPDEKD4MhDSPazhVf3ckKFTZhqYyRHqk5r1M/M/rxjq46CeUR7EmHM8GBTGDWsAsD+hTSbBmE0MQltTsCvEISYS1Sa1oQnDnT14krdOqe1atXddK9VoeRwEcgWNQBi44B3VwBRqgCTB4BM/gFbxZT9aL9W59zEqXrLznAPyB9fkDYk6XYQ==</latexit>\np ( t | z\n( d )\n)\n<latexit sha1_base64=\"688zGdgAC75N660YXj18uJlaTdA=\">AAACBHicdVDLSgMxFM3UV62vqstugkVwISWpYttdwY3LCvYBnVIyadqGZh4kd4QydOHGX3HjQhG3foQ7/8ZMW0FFD4QczrmXe+/xIiUNEPLhZFZW19Y3spu5re2d3b38/kHLhLHmoslDFeqOx4xQMhBNkKBEJ9KC+Z4SbW9ymfrtW6GNDIMbmEai57NRIIeSM7BSP18gpTP31B0zSFwvVAMz9e2XwGzWL/fzRVIihFBKcUpo5YJYUqtVy7SKaWpZFNESjX7+3R2EPPZFAFwxY7qURNBLmAbJlZjl3NiIiPEJG4mupQHzhekl8yNm+NgqAzwMtX0B4Ln6vSNhvkm3s5U+g7H57aXiX143hmG1l8ggikEEfDFoGCsMIU4TwQOpBQc1tYRxLe2umI+ZZhxsbjkbwtel+H/SKpfoRen8+rxYJ8s4sqiAjtAJoqiC6ugKNVATcXSHHtATenbunUfnxXldlGacZc8h+gHn7ROxCpgg</latexit>\n0 . 3 ÀÜt 2\n<latexit sha1_base64=\"CNg0/399wEBNjInC9KXhmVCiMZk=\">AAACBHicdVDLSgMxFM34rPVVddlNsAguZMiUsba7ghuXFewDOqVk0rQNzTxI7ghl6MKNv+LGhSJu/Qh3/o2ZtoKKHgg5nHMv997jx1JoIOTDWlldW9/YzG3lt3d29/YLB4ctHSWK8SaLZKQ6PtVcipA3QYDknVhxGviSt/3JZea3b7nSIgpvYBrzXkBHoRgKRsFI/UKR2I535o0ppJ4fyYGeBuZLYTbru/1Cidi1aoW4ZUxsQqplUjHknDg1p4Ydo2QooSUa/cK7N4hYEvAQmKRadx0SQy+lCgSTfJb3Es1jyiZ0xLuGhjTgupfOj5jhE6MM8DBS5oWA5+r3jpQGOtvOVAYUxvq3l4l/ed0EhtVeKsI4AR6yxaBhIjFEOEsED4TiDOTUEMqUMLtiNqaKMjC55U0IX5fi/0mrbDsV2712S3WyjCOHiugYnSIHXaA6ukIN1EQM3aEH9ISerXvr0XqxXhelK9ay5wj9gPX2CclWmDA=</latexit>\n0 . 1 ÀÜt 4\n<latexit sha1_base64=\"b+WX/rcy7ThFNfGaxvUSzu3ZueI=\">AAACBXicdVDLSgMxFM3UV62vqktdBIvgQobM2Gq7K7hxWcE+oFNKJk3b0MyD5I5Qhm7c+CtuXCji1n9w59+YPgQVPRByOOde7r3Hj6XQQMiHlVlaXlldy67nNja3tnfyu3sNHSWK8TqLZKRaPtVcipDXQYDkrVhxGviSN/3R5dRv3nKlRRTewDjmnYAOQtEXjIKRuvlDYpOSd+oNKaSeH8meHgfmS2Ey6Za6+QKxKxVSLJYwsUvEdd2yIeTMLVcc7NhkhgJaoNbNv3u9iCUBD4FJqnXbITF0UqpAMMknOS/RPKZsRAe8bWhIA6476eyKCT42Sg/3I2VeCHimfu9IaaCn25nKgMJQ//am4l9eO4F+uZOKME6Ah2w+qJ9IDBGeRoJ7QnEGcmwIZUqYXTEbUkUZmOByJoSvS/H/pOHazrldvC4WqmQRRxYdoCN0ghx0garoCtVQHTF0hx7QE3q27q1H68V6nZdmrEXPPvoB6+0TSNGYcg==</latexit>\n0 . 05 ÀÜt 5\n<latexit sha1_base64=\"ymEHqb2Q+hNzNQGljcsCpg+49Bw=\">AAACBXicdVDLSgMxFM3UV62vqktdBIvgQoaMtrXdFdy4rGAf0Cklk6ZtaOZBckcoQzdu/BU3LhRx6z+482/MtBVU9EDI4Zx7ufceL5JCAyEfVmZpeWV1Lbue29jc2t7J7+41dRgrxhsslKFqe1RzKQLeAAGStyPFqe9J3vLGl6nfuuVKizC4gUnEuz4dBmIgGAUj9fKHxHZK7qk7opC4Xij7euKbL4HptHfeyxeIXSJOtVzGxCbEKVYcQ6rVihGxY5QUBbRAvZd/d/shi30eAJNU645DIugmVIFgkk9zbqx5RNmYDnnH0ID6XHeT2RVTfGyUPh6EyrwA8Ez93pFQX6fbmUqfwkj/9lLxL68Tw6DSTUQQxcADNh80iCWGEKeR4L5QnIGcGEKZEmZXzEZUUQYmuJwJ4etS/D9pntlO2S5eFws1sogjiw7QETpBDrpANXSF6qiBGLpDD+gJPVv31qP1Yr3OSzPWomcf/YD19glG/phx</latexit>\n0 . 15 ÀÜt 3\n<latexit sha1_base64=\"uAmKMo11Akb+TsCDln1/eyoYTk4=\">AAACBHicdVDLSgMxFM3UV62vqstugkVwIUNS2tplwY3LCvYBnVIyadqGZh4kd4QydOHGX3HjQhG3foQ7/8b0IajogZDDOfdy7z1+rKQBQj6czNr6xuZWdju3s7u3f5A/PGqZKNFcNHmkIt3xmRFKhqIJEpToxFqwwFei7U8u5377Vmgjo/AGprHoBWwUyqHkDKzUzxeIW/bOvTGD1PMjNTDTwH4pzGZ92s8XiUuqlTKtYeJWCK3RiiWlCiWkhKlLFiiiFRr9/Ls3iHgSiBC4YsZ0KYmhlzINkisxy3mJETHjEzYSXUtDFgjTSxdHzPCpVQZ4GGn7QsAL9XtHygIz385WBgzG5rc3F//yugkMa71UhnECIuTLQcNEYYjwPBE8kFpwUFNLGNfS7or5mGnGweaWsyF8XYr/J62SS6tu+bpcrJNVHFlUQCfoDFF0geroCjVQE3F0hx7QE3p27p1H58V5XZZmnFXPMfoB5+0TssiYIA==</latexit>\n0 . 4 ÀÜt 1\n<latexit sha1_base64=\"5a3jtcUqnyUp6j2FJhbbgZl2/AM=\">AAAB6XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF49V7Ae0oWy2m3bpZhN2J0IJ/QdePCji1X/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GtzO//cS1EbF6xEnC/YgOlQgFo2ilh960X664VXcOskq8nFQgR6Nf/uoNYpZGXCGT1Jiu5yboZ1SjYJJPS73U8ISyMR3yrqWKRtz42fzSKTmzyoCEsbalkMzV3xMZjYyZRIHtjCiOzLI3E//zuimG134mVJIiV2yxKEwlwZjM3iYDoTlDObGEMi3srYSNqKYMbTglG4K3/PIqaV1Uvctq7b5Wqd/kcRThBE7hHDy4gjrcQQOawCCEZ3iFN2fsvDjvzseiteDkM8fwB87nD6F6jXA=</latexit>\n}\n<latexit sha1_base64=\"k+ks0qtuCct89DxERayMtoaX1Nw=\">AAACA3icbVDLSgMxFM3UV62vUXe6CRahbsqMFHVZcOOygn1AO5ZMJtOGZpIhyQhlGHDjr7hxoYhbf8Kdf2OmnYW2Hgg5nHMv997jx4wq7TjfVmlldW19o7xZ2dre2d2z9w86SiQSkzYWTMiejxRhlJO2ppqRXiwJinxGuv7kOve7D0QqKvidnsbEi9CI05BipI00tI8GPpLpwBcsUNPIfOk4y+7TWnCWDe2qU3dmgMvELUgVFGgN7a9BIHASEa4xQ0r1XSfWXoqkppiRrDJIFIkRnqAR6RvKUUSUl85uyOCpUQIYCmke13Cm/u5IUaTyDU1lhPRYLXq5+J/XT3R45aWUx4kmHM8HhQmDWsA8EBhQSbBmU0MQltTsCvEYSYS1ia1iQnAXT14mnfO6e1Fv3DaqzUYRRxkcgxNQAy64BE1wA1qgDTB4BM/gFbxZT9aL9W59zEtLVtFzCP7A+vwBwIWYMQ==</latexit>\n¬Øh\n( d )\nreconstruct\nFigure 2: Overview of TopClus. We assume that the ùêæ-topic structure exists in a latent spherical space ùíÅ. We jointly learn the at-\ntention weights for document embeddings and the latent space generation model via three objectives: (1) a clustering loss that\nencourages distinctive topic learning in the latent space, (2) a topical reconstruction loss of documents that promotes mean-\ningful topic representations for summarizing document semantics and (3) an embedding space preserving loss that maintains\nthe semantics of the original embedding space. The PLM is not fine-tuned.\nSpecifically, the vMF distribution (can be seen as the spherical coun-\nterpart of the Gaussian distribution) of a topicùë°is parameterized by\na mean vector ùíï and a concentration parameter ùúÖ. The probability\ndensity closer to ùíï is greater and the spread is controlled by ùúÖ. Intu-\nitively, words and documents are more likely to be correlated with\na topic ùë° if their latent space representations are closer to the topic\nvector ùíï. Formally, a unit random vectorùíõ ‚ààSùëü‚Ä≤‚àí1 has the ùëü‚Ä≤-variate\nvMF distribution vMFùëü‚Ä≤(ùíï,ùúÖ)if its probability density function is\nùëù(ùíõ; ùíï,ùúÖ)= ùëõùëü‚Ä≤(ùúÖ)exp (ùúÖ¬∑cos(ùíõ,ùíï)),\nwhere ‚à•ùíï ‚à•= 1 is the center direction, ùúÖ ‚â•0 is the concentration\nparameter, cos(ùíõ,ùíï)is the cosine similarity between ùíõ and ùíï, and\nthe normalization constant ùëõùëü‚Ä≤(ùúÖ)is given by\nùëõùëü‚Ä≤(ùúÖ)= ùúÖùëü‚Ä≤/2‚àí1\n(2ùúã)ùëü‚Ä≤/2ùêºùëü‚Ä≤/2‚àí1 (ùúÖ)\n,\nwhere ùêºùëü‚Ä≤/2‚àí1 (¬∑)represents the modified Bessel function of the first\nkind at orderùëü‚Ä≤/2‚àí1. We assume all topics‚Äô vMF distributions share\nthe same concentration parameterùúÖ(i.e., the topic terms are equally\nconcentrated around the topic center for all topics) which can be\nset as a hyperparameter.\nEvery word embedding ùíâ(ùë§)\nùëñ ‚ààùëØ from the original space is\nassumed to be generated through the following process : (1) A topic\nùë°ùëò is sampled from a uniform distribution over the ùêæ topics. (2)\nA latent embedding ùíõ(ùë§)\nùëñ is generated from the vMF distribution\nassociated with topic ùë°ùëò. (3) A function ùëî: ùíÅ ‚ÜíùëØ maps the latent\nembedding ùíõ(ùë§)\nùëñ to the original embedding ùíâ(ùë§)\nùëñ corresponding to\nword ùë§ùëñ. The generative process is summarized as follows:\nùë°ùëò ‚àºUniform(ùêæ), ùíõ(ùë§)\nùëñ ‚àºvMFùëü‚Ä≤(ùíïùëò,ùúÖ), ùíâ(ùë§)\nùëñ = ùëî(ùíõ(ùë§)\nùëñ ). (1)\nThe generative process of document embedding ùíâ(ùëë) ‚ààùëØ is\nsimilar since it resides in the same word embedding space:\nùë°ùëò ‚àºUniform(ùêæ), ùíõ(ùëë)‚àºvMFùëü‚Ä≤(ùíïùëò,ùúÖ), ùíâ(ùëë)= ùëî(ùíõ(ùëë)). (2)\nWe assume that the mapping function ùëî can be nonlinear to\nmodel arbitrary transformations, and we parameterize ùëîas a deep\nneural network (DNN) since DNNs can approximate any nonlinear\nfunction [27]. Each layer ùëô in the DNN is a linear layer with ReLU\nactivation function, taking ùíôùëô as input and outputting ùíöùëô:\nùíöùëô = ReLU(ùëæùëôùíôùëô +ùíÉùëô),\nwhere ùëæùëô and ùíÉùëô are the learnable parameters in the layer. We also\njointly learn the mapping ùëì : ùëØ ‚ÜíùíÅ from the original space to the\nlatent space (i.e., the inverse function of ùëî, also parameterized by\na DNN) to map unseen word/document embeddings to the latent\nspace. Such joint learning of two nonlinear functions follows an au-\ntoencoder [25] setup where an encoding network maps data points\nfrom the original space to the latent space, and a decoding network\nconverts latent space data back to an approximate reconstruction\nof the original data.\n3.3 Model Training\nTo jointly train the attention module for document embeddings in\nSection 3.1 and the latent generative model in Section 3.2, we intro-\nduce three objectives: (1) a clustering loss that enforces separable\ncluster structures in the latent space for distinctive topic learning,\n(2) a topical reconstruction loss of documents to ensure the discov-\nered topics are meaningful summaries of document semantics, and\n(3) an embedding space preserving loss to maintain the semantic\ninformation in the original space.\nDistinctive Topic Clustering. The first clustering objective in-\nduces a latent space with ùêæ well-separated clusters by gradually\nsharpening the posterior topic-word distributions via an expecta-\ntion‚Äìmaximization (EM) algorithm. In the E-step, we estimate a\nnew (soft) cluster assignment of each word based on the current\nparameters; in the M-step, we update the model parameters given\nthe cluster assignments. The process is illustrated in Figure 3.\nE-Step. To estimate the cluster assignment of each word, we compute\nthe posterior topic distribution obtained via the Bayes rule:\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n=\nùëù\n\u0010\nùíõ(ùë§)\nùëñ\n\f\fùë°ùëò\n\u0011\nùëù(ùë°ùëò)\n√çùêæ\nùëò‚Ä≤=1 ùëù\n\u0010\nùíõ(ùë§)\nùëñ\n\f\fùë°ùëò‚Ä≤\n\u0011\nùëù(ùë°ùëò‚Ä≤)\n,\n<latexit sha1_base64=\"8YSojf0y3wH45JzEu6XDvEA32wg=\">AAAB+XicdVDLSsNAFJ3UV62vqEs3g0VwFSah1boruHFZwdZCG8JkMmmHTiZhZlIooX/ixoUibv0Td/6Nk7aCih4Y7uHce7lnTphxpjRCH1ZlbX1jc6u6XdvZ3ds/sA+PeirNJaFdkvJU9kOsKGeCdjXTnPYzSXEScnofTq7L/v2USsVScadnGfUTPBIsZgRrIwW2PQxTHqlZYkqh54EX2HXkeE3kNlyIHFNd1DAEodaVdwFdQ0rUwQqdwH4fRinJEyo04VipgYsy7RdYakY4ndeGuaIZJhM8ogNDBU6o8ouF8zk8M0oE41SaJzRcqN83Cpyo0pyZTLAeq9+9UvyrN8h13PILJrJcU0GWh+KcQ53CMgYYMUmJ5jNDMJHMeIVkjCUm2oRVMyF8/RT+T3qe4zYddNuotxurOKrgBJyCc+CCS9AGN6ADuoCAKXgAT+DZKqxH68V6XY5WrNXOMfgB6+0TOteUAg==</latexit>\nt 2\n<latexit sha1_base64=\"sZPk4K4gYvCl/yxEVgr7N5F3nq4=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwVNqtbvM28OJxgvuArZQ0TbewNC1JOhhl/4kXD4p49T/x5n9juk1Q0Qchj/d+P/LygpRRqWz7wyhtbG5t75R3K3v7B4dH5vFJTyaZwKSLE5aIQYAkYZSTrqKKkUEqCIoDRvrB9Kbw+zMiJE34vZqnxIvRmNOIYqS05JvmKEhYKOexvnK18Ou+WbWt61aj5jagbdl206k5Bak13boLHa0UqII1Or75PgoTnMWEK8yQlEPHTpWXI6EoZmRRGWWSpAhP0ZgMNeUoJtLLl8kX8EIrIYwSoQ9XcKl+38hRLItwejJGaiJ/e4X4lzfMVNTycsrTTBGOVw9FGYMqgUUNMKSCYMXmmiAsqM4K8QQJhJUuq6JL+Pop/J/0apZzZdl3brXtrusogzNwDi6BA5qgDW5BB3QBBjPwAJ7As5Ebj8aL8boaLRnrnVPwA8bbJ1X4lBQ=</latexit>\nt 3\n<latexit sha1_base64=\"DNBgaawlu8FhwfKgGcwP4roE7O0=\">AAAB+XicdVBLSwMxGMz6rPW16tFLsAielmzt0u6t4MVjBfuAdlmy2Wwbmn2QZAtl6T/x4kERr/4Tb/4bs20FFR0IGWa+j0wmyDiTCqEPY2Nza3tnt7JX3T84PDo2T057Ms0FoV2S8lQMAiwpZwntKqY4HWSC4jjgtB9Mb0q/P6NCsjS5V/OMejEeJyxiBCst+aY5ClIeynmsr0ItfNs3a8hCjttsuhBZDrpuIUcT123VXRfaFlqiBtbo+Ob7KExJHtNEEY6lHNooU16BhWKE00V1lEuaYTLFYzrUNMExlV6xTL6Al1oJYZQKfRIFl+r3jQLHsgynJ2OsJvK3V4p/ecNcRS2vYEmWK5qQ1UNRzqFKYVkDDJmgRPG5JpgIprNCMsECE6XLquoSvn4K/ye9umU7Frpr1NqNdR0VcA4uwBWwQRO0wS3ogC4gYAYewBN4Ngrj0XgxXlejG8Z65wz8gPH2CYj8lDg=</latexit>\nt 1\nStart of EM Algorithm\n<latexit sha1_base64=\"+CAtpXlswOAnKzTaKvrlmE1YY78=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gHYt2TRtQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRyXgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaHJeUsoHXFFKetSFDse5w2vdFZ6jevqZAsDC7VOKKujwcB6zOClZa65n7HC3lPjn19JbeTrn2V5G+OJ10zhyxUcsplByKrhE4qqKSJ41QKjgNtC02RA3PUuuZ7pxeS2KeBIhxL2bZRpNwEC8UIp5NsJ5Y0wmSEB7StaYB9Kt1kmn8Cj7TSg/1Q6BMoOFW/byTYl2lEPeljNZS/vVT8y2vHql9xExZEsaIBmT3UjzlUIUzLgD0mKFF8rAkmgumskAyxwETpyrK6hK+fwv9Jo2DZp1bxopirFud1ZMABOAR5YIMyqIJzUAN1QMAdeABP4Nm4Nx6NF+N1NrpgzHf2wA8Yb5/VKZad</latexit>\nz\n( w )\n1\n<latexit sha1_base64=\"p8N9OG07gVyuszOtbinc1kRCayA=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gLaWbJptQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRybgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaLJeUsoHXFFKetSFDsu5w23dFZ6jevqZAsDC7VOKJdHw8C5jGClZZ65n7HDXlfjn19JbeTXuEqyd8cT3pmDlmo5JTLDkRWCZ1UUEkTx6kUHAfaFpoiB+ao9cz3Tj8ksU8DRTiWsm2jSHUTLBQjnE6ynVjSCJMRHtC2pgH2qewm0/wTeKSVPvRCoU+g4FT9vpFgX6YR9aSP1VD+9lLxL68dK6/STVgQxYoGZPaQF3OoQpiWAftMUKL4WBNMBNNZIRligYnSlWV1CV8/hf+TRsGyT63iRTFXLc7ryIADcAjywAZlUAXnoAbqgIA78ACewLNxbzwaL8brbHTBmO/sgR8w3j4B1rOWng==</latexit>\nz\n( w )\n2\n<latexit sha1_base64=\"ImfnfWoymiH6OEy2JYooXK9l1G8=\">AAAB/3icdVBJSwMxGM3UrdZtVPDiJViEehnSzXZuBS8eK9gF2nHIpGkbmllIMkode/CvePGgiFf/hjf/jekiqOiDkMd730denhdxJhVCH0ZqaXlldS29ntnY3NreMXf3mjKMBaENEvJQtD0sKWcBbSimOG1HgmLf47Tljc6mfuuaCsnC4FKNI+r4eBCwPiNYack1D7peyHty7OsruZ24xaskd3Mycc0sslDZrlRsiKwyKlZRWRPbrhZsG+YtNEMWLFB3zfduLySxTwNFOJayk0eRchIsFCOcTjLdWNIIkxEe0I6mAfapdJJZ/gk81koP9kOhT6DgTP2+kWBfTiPqSR+rofztTcW/vE6s+lUnYUEUKxqQ+UP9mEMVwmkZsMcEJYqPNcFEMJ0VkiEWmChdWUaX8PVT+D9pFqz8qVW6KGVrpUUdaXAIjkAO5EEF1MA5qIMGIOAOPIAn8GzcG4/Gi/E6H00Zi5198APG2yfYPZaf</latexit>\nz\n( w )\n3\n<latexit sha1_base64=\"U7JG7zABr4eNKyBVj5kxCqGfmU0=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6qXs1qWPW8GLxwr2AW1dstm0Dc1uliSr1LUH/4oXD4p49W9489+YbSuo6EDIMPN9ZDJexKhUlvVhZJaWV1bXsuu5jc2t7R1zd68leSwwaWLOuOh4SBJGQ9JUVDHSiQRBgcdI2xufpX77mghJeXipJhHpB2gY0gHFSGnJNQ96Hme+nAT6Sm6nbu0qKdycTF0zbxVr1XLJOYVW0bIcu2ynpFSpOFVoayVFHizQcM33ns9xHJBQYYak7NpWpPoJEopiRqa5XixJhPAYDUlX0xAFRPaTWf4pPNaKDwdc6BMqOFO/byQokGlEPRkgNZK/vVT8y+vGalDtJzSMYkVCPH9oEDOoOEzLgD4VBCs20QRhQXVWiEdIIKx0ZTldwtdP4f+kVSra5aJz4eTrzqKOLDgER6AAbFABdXAOGqAJMLgDD+AJPBv3xqPxYrzORzPGYmcf/IDx9gmzrJaF</latexit>\nz\n( w )\n9\n<latexit sha1_base64=\"l8PtAsZJgDOBLkHG/nIAVEQCkkI=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurTrreDFYwX7gHZdstlsG5p9kGSVuvbgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMZLGBXSMD60hcWl5ZXVwlpxfWNza7u0s9sWccoxaeGYxbzrIUEYjUhLUslIN+EEhR4jHW90lvuda8IFjaNLOU6IE6JBRAOKkVSSW9rvezHzxThUV3Y7ce2rrHJzPHFLZUM/tWtV6wQaumFYZs3MSbVet2xoKiVHGczRdEvvfT/GaUgiiRkSomcaiXQyxCXFjEyK/VSQBOERGpCeohEKiXCyaf4JPFKKD4OYqxNJOFW/b2QoFHlENRkiORS/vVz8y+ulMrCdjEZJKkmEZw8FKYMyhnkZ0KecYMnGiiDMqcoK8RBxhKWqrKhK+Pop/J+0q7pZ060Lq9yw5nUUwAE4BBVggjpogHPQBC2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7IiloQ=</latexit>\nz\n( w )\n8\n<latexit sha1_base64=\"b48HWXE8y3b1Vfuehkq3Cb70B0o=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurT1VvDisYJ9QLsu2Wy2Dc0+SLJKXffgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMaNGRXSMD60hcWl5ZXVwlpxfWNza7u0s9sRUcIxaeOIRbznIkEYDUlbUslIL+YEBS4jXXd8lvvda8IFjcJLOYmJHaBhSH2KkVSSU9ofuBHzxCRQV3qbOfWrtHJznDmlsqGfNmpV6wQaumFYZs3MSbVetxrQVEqOMpij5ZTeB16Ek4CEEjMkRN80YmmniEuKGcmKg0SQGOExGpK+oiEKiLDTaf4MHinFg37E1QklnKrfN1IUiDyimgyQHInfXi7+5fUT6TfslIZxIkmIZw/5CYMygnkZ0KOcYMkmiiDMqcoK8QhxhKWqrKhK+Pop/J90qrpZ060Lq9y05nUUwAE4BBVggjpognPQAm2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7CYloM=</latexit>\nz\n( w )\n7\n<latexit sha1_base64=\"6oni0ouPRHGymUnXqx3TvCoJeyQ=\">AAAB/3icdVBLSwMxGMzWV62vquDFS7AI9bJkS7f2WPDisYJ9QFuXbDZtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk3EjzqRC6MPILS2vrK7l1wsbm1vbO8XdvbYMY0Foi4Q8FF0XS8pZQFuKKU67kaDYdzntuJOzzO9cUyFZGFyqaUQHPh4FbMgIVlpyigd9N+SenPr6Sm5Tx75KyjcnqVMsIRPV7KpVh8i0kVW3bE0qtoVQBVommqEEFmg6xfe+F5LYp4EiHEvZs1CkBgkWihFO00I/ljTCZIJHtKdpgH0qB8ksfwqPteLBYSj0CRScqd83EuzLLKKe9LEay99eJv7l9WI1rA8SFkSxogGZPzSMOVQhzMqAHhOUKD7VBBPBdFZIxlhgonRlBV3C10/h/6RdMa2aWb2olhrVRR15cAiOQBlY4BQ0wDloghYg4A48gCfwbNwbj8aL8TofzRmLnX3wA8bbJ5PSlm8=</latexit>\nz\n( w )\n5\n<latexit sha1_base64=\"SCpRfPzOUN6zXGNSVtw/YJX+pIE=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6mXJlt3aY8GLxwr2AW0t2TRtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk/EizqRC6MPILS2vrK7l1wsbm1vbO+buXlOGsSC0QUIeiraHJeUsoA3FFKftSFDse5y2vMlZ5reuqZAsDC7VNKI9H48CNmQEKy31zYOuF/KBnPr6Sm7TfuUqKd2cpH2ziCxUcR27CpHlIrtqu5qUXRuhMrQtNEMRLFDvm+/dQUhinwaKcCxlx0aR6iVYKEY4TQvdWNIIkwke0Y6mAfap7CWz/Ck81soADkOhT6DgTP2+kWBfZhH1pI/VWP72MvEvrxOrYbWXsCCKFQ3I/KFhzKEKYVYGHDBBieJTTTARTGeFZIwFJkpXVtAlfP0U/k+aZcuuWM6FU6w5izry4BAcgRKwwSmogXNQBw1AwB14AE/g2bg3Ho0X43U+mjMWO/vgB4y3T5VclnA=</latexit>\nz\n( w )\n6\n<latexit sha1_base64=\"PEJr7vmsZpuShKLGgU0vqyYjFyo=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpa0trZ7K3jxWME+oK1LNpttQ7MPkqxS1z34V7x4UMSrf8Ob/8b0IajoQMgw831kMk7EmVQIfRgLi0vLK6uZtez6xubWdm5ntyXDWBDaJCEPRcfBknIW0KZiitNOJCj2HU7bzuhs4revqZAsDC7VOKJ9Hw8C5jGClZbs3H7PCbkrx76+ktvULl8lhZvj1M7lkYkqVrVqQWRW0EkNVTSxrFrJsmDRRFPkwRwNO/fec0MS+zRQhGMpu0UUqX6ChWKE0zTbiyWNMBnhAe1qGmCfyn4yzZ/CI6240AuFPoGCU/X7RoJ9OYmoJ32shvK3NxH/8rqx8mr9hAVRrGhAZg95MYcqhJMyoMsEJYqPNcFEMJ0VkiEWmChdWVaX8PVT+D9plcziqVm+KOfr5XkdGXAADkEBFEEV1ME5aIAmIOAOPIAn8GzcG4/Gi/E6G10w5jt74AeMt0/Zx5ag</latexit>\nz\n( w )\n4\n(a) Start of EM Algorithm.\nE-Step: Compute New Posterior \n<latexit sha1_base64=\"8YSojf0y3wH45JzEu6XDvEA32wg=\">AAAB+XicdVDLSsNAFJ3UV62vqEs3g0VwFSah1boruHFZwdZCG8JkMmmHTiZhZlIooX/ixoUibv0Td/6Nk7aCih4Y7uHce7lnTphxpjRCH1ZlbX1jc6u6XdvZ3ds/sA+PeirNJaFdkvJU9kOsKGeCdjXTnPYzSXEScnofTq7L/v2USsVScadnGfUTPBIsZgRrIwW2PQxTHqlZYkqh54EX2HXkeE3kNlyIHFNd1DAEodaVdwFdQ0rUwQqdwH4fRinJEyo04VipgYsy7RdYakY4ndeGuaIZJhM8ogNDBU6o8ouF8zk8M0oE41SaJzRcqN83Cpyo0pyZTLAeq9+9UvyrN8h13PILJrJcU0GWh+KcQ53CMgYYMUmJ5jNDMJHMeIVkjCUm2oRVMyF8/RT+T3qe4zYddNuotxurOKrgBJyCc+CCS9AGN6ADuoCAKXgAT+DZKqxH68V6XY5WrNXOMfgB6+0TOteUAg==</latexit>\nt 2\n<latexit sha1_base64=\"sZPk4K4gYvCl/yxEVgr7N5F3nq4=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwVNqtbvM28OJxgvuArZQ0TbewNC1JOhhl/4kXD4p49T/x5n9juk1Q0Qchj/d+P/LygpRRqWz7wyhtbG5t75R3K3v7B4dH5vFJTyaZwKSLE5aIQYAkYZSTrqKKkUEqCIoDRvrB9Kbw+zMiJE34vZqnxIvRmNOIYqS05JvmKEhYKOexvnK18Ou+WbWt61aj5jagbdl206k5Bak13boLHa0UqII1Or75PgoTnMWEK8yQlEPHTpWXI6EoZmRRGWWSpAhP0ZgMNeUoJtLLl8kX8EIrIYwSoQ9XcKl+38hRLItwejJGaiJ/e4X4lzfMVNTycsrTTBGOVw9FGYMqgUUNMKSCYMXmmiAsqM4K8QQJhJUuq6JL+Pop/J/0apZzZdl3brXtrusogzNwDi6BA5qgDW5BB3QBBjPwAJ7As5Ebj8aL8boaLRnrnVPwA8bbJ1X4lBQ=</latexit>\nt 3\n<latexit sha1_base64=\"DNBgaawlu8FhwfKgGcwP4roE7O0=\">AAAB+XicdVBLSwMxGMz6rPW16tFLsAielmzt0u6t4MVjBfuAdlmy2Wwbmn2QZAtl6T/x4kERr/4Tb/4bs20FFR0IGWa+j0wmyDiTCqEPY2Nza3tnt7JX3T84PDo2T057Ms0FoV2S8lQMAiwpZwntKqY4HWSC4jjgtB9Mb0q/P6NCsjS5V/OMejEeJyxiBCst+aY5ClIeynmsr0ItfNs3a8hCjttsuhBZDrpuIUcT123VXRfaFlqiBtbo+Ob7KExJHtNEEY6lHNooU16BhWKE00V1lEuaYTLFYzrUNMExlV6xTL6Al1oJYZQKfRIFl+r3jQLHsgynJ2OsJvK3V4p/ecNcRS2vYEmWK5qQ1UNRzqFKYVkDDJmgRPG5JpgIprNCMsECE6XLquoSvn4K/ye9umU7Frpr1NqNdR0VcA4uwBWwQRO0wS3ogC4gYAYewBN4Ngrj0XgxXlejG8Z65wz8gPH2CYj8lDg=</latexit>\nt 1\n<latexit sha1_base64=\"U7JG7zABr4eNKyBVj5kxCqGfmU0=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6qXs1qWPW8GLxwr2AW1dstm0Dc1uliSr1LUH/4oXD4p49W9489+YbSuo6EDIMPN9ZDJexKhUlvVhZJaWV1bXsuu5jc2t7R1zd68leSwwaWLOuOh4SBJGQ9JUVDHSiQRBgcdI2xufpX77mghJeXipJhHpB2gY0gHFSGnJNQ96Hme+nAT6Sm6nbu0qKdycTF0zbxVr1XLJOYVW0bIcu2ynpFSpOFVoayVFHizQcM33ns9xHJBQYYak7NpWpPoJEopiRqa5XixJhPAYDUlX0xAFRPaTWf4pPNaKDwdc6BMqOFO/byQokGlEPRkgNZK/vVT8y+vGalDtJzSMYkVCPH9oEDOoOEzLgD4VBCs20QRhQXVWiEdIIKx0ZTldwtdP4f+kVSra5aJz4eTrzqKOLDgER6AAbFABdXAOGqAJMLgDD+AJPBv3xqPxYrzORzPGYmcf/IDx9gmzrJaF</latexit>\nz\n( w )\n9\n<latexit sha1_base64=\"l8PtAsZJgDOBLkHG/nIAVEQCkkI=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurTrreDFYwX7gHZdstlsG5p9kGSVuvbgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMZLGBXSMD60hcWl5ZXVwlpxfWNza7u0s9sWccoxaeGYxbzrIUEYjUhLUslIN+EEhR4jHW90lvuda8IFjaNLOU6IE6JBRAOKkVSSW9rvezHzxThUV3Y7ce2rrHJzPHFLZUM/tWtV6wQaumFYZs3MSbVet2xoKiVHGczRdEvvfT/GaUgiiRkSomcaiXQyxCXFjEyK/VSQBOERGpCeohEKiXCyaf4JPFKKD4OYqxNJOFW/b2QoFHlENRkiORS/vVz8y+ulMrCdjEZJKkmEZw8FKYMyhnkZ0KecYMnGiiDMqcoK8RBxhKWqrKhK+Pop/J+0q7pZ060Lq9yw5nUUwAE4BBVggjpogHPQBC2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7IiloQ=</latexit>\nz\n( w )\n8\n<latexit sha1_base64=\"b48HWXE8y3b1Vfuehkq3Cb70B0o=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurT1VvDisYJ9QLsu2Wy2Dc0+SLJKXffgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMaNGRXSMD60hcWl5ZXVwlpxfWNza7u0s9sRUcIxaeOIRbznIkEYDUlbUslIL+YEBS4jXXd8lvvda8IFjcJLOYmJHaBhSH2KkVSSU9ofuBHzxCRQV3qbOfWrtHJznDmlsqGfNmpV6wQaumFYZs3MSbVetxrQVEqOMpij5ZTeB16Ek4CEEjMkRN80YmmniEuKGcmKg0SQGOExGpK+oiEKiLDTaf4MHinFg37E1QklnKrfN1IUiDyimgyQHInfXi7+5fUT6TfslIZxIkmIZw/5CYMygnkZ0KOcYMkmiiDMqcoK8QhxhKWqrKhK+Pop/J90qrpZ060Lq9y05nUUwAE4BBVggjpognPQAm2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7CYloM=</latexit>\nz\n( w )\n7\n<latexit sha1_base64=\"+CAtpXlswOAnKzTaKvrlmE1YY78=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gHYt2TRtQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRyXgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaHJeUsoHXFFKetSFDse5w2vdFZ6jevqZAsDC7VOKKujwcB6zOClZa65n7HC3lPjn19JbeTrn2V5G+OJ10zhyxUcsplByKrhE4qqKSJ41QKjgNtC02RA3PUuuZ7pxeS2KeBIhxL2bZRpNwEC8UIp5NsJ5Y0wmSEB7StaYB9Kt1kmn8Cj7TSg/1Q6BMoOFW/byTYl2lEPeljNZS/vVT8y2vHql9xExZEsaIBmT3UjzlUIUzLgD0mKFF8rAkmgumskAyxwETpyrK6hK+fwv9Jo2DZp1bxopirFud1ZMABOAR5YIMyqIJzUAN1QMAdeABP4Nm4Nx6NF+N1NrpgzHf2wA8Yb5/VKZad</latexit>\nz\n( w )\n1\n<latexit sha1_base64=\"p8N9OG07gVyuszOtbinc1kRCayA=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gLaWbJptQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRybgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaLJeUsoHXFFKetSFDsu5w23dFZ6jevqZAsDC7VOKJdHw8C5jGClZZ65n7HDXlfjn19JbeTXuEqyd8cT3pmDlmo5JTLDkRWCZ1UUEkTx6kUHAfaFpoiB+ao9cz3Tj8ksU8DRTiWsm2jSHUTLBQjnE6ynVjSCJMRHtC2pgH2qewm0/wTeKSVPvRCoU+g4FT9vpFgX6YR9aSP1VD+9lLxL68dK6/STVgQxYoGZPaQF3OoQpiWAftMUKL4WBNMBNNZIRligYnSlWV1CV8/hf+TRsGyT63iRTFXLc7ryIADcAjywAZlUAXnoAbqgIA78ACewLNxbzwaL8brbHTBmO/sgR8w3j4B1rOWng==</latexit>\nz\n( w )\n2\n<latexit sha1_base64=\"ImfnfWoymiH6OEy2JYooXK9l1G8=\">AAAB/3icdVBJSwMxGM3UrdZtVPDiJViEehnSzXZuBS8eK9gF2nHIpGkbmllIMkode/CvePGgiFf/hjf/jekiqOiDkMd730denhdxJhVCH0ZqaXlldS29ntnY3NreMXf3mjKMBaENEvJQtD0sKWcBbSimOG1HgmLf47Tljc6mfuuaCsnC4FKNI+r4eBCwPiNYack1D7peyHty7OsruZ24xaskd3Mycc0sslDZrlRsiKwyKlZRWRPbrhZsG+YtNEMWLFB3zfduLySxTwNFOJayk0eRchIsFCOcTjLdWNIIkxEe0I6mAfapdJJZ/gk81koP9kOhT6DgTP2+kWBfTiPqSR+rofztTcW/vE6s+lUnYUEUKxqQ+UP9mEMVwmkZsMcEJYqPNcFEMJ0VkiEWmChdWUaX8PVT+D9pFqz8qVW6KGVrpUUdaXAIjkAO5EEF1MA5qIMGIOAOPIAn8GzcG4/Gi/E6H00Zi5198APG2yfYPZaf</latexit>\nz\n( w )\n3\n<latexit sha1_base64=\"6oni0ouPRHGymUnXqx3TvCoJeyQ=\">AAAB/3icdVBLSwMxGMzWV62vquDFS7AI9bJkS7f2WPDisYJ9QFuXbDZtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk3EjzqRC6MPILS2vrK7l1wsbm1vbO8XdvbYMY0Foi4Q8FF0XS8pZQFuKKU67kaDYdzntuJOzzO9cUyFZGFyqaUQHPh4FbMgIVlpyigd9N+SenPr6Sm5Tx75KyjcnqVMsIRPV7KpVh8i0kVW3bE0qtoVQBVommqEEFmg6xfe+F5LYp4EiHEvZs1CkBgkWihFO00I/ljTCZIJHtKdpgH0qB8ksfwqPteLBYSj0CRScqd83EuzLLKKe9LEay99eJv7l9WI1rA8SFkSxogGZPzSMOVQhzMqAHhOUKD7VBBPBdFZIxlhgonRlBV3C10/h/6RdMa2aWb2olhrVRR15cAiOQBlY4BQ0wDloghYg4A48gCfwbNwbj8aL8TofzRmLnX3wA8bbJ5PSlm8=</latexit>\nz\n( w )\n5\n<latexit sha1_base64=\"SCpRfPzOUN6zXGNSVtw/YJX+pIE=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6mXJlt3aY8GLxwr2AW0t2TRtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk/EizqRC6MPILS2vrK7l1wsbm1vbO+buXlOGsSC0QUIeiraHJeUsoA3FFKftSFDse5y2vMlZ5reuqZAsDC7VNKI9H48CNmQEKy31zYOuF/KBnPr6Sm7TfuUqKd2cpH2ziCxUcR27CpHlIrtqu5qUXRuhMrQtNEMRLFDvm+/dQUhinwaKcCxlx0aR6iVYKEY4TQvdWNIIkwke0Y6mAfap7CWz/Ck81soADkOhT6DgTP2+kWBfZhH1pI/VWP72MvEvrxOrYbWXsCCKFQ3I/KFhzKEKYVYGHDBBieJTTTARTGeFZIwFJkpXVtAlfP0U/k+aZcuuWM6FU6w5izry4BAcgRKwwSmogXNQBw1AwB14AE/g2bg3Ho0X43U+mjMWO/vgB4y3T5VclnA=</latexit>\nz\n( w )\n6\n<latexit sha1_base64=\"PEJr7vmsZpuShKLGgU0vqyYjFyo=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpa0trZ7K3jxWME+oK1LNpttQ7MPkqxS1z34V7x4UMSrf8Ob/8b0IajoQMgw831kMk7EmVQIfRgLi0vLK6uZtez6xubWdm5ntyXDWBDaJCEPRcfBknIW0KZiitNOJCj2HU7bzuhs4revqZAsDC7VOKJ9Hw8C5jGClZbs3H7PCbkrx76+ktvULl8lhZvj1M7lkYkqVrVqQWRW0EkNVTSxrFrJsmDRRFPkwRwNO/fec0MS+zRQhGMpu0UUqX6ChWKE0zTbiyWNMBnhAe1qGmCfyn4yzZ/CI6240AuFPoGCU/X7RoJ9OYmoJ32shvK3NxH/8rqx8mr9hAVRrGhAZg95MYcqhJMyoMsEJYqPNcFEMJ0VkiEWmChdWVaX8PVT+D9plcziqVm+KOfr5XkdGXAADkEBFEEV1ME5aIAmIOAOPIAn8GzcG4/Gi/E6G10w5jt74AeMt0/Zx5ag</latexit>\nz\n( w )\n4\n<latexit sha1_base64=\"ajFDUxGch2dMFA3w0mSOFwPyIFM=\">AAACDXicbVC7TsNAEDzzDOFloKSxCEhJE9koAspINJRBIg8pNtH5fE5OOT+4W4OC8Q/Q8Cs0FCBES0/H33BOUkDCSKcbzexqd8eNOZNgmt/awuLS8spqYa24vrG5ta3v7LZklAhCmyTikei4WFLOQtoEBpx2YkFx4HLadofnud++pUKyKLyCUUydAPdD5jOCQUk9/fDG5tSHMjzYbsQ9OQrUl95n12n5rpLZgvUHUOnpJbNqjmHME2tKSmiKRk//sr2IJAENgXAsZdcyY3BSLIARTrOinUgaYzLEfdpVNMQBlU46viYzjpTiGX4k1AvBGKu/O1IcyHxPVRlgGMhZLxf/87oJ+GdOysI4ARqSySA/4QZERh6N4TFBCfCRIpgIpnY1yAALTEAFWFQhWLMnz5PWcdU6qdYua6V6bRpHAe2jA1RGFjpFdXSBGqiJCHpEz+gVvWlP2ov2rn1MShe0ac8e+gPt8wd0C5xk</latexit>\nq\n‚á£\nt | z\n( w )\n‚åò (b) E-Step.\n<latexit sha1_base64=\"Ew6ly8oom7T6mt1n1MtMW1UHRMg=\">AAAB/3icdVBLSwMxGMzWV62vquDFS7AI9bJky7b2WPDisYJ9QFuXbDZtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk3EjzqRC6MPILS2vrK7l1wsbm1vbO8XdvbYMY0Foi4Q8FF0XS8pZQFuKKU67kaDYdzntuJOzzO9cUyFZGFyqaUQHPh4FbMgIVlpyigd9N+SenPr6Sm5Tx75KyjcnqVMsIRPVqrZVh8isIqtuVTWpVC2EKtAy0QwlsEDTKb73vZDEPg0U4VjKnoUiNUiwUIxwmhb6saQRJhM8oj1NA+xTOUhm+VN4rBUPDkOhT6DgTP2+kWBfZhH1pI/VWP72MvEvrxerYX2QsCCKFQ3I/KFhzKEKYVYG9JigRPGpJpgIprNCMsYCE6UrK+gSvn4K/yftimnVTPvCLjXsRR15cAiOQBlY4BQ0wDloghYg4A48gCfwbNwbj8aL8TofzRmLnX3wA8bbJ5JIlm4=</latexit>\nz\n( w )\n4\n<latexit sha1_base64=\"8YSojf0y3wH45JzEu6XDvEA32wg=\">AAAB+XicdVDLSsNAFJ3UV62vqEs3g0VwFSah1boruHFZwdZCG8JkMmmHTiZhZlIooX/ixoUibv0Td/6Nk7aCih4Y7uHce7lnTphxpjRCH1ZlbX1jc6u6XdvZ3ds/sA+PeirNJaFdkvJU9kOsKGeCdjXTnPYzSXEScnofTq7L/v2USsVScadnGfUTPBIsZgRrIwW2PQxTHqlZYkqh54EX2HXkeE3kNlyIHFNd1DAEodaVdwFdQ0rUwQqdwH4fRinJEyo04VipgYsy7RdYakY4ndeGuaIZJhM8ogNDBU6o8ouF8zk8M0oE41SaJzRcqN83Cpyo0pyZTLAeq9+9UvyrN8h13PILJrJcU0GWh+KcQ53CMgYYMUmJ5jNDMJHMeIVkjCUm2oRVMyF8/RT+T3qe4zYddNuotxurOKrgBJyCc+CCS9AGN6ADuoCAKXgAT+DZKqxH68V6XY5WrNXOMfgB6+0TOteUAg==</latexit>\nt 2\n<latexit sha1_base64=\"sZPk4K4gYvCl/yxEVgr7N5F3nq4=\">AAAB+XicdVDNS8MwHE3n15xfVY9egkPwVNqtbvM28OJxgvuArZQ0TbewNC1JOhhl/4kXD4p49T/x5n9juk1Q0Qchj/d+P/LygpRRqWz7wyhtbG5t75R3K3v7B4dH5vFJTyaZwKSLE5aIQYAkYZSTrqKKkUEqCIoDRvrB9Kbw+zMiJE34vZqnxIvRmNOIYqS05JvmKEhYKOexvnK18Ou+WbWt61aj5jagbdl206k5Bak13boLHa0UqII1Or75PgoTnMWEK8yQlEPHTpWXI6EoZmRRGWWSpAhP0ZgMNeUoJtLLl8kX8EIrIYwSoQ9XcKl+38hRLItwejJGaiJ/e4X4lzfMVNTycsrTTBGOVw9FGYMqgUUNMKSCYMXmmiAsqM4K8QQJhJUuq6JL+Pop/J/0apZzZdl3brXtrusogzNwDi6BA5qgDW5BB3QBBjPwAJ7As5Ebj8aL8boaLRnrnVPwA8bbJ1X4lBQ=</latexit>\nt 3\n<latexit sha1_base64=\"DNBgaawlu8FhwfKgGcwP4roE7O0=\">AAAB+XicdVBLSwMxGMz6rPW16tFLsAielmzt0u6t4MVjBfuAdlmy2Wwbmn2QZAtl6T/x4kERr/4Tb/4bs20FFR0IGWa+j0wmyDiTCqEPY2Nza3tnt7JX3T84PDo2T057Ms0FoV2S8lQMAiwpZwntKqY4HWSC4jjgtB9Mb0q/P6NCsjS5V/OMejEeJyxiBCst+aY5ClIeynmsr0ItfNs3a8hCjttsuhBZDrpuIUcT123VXRfaFlqiBtbo+Ob7KExJHtNEEY6lHNooU16BhWKE00V1lEuaYTLFYzrUNMExlV6xTL6Al1oJYZQKfRIFl+r3jQLHsgynJ2OsJvK3V4p/ecNcRS2vYEmWK5qQ1UNRzqFKYVkDDJmgRPG5JpgIprNCMsECE6XLquoSvn4K/ye9umU7Frpr1NqNdR0VcA4uwBWwQRO0wS3ogC4gYAYewBN4Ngrj0XgxXlejG8Z65wz8gPH2CYj8lDg=</latexit>\nt 1\n<latexit sha1_base64=\"U7JG7zABr4eNKyBVj5kxCqGfmU0=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6qXs1qWPW8GLxwr2AW1dstm0Dc1uliSr1LUH/4oXD4p49W9489+YbSuo6EDIMPN9ZDJexKhUlvVhZJaWV1bXsuu5jc2t7R1zd68leSwwaWLOuOh4SBJGQ9JUVDHSiQRBgcdI2xufpX77mghJeXipJhHpB2gY0gHFSGnJNQ96Hme+nAT6Sm6nbu0qKdycTF0zbxVr1XLJOYVW0bIcu2ynpFSpOFVoayVFHizQcM33ns9xHJBQYYak7NpWpPoJEopiRqa5XixJhPAYDUlX0xAFRPaTWf4pPNaKDwdc6BMqOFO/byQokGlEPRkgNZK/vVT8y+vGalDtJzSMYkVCPH9oEDOoOEzLgD4VBCs20QRhQXVWiEdIIKx0ZTldwtdP4f+kVSra5aJz4eTrzqKOLDgER6AAbFABdXAOGqAJMLgDD+AJPBv3xqPxYrzORzPGYmcf/IDx9gmzrJaF</latexit>\nz\n( w )\n9\n<latexit sha1_base64=\"l8PtAsZJgDOBLkHG/nIAVEQCkkI=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurTrreDFYwX7gHZdstlsG5p9kGSVuvbgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMZLGBXSMD60hcWl5ZXVwlpxfWNza7u0s9sWccoxaeGYxbzrIUEYjUhLUslIN+EEhR4jHW90lvuda8IFjaNLOU6IE6JBRAOKkVSSW9rvezHzxThUV3Y7ce2rrHJzPHFLZUM/tWtV6wQaumFYZs3MSbVet2xoKiVHGczRdEvvfT/GaUgiiRkSomcaiXQyxCXFjEyK/VSQBOERGpCeohEKiXCyaf4JPFKKD4OYqxNJOFW/b2QoFHlENRkiORS/vVz8y+ulMrCdjEZJKkmEZw8FKYMyhnkZ0KecYMnGiiDMqcoK8RBxhKWqrKhK+Pop/J+0q7pZ060Lq9yw5nUUwAE4BBVggjpogHPQBC2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7IiloQ=</latexit>\nz\n( w )\n8\n<latexit sha1_base64=\"b48HWXE8y3b1Vfuehkq3Cb70B0o=\">AAAB/3icdVBLSwMxGMz6rPVVFbx4CRahXpbdurT1VvDisYJ9QLsu2Wy2Dc0+SLJKXffgX/HiQRGv/g1v/huzbQUVHQgZZr6PTMaNGRXSMD60hcWl5ZXVwlpxfWNza7u0s9sRUcIxaeOIRbznIkEYDUlbUslIL+YEBS4jXXd8lvvda8IFjcJLOYmJHaBhSH2KkVSSU9ofuBHzxCRQV3qbOfWrtHJznDmlsqGfNmpV6wQaumFYZs3MSbVetxrQVEqOMpij5ZTeB16Ek4CEEjMkRN80YmmniEuKGcmKg0SQGOExGpK+oiEKiLDTaf4MHinFg37E1QklnKrfN1IUiDyimgyQHInfXi7+5fUT6TfslIZxIkmIZw/5CYMygnkZ0KOcYMkmiiDMqcoK8QhxhKWqrKhK+Pop/J90qrpZ060Lq9y05nUUwAE4BBVggjpognPQAm2AwR14AE/gWbvXHrUX7XU2uqDNd/bAD2hvn7CYloM=</latexit>\nz\n( w )\n7\n<latexit sha1_base64=\"+CAtpXlswOAnKzTaKvrlmE1YY78=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gHYt2TRtQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRyXgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaHJeUsoHXFFKetSFDse5w2vdFZ6jevqZAsDC7VOKKujwcB6zOClZa65n7HC3lPjn19JbeTrn2V5G+OJ10zhyxUcsplByKrhE4qqKSJ41QKjgNtC02RA3PUuuZ7pxeS2KeBIhxL2bZRpNwEC8UIp5NsJ5Y0wmSEB7StaYB9Kt1kmn8Cj7TSg/1Q6BMoOFW/byTYl2lEPeljNZS/vVT8y2vHql9xExZEsaIBmT3UjzlUIUzLgD0mKFF8rAkmgumskAyxwETpyrK6hK+fwv9Jo2DZp1bxopirFud1ZMABOAR5YIMyqIJzUAN1QMAdeABP4Nm4Nx6NF+N1NrpgzHf2wA8Yb5/VKZad</latexit>\nz\n( w )\n1\n<latexit sha1_base64=\"p8N9OG07gVyuszOtbinc1kRCayA=\">AAAB/3icdVBLSwMxGMz6rPW1KnjxEixCvSzZ2trureDFYwX7gLaWbJptQ7MPkqxS1x78K148KOLVv+HNf2O2raCiAyHDzPeRybgRZ1Ih9GEsLC4tr6xm1rLrG5tb2+bObkOGsSC0TkIeipaLJeUsoHXFFKetSFDsu5w23dFZ6jevqZAsDC7VOKJdHw8C5jGClZZ65n7HDXlfjn19JbeTXuEqyd8cT3pmDlmo5JTLDkRWCZ1UUEkTx6kUHAfaFpoiB+ao9cz3Tj8ksU8DRTiWsm2jSHUTLBQjnE6ynVjSCJMRHtC2pgH2qewm0/wTeKSVPvRCoU+g4FT9vpFgX6YR9aSP1VD+9lLxL68dK6/STVgQxYoGZPaQF3OoQpiWAftMUKL4WBNMBNNZIRligYnSlWV1CV8/hf+TRsGyT63iRTFXLc7ryIADcAjywAZlUAXnoAbqgIA78ACewLNxbzwaL8brbHTBmO/sgR8w3j4B1rOWng==</latexit>\nz\n( w )\n2\n<latexit sha1_base64=\"ImfnfWoymiH6OEy2JYooXK9l1G8=\">AAAB/3icdVBJSwMxGM3UrdZtVPDiJViEehnSzXZuBS8eK9gF2nHIpGkbmllIMkode/CvePGgiFf/hjf/jekiqOiDkMd730denhdxJhVCH0ZqaXlldS29ntnY3NreMXf3mjKMBaENEvJQtD0sKWcBbSimOG1HgmLf47Tljc6mfuuaCsnC4FKNI+r4eBCwPiNYack1D7peyHty7OsruZ24xaskd3Mycc0sslDZrlRsiKwyKlZRWRPbrhZsG+YtNEMWLFB3zfduLySxTwNFOJayk0eRchIsFCOcTjLdWNIIkxEe0I6mAfapdJJZ/gk81koP9kOhT6DgTP2+kWBfTiPqSR+rofztTcW/vE6s+lUnYUEUKxqQ+UP9mEMVwmkZsMcEJYqPNcFEMJ0VkiEWmChdWUaX8PVT+D9pFqz8qVW6KGVrpUUdaXAIjkAO5EEF1MA5qIMGIOAOPIAn8GzcG4/Gi/E6H00Zi5198APG2yfYPZaf</latexit>\nz\n( w )\n3\n<latexit sha1_base64=\"6oni0ouPRHGymUnXqx3TvCoJeyQ=\">AAAB/3icdVBLSwMxGMzWV62vquDFS7AI9bJkS7f2WPDisYJ9QFuXbDZtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk3EjzqRC6MPILS2vrK7l1wsbm1vbO8XdvbYMY0Foi4Q8FF0XS8pZQFuKKU67kaDYdzntuJOzzO9cUyFZGFyqaUQHPh4FbMgIVlpyigd9N+SenPr6Sm5Tx75KyjcnqVMsIRPV7KpVh8i0kVW3bE0qtoVQBVommqEEFmg6xfe+F5LYp4EiHEvZs1CkBgkWihFO00I/ljTCZIJHtKdpgH0qB8ksfwqPteLBYSj0CRScqd83EuzLLKKe9LEay99eJv7l9WI1rA8SFkSxogGZPzSMOVQhzMqAHhOUKD7VBBPBdFZIxlhgonRlBV3C10/h/6RdMa2aWb2olhrVRR15cAiOQBlY4BQ0wDloghYg4A48gCfwbNwbj8aL8TofzRmLnX3wA8bbJ5PSlm8=</latexit>\nz\n( w )\n5\n<latexit sha1_base64=\"SCpRfPzOUN6zXGNSVtw/YJX+pIE=\">AAAB/3icdVBLSwMxGMzWV62vVcGLl2AR6mXJlt3aY8GLxwr2AW0t2TRtQ7MPkqxS1z34V7x4UMSrf8Ob/8ZsW0FFB0KGme8jk/EizqRC6MPILS2vrK7l1wsbm1vbO+buXlOGsSC0QUIeiraHJeUsoA3FFKftSFDse5y2vMlZ5reuqZAsDC7VNKI9H48CNmQEKy31zYOuF/KBnPr6Sm7TfuUqKd2cpH2ziCxUcR27CpHlIrtqu5qUXRuhMrQtNEMRLFDvm+/dQUhinwaKcCxlx0aR6iVYKEY4TQvdWNIIkwke0Y6mAfap7CWz/Ck81soADkOhT6DgTP2+kWBfZhH1pI/VWP72MvEvrxOrYbWXsCCKFQ3I/KFhzKEKYVYGHDBBieJTTTARTGeFZIwFJkpXVtAlfP0U/k+aZcuuWM6FU6w5izry4BAcgRKwwSmogXNQBw1AwB14AE/g2bg3Ho0X43U+mjMWO/vgB4y3T5VclnA=</latexit>\nz\n( w )\n6\nM-Step: Jointly Update \n<latexit sha1_base64=\"0TM8bo79mtt8FX8GutUU2QyKdPg=\">AAAB/XicbVA7T8MwGHTKq5RXeGwsFhVSWaoEVcBGJRbGItGH1IbKcZzWquNEtgNqo4jfARMLAwixIv4GG/8Gp+0ALSdZPt19n3w+N2JUKsv6NnILi0vLK/nVwtr6xuaWub3TkGEsMKnjkIWi5SJJGOWkrqhipBUJggKXkaY7uMj85i0Rkob8Wg0j4gSox6lPMVJa6pp7HTdknhwG+kpG6U1SujtKu2bRKltjwHliT0nx/PMhw2Ota351vBDHAeEKMyRl27Yi5SRIKIoZSQudWJII4QHqkbamHAVEOsk4fQoPteJBPxT6cAXH6u+NBAUyC6gnA6T6ctbLxP+8dqz8MyehPIoV4XjykB8zqEKYVQE9KghWbKgJwoLqrBD3kUBY6cIKugR79svzpHFctk/KlSurWK2ACfJgHxyAErDBKaiCS1ADdYDBCDyBF/Bq3BvPxpvxPhnNGdOdXfAHxscPKz2aHA==</latexit>\nz\n( w )\n<latexit sha1_base64=\"0TM8bo79mtt8FX8GutUU2QyKdPg=\">AAAB/XicbVA7T8MwGHTKq5RXeGwsFhVSWaoEVcBGJRbGItGH1IbKcZzWquNEtgNqo4jfARMLAwixIv4GG/8Gp+0ALSdZPt19n3w+N2JUKsv6NnILi0vLK/nVwtr6xuaWub3TkGEsMKnjkIWi5SJJGOWkrqhipBUJggKXkaY7uMj85i0Rkob8Wg0j4gSox6lPMVJa6pp7HTdknhwG+kpG6U1SujtKu2bRKltjwHliT0nx/PMhw2Ota351vBDHAeEKMyRl27Yi5SRIKIoZSQudWJII4QHqkbamHAVEOsk4fQoPteJBPxT6cAXH6u+NBAUyC6gnA6T6ctbLxP+8dqz8MyehPIoV4XjykB8zqEKYVQE9KghWbKgJwoLqrBD3kUBY6cIKugR79svzpHFctk/KlSurWK2ACfJgHxyAErDBKaiCS1ADdYDBCDyBF/Bq3BvPxpvxPhnNGdOdXfAHxscPKz2aHA==</latexit>\nz\n( w )\n  and \n<latexit sha1_base64=\"0TM8bo79mtt8FX8GutUU2QyKdPg=\">AAAB/XicbVA7T8MwGHTKq5RXeGwsFhVSWaoEVcBGJRbGItGH1IbKcZzWquNEtgNqo4jfARMLAwixIv4GG/8Gp+0ALSdZPt19n3w+N2JUKsv6NnILi0vLK/nVwtr6xuaWub3TkGEsMKnjkIWi5SJJGOWkrqhipBUJggKXkaY7uMj85i0Rkob8Wg0j4gSox6lPMVJa6pp7HTdknhwG+kpG6U1SujtKu2bRKltjwHliT0nx/PMhw2Ota351vBDHAeEKMyRl27Yi5SRIKIoZSQudWJII4QHqkbamHAVEOsk4fQoPteJBPxT6cAXH6u+NBAUyC6gnA6T6ctbLxP+8dqz8MyehPIoV4XjykB8zqEKYVQE9KghWbKgJwoLqrBD3kUBY6cIKugR79svzpHFctk/KlSurWK2ACfJgHxyAErDBKaiCS1ADdYDBCDyBF/Bq3BvPxpvxPhnNGdOdXfAHxscPKz2aHA==</latexit>\nz\n( w )\n<latexit sha1_base64=\"0TM8bo79mtt8FX8GutUU2QyKdPg=\">AAAB/XicbVA7T8MwGHTKq5RXeGwsFhVSWaoEVcBGJRbGItGH1IbKcZzWquNEtgNqo4jfARMLAwixIv4GG/8Gp+0ALSdZPt19n3w+N2JUKsv6NnILi0vLK/nVwtr6xuaWub3TkGEsMKnjkIWi5SJJGOWkrqhipBUJggKXkaY7uMj85i0Rkob8Wg0j4gSox6lPMVJa6pp7HTdknhwG+kpG6U1SujtKu2bRKltjwHliT0nx/PMhw2Ota351vBDHAeEKMyRl27Yi5SRIKIoZSQudWJII4QHqkbamHAVEOsk4fQoPteJBPxT6cAXH6u+NBAUyC6gnA6T6ctbLxP+8dqz8MyehPIoV4XjykB8zqEKYVQE9KghWbKgJwoLqrBD3kUBY6cIKugR79svzpHFctk/KlSurWK2ACfJgHxyAErDBKaiCS1ADdYDBCDyBF/Bq3BvPxpvxPhnNGdOdXfAHxscPKz2aHA==</latexit>\nz\n( w )\n   (c) M-Step.\nFigure 3: One iteration of EM algorithm. During the E-Step, we compute new posterior topic-word distribution ùëû(ùë°|ùíõ(ùë§))that\nsharpens the original posterior ùëù(ùë°|ùíõ(ùë§))(resulting in lower entropy of ùëù(ùë°|ùíõ(ùë§))denoted by the smaller colored area around\nùíõ(ùë§)) and meanwhile encourage balanced cluster distribution (resulting in some cluster assignment change). During the M-\nStep, we update topic embeddings ùíï and word embeddings ùíõ(ùë§)= ùëì(ùíâ(ùë§))according to the new posteriors.\nwhere ùëù\n\u0010\nùíõ(ùë§)\nùëñ |ùë°ùëò\n\u0011\n= vMFùëü‚Ä≤(ùíïùëò,ùúÖ)= ùëõùëü‚Ä≤(ùúÖ)exp\n\u0010\nùúÖ¬∑cos(ùíõ(ùë§)\nùëñ ,ùíïùëò)\n\u0011\nand ùëù(ùë°ùëò)= 1/ùêæ according to Eq. (1). The posterior is simplified as\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n=\nexp\n\u0010\nùúÖ¬∑cos\n\u0010\nùíõ(ùë§)\nùëñ ,ùíïùëò\n\u0011\u0011\n√çùêæ\nùëò‚Ä≤=1 exp\n\u0010\nùúÖ¬∑cos\n\u0010\nùíõ(ùë§)\nùëñ ,ùíïùëò‚Ä≤\n\u0011\u0011 .\nThen we compute a new estimate of the cluster assignmentsùëû(ùë°ùëò|ùíõ(ùë§)\nùëñ )\nto be used for updating the model in the M-Step following [66]:\nùëû\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n=\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u00112\n/ùë†ùëò\n√çùêæ\nùëò‚Ä≤=1 ùëù\n\u0010\nùë°ùëò‚Ä≤\n\f\fùíõ(ùë§)\nùëñ\n\u00112\n/ùë†ùëò‚Ä≤\n, ùë†ùëò =\nùëÅ‚àëÔ∏Å\nùëñ=1\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n, (3)\nwhere ùëÅ is the total number of tokens in the corpus. Using Eq.(3) to\nobtain the target cluster assignment has the following two favorable\neffects: (1) Distinctive topic learning . Squaring-then-normalizing the\nposterior distribution ùëù(ùë°ùëò|ùíõ(ùë§)\nùëñ )has a sharpening effect that skews\nthe distribution towards its most confident cluster assignment, and\nthe so learned latent space will have gradually well-separated clus-\nters for distinctive topic interpretation. This is similar in spirit to the\nDirichlet prior used in LDA that promotes sparse topic distributions.\n(2) Topic prior regularization . The soft cluster frequency ùë†ùëò should\nencode the uniform topic prior assumed in Eq. (1), and dividing the\nsharpened ùëù(ùë°ùëò|ùíõ(ùë§)\nùëñ )2 by ùë†ùëò encourages balanced clusters.\nM-Step. We update the model parameters to maximize the expected\nlog-probability of the current cluster assignment under the new\ncluster assignment estimate Eùëû[log ùëù], which is equivalent to mini-\nmizing the following cross entropy loss:\nLclus = ‚àí\nùëÅ‚àëÔ∏Å\nùëñ=1\nùêæ‚àëÔ∏Å\nùëò=1\nùëû\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\nlog ùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n, (4)\nwhere ùëù is updated to approximate ùëûwhich is a fixed target. Using\nEq. (4) to update the model parameters has a notable difference\nfrom standard clustering algorithms: Since ùëù(ùë°ùëò|ùíõ(ùë§)\nùëñ )is jointly\ndetermined by the topic center vector ùíïùëò and latent representation\nùíõ(ùë§)\nùëñ , both of them will be updated to fit the new estimateùëû(ùë°ùëò|ùíõ(ùë§)\nùëñ )\nwhich encourages distinctive cluster distribution. Therefore, the\nmapping function ùëì will be adjusted accordingly to induce a latent\nspace with a ùêæ-cluster structure and the topic center vectors will\nbecome ùêæ anchoring points surrounded by topic-representative\nwords. In contrast, standard clustering algorithms only update the\ncluster parameters without changing the data representations.\nTopical Reconstruction of Documents. The second objective\naims to reconstruct document semantics with topic representa-\ntions so that the learned latent topics are meaningful summaries\nof the documents. Specifically, the reconstructed document embed-\nding ÀÜùíâ(ùëë)is obtained by combining all projected topic vectors ÀÜùíïùëò\nweighted by the document-topic distribution ùëù(ùë°ùëò|ùíõ(ùëë)):\nÀÜùíâ(ùëë)=\nùêæ‚àëÔ∏Å\nùëò=1\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùëë)\n\u0011\nÀÜùíïùëò, ÀÜùíïùëò = ùëî(ùíïùëò),\nwhere ùëù(ùë°ùëò|ùíõ(ùëë))is obtained according to Eq. (2):\nùëù\n\u0010\nùë°ùëò\n\f\fùíõ(ùëë)\n\u0011\n=\nexp\n\u0010\nùúÖ¬∑cos\n\u0010\nùíõ(ùëë),ùíïùëò\n\u0011\u0011\n√çùêæ\nùëò‚Ä≤=1 exp \u0000ùúÖ¬∑cos \u0000ùíõ(ùëë),ùíïùëò‚Ä≤\n\u0001\u0001 .\nWe require the reconstructed document embedding to be a good\napproximation of the original content by minimizing the following\nreconstruction loss:\nLrec =\n‚àëÔ∏Å\nùëë‚ààD\n\r\rÀÜùíâ(ùëë)‚àí¬Øùíâ(ùëë)\r\r2, (5)\nwhere ¬Øùíâ(ùëë)is the average of word embeddings in the document\nserving as the generic document embedding.\nPreservation of Original PLM Embeddings. We need to ensure\nthe latent space preserves the important semantic information of\nthe original embedding space, and the third objective encourages\nthe output of the autoencoder to faithfully recover the structure of\nthe original embedding space by minimizing the the following loss:\nLpre =\nùëÅ‚àëÔ∏Å\nùëñ=1\n\r\rùíâ(ùë§)\nùëñ ‚àíùëî\n\u0010\nùëì\n\u0010\nùíâ(ùë§)\nùëñ\n\u0011\u0011 \r\r2. (6)\nOverall Algorithm. We summarize the training of TopClus in Al-\ngorithm 1. We first pretrain the mapping functions ùëì and ùëîonly\nusing the preservation loss in Eq. (6) as it provides a stable ini-\ntialization of the latent space [66]. During training, we apply the\nEM algorithm to iteratively update all model parameters with the\nsummed objectives (the clustering loss is weighed by ùúÜ).\nComplexity. In the E-Step of the algorithm, ùëû(ùë°ùëò|ùíõ(ùë§)\nùëñ )is com-\nputed for every latent representation over each topic, resulting in\nan O(ùëÅùêæùëü‚Ä≤)complexity per iteration. The M-Step updates DNN pa-\nrameters whose complexity is related to the number of parameters\nin the model and the optimization method.\nAlgorithm 1: TopClus Training.\nInput: D: Text corpus; ùëÄ: PLM; ùêæ: Number of topics.\nParameter: ùë®: Attention mechanism parameters; ùëì,ùëî :\nEncoding/decoding functions; ùëª: Topic\nembeddings.\nHyperparameter: ùê∏: Training epochs; ùúÜ: Clustering loss\nweight.\nOutput: Topic-word distributions ùëù\n\u0010\nùëß(ùë§)\nùëñ\n\f\fùë°ùëò\n\u0011\n;\ndocument-topic distributions ùëù\n\u0010\nùë°ùëò\n\f\fùëß(ùëë)\n\u0011\n.\nùëì,ùëî ‚Üêarg minùëì,ùëî Lpre; // Pretrain ùëì,ùëî via Eq. (6);\nùëª = ùíïùëò\n\f\fùêæ\nùëò=1 ‚ÜêInitialize with ùêæ-means on Sùëü‚Ä≤‚àí1;\nfor ùëó ‚àà[1,2,...,ùê∏ ]do\n// E-Step: Update cluster assignment estimation;\nùëû\n\u0010\nùë°ùëò\n\f\fùíõ(ùë§)\nùëñ\n\u0011\n‚ÜêEq. (3);\n// M-Step: Update model parameters;\nùë®,ùëì,ùëî, ùëª ‚Üêarg minùë®,ùëì,ùëî,ùëª\n\u0000ùúÜLclus +Lrec +Lpre\n\u0001;\nreturn ùëù\n\u0010\nùëß(ùë§)\nùëñ\n\f\fùë°ùëò\n\u0011\n,ùëù\n\u0010\nùë°ùëò\n\f\fùëß(ùëë)\n\u0011\n;\n4 EXPERIMENTS\n4.1 Experiment Setup\nSettings. We use two benchmark datasets in different domains\nwith long/short texts for evaluation: (1) The New York Times an-\nnotated corpus ( NYT) [58]; and (2) The Yelp Review Challenge\ndataset (Yelp). The dataset statistics can be found in Table 4. The\nimplementation details and parameters of TopClus are shown in\nAppendix C. For both datasets, we set the number of topicsùêæ = 100\nfor all compared methods.\nCompared Methods. We compare TopClus with the following\nstrong baselines:\n‚Ä¢LDA [11]: LDA is the standard topic model that learns topic-\nword and document-topic distributions by modeling the generative\nprocess of the corpus.\n‚Ä¢CorEx [19]: CorEx does not rely on generative assumptions and\nlearns maximally informative topics measured by total correlation.\n‚Ä¢ETM [17]: ETM models word topic correlations via distributed\nrepresentations to improve the expressiveness of topic models.\n‚Ä¢BERTopic [23]: BERTopic first clusters document embeddings\nfrom BERT and then uses TF-IDF to extract topic representative\nwords, which does not leverage word embeddings from PLMs.\n4.2 Topic Discovery Evaluation\nEvaluation Metrics. We evaluate the quality of the topics from\ntwo aspects: topic coherence and topic diversity . Good topic results\nshould be both coherent for humans to interpret and diverse to cover\nmore information about the corpus. We evaluate the effectiveness\nof document-level topic modeling by document clustering.\nFor topic coherence, we use three metrics including both human\nand automatic evaluations:\n‚Ä¢UMass [49]: UMass computes the log-conditional probability\nof every top word in each topic given every other top word that\nhas a higher order in the ranking of that topic. The probability is\ncomputed based on document-level word co-occurrence.\n‚Ä¢UCI [51]: UCI computes the average pointwise mutual informa-\ntion of all pairs of top words in each topic. The word co-occurrence\ncounts are derived using a sliding window of size 10.\n‚Ä¢Intrusion: Given the top terms of a topic, we inject an intrusion\nterm that is randomly chosen from another topic. Then a human\nevaluator is asked to identify the intruded term. The more coher-\nent the top terms are, the more likely an evaluator can correctly\nidentify the fake term, and thus we compute the ratio of correctly\nidentified intrusion instances as the topic coherence score given\nby the intrusion test. The topics from all compared methods are\nrandomly shuffled during evaluation to avoid the bias of human\nevaluators.\nFor topic diversity, we report the percentage of unique words in\nthe top words of all topics following the definition in [17].\nQualitative Evaluation. We randomly select several ground truth\ntopics from both datasets, and manually match the most relevant\ntopic generated by all methods. Table 1 shows the top- 5 words\nper topic. All methods are able to generate relevant topics to the\nground truth ones. LDA and CorEx results contain noises that\nare semantically irrelevant to the topic; ETM improves LDA by\nincorporating word embeddings, but still generates slightly off-\ntopic terms; BERTopic also has noisy terms in the results, as it uses\nTF-IDF metrics without exploiting word representations from BERT\nfor obtaining top words.TopClus consistently outputs coherent and\nmeaningful topics.\nQuantitative Evaluation. We report the performance of all meth-\nods under the four metrics in Table 2. Overall, the quantitative\nevaluation coincides with the previous qualitative results. TopClus\ngenerates not only the most coherent but also diverse topics, under\nboth automatic and human evaluations.\n4.3 Document Clustering Evaluation\nEvaluation Metrics. We use the learned latent document embed-\nding ùíõ(ùëë)as the feature toùêæ-Means for obtaining document clusters,\nthen we report the Normalized Mutual Information (NMI) score be-\ntween the clustering results and the ground truth document labels.\nWe use the topic label set (e.g., politics, sports) and location label\nset (e.g., United States, China) on the NYT dataset. The detailed\nlabel statistics can be found in [ 39]. On the two label sets, the\ndocument-topic distribution learned by TopClus consistently yields\nthe best clustering results among all methods as shown in Table 3.\n4.4 Study of TopClus Training\nJoint Learning Latent Space and Clustering Improves Topic\nQuality. Figure 5 shows the improvement in topic quality (mea-\nsured by both intrusion test score and topic diversity) and document\nclustering performance during TopClus training. At epoch 0, the\nresult is equivalent to first applying dimensionality reduction (i.e.,\npretraining autoencoder with Lpre) and then clustering with ùêæ-\nmeans, the ‚Äúnaive approach‚Äù mentioned in the second paragraph of\nSection 3.2. Its inferior performance confirms that conducting the\ntwo steps separately does not generate satisfactory topics. Topic\nquality and document clustering performance improve when the\nmodel is trained longer, showing that joint latent space learning and\nclustering indeed helps generate coherent and distinctive topics.\nVisualization. To intuitively understand howTopClusjointly learns\nthe latent space structure and performs clustering, we visualize the\nlearned latent embeddings at different training epochs in Figure 4.\nTable 1: Qualitative evaluation of topic discovery. We select several ground truth topics and manually find the most relevant\ntopic generated by all methods. Words not strictly belonging to the corresponding topic are italicized and underlined.\nMethods\nNYT Yelp\nTopic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5\n(sports) (politics) (research) (france) (japan) (positive) (negative) (vegetables) (fruits) (seafood)\nLDA\nolympic mr said french japanese amazing loud spinach mango fish\nyear bush report union tokyo really awful carrots strawberry roll\nsaid president evidence germany year place sunday greens vanilla salmon\ngames white findings workers matsui phenomenal like salad banana fresh\nteam house defense paris said pleasant slow dressing peanut good\nCorEx\nbaseball house possibility french japanese great even garlic strawberry shrimp\nchampionship white challenge italy tokyo friendly bad tomato caramel beef\nplaying support reasons paris index atmosphere mean onions sugar crab\nfans groups give francs osaka love cold toppings fruit dishes\nleague member planned jacques electronics favorite literally slices mango salt\nETM\nolympic government approach french japanese nice disappointed avocado strawberry fish\nleague national problems students agreement worth cold greek mango shrimp\nnational plan experts paris tokyo lunch review salads sweet lobster\nbasketball public move german market recommend experience spinach soft crab\nathletes support give american european friendly bad tomatoes flavors chips\nBERTopic\nswimming bush researchers french japanese awesome horrible tomatoes strawberry lobster\nfreestyle democrats scientists paris tokyo atmosphere quality avocado mango crab\npopov white cases lyon ufj friendly disgusting soups cup shrimp\ngold bushs genetic minister company night disappointing kale lemon oysters\nolympic house study billion yen good place cauliflower banana amazing\nTopClus\nathletes government hypothesis french japanese good tough potatoes strawberry fish\nmedalist ministry methodology seine tokyo best bad onions lemon octopus\nolympics bureaucracy possibility toulouse osaka friendly painful tomatoes apples shrimp\ntournaments politicians criteria marseille hokkaido cozy frustrating cabbage grape lobster\nquarterfinal electoral assumptions paris yokohama casual brutal mushrooms peach crab\n‚ñ°100 0 100\n‚ñ°100\n0\n100\n(a) Epoch 0.\n‚ñ°100 0 100\n‚ñ°100\n0\n100 (b) Epoch 2.\n‚ñ°100 0 100\n‚ñ°100\n0\n100 (c) Epoch 4.\n‚ñ°100 0 100\n‚ñ°100\n0\n100 (d) Epoch 8.\nFigure 4: Visualization using t-SNE of 3,000 randomly sampled latent word embeddings during training. Embeddings assigned\nto the same cluster are in the same color. The latent space gradually exhibits distinctive and balanced cluster structure.\nBefore the training starts (epoch0), the latent embedding space does\nnot have clear cluster structures, just like the original space. During\ntraining, the latent embeddings are becoming well-separated and\nthe cluster structure is gradually more distinctive and balanced,\nresulting in coherent and diverse topics.\n5 RELATED WORK\n5.1 Topic Models\nTopic models aim to discover underlying topics and semantic struc-\ntures from text corpora. Despite extensive studies of topic models\nfollowing LDA, most approaches suffer from one or more of the\nfollowing limitations: (1) The ‚Äúbag-of-words‚Äù assumption that pre-\nsumes words in the document are generated independently from\neach other. (2) The reliance on local corpus statistics , which could\nbe improved by leveraging general knowledge such as pretrained\nlanguage models [ 16]. (3) The intractable posterior that requires\napproximation techniques during model inference.\nTopic modeling approaches can be divided into three major cat-\negories: (1) LDA-based approaches use pLSA [26] or LDA [11] as\nthe backbone. The idea is to characterize documents as mixtures of\nlatent topics and represent each topic as a distribution over words.\nPopular models in this category include Hierarchical LDA [ 22],\nDynamic Topic Models [8], Correlated Topic Models [7], Pachinko\nTable 2: Quantitative evaluation of topic discovery. We eval-\nuate all methods with three topic coherence metrics UCI,\nUMAss and Intrusion (Int.) and a topic diversity (Div.) met-\nric. Higher score means better for all metrics. We do not re-\nport Div. for CorEx because it requires topics to have non-\noverlapping words by design.\nMethods NYT Yelp\nUMass UCI Int. Div. UMass UCI Int. Div.\nLDA -3.75 -1.76 0.53 0.78 -4.71 -2.47 0.47 0.65\nCorEx -3.83 -0.96 0.77 - -4.75 -1.91 0.43 -\nETM -2.98 -0.98 0.67 0.30 -3.04 -0.33 0.47 0.16\nBERTopic -3.78 -0.51 0.70 0.61 -6.37 -2.05 0.73 0.36\nTopClus -2.67 -0.45 0.93 0.99 -1.35 -0.27 0.87 0.96\nTable 3: Document clustering NMI scores on NYT\n(Topic/Location label set).\nLDA CorEx ETM BERTopic TopClus\n0.39/0.20 0.29/0.20 0.41/0.21 0.26/0.22 0.46 /0.28\n0 1 2 4 10\nTraining Epochs\n0.4\n0.6\n0.8\n1.0Score\nIntrusion\nDiversity\n(a) Topic Quality.\n0 1 2 4 6 8 10\nTraining Epochs\n0.2\n0.3\n0.4\n0.5NMI\nTopic\nLocation (b) Document Clustering.\nFigure 5: Study of TopClustraining on NYT. We show (a) topic\ncoherence measured by intrusion test and topic diversity\nand (b) document clustering NMI scores over training.\nAllocation [34], Supervised Topic Models [10] and Labeled LDA [55].\nMost of these models suffer from all three limitations mentioned\nabove. (2) Topic models with word embeddings have been broadly\nstudied after word2vec [48] came out. The common strategy is to\nconvert the discrete text into continuous representations of em-\nbeddings, and then adapt LDA to generate real-valued data. Such\nkind of models include Gaussian LDA [15], Spherical Hierarchical\nDirichlet Process [3] and WELDA [12]. There are some other strate-\ngies combining topic modeling and word embedding. For example,\nLFTM [52] models a mixture of the multinomial distribution and a\nlink function between word and topic embeddings. TWE [35] uses\npretrained topic structures to learn topic embeddings and improve\nword embeddings. Although these models consider word embed-\ndings to make up for the ‚Äúbag-of-words‚Äù assumption, they are not\nequipped with general knowledge from pretrained language mod-\nels. (3) Neural topic models are inspired by deep generative models\nsuch as VAE [32]. NVDM [47] encodes documents with variational\nposteriors in the latent topic space. Instead, ProdLDA [60] proposes\na Laplace approximation of Dirichlet distributions to enable repa-\nrameterization. Although these neural topic models improve the\nposterior approximation with neural networks, they still do not\nutilize general knowledge such as pretrained language models.\n5.2 Pretrained Language Models\nBengio et al. [ 4] propose the Neural Network Language Model\nwhich pioneers the study of modern word embedding. Mikolov\net al. [48] introduce two architectures, CBOW and Skip-Gram, to\ncapture local context semantics of each word.\nAlthough word embeddings have been shown effective in NLP\ntasks, they are context-independent. Meanwhile, most NLP tasks\nare beyond word-level, thus it is beneficial to derive word seman-\ntics based on specific contexts. Therefore, contextualized PLMs are\nwidely studied recently. For example, BERT [16] and RoBERTa [36]\nadopt masked token prediction as the pretraining task to leverage\nbidirectional contexts. XLNet [67] proposes a new pretraining objec-\ntive on a random permutation of input sequences. ELECTRA [14],\nCOCO-LM [43] and AMOS [44] use a generator to replace some\ntokens of a sequence and predict whether a token is replaced given\nits surrounding context. For more related studies, one can refer\nto a recent survey [54]. There have been a few recent studies that\nattempt to incorporate PLM representations into the topic modeling\nframework for different purposes [ 6, 13, 24, 28, 61]. By contrast,\nour approach features a latent space clustering framework that\nleverages the inherent representations of PLMs for topic discovery\nwithout following the topic modeling setup.\n6 CONCLUSION\nWe explore a new alternative to topic models via latent space cluster-\ning of PLM representations. We first analyze the challenges of using\nPLM embeddings to generate topic structures, and then propose\na joint latent space learning and clustering approach TopClus to\naddress the identified challenges. TopClus generates coherent and\ndistinctive topics and outperforms strong topic modeling baselines\nin both topic quality and topical document representations. We also\nconduct studies to provide insights on how the joint learning setup\nin TopClus gradually improves the generated topic quality.\nTopClus is conceptually simple which facilitates future exten-\nsions such as integrating with new PLMs and advanced clustering\ntechniques. TopClus may also be extended to perform hierarchical\ntopic discovery, perhaps via top-down clustering in the latent space.\nOther related tasks like taxonomy construction [30] and weakly-\nsupervised text classification [29, 41, 42, 45, 68] may benefit from\nthe coherent and distinctive topics generated by TopClus.\nACKNOWLEDGMENTS\nResearch was supported in part by US DARPA KAIROS Program\nNo. FA8750-19-2-1004, SocialSim Program No. W911NF-17-C-0099,\nand INCAS Program No. HR001121C0165, National Science Founda-\ntion IIS-19-56151, IIS-17-41317, and IIS 17-04532, and the Molecule\nMaker Lab Institute: An AI Research Institutes program supported\nby NSF under Award No. 2019897. Any opinions, findings, and\nconclusions or recommendations expressed herein are those of\nthe authors and do not necessarily represent the views, either ex-\npressed or implied, of DARPA or the U.S. Government. Yu Meng is\nsupported by the Google PhD Fellowship. We thank anonymous\nreviewers for valuable and insightful feedback.\nREFERENCES\n[1] Hagai Attias. 2000. A variational Bayesian framework for graphical models.\nNIPS.\n[2] Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. 2005.\nClustering on the Unit Hypersphere using von Mises-Fisher Distributions. J.\nMach. Learn. Res. (2005).\n[3] Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, and Sam Gersh-\nman. 2016. Nonparametric spherical topic modeling with word embeddings. In\nACL.\n[4] Yoshua Bengio, R√©jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A\nneural probabilistic language model. JMLR (2003).\n[5] Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, and Uri Shaft. 1999.\nWhen is ‚Äúnearest neighbor‚Äù meaningful?. In ICDT.\n[6] Federico Bianchi, Silvia Terragni, and Dirk Hovy. 2021. Pre-training is a Hot\nTopic: Contextualized Document Embeddings Improve Topic Coherence. InACL.\n[7] David Blei and John Lafferty. 2006. Correlated topic models. In NIPS.\n[8] David M Blei and John D Lafferty. 2006. Dynamic topic models. In ICML.\n[9] David M Blei, John D Lafferty, et al. 2007. A correlated topic model of science.\nThe annals of applied statistics (2007).\n[10] David M Blei and Jon D Mcauliffe. 2008. Supervised topic models. In NIPS.\n[11] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet\nAllocation. In NIPS.\n[12] Stefan Bunk and Ralf Krestel. 2018. WELDA: Enhancing Topic Models by Incor-\nporating Local Word Context. In JCDL.\n[13] Yatin Chaudhary, Pankaj Gupta, Khushbu Saxena, Vivek Kulkarni, Thomas A.\nRunkler, and Hinrich Sch√ºtze. 2020. TopicBERT for Energy Efficient Document\nClassification. In EMNLP Findings .\n[14] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2020.\nELECTRA: Pre-training text encoders as discriminators rather than generators.\narXiv preprint arXiv:2003.10555 (2020).\n[15] Rajarshi Das, Manzil Zaheer, and Chris Dyer. 2015. Gaussian LDA for topic\nmodels with word embeddings. In ACL.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL.\n[17] Adji B Dieng, Francisco JR Ruiz, and David M Blei. 2020. Topic modeling in\nembedding spaces. TACL (2020).\n[18] J. R. Firth. 1957. A synopsis of linguistic theory 1930-55. (1957).\n[19] Ryan J Gallagher, Kyle Reing, David Kale, and Greg Ver Steeg. 2017. Anchored\ncorrelation explanation: Topic modeling with minimal domain knowledge. TACL\n(2017).\n[20] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A.\nSmith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in\nLanguage Models. In EMNLP Findings .\n[21] Siddharth Gopal and Yiming Yang. 2014. Von Mises-Fisher Clustering Models. In\nICML.\n[22] Thomas L Griffiths, Michael I Jordan, Joshua B Tenenbaum, and David M Blei.\n2004. Hierarchical topic models and the nested Chinese restaurant process. In\nNIPS.\n[23] Maarten Grootendorst. 2020. BERTopic: Leveraging BERT and c-TF-IDF to create\neasily interpretable topics. Zenodo: 10.5281/zenodo.4430182 (2020).\n[24] Pankaj Gupta, Yatin Chaudhary, and Hinrich Sch√ºtze. 2021. Multi-source Neural\nTopic Modeling in Multi-view Embedding Spaces. In NAACL.\n[25] Geoffrey E Hinton and Richard S Zemel. 1993. Autoencoders, Minimum Descrip-\ntion Length and Helmholtz Free Energy. In NIPS.\n[26] Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM\nTOIS (2004).\n[27] Kurt Hornik. 1991. Approximation capabilities of multilayer feedforward net-\nworks. Neural networks (1991).\n[28] Alexander Miserlis Hoyle, Pranav Goel, and Philip Resnik. 2020. Improving\nNeural Topic Models Using Knowledge Distillation. In EMNLP.\n[29] Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, and Jiawei Han. 2020. Weakly-\nSupervised Aspect-Based Sentiment Analysis via Joint Aspect-Sentiment Topic\nEmbedding. In EMNLP.\n[30] Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, and Jiawei Han. 2020. CoRel:\nSeed-Guided Topical Taxonomy Construction by Concept Learning and Relation\nTransferring. In KDD.\n[31] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\nmization. In ICLR.\n[32] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[33] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. 2020.\nOn the Sentence Embeddings from Pre-trained Language Models. In EMNLP.\n[34] Wei Li and Andrew McCallum. 2006. Pachinko allocation: DAG-structured\nmixture models of topic correlations. In ICML.\n[35] Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical word\nembeddings. In AAAI.\n[36] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[37] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\ninformation theory (1982).\n[38] Xinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020. PowerTrans-\nformer: Unsupervised Controllable Revision for Biased Language Correction. In\nEMNLP.\n[39] Yu Meng, Jiaxin Huang, Guangyuan Wang, Zihan Wang, Chao Zhang, Yu Zhang,\nand Jiawei Han. 2020. Discriminative topic mining via category-name guided\ntext embedding. In WWW.\n[40] Yu Meng, Jiaxin Huang, Guangyuan Wang, Chao Zhang, Honglei Zhuang,\nLance M. Kaplan, and Jiawei Han. 2019. Spherical Text Embedding. In NeurIPS.\n[41] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2018. Weakly-Supervised\nNeural Text Classification. In CIKM.\n[42] Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han. 2019. Weakly-Supervised\nHierarchical Text Classification. In AAAI.\n[43] Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei\nHan, and Xia Song. 2021. COCO-LM: Correcting and contrasting text sequences\nfor language model pretraining. In NeurIPS.\n[44] Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei\nHan, and Xia Song. 2022. Pretraining Text Encoders with Adversarial Mixture of\nTraining Signal Generators. In ICLR.\n[45] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang,\nand Jiawei Han. 2020. Text Classification Using Label Names Only: A Language\nModel Self-Training Approach. In EMNLP.\n[46] Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao Zhang, and Jiawei Han.\n2020. Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding. In\nKDD.\n[47] Yishu Miao, Lei Yu, and Phil Blunsom. 2016. Neural variational inference for text\nprocessing. In ICML.\n[48] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.\n2013. Distributed Representations of Words and Phrases and their Composition-\nality. In NIPS.\n[49] David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew\nMcCallum. 2011. Optimizing semantic coherence in topic models. In EMNLP.\n[50] Radford M Neal. 1993. Probabilistic inference using Markov chain Monte Carlo\nmethods.\n[51] David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic\nevaluation of topic coherence. In NAACL.\n[52] Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. 2015. Improv-\ning topic models with latent feature word representations. TACL (2015).\n[53] Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W. Black.\n2018. Style Transfer Through Back-Translation. In ACL.\n[54] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing\nHuang. 2020. Pre-trained Models for Natural Language Processing: A Survey.\narXiv preprint arXiv:2003.08271 (2020).\n[55] Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D Manning. 2009.\nLabeled LDA: A supervised topic model for credit attribution in multi-labeled\ncorpora. In EMNLP.\n[56] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In EMNLP-IJCNLP.\n[57] Tim Salimans, Diederik Kingma, and Max Welling. 2015. Markov chain monte\ncarlo and variational inference: Bridging the gap. In ICML.\n[58] Evan Sandhaus. 2008. The New York Times Annotated Corpus.\n[59] Suzanna Sia, Ayush Dalmia, and Sabrina J Mielke. 2020. Tired of Topic Models?\nClusters of Pretrained Word Embeddings Make for Fast and Good Topics Too!.\nIn EMNLP.\n[60] Akash Srivastava and Charles Sutton. 2017. Autoencoding Variational Inference\nFor Topic Models. In ICLR.\n[61] Laure Thompson and David Mimno. 2020. Topic Modeling with Contextualized\nWord Representation Clusters. ArXiv abs/2010.12626 (2020).\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NIPS.\n[63] Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong Gong. 2009. Multi-document\nsummarization using sentence-based topic models. In ACL.\n[64] Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye,\nand Haiyang Xu. 2020. Neural Topic Modeling with Bidirectional Adversarial\nTraining. In ACL.\n[65] Xuerui Wang, Andrew McCallum, and Xing Wei. 2007. Topical n-grams: Phrase\nand topic discovery, with an application to information retrieval. In ICDM.\n[66] Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding\nfor clustering analysis. In ICML.\n[67] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and\nQuoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. In NeurIPS.\n[68] Yu Zhang, Shweta Garg, Yu Meng, Xiusi Chen, and Jiawei Han. 2022. MotifClass:\nWeakly Supervised Text Classification with Higher-order Metadata Information.\nIn WSDM.\nA ETHICAL CONSIDERATIONS\nPLMs have been shown to contain potential biases [ 53] which\nmay be carried to the downstream applications. Our work focuses\non using representations from PLMs for discovery of topics in a\ntarget corpus, and the results will be related to both the PLMs and\nthe corpus statistics. We suggest applying our method together\nwith bias reduction and correction techniques for PLMs [ 20, 38]\nand filtering out biased contents in the target corpus to mitigate\npotential risks and harms.\nB PROOF OF THEOREM 2.1\nProof. The MLM objective of BERT trains contextualized word\nembeddings to predict the masked tokens in a sequence. Formally,\ngiven an input sequence ùíÖ = [ùë§1,ùë§2,...,ùë§ ùëõ], a random subset of\ntokens (e.g., usually 15% from the original sequence) Mis selected\nand replaced with [MASK] symbols. Then the BERT encoder maps\nthe masked sequence ÀÜùíÖ to a sequence of contextualized represen-\ntations [ùíâ1,ùíâ2,..., ùíâùëõ]where ùíâùëñ ‚ààRùëü (ùëü = 768 in the BERT base\nmodel). BERT is trained by maximizing the log-probability of cor-\nrectly predicting every masked word with a Softmax layer over the\nvocabulary ùëâ:\nmax\nùíÜ,ùíâ,ùíÉ\n‚àëÔ∏Å\nùë§ùëñ ‚ààM\nlog\nexp\n\u0010\nùíÜ‚ä§ùë§ùëñ ùíâùëñ +ùëèùë§ùëñ\n\u0011\n√ç|ùëâ|\nùëó=1 exp\n\u0010\nùíÜ‚ä§ùë§ùëó ùíâùëñ +ùëèùë§ùëó\n\u0011, (7)\nwhere ùíÜùë§ùëñ ‚ààRùëü is the token embedding; andùëèùë§ùëñ ‚ààR is a bias value\nfor token ùë§ùëñ.\nNext, we construct a multivariate GMM parameterized by the\nlearned token embeddings ùíÜ and bias vector ùíÉ of BERT, and we\nshow that the MLM objective (Eq. (7)) optimizes the posterior prob-\nability of contextualized embeddings ùíâ generated from this GMM.\nWe consider the following GMM with |ùëâ|mixture components,\nwhere each component ùëñ is a multivariate Gaussian distribution\nN(ùùÅùëñ,ùö∫ùëñ)with mean vector ùùÅùëñ ‚ààRùëü, covariance matrix ùö∫ùëñ ‚ààRùëü√óùëü\nand mixture weight ùúãùëñ (i.e., the prior probability) defined as follows:\nùùÅùëñ B ùö∫ ùíÜùë§ùëñ , ùö∫ùëñ B ùö∫, ùúãùëñ B\nexp\n\u0010\n1\n2 ùíÜ‚ä§ùë§ùëñ ùö∫ ùíÜùë§ùëñ +ùëèùë§ùëñ\n\u0011\n√ç\n1‚â§ùëó‚â§|ùëâ|exp\n\u0010\n1\n2 ùíÜ‚ä§ùë§ùëó ùö∫ ùíÜùë§ùëó +ùëèùë§ùëó\n\u0011,\nwhere all components share the same covariance matrix ùö∫.\nThe contextualized embeddings ùíâùëñ are generated by first sam-\npling a token ùë§ùëñ according to the prior distribution, and then sam-\npling from the Gaussian distribution corresponding to ùë§ùëñ, as fol-\nlows:\nùë§ùëñ ‚àºCategorical(ùùÖ), ùíâùëñ ‚àºN \u0000ùö∫ ùíÜùë§ùëñ , ùö∫\u0001 .\nBased on the above generative process, the prior probability of\ntoken ùë§ùëñ is\nùëù(ùë§ùëñ)= ùúãùëñ =\nexp\n\u0010\n1\n2 ùíÜ‚ä§ùë§ùëñ ùö∫ ùíÜùë§ùëñ +ùëèùë§ùëñ\n\u0011\n√ç|ùëâ|\nùëó=1 exp\n\u0010\n1\n2 ùíÜ‚ä§ùë§ùëó ùö∫ ùíÜùë§ùëó +ùëèùë§ùëó\n\u0011,\nand the likelihood of generating ùíâùëñ given ùë§ùëñ is\nùëù(ùíâùëñ|ùë§ùëñ)=\nexp\n\u0010\n‚àí1\n2 (ùíâùëñ ‚àíùö∫ ùíÜùë§ùëñ )‚ä§ùö∫‚àí1 \u0000ùíâùëñ ‚àíùö∫ ùíÜùë§ùëñ\n\u0001\u0011\n(2ùúã)ùëü/2|ùö∫|1/2 .\nThe posterior probability can be obtained using the Bayes rule:\nùëù(ùë§ùëñ|ùíâùëñ)= ùëù(ùíâùëñ|ùë§ùëñ)ùëù(ùë§ùëñ)\n√ç|ùëâ|\nùëó=1 ùëù\u0000ùíâùëñ|ùë§ùëó\n\u0001 ùëù(ùë§ùëó)\n,\nwhere the numerator ùëù(ùíâùëñ|ùë§ùëñ)ùëù(ùë§ùëñ)is\nexp\n\u0010\n‚àí1\n2 ùíâ‚ä§\nùëñ ùö∫‚àí1ùíâùëñ +ùíâ‚ä§\nùëñ ùíÜùë§ùëñ ‚àí\u0018\u0018\u0018\u0018\u00181\n2 ùíÜ‚ä§ùë§ùëñ ùö∫ ùíÜùë§ùëñ +\u0018\u0018\u0018\u0018\u00181\n2 ùíÜ‚ä§ùë§ùëñ ùö∫ ùíÜùë§ùëñ +ùëèùë§ùëñ\n\u0011\n(2ùúã)ùëü/2|ùö∫|1/2 √ç|ùëâ|\nùëó=1 exp\n\u0010\n1\n2 ùíÜ‚ä§ùë§ùëó ùö∫ ùíÜùë§ùëó +ùëèùë§ùëó\n\u0011 .\nThe terms in the denominator are in a similar form and many com-\nmon factors between the numerator and the denominator cancel\nout. Finally, the above posterior probability is simplified as:\nùëù(ùë§ùëñ|ùíâùëñ)=\nexp\n\u0010\nùíÜ‚ä§ùë§ùëñ ùíâùëñ +ùëèùë§ùëñ\n\u0011\n√ç|ùëâ|\nùëó=1 exp\n\u0010\nùíÜ‚ä§ùë§ùëó ùíâùëñ +ùëèùë§ùëó\n\u0011,\nwhich is precisely the probability maximized by the MLM objective\n(Eq. (7)). Therefore, the MLM pretraining objective of BERT assumes\nthat the contextualized representations are generated from a |ùëâ|-\ncomponent GMM. ‚ñ°\nC IMPLEMENTATION DETAILS AND\nPARAMETERS\nWe preprocess the corpora by discarding infrequent words that\nappear less than 5 times. We use the default hyperparameters of\nbaseline methods. The hyperparameters of TopClus are set as fol-\nlows: Latent space dimension ùëü‚Ä≤ = 100; training epochs ùê∏ = 20;\nclustering loss weight ùúÜ = 0.1; DNN hidden dimensions are 500-\n500-1000 for learning ùëì and 1000-500-500 for learning ùëî; the shared\nconcentration parameter of topic vMF distributions ùúÖ = 10. We\nuse the BERT [ 16] base model to obtain pretrained embeddings,\nand use Adam [31] with 5ùëí‚àí4 learning rate to optimize the DNNs\nwith batch size 32. When computing the generic document as an\naverage of word embeddings in Eq. (5), we only use the words\nthat are nouns, verbs, or adjectives because they are usually the\ntopic-indicative ones.\nTable 4: Dataset statistics.\nCorpus # documents # words/doc. Vocabulary\nNYT 31,997 690 25,903\nYelp 29,280 114 11,419",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8033195734024048
    },
    {
      "name": "Topic model",
      "score": 0.7988585233688354
    },
    {
      "name": "Inference",
      "score": 0.6603591442108154
    },
    {
      "name": "Cluster analysis",
      "score": 0.6062767505645752
    },
    {
      "name": "Artificial intelligence",
      "score": 0.59189373254776
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5794110894203186
    },
    {
      "name": "Language model",
      "score": 0.5591835379600525
    },
    {
      "name": "Representation (politics)",
      "score": 0.5371108651161194
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5355883240699768
    },
    {
      "name": "Natural language processing",
      "score": 0.5277909636497498
    },
    {
      "name": "Space (punctuation)",
      "score": 0.5265482068061829
    },
    {
      "name": "Word (group theory)",
      "score": 0.49055737257003784
    },
    {
      "name": "Machine learning",
      "score": 0.3273624777793884
    },
    {
      "name": "Information retrieval",
      "score": 0.3263664245605469
    },
    {
      "name": "Linguistics",
      "score": 0.1206708550453186
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}