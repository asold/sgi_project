{
  "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
  "url": "https://openalex.org/W4389519864",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4321838749",
      "name": "Henry Sprueill",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2137583965",
      "name": "Carl Edwards",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4281360557",
      "name": "Mariefel Olarte",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2585053567",
      "name": "Udishnu Sanyal",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2127420617",
      "name": "Heng Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153547168",
      "name": "Sutanay Choudhury",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2771222095",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3198449425",
    "https://openalex.org/W4242308659",
    "https://openalex.org/W4376644341",
    "https://openalex.org/W4385573003",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4321085451",
    "https://openalex.org/W2042880492",
    "https://openalex.org/W4385572408",
    "https://openalex.org/W3211951295",
    "https://openalex.org/W4378771157",
    "https://openalex.org/W4284882687",
    "https://openalex.org/W1973836541",
    "https://openalex.org/W4220681836",
    "https://openalex.org/W4327564965",
    "https://openalex.org/W2778051509",
    "https://openalex.org/W3158834551",
    "https://openalex.org/W4365211638",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W3045928028",
    "https://openalex.org/W4295846611",
    "https://openalex.org/W3014689923",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4287126984",
    "https://openalex.org/W4324026004",
    "https://openalex.org/W4320342754",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4385572965",
    "https://openalex.org/W4389888290",
    "https://openalex.org/W4318718899",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W4295951229",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W3109892317",
    "https://openalex.org/W4293998609",
    "https://openalex.org/W4247259022",
    "https://openalex.org/W4205731604",
    "https://openalex.org/W2038702914",
    "https://openalex.org/W4323572088",
    "https://openalex.org/W3126401649",
    "https://openalex.org/W4361989906",
    "https://openalex.org/W4365211632",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4285659244"
  ],
  "abstract": "Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8% and find that our approach can augment scientist’s reasoning and discovery process with novel insights.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8348–8365\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMonte Carlo Thought Search: Large Language Model Querying for\nComplex Scientific Reasoning in Catalyst Design\nHenry W. Sprueill1, Carl Edwards2, Mariefel V . Olarte1, Udishnu Sanyal1,\nHeng Ji2, Sutanay Choudhury1\n1Pacific Northwest National Laboratory, Richland, Washington, USA\n2University of Illinois Urbana-Champaign, Urbana, Illinois, USA\n{henry.sprueill, mariefel.olarte, udishnu.sanyal, sutanay.choudhury}@pnnl.gov\n{cne2, hengji}@illinois.edu\nAbstract\nDiscovering novel catalysts requires complex\nreasoning involving multiple chemical proper-\nties and resultant trade-offs, leading to a combi-\nnatorial growth in the search space. While large\nlanguage models (LLM) have demonstrated\nnovel capabilities for chemistry through com-\nplex instruction following capabilities and high\nquality reasoning, a goal-driven combinatorial\nsearch using LLMs has not been explored in\ndetail. In this work, we present a Monte Carlo\nTree Search-based approach that improves be-\nyond state-of-the-art chain-of-thought prompt-\ning variants to augment scientific reasoning.\nWe introduce two new reasoning datasets: 1)\na curation of computational chemistry simu-\nlations, and 2) diverse questions written by\ncatalysis researchers for reasoning about novel\nchemical conversion processes. We improve\nover the best baseline by 25.8% and find that\nour approach can augment scientist’s reasoning\nand discovery process with novel insights.1\n1 Introduction\nScientific discovery thrives on uncovering the opti-\nmal combinations of factors that maximize a prop-\nerty of interest. For example, to discover new ef-\nficient fuels (Yang et al., 2019; Tran et al., 2023;\nZitnick et al., 2020) or chemical conversion pro-\ncesses requiring less energy, a scientist would need\nto consider the chemical reaction, the reactants that\nundergo the reaction, the catalysts that improve the\nrate of reaction, and find the optimal combination\nof operating conditions (Fig. 2). Mathematically,\none could represent this as an optimization prob-\nlem where we model a chemical process as a func-\ntion and formulate the search problem as finding\nthe optimal combination of all process parameters\nthat minimizes a cost function modeled around en-\nergy efficiency. For highly empirical fields such\nas chemistry, these combinatorial searches require\n1Our codebase and datasets are freely available at https:\n//github.com/pnnl/chemreasoner\nFigure 1: An example prompt design via tree search.\nThe search begins with a generic query at the root node.\nThe answer from each node is passed to the child nodes\nand additional criterion are added to the prompt. For\ninstance, low cost. Information passed to children nodes\nis color coded to show the reasoning pathway.\nexpert reasoning with knowledge of the scientific\nliterature that dates back a century. The emerg-\ning capability of large language models (LLMs)\n(Wei et al., 2022; Ouyang et al., 2022; Taylor et al.,\n2022; Lai et al., 2023; OpenAI, 2023) provides an\nopportunity to automatically reason with a large\nknowledge space in a human-interpretable way.\nDespite their promise, the brittleness of language\nmodels to their inputs and hallucination remain ar-\neas for concern (Creswell and Shanahan, 2022; Tay-\nlor et al., 2022). Our initial investigation of LLMs\nrevealed that basic prompting (such as \"What is a\ngood catalyst for reaction X?\") leads to basic an-\nswers that could be found on a Wikipedia page. To\nimprove the quality of answers, one can incorpo-\nrate desirable properties into the prompt which lead\nthe LLM to produce more specific answers (such\nas \"What is a good catalyst with low cost for reac-\ntion X?\"). Additionally, LLMs often hallucinate,\nproducing answers without grounding in scientific\nfact. Achieving accurate answers with high speci-\n8348\nFigure 2: Illustration of the combinatorial thinking used by human experts to reason about a catalyst (best viewed\nin color). They successively “think in terms of\" different constraints and factors, each of which are related via\nscientific principles, and narrow down the set of possible candidates. Our Monte Carlo Reasoner emulates such\ncognitive thinking by prompting a language model with different combinations, yielding a tree-structured space of\nqueries and potential candidates, and returns the optimal answer via efficient exploration of the possible space.\nficity and which use key technical terminology (Fig.\n2) is essential to earn the scientific community’s\ntrust and pave the way for the adoption of machine\nreasoning systems.\nIn this work, we focus on the problem of prompt-\ning an LLM to return the top- k catalysts for a\ngiven chemical reaction and generating the rea-\nsoning for each candidate. In collaboration with\nresearchers from the catalysis community, we de-\nvelop a new dataset, BioFuels Question Reason-\ning (BioFuelQR), consisting of complex reasoning\nquestions and answers. We observe the reason-\ning pathways used by domain scientists and con-\nclude that it is important for LLMs to progress from\n“thinking step-by-step” to “thinking step-by-step in\nterms of relevant properties”. In this setting, we\nare given a question which has some relevant cata-\nlyst properties P,|P|= n(e.g. {“crystal planes”,\n“toxicity”}) and we want to identify the best sub-\nset R ⊂P,|R|= rof properties for the language\nmodel to “think” in terms of. Considering that lan-\nguage models are sensitive to permutations of their\ninputs, there are Pn\nr = n!\n(n−r)! possible prompts to\nsearch through. This goal can be accomplished by\nlearning to prompt the LLM with the most relevant\nsubset of properties (Deng et al., 2022) or decom-\nposing the set into a sequence of chained queries\n(Dohan et al., 2022). In both cases, identification\nof the prompt-generating property subset becomes\nthe limiting factor.\nTo solve this problem, we propose the Monte\nCarlo Reasoner (MCR), a generic heuristic search\nmethodology that addresses the combinatorial chal-\nlenge of query decomposition. Considering the\npractical challenges of learning prompts that are\nboth human comprehensible (a key consideration\nfor scientists) and provide the best performance\n(Deng et al., 2022), we pursue a stochastic, heuris-\ntic search-based approach that leverages LLMs\ntrained on scientific literature with sophisticated\ninstruction following capabilities (Ouyang et al.,\n2022).\nWe formulate the task as a search problem in\nwhich an agent performs a query in an uncertain\nenvironment (represented by the LLM) and de-\ntermines a query variant to pursue based on the\nevaluated reward. Given an initial query, we con-\nstruct a tree structure of these unique query vari-\nants in order to progressively refine the original\nquery (the root) into property-specific variations\n(the leaves). Our methodology demonstrates im-\nprovement over basic querying of the LLM without\nany additional training of the LLM. Instead, we\nuse a Monte Carlo Tree Search algorithm (MCTS)\nto perform a stochastic search over the existing\nknowledge space of an LLM to achieve more sci-\nentifically valuable answers.\nOur second major contribution is demonstrating\nthe efficacy of using a scientific domain-specific re-\nward function in LLM-based computations for our\ntop-kcatalyst problem. Estimation of the “adsorp-\ntion energy” of a chemical structure is at the core\nof developing efficient chemical reactions (see Ap-\npendix A.2 for details). Finding catalysts that can\nenable chemical reactions with the least amount of\nexternal energy is key to developing environmen-\n8349\ntally friendly industrial processes. In this work, we\nimplement such energy function specific consid-\nerations via a LLM-derived reward function. Our\nexperiments (using questions detailed in Table 4)\nshow that even a simplistic reward function dramat-\nically improves the specificity of answers and their\nassociated reasoning from the LLM.\nIn summary, we make the following contributions:\n1. We presentMonte Carlo Reasoner (MCR), an al-\ngorithm to prompt LLMs for zero-shot complex\nreasoning tasks involving combinatorial search.\n2. We introduce a new chemistry-focused dataset,\nBioFuelQR, that captures key reasoning chal-\nlenges in hypothesis generation and testing\nfaced daily by scientists. We present in-depth\nqualitative analysis of MCR on BioFuelQR.\n3. We demonstrate that a domain-specific reward\nfunction that represents a fundamental scientific\nconcept can lead to dramatic improvement in\nthe quality and specificity of LLM answers.\n2 Monte Carlo Reasoner\nProblem definition Our goal is find the optimal\nprompt, Po, which leads the LLM to return the best\ncandidate catalysts for a specific problem. Start-\ning with a general initial prompt P0, we use a set\nof actions to automatically modify the prompt to\nimprove the LLM output with respect to a reward\nfunction, R.\nFor instance, suppose P0 is the prompt given in\nFigure 2(left). Each prompt is a template, where we\nuse actions a∈A to create better prompts, based\non how experts might modify their own queries, so\nthat the LLM will suggest superior catalysts. See\nAppendix C.1 for a more detailed explanation of\nthe actions and prompt. By modifying prompts, we\ncreate a tree of prompts, answers, and rewards, as\ndemonstrated in Figure 1. We call a path from the\nroot to a leaf node a “reasoning pathway”. These\nreasoning pathways can be constructed in several\ndifferent ways. For instance, we can take an action\nto introduce additional catalyst properties to con-\nsider (such as “composition of metals\" and “elec-\ntronic structure\" in Fig. 2 (right)) so that the LLM\nwill include or exclude certain catalysts in its an-\nswer. Also, for each prompt P after P0, we in-\nclude P’s parent node’s answer in P to provide\nthe LLM with additional context about the previ-\nous answer. Further, at each node, we prompt the\nLLM to produce catalysts with either “new ele-\nments”, “similar elements”, or “different elements”\nto the parent node’s answer candidates (switching\nbetween these possibilities is an action). Finally,\nwe can take an action to change the type of cat-\nalyst requested (unary, binary, ternary, or -oxide\ncatalysts). Clearly, the number of possible reason-\ning pathways increases drastically with tree depth\ndue to the possible combinations of actions. Thus,\nwe apply Monte Carlo Tree Search, an efficient\nmethod to optimize a sequence of actions with a\nreward function, R.\nIn MCTS, each prompt P is stored as a node\nin a tree T, where edges are prompt-action pairs\n(Pi,aj). The search tree decides at each prompt\nwhich action to take to obtain the best reward based\non previous results. Typically, prompt-action pairs\nare weighted by a policy, which determinesa-priori\nthe importance of each action for a prompt, given\nas prior probabilities. Here, we assign equal weight\nto all possible actions. Impossible actions are as-\nsigned weight of 0 (see Appendix C.2).\nIn MCTS, each edge stores a count N(P,a), a\nweight representing a prior probabilityp(P,a), and\nthe total downstream reward V(P,a) where\nV(P,a) =\n∑\nP′∈successor(P)\nγdR(P′) (1)\nHere, γis a discount factor and dis the (tree) dis-\ntance of P′ from P. If there are no discovered\nsuccessors to P, then we set V(P,a) = 0. The\nsearch determines the next action to take with pol-\nicy P(V,N,p ):\nargmax\nai∈A\n(\nV (P,ai)\nN(P,ai) + cp(P,ai)\n√∑\nj N(P,aj)\n1+N(P,ai)\n)\n(2)\nwhere c is an exploration-exploitation trade-off.\nThe simulation starts at the root node each time\nand traverses the constructed tree until a new state\nis reached. Then, its answer and reward are calcu-\nlated, stored, and the upstream values of V, N are\nupdated. This is repeated to generate the desired\nnumber of prompts (in our case 300). MCTS is\nsuperior to re-sampling methods because it avoids\nrepeatedly sampling the same prompt and it is su-\nperior to brute-force tree search methods such as\nBFS and DFS because it selects trajectories in the\ntree that show promising results.\n2.1 Reward Function\nOur reward function, R, measures the effective-\nness of the catalysts proposed by the LLM for a\ngiven prompt, P. Here, we measure effectiveness\n8350\nAlgorithm 1: Run MCR search. Here, at\nindicates tth action from root.\n1 Require: LLM, initial prompt P0, number\nof candidate catalysts k, number of\nprompts to generate M\n2 Initialize tree T. Define nodes P and edges\n(P,aj), discount γ, stored values N(P,aj),\nV(P,aj), p(P,aj), and reward function R.\n3 root(T) ←P0\n4 for j = 1,...,M do\n5 Current Node P0 = root(T)\n6 Current Depth t←0\n7 while Pt ∈T do\n8 Select at ∼P(Pt,ai,N,V,p )\n9 Increment N(Pt,at)\n10 Pt+1 = at(Pt) \u0003 Apply\naction\n11 Increment t\n12 end\n13 Send Pt to LLM\n14 Save Pt, (Pt,at) in T\n15 r←R(Pt) \u0003 Calculate reward\nusing answer from LLM\n16 for t′= t−1,..., 0 do\n17 V(P′\nt,at′\n) =V(P′\nt,at′\n) +γt−t′\nr\n18 end\n19 end\n20 return arg maxP∈T (R(P))\nof a catalyst by querying the LLM to produce ad-\nsorption energies for a given adsorbate in electron\nvolts (eV). We describe the prompt used to gener-\nate the adsorption energy in Appendix C.1. The\nsignificance of adsorption energy for catalysis de-\nsign is explained in Appendix A.2. The reward is\ncalculated as\nR(P) =\n∑\na∈adsorbates\n|LLM(a,C(P))| (3)\nwhere C(P) is the top-kcatalysts from prompt P.\n3 Experiments\nExperimental setup We conduct our experiments\non two new chemistry-focused reasoning query\nbenchmarks containing 130 queries (Table 2). We\ncompile OpenCatalysis from the OC20 (Chanus-\nsot et al., 2010) and OC22 (Tran et al., 2023)\ncatalyst datasets (Zitnick et al., 2020). Second,\nwe develop BioFuelQR–a query dataset targeting\nbiofuels-focused catalyst discovery (see Table 4\nTable 1: Final catalyst suggestion results. NP is number\nof prompts evaluated and dmax is maximum search tree\ndepth. Values are averaged over evaluated examples.\nOpenCatalysis BioFuelQR\nMethod RewardNP dmax RewardNP dmax\nCoT 2.04 1 N/A 2.27 1 N/A\nCoT w/ Self-consistency4.04 10 N/A 6.38 10 N/A\nToT (breadth-first-search)9.91 253 5 13.8 253 5\nMCR (ours) 12.47 301 9.33 15.6 301 9.5\nfor an example). We collected two answers from\ncatalysis researchers for a subset of 51 queries to\nobserve different reasoning patterns and human bi-\nases. See section C for details on dataset design.\nBaselines We benchmark MCR’s performance\nwith three recent methods: 1) Chain-Of-Thought\n(CoT) prompting (Kojima et al., 2022), 2) Self-\nconsistency-based CoT (Wang et al., 2022), 3)\nbreadth-first-search (BFS) based Tree-of-Thoughts\n(ToT) (Yao et al., 2023) (a contemporary work\nto ours). Experiments are based on GPT-3 text-\ndavinci-003 2. Table 1 shows MCR improves by\n25.8% and 13% over the reward obtained by BFS\non OpenCatalysis and BioFuelQR, respectively.\nPerformance improves by ∼600% over CoT.\nQuery Cost Despite significant effort with the\ndataset creation, our results in Table 2 are obtained\nfrom 11/130 queries. MCR and baselines are im-\nplemented using OpenAI text-davinci-003 for con-\nsistency. MCR and ToT method is computation-\nally expensive (Table 2), so evaluation of all 130\nqueries over all methods requires approximately\n174,470 API calls, and we could not secure com-\npute capacity from OpenAI to evaluate more than\n11 queries with each method. We further discuss\nthe limitations that arose in Limitations (4).\nKey Takeaways We find that MCR’s use of\nstochastic search prunes the more uniform explo-\nration of search space conducted by ToT (Yao et al.,\n2023). Table 2 shows given a maximum query limit,\nMCR was able to search significantly deeper (re-\nported by dmax) than ToT. While MCR reached\na higher reward than ToT, MCR generated more\nnodes than ToT (see C.4). However, we are not able\nto definitively declare that both tree-based methods\noutperformed CoT and CoT w/ Self-consistency.\nTo confirm if the increased reward over CoT in-\ndeed translates into better reasoning quality, two\ncatalysis experts compared the best answer gener-\nated by MCR with the GPT-3 CoT implementation.\nOverall, the experts preferred MCR to CoT (Fig-\n2OpenAI has now discontinued support for this model. In\nthe future, using open models to avoid this issue would be\ndesirable, but we found their performance lacking.\n8351\nFigure 3: Domain expert evaluation of LLM answers on\nthe reasoning path to the final node with highest reward.\nures 8, 9). Figures 5-9 illustrates the evaluation for\none such query. Review from both experts (Fig-\nures 8, 9) deemed the CoT response in Figure 6\nincorrect and MCR correct (Figure 8).\nThe experts also evaluated how the prompts and\nLLM answers evolve as MCR searches deeper in\nthe prompt tree (Figures 3 and 10)–in many cases\nthey found the LLM answers to be logically co-\nherent and in some cases even insightful enough\nfor follow-up experimentation (see the second user\nfeedback in Figure 3). Overall, both experts pre-\nferred MCR for having higher specificity over CoT\nand reasoning in terms of correct properties (de-\ntailed in Figures 8, 9).\n4 Conclusion and Future Work\nLLMs offer major promise to automate the cycle\nof scientific hypothesis generation and testing. Our\nwork tackles the challenge of identifying key prop-\nerties for augmenting a chemist’s reasoning via use\nof a domain-specific reward function, enabling gen-\neration of relevant scientific explanations with high\nspecificity. MCR is a zero-shot reasoning method-\nology that circumvents the need for large-scale,\nhard-to-obtain, domain-specific training data. We\napply it to catalyst research: a highly empirical,\nreasoning-heavy scientific field dating back a cen-\ntury. Future work can investigate large-scale evalu-\nation of our benchmark, integration with atomistic\nprediction models trained on quantum chemistry\ndatasets for more trustworthy reward functions, and\nfinetuned language models.\nLimitations\nIn this work, we consider applications of large lan-\nguage models in the scientific domain. In general,\nthis comes with a number of limitations. First,\nLLMs display largely black box behavior, which\nis exacerbated by many strong models only being\naccessible as APIs. Second, generative modeling\nin the scientific domain is incredibly difficult to\nevaluate, requiring laboratory verification in many\nsettings. Third, hallucination about factual informa-\ntion is a concern. One benefit of our method is that\nit provides reasonings based on refined prompts,\nwhich we show can be inspirational to domain ex-\nperts searching for a solution.\nOur work demonstrates that tree-search methods\nhave a strong value proposition over existing meth-\nods for LLM reasoning (CoT, self-consistency etc.).\nSince ToT is contemporary to our methodology, an\nimportant contribution of this work is demonstrat-\ning the merit of tree-based reasoning approaches\nfor complex scientific reasoning tasks; scientific\nreasoning is not discussed in (Yao et al., 2023). We\ndo not claim that MCR is necessarily superior to\nToT in all settings. In fact, further experiments\nhave shown the two methods can be quite compa-\nrable. However, we are limited in this work by the\ncost of experimentation that we cannot perform an\nideal comparison of MCR to ToT.\nIn particular, our reward function based on LLM\noutputs of scientific questions can be considered\na limitation. However, it allows for much quicker\nvalidation of ideas and we find it to be an effec-\ntive proxy (which on its own is interesting). In\nthe future, comparatively costly atomistic simula-\ntions can be used to replace our reward function.\nThese can be quite time-consuming and computa-\ntionally expensive, so we focus on our algorithmic\n8352\ncontribution in this work. Because of the efficacy\nwe demonstrate using LLM rewards, it may also\nbe possible to use a hybrid approach to save on\ncomputational chemistry simulations. This could\ninitially leverage LLM embeddings as an initial re-\nward to narrow down promising search sub-trees by\nselecting the most promising nodes in the first few\nlayers of the search tree. Advanced simulations can\nthen be used for searching final answers in these\nsub-trees. Alternatively, simulations can be used\nas a limited-use oracle like in active learning. We\nleave this for future work.\nOur method’s improvement comes with higher\ncost of inference, similar to Tree-of-Thought.\nWhen doing inference locally, this may not be a\nproblem. However, we utilize third-party APIs\nwhich are both expensive and rate-limited. We\nfound existing open-source models trained on\nchemistry text did not possess sufficient instruction-\nfollowing capabilities to be reliable or effective\nhere. Thus, we were limited in quantity of exper-\niments that could be done, as well as the models\nwhich could be accessed. This is because our ap-\nproach requires an average of 750 API calls per tree\nsearch. Although we evaluate on relatively few ini-\ntial questions, our in-depth expert-performed anal-\nysis is based on ∼7,200 queries.\nEthical Considerations\nWe propose a zero-shot prompting methodology for\nLLMs that enables reasoning for complex queries\nin the scientific domain. Like most applications of\nLLMs, this has similar ethical considerations, espe-\ncially in regards to implicit biases from large-scale\npretraining and the hallucination of false informa-\ntion. Thus, it is still important for human oversight\nand careful evaluation of language model output.\nOne consideration of our method is that it may en-\nable discovery of molecules, materials, or other\nscientific products which can be used for harm-\nful applications (Urbina et al., 2022). Overall, we\nbelieve these downsides are outweighed by the ben-\nefits of this work to both the NLP community and\nother scientific communities which may benefit.\nAcknowledgements\nThis work is supported in part by the PNNL seed\nLaboratory Directed Research and Development\n(LDRD) program, Data-Model Convergence initia-\ntive. This work is also in part based upon work\nsupported by the Molecule Maker Lab Institute: an\nAI research institute program supported by NSF\nunder award No. 2019897 and No. 2034562. The\nviews and conclusions contained herein are those\nof the authors and should not be interpreted as\nnecessarily representing the official policies, ei-\nther expressed or implied, of the U.S. Government.\nThe U.S. Government is authorized to reproduce\nand distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.\nReferences\nJens Artz, Thomas E Müller, Katharina Thenert, Jo-\nhanna Kleinekorte, Raoul Meys, André Sternberg,\nAndré Bardow, and Walter Leitner. 2018. Sustain-\nable conversion of carbon dioxide: an integrated re-\nview of catalysis and life cycle assessment. Chemical\nreviews, 118(2):434–504.\nDaniil A Boiko, Robert MacKnight, and Gabe Gomes.\n2023. Emergent autonomous scientific research ca-\npabilities of large language models. arXiv preprint\narXiv:2304.05332.\nAndres M Bran, Sam Cox, Andrew D White, and\nPhilippe Schwaller. 2023. Chemcrow: Augmenting\nlarge-language models with chemistry tools. arXiv\npreprint arXiv:2304.05376.\nMustafa Canakci and Jon Van Gerpen. 1999. Biodiesel\nproduction viaacid catalysis. Transactions of the\nASAE, 42(5):1203–1210.\nCayque Monteiro Castro Nascimento and André Silva\nPimentel. 2023. Do large language models un-\nderstand chemistry? a conversation with chatgpt.\nJournal of Chemical Information and Modeling ,\n63(6):1649–1655.\nL Chanussot, A Das, S Goyal, T Lavril, M Shuaibi,\nM Riviere, K Tran, J Heras-Domingo, C Ho, W Hu,\net al. 2010. The open catalyst 2020 (oc20) dataset\nand community challenges. arxiv. arXiv.\nShuan Chen and Yousung Jung. 2022. A generalized-\ntemplate-based graph neural network for accurate\norganic reactivity prediction. Nature Machine Intelli-\ngence, 4(9):772–780.\nAustin H Cheng, Andy Cai, Santiago Miret, Gustavo\nMalkomes, Mariano Phielipp, and Alán Aspuru-\nGuzik. 2023. Group selfies: a robust fragment-based\nmolecular string representation. Digital Discovery.\nSeyone Chithrananda, Gabe Grand, and Bharath\nRamsundar. 2020. Chemberta: Large-scale self-\nsupervised pretraining for molecular property pre-\ndiction. arXiv preprint arXiv:2010.09885.\nDimitrios Christofidellis, Giorgio Giannone, Jannis\nBorn, Ole Winther, Teodoro Laino, and Matteo Man-\nica. 2023. Unifying molecular and textual represen-\ntations via multi-task language modelling. arXiv\npreprint arXiv:2301.12586.\n8353\nAntonia Creswell and Murray Shanahan. 2022. Faith-\nful reasoning using large language models. arXiv\npreprint arXiv:2208.14271.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. arXiv\npreprint arXiv:2205.09712.\nYolanda A Daza and John N Kuhn. 2016. Co 2 con-\nversion by reverse water gas shift catalysis: compari-\nson of catalysts, mechanisms and their consequences\nfor co 2 conversion to liquid fuels. RSC advances,\n6(55):49675–49691.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yi-\nhan Wang, Han Guo, Tianmin Shu, Meng Song, Eric\nXing, and Zhiting Hu. 2022. RLPrompt: Optimizing\ndiscrete text prompts with reinforcement learning.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3369–3391, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. arXiv preprint arXiv:2207.10342.\nCarl Edwards, Tuan Lai, Kevin Ros, Garrett Honke,\nKyunghyun Cho, and Heng Ji. 2022. Translation\nbetween molecules and natural language. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 375–413,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nCarl Edwards, ChengXiang Zhai, and Heng Ji. 2021.\nText2Mol: Cross-modal molecule retrieval with nat-\nural language queries. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 595–607, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nBenedek Fabian, Thomas Edlich, Héléna Gaspar, Mar-\nwin Segler, Joshua Meyers, Marco Fiscato, and Mo-\nhamed Ahmed. 2020. Molecular representation learn-\ning with language models and domain-relevant auxil-\niary tasks. arXiv preprint arXiv:2011.13230.\nJohannes Gasteiger, Florian Becker, and Stephan Gün-\nnemann. 2021. Gemnet: Universal directional graph\nneural networks for molecules. Advances in Neural\nInformation Processing Systems, 34:6790–6802.\nGlen M Hocky and Andrew D White. 2022. Natural\nlanguage processing models that automate program-\nming will transform chemistry research and teaching.\nDigital discovery, 1(2):79–83.\nKevin Maik Jablonka, Philippe Schwaller, Andres\nOrtega-Guerrero, and Berend Smit. 2023. Is gpt-\n3 all you need for low-data discovery in chemistry?\nChemRxiv preprint.\nWengong Jin, Regina Barzilay, and Tommi Jaakkola.\n2018. Junction tree variational autoencoder for\nmolecular graph generation. In International confer-\nence on machine learning, pages 2323–2332. PMLR.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.\narXiv preprint arXiv:2205.11822.\nShyam Kattel, Ping Liu, and Jingguang G Chen. 2017.\nTuning selectivity of co2 hydrogenation reactions at\nthe metal/oxide interface. Journal of the American\nChemical Society, 139(29):9739–9754.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nMario Krenn, Florian Häse, AkshatKumar Nigam, Pas-\ncal Friederich, and Alan Aspuru-Guzik. 2020. Self-\nreferencing embedded strings (selfies): A 100% ro-\nbust molecular string representation. Machine Learn-\ning: Science and Technology, 1(4):045024.\nTuan M. Lai, Chengxiang Zhai, and Heng Ji. 2023.\nKnowledge-enhanced biomedical language models.\nIn Journal of Biomedical Informatics.\nShengchao Liu, Weili Nie, Chengpeng Wang, Jiarui\nLu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei\nXiao, and Anima Anandkumar. 2022. Multi-modal\nmolecule structure-text model for text-based retrieval\nand editing. arXiv preprint arXiv:2212.10789.\nShengchao Liu, Jiongxiao Wang, Yijin Yang, Cheng-\npeng Wang, Ling Liu, Hongyu Guo, and Chaowei\nXiao. 2023a. Chatgpt-powered conversational drug\nediting using retrieval and domain feedback. arXiv\npreprint arXiv:2305.18090.\nShengchao Liu, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili\nNie, Anthony Gitter, Chaowei Xiao, Jian Tang,\nHongyu Guo, and Anima Anandkumar. 2023b. A\ntext-guided protein design framework. arXiv preprint\narXiv:2302.04611.\nZequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang\nXie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023c.\nMolxpt: Wrapping molecules with text for generative\npre-training. arXiv preprint arXiv:2305.10688.\nAhmad Mukhtar, Sidra Saqib, Hongfei Lin, Mansoor\nUl Hassan Shah, Sami Ullah, Muhammad Younas,\nMashallah Rezakazemi, Muhammad Ibrahim, Abid\nMahmood, Saira Asif, et al. 2022. Current status\nand challenges in the heterogeneous catalysis for\nbiodiesel production. Renewable and Sustainable\nEnergy Reviews, 157:112012.\nJens K Nørskov, Frank Abild-Pedersen, Felix Studt, and\nThomas Bligaard. 2011. Density functional theory in\nsurface chemistry and catalysis. Proceedings of the\nNational Academy of Sciences, 108(3):937–943.\n8354\nNVIDIA Corporation. 2022. Megamolbart v0.2.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nMayk Caldas Ramos, Shane S Michtavy, Marc D\nPorosoff, and Andrew D White. 2023. Bayesian opti-\nmization of catalysts with in-context learning. arXiv\npreprint arXiv:2304.05341.\nKristof T Schütt, Huziel E Sauceda, P-J Kinder-\nmans, Alexandre Tkatchenko, and K-R Müller. 2018.\nSchnet–a deep learning architecture for molecules\nand materials. The Journal of Chemical Physics ,\n148(24):241722.\nPhilippe Schwaller, Teodoro Laino, Théophile Gaudin,\nPeter Bolgar, Christopher A Hunter, Costas Bekas,\nand Alpha A Lee. 2019. Molecular transformer: a\nmodel for uncertainty-calibrated chemical reaction\nprediction. ACS central science, 5(9):1572–1583.\nPhilippe Schwaller, Daniel Probst, Alain C Vaucher,\nVishnu H Nair, David Kreutter, Teodoro Laino, and\nJean-Louis Reymond. 2021. Mapping the space of\nchemical reactions using attention-based neural net-\nworks. Nature Machine Intelligence, 3(2):144–152.\nPhilipp Seidl, Andreu Vall, Sepp Hochreiter, and\nGünter Klambauer. 2023. Enhancing activity pre-\ndiction models in drug discovery with the ability\nto understand human language. arXiv preprint\narXiv:2303.03363.\nJacek K Stolarczyk, Santanu Bhattacharyya, Lakshmi-\nnarayana Polavarapu, and Jochen Feldmann. 2018.\nChallenges and prospects in solar water splitting and\nco2 reduction with inorganic and hybrid nanostruc-\ntures. ACS Catalysis, 8(4):3602–3635.\nThomas J Struble, Juan C Alvarez, Scott P Brown, Mi-\nlan Chytil, Justin Cisar, Renee L DesJarlais, Ola En-\ngkvist, Scott A Frank, Daniel R Greve, Daniel J Grif-\nfin, et al. 2020. Current and future roles of artificial\nintelligence in medicinal chemistry synthesis. Jour-\nnal of medicinal chemistry, 63(16):8667–8682.\nBing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiang-\nmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and Ji-\nRong Wen. 2022. A molecular multimodal founda-\ntion model associating molecule graphs with natural\nlanguage. arXiv preprint arXiv:2209.05481.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nRichard Tran, Janice Lan, Muhammed Shuaibi, Bran-\ndon M Wood, Siddharth Goyal, Abhishek Das, Javier\nHeras-Domingo, Adeesh Kolluru, Ammar Rizvi,\nNima Shoghi, et al. 2023. The open catalyst 2022\n(oc22) dataset and challenges for oxide electrocata-\nlysts. ACS Catalysis, 13(5):3066–3084.\nEmma P Tysinger, Brajesh K Rai, and Anton V Sinitskiy.\n2023. Can we quickly learn to “translate” bioactive\nmolecules with transformer models? Journal of\nChemical Information and Modeling , 63(6):1734–\n1744.\nFabio Urbina, Filippa Lentzos, Cédric Invernizzi, and\nSean Ekins. 2022. Dual use of artificial-intelligence-\npowered drug discovery. Nature Machine Intelli-\ngence, 4(3):189–191.\nAndreu Vall, Sepp Hochreiter, and Günter Klambauer.\n2021. Bioassayclr: Prediction of biological activity\nfor novel bioassays based on rich textual descriptions.\nIn ELLIS ML4Molecules workshop.\nAlain C Vaucher, Philippe Schwaller, Joppe Geluykens,\nVishnu H Nair, Anna Iuliano, and Teodoro Laino.\n2021. Inferring experimental procedures from text-\nbased representations of chemical reactions. Nature\ncommunications, 12(1):2573.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv\npreprint arXiv:2203.11171.\nLogan Ward, Jenna A Bilbrey, Sutanay Choudhury,\nNeeraj Kumar, and Ganesh Sivaraman. 2021. Bench-\nmarking deep graph generative models for optimiz-\ning new drug molecules for covid-19. arXiv preprint\narXiv:2102.04977.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nDavid Weininger. 1988. Smiles, a chemical language\nand information system. 1. introduction to methodol-\nogy and encoding rules. Journal of chemical infor-\nmation and computer sciences, 28(1):31–36.\nDavid Weininger, Arthur Weininger, and Joseph L\nWeininger. 1989. Smiles. 2. algorithm for genera-\ntion of unique smiles notation. Journal of chemical\ninformation and computer sciences, 29(2):97–101.\nAndrew D White, Glen M Hocky, Heta A Gandhi,\nMehrad Ansari, Sam Cox, Geemi P Wellawatte, Sub-\narna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh,\net al. 2022. Do large language models know chem-\nistry? ChemRxiv preprint.\nAndrew D White, Glen M Hocky, Heta A Gandhi,\nMehrad Ansari, Sam Cox, Geemi P Wellawatte, Sub-\narna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh,\net al. 2023. Assessment of chemistry knowledge in\n8355\nlarge language models that generate code. Digital\nDiscovery, 2(2):368–376.\nHanwen Xu and Sheng Wang. 2022. Protranslator:\nzero-shot protein function prediction using textual\ndescription. In Research in Computational Molecu-\nlar Biology: 26th Annual International Conference,\nRECOMB 2022, San Diego, CA, USA, May 22–25,\n2022, Proceedings, pages 279–294. Springer.\nMinghao Xu, Xinyu Yuan, Santiago Miret, and Jian\nTang. 2023. Protst: Multi-modality learning of pro-\ntein sequences and biomedical texts. arXiv preprint\narXiv:2301.12040.\nShenzhen Xu and Emily A Carter. 2018. Theoretical in-\nsights into heterogeneous (photo) electrochemical\nco2 reduction. Chemical reviews, 119(11):6631–\n6669.\nWenhong Yang, Timothy Tizhe Fidelis, and Wen-Hua\nSun. 2019. Machine learning in catalysis, from pro-\nposal to practicing. ACS omega, 5(1):83–88.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nJiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and\nJure Leskovec. 2018. Graph convolutional policy net-\nwork for goal-directed molecular graph generation.\nAdvances in neural information processing systems,\n31.\nZheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\n2022. A deep-learning system bridging molecule\nstructure and biomedical text with comprehension\ncomparable to human professionals. Nature commu-\nnications, 13(1):862.\nWenyu Zhao, Dong Zhou, Buqing Cao, Kai Zhang, and\nJinjun Chen. 2023. Adversarial modality alignment\nnetwork for cross-modal molecule retrieval. IEEE\nTransactions on Artificial Intelligence.\nC Lawrence Zitnick, Lowik Chanussot, Abhishek Das,\nSiddharth Goyal, Javier Heras-Domingo, Caleb Ho,\nWeihua Hu, Thibaut Lavril, Aini Palizhati, Morgane\nRiviere, et al. 2020. An introduction to electrocat-\nalyst design using machine learning for renewable\nenergy storage. arXiv preprint arXiv:2010.09435.\nA Background\nA.1 Scientific Drivers from Catalysis\nDiscovery of novel catalysts is essential for accel-\nerating the transition to a sustainable future. De-\nspite the significant progress in the development\nof highly efficient catalysts, heterogeneous cataly-\nsis remains largely an empirical science owing to\nthe complexity of the underlying surface chemistry\n(Nørskov et al., 2011). Currently, there is a lack\nof data and design guidelines for heterogeneous\ncatalysis because the computational cost of obtain-\ning accurate theoretical models for such complex\nsystems is currently prohibitively high while high-\nthroughput experimental methods that have been\napplied successfully in related fields have not yet\nbeen thoroughly explored (Yang et al., 2019).\nExperimental validation of a new catalyst and its\nperformance is expensive (Yang et al., 2019). Artifi-\ncial intelligence-driven computing approaches aims\nto accelerate such discovery by down-selecting can-\ndidates that are most promising and merit extensive\nevaluation in a laboratory (Ward et al., 2021). The\npast few years have seen a lot of developments for\napplying AI to chemistry that range from predict-\ning properties of atomistic structures, or outcomes\nof reactions (Schwaller et al., 2019; Chen and Jung,\n2022). Generative models (Jin et al., 2018) or deep\nreinforcement learning methods (You et al., 2018)\nhave demonstrated abilities to propose novel chem-\nical compounds that satisfy unique property con-\nstraints, and then suggest synthesis pathways for\nproducing such compounds (Struble et al., 2020).\nGenerally, such models are trained on representa-\ntions of atomistic structures, or reactions between\nmultiple structures (Struble et al., 2020; Chen and\nJung, 2022).\nA.2 Motivation for molecular energy\nprediction as a reward function\nElectronic structure calculations play a crucial role\nin developing atomistic-level understanding of the\ninteraction of liquid or gaseous molecules with\nsolids, as a functional of the topological property\nof the solid surface (Nørskov et al., 2011). Much of\nthe literature from machine-learning for atomistic\nsystems have focused on training system-level prop-\nerties such as potential energy functions (Schütt\net al., 2018; Gasteiger et al., 2021). The follow-\ning paragraph explains why estimating the energy\nfunctions associated with a molecular structure is\ncritical to discovering processes with lower energy\nrequirements.\nThe amount of usable energy for a physical sys-\ntem with constant temperature and pressure is re-\nferred to as the Gibbs free energy, or Gibbs energy\nand is defined as: G = H−TS, where H is the\nenergy contained in the bonds between atoms, T is\nthe temperature and Sis the entropy (Zitnick et al.,\n2020). The entropy of a system increases when\n8356\nmolecules break their bonds and decreases when\nthey form new ones. The computation of H in-\nvolves the potential energy between atoms. When\nGibbs energy is negative, it means that the energy\ncontained in the bonds is higher, and a system will\nnaturally approach a lower energy state. Thus, a\nreaction or process will proceed spontaneously. On\nthe contrary, a positive Gibbs energy indicates that\nthe extrinsic energy is required to enable a target\nprocess or reaction. The path to decarbonization\nlies with discovering chemical processes that re-\nquire lesser amount of extrinsic energy.\nB Related work\nWe begin with providing an overview of the broader\nliterature around language models and their appli-\ncations into chemistry, then specifically focus on\nlarge-language models. Finally, we finish with an\noverview of various chain-of-thought prompting\nmethods that have been instrumental in improving\nthe reasoning capability of LLMs.\nB.1 Multi-modal models for Chemistry\nRecently, advances in NLP have found surprising,\nstrong results in the chemistry domain by train-\ning LLMs (Fabian et al., 2020; Chithrananda et al.,\n2020; Vaucher et al., 2021; Schwaller et al., 2021;\nNVIDIA Corporation, 2022; Tysinger et al., 2023)\non string representations of molecules (Weininger,\n1988; Weininger et al., 1989; Krenn et al., 2020;\nCheng et al., 2023). To enable higher-level control\nover molecular design, multi-modal models (Ed-\nwards et al., 2021; Vall et al., 2021; Zeng et al.,\n2022; Xu and Wang, 2022; Su et al., 2022; Seidl\net al., 2023; Xu et al., 2023; Zhao et al., 2023; Liu\net al., 2023b) have been proposed. Existing work\nfocuses on cross-modal retrieval (Edwards et al.,\n2021; Zeng et al., 2022), translation (Edwards et al.,\n2022; Liu et al., 2023c; Christofidellis et al., 2023),\nand editing (Liu et al., 2022).\nB.2 LLMs for Chemistry\nDue to recent progress in chat-oriented models\nsuch as GPT-4 (OpenAI, 2023), interest has grown\nin uncovering chemical knowledge and molecu-\nlar discovery from existing general LLMs (Hocky\nand White, 2022; White et al., 2022, 2023; Cas-\ntro Nascimento and Pimentel, 2023). This has been\nextended to work in the few-shot setting (Ramos\net al., 2023; Jablonka et al., 2023). In particular,\nthere is an interest in endowing LLMs with scien-\ntific tools (Bran et al., 2023; Boiko et al., 2023;\nLiu et al., 2023a). In general, these studies assess\nthe inherent chemistry knowledge in LLMs and the\neffect of integrating chemistry data via in-context\nlearning or finetuning. This differs from our contri-\nbution, where we propose an algorithmic approach\nfor improving model output using domain-specific\nrewards. A future research direction may be able\nto incorporate these two approaches together for\nexciting results.\nB.3 Chain-of-Thought (CoT) Variants\nSeveral works have considered improving LLM\noutput on complex reasoning tasks via formulating\nmultiple queries. (Creswell et al., 2022) explored\nthe decomposition of complex queries into smaller,\nmore reliable operators. (Creswell and Shanahan,\n2022) presents a methodology for generating the\nanswer in a step-by-step fashion and uses another\nmodel or function to pick the top-ranked answers,\nand avoids hallucination by constraining the output\nto a narrower set. (Jung et al., 2022) proposed an\nalternate approach to generate a tree of possible\nexplanations (both correct and incorrect), and then\nanalyzes their relationships to infer the correct set\nof answers. (Wang et al., 2022) improves reliability\nby sampling multiple explanations and answers\nfrom the model and then selecting the final answer\nthat appears most often. Tree-of-Thoughts (ToT)\n(Yao et al., 2023) generalizes the CoT approach\nto enable exploration over coherent units of text\n(thoughts) to perform deliberate decision making\nby considering multiple different reasoning paths.\nWe benchmark against (Kojima et al., 2022; Wang\net al., 2022; Yao et al., 2023) in our work.\nC Dataset Design\nWe propose two task datasets related to catalyst\ndesign: the first is derived from the Open Catalyst\n(OC) Project (Zitnick et al., 2020) and the second\nconsists of complex reasoning queries designed by\ncatalysis experts. Our multi-disciplinary team in-\nvolves researchers who actively work on designing\nnew catalysts for bio-fuels development.\nC.1 Action-Driven Prompt Design\nTo apply MCR to catalyst discovery, we define\na set of prompt templates and a set of actions to\nmodify the fields of those templates. The exact\nstructure of the prompt templates varies between\n8357\ntask datasets, but there are several common ele-\nments. Table 3 lists the action types that we use.\nFirstly, all prompts query the language model to\nreturn “top-k” catalysts as , where k is given by\nthe user. Secondly, each template has a list of “in-\nclude properties” and “exclude properties”, which\nspecify contexts for the LLM to consider positively\nwhen determining catalysts to include and exclude,\nrespectively. Next, each prompt in both ToT (Yao\net al., 2023) breadth-first-search andMCR after the\ninitial prompt uses the previous list of candidate\ncatalysts. The LLM is prompted to either include\nelements “similar to” or “different from” the previ-\nous list or to “include elements from” or introduce\n“new elements to” the list. Finally, the template\nincludes a field to prompt for a certain kind of cata-\nlyst: unary, binary, trinary, and oxides. Of course,\na prompt can have no specification on the catalyst\ntype.\nThe specific template depends on the task dataset\nand the original query.\nC.2 Open Catalyst Dataset\nThe Open Catalyst project (Zitnick et al., 2020) is\nan online repository of datasets intended for train-\ning surrogate models for computational chemistry\nsimulations related to catalysis. The dataset con-\ntains hundreds of thousands of adsorption energies\nfor adsorbate-catalyst pairs calculated using den-\nsity function theory (DFT), an accurate method\nfor computing energies of atomic configurations.\nWe use the Open Catalyst dataset to build an eval-\nuation dataset consisting of 79 adsorbates. This\ndataset targets the LLM’s ability to reason about\nthe adsorption of specific adsorbates.\nWe use the following template for this dataset:\nGenerate a list of candidate {catalyst\nlabel} {candidate list statement} for\nthe adsorption of {adsorbate}. {include\nstatement} {exclude statement} Let’s\nthink step-by-step and return a list of\ntop {k} answers and their explanations as\na list of pairs.\nHere, {}denote fields that need to be filled. The\nfields provided in the base query are the number of\ncandidate catalysts ‘k’(k=5 for the OC dataset)\nand enters the adsorbate symbols ‘adsorbate’\nfrom the OC dataset. ‘Include statement’ and\n‘exclude statement’ are phrases built from the\nlist of properties to include and exclude, respec-\ntively. These statements are affected by the Add\nTable 2: Dataset Summary\nOpenCatalysis BioFuelQR\n#Queries 79 51\nAdsorbates ✓ ✓\nReactions ✗ ✓\nHuman\nAnswers\n✗ ✓\nInclude Property and Add Exclude Property action\ntypes in Table 3. The ‘catalyst label’ field de-\ntermines which kind of catalyst the LLM should\nreturn. It’s value is set by the Change Catalyst\nType action and the Toggle Oxide action can set\nthis field to query for oxide catalysts. Finally, the\ncandidate list statement is a phrase built from the\nlist of candidates generated by the parent prompt.\nSince the candidate list can have an impact on the\noutput of the LLM, we include an action to re-run\nthe previous query with the candidate list from the\nprevious query’s output.\nPossible actions are weighed with equal prior\nprobabilities p(see Section 2) and impossible ac-\ntions are given prior probability zero. Actions are\nimpossible if they: add a property to a list which\nalready has that property, add a relationship to the\nprevious candidate list when there is no candidate\nlist, or if they would allow the next action to not\nhave a relationship to the previous candidate list\nwhile the candidate list is not empty.\nC.3 BioFuelQR Dataset\nOur application focus is driven by the design of\ncatalysis for reverse order gas reaction that is key\nto generation of synthetic biofuels with higher se-\nlectivity (Canakci and Van Gerpen, 1999; Daza\nand Kuhn, 2016; Kattel et al., 2017; Artz et al.,\n2018; Stolarczyk et al., 2018; Xu and Carter, 2018;\nMukhtar et al., 2022).\nQuestions in the BioFuelQR dataset uses the\nfollowing template:\nWhat are the top-3 {catalyst label}\n{candidate list statement} that perform\nthe RWGS reaction at a lower temperature\n(<200 C) and demonstrate higher\nadsorption energy for both CO2 and\nH2 (or facilitates both CO2 and H2\nadsorption)?. {include statement}\n{exclude statement} Provide scientific\nexplanations and return a list of top 3\nanswers and their explanations as a list\nof pairs. Let’s think step-by-step.\n8358\nTable 3: List of actions and their possibilities.\nAction\nType\nPossible Values # possible\nAdd Prop-\nerty to In-\nclude\nhigh activity, high selec-\ntivity, high stability, nov-\nelty, low cost, low tox-\nicity, high surface area,\nhigh porosity, crystal\nfacet, availability\n11\nAdd Prop-\nerty to Ex-\nclude\nlow activity, low selec-\ntivity, low stability, high\ncost, high toxicity, low\ndispersion, low porosity,\nhigh scarcity\n9\nChange\nCatalyst\nType\nunary catalyst, binary cat-\nalyst, trinary catalyst, cat-\nalyst\n4\nToggle\nOxide\non/off 1\nChange\nRelation\nto Prev.\nAnswer\nincluding elements that\nare different from, in-\ncluding elements similar\nto, introducing new ele-\nments to, including ele-\nments from\n4\nRepeat\nPrompt\nN/A 1\nC.4 Baseline implementations\nHere we define the parameters for the evaluations\nof the Baseline and MCR methods.\nChain-of-Thought (CoT) For the CoT baseline,\nwe generated a prompt for each query following the\ntemplates described in Appendix C.1. We evaluated\n9 adsorbates from the Open Catalysis Dataset and\n2 prompts from the BFR dataset. For CoT, we\nsimply send one prompt to the LLM to generate\na list of candidate catalysts, including the phrases\n“Provide a scientific explanation” and “Let’s think\nstep-by-step”. The reward of the result is reported.\nCoT with Self-Consistency For the self consis-\ntency baseline, the query was evaluated 10 times\nindependently using the same prompt from CoT.\nWe checked the answer for consistency. However,\nthere was no consistency between the top- k an-\nswers from the LLM over the 10 trials. Perhaps\ndue to the large diversity in catalyst compositions.\nThus, the reward estimate returned in Table 1 is\nsimply the maximum reward over the 10 trials.\nTree-of-Thoughts (ToT)For ToT, keeping com-\nputational cost in mind, we set a branching factor\nb= 6. This controls the number of nodes expanded\nat each point in the search. Thus, at each level the\nnodes with the top 6 rewards are expanded. To re-\nduce computational cost, we restricted the number\nof actions to the top 12 actions with the highest\nprior probability p(P,ai). This way, we reduce the\nnumber of actions simulated at each step. If there\nare not 12 actions with nonzero prior probability for\na node, we generate as many children as possible.\nThis happens, for instance, at the second level of\nthe search tree, where the action “change relation\nto previous answer” must be taken (of which there\nare 4 possibilities). This is because they will pass\ntheir candidate catalysts to their successor prompts.\nThe ToT method was run for 5 steps to generate\na tree with depth 5. If all actions were possible at\nevery level, we would generate 300 nodes in BFS\n(not including the root node), but only 252 nodes\nwere generate on average. Still, we were able to\nselect at least 6 best nodes at each level. We did not\nexperience a similar discrepancy in MCR because\nMCR has a more flexible branching policy. The\naverage observed number of nodes in the final trees\nis reported in Table 1.\nWe did not include the depth-first-search method\nfrom Tree-of-Thoughts because our search does\nnot support a specific ending criterion.\nMCR For MCR , we set a discount factor,\nγ = 0.9 and exploration-exploitation trade-off of\nc = 15to control the branching and depth of the\nsearch tree. Generally, decreasing γdecreases the\nlength of chains in the search tree while increasing\ncincreases the branching of the tree. We generated\n300 nodes after the root node, meaning 301 nodes\nwere in the final search tree.\nMCR utilizes the policy in Equation 2 to de-\ntermine which actions to carry out at which step.\nHowever, the policy must be modified in two cases.\nFirst, if a node is a leaf node, the policy is replaced\nby the prior probability distribution over actions,\np(Pt,ai) (see Section 2). Secondly, if a node ac-\ntion pair has no visits (N(Pt,ai) = 0) then the first\nterm of Equation 2 is dropped to avoid dividing by\nzero.\nC.5 Reward Query\nTo query the language model to return adsorption\nenergies, we use another prompt template:\nGenerate a list of adsorption energies,\nin eV, for the adsorbate {adsorbate} to\nthe surface of each of the following\ncatalysts: {candidate list}. Return the\nadsorption energies as a list of only\n{len(candidate list)} numbers in the order\n8359\nspecified.\nThe LLM should return a list of numbers which\ncan be averaged to produce a final energy. Since\nadsorption energies are negative we take the abso-\nlute value of the numbers listed by the LLM. units\nare in eV . If multiple adsorbates are given, as in\nthe BFR examples, multiple prompts are generated\nand the results are summed over. Occasionally, the\nLLM does not give an output that can be easily\nparsed into a list of floats. In these cases, the query\nis re-run a maximum of 3 times. Such examples\ninclude but are not limited to uncommon delimiters\nand sporadic phrases in the output.\nD Qualitative Analysis\n8360\nFigure 4: Example queries from the BioFuelQR dataset representing reasoning with different combinations of\nchemical descriptors.\n8361\nFigure 5: Example question and human answer from our compiled QA-dataset.\nFigure 6: Response to above query returned by Chain-of-Thought promting with GPT-3.\nFigure 7: Response to above query returned by MCR.\n8362\nFigure 8: Comparison of MCR vs standard Chain-Of-Thought prompting (via GPT-3) by domain expert 1.\n8363\nFigure 9: Comparison of MCR vs standard Chain-Of-Thought prompting (via GPT-3) by domain expert 2.\n8364\nFigure 10: Illustration of an evaluation by a domain expert on the progression of top search results found on the\npath to the answer with highest reward.\n8365",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7163271307945251
    },
    {
      "name": "Process (computing)",
      "score": 0.4399104118347168
    },
    {
      "name": "Space (punctuation)",
      "score": 0.4147060215473175
    },
    {
      "name": "Monte Carlo tree search",
      "score": 0.4132845401763916
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4132389426231384
    },
    {
      "name": "Theoretical computer science",
      "score": 0.37485843896865845
    },
    {
      "name": "Monte Carlo method",
      "score": 0.35473164916038513
    },
    {
      "name": "Artificial intelligence",
      "score": 0.341524600982666
    },
    {
      "name": "Programming language",
      "score": 0.1911340057849884
    },
    {
      "name": "Epistemology",
      "score": 0.1276080310344696
    },
    {
      "name": "Mathematics",
      "score": 0.08904734253883362
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}