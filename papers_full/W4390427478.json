{
  "title": "An interpretable and transferrable vision transformer model for rapid materials spectra classification",
  "url": "https://openalex.org/W4390427478",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2913471017",
      "name": "Zhenru CHEN",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2486780089",
      "name": "Yunchao Xie",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2117968873",
      "name": "Yuchao Wu",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2099382714",
      "name": "Yu-Yi Lin",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2112905669",
      "name": "Shigetaka Tomiya",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095709260",
      "name": "Jian Lin",
      "affiliations": [
        "University of Missouri"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285493748",
    "https://openalex.org/W2799915630",
    "https://openalex.org/W2908837618",
    "https://openalex.org/W3122079685",
    "https://openalex.org/W4308798344",
    "https://openalex.org/W4385490607",
    "https://openalex.org/W2996772221",
    "https://openalex.org/W2915603306",
    "https://openalex.org/W2940802411",
    "https://openalex.org/W3013216111",
    "https://openalex.org/W3010662401",
    "https://openalex.org/W4200440680",
    "https://openalex.org/W3170380937",
    "https://openalex.org/W4324018525",
    "https://openalex.org/W2994716258",
    "https://openalex.org/W4290725197",
    "https://openalex.org/W4210615770",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3093934881",
    "https://openalex.org/W3122158565",
    "https://openalex.org/W4380610924",
    "https://openalex.org/W4367180827",
    "https://openalex.org/W4383613927",
    "https://openalex.org/W4385808309",
    "https://openalex.org/W4286859046",
    "https://openalex.org/W4321791170",
    "https://openalex.org/W4298147611",
    "https://openalex.org/W4377232703",
    "https://openalex.org/W4310482569",
    "https://openalex.org/W4384816973",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W2558921396",
    "https://openalex.org/W2041899972",
    "https://openalex.org/W2914001277",
    "https://openalex.org/W2950328304",
    "https://openalex.org/W4288331351",
    "https://openalex.org/W4287792756",
    "https://openalex.org/W2990623796",
    "https://openalex.org/W4324032752",
    "https://openalex.org/W3047885487",
    "https://openalex.org/W2991293807",
    "https://openalex.org/W3103092523"
  ],
  "abstract": "An interpretable and transferrable Vision Transformer (ViT) model was developed for classifying individual materials from their XRD and FTIR spectra.",
  "full_text": "An interpretable and transferrable vision\ntransformer model for rapid materials spectra\nclassiﬁcation†\nZhenru Chen,a Yunchao Xie, *a Yuchao Wu,a Yuyi Lin,a Shigetaka Tomiyab\nand Jian Lin *a\nRapid analysis of materials characterization spectra is pivotal for preventing the accumulation of unwieldy\ndatasets, thus accelerating subsequent decision-making. However, current methods heavily rely on\nexperience and domain knowledge, which not only proves tedious but also makes it hard to keep up\nwith the pace of data acquisition. In this context, we introduce a transferable Vision Transformer (ViT)\nmodel for the identiﬁcation of materials from their spectra, including XRD and FTIR. First, an optimal ViT\nmodel was trained to predict metal organic frameworks (MOFs) from their XRD spectra. It attains\nprediction accuracies of 70%, 93%, and 94.9% for Top-1, Top-3, and Top-5, respectively, and a shorter\ntraining time of 269 seconds (∼30% faster) in comparison to a convolutional neural network model. The\ndimension reduction and attention weight map underline its adeptness at capturing relevant features in\nthe XRD spectra for determining the prediction outcome. Moreover, the model can be transferred to\na new one for prediction of organic molecules from their FTIR spectra, attaining remarkable Top-1, Top-\n3, and Top-5 prediction accuracies of 84%, 94.1%, and 96.7%, respectively. The introduced ViT-based\nmodel would set a new avenue for handling diverse types of spectroscopic data, thus expediting the\nmaterials characterization processes.\nIntroduction\nGlobal challenges in clean energy, sustainability, medicine and\nhealthcare have sparked an unprecedented demand for inno-\nvative functional materials.\n1 Given the urgency of these chal-\nlenges, there is a compelling need to transition the research\nparadigm from a labor-intensive and empirical one to an\nautonomous one. This transformation spans several crucial\nstages, encompassing synthesis, characterization, performance\ntesting, and informed decision making.\n2–5 Within these stages,\ncollection of characterization data assumes a paramount role.\nSpectroscopic techniques including X-ray di ﬀraction (XRD),\nFourier-transform infrared (FTIR), Raman, nuclear magnetic\nresonance (NMR), and mass spectrometry (MS), as well as\nmicroscopic methods like scanning electron microscopy,\ntransmission electron microscopy, and atomic force micros-\ncopy, witness an exponential surge in acquisition. This neces-\nsitates real-time processing of this characterization data to\nprevent accumulation of the massive datasets, which otherwise\ncould signi cantly impede the momentum of subsequent\ndecision-making steps. But current mainstream data analysis\npractices predominantly rely on experience and domain\nknowledge, a process that is not only monotonous but also is\nincapable of matching the data acquisition pace. Consequently,\nit is highly desirable to establish a rapid and precise technique\nfor processing characterization data with automation to expe-\ndite the advancement of novel materials.\nRecent advances in machine learning (ML), especially deep\nlearning (DL), oﬀer an exciting opportunity to reshape scientic\nresearch within the domains of chemical and materials\nscience.\n6–8 This is particularly evident in facilitating rapid\nanalysis of intricate data, including but not limited to XRD,9,10\nIR/FTIR,11,12 Raman,13,14 and MS data.15,16 For example, Oviedo\nand coworkers have demonstrated deployment of convolutional\nneural networks (CNNs) to eﬀectively classify the dimensional-\nities and space groups of thin-lm metal halides from XRD\nspectra.\n9 This application showcases the potential of utilizing\nadvanced DL techniques to enhance the accuracy and eﬃciency\nof materials characterization. Fine et al. developed CNNs for\nidentifying functional groups of unknown compounds from\nfused FTIR and MS spectra.11 Despite much progress, applica-\ntion of DL in spectrum analysis still faces several challenges.\nFirst, with the increase in input data size, CNNs may not be\nideal for chemical spectra analysis because theirlters have\naDepartment of Mechanical and Aerospace Engineering, University of Missouri,\nColumbia, MO 65201, USA. E-mail: linjian@missouri.edu; yxpx3@umsystem.edu\nbData Science Center, Graduate School of Advanced Science and Technology, Material\nScience Division, Nara Institute of Science and Technology (NAIST), 8916-5\nTakayamacho, Ikoma City, Nara Prefecture 630-0192, Japan\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d3dd00198a\nCite this:Digital Discovery,2 0 2 4 ,3,\n369\nReceived 4th October 2023\nAccepted 28th December 2023\nDOI: 10.1039/d3dd00198a\nrsc.li/digitaldiscovery\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 369\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\na local receptive eld, limiting their ability to capture global\npatterns in the data.17 Furthermore, in the past studies, the DL\nmodels lack generality to be transferred across diﬀerent mate-\nrials and/or spectrum types. Consequently, one would need to\ninitiate the training process for a new model from scratch for\neach distinct application.\nTransformer, initially introduced in 2017 for sequential data\nprocessing,\n18 has become a predominant architecture for\nnatural language processing (NLP). This is attributed to its\nadeptness in extracting broadly applicable representations from\nthe textual information that it encodes. The self-attention layers\ninherent in the Transformer enable simultaneous handling of\nsequential data, overcoming challenges associated with long-\nrange dependencies. This in turn facilitates eﬃcient training\nof neural networks using extensive datasets. Built upon the\nfoundation of the Transformer architecture, large language\nmodels like ChatGPT, Bard, LLaMA, and CLAUDE\n19–22 have\nshown surprisingly emergent ability in generating text and\nperforming zero- and few-shot learning scenarios. They hold\nsignicant promise across diﬀerent application domains.\n23 For\ninstance, Transformer has paved the way to image recognition.\nThis diversication into visual modalities is prominently illus-\ntrated by Vision Transformer (ViT).\nWith its success in processing sequential data, Transformer\nhas recently demonstrated its versatility and far-reaching\nimpact in chemical and materials sciences, spanning from\nliterature mining to physiochemical property prediction.\n24–28 An\nexemplary promise is re ected in its power for data\nanalysis.17,29–36 In a recent study, a Mass2SMILES model based\non Transformer was employed to predict functional groups and\nSMILES descriptors from the high-resolution MS/MS spectra,\n29\nshowing mean square errors (MSEs) of 0.0001 and 0.24 for the\nfunctional groups and SMILES descriptors, respectively.\nAnother Transformer model was trained to predict molecular\nstructures from the\n1H/13C NMR spectra, showing a Top-1\naccuracy of 67%. 30 When the input 1H NMR spectra are\ncombined with a set of likely compounds, the Top-1 accuracy is\nincreased to a remarkable value of 96%. In contrast to the MS\nand NMR spectra showing sharp, discrete peaks corresponding\nto the molecular features, XRD, Raman, and FTIR spectra oen\nproduce broader absorption or emission bands, re ecting\na range of various features. These much-broadened bands\nwould make it diﬃcult for many ML/DL models to predict\naccurate results but could be well suited for the ViT models to\nhandle. Very recently, a ViT model was developed to identify\nbacterial Gram types, species, and antibiotic-resistant strains in\nbloodstream infections from the surface-enhanced Raman\nscattering (SERS) spectra, achieving accuracies of 99.30% for\nclassifying the Gram types and 97.56% for the species.\n34 Despite\nthe progress, application of ViT in characterization data anal-\nysis is still in its infancy. Particularly, exploration of their\ngenericity for applications from one material to another and\nfrom one spectrum type to another has been quite limited if not\nany.\nHerein, we demonstrate a transferable ViT model for accu-\nrate and rapid identi cation of metal organic frameworks\n(MOFs) and organic molecules from XRD and FTIR spectra,\nrespectively. ViT for XRD (ViT-XRD) achieved prediction higher\naccuracies of 70%, 93%, and 94.9% for Top-1, Top-3, and Top-5,\nrespectively, and a shorter training time of 269 seconds (∼30%\nfaster) than those of CNN-XRD (60.4%, 88.1%, 89.9%, and 378\nseconds, respectively). Fine hyperparameter tuning reveals that\nthe length of the segmented spectra plays a critical role in\ndetermining the predicted outcomes. Dimension reduction by t-\nSNE shows that the ViT-XRD model is more adept at classifying\nthese XRD spectra than the CNN-XRD model. The derived\nattention weight heatmap reveals that the ViT-XRD model\nexhibits concentrated attention on the minor peaks to distin-\nguish very close spectra showing close characteristics of the\nprimary peaks, while the CNN model more relies on the primary\npeaks to do so. Furthermore, the ViT-XRD model can be trans-\nferred for FTIR spectra classication of a diﬀerent material type\n(organic molecules). This model is denoted as ViT-TL-FTIR.\nClassication of the FTIR spectra is a more diﬃcult task since\nthe characteristics of the FTIR spectra are much more irregular\nthan those of the XRD spectra. Nevertheless, the ViT-TL-FTIR\nmodel achieved prediction accuracies of 84%, 94.1%, and\n96.7% for Top-1, Top-3, and Top-5, respectively, which are much\nhigher than those of the non-transferred one and the trans-\nferred one from the CNN model (CNN-TL-FTIR). It is worth\nnoting that these results were attained without the noise\nreduction in the raw spectra, thereby drastically expediting the\ndata analysis.\nThe contribution of this work can be summarized as follows.\nFirst, we innovated the use of a Vision Transformer architecture\nfor classifying XRD spectra of MOFs, demonstrating higher\nprediction accuracies compared to those of the CNN models.\nSecond, results from the dimension reduction and the attention\nweight map uncover the mechanism of discerning key features\nof the XRD spectra, thus improving the interpretability of the\nmodel. Third, transferability of a pre-trained model to a new\none for analyzing the FTIR spectra of a diﬀerent material type\naccentuates the generality of Transformer for this purpose, thus\nopening a new avenue to future research in integrating and\nsynthesizing the diverse spectroscopic data sources, e.g.,\nRaman, NMR, and MS. This integration can further be enriched\nby combining other chemical information, such as structures\nand properties of the materials, thereby developing a compre-\nhensive and multifaceted approach to materials discovery.\nResults and discussion\nDevelopment of CNN and ViT models\nThe architectures of CNN-XRD and ViT-XRD models are illus-\ntrated in Fig. 1. Derived from the LeNet-5 architecture, the CNN-\nXRD model is composed of multiple layers, each contributing to\nthe overall model's functionality (Fig. 1a). This architecture\nincludes an input layer, four convolutional blocks, oneattened\nlayer, three fully connected layers, and an output layer. The\ninput layer processes the complete XRD spectra spanning\n2theta (2 q) in a range of 5 – 50°. Subsequently, the data\nundergoes a series of transformations with four consecutive\nconvolution blocks. Each block comprises a convolutional layer\nresponsible for feature extraction, a max pooling layer for\n370 | Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nspatial down-sampling, and a dropout layer to prevent over-\ntting. Following these convolutional operations, the data pass\nthrough a attened layer followed by three fully connected\nlayers. These layers enable the model to comprehend patterns\nwithin the data. Finally, the output layer aﬀords the classica-\ntion of the input data based on the operations in the preceding\nlayers. The detailed architecture of the CNN-XRD model can be\nfound in Fig. S1.†\nThe ViT-XRD model is constructed as a deep neural network,\nleveraging a self-attention mechanism as its foundation\n(Fig. 1b). It begins with segmenting the XRD spectra as the\ninput. For the spectra that cannot be evenly segmented into an\ninteger, the trailing portion of the data is discarded. Specically,\nembedding of the spectra adds a class [CLS] token to symbolize\nthe start of embedding. To capture positional information,\nposition encoding is added to each segmented spectrum. Then,\nthe embedding is processed by a sequence of the Transformer\nencoder stacks, each of which comprises a multi-head attention\n(MHA) layer and a multilayer perceptron (MLP) layer (right\npanel of Fig. 1b) with both residual connection and layer\nnormalization. In each attention head, the input embedding is\nmultiplied by three learnable weight vectorsW\nq, Wk, and Wv,\ntransforming it into a query, key, and value vector (Q, K, andV).\nThe scaled dot-product attention A is calculated from the\nequation: A = somax((Q × KT)/(dk)1/2) × V, where dk denotes\nthe dimension ofQ and K. The randomly initializedWq, Wk, and\nWv vectors enable the ViT-XRD model to grasp contextual\ninformation in the segmented spectra. All attention heads are\nconcatenated and then passed through the MLP for projecting\nthe output to match the dimension of the embedded input. The\nself-attention mechanism permits the incorporation of infor-\nmation from the full spectra into individual embeddings.\nConsequently, each of these embeddings stands as a represen-\ntative of the entire sequence. The encoder iterates this process\nthrough a dened number of layers, where a stochastic depth\ndropout is incorporated at each layer for additional regulariza-\ntion. Ultimately, only the [CLS] token enter an MLP regression\nlayer for the output classication.\nDatasets and data preprocessing\nA total of 2000 theoretical MOF XRD spectra were sourced from\nthe Cambridge Crystallographic Data Centre (CCDC) website\nand subsequently truncated tot within a 2q range spanning\nfrom 5 to 50°. Then, they were augmented by a factor of 200\nusing a physics-informed, three-step approach of peak elimi-\nnation, scaling, and shi (Fig. S2†).\n9 Details can be referred to\nESI Note S1.† Inspired by the augmentation techniques such as\nrandom crop and erasing in the domain of image classica-\ntion,37 instead of augmenting data in a xed 2q range,9 we\naugmented it in a randomized 2q range to obtain more diverse\ntraining data. As a result, the trained model aﬀords higher\nprediction accuracies, as depicted in Fig. S3. † To test the\nmodels, 30 experimental XRD spectra were collected from ten\nwell-known MOFs that were synthesized by three di ﬀerent\nmethods.\n10 These experimental XRD spectra were subjected to\nsubsequent preprocessing steps of Savitzky– Golay smoothing\nand background subtraction (ESI Note S2†).9 Fig. S4† shows\naugmented, theoretical, and experimental XRD spectra of the\nFig. 1 Pipelines of (a) CNN-XRD and (b) ViT-XRD models.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 371\nPaper Digital Discovery\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nten representative MOFs. The augmented theoretical XRD\nspectra are split into training and validation datasets with\na ratio of 4 : 1, while the experimental XRD spectra serve as the\ntesting data.\nPerformance of ViT-XRD and CNN-XRD models\nFig. 2 depicts the performance of both CNN-XRD and ViT-XRD\nmodels. Each model was trained 100 times with slightly varied\nprediction accuracies and training durations each time. Their\nstatistical results are reported here. The optimal ViT-XRD model\nshows average prediction accuracies for Top-1 (69.1%), Top-3\n(93.2%), and Top-5 (94.9%), respectively, which are higher\nthan those of the CNN-XRD model (60%, 87.6%, and 89.5%,\nrespectively). This indicates that the ViT-XRD model can extract\nmore critical features from the XRD spectra than the CNN-XRD\nmodel can. It is noteworthy that the ViT-XRD model requires an\naverage training duration of 269 seconds, which is 110 seconds\n(∼30%) shorter than that of the CNN-XRD model. In compar-\nison to the CNN-XRD model, the superior performance of the\nViT-XRD model can be attributed to key factors such as the self-\nattention mechanism and parallelism.\n18 The self-attention\nmechanism in the Transformer architecture allows for e ﬃ-\ncient capture of long-range dependencies within the spectra,\nthereby facilitating faster convergence. Unlike CNNs that rely on\nlocal sliding windows to process sequences, Transformer is\ninherently designed for high parallelism. This enables them to\nperform computations simultaneously at diﬀerent positions in\na sequence, thus signicantly reducing the training time.\nIn addition to the CNN-XRD and ViT-XRD models, ve\ntraditional ML models including Na¨ıve Bayes (NB), k-nearest\nneighbors (KNNs), logistic regression (LR), random forest\n(RF), extreme gradient boosting (XGB) were also trained to\nclassify the XRD spectra. As summarized in Table S1,† though\nimpressive performance in performing various tasks,\n7,38 the\nensemble models including RF and XGB were found to be\nentirely inappropriate for spectra identi cation, requiring\nexorbitant computational times and yielding near-zero accura-\ncies. NB exhibited prediction accuracies of less than 20% across\nTop-1 to Top-5 and training time of∼4 seconds, while KNN\nshowed higher prediction accuracies (36.7%, 63.3%, and\n66.7%) and shorter training time (1.8 seconds). In contrast, LR,\npreviously used for materials spectra analysis,\n39,40 demonstrated\npretty high prediction accuracies. However, it required\na training time of 4100 seconds, which is >10 times longer than\nthose of the CNN-XRD and ViT-XRD models. This is mainly\nbecause LR does not inherently support parallel computation\nand cannot fully utilize the advantage of parallelization capa-\nbilities embedded in modern GPUs.\nHyperparameter tuning for the ViT-XRD model\nTo improve model's generalizability and robustness, tuning the\nhyperparameters of the ViT-XRD model was performed using\na grid search technique. Fig. 3 shows the prediction accuracies\nwhen three hyperparameters of Embed_dim, Depth, and\nNum_head are tuned. The Embed_dim sets the length of the\nsegmented XRD spectra, directly inuencing their positional\ninformation. As shown in Fig. 3a, the prediction accuracies\nincrease with the increased Embed_dim, peaking at 66.9%,\n94.6%, and 96.2% for Top-1, Top-3, and Top-5, respectively,\nwhen Embed_dim is 120. But a further increase in Embed_dim\ndecreases the accuracies. Notably, the corresponding training\ntime shows the opposite trend. Embed_dim of 120 requires the\nlowest training time of∼420 s. Depth signies the number of\nthe Transformer's encoder stacks in deciphering intricate rela-\ntionships within the spectra. As depicted in Fig. 3b, an optimal\nvalue of 7 for Depth achieves satisfactory prediction accuracies\nalthough a training time of 336 s is slightly larger than that\nachieved in the model trained with Depth of 4. Num_head\ngoverns the number of self-attention heads for parallel pro-\ncessing. The prediction accuracies for Top-1, Top-3, and Top-5\noccur when Num_head is 4 without signicantly increasing\nthe training time (Fig. 3c). Hence, the optimal three hyper-\nparameters were determined to be 120 for Embed_dim, 7 for\nDepths, and 4 for Num_head. To investigate the importance of\nthese hyperparameters on performance, a set of decision trees\nwas trained (ESI Note S3 and Fig. S5– S7†). The results from\nFig. S5 – S7† are summarized in Fig. 3d, revealing that\nEmbed_dim plays the most important role in classifying the\nXRD spectra as it occupies an importance score of ∼90%,\nconsistent with the analysis shown in Fig. 3a. When the number\nis larger or less than 120, the prediction accuracies are greatly\nreduced. Num_head takes∼10% in the importance score, while\nthe importance of Depth is negligible. It is worth noting that we\ntried many reasonable hyperparameter combinations. The\naﬀorded prediction accuracies by the ViT-XRD model are\nconsistently higher than those by the CNN-XRD model.\nVisualization of attention weight maps output from the ViT-\nXRD model\nUnderstanding how the ViT model can eﬃciently classify the\nXRD spectra is quite desired. To do that, t-distributed stochastic\nneighbor embedding (t-SNE) was rst employed. t-SNE is\na dimensionality reduction technique commonly used in data\nvisualization and pattern recognition.\n41 It represents the high-\ndimensional data in a lower-dimensional space while\nFig. 2 Comparison performance of the CNN-XRD and ViT-XRD\nmodels in terms of prediction accuracies and training time.\n372 | Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\npreserving the pairwise similarities among them. The t-SNE plot\ncan reveal clusters, patterns, or structures that might appear in\nthe original high-dimensional space. The t-SNE plot of the 2000\ntheoretical XRD spectra is depicted in Fig. 4a. It is evident that\nthe XRD spectra sharing similar patterns are clustered together\nwhile those less similar spectra are furthered away,e.g., the dots\nrepresenting MOF-2, MOF-5, ZIF-71, and ZIF-90 are scattered\napart. Close observation shows that the dots belonging to ZIF-8\nand ZIF-67 are overlapped, like those of ZIF-7 and ZIF-9, MOF-74\nand MOF-199, which is consistent with the results shown in\nFig. S3,† indicating similarity of their XRD spectra. The close\nsimilarity leads to the decreased prediction accuracy by the\nCNN-XRD model. But the ViT-XRD model seems to easily\ndistinguish them. It inspires us to explore the mechanism\nbehind it.\nTo do that, representations of the corresponding spectra\nlearned by the CNN-XRD and ViT-XRD models were visualized\nby t-SNE (Fig. 4b and c). Surprisingly, ZIF-8 and ZIF-67, MOF-74\nand MOF-199, and ZIF-7 and ZIF-9 no longer overlapped.\nInstead, they are scattered and easily dispersible. But the\nrepresentations extracted from the CNN-XRD model for ZIF-8,\nZIF-67, and ZIF-90 still overlapped. This suggests that the ViT-\nXRD model is more adept at classifying these XRD spectra\nwith higher accuracies than the CNN-XRD model. To test this\nhypothesis, two sets of spectra for a total of 10 MOFs were\nchosen. Details of selection criteria are explained in ESI Note\nS4,† and their full names are listed in Table S2.† The rst set\ncontains the ve MOFs that are maximally distant from their\nnearest neighbors (yellow dots in Fig. 4a), which still maintain\na distinguishable distance from other MOFs in t-SNE maps\n(yellow dots in Fig. 4b and c). The second set comprises another\nve MOFs that are the most closely clustered together (purple\ndots in Fig. 4a), which are widely distributed across the feature\nspace by the CNN-XRD model with reduced localized concen-\ntration (purple dots in Fig. 4b). But the ViT-XRD model succeeds\nin dispersing them while still maintaining them within the\nsame region, thereby retaining a visible indication of their\nintrinsic similarities (purple dots in Fig. 4c).\nTo deeply understand how these two models identify XRD\nspectra, two representative ZIFs including ZIF-8 and ZIF-67\nsharing nearly similar XRD spectra were chosen. Fig. 5a pres-\nents the XRD spectra of ZIF-8 and ZIF-67, annotated with crystal\nplanes at respective peaks. Obviously, three primary peaks at\n7.4° and 12.8°, corresponding to the (011) and (012) planes are\nvirtually identical for two ZIFs. In contrast, a few minor peaks\nlocated at 16.5°, 18.1°, 24.6°, and 26.8°, corresponding to the\n(013), (222), (233), and (134) planes, exhibit diﬀerent intensities,\nwhich are the main disparities between these two spectra. Since\nCNN can't classify them while ViT can, herein, we aim to\ndisclose how they make such diﬀerent decisions. Heatmap,\na graphical representation to visualize the intensity or impor-\ntance of certain values/regions, is useful for interpreting the\noutcome of neural networks. For CNNs, a class activation map\n(CAM), highlighting the regions in the input spectra that most\nFig. 3 Performance of the ViT-XRD models in terms of prediction accuracies and training time when trained with varied hyperparameters of (a)\nEmbed_dim while setting Depth and Num_head to be 10 and 10, respectively; (b) Depth while setting Embed_dim and Num_head to be 120 and\n10, respectively; and (c) Num_head while setting Embed_dim and Depth to be 120 and 7, respectively; (d) hyperparameter importance scores\namong Embed_dim, Depth, and Num_head.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 373\nPaper Digital Discovery\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ninuence the classication result, was used for a comparative\nanalysis.42 The CAMs for ZIF-8 and ZIF-67 were plotted by\nutilizing the output of the last convolutional layer of the CNN-\nXRD model, and the details can be found in the Methods. As\nshown in Fig. 5b and c, the red regions in CAMs reveal that the\nCNN-XRD model predominantly focuses on the two primary\npeaks at 7.4°, 12.8° with a slight blue-shi (∼3°) when making\nthe classifying decision. Such a mechanism may lead to the\nwrong classication when the model is fed with very similar\nspectra in the primary peaks like the ones of ZIF-8 and ZIF-67.\nIn the context of the ViT model, the learned attention\nweights can be visualized to investigate the attention allocated\nto diﬀerent regions of the input XRD spectra, highlighting the\nextent to which each input element contributes to the model's\ndecision-making process.\n43,44 For each XRD spectrum, a total of\n28 attention weight maps can be obtained from the seven\nencoder layers and four attention heads. Fig. S8† showcases the\nattention maps for ZIF-8 and ZIF-67 as well as MOF-74 and\nMOF-199 as these respective XRD spectra are similar to closed\nprimary peaks. Additional examples are available on GitHub. In\nthe rst layer, attention disperses across the spectra segments,\nimplying the model's eﬀort to understand the primary patterns.\nAs the ViT-XRD model delves into deeper encoder layers, the\nattention shis noticeably to the interrelationships among\ndiﬀerent spectra segments, leveraging the inherent advantages\nof the Transformer's attention mechanism. This transition\nsignies the model's encompassment of various data slices\nfrom their simple patterns to complex ones, from a localized\nrelationship to a global one. Close observation found that the\nattention maps for ZIF-8, ZIF-67, MOF-74, and MOF-199 share\nsimilar trends in therst few layers, indicating a broad focus on\nkey features. However, a divergence in attention patterns\nbetween ZIF-8/ZIF-67 and MOF-74/MOF-199 becomes evident in\nthe deeper layers. Given that the XRD spectra of MOF-74 and\nMOF-199 are totally diﬀerent from those of ZIF-8 and ZIF-67,\nsuch divergence highlights the capability of the ViT model to\nne-tune its focus on subtle peak diﬀerences. The attention\nmechanism in the Transformer architecture allows the model to\ncapture long-range dependencies and contextual information of\nthe XRD spectra, resulting in higher prediction accuracies.\nWhen it evolves to the last encoder layer (Fig. 6a and b),\ndiﬀerent attention heads play diverse roles. As for the attention\nweight map of ZIF-8 and ZIF-67, Heads 1, 3 and 4 exhibit a few\nobvious vertical patterns, while Head 2 focuses on more specic\nregions. For instance, Head 1 shows two vertical patterns\nlocated at the regions of 5– 7.4° and 14.6– 17°, corresponding to\nthe (011) plane, (022)/(013) planes, respectively. Head 3\npossesses an obvious vertical pattern located at the regions of\n9.8– 12.2° corresponding to the (022)/(013) planes. Head 4\nfocuses more on the peaks at 21.8– 24.2° for ZIF-8 while the\npeaks at 7.4– 9.8° and 26.6– 29° for ZIF-67. As for the specic\nregions of ZIF-8, in Head 2, the peaks at the 9.8– 12.2° region\ncorrespond to the (022) plane. For ZIF-67, two large attention\nweights in Head 1 are related to the peaks of the (011) and (044)\nplanes and the peaks of the (114) and (044)/(344) planes. Head 2\nshows the large attention weights to the peaks of the (112) and\n(114) planes. Head 3 shows large attention weights to the peaks\nof the (114) and (123) planes, while Head 4 exhibits the large\nones to the peaks of the (011) and (233)/(224) planes.\nTo directly compare how attention is distributed across the\nregions of the spectra, an attention rollout map (ARM), as\nshown in Fig. 6c, is averaged from therst rows of the attention\nweights from the XRD spectra of ZIF-8 and ZIF-67 (red squares\nFig. 4 t-SNE plots of (a) theoretical XRD spectra of 2000 MOFs,\nrepresentations learned from (b) the CNN-XRD model and (c) the ViT-\nXRD model. Red: ten representative XRD spectra of MOFs. Purple:ﬁve\nMOFs with maximal distance to their respective nearest neighbors.\nYellow: ﬁve most clustered MOFs. The CCDC numbers and full names\nof these 10 MOFs are listed in Table S2.†\n374\n| Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nin Fig. 6a and b).44 It represents the attention weights of the\n[CLS] token query over the spectra segments, oﬀering inter-\npretability into the mechanism of a Transformer model on\nmaking decisions. The ARM clearly shows that the highest (%\n7E30%) attention from the VIT-XRD model was concentrated on\nthe (022)/(013) peaks, while the remaining attentions are paid to\nthe other peaks. These results indicated that the ViT model can\ndetect less apparent but potentially relevant peaks by detecting\nFig. 5 (a) XRD spectra of ZIF-8 and ZIF-67. Class activation maps derived from the CNN-XRD model on (b) ZIF-8 and (c) ZIF-67.\nFig. 6 Heat maps of the learned attention weights from the ViT-XRD model's last layer over the XRD spectrum of (a) ZIF-8 and (b) ZIF-67.\nNormalized attention rollout map of (c) ZIF-8 and (d) ZIF-67.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 375\nPaper Digital Discovery\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nthe relevance of the distances and intensity ratios between the\npeaks when classifying the spectra, thus uncovering the mech-\nanism of how the ViT model can better distinguish very similar\nspectra than the CNN does.\nReduced 2q range\nVisualization of self-attention weights reveals that the ViT-XRD\nmodel focuses more on the initial segments of XRD data for\nmaking decisions. This observation prompts us to assess the\nbalance between accuracy and the range of the 2q angle. This is\nbecause narrowing the range will reduce the data amount and\nsubsequently the model training time. Herein, we investigated\nhow narrowing the 2q range would change the predictive accu-\nracy of the ViT-XRD model (Fig. S9†). The initial 2q range in 5– 50°\nserves as a baseline. Then it is narrowed to 5–45°, 5– 40°, 5–35°,\nand 5–30° by directly truncating the data points out of these\nranges. Subsequently, the ViT-XRD models were retrained using\nthese reduced datasets. In comparison with the original model,\nthe prediction accuracies for Top-1, Top-3, and Top-5 from the\nretrained models are marginally decreased, but the training time\nis signicantly decreased, highlighting the robustness of the\nmodel for rapid classication. For instance, if taking the model\ntrained with 2q in the range of 5–30° as an example, the Top-1\naccuracy slightly decreases from 96.7% to 92%, while the\nacquisition time is shortened from 11.25 to 6.25 minutes given\na scan rate of 4° per minute, which may be further reduced by\nincreasing the scan rate. These results prove that the crucial\ncharacteristic features required for MOF classi cation are\npredominantly contained within the smaller 2q ranges.\nTransfer learning from XRD to FTIR\nThe ViT model has exhibited remarkable prediction accuracy in\nclassication of the XRD spectra. Retraining a new model for\napplication in di ﬀerent types of spectra, e.g., FTIR, for\nad iﬀerent type of material can be time-consuming, labor-\nintensive, and oen impractical due to the challenges of gath-\nering and curating extensive data. This limitation poses\na substantial obstacle to the application of DL in chemical and\nmaterials science, where data limitation is an issue. An alter-\nnative solution to this issue is to use transfer learning (TL). TL\nleverages knowledge gained from a source domain and adapts it\nto another one. This approach has garnered much attention as\nit mitigates the need for massive datasets and reduces compu-\ntation. Tian et al. demonstrated a TL strategy to improve the\naccuracy of classifying Raman spectra trained by limited data.\n45\nAnother study by Kim and colleagues showcased the universal\ntransferability of a MOFTransformer model.46 They achieved\nthis byne-tuning an already trained model for predictions of\ndiverse MOF properties like gas adsorption, diﬀusivity, and\nelectronic properties. These accomplishments motivate us to\ninvestigate the transferability of our ViT-XRD model to classify\nanother type of spectrum, e.g., FTIR, for a diﬀerent type of\nmaterial. The FTIR spectra provide intricate insights into\nchemical bonding and molecular structures. Each chemical\nbond possesses distinct light absorption frequencies, resulting\nin an FTIR spectrum that acts as a molecular“ngerprint”.I t\ncan be used to identify unknown substances and quantify\nspecic compounds within mixtures. However, it poses a chal-\nlenge in analysis and interpretation due to irregular peak\nshapes, containing various absorptions originating from the\ndistinct functional groups.\n47,48 These functional groups are\ninevitably subjected to varying degrees of inuence from nearby\nmolecular features and environmental conditions. Moreover,\nthe presence or absence of a particular functional group is not\nsolely determined by the presence or absence of a single spec-\ntral band; it is also by intricate spectral regions. These\ncomplexities make the analysis of FTIR time-consuming and\nerror-prone, necessitating the development of powerful and\nrobust analysis techniques to expedite this process.\nGiven the complexities associated with FTIR analysis, it was\nchosen as a demo to evaluate the transferability of the ViT-XRD\nmodel. Fig. 7a depicts the TL procedure, wherein the ViT-XRD\nmodel that was originally trained by the XRD spectra was\ntransferred to classify the experimental FTIR spectra of 3753\norganic molecules. They were selected by criteria on the pres-\nence of carbon, hydrogen, nitrogen, sulfur, anduorine atoms\nwhile the number of carbon atoms ranges from 6 to 20.\nSubsequently, these FTIR spectra underwent a series of pre-\nprocessing steps, encompassing transmission-to-absorption\nconversion, wavelength-to-wavenumber conversion, trunca-\ntion, interpolation, and normalization. It is worth mentioning\nthat neither noise nor background reduction was employed to\npreprocess the raw FTIR spectra.\nThe transferred ViT model can harness its prior under-\nstanding from the XRD spectra to eﬀectively classify the FTIR\nspectra, even though they diﬀer largely in the spectra character-\nistics. To train a new ViT model for the FTIR classication by TL,\nthe weights, and biases of the pre-trained ViT-XRD model were\nused as initial parameters without any subsequent modication\nor changes of the model components. This model is denoted as\nViT-TL-FTIR. As a control, a separate ViT-FTIR model was trained\nfrom scratch using the same FTIR spectra. It is worth noting that\nthe congurations with the 10 attention heads and 10 encoders\nwere set for both the ViT-TL-FTIR and ViT-FTIR models. As\na control study, a transferred CNN-XRD model, denoted as CNN-\nTL-FTIR was also trained, while a CNN-FTIR model without TL\nwas developed. Fig. 6b and c show the Top-1, Top-3, and Top-5\nprediction accuracies from these models. Generally, the trans-\nferred models show enhanced prediction accuracies compared to\nthe non-transferred ones.\n45,46 Notably, the ViT-TL-FTIR model\noutperforms the CNN-TL-FTIR model, with Top-1, Top-3, and\nTop-5 prediction accuracies of 84%, 94.1%, and 96.7%, respec-\ntively, highlighting the inherent advantages of the Transformer\narchitecture, while the ViT-FTIR model aﬀords much lower cor-\nresponding accuracies of only 72.5%, 85.4%, and 88.9% (Fig. 7b).\nSimilarly, the CNN-TL-FTIR model delivers prediction accuracies\nof 50.6%, 66.9% and 73.1% for Top-1, Top-3, and Top-5, respec-\ntively, which are higher than those predicted by the CNN-FTIR\nmodel (Fig. 7c). But they are respectively lower than those aﬀor-\nded by the ViT-TL-FTIR model, agreeing well with the conclusion\nthat Transformer is superior to CNN for this application.\nFurthermore, eﬀects of Embed_dim, augmentation times,\nand classication categories on the prediction accuracies of the\n376 | Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nViT-TL-FTIR model were investigated (Fig. S10–S12†). Fig. S10†\nshows that the reduction of Embed_dim to 120 decreases the\nprediction accuracies to 65.5%, 80.2%, and 84.4% for Top-1, Top-\n3, and Top-5, respectively. A decrease in the augmentation times\nreduces the prediction accuracies as well as the training time\n(Fig. S11†). For instance, if the model is trained by data\naugmented 10 times, the accuracies for Top-1, Top-3, and Top-5\ndecrease to 68.7%, 83.7%, and 88.5%, and the training time\ndecreases from 420 to 132 seconds. We also investigated the\neﬀect of classes (the number of organic molecules) on the model\nperformance. As shown in Fig. S12,† the Top-1 prediction accu-\nracy aﬀorded by the model trained for 500 molecules is 94.4%,\nwhich reduces to 84.7% when the number of the molecules\nincreases to 3000. The decrease in the Top-1 prediction accuracy\nwith the increase of classes is common in a classication task.\n10\nConclusions\nIn this study, we demonstrate an interpretable and transferrable\nViT model for material classication from their spectra. The ViT\nmodel rst trained by the XRD spectra of MOFs performs better\nthan the CNN model. Visualization of the attention weight maps\nillustrates that the self-attention mechanism helps the model to\ncapture long-range dependencies of the tokens in the XRD\nspectra. Then, the pre-trained ViT-XRD model was successfully\ntransferred to classify the FTIR spectra of organic molecules.\nDespite the higher characteristic complexity in the FTIR spectra,\nthe transferred models exhibit superior performance to the\nnon-transferred ones. It indicates that by leveraging the TL\nstrategy, the issues of lacking enough high-quality data in the\nchemical and materialelds can be mitigated. This ViT model\nprovides an accurate and interpretable approach to identify\nmaterials from their spectral ngerprints, laying a broader\nplatform for analyzing other spectroscopic modalities, such as\nRaman and NMR. Importantly, the inherent structure of the\nTransformer models holds great promise for multimodal\nlearning by fusing diverse types of characterization data. Such\na multimodal Transformer model, coupled with transferability\nas demonstrated in this study, would lead to a new route to\ncomprehensive structure– property analysis.\nMethods\nTheoretical and experimental XRD data: collection and\nprocessing\nA total of 2000 theoretical XRD spectra in the Crystallographic\nInformation File (CIF) were sourced from an open-source\ndatabase of the Cambridge Crystallographic Data Centre\n(CCDC). Then, all CIFs were converted in a batch mode to a tab-\nseparated format using Mercury soware for subsequent data\nprocessing. To collect the experimental XRD, ten MOFs (ZIF-7,\nZIF-8, ZIF-9, ZIF-67, ZIF-71, ZIF-90, MOF-2, MOF-5, MOF-74\nand MOF-199) were synthesized by three common methods,\nresulting in a total of thirty MOF samples.\n10 Then experimental\nFig. 7 The workﬂow and results of the transferred ViT model for FTIR classiﬁcation. (a) Sources of XRD and FTIR spectra and the schematic of\ntransfer learning the ViT-XRD model to the ViT-TL-FTIR model. Prediction accuracies and training times of the ViT-FTIR and ViT-TL-FTIR models\n(b) as well as the CNN-FTIR and CNN-TL-FTIR models (c) for classifying 3753 molecules.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 377\nPaper Digital Discovery\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nXRD spectra were collected from these samples using a Bruker\nD8 Advance XRD. The spectra underwent processing procedures\nof noise reduction and background subtraction and then were\naugmented. Details are explained in ESI Note S2.† To maintain\nconsistency, all XRD spectra were truncated to the same 2q\nrange of 5– 50°, and then rescaled to a range of 0– 1.\nFTIR data collection and processing\nA total of 3753 organic molecules were sourced from the\nNational Institute for Science and Technology (NIST) Chemistry\nWebBook. Specically, the molecules that contain 6– 20 carbon\natoms, hydrogen, nitrogen, sulfur, anduorine were selected.\nThese FTIR spectra were standardized to the absorption type\nwith the same wavenumber unit. Subsequently, a three-step\ndata processing by truncation, interpolation, and intensity\nnormalization was employed to ensure a constant wavenumber\nin the same range of 700– 3500 and a standardized absorption\nintensity in the range of 0– 1. Note that they did not undergo\nnoise or background reduction. Note that among the 5– 10 FTIR\nspectra for each molecule, one spectrum was designed as the\ntest set. The remaining ones were randomly selected for\naugmentation to a total of 50 spectra. These augmented data-\nsets were subsequently partitioned into training and validation\nsubsets with a ratio of 4 : 1.\nModel training\nNB, KNN, LR, RF, XGB, CNN, and ViT were trained. A grid-\nsearch strategy was applied to nd the optimal hyper-\nparameters. To prevent overtting, an early stopping strategy\nwas implemented when training the CNN and ViT models. The\ntraining was terminated prematurely if it surpassed a patience\nlevel of 3 epochs without a signicant decrease in the loss.\nUnless specied, for each model, the training was replicated ten\ntimes to obtain the mean and standard deviations of the\nprediction accuracies. The model performance was evaluated\nusing Top-N accuracy on the test datasets. In detail, Top-1\naccuracy refers to the ViT model's capability to correctly rank\nan MOF sample at therst position. Meanwhile, Top-3 and Top-\n5 accuracies assess the model's accuracy in ranking the sample\nwithin the top three and topve positions, respectively.\n10 All\ncomputations were conducted on a desktop equipped with an\nIntel Core i7-12700K processor, an NVIDIA GeForce 2080 GPU,\nand 64 GB of RAM, running on the Ubuntu 22.04.2 operating\nsystem. The codes were implemented using Python 3.7.9. For\ndata processing, we utilized NumPy version 1.19.2 and Pandas\nversion 1.2.1. The data processing and analysis on the tradi-\ntional ML models were undertaken using Scikit-learn 1.0.2. The\nCNN model was constructed using the TensorFlow 2.2.0\nframework, while the ViT model was built using PyTorch\n1.13.1+cu117.\nHeatmap\nARM and CAM for ViT-XRD and CNN-XRD models, respectively,\nwere plotted. For the ARM, the attention weights associated\nwith the‘CLS’ token were extracted from each attention head in\nthe last layer of the Transformer encoder. These attention\nweights indicate the importance of diﬀerent positions in the\ninput sequence relative to the‘CLS’ token. These weights were\naveraged across all attention heads to create a composite\nattention vector, which illustrates the cumulative attention in\nthe model allocated to the CLS token. Each composite vector\nwas mapped to the corresponding XRD spectrum. The CAM was\nplot by utilizing the output of the last convolutional layer of the\nCNN-XRD model. Specically, we took the weights from the fully\nconnected layer and performed a matrix multiplication with the\nfeature maps from the last convolutional layer.\nData availability\nAll codes are publicly available at https://github.com/\nlinresearchgroup/ViT_Materials_Spectra. For the source data,\nthe theoretical XRD spectra are available from the CCDC. The\nexperimental FTIR spectra can be sourced from the NIST\nWebBook and are copyrighted by NIST. Additional attention\nmaps for the XRD spectra of other MOFs are summarized in\n(https://github.com/linresearchgroup/ViT_Materials_Spectra/\ntree/main/Visualization). Additional data can be made available\nfrom the corresponding author upon request.\nAuthor contributions\nZ. C. designed and implemented data collection and pre-\nprocessing, model training and testing, and data analysis. Y. X.\nperformed supervision of material synthesis and characteriza-\ntion, and machine learning algorithm development. Y. W.\nprovided the suggestion for the analysis of FTIR spectra. J. L.\nconceived the project, managed the research progress, and\nprovided regular guidance. Z. C. and Y. X. draed the manu-\nscript which was thoroughly revised by J. L. S. T. and Y. L.\noﬀered regular feedback during the project implementation. S.\nT. made a minor revision of the manuscript. All authors com-\nmented and agreed on thenal version of the manuscript.\nConﬂicts of interest\nThe authors declare no competing interests.\nAcknowledgements\nJ. L. thanks the nancial support from the National Science\nFoundation (award number: 2154428), U.S. Army Corps of\nEngineers, ERDC (grant number: W912HZ-21-2-0050), DOE\nNational Energy Technology Laboratory (award number: DE-\nFE0031988), and Sony Research Award Program (2020). We\nalso acknowledge help from Kotaro Satori at Sony Semi-\nconductor Solutions Corporation.\nReferences\n1 J. Meckling, J. E. Aldy, M. J. Kotchen, S. Carley, D. C. Esty,\nP. A. Raymond, B. Tonkonogy, C. Harper, G. Sawyer and\nJ. Sweatman, Busting the myths around public investment\nin clean energy,Nat. Energy, 2022,7, 563– 565.\n378 | Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n2 D. P. Tabor, L. M. Roch, S. K. Saikin, C. Kreisbeck,\nD. Sheberla, J. H. Montoya, S. Dwaraknath, M. Aykol,\nC. Ortiz, H. Tribukait, C. Amador-Bedolla, C. J. Brabec,\nB. Maruyama, K. A. Persson and A. Aspuru-Guzik,\nAccelerating the discovery of materials for clean energy in\nthe era of smart automation,Nat. Rev. Mater., 2018,3,5 – 20.\n3 P. S. Gromski, A. B. Henson, J. M. Granda and L. Cronin,\nHow to explore chemical space using algorithms and\nautomation, Nat. Rev. Chem, 2019,3, 119– 128.\n4 Y. Shi, P. L. Prieto, T. Zepel, S. Grunert and J. E. Hein,\nAutomated Experimentation Powers Data Science in\nChemistry, Acc. Chem. Res., 2021,54, 546– 555.\n5 Y. Xie, K. Sattari, C. Zhang and J. Lin, Toward autonomous\nlaboratories: Convergence of arti cial intelligence and\nexperimental automation, Prog. Mater. Sci. , 2023, 132,\n101043.\n6 H. Wang, T. Fu, Y. Du, W. Gao, K. Huang, Z. Liu, P. Chandak,\nS. Liu, P. Van Katwyk, A. Deac, A. Anandkumar, K. Bergen,\nC. P. Gomes, S. Ho, P. Kohli, J. Lasenby, J. Leskovec,\nT.-Y. Liu, A. Manrai, D. Marks, B. Ramsundar, L. Song,\nJ. Sun, J. Tang, P. Veli ˇckovi´c, M. Welling, L. Zhang,\nC. W. Coley, Y. Bengio and M. Zitnik, Scientic discovery\nin the age of articial intelligence,Nature, 2023,620,4 7– 60.\n7 Y. Xie, C. Zhang, X. Hu, C. Zhang, S. P. Kelley, J. L. Atwood\nand J. Lin, Machine Learning Assisted Synthesis of Metal–\nOrganic Nanocapsules, J. Am. Chem. Soc., 2020, 142, 1475–\n1481.\n8 Y. Dong, C. Wu, C. Zhang, Y. Liu, J. Cheng and J. Lin,\nBandgap prediction by deep learning in congurationally\nhybridized graphene and boron nitride, npj Comput.\nMater., 2019,5, 26.\n9 F. Oviedo, Z. Ren, S. Sun, C. Settens, Z. Liu, N. T. P. Hartono,\nS. Ramasamy, B. L. DeCost, S. I. P. Tian, G. Romano, A. Gilad\nKusne and T. Buonassisi, Fast and interpretable\nclassication of small X-ray diﬀraction datasets using data\naugmentation and deep neural networks, npj Comput.\nMater., 2019,5, 60.\n10 H. Wang, Y. Xie, D. Li, H. Deng, Y. Zhao, M. Xin and J. Lin,\nRapid Identication of X-ray Diﬀraction Patterns Based on\nVery Limited Data by Interpretable Convolutional Neural\nNetworks, J. Chem. Inf. Model., 2020,60, 2004– 2011.\n11 J. A. Fine, A. A. Rajasekar, K. P. Jethava and G. Chopra,\nSpectral deep learning for prediction and prospective\nvalidation of functional groups,Chem. Sci., 2020, 11, 4618–\n4630.\n12 A. Angulo, L. Yang, E. S. Aydil and M. A. Modestino, Machine\nlearning enhanced spectroscopic analysis: towards\nautonomous chemical mixture characterization for rapid\nprocess optimization,Digital Discovery, 2022,1,3 5– 44.\n13 T.-Y. Huang and J. C. C. Yu, Development of Crime Scene\nIntelligence Using a Hand-Held Raman Spectrometer and\nTransfer Learning,Anal. Chem., 2021,93, 8889– 8896.\n14 X. Fan, Y. Wang, C. Yu, Y. Lv, H. Zhang, Q. Yang, M. Wen,\nH. Lu and Z. Zhang, A Universal and Accurate Method for\nEasily Identifying Components in Raman Spectroscopy\nBased on Deep Learning,Anal. Chem., 2023,95, 4863– 4870.\n15 A. D. Melnikov, Y. P. Tsentalovich and V. V. Yanshole, Deep\nLearning for the Precise Peak Detection in High-Resolution\nLC-MS Data,Anal. Chem., 2020,92, 588– 592.\n16 D. A. Boiko, K. S. Kozlov, J. V. Burykina, V. V. Ilyushenkova\nand V. P. Ananikov, Fully Automated Unconstrained\nAnalysis of High-Resolution Mass Spectrometry Data with\nMachine Learning, J. Am. Chem. Soc., 2022, 144, 14590–\n14606.\n17 Z. Zhao, X. Wu and H. Liu, Vision transformer for quality\nidentication of sesame oil with stereoscopicuorescence\nspectrum image,Lebensm.-Wiss. Technol., 2022,158, 113173.\n18 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, Attention Is All\nYou Need, arXiv, 2017, preprint, arXiv:1706.03762, DOI:\n10.48550/arXiv.1706.03762.\n19 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova: Pre-\ntraining of Deep Bidirectional Transformers for Language\nUnderstanding, arXiv, 2018, preprint, arXiv:1810.04805,\nDOI: 10.48550/arXiv.1810.04805.\n20 T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter,\nC. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever\nand D. Amodei, Language Models are Few-Shot Learners,\narXiv, 2020, preprint, arXiv:2005.14165, DOI: 10.48550/\narXiv.2005.14165.\n21 A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton,\nS. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko,\nJ. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer,\nV. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke,\nA. Levskaya, S. Ghemawat, S. Dev, H. Michalewski,\nX. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou,\nD. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,\nR. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. Sankaranarayana Pillai, M. Pellat, A. Lewkowycz,\nE. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang,\nB. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-\nHellstern, D. Eck, J. Dean, S. Petrov and N. Fiedel,PaLM:\nScaling Language Modeling , with Pathways, arXiv, 2022,\npreprint, arXiv:2204.02311, DOI:10.48550/arXiv.2204.02311.\n22 H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar,\nA. Rodriguez, A. Joulin, E. Grave and G. Lample, LLaMA:\nOpen and Eﬃ\ncient Foundation Language Models, arXiv,\n2023, preprint, arXiv:2302.13971, DOI: 10.48550/\narXiv.2302.13971.\n23 K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne,\nM. Seneviratne, P. Gamble, C. Kelly, A. Babiker, N. Sch¨arli,\nA. Chowdhery, P. Mans eld, D. Demner-Fushman,\nB. Agüera y Arcas, D. Webster, G. S. Corrado, Y. Matias,\nK. Chou, J. Gottweis, N. Tomasev, Y. Liu, A. Rajkomar,\nJ. Barral, C. Semturs, A. Karthikesalingam and\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 6 9–380 | 379\nPaper Digital Discovery\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nV. Natarajan, Large language models encode clinical\nknowledge, Nature, 2023,620, 172– 180.\n24 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter,\nC. Bekas and A. A. Lee, Molecular Transformer: A Model\nfor Uncertainty-Calibrated Chemical Reaction Prediction,\nACS Cent. Sci., 2019,5, 1572– 1583.\n25 S. Chithrananda, G. Grand and B. Ramsundar, ChemBERTa:\nLarge-Scale Self-Supervised Pretraining for Molecular\nProperty Prediction, arXiv, 2020, preprint,\narXiv:2010.09885, DOI:10.48550/arXiv.2010.09885.\n26 V. Mann and V. Venkatasubramanian, Predicting chemical\nreaction outcomes: A grammar ontology-based transformer\nframework, AIChE J., 2021,67, e17190.\n27 T. Jin, Q. Zhao, A. B. Schoeld and B. M. Savoie, Machine\nLearning Models Capable of Chemical Deduction for\nIdentifying Reaction Products, ChemRxiv, 2023, preprint,\nDOI: 10.26434/chemrxiv-2023-l6lzp.\n28 H. Park, Y. Kang and J. Kim, PMTransformer: Universal\nTransfer Learning and Cross-material Few-shot Learning in\nPorous Materials,ChemRxiv, 2023, preprint, DOI:10.26434/\nchemrxiv-2023-979mt.\n29 D. Elser, F. Huber and E. Gaquerel, Mass2SMILES: deep\nlearning based fast prediction of structures and functional\ngroups directly from high-resolution MS/MS spectra,\nbioRxiv, 2023, preprint, DOI:10.1101/2023.07.06.547963.\n30 M. Alberts, F. Zipoli and A. C. Vaucher, Learning the\nLanguage of NMR: Structure Elucidation from NMR\nspectra using Transformer Models, ChemRxiv, 2023,\npreprint, DOI:10.26434/chemrxiv-2023-8wxcz.\n31 A. Young, B. Wang and H. Röst: Tandem Mass Spectrum\nPrediction for Small Molecules using Graph Transformers ,\narXiv, 2021, preprint, arXiv:2111.04824, DOI: 10.48550/\narXiv.2111.04824.\n32 B. Liu, K. Liu, X. Qi, W. Zhang and B. Li, Classication of\ndeep-sea cold seep bacteria by transformer combined with\nRaman spectroscopy,Sci. Rep., 2023,13, 3240.\n33 B. L. Thomsen, J. B. Christensen, O. Rodenko, I. Usenov,\nR. B. Grønnemose, T. E. Andersen and M. Lassen, Accurate\nand fast identi cation of minimally prepared bacteria\nphenotypes using Raman spectroscopy assisted by\nmachine learning,Sci. Rep., 2022,12, 16436.\n34 Y.-M. Tseng, K.-L. Chen, P.-H. Chao, Y.-Y. Han and\nN.-T. Huang, Deep Learning – Assisted Surface-Enhanced\nRaman Scattering for Rapid Bacterial Identication, ACS\nAppl. Mater. Interfaces, 2023,15, 26398– 26406.\n35 T. Zhang, S. Chen, A. Wulamu, X. Guo, Q. Li and H. Zheng,\nTransG-net: transformer and graph neural network based\nmulti-modal data fusion network for molecular properties\nprediction, Appl. Intell., 2023,\n53, 16077– 16088.\n36 S. Goldman, J. Xin, J. Provenzano and C. W. Coley: Chemical\nformula inference from tandem mass spectra,arXiv, 2023,\npreprint, arXiv:2307.08240, DOI:10.48550/arXiv.2307.08240.\n37 C. Shorten and T. M. Khoshgoaar, A survey on Image Data\nAugmentation for Deep Learning,J. Big Data, 2019,6, 60.\n38 P. Nikolaev, D. Hooper, F. Webber, R. Rao, K. Decker,\nM. Krein, J. Poleski, R. Barto and B. Maruyama, Autonomy\nin materials research: a case study in carbon nanotube\ngrowth, npj Comput. Mater., 2016,2, 16031.\n39 M. Blanco, J. Coello, H. Iturriaga, S. Maspoch and C. P´erez-\nMaseda, Determination of polymorphic purity by near\ninfrared spectrometry,Anal. Chim. Acta, 2000,407, 247– 254.\n40 X. Fan, W. Ming, H. Zeng, Z. Zhang and H. Lu, Deep\nlearning-based component identi cation for the Raman\nspectra of mixtures,Analyst, 2019,144, 1789– 1798.\n41 L. Van der Maaten and G. Hinton, Visualizing Data using t-\nSNE, J. Mach. Learn. Res., 2008,9, 2579– 2605.\n42 B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba,\nLearning Deep Features for Discriminative Localization,\narXiv, 2015, preprint, arXiv:1512.04150, DOI: 10.48550/\narXiv.1512.04150.\n43 J. Vig, A Multiscale Visualization of Attention in the\nTransformer Model, arXiv, 2019, preprint,\narXiv:1906.05714, DOI:10.48550/arXiv.1906.05714.\n44 S. Abnar and W. Zuidema, Quantifying Attention Flow in\nTransformers, arXiv, 2020, preprint, arXiv:2005.00928, DOI:\n10.48550/arXiv.2005.00928.\n45 R. Zhang, H. Xie, S. Cai, Y. Hu, G.-k. Liu, W. Hong and\nZ.-q. Tian, Transfer-learning-based Raman spectra\nidentication, J. Raman Spectrosc., 2020,51, 176– 186.\n46 Y. Kang, H. Park, B. Smit and J. Kim, A multi-modal pre-\ntraining transformer for universal transfer learning in\nmetal– organic frameworks, Nat. Mach. Intell., 2023, 5, 309–\n318.\n47 Z. Wang, X. Feng, J. Liu, M. Lu and M. Li, Functional groups\nprediction from infrared spectra based on computer-assist\napproaches, Microchem. J., 2020,159, 105395.\n48 F. Zhang, R. Zhang, W. Wang, W. Yang, L. Li, Y. Xiong,\nQ. Kang and Y. Du, Ridge regression combined with model\ncomplexity analysis for near infrared (NIR) spectroscopic\nmodel updating, Chemom. Intell. Lab. Syst. , 2019, 195,\n103896.\n380 | Digital Discovery,2 0 2 4 ,3,3 6 9–380 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 29 December 2023. Downloaded on 11/5/2025 7:12:45 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6771217584609985
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5175992250442505
    },
    {
      "name": "Computer science",
      "score": 0.43944522738456726
    },
    {
      "name": "Spectral line",
      "score": 0.4292134940624237
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3948342800140381
    },
    {
      "name": "Machine learning",
      "score": 0.37114375829696655
    },
    {
      "name": "Engineering",
      "score": 0.3107464909553528
    },
    {
      "name": "Electrical engineering",
      "score": 0.11088144779205322
    },
    {
      "name": "Physics",
      "score": 0.10986965894699097
    },
    {
      "name": "Voltage",
      "score": 0.08616530895233154
    },
    {
      "name": "Astronomy",
      "score": 0.0
    }
  ]
}