{
  "title": "LM4HPC: Towards Effective Language Model Application in High-Performance Computing",
  "url": "https://openalex.org/W4382490660",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2106377209",
      "name": "Chen Le",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286813877",
      "name": "Lin, Pei-Hung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4291220774",
      "name": "Vanderbruggen, Tristan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359383831",
      "name": "Liao Chun-hua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286951913",
      "name": "Emani, Murali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4207557337",
      "name": "de Supinski, Bronis",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2080592089",
    "https://openalex.org/W4376166877",
    "https://openalex.org/W1510263988",
    "https://openalex.org/W4289827540",
    "https://openalex.org/W4311554032",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W4200555459",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4322795029",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4384434936",
    "https://openalex.org/W3186799149",
    "https://openalex.org/W2282866165",
    "https://openalex.org/W4281808401",
    "https://openalex.org/W4321446116",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4318710937",
    "https://openalex.org/W3089307846",
    "https://openalex.org/W4367369802"
  ],
  "abstract": "In recent years, language models (LMs), such as GPT-4, have been widely used in multiple domains, including natural language processing, visualization, and so on. However, applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper, we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets, AI models, and pipelines, our framework is built on top of a range of components from different levels of the machine learning software stack, with Hugging Face-compatible APIs. Using three representative tasks, we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.",
  "full_text": "LM4HPC: Towards Effective Language Model\nApplication in High-Performance Computing\nLe Chen1,2 , Pei-Hung Lin1 , Tristan Vanderbruggen1 , Chunhua Liao1 ,\nMurali Emani3 , and Bronis de Supinski 1\n1 Lawrence Livermore National Laboratory, Livermore CA 94550, USA\n2 Iowa State University, Ames, IA 50010, USA\n3 Argonne National Laboratory, Lemont, IL 60439, USA\nAbstract. In recent years, language models (LMs), such as GPT-4, have\nbeen widely used in multiple domains, including natural language pro-\ncessing, visualization, and so on. However, applying them for analyz-\ning and optimizing high-performance computing (HPC) software is still\nchallenging due to the lack of HPC-specific support. In this paper, we\ndesign the LM4HPC framework to facilitate the research and develop-\nment of HPC software analyses and optimizations using LMs. Tailored\nfor supporting HPC datasets, AI models, and pipelines, our framework\nis built on top of a range of components from different levels of the ma-\nchine learning software stack, with Hugging Face-compatible APIs. Using\nthree representative tasks, we evaluated the prototype of our framework.\nThe results show that LM4HPC can help users quickly evaluate a set of\nstate-of-the-art models and generate insightful leaderboards.\nKeywords: Language model · Programming language processing· High-\nperformance computing\n1 Introduction\nLanguage models (LMs) are models designed to understand and generate hu-\nman language. In recent years, large language models (LLMs) trained on large\namounts of text data have demonstrated stunning capabilities in various natural\nlanguage processing and visualization tasks. They have also been widely used\nto process programming languages due to the similarities between natural lan-\nguages and programming languages. For example, GPT-4 [5] shows early signs\nof artificial general intelligence. Based on a large language model trained on\ncode [10], GitHub provides an AI assistant for developing software.\nGiven the rise of LLMs, it is natural for researchers and developers in the\nhigh-performance computing community to start exploiting LMs for addressing\nvarious challenges in HPC, including code analysis, code generation, performance\noptimization, question answering, and so on. However, mainstream frameworks\nof LMs were originally designed to serve natural language processing. It is dif-\nficult for newcomers in HPC to quickly access HPC-specific datasets, models,\nand pipelines. For example, the current popular Hugging Face platform does not\narXiv:2306.14979v1  [cs.LG]  26 Jun 2023\n2 Chen et al.\ninclude dedicated pipelines for software analyses and optimizations. Another\nchallenge is the entire field is evolving quickly, with new techniques emerging\nalmost weekly, making it challenging for HPC users to keep up with the latest\ntechniques and find relevant ones. Last but not least, there is a lack of stan-\ndard, reproducible evaluation processes for LMs focusing on HPC-specific tasks.\nTherefore, it is difficult to have a fair comparison among different models for a\ngiven HPC task.\nIn this paper, we propose a framework (named LM4HPC) designed to serve\nHPC users as first-class citizens by including internal components and external\nAPIs relevant to HPC-specific tasks. LM4HPC’s components include models,\ndatasets, pipelines, and so on, while the APIs allow users to interact with these\ncomponents to finish given HPC tasks. We highlight the contributions of our\nwork as follows:\n– We design an extensible framework for including and exposing relevant ma-\nchine learning components to facilitate the adoption of large language models\nfor HPC-specific tasks.\n– The framework provides a set of APIs to facilitate essential operations, in-\ncluding code preprocessing, tokenization, integration with new data, and\nevaluation.\n– A set of pipelines have been developed to support common HPC tasks, in-\ncluding code similarity analysis, parallelism detection, question answering,\nand so on.\n– We provide HPC-specific datasets such as DRB-ML, OMP4Par, and OM-\nPQA to support various HPC pipelines.\n– Our work introduces standardized workflows and metrics to enable fair and\nreproducible evaluation of LLMs for HPC-specific tasks.\n– Using three representative tasks, we demonstrated how the framework can\nbe used to test a set of language models and generate leaderboards.\n2 Background\nLanguage models (LMs) are machine learning models designed to comprehend\nand generate human language. They can be used to facilitate natural and in-\ntuitive interactions between humans and machines. Early generations of LMs,\nusing recurrent neural networks (RNNs), showed inspiring results for various\nnatural language processing (NLP) tasks. A transformative evolution by the\nTransformer [24] reveals remarkable potentials of LMs. Introduced by Vaswani\net al., transformer models utilize the attention mechanism to capture the de-\npendencies between all words in an input sentence, irrespective of their posi-\ntions. Compared to RNNs, transformers process data in parallel rather than\nsequentially and significantly improve the efficiency of model training and in-\nference. Transformers further enables the inauguration of the large language\nmodels (LLMs). Compared to LMs, LLMs are trained on a vast amount of data\nand possess parameter counts on the order of billions or more, allowing them\nto generate more detailed and nuanced responses. Examples of LLMs include\nLM4HPC: Towards Effective LM Application in HPC 3\nOpenAI’s GPT-3, GPT-4 and Google’s BARD. Nowadays, LLMs have shown\nremarkable capabilities in NLP tasks like translation, question answering, and\ntext generation.\nTable 1.Language models, associated training data and tasks\nModel Training data Token\nLimit Avail.Name Released Size Type Size\nBERT 2018/10 340M Text 3.5B words 512 Weights\nCodeBERT 2020/11 125M Mixed 2.1M(bimodal)\n6.4M (unimodal) 512 Weights\nMegatron 2021/04 1T Text 174 GB 512 Weights\nGraphCodeBERT 2021/05 110M Code 2.3M functions 512 Weights\nCodeT5 2021/11 770M Code 8.35M instances 512 Weights\nGPT-3 2022/03/15 175B Mixed 500B tokens 4096 Weights\nLLaMA 2023/02/24 7∼65B Mixed 1.4T tokens 4096 Weights*\nGPT-4 2023/03/14 1T Mixed undisclosed 8k/32k API*\nBARD 2023/03/21 1.6B Mixed 1.56T words 1000 API\nCerebras-GPT 2023/03/28 0.11∼13B Text 800 GB 2048 Weights\nDolly 2.0 2023/04/12 3∼12B Text 15k instr./resp. 2048 Weights\nStarCoder 2023/05/4 15B Code 1T tokens 8192 Weights\nStarChat-Alpha 2023/05/4 16B Code 31k instr./resp. 8192 Weights\nTable 1 shows some example language models and their release dates, sizes,\ntraining data, input token length limits, and availability. LLaMA [23]’s weights\ncan be obtained after filling out some form. GPT-4 has a waiting list to use its\nAPI. At the time of writing this paper, we have not yet obtained its access.\nLMs are trained mainly by text data with a focus on NLP. The sources of\nthe training data mainly come from books, web content, newspapers, scientific\narticles, and other text data in various natural languages. Latest LLMs have\ndemonstrated rich skill sets in NLP including text prediction, common sense\nreasoning, reading comprehension, translation and question answering.\nThere has been a keen interest in deploying NLP techniques to program-\nming language processing (PLP) tasks, such as code summarization, code gen-\neration, and code similarity analysis [8,14]. Previous studies have demonstrated\nsuccessful applications of traditional language models to PLP tasks, showing the\nfeasibility of this approach [12]. CodeBERT [13], for example, is a transformer-\nbased model trained with a diverse range of programming languages and can\nbe used for a variety of programming-related tasks. Similarly, CodeT5 [26] is a\nvariant of Google’s T5 language model, trained specifically on code datasets to\nperform advanced programming tasks like code completion, bug detection, and\ncode summarization. Lately, StarCoder [18], a 15B parameter model trained with\n1 trillion tokens sourced from a large collection of permissively licensed GitHub\nrepositories, is developed to be a Large Language Model mainly for code gener-\nation or completion. StarChat-Alpha is a GPT-like chat model fine-tuned from\nStarCoder to act as a helpful coding assistant.\n4 Chen et al.\n2.1 LMs for HPC\nWith the recent breakthroughs in Generative Pretrained Transformer (GPT)\nlarge language models [4], it has become increasingly intriguing to explore the\napplication of large language models (LLMs) for HPC tasks. However, their\ndeployment in the HPC domain is still relatively unexplored. This venture comes\nwith various challenges, including:\n1. Pipelines: Traditional language model frameworks like Hugging Face were\ndesigned to support natural language processing or compute vision problems.\nExpanding LMs to any new domain, including HPC, requires the addition\nof new pipelines designed to finish domain-specific tasks.\n2. Datasets: The HPC domain encompasses an extensive amount of code span-\nning various fields, including biology and climate modeling. However, prepar-\ning this data for machine learning training, such as labeling parallelizable\nloops in HPC programs for parallelism detection, presents significant chal-\nlenges. The scarcity of ready-to-use, pre-labeled HPC datasets poses a partic-\nular obstacle for training language models, especially large ones, highlighting\nthe need for more shared resources in the community.\n3. Pre-processing: Pre-processing in the context of LMs for HPC typically in-\nvolves the conversion of source files into a sequence of tokens. However, the\ndirect application of NLP tokenizers to code can be sub-optimal. For in-\nstance, an NLP tokenizer might split a variable name into two tokens, a\nscenario that is not desirable for PLP analysis. Also, models designed for\nprocessing source code may use graph representations, such as abstract syn-\ntax trees, to have better performance.\n4. Input size limit: Language models often have limited input token lengths\n(such as 512 to a few thousand of tokens). HPC tasks often involve processing\nlarge-scale software packages with millions of lines of source code.\n5. Evaluation: There is a pressing need for standardized and reproducible eval-\nuation of different models in the context of various HPC tasks, using metrics\nsuitable for domain-specific requirements.\n3 Approach\nTo address the challenges discussed in Section 2, we introduce LM4HPC, a com-\nprehensive framework that encapsulates a suite of machine learning components\nwithin user-friendly APIs. This framework is tailored for HPC users, simplifying\nthe implementation process and making the robust capabilities of language mod-\nels more accessible and user-friendly within the HPC community. The primary\ngoal of LM4HPC is to reduce the complexities inherent in employing language\nmodels, thus enabling HPC users to leverage their powerful capabilities more\neffectively and efficiently.\n3.1 LM4HPC design overview\nFigure 1 provides the overview of the LM4HPC framework. It is built on top of\nmultiple internal machine learning components with Hugging Face-compatible\nLM4HPC: Towards Effective LM Application in HPC 5\nAPIs. Higher-level components provide concepts and interfaces to users, while\nmiddle or lower-level components provide implementation support. Table 2 shows\nthe example classes and functions in LM4HPC API, including those supporting\nHPC-specific language models, tokenizers for programming languages, datasets,\ninference pipelines, and evaluation. We elaborate on some essential components\nin the following subsections.\nCode Similarity Analysis Parallelism Detection OMP Q&A\ndatasets\nHugging Face LangChain\nPyTorch Tensorflow …onnx\nCPU GPU Cluster\nHigh-Level \nComponents \n&API\nRuntime\nLibrary\nML\nFramework\nHardware\nLMs\n…\npipelines\nMetricsEvaluation Harness Code Processing Tools\ncode2seq code2tree code2graphtokenization\n…vector stores\nFig. 1.Overview of the LM4HPC framework\n3.2 HPC Tasks and Inference Pipelines\nHPC users are interested in a wide range of tasks related to programming lan-\nguage processing. Table 3 outlines one way to categorize HPC-specific tasks. The\npurpose here is not to provide a comprehensive taxonomy of all tasks but a start-\ning point for common tasks we are interested in supporting in our framework.\nMost tasks are self-explanatory by names and each may have further sub-tasks.\nFor example, clone detection can be viewed as a specialized sub-task under code\nsimilarity analysis.\nIn the context of machine learning, a pipeline represents a sequence of data\nprocessing stages to complete a task. Our LM4HPC framework extends the\npipeline function provided by Hugging Face, adapting it for HPC tasks. We have\ndeveloped three inference pipelines: code similarity analysis, parallelism detec-\ntion, and OpenMP question answering. Code similarity analysis determines the\nsimilarity between a pair of code snippets. Parallel detection is defined to check\n6 Chen et al.\nTable 2.LM4HPC API: example classes and functions. Each class can be imported\nusing “from lm4hpc import *” in Python.\nLM4HPC\nclasses\nDescription Example API functions\nhpcmodel\nFine-tune text-based\n(HF, OpenAI) and\ngraph-based models,\nincluding local private\nones, for HPC tasks\nhpcmodel.from pretrained(model name or path:\nOptional[str], *model args, **kwargs)\nhpcmodel.save pretrained(model name or path: str,\n*model args, **kwargs)\nhpcmodel.finetune()\nhpctokenizer\nAPIs to represent code\nin either tokenized text,\ntrees, or graphs\nhpctokenizer.from pretrained(model name or path:\nOptional[str], *model args, **kwargs)\nhpctokenizer.addtokens(contentsingle word=False,\nstrip=False, normalized=True)\nhpctokenizer.encoding()\nhpcdatasets Load and process HPC\ndatasets\nhpcdatasets.load(path: str, data files: Union[str,\nList, Dict, None], **kwargs)\nhpcdatasets.split(dataset: hpcdatasets, partition:\n[float, float, float], **kwargs)\nhpcdatasets.shuffle(dataset: hpcdatasets, **kwargs)\nhpcdatasets.sort(dataset: hpcdatasets, **kwargs)\nhpcpipeline Pre-built pipelines for\ncommon PLP tasks\nhpcpipeline(task: str, model name or path: str,\n*model args, **kwargs)\nhpceval Evaluate the results of\nvarious models\nhpceval.compute(task: str, models name or path:\n[[str]], data files: Union[str, List, Dict, None],\n*model args, **kwargs)\nhpceval.plot(shape: str)\nTable 3.HPC Tasks for Programming Language Processing: Categories and Examples\nCode Analysis Code Generation Others\nCompiler Analysis Code Completion Test Case Generation\nAlgorithm Classification Natural Language-to-Code Code Search\nCode Similarity Analysis Code Translation Question Answering\nDocumentation Generation Code Repair Code Review\nParallelism Detection Code Migration Decompilation\nDefect Detection Compilation IR-to-Source Translation\nif an input code snippet can be parallelized or not using OpenMP. The OpenMP\nquestion answering pipeline is designed to use models to generate answers to\nOpenMP-related questions.\nTokenizers are responsible for preprocessing input into an array of numbers\nas inputs to a model. They are essential components used by pipelines. Most LM\ntokenizers are primarily designed for NLP tasks. For instance, given a function\nname my func, a typical NLP tokenizer like BERT might split it into separate\ntokens (such as ‘my’, ‘ ’, and ‘func’) while a code-aware tokenizer may treat the\nfunction name as a single entity to ensure a more meaningful representation.\nTo overcome this, we developed the LM4HPC tokenizer, leveraging the treesit-\nter [2] and programl [1] library. Our tokenizer is specifically designed to handle\nthe pre-processing of code data required for a language model. It includes tok-\nLM4HPC: Towards Effective LM Application in HPC 7\nenizers such as the ast-tokenizer. As a result, LM4HPC can accommodate models\n(such as augAST [9]) that require AST as input in the pipeline.\n3.3 Datasets\nDatasets are crucial for any machine learning application. Within the LM4HPC\nframework, we contribute HPC-specific datasets either by converting existing\nones into Hugging Face-compatible formats or by creating new ones from scratch.\nWe have converted three existing datasets to be compatible with Hugging\nFace dataset API: POJ-104 [21], DRB-ML [20], and OMP4Par [9]. POJ-104 is\nderived from a pedagogical programming open judge (OJ) system that auto-\nmatically evaluates the validity of submitted source code for specific problems\nby executing the code. This dataset is particularly useful for the code similarity\ntask. The DRB-ML dataset contains 658 C/C++ OpenMP kernels derived from\nDataRaceBench [19]. We extended it to have labels indicating if a kernel is paral-\nlelizable or not. The OMP4Par dataset is an open-source benchmark composed\nof data from three resources: code crawled from GitHub, OpenMP benchmarks\nsuch as Nas Parallel Benchmarks [17] and Rodinia [7], and synthetic code. This\ndataset contains loops with labels indicating whether a loop is parallel and, if\nparallelizable, the corresponding OpenMP directive associated with the loop.\nFurthermore, we have manually created a new OpenMP question answering\ndataset called OMPQA in order to probe the capabilities of language models\nin single-turn interactions with users. Similar to other QA datasets, we include\nsome request-response pairs which are not strictly question-answering pairs. The\ncategories and examples of questions in the OMPQA dataset can be found in\nTable 4.\nCategory Count Example Questions\nBasics 40 What is a worksharing construct in OpenMP?\nExamples 20 Give an example OpenMP C code for computing PI using numerical\nintegration.\nCompilers 24 In what language is LLVM written?\nHow is a parallel region represented in Clang?\nBenchmarks 23 What are the NAS Parallel benchmarks?\nWhich benchmark assesses data race detection tools?\nTable 4.OMPQA: categories and examples of questions\n3.4 Integration With New Data\nLanguage models derive knowledge from training datasets and store this knowl-\nedge in internal weights within the model’s neural network architecture. How-\never, incorporating new information into a trained model presents a challenge.\nTraditionally, one might fine-tune pre-trained models with their own data for\nspecific tasks, but this approach requires substantial relevant data and can be\n8 Chen et al.\nresource-intensive. An alternative approach involves integrating new data as con-\ntext information into a user prompt, but this is constrained by the limited input\ntoken lengths of current models.\nTo address this challenge, LM4HPC leverages the LangChain framework [6]\nto easily integrate new data. LangChain aggregates a wide variety of components\nto build applications using LLMs. Particularly, it provides APIs allowing LLM\napplications to store large amounts of text in semantic databases called vector\nstores. The way to integrate new data can be done in two steps. First, text data\nis chunked and embedded with an LLM before being saved into a vector store.\nLater, user prompts are matched with relevant chunks in the vector store using\nsimilarity analysis. The top-matched chunks are then injected into the original\nprompts to form a new prompt with relevant context information. By employing\nthis new prompt, language models can generate answers that incorporate new\nand relevant user data while still staying within the token length limits.\n3.5 Evaluation\nAn easily accessible harness for evaluating different language models on HPC\ntasks is crucial. Standard and reproducible results from such evaluations can\nprovide researchers and developers with insightful starting points, helping them\nselect suitable models for their specific needs and identify research opportunities.\nIn response to this need, we developed an evaluator API in LM4HPC. One\nchallenge we encountered is the lack of standardized metrics for code evaluation.\nUnlike natural language tasks, where metrics such as BLEU, ROUGE, and ME-\nTEOR are commonly used, the domain of code lacks such universally accepted\nmeasures of quality. We are adding various LLM metrics such as CodeBLEU [22]\nfor code output. Another challenge is that language models may generate dif-\nferent answers for the same input in different inference runs. Evaluation should\nconsider consistent sampling settings (such as temperatures) and control over\nrandom seeds to improve reproducibility.\nUltimately, many users are interested in seeing leaderboards that showcase\nmainstream models competing on common HPC tasks. To satisfy this interest,\nwe create and release a set of test harnesses scripts to enable standard and\nreproducible evaluation for supported HPC tasks.\n4 Preliminary Results\nIn this section, we evaluate the current prototype implementation of LM4HPC\nthrough experiments designed to generate leaderboards for three representative\ntasks: Code Similarity Analysis, Parallelism Detection, and OpenMP Question\nAnswering. LM4HPC utilizes LangChain v0.0.174, Hugging Face’s transformers\nv4.29.0 and datasets v2.12.0 as our runtime libraries. Details of the models and\ndatasets will be discussed in subsequent subsections.\nOur experiments were conducted on two machines: 1) a Google Colab VM\nwith a 6-core Xeon processor operating at 2.20GHz, 83.5 GB main memory,\n166GB HDD drive, and an NVIDIA A100 GPU with 40 GB memory. 2) a Dell\nworkstation equipped with a dual Intel Xeon 6238 CPU operating at 2.10GHz,\nLM4HPC: Towards Effective LM Application in HPC 9\n128 GB main memory, 1TB SSD drive, and an NVIDIA Quadro RTX 6000 GPU\nwith 24GB memory. The majority of our experiments were run on the Google\nColab machine to leverage its superior GPU memory. However, we encountered\ndifficulties running Cerebras-GPT on the Colab machine and were compelled to\nuse the Dell workstation with larger CPU memory instead.\n4.1 Code Similarity Analysis\nThe code similarity task is designed to measure the syntactic and/or semantic\nsimilarity between two code snippets. Such analysis information can be beneficial\nin various scenarios such as plagiarism detection, code reuse and refactoring, bug\ndetection and repair, licensing compliance, malware detection, and so on.\nPreparing Datasets and Ground Truth.Two datasets introduced in\nSection 3.3, POJ-104 and DRB-ML, are loaded through LM4HPC’s datasets\nAPI for this experiment. For each pair of code snippets in the POJ-104 dataset,\nwe assign a binary similarity label based on their functional labels. A similarity\nlabel of 1 signifies that the snippet pair shares the same functional label and\nwe assign a similarity score of 1. Otherwise, the label is 0. We have processed\nthe DRB-ML dataset using a similar methodology to generate code pairs and\nlabels. The main difference is that the similarity ground truth for DRB-ML is\nderived from its own similarity score table [11], providing a precise and reliable\nsimilarity measurement between code snippets in the dataset.\nInference Experiments and Evaluation.We employ LM4HPC’s code\nsimilarity pipeline to test various models. The default model for this pipeline\nis CodeBERT. We additionally select four models from Table 1 for evaluation:\nGraphCodeBERT, gpt-3.5-turbo, Dolly 2.0 (12B), and Cerebras-GPT (13B).\nWe set the maximum token length for the model output to 256. This limits\nthe verbosity of the model and keeps its responses concise. Additionally, we set\nthe temperature parameter to 0 when applicable. For models like Dolly 2.0 that\nrequire positive temperature, we set the temperature to be 1×10−6. This setting\nensures that the model’s responses are consistent and deterministic, minimizing\nvariability and uncertainty in its output.\nWithin LM4HPC, the approach of processing input code pairs depends on\nthe type of the model employed. Models like CodeBert and GraphCodeBert\nare specifically devised and trained on a variety of programming languages. We\ndirectly feed a pair of code snippets to generate a similarity prediction. On the\nother hand, large language models like gpt-3.5-turbo, Dolly 2.0, and Cerebras-\nGPT are evaluated using the following prompt template: “Code 1: {...} Code 2:\n{...} Determine whether the two code snippets are similar. If the code snippets\nare similar, output 1; otherwise, output 0.”.\nResults. The Code Similarity Analysis leaderboards generated using the\ntwo datasets are shown in Table 5. Notably, gpt-3.5-turbo demonstrates supe-\nrior performance. Two other models, StarChat-Alpha and Dolly 2.0, also exhibit\ncommendable performance. Most large language models outperform traditional\nmodels (GraphCodeBERT and CodeBERT) that were specifically trained for\ncode analysis. However, Cerebras-GPT struggled to comprehend the code and\n10 Chen et al.\nmostly returned arbitrary word tokens, indicating a lack of effective code under-\nstanding since it is mostly designed for natural language processing.\nTable 5.Code Similarity Analysis Leaderboard: POJ-104 (left) and DRB-ML (right)\nModel Precision Recall F1\ngpt-3.5-turbo 78.4 74.2 76.2\nDolly 2.0 12B 61.9 61.3 61.6\nStarChat-Alpha 59.4 56.2 57.8\nGraphCodeBERT 52.7 60.3 56.3\nCodeBERT 51.5 59.4 55.2\nCerebras-GPT 13B 0 0 0\nModel Precision Recall F1\ngpt-3.5-turbo 82.4 81.3 81.8\nStarChat-Alpha 79.6 77.4 78.5\nDolly 2.0 12B 74.3 73.2 73.7\nGraphCodeBERT 79.4 77.9 78.6\nCodeBERT 76.9 74.5 75.7\nCerebras-GPT 13B 0 0 0\n4.2 Parallelism Detection\nThe parallelism detection task aims to identify parallelism opportunities within\na given code snippet. We utilized two datasets, OMP4Par and DRB-ML intro-\nduced in Section 3.3, for the experiment.\nPreparing Datasets and Ground Truth.The OMP4Par dataset is specif-\nically designed for parallelism detection. Its existing labeling scheme allows us to\nprepare the data for binary classification models. Similarly, we prepared DRB-\nML dataset with a label indicating whether each code snippet is parallelizable\nusing OpenMP or not.\nIt is worth noting that both datasets have undergone source code pre-processing\nsteps, including comment removal and code snippet extraction. These steps are\ncommon practice [9] to ensure that code snippets are small enough to be fed into\nlanguage models with limited input token sequence sizes. However, the resulting\ncode snippets may lose their context information, such as variable declarations.\nThis is a serious limitation of language models with limited input sizes when\napplied to process large source files.\nInference Experiments and Results.We selected six models to generate\nparallelism detection leaderboards. Four of them are introduced in Section 2.\nThey take the code snippets in a prompt template: “As an OpenMP expert, you\nwill analyze the given code snippet to determine if it can be parallelized. Code:\n{...}. Answer yes or no first:”. The other two are augAST [9] and DeepSCC-\nbased [15], which are pre-trained models using OMP4Par’s training dataset. We\nfed code snippets to these two models to directly obtain predicted labels.\nTable 6 presents the resulting leaderboards. The highest F1 score reaches\n93.9, indicating that LMs can be very effective for detecting parallelism. How-\never, the datasets contain small-scale code snippets that are significantly sim-\npler than real HPC codes. Again, gpt-3.5-turbo outperforms all other models\noverall, including specially trained models like augAST and DeepSCC. AugAST\nperforms better than gpt-3.5-turbo in terms of precision, suggesting it’s more\neffective in predicting a positive class, which, in this case, is parallelizable code.\nFinally, Cerebras-GPT did not perform well in this code analysis task.\nLM4HPC: Towards Effective LM Application in HPC 11\nTable 6.Parallelism Detection Leaderboards: OMP4Par (left) and DRB-ML(right)\nModel PrecisionRecall F1\ngpt-3.5-turbo 90.6 89.3 89.9\naugAST 92.1 82.4 87.0\nDeepSCC 82.7 81.4 82.0\nStarChat-Alpha 85.7 68.2 75.9\nDolly 2.0 12B 64.2 63.7 63.9\nCerebras-GPT 13B 0 0 0\nModel PrecisionRecall F1\ngpt-3.5-turbo 90.0 98.9 94.2\naugAST 91.4 72.3 80.7\nDeepSCC 80.4 79.5 79.9\nStarChat-Alpha 81.9 20.3 32.5\nDolly 2.0 12B 40.0 11.2 2.17\nCerebras-GPT 13B 0 0 0\n4.3 OpenMP Q&A\nIn this experiment, we utilized LM4HPC to evaluate the capabilities of several\nlanguage models in answering questions related to OpenMP. This evaluation was\nconducted using the OMPQA dataset, introduced in Section 3.3.\nExperiment Settings.\nEach model receives the question in the following prompt template: “You are\nan OpenMP expert. Please answer this question. Question: {question}”. Two\nmetrics are selected to evaluate the quality of answers: the Bilingual Evaluation\nUnderstudy (BLEU) and ROUGE-L metrics. BLEU is a precision-oriented met-\nric measuring the overlap of n-grams between the generated text and a set of\nreference texts. ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation\n- Longest Common Subsequence) calculates the longest common subsequence\n(LCS) that appears in a left-to-right sequence in both the system-generated and\nreference summaries, thus providing a measure of the coherence and fluidity of\nthe generated text.\nResults. Table 7 displays the Q&A leaderboard of several selected models.\nWe additionally include the memory and execution time information. The ex-\nperiments using gpt-3.5-turbo do not consume any local GPU memory since the\nmodel is invoked remotely through OpenAI’s API.\nAgain, gpt-3.5-turbo unsurprisingly outperforms other LLMs, including StarChat-\nAlpha, Dolly 2.0, and Cerebras-GPT. However, the highest ROUGE-L F1 score\nof 0.259 indicates that all models have room for improvement in answering\nOpenMP questions. One reason is that many questions in OMPQA are open-\nended and do not necessarily have a single correct answer. Also, the two metrics\nused do not sufficiently consider semantics.\nTo enhance the capacity of large language models (LLMs) in accurately re-\nsponding to OpenMP queries, we integrate the official OpenMP documentation\ninto our process. We employ LangChain, a mechanism designed to efficiently\nstore and retrieve language model embeddings, enabling us to accommodate\nlarge volumes of new data. To assess the efficacy of using LangChain to incorpo-\nrate additional user data, we leverage its API to create a vector store. This vector\nstore holds embeddings of text chunks derived from the OpenMP API Specifi-\ncation v5.2 (669 pages) and the OpenMP Application Programming Interface\nExamples v5.2.1 (575 pages). We then select two LangChain-supported models,\nGPT-3.5 and Dolly 2.0, to utilize the vector store as an additional resource for\nanswering queries, thereby demonstrating the practical utility of our approach.\nThe results indicate slight improvements in both the BLEU and ROUGE-L F1\n12 Chen et al.\nTable 7.Q&A Leaderboard using the OMPQA dataset. The arrow indicates the per-\nformance changes when augmenting external knowledge base by LangChain.\nROUGEModel CPU\nMem. (GB)\nGPU\nMem. (GB) Time(s) BLEU Recall Prescision F1\ngpt-3.5-turbo\n+ LangChain 4.1 0 21.452 0.147↑ 0.347↓ 0.262↓ 0.259↑\ngpt-3.5-turbo 4.2 0 12.749 0.139 0.446 0.231 0.257\nStarChat-Alpha 6.8 18.9 29.732 0.082 0.322 0.149 0.172\nDolly 2.0 12B\n+ LangChain 27.4 39.8 7.217 0.084↑ 0.228↑ 0.232↓ 0.182↑\nDolly 2.0 12B 27.1 39.2 8.147 0.06 0.208 0.312 0.148\nCerebras-GPT 13B 52.6 11.7 590.763 0.071 0.319 0.089 0.112\nscores, increasing from 0.139 to 0.147 and from 0.257 to 0.259, respectively. How-\never, there are mixed results for recall and precision metrics. gpt-3.5-turbo has\na better recall, 0.446, compared to 0.347 of the Langchain approach.\nFurther, we examine the effectiveness of the LangChain approach across dif-\nferent question categories. When addressing ‘Basic’ questions, the BLEU scores\nrise by 20.7% and 9.8% for gpt-3.5-turbo and Dolly 2.0, respectively. Addi-\ntionally, we assess the LangChain performance using the CodeBLEU metric[22]\nfor the ‘Examples’ category, observing a score increase of 6.1% and 12.2% for\ngpt-3.5-turbo and Dolly 2.0, respectively. These observations indicate that aug-\nmenting LLMs with documentation via LangChain improves performance for\n‘Basic’ and ‘Examples’ categories. However, for ‘Compilers’ and ‘Benchmarks’\ncategories, the performance of gpt-3.5-turbo and Dolly 2.0 diminishes when uti-\nlizing LangChain, recording an average BLEU score drop of 8.0% and 7.9%,\nrespectively. This drop is likely because our documentation does not include\ninformation relevant to compiler and benchmark topics.\nWe also manually investigated the answers generated by the models. Overall,\nStarChat-Alpha delivers competitive results compared to GPT-3.5. It seems to\nbe a good choice for people who want to use open-source language models based\non our experiments. Research has indicated that GPT-4 surpasses GPT-3.5 in\na variety of domains. However, as of now, API accessibility for GPT-4 has not\nbeen made publicly available. We plan to assess GPT-4’s performance as soon\nas it becomes accessible and incorporate it into our framework if it benefits HPC\ntasks.\n5 Related Work\nPyTorch and TensorFlow are the most popular frameworks, backed by Meta AI\nand Google, respectively. Both frameworks are similar in many respects, includ-\ning 1) providing low-level APIs for development, 2) supporting a rich collection\nof libraries, and 3) maintaining dedicated hubs - PyTorch Hub and TensorFlow\nHub - for providing pre-trained ML models. Hugging Face is a large open-source\ncommunity that builds tools to enable users to build, train, and deploy machine\nlearning models based on open-source code and technologies. Hugging Face is\nLM4HPC: Towards Effective LM Application in HPC 13\nbest known for its Transformers library, which exposes a collection of Python\nAPIs to leverage state-of-the-art deep learning architectures for NLP tasks. With\nthe goal to simplify end-to-end NLP tasks, Hugging Face Transformers offers a\npipeline that performs all pre- and post-processing steps on the given input text\ndata. The overall process of the model inference is encapsulated within these\npipelines. With the pipeline, users only need to provide the input texts and the\nmodel for the task. The remaining connections among a model and required pre-\nand post-processing steps are hidden within the pipeline implementation.\nThere were various research works and developments to improve the ML\necosystem to be Findable, Accessible, Interoperable, and Reproducible (FAIR).\nThese existing frameworks aim to make the models, datasets, or both FAIR.\nAmong these frameworks, HPCFAIR [25] focuses on providing support for model\ninteroperability, search capabilities for datasets and models, and seamless inte-\ngration into HPC workflows. The work in [28] extended this work to include\nsupport for interoperability across different framework implementations using\nONNX and provision to retrain a model with transfer learning. However, HPC-\nFAIR framework relies on users to handle data pre- and post-processing. In\ncomparison, LM4HPC is equipped to manage data processing within the pipeline\ndesign and generate leaderboards for supported HPC tasks.\nGeneral LLMs are trained with data covering general knowledge and infor-\nmation that is usually collected from public domains. Domain-specific datasets\ncan be collected for the training of a specialized model or for the fine-tuning of\na general-purpose model. MedQA[16] is an example of domain-specific datasets\ncollecting question-answer pairs and textbooks from professional medical board\nexams. ExeBench [3], another domain-specific dataset for tasks in compilation\nand software engineering, contains millions of runnable and representative C\nfunctions collected from GitHub. In addition to collecting existing data, ML re-\nsearch has started to automate dataset creation with assistance from the LLMs.\nThe developers of LaMini-LM [27] develop a large set of 2.58M instruction and\nresponse pairs based on both existing and newly-generated instructions. A hand-\nful of seed examples from the existing LLM prompts and 2.2M categories from\nWikipedia from existing are submitted to the gpt-3.5-turbo to generate rele-\nvant instructions. Similarly, the responses for the generated instructions are also\ngenerated by the gpt-3.5-turbo.\n6 Conclusion\nIn this paper, we presented our efforts to facilitate the application of language\nmodels for tasks specific to High-Performance Computing. We have developed\nthe LM4HPC framework to encompass and expose relevant machine learning\ncomponents via corresponding APIs. Our experimental findings suggest that\nGPT-3 performs competitively, despite not being specifically designed for HPC\ntasks. However, there is significant room for improvement in answering OpenMP\nquestions. Furthermore, the input size limitation of language models adds com-\nplexity to certain tasks, such as parallelism detection. Finally, an obstacle to\n14 Chen et al.\nadvancing the application of language models for HPC tasks is the absence of\nHPC-specific training and evaluation datasets.\nLooking ahead, our future work will explore automated approaches to gen-\nerating HPC-specific datasets. We intend to enhance LM4HPC’s capabilities to\nsupport the fine-tuning of models for HPC-related tasks, including those related\nto the Message Passing Interface (MPI), and to provide performance analysis\nand optimization suggestions.\nAcknowledgement\nPrepared by LLNL under Contract DE-AC52-07NA27344 (LLNL-CONF-849438)\nand supported by the U.S. Department of Energy, Office of Science, Advanced\nScientific Computing Research.\nReferences\n1. ProGraML: Program Graphs for Machine Learning (2022), https://pypi.org/\nproject/programl/, accessed: 2023-05-15\n2. The py-tree-sitter project (2023), https://pypi.org/project/\ntree-sitter-builds/, accessed: 2023-05-15\n3. Armengol-Estap´ e, J., Woodruff, J., Brauckmann, A., Magalh˜ aes, J.W.d.S.,\nO’Boyle, M.F.: ExeBench: an ML-scale dataset of executable C functions. In: Pro-\nceedings of the 6th ACM SIGPLAN International Symposium on Machine Pro-\ngramming. pp. 50–59 (2022)\n4. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot\nlearners. Advances in neural information processing systems 33, 1877–1901 (2020)\n5. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee,\nP., Lee, Y.T., Li, Y., Lundberg, S., et al.: Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023)\n6. Chase, H.: LangChain: Next Generation Language Processing (2023), https://\nlangchain.com/, accessed: 2023-05-15\n7. Che, S., Boyer, M., Meng, J., Tarjan, D., Sheaffer, J.W., Lee, S.H., Skadron, K.:\nRodinia: A benchmark suite for heterogeneous computing. In: 2009 IEEE interna-\ntional symposium on workload characterization (IISWC). pp. 44–54. Ieee (2009)\n8. Chen, L., Mahmud, Q.I., Jannesari, A.: Multi-view learning for parallelism discov-\nery of sequential programs. In: 2022 IEEE International Parallel and Distributed\nProcessing Symposium Workshops (IPDPSW). pp. 295–303. IEEE (2022)\n9. Chen, L., Mahmud, Q.I., Phan, H., Ahmed, N.K., Jannesari, A.: Learning to Par-\nallelize with OpenMP by Augmented Heterogeneous AST Representation. arXiv\npreprint arXiv:2305.05779 (2023)\n10. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards,\nH., Burda, Y., Joseph, N., Brockman, G., et al.: Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374 (2021)\n11. Chen, W., Vanderbruggen, T., Lin, P.H., Liao, C., Emani, M.: Early Experi-\nence with Transformer-Based Similarity Analysis for DataRaceBench. In: 2022\nIEEE/ACM Sixth International Workshop on Software Correctness for HPC Ap-\nplications (Correctness). pp. 45–53. IEEE (2022)\nLM4HPC: Towards Effective LM Application in HPC 15\n12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n13. Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu,\nT., Jiang, D., et al.: Codebert: A pre-trained model for programming and natural\nlanguages. arXiv preprint arXiv:2002.08155 (2020)\n14. Flynn, P., Vanderbruggen, T., Liao, C., Lin, P.H., Emani, M., Shen, X.: Finding\nReusable Machine Learning Components to Build Programming Language Pro-\ncessing Pipelines. arXiv preprint arXiv:2208.05596 (2022)\n15. Harel, R., Pinter, Y., Oren, G.: Learning to parallelize in a shared-memory en-\nvironment with transformers. In: Proceedings of the 28th ACM SIGPLAN An-\nnual Symposium on Principles and Practice of Parallel Programming. pp. 450–452\n(2023)\n16. Jin, D., Pan, E., Oufattole, N., Weng, W.H., Fang, H., Szolovits, P.: What Disease\ndoes this Patient Have? A Large-scale Open Domain Question Answering Dataset\nfrom Medical Exams. arXiv preprint arXiv:2009.13081 (2020)\n17. Jin, H.Q., Frumkin, M., Yan, J.: The OpenMP implementation of NAS parallel\nbenchmarks and its performance (1999)\n18. Li, R., Allal, L.B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M.,\nAkiki, C., Li, J., Chim, J., et al.: Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161 (2023)\n19. Liao, C., Lin, P.H., Asplund, J., Schordan, M., Karlin, I.: Dataracebench: a bench-\nmark suite for systematic evaluation of data race detection tools. In: Proceedings\nof the International Conference for High Performance Computing, Networking,\nStorage and Analysis. pp. 1–14 (2017)\n20. Lin, P.H., Liao, C.: Drb-ml-dataset (12 2022). https://doi.org/10.11579/1958879\n21. Mou, L., Li, G., Zhang, L., Wang, T., Jin, Z.: Convolutional neural networks over\ntree structures for programming language processing. In: Proceedings of the AAAI\nconference on artificial intelligence. vol. 30 (2016)\n22. Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M.,\nBlanco, A., Ma, S.: CodeBLEU: a method for automatic evaluation of code syn-\nthesis. arXiv preprint arXiv:2009.10297 (2020)\n23. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n24. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n25. Verma, G., Emani, M., Liao, C., Lin, P.H., Vanderbruggen, T., Shen, X., Chapman,\nB.: HPCFAIR: Enabling FAIR AI for HPC Applications. In: 2021 IEEE/ACM\nWorkshop on Machine Learning in High Performance Computing Environments\n(MLHPC). pp. 58–68. IEEE (2021)\n26. Wang, Y., Wang, W., Joty, S., Hoi, S.C.: Codet5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understanding and generation. arXiv\npreprint arXiv:2109.00859 (2021)\n27. Wu, M., Waheed, A., Zhang, C., Abdul-Mageed, M., Aji, A.F.: LaMini-LM: A\nDiverse Herd of Distilled Models from Large-Scale Instructions. arXiv preprint\narXiv:2304.14402 (2023)\n28. Yu, S., Emani, M., Liao, C., Lin, P.H., Vanderbruggen, T., Shen, X., Jannesari,\nA.: Towards Seamless Management of AI Models in High-Performance Computing\n(2022)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8718122243881226
    },
    {
      "name": "Supercomputer",
      "score": 0.6286378502845764
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5739240050315857
    },
    {
      "name": "Software",
      "score": 0.5721226334571838
    },
    {
      "name": "Visualization",
      "score": 0.5389692187309265
    },
    {
      "name": "Software engineering",
      "score": 0.47848886251449585
    },
    {
      "name": "Programming language",
      "score": 0.3743492364883423
    },
    {
      "name": "Machine learning",
      "score": 0.3514760732650757
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33505693078041077
    },
    {
      "name": "Operating system",
      "score": 0.25528764724731445
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173911158",
      "name": "Iowa State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1282311441",
      "name": "Lawrence Livermore National Laboratory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1282105669",
      "name": "Argonne National Laboratory",
      "country": "US"
    }
  ]
}