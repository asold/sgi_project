{
  "title": "Bodyformer: Semantics-guided 3D Body Gesture Synthesis with Transformer",
  "url": "https://openalex.org/W4385275735",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3198465175",
      "name": "Pang Kunkun",
      "affiliations": [
        "Guangdong Academy of Sciences",
        "Guangdong Institute of Intelligent Manufacturing"
      ]
    },
    {
      "id": "https://openalex.org/A4377460472",
      "name": "Qin, Dafei",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4202063429",
      "name": "Fan, Yingruo",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Habekost, Julian",
      "affiliations": [
        "University of Edinburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2099798306",
      "name": "Shiratori, Takaaki",
      "affiliations": [
        "META Health"
      ]
    },
    {
      "id": "https://openalex.org/A2242378100",
      "name": "Yamagishi, Junichi",
      "affiliations": [
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A2744265533",
      "name": "Komura, Taku",
      "affiliations": [
        "University of Hong Kong",
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098994456",
    "https://openalex.org/W3115266783",
    "https://openalex.org/W3009042479",
    "https://openalex.org/W4303448003",
    "https://openalex.org/W3188493377",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2169262156",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2901285216",
    "https://openalex.org/W2981802563",
    "https://openalex.org/W2924334974",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W4321351659",
    "https://openalex.org/W2962795401",
    "https://openalex.org/W4285981714",
    "https://openalex.org/W3198131199",
    "https://openalex.org/W2945629925",
    "https://openalex.org/W2922298118",
    "https://openalex.org/W3002310794",
    "https://openalex.org/W3133090439",
    "https://openalex.org/W3009385801",
    "https://openalex.org/W4230429791",
    "https://openalex.org/W4304080460",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W3194872882",
    "https://openalex.org/W2780124704",
    "https://openalex.org/W4200526174",
    "https://openalex.org/W2007337857",
    "https://openalex.org/W3194605252",
    "https://openalex.org/W3083173864",
    "https://openalex.org/W2967443589",
    "https://openalex.org/W2801567109",
    "https://openalex.org/W2949924544",
    "https://openalex.org/W4386076103",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3107914916",
    "https://openalex.org/W4312674262",
    "https://openalex.org/W3125775899",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2727071501",
    "https://openalex.org/W4221142137",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4299390018",
    "https://openalex.org/W112381816",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3181695292",
    "https://openalex.org/W3093647117",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W1974604838",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W3102619627",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3206228555",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3123650687",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W3099073275",
    "https://openalex.org/W3153551559"
  ],
  "abstract": "Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system is trained with either the Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset. The results show that our system can produce more realistic, appropriate, and diverse body gestures compared to existing state-of-the-art approaches.",
  "full_text": "BodyFormer: Semantics-guided 3D Body Gesture Synthesis with\nTransformer\nKUNKUN PANG‚àó, Guangdong Key Laboratory of Modern Control Technology, Institute of Intelligent Manufacturing,\nGuangdong Academy of Sciences, China\nDAFEI QIN‚àó, The University of Hong Kong, Hong Kong\nYINGRUO FAN, The University of Hong Kong, Hong Kong\nJULIAN HABEKOST, The University of Edinburgh, UK\nTAKAAKI SHIRATORI,Meta Reality Labs Research, US\nJUNICHI YAMAGISHI,National Institute of Informatics, Japan\nTAKU KOMURA‚Ä†, The University of Hong Kong, Hong Kong and Research Institute of Electrical Communication, Tohoku\nUniversity, Japan\nTransformer \nEncoder\nSpeech \nPretraining\nTransformer \nDecoder\nPrevious \nMotion\nMode\nMotion \nPretraining\nCross-Modal Learning\n‚Äú‚Ä¶ and their student \ngoes flipping like ‚Ä¶‚Äù\nSpeech\nNon-Speaking\nShort-Speaking\nLong-Speaking\nMode \nPE\nMode \nPE\n‚Äú‚Ä¶ and their       students go flipping\nlike off           ---- into the             ---- wall ‚Ä¶ ‚Äù \n----           ----\nFig. 1. Given an arbitrary input speech, our proposed transformer-based model, BodyFormer, can generate a sequence of vivid 3D body gestures.\nAutomatic gesture synthesis from speech is a topic that has attracted re-\nsearchers for applications in remote communication, video games and Meta-\nverse. Learning the mapping between speech and 3D full-body gestures\nis difficult due to the stochastic nature of the problem and the lack of a\nrich cross-modal dataset that is needed for training. In this paper, we pro-\npose a novel transformer-based framework for automatic 3D body gesture\n‚àóKunkun Pang and Dafei Qin are joint first authors\n‚Ä†Corresponding author\nAuthors‚Äô addresses: Kunkun Pang, kk.pang@giim.ac.cn, Guangdong Key Laboratory\nof Modern Control Technology, Institute of Intelligent Manufacturing, Guangdong\nAcademy of Sciences, China; Dafei Qin, qindafei@connect.hku.hk, The University of\nHong Kong, Hong Kong; Yingruo Fan, yingruo@connect.hku.hk, The University of\nHong Kong, Hong Kong; Julian Habekost, julian.habekost@ed.ac.uk, The University of\nEdinburgh, UK; Takaaki Shiratori, tshiratori@meta.com, Meta Reality Labs Research,\nUS; Junichi Yamagishi, jyamagis@nii.ac.jp, National Institute of Informatics, Japan;\nTaku Komura, taku@cs.hku.hk, The University of Hong Kong, Hong Kong and Research\nInstitute of Electrical Communication, Tohoku University, Japan.\n¬© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author‚Äôs version of the work. It is posted here for your personal use. Not for\nredistribution. The definitive Version of Record was published in ACM Transactions on\nGraphics, https://doi.org/10.1145/3592456.\nsynthesis from speech. To learn the stochastic nature of the body gesture\nduring speech, we propose a variational transformer to effectively model a\nprobabilistic distribution over gestures, which can produce diverse gestures\nduring inference. Furthermore, we introduce a mode positional embedding\nlayer to capture the different motion speeds in different speaking modes.\nTo cope with the scarcity of data, we design an intra-modal pre-training\nscheme that can learn the complex mapping between the speech and the\n3D gesture from a limited amount of data. Our system is trained with either\nthe Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset.\nThe results show that our system can produce more realistic, appropriate,\nand diverse body gestures compared to existing state-of-the-art approaches.\nCCS Concepts: ‚Ä¢ Computing Methodologies‚ÜíMachine Learning; ‚Ä¢ Com-\nputer Graphics‚ÜíAnimation;\nAdditional Key Words and Phrases: motion generation, transformer, deep\nlearning\nACM Reference Format:\nKunkun Pang, Dafei Qin, Yingruo Fan, Julian Habekost, Takaaki Shiratori,\nJunichi Yamagishi, and Taku Komura. 2023. BodyFormer: Semantics-guided\n3D Body Gesture Synthesis with Transformer. ACM Trans. Graph. 42, 4,\nArticle 111 (August 2023), 12 pages. https://doi.org/10.1145/3592456\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\narXiv:2310.06851v1  [cs.CV]  7 Sep 2023\n111:2 ‚Ä¢ Pang and Qin et al.\n1 INTRODUCTION\nAutomatic synthesis of 3D body gestures from speech is a problem\nthat researchers in psychology, computer graphics and computer\nvision have been tackling. Most classic approaches are either rule-\nbased where the corresponding gesture for each context is carefully\ndesigned based on observation, or make use of low-level features\nof speech such as prosody to produce movements that are well\nsynchronized with the speech. We wish to go beyond such carefully\ndesigned architectures and learn a mapping from speech to body\ngesture automatically from a mid-size dataset.\nHowever, there are various difficulties in learning the task of body\ngesture synthesis from speech. First of all, this is a cross-modality\nlearning problem that requires a significant amount of training data\nfor producing a proper mapping. Unlike training deep language\nmodels (e.g. BERT [Devlin et al . 2019]) and vision models (e.g.,\nViT [Dosovitskiy et al. 2021]), collecting a large amount of training\ndata, which is motion capture data synchronized with speech data,\nmay not be practical. Secondly, the correlation between speech and\ngesture is rather weak. Regressing body gestures from low-level\nspeech features may easily fail due to the ambiguity of the mapping.\nThirdly, gestures are usually related to either the high-level context\nof the speech contents or the mode of the current conversation, such\nas moving the hands and fingers to express the adjective of a subject,\nnodding while agreeing and side-stepping while listening, which\nare difficult to be learned unless high-level semantic information\nand mode label is involved.\nIn this paper, we present BodyFormer, a novel transformer-based\nframework for speech-driven 3D body gesture synthesis, as de-\npicted in fig. 1. Since human body movement is a stochastic pro-\ncess, and there is significant ambiguity between speech and motion,\nwe propose a generative transformer architecture with variational\ninference to synthesize 3D body gestures. To cope with the diffi-\nculty of cross-modality learning, we use a transformer encoder-\ndecoder framework with early and intermediate modality fusion.\nMore specifically, the transformer encoder encodes low-level and\nhigh-level information, where the low-level features preserve the\naudio frequency and the high-level semantics strengthens the weak\ncorrelation between the full-body motion and the speech signal.\nSuch information is fused before being fed into the transformer\nencoder. Then, the transformer decoder with a multi-head attention\nscheme can merge both speech and the previous motion modality\nfrom the intermediate layers to produce the speech-related gesture.\nTo cope with the data-hungry nature of cross-modal transformer\ntraining [Khan et al. 2021], we devise an intra-modal pre-training\nstrategy, which alleviates the difficulty of cross-modal learning for\nspeech and body gestures. Additionally, to learn the motions of\ndifferent speeds in various speaking modes, e.g. not-speaking, short-\nspeaking, and long-speaking, we propose a mode positional embed-\nding layer to learn mode-dependent period parameters to capture\nsuch differences.\nWe conduct comprehensive experiments to evaluate our system\nusing the Trinity speech-gesture dataset [Ferstl and McDonnell\n2018] and TalkingWithHands 16.2M dataset [Lee et al. 2019a]. Vivid\ngestures that represent the semantic expression of the speaker can\nbe generated from the speech. We conduct quantitative and qual-\nitative comparisons with the baselines and show that our model\noutperforms the state-of-the-art. Our system can be applied to con-\ntrolling virtual presenters, characters in films, and AI agents in\nvirtual environments.\nThe contribution of the paper is summarized as follows:\n‚Ä¢A transformer framework for synthesizing 3D body gestures\nfrom speech, which can produce expressive movements where\nthe gestures are in sync with the speech and also contextually\nrelated,\n‚Ä¢introduction of mode positional embedding that helps the\ntransformer to capture the different motion speeds in different\nspeaking modes and\n‚Ä¢an intra-modal pre-training scheme that enables our trans-\nformer framework to learn from a relatively small motion\nand speech dataset.\n2 RELATED WORKS\nWeak Correlation between Signals. The correlation between speech\nand gesture has been a long-term interest in psychology [Cassell\net al. 1999; McNeill 1992; Wagner et al. 2014]. Kendon [1972] ana-\nlyzes the synchronization of the speech and gesture and finds that\nthe gesture appears even earlier than the speech. McNeill [1992]\ninsists that gesture and speech are occurring from a common source.\nDe Reiter et al. [2012] claim that gesture and speech are comple-\nmentary to each other to convey the speaker‚Äôs intention.\nTrade-off Between Quality and Scale. While psychologists are keen\non discovering the relationship between signals, speech-driven ges-\nture synthesis is also appealing in the computer graphics and vision\ncommunity. Not only does signals‚Äô correlation need to be well un-\nderstood, but the quality of generated poses matters. Due to the\nhigh cost of the motion capture process, collecting large-scale, high-\nquality data is impractical. On the contrary, noisy key points could\nbe extracted from videos via pose estimation techniques. This intro-\nduces a trade-off between data quality and the dataset scale, which\nleads to a different research focus. A number of studies [Ahuja et al.\n2020a,b; Ginosar et al. 2019; Qian et al. 2021] are aimed at gener-\nating 2D body gestures from speech. These 2D gesture generation\napproaches are not directly applicable to 3D scenarios, for instance,\ncontrolling a virtual character in films or video games. Recently\nresearchers [Habibie et al. 2021; Liu et al. 2022b; Yoon et al. 2020;\nZhu et al. 2023] shifted their focus to learn the estimated 3D poses\nfrom in-the-wild videos, such as the 144-hours TED [Yoon et al .\n2019], and 251-hours PATS [Ahuja et al. 2020b; Ginosar et al. 2019]\ndatasets. However, the estimated 3D poses are still noisy and lack\njoint rotation information. This results in generating ambiguous\nand less accurate poses, which are hard to feed into the graphics\npipeline. Other researches [Alexanderson et al. 2020; Ao et al. 2022;\nKucherenko et al. 2020] focus on learning more accurate body ges-\ntures from relatively smaller motion capture datasets synchronized\nwith speech, such as 4-hours Trinity [Ferstl and McDonnell 2018],\nand 20-hours TalkingWithHands 16.2M [Lee et al. 2019a] dataset. It\nis noted that those motion capture datasets are 10-30 times smaller\nthan the in-the-wild video datasets. Hence, it is challenging to train\na data-hungry model on these relatively smaller datasets.\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\nBodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer ‚Ä¢ 111:3\nDeep Motion Generation Methods. The recent deep learning meth-\nods typically utilize convolution [Ahuja et al. 2020b; Ginosar et al.\n2019], feed-forward [Kucherenko et al. 2019, 2020] and Recurrent\nNeural Networks (RNN) [Shlizerman et al. 2018; Yoon et al. 2019] to\nlearn the mapping from speech to gestures. For instance, Ginosar\net al. [2019] propose a CNN-based framework to predict gestures\nfrom audio. Ahuja et al. [2020b] construct a Temporal Convolution\nNetwork (TCN) to formulate a stylized gesture space. Kucherenko\net al. [2019] explore different types of audio features. Their follow-\nup work [Kucherenko et al. 2020] takes both acoustic and textual\nfeatures as the input of a feed-forward neural network for gesture\ngeneration. The probabilistic method [Li et al . 2021a] employs a\nconditional VAE to generate diverse body gestures. On the other\nhand, RNN-based models are also wildly used in speech-driven\ngesture generation [Ahuja et al. 2020b; Bhattacharya et al. 2021a;\nFerstl et al. 2019; Liu et al. 2022b; Shlizerman et al. 2018; Yoon et al.\n2019]. Shlizerman et al. [2018] train an LSTM model to learn the\ncorrelation between audio features and body key points. Yoon et\nal. [2019] build a GRU-based model for gesture generation, where\nthe model is trained on the TED dataset. Wang et al. [2021a] pro-\npose to improve the motion quality by jointly synthesizing speech\nand gestures from the text in an integrated LSTM architecture. Liu\net al. [2022c] propose a cascaded LSTM and MLP by integrating\nemotion, speaker identity, and style features for motion synthe-\nsis. Liu et al. [2022a] further improve the LSTM generator with a\ncontent-balanced distribution to tackle the imbalance learning prob-\nlem caused by fewer frequent motion sequences. Ao et al. [2022]\nimprove the audio-gesture synchronization by cutting inputs into\nshort clips according to beats. They incorporate gesture lexeme\nencoder modules and disentangle audio features to build a better\nmapping between speech and gesture. Besides, Habibie et al.[2022]\nsearch for the most plausible audio-motion sequence from the data-\nbase with the K-nearest neighbors and refine the motion with the\nconvolution network. However, these previous deep models can\nonly capture limited contextual information, e.g., RNN-based mod-\nels inevitably ‚Äòforget‚Äô the past information [Dai et al. 2019; Gers et al.\n1999]. Although several methods [Kucherenko et al. 2020; Yoon et al.\n2020] extract BERT [Devlin et al. 2019] features from the text, they\ndo not consider the long-term audio context.\nTransformer based models. Compared to CNN and RNN based\nmodels, the transformer model [Vaswani et al. 2017] is relatively less\nexplored in the audio-driven motion synthesis. Saeed et al. [2023]\npresent a variational transformer for encoding style information,\nwhereas they adopt recurrent networks to model motion genera-\ntion from both speech and style. Valle-P√©rez et al. [2021] and Li et\nal. [2021b] propose generative transformer approaches with nor-\nmalizing flow for dancing motion synthesis from music. However,\nusing the low-level audio feature is less likely to produce contextual\nmovement in our task. Unlike Transflower [Li et al . 2021b; Valle-\nP√©rez et al. 2021], Text2gestures [Bhattacharya et al. 2021b] use text\nand a deterministic transformer to generate body gestures without\nthe low-level audio features. Audio features complement text infor-\nmation, such as pitch, tempo, and loudness, which may influence\nbody gestures. Therefore, we exploit both low-level audio features\nand the text‚Äôs contextual semantic features.\nIn this paper, we present a novel system for synthesizing motion\nfrom speech, which incorporates state-of-the-art features from nat-\nural language processing, speech processing, and human motion\nsynthesis. In contrast to existing methods, which overlook the dis-\ntinct body language patterns during different speaking modes, our\nproposed system for synthesizing motion from speech incorporates\na mode positional embedding method to capture such variations,\nwhich enhances the vividness of motion. Besides, the proposed intra-\nmodal pre-training helps prevent the model from overfitting to the\ntraining data.\n3 BODYFORMER\nGenerating the body gesture from the speech is an inherently am-\nbiguous problem where different realistic gestures could match to\nthe same speech. To cope with this issue, we propose a generative\nmodel where the system samples noise per sequence to produce\nvariations of output movements. In this section, we first introduce\nthe data pre-processing step, and then, the proposed architecture\nand its training scheme.\n3.1 Pre-processing\nRaw speech data contain both low-level and high-level features.\nThe low-level features have syllables and phonetic information, and\nthe high-level features provide semantic contextualized informa-\ntion. We use the librosa [McFee et al . 2015] library to transform\nthe audio signal to mel-spectrogram with 27 channels to extract\nsuch low-level features [Alexanderson et al. 2020]. For computing\nhigh-level features based on the context, we first apply Automatic\nSpeech Recognition (ASR) to extract sentences from speech, and\nthen compute the BERT feature [Devlin et al. 2019] of each word.\nSince the size of the BERT feature is much larger than that of the\nmel-spectrogram feature, we reduce the dimension of the BERT\nfeatures to 32 by applying PCA. The mel-spectrogram and BERT\nfeatures are concatenated to form the speech feature at frame ùë°,\nrepresented by ùíôùë° ‚ààR59.\nWe construct an equal time-length representation of the audio\nand words whose temporal resolution is the same as that of the\nmotion (20Hz). For the audio feature, if the sound sampling rate is\n48KHz in the original data, we extract 2400 features between two\nsample points (48K/20 = 2400) and concatenate them to be the audio\nfeature of that frame. The BERT feature of the corresponding word\nis set as the high-level feature of that frame. For the frames where\nthe sound level is low, the word feature is set to zero vectors. The\nspeaking mode label, i.e., not-speaking (NS), short-speaking (SS),\nand long-speaking (LS), of the speaker is computed automatically\nby measuring the time length of the sentence and added as a feature\n(see appendix A for more details).\nTo representation motion, we use the 6D representation for joint\nrotation [Zhang et al. 2018; Zhou et al. 2019]. The pose representa-\ntion is the concatenation of all joint rotationsùíöùë° = [ùíõùë°,1,¬∑¬∑¬∑ ,ùíõùë°,ùêΩ ]‚àà\nR6ùêΩ where ùíõ is the joint rotation and ùêΩ is the number of joints. In\nthis paper, the system will output the upper body and finger pose\nfor the Trinity speech-gesture dataset [Ferstl and McDonnell 2018]\nand the full body pose for the Talking with hands 16.2M dataset\n[Lee et al. 2019a].\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\n111:4 ‚Ä¢ Pang and Qin et al.\nGlobal Positional Embedding\nSpeech Projection Net\n‚Ä¶ Motion Projection Net\nMotion Projection Net\n‚Ä¶\nGenerated \nMotions\n‚Ä¶\n‚Ä¶ Previous \nMotions\n5 x Transformer Encoder Layer\nMode Positional Embedding\nSpeech Projection Net\n‚Ä¶\n5 x Transformer Decoder Layer\nGlobal Positional Embedding\nMode Positional Embedding Seq Emb \nTransformer\nSampling\nReconstructed \nSpeech Features \nNo-Speaking \nShort-Speaking \nLong-Speaking \nThree modes\nAudio Text\nSpeech Features\nHigh-level features\nLow-level features\n: Training \n: Training and inference \nCross-Modal Fusion\n‚Ä¶‡∑úùë¶1 ‡∑úùë¶2 ‡∑úùë¶3 ‡∑úùë¶ùëá+1\nùë¶0 ùë¶1 ùë¶T\n‚Ä¶‡∑úùë•0 ‡∑úùë•1 ‡∑úùë•2 ‡∑úùë•ùëá\n‚Ä¶ùë•0 ùë•1 ùë•2 ùë•ùëá\nFig. 2. The architecture of the proposed model, BodyFormer. Our model\ntakes both low-level and high-level speech features as input and generates\na sequence of realistic 3D body gestures in an auto-regressive manner. The\nembedding layer consists of global positional embedding (GPE) and mode\npositional embedding (MPE). GPE captures the global order information\nfor each sequence, whereas MPE depicts the local time information for\nevery mode (NS, SS and LS) by learning the mode-dependent period param-\neters. Besides, a sequence embedding transformer is applied to learn the\nmotion distribution conditioned on the previous motions. During testing,\nthe encoded previous motions are summed with the noise sampled from\nthe learned distribution for variational inference.\n3.2 Architecture\nAs shown in the fig. 2, our proposed system is composed of five sub-\nnetworks, including element-wise projection nets, embedding layer,\ntransformer encoder-decoder, sequence embedding transformer, and\nsampling layer.\nProjection net. The element-wise projection net aims to map the\ndata to the desired dimensions. More specifically, each projection net\nconsists of 2 fully connected layers with GELU activation functions.\nAs shown in the fig. 2, both speech and motion features will be\nsent to their corresponding projection net and transformed to 512\ndimensions. Similarly, the transformers‚Äô outputs are projected to\ntheir desired output size with a linear layer: We call the linear layer\nthe projection layer for consistency.\nTransformer. Similar to the vanilla transformer [Vaswani et al .\n2017], we use the same encoder-decoder framework to model the\nspeech-driven motion generation task. The encoder takes speech\nùíô = (ùíô1,ùíô2,..., ùíôùëá)as input and encodes it into contextualized\nrepresentation ùíâ1‚Üíùëá = ùëì(ùíô‚Ä≤\n1‚Üíùëá)without masks, whereùíô‚Ä≤is speech\nfeature after the speech projection net and embedding layer. The\ndecoder will receive both previous motionsùíö<ùë° = (ùíö1,ùíö2,..., ùíöùë°‚àí1),\nthe encoded speech information, and a lower-triangulated mask to\nproduce the current pose:\nùíöùë° = ùëî(ùíö<ùë°,ùëì (ùíô‚Ä≤)), (1)\nwhere ùëì and ùëîdenote the encoder and decoder, respectively. The\ntriangulated mask ensures the decoder only uses the past motion\nwithout cheating on using the future pose information. Unlike the\nbidirectional model [Li et al . 2021b], our decoder is trained with\nshifted-by-1 supervision such that the decoder can learn the pose\nprediction in an auto-regressive manner.\nEmbedding layer. The embedding layer is composed of Global\nPositional Embedding and Mode Positional Embedding. As stated\nin [Vaswani et al. 2017], the transformer lacks recurrence and con-\nvolution to make use of the order of sequence. They propose Global\nPositional Encoding (GPE) , which provides time and order informa-\ntion to enable the model to process the entire sequential data and\npreserve continuity. Here, we follow [Wang et al. 2021b] and use a\nlearnable sinusoidal function:\nùê∫ùëÉùê∏(ùë°)= [sin(ùúîùë°),cos(ùúîùë°)],ùë° = {0,1,...,ùëá }. (2)\nwhere ùúî is the learnable period parameter.\nApart from this, we hypothesize that the motion speed tends to be\ndifferent in different modes. The global positional embedding layer\nmay potentially average the movement speed for all modes since\nmodes could randomly occur in a long sequence. Besides, the period\nparameter of GPE is identical in either speaking or not speaking.\nThis may lead the generated motion to be less active during speaking,\nand not salient during not-speaking. Thus, we present an additional\nMode Position Embedding (MPE) to learn mode-dependent period\nparameters ùúîùëö,\nùëÄùëÉùê∏(ùëö,ùë°‚Ä≤)= [sin(ùúîùëöùë°‚Ä≤),cos(ùúîùëöùë°‚Ä≤)],\nùë°‚Ä≤= {0,1,...,ùëá ‚Ä≤}, (3)\nwhere ùëöis a speaking mode label (NS, SS or LS) andùëá‚Ä≤is the length\nof a detected speaking mode. One noticeable difference between GPE\nand MPE is the time horizon. GPE describes global order information\nfor every sequence with a fixed lengthùëá, whereas MPE has dynamic\ntime horizons ùëá‚Ä≤based on the length of the corresponding mode.\nEach time step ùë°‚Ä≤of MPE depicts the local time information for\nevery speaking mode individually.\nFinally, we combine both local and global embedding to com-\npose the embedding layer ùê∏ùëöùëèùëíùëë(ùë•,ùë°,ùë° ‚Ä≤,ùëö)= LayerNorm(ùëê‚àó(ùë•+\nùê∫ùëÉùê∏(ùë°))+ùëÄùëÉùê∏(ùëö,ùë°‚Ä≤)), whereùëê =\n‚àöÔ∏Å\n512/3 is a constant to strengthen\nthe information from the raw inputs and the global time.\nVariational Inference. The system is also expected to generate\nvarious motion sequences based on the states of previous motion\nand corresponding speech. Here, we propose to model the posterior\ndistribution with a sequence embedding transformer that encodes\nthe sequence into a single vector. The prior distribution will be\nmodeled as a learnable multivariate distribution ùúº ‚àºN( ùùÅ,ùùà). In\ntraining time, we train both sequence embedding transformer and\nthe multivariate normal distribution simultaneously. In test time,\nthe sequence embedding transformer will be disabled, and we will\nrandomly sample from the learned multivariate normal distribution.\nSpecifically, our sequence embedding transformer uses the same\narchitecture as the set transformer [Lee et al . 2019b]. We use a\nshallower set transformer ùúô(¬∑)with only two multi-head attention\nblocks (MAB) and a pooling layer with multi-head attention (PMA).\nThe multi-head attention block is identical to the vanilla transformer\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\nBodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer ‚Ä¢ 111:5\nencoder and decoder:\nùëÄùê¥ùêµ(ùëÑ,ùêæ,ùëâ )= LayerNorm(ùêª +FF(ùêª)), (4)\nùêª = LayerNorm(ùëÑ+Multihead(ùëÑ,ùêæ,ùëâ )), (5)\nwhere FF, LayerNorm, and Multihead represent the fully connected\nlayer, layer normalization, and multi-head attention, respectively. Q,\nK and V indicate queries, keys and values, which are the inputs of\nmulti-head attention. In our case, they all come from the previous\nmode position embedding layer. Instead of applying an element-\nwise average or maximum of the instances, we pool the data by\nmulti-head attention. To apply multi-head attention as a pooling\nfunction, PMA has a learnable seed vector ùëÜ ‚ààRùëò√óùëë. Let us denote\nùêªùë†ùëíùë° ‚ààRùëõ√óùëë as the set of features:\nPMAùëò(ùêªùë†ùëíùë°)= ùëÄùê¥ùêµ(ùëÜ,ùêπùêπ (ùêªùë†ùëíùë°)). (6)\nThen, the output of the PMA will be a set of ùëò items. Note that we\nuse one seed vector (ùëò = 1) for this task.\n3.3 Objective Functions\nTo train this system, we introduce three loss functions for learn-\ning the relation between speech and motion: Joint Prediction Loss ,\nMagnitude Loss , and KL Divergence.\nJoint Prediction Loss: Firstly, we introduce an MSE loss to learn\nto predict the corresponding pose for every time step. This loss\nis mainly to learn the correlation between speech and the joints‚Äô\nspatial information:\nùêøùëî = 1\nùëá\nùëá‚àëÔ∏Å\nùë°\n(ÀÜùíöùë° ‚àíùíö‚àó\nùë°)2, (7)\nwhere ÀÜùíöùë°, ùíö‚àó\nùë° are the predicted and ground-truth poses, respectively.\nMagnitude Loss: We further introduce a magnitude loss to learn\ntemporally coherent motion. This loss is used to encourage the\ngenerated motion changes to be similar to the ground truth. This\nloss can smooth the generated motion and make it look more vivid:\nùêøùëö = 1\nùëáùêΩ\n‚àëÔ∏Å\nùëá\n‚àëÔ∏Å\nùêΩ\n\u0010\n||ÀÜùíõùë°,ùëó ‚àíÀÜùíõùë°‚àí1,ùëó||ùêπ ‚àí||ùíõ‚àó\nùë°,ùëó ‚àíùíõ‚àó\nùë°‚àí1,ùëó||ùêπ\n\u00112\n, (8)\nwhere ÀÜùíõùë°,ùëó, ùíõ‚àó\nùë°,ùëó are the components of ÀÜùíöùë°, ùíö‚àó\nùë°, respectively.\nKL Divergence. Additionally, we minimize the KL divergence be-\ntween the vectorized sequential distribution and a learnable multi-\nvariate normal distribution:\nùêøùêæùêø = ùêæùêøùê∑(ùëû(ùúÇ|ùúô(ùíö))|ùëù(ùúÇ)) (9)\nwhere ùëû(ùúÇ|ùúô(ùíö))is the posterior distribution from the output of the\nsequence embedding transformer and ùëù(ùúÇ)= N(ùùÅ,ùùà)is the learn-\nable multivariate normal distribution with a diagonal covariance\nmatrix. This loss is introduced to encourage the model to be less\ndeterministic, as well as to serve as regularization that prevents\noverfitting.\nFinally, we combine all three terms together:\n‚Ñì = ùúÜ1ùêøùëî +ùúÜ2ùêøùëö +ùúÜ3ùêøùêæùêø (10)\nwhere we set ùúÜ1 = 1,ùúÜ2 = 0.3 to weight the two different loss\nfunctions, and ùúÜ3 is an additional parameter to prevent the posterior\ncollapse, which is controlled by the Cyclical Annealing Schedule\n[Fu et al. 2019]. In our case, our ùúÜ3 is uniformly increased from 0.2\nto 3 in 10 epochs and then reset to 0.2 again.\n3.4 Learning Strategy\nThe nature of the body gesture synthesis from the speech is a cross-\nmodal problem; tackling such a problem is one of the most critical\nparts of producing high-quality human gestures. Due to the differ-\nence between the distribution of speech and motion, we simplify\nthe problem by pre-training the speech transformer encoder and\nmotion transformer decoder individually so that the cross-modal\nlearning will start from a reasonable intra-modal manifold.\nIntra-modal pre-training. We pre-train the encoder and the de-\ncoder by optimizing Masked Speech Modelling (MSM) and Masked\nMotion Modelling (MMM). Both MSM and MMM are similar to\nMasked Language Modelling (MLM), which estimates the missing\nspeech and pose based on the available frames, respectively. Such\nunified intra-modal pre-training tasks benefit the encoder and de-\ncoder as the pre-trained embedding with MSM and MMM can be\nhelpful for successive tasks in either bidirectional or autoregressive.\nUnlike pre-training BERT [Devlin et al. 2019] and ViT [Dosovit-\nskiy et al. 2021], the size of the 3D motion capture and speech dataset\nis much smaller than the datasets for pre-training in the natural\nlanguage processing [Zhu et al . 2015] and computer vision com-\nmunity [Deng et al. 2009]. This limited size of motion and speech\ntraining samples makes the pre-training even more challenging if\nwe use the same hyperparameter from BERT. Thus, we choose an\neasier hyper-parameter without mask token during the pre-training\nphase. Similar to BERT [Devlin et al. 2019], we modify 20% of the\nspeech data or motion data, where 10% of the modified data are\nreplaced as noise drawn from a normal distribution, and the rest\nwill be set as zero.\nThe transformer encoder and decoder are trained separately in\nthis phase. Unlike the speech transformer encoder, the motion trans-\nformer decoder has both the multi-head self-attention and the multi-\nhead attention block. During pre-training, we disable the multi-head\nattention block of the transformer decoder, and the data flow by-\npasses the next corresponding block. Note that the data flow does\nnot go through the sequence embedding transformer and the learn-\nable prior distribution.\nCross-modal learning. After pre-training both the speech encoder\nand motion decoder, we perform cross-modal learning. Specifically,\nmulti-head attention learns to address the cross-modal learning\nof speech and motion. We follow the same routine as the vanilla\ntransformer decoder framework [Vaswani et al. 2017] that uses the\nmulti-head self-attention module to encode the past motion and the\nmulti-head attention module to get the information from speech fea-\ntures. The transformer decoder receives the encoded speech features\nfrom the transformer encoder and the embedded pose features from\nmotion embedding to compute the next pose in an auto-regressive\nfashion where their correlation are learned. All system parameters\nare updated in this phase by minimizing the objective functions. In\naddition, the sequence embedding transformer and the multivariate\nnormal distribution are also learned in an end-to-end manner.\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\n111:6 ‚Ä¢ Pang and Qin et al.\n3.5 Implementation Details\nTraining a deep transformer is often challenging due to the strong\ncapability of the architecture, which leads the neural network to fall\ninto poor local minima, i.e. overfit to the training data and failure\nto generalize to real-world data. We use several tricks to train such\na complex model successfully.\nWarmup Scheduler. Similar to the tricks for training transformer\n[Devlin et al. 2019; Vaswani et al . 2017], we follow the same co-\nsine warmup learning rate scheduler. This warmup learning rate\nscheduler has the benefit of stabilizing the training process from a\nrelatively better initialization than random by tuning the attention\nmodule at the early learning stage. For all experiments, we set the\nwarmup step to 400 epochs.\nSpec Augmentation. To train our model with a relatively small\nbody gesture-speech dataset, we use spec augmentation [Park et al.\n2019]. This increases the size of the training data and improves the\ngeneralization performance. For simplicity, we randomly zero out\nconsecutive frames of audio or words, respectively, with the range\nof 0% ‚àí20%.\nDropout Annealing. Training the transformer or other time-series\nmodels in an auto-regressive manner may easily fail in the test\ntime. Since the pose between the current and the next step is often\nsimilar, the link between speech and pose is unapparent. This leads\nto the deep transformer or other auto-regressive model ignoring the\nspeech information‚Äôs importance and overfits to the previous poses.\nIn [Alexanderson et al . 2020], they apply a frame-level dropout\non previous motions to strengthen the connection from speech.\nHowever, using the same frame-level dropout to the input motion\nmay occasionally fail to learn the reasonable attention heatmap. The\ntransformer may overfit and ignore the speech around the current\nframe. In our case, we apply a dropout annealing on the motion\ninput where the dropout probability will be annealed from 100% to\n60%. This will force the multi-head attention to capture as much\ninformation as possible during the warmup phase.\n4 EXPERIMENTS AND RESULTS\nWe compare our model with various baseline models by qualitative\ncase studies and quantitative metrics. Since human gestures are\nsubtle to evaluate, we conduct extensive user studies to demonstrate\nthe quality of our generated gestures.\n4.1 Experiment Design\nWe train and evaluate our proposed system on two datasets: Trinity\nSpeech-Gesture Dataset [Ferstl and McDonnell 2018] and Talking\nWith Hands 16.2M (TWH) dataset [Lee et al . 2019a]. The Trinity\ndataset is often used for the co-speech body gesture synthesis task.\nWe use the training and test splits provided by the 2020 GENEA\nbody gesture challenge [Kucherenko et al . 2021]. Regarding the\nTWH dataset, we use the deep capture set that is composed of full-\nbody motion during the conversations of two subjects. As only one\nperson‚Äôs motion is available in most conversations, and the finger\nmotion may occasionally fail to capture, we use the mocap data of\nthe single person with reasonable finger motion for training.\nWe evaluate the performance with the Trinity and TWH datasets.\nOn the Trinity dataset, we compare our model (BodyFormer) with\nStyleGestures (StyleGest) [Alexanderson et al. 2020; Henter et al.\n2020], Trimodal [Yoon et al. 2020], Gesticulator (Gest) [Kucherenko\net al. 2020] and Aud2Repr2Pose (A2R2P) [Kucherenko et al. 2019].\nWe also compare with a baseline where the mocap data is paired with\na random audio sequence. We call this baseline as Unmatched com-\npetitor (Unmatch). On the TWH dataset, we compare BodyFormer\nwith Trimodal, Gesticulator, and Unmatch.\nOur model is trained on PyTorch [Paszke et al. 2019] using AdamW\noptimizer [Loshchilov and Hutter 2019] with hyper-parameters\n(ùõΩ1,ùõΩ2) = (0.9,0.999). The learning rate is set to 10‚àí4 for pre-\ntraining and 10‚àí5 for cross-modal learning. The weight decay is set\nto 10‚àí2 for the pre-training and cross-modal learning phase. Both\nphases are run on 4 √óNvidia 2080Ti GPUs, and trained with (50,\n1500) epochs with a batch size of 32, respectively. The training takes\n7 days in total.\n4.2 Qualitative Evaluation\nOur model can produce smooth and realistic gestures that are in\nsync with the speech. We highlight some of the interesting motion\nthat are produced by our system. The model trained with the Trinity\ndataset produces rich, active gestures that contextually match the\nspeech. When the speaker is describing his experience, he mentions\n\"It was SO interesting... \", and spreads his arms in sync with the\nword \"so\", to express the amount of interest that the speaker felt\n(see fig. 3, left). In another example, the speaker mentions ‚Äúkind of\ngeared towards LIKE... ‚Äù. When he speaks the word ‚Äúlike‚Äù, the hands\nare retained closer to the chest so as to reflect possession of a notion\nfor which an immediate vocabulary is not available, which often\nappears in real-life (see fig. 3, right).\nIn fig. 4, we visually compare the quality of gestures generated\nfrom BodyFormer, Trimodal and the ground truth. BodyFormer\noutputs either similar gestures to the ground truth (fig. 4, left, right),\nor different but semantically correlated gestures (fig. 4, middle).\nHowever, the gestures from Trimodal are static, fail to emphasize\nthe speech. In fig. 5 we evaluate the semantics of the gestures from\nBodyFormer, StyleGestures and the ground truth. While the gestures\nof our model are vivid (fig. 5, middle) and meaningful (fig. 5, left,\nright), the gestures of StyleGestures clearly fall into a mean pose\nwith little variation.\nThe model trained with the TWH dataset produces a nodding mo-\ntion in sync with an agreement, ‚Äúyeah\" during the conversation (see\nthe supplementary video). Overall, all the baselines produce repeti-\ntive motions without much contextual relationship with the speech,\nwith limited gesture variations. On the other hand, BodyFormer can\ngenerate gestures that are closely correlated to the semantics of the\nspeech and produces a wild range of different motions.\n4.3 User Study\nWe compare our method with the state-of-the-art methods through\nthe user study. We adopt the same criteria by [Alexanderson et al.\n2020] for the subjective evaluation: TheHuman-likeness is a measure\nof ‚Äúhow human-like the gesture appears‚Äù without considering the\ncorrelation between the speech and gesture. The appropriateness\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\nBodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer ‚Ä¢ 111:7\n‚Ä¶ so interesting ‚Ä¶ ‚Ä¶ geared towards like ‚Ä¶ (breathing) ‚Ä¶ \nFig. 3. Gesture synchronization. Left: The character spreads the arms to express the amount of interest it has felt. Right: The character takes a breath and\nholds the arms to the chest so as to reflect possession of a notion for which an immediate vocabulary is not available.\n‚Ä¶ different level was ‚Ä¶ ‚Ä¶ like different kingdom ‚Ä¶ ‚Ä¶ run by different lords ‚Ä¶\nGT\nOurs\nTrimodal\nFig. 4. Left: To emphasize the word level, our character raises his right hand while the ground truth opens both hands. Middle: Our character also has an\naction, though distinct from the ground truth, to express the meaning of different. Right: The character raises his hands in both our model and the ground\ntruth. The discrepancy is that the ground truth is exaggerated. However, in Trimodal, the character always puts his hands in front of the chest.\nTable 1. The mean and standard deviation about the sum of the norm of velocity.\nTrinity Talking with Hands\nSpeaking mode NS SS LS NS SS LS\nGround Truth 38.97 ¬±19.66 40.37 ¬±22.59 46.83 ¬±24.63 38.19 ¬±16.20 44.90 ¬±22.50 53.32 ¬±28.35\nOur Proposed Embedding Layer 32.97 ¬±11.09 42.53 ¬±13.60 40.69 ¬±11.86 27.32 ¬±07.52 30.26 ¬±07.62 40.09 ¬±13.14\nGlobal Positional Embedding 26.33 ¬±14.20 23.80 ¬±10.99 23.75 ¬±09.77 19.34 ¬±06.68 19.31 ¬±04.58 28.91 ¬±10.93\nw/o Motion Decoder Pre-training 23.42 ¬±11.38 23.27 ¬±07.63 28.41 ¬±10.28 26.26 ¬±07.09 29.70 ¬±06.21 38.13 ¬±12.32\nmeasures ‚Äúhow appropriate are the gestures for the speech?‚Äù. The\nparticipants are expected to evaluate the tightness between motion\nand speech rather than the motion quality [Kucherenko et al. 2021].\nFor the user study, we do an A-B testing where the user labels\nthe winner for every pairwise video, which is a combination of\nvideos of a character controlled by our method and that by the other\ncompetitors. We render the videos of the character with Blender.\nFor the methods whose outputs are joint positions or directional\nvectors, we do the inverse kinematics to recover the joint rotations.\nThe subjective evaluation results are collected via the Amazon\nMturk platform1, where the workers were hired with at least a histor-\nical accept rate of 98%. More specifically, for the Trinity experiment,\nwe obtained 40 non-overlapping 15-second audio clips, and for each\nclip, we created 11 pairwise videos: ten for our model versus the\nbaseline and one attention check for ground truth versus a still pose.\nFailing the attention check would redo the whole evaluation until\npass. The order and pairs of the videos were randomized, and each\n1https://www.mturk.com/worker\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\n111:8 ‚Ä¢ Pang and Qin et al.\n‚Ä¶ they start off ‚Ä¶ ‚Ä¶ able to do raaaah ‚Ä¶ ‚Ä¶ what I find with dungeons ‚Ä¶\nGT\nOurs\nGest\nFig. 5. Left: GT and Ours emphasize start and off, respectively. Middle: To accompany with the onomatopoeia raaaah, our model produces a specific gesture.\nRight: GT picks a unique gesture to exemplify the word dungeons. Ours and Gest both move one hand to emphasize.\nGT StyleGest A2R2P Gest Trimodal\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85%  winning over\n \n***\n***\n***\n***\nHuman-Likeness\nGT Unmatch StyleGestA2R2P Gest Trimodal\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85% winning over\n** **\n***\n***\n***\n***\nAppropriateness\n(a) (b)\nFig. 6. The pairwise winning proportion of the competitors for (a) Human-\nlikeness and (b) Appropriateness on the Trinity dataset. Asterisks indicate\nsignificant effects (**:p<0.05, ***:p<0.001).\nGT Gest Trimodal\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85% winning over\n \n***\n***\nHuman-Likeness\nGT Unmatch Gest Trimodal\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85% winning over\n \n**\n***\n***\nAppropriateness\n(a) (b)\nFig. 7. The pairwise winning proportion of the competitors for (a) Human-\nlikeness and (b) Appropriateness, on the TWH Dataset. Asterisks indicate\nsignificant effects (**:p<0.05, ***:p<0.001).\nclip was evaluated by five workers, resulting in 2200 comparisons\nand 152 unique participants. The same procedure was followed for\nthe TWH experiment, where we acquired 80 non-repeated audio\nclips with six pairwise videos each, resulting in 2400 comparisons\nand 302 unique participants.\nWe show the winning proportion in fig. 6. From the results on\nTrinity Speech-Gesture Dataset, we can see that users prefer our\nmethod over other competitors (StyleGestures, Aud2Repr2Pos, Ges-\nticulator and Trimodal) in terms of Appropriateness and Human-\nlikeness. Among the four competitors, StyleGesture performs the\nbest. Turkers perceive the gestures generated by our method more\nhuman-like and more appropriate than StyleGesture (Human-likenss:\n70.5%, Appropriateness: 69.0%). We notice that Aud2Repr2Pos, Ges-\nticulator and Trimodal generate relatively static gestures.\nWe infer that those CNN and RNN-based gesture generators are\nrestricted to training on short sequence generation, which may\npotentially lose long-term information due to the convolutional\nfilter‚Äôs limited range or the recurrent module‚Äôs inevitable forget-\nting. The transformer is suitable for automatically discovering both\nshort and long dependencies, allowing it to produce contextually\nmeaningful long motion sequences (in our case, 15 seconds). As\nshown in fig. 10, the transformer can capture dynamic and longer\ndependencies across sequences which helps improve the motion\nquality. While StyleGestures performs slightly better, the range of\nmotion is relatively small, and some of the body motions are not\nsemantically correlated. One possible reason is that StyleGestures\ndoes not consider different motion speeds in different speaking\nmodes, whereas our method can capture such variations. Interest-\ningly, when comparing our method to the ground truth, our method\nranks slightly better than the ground truth on the appropriateness\n(Human-likeness: 48.5%, Appropriateness: 59.5%). In addition, ac-\ncording to the fig. 6 and fig. 7, the paired t-tests further reveal that\nour model performed significantly better than all baselines (p<0.001)\nexcept the ground truth, both for the Trinity and TWH datasets.\nOurs was also significantly better than ground truth (p<0.05) for\nappropriateness on the Trinity dataset.\nApart from the qualitative evaluation, we adopt two commonly\nused quantitative evaluation metrics of motion synthesis tasks to\ncompare all methods: Mean Absolute Joint Error (MAJE) and Frechet\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\nBodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer ‚Ä¢ 111:9\nTable 2. Trinity Quantitative Metrics\nOurs w/o D.P Tri StyleGest Gest A2R2P\nMAJE 70.05 71.92 126.71 107.95 86.04 125.50\nFGD 9.66 10.83 254.90 20.81 51.53 346.50\nTable 3. Talking with Hands Quantitative Metrics\nOurs w/o D.P Tri Gest\nMAJE 81.66 82.79 187.47 157.28\nFGD 12.12 16.75 47.02 55.26\nw/o M.\nw/o P.\nOurs\n‚Ä¶ for all three of the players ‚Ä¶\nFig. 8. The character delivers the word three perfectly in our model, while\nthe gesture does not match the onset of three without pre-training and his\nhands remain static without MPE\nGesture Distance (FGD) [Ao et al. 2022; Yoon et al. 2020]. The MAJE\nhelps to measure the closeness of joint positions between the natu-\nral human and the generated motion, and the FGD quantifies the\ndivergence of distributions under the Gaussian assumption. Tables 2\nand 3 summarize performance of all the methods on the Trinity and\nTWH datasets, respectively. Not only does the proposed approach\nconsistently outperform the competitors on the MAJE, but it also\nshows a more plausible generated motion in terms of FGD.\n4.4 Ablation Study\nIntra-modal Pre-training. Since the motion datasets are of rela-\ntively small size to train a transformer, appropriate pre-training\nbecomes crucial. To validate our proposed Intra-modal Pre-training ,\nwe test a version of our model without using this strategy, denoted\nas ‚Äúw/o P‚Äù. Figs. 8 and 9 visualize the generated gestures of our\nmodel (1st row) and ‚Äúw/o P‚Äù (2nd row). Note that in fig. 8, while\nthe gesture of ‚Äúw/o P‚Äù fails to match the onset of the word ‚Äúthree‚Äù,\nour model can flawlessly deliver the sign language for ‚Äúthree. ‚Äù This\nsuggests that our model benefits from the intra-modal pre-training.\nThis is also supported by the attention map (fig. 10) showing that our\nmodel can automatically learn the correspondence between the con-\ntextualized speech representations and the motions. The user study\nresults (fig. 11) also suggest the effectiveness of the intra-modal\npre-training (Ours vs. ‚Äúw/o P‚Äù - Human-likeness: 64.5%, Appropri-\nateness: 64.5%). The readers are referred to the supplementary video\nfor the animation comparison.\nTo further analyze the effectiveness of the intra-modal pre-training,\nwe provide an ablation study to validate the necessity of motion\ndecoder pre-training. Though tables 2 and 3 show that with and\nwithout motion decoder pre-training ‚Äúw/o D.P‚Äù has similar MAJE\nand FGD, table 1 shows a significant difference in cumulative ve-\nlocity. With the motion decoder pre-training, BodyFormer has a\nsimilar distribution to the ground truth on the three speaking modes\nin the Trinity dataset. The norm of velocity decreases significantly\nwithout the motion decoder pre-training, which indicates a loss of\ngesture quality. We argue that the motion decoder pre-training can\nlearn non-verbal motion to help generate active and fast-changing\ngestures.\nMode Position Embedding. We present the ablation study on the\nmode position embedding. The result of our model with the mode\nposition embedding layer removed is shown in fig. 9, ‚Äúw/o M‚Äù (row\n3). The user study results (fig. 11) show that removing the position\nembedding leads to less human-like and appropriate body gestures\n(Ours vs. ‚Äúw/o M‚Äù - Human-likeness: 62.5%, Appropriateness: 66.5%).\nIn addition, we also compare the motion variations between the\nproposed embedding and the global positional embedding. We mea-\nsure the cumulative sum of L2 norm of velocity for every mode.\nAs shown in the table 1, our proposed embedding has a higher cu-\nmulative sum of the L2 norm of velocity than the global positional\nembedding and is comparable with the ground truth on the Trinity\ndataset. Though the statistics of our proposed embedding still have\na gap between the ground truth on the TWH dataset, our motion\nspeed is still faster than the global positional embedding. This shows\nthat the global positional embedding potentially over-smooths gen-\nerated motion by averaging the motions across all modes.\nUsefulness of Audio and Words. We provide an ablation study\nof the usefulness of audio and words. According to fig. 11, our\nmethod scores better than ‚Äúw/o Word‚Äù and ‚Äúw/o Audio‚Äù on both\nHuman-likeness and Appropriateness aspects. Intuitively, the mel-\nspectrogram feature is expected to capture the synchronization\nbetween audio and motion, resulting in more natural-looking gen-\nerated motion. However, the BERT feature alone may not capture\nthis information. As a result, combining both the audio and word\nfeatures in our method results in superior performance compared to\nusing only the audio feature (‚Äúw/o Word‚Äù) or only the word feature\n(‚Äúw/o Audio‚Äù). This is because using only audio without semantic\ninformation may cause the model to struggle with generating com-\nplex motion. High-level context information provided by the BERT\nfeature can complement the audio, resulting in better overall per-\nformance. We provide a detailed comparison in our video. Readers\nare referred to the video for further comparison.\n5 LIMITATIONS\nThough we provide a way to measure the performance of speech-to-\nbody motion synthesis by comparing the average motion speed for\neach mode, it is still an open question of how to evaluate the quality\nof the synthesized motion. Our system is a seq2seq framework\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\n111:10 ‚Ä¢ Pang and Qin et al.\n‚Ä¶ the light shining off different walls ‚Ä¶ ‚Ä¶ trying to rush through like ehhh ‚Ä¶ ‚Ä¶ and be like rushing you‚Ä¶\nw/o M.\nw/o P.\nOurs\nFig. 9. Left: When the speaker says shining off, the character raises his right hand up and then put it down in front of his chest in our model, trying to\nemphasize the verb, while his right-hand swings meaninglessly without pre-training and the motion is nearly static without MPE. Middle: In our model, his\nhands stop suddenly as the speaker utters the hesitation marker ehh. But the pause does not show without pre-training and is lagged behind the speech\nwithout MPE. Right: The hands move quickly to express the keyword rushing in our model. The hands stay in the rest pose without pre-training and move\nslowly without MPE. (See the supplementary video for dynamic comparisons.)\n0 50 100 150 200 250 300\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0 50 100 150 200 250 300\nAudio Timestep\n0\n50\n100\n150\n200\n250\n300\nAudio Timestep\nMotion Timestep\nFig. 10. Attention heatmap average across all test sequences without repe-\ntitions where (a) w/o P (b) Ours.\nw/o Word w/o Audio w/o M. w/o P.\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85% winning over\n \n \n \n \nHuman-Likeness\nw/o Word w/o Audio w/o M. w/o P.\nCompetitors\n50\n55\n60\n65\n70\n75\n80\n85% winning over\n  \n \n \nAppropriateness\n(a) (b)\nFig. 11. The pairwise winning proportion of the competitors (a) Human-\nlikeness and (b) Appropriateness, on Trinity Speech-Gesture Dataset.\nthat uses future text and speech information for generating the\ngesture. This could be an issue for real-time applications such as\nremote meetings. A possible solution can be to predict the rest of\nthe sentences from the speech up to now and input that into the\nsystem for motion synthesis.\n6 CONCLUSION AND FUTURE WORK\nIn this paper, we present BodyFormer, a method to synthesize body\ngestures from speech data using a variational transformer model.\nThe system produces realistic and vivid motion and outperforms\nexisting models both in quantitative and qualitative evaluation. We\nare interested in further improving the model through a better\nevaluation metric and a training scheme that can learn from a wider\nrange of data. We are also interested in constructing a multi-modality\nmodel that also learns the facial expressions and gaze for metaverse\napplications.\nACKNOWLEDGMENTS\nThis research is supported by Meta Reality Labs, Innovation and\nTechnology Commission (Ref:ITS/319/21FP) and Research Grant\nCouncil (Ref: 17210222), Hong Kong.\nREFERENCES\nChaitanya Ahuja, D Lee, Ryo Ishii, and L-P Morency. 2020a. No gestures left behind:\nLearning relationships between spoken language and freeform gestures. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natural Language Processing:\nFindings (EMNLP) .\nChaitanya Ahuja, Dong Won Lee, Yukiko I Nakano, and Louis-Philippe Morency. 2020b.\nStyle transfer for co-speech gesture animation: A multi-speaker conditional-mixture\napproach. In European Conference on Computer Vision . Springer, 248‚Äì265.\nSimon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. 2020.\nStyle-Controllable Speech-Driven Gesture Synthesis Using Normalising Flows.Com-\nput. Graph. Forum 39, 2 (2020), 487‚Äì496. https://doi.org/10.1111/cgf.13946\nTenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and Libin Liu. 2022. Rhythmic\nGesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural\nEmbeddings. ACM Trans. Graph. 41, 6, Article 209 (nov 2022), 19 pages. https:\n//doi.org/10.1145/3550454.3555435\nUttaran Bhattacharya, Elizabeth Childs, Nicholas Rewkowski, and Dinesh Manocha.\n2021a. Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Gener-\native Adversarial Affective Expression Learning. In Proceedings of the 29th ACM\nInternational Conference on Multimedia (Virtual Event, China) (MM ‚Äô21). Association\nfor Computing Machinery, New York, NY, USA, 2027‚Äì2036. https://doi.org/10.1145/\n3474085.3475223\nUttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket\nBera, and Dinesh Manocha. 2021b. Text2Gestures: A Transformer-Based Network\nfor Generating Emotive Body Gestures for Virtual Agents. In 2021 IEEE Virtual\nReality and 3D User Interfaces (VR) . IEEE, 1‚Äì10.\nJustine Cassell, David McNeill, and Karl-Erik McCullough. 1999. Speech-gesture mis-\nmatches: Evidence for one underlying representation of linguistic and nonlinguistic\ninformation. Pragmatics & cognition 7, 1 (1999), 1‚Äì34.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhut-\ndinov. 2019. Transformer-XL: Attentive Language Models beyond a Fixed-Length\nContext. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics . Association for Computational Linguistics, Florence, Italy,\n2978‚Äì2988. https://doi.org/10.18653/v1/P19-1285\nJan P De Ruiter, Adrian Bangerter, and Paula Dings. 2012. The interplay between\ngesture and speech in the production of referring expressions: Investigating the\ntradeoff hypothesis. Topics in Cognitive Science 4, 2 (2012), 232‚Äì248.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision\nand pattern recognition . Ieee, 248‚Äì255.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\nBodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer ‚Ä¢ 111:11\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers) . Association for Computational Linguistics, Minneapolis, Minnesota,\n4171‚Äì4186. https://doi.org/10.18653/v1/N19-1423\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In International Conference on\nLearning Representations. https://openreview.net/forum?id=YicbFdNTTy\nYlva Ferstl and Rachel McDonnell. 2018. Investigating the use of recurrent motion\nmodelling for speech gesture generation. In Proceedings of the 18th International\nConference on Intelligent Virtual Agents . https://trinityspeechgesture.scss.tcd.ie\nYlva Ferstl, Michael Neff, and Rachel McDonnell. 2019. Multi-objective adversarial\ngesture generation. In Motion, Interaction and Games . ACM, 3.\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence\nCarin. 2019. Cyclical Annealing Schedule: A Simple Approach to Mitigating KL\nVanishing. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) . Association for Computational Linguistics, Minneapolis,\nMinnesota, 240‚Äì250. https://doi.org/10.18653/v1/N19-1021\nF.A. Gers, J. Schmidhuber, and F. Cummins. 1999. Learning to forget: continual predic-\ntion with LSTM. In 1999 Ninth International Conference on Artificial Neural Networks\nICANN 99. (Conf. Publ. No. 470) , Vol. 2. 850‚Äì855 vol.2. https://doi.org/10.1049/cp:\n19991218\nSaeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F. Troje, and Marc-Andr√© Carbon-\nneau. 2023. ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech.\nComputer Graphics Forum 42, 1 (2023), 206‚Äì216. https://doi.org/10.1111/cgf.14734\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14734\nShiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra\nMalik. 2019. Learning Individual Styles of Conversational Gesture. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition . 3497‚Äì3506.\nIkhsanul Habibie, Mohamed Elgharib, Kripashindu Sarkar, Ahsan Abdullah, Simbarashe\nNyatsanga, Michael Neff, and Christian Theobalt. 2022. A Motion Matching-based\nFramework for Controllable Gesture Synthesis from Speech. In SIGGRAPH ‚Äô22\nConference Proceedings .\nIkhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard\nPons-Moll, Mohamed Elgharib, and Christian Theobalt. 2021. Learning speech-\ndriven 3D conversational gestures from video. In Proceedings of the 21st ACM Inter-\nnational Conference on Intelligent Virtual Agents . 101‚Äì108.\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. MoGlow: Probabilistic\nand controllable motion synthesis using normalising flows. ACM Transactions on\nGraphics 39, 4 (2020), 236:1‚Äì236:14. https://doi.org/10.1145/3414685.3417836\nAdam Kendon. 1972. Some relationships between body motion and speech. Studies in\ndyadic communication 7, 177 (1972), 90.\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz\nKhan, and Mubarak Shah. 2021. Transformers in vision: A survey. ACM Computing\nSurveys (CSUR) (2021).\nTaras Kucherenko, Dai Hasegawa, Gustav Eje Henter, Naoshi Kaneko, and Hedvig\nKjellstr√∂m. 2019. Analyzing input and output representations for speech-driven\ngesture generation. In Proceedings of the 19th ACM International Conference on\nIntelligent Virtual Agents . 97‚Äì104.\nTaras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, Simon Alexan-\ndersson, Iolanda Leite, and Hedvig Kjellstr√∂m. 2020. Gesticulator: A framework for\nsemantically-aware speech-driven gesture generation. In Proceedings of the 2020\nInternational Conference on Multimodal Interaction . 242‚Äì250.\nTaras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje Henter.\n2021. A Large, Crowdsourced Evaluation of Gesture Generation Systems on Common\nData: The GENEA Challenge 2020. In 26th International Conference on Intelligent\nUser Interfaces (College Station, TX, USA) (IUI ‚Äô21) . Association for Computing\nMachinery, New York, NY, USA, 11‚Äì21. https://doi.org/10.1145/3397481.3450692\nGilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S. Srinivasa,\nand Yaser Sheikh. 2019a. Talking With Hands 16.2M: A Large-Scale Dataset of\nSynchronized Body-Finger Motion and Audio for Conversational Motion Analysis\nand Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV) .\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye\nTeh. 2019b. Set Transformer: A Framework for Attention-based Permutation-\nInvariant Neural Networks. In Proceedings of International Conference on Machine\nLearning. 3744‚Äì3753.\nJing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Zhenyu He, and Linchao Bao.\n2021a. Audio2Gestures: Generating Diverse Gestures from Speech Audio with\nConditional Variational Autoencoders. In Proceedings of the IEEE/CVF International\nConference on Computer Vision . 11293‚Äì11302.\nRuilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. 2021b. Learn to Dance\nwith AIST++: Music Conditioned 3D Dance Generation. In Proceedings of IEEE/CVF\nInternational Conference on Computer Vision .\nHaiyang Liu, Naoya Iwamoto, Zihao Zhu, Zhengqing Li, You Zhou, Elif Bozkurt, and\nBo Zheng. 2022a. DisCo: Disentangled Implicit Content and Rhythm Learning for\nDiverse Co-Speech Gestures Synthesis. In Proceedings of the 30th ACM International\nConference on Multimedia (Lisboa, Portugal) (MM ‚Äô22) . Association for Comput-\ning Machinery, New York, NY, USA, 3764‚Äì3773. https://doi.org/10.1145/3503161.\n3548400\nHaiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou,\nElif Bozkurt, and Bo Zheng. 2022c. BEAT: A Large-Scale Semantic and Emo-\ntional Multi-Modal Dataset for Conversational Gestures Synthesis. arXiv preprint\narXiv:2203.05297 (2022).\nXian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou,\nWayne Wu, Bo Dai, and Bolei Zhou. 2022b. Learning Hierarchical Cross-Modal\nAssociation for Co-Speech Gesture Generation. arXiv preprint arXiv:2203.13161\n(2022).\nIlya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In\nInternational Conference on Learning Representations . https://openreview.net/forum?\nid=Bkg6RiCqY7\nBrian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg,\nand Oriol Nieto. 2015. librosa: Audio and music signal analysis in python. In\nProceedings of the 14th python in science conference , Vol. 8.\nDavid McNeill. 1992. Hand and mind: What gestures reveal about thought . University of\nChicago press.\nDaniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin Dogus\nCubuk, and Quoc V. Le. 2019. SpecAugment: A Simple Data Augmentation Method\nfor Automatic Speech Recognition. In INTERSPEECH.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des-\nmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-\ntala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library.\nIn Advances in Neural Information Processing Systems 32 , H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates,\nInc., 8024‚Äì8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-\nhigh-performance-deep-learning-library.pdf\nShenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, and Shenghua Gao. 2021. Speech Drives\nTemplates: Co-Speech Gesture Synthesis with Learned Templates. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision . 11077‚Äì11086.\nEli Shlizerman, Lucio Dery, Hayden Schoen, and Ira Kemelmacher-Shlizerman. 2018.\nAudio to body dynamics. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition . 7574‚Äì7583.\nGuillermo Valle-P√©rez, Gustav Eje Henter, Jonas Beskow, Andre Holzapfel, Pierre-Yves\nOudeyer, and Simon Alexanderson. 2021. Transflower: Probabilistic Autoregressive\nDance Generation with Multimodal Attention. ACM Trans. Graph. 40, 6, Article 195\n(dec 2021), 14 pages. https://doi.org/10.1145/3478513.3480570\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need.\nIn Advances in Neural Information Processing Systems , I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),\nVol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\nPetra Wagner, Zofia Malisz, and Stefan Kopp. 2014. Gesture and speech in interaction:\nAn overview.\nBenyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and\nJakob Grue Simonsen. 2021b. On Position Embeddings in BERT. In Proceedings\nof International Conference on Learning Representations . https://openreview.net/\nforum?id=onxoVA9FxMw\nSiyang Wang, Simon Alexanderson, Joakim Gustafson, Jonas Beskow, Gustav Eje Henter,\nand √âva Sz√©kely. 2021a. Integrated Speech and Gesture Synthesis. In Proceedings of\nthe 2021 International Conference on Multimodal Interaction (Montr√©al, QC, Canada)\n(ICMI ‚Äô21) . Association for Computing Machinery, New York, NY, USA, 177‚Äì185.\nhttps://doi.org/10.1145/3462244.3479914\nYoungwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and\nGeehyuk Lee. 2020. Speech Gesture Generation from the Trimodal Context of Text,\nAudio, and Speaker Identity. ACM Trans. Graph. 39, 6 (nov 2020).\nYoungwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee.\n2019. Robots learn social skills: End-to-end learning of co-speech gesture generation\nfor humanoid robots. In 2019 International Conference on Robotics and Automation\n(ICRA). IEEE, 4303‚Äì4309.\nHe Zhang, Sebastian Starke, Taku Komura, and Jun Saito. 2018. Mode-adaptive neural\nnetworks for quadruped motion control. ACM Transactions on Graphics (TOG) 37, 4\n(2018), 145.\nYi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li Hao. 2019. On the Continuity\nof Rotation Representations in Neural Networks. In Proc. IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) .\nLingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. 2023. Taming\nDiffusion Models for Audio-Driven Co-Speech Gesture Generation. In Proceedings\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.\n111:12 ‚Ä¢ Pang and Qin et al.\nof IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\nAntonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-\nLike Visual Explanations by Watching Movies and Reading Books. In Proceedings\nof IEEE/CVF International Conference on Computer Vision . IEEE Computer Society,\n19‚Äì27.\nA AUTOMATIC MODE LABELING\nThe mode labeling is determined by the time length of each speech.\n‚Ä¢Long Speaking: the speech longer than 2 seconds.\n‚Ä¢Short Speaking: the speech shorter than 2 seconds.\n‚Ä¢Non Speaking: no word recognized by the Google ASR tools.\nB BASELINE IMPLEMENTATIONS\nFor baselines that output the global position of each joint, we ap-\nply Inverse-Kinematics (IK) to recover the joint rotations. For the\nTWH dataset, we compare BodyFormer with Trimodal and Ges-\nticulator. Since TWH has a non-standard T-pose, direct IK on the\nTWH skeleton leads to failure. To map the baseline motions to the\nTWH skeleton, we create a proxy skeleton with a standard T-pose\nand apply IK to it. Then, we utilize the Rokoko blender plugin 2 to\ntransfer motion to the TWH skeleton. We also tackle motion defects\nfrom IK by applying rotation offsets to problematic joints. See fig. 12\nfor an illustration.\nFig. 12. Left: A motion frame generated by Gesticulator on TWH dataset.\nNote that the left shoulder is wrongly rotated due to the error of inverse\nrigging. Right: The same frame after the manual fix. As this error is con-\nsistent between different frames, we fix it by simply applying a rotational\noffset on all frames.\nWe use the provided codes of each author to implement the base-\nlines. As none of them considered finger motions, we adopted a\ndefault pose to prevent finger collapse. All methods are rendered\nby Blender under the same setting of 20 fps, except for Trimodal,\nwhich is rendered in 15 fps to follow their original implementation.\nC EXAMPLE OUTPUT OF USER STUDY\nAs shown in the fig. 13, an example output of our user study focused\non appropriateness. A question that the participants were asked was:\n\"Which body gesture looks more appropriate to the given speech?\"\nTo answer this question, participants could click the \"play\" button to\n2https://www.rokoko.com/integrations/blender\nlisten to the audio clip and then select the appropriate radio button\nbefore submitting their user study. We also conducted a similar\nstudy focused on human-likeness, which used the same framework\nbut without audio.\nFig. 13. The example of the user study\nACM Trans. Graph., Vol. 42, No. 4, Article 111. Publication date: August 2023.",
  "topic": "Gesture",
  "concepts": [
    {
      "name": "Gesture",
      "score": 0.9198814630508423
    },
    {
      "name": "Computer science",
      "score": 0.7814996838569641
    },
    {
      "name": "Gesture recognition",
      "score": 0.560316801071167
    },
    {
      "name": "Transformer",
      "score": 0.5285716652870178
    },
    {
      "name": "Embedding",
      "score": 0.5072423815727234
    },
    {
      "name": "Speech recognition",
      "score": 0.4750021696090698
    },
    {
      "name": "Inference",
      "score": 0.44750720262527466
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4356991648674011
    },
    {
      "name": "Probabilistic logic",
      "score": 0.43055158853530884
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.4111037254333496
    },
    {
      "name": "Computer vision",
      "score": 0.3614013195037842
    },
    {
      "name": "Natural language processing",
      "score": 0.3364698886871338
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.33484768867492676
    },
    {
      "name": "Engineering",
      "score": 0.08397141098976135
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103524",
      "name": "Guangdong Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210142539",
      "name": "Guangdong Institute of Intelligent Manufacturing",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I4210128585",
      "name": "META Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ],
  "cited_by": 15
}