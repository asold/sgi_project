{
    "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
    "url": "https://openalex.org/W4389520494",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2115670440",
            "name": "Jiho Kim",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A5016151278",
            "name": "Yeonsu Kwon",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2139737370",
            "name": "Yohan Jo",
            "affiliations": [
                "Seoul National University"
            ]
        },
        {
            "id": "https://openalex.org/A2183278384",
            "name": "Edward Choi",
            "affiliations": [
                "Seoul National University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4385571920",
        "https://openalex.org/W2890961898",
        "https://openalex.org/W3034643750",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W3034862985",
        "https://openalex.org/W3170403598",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W2950336186",
        "https://openalex.org/W2945844671",
        "https://openalex.org/W3116847845",
        "https://openalex.org/W4303648884",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1552847225",
        "https://openalex.org/W4377121468",
        "https://openalex.org/W4310744116",
        "https://openalex.org/W2755637027",
        "https://openalex.org/W4221143046"
    ],
    "abstract": "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9410–9421\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nKG-GPT: A General Framework for Reasoning on Knowledge Graphs\nUsing Large Language Models\nJiho Kim1, Yeonsu Kwon1, Yohan Jo2, Edward Choi1\n1KAIST 2Seoul National University\n{jiho.kim, yeonsu.k, edwardchoi}@kaist.ac.kr\nyohan.jo@snu.ac.kr\nAbstract\nWhile large language models (LLMs) have\nmade considerable advancements in under-\nstanding and generating unstructured text, their\napplication in structured data remains underex-\nplored. Particularly, using LLMs for complex\nreasoning tasks on knowledge graphs (KGs)\nremains largely untouched. To address this,\nwe propose KG-GPT, a multi-purpose frame-\nwork leveraging LLMs for tasks employing\nKGs. KG-GPT comprises three steps: Sentence\nSegmentation, Graph Retrieval, and Inference,\neach aimed at partitioning sentences, retriev-\ning relevant graph components, and deriving\nlogical conclusions, respectively. We evalu-\nate KG-GPT using KG-based fact verification\nand KGQA benchmarks, with the model show-\ning competitive and robust performance, even\noutperforming several fully-supervised models.\nOur work, therefore, marks a significant step\nin unifying structured and unstructured data\nprocessing within the realm of LLMs.1\n1 Introduction\nThe remarkable advancements in large language\nmodels (LLMs) have notably caught the eye of\nscholars conducting research in the field of natural\nlanguage processing (NLP) (Brown et al., 2020;\nChowdhery et al., 2022; OpenAI, 2023a,b; Anil\net al., 2023). In their endeavor to create LLMs\nthat can mirror the reasoning capabilities inherent\nto humans, past studies have primarily centered\ntheir attention on unstructured textual data. This\nincludes, but is not limited to, mathematical word\nproblems (Miao et al., 2020; Cobbe et al., 2021;\nPatel et al., 2021), CSQA (Talmor et al., 2019), and\nsymbolic manipulation (Wei et al., 2022). While\nsignificant strides have been made in this area, the\ndomain of structured data remains largely unex-\nplored.\n1Code is available at https://github.com/jiho283/\nKG-GPT.\nStructured data, particularly in the form of\nknowledge graphs (KGs), serves as a reservoir of in-\nterconnected factual information and associations,\narticulated through nodes and edges. The inherent\nstructure of KGs offers a valuable resource that\ncan assist in executing complex reasoning tasks,\nlike multi-hop inference. Even with these advan-\ntages, to the best of our knowledge, there is no\ngeneral framework for performing KG-based tasks\n(e.g. question answering, fact verification) using\nauto-regressive LLMs.\nTo this end, we propose a new general frame-\nwork, called KG-GPT, that uses LLMs’ reasoning\ncapabilities to perform KG-based tasks. KG-GPT\nis similar to StructGPT (Jiang et al., 2023) in that\nboth reason on structured data using LLMs. How-\never, unlike StructGPT which identifies paths from\na seed entity to the final answer entity within KGs,\nKG-GPT retrieves the entire sub-graph and then\ninfers the answer. This means KG-GPT can be\nused not only for KGQA but also for tasks like\nKG-based fact verification.\nKG-GPT consists of three steps: 1) Sentence\n(Claim / Question) Segmentation, 2) Graph Re-\ntrieval, and 3) Inference. During Sentence Seg-\nmentation, a sentence is partitioned into discrete\nsub-sentences, each aligned with a single triple (i.e.\n[head, relation, tail]). The subsequent step, namely\nGraph Retrieval, retrieves a potential pool of rela-\ntions that could bridge the entities identified within\nthe sub-sentences. Then, a candidate pool of evi-\ndence graphs (i.e. sub-KG) is obtained using the\nretrieved relations and the entity set. In the final\nstep, the obtained graphs are used to derive a logi-\ncal conclusion, such as validating a given claim or\nanswering a given question.\nTo evaluate KG-GPT, we employ KG-based\nfact verification and KGQA benchmarks, both de-\nmanding complex reasoning that utilizes structured\nknowledge of KGs. In KG-based fact verification,\nwe use FACT KG (Kim et al., 2023), which includes\n9410\nFigure 1: An overview of KG-GPT. The framework comprises three distinct phases: Sentence Segmentation,\nGraph Retrieval, and Inference. The given example comes from FACT KG. It involves a 2-hop inference from\n‘William_Anders’ to ‘Frank_Borman’, requiring verification through an evidence graph consisting of three triples.\nBoth ‘William_Anders’ and ‘Frank_Borman’ serve as internal nodes in DBpedia (Lehmann et al., 2015), while\n“AFIT, M.S. 1962” acts as a leaf node. Moreover, artificial satellite represents the Type information absent from the\nprovided entity set.\nvarious graph reasoning patterns, and KG-GPT\nshows competitive performance compared to other\nfully-supervised models, even outperforming some.\nIn KGQA, we use MetaQA (Zhang et al., 2018), a\nQA dataset composed of 1-hop, 2-hop, and 3-hop\ninference tasks. KG-GPT shows performance com-\nparable to fully-supervised models. Notably, the\nperformance does not significantly decline with the\nincrease in the number of hops, demonstrating its\nrobustness.\n2 Method\nKG-GPT is composed of three stages: Sentence\nSegmentation, Graph Retrieval, and Inference, as\ndescribed in Fig. 1.\nWe assume a graph G(knowledge graph con-\nsisting of entities Eand relations R), a sentence\nS (claim or question), and all entities involved in\nS, ES ⊂E are given. In order to derive a logical\nconclusion, we need an accurate evidence graph\nGE ⊂G, which we obtain in two stages, Sentence\nSegmentation and Graph Retrieval. Furthermore,\nall the aforementioned steps are executed employ-\ning the in-context learning methodology to max-\nimize the LLM’s reasoning ability. The prompts\nused for each stage are in Appendix A.\n2.1 Sentence Segmentation\nMany KG-based tasks require multi-hop reasoning.\nTo address this, we utilize a Divide-and-Conquer\napproach. By breaking down a sentence into sub-\nsentences that correspond to a single relation, iden-\ntifying relations in each sub-sentence becomes eas-\nier than finding n-hop relations connected to an\nentity from the original sentence all at once.\nWe assume S can be broken down into sub-\nsentences: S1, S2, ..., Sn where Si consists of a\nset of entities Ei ⊂E and a relation ri ∈R. Each\ne(j)\ni ∈Ei can be a concrete entity (e.g. William An-\nders in Fig. 1-(1)), or a type (e.g. artificial satellite\nin Fig. 1-(1)). ri can be mapped to one or more\nitems in R, as there can be multiple relations with\nsimilar semantics (e.g. birthPlace, placeOfBirth).\n2.2 Graph Retrieval\nTo effectively validate a claim or answer a question,\nit is crucial to obtain an evidence graph ( i.e. sub-\n9411\nKG) that facilitates logical conclusions. In this\nstage, we first aim to retrieve the corresponding\nrelations for each sub-sentence Si to extract GE.\nFor each Si, we use the LLM to map ri to one\nor more items in Ras accurately as possible. To\ndo so, we first define Ri ⊂R, which is a set of\nrelations connected to all e(j)\ni ∈Ei according to the\nschema of G(i.e. relation candidates in Fig. 1-(2)).\nThis process considers both the relations connected\nto a specific entity and the relations associated with\nthe entity’s type in G. We further elaborate on the\nprocess in Appendix B. Then, we feedSi and Ri to\nthe LLM to retrieve the set of final top-K relations\nRi,k. In detail, relations in Ri are linearized (e.g.\n[location, birthYear, ..., birthDate]) and combined\nwith the corresponding sub-sentence Si to estab-\nlish prompts for the LLM and the LLM generates\nRi,k = {r(1)\ni , ..., r(k)\ni } as output. In the final graph\nretrieval step, we can obtain GE, made up of all\ntriples whose relations come from Ri,k and whose\nentities come from Ei across all Si.\n2.3 Inference\nThen, we feed S and GE to the LLM to derive a\nlogical conclusion. In order to represent GE in the\nprompt, we linearize the triples associated with\nGE (i.e. [[head1, rel1, tail1], ..., [ headm, relm,\ntailm]]), and then concatenate these linearized\ntriples with the sentence S. In fact verification, the\ndetermination of whether S is supported or refuted\nis contingent upon GE. In question answering, the\nLLM identifies an entity inGE as the most probable\nanswer to S.\n3 Experiments\nWe evaluate our framework on two tasks that re-\nquire KG grounding: fact-verification and question-\nanswering. A detailed description of experimental\nsettings can be found in Appendix C.\n3.1 Dataset\n3.1.1 F ACTKG\nFACT KG (Kim et al., 2023) serves as a practical\nand challenging dataset meticulously constructed\nfor the purpose of fact verification, employing a\nknowledge graph for validation purposes. It encom-\npasses 108K claims that can be verified via DBpe-\ndia (Lehmann et al., 2015), which is one of the\navailable comprehensive knowledge graphs. These\nclaims are categorized as either Supported or Re-\nfuted. FACT KG embodies five diverse types of\nreasoning that represent the intrinsic characteris-\ntics of the KG: One-hop, Conjunction, Existence,\nMulti-hop, and Negation. To further enhance its\npractical use, FACT KG integrates claims in both\ncolloquial and written styles. Examples of claims\nfrom FACT KG can be found in Appendix D.\n3.1.2 MetaQA\nMetaQA (Zhang et al., 2018) is a carefully curated\ndataset intended to facilitate the study of question-\nanswering that leverages KG-based approaches in\nthe field of movies. The dataset encompasses over\n400K questions, including instances of 1-hop, 2-\nhop, and 3-hop reasoning. Additionally, it covers a\ndiverse range of question styles. Examples of ques-\ntions from MetaQA can be found in Appendix D.\n3.2 Baselines\nFor evaluation on FACT KG, we use the same base-\nlines as in Kim et al. (2023). These baselines are\ndivided into two distinct categories: Claim Only\nand With Evidence. In the Claim Only setting, the\nmodels are provided only with the claim as their\ninput and predict the label. For this setting, in ad-\ndition to the existing baselines, we implement a\n12-shot ChatGPT (OpenAI, 2023b) baseline. In\nthe With Evidence scenario, models consist of an\nevidence graph retriever and a claim verification\nmodel. We employ the KG version of GEAR (Zhou\net al., 2019) as a fully supervised model.\nIn our exploration of MetaQA, we use a selection\nof prototypical baselines well-known in the field\nof KGQA. These include models such as KV-Mem\n(Xu et al., 2019), GraftNet (Sun et al., 2018), Em-\nbedKGQA (Saxena et al., 2020), NSM (He et al.,\n2021), and UniKGQA (Jiang et al., 2022), which\noperate in a fully supervised fashion. Additionally,\nwe implement a 12-shot ChatGPT baseline.\n4 Results & Analysis\n4.1 F ACTKG\nWe evaluated the models’ prediction capability for\nlabels (i.e. Supported or Refuted) and presented\nthe accuracy score in Table 1. As a result, KG-GPT\noutperforms Claim Only models BERT, BlueBERT,\nand Flan-T5 with performance enhancements of\n7.48%, 12.75%, and 9.98% absolute, respectively.\nIt also outperforms 12-shot ChatGPT by 4.20%.\nThese figures emphasize the effectiveness of our\nframework in extracting the necessary evidence for\nclaim verification, highlighting the positive impact\n9412\nInput Type Training\nStrategy Methods Accuracy\nClaim Only\nfull BERT 65.20\nBlueBERT 59.93\nzero-shot Flan-T5 62.70\n12-shot ChatGPT 68.48\nWith Evidence\nfull GEAR 77.65\n12-shot\nKG-GPT\n72.68\n8-shot 67.68\n4-shot 59.53\nTable 1: The performance of the models on FACT KG.\nExcept for ChatGPT and KG-GPT, all performances are\nobtained from Kim et al. (2023).\nTraining Methods MetaQA MetaQA MetaQA\nStrategy 1-hop 2-hop 3-hop\nfull\nKV-Mem 96.2 82.7 48.9\nGraftNet 97.0 94.8 77.7\nEmbedKGQA 97.5 98.8 94.8\nNSM 97.1 99.9 98.9\nUniKGQA 97.5 99.0 99.1\n12-shot ChatGPT 60.0 23.0 38.7\n12-shot 96.3 94.4 94.0\n8-shot KG-GPT 95.8 93.8 68.8\n4-shot 94.7 92.8 46.6\nTable 2: The performance of the models on MetaQA\n(Hits@1). The best results for each task and those of\n12-shot KG-GPT are in bold.\nof the sentence segmentation and graph retrieval\nstages. The qualitative results including the graphs\nretrieved by KG-GPT are in Appendix E.1.\nNonetheless, when compared to GEAR, a fully\nsupervised model built upon KGs, KG-GPT ex-\nhibits certain limitations. KG-GPT achieves an ac-\ncuracy score of 72.68%, which is behind GEAR’s\n77.65%. This performance gap illustrates the ob-\nstacles encountered by KG-GPT in a few-shot sce-\nnario, namely the difficulty in amassing a suffi-\ncient volume of information from the restricted\ndata available. Hence, despite the notable progress\nachieved with KG-GPT, there is clear room for im-\nprovement to equal or surpass the performance of\nKG-specific supervised models like GEAR.\n4.2 MetaQA\nThe findings on MetaQA are presented in Table 2.\nThe performance of KG-GPT is impressive, scoring\n96.3%, 94.4%, and 94.0% on 1-hop, 2-hop, and 3-\nhop tasks respectively. This demonstrates its strong\nability to generalize from a limited number of ex-\namples, a critical trait when handling real-world\napplications with varying degrees of complexity.\nStage FactKGMetaQA MetaQA MetaQA\n1-hop 2-hop 3-hop\nSentence 39 3 63 100Segmentation\nGraph Retrieval17 4 3 0\nInference 44 93 34 0\nTable 3: Number of errors from 100 incorrect samples\nacross each dataset.\nInterestingly, the performance of KG-GPT\nclosely matches that of a fully-supervised model.\nParticularly, it surpasses KV-Mem by margins of\n0.1%, 11.7%, and 45.1% across three distinct tasks\nrespectively, signifying its superior performance.\nWhile the overall performance of KG-GPT is simi-\nlar to that of GraftNet, a noteworthy difference is\npronounced in the 3-hop task, wherein KG-GPT\noutperforms GraftNet by 16.3%. The qualitative\nresults including the graphs retrieved by KG-GPT\nare in Appendix E.2.\n4.3 Error Analysis\nIn both FACT KG and MetaQA, there are no cor-\nresponding ground truth graphs containing seed\nentities. This absence makes a quantitative step-by-\nstep analysis challenging. Therefore, we carried\nout an error analysis, extracting 100 incorrect sam-\nples from each dataset: FACT KG, MetaQA-1hop,\nMetaQA-2hop, and MetaQA-3hop. Table 3 shows\nthe number of errors observed at each step. No-\ntably, errors during the graph retrieval phase are\nthe fewest among the three steps. This suggests\nthat once sentences are correctly segmented, iden-\ntifying relations within them becomes relatively\neasy. Furthermore, a comparative analysis between\nMetaQA-1hop, MetaQA-2hop, and MetaQA-3hop\nindicates that as the number of hops increases, so\ndoes the diversity of the questions. This heightened\ndiversity in turn escalates the errors in Sentence\nSegmentation.\n4.4 Ablation Study\n4.4.1 Number of In-context Examples\nThe results for the 12-shot, 8-shot, and 4-shot\nfrom the FACT KG and MetaQA datasets are re-\nported in Table 1 and Table 2, respectively. Though\nthere was a predicted improvement in performance\nwith the increase in the number of shots in both\nFACT KG and MetaQA datasets, this was not uni-\nformly observed across all scenarios. Notably,\nMetaQA demonstrated superior performance, ex-\n9413\nceeding 90%, in both the 1-hop and 2-hop scenar-\nios, even with a minimal set of four examples. In\ncontrast, in both the FACT KG and MetaQA 3-hop\nscenarios, the performance of the 4-shot learning\nscenario was similar to that of the baselines which\ndid not utilize graph evidence. This similarity sug-\ngests that LLMs may struggle to interpret com-\nplex data features when equipped with only four\nshots. Thus, the findings highlight the importance\nof formulating in-context examples according to\nthe complexity of the task.\n4.4.2 Top-K Relation Retrieval\nTable 11 shows the performance according to the\nvalue of k in FACT KG. As a result, performance\ndid not significantly vary depending on the value of\nk. Table 12 illustrates the average number of triples\nretrieved for both supported and refuted claims, de-\npending on k. Despite the increase in the number\nof triples as the value of k grows, it does not im-\npact the accuracy. This suggests that the additional\ntriples are not significantly influential.\nIn MetaQA, the performance and the average\nnumber of retrieved triples are also depicted in\nTable 13 and Table 14, respectively. Unlike the\nFACT KG experiment, as the value of k rises in\nMetaQA, it appears that more significant triples are\nretrieved, leading to improved performance.\n5 Conclusion\nWe suggest KG-GPT, a versatile framework that\nutilizes LLMs for tasks that use KGs. KG-GPT is\ndivided into three stages: Sentence Segmentation,\nGraph Retrieval, and Inference, each designed for\nbreaking down sentences, sourcing related graph\nelements, and reaching reasoned outcomes, respec-\ntively. We assess KG-GPT’s efficacy using KG-\nbased fact verification and KGQA metrics, and\nthe model demonstrates consistent, impressive re-\nsults, even surpassing a number of fully-supervised\nmodels. Consequently, our research signifies a sub-\nstantial advancement in combining structured and\nunstructured data management in the LLMs’ con-\ntext.\nLimitations\nOur study has two key limitations. Firstly, KG-GPT\nis highly dependent on in-context learning, and its\nperformance varies significantly with the number\nof examples provided. The framework struggles\nparticularly with complex tasks when there are in-\nsufficient or low-quality examples. Secondly, de-\nspite its impressive performance in fact-verification\nand question-answering tasks, KG-GPT still lags\nbehind fully supervised KG-specific models. The\ngap in performance highlights the challenges faced\nby KG-GPT in a few-shot learning scenario due to\nlimited data. Future research should focus on op-\ntimizing language models leveraging KGs to over-\ncome these limitations.\nAcknowledgements\nThis work was supported by Institute of Informa-\ntion & Communications Technology Planning &\nEvaluation (IITP) grant (No.2019-0- 00075), Na-\ntional Research Foundation of Korea (NRF) grant\n(NRF-2020H1D3A2A03100945), and the Korea\nHealth Industry Development Institute (KHIDI)\ngrant (No.HR21C0198), funded by the Korea gov-\nernment (MSIT, MOHW).\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin Zhao, and\nJi-Rong Wen. 2021. Improving multi-hop knowledge\nbase question answering by learning intermediate\nsupervision signals. In Proceedings of the 14th ACM\ninternational conference on web search and data\nmining, pages 553–561.\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\n9414\nArabella Sinclair, et al. 2022. State-of-the-art gen-\neralisation research in nlp: a taxonomy and review.\narXiv preprint arXiv:2210.03050.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Structgpt:\nA general framework for large language model to\nreason on structured data.\nJinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji-Rong\nWen. 2022. Unikgqa: Unified retrieval and reason-\ning for solving multi-hop question answering over\nknowledge graph. arXiv preprint arXiv:2212.00959.\nJiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James\nThorne, and Edward Choi. 2023. FactKG: Fact veri-\nfication via reasoning on knowledge graphs. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 16190–16206, Toronto, Canada. As-\nsociation for Computational Linguistics.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nSören Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic web, 6(2):167–195.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975–984, Online.\nAssociation for Computational Linguistics.\nOpenAI. 2023a. Gpt-4 technical report.\nOpenAI. 2023b. Introducing chatgpt.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nApoorv Saxena, Aditay Tripathi, and Partha Talukdar.\n2020. Improving multi-hop question answering over\nknowledge graphs using knowledge base embeddings.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 4498–\n4507, Online. Association for Computational Lin-\nguistics.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William W Co-\nhen. 2018. Open domain question answering using\nearly fusion of knowledge bases and text. arXiv\npreprint arXiv:1809.00782.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nKun Xu, Yuxuan Lai, Yansong Feng, and Zhiguo Wang.\n2019. Enhancing key-value memory neural networks\nfor knowledge based question answering. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2937–2947, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexan-\nder J Smola, and Le Song. 2018. Variational reason-\ning for question answering with knowledge graph. In\nAAAI.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: Graph-based evidence aggregating and rea-\nsoning for fact verification. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 892–901, Florence, Italy.\nAssociation for Computational Linguistics.\n9415\nA Prompts\nThe prompts for Sentence Segmentation, Graph\nRetrieval, and Inference can be found in Table 4,\nTable 5 and Table 6, respectively.\nB Relation Candidates Extraction\nAlgorithm\nB.1 F ACTKG\nIn FACT KG, we develop a new KG called Type-\nDBpedia. This graph comprises types found in DB-\npedia and connects them through relations, thereby\nenhancing the usability of KG content. We describe\nthe detailed process of incorporating Ri using DB-\npedia and TypeDBpedia in Algorithm 1.\nWe denote the entities as E1 and E2 because the\nsub-sentence always includes two entities in FAC-\nTKG. Relations (e, DBpedia) represents the set\nof relations connected to e in DBpedia. Similarly,\nRelations (T, TypeDBpedia ) represents the set of\nrelations connected to T in TypeDBpedia .\nAlgorithm 1:Extract Relation Candidates\nInput: Entity Set E = {E1, E2},\nDBpedia, TypeDBpedia\nOutput: Relation Candidates Ri\nInitialization: T = ∅, RT = ∅, RE = ∅,\nRi = ∅\nfor each entity e in E do\nif isType (e) then\nT ←T ∪{e}\nend\nelse\nif isEmpty (RE) then\nRE ←Relations (e, DBpedia)\nend\nelse\nRE ←RE ∩Relations (e,\nDBpedia)\nend\nend\nend\nif not isEmpty (T) then\nRT ←Relations (T, TypeDBpedia )\nRi ←RE ∩RT\nend\nelse\nRi ←RE\nend\nB.2 MetaQA\nFor the n-hop task in MetaQA, Ri is constructed\nfrom the relations within n-hops from the seed\nentity.\nC Experimental Settings\nWe utilize ChatGPT2 (OpenAI, 2023b) across all\ntasks, and to acquire more consistent responses, we\ncarry out inference with the temperature and top_p\nparameters set to 0.2 and 0.1, respectively. For each\nstage of KG-GPT, 12 pieces of training samples\nwere made into in-context examples and added\nto the prompt. In FACT KG, there are over 500\nexisting relations (|R|> 500), so we set k = 5for\nTop-K relation retrieval. Conversely, in MetaQA,\nthere are only 9 existing relations (|R|= 9), so we\nset k = 3.\nD Data Examples\nExamples of data from FACT KG and MetaQA can\nbe found in Tables 7 and 8, respectively.\nE Qualitative Results\nE.1 F ACTKG\nTable 9 includes the graphs retrieved by KG-GPT,\nalong with the prediction results, for five different\nclaims.\nE.2 MetaQA\nTable 10 includes the graphs retrieved by KG-GPT,\nalong with the prediction results, for nine different\nquestions.\nF Top-K Relation Retrieval\nF.1 F ACTKG\nThe performance and the average number of re-\ntrieved triples are depicted in Table 11 and Table 12,\nrespectively.\nF.2 MetaQA\nThe performance and the average number of re-\ntrieved triples are depicted in Table 13 and Table 14,\nrespectively.\n2https://platform.openai.com/docs/guides/gpt/chat-\ncompletions-api\n9416\nSentence Segmentation Prompt\nPlease divide the given sentence into several sentences each of which can be represented by one triplet. The generated\nsentences should be numbered and formatted as follows: #(number). (sentence), (entity set). The entity set for each\nsentence should contain no more than two entities, with each entity being used only once in all statements. The ‘##’\nsymbol should be used to indicate an entity set. In the generated sentences, there cannot be more than two entities in\nthe entity set. (i.e., the number of ## must not be larger than two.)\nExamples)\nSentence A: Ahmad Kadhim Assad’s club is Al-Zawra’a SC.\nEntity set: [‘Ahmad_Kadhim_Assad’ ## “Al-Zawra’a_SC”]\n–>Divided:\n1. Ahmad Kadhim Assad’s club is Al-Zawra’a SC., Entity set: [‘Ahmad_Kadhim_Assad’ ## “Al-Zawra’a_SC”]\n...\nSentence L: An academic journal with code IJPHDE is also Acta Math. Hungar.\nEntity set: [“Acta Math. Hungar.” ## “IJPHDE”]\n–>Divided:\n1. An academic journal is with code IJPHDE., Entity set: [‘academic journal’ ## “IJPHDE”]\n2. An academic journal is also Acta Math. Hungar., Entity set: [‘academic journal’ ## “Acta Math. Hungar.”]\nYour Task)\nSentence: <<<<CLAIM>>>>\nEntity set: <<<<ENTITY_SET>>>>\n–>Divided:\nTable 4: Sentence Segmentation Prompt.\nRelation Retrieval Prompt\nI will give you a set of words.\nFind the top <<<<TOP_K>>>>elements from Words set which are most semantically related to the given sentence.\nYou may select up to <<<<TOP_K>>>>words. If there is nothing that looks semantically related, pick out any\n<<<<TOP_K>>>>elements and give them to me.\nExamples)\nSentence A: Ahmad Kadhim Assad’s club is Al-Zawra’a SC.\nWords set: [‘club’, ‘clubs’, ‘parent’, ‘spouse’, ‘birthPlace’, ‘deathYear’, ‘leaderName’, ‘awards’, ‘award’,\n‘vicepresident’, ‘vicePresident’]\nTop 2 Answer: [‘club’, ‘clubs’]\n...\nSentence L: An academic journal with code IJPHDE is also Acta Math. Hungar.\nWords set: [‘abbreviation’, ‘placeOfBirth’, ‘owner’, ‘coden’, ‘almaMater’, ‘dean’, ‘coach’, ‘writer’, ‘firstAired’,\n‘director’, ‘formerTeam’, ‘starring’, ‘birthPlace’]\nTop 2 Answer: [‘abbreviation’, ‘coden’]\nNow let’s find the top <<<<TOP_K>>>>elements.\nSentence: <<<<SENTENCE>>>>\nWords set: <<<<RELATION_SET>>>>\nTop <<<<TOP_K>>>>Answer:\nTable 5: Relation Retrieval Prompt. The prompt is used when retrieving a relation to retrieve a graph.\n9417\nInference Prompt\nYou should verify the claim based on the evidence set.\nEach evidence is in the form of [head, relation, tail] and it means “head’s relation is tail.”.\nVerify the claim based on the evidence set. (True means that everything contained in the claim is\nsupported by the evidence.)\nPlease note that the unit is not important. (e.g. “98400” is also same as 98.4kg)\nChoose one of {True, False}, and give me the one-sentence evidence.\nExamples)\nClaim A: Ahmad Kadhim Assad’s club is Al-Zawra’a SC.\nEvidence set: [[‘Ahamad_Kadhim’, ‘clubs’, “Al-Zawra’a SC”]]\nAnswer: True, based on the evidence set, Ahmad Kadhim Assad’s club is Al-Zawra’a SC.\n...\nClaim L: The place, designed by Huseyin Butuner and Hilmi Guner, is located in a country,\nwhere the leader is Paul Nurse.\nEvidence set: [[“Baku_Turkish_Martyrs’_Memorial”, ‘designer’, “Hüseyin Bütüner and Hilmi Güner”],\n[“Baku_Turkish_Martyrs’_Memorial”, ‘location’, ‘Azerbaijan’]]\nAnswer: False, there is no evidence for Paul Nurse.\nNow let’s verify the Claim based on the Evidence set.\nClaim: <<<<CLAIM>>>>\nEvidence set: <<<<EVIDENCE_SET>>>>\nAnswer:\nTable 6: Inference Prompt.\nReasoning Type Claim Example Graph\nOne-hop AIDAstella was built by Meyer Werft. s mr2\nConjunction AIDA Cruise line operated the AIDAstella which was built by\nMeyer Werft. c s mr3 r2\nExistence Meyer Werft had a parent company. m r1\nMulti-hop AIDAstella was built by a company in Papenburg. s x pr2 r4\nNegation AIDAstella was not built by Meyer Werft in Papenburg. s m pr2 r4\nTable 7: Five different reasoning types of FACT KG. r1: parentCompany, r2: shipBuilder, r3: shipOperator, r4:\nlocation, m: Meyer Werft, s: AIDAstella, c: AIDA Cruises.\n9418\nTask Question Examples\n1-hop\n1. what does [Helen Mack] star in?\n2. what is the main language in [Karate-Robo Zaborgar]?\n3. who is the writer of [Boyz n the Hood]?\n2-hop\n1. who are movie co-directors of [Delbert Mann]?\n2. what genres do the films starred by [Al St. John] fall under?\n3. which films share the screenwriter with [King Arthur]?\n3-hop\n1. the films that share directors with the film [Catch Me If You Can] were in which languages?\n2. who are the directors of the movies written by the writer of [She]?\n3. when did the movies release whose actors also appear in the movie [Operator 13]?\nTable 8: Question examples from MetaQA.\nType Claim Retrieved Graph Prediction\nConjunctionYes, Agra Airport is located in India\nwhere the leader is Narendra Modi.\n[‘Agra_Airport’, location, ‘India’],\n[‘India’, leader, ‘Narendra_Modi’],\n[‘India’, leaderName, ‘Narendra_Modi’],\n[‘Narendra_Modi’, birthPlace, ‘India’]\nSupported\nConjunction\nI wasn’t aware that 103 Colmore Row,\nlocated in Birmingham, with 23\nfloors, was completed in 1976.\n[‘103_Colmore_Row’, location, ‘Birmingham’],\n[‘103_Colmore_Row’, floorCount, “23”],\n[‘103_Colmore_Row’, completionDate, “1976”],\n[‘103_Colmore_Row’, buildingEndDate, “1976”]\nSupported\nMulti-hop\nAlfredo Zitarrosa died in a city,\nUruguay (which has Raul Fernando\nSendic Rodriguez as leader).\n[‘Alfredo_Zitarrosa’, deathPlace, ‘Uruguay’],\n[‘Alfredo_Zitarrosa’, birthPlace, ‘Uruguay’],\n[‘Montevideo’, country, ‘Uruguay’],\n[‘Alfredo_Zitarrosa’, deathPlace, ‘Montevideo’],\n[‘Alfredo_Zitarrosa’, birthPlace, ‘Montevideo’],\n[‘Uruguay’, capital, ‘Montevideo’],\n[‘Uruguay’, leader, ‘Raúl_Fernando_Sendic_Rodríguez’],\n[‘Uruguay’, leaderName, ‘Raúl_Fernando_Sendic_Rodríguez’]\nSupported\nNegation Al-Taqaddum Air Base is located\nin Fallujah which is not in Iraq.\n[‘Al-Taqaddum_Air_Base’, city, ‘Fallujah’],\n[‘Al-Taqaddum_Air_Base’, cityServed, ‘Fallujah’],\n[‘Fallujah’, country, ‘Iraq’]\nRefuted\nMulti-hop\nA country is the location of the\nAdare Manor, is run by leader\nEnda Kenny and the natives are\nIrish people.\n[‘Adare_Manor’, country, ‘Republic_of_Ireland’],\n[‘Republic_of_Ireland’, leader, ‘Enda_Kenny’],\n[‘Republic_of_Ireland’, leaderName, ‘Enda_Kenny’],\n[‘Adare_Manor’, locationCountry, ‘Republic_of_Ireland’],\n[‘Republic_of_Ireland’, demonym, ‘Irish_people’]\nSupported\nTable 9: Qualitative results from FACT KG.\nTask Question Retrieved Graph Prediction\nwhat films does [Brigitte Nielsen] appear in?[‘Cobra’, starred_actors, ‘Brigitte Nielsen’],\n[‘Red Sonja’, starred_actors, ‘Brigitte Nielsen’] ‘Cobra’\n1-hopcan you name a film directed by\n[Nikolai Müllerschön]? [‘The Red Baron’, directed_by, ‘Nikolai Müllerschön’]‘The Red Baron’\nwhat type of film is [Six Shooter]?[‘Six Shooter’, has_genre, ‘Short’] ‘Short’\nwhen did the films starred by\n[Deborah Van Valkenburgh] release?\n[‘Mean Guns’, starred_actors, ‘Deborah Van Valkenburgh’],\n[‘Mean Guns’, release_year, ‘1997’] ‘1997’\n2-hopwhich films have the same director of\n[The Duellists]?\n[‘The Duellists’, directed_by, ‘Ridley Scott’],\n[‘The Counselor’, directed_by, ‘Ridley Scott’] ‘The Counselor’\nwhat genres are the movies written by\n[Robert Kenner] in?\n[‘Food, Inc.’, written_by, ‘Robert Kenner’],\n[‘Food, Inc.’, has_genre, ‘Documentary’] ‘Documentary’\nwhat are the genres of the movies whose\nwriters also wrote\n[The Lives of a Bengal Lancer]?\n[‘The Lives of a Bengal Lancer’, written_by, ‘John L. Balderston’],\n[‘Frankenstein’, written_by, ‘John L. Balderston’],\n[‘Frankenstein’, has_genre, ‘Horror’]\n‘Horror’\n3-hopwhen did the movies starred by\n[Seeking Justice] actors release?\n[‘Seeking Justice’, starred_actors, ‘Nicolas Cage’],\n[‘World Trade Center’, starred_actors, ‘Nicolas Cage’],\n[‘World Trade Center’, release_year, ‘2006’]\n‘2006’\nwhat types are the movies starred by actors\nin [A Thin Line Between Love and Hate]?\n[‘A Thin Line Between Love and Hate’, directed_by, ‘Martin Lawrence’],\n[“Big Momma’s House”, starred_actors, ‘Martin Lawrence’],\n[“Big Momma’s House”, has_genre, ‘Comedy’],\n[‘A Thin Line Between Love and Hate’, written_by, ‘Martin Lawrence’],\n[‘A Thin Line Between Love and Hate’, starred_actors, ‘Martin Lawrence’]\n‘Comedy’\nTable 10: Qualitative results from MetaQA.\n9419\nInput Type Method k Accuracy\nWith Evidence KG-GPT\n3 72.12\n5 72.68\n10 72.40\nTable 11: Performance changes according to the change\nin the k value in FACT KG.\nk FactKG\nSupported Refuted\n3 1.93 1.08\n5 2.52 1.39\n10 3.13 1.86\nTable 12: The average number of retrieved triples\nchanges according to the change in the k value in FAC-\nTKG.\nMethod k MetaQA\n1hop\nMetaQA\n2hop\nMetaQA\n3hop\n1 97.0 93.4 89.4\nKG-GPT 3 96.3 94.4 94.0\n5 96.7 95.0 93.7\nTable 13: Performance changes according to the change\nin the k value in MetaQA.\nk MetaQA\n1-hop 2-hop 3-hop\n1 2.06 4.43 3.25\n3 2.16 4.53 3.59\n5 2.12 4.51 3.65\nTable 14: The average number of retrieved triples\nchanges according to the change in the k value in\nMetaQA.\n9420\nMotivation\nPractical Cognitive Intrinsic Fairness\n4.1 4.2\nGeneralisation type\nCompositional Structural Cross Task Cross Language Cross Domain Robustness\n4.1 4.2 4.1 4.2\nShift type\nCovariate Label Full Assumed\n4.1 4.2\nShift source\nNaturally occuring Partitioned natural Generated shift Fully generated\n4.1 4.2\nShift locus\nTrain–test Finetune train–test Pretrain–train Pretrain–test\n4.1 4.2\nTable 15: GenBench Evaluation Card from Hupkes et al. (2022).\n9421"
}