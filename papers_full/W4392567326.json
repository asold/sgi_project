{
    "title": "SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture",
    "url": "https://openalex.org/W4392567326",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3092181562",
            "name": "Martin Vastl",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A5038140870",
            "name": "Jonáš Kulhánek",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A2082903515",
            "name": "Jiří Kubalík",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A2769456950",
            "name": "Erik Derner",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        },
        {
            "id": "https://openalex.org/A4214801287",
            "name": "Robert Babuška",
            "affiliations": [
                "Czech Technical University in Prague"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6810553760",
        "https://openalex.org/W6811216683",
        "https://openalex.org/W3206768895",
        "https://openalex.org/W6746179004",
        "https://openalex.org/W6793494230",
        "https://openalex.org/W1979769287",
        "https://openalex.org/W3017946098",
        "https://openalex.org/W2023140507",
        "https://openalex.org/W1991542861",
        "https://openalex.org/W2954765142",
        "https://openalex.org/W6773942789",
        "https://openalex.org/W3209138666",
        "https://openalex.org/W6796097400",
        "https://openalex.org/W6797795804",
        "https://openalex.org/W3164991976",
        "https://openalex.org/W4312384983",
        "https://openalex.org/W4289765664",
        "https://openalex.org/W2216139323",
        "https://openalex.org/W2991933648",
        "https://openalex.org/W2544914327",
        "https://openalex.org/W6728431463",
        "https://openalex.org/W6752296227",
        "https://openalex.org/W6795296736",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6755477022",
        "https://openalex.org/W6810210838",
        "https://openalex.org/W6674330103",
        "https://openalex.org/W6726497184",
        "https://openalex.org/W6771324808",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W2061011797",
        "https://openalex.org/W1998329127",
        "https://openalex.org/W1519072852",
        "https://openalex.org/W2502655619",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4323050423",
        "https://openalex.org/W1576818901",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W4293471604",
        "https://openalex.org/W2963672746",
        "https://openalex.org/W4245407374",
        "https://openalex.org/W4287996090",
        "https://openalex.org/W3106411828",
        "https://openalex.org/W1511336021",
        "https://openalex.org/W4286890432",
        "https://openalex.org/W4293842096",
        "https://openalex.org/W3149220446",
        "https://openalex.org/W3175526646",
        "https://openalex.org/W3162815236",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2995359496",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4327681667",
        "https://openalex.org/W3166835815",
        "https://openalex.org/W4226500500"
    ],
    "abstract": "&lt;p&gt;Many real-world systems can be naturally described by mathematical formulas. The task of automatically constructing formulas to fit observed data is called symbolic regression. Evolutionary methods such as genetic programming have been commonly used to solve symbolic regression tasks, but they have significant drawbacks, such as high computational complexity. Recently, neural networks have been applied to symbolic regression, among which the transformer-based methods seem to be most promising. After training a transformer on a large number of formulas, the actual inference, i.e., finding a formula for new, unseen data, is very fast (in the order of seconds). This is considerably faster than state-of-the-art evolutionary methods. The main drawback of transformers is that they generate formulas without numerical constants, which have to be optimized separately, yielding suboptimal results. We propose a transformer-based approach called SymFormer, which predicts the formula by outputting the symbols and the constants simultaneously. This helps to generate formulas that fit the data more accurately. In addition, the constants provided by SymFormer serve as a good starting point for subsequent tuning via gradient descent to further improve the model accuracy. We show on several benchmarks that SymFormer outperforms state-of-the-art methods while having faster inference.&lt;/p&gt;",
    "full_text": "Date of publication XXXX XX, 2023, date of current version XXXX XX, 2023.\nDigital Object Identifier 10.1109/ACCESS.2023.DOI\nSymFormer: End-to-End Symbolic\nRegression Using Transformer-Based\nArchitecture\nMARTIN VASTL1,2, JONÁŠ KULHÁNEK1,3, JI ˇRÍ KUBALÍK1, ERIK DERNER1, and ROBERT\nBABUŠKA1,4 (Member, IEEE)\n1Czech Institute of Informatics, Robotics, and Cybernetics, Czech Technical University in Prague, Prague, 16000, Czech Republic\n(e-mail: {martin.vastl, jonas.kulhanek, jiri.kubalik, erik.derner}@cvut.cz)\n2Faculty of Mathematics and Physics, Charles University, Prague, 12116, Czech Republic\n3Faculty of Electrical Engineering, Czech Technical University in Prague, Prague, 16000, Czech Republic\n4Department of Cognitive Robotics, Delft University of Technology, Delft, 2628 CD, The Netherlands (e-mail: r.babuska@tudelft.nl)\nCorresponding author: Erik Derner (e-mail: erik.derner@cvut.cz).\nThis work was supported by the European Regional Development Fund under the project Robotics for Industry 4.0\n(reg. no. CZ.02.1.01/0.0/0.0/15_003/0000470) and by the Ministry of Education, Youth and Sports of the Czech Republic through the\ne-INFRA CZ (ID:90140).\nABSTRACT Many real-world systems can be naturally described by mathematical formulas. The task\nof automatically constructing formulas to fit observed data is called symbolic regression. Evolutionary\nmethods such as genetic programming have been commonly used to solve symbolic regression tasks,\nbut they have significant drawbacks, such as high computational complexity. Recently, neural networks\nhave been applied to symbolic regression, among which the transformer-based methods seem to be most\npromising. After training a transformer on a large number of formulas (in the order of days), the actual\ninference, i.e., finding a formula for new, unseen data, is very fast (in the order of seconds). This is\nconsiderably faster than state-of-the-art evolutionary methods. The main drawback of transformers is\nthat they generate formulas without numerical constants, which have to be optimized separately, yielding\nsuboptimal results. We propose a transformer-based approach called SymFormer, which predicts the\nformula by outputting the symbols and the constants simultaneously. This helps to generate formulas\nfitting the data more accurately. In addition, the constants provided by SymFormer serve as a good starting\npoint for subsequent tuning via gradient descent to further improve the model accuracy. We show on\nseveral benchmarks that SymFormer outperforms state-of-the-art methods while having faster inference.\nINDEX TERMS Symbolic regression, neural networks, transformers.\nI. INTRODUCTION\nMany systems in various fields ranging from industrial\nprocesses to social sciences can be described by mathematical\nformulas. Knowing the governing equations of a nonlinear\nsystem provides insight into the system’s inner workings and\nit also allows us to predict how the system will behave in the\nfuture. Deriving mathematical models from first principles\nis often tedious and for some systems even impossible. In\nsuch a case, methods to automatically construct formulas\nfitting the data observed on the system can be used. The\ntask of finding such a formula from the observed data is\ncalled symbolic regression (SR). This method has already\nbeen applied to a variety of real-world problems, e.g., in\nphysics [1], [2], robotics [3], [4], or machine learning [5].\nIn the past decades, symbolic regression tasks [6], [7]\nwere commonly solved by means of genetic programming\n[8]–[12]. However, discovering formulas in this way is\nslow and computationally expensive. For each SR instance,\nan entire population of formulas has to be evolved and\nevaluated repeatedly through many generations. In recent\nyears, approaches based on neural networks emerged [13]–\n[16]. Among them, the most efficient ones are methods that\ntrain a transformer model on a large collection of data and the\ncorresponding formulas [15], [16]. The transformer uses the\ndata to autoregressively generate formulas by predicting each\nsymbol conditioned on the previously generated symbols.\nThe expressions are decoded without constants, i.e., all\nconstants are replaced by a special symbol, and are sought\nVOLUME XX, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\nafterward, using global optimization [15], [16]. However,\nthe values of the constants have a large impact on how\nwell the generated function fits the data. Without generating\nthe constants simultaneously with the equations, the model\nwill not represent the data well. For example, consider a\nsimple model returning a sum of sine and cosine functions\nwith increasing frequencies. Since every smooth function\ncan be approximated well using the Fourier basis, one can\nrepresent functions with a small error by merely changing\nthe constants.\nInspired by [17], where a similar idea was applied to the\nproblem of recurrent sequences, we propose SymFormer,\na novel transformer-based architecture trained end-to-end\non hundreds of millions of formulas. Our work makes the\nfollowing contributions:\n1) SymFormer generates a symbolic representation of the\nformula, including the numerical values of the constants.\nThis allows the symbolic decoder to use the constants\ngenerated so far and so to improve the model’s accuracy.\n2) We use the generated constants to initialize the local\ngradient search to fine-tune the final constants.\n3) Our approach is thoroughly evaluated and compared to\nrelevant alternative methods on a large set of univariate\nand bivariate functions. We also study the effect of\nusing different constant encoding methods.\n4) The source code and the pre-trained model checkpoints\nare publicly available at https://github.com/vastlik/\nsymformer.\nII. RELATED WORK\nGenetic Programming approachesare a traditional way\nof solving SR [8], [18], [19]. Genetic programming (GP)\nevolves expressions encoded as trees using selection,\ncrossover, and mutation. The drawbacks of GP-based ap-\nproaches are that they evolve each equation from scratch,\nwhich is slow, and that the models tend to increase in\ncomplexity without much performance improvement [20],\n[21]. GP is also inefficient in fine-tuning the constants only\nby using genetic operators [22], [23].\nNeural Network approachescan be generally divided\ninto three categories. The first one includes approaches based\non the Equation learner (EQL) [24]–[26]. The idea behind\nEQL is to find a function f(x) = y by training a neural\nnetwork on x as the input and y as the output. By means\nof regularization, the neural network is forced to use as few\nnetwork weights as possible. Elementary functions ( sin, log,\n. . . ) are used as activation functions, and after the training,\nthey are read from the network with the corresponding\nweights. A limitation of the EQL-like approaches is that\none has to design a strategy to force convergence towards a\nsparse neural network model representing a compact analytic\nformula. Typically, such a strategy implements a trade-off\nbetween neural network accuracy and sparsity. Lastly, these\napproaches can be slow as they need to find each equation\nfrom scratch.\nThe second set of approaches is based on training a\nrecurrent neural network (RNN) using reinforcement learning\n[13]. The idea is to let the RNN generate the equation and\nthen calculate the reward as an error between the ground-\ntruth f(x) values and the values from the predicted function\nˆf(x). An interesting extension is proposed in [14], where\nthe RNN is used to sample an initial population for a genetic\nalgorithm. A limitation of both of these approaches is that\nthe model does not predict the constants, which have to\nbe found through nonlinear optimization, slowing down the\nwhole training loop and limiting the achievable accuracy of\nthe model [13], [14].\nA transformer-based approach was introduced in [15]–[17],\nwhere a large amount of training data is generated and used\nto train a transformer [27] in a supervised manner. A similar\napproach is proposed in [16], where the GPT-2 [28] model\nis trained on the input-output data with the corresponding\nsymbolic expression as the output. Global optimization finds\nthe constants for each equation. In [15], the encoder from\nthe Set Transformer [29] and the decoder from the original\ntransformer architecture [27] are used. Similar to [16], the\nmodels are trained only on skeletons (expressions without\nconstants), and afterward, the constants are fitted using global\noptimization. An approach where the transformer directly\ngenerates the constants is introduced in [17]. The constants\nare predicted jointly by encoding them into the symbolic\noutput. Integers are represented through the base- b direct\nencoding and the IEEE 754 float representation is used with\nthe mantissa rounded to the four most significant digits. New\ntokens are introduced to represent exponents. A disadvantage\nof this approach is that the mantissa has a finite precision.\nThe authors also observed that when approximating difficult\nfunctions, the symbolic model typically only predicts the\nlargest terms in its asymptotic expansion. This method was\nfurther extended in [30] where the constants found by the\nmodel serve as initial values for global optimization to further\nimprove the accuracy.\nIII. METHOD\nThe symbolic regression task can be formulated as finding an\nunknown function f given a finite set of input points along\nwith the corresponding outputs. The goal is to construct a\nfunction (mathematical formula) ˆf minimizing the squared\ndifference between the function’s output on the input points\nand the outputs of the unknown function f. In our work, we\nfocus on univariate and bivariate functions.\nGiven a set of observed input-output pairs, the model\ngenerates the structure of the formula together with the\nvalues of all constants present in the formula in a single\nforward pass of the transformer model [27]. This is visualized\nin Figure 1. The input-output pairs are concatenated into a\nsequence which is processed by the transformer’s encoder.\nThe transformer’s decoder attends to the encoder’s resulting\nsequence, and autoregressively predicts the formula as a\nsequence of individual symbols in the formula and the\ncorresponding constants. In fact, the decoder models the\n2 VOLUME XX, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n(x 1 ,y 1 ), ..., (x n ,y n )\nEncoder\nInternal \nFunction\nRepresentation\nDecoder\nSelect equat ion w ith a m in imal\nerr or on input po ints\nMultiple \ncandidate \nequations\nMultiple \ncandidate \nequations\n*\n*\nsin\nsin\n0.0\n0.0\n0.0\n0.0\n0.3\n0.2\n0.0\n0.0\nC0\nC0\nx\nx\nLOCAL  GRADIENT SEARCH\nFor each predicted equation\n1\n2\n3\nsin(0.2*x)\n4\nFIGURE 1: Schematic diagram of SymFormer inference. The input-output data are passed through the transformer, generating\nseveral candidate equations using Top-K sampling. These candidates are further improved using gradient descent. The final\nequation is then selected by minimizing the mean squared error.\nN\nAdd & Norm\nLinear\nDr opout\nMAB\nMAB\nI\nCoefficient embedding\nP oint s embedding\nSymbolic embedding\nConcat\nCoefficient s\nP oint s\nSymbolic input Coefficient input\nP ositional\nembedding\nSymbolic output\nLinear\nSoftmax\nMAB\nF eed\nF or war d\nAdd & Norm\nMulti-Head\nA tt ention\nAdd & Norm\nMask ed\nMulti-Head\nA tt ention\nF eed\nF or war d\nS\nk outputs\nX\nY\nIS AB\nPMA\nM\nF eed\nF or war d\nHMulti-Head\nA tt ention\nMAB Detail\nDecoderEncoder\nFIGURE 2: Schematic overview of the SymFormer architec-\nture. MAB refers to Multihead Attention Block, ISAB is the\nInduced Set Attention Block, and PMA stands for Pooling\nby Multihead Attention.\nprobability distribution over the next symbol and the value\nof the constant, given the expression prefix and the associated\nvalues of the constants.\nA. MODEL ARCHITECTURE\nThe SymFormer model architecture is depicted in Figure 2.\nFollowing [27], our model consists of an encoder and a\ndecoder. The encoder takes as its input the data points (input-\noutput pairs) sampled from the unknown formula. Each data\npoint is first passed through an affine layer to increase the\ndimensionality to the hidden size of the transformer. The\nresulting vectors are then concatenated to build the sequence\nthat the transformer encoder processes. The order of these\nvectors is not important since the positional encodings are\nnot used, and, therefore, the encoder treats each vector\nindependently of its position in the sequence. We pass\nthe sequence of hidden vectors through four induced set\nattention blocks [29], each of which consists of two cross-\nattention layers and two traditional feed-forward blocks [27].\nFollowing the standard practice in transformer architectures\n[27], for all cross-attention layers and feed-forward blocks,\nwe pairwise sum the output with the layer’s input before\npassing it through a normalization layer [31].\nThe first cross-attention uses the hidden vectors as its\nkeys and values and a set of trainable vectors as the queries.\nThe resulting sequence will have the same length as the\nnumber of these trainable query vectors. Note that each\nencoder block has its own independent set of query vectors.\nThe resulting sequence is passed through the first feed-\nforward block. The second cross-attention layer then uses the\npreviously generated sequence as the keys and values, and it\nuses the original hidden sequence (the input to the first cross-\nattention) as its queries, which results in the same sequence\nlength as the length of the original input. The second cross-\nattention is followed by the second feed-forward block and\na dropout layer [32]. Finally, to get a representation of the\ninput independent of the number of data points, we compute\nthe cross-attention between the output of the last block and\na set of trainable vectors. Similarly, as in the individual\nencoder blocks, this cross-attention fixes the sequence length\nto the number of trainable query vectors.\nThe input to the decoder is a sequence of trainable\npositional embeddings that are summed with embedded\nground truth output symbols and concatenated with affine-\nVOLUME XX, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\nprojected ground-truth constants. The decoder then passes\nthe sequence through four standard transformer blocks [27],\nconsisting of a self-attention layer, a cross-attention between\nthe hidden sequence and the encoder’s output, and a feed-\nforward block. Two heads are applied to the output of the\nlast block: a classification head outputting the symbols and\na regression head outputting the values of constants.\nB. TRAINING AND INFERENCE\nThe model is trained to predict the next symbol and the\nassociated value of a constant given the previous symbols\nin the expression. Thanks to the transformer architecture\n[27], the model can be optimized for all symbols in the\nsequence with linear overhead. The classification head is\ntrained using the cross-entropy loss Lclass with the symbols\nas the target. The regression head is trained using the mean\nsquared error (MSE) LMSE to output the value of the constant\nif the associated symbol is a constant. The regression loss\nis masked such that it is optimized only on positions where\nthe associated symbols are the constants. The total loss L is\na weighted sum of the two losses:\nL = Lclass + λLMSE, (1)\nwhere λ is a hyperparameter. At the beginning of the training,\nwe set λ to zero, and after approximately 97 700 gradients\nsteps, we gradually increase it using the cosine schedule\n[33]. During training, we also found it beneficial to add a\nsmall random noise sampled from N(0, σ2) to the constants\nbecause the constants are not always precise during the\ninference. The parameter σ is also decreased according to\nthe cosine schedule.\nAt inference time, we encode the set of input-output pairs\nusing the encoder. The decoder then autoregressively decodes\na sequence of symbols and constants one at a time by first\nsampling from the categorical distribution of the symbol\nclassification head and then taking the prediction from the\nregression head if the predicted symbol was a constant. This\nprocess is terminated when the model reaches the end-of-\nsequence (EOS) token. In practice, we sample not one but\nmultiple independent sequences. For all these sequences, we\nfine-tune the values of all constants by minimizing the MSE\non the observed data samples, see Section III-C. Finally, we\nselect the formula with the lowest error after the optimization.\nC. CONSTANTS FINE-TUNING\nTo optimize the constants in the predicted expressions, we\nuse a stochastic gradient descent (SGD) method to minimize\nthe MSE between the function prediction and the observed\noutputs in the training data. For each expression, we initialize\nthe gradient descent procedure with the learning rate of 0.001\nand the momentum of 0.9. Next, we run a gradient-descent\niteration while clipping the gradient norm to 10 and calculate\nthe MSE with the newly found constants. If the loss does not\ndecrease by at least 0.1 %, we divide the learning rate by 10.\nThis process continues until the loss stops decreasing, i.e.,\nit does not improve by at least 0.1 %for five consecutive\niterations.\nThis process and its parameters were inspired by classical\noptimization, balancing the trade-off between convergence\nrate and performance. The SGD optimization process starts\nwith the constant values predicted by the model and aims\nto fine-tune these constants rather than finding significantly\ndifferent ones. In our empirical evaluation, the optimization\nhas shown to be efficient thanks to the initial constant\nvalues being closely aligned with the desired ones. After\nthe constants are optimized for all predicted expressions, we\nselect the formula with the lowest error.\nD. EXPRESSION ENCODING\nTechnically, we encode each expression as a sequence\nof symbols and a sequence of real values. The symbols\ncorrespond to different mathematical operators, e.g., + for\naddition, · for multiplication, etc., expression’s variables,\ne.g., x, y, or expression’s constants – numbers occurring in\nthe expression. All possible symbols constitute the model’s\nvocabulary. The constants are encoded using a scientific-like\nnotation where a constant α is represented as a tuple of the\nexponent ce and the mantissa cm:\nα ≈ cm · 10ce, c e = ⌈log10 α⌉, c m = α\n10ce\n. (2)\nIn this representation, the mantissa is in the range [−1, 1], and\nthe exponent is an integer. The integer exponent is encoded\nas a special symbol starting with ‘C’ and followed by the\nexponent ce. The mantissa is kept as the real number (it is\noptimized using the regression loss). In order to transform\nthe expression tree into this representation, we flatten the\ntree using the preorder traversal [34]. For example, the\nexpression 0.017 · x + 1781.5 has symbols [+, ·, x, C-1,\nC4] and constants [0, 0, 0, 0.17, 0.17815]. To further help\nthe model represent common integers, we add all integers\nfrom the interval [−5, 5] to the model vocabulary. In contrast\nto the approach in [17], which is able to express constants\nonly up to the four most significant digits, our approach\nachieves full float precision.\nIV. EXPERIMENTS\nThis section describes our training setup and the metrics that\nwe used to demonstrate the model’s ability to predict the\nformulas and compare our model to relevant approaches\nfrom the literature. We also present an ablation study\ncomparing different encodings and their impact on the\nmodel’s performance. In our experiments, we refer to the\nmodel trained only on univariate functions as the Univariate\nSymFormer and the model trained on both the univariate\nand bivariate functions as the Bivariate SymFormer.\nTo train and test our method, we have generated two\ndatasets: one containing 130 million univariate functions\nand another one containing 100 million bivariate functions.\nThe univariate and bivariate test datasets contain 10 000\nrandomly sampled equations each. To generate the formulas,\n4 VOLUME XX, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n(a) GT: (1 +x−2)−0.5, Pred: sin(|atan(x)|)\n−10 −5 0 5 10\n0.00\n0.25\n0.50\n0.75\n1.00\nGT\nPred (b) GT: 7.7 + 3 ln(x), Pred: 3 ln(x) + 7.9\n0 2 4 6 8 10\n−5\n0\n5\n10\n15\nGT\nPred\n(c) GT: −7.46 − 0.8x + x cos(tan(x)),\nPred: −7.7 − x + x cos(tan(x))\n−10 −5 0 5 10\n−20\n−10\n0\n10\nGT\nPred (d) GT: 0.2 cos(4x), Pred: asin(0.2 cos(4x))\n−10 −5 0 5 10\n−0.2\n−0.1\n0.0\n0.1\n0.2\nGT\nPred\n(e) GT: −60.9 · x · exp(−x),\nPred: 0.002x3 − 61.2 · x · exp(−x)\n0 2 4 6 8 10\n−20\n−10\n0\nGT\nPred (f) GT: 0.34x + (((x)2) + sin (0.96 +x),\nPred: x2 + cos(x) + atan(x)\n−10 −5 0 5 10\n0\n25\n50\n75\n100\nGT\nPred\n(g) GT: 7 · log (5 + 2x2),\nPred: 7 · log (5 + 2x2)\n−10 −5 0 5 10\n10\n20\n30\nGT\nPred (h) GT: −4x − x tan (cos (5x)),\nPred: −4x − x tan (cos (5x))\n−10 −5 0 5 10\n−50\n−25\n0\n25\n50\nGT\nPred\nFIGURE 3: Examples of model predictions from the Univariate SymFormer. The shaded area represents the training range.\n‘GT’ denotes the ground truth and ‘Pred’ is the model prediction.\nVOLUME XX, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n(a) GT: 5x + x2 − x cos(x2 + xy), Pred: 5x + x2 + x cos(x2 + xy + 3.14)\nx\n−10\n−5\n0\n5\n10\ny\n−10\n−5\n0\n5\n10\n−100\n−50\n0\n50\n100\nx\n−10\n−5\n0\n5\n10\ny\n−10\n−5\n0\n5\n10\n−100\n−50\n0\n50\n100\nFIGURE 4: Example of model prediction using the Bivariate SymFormer. The inputs x and y were sampled from the range\n[−5, 5]×[−5, 5]. ‘GT’ denotes the ground truth and ‘Pred’ is the model prediction. We visualize the range [−10, 10]×[−10, 10]\nto demonstrate the extrapolation abilities.\nwe modified the algorithm described in [34] with a maximum\nof ten operators. Then, we sampled uniformly at random\n100 points (200 for bivariate functions) from the interval\n[−5, 5]. The interval was reduced if the function was not\ndefined on the full interval.\nA. TRAINING & EVALUATION\nWe train our model using the Adam optimizer [35] for\n3 epochs on 8 NVIDIA A100 GPUs. The training of the\nmodel takes roughly 33 hours. We use a training schedule\nsimilar to the original transformer [27]. However, we divide\nthe learning rate by five since the training often diverges when\nusing the original learning rate. The regression parameter λ\nis set according to the cosine schedule and delayed for 97 700\ngradient steps, reaching 1.0 at the end of the training 1. The\nrandom noise is sampled from N(0, ϵ) where ϵ is initially set\nto 0.1 and decreased to zero during training using the same\nschedule. The same setup is used both for the Univariate\nand Bivariate SymFormer.\nThe hyperparameters were chosen empirically based on\nseveral trial-and-error experiments. Systematic hyperparame-\nter tuning is impractical for a transformer application of this\ncomplexity. We discovered that using the proposed schedule\nis essential, as the model needs sufficient precision in the\nsymbolic representation in order for the constant tuning to\nbe meaningful. Without the schedule, the learning process\ndid not converge. However, we found that the schedule is not\nparticularly sensitive to the exact setting of its parameters.\nTo evaluate the SymFormer’s performance, we use 1024\ntest equations, and for each of them (if not stated otherwise),\nwe generate 256 candidate equations using Top-K sampling\nwith K = 16and further improve them using a local gradient\n1The regression parameter λ and the learning rate were updated every\n106/1024 ≈ 977 gradient steps (batch size was 1024).\nsearch with early stopping. The best candidate equation is\nthen selected based on the lowest error on the input points.\nTo be able to compare with previous approaches, we used\nthe same metrics and hyperparameters used in these methods,\nnamely the percentage of close points (with atol=0.001\nand rtol=0.05) [15], the recovery rate with tolerance ( RR)\nintroduced in [15] – percentage of equations for which at\nleast 95 %of points fall within a tolerance (atol=0.001 and\nrtol=0.05), the coefficient of determination ( R2) [36], and\nthe average time of prediction (in seconds).\nB. IN-DOMAIN PERFORMANCE\nFigures 3 and 4 demonstrate SymFormer’s ability to predict\nformulas successfully, showing several plots of the model’s\npredictions. The Univariate SymFormer achieved an R2 of\n0.9995, and when we used the local gradient search, the\nperformance further improved to R2 = 1. The Bivariate\nSymFormer achieved an R2 of 0.9996 and R2 = 1 when\nthe local gradient search was employed.\nC. PERFORMANCE BASED ON THE NUMBER OF\nSAMPLES\nThe transformer used a fixed number of points during training.\nThe trained model, however, is robust to the change in the\nnumber of input points. To demonstrate it, we performed\nan experiment where we varied the number of input points\nduring inference. While the model was trained on 100 points,\nwe observe only 2.9 %and 6.5 %decrease in performance\nwhen we use 50 and 20 points respectively, and 3.6 %drop\nin performance when we increase to 1 000points.\nD. COMPARISON TO ALTERNATIVE APPROACHES\nTo compare our results to state-of-the-art approaches, we use\nthe Nguyen [37], R rationals [38], Livermore, [13], Keijzer\n6 VOLUME XX, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n0 20 40 60 80 ≥ 100\nRelative Prediction Error (%)\n10−3\n10−2\n10−1\n100\nRelative Frequency (%)\nDistribution of Relative Prediction Errors for 1D Functions\n0 20 40 60 80 ≥ 100\nRelative Prediction Error (%)\n10−3\n10−2\n10−1\n100\nRelative Frequency (%)\nDistribution of Relative Prediction Errors for 2D Functions\nFIGURE 5: Relative prediction errors on all benchmark functions reported in this paper. The logarithmic vertical axis\nrepresents the frequency of the error represented by the given bin.\nTABLE 1: Comparison of SymFormer with state-of-the-art methods on various benchmarks. SymFormer uses Top-K sampling\nwith K = 20while generating 1024 samples and improving them using local gradient search with early stopping. We report\nR2 and the average time to generate an equation.\nSymFormer NSRS [15] DSO [14]\nBenchmark R2 ↑ Time (s)↓ R2 ↑ Time (s)↓ R2 ↑ Time (s)↓\nNguyen 0.99998 47.50 0.96744 169.46 0.99297 140.25\nR 0.99986 94.33 1.00000 95.67 0.97488 855.33\nLivermore 0.99996 43.00 0.88551 193.09 0.99651 276.32\nKoza 1.00000 101.00 0.99999 111.50 1.00000 217.50\nKeijzer 0.99904 48.67 0.97392 255.50 0.95302 3929.50\nConstant 0.99998 90.88 0.88742 230.38 1.00000 2816.19\nOverall avg. 0.99978 52.95 0.92901 199.63 0.99443 326.53\n[39], Koza [40], and Constant benchmarks. We strive to\nmake the comparison as fair as possible given the following\nlimitations. The first one is that some methods use a restricted\nvocabulary and thus have a smaller search space giving them\nan advantage over our method. The second problem arises\nfrom the different number of training input points and the\nranges they are sampled from.\nWe compare our approach to two state-of-the-art ap-\nproaches: the transformer-based Neural Symbolic Regression\nthat Scales (NSRS) [15], which is a pre-trained transformer\nmodel, and the RL-based Deep Symbolic Optimization\n(DSO) [14], trained for each equation from scratch.\nWe use Top-K sampling with K = 20and 1024 candidate\nequations with early stopping for the benchmark which in\ncomparison to previously used K = 16and 256 candidate\nequations offers larger exploration at the cost of an increased\ninference time. The results in Table 1 show that the\nSymFormer is competitive in terms of model performance\non all of the benchmarks while outperforming both NSRS\nand DSO in the time required to find the equation.\nFurthermore, to demonstrate the Bivariate SymFormer’s\nperformance on both the univariate and bivariate functions,\nwe evaluated a single model on all univariate and bivariate\nTABLE 2: Comparison of SymFormer and Bivariate Sym-\nFormer performance on all benchmarks. The (Bivariate)\nSymFormer uses Top-K sampling with K = 20 while\ngenerating 1024 samples and improving them using local\ngradient search with early stopping. We report R2 and the\naverage time to generate an equation.\nSymFormer Bivariate SymFormer\nBenchmark R2 ↑ Time (s)↓ R2 ↑ Time (s)↓\nNguyen 0.99998 47.50 0.99996 139.46\nR 0.99986 94.33 0.99985 418.67\nLivermore 0.99996 43.00 0.99992 170.00\nKoza 1.00000 101.00 1.00000 81.50\nKeijzer 0.99904 48.67 0.99884 250.66\nConstant 0.99998 90.88 0.99997 188.50\nOverall avg. 0.99978 52.95 0.99946 174.32\nfunctions using the same benchmark. Note that the bench-\nmark functions are mostly univariate. In Table 2, we can\nnotice only a slight drop in performance. However, the\naverage inference time increased, which could be explained\nby the larger search space the model needed to handle during\nVOLUME XX, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n0 20 40 60 80\nDistance from the sampling range\n0.78\n0.80\n0.82\n0.84\n0.86\nPercentage of close predictions\n0 20   40   60  80 \nDistance from the training range\n0.9999965\n0.9999970\n0.9999975\n0.9999980\n0.9999985\nR2\nFIGURE 6: The SymFormer’s out-of-domain performance.\nthe optimization of constants.\nFigure 5 shows the histogram of relative prediction errors,\ncalculated as an absolute value of the difference between the\nmodel output and the ground truth, divided by the absolute\nvalue of the ground truth plus a constant of 10−8. The\nrelative prediction errors were calculated on all benchmark\nfunctions listed in Section IV-D. For each of the functions,\n10 000 points were sampled uniformly from their domains.\nThe results indicate that most of the errors are smaller than\n10 %.\nE. OUT-OF-DOMAIN PERFORMANCE\nA desirable property of the algorithm is its ability to predict\ncorrect values outside the training data range. To test it, we\nfirst run the inference on the points sampled from the training\nrange and then evaluate these predicted functions on points\noutside of the training range. More formally, we calculate\nthe metrics on the function values for points sampled from\nthe set {x ∈ R | 5 < |x| < 5 +d}, where d is the maximal\ndistance. The effect of the distance on the percentage of\nclose predictions and R2 can be seen in Figure 6. As we can\nsee from the figure, the model still performs exceptionally\nwell even for larger distances from the sampled domain. This\ndemonstrates that the model generalizes outside the sampled\ndomain and fine-tuning the constants using the local gradient\nsearch does not hamper the extrapolation capabilities.\nTABLE 3: Comparison of expression encoding strategies and\nLGS. SymFormer uses both extended encoding and LGS.\nWe report R2, RR, and the percentage of close predictions.\nThe base encoding refers to the case when no preprocessing\nfor the constants is employed. BFGS init refers to a situation\nwhen the predicted constants serve as a starting point for\nthe BFGS [41]. LGS refers to the case when the gradient\nsearch was used to find or improve the constants further.\nModel GS R2 ↑ RR ↑ % of close pred. ↑\nNo constants + BFGS ✓ 0.9929 30.40 55.52\nBase encoding ✗ 0.9979 31.23 50.62\nBase encoding + LGS ✓ 1.0000 53.78 69.09\nExtended encoding ✗ 0.9995 48.99 70.40\nExtended encoding + BFGS init ✓ 0.9998 48.37 71.84\nSymFormer ✓ 1.0000 68.29 84.87\nF. STUDY OF CONSTANT ENCODINGS\nIn order to analyze the effectiveness of our constants’ repre-\nsentation, we conduct a study comparing the representation\nto other alternative options. In particular, we consider the\nfollowing variants:\na) ‘No constants + BFGS’ : For each constant in the\nexpression, the transformer outputs a special symbol\nC. At inference, after decoding a final expression,\nthe values of all expression’s constants (all symbols\nC) are found using a posteriori global optimization\n(Broyden–Fletcher–Goldfarb–Shanno algorithm, BFGS).\nThis setup is the same as in [15].\nb) ‘Base encoding’: The model outputs C for all constants\nand regression head is used to predict the constant’s\nvalues.\nc) ‘Base decoding + LGS’ : Same training as ‘Base\nencoding’, however, at inference, we fine-tune the initial\nestimates of the constants’ values with a local gradient\nsearch (LGS).\nd) ‘Extended encoding’: Setup described in Section III-D –\nwe split the constants into exponents and mantissas and\nuse special symbols for different constants’ exponents.\nThe mantissas are outputted by the regression head. At\ninference, we do not fine-tune the constants’ values\nafter they are decoded.\ne) ‘Extended encoding + BFGS init’ : The training is\nthe same as for the ‘Extended encoding’, however, at\ninference, we throw away the initial estimates of the\nconstants’ values obtained from the model and run the\nBFGS from scratch to get the constants’ values.\nf) ‘SymFormer’: The final model. We use the exponent-\nmantissa encoding and at inference, we optimize the\nvalues of the constants using an LGS initialized from\nthe decoded values.\nFrom the results in Table 3, we can see that using the\nconstants improves the model’s performance in terms of\nR2, the recovery rate with tolerance, and also in terms\n8 VOLUME XX, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\nof the percentage of close predictions. One can therefore\nconclude that the performance of SymFormer in comparison\nto NSRS is better not because of a different dataset or a\nlarger model but because of the use of the constants during\ntraining. Furthermore, the last row shows the results for the\nextended encoding which uses a local gradient search to\nimprove the constants further. The extended encoding clearly\noutperforms the base encoding in terms of R2, the recovery\nrate with tolerance, and the percentage of close predictions.\nWe believe this to be the case because it is easier for the\nmodel to attend to previous symbolic tokens than to real\nvalues. Therefore, the model can make a more informed\ndecision when predicting the next symbol in the sequence.\nV. CONCLUSION\nTo tackle the problem of symbolic regression, we introduced\na novel transformer-based approach called SymFormer that\nuses a neural network trained on hundreds of millions of\nformulas. SymFormer is able to efficiently generate an\nanalytic expression of a previously unseen formula given\na set of input-output pairs. The model jointly predicts the\nstructure of the formula and the values of all of its constants\nin a single forward pass of a neural network. A local gradient\nsearch is used to improve the constants further. On most\nof the benchmarks, SymFormer outperforms the state-of-\nthe-art methods not only in terms of R2 but also in terms\nof the time required to find the underlying equation. We\nalso validated the importance of the proposed encoding of\nconstants. Finally, by evaluating SymFormer outside the\ntraining range, we demonstrated its remarkable extrapolation\ncapabilities.\nLimitations. A limitation of the deep-learning-based\narchitecture is that the number of dimensions and the\nsampling range are given by the training dataset, and there\nare no guarantees on the behavior outside the dataset’s\ndistribution. Even though the inference is fast and efficient,\nthe training uses a lot of compute and takes considerable\ntime on current hardware.\nWe have restricted our study to univariate and bivariate\nfunctions, most of which did not contain discontinuities or\nsingularities. However, the architecture and training method\ncan easily be extended to include functions of more variables,\nas well as discontinuous functions. These limitations could be\naddressed in future work. Furthermore, the method could be\nextended to model differential equations, as they have a large\nnumber of applications in physics and robotics. Analyzing\nthe latent space produced by the encoder may also prove\ninsightful.\nREFERENCES\n[1] D. Wadekar, L. Thiele, F. Villaescusa-Navarro, J. C. Hill, M. Cranmer,\nD. N. Spergel, N. Battaglia, D. Anglés-Alcázar, L. Hernquist, and\nS. Ho, “Augmenting astrophysical scaling relations with machine learning:\napplication to reducing the Sunyaev-Zeldovich flux-mass scatter,” 2022.\n[Online]. Available: https://arxiv.org/abs/2201.01305\n[2] K. T. Matchev, K. Matcheva, and A. Roman, “Analytical modelling of\nexoplanet transit spectroscopy with dimensional analysis and symbolic\nregression,” 2021. [Online]. Available: https://arxiv.org/abs/2112.11600\n[3] J. Kubalík, E. Derner, J. Žegklitz, and R. Babuška, “Symbolic regression\nmethods for reinforcement learning,” IEEE Access, vol. 9, pp. 139 697–\n139 711, 2021.\n[4] D. Hein, S. Udluft, and T. A. Runkler, “Interpretable policies\nfor reinforcement learning by genetic programming,” CoRR, vol.\nabs/1712.04170, 2017. [Online]. Available: http://arxiv.org/abs/1712.04170\n[5] C. Wilstrup and J. Kasak, “Symbolic regression outperforms other models\nfor small data sets,” CoRR, vol. abs/2103.15147, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2103.15147\n[6] M. Schmidt and H. Lipson, “Distilling free-form natural laws from\nexperimental data,” Science, vol. 324, no. 5923, pp. 81–85, 2009. [Online].\nAvailable: https://www.science.org/doi/abs/10.1126/science.1165893\n[7] J. Kubalík, E. Derner, and R. Babuška, “Symbolic regression driven by\ntraining data and prior knowledge,” inProceedings of the 2020 Genetic and\nEvolutionary Computation Conference, 2020, pp. 958–966.\n[8] J. R. Koza, Genetic Programming: On the Programming of Computers by\nMeans of Natural Selection. Cambridge, MA, USA: MIT Press, 1992.\n[9] M. Schmidt and H. Lipson, “Distilling free-form natural laws from\nexperimental data,” Science, vol. 324, no. 5923, pp. 81–85, 2009.\n[10] N. Staelens, D. Deschrijver, E. Vladislavleva, B. Vermeulen, T. Dhaene, and\nP. Demeester, “Constructing a no-reference H.264/A VC bitstream-based\nvideo quality metric using genetic programming-based symbolic regression,”\nIEEE Trans. Cir. and Sys. for Video Technol., vol. 23, no. 8, p. 1322–1333,\nAug. 2013.\n[11] I. Arnaldo, U.-M. O’Reilly, and K. Veeramachaneni, “Building predictive\nmodels via feature synthesis,” in Proceedings of the 2015 Annual Confer-\nence on Genetic and Evolutionary Computation, ser. GECCO ’15. New\nYork, NY , USA: Association for Computing Machinery, 2015, p. 983–990.\n[12] I. Bł ˛ adek and K. Krawiec, “Solving symbolic regression problems with\nformal constraints,” in Proceedings of the Genetic and Evolutionary\nComputation Conference, ser. GECCO ’19. New York, NY , USA: ACM,\n2019, pp. 977–984.\n[13] B. K. Petersen, “Deep symbolic regression: Recovering mathematical\nexpressions from data via policy gradients,” CoRR, vol. abs/1912.04871,\n2019. [Online]. Available: http://arxiv.org/abs/1912.04871\n[14] T. N. Mundhenk, M. Landajuela, R. Glatt, C. P. Santiago, D. M. Faissol,\nand B. K. Petersen, “Symbolic regression via neural-guided genetic\nprogramming population seeding,” CoRR, vol. abs/2111.00053, 2021.\n[Online]. Available: https://arxiv.org/abs/2111.00053\n[15] L. Biggio, T. Bendinelli, A. Neitz, A. Lucchi, and G. Parascandolo,\n“Neural symbolic regression that scales,” CoRR, vol. abs/2106.06427, 2021.\n[Online]. Available: https://arxiv.org/abs/2106.06427\n[16] M. Valipour, B. You, M. Panju, and A. Ghodsi, “SymbolicGPT: A generative\ntransformer model for symbolic regression,” CoRR, vol. abs/2106.14131,\n2021. [Online]. Available: https://arxiv.org/abs/2106.14131\n[17] S. d’Ascoli, P.-A. Kamienny, G. Lample, and F. Charton, “Deep symbolic\nregression for recurrence prediction,” in International Conference on\nMachine Learning. PMLR, 2022, pp. 4520–4536.\n[18] G. Dorgo, T. Kulcsar, and J. Abonyi, “Genetic programming-based\nsymbolic regression for goal-oriented dimension reduction,” Chemical\nEngineering Science, vol. 244, p. 116769, 2021.\n[19] R. Zhang, A. Lensen, and Y . Sun, “Speeding up genetic programming based\nsymbolic regression using gpus,” in Pacific Rim International Conference\non Artificial Intelligence. Springer, 2022, pp. 519–533.\n[20] P. Orzechowski, W. La Cava, and J. H. Moore, “Where are we now? A large\nbenchmark study of recent symbolic regression methods,” in Proceedings\nof the Genetic and Evolutionary Computation Conference , ser. GECCO\n’18. New York, NY , USA: Association for Computing Machinery, 2018, p.\n1183–1190. [Online]. Available: https://doi.org/10.1145/3205455.3205539\n[21] L. Trujillo, L. Muñoz, E. Galván-López, and S. Silva, “neat genetic\nprogramming: Controlling bloat naturally,” Information Sciences , vol.\n333, pp. 21–43, 2016. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S0020025515008038\n[22] M. Kommenda, B. Burlacu, G. Kronberger, and M. Affenzeller, “Parameter\nidentification for symbolic regression using nonlinear least squares,”\nGenetic Programming and Evolvable Machines, vol. 21, no. 3, pp. 471–501,\n2020.\n[23] L. Trujillo, P. S. Juárez-Smith, P. Legrand, S. Silva, M. Castelli, L. Van-\nneschi, O. Schütze, L. Muñoz et al. , “Local search is underused in\ngenetic programming,” in Genetic Programming Theory and Practice XIV.\nSpringer, 2018, pp. 119–137.\n[24] G. Martius and C. H. Lampert, “Extrapolation and learning equations,”\nCoRR, vol. abs/1610.02995, 2016. [Online]. Available: http://arxiv.org/abs/\n1610.02995\nVOLUME XX, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nVastl et al.: SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture\n[25] S. S. Sahoo, C. H. Lampert, and G. Martius, “Learning equations for\nextrapolation and control,” CoRR, vol. abs/1806.07259, 2018. [Online].\nAvailable: http://arxiv.org/abs/1806.07259\n[26] M. Werner, A. Junginger, P. Hennig, and G. Martius, “Informed\nequation learning,” CoRR, vol. abs/2105.06331, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2105.06331\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” CoRR, vol.\nabs/1706.03762, 2017. [Online]. Available: http://arxiv.org/abs/1706.03762\n[28] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[29] J. Lee, Y . Lee, J. Kim, A. R. Kosiorek, S. Choi, and Y . W. Teh, “Set\ntransformer,” CoRR, vol. abs/1810.00825, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1810.00825\n[30] P.-A. Kamienny, S. d’Ascoli, G. Lample, and F. Charton, “End-to-end\nsymbolic regression with transformers,” 2022. [Online]. Available:\nhttps://arxiv.org/abs/2204.10532\n[31] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[32] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,\n“Dropout: A simple way to prevent neural networks from overfitting,”\nJournal of Machine Learning Research, vol. 15, no. 56, pp. 1929–1958,\n2014. [Online]. Available: http://jmlr.org/papers/v15/srivastava14a.html\n[33] I. Loshchilov and F. Hutter, “SGDR: Stochastic gradient descent\nwith restarts,” CoRR, vol. abs/1608.03983, 2016. [Online]. Available:\nhttp://arxiv.org/abs/1608.03983\n[34] G. Lample and F. Charton, “Deep learning for symbolic mathematics,”\nCoRR, vol. abs/1912.01412, 2019. [Online]. Available: http://arxiv.org/abs/\n1912.01412\n[35] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nCoRR, vol. abs/1412.6980, 2014. [Online]. Available: https://arxiv.org/abs/\n1412.6980\n[36] S. Glantz and B. Slinker, Primer of Applied Regression & Analysis\nof Variance . McGraw-Hill Education, 2000. [Online]. Available:\nhttps://books.google.cz/books?id=fzV2QgAACAAJ\n[37] N. Q. Uy, N. X. Hoai, M. O’Neill, R. I. McKay, and E. Galvan-Lopez,\n“Semantically-based crossover in genetic programming: Application to\nreal-valued symbolic regression,” Genetic Programming and Evolvable\nMachines, vol. 12, no. 2, pp. 91–119, Jun. 2011.\n[38] K. Krawiec and T. Pawlak, “Approximating geometric crossover by\nsemantic backpropagation,” in Proceedings of the 15th Annual Conference\non Genetic and Evolutionary Computation, ser. GECCO ’13. New York,\nNY , USA: Association for Computing Machinery, 2013, p. 941–948.\n[Online]. Available: https://doi.org/10.1145/2463372.2463483\n[39] M. Keijzer, “Improving symbolic regression with interval arithmetic and\nlinear scaling,” in Genetic Programming, C. Ryan, T. Soule, M. Keijzer,\nE. Tsang, R. Poli, and E. Costa, Eds. Berlin, Heidelberg: Springer Berlin\nHeidelberg, 2003, pp. 70–82.\n[40] J. R. Koza, Genetic Programming II: Automatic Discovery of Reusable\nPrograms. Cambridge, MA, USA: MIT Press, 1994.\n[41] R. Fletcher, Practical Methods of Optimization, 2nd ed. New York, NY ,\nUSA: John Wiley & Sons, 1987.\nMARTIN VASTL received the B.Sc. in computer\nscience from the Czech Technical University in\nPrague and the M.Sc. in artificial intelligence from\nCharles University. He currently works in the\nfield of autonomous driving. His research interests\ninclude symbolic regression, language modeling,\nsequence-to-sequence models, and all areas of\nautonomous driving.\nJONÁŠ KULHÁNEK is currently pursuing his\nPh.D. in Computer Science under the supervision\nof Torsten Sattler. He holds a B.Sc. in Computer\nScience from the Czech Technical University in\nPrague and an M.Sc. from Charles University in\nArtificial Intelligence. His research focuses mainly\non implicit 3D scene representations and neural\nrendering, with additional interests in deep rein-\nforcement learning and transformer-based language\nmodels.\nDR. JI ˇRÍ KUBALÍK received the M.Sc. degree in\ncomputer science and the Ph.D. degree in artificial\nintelligence and biocybernetics from the Czech\nTechnical University in Prague in 1994 and 2001,\nrespectively. He is a senior researcher at CTU in\nPrague, the Czech Institute of Informatics, Robotics\nand Cybernetics. His research has mainly been\nfocused on various types of evolutionary compu-\ntation techniques and their applications to hard\noptimization problems. He is a (co-)author of more\nthan 30 papers in this area.\nDR. ERIK DERNER was awarded the Ph.D.\ndegree in Control Engineering and Robotics from\nthe Czech Technical University (CTU) in Prague,\nCzech Republic, in 2022. He received his M.Sc.\n(Hons.) degree in Artificial Intelligence and Com-\nputer Vision from CTU. His research interests\ninclude human-centric artificial intelligence, large\nlanguage models, robotics, sample-efficient model\nlearning, genetic algorithms, and computer vision.\nThe central topics in his research are currently the\nsafety, security, and ethical aspects of generative and conversational artificial\nintelligence.\nPROF . DR. ROBERT BABUŠKAreceived the\nM.Sc. (Hons.) degree in control engineering from\nthe Czech Technical University in Prague, in 1990,\nand the Ph.D. (cum laude) degree from Delft\nUniversity of Technology, the Netherlands, in 1997.\nHe has had faculty appointments with the Czech\nTechnical University in Prague and with the Electri-\ncal Engineering Faculty, TU Delft. Currently, he is\na full professor of Intelligent Control and Robotics\nat TU Delft, Faculty of Mechanical Engineering,\nDepartment of Cognitive Robotics. In the past, he made seminal contributions\nto the field of nonlinear control and identification with the use of fuzzy\nmodeling techniques. His current research interests include reinforcement\nlearning, adaptive and learning robot control, nonlinear system identification\nand state estimation. He has been involved in the applications of these\ntechniques in various fields, ranging from process control to robotics and\naerospace.\n10 VOLUME XX, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3374649\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}