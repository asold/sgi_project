{
  "title": "Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping",
  "url": "https://openalex.org/W3097132740",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2348023029",
      "name": "Zhang, Minjia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747894005",
      "name": "He Yuxiong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2949242443",
    "https://openalex.org/W3101543398",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2930786691",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2949892913",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W2781596748",
    "https://openalex.org/W2946379006",
    "https://openalex.org/W3013571468"
  ],
  "abstract": "Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language modeling. In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efficiency. Extensive experiments on BERT show that the proposed method achieves a 24% time reduction on average per sample and allows the pre-training to be 2.5 times faster than the baseline to get a similar accuracy on downstream tasks. While being faster, our pre-trained models are equipped with strong knowledge transferability, achieving comparable and sometimes higher GLUE score than the baseline when pre-trained with the same number of samples.",
  "full_text": "Accelerating Training of Transformer-Based\nLanguage Models with Progressive Layer Dropping\nMinjia Zhang Yuxiong He\nMicrosoft Corporation\n{minjiaz,yuxhe}@microsoft.com\nAbstract\nRecently, Transformer-based language models have demonstrated remarkable\nperformance across many NLP domains. However, the unsupervised pre-training\nstep of these models suffers from unbearable overall computational expenses.\nCurrent methods for accelerating the pre-training either rely on massive parallelism\nwith advanced hardware or are not applicable to language modeling. In this\nwork, we propose a method based on progressive layer dropping that speeds\nthe training of Transformer-based language models, not at the cost of excessive\nhardware resources but from model architecture change and training technique\nboosted efﬁciency. Extensive experiments on BERT show that the proposed method\nachieves a 24% time reduction on average per sample and allows the pre-training\nto be 2.5×faster than the baseline to get a similar accuracy on downstream tasks.\nWhile being faster, our pre-trained models are equipped with strong knowledge\ntransferability, achieving comparable and sometimes higher GLUE score than the\nbaseline when pre-trained with the same number of samples.\n1 Introduction\nNatural language processing (NLP) tasks, such as natural language inference [ 1, 2] and question\nanswering [3–5], have achieved great success with the development of neural networks. It has\nbeen demonstrated recently that Transformer-based networks have obtained superior performance in\nmany NLP tasks (e.g., the GLUE benchmark [6] and the challenging multi-hop reasoning task [7])\nthan recurrent neural networks or convolutional neural networks. BERT trains a deep bidirectional\nTransformer and obtains outstanding results with transfer learning [ 3]. RoBERTa [2], which is a\nrobustly optimized version of BERT trained with more steps and larger corpora, achieves state-of-\nthe-art results on 9 GLUE tasks. Megatron-LM [ 8] further advances the state-of-the-art in NLP\nby signiﬁcantly increasing the size of BERT model. Finally, there are multiple research proposing\ndifferent enhanced versions of Transformer-based networks, such as GPT-2/3 [9, 10], XLNet [1],\nSpanBERT [11], BioBERT [12], UniLM [13], Turing-NLG [14], and T5 [15]. Due to the exciting\nprospect, pre-training Transformer networks with a large corpus of text followed by ﬁne-tuning on\nspeciﬁc tasks has become a new paradigm for natural language processing.\nDespite great success, a big challenge of Transformer networks comes from the training efﬁciency –\neven with self-attention and parallelizable recurrence [16], and extremely high performance hard-\nware [17], the pre-training step still takes a signiﬁcant amount of time. To address this challenge,\nmixed-precision training is explored [8, 18], where the forward pass and backward pass are computed\nin half-precision and parameter update is in single precision. However, it requires Tensor Cores [19],\nwhich do not exist in all hardware. Some work resort to distributed training [ 20, 21, 8]. However,\ndistributed training uses large mini-batch sizes to increase the parallelism, where the training often\nconverges to sharp local minima with poor generalizability even with signiﬁcant hyperparameter\ntuning [22]. Subsequently, Yang et al. propose a layer-wise adaptive large batch optimizer called\nLAMB [23], allowing to train BERT with 32K batch size on 1024 TPU chips. However, this type of\napproach often requires dedicated clusters with hundreds or even thousands of GPUs and sophisti-\narXiv:2010.13369v1  [cs.LG]  26 Oct 2020\ncated system techniques at managing and tuning distributed training, not to mention that the amount\nof computational resources is intractable for most research labs or individual practitioners.\nIn this paper, we speedup pre-training Transformer networks by exploring architectural change and\ntraining techniques, not at the cost of excessive hardware resources. Given that the training cost grows\nlinearly with the number of Transformer layers, one straightforward idea to reduce the computation\ncost is to reduce the depth of the Transformer networks. However, this is restrictive as it often\nresults in lower accuracy in downstream tasks compared to full model pre-training, presumably\nbecause of having smaller model capacities [24, 25]. Techniques such as Stochastic Depth have been\ndemonstrated to be useful in accelerating supervised training in the image recognition domain [26].\nHowever, we observe that stochastically removing Transformer layers destabilizes the performance\nand easily results in severe consequences such as model divergence or convergence to bad/suspicious\nlocal optima. Why are Transformer networks difﬁcult to train with stochastic depth? Moreover, can\nwe speed up the (unsupervised) pre-training of Transformer networks without hurting downstream\nperformance?\nTo address the above challenges, we propose to accelerate pre-training of Transformer networks by\nmaking the following contributions. (i) We conduct a comprehensive analysis to answer the question:\nwhat makes Transformer networks difﬁcult to train with stochastic depth. We ﬁnd that both the choice\nof Transformer architecture as well as training dynamics would have a big impact on layer dropping.\n(ii) We propose a new architecture unit, called the Switchable-Transformer (ST) block, that not only\nallows switching on/off a Transformer layer for only a set portion of the training schedule, excluding\nthem from both forward and backward pass but also stabilizes Transformer network training. (iii) We\nfurther propose a progressive schedule to add extra-stableness for pre-training Transformer networks\nwith layer dropping – our schedule smoothly increases the layer dropping rate for each mini-batch as\ntraining evolves by adapting in time the parameter of the Bernoulli distribution used for sampling.\nWithin each gradient update, we distribute a global layer dropping rate across all the ST blocks to\nfavor different layers. (iv) We use BERT as an example, and we conduct extensive experiments to\nshow that the proposed method not only allows to train BERT 24% faster than the baseline under the\nsame number of samples but also allows the pre-training to be 2.5×faster to get similar accuracy on\ndownstream tasks. Furthermore, we evaluate the generalizability of models pre-trained with the same\nnumber of samples as the baseline, and we observe that while faster to train, our approach achieves a\n1.1% higher GLUE score than the baseline, indicating a strong knowledge transferability.\n2 Background and Related Work\nPre-training with Transformer-based architectures like BERT [3] has been demonstrated as an effective\nstrategy for language representation learning [ 2, 1, 27, 8]. The approach provides a better model\ninitialization for downstream tasks by training on large-scale unlabeled corpora, which often leads to\na better generalization performance on the target task through ﬁne-tuning on small data. Consider\nBERT, which consists a stack ofLTransformer layers [16]. Each Transformer layer encodes the the\ninput of the i-th Transformer layer xi with hi = fLN(xi + fS−ATTN (xi)), which is a multi-head\nself-attention sub-layer fATTN , and then by xi+1 = fLN(hi + fFFN (hi)), which is a feed-forward\nnetwork fFFN , where xi+1 is the output of the i-th Transformer layer. Both sub-layers have an\nAddNorm operation that consists a residual connection [28] and a layer normalization (fLN) [29].\nThe BERT model recursively applies the transformer block to the input to get the output.\nWhile the Transformer-based architecture has achieved breakthrough results in modeling sequences\nfor unsupervised language modeling [3, 9], previous work has also highlighted the training difﬁculties\nand excessively long training time [2]. To speed up the pre-training, ELECTRA [30] explores the\nadversarial training scheme by replacing masked tokens with alternatives sampled from a generator\nframework and training a discriminator to predict the replaced token. This increases the relative\nper-step cost, but leads to fewer steps, leading to the overall reduced costs. Another line of work\nfocus on reducing the per-step cost. Since the total number of ﬂoating-point operations (FLOPS) of\nthe forward and backward passes in the BERT pre-training process is linearly proportional to the\ndepth of the Transformer blocks, reducing the number of Transformer layers brings opportunities to\nsigniﬁcantly speed up BERT pre-training. To show this, we plot the FLOPS per training iteration\nin Fig. 6, assuming we can remove a fraction of layers at each step. Each line in the ﬁgure shows\nthe FLOPS using different layer removal schedules. Regardless of which schedule to choose, the\n2\nmajority of FLOPS are reduced in the later steps, with the rate of keep probability saturating to a\nﬁxed value ¯θ(e.g., 0.5). We will describe our schedule in Section 4.2.\nFigure 1: The norm of the gradi-\nent with respect to the weights,\nwith PostLN and PreLN.\nFigure 2: The norm preserving\nratio with respect to the inputs,\nwith PostLN and PreLN.\nFigure 3: Lesioning analysis\nwith PostLN and PreLN.\nDespite the FLOPS reduction, directly training models like BERT with a smaller depth incurs a\nsigniﬁcant loss in accuracy even with knowledge distillation [24, 25]. Prior work [31] proposes to\naccelerate pre-training by ﬁrst training a 3-layer BERT model and then growing the network depth\nto 6-layer and subsequently 12-layer. However, the number of steps required at each depth before\nthe network growth is not known a prior, making applying this approach challenging in practice.\nOn the other hand, stochastic depth has been successfully demonstrated to train deep models with\nreduced expected depth [26, 32]. However, we observe that directly pre-training BERT with randomly\ndropping fATTN and fFFN converges to bad/suspicious local optima under the same hyperparameter\nsetting. When increasing the learning rate, the training often diverges even by tuning the warmup\nratio. What causes the instability of BERT pre-training with layer drop?\n3 Preliminary Analysis\nThis section presents several studies that guided the design of the approach introduced in Section 4.\nWe used BERT trained on Bookcorpus and Wikipedia dataset from Devlin et. al. with standard\nsettings as the baseline 1. First, we carry out a comparison between BERT with PostLN and PreLN.\nOur goal is to measure how effective these two methods at stabilizing BERT training. Our second\nanalysis considers measuring the dynamics of BERT pre-training, including both spatial and temporal\ndimensions. Finally, we analyze the effect of the removal of the Transformer layers. This leads us to\nidentify appealing choices for our target operating points.\n3.1 Training Stability: PostLN or PreLN?\nWe consider two variants of BERT, namely the PostLN and PreLN. The default BERT employs\nPostLN, with layer normalization applied after the addition in Transformer blocks. The PreLN\nchanges the placement of the location offLN by placing it only on the input stream of the sublayers so\nthat hi = xi+ fS−ATTN (fLN(xi)) and then xi+1 = hi+ fFFN (fLN(hi)), which is a modiﬁcation\ndescribed by several recent works to establish identity mapping for neural machine translation [33–\n37]. Fig. 1 reports the norm of gradients with respect to weights in backward propagation for both\nmethods, varying the depth L(e.g., 12, 24, 48). The plot shows that while PostLN suffers from\nunbalanced gradients (e.g., vanishing gradients as the layer ID decreases), PreLN eliminates the\nunbalanced gradient problem (solid green lines) and the gradient norm stays almost same for any\nlayer. Furthermore, Fig. 2 shows that for PreLN the gradients with respect to input xi have very\nsimilar magnitudes (norm preserving ratio close to 1) at different layers, which is consistent with\nprior ﬁndings that a neural model should preserve the gradient norm between layers so as to have\nwell-conditioning and faster convergence [38, 39]. Indeed, we ﬁnd that PostLN is more sensitive to\nthe choice of hyperparameters, and training often diverges with more aggressive learning rates (more\nresults in Section 5), whereas PreLN avoids vanishing gradients and leads to more stable optimization.\nWe also provide preliminary theoretical results in Appendix B on why PreLN is beneﬁcial.\n1Appendix A provides detailed training hyperparameters.\n3\n3.2 Corroboration of Training Dynamics\nHereafter we investigate the representation xi learned at different phases of BERT pre-training and\nat different layers. Fig. 4 shows the L2 norm distances and cosine similarity, which measures the\nangle between two vectors and ignores their norms, between the input and output embeddings, with\nPostLN and PreLN, respectively. We draw several observations.\nFirst, the dissimilarity (Fig. 4a and Fig. 4b) stays high for both PostLN and PreLN at those higher\nlayers in the beginning, and the L2 and cosine similarity seems to be less correlated (e.g., step =\n300). This is presumably because, at the beginning of the training, the model weights are randomly\ninitialized, and the network is still actively adjusting weights to derive richer features from input data.\nSince the model is still positively self-organizing on the network parameters toward their optimal\nconﬁguration, dropping layers at this stage is not an interesting strategy, because it can create inputs\nwith large noise and disturb the positive co-adaption process.\nSecond, as the training proceeds (Fig. 4c and Fig. 4d), although the dissimilarity remains relatively\nhigh and bumpy for PostLN, the similarity from PreLN starts to increase over successive layers,\nindicating that while PostLN is still trying to produce new representations that are very different\nacross layers, the dissimilarity from PreLN is getting close to zero for upper layers, indicating that\nthe upper layers are getting similar estimations. This can be viewed as doing an unrolled iterative\nreﬁnement [40], where a group of successive layers iteratively reﬁne their estimates of the same\nrepresentations instead of computing an entirely new representation. Although the viewpoint was\noriginally proposed to explain ResNet, we demonstrate that it is also true for language modeling and\nTransformer-based networks. Appendix C provides additional analysis on how PreLN provides extra\npreservation of feature identity through unrolled iterative reﬁnement.\n(a)\n (b)\n (c)\n (d)\nFigure 4: The L2 distance and cosine similarity of the input and output embeddings for BERT with\nPostLN and PreLN, at different layers and different steps. We plot the inverse of cosine similarity\n(arccosine) in degrees, so that for both L2 and arccosine, the lower the more similar.\n3.3 Effect of Lesioning\nWe randomly drop layers with a keep ratio θ= 0.5 to test if dropping layers would break the training\nbecause dropping any layer changes the input distribution of all subsequent layers. The results are\nshown in Fig. 3. As shown, removing layers in PostLN signiﬁcantly reduces performance. Moreover,\nwhen increasing the learning rate, it results in diverged training. In contrast, this is not the case for\nPreLN. Given that later layers in PreLN tend to reﬁne an estimate of the representation, the model\nwith PreLN has less dependence on the downsampling individual layers. As a result, removing\nTransformer layers with PreLN has a modest impact on performance (slightly worse validation loss\nat the same number of training samples). However, the change is much smaller than with PostLN.\nIt further indicates that if we remove layers, especially those higher ones, it should have only a\nmild effect on the ﬁnal result because doing so does not change the overall estimation the next layer\nreceives, only its quality. The following layers can still perform mostly the same operation, even with\nsome relatively little noisy input. Furthermore, as Fig. 4 indicates, since the lower layers remain to\nhave a relatively high dissimilarity (deriving new features), they should be less frequently dropped.\nOverall, these results show that, to some extent, the structure of a Transformer network with PreLN\ncan be changed at runtime without signiﬁcantly affecting performance.\n4\n4 Our Approach: Progressive Layer Dropping\nThis section describes our approach, namely progressive layer dropping (PLD), to accelerate the\npre-training of Transformer-based models. We ﬁrst present the Switchable-Transformer blocks, a\nnew unit that allows us to train models like BERT with layer drop and improved stability. Then we\nintroduce the progressive layer drop procedure.\n4.1 Switchable-Transformer Blocks\nIn this work, we propose a novel transformer unit, which we call \"Switchable-Transformer \" (ST)\nblock. Compared with the original Transformer block (Fig. 5a), it contains two changes.\nIdentity mapping reordering. The ﬁrst change is to establish identity mapping within a trans-\nformer block by placing the layer normalization only on the input stream of the sublayers (i.e., use\nPreLN to replace PostLN) (Fig. 5b) for the stability reason described in Section 3.1.\n(a) Original\n (b) Identity map-\nping reordering\n(c) Switchable Trans-\nformer\nFigure 5: Transformer variants, showing a single layer block.\nFigure 6: FLOPS per training iter-\nation normalized to the baseline.\nSwitchable gates. Next, we extend the architecture to include a gate for each sub-layer (Fig. 5c),\nwhich controls whether a sub-layer is disabled or not during training. In particular, for each mini-batch,\nthe two gates for the two sublayers decide whether to remove their corresponding transformation\nfunctions and only keep the identify mapping connection, which is equivalent to applying a conditional\ngate function Gto each sub-layer as follows:\nhi = xi + Gi ×fS−ATTN (fLN(xi)) × 1\npi\nxi+1 = hi + Gi ×fFFN (fLN(hi)) × 1\npi\n(1)\nIn our design, the function Gi only takes 0 or 1 as values, which is chosen randomly from a Bernoulli\ndistribution (with two possible outcomes), Gi ∼B(1,pi), where pi is the probability of choosing 1.\nBecause the blocks are selected with probability pi during training and are always presented during\ninference, we re-calibrate the layers’ output by a scaling factor of 1\npi\nwhenever they are selected.\n4.2 A Progressive Layer Dropping Schedule\nBased on the insights from Section 3.2, and inspired by prior work on curriculum learning [41, 42]\nwe propose a progressive schedule θ(t) – a temporal schedule for the expected number of ST blocks\nthat are retained. We limit ourselves to monotonically decreasing functions so that the likelihood of\nlayer dropping can only increase along the temporal dimension. We constrain θ(t) to be θ(t) ≥¯θfor\nany t, where ¯θis a limit value, to be taken as 0.5 ≤¯θ≤0.9 (Section 5). Based on this, we deﬁne the\nprogressive schedule θ(t) as:\nDeﬁnition 4.1. A progressive schedule is a functiont→θ(t) such that θ(0) = 1 and limt→∞θ(t) →\n¯θ, where ¯θ∈(0,1].\n5\nProgress along the time dimension. Starting from the initial condition θ(0) = 1 where no layer\ndrop is performed, layer drop is gradually introduced. Eventually (i.e., when tis sufﬁciently large),\nθ(t) →¯θ. According to Def. 4.1, in our work, we use the following schedule function:\n¯θ(t) = (1 −¯θ)exp(−γ·t) + ¯θ,γ >0 (2)\nBy considering Fig. 6, we provide intuitive and straightforward motivation for our choice. The blue\ncurve in Fig. 6 are polynomials of increasing degree δ= {1,.., 8}(left to right). Despite fulﬁlling the\ninitial constraint θ(0) = 1, they have to be manually thresholded to impose θ(t) →¯θwhen t→∞,\nwhich introduces two more parameters (δand the threshold). In contrast, in our schedule, we ﬁx\nγ using the following simple heuristics γ = 100\nT , since it implies |θ(T) −¯θ|<10−5 for θ(t) ≈¯θ\nwhen t≈T, and it is reasonable to assume that T is at the order of magnitude of 104 or 105 when\ntraining Transformer networks. In other words, this means that for a big portion of the training, we\nare dropping (1 −¯θ) ST blocks, accelerating the training efﬁciency.\nDistribution along the depth dimension. The above progressive schedule assumes all gates in ST\nblocks take the same pvalue at each step t. However, as shown in Fig. 4, the lower layers of the\nnetworks should be more reliably present. Therefore, we distribute the global ¯θacross the entire stack\nso that lower layers have lower drop probability linearly scaled by their depth according to equation 3.\nFurthermore, we let the sub-layers inside each block share the same schedule, so when Gi = 1, both\nthe inner function fATTN and fFFN are activated, while they are skipped when Gi = 0. Therefore,\neach gate has the following form during training:\npl(t) = i\nL(1 −¯θ(t)) (3)\nCombining Eqn. 3 and Eqn. 2, we have the progressive schedule for an ST block below.\nθi(t) = i\nL(1 −(1 −¯θ(t))exp(−γ·t) −¯θ(t)) (4)\nAlgorithm 1 Progressive_Layer_Dropping\n1: Input: keep_prob ¯θ\n2: InitBERT(switchable_transformer_block)\n3: γ ←100\nT\n4: for t ←1 to T do\n5: θt ←(1 −¯θ)exp(−γ·t) + ¯θ\n6: step ←1−θt\nL\n7: p←1\n8: for l ←1 to L do\n9: action ∼Bernoulli(p)\n10: if action == 0 then\n11: xi+1 ←xi\n12: else\n13: x\n′\ni ←xi+ fATTN (fLN(xi)) ×1\np\n14: xi+1 ←x\n′\ni+fFFN (fLN(x\n′\ni))×1\np\n15: xi ←xi+1\n16: p ←p - step\n17: Y ←output_layer(xL)\n18: loss ←loss_fn( ¯Y,Y )\n19: backward(loss)\nPutting it together. Note that because of the\nexist of the identity mapping, when an ST block\nis bypassed for a speciﬁc iteration, there is no\nneed to perform forward-backward computation\nor gradient updates, and there will be updates\nwith signiﬁcantly shorter networks and more di-\nrect paths to individual layers. Based on this, we\ndesign a stochastic training algorithm based on\nST blocks and the progressive layer-drop sched-\nule to train models like BERT faster, which we\ncall progressive layer dropping (Alg. 1). The\nexpected network depth, denoted as ¯L, becomes\na random variable. Its expectation is given by:\nE(¯L) = ∑T\nt=0\n∑L\ni=1 θ(i,t). With ¯θ = 0.5, the\nexpected number of ST blocks during training\nreduces to E(¯L) = (3L−1)/4 or E(¯L) ≈3L/4\nwhen T is large. For the 12-layer BERT model\nwith L=12 used in our experiments, we have\nE(¯L) ≈9. In other words, with progressive layer\ndropping, we train BERT with an average num-\nber of 9 layers. This signiﬁcantly improves the\npre-training speed of the BERT model. Follow-\ning the calculations above, approximately 25% of\nFLOPS could be saved under the drop schedule with ¯θ= 0.5. We recover the model with full-depth\nblocks at ﬁne-tuning and testing time.\n5 Evaluation\nWe show that our method improves the pre-training efﬁciency of Transformer networks, and the\ntrained models achieve competitive or even better performance compared to the baseline on transfer\nlearning downstream tasks. We also show ablation studies to analyze the proposed training techniques.\n6\n(a)\n (b)\nFigure 7: The convergence curve of the baseline and our proposed\nmethod regarding the wall-clock time.\nTable 1: Training time compar-\nison. Sample RD standards for\nsample reduction. SPD repre-\nsents speedup.\nTraining\nTime\nSample\nRD SPD\nBaseline\nckp186 38.45h 0 1\nPLD\nckp186 29.22h 0 1.3×\nPLD\nckp100 15.56h 46% 2.5×\nPLD\nckp87 13.53h 53% 2.8×\nDatasets. We follow Devlin et al. [3] to use English Wikipedia corpus and BookCorpus for pre-\ntraining. By concatenating the two datasets, we obtain our corpus with roughly 2.8B word tokens in\ntotal, which is comparable with the data corpus used in Devlin et al. [3]. We segment documents\ninto sentences with 128 tokens; We normalize, lower-case, and tokenize texts using Wordpiece\ntokenizer [3]. The ﬁnal vocabulary contains 30,528 word pieces. We split documents into one training\nset and one validation set (300:1). For ﬁne-tuning, we use GLUE (General Language Understanding\nEvaluation), a collection of 9 sentence or sentence-pair natural language understanding tasks including\nquestion answering, sentiment analysis, and textual entailment. It is designed to favor sample-efﬁcient\nlearning and knowledge-transfer across a range of different linguistic tasks in different domains.\nTraining details. We use our own implementation of the BERT model [3] based on the Hugging-\nface[1] PyTorch implementation. All experiments are performed on 4×DGX-2 boxes with 64×V100\nGPUs. Data parallelism is handled via PyTorch DDP (Distributed Data Parallel) library [ 43]. We\nrecognize and eliminate additional computation overhead: we overlap data loading with computation\nthrough the asynchronous prefetching queue; we optimize the BERT output processing through sparse\ncomputation on masked tokens. Using our pre-processed data, we train a 12-layer BERT-base model\nfrom scratch as the baseline. We use a warm-up ratio of 0.02 with lrmax=1e−4. Following [3], we\nuse Adam as the optimizer. We train with batch size 4K for 200K steps, which is approximately 186\nepochs. The detailed parameter settings are listed in the Appendix A. We ﬁne-tune GLUE tasks for 5\nepochs and report the median development set results for each task over ﬁve random initializations.\n5.1 Experimental Results\nPre-training convergence comparisons. Fig. 7a visualizes the convergence of validation loss\nregarding the computational time. We make the following observations. First, with lrmax=1e−4, the\nconvergence rate of our algorithm and the baseline is very close. This veriﬁes empirically that our\nprogressive layer dropping method does not hurt model convergence. Second, when using a larger\nlearning rate lrmax=1e−3, the baseline diverges. In contrast, our method shows a healthy convergence\ncurve and is much faster. This conﬁrms that our architectural changes stabilize training and allows\nBERT training with more aggressive learning rates.\nSpeedup. Fig. 7b shows both the training curve (dotted) and the validation curve (solid) of the\nbaseline and PLD with a zoomed-in view. The baseline curve becomes almost ﬂat at epoch 186,\ngetting a validation loss of 1.75. In contrast, PLD reaches the same validation loss at epoch 87, with\n53% fewer training samples. Furthermore, PLD achieves a 24% time reduction when training the same\nnumber of samples. This is because our approach trains the model with a smaller number of expected\ndepth for the same number of steps. It is slightly lower than the 25% GFLOPS reduction in the\nanalysis because the output layer still takes a small amount of computation even after optimizations.\nThe combination of these two factors, yields 2.8×speedup in end-to-end wall-clock training time\nover the baseline, as shown in Table 1.\nDownstream task accuracy. Despite improved training speed, one may still wonder whether such\na method is as effective as the baseline model on downstream tasks. Table 2 shows our results on the\nGLUE dataset compared to the baseline. Our baseline is comparable with the original BERT-Base\n7\n(on the test set), and our PLD method achieves a higher GLUE score than our baseline (83.2 vs. 82.1)\nwhen ﬁne-tuning the checkpoint (186). We also dump model checkpoints from different epochs\nduring pre-training and ﬁne-tune these models. The checkpoint 87 corresponds to the validation\nloss at 1.75 achieved by PLD. The GLUE score is slightly worse than the baseline (81.6 vs. 82.1).\nHowever, by ﬁne-tuning at checkpoint 100, PLD achieves a higher score than the baseline (82.3 vs.\n82.1) at checkpoint 186. In terms of the pre-training wall clock time, PLD requires 15.56h vs. the\nbaseline with 39.15h to get similar accuracy on downstream tasks, which corresponds to a 2.5 ×\nspeedup.\nTable 2: The results on the GLUE benchmark. The number below each task denotes the number of\ntraining examples. The metrics for these tasks can be found in the GLUE paper [6]. We compute the\ngeometric mean of the metrics as the GLUE score.\nModel\nRTE\n(Acc.)\nMRPC\n(F1/Acc.)\nSTS-B\n(PCC/SCC)\nCoLA\n(MCC)\nSST-2\n(Acc.)\nQNLI\n(Acc.)\nQQP\n(F1/Acc.)\nMNLI-mm\n-/m (Acc.) GLUE\n2.5K 3.7K 5.7K 8.5K 67K 108K 368K 393K\nBERTbase (original) 66.4 88.9/84.8 87.1/89.2 52.1 93.5 90.5 71.2/89.2 84.6/83.4 80.7\nBERTbase (Baseline, ckp186) 67.8 88.0/86.0 89.5/89.2 52.5 91.2 87.1 89.0/90.6 82.5/83.4 82.1\nBERTbase (PLD, ckp87) 66 88.2/85.6 88.9/88.4 54.5 91 86.3 87.4/89.1 81.6/82.4 81.6\nBERTbase (PLD, ckp100) 68.2 88.2/85.8 89.3/88.9 56.1 91.5 86.9 87.7/89.3 82.4/82.6 82.3\nBERTbase (PLD, ckp186) 69 88.9/86.5 89.6/89.1 59.4 91.8 88 89.4/90.9 83.1/83.5 83.2\nFig. 8 illustrates the ﬁne-tuning results between the baseline and PLD on GLUE tasks over different\ncheckpoints Overall, we observe that PLD not only trains BERT faster in pre-training but also\npreserves the performance on downstream tasks. In each ﬁgure, we observe that both curves have a\nsimilar shape at the beginning because no layer drop is added. For later checkpoints, PLD smoothly\nadds layer drop. Interestingly, we note that the baseline model has ﬂuctuations in testing accuracy. In\ncontrast, the downstream task accuracy from PLD is consistently increasing as the number of training\nepochs increases. This indicates that PLD takes a more robust optimization path toward the optimum.\nWe also observe that our model achieves higher performance on MNLI, QNLI, QQP, RTE, SST-2,\nand CoLA on later checkpoints, indicating that the model trained with our approach also generalizes\nbetter on downstream tasks than our baseline does.\nFrom a knowledge transferability perspective, the goal of training a language model is to learn a good\nrepresentation of natural language that ideally ignores the data-dependent noise and generalizes well\nto downstream tasks. However, training a model with a constant depth is at least somewhat noisy\nand can bias the model to prefer certain representations, whereas PLD enables more sub-network\nconﬁgurations to be created during training Transformer networks. Each of the L ST blocks is either\nactive or inactive, resulting in2L possible network combinations. By selecting a different submodular\nin each mini-batch, PLD encourages the submodular to produce good results independently. This\nallows the unsupervised pre-training model to obtain a more general representation by averaging the\nnoise patterns, which helps the model to better generalize to new tasks. On the other hand, during\ninference, the full network is presented, causing the effect of ensembling different sub-networks.\n5.2 Ablation Studies\nDownstream task ﬁne-tuning sensitivity. To further verify that our approach not only stabilizes\ntraining but also improves downstream tasks, we show a grid search on learning rates {1e-5, 3e-5,\n5e-5, 7e-5, 9e-5, 1e-4}. As illustrated in Fig. 9, the baseline is vulnerable to the choice of learning\nrates. Speciﬁcally, the ﬁne-tuning results are often much worse with a large learning rate, while PLD\nis more robust and often achieves better results with large learning rates.\nThe Effect of ¯θ. We test different values of the keep ratio ¯θand identify 0.5 ≤¯θ≤0.9 as a good\nrange, as shown in Fig. 10 in the Appendix. We observe that the algorithm may diverge if ¯θis too\nsmall (e.g., 0.3).\nPLD vs. PreLN. To investigate the question on how PLD compares with PreLN, we run both\nPreLN with the hyperparameters used for training PostLN (lr=1e-4) and the hyperparameters used for\nPLD (lr=1e-3) to address the effect from the choice of hyperparameters. We train all conﬁgurations\nfor the same number of epochs and ﬁne-tune following the standard procedure. In both cases, PreLN\nis 24% slower than PLD, because PreLN still needs to perform the full forward and backward\npropagation in each iteration.\n8\n(a) MNLI-m\n (b) MNLI-mm\n (c) QNLI\n(d) QQP\n (e) RTE\n (f) SST-2\n(g) WNLI\n (h) CoLA\n (i) MRPC (acc.)\n(j) MRPC (F1.)\n (k) SST-B (PCC)\n (l) SST-B (SCC)\nFigure 8: The ﬁne-tuning results at different checkpoints.\nTable 3 shows the ﬁne-tuning results on GLUE tasks. When trained with the same hyperparameters\nas PostLN, PreLN appears to have a much worse GLUE score (80.2) compared with PostLN (82.1)\non downstream tasks. This is because PreLN restricts layer outputs from depending too much on\ntheir own residual branches and inhibits the network from reaching its full potential, as recently\nstudied in [44]. When trained with the large learning rate as PLD, PreLN’s result have improved\nto 82.6 but is 0.6 points worse than PLD (83.2), despite using 24% more compute resource. PLD\nachieves better accuracy than PreLN because it encourages each residual branch to produce good\nresults independently.\nPLD vs. Shallow network. Shallow BERT + PreLN + Large lr in Table 3 shows the downstream\ntask accuracy of the 9-layer BERT. Although having the same same number of training computational\nGFLOPS as ours, the shallow BERT underperforms PreLN by 0.8 points and is 1.4 points worse than\nPLD likely because the model capacity has been reduced by the loss of parameters.\nPLD vs. Random drop. BERT + PreLN + Large lr + Random drops layers randomly with a ﬁxed\nratio (i.e., it has the same compute cost but without any schedule), similar to Stochastic Depth [26].\nThe GLUE score is 0.9 points better than shallow BERT under the same compute cost and 0.1 point\n9\n(a) MNLI-m\n (b) MNLI-mm\n (c) QNLI\n(d) QQP\n (e) RTE\n (f) SST-2\nFigure 9: The ﬁne-tuning results at different checkpoints.\nFigure 10: Convergence curves varying the keep ratio ¯θ.\nTable 3: Ablation studies of the ﬁne-tuning results on the GLUE benchmark.\nModel RTE\n(Acc.)\nMRPC\n(F1/Acc.)\nSTS-B\n(PCC/SCC)\nCoLA\n(MCC)\nSST-2\n(Acc.)\nQNLI\n(Acc.)\nQQP\n(F1/Acc.)\nMNLI-m/mm\n(Acc.) GLUE\nBERT (Original) 66.4 88.9/84.8 87.1/89.2 52.1 93.5 90.5 71.2/89.2 84.6/83.4 80.7\nBERT + PostLN 67.8 88.0/86.0 89.5/89.2 52.5 91.2 87.1 89.0/90.6 82.5/83.4 82.1\nBERT + PreLN + Same lr 66.0 85.9/83.3 88.2/87.9 46.4 90.5 85.5 89.0/90.6 81.6/81.6 80.2\nBERT + PreLN + lr↑ 67.8 86.7/84.5 89.6/89.1 54.6 91.9 88.1 89.3/90.9 83.6/83.7 82.6\nShallow BERT + PreLN + lr↑66.0 85.9/83.5 89.5/88.9 54.7 91.8 86.1 89.0/90.6 82.7/82.9 81.8\nBERT + PreLN + lr↑+ Rand. 68.2 88.2/86.2 89.3/88.8 56.8 91.5 87.2 88.6/90.3 82.9/83.3 82.7\nBERT + PreLN + lr↑+ TD 68.2 88.6/86.7 89.4/88.9 55.9 91.3 86.8 89.1/90.7 82.7/83.1 82.7\nBERT + PreLN + lr↑+ PLD 69.0 88.9/86.5 89.6/89.1 59.4 91.8 88.0 89.4/90.9 83.1/83.5 83.2\nbetter than PreLN while being 24% faster, indicating the strong regularization effect from stochastic\ndepth. It is 0.5 points worse than PLD, presumably because a ﬁxed ratio does not take into account\nthe training dynamics of Transformer networks.\nSchedule impact analysis. BERT + PreLN + Large lr + TD only (32-bit*) disables the schedule\nalong the depth dimension (DD) and enables only the schedule along the temporal dimension (TD)\nin training. Its GLUE score matches \"Random\", suggesting that the temporal schedule has similar\nperformance as the ﬁxed constant schedule along the time dimension and accuracy gains of PLD\nis mostly from the depth dimension. However, without the temporal schedule enabled, the model\ndiverges with NaN in the middle of half-precision (16-bit) training and has to switch to full-precision\n10\n(32-bit) training, slowing down training speed. Furthermore, this concept of starting-easy and\ngradually increasing the difﬁculty of the learning problem has its roots in curriculum learning and\noften makes optimization easier. We adopt the temporal schedule since it is robust and helpful for\ntraining stability, retaining similar accuracy while reducing training cost considerably.\n6 Conclusion\nUnsupervised language model pre-training is a crucial step for getting state-of-the-art performance\non NLP tasks. The current time for training such a model is excruciatingly long, and it is very\nmuch desirable to reduce the turnaround time for training such models. In this paper, we study\nthe efﬁcient training algorithms for pre-training BERT model for NLP tasks. We have conducted\nextensive analysis and found that model architecture is important when training Transformer-based\nmodels with stochastic depth. Using this insight, we propose the Switchable-Transformer block and a\nprogressive layer-wise drop schedule. Our experiment results show that our training strategy achieves\ncompetitive performance to training a deep model from scratch at a faster rate.\nReferences\n[1] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS\n2019, 8-14 December 2019, Vancouver, BC, Canada, pages 5754–5764, 2019.\n[2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach.\nCoRR, abs/1907.11692, 2019.\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019), pages 4171–4186, 2019.\n[4] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question\nanswering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages\n4149–4158, 2019.\n[5] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-\nend open-domain question answering with bertserini. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations, pages 72–77, 2019.\n[6] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In 7th International\nConference on Learning Representations, 2019.\n[7] Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading\ncomprehension at scale. In Proceedings of the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2694–2703,\n2019.\n[8] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. CoRR,\nabs/1909.08053, 2019.\n[9] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. Online post, 2019.\n[10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\n11\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners. CoRR, abs/2005.14165, 2020.\n[11] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. Spanbert:\nImproving pre-training by representing and predicting spans. CoRR, abs/1907.10529, 2019.\n[12] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinform.,\n36(4):1234–1240, 2020.\n[13] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and\nHsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding and generation.\nIn Advances in Neural Information Processing Systems, pages 13042–13054, 2019.\n[14] Turing-NLG: A 17-billion-parameter language model by Microsoft.\nhttps : / / www . microsoft . com / en-us / research / blog /\nturing-nlg-a-17-billion-parameter-language-model-by-microsoft/ . Ac-\ncessed: 19-May-2020.\n[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nCoRR, abs/1910.10683, 2019.\n[16] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems 30: Annual Conference on Neural Information Processing Systems 2017, pages 5998–6008, 2017.\n[17] Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah\nBates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark,\nJeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra\nGottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt,\nDan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew,\nAndy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan\nLiu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller,\nRahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana\nPenukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory\nSizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson,\nBo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter, Walter Wang, Eric Wilcox, and\nDoe Hyun Yoon. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the\n44th Annual International Symposium on Computer Architecture, ISCA ’17, pages 1–12, 2017.\n[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.\nIn 6th International Conference on Learning Representations, ICLR 2018, 2018.\n[19] Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, and Jeffrey S. Vetter. NVIDIA tensor\ncore programmability, performance & precision. In 2018 IEEE International Parallel and Distributed\nProcessing Symposium Workshops, IPDPS Workshops 2018, Vancouver, BC, Canada, May 21-25, 2018,\npages 522–531, 2018.\n[20] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V . Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efﬁcient training of giant neural\nnetworks using pipeline parallelism. In Advances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, pages 103–112, 2019.\n[21] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter\nHawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and Blake A. Hechtman. Mesh-\ntensorﬂow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems 31:\nAnnual Conference on Neural Information Processing Systems 2018, pages 10435–10444, 2018.\n[22] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.\nOn large-batch training for deep learning: Generalization gap and sharp minima. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference\nTrack Proceedings, 2017.\n[23] Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui Hsieh. Reducing BERT\npre-training time from 3 days to 76 minutes. CoRR, abs/1904.00962, 2019.\n12\n[24] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of BERT:\nsmaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019.\n[25] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinybert: Distilling BERT for natural language understanding. CoRR, abs/1909.10351, 2019.\n[26] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic\ndepth. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part IV, pages 646–661, 2016.\n[27] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\nALBERT: A lite BERT for self-supervised learning of language representations. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, 2020.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\n[29] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,\n2016.\n[30] Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. ELECTRA: pre-training\ntext encoders as discriminators rather than generators. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\n[31] Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tie-Yan Liu. Efﬁcient training of BERT by\nprogressively stacking. In Proceedings of the 36th International Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA, pages 2337–2346, 2019.\n[32] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured\ndropout. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net, 2020.\n[33] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. Learning\ndeep transformer models for machine translation. In Proceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\npages 1810–1822, 2019.\n[34] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. arXiv preprint\narXiv:2002.04745, 2020.\n[35] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In 7th\nInternational Conference on Learning Representations, 2019.\n[36] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. CoRR, abs/1904.10509, 2019.\n[37] Toan Q. Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-\nattention. CoRR, abs/1910.05895, 2019.\n[38] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pages 249–256, 2010.\n[39] Alireza Zaeemzadeh, Nazanin Rahnavard, and Mubarak Shah. Norm-preservation: Why residual networks\ncan become extremely deep? CoRR, abs/1805.07477, 2018.\n[40] Klaus Greff, Rupesh Kumar Srivastava, and Jürgen Schmidhuber. Highway and residual networks learn\nunrolled iterative estimation. In 5th International Conference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.\n[41] Pietro Morerio, Jacopo Cavazza, Riccardo V olpi, René Vidal, and Vittorio Murino. Curriculum dropout. In\nIEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages\n3564–3572, 2017.\n[42] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\nProceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal,\nQuebec, Canada, June 14-18, 2009, pages 41–48, 2009.\n13\n[43] PyTorch Distributed Data Parallel. https://pytorch.org/docs/stable/notes/ddp.html.\nAccessed: 28-April-2020.\n[44] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difﬁculty of\ntraining transformers. CoRR, abs/2004.08249, 2020.\n[45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks.\nIn Computer Vision - ECCV 2016 - 14th European Conference, pages 630–645, 2016.\n[46] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.\nAutomatic differentiation in machine learning: a survey. J. Mach. Learn. Res., 18:153:1–153:43, 2017.\n14\nA Pre-training Hyperparameters\nTable 4 describes the hyperparameters for pre-training the baseline and PLD.\nTable 4: Hyperparameters for pre-training the baseline and PLD.\nHyperparameter Baseline PLD\nNumber of Layers 12 12\nHidden zies 768 768\nAttention heads 12 12\nDropout 0.1 0.1\nAttention dropout 0.1 0.1\nTotal batch size 4K 4K\nTrain micro batch size per gpu 16 16\nOptimizer Adam Adam\nPeak learning rate 1e-04 1e-03\nLearning rate scheduler warmup_linear_decay_exp warmup_linear_decay_exp\nWarmup ratio 0.02 0.02\nDecay rate 0.99 0.99\nDecay step 1000 1000\nMax Training steps 200000 200000\nWeight decay 0.01 0.01\nGradient clipping 1 1\nB Establishing Identity Mapping with PreLN\nPrior studies [28, 45] suggest that establishing identity mapping to keep a clean information path (no\noperations except addition) is helpful for easing optimization of networks with residual connections.\nWith the change of PreLN, we can express the output of the i-th Transformer layer as the input xi of\nthat layer plus a residual transformation function fRT = fS−ATTN (fLN(xi)) + fFFN (fLN(x\n′\ni)),\nand the output layer xL = xl + ∑L−1\ni=l fRT(xi) as the recursive summation of preceding fRT\nfunctions in shallower layers (plus xl). If we denote the loss function as E, from the chain rule of\nbackpropagation [46] we have:\n∂E\n∂xl\n= ∂E\n∂xL\n∂xL\n∂xl\n= ∂E\n∂xL\n(1 + ∂\n∂xl\nL−1∑\ni=l\nfRT(xi)) (5)\nEqn. 5 indicates that the gradient ∂E\n∂Xl\ncan be decomposed into two additive terms: a term of ∂E\n∂XL\nthat\npropagates information directly back to any shallower l-th block without concerning how complex\n∂\n∂xl\n∑L−1\ni=l fRT(xi)) would be, and another term of ∂E\n∂XL\n( ∂\n∂Xl\n∑L−1\ni=l fRT(Xi)) that propagates\nthrough the Transformer blocks. The equation also suggests that it is unlikely for the gradient ∂\n∂Xl\nto be canceled out for a mini-batch, and in general the term ∂\n∂Xl\n∑L−1\ni=l fRT(Xi) cannot be always\n-1 for all samples in a mini-batch. This explains why the gradients of Transformer layers in Fig. 1\nbecome more balanced and do not vanish after identity mapping reordering. In contrast, the PostLN\narchitecture has a series of layer normalization operations that constantly alter the signal that passes\nthrough the skip connection and impedes information propagation, causing both vanishing gradients\nand training instability. Overall, PreLN results in several useful characteristics such as avoiding\nvanishing/exploding gradient, stable optimization, and performance gain.\nC PreLN From the View of Unrolled Iterative Reﬁnement\nFrom a theoretical point of view [40], a noisy estimate for a representation by the ﬁrst Transformer\nlayer should, on average, be correct even though it might have high variance. The unrolled iterative\nreﬁnement view says if we treat \"identity mapping\" (as in PreLN) as being an unbiased estimator for\nthe target representation, then beyond the ﬁrst layer, the subsequent Transformer layer outputsxn\ni (e.g.,\ni∈2...L) are all estimators for the same latent representation Hn, where Hn refers to the (unknown)\n15\nvalue towards which the n-th representation is converging. The unbiased estimator condition can\nthen be written as the expected difference between the estimator and the ﬁnal representation:\nE\nx∈X\n[xn\ni −Hn] = 0 (6)\nWith the PreLN equation, it follows that the expected difference between outputs of two consecutive\nlayers is zero, because\nE[xn\ni −Hn] −E[xn\ni−1 −Hn] = 0 ⇒E[xn\ni −xn\ni−1] = 0 (7)\nIf we write representation xn\ni as a combination of xi−1n and a residual fRT\nn, it follows from the\nabove equation that the residual has to be zero-mean:\nxn\ni = xn\ni−1 + fRT\nn ⇒E[fRT\nn] = 0 (8)\nwhich we have empirically veriﬁed to be correct, as shown in Figure 2. Therefore, PreLN ensures\nthat the expectation of the new estimate will be correct, and the iterative summation of the residual\nfunctions in the remaining layers determines the variance of the new estimate E[FRTi].\nThe effect of learning rates on downstream tasks. We focus on evaluating larger datasets and\nexclude very small datasets, as we ﬁnd that the validation scores on those datasets have a large\nvariance for different random seeds.\nFor ﬁne-tuning models on downstream tasks, we consider training with batch size 32 and performing\na linear warmup for the ﬁrst 10% of steps followed by a linear decay to 0. We ﬁne-tune for 5 epochs\nand perform the evaluation on the development set. We report the median development set results for\neach task over ﬁve random initializations, without model ensemble.\nResults are visualized in Fig. 9, which shows that the baseline is less robust on the choice of learning\nrates. Speciﬁcally, the ﬁne-tuning results are often much worse with a large learning rate. In\ncomparison, PLD is more robust and often achieves better results with large learning rates.\n16",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8018235564231873
    },
    {
      "name": "Transformer",
      "score": 0.7535955905914307
    },
    {
      "name": "Language model",
      "score": 0.6244513988494873
    },
    {
      "name": "Transferability",
      "score": 0.6064618229866028
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5748700499534607
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43521198630332947
    },
    {
      "name": "Machine learning",
      "score": 0.4154003858566284
    },
    {
      "name": "Natural language processing",
      "score": 0.3762989938259125
    },
    {
      "name": "Voltage",
      "score": 0.14711415767669678
    },
    {
      "name": "Engineering",
      "score": 0.06894057989120483
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Logit",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 30
}