{
  "title": "Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction",
  "url": "https://openalex.org/W4385893874",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2616627182",
      "name": "Simon Meoni",
      "affiliations": [
        "Arkema (France)"
      ]
    },
    {
      "id": "https://openalex.org/A4208575899",
      "name": "Eric De La Clergerie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900189074",
      "name": "Théo Ryffel",
      "affiliations": [
        "Arkema (France)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312220150",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3153866062",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4226250826",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W3200808010",
    "https://openalex.org/W4321276774",
    "https://openalex.org/W4287822362",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3176553463",
    "https://openalex.org/W1541954861",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W4385572244",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2951562155",
    "https://openalex.org/W3126592082",
    "https://openalex.org/W3113662377",
    "https://openalex.org/W3145725122",
    "https://openalex.org/W3103187652",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4286983269",
    "https://openalex.org/W4285116174",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385573637",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4285185841"
  ],
  "abstract": "International audience",
  "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 178–190\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models as Instructors: A Study on Multilingual Clinical\nEntity Extraction\nSimon Meoni\nInria/Arkhn\nParis, France\nsimon.meoni@arkhn.com\nThéo Ryffel\nArkhn\nParis, France\ntheo@arkhn.com\nÉric de la Clergerie\nInria\nParis, France\nEric.De_La_Clergerie@inria.fr\nAbstract\nIn clinical and other specialized domains, data\nare scarce due to their confidential nature. This\nlack of data is a major problem when fine-\ntuning language models. Nevertheless, very\nlarge language models (LLMs) are promising\nfor the medical domain but cannot be used di-\nrectly in healthcare facilities due to data con-\nfidentiality issues. We explore an approach of\nannotating training data with LLMs to train\nsmaller models more adapted to our problem.\nWe show that this method yields promising re-\nsults for information extraction tasks.\n1 Introduction\nClinical notes contain the interactions between the\npatient and healthcare staff. Professionals record\ntheir impressions, observations, and various med-\nical procedures performed. Despite the comput-\nerization of clinical documents, notes should re-\nmain fairly expressive and in a free format to save\ntime for healthcare personnel and allow for the de-\nscription of unusual situations (Rosenbloom et al.,\n2011). Moreover, a large amount of crucial infor-\nmation is exclusively contained in clinical notes.\nAccording to a study by Escudié et al. (2017), ap-\nproximately 80% of patient phenotypes (a set of\nobservable biological and physical characteristics\nthat can characterize a disease) are present only\nin free text. These documents are difficult to use\nwithout advanced methods such as deep learning\nin NLP. The use of such methods requires the col-\nlection and annotation of a significant amount of\nmedical data. However, Fries et al. (2022) proposes\nthe term \"dataset debt,\" highlighting that learning\ndata in the biomedical field is poorly accessible,\npoorly documented, and opaque as to its reusability\nin a commercial or a hospital context. According to\nthe article, only 13% of the 167 analyzed datasets\nare accessible and downloadable, 22% use a stan-\ndard structured format, and 40% are in the public\ndomain. In recent years, large language models\n(LLMs) have proved their ability to perform a wide\nrange of tasks with high accuracy in a zero-shot or\na few-shots contexts. This trend holds great poten-\ntial for clinical NLP, as preliminary results show\npromise for information extraction tasks (Agrawal\net al., 2022). However, the clinical domain presents\nunique challenges due to the confidential and lin-\nguistically specific nature of its data, which can\nmake collection and annotation time-consuming\nand expensive. Using LLMs for efficient informa-\ntion extraction without training data could be at-\ntractive, but it raises confidentiality concerns. The\nmodel deployment should be controllable, and the\nmodel’s predictions should evolve to fit a specific\nand changing annotation guideline. Most multi-\nlingual LLMs are not freely available (Scao et al.,\n2022; Ouyang et al., 2022; Thoppilan et al., 2022),\nto the best of our knowledge, only BLOOM is open-\nsource and deployable in a custom infrastructure.\nThe computing resources to use these models re-\nmains challenging for healthcare establishments.\nOne approach to address these issues is to distil\nLLMs into a smaller model via weak supervision.\nWeak supervision has recently gained community\nattention because it alleviates the annotation task.\nThis technique refers to annotating datasets using\nrule-based, heuristic, dictionary extraction or more\nadvanced methods and then training the smaller\nmodel on this dataset. In the same way, knowledge\ndistillation aims to transfer knowledge from a mas-\nter model to a student model. It has often been\nused to compress large-scale models to improve\nmemory footprint and the inference speed (Li et al.,\n2021). Moreover, student models trained through\nknowledge distillation can be more easily moni-\ntored and versioned. Hosting them increases the\nhealthcare centre’s sovereignty, and they become\nmore compliant with existing privacy policies, as\ninput data or predictions don’t leave the building.\n178\n2 Motivation and Contributions\nWe study the use of LLMs in the knowledge distil-\nlation technique via weak supervision in the Multi-\nlingual Clinical domain, especially in clinical en-\ntity extraction. We extend the Agrawal et al. (2022)\nstudy in the sense that we propose an in-depth study\nof the use of InstructGPT-3 to annotate a training\ndataset. Our work 1 mainly aims to compare the\nannotation quality using weak supervision tasks on\na smaller model (Figure 1). Finally, we propose\nto combine annotations provided by InstructGPT-3\nand the dictionary extraction method.\nThis takes form in these contributions:\n• We show that InstructGPT-3 distillation (Fig-\nure 1 middle) is a competitive technique com-\npared to classic weak-supervision techniques\nin a multilingual clinical domain;\n• We propose a weak supervision approach\n(Figure 1 bottom) that combines annotations\nfrom dictionary extraction and InstructGPT-3,\nwhich outperform the approach with only\nInstructGPT-3 annotation.\n3 Related Works\nWeak Supervision deep learning approach has\nachieved remarkable success in several domains be-\nyond NLP (Zhang et al., 2022). However, the main\nbottleneck is collecting massively annotated data.\nTo address this issue, weak supervision replaces\nground-truth annotation with automatic annotation\nbased on heuristic rules, gazetteers or constraints\nlinguistic rules to address. Some techniques called\ndistant supervision exploit semantic links from\nknowledge bases or ontologies (Lison et al., 2021).\nKaramanolakis et al. (2021) proposes an iterative\nself-training method to combine classic weak su-\npervision and inference of the learning model to\nextract entities not covered by the initial heuristic\nrules. In the clinical domain, weak supervision has\nalready been used for specific use cases (Cusick\net al., 2021; Fries et al., 2021; Wang et al., 2019).\nClinical Language Models In the clinical con-\ntext, some specific terms are underrepresented or\nabsent in the general domain. As a result, the\nclinical NLP community has pretrained Language\nModels (LMs) (Alsentzer et al., 2019; Lee et al.,\n2020; Alsentzer et al., 2019) over domain-specific\n1codebase: https://github.com/arkhn/bio-nlp2023.\ncorpora (i.e. MIMIC-III (Johnson et al., 2016),\nPubmed abstracts). These models could be trained\nfrom scratch or from checkpoint to specialize a\ndomain-agnostic model (Gururangan et al., 2020).\nThough, the performance gains are marginal\ncompared to the general language model. The\nstructure and the abbreviated text present in clinical\nnotes hurt performance. Instead of pretraining a\nspecialized clinical model, machine learning prac-\ntitioners can fine-tune agnostic-domain LLMs such\nas the GPT family of models or T5 on the clini-\ncal task. Fine-tuned general-purpose models have\nproven effective in clinical question-answering,\nprotected health information de-identification, and\nrelation extraction (Lehman et al., 2023). But this\napproach requires an important infrastructure and\na regular re-finetuning if the data distribution of\nthe EHR shifts. Nevertheless, some LLMs have\nbeen trained from scratch over clinical domain-\nspecific notes such as GatorTron (Yang et al., 2022),\nBioGPT (Luo et al., 2022) or ClinicalT5 (Lu et al.,\n2022) who achieved promising performance on sev-\neral tasks. Additionally, in-context learning with\nagnostic LLMs such as InstructGPT-3 (Ouyang\net al., 2022) where no weight is modified shows\ngood results (Agrawal et al., 2022; Brown et al.,\n2020) and outperforms specialized smaller models\non several clinical tasks.\nPrompt-based Method Prompt-based learning\nfor generative language model treats a downstream\ntask as a language modelling problem where a lan-\nguage model predicts the next tokens of the instruc-\ntion given a textual prompt (Sainz et al., 2021).\nIn this paradigm, instead of fine-tuning a model\nto a downstream task ( \"pre-train, fine-tune and\npredict\"), we manipulate the behaviour of a pre-\ntrained LM using an appropriate prompt to give\nthe desired output ( \"pre-train, prompt and pre-\ndict\"). prompt engineering explores the most suit-\nable prompt method applied to a LM to solve a task.\nThis way, an unsupervised pre-trained LM can be\nused for many tasks (Liu et al., 2023).\nAmong these methods, in-context learning is the\nmost popular method for information extraction,\nquestion-answering or sentiment analysis. In the\nclinical domain, some works exist on information\nretrieval and question-answering. The prompt con-\ntains three components: the examples’ template,\nthe set of examples and the ordering of prompts,\nsuch as present in Figure 5. The aim is to provide\nsome training examples in the prompt before the\n179\nFigure 1: The different workflows we experiment with. The last workflow used a combination of InstructGPT-3 and\ndictionary annotations; we tested different proportions of these annotations as described in 5.2.\ntest example. However, the chosen examples and\ntheir ordering and the format could impact perfor-\nmance (Zhao et al., 2021); these three components\nmust be tuned to optimize performance.\nIn another way, we can cite works around the\nchain of thoughts (CoT). This encourages the LLM\nto explain its reasoning to get more accurate results,\nespecially in mathematical and logical reasoning\n(Wei et al., 2022; Cobbe et al., 2021). This tech-\nnique could be used with reason edited manually in\nthe prompt or with two separate prompts where the\nfirst involves a reasoning task, then concatenated\nwith the second prompt involving the main tasks.\nOther techniques involve generated knowledge\nsimilar to CoT. Instead of reasoning, the first\nprompt generates potentially useful information\nassociated with the tasks concatenated in the final\nprompt (Liu et al., 2022).\n4 Method\n4.1 Creating Annotations and Knowledge\nDistillation via Weak Supervision\nAnnotation Extraction from LLM Output Our\nstudy is inspired by the method developed in this\npaper (Agrawal et al., 2022). Their works bench-\nmark how InstructGPT-3 (Ouyang et al., 2022) per-\nform clinical NLP tasks in English. They show\nthat InstructGPT-3performs well in several clinical\ntasks. They introduce 3 new datasets to benchmark\nfew-shot clinical information extraction to achieve\nthis. Also, they introduce guided prompt design\nto induce easy-to-structure output with resolvers\n(or parsers) to convert the output into a structured\nprediction easily. Our work differs in the following:\n1. Our studies areas are knowledge distillation\nvia weak supervision and the improvement\nof this technique combining annotations from\nLLMs and dictionary extraction;\n2. our methods are applied in a multilingual con-\ntext, the initial work was only done in English;\n3. we focus on the clinical entities extraction task\nbased on the E3C dataset guidelines.\nIn this work, the LLM is used only as a predictor;\nwe only query the model, no additional fine-tuning\nstep has been realized, and we can only access\ninference parameters such as temperature, top p,\nfrequency or presence penalty. We set the temper-\nature and top p to 0 to control randomness and\nhave a deterministic behaviour. So as not to pe-\nnalize repetitions, we set the presence penalty and\n180\nfrequency penalty to 0. We use an InstructGPT-3\nmodel (text-davinci-003) (Ouyang et al., 2022) to\ninfer the whole annotations for all our experiments.\nWe provide the model with an instruction concate-\nnated by the example to be predicted (Figure 5).\nThe output of InstructGPT-3 is a string of charac-\nters that we must structure to align the predicted\nclinical entities with the initial text (Figure 2).\nThe task is to annotate the words (or tokens) of\na sentence x ∈Σ∗with a set of labels such that\nL= {O,Bclin,Iclin}where O denotes a word in the\ntext without a label, Bclin the first word of a clini-\ncal entity and Iclin the following words according\nto the format IOB (Ramshaw and Marcus, 1995).\nThe goal is to identify the labels O, Bclin, Iclin\nand their character offsets in the sentence x. The\ntask output is defined as ˆy= [y1,y2,...,y n] ∈Y,\nwhere ˆy is the set of predicted annotations, and\nyi = ⟨si,ei,li⟩such that si is the start offset, ei is\nthe end offset and li ∈Lof the ith annotation.\nAs mentioned, a prompt-based method requires\nconcatenating a template t∈Σ∗with our sentence\nxto give our prompt, such asp= concat(t,x). We\nproduce our output o∈Σ∗from our LLM model Φ\nsuch as o= Φ(p,θh), where θh represents the set\nof hyperparameters (temperature, top p, frequence\npenalty, presence penalty).\nThen, we structure osuch as Σ∗ →Y using a\nsimple string-matching function to produce a set of\nlabels ˆywhere ris our resolver applying the string\nmatching function: r(o,x) = ˆy.\nKnowledge Distillation via Weak Supervi-\nsion Finally, the annotations generated via\nInstructGPT-3 prediction are used as a training\ndataset to fine-tune a smaller language model to\na NER downstream task. For smaller language\nmodels, we limit our study to encoder models as\nmentioned in Table 1.\n4.2 Prompting\nWe prime the model with three annotated data\npoints, each corresponding to a sentence from our\ncorpus (Table 2). For each language, we try 3\nsets of data points. For each of them, we test\nthe F1-Score performance of InstructGPT-3 on the\ntest dataset (gold standard), and we select the set\nwith the best F1-Score to perform prediction on\nthe unannotated dataset. We insert keywords as-\nsociated with the E3C guideline definition of the\nclinical entities into prompts. We add guidance to\nexplicit the response structure to facilitate parsing\nthe output (Agrawal et al., 2022) (Figure 5.2).\n5 Experiments\n5.1 Dataset\nWe use the annotated E3C multilingual dataset\n(Magnini et al., 2020) for our experiments, con-\nsisting of two annotation types: temporal and clini-\ncal entities. The languages supported are English\n(en), Basque (eu), Spanish (es), French (fr) and\nItalian (it). Clinical entities are identified as pa-\ntient disorders which could map to the UMLS meta\nthesaurus. The annotators have linked extracted\nclinical entities and UMLS concepts. In our ex-\nperiments, we only extract clinical entities without\nmapping UMLS concepts. The E3C dataset is or-\nganized into 3 layers. A layer consists of a subset\nof files from each language annotated in a certain\nway (manually, semi-automatically) depending on\nthe layer:\n• the first layer (gold standard) consists of the\nfull manual annotation; we used this layer as\na test set for our experiments;\n• The second layer consists of semi-automatic\nannotation; we use this layer as a train set\nwith the initial annotation or the annotation in-\nferred by InstructGPT-3. Moreover, we have\naccess to two states of this layer; the first is\nthe layer entirely annotated with dictionary ex-\ntraction (silver); the second is a subset of this\nlayer (only 10%) that has been fixed manually\n(silver with fixed annotations). The dictio-\nnary contains terms from UMLS and terms\nextracted from gold standard;\n• Finally, the third layer ( layer 3) is unanno-\ntated, which we don’t use for our experiments.\nAs mentioned above, the E3C dataset is well-\nsuited for our weak-supervision studies. But, the\ndataset has limited data in its various layers (Table\n2). To address this limitation, we divide silver into\nfive parts using 5-fold cross-validation. For silver\nwith fixed annotations, we use the entire data as\nthe training set. Our experiments employ multi-\nple models for each language and relied solely on\nxlm-roberta-base in a multilingual context. The\nresults presented in our work are an aggregation of\nthe means and standard deviations across models\nand folds. However, each experiment result and\nmodel are reported in Appendix 7.\n181\nx= ’The patient had presented a progressive deterioration of the general condition,\na fever and night sweats.’\np= concat(t,x)\no= Φ(p,θh) =’-\"fever\"\n-\"night sweats\"’\nr(o,x) = [\n(The,0,3,O),(patient,4,11,O),...,\n(fever,72,77,Bclin),(and,79,82,O),\n(night,83,87,Bclin),(sweats,88,94,Iclin),...\n]\nFigure 2: our method’s prediction and structuring steps on an example. The ttemplate is illustrated in Figure 5.\nLanguage Models\nen emilyalsentzer/Bio_ClinicalBERT(Alsentzer et al., 2019)\nroberta-base(Liu et al., 2019)\nxlm-roberta-base(Conneau et al., 2019)\nes BSC-LT/roberta-base-biomedical-es(Carrino et al., 2022)\ndccuchile/bert-base-spanish-wwm-cased(Cã et al., 2020)\nxlm-roberta-base\neu ixa-ehu/berteus-base-cased(Agerri et al., 2020)\nxlm-roberta-base\nfr Dr-BERT/DrBERT-7GB(Labrak et al., 2023)\ncamembert-base(Martin et al., 2019)\nxlm-roberta-base\nit dbmdz/bert-base-italian-cased(Schweter, 2020)\nxlm-roberta-base\nTable 1: The models used for each language during our\nexperiments. We mention in this table the name of the\nmodel in the huggingface model repository\n5.2 Experimental Setup\nWe conduct experiments on the clinical entity ex-\ntraction tasks. For each language, we use models\nmentioned in Table 1 as a student model for the\nknowledge distillation step. We conduct our ex-\nperiments we use five different dataset settings.\nFor Monolingual Setting (SMonoSilver ), Gold\nSetting (SMonoGold ), SMonoSilver ∩SMonoGold\n(SMonoGold ∩MonoSilver ) and each language, we\nuse the silver of the corresponding language. For\nthe Multilingual Setting (SMultiSilver ) and each\nlanguage, we concatenate the silver of the whole\nlanguages in E3C to constitute the train set. Fi-\nnally, for all settings and each language, we test\nour method on the gold standard of the language\nwe experiment with.\n• Monolingual Setting (SMonoSilver ): We use\na ratio r to control the mix of annotations,\nwith rrepresenting the proportion of annota-\ntions from dictionary extraction and (1 −r)\nrepresenting the proportion of annotations\nfrom InstructGPT-3. If r= 1, the models are\ntrained using only InstructGPT-3 annotations,\nwhile if r = 0, the models are trained exclu-\nsively with dictionary extraction annotations.\nWe test and compare the performance of the\ntrained models using various ratio values of r.\n• Gold Setting (SMonoGold ): we use silver\nwith fixed annotationsas the train set, and we\ncompare encoder models trained on manually\ncorrected annotation (r= 0) and an encoder\nmodel trained on the same subset but using\nInstructGPT-3 prediction annotations (r= 1);\n• SMonoGold ∩MonoSilver : we use silver as the\ntrain set. Still, we replace weak-supervision\nannotations with the annotation fixed in silver\nwith fixed annotations. So, a small part of\nthe InstructGPT-3 prediction annotations and\nthe dictionary extraction annotations has been\nreplaced by manual annotations;\n• Multilingual Setting (SMultiSilver ): we use\nthe same setting as SMonoSilver except we are\non multilingual training context. For this set-\nting, our trained models are multilingual lan-\nguage models. We use xlm-roberta-base.\n5.3 Results\nInstructGPT-3 Prediction Analysis For silver,\nwe observe that InstructGPT-3 extracts more enti-\nties than original extraction (Table 2). This trend\nis reduced in English and Spanish even if we ob-\nserved a more important quantity of Iclin in tokens\nannotated by InstructGPT-3 for all languages. For\nsilver with fixed annotations, the quantity of to-\nkens annotated by both methods (InstructGPT-3 vs\n182\nLanguage Layer TokensBclin Iclin Bclin+Iclin\nGold GPT Gold GPT Gold GPT\nen l2 5852021341438 10361595 31703033lval 6646 254 149 137 130 391 279\nes l2 5706526252245 1298185739234102lval 6291 329 236 269 159 598 395\neu l2 18365 482800 63 482 545 1282lval 4819 327 207 245 143 572 350\nfr l2 59998 20132402840 223928534641lval 6452 267 295 244 225 511 520\nit l2 60248 16432099793 162824363727lval 6538 224 223 147 199 371 422\nTable 2: The number of annotated tokens for each an-\nnotation type for the silver (l2) and silver with fixed\nannotations (lval). The notation Gold corresponds to\nthe original extraction, and the notationGPT correspond\nto the InstructGPT-3 annotation.\nLanguage F1-Score\nInstructGPT-3 distilled models\nen 0.71 0.66 ± 0.01\nes 0.74 0.70 ± 0.02\neu 0.60 0.61 ± 0.04\nfr 0.74 0.75 ± 0.01\nit 0.63 0.75 ± 0.01\nTable 3: The mean F1-score of the models for each lan-\nguage in E3C using gold standard as evaluation set.\nWe evaluate the direct output ofInstructGPT-3 and the\naggregated mean score of each model for each language\nlisted in Table 1 using SMonoSilver withr ∈{0,1}\nand InstructGPT-3 annotation as a train set.\nmanually corrected annotation) is relatively equiva-\nlent.\nKnowledge Distillation Evaluation We com-\npare the distilled model and InstructGPT-3 on the\ngold standard (Table 3). Distillation is beneficial\nin terms of performance for Basque, French and\nItalian. Moreover, we denote a remarkable gap\nbetween InstructGPT-3 (0.63) and distilled mod-\nels (0.75) in Italian. Spanish and English have re-\nversed trends: InstructGPT-3 performs better than\ndistilled models. This echoed the exception we ob-\nserved in the InstructGPT-3 Prediction Analysis\nparagraph.\nSMonoSilver withr ∈{0,1} If we compare the\nglobal F1-Score (Table 4), the distilled models\n(r = 1) perform better than the weak-supervised\nmodels (r= 0) trained. In detail, distilled models\ndisplay a better recall and recognizes multi-word\nclinical terms more easily. Still, this flexibility, bal-\nanced by the too-biased detection of false positive\nen es eu fr it\nLanguage\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8F1-Score\nFigure 3: A graph with the mean F1-score of the models\non the y-axis and the different language on the x-axis\nfor the SMonoSilver withr ∈{0,1}. The orange bar\nrepresents the distilled models F1-Scores whereas the\ndotted blue bars represent the weak-supervised models\nF1-score.\nterms, lowered the precision score. In comparing\ndistilled models versus weak-supervised models\n(Figure 3), we note a noticeable performance gain\nof almost 0.1 in Basque, followed by French and\nItalian. In English, the F1-score of both models\nis relatively equivalent. For Spanish, the weak-\nsupervised models outperformed the distilled mod-\nels and still has our highest F1-Score.\nSMonoGold The amount of annotated tokens in\nsilver with fixed annotations is relatively small\ncompared to silver (Figure 2). This hurts the re-\nsult (Table 4) of the distilled models (0.61 with\nsilver with fixed annotations vs 0.70 with silver)\nin contrast to the weak-supervised models, where\nperformance has gained0.03 (0.70 with silver with\nfixed annotations vs 0.67 with silver). The weak-\nsupervised models performance is relatively bet-\nter than the distilled models. Moreover, the dis-\ntilled models recall performance seems to be af-\nfected by the small amount of data and annotated\ntokens (0.68 for SMonoSilver withr = 1 vs 0.58\nfor SMonoGold withr = 1).\nSMonoGold ∩MonoSilver The results (Table 4)\nshow better performance for both models when\nwe mix a slight quantity of manually annotated\ndata with silver. The distilled models outperforms\nthe weak-supervised models with a gain of 0.03.\nIn both cases, the F1-Score gain is due to the im-\nprovement of the recall: we obtain a gain of 0.05\ncompared to the SMonoSilver withr ∈{0,1}.\n183\nSMonoSilver For all languages except Basque, we\nobtained better results when we mixed weak super-\nvised and InstructGPT-3 annotations. The local\noptimum for these languages is reached when r∈\n[0.4,0.6] (Figure 4). The Basque doesn’t follow\nthis trend; using a dataset with only InstructGPT-3\nannotations (where r = 1 ) gives the best result\namong all tried rvalues.\nSMultiSilver Using a multilingual train set and\nLM ( xlm-roberta-base) gives inferior results\ncompared to SMonoSilver (Table 5). Though we\nobtain better results in Italian than the SMonoSilver\n(+0.01); the optimum is set tor= 0.8. In the other\ncase, mixing annotations described in 5.2 don’t af-\nfect results as observed in SMonoSilver due to the\nnoise generated by the multilingual nature of the\ntrain set.\n5.4 Discussion\nOur experiments using the E3C dataset demon-\nstrate the potential of knowledge distillation and\nweak supervision in the context of clinical entity\nextraction tasks. We observe that distilled models\noutperform classic weak supervision approaches,\nespecially in Basque, French, and Italian languages.\nHowever, we notice some interesting trends in En-\nglish and Spanish that require further analysis.\nThe trend is reversed in Spanish, with the weak-\nsupervised models performing better than the dis-\ntilled ones. For all the models we trained for Span-\nish (Table 1), we don’t distinguish any difference\nbetween monolingual, agnostic domain, multilin-\ngual, or medical monolingual language models.\nOne possible explanation for these trends is the\ndifference in data sources. While the corpora for\nother languages come from the Pan African Journal\nor Pubmed, the Spanish corpus is sourced from the\nSPACCC corpus. The clinical entity distribution\nand semantic differences from this source could\nbias our results. Moreover, additional data cleaning\nhas been applied to layer 1, such as sentence and\npunctuation removal and capitalization, which may\nreinforce this difference between the languages.\nIn English, the difference in performance be-\ntween distilled and weak-supervised models is rel-\natively small compared to other languages. This\ncan be attributed to the superior quality of annota-\ntions in the silver. The English lexicon resource\n(supplied by the UMLS meta-thesaurus and terms\nextracted in gold standard) employed for mapping\nclinical entities in the text is likely more exten-\n0.80.60.50.40.20.0 1.0\nRatio\n0.78\n0.75\n0.71\n0.65\n0.61\n0.55\n0.50\n0.45 F1 Score\nen es eu fr it\n(a) SMonoSilver\n1.00.80.60.50.40.20.0\nRatio\n0.78\n0.75\n0.71\n0.65\n0.61\n0.55\n0.50\n0.45 F1 Score\nen es eu fr it\n(b) SMultiSilver\nFigure 4: The line plots with the mean F1-score of the\nmodels on the y-axis and the ratio of dictionary anno-\ntations and annotations via InstructGPT-3 on the x-axis\nfor SMonoSilver and SMultiSilver as described in 5.2.\nA ratio of r = 0 indicates the presence of only dictio-\nnary annotations, while a ratio of r= 1 corresponds to\nexclusively InstructGPT-3 annotations. Each coloured\nline represents the result for a language\nsive and precise than those accessible for other\nlanguages with fewer linguistic resources.\nFurthermore, SMonoSilver reveals that com-\nbining annotations from Dictionary extraction\nand InstructGPT-3 marginally outperforms when\nr= 1. Integrating various annotation sources\nshows promise and typically enhances model gen-\neralization. However, in the case of Basque,\nSMonoSilver does not yield the best results when\nwe have only InstructGPT-3 annotations (r= 1).\nAs we raise the ratio r, we observe a gradual im-\nprovement in F1-score. It can be explained by the\noriginal annotations from silver in Basque was cre-\nated using a low-resource lexicon. As shown in\nTable 2, only 63 Iclin tokens were initially anno-\n184\nSetting F1-Score Precision Recall\nr = 1 r = 0 r = 1 r = 0 r = 1 r = 0\nSMonoSilver 0.70 ± 0.06 0.67 ± 0.09 0.73 ± 0.03 0.78 ± 0.09 0.68 ± 0.09 0.63 ± 0.10\nSMonoGold 0.61 ± 0.09 0.70 ± 0.06 0.72 ± 0.03 0.75 ± 0.04 0.58 ± 0.10 0.69 ± 0.05\nSMonoGold ∩ MonoSilver 0.73 ± 0.03 0.71 ± 0.08 0.74 ± 0.04 0.78 ± 0.05 0.73 ± 0.06 0.68 ± 0.08\nTable 4: The F1-score, Precision and the Recall for the different settings in section 5.2. r= 1 corresponds to the\ndistilled models and the r= 0 corresponds to the weak-supervised models.\nSetting F1-Score Precision Recall\nrmax rmax rmax\nSMonoSilver 0.72 ± 0.06 0.75 ± 0.02 0.71 ± 0.08\nSMultiSilver 0.69 ± 0.06 0.79 ± 0.03 0.65 ± 0.08\nTable 5: The F1-score, Precision, and Recall for\nSMonoSilver and SMultiSilver as described in Sec-\ntion 5.2. The scores are aggregated across languages,\nwith rmax representing the optimal value of r.\ntated, in contrast to 482 tokens for InstructGPT-3\nannotations.\nIn the case of SMultiSilver , we did not observe\nany significant results. The performance of Span-\nish, Italian, and French languages either experi-\nenced a slight improvement or was unaffected by\nthe multilingual composition of the training dataset.\nHowever, this setting negatively impacted English\nand Basque. The predominance of Romance lan-\nguages in the dataset could be the cause.\nMoreover, Basque is a distinct and isolated lan-\nguage with unique linguistic structures. The other\nlanguages in the training dataset are linguistically\ndistant, which may introduce noise during the train-\ning process and consequently affect the perfor-\nmance of the Basque model.\nAnother interesting observation is that\nInstructGPT-3 extracts almost twice as many\nentities as the original extraction method (Figure\n2). This trend is more pronounced in silver, while\nthe number of annotated tokens insilver with fixed\nannotations is almost equivalent between both\nannotation sets, likely due to human validation.\nThis difference could be explained by the fact\nthat InstructGPT-3 has no access to the guidelines,\nand the prompt mentioned to extract \"disorders,\"\n\"disease,\" or \"symptoms\" is less restrictive than\nthe E3C guideline annotation.\nOur results highlight the potential of knowledge\ndistillation and weak supervision for clinical entity\nextraction, particularly for languages with more\nlimited resources. Though, data sources, annota-\ntion quality, and the comprehensiveness of linguis-\ntic resources influence the performance of these\nmethods. Further research is needed to address\nthese challenges and improve our methods.\n6 Limitation\nOne limitation of our study is the small size of the\ntest set, which may impact the generalizability of\nour results. Additionally, we restrained our work\non clinical entity extraction; in future work, we\nwould investigate more in several tasks using the\nE3C temporality layer to cover a task of Name\nEntity Recognition and Relation Extraction tasks.\nFinally, the E3C guidelines have been designed\nfor clinical entity extraction and entity-linking via\nUMLS entities. After the first step of manual anno-\ntation, some spans of the entities have been mod-\nified to fit as close as possible to the semantical\nconcepts found in UMLS (Magnini et al., 2020).\nFor instance, clinical entities could be split into sep-\narate disorder concepts, and the extent of a disorder\ncandidate could be reduced to fit with a concept.\nThese biases could induce additional difficulties in\nfinding the correct span for a given model.\n7 Conclusion\nOur results demonstrate that the knowledge distilla-\ntion with InstructGPT-3 outperforms the dictionary\nsupervision for extracting clinical entities.\nWe show that mixing these approaches to build a\ntraining dataset brings diversity to the annotations\nand improves the distilled model performance.\nWeak-supervision approach with LLMs is rel-\natively promising for creating a training dataset.\nThis reduces the annotation cost and, at the same\ntime, focuses the manual annotation on the test set,\nwhich is one of the most prominent parts of high-\nstake domains like healthcare. Furthermore, the\ninterest of the approach is also to fine-tune a small\nto medium-sized LM that may be used locally with-\nout the leak of confidential medical data and with\na reduced energy cost. In a low-resource context,\nsuch as Basque, LLMs offer a competitive alter-\n185\nnative to the classic weak supervision technique,\nwhich requires linguistic resources.\nFurthermore, we aim to investigate advanced\ntechniques to combine various annotations by in-\ncorporating confidence measures from the different\npredictions. Using other LLMs predictions and en-\nsemble, the difference could be pertinent because\nthe annotation diversity can improve a model’s per-\nformance, as we observed on SMonoSilver (Figure\n4). Additionally, we will consider utilising per-\nformance metrics (such as recall and precision) to\ndecide which type of annotations (begin or inner-\ntokens) to retain for each prediction method.\nFinally, adapting CoT or generated knowledge\n(Wei et al., 2022; Cobbe et al., 2021) for clinical\nentity extraction could improve LLM’s precision.\nTo our knowledge, none of these techniques has\nbeen adapted to clinical information retrieval. We\ncould craft a prompt with different annotation steps\nthrough different examples. At each annotation\nstep, we describe a precise instruction and its result.\nFor example, incorporating the three steps of the\nE3C annotation into the prompt could help encour-\nage the LLM to better adhere to the guideline.\nReferences\nRodrigo Agerri, Iñaki San Vicente, Jon Ander Campos,\nAnder Barrena, Xabier Saralegi, Aitor Soroa, and\nEneko Agirre. 2020. Give your Text Representation\nModels some Love: the Case for Basque. LREC\n2020 - 12th International Conference on Language\nResources and Evaluation, Conference Proceedings,\npages 4781–4788.\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large Lan-\nguage Models are Few-Shot Clinical Information Ex-\ntractors. Association for Computational Linguistics,\npages 1998–2022.\nEmily Alsentzer, John R. Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nB. A. McDermott. 2019. Publicly Available Clinical\nBERT Embeddings. Association for Computational\nLinguistics, pages 72–78.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam Mccandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei Ope-\nnai. 2020. Language Models are Few-Shot Learners.\nAdvances in neural information processing systems,\n33:1877–1901.\nJosé Cã, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui\nHo, Hojin Kang, and Jorge Pérez. 2020. SPANISH\nPRE-TRAINED BERT MODEL AND EV ALUA-\nTION DATA.ICLR, pages 1–10.\nCasimiro Pio Carrino, Jordi Armengol-Estapé, Asier\nGutiérrez-Fandiño, Joan Llop-Palao, Marc Pàmies,\nAitor Gonzalez-Agirre, and Marta Villegas. 2022.\nPretrained Biomedical Language Models for Clini-\ncal NLP in Spanish. Association for Computational\nLinguistics, pages 193–199.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training Verifiers to Solve Math Word Prob-\nlems.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\nCross-lingual Representation Learning at Scale. Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics, pages 8440–8451.\nMarika Cusick, Prakash Adekkanattu, Thomas R. Cam-\npion, Evan T. Sholle, Annie Myers, Samprit Banerjee,\nGeorge Alexopoulos, Yanshan Wang, and Jyotish-\nman Pathak. 2021. Using weak supervision and deep\nlearning to classify clinical notes for identification\nof current suicidal ideation. Journal of Psychiatric\nResearch, 136:95–102.\nJean-Baptiste Escudié, Bastien Rance, Georgia Mala-\nmut, Sherine Khater, Anita Burgun, Christophe Cel-\nlier, and Anne-Sophie Jannot. 2017. A novel data-\ndriven workflow combining literature and electronic\nhealth records to estimate comorbidities burden for a\nspecific disease: a case study on autoimmune comor-\nbidities in patients with celiac disease. BMC Medical\nInformatics and Decision Making, 17(1):140.\nJason A. Fries, Ethan Steinberg, Saelig Khattar, Scott L.\nFleming, Jose Posada, Alison Callahan, and Nigam H.\nShah. 2021. Ontology-driven weak supervision\nfor clinical entity classification in electronic health\nrecords. Nature Communications, 12(1).\nJason Alan Fries, Natasha Seelam, Gabriel Altay, Leon\nWeber, Myungsun Kang, Debajyoti Datta, Ruisi Su,\nSamuele Garda, Bo Wang, Simon Ott, Matthias\nSamwald, and Wojciech Kusa. 2022. Dataset Debt in\nBiomedical Language Modeling. Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, 5:137–145.\nSuchin Gururangan, Ana Marasovícmarasovíc, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah Smith. 2020. Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks. 58th\n186\nAnnual Meeting of the Association for Computational\nLinguistics, pages 8342–8360.\nAlistair E.W. Johnson, Tom J. Pollard, Lu Shen,\nLi Wei H. Lehman, Mengling Feng, Mohammad\nGhassemi, Benjamin Moody, Peter Szolovits, Leo\nAnthony Celi, and Roger G. Mark. 2016. MIMIC-III,\na freely accessible critical care database. Scientific\ndata, 3.\nGiannis Karamanolakis, Subhabrata Mukherjee, Guo-\nqing Zheng, and Ahmed Hassan Awadallah. 2021.\nSelf-Training with Weak Supervision. Association\nfor Computational Linguistics, pages 845–863.\nYanis Labrak, Adrien Bazoge, Richard Dufour, Mick-\nael Rouvier, Emmanuel Morin, Béatrice Daille, and\nPierre-Antoine Gourraud. 2023. DrBERT: A Ro-\nbust Pre-trained Model in French for Biomedical and\nClinical domains. Association for Computational\nLinguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. BioBERT: A pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nEric Lehman, Evan Hernandez, Diwakar Mahajan,\nJonas Wulff, Micah J. Smith, Zachary Ziegler, Daniel\nNadler, Peter Szolovits, Alistair Johnson, and Emily\nAlsentzer. 2023. Do We Still Need Clinical Language\nModels? arXiv preprint arXiv:2302.08091.\nLei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou,\nand Xu Sun. 2021. Dynamic Knowledge Distillation\nfor Pre-trained Language Models. Association for\nComputational Linguistics, pages 379–389.\nPierre Lison, Jeremy Barnes, and Aliaksandr Hubin.\n2021. skweak: Weak Supervision Made Easy for\nNLP. arXiv preprint arXiv:2104.09683.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, Hannaneh Ha-\njishirzi, and Paul G Allen. 2022. Generated Knowl-\nedge Prompting for Commonsense Reasoning. Asso-\nciation for Computational Linguistics, 1:3154–3169.\nPengfei Liu, Weizhe Yuan, Zhengbao Jiang, Hiroaki\nHayashi, Graham Neubig, Jinlan Fu, W Yuan,\nZ Jiang, H Hayashi, G Neubig, and J Fu. 2023. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\nACM Computing Surveys, 55(9):1–35.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, Veselin Stoyanov, and\nPaul G Allen. 2019. RoBERTa: A Robustly Op-\ntimized BERT Pretraining Approach. arXiv preprint\narXiv:1907.11692.\nQiuhao Lu, Dejing Dou, and Thien Huu Nguyen. 2022.\nClinicalT5: A Generative Language Model for Clini-\ncal Text.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBioGPT: Generative Pre-trained Transformer for\nBiomedical Text Generation and Mining. Briefings\nin bioinformatics, 23(6).\nBernardo Magnini, Begoña Altuna, Alberto Lavelli,\nManuela Speranza, and Roberto Zanoli. 2020. The\nE3C Project: Collection and Annotation of a Multi-\nlingual Corpus of Clinical Cases. CEUR Workshop\nProceedings, 2769.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2019. CamemBERT: a Tasty French Lan-\nguage Model. Association for Computational Lin-\nguistics, pages 7203–7219.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. Advances in Neural Information\nProcessing Systems.\nLance A. Ramshaw and Mitchell P. Marcus. 1995.\nText Chunking using Transformation-Based Learn-\ning. Third Workshop on Very Large Corpora, pages\n157–176.\nS. Trent Rosenbloom, Joshua C. Denny, Hua Xu, Nancy\nLorenzi, William W. Stead, and Kevin B. Johnson.\n2011. Data from clinical notes: A perspective on\nthe tension between structure and flexible documen-\ntation. Journal of the American Medical Informatics\nAssociation, 18(2):181–186.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka,\nAnder Barrena, and Eneko Agirre. 2021. Label Ver-\nbalization and Entailment for Effective Zero- and\nFew-Shot Relation Extraction. EMNLP 2021 - 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, Proceedings, pages 1199–1212.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, and et al. 2022. BLOOM: A 176B-\nParameter Open-Access Multilingual Language\nModel. arXiv preprint arXiv:2211.05100.\nStefan Schweter. 2020. Italian BERT and ELECTRA\nmodels.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaguang Li, Hongrae Lee Huaixiu, Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\n187\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel, Morris Tulsee,\nDoshi Renelito, Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe Google. 2022. LaMDA: Language Models for Di-\nalog Applications. arXiv preprint arXiv:2201.08239.\nYanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen\nShen, Liwei Wang, Elizabeth J. Atkinson, Shreyasee\nAmin, and Hongfang Liu. 2019. A clinical text clas-\nsification paradigm using weak supervision and deep\nrepresentation. BMC Medical Informatics and Deci-\nsion Making, 19(1):1–13.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,\nand Denny Zhou. 2022. Chain of Thought Prompt-\ning Elicits Reasoning in Large Language Models.\nNeurIPS.\nXi Yang, Aokun Chen, Nima PourNejatian, Hoo Chang\nShin, Kaleb E. Smith, Christopher Parisien, Colin\nCompas, Cheryl Martin, Anthony B. Costa, Mona G.\nFlores, Ying Zhang, Tanja Magoc, Christopher A.\nHarle, Gloria Lipori, Duane A. Mitchell, William R.\nHogan, Elizabeth A. Shenkman, Jiang Bian, and\nYonghui Wu. 2022. A large language model for elec-\ntronic health records. npj Digital Medicine, 5(1).\nJieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang,\nand Alexander Ratner. 2022. A Survey on Pro-\ngrammatic Weak Supervision. arXiv preprint\narXiv:2202.05433.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate Before Use: Improv-\ning Few-Shot Performance of Language Models. In-\nternational Conference on Machine Learning.\nAppendix\nInput: The evolution was marked two months later,\nby the appearance of angiomatous plaques on the\nright forearm, [...]\nextract the exact match of disorders, diseases or\nsymptoms mentioned in the text or\nreturn None if there is no clinical entity:\n- \"angiomatous plaques\"\n- \"lymphedema\"\n- \"lesions\"\nInput: At the same time, the patient had presented\na progressive alteration of the general condition,\na fever and night sweats\nextract the exact match of disorders, diseases\nor symptoms mentioned in the text or\nreturn None if there is no clinical entity:\n- \"progressive alteration of the general condition\"\n- \"fever\"\n- \"night sweats\"\nInput: The sedimentation rate was 35mm at the\nfirst hour, C-reactive protein was negative\nand ferritin level was 900µg/l\n(i.e., 4 times the normal value).\nextract the exact match of disorders, diseases\nor symptoms mentioned in the text or\nreturn None if there is no clinical entity:\n- \"None\"\nInput: The interview revealed no history\nof any pathological events, in particular\nskin rash, gastrointestinal disorders, jaundice,\nrespiratory infection or recent vaccination.\nextract the exact match of disorders, diseases\nor symptoms mentioned in the text or\nreturn None if there is no clinical entity:\n- \"\nFigure 5: An example of the prompt used in our ex-\nperiment. The formatted examples are shown in blue ,\nwhile the formatted examples to predict are shown in\norange . The instructions are shown in purple , and the\nguidance, as used in Agrawal et al. (2022), is shown in\ngreen . For all languages, instruction is still in English,\nbut the formatted examples are in the source language.\n188\nLanguage Model F1-Score\nr = 1 r = 0\nen emilyalsentzer/Bio_ClinicalBERT 0.66 ± 0.01 0.68 ± 0.01\nroberta-base 0.67 ± 0.01 0.65 ± 0.01\nxlm-roberta-base 0.66 ± 0.01 0.65 ± 0.01\nes BSC-LT/roberta-base-biomedical-es 0.72 ± 0.01 0.78 ± 0.01\ndccuchile/bert-base-spanish-wwm-cased 0.69 ± 0.01 0.76 ± 0.01\nxlm-roberta-base 0.68 ± 0.02 0.73 ± 0.01\neu ixa-ehu/berteus-base-cased 0.60 ± 0.03 0.54 ± 0.03\nxlm-roberta-base 0.61 ± 0.04 0.47 ± 0.01\nfr Dr-BERT/DrBERT-7GB 0.74 ± 0.01 0.70 ± 0.01\ncamembert-base 0.75 ± 0.01 0.69 ± 0.04\nxlm-roberta-base 0.74 ± 0.01 0.72 ± 0.02\nit dbmdz/bert-base-italian-cased 0.74 ± 0.01 0.73 ± 0.00\nxlm-roberta-base 0.75 ± 0.01 0.72 ± 0.02\nTable 6: This table reports the F1-Scores for the different models and annotation ratios r∈{0,1}for SMonoSilver\ndescribed in Section 5.2.\nLanguage Model F1-Score\nr = 1 r = 0\nen emilyalsentzer/Bio_ClinicalBERT 0.60 0.65\nroberta-base 0.61 0.70\nxlm-roberta-base 0.43 0.69\nes BSC-LT/roberta-base-biomedical-es 0.71 0.78\ndccuchile/bert-base-spanish-wwm-cased 0.70 0.77\nxlm-roberta-base 0.62 0.73\neu ixa-ehu/berteus-base-cased 0.54 0.72\nxlm-roberta-base 0.51 0.68\nfr Dr-BERT/DrBERT-7GB 0.73 0.75\ncamembert-base 0.71 0.60\nxlm-roberta-base 0.68 0.68\nit dbmdz/bert-base-italian-cased 0.63 0.70\nxlm-roberta-base 0.55 0.57\nTable 7: This table reports the F1-Scores for the different models and annotation ratios r∈{0,1}for SMonoGold\ndescribed in Section 5.2.\n189\nLanguage Model F1-Score\nr = 1 r = 0\nen emilyalsentzer/Bio_ClinicalBERT 0.70 ± 0.01 0.69 ± 0.01\nroberta-base 0.70 ± 0.01 0.68 ± 0.01\nxlm-roberta-base 0.67 ± 0.01 0.67 ± 0.01\nes BSC-LT/roberta-base-biomedical-es 0.77 ± 0.02 0.80 ± 0.01\ndccuchile/bert-base-spanish-wwm-cased 0.76 ± 0.01 0.78 ± 0.00\nxlm-roberta-base 0.76 ± 0.01 0.76 ± 0.01\neu ixa-ehu/berteus-base-cased 0.70 ± 0.03 0.72 ± 0.02\nxlm-roberta-base 0.70 ± 0.01 0.57 ± 0.09\nfr Dr-BERT/DrBERT-7GB 0.75 ± 0.01 0.73 ± 0.02\ncamembert-base 0.76 ± 0.01 0.74 ± 0.01\nxlm-roberta-base 0.74 ± 0.01 0.69 ± 0.04\nit dbmdz/bert-base-italian-cased 0.75 ± 0.01 0.75 ± 0.01\nxlm-roberta-base 0.74 ± 0.00 0.74 ± 0.01\nTable 8: This table reports F1-Scores for different models and annotation ratios r ∈ {0,1} for\nSMonoGold ∩ MonoSilver described in Section 5.2.\nLanguage Model rmax r= 1 rmax r= 0F1-Score Precision Recall F1-Score Precision Recall F1-Score Precision Recall\nen xlm-roberta-base0.5 0.65±0.01 0.72±0.01 0.63±0.01 0.71±0.01 0.71±0.03 0.71±0.02 0.66±0.01 0.68±0.03 0.64±0.02roberta-base 0.4 0.65±0.01 0.72±0.01 0.62±0.02 0.71±0.01 0.72±0.01 0.71±0.03 0.67±0.01 0.70±0.01 0.65±0.01emilyalsentzer/Bio_ClinicalBERT0.6 0.68±0.01 0.73±0.01 0.66±0.02 0.72±0.01 0.75±0.02 0.70±0.02 0.66±0.01 0.71±0.02 0.63±0.01\nes xlm-roberta-base0.4 0.73±0.01 0.80±0.01 0.70±0.01 0.76±0.01 0.77±0.02 0.76±0.02 0.68±0.02 0.74±0.02 0.65±0.04dccuchile/bert-base-spanish-wwm-cased0.4 0.76±0.01 0.81±0.01 0.73±0.02 0.78±0.00 0.80±0.02 0.76±0.02 0.69±0.01 0.72±0.02 0.66±0.03BSC-LT/roberta-base-biomedical-es0.4 0.78±0.01 0.82±0.01 0.75±0.01 0.79±0.01 0.80±0.04 0.79±0.03 0.72±0.01 0.75±0.02 0.69±0.02\neu xlm-roberta-base1 0.47±0.01 0.63±0.14 0.44±0.01 - - - 0.61 ±0.04 0.73±0.02 0.56±0.05ixa-ehu/berteus-base-cased1 0.54±0.03 0.91±0.01 0.49±0.02 - - - 0.60 ±0.03 0.74±0.02 0.54±0.03\nfr xlm-roberta-base0.4 0.72±0.02 0.80±0.02 0.67±0.03 0.75±0.01 0.75±0.02 0.75±0.02 0.74±0.01 0.74±0.02 0.74±0.02camembert-base 0.6 0.69±0.04 0.83±0.00 0.64±0.04 0.76±0.00 0.79±0.01 0.74±0.01 0.75±0.01 0.74±0.01 0.76±0.01Dr-BERT/DrBERT-7GB0.6 0.70±0.01 0.84±0.00 0.64±0.01 0.76±0.01 0.76±0.01 0.77±0.01 0.74±0.01 0.77±0.04 0.73±0.04\nit xlm-roberta-base0.6 0.72±0.02 0.78±0.02 0.70±0.04 0.75±0.02 0.76±0.02 0.76±0.02 0.75±0.01 0.75±0.02 0.75±0.02dbmdz/bert-base-italian-cased0.8 0.73±0.00 0.76±0.02 0.73±0.02 0.75±0.00 0.72±0.02 0.81±0.02 0.74±0.01 0.70±0.02 0.81±0.02\nTable 9: This table presents F1-Scores, Precision, and Recall for different models at annotation ratios r∈{0,1}\nand at the optimal rvalue, rmax for SMonoSilver , as described in Section 5.2.\nLanguage Model rmax r= 1 rmax r= 0F1-Score Precision Recall F1-Score Precision Recall F1-Score Precision Recall\nen xlm-roberta-base1 0.64±0.01 0.74±0.01 0.60±0.01 - - - 0.66 ±0.03 0.75±0.03 0.62±0.05es xlm-roberta-base0.6 0.71±0.01 0.82±0.01 0.65±0.02 0.74±0.01 0.83±0.02 0.69±0.02 0.71±0.01 0.80±0.03 0.65±0.02eu xlm-roberta-base0.5 0.57±0.02 0.86±0.02 0.51±0.02 0.59±0.02 0.79±0.02 0.54±0.02 0.57±0.01 0.76±0.04 0.51±0.01fr xlm-roberta-base0.6 0.69±0.02 0.83±0.01 0.62±0.02 0.71±0.01 0.81±0.02 0.65±0.02 0.70±0.02 0.80±0.02 0.64±0.03it xlm-roberta-base0.8 0.73±0.01 0.78±0.01 0.71±0.01 0.76±0.01 0.76±0.01 0.77±0.02 0.76±0.01 0.75±0.03 0.78±0.02\nTable 10: This table presents F1-Scores, Precision, and Recall for different models at annotation ratios r∈{0,1}\nand at the optimal rvalue, rmax for SMultiSilver , as described in Section 5.2.\n190",
  "topic": "Confidentiality",
  "concepts": [
    {
      "name": "Confidentiality",
      "score": 0.7966885566711426
    },
    {
      "name": "Computer science",
      "score": 0.7390457391738892
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5897996425628662
    },
    {
      "name": "Information extraction",
      "score": 0.47738951444625854
    },
    {
      "name": "Language model",
      "score": 0.4740181565284729
    },
    {
      "name": "Natural language processing",
      "score": 0.4452053904533386
    },
    {
      "name": "Health care",
      "score": 0.4367813467979431
    },
    {
      "name": "Data modeling",
      "score": 0.4298369288444519
    },
    {
      "name": "Data extraction",
      "score": 0.42365899682044983
    },
    {
      "name": "Data science",
      "score": 0.40650439262390137
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37422436475753784
    },
    {
      "name": "Computer security",
      "score": 0.17518573999404907
    },
    {
      "name": "MEDLINE",
      "score": 0.1746523678302765
    },
    {
      "name": "Database",
      "score": 0.16628366708755493
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I151809719",
      "name": "Arkema (France)",
      "country": "FR"
    }
  ],
  "cited_by": 11
}