{
  "title": "A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification",
  "url": "https://openalex.org/W3150226983",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2354182772",
      "name": "Liu Xuehu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1900172642",
      "name": "Zhang Ping-ping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202112687",
      "name": "Yu, Chenyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227143521",
      "name": "Lu, Huchuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378534183",
      "name": "Qian Xue-sheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1872726248",
      "name": "Yang Xiaoyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2622829582",
    "https://openalex.org/W3042662552",
    "https://openalex.org/W2952186347",
    "https://openalex.org/W3134396714",
    "https://openalex.org/W1596233070",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3035252826",
    "https://openalex.org/W3034417718",
    "https://openalex.org/W2959022568",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W760855798",
    "https://openalex.org/W3099899804",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2953003997",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2948383821",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W2963762690",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2963574614",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2986451890",
    "https://openalex.org/W2531440880",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W114517082",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2963960612",
    "https://openalex.org/W3106853541",
    "https://openalex.org/W3096748692",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2463071499",
    "https://openalex.org/W2798329462",
    "https://openalex.org/W2520433280"
  ],
  "abstract": "Video-based person re-identification (Re-ID) aims to retrieve video sequences of the same person under non-overlapping cameras. Previous methods usually focus on limited views, such as spatial, temporal or spatial-temporal view, which lack of the observations in different feature domains. To capture richer perceptions and extract more comprehensive video representations, in this paper we propose a novel framework named Trigeminal Transformers (TMT) for video-based person Re-ID. More specifically, we design a trigeminal feature extractor to jointly transform raw video data into spatial, temporal and spatial-temporal domain. Besides, inspired by the great success of vision transformer, we introduce the transformer structure for video-based person Re-ID. In our work, three self-view transformers are proposed to exploit the relationships between local features for information enhancement in spatial, temporal and spatial-temporal domains. Moreover, a cross-view transformer is proposed to aggregate the multi-view features for comprehensive video representations. The experimental results indicate that our approach can achieve better performance than other state-of-the-art approaches on public Re-ID benchmarks. We will release the code for model reproduction.",
  "full_text": "A Video Is Worth Three Views: Trigeminal Transformers for Video-based\nPerson Re-identiÔ¨Åcation\nXuehu Liu‚Ä†¬ß Pingping Zhang‚Ä†‚Ä°¬ß Chenyang Yu‚Ä†¬ß Huchuan Lu‚Ä†¬ß Xuesheng Qian¬£ Xiaoyun Yang‚ãÜ\n‚Ä†School of Information and Communication Engineering, Dalian University of Technology,\n¬ßNingbo Institute, Dalian University of Technology,\n‚Ä°School of ArtiÔ¨Åcial Intelligence, Dalian University of Technology,\n¬£China Science IntelliCloud Technology Co.Ltd,\n‚ãÜRemark Holdings\nAbstract\nVideo-based person re-identiÔ¨Åcation (Re-ID) aims to re-\ntrieve video sequences of the same person under non-\noverlapping cameras. Previous methods usually focus on\nlimited views, such as spatial, temporal or spatial-temporal\nview, which lack of the observations in different feature\ndomains. To capture richer perceptions and extract more\ncomprehensive video representations, in this paper we pro-\npose a novel framework named Trigeminal Transformers\n(TMT) for video-based person Re-ID. More speciÔ¨Åcally,\nwe design a trigeminal feature extractorto jointly trans-\nform raw video data into spatial, temporal and spatial-\ntemporal domain. Besides, inspired by the great success\nof vision transformer, we introduce the transformer struc-\nture for video-based person Re-ID. In our work, three self-\nview transformersare proposed to exploit the relationships\nbetween local features for information enhancement in spa-\ntial, temporal and spatial-temporal domains. Moreover, a\ncross-view transformeris proposed to aggregate the multi-\nview features for comprehensive video representations. The\nexperimental results indicate that our approach can achieve\nbetter performance than other state-of-the-art approaches\non public Re-ID benchmarks. We will release the code for\nmodel reproduction.\n1. Introduction\nPerson re-identiÔ¨Åcation (Re-ID) aims to retrieve given\npedestrians across different times and places. Recently,\ndue to the needs of safe communities, intelligent surveil-\nlance and criminal investigations, this task has become a\nhot research topic. Meanwhile, since the widespread de-\nployment of video surveillance in cities and the massive\ndata of pedestrians in videos, researchers are paying more\nand more attention to video-based person Re-ID. Similar to\nFrame\nVideo representation\n3-D\nCNN\nR\nN\nN\nR\nN\nN\nR\nN\nN\nTemporal\nTemp\nTransformer\nSpa\nTransformer\nSpa&Temp\nTransformer\n2-D\nCNN\n2-D\nCNN\n2-D\nCNN\nSpatialSpatio-temporal Trigeminal\nSingle view Three view\nFrame\nVideo representation\n3-D\nCNN\nR\nN\nN\nR\nN\nN\nR\nN\nN\n(a) Temporal view\nTemporal\ntransformer\nSpatial\ntransformer\nSpatial&\nTemporal\ntransformer\n2-D\nCNN\n2-D\nCNN\n2-D\nCNN\n(b) Spatial view\n(c) Spatio-temporal view (d) Trigeminal view\nSingle view\nThree view\nFigure 1. The (a) (b) (c) shows the previous works with single-\nview. The (d) shows the trigeminal views in our work.\nimage-based person Re-ID, video-based person Re-ID also\nfaces many challenges, such as illumination changes, view\npoint difference, complicated backgrounds and person oc-\nclusions. Different from image-based person Re-ID, video-\nbased person Re-ID utilizes multiple image frames as inputs\nrather than single image, which will contain additional mo-\ntion cues, pose variations and multi-view observations. Al-\nthough those information is conducive to pedestrian recog-\nnition, it also brings more noises and misalignments. Thus,\nhow to fully utilize the abundant information in sequences\nis worthy of research in video-based person Re-ID.\nIn video-based person Re-ID, researchers have explored\nvarious methods to process video data. Some of previous\nmethods utilize spatial feature extractors to obtain atten-\ntive feature maps from spatial view. Li et al. [22] design\na set of diverse spatial attention modules to extract aligned\nlocal features across multiple images. Meanwhile, tempo-\narXiv:2104.01745v1  [cs.CV]  5 Apr 2021\nral learning networks are usually designed for discovering\nthe temporal relationships in temporal domain, such as Re-\ncurrent Neural Network (RNN), Long Short Term Memory\n(LSTM). For example, Mclaughlin et al. [26] introduce a\nrecurrent architecture to model temporal cues cross frames.\nLiu et al. [25] propose a reÔ¨Åning recurrent unit to integrate\nvideo representation. Besides, for spatial-temporal obser-\nvation, Li et al. [21] construct a multi-scale 3D network\nto learn the multi-scale spatial-temporal cues in video se-\nquences. However, previous methods often focus on sin-\ngle views, which lacks of the multi-view observations from\ndifferent view domains. There are a thousand Hamlets in\na thousand people‚Äôs eyes. After observing from different\nperspectives to video data, the model could get more com-\nprehensive and robust video representation. In our work,\nwe attempt to capture three different observations from spa-\ntial, temporal and spatial-temporal domains simultaneously.\nThe intuitive comparisons are shown in Fig. 1.\nRecently, transformers [31, 9, 2] have shown the strong\nrepresentation ability and gained a great success in Nat-\nural Language Processing (NLP). Nowadays, researchers\nextend transformers for numerous computer-vision appli-\ncations. The transformers in vision are mainly develop-\ning in two directions: pure-transformer methods [10, 15]\nand ‚ÄúCNN + transformer‚Äù methods [29, 3]. Although pure-\ntransformer methods show great potentiality and are seen\nas an alternative of CNNs, it is limited by the need of\namounts of data for pre-training. Thus, it is difÔ¨Åcult to\ndeploy quickly on a variety of visual tasks. The ‚ÄúCNN +\ntransformer‚Äù methods retain the powerful spatial feature ex-\ntraction capability of CNNs, at the same time, introduce the\ntransformers to model the relationships of local features in\nhigh-dimensional space. It becomes a popular paradigm for\ncomputer vision. Inspired by this, in this work, we com-\nbine CNNs and transformers for video-based person Re-ID.\nActually, the interactions in transformers will help to as-\nsign different attention weights to local features in spatial-\ntemporal for better video representations.\nIn this paper, we proposed a novel Trigeminal Trans-\nformer (TMT) for video-based person Re-ID. Our method\nattempts to capture three observations from spatial, tempo-\nral and spatial-temporal domains, simultaneously. Then, in\neach view domain, we explore the relationships among local\nfeatures. Meanwhile, in order to aggregate multi-view cues\nand enhance the Ô¨Ånal representation, the cross-view inter-\nactions are taken into consideration in our work. Specif-\nically, our framework mainly consists of three key mod-\nules. Firstly, we design a trigeminal feature extractor to\nobtain three different features under spatial, temporal and\nspatial-temporal views, simultaneously. Secondly, we in-\ntroduce three independent transformers as self-view trans-\nformers for different views. Each transformer takes the\ncoarse features as inputs and exploits the relationships be-\ntween local features for information enhancement. Finally,\nwe propose a novel cross-view transformer, which models\nthe interactions among features of different views and ag-\ngregates them for the Ô¨Ånal video representation. Based on\nabove modules, our TMT can not only capture different per-\nceptions in spatial, temporal and spatial-temporal view do-\nmains, but also fully aggregate multi-view information to\ngenerate more comprehensive video representations. Ex-\ntensive experiments on public benchmarks demonstrate that\nour approach outperforms several state-of-the-art methods.\nIn summary, our contributions are four folds:\n‚Ä¢ We propose a novel Trigeminal Transformer (TMT)\nframework for video-based person Re-ID.\n‚Ä¢ We design a trigeminal feature extractor to trans-\nform raw video data into spatial, temporal and spatial-\ntemporal view domains for different observations.\n‚Ä¢ We introduce transformer structures for video-based\nperson Re-ID. SpeciÔ¨Åcally, we propose a self-view\ntransformer to reÔ¨Åne single-view features and a cross-\nview transformer to aggregate multi-view features.\n‚Ä¢ Extensive experiments on public benchmarks demon-\nstrate that our framework synthetically attains a better\nperformance than several state-of-the-art methods.\n2. Related works\n2.1. Video-based person re-identiÔ¨Åcation\nIn recent years, with the rise of deep learning [8, 19],\nperson Re-ID has got a great success and the recognition\naccuracy has been improved signiÔ¨Åcantly. Recently, due to\nthe urgent need for video matching in the intelligent com-\nmunity system, video-based person Re-ID has drawn more\nand more researchers‚Äô interest. Compared with static im-\nages, videos contain more views which are worth of ob-\nserving, such as spatial view, temporal view and spatial-\ntemporal view. Thus, in video-based person Re-ID, some\nexisting works [22, 39, 11, 40] concentrate on extracting at-\ntentive spatial features in the spatial view. Meanwhile, some\nmethods [26, 7, 25, 17] attempt to obtain temporal obser-\nvations by temporal learning mechanisms. Besides, some\napproaches [23, 21, 34, 13] utilize 3D-CNN to jointly ex-\nplore spatial-temporal cues. For example, for spatial infor-\nmation, Li et al. [22] attempt to extract extract aligned spa-\ntial features across multiple images by diverse spatial atten-\ntion modules. Zhao et al. [40] disentangle frame-wise fea-\ntures for various attribute-aware representations in spatial.\nFor temporal information, Mclaughlin et al. [26] utilize re-\ncurrent neural networks cross frames for temporal learning.\nLiu et al. [25] propose a reÔ¨Åning recurrent unit to integrate\ntemporal cues frame by frame. For spatial-temporal infor-\nmation, Gu et al. [13] propose an appearance preserving\nÔÇ¥\nÔÇ¥\n‚Ä¢‚Ä¢‚Ä¢\n‚Ä¢‚Ä¢‚Ä¢\n‚Ä¢‚Ä¢‚Ä¢\nSpatial\nPooling\nSpatio-\ntemporal\nPooling\ns\nPt\ns\nPs\ns\nPst\nTemporal\nPooling\nConcatenate\nTest\nTrigeminal Feature Extractor Self-view Transformer Cross-view Transformer\nTemporal View\nSpatio-temporal View\nSpatial View\nSpatial SAP\nTemporal SAP\n‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢\nF\nF\nN\nAdd&\nNorm\nSelf-\nattention\nHead\nAdd&\nNorm\nF\nF\nN\nAdd&\nNorm\nSelf-\nattention\nHead\nAdd&\nNorm\nF\nF\nN\nAdd&\nNorm\nSelf-\nattention\nHead\nAdd&\nNorm\nF\nF\nN\nAdd&\nNorm\nAdd&\nNorm\nF\nF\nN\nAdd&\nNorm\nAdd&\nNorm\nF\nF\nN\nAdd&\nNorm\nAdd&\nNorm\nCross-\nattention\nHead\nCross-\nattention\nHead\nCross-\nattention\nHead\nCross-\nattention\nHead\nCross-\nattention\nHead\n‚Ä¢‚Ä¢‚Ä¢\n‚Ä¢‚Ä¢‚Ä¢\n‚Ä¢‚Ä¢‚Ä¢\nCross-\nattention\nHead\nLoss\nLoss\nLoss Loss\nLoss\nLoss Loss\nLoss\nLoss\nFigure 2. The overall structure of our proposed method. Firstly, the trigeminal feature extractor is utilized to transform raw video data into\ndifferent view domains individually. Then, the self-view transformers are introduced for feature enhancement. After that, a cross-view\ntransformer is designed to model the interactions between multiple views for comprehensive observations.\n3D convolutional network to address appearance destruc-\ntion and model temporal information. Meanwhile, beyond\nsingle view, Liet al. [21] design a two-stream convolutional\nnetwork to explicitly leverage spatial and temporal cues.\nDifferent from previous methods, in this paper, we design a\ntrigeminal network to transform raw video data into spatial,\ntemporal and spatial-temporal feature space in three differ-\nent views. Besides, the self-view transformer and the cross-\nview transformer are proposed to fully explore discrimina-\ntive cues in self-view domain and aggregate diverse obser-\nvations cross multi-view domains for Ô¨Ånal comprehensive\nand robust representations.\n2.2. Transformer in vision\nTransformer [31] is initially proposed for NLP tasks and\nbrings signiÔ¨Åcant improvement in many tasks [9, 2]. In-\nspired by the powerful ability of transformer for handling\nsequential data, the transformer-based vision models are\nspringing up like mushrooms, showing the great potential\nin vision Ô¨Åelds. Researchers have extended transformer for\ncomputer vision tasks, such as image and video classiÔ¨Åca-\ntion [12, 10, 6], object detection [3], semantic segmenta-\ntion [33], video inpainting [38] and so on. For example,\nGirdhar et al. [12] introduce an action transformer model\nto recognize and localize human behaviors in videos. Car-\nion et al. [3] utilize transformers to redesign an end-to-end\nobject detector. Dosovitskiy et al. [10] propose the Vision\nTransformer (ViT) , which applies transformer directly to\nsequences of image patches and achieves promising results.\nRecently, He et al. [15] utilize a pure transformer with a jig-\nsaw patch module for image-based Re-ID. Compared with\nexisting transformers in vision, our transformers are con-\nstructed in spatial, temporal and spatial-temporal domains\nfor video-based Re-ID. Besides, we propose a novel cross-\nview transformer to aggregate multi-view cues for compre-\nhensive video representations.\n3. Proposed method\nIn this section, we introduce the proposed Trigeminal\nTransformers (TMT). We Ô¨Årst give an overview of the pro-\nposed TMT. Then, we elaborate the key modules in the fol-\nlowing subsections.\n3.1. Overview\nThe TMT is shown in Fig. 2. The overall framework\nmainly consists of three key modules: Trigeminal Feature\nExtractor, Self-view Transformer and Cross-view Trans-\nformer. To begin with, we adopt the Restricted Random\nSampling (RRS) [22] to generate sequential frames as in-\nputs. Then, we use the designed trigeminal feature ex-\ntractor to transform the raw video data into different high-\ndimensional spaces, which consists of three non-shared em-\nbedding branches for multi-view observations. For ex-\ntracted initial features in each single-view domain, we uti-\nlize the self-view transformer to explore the relationships\nbetween local features for information enhancement. Af-\nterwards, the cross-view transformer is proposed to capture\nthe interaction among multi-view features and adaptively\naggregate them for comprehensive representations. Finally,\nto train our model, we introduce the Online Instance Match-\ning (OIM) [35] loss and veriÔ¨Åcation loss. In the test stage,\nthe feature vectors of different views are concatenated for\nthe retrieval list.\nH Softmax\n(a) Temporal Self-attention Pooling\nSum\nTÔÇ¥TLinear\nW\nT\nT\nH\nW\nSum\nL2 \nNorm\nÔÇ¥\n(b) Spatial Self-attention Pooling\nSum\nT\nT\nHW\nT\nHW\nSoftmax\nSum\nLinear\nL2 \nNorm\n‚Ä¢\nHW\nHWÔÇ¥HW\n‚Ä¢\nÔÇ¥\nFigure 3. The temporal self-attention pooling (a) and the spatial\nself-attention pooling (b).\n3.2. Trigeminal feature extractor\nMulti-stream networks are usually used to handle se-\nquential data in action recognition and video classiÔ¨Åca-\ntion [21, 37, 27]. Actually, the success of multi-stream net-\nworks can be attributed to the assemble multiple observa-\ntions from different views, which help to extract more com-\nprehensive representations. Inspired by this, in our work,\nwe design a trigeminal feature extractor for spatial, tempo-\nral and spatial-temporal views.\nThe structure of our proposed trigeminal feature extrac-\ntor is shown in the left of Fig. 2. Formally, given a long\nsequence, we Ô¨Årstly sample T frames {I1,I2,..., IT }as the\ninputs of our network. T is the length of the sequence. In\nour work, the ResNet-50 [14] is used as the basic backbone\nfor frame-wise feature maps. Different from the previous\nmulti-steam works, we deploy the parameter-shared shal-\nlow residual blocks from conv1 to conv3x in ResNet-50 for\nreducing network parameters. Three non-shared conv4 x\nare deployed after basic CNN as the embedding network\nand output three video feature cubes, Xs,Xt,Xst, which\nhave same sizes ofT√óH√óW√óC. H,W,C represents the\nheight, the width and the number of channels, respectively.\nTo further separate the feature cubes in high-dimensional\nspace, we propose a self-attention pooling to project the fea-\ntures into spatial and temporal domains for different views.\nSelf-attention pooling. The spatial self-attention pool-\ning and temporal self-attention pooling have the similar\nstructures and are shown in Fig. 3. Given a feature cube\nXs ‚àà ‚ÑúT√óHW √óC, we utilize the temporal self-attention\npooling to project Xs into spatial domain. To begin with, a\nlinear projection is applied to each spatial local feature Xs\ni\n‚àà‚ÑúT√óC, i‚àà[1,H √óW] and generates Fi ‚àà‚ÑúT√óC by\nFi = WXs\ni (1)\nwhere W is the parameter of the linear projection. Then, we\napply a matrix multiplication between Fi and its transposi-\ntion to generate the self-attention matrix Mi ‚àà‚ÑúT√óT .\nMi = FiFi\nT , (2)\nwhere (¬∑)T indicates the transpose operation. After that, we\nsum the self-attention matrix along one dimension and then\nperform a softmax operation along other dimension to infer\nthe temporal attention vector ai ‚àà‚ÑúT .\nai = Softmax(\nT‚àë\nj=1\nMj\ni ) (3)\nwhere j ‚àà[1,T]. Then, a dot production is applied between\neach local spatial feature Xs\ni and its temporal attention vec-\ntor ai. By this way, we obtain the attentive spatial feature\nÀÜX\ns\ni ‚àà‚ÑúT√óC\nÀÜX\ns\ni = Xs\ni ‚äôai (4)\ngi =\nT‚àë\nt\nXs\ni,t (5)\nwhere ‚äô represent the matrix dot multiplication and\ngi ‚àà ‚ÑúC. Finally, the local spatial feature sets Fs =\n{g1,...,g i,...,g H√óW }(i ‚àà[1,H √óW]) as the output of\ntemporal self-attention pooling .\nThe resulting spatial features can be regard as the\nweighted feature in temporal, which utilizes the relation-\nships cross frames. By this way, we can pool the feature\ncube Xs ‚àà‚ÑúT√óHW √óC to Fs ‚àà‚ÑúHW √óC. Noted that, the\nspatial self-attention pooling has the similar process as the\ntemporal self-attention pooling. The difference is the size of\nthe self attention matrices. In the spatial self-attention pool-\ning, the size of the spatial attention matrix is HW √óHW,\nand the output can be represented by Ft ‚àà‚ÑúT√óC. Thus,\nour trigeminal feature extractor utilizes a partially shared\nnetwork to project raw video data into independent spatial,\ntemporal and spatial-temporal domains for multi-view ob-\nservations. After that, self-view transformers and a cross-\nview transformer are deployed for further global feature en-\nhancement.\n3.3. Self-view transformer\nBased on the above trigeminal feature extractor, we can\neasily obtain particular features in different views. How-\never, these features lack global observations in each self-\nview domain. Recently, transformer models have been pro-\nposed to explore the interactions among contextual informa-\ntion and deliver a great success. Actually, the interactions\nLinear\nLinear\nLinear Linear Linear\nNorm\nNorm\n+\n+\nMatMul\nMatMul\nSoftmax\nConcatenation\nNorm\nLinear\nNorm\n+\n+\nLinear\nReLUReLU\nùë≠ùë†ùë≠ùë°\nùë≠ùë†ùë° ùë≠ùë°\nSelf-view Transformer\nin Temporal Domain\nCross-view Transformer\nin Temporal Domain\nùë∏ ùë≤ ùëΩ ùëΩùíîùíï ùëΩùíîùë∏ùë†ùë° ùë≤ùíîùë≤ùíîùíï\nLinear Linear Linear Linear Linear Linear\nMatMul\nMatMul MatMul\nMatMul\nùë∏ùë†ùë°\nùë°\nT\nHW\nNh\nSelf-attention map\nT\nT\nNh\nSelf-attention map\nNhT\nTHW\nSelf-attention map\nConcatenation Concatenation\nSoftmax Softmax\nFigure 4. The self-view transformer (Left) and the cross-view transformer (Right) in temporal domain.\namong global features are beneÔ¨Åcial to mine more discrim-\ninative cues. Inspired by the strong capacity of transformer\nand its great potentiality in vision Ô¨Åelds, in our work, we\nintroduce the transformer structure into different view do-\nmains for feature enhancement. Self-view transformers are\nmulti-layer architectures formed by stacking blocks on top\nof one another. Each block of transformer is composed\nof a multi-head self-attention layer, a position-wise feed-\nforward network, layer normalization modules and resid-\nual connections. For the temporal domain, the obtained\nfeature Ft ‚àà ‚ÑúT√óC from the trigeminal feature extractor\nis passed into the self-view transformer (The left part of\nFig. 4), which can be formulated as follows. First, a po-\nsitional encoding is added to the original feature for the\npositional information on temporal, which can be trainable\nlike [10]. Then, the feature with position is passed through\nthe multi-head self-attention layer. In each head, the feature\nis Ô¨Årstly feed into three linear transformations to generate\nfeature Q, K and V, where Q,K,V ‚àà‚ÑúT√ód. d= C\nNh\nand\nNh is the number of heads. The self attention operation is\ndeÔ¨Åned as:\nAh = Softmax(QKT\n‚àö\nd\n)V, (6)\nThe outputs of multiple heads, A1,¬∑¬∑¬∑ ,ANh, are con-\ncatenated together as the output of the multi-headed self-\nattention layer. The input and output of the multi-head self-\nattention layer are connected by residual connections and a\nnormalization layer,\nFt = LayerNorm(Ft + MultiHead(Ft)). (7)\nThe position-wise feed-forward network (FFN) is applied\nafter the multi-head self-attention layer. SpeciÔ¨Åcally, the\nFFN consists of two linear transformation layers and a\nReLU activation function within them, which can be de-\nnoted as the following function:\nFt = W2œÉ(W1Ft), (8)\nwhere W1 and W2 are the parameters of two linear trans-\nformation layers and œÉis the ReLU activation function.\nIn the spatial and spatial-temporal views, the Fs and Fst\nobtained from the trigeminal feature extractor are passed\nthrough the spatial transformer and spatial-temporal trans-\nformer individually, which have the same structures as the\ntemporal transformer. In our work, we deploy the trans-\nformers in each self-view domain, called self-view trans-\nformers, to model the relationships among local features for\nfurther feature enhancement.\n3.4. Cross-view transformer\nIn each domain, self-view transformers help to reÔ¨Åne\nthe initial feature by exploiting the attention relationships\nin current view. Actually, the attention relationships cross\nmultiple views are also instructive to the feature reÔ¨Åne-\nment. Considering this, in this paper, a cross-view trans-\nformer is constructed cross different view domains, which\nhas spatial, temporal and spatial-temporal views simultane-\nously. The right part of Fig. 4 shows the detailed structure\nof the proposed cross-view transformer, which consists of\nmulti-head cross-attention layer, domain-wise feed-forward\nnetworks, layer normalization modules and residual con-\nnections. Formally, in the temporal-based cross-attention\nhead, six linear projections are applied to generate six fea-\ntures Qt\ns,Qt\nst,Ks,Kst,Vs,Vst, where Qt\ns,Qt\nst ‚àà ‚ÑúT√ód\nare from Ft, Ks,Vs ‚àà‚ÑúHW √ód are from Fs and Kst,Vst\n‚àà‚ÑúTHW √ód are from Fst. d = C\nNh\nand Nh is the number\nof heads. In one head, the cross-attention is formulated as:\nÀÜF\nt\ns = Softmax(Qt\nsKT\ns‚àö\nd\n)Vs, (9)\nÀÜF\nt\nst = Softmax(Qt\nstKT\nst‚àö\nd\n)Vst, (10)\nwhere (¬∑)T is the transposition. The concatenation of multi-\nhead outputs can be represented by Ft\ns and Ft\nst, which have\nthe same size to Ft. Then, the output of multi-head cross-\nattention head can be expressed as:\nFt = LayerNorm(Ft + Ft\ns + Ft\nst). (11)\nAfter that, similar to self-view transformer, a domain-wise\nfeed-forward network is applied.\nFt = W4œÉ(W3Ft), (12)\nwhere W3 and W4 are parameters of the two linear transfor-\nmation layers and œÉ represents the ReLU activation func-\ntion. By this way, in the temporal domain, its feature can\naggregate the related information from other views. For the\nspatial and spatial-temporal views, the interactions are ex-\nplored in similar cross-view transformers. Finally, the fea-\ntures from different views are adaptively integrated by pro-\nposed cross-view transformer and generate more compre-\nhensive representations.\n4. Experiments\n4.1. Datasets and evaluation protocols\nIn this paper, we adopt three widely-used benchmarks to\nevaluate our proposed method,i.e., iLIDS-VID [32], PRID-\n2011 [16] and MARS [41]. iLIDS-VID [32] and PRID-\n2011 [16] are two small datasets. Both of them collected\nimages by two cameras. iLIDS-VID [32] has 600 video\nsequences of 300 different identities. PRID-2011 [16] con-\nsists of 400 image sequences for 200 identities from two\nnon-overlapping cameras. MARS [41] is one of large-scale\ndatasets, and consists of 1,261 identities around 18,000\nvideo sequences. All the video sequences are captured by\nat least 2 cameras. Noted that, there are round 3,200 dis-\ntractors sequences in the dataset to simulate actual detect\nconditions. For evaluation, the Cumulative Matching Char-\nacteristic (CMC) table and mean Average Precision (mAP)\nare adopted following previous works for MARS dataset.\nFor more details, we refer readers to the original paper [42].\nIn terms of iLIDS-VID and PRID2011, there is one single\ncorrect match in the gallery set. Thus, only the cumulative\nre-identiÔ¨Åcation accuracy is reported.\n4.2. Implementation details\nWe implement our framework based on the Pytorch 1\ntoolbox. The experimental devices include an Intel i4790\nCPU and two NVIDIA 3090 (24G memory). Experimen-\ntally, we set the batchsize = 16. In our work, if not spec-\niÔ¨Åed, we set the length of sequence T = 8 and the depth\nof transformer to 2. Each image in a sequence is resized\nto 256√ó128 and augmented by random cropping, horizon-\ntal Ô¨Çipping and random erasing. The ResNet-50 [14] pre-\ntrained on the ImageNet dataset [8] is used as our backbone\nnetwork. Following previous works [30], we remove the\nlast spatial down-sampling operation to increase the feature\nresolution. In a mini-batch, we selected a number of pos-\nitive and negative sequence pairs for veriÔ¨Åcation loss. Be-\nsides, we utilize frame-wise and video-wise OIM losses to\nsupervise the whole network following [4]. In experiments,\nthe outputs of backbone in three branches are also super-\nvised to strength the learning of the underlying network.\nDuring training, we train our network for 50 epochs and\nthe learning rate is decayed by 10 at every 15 epochs. The\nwhole network is updated by stochastic gradient descent [1]\nalgorithm with an initial learning rate of 10‚àí3, weight de-\ncay of 5 √ó10‚àí4 and nesterov momentum of 0.9. We will\nrelease the source code for model reproduction.\n4.3. Ablation study\nTo investigate the effectiveness of our TMT, we con-\nduct a series of experiments on three public benchmarks:\nMARS, iLIDS-VID and PRID2011.\nEffectiveness of key components. We gradually add our\nmodules to backbone for ablation analysis. The results are\nshown in Tab. 1. In this table, ‚ÄúBaseline‚Äù represents the\nResNet-50 as the feature extractor following directly spa-\ntial and temporal average pooling for the Ô¨Ånal video repre-\nsentation. ‚Äú+ Three branches‚Äù means that three non-shared\nRes4 x blocks in ResNet-50 are deployed as same as the\ntrigeminal feature extractor, which brings slight improve-\nments. Compared with ‚Äú+ Three branches‚Äù, ‚Äú+ Trigeminal\nfeature extractor‚Äù represents the proposed temporal and spa-\ntial self-attention pooling replacing the direct average pool-\ning in spatial and temporal branches respectively. We can\nsee that our proposed self-attention pooling is beneÔ¨Åcial\nto transform same features into different domains and im-\nproves 1.9% in term of mAP on MARS. Moreover, we add\nthree self-view transformers in spatial, temporal and spatial-\ntemporal domains for ‚Äú + Self-view transformer‚Äù. The indi-\nvidual view-wise feature attains a signiÔ¨Åcant improvement\nthan ‚Äú+ Trigeminal feature extractor‚Äù on three benchmarks.\nCompared with ‚ÄúBaseline‚Äù, our proposed three self-view\ntransformers can improve the mAP by 4.4% and the Rank-\n1 accuracy by 2.3% on MARS. The improvements indicate\n1https://pytorch.org/\nTable 1. Ablation results of key components on MARS, iLIDS-VID and PRID2011.\nMARS iLIDS-VID PRID2011\nMethod mAP Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20\nBaseline 81.2 88.5 95.5 97.9 88.2 97.6 99.7 93.5 98.7 99.7\n+ Three branches 82.0 88.9 95.6 98.0 89.3 97.8 99.6 94.4 99.1 99.8\n+ Trigeminal feature extractor83.9 89.7 96.1 98.2 89.7 98.4 99.7 94.8 99.1 100\nSpatial view 80.2 86.9 95.2 97.5 86.7 97.7 99.5 91.3 98.2 99.8\nTemporal view 82.8 89.3 95.8 98.1 86.3 98.0 99.8 93.2 98.7 100\nSpatial-temporal view 83.4 89.5 95.7 98.1 87.6 97.9 99.6 92.7 98.6 99.8\n+ Self-view transformer 85.6 90.8 96.7 98.4 90.6 98.2 100 95.7 99.0 99.8\nSpatial view 83.4 90.0 96.1 98.2 88.9 97.8 99.7 93.3 98.5 99.8\nTemporal view 83.5 89.6 95.7 98.0 88.4 97.5 99.8 94.7 98.8 100\nSpatial-temporal view 84.6 90.2 96.6 98.2 90.2 98.2 99.9 93.7 98.7 99.6\n+ Cross-view transformer 85.8 91.2 97.3 98.8 91.3 98.6 100 96.4 99.3 100\n84.4\n83 83.9 83\n89.9 89 89.7 89.3\n75\n80\n85\n90\n95\n2 4 6 8\nSpatial \nTransformer\n88.9 88.9 87.9\n89.3\n84 83.6\n82.1 82.8\n75\n80\n85\n90\n95\n2 4 6 8\n88.9 89.1 89 88.6\n83.6 83 83.5 82.8\n75\n80\n85\n90\n95\n2 4 6 8\n83.5 84.2 83.6 83.5\n90.1 90.4 89.6 88.6\n75\n80\n85\n90\n95\n1 2 3 4\nTemporal \nTransformer\nSpatio-Temporal \nTransformer\nCross-view\nTransformer\nmAP\nRank-1\nPerformance(%)\nDepth of transformer Depth of transformer Depth of transformer Depth of transformer\nFigure 5. Ablation results on the depth of self-view transformers and the cross-view transformer in on MARS.\nTable 2. Ablation results on the length of sequences on MARS.\nMethods Temporal Spatial-temporal\nTransformer Transformer\nLength (T) mAP Rank-1 Rank-20 mAP Rank-1 Rank-20\n6 83.4 88.6 98.0 82.6 89.5 97.8\n8 84.0 88.7 98.1 84.0 89.8 98.5\n10 83.2 89.1 97.8 83.8 89.2 97.9\n12 83.0 88.6 97.9 83.7 88.7 98.0\nTable 3. Ablation results on the size of spatial feature maps on\nMARS.\nMethods Spatial Spatial-temporal\nTransformer Transformer\nSize (H√óW) mAP Rank-1 Rank-20 mAP Rank-1 Rank-20\n8√ó4 82.4 89.2 98.0 83.2 90.0 97.8\n16√ó8 84.4 89.9 98.4 84.0 89.8 98.5\nthat self-view transformers could help to model the rela-\ntionships among global features for better performances.\nNoted that, the combination of three views boosts higher\nresults than single view, which clariÔ¨Åes our statement: a\nvideo is worth three views. Last but not least, we apply the\nproposed cross-view transformer in ‚Äú + Cross-view trans-\nformer‚Äù. The performance has been further improved on\nthree benchmarks. It gains 0.4%, 0.7% and 0.7% in term of\nthe Rank-1 accuracy on MARS, iLIDS-VID and PRID2011\nrespectively. It indicates that the cross-view transformer is\nbeneÔ¨Åcial to aggregate multi-view observations for compre-\nhensive video representations.\nEffect of the length of sequences . In Tab. 2, the abla-\ntion results show the inÔ¨Çuence of different sequence length\non MARS. The temporal or spatial-temporal transformer is\napplied to the baseline for single-view observation. We can\nsee that, the temporal and spatial-temporal transformer are\nsensitive to the length of the sequences. When varying the\nlength of the sequence to 8, both of temporal and spatial-\ntemporal transformers get best performance.\nEffect of the size of spatial feature map . We also per-\nform ablation experiments to investigate the effect of vary-\ning the spatial size of feature maps. The standard ResNet-\n50 is utilized and generates the frame-wise feature maps,\nwhich have the spatial size of 8 √ó4. When we remove the\nlast spatial down-sampling operation of ResNet-50, the size\nof feature maps will increase to16√ó8. In this way, the pro-\nposed the spatial or spatial-temporal transformer could be\napplied to different features with different sizes. The abla-\ntion results for spatial and spatial-temporal transformer are\nreported in Tab. 3. The results show that the increments of\nspatial size will gain signiÔ¨Åcant improvement.\nEffect of the depth of transformer. The depth of trans-\nformer is an important factor for exploring the interaction\nof contextual information. The increments of depth will im-\nprove the representation capacity of transformer, but it will\nalso bring training difÔ¨Åculties. In Fig. 5, performances with\ndifferent depths are reported on different view-wise trans-\nformers. The experiments are conducted on MARS. We\nTable 4. Comparison with state-of-the-art video-based person re-identiÔ¨Åcation methods on MARS, iLIDS-VID and PRID2011.\nMARS iLIDS-VID PRID2011\nMethods Source mAP Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20\nSeeForest [43]CVPR17 50.7 70.6 90.0 97.6 55.2 86.5 97.0 79.4 94.4 99.3\nASTPN [36] ICCV17 - 44 70 81 62 86 98 77 95 99\nSnippet [4] CVPR18 76.1 86.3 94.7 98.2 85.4 96.7 99.5 93.0 99.3 100\nSTAN [22] CVPR18 65.8 82.3 - - 80.2 - - 93.2 - -\nSTMP [25] AAAI19 72.7 84.4 93.2 96.3 84.3 96.8 99.5 92.7 98.8 99.8\nM3D [21] AAAI19 74.0 84.3 93.8 97.7 74.0 94.3 - 94.4 100 -\nAttribute [40] CVPR19 78.2 87.0 95.4 98.7 86.3 87.4 99.7 93.9 99.5 100\nVRSTC [18] CVPR19 82.3 88.5 96.5 97.4 83.4 95.5 99.5 - - -\nGLTR [20] ICCV19 78.5 87.0 95.8 98.2 86.0 98.0 - 95.5 100 -\nCOSAM [28] ICCV19 79.9 84.9 95.5 97.9 79.6 95.3 - - - -\nMGRA [39] CVPR20 85.9 88.8 97.0 98.5 88.6 98.0 99.7 95.9 99.7 100\nSTGCN [37] CVPR20 83.7 89.9 - - - - - - - -\nAFA [5] ECCV20 82.9 90.2 96.6 - 88.5 96.8 99.7 - - -\nTCLNet [17] ECCV20 85.1 89.8 - - 86.6 - - - - -\nGRL [24] CVPR21 84.8 91.0 96.7 98.4 90.4 98.3 99.8 96.2 99.7 100\nTMT(Ours) - 85.8 91.2 97.3 98.8 91.3 98.6 100 96.4 99.3 100\nadd the spatial, temporal or spatial-temporal transformer\nto ‚Äú Baseline‚Äù and add the cross-view transformer to ‚Äú +\nThree branches‚Äù respectively. From the results, we can see\nthat, the depths of achieving best performance in one trans-\nformers are different to other transformers. Besides, for the\ncross-view transformers, a two-layer structure gains better\nthan shallower transformers or deeper transformers.\n4.4. Compared with the state of the arts\nIn this section, our TMT is compared with state-of-the-\nart methods on MARS, iLIDS-VID and PRID2011. The re-\nsults are reported in Tab. 4. One can observe that, on MARS\ndataset, our TMT attains comparable even better perfor-\nmances than other compared methods. In addition, our\nmethod achieves highest Rank-1 accuracy on three bench-\nmarks. Compared with existing methods, our method inte-\ngrates multi-view cues to obtain more comprehensive video\nrepresentations. MGRA [39] extracts multi-granularity spa-\ntial cues under the guidance of a global view, which gains\nremarkable 85.9% mAP on MARS dataset. Attribute [40]\nmines various attribute-aware features in spatial for align-\nment. Those methods focus on capturing diverse spa-\ntial features and gain remarkable performances. Even so,\ncomapred with MGRA [39], our method improves the per-\nformances by 2.4% and 2.7% in terms of mAP and Rank-\n1 accuracy on MARS and iLIDS. GLTR [20] attempts to\nmodel the multi-granular temporal dependencies for short\nand long-term temporal cues. GRL [24] utilizes a bi-\ndirection temporal learning to reÔ¨Åne and accumulate dis-\nentangled spatial features. Those methods are indeed help-\nful to capture the discriminative cues in temporal for bet-\nter recognition accuracy. Especially, GRL [24] attains an\nexpressive Rank-1 accuracy on iLIDS-VID datasets. Our\nmethod has still gained 0.9% improvements about Rank-1\naccuracy than GRL [24]. Besides, STA [11] introduces a\nfree-parameter spatial-temporal attention module to weight\nlocal features in spatial-temporal domain. Different from\nthe above single-view methods, our proposed TMT resem-\nbles spatial, temporal and spatial-temporal observations and\nachieves better results on three public datasets. Meanwhile,\nit is worth noting that some methods construct two-stream\nnetworks for different feature representations. For exam-\nple, M3D [21] combines 3D-CNN and 2D-CNN to explic-\nitly leverage spatial and temporal cues. STGCN [37] con-\nstructs two parallel graph convolutional networks to explore\nthe relations in spatial and temporal domains. Compared\nwith these two-stream approaches, our method introduces\na self-view transformer into single-view domain for fea-\nture enhancement, and a cross-view transformer to aggre-\ngate multiple views for better representations. In this way,\nour method surpasses STGCN [37] by 2.1%, 1.3% in terms\nof mAP and Rank-1 accuracy on MARS. In summary, our\nmethod performs better than most existing state-of-the-arts.\nThese results validate the superiority of our method.\n5. Conclusion\nIn this paper, we propose a novel framework named\nTrigeminal Transformer for video-based person Re-ID. A\ntrigeminal feature extractor is designed to capture spatial,\ntemporal and spatial-temporal view domains, in which the\nproposed temporal self-attention pooling and spatial self-\nattention pooling are applied to transform features into spa-\ntial and temporal domains respectively. Besides, self-view\ntransformers are introduced to explore the relationships\namong global features for feature enhancement in self-view\ndomains. In addition, a cross-view transformer is proposed\nto model the interactions among multi-view features for\nmore comprehensive representations. Based on our pro-\nposed modules, we model a video from spatial, temporal\nand spatial-temporal views. Extensive experiments on there\npublic benchmarks demonstrate that our approach performs\nbetter than some state-of-the-arts.\nReferences\n[1] L ¬¥eon Bottou. Large-scale machine learning with stochastic\ngradient descent. In COMPSTAT, pages 177‚Äì186. 2010. 6\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2, 3\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, pages\n213‚Äì229, 2020. 2, 3\n[4] Dapeng Chen, Hongsheng Li, Tong Xiao, Shuai Yi, and Xi-\naogang Wang. Video person re-identiÔ¨Åcation with compet-\nitive snippet-similarity aggregation and co-attentive snippet\nembedding. In CVPR, pages 1169‚Äì1178, 2018. 6, 8\n[5] Guangyi Chen, Yongming Rao, Jiwen Lu, and Jie Zhou.\nTemporal coherence or temporal motion: Which is more\ncritical for video-based person re-identiÔ¨Åcation? In ECCV,\npages 660‚Äì676, 2020. 8\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In ICML, pages 1691‚Äì1703, 2020. 3\n[7] Ju Dai, Pingping Zhang, Dong Wang, Huchuan Lu, and H\nWang. Video person re-identiÔ¨Åcation by temporal residual\nlearning. TIP, 28:1366‚Äì1377, 2019. 2\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248‚Äì255, 2009. 2, 6\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2, 3\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3, 5\n[11] Yang Fu, Xiaoyang Wang, Yunchao Wei, and Thomas\nHuang. Sta: Spatial-temporal attention for large-scale video-\nbased person re-identiÔ¨Åcation. In AAAI, pages ‚Äì, 2019. 2, 8\n[12] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, pages\n244‚Äì253, 2019. 3\n[13] Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang,\nand Xilin Chen. Appearance-preserving 3d convolution for\nvideo-based person re-identiÔ¨Åcation. In ECCV, pages 228‚Äì\n243, 2020. 2\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770‚Äì778, 2016. 4, 6\n[15] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiÔ¨Åcation. arXiv preprint arXiv:2102.04378 , 2021. 2,\n3\n[16] Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst\nBischof. Person re-identiÔ¨Åcation by descriptive and discrim-\ninative classiÔ¨Åcation. In SCIA, pages 91‚Äì102, 2011. 6\n[17] Ruibing Hou, Hong Chang, Bingpeng Ma, Shiguang Shan,\nand Xilin Chen. Temporal complementary learning for video\nperson re-identiÔ¨Åcation. arXiv preprint arXiv:2007.09357 ,\n2020. 2, 8\n[18] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu,\nShiguang Shan, and Xilin Chen. Vrstc: Occlusion-free video\nperson re-identiÔ¨Åcation. In CVPR, pages 7183‚Äì7192, 2019.\n8\n[19] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv:1502.03167, 2015. 2\n[20] Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang\nZhang. Global-local temporal representations for video per-\nson re-identiÔ¨Åcation. In ICCV, pages 3958‚Äì3967, 2019. 8\n[21] Jianing Li, Shiliang Zhang, and Tiejun Huang. Multi-\nscale 3d convolution network for video based person re-\nidentiÔ¨Åcation. In AAAI, pages 8618‚Äì8625, 2019. 2, 3, 4,\n8\n[22] Shuang Li, Slawomir Bak, Peter Carr, and Xiaogang Wang.\nDiversity regularized spatiotemporal attention for video-\nbased person re-identiÔ¨Åcation. In CVPR, pages 369‚Äì378,\n2018. 1, 2, 3, 8\n[23] Jiawei Liu, Zheng-Jun Zha, Xuejin Chen, Zilei Wang, and\nYongdong Zhang. Dense 3d-convolutional neural network\nfor person re-identiÔ¨Åcation in videos. TOMM, 15:1‚Äì19,\n2019. 2\n[24] Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu,\nand Xiaoyun Yang. Watching you: Global-guided recipro-\ncal learning for video-based person re-identiÔ¨Åcation, 2021.\n8\n[25] Yiheng Liu, Zhenxun Yuan, Wengang Zhou, and Houqiang\nLi. Spatial and temporal mutual promotion for video-based\nperson re-identiÔ¨Åcation. In AAAI, pages 8786‚Äì8793, 2019.\n2, 8\n[26] Niall McLaughlin, Jesus Martinez del Rincon, and Paul\nMiller. Recurrent convolutional network for video-based per-\nson re-identiÔ¨Åcation. In CVPR, pages 1325‚Äì1334, 2016. 2\n[27] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. arXiv\npreprint arXiv:1406.2199, 2014. 4\n[28] Arulkumar Subramaniam, Athira Nambiar, and Anurag Mit-\ntal. Co-segmentation inspired attention networks for video-\nbased person re-identiÔ¨Åcation. In ICCV, pages 562‚Äì572,\n2019. 8\n[29] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,\nXinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and\nPing Luo. Transtrack: Multiple-object tracking with trans-\nformer. arXiv preprint arXiv:2012.15460, 2020. 2\n[30] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin\nWang. Beyond part models: Person retrieval with reÔ¨Åned\npart pooling (and a strong convolutional baseline). InECCV,\npages 480‚Äì496, 2018. 6\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 2, 3\n[32] Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin\nWang. Person re-identiÔ¨Åcation by video ranking. In ECCV,\npages 688‚Äì703, 2014. 6\n[33] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-\nend video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020. 3\n[34] Lin Wu, Yang Wang, Ling Shao, and Meng Wang. 3-d\npersonvlad: Learning deep global representations for video-\nbased person reidentiÔ¨Åcation. NNLS, 30:3347‚Äì3359, 2019.\n2\n[35] Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiao-\ngang Wang. Joint detection and identiÔ¨Åcation feature learn-\ning for person search. In CVPR, pages 3415‚Äì3424, 2017.\n3\n[36] Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu\nChang, and Pan Zhou. Jointly attentive spatial-temporal\npooling networks for video-based person re-identiÔ¨Åcation. In\nICCV, pages 4733‚Äì4742, 2017. 8\n[37] Jinrui Yang, Wei-Shi Zheng, Qize Yang, Ying-Cong Chen,\nand Qi Tian. Spatial-temporal graph convolutional network\nfor video-based person re-identiÔ¨Åcation. In CVPR, pages\n3289‚Äì3299, 2020. 4, 8\n[38] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn ECCV, pages 528‚Äì543, 2020. 3\n[39] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, and Zhibo\nChen. Multi-granularity reference-aided attentive feature ag-\ngregation for video-based person re-identiÔ¨Åcation. In CVPR,\npages 10407‚Äì10416, 2020. 2, 8\n[40] Yiru Zhao, Xu Shen, Zhongming Jin, Hongtao Lu, and\nXian-sheng Hua. Attribute-driven feature disentangling and\ntemporal aggregation for video person re-identiÔ¨Åcation. In\nCVPR, pages 4913‚Äì4922, 2019. 2, 8\n[41] Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su,\nShengjin Wang, and Qi Tian. Mars: A video benchmark for\nlarge-scale person re-identiÔ¨Åcation. In ECCV, pages 868‚Äì\n884, 2016. 6\n[42] Liang Zheng, Yi Yang, and Alexander G Haupt-\nmann. Person re-identiÔ¨Åcation: Past, present and future.\narXiv:1610.02984, 2016. 6\n[43] Zhen Zhou, Yan Huang, Wei Wang, Liang Wang, and Tie-\nniu Tan. See the forest for the trees: Joint spatial and tem-\nporal recurrent neural networks for video-based person re-\nidentiÔ¨Åcation. In CVPR, pages 4747‚Äì4756, 2017. 8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7822364568710327
    },
    {
      "name": "Exploit",
      "score": 0.6637570858001709
    },
    {
      "name": "Transformer",
      "score": 0.6003592610359192
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5822792649269104
    },
    {
      "name": "Computer vision",
      "score": 0.47127482295036316
    },
    {
      "name": "Engineering",
      "score": 0.08044469356536865
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}