{
    "title": "A Combination of BERT and Transformer for Vietnamese Spelling Correction",
    "url": "https://openalex.org/W4312609845",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5021089045",
            "name": "Trung Hieu Ngo",
            "affiliations": [
                "Saigon International University"
            ]
        },
        {
            "id": "https://openalex.org/A5022827223",
            "name": "Duong Tran Ham",
            "affiliations": [
                "Saigon International University"
            ]
        },
        {
            "id": "https://openalex.org/A5055531030",
            "name": "Tin Van Huynh",
            "affiliations": [
                "Saigon International University"
            ]
        },
        {
            "id": "https://openalex.org/A5053580743",
            "name": "Kiem Hoang",
            "affiliations": [
                "Saigon International University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6702248584",
        "https://openalex.org/W2741908291",
        "https://openalex.org/W3093050310",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3035010485",
        "https://openalex.org/W2144947589",
        "https://openalex.org/W2964322605",
        "https://openalex.org/W2970521905",
        "https://openalex.org/W2963212250",
        "https://openalex.org/W3098637735",
        "https://openalex.org/W3038968242",
        "https://openalex.org/W105181109",
        "https://openalex.org/W2133082374",
        "https://openalex.org/W2995333594",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3038626969",
        "https://openalex.org/W2967423108",
        "https://openalex.org/W2470324779"
    ],
    "abstract": null,
    "full_text": "A Combination of BERT and Transformer\nfor Vietnamese Spelling Correction\nHieu Ngo Trung 1, Duong Tran Ham 1, Tin Huynh 1, Kiem Hoang 1\nThe Saigon International University, Vietnam\n{ngotrunghieu,tranhamduong,huynhngoctin,hoangkiem}@siu.edu.vn\nAbstract Recently, many studies have shown the efficiency of using Bidirectional\nEncoder Representations from Transformers (BERT) in various Natural Lan-\nguage Processing (NLP) tasks. Specifically, English spelling correction task that\nuses Encoder-Decoder architecture and takes advantage of BERT has achieved\nstate-of-the-art result. However, to our knowledge, there is no implementation\nin Vietnamese yet. Therefore, in this study, a combination of Transformer archi-\ntecture (state-of-the-art for Encoder-Decoder model) and BERT was proposed\nto deal with Vietnamese spelling correction. The experiment results have shown\nthat our model outperforms other approaches as well as the Google Docs Spell\nChecking tool, achieves an 86.24 BLEU score on this task.\nKeywords: Vietnamese Spelling Correction · BERT · Transformer\n1 Introduction\nA spelling error is a word written in a wrong spelling standard, including various forms:\nhomophone, acronym, uppercase, or fairly the phenomenon of wrong-written words.\nUsually, there are many groups of origination causing the spelling errors to happen in\nVietnamese: typing, semantic confusion, local pronunciation, rules, and standards in\nthe written text, not mastery in grammar and influence of social network language, etc\n[29].\nSpelling correction is a Natural Language Processing task that focuses on correcting\nspelling errors in text or a document. The spelling correction task keeps a critical role\nin enhancing the user typing experience and guarantees the information integrity of\nVietnamese. Besides, one primary application is the ability to incorporate with other\ntasks. For example, when using the spelling correction attached to the last phase of\nthe Scene Text Detection / Optical Character Recognition (OCR) task, the results are\nimproved significantly [1,11]. Consider the chatbot task, if spelling correction is applied\nto preprocess user inputs, the chatbot will have better accuracy and performance in\nunderstanding the user requests [27].\nFrequently, the spelling correction can be divided into two steps, including spell\nchecking and spell correcting [7]. In the first phase, mistakes are investigated if there\nare any in the given input and then try to transform the wrong words into corrected\nwords in the second phase.\nContrary to English and other languages, the Vietnamese possess up to six com-\nplex diacritic marks and uses them as a discrimination sign. Therefore, a word that\narXiv:2405.02573v1  [cs.CL]  4 May 2024\n2 Hieu et al.\ncombines with different diacritic marks can create up to six written forms, and each\nof them also has independent meaning and usage. For instance, the word “ma” (ghost)\ncan be written in 5 more ways with 5 different diacritic marks: “má” (mother), “mà”\n(nevertheless), “mả\"(tomb), “mã\"(code), “mạ\"(rice seedlings). All the originations and\nelements described above have made the Vietnamese spelling correction problem a very\nchallenging task.\nThere are many initial approaches to the Vietnamese Spelling Correction task that\nhas been carried out such as applying rule-based methods [10], using edit-distance al-\ngorithm [20], collating with dictionaries, using n-gram/big-gram language model [19],\netc. However, most of these approaches neither adapted to out-of-vocabulary words\nnor did they take the contextualized word embeddings into account. In order to deal\nwith these gaps, many deep learning models using Recurrent Neural Network (RNN) or\nLong Short-term Memory (LSTM) networks have been proposed and achieved impres-\nsive performance [21].\nRecently, spelling correction studies that took advantage of the Encoder-Decoder\nmodel have attracted much attention and achieved state-of-the-art in the English spelling\ncorrection task [14,30]. This is a novel approach, which is notably potential because of\nthe optimal utilization of the parallelism calculation ability and the strength of powerful\npre-trained language models. One of the most attention is the usage of the Transformer\narchitecture [28] with the language model known as BERT [2]. Despite its success in\nEnglish [31], there is still no implementation in Vietnamese that can be used in practice.\nTherefore, this paper aims to apply these architectures and techniques to improve the\nperformance of correcting Vietnamese spelling errors. The experiment results show\nthat the proposed solution achieves considerable efficiency and is able to integrate with\npractical services. The main contributions of this study could be summarized as follows:\n– Applying the Transformer architecture and leveraging the pre-trained BERT to pro-\nvide a solution to the Vietnamese spelling correction problem.\n– Constructing a large and creditable dataset based on the most common practical\nVietnamese spelling errors. The evaluation dataset is published for the Vietnamese\nNLP community using in related works.\nThe remainder of this paper is organized as follows. In section 2, related works\nare presented and discussed. Section 3 describes the proposed methods in detail. The\ndataset, experimental results, and discussion are provided in section 4. Section 5 sum-\nmarizes, concludes and gives future orientation.\n2 Related Works\nSpelling correction is not a new problem in NLP tasks. Earlier there have been many\napproaches for this problem, from straightforward approach using probability, such as\nimplementing the Naive Bayes algorithm (Peter Norvig 1). The large N-gam-based lan-\nguage modeling approach of both left and right side has improved the performance of\nspelling correction tasks [19]. After training with a large corpus, this model can predict\n1 https://norvig.com/spell-correct.html\nBERT & Transformer for Vietnamese Spelling Correction 3\nthe probabilities of multiple N-gram candidates for correcting words. Large N-gram\nLM is a pure probability approach. It expects high memory resources to store all pre-\ncalculated probabilities of N-gram pairs and can not handle a not-pre-trained error,\nwhich leads to all of the probability of N-gram pairs to zero.\nThe advantages of contextual embedding in word presentation model, likes Word2Vec\n[16], Glove [24], etc, is being taken into the spelling correction task [4]. An edit-distance\nalgorithm generates the candidates, then each candidate’s score is calculated by the co-\nsine similarity between the candidate vector and the target word vector, the highest score\nranking candidate will be selected. This method has shown significant results in the\nspelling correction task and is suitable for many languages, especially in Vietnamese,\nEnglish, etc. On the other hand, this approach requires many resources to represent the\nrich context embeddings of a language accurately. Also, out-of-vocabulary (OOV) is a\nlarge major problem to the ranking system.\nAnother approach to using deep learning has been developed through the use of\nLSTM network [18]. A LSTM network [8] is constructed that encodes the input se-\nquence and then decodes it to the expected correct output sequence, respectively. The\naccuracy of their model makes a significant gap compared to the current state-of-the-\nart model [19]. Studies have reported that spelling correction can be beneficial from\nEncoder-Decoder architecture [9,25]. A state-of-the-art approach in English is imple-\nmented by the Encoder-Decoder architecture [26] and also makes use of the powerful\npretrained BERT model [9]. They first fine-tuned the BERT model and then used its\nlast hidden presentation output as additional features to the an error correction model,\nwhich is a customized Transformers [28] architecture. A similar method for Vietnamese\ngrammatical error correction using OpenNMT framework [13] instead of the Trans-\nformer architecture [25]. This method, respectively, depends on using the Microsoft\nOffice spelling tool to check and detect the incorrect text before the correction step.\nThrough previous related works, deep learning approaches to spelling correction\nare our focus. The approach is receiving much concern, gains state-of-the-art perfor-\nmance is the Encoder-Decoder architecture with prominent pre-trained MLM. Both\nwell-known pre-trained Google Multilingual [2] and vinai/phobert [17] are used to ex-\ntract hidden presentations and implement the transformer architecture into a specific\nVietnamese spelling correction task.\n3 Our approach\n3.1 Introduction to the Vietnamese language\nThis section briefly presents the characteristics and differences from English of the\nVietnamese language. Unlike neighbor countries, the Vietnamese does not use hiero-\nglyphic letters, but a modified Latin (Roman) alphabet. The Vietnamese alphabet uses\n29 letters, unlike the English alphabet, it does not use 4 letters ’w’, ’f’, ’j’, ’z’ and uses\n6 more vowel letters (with special mark): ’ă’, ’â’, ’ê’, ’ô’, ’ơ’, ’ê’, and the letter ’đ’ [3,6].\nAlong with the above 6 types of diacritics, it forms up to 67 separate letters (nearly\n3 times larger than the number of letters in English). Therefore, spelling mistakes are\nmuch more common in Vietnamese than in English.\n4 Hieu et al.\n3.2 Analyzing of Vietnamese common spelling error\nIn this section, the concept of common error type in the Vietnamese language are pre-\nsented. Due to the lacks of scientific public research or national survey constructed on\nthis topic, various types of Vietnamese error type from previous related work [18,19,20]\nare summarized and divided them into six groups:\n– Abbreviation: There are a wide variety of abbreviation for common words in Viet-\nnamese writing. Despite its convenience, this style of writing may raise misunder-\nstanding, make the writing less formal and not accepted by most people. To de-\ntermine this error cases, a list of most common abbreviation substitutions in Viet-\nnamese is compiled from the Internet.\n– Region: The region error type is the most complicated type to analyze owing to its\nvariety of happening contexts. The region error type comes from different region\npronunciation across the Vietnam territory. When people tend to write a word the\nsame way they pronounce it, this error occurs. Many adults may mistake this type\nof error if not a native speaker or do not have enough knowledge of the Vietnamese\nlanguage. An incorrect word with region type stands alone, may still have meaning.\nSome examples of region error type are described in table 1.\nTable 1. Some examples of region error type\nOriginal Usually mistake for Original Usually mistake for\nch- tr- -nh -n\ntr- ch c- k-\n-n -ng k- c-\n-ng -n ngh- ng-\ng- gi- gi- g-\n... ... ... ...\n– Teencode: Teencode (or Teen-code) is a method of writing used by teenagers on\nsocial media or through messaging. Those teenagers put words into special encryp-\ntion so the adults can not understand.\n– Telex: Telex is a convention for encoding Vietnamese text in plain ASCII charac-\nters, used initially for transmitting Vietnamese text over telex systems. Forgetting to\nturn on the language encoder or entering the wrong Vietnamese Telex rules leads\nto this type of error.\n– Fat Finger Fat Finger, also known as the clumsy finger, means when typing through\na cell phone or computer keyboard, the user’s finger mistypes the surrounding key\ninstead of the target key, causing the wrong words.\n– Edit Distance Edit Distance is a pseudo error generation strategy in which several\ncharacters equal to a ’distance’ to the original are randomly replaces. Although this\nerror rarely happens logically, a low percentage amount is still generated in our data\nset.\nFor the convenience of observation, a list of examples corresponding to the type of\nerror is presented in table 2.\nBERT & Transformer for Vietnamese Spelling Correction 5\nTable 2. A summary about error types\nError type When it happends Examples\nCorrect Incorrect\nAbbreviation\nTo make writing faster and more conv-\nenient, people use abbreviation instead of\nfull words.\nKhông (No)\nMọi người (Everyone)\nBình thường (Normal)\nKg\nMn\nBt\nTeenTeencodeWhen teenagers try to encode their text in\norder to hide information.\nVí dụ (Example)\nChồng (Husband)\nĐiện thoại (Phone)\nVj du\nCho‘ng\nDj3n tk04j\nFat-Finger\nWhile typing with a virtual keyboard, the\nuser’s finger mistypes the surroundings in-\nstead of the target key.\nXin chào (Hello)\nTrường học (School)\nĐiện thoại (Phone)\nXim chào\nTrường hịc\nĐiện thọak\nTelex\nForgetting to turn on the language encoder\nor entering the wrong Vietnamese Telex\nrules.\nXin chào (Hello)\nTrường học (School)\nĐiện thoại (Phone)\nXin chafo\nTruowng hojc\nDdieejn thoaji\nRegion\nDifferent region pronunciation across the\nVietnam (as people tend to write the same\nas as they pronounce).\nTranh (Painting)\nLạnh (Cold)\nNghỉ (Rest)\nChanh (Lemon)\nNạnh\nNghĩ(Think)\nEdit-Distance A common pseudo misspelling error gen-\neration strategy.\nTranh (Painting)\nLạnh (Cold)\nMưa (Rain)\nThanh (Bar)\nLhoạnh\nMtưa\n3.3 BERT\nBERT is a language representation model based on multi-layer bidirectional transform-\ners encoder architecture. There is a wide variety of challenging natural language tasks\nthat BERT can handle and achieve state-of-the-art performance, from classification,\nquestion answering, and sequence-to-sequence learning task, etc. BERT can repre-\nsent sentence effectively by its encoding mechanism, including various embedding step\nas token embeddings, sentence embeddings, and transformers positional embeddings.\nThen, this BERT’s last hidden presentation output from BERT is used as the input into\nthe transformers architecture.\nIn this study, two well pre-trained BERT on the Vietnamese: the Google Multilin-\ngual BERT2 (bert-base-multilingual-cased) and VinAI/phoBERT3 are considered. They\nare both trained with extensive Vietnamese corpus, while the multilingual BERT is the\nBERT base model, and phoBERT is a RoBERTa model [15] (which is a modified ver-\nsion from the base model).\n3.4 Transformers\nBefore the transformers architecture, Encoder-Decoder architecture using RNN/ LSTM/\nGRU (Gated Recurrent Unit) cell is used widely in machine translation and sequence-to-\nsequence tasks. This Encoder-Decoder architecture, also known as the Seq2Seq model,\n2 Github: https://github.com/google-research/bert\n3 Github: https://github.com/VinAIResearch/PhoBERT\n6 Hieu et al.\nuses several RNN cells to encode the input tokens to hidden states and then sum all\nhidden states up before sending them to the decoder. Thanks to this hidden state, the\ndecoders receive all previously encoded information and use it for the output token\nprediction task. Despite large capacities in handling sequence-to-sequence tasks, the\nSeq2Seq decoder may fail to fully capture the meaning and context of the last hidden\npresentation from the encoder. That means the more extended and more complex the\ninput sequence, the less effective the hidden presentation can represent, which is known\nas the bottleneck problem.\nWhile the attention mechanism takes several inputs simultaneously, construct weight\nmatrices captured from each hidden presentation input to calculate a weighted sum of\nall the past encoder states. The decoder will then take the inputs and the provided at-\ntention weights, and through that, the decoder knows how to ’pay much attention’ to\nwhich hidden presentation and vice versa. Another limitation of Seq2Seq architecture\nis that it handles the input sequentially, which means to compute for the current to-\nken at time t, we need the previous hidden state t-1 and so on. Therefore, especially\nin the spelling correction task, if many erroneous tokens stick together, the correction\nof the last tokens can be poorly affected. The transformer architecture and attention\nmechanism come to cross those boundaries of those previous architectures. Based on\nthe Encoder-Decoder architecture [26], the transformers architecture [28] uses stacked\nmulti-head self-attention and fully connected layers. The transformer is designed to al-\nlow parallel computation and reduce the drop in performance due to long dependencies.\nIt uses positional embeddings and multi-head self-attention to encode more information\nabout the position of a token and the relation between each token.\nAs Shun has provided an advanced insights to incorporating pseudo data into the\nspelling correction task [12]. Consequently, a vast pseudo training dataset can be gen-\nerated so that not only can the transformer maximize its parallelization ability but also\nBERT can represent its rich contextual embedding vectors.\n3.5 Incorporate BERT into Transformers\nAs mentioned in section 3.3, BERT is capable of deep language understanding by cap-\nturing contextual embedding of different words in a sequence. In addition, the Trans-\nformer model has been proved to be more efficient than popular Encoder-Decoder ar-\nchitectures, especially in the Machine Translation problem.\nSome recent studies have treated the spelling correction problem as a machine trans-\nlation job where the error sentence is the source sequence and the corrected sentence\nis the target sequence. And the combination of BERT and Transformer achieves state-\nof-the-art results for the English spelling correction task [9]. However, to the best of\nour knowledge, there has not been any research combining BERT and Transformer for\nVietnamese spelling correction problem. The combination can be briefly summarized\nin the following steps:\n– Step 1 : Let the input sentence notated as X = ( x1, ..., xn), where n is the number\nof its tokens; xi is the i-th token in X. BERT receives the input sequence tokens,\nand through its layers, BERT extracts them to hidden presentations notated as HB\n= (h1, h2, ..., hn), where HB is the output of the last layers in BERT.\nBERT & Transformer for Vietnamese Spelling Correction 7\n– Step 2 : The Encoder will take HB from the previous step and encode the represen-\ntation of each l layer Hl\nE. The final contextual representation of the last encoder\nlayer HL\nE is the output of the Encoder. The Encoder components consist of the\nmulti-head self-attention mechanism, position-wise fully connected feed-forward\nnetwork. A residual connection around each of the two sub-layers, followed by layer\nnormalization. A Multi-head Attention is a component allowing the model to jointly\nattend to information from different representations and helps the encoder look at\nother words in the input sentence as it encodes a specific word for better-capturing\ncontextual embedding.\n– Step 3 : The Decoder receives the representation HL\nE from the Encoder and decodes\nthrough its layers into final representation HL\nD. Similar to the Encoder, the Decoder\npossessed the same components of the Encoder. These Encoder HL\nE are to be used\nby each Decoder in its “encoder-decoder attention” layer which helps the Decoder\nfocus on appropriate places in the input sequence.\n– Step 4 : Finally, the Decoder final representation HL\nD is mapped via a linear trans-\nformation and softmax to get the t-th predicted word ˆy. The decoding process con-\ntinues until meeting the end-of-sentence token.\nAn illustration of our proposed method is shown in Figure 1.\n4 Experimental Evaluation\nThis section includes dataset description, evaluation method, model hyper-parameter\nsetting as well as experimental results of applying the Transformer architecture and\nBERT to Vietnamese spelling correction.\n4.1 Experimental dataset\nThis section describes the process of creating our training and testing set based on the\nBinhvq News Corpus 4 which contains 14,896,998 Vietnamese news crawled from the\nInternet and preprocessed, including steps like HTML tag removal, duplicate removal,\nNFC standardization, and sentence segmentation. The corpus is gathered from reputable\nnews and media sites in Vietnam, so the data is very reliable in terms of spelling. For\nthe purpose of training and evaluating spelling correction, our newly constructed dataset\nmust consist of two fields that can be described as a pair of correct and incorrect spelling\nsentences.\nTo the best of our knowledge, there is no specific survey as well as assessment on\nthe rate of error types appearing in Vietnamese. However, Vietnamese often has com-\nmon spelling mistakes: Region, FatFinger, Telex. Besides, some other types of errors\nare concerned, such as Edit-Distance, Abbreviation, Teencode, but rarely happened in\npractice. Therefore, error rate is reproduced based on our experience. Details of the\nrates of error types in the generated data set are listed in table 3.\n4 Github: https://github.com/binhvq/news-corpus\n8 Hieu et al.\nFigure 1. Proposed combination between BERT and Transformer\nTable 3. Training and Testing sets\nError Type Error Ratio (%)\nAcronym 3.0\nTeencode sets 3.0\nEdit-Distance 3.0\nFatFinger 30.0\nTelex 30.0\nRegion 31.0\nBERT & Transformer for Vietnamese Spelling Correction 9\nThe training set is composed by randomly selecting 4,000,000 sentences from the\nBinhvq corpus and then apply the pseudo error generator to these correct ones. All the\nsentences must have an average word count between 50-60 words per sentence. Simi-\nlarly, validating set and testing set are generated with the number of correct sentences\nchosen from the above corpus 20,000 and 6,000 respectively. Details of the dataset can\nbe summarized in table 4. The testing set is public and can be downloaded at the fol-\nlowing link 5.\nTable 4. Training and Testing sets\nDataset Size (#Pair of sentence) Avg. Length per sentence (#token)\nTraining sets 4,000,000 60\nValidating sets 20,000 60\nTesting sets 6,000 60\n4.2 Evaluating Metric\nIn the perspective of a spelling correction task, many traditional approaches used Accu-\nracy, Precision, Recall, and F1 for evaluation [18,19]. These metrics require the predic-\ntions’ words to have the same length as labels’ words. Recently, BLEU score is chosen,\nespecially in deep learning models because of its ability to adapt to different prediction\nlengths [9,31]. Therefore, BLUE [23] is selected for the evaluating task. BLEU, or the\nBilingual Evaluation Understudy, is a score for comparing a candidate translation of text\nto one or more reference translations. Although developed for translation, it can be used\nto evaluate text generated for a suite of natural language processing tasks. Our BLEU\nconfiguration uses four n-grams settings because the spelling correction task critically\nrequires the order of words in the sentence.\nBP =\n\u001a 1 if r > c\ne(1−r/c) if r ≤ c (1)\nBLEU = BP · exp\n NX\nn=1\nwn log pn\n!\n(2)\nWhere BP stands for Brevity Penalty. c, r is the length of the predictions and labels,\nrespectively. BP will penalty cases where the model failed to propose correction, or\nthe change happens more than allowed (as the number of words need to be corrected\nmust be equal to the actual corrected). pn stands for modified n-gram precision, using\nn-grams up to length N and positive weights wn summing to one. The n-gram precision\ncan be simply understands as ’the number of corrected words which occur in reference\nsentence (ground-truth)’ divided by ’the number of words after sentence transformed’.\nTherefore, the BLEU metrics has potential to not only to keep track strictly of word\nordering by measuring n-gram (up-to-4) overlapping but also evaluate how a sentence\nhas been corrected from the original despite the action (remove, edit, add more words).\n5 Github: https://github.com/tranhamduong/Vietnamese-Spelling-Correction-testset\n10 Hieu et al.\n4.3 Model Settings\nOur models are implemented by fairseq toolkit [22] which is an re-implementation on\nthe base Transformer architecture [28]. To find the appropriate hyperparameters for the\nproposed model, experiments with multiple model designations has been reviewed and\nthe configuration of Jinhua work [31] are selected. Training details with hyperparameter\nsettings are in the table 5:\nTable 5. Hyperparameters of Transformer model\nBERT model bert-base-multilingual-cased\nvinai/phobert-base\nNumber of epochs 100\nDropout 0.3\nLoss Function labeled smoothed cross-entropy\nOptimizer Adam(0.9,0.98)\nLearning Rate 0.005\nLabel Smoothing 0.1\nWeight Decay 0.0001\nBeam Search 5\nMax tokens 1280\n4.4 Experimental Results and Discussion\nIn this phase, we compared with the Google Docs spell checking tool 6 and other meth-\nods. From the results showed in Figure 6, two versions of our model, Transformer+vinai/\nphoBERT and Transformer+BERT-multi-cased, achieved better results than the previ-\nous methods. This partly reinforces our hypothesis that using a pre-trained language\nmodel BERT brings two benefits to the spelling correction problem: being applicable in\nthe spelling correction task and taking advantage of contextualized word embeddings.\nFirstly, as mentioned in the BERT paper, tasks such as Text Classification, Question\nand Answering, Sentence Tagging, etc, are recommended to be used in the fined-tuning\nphase but the spelling correction task. Due to our modification, at the first step, BERT\nis verified to be beneficial for the correction task. Secondly, when correcting an error\nword, the action of choosing a suitable candidate based on context words is the main\ncharacteristic of the spelling correction problem. Concretely, BERT produces contex-\ntualized word embedding (the same word for different contexts have different embed-\ndings) helps the models to better utilize word embedding at correcting phase. Besides,\nthe pre-training of BERT on a huge data set also makes fine-tuning for our model easier\n6 The tool can be found on the Google Docs website (https://docs.google.com/). We collected\nsamples by using a web browser behavior simulator based on Selenium framework that manip-\nulate the Google spell checking tool to correct all of its possible suggestions.\nBERT & Transformer for Vietnamese Spelling Correction 11\nbecause of no need to re-train from the beginning, taking advantage of the knowledge\nfrom the language model.\nTable 6. BLEU scores on models\nModel BLEU score\nGoogle Docs spellchecking tool 0.6829\nTransformer + vinai/phobert-base 0.8027\nWord2Vec 0.8222\nTransformer + bert-multi-cased 0.8624\nTransformer + vinai/phobert-base: The proposed model based on the Transformer architec-\nture and PhoBERT [17].\nWord2Vec: The reimplementation of the Word2Vec approach in spelling correction [5].\nTransformer + bert-multi-cased: The proposed model based on the Transformer architecture\nand BERT multilingual model [2].\nFor the objective of comparison and practical application, there are a few patterns\nthat our excellent model gain out performance: telex and edit-distance error types, com-\npared to the google docs spellchecking tool. This happened partially because we desig-\nnated more of these types of error to achieve our goal. More tuning is needed in future\nwork on the error type distribution to improve performance for other types of errors.\nThe google docs spellchecking tool has another advantage over our model is the\nability to restrict unnecessary correction. Additionally, the emergence of proper nouns\nalso makes our model ineffective. When it comes to a proper noun, especially Viet-\nnamese proper names, our model tends to correct them, which should not be the case.\nTo overcome this weakness, some supporting components can be developed to the pro-\nposed architecture: Applying a name entity recognition component or an independent\nspellchecker to determine to correct a word or not.\n5 Conclusion\nIn this paper, a combination of BERT and Transformer architecture is implemented for\nthe Vietnamese spell correction task. The experimental results show that our model\noutperforms other approaches with a 0.86 BLUE score and can be used in real-world\napplications. Besides, a dataset is constructed for related works based on a breakdown\nof the spelling correction problem to define which errors commonly happened and need\nmore attention.\nTo our concern, despite the improvement in the model’s performance, there are late\ninferences due to large and complex architecture. In addition, due to the different dis-\ntribution of data in the pre-trained model compared to data for the spelling correction\ntask, we can not fully utilize the representation of pre-trained words, resulting in the\nmodel sometimes try to correct the unwanted words.\nIn the future, our architecture will be experimented with other existing pre-trained\nlanguage models to see how well the compatibility they are. Moreover, we also evaluate\n12 Hieu et al.\nour model’s accuracy on a bigger dataset. Finally, investigating and analyzing errors that\nmay happen in practices is our priority in order to create a better error pseudo generator.\nReferences\n1. Bassil, Y., Alwani, M.: Ocr post-processing error correction algorithm using google online\nspelling suggestion. Journal of Emerging Trends in Computing and Information Sciences\"(04\n2012)\n2. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirectional trans-\nformers for language understanding pp. 4171–4186 (2019)\n3. of Education Vietnam, M.: Sách Giáo khoa tiếng Việt 1 (Tập Một). Ministry of Education\nPublisher (2002)\n4. Fivez, P., ˇSuster, S., Daelemans, W.: Unsupervised context-sensitive spelling correction of\nclinical free-text with word and character n-gram embeddings. In: BioNLP 2017. pp. 143–\n148. Association for Computational Linguistics, Vancouver, Canada, (Aug 2017)\n5. Fivez, P., Suster, S., Daelemans, W.: Unsupervised context-sensitive spelling correction of\nenglish and dutch clinical free-text with word and character n-gram embeddings (2017)\n6. Hao, C.X.: Tiếng Việt, văn Việt, người Việt. Youth Publisher (2003)\n7. Hladek, D., Sta ˇs, J., Pleva, M.: Survey of automatic spelling correction. Electronics 9, 1670\n(10 2020)\n8. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation 9, 1735–80\n(12 1997)\n9. Kaneko, M., Mita, M., Kiyono, S., Suzuki, J., Inui, K.: Encoder-decoder models can benefit\nfrom pre-trained masked language models in grammatical error correction. In: Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4248–\n4254. Association for Computational Linguistics (2020)\n10. Khanh, P.H.: Good spelling of vietnamese texts, one aspect of computational linguistics in\nvietnam. In: Proceedings of the 38th Annual Meeting on Association for Computational\nLinguistics. p. 1–2. ACL ’00, Association for Computational Linguistics, USA (2000)\n11. Kissos, I., Dershowitz, N.: Ocr error correction using character correction and feature-based\nword classification. In: 2016 12th IAPR Workshop on Document Analysis Systems (DAS).\npp. 198–203. IEEE (2016)\n12. Kiyono, S., Suzuki, J., Mita, M., Mizumoto, T., Inui, K.: An empirical study of incorporating\npseudo data into grammatical error correction. In: Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP). pp. 1236–1242. Association for\nComputational Linguistics, Hong Kong, China (Nov 2019)\n13. Klein, G., Kim, Y., Deng, Y., Senellart, J., Rush, A.: OpenNMT: Open-source toolkit for\nneural machine translation. In: Proceedings of ACL 2017, System Demonstrations. pp. 67–\n72. Association for Computational Linguistics, Vancouver, Canada (Jul 2017)\n14. Liu, J., Cheng, F., Wang, Y., Shindo, H., Matsumoto, Y.: Automatic error correction on\nJapanese functional expressions using character-based neural machine translation. In: Pro-\nceedings of the 32nd Pacific Asia Conference on Language, Information and Computation.\nAssociation for Computational Linguistics, Hong Kong (1–3 Dec 2018)\n15. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\nL., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692 (2019)\n16. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations\nin vector space. In: 1st International Conference on Learning Representations, ICLR 2013,\nScottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings (2013)\nBERT & Transformer for Vietnamese Spelling Correction 13\n17. Nguyen, D.Q., Nguyen, A.T.: PhoBERT: Pre-trained language models for Vietnamese. In:\nFindings of the Association for Computational Linguistics: EMNLP 2020. pp. 1037–1042\n(2020)\n18. Nguyen, H., Dang, T.B., Nguyen, L.M.: Deep learning approach for vietnamese consonant\nmisspell correction. In: Nguyen, L., Phan, X., Hasida, K., Tojo, S. (eds.) Computational Lin-\nguistics - 16th International Conference of the Pacific Association for Computational Lin-\nguistics, PACLING 2019, Hanoi, Vietnam, October 11-13, 2019, Revised Selected Papers.\nCommunications in Computer and Information Science, vol. 1215, pp. 497–504. Springer\n(2019)\n19. Nguyen, H., Dang, T., Nguyen, T.T., Le, C.: Using large n-gram for vietnamese spell check-\ning. Advances in Intelligent Systems and Computing 326, 617–627 (01 2015)\n20. Nguyen, P.H., Ngo, T.D., Phan, D.A., Dinh, T.P., Huynh, T.Q.: Vietnamese spelling detec-\ntion and correction using bi-gram, minimum edit distance, soundex algorithms with some\nadditional heuristics. In: 2008 IEEE International Conference on Research, Innovation and\nVision for the Future in Computing and Communication Technologies. pp. 96–102. IEEE\n(2008)\n21. Nguyen, Q.D., Le, D.A., Zelinka, I.: Ocr error correction for unconstrained vietnamese hand-\nwritten text. pp. 132–138 (12 2019)\n22. Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., Auli, M.: fairseq: A\nfast, extensible toolkit for sequence modeling. In: Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics (Demonstrations).\npp. 48–53. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019)\n23. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of ma-\nchine translation. In: Proceedings of the 40th Annual Meeting of the Association for Compu-\ntational Linguistics. pp. 311–318. Association for Computational Linguistics, Philadelphia,\nPennsylvania, USA (Jul 2002)\n24. Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word representation. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). pp. 1532–1543. Association for Computational Linguistics, Doha, Qatar (Oct\n2014)\n25. Pham, N.L., Nguyen, T.H., Nguyen, V.V.: Grammatical error correction for vietnamese using\nmachine translation. In: International Conference of the Pacific Association for Computa-\ntional Linguistics. pp. 505–512. Springer (2019)\n26. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural networks. p.\n3104–3112. NIPS’14, MIT Press, Cambridge, MA, USA (2014)\n27. Tedjopranoto, M., Wijaya, A., Santoso, L., Suhartono, D.: Correcting typographical error and\nunderstanding user intention in chatbot by combining n-gram and machine learning using\nschema matching technique. International Journal of Machine Learning and Computing 9,\n471–476 (08 2019)\n28. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polo-\nsukhin, I.: Attention is all you need. In: Advances in neural information processing systems.\npp. 5998–6008 (2017)\n29. Xuan, P.: Solutions to spelling mistakes in written vietnamese. VNU Journal of Science:\nEducation Research 33(2) (2017)\n30. Yuan, Z., Briscoe, T.: Grammatical error correction using neural machine translation. In:\nProceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. pp. 380–386. Association for\nComputational Linguistics (Jun 2016)\n31. Zhu, J., Xia, Y., Wu, L., He, D., Qin, T., Zhou, W., Li, H., Liu, T.: Incorporating BERT into\nneural machine translation. In: Eighth International Conference on Learning Representations\n(2020)"
}