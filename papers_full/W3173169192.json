{
  "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning",
  "url": "https://openalex.org/W3173169192",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2808328157",
      "name": "Ryuichi Takanobu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127420617",
      "name": "Heng Ji",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2162268045",
      "name": "Minlie Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2980360762",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W2181042685",
    "https://openalex.org/W3104390324",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3098266846",
    "https://openalex.org/W1713614699",
    "https://openalex.org/W3021224558",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2129767020",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W1566346388",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2157444450",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2626154462",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3102663935",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2951328433",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W4288104771",
    "https://openalex.org/W2250521169",
    "https://openalex.org/W2952179106",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2995040292",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2983102021",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2912924812"
  ],
  "abstract": "Yujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu, Peng Li, Heng Ji, Minlie Huang, Maosong Sun, Jie Zhou. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 3350–3363\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n3350\nERICA: Improving Entity and Relation Understanding for\nPre-trained Language Models via Contrastive Learning\nYujia Qin♣♠♦, Yankai Lin♦, Ryuichi Takanobu♣♦, Zhiyuan Liu♣∗, Peng Li♦, Heng Ji♠∗,\nMinlie Huang♣, Maosong Sun♣, Jie Zhou♦\n♣Department of Computer Science and Technology, Tsinghua University, Beijing, China\n♠University of Illinois at Urbana-Champaign\n♦Pattern Recognition Center, WeChat AI, Tencent Inc.\nyujiaqin16@gmail.com\nAbstract\nPre-trained Language Models (PLMs) have\nshown superior performance on various down-\nstream Natural Language Processing (NLP)\ntasks. However, conventional pre-training ob-\njectives do not explicitly model relational facts\nin text, which are crucial for textual under-\nstanding. To address this issue, we propose a\nnovel contrastive learning framework ERICA\nto obtain a deep understanding of the entities\nand their relations in text. Speciﬁcally, we de-\nﬁne two novel pre-training tasks to better un-\nderstand entities and relations: (1) the entity\ndiscrimination task to distinguish which tail\nentity can be inferred by the given head en-\ntity and relation; (2) the relation discrimination\ntask to distinguish whether two relations are\nclose or not semantically, which involves com-\nplex relational reasoning. Experimental results\ndemonstrate that ERICA can improve typical\nPLMs (BERT and RoBERTa) on several lan-\nguage understanding tasks, including relation\nextraction, entity typing and question answer-\ning, especially under low-resource settings.1\n1 Introduction\nPre-trained Language Models (PLMs) (Devlin\net al., 2018; Yang et al., 2019; Liu et al., 2019) have\nshown superior performance on various Natural\nLanguage Processing (NLP) tasks such as text clas-\nsiﬁcation (Wang et al., 2018), named entity recog-\nnition (Sang and De Meulder, 2003), and question\nanswering (Talmor and Berant, 2019). Beneﬁting\nfrom designing various effective self-supervised\nlearning objectives, such as masked language mod-\neling (Devlin et al., 2018), PLMs can effectively\ncapture the syntax and semantics in text to gener-\nate informative language representations for down-\nstream NLP tasks.\n∗Corresponding author.\n1Our code and data are publicly available at https://\ngithub.com/thunlp/ERICA.\n[1]CuliacánisacityinnorthwesternMexico.[2]CuliacánisthecapitalofthestateofSinaloa.[3]CuliacánisalsotheseatofCuliacánMunicipality.[4]Ithadanurbanpopulationof785,800in2015while905,660livedintheentiremunicipality.[5]WhileCuliacánMunicipalityhasatotalareaof4,758k!!,Culiacánitselfisconsiderablysmaller,measuringonly.[6]CuliacánisarailjunctionandislocatedonthePanamericanHighwaythatrunssouthtoGuadalajaraandMexicoCity.[7]CuliacánisconnectedtothenorthwithLosMochis,andtothesouthwithMazatlán,Tepic.\nCuliacán\nQ: where is Guadalajara?Culiacán\nMexico\nPanamericanHighway\ncity of south to\nlocate onA: Mexico.\nCuliacánMunicipalitySinaloaGuadalajaraMexico CityLos Mochis\nFigure 1: An example for a document “Culiacán”, in\nwhich all entities are underlined. We show entities and\ntheir relations as a relational graph, and highlight the\nimportant entities and relations to ﬁnd out “where is\nGuadalajara”.\nHowever, conventional pre-training objectives\ndo not explicitly model relational facts, which fre-\nquently distribute in text and are crucial for under-\nstanding the whole text. To address this issue, some\nrecent studies attempt to improve PLMs to better\nunderstand relations between entities (Soares et al.,\n2019; Peng et al., 2020). However, they mainly\nfocus on within-sentence relations in isolation, ig-\nnoring the understanding of entities, and the inter-\nactions among multiple entities at document level,\nwhose relation understanding involves complex rea-\nsoning patterns. According to the statistics on a\nhuman-annotated corpus sampled from Wikipedia\ndocuments by Yao et al. (2019), at least 40.7% re-\nlational facts require to be extracted from multiple\nsentences. Speciﬁcally, we show an example in Fig-\nure 1, to understand that “Guadalajara is located in\nMexico”, we need to consider the following clues\njointly: (i) “Mexico” is the country of “Culiacán”\nfrom sentence 1; (ii) “Culiacán” is a rail junction lo-\n3351\ncated on “Panamerican Highway” from sentence 6;\n(iii) “Panamerican Highway” connects to “Guadala-\njara” from sentence 6. From the example, we can\nsee that there are two main challenges to capture\nthe in-text relational facts:\n1. To understand an entity, we should consider\nits relations to other entities comprehensively. In\nthe example, the entity “Culiacán”, occurring in\nsentence 1, 2, 3, 5, 6 and 7, plays an important\nrole in ﬁnding out the answer. To understand “Culi-\nacán”, we should consider all its connected entities\nand diverse relations among them.\n2. To understand a relation, we should consider\nthe complex reasoning patterns in text. For exam-\nple, to understand the complex inference chain in\nthe example, we need to perform multi-hop reason-\ning, i.e., inferring that “Panamerican Highway” is\nlocated in “Mexico” through the ﬁrst two clues.\nIn this paper, we propose ERICA, a novel frame-\nwork to improve PLMs’ capability of Entity and\nRelatIon understanding via ContrAstive learning,\naiming to better capture in-text relational facts by\nconsidering the interactions among entities and re-\nlations comprehensively. Speciﬁcally, we deﬁne\ntwo novel pre-training tasks: (1) the entity discrim-\nination task to distinguish which tail entity can\nbe inferred by the given head entity and relation.\nIt improves the understanding of each entity via\nconsidering its relations to other entities in text;\n(2) the relation discrimination task to distinguish\nwhether two relations are close or not semantically.\nThrough constructing entity pairs with document-\nlevel distant supervision, it takes complex relational\nreasoning chains into consideration in an implicit\nway and thus improves relation understanding.\nWe conduct experiments on a suite of language\nunderstanding tasks, including relation extraction,\nentity typing and question answering. The experi-\nmental results show that ERICA improves the per-\nformance of typical PLMs (BERT and RoBERTa)\nand outperforms baselines, especially under low-\nresource settings, which demonstrates that ERICA\neffectively improves PLMs’ entity and relation un-\nderstanding and captures the in-text relational facts.\n2 Related Work\nDai and Le (2015) and Howard and Ruder (2018)\npropose to pre-train universal language representa-\ntions on unlabeled text, and perform task-speciﬁc\nﬁne-tuning. With the advance of computing power,\nPLMs such as OpenAI GPT (Radford et al., 2018),\nBERT (Devlin et al., 2018) and XLNet (Yang et al.,\n2019) based on deep Transformer (Vaswani et al.,\n2017) architecture demonstrate their superiority in\nvarious downstream NLP tasks. Since then, nu-\nmerous PLM extensions have been proposed to\nfurther explore the impacts of various model ar-\nchitectures (Song et al., 2019; Raffel et al., 2020),\nlarger model size (Raffel et al., 2020; Lan et al.,\n2020; Fedus et al., 2021), more pre-training cor-\npora (Liu et al., 2019), etc., to obtain better general\nlanguage understanding ability. Although achiev-\ning great success, these PLMs usually regard words\nas basic units in textual understanding, ignoring the\ninformative entities and their relations, which are\ncrucial for understanding the whole text.\nTo improve the entity and relation understand-\ning of PLMs, a typical line of work is knowledge-\nguided PLM, which incorporates external knowl-\nedge such as Knowledge Graphs (KGs) into PLMs\nto enhance the entity and relation understanding.\nSome enforce PLMs to memorize information\nabout real-world entities and propose novel pre-\ntraining objectives (Xiong et al., 2019; Wang et al.,\n2019; Sun et al., 2020; Yamada et al., 2020). Oth-\ners modify the internal structures of PLMs to fuse\nboth textual and KG’s information (Zhang et al.,\n2019; Peters et al., 2019; Wang et al., 2020; He\net al., 2020). Although knowledge-guided PLMs\nintroduce extra factual knowledge in KGs, these\nmethods ignore the intrinsic relational facts in text,\nmaking it hard to understand out-of-KG entities or\nknowledge in downstream tasks, let alone the errors\nand incompleteness of KGs. This veriﬁes the ne-\ncessity of teaching PLMs to understand relational\nfacts from contexts.\nAnother line of work is to directly model entities\nor relations in text in pre-training stage to break\nthe limitations of individual token representations.\nSome focus on obtaining better span representa-\ntions, including entity mentions, via span-based\npre-training (Sun et al., 2019; Joshi et al., 2020;\nKong et al., 2020; Ye et al., 2020). Others learn\nto extract relation-aware semantics from text by\ncomparing the sentences that share the same entity\npair or distantly supervised relation in KGs (Soares\net al., 2019; Peng et al., 2020). However, these\nmethods only consider either individual entities or\nwithin-sentence relations, which limits the perfor-\nmance in dealing with multiple entities and rela-\ntions at document level. In contrast, our ERICA\nconsiders the interactions among multiple entities\n3352\nFigure 2: An example of Entity Discrimination task.\nFor an entity pair with its distantly supervised relation\nin text, the ED task requires the ground-truth tail entity\nto be closer to the head entity than other entities.\nand relations comprehensively, achieving a better\nunderstanding of in-text relational facts.\n3 Methodology\nIn this section, we introduce the details of ERICA.\nWe ﬁrst describe the notations and how to represent\nentities and relations in documents. Then we detail\nthe two novel pre-training tasks: Entity Discrimi-\nnation (ED) task and Relation Discrimination (RD)\ntask, followed by the overall training objective.\n3.1 Notations\nERICA is trained on a large-scale unlabeled cor-\npus leveraging the distant supervision from an ex-\nternal KG K. Formally, let D = {di}|D|\ni=1 be a\nbatch of documents and Ei = {eij}|Ei|\nj=1 be all\nnamed entities in di, where eij is the j-th entity\nin di. For each document di, we enumerate all\nentity pairs (eij,eik) and link them to their corre-\nsponding relation ri\njk in K(if possible) and obtain\na tuple set Ti = {ti\njk = (di,eij,ri\njk,eik)|j ̸= k}.\nWe assign no_relation to those entity pairs with-\nout relation annotation in K. Then we obtain the\noverall tuple set T = T1\n⋃T2\n⋃...⋃T|D|for this\nbatch. The positive tuple set T+ is constructed\nby removing all tuples with no_relation from\nT. Beneﬁting from document-level distant su-\npervision, T+ includes both intra-sentence (rel-\natively simple cases) and inter-sentence entity pairs\n(hard cases), whose relation understanding involves\ncross-sentence, multi-hop, or coreferential reason-\ning, i.e., T+ = T+\nsingle\n⋃T+\ncross.\n3.2 Entity & Relation Representation\nFor each document di, we ﬁrst use a PLM to\nencode it and obtain a series of hidden states\n{h1,h2,..., h|di|}, then we apply mean pooling op-\neration over the consecutive tokens that mentioneij\nto obtain local entity representations. Note eij may\nappear multiple times in di, the k-th occurrence of\neij, which contains the tokens from index nk\nstart to\nnk\nend, is represented as:\nmk\neij = MeanPool(hnk\nstart\n,..., hnk\nend\n). (1)\nTo aggregate all information about eij, we aver-\nage2 all representations of each occurrence mk\neij\nas the global entity representation eij. Follow-\ning Soares et al. (2019), we concatenate the ﬁnal\nrepresentations of two entities eij1 and eij2 as their\nrelation representation, i.e., ri\nj1j2 = [eij1 ; eij2 ].\n3.3 Entity Discrimination Task\nEntity Discrimination (ED) task aims at inferring\nthe tail entity in a document given a head entity\nand a relation. By distinguishing the ground-truth\ntail entity from other entities in the text, it teaches\nPLMs to understand an entity via considering its\nrelations with other entities.\nAs shown in Figure 2, in practice, we ﬁrst\nsample a tuple ti\njk = (di,eij, ri\njk, eik) from T+,\nPLMs are then asked to distinguish the ground-\ntruth tail entity eik from other entities in the\ndocument di. To inform PLMs of which head\nentity and relation to be conditioned on, we\nconcatenate the relation name of ri\njk, the men-\ntion of head entity eij and a separation token\n[SEP] in front of di, i.e., d∗\ni =“relation_name\nentity_mention[SEP] di”3. The goal of entity\ndiscrimination task is equivalent to maximizing the\nposterior P(eik|eij,ri\njk) =softmax(f(eik)) (f(·)\nindicates an entity classiﬁer). However, we empiri-\ncally ﬁnd directly optimizing the posterior cannot\nwell consider the relations among entities. Hence,\nwe borrow the idea of contrastive learning (Hadsell\net al., 2006) and push the representations of pos-\nitive pair (eij, eik) closer than negative pairs, the\nloss function of ED task can be formulated as:\nLED = −\n∑\nti\njk∈T+\nlog\nexp(cos(eij,eik)/τ)\n|Ei|∑\nl=1, l̸=j\nexp(cos(eij,eil)/τ)\n,\n(2)\n2Although weighted summation by attention mechanism\nis an alternative, the speciﬁc method of entity information\naggregation is not our main concern.\n3Here we encode the modiﬁed document d∗\ni to obtain the\nentity representations. The newly added entity_mention is\nnot considered for head entity representation.\n3353\nDocument 1\nDocument 2\nDocument 3\nDocument 3\nsingle-sentence\ncross-sentence\nsingle-sentence\ncross-sentence\nfounded by…Since1773,whentheRoyalSwedishOperawasfoundedbyGustavIIIofSweden…\n…GatesisanAmericanbusinessmagnate,softwaredeveloper,andphilanthropist…HelefthisboardpositionsatMicrosoft…\n…SamarindaisthecapitalofEastKalimantan,Indonesia,ontheislandofBorneo…Samarindaisknownforitstraditionalfoodamplang,aswellastheclothSarungSamarinda…\n…SamarindaisthecapitalofEastKalimantan,Indonesia,ontheislandofBorneo…Samarindaisknownforitstraditionalfoodamplang,aswellastheclothSarungSamarinda…\nfounded by\ncapital ofcountry\nPre-trained Language Model\nFigure 3: An example of Relation Discrimination task.\nFor entity pairs belonging to the same relations, the RD\ntask requires their relation representations to be closer.\nwhere cos(·,·) denotes the cosine similarity be-\ntween two entity representations and τ (temper-\nature) is a hyper-parameter.\n3.4 Relation Discrimination Task\nRelation Discrimination (RD) task aims at distin-\nguishing whether two relations are close or not\nsemantically. Compared with existing relation-\nenhanced PLMs, we employ document-level rather\nthan sentence-level distant supervision to further\nmake PLMs comprehend the complex reasoning\nchains in real-world scenarios and thus improve\nPLMs’ relation understanding.\nAs depicted in Figure 3, we train the text-based\nrelation representations of the entity pairs that share\nthe same relations to be closer in the semantic\nspace. In practice, we linearly 4 sample a tuple\npair tA = (dA,eA1 , rA, eA2 ) and tB = (dB,eB1 ,\nrB, eB2 ) from T+\ns (T+\nsingle) or T+\nc (T+\ncross), where\nrA = rB. Using the method mentioned in Sec. 3.2,\nwe obtain the positive relation representations rtA\nand rtB for tA and tB. To discriminate positive\nexamples from negative ones, similarly, we adopt\ncontrastive learning and deﬁne the loss function of\nRD task as follows:\nL\nT1,T2\nRD = −\n∑\ntA∈T1,tB∈T2\nlog exp(cos(rtA,rtB )/τ)\nZ ,\nZ=\nN∑\ntC∈T/{tA}\nexp(cos(rtA,rtC )/τ),\nLRD = LT+\ns ,T+\ns\nRD + L\nT+s ,T+c\nRD + L\nT+c ,T+s\nRD + L\nT+c ,T+c\nRD ,\n(3)\n4The sampling rate of each relation is proportional to its\ntotal number in the current batch.\nwhere N is a hyper-parameter. We ensure tB is\nsampled in Zand construct N −1 negative exam-\nples by sampling tC (rA ̸= rC) from T, instead\nof T+5. By additionally considering the last three\nterms of LRD in Eq.3, which require the model to\ndistinguish complex inter-sentence relations with\nother relations in the text, our model could have bet-\nter coverage and generality of the reasoning chains.\nPLMs are trained to perform reasoning in an im-\nplicit way to understand those “hard” inter-sentence\ncases.\n3.5 Overall Objective\nNow we present the overall training objective\nof ERICA. To avoid catastrophic forgetting (Mc-\nCloskey and Cohen, 1989) of general language\nunderstanding ability, we train masked language\nmodeling task (LMLM) together with ED and RD\ntasks. Hence, the overall learning objective is for-\nmulated as follows:\nL= LED + LRD + LMLM. (4)\nIt is worth mentioning that we also try to mask\nentities as suggested by Soares et al. (2019) and\nPeng et al. (2020), aiming to avoid simply relearn-\ning an entity linking system. However, we do not\nobserve performance gain by such a masking strat-\negy. We conjecture that in our document-level set-\nting, it is hard for PLMs to overﬁt on memoriz-\ning entity mentions due to the better coverage and\ngenerality of document-level distant supervision.\nBesides, masking entities creates a gap between\npre-training and ﬁne-tuning, which may be a short-\ncoming of previous relation-enhanced PLMs.\n4 Experiments\nIn this section, we ﬁrst describe how we construct\nthe distantly supervised dataset and pre-training\ndetails for ERICA. Then we introduce the experi-\nments we conduct on several language understand-\ning tasks, including relation extraction (RE), en-\ntity typing (ET) and question answering (QA).\nWe test ERICA on two typical PLMs, including\nBERT and RoBERTa (denoted asERICABERT and\nERICARoBERTa)6. We leave the training details\n5In experiments, we ﬁnd introducing no_relation entity\npairs as negative samples further improves the performance\nand the reason is that increasing the diversity of training entity\npairs is beneﬁcial to PLMs.\n6Since our main focus is to demonstrate the superiority\nof ERICA in improving PLMs to capture relational facts and\nadvance further research explorations, we choose base models\n3354\nfor downstream tasks and experiments on GLUE\nbenchmark (Wang et al., 2018) in the appendix.\n4.1 Distantly Supervised Dataset\nConstruction\nFollowing Yao et al. (2019), we construct our pre-\ntraining dataset leveraging distant supervision from\nthe English Wikipedia and Wikidata. First, we\nuse spaCy7 to perform Named Entity Recognition,\nand then link these entity mentions as well as\nWikipedia’s mentions with hyper-links to Wikidata\nitems, thus we obtain the Wikidata ID for each en-\ntity. The relations between different entities are\nannotated distantly by querying Wikidata. We keep\nthe documents containing at least 128 words, 4\nentities and 4 relational triples. In addition, we\nignore those entity pairs appearing in the test sets\nof RE and QA tasks to avoid test set leakage. In\nthe end, we collect 1,000,000 documents (about\n1G storage) in total with more than 4,000 relations\nannotated distantly. On average, each document\ncontains 186.9 tokens, 12.9 entities and 7.2 rela-\ntional triples, an entity appears 1.3 times per docu-\nment. Based on the human evaluation on a random\nsample of the dataset, we ﬁnd that it achieves an F1\nscore of 84.7% for named entity recognition, and\nan F1 score of 25.4% for relation extraction.\n4.2 Pre-training Details\nWe initialize ERICABERT and ERICARoBERTa with\nbert-base-uncased and roberta-base checkpoints\nreleased by Google8 and Huggingface9. We adopt\nAdamW (Loshchilov and Hutter, 2017) as the opti-\nmizer, warm up the learning rate for the ﬁrst 20%\nsteps and then linearly decay it. We set the learning\nrate to 3 ×10−5, weight decay to 1 ×10−5, batch\nsize to 2,048 and temperature τ to 5 ×10−2. For\nLRD, we randomly select up to 64 negative sam-\nples per document. We train both models with 8\nNVIDIA Tesla P40 GPUs for 2,500 steps.\n4.3 Relation Extraction\nRelation extraction aims to extract the relation be-\ntween two recognized entities from a pre-deﬁned\nrelation set. We conduct experiments on both\ndocument-level and sentence-level RE. We test\nfor experiments.\n7https://spacy.io/\n8https://github.com/google-research/bert\n9https://github.com/huggingface/\ntransformers\nSize 1% 10% 100%\nMetrics F1 IgF1 F1 IgF1 F1 IgF1\nCNN - - 42.3 40.3\nBILSTM - - 51.1 50.3\nBERT 30.4 28.9 47.1 44.9 56.8 54.5\nHINBERT - - 55.6 53.7\nCorefBERT 32.8 31.2 46.0 43.7 57.0 54.5\nSpanBERT 32.2 30.4 46.4 44.5 57.3 55.0\nERNIE 26.7 25.5 46.7 44.2 56.6 54.2\nMTB 29.0 27.6 46.1 44.1 56.9 54.3\nCP 30.3 28.7 44.8 42.6 55.2 52.7\nERICABERT 37.8 36.0 50.8 48.3 58.2 55.9\nRoBERTa 35.3 33.5 48.0 45.9 58.5 56.1\nERICARoBERTa 40.1 38.0 50.3 48.3 59.0 56.6\nTable 1: Results on document-level RE (DocRED). We\nreport micro F1 (F1) and micro ignore F1 (IgF1) on test\nset. IgF1 metric ignores the relational facts shared by\nthe train and dev/test sets.\nDataset TACRED SemEval\nSize 1% 10% 100% 1% 10% 100%\nBERT 36.0 58.5 68.1 43.6 79.3 88.1\nMTB 35.7 58.8 68.2 44.2 79.2 88.2\nCP 37.1 60.6 68.1 40.3 80.0 88.5\nERICABERT 36.5 59.7 68.5 47.9 80.1 88.0\nRoBERTa 26.3 61.2 69.7 46.0 80.3 88.8\nERICARoBERTa 40.0 61.9 69.8 46.3 80.4 89.2\nTable 2: Results (test F1) on sentence-level RE (TA-\nCRED and SemEval-2010 Task8) on three splits (1%,\n10% and 100%).\nthree partitions of the training set ( 1%, 10% and\n100%) and report results on test sets.\nDocument-level RE For document-level RE, we\nchoose DocRED (Yao et al., 2019), which requires\nreading multiple sentences in a document and syn-\nthesizing all the information to identify the relation\nbetween two entities. We encode all entities in the\nsame way as in pre-training phase. The relation rep-\nresentations are obtained by adding a bilinear layer\non top of two entity representations. We choose the\nfollowing baselines: (1) CNN (Zeng et al., 2014),\nBILSTM (Hochreiter and Schmidhuber, 1997),\nBERT (Devlin et al., 2018) and RoBERTa (Liu\net al., 2019), which are widely used as text encoders\nfor relation extraction tasks; (2) HINBERT (Tang\net al., 2020) which employs a hierarchical infer-\nence network to leverage the abundant information\nfrom different sources; (3) CorefBERT (Ye et al.,\n2020) which proposes a pre-training method to help\nBERT capture the coreferential relations in context;\n(4) SpanBERT (Joshi et al., 2020) which masks\n3355\nMetrics Macro F1 Micro F1\nBERT 75.50 72.68\nMTB 76.37 72.94\nCP 76.27 72.48\nERNIE 76.51 73.39\nERICABERT 77.85 74.71\nRoBERTa 79.24 76.38\nERICARoBERTa 80.77 77.04\nTable 3: Results on entity typing (FIGER). We report\nmacro F1 and micro F1 on the test set.\nand predicts contiguous random spans instead of\nrandom tokens; (5) ERNIE (Zhang et al., 2019)\nwhich incorporates KG information into BERT to\nenhance entity representations; (6) MTB (Soares\net al., 2019) and CP (Peng et al., 2020) which in-\ntroduce sentence-level relation contrastive learning\nfor BERT via distant supervision. For fair compari-\nson, we pre-train these baselines on our constructed\npre-training data10 based on the implementation re-\nleased by Peng et al. (2020) 11. From the results\nshown in Table 1, we can see that: (1) ERICA\noutperforms all baselines signiﬁcantly on each su-\npervised data size, which demonstrates that ER-\nICA could better understand the relations among\nentities in the document via implicitly considering\ntheir complex reasoning patterns in the pre-training;\n(2) both MTB and CP achieve worse results than\nBERT, which means sentence-level pre-training,\nlacking consideration for complex reasoning pat-\nterns, hurts PLM’s performance on document-level\nRE tasks to some extent; (3) ERICA outperforms\nbaselines by a larger margin on smaller training\nsets, which means ERICA has gained pretty good\ndocument-level relation reasoning ability in con-\ntrastive learning, and thus obtains improvements\nmore extensively under low-resource settings.\nSentence-level RE For sentence-level RE, we\nchoose two widely used datasets: TACRED (Zhang\net al., 2017) and SemEval-2010 Task 8 (Hendrickx\net al., 2019). We insert extra marker tokens to in-\ndicate the head and tail entities in each sentence.\nFor baselines, we compare ERICA with BERT,\nRoBERTa, MTB and CP. From the results shown\nin Table 2, we observe that ERICA achieves almost\ncomparable results on sentence-level RE tasks with\nCP, which means document-level pre-training in\n10In practice, documents are split into sentences and we\nonly keep within-sentence entity pairs.\n11https://github.com/thunlp/\nRE-Context-or-Names\nSetting Standard Masked\nSize 1% 10% 100% 1% 10% 100%\nFastQA - 27.2 - 38.0\nBiDAF - 49.7 - 59.8\nBERT 35.8 53.7 69.5 37.9 53.1 73.1\nCorefBERT 38.1 54.4 68.8 39.0 53.5 70.7\nSpanBERT 33.1 56.4 70.7 34.0 55.4 73.2\nMTB 36.6 51.7 68.4 36.2 50.9 71.7\nCP 34.6 50.4 67.4 34.1 47.1 69.4\nERICABERT 46.5 57.8 69.7 40.2 58.1 73.9\nRoBERTa 37.3 57.4 70.9 41.2 58.7 75.5\nERICARoBERTa 47.4 58.8 71.2 46.8 63.4 76.6\nTable 4: Results (accuracy) on the dev set of WikiHop.\nWe test both the standard and masked settings on three\nsplits (1%, 10% and 100%).\nSetting SQuAD TriviaQA NaturalQA\nSize 10% 100% 10% 100% 10% 100%\nBERT 79.7 88.9 60.8 70.7 68.4 78.4\nMTB 63.5 87.1 52.0 67.8 61.2 76.7\nCP 69.0 87.1 52.9 68.1 63.3 77.3\nERICABERT 81.8 88.9 63.5 71.9 70.2 79.1\nRoBERTa 82.9 90.5 63.6 72.0 71.8 80.0\nERICARoBERTa 85.0 90.4 63.6 72.1 73.7 80.5\nTable 5: Results (F1) on extractive QA (SQuAD, Triv-\niaQA and NaturalQA) on two splits (10% and 100%).\nResults on 1% split are left in the appendix.\nERICA does not impair PLMs’ performance on\nsentence-level relation understanding.\n4.4 Entity Typing\nEntity typing aims at classifying entity men-\ntions into pre-deﬁned entity types. We choose\nFIGER (Ling et al., 2015), which is a sentence-\nlevel entity typing dataset labeled with distant\nsupervision. BERT, RoBERTa, MTB, CP and\nERNIE are chosen as baselines. From the results\nlisted in Table 3, we observe that, ERICA outper-\nforms all baselines, which demonstrates that ER-\nICA could better represent entities and distinguish\nthem in text via both entity-level and relation-level\ncontrastive learning.\n4.5 Question Answering\nQuestion answering aims to extract a speciﬁc an-\nswer span in text given a question. We conduct\nexperiments on both multi-choice and extractive\nQA. We test multiple partitions of the training set.\nMulti-choice QA For Multi-choice QA, we\nchoose WikiHop (Welbl et al., 2018), which re-\nquires models to answer speciﬁc properties of an\n3356\nentity after reading multiple documents and con-\nducting multi-hop reasoning. It has both stan-\ndard and masked settings, where the latter setting\nmasks all entities with random IDs to avoid in-\nformation leakage. We ﬁrst concatenate the ques-\ntion and documents into a long sequence, then\nwe ﬁnd all the occurrences of an entity in the\ndocuments, encode them into hidden representa-\ntions and obtain the global entity representation\nby applying mean pooling on these hidden rep-\nresentations. Finally, we use a classiﬁer on top\nof the entity representation for prediction. We\nchoose the following baselines: (1) FastQA (Weis-\nsenborn et al., 2017) and BiDAF (Seo et al., 2016),\nwhich are widely used question answering systems;\n(2) BERT, RoBERTa, CorefBERT, SpanBERT,\nMTB and CP, which are introduced in previous\nsections. From the results listed in Table 4, we ob-\nserve that ERICA outperforms baselines in both set-\ntings, indicating that ERICA can better understand\nentities and their relations in the documents and\nextract the true answer according to queries. The\nsigniﬁcant improvements in the masked setting also\nindicate that ERICA can better perform multi-hop\nreasoning to synthesize and analyze information\nfrom contexts, instead of relying on entity mention\n“shortcuts” (Jiang and Bansal, 2019).\nExtractive QA For extractive QA, we adopt\nthree widely-used datasets: SQuAD (Rajpurkar\net al., 2016), TriviaQA (Joshi et al., 2017) and Natu-\nralQA (Kwiatkowski et al., 2019) in MRQA (Fisch\net al., 2019) to evaluate ERICA in various domains.\nSince MRQA does not provide the test set for each\ndataset, we randomly split the original dev set into\ntwo halves and obtain the new dev/test set. We fol-\nlow the QA setting of BERT (Devlin et al., 2018):\nwe concatenate the given question and passage into\none long sequence, encode the sequence by PLMs\nand adopt two classiﬁers to predict the start and end\nindex of the answer. We chooseBERT, RoBERTa,\nMTB and CP as baselines. From the results listed\nin Table 5, we observe that ERICA outperforms\nall baselines, indicating that through the enhance-\nment of entity and relation understanding, ERICA\nis more capable of capturing in-text relational facts\nand synthesizing information of entities. This abil-\nity further improves PLMs for question answering.\n5 Analysis\nIn this section, we ﬁrst conduct a suite of ablation\nstudies to explore how LED and LRD contribute to\nDataset DocRED FIGER WikiHop\nBERT 44.9 72.7 53.1\n-NSP 45.2 72.6 53.6\n-NSP+LED 47.6 73.8 59.8\n-NSP+L\nT+\nc ,T+\nc\nRD 46.4 72.6 52.2\n-NSP+L\nT+\ns,T+\ns\nRD 47.3 73.5 51.2\n-NSP+LRD 48.0 74.0 52.0\nERICABERT 48.3 74.7 58.1\nTable 6: Ablation study. We report test IgF1 on Do-\ncRED (10%), test micro F1 on FIGER and dev accu-\nracy on the masked setting of WikiHop (10%).\nERICA. Then we give a thorough analysis on how\npre-training data’s domain / size and methods for\nentity encoding impact the performance. Lastly,\nwe visualize the entity and relation embeddings\nlearned by ERICA.\n5.1 Ablation Study\nTo demonstrate that the superior performance of\nERICA is not owing to its longer pretraining (2500\nsteps) on masked language modeling, we include\na baseline by optimizing LMLM only (removing\nthe Next Sentence Prediction (-NSP) loss (Devlin\net al., 2018)). In addition, to explore how LED and\nLRD impact the performance, we keep only one of\nthese two losses and compare the results. Lastly,\nto evaluate how intra-sentence and inter-sentence\nentity pairs contribute to RD task, we compare\nthe performances of only sampling intra-sentence\nentity pairs (L\nT+s ,T+s\nRD ) or inter-sentence entity pairs\n(L\nT+c ,T+c\nRD ), and sampling both of them (LRD) during\npre-training. We conduct experiments on DocRED,\nWikiHop (masked version) and FIGER. For Do-\ncRED and WikiHop, we show the results on 10%\nsplits and the full results are left in the appendix.\nFrom the results shown in Table 6, we can see\nthat: (1) extra pretraining (-NSP) only contributes a\nlittle to the overall improvement. (2) For DocRED\nand FIGER, either LED or LRD is beneﬁcial, and\ncombining them further improves the performance;\nFor WikiHop, LED dominates the improvement\nwhile LRD hurts the performance slightly, this is\npossibly because question answering more resem-\nbles the tail entity discrimination process, while\nthe relation discrimination process may have con-\nﬂicts with it. (3) For LRD, both intra-sentence and\ninter-sentence entity pairs contribute, which demon-\nstrates that incorporating both of them is necessary\nfor PLMs to understand relations between entities\nin text comprehensively. We also found empiri-\n3357\nSize 1% 10% 100%\nBERT 28.9 44.9 54.5\nERICABERT 36.0 48.3 55.9\nERICADocRED\nBERT 36.3 48.6 55.9\nTable 7: Effects of pre-training data’s entity distribu-\ntion shifting. We report test IgF1 on DocRED.\n0% 30% 50% 70% 100%\n1% DocRED\n30\n32\n34\n36 \n0% 30% 50% 70% 100%\n10% DocRED\n45\n46\n47\n48 \n0% 30% 50% 70% 100%\n100% DocRED\n54.5\n55.0\n55.5 \nFigure 4: Impacts of relation distribution shifting. X\naxis denotes different ratios of relations, Y axis denotes\ntest IgF1 on different partitions of DocRED.\ncally that when these two auxiliary objectives are\nonly added into the ﬁne-tuning stage, the model\ndoes not have performance gain. The reason is that\nthe size and diversity of entities and relations in\ndownstream training data are limited. Instead, pre-\ntraining with distant supervision on a large corpus\nprovides a solution for increasing the diversity and\nquantity of training examples.\n5.2 Effects of Domain Shifting\nWe investigate two domain shifting factors: entity\ndistribution and relation distribution, to explore\nhow they impact ERICA’s performance.\nEntity Distribution Shifting The entities in su-\npervised datasets of DocRED are recognized by\nhuman annotators while our pre-training data is\nprocessed by spaCy. Hence there may exist an en-\ntity distribution gap between pre-training and ﬁne-\ntuning. To study the impacts of entity distribution\nshifting, we ﬁne-tune a BERT model on training\nset of DocRED for NER tagging and re-tag enti-\nties in our pre-training dataset. Then we pre-train\nERICA on the newly-labeled training corpus (de-\nnoted as ERICADocRED\nBERT ). From the results shown in\nTable 7, we observe that it performs better than the\noriginal ERICA, indicating that pre-training on a\ndataset that shares similar entity distributions with\ndownstream tasks is beneﬁcial.\nRelation Distribution Shifting Our pre-training\ndata contains over 4,000 Wikidata relations. To\ninvestigate whether training on a more diverse rela-\ntion domain beneﬁts ERICA, we train it with the\npre-training corpus that randomly keeps only 30%,\n50% and 70% the original relations, and compare\n0% 10% 30% 50% 70%100%\n1% DocRED\n30\n32\n34\n36 \n0% 10% 30% 50% 70%100%\n10% DocRED\n45\n46\n47\n48 \n0% 10% 30% 50% 70%100%\n100% DocRED\n54.5\n55.0\n55.5 \nFigure 5: Impacts of pre-training data’s size. X axis\ndenotes different ratios of pre-training data, Y axis de-\nnotes test IgF1 on different partitions of DocRED.\nSize 1% 10% 100%\nMetrics F1 IgF1 F1 IgF1 F1 IgF1\nMean Pool\nBERT 30.4 28.9 47.1 44.9 56.8 54.5\nERICABERT 37.8 36.0 50.8 48.3 58.2 55.9\nERICADocRED\nBERT 38.5 36.3 51.0 48.6 58.2 55.9\nEntity Marker\nBERT 23.0 21.8 46.5 44.3 58.0 55.6\nERICABERT 34.9 33.0 50.2 48.0 59.9 57.6\nERICADocRED\nBERT 36.9 34.8 52.5 50.3 60.8 58.4\nTable 8: Results (IgF1) on how entity encoding strat-\negy inﬂuences ERICA’s performance on DocRED. We\nalso show the impacts of entity distribution shifting\n(ERICADocRED\nBERT and ERICADocRED\nBERT ) as is mentioned in the\nmain paper.\ntheir performances. From the results in Figure 4,\nwe observe that the performance of ERICA im-\nproves constantly as the diversity of relation do-\nmain increases, which reveals the importance of\nusing diverse training data on relation-related tasks.\nThrough detailed analysis, we further ﬁnd that ER-\nICA is less competent at handling unseen relations\nin the corpus. This may result from the construc-\ntion of our pre-training dataset: all the relations are\nannotated distantly through an existing KG with\na pre-deﬁned relation set. It would be promising\nto introduce more diverse relation domains during\ndata preparation in future.\n5.3 Effects of Pre-training Data’s Size\nTo explore the effects of pre-training data’s size, we\ntrain ERICA on 10%, 30%, 50% and 70% of the\noriginal pre-training dataset, respectively. We re-\nport the results in Figure 5, from which we observe\nthat with the scale of pre-training data becoming\nlarger, ERICA is performing better.\n5.4 Effects of Methods for Entity Encoding\nFor all the experiments mentioned above, we en-\ncode each occurrence of an entity by mean pooling\nover all its tokens in both pre-training and down-\nstream tasks. Ideally, ERICA should have consis-\n3358\ntent improvements on other kinds of methods for\nentity encoding. To demonstrate this, we try an-\nother entity encoding method mentioned by Soares\net al. (2019) on three splits of DocRED (1%, 10%\nand 100%). Speciﬁcally, we insert a special start\ntoken [S] in front of an entity and an end token\n[E] after it. The representation for this entity is\ncalculated by averaging the representations of all\nits start tokens in the document. To help PLMs\ndiscriminate different entities, we randomly assign\ndifferent marker pairs ([S1], [E1]; [S2], [E2], ...)\nfor each entity in a document in both pre-training\nand downstream tasks12. All occurrences of one\nentity in a document share the same marker pair.\nWe show in Table 8 that ERICA achieves consistent\nperformance improvements for both methods (de-\nnoted as Mean Pooland Entity Marker), indicat-\ning that ERICA is applicable to different methods\nfor entity encoding. Speciﬁcally, Entity Marker\nachieves better performance when the scale of train-\ning data is large whileMean Poolis more powerful\nunder low-resource settings. We also notice that\ntraining on a dataset that shares similar entity dis-\ntributions is more helpful for Mean Pool, where\nERICADocRED\nBERT achieves 60.8 (F1) and 58.4 (IgF1)\non 100% training data.\n5.5 Embedding Visualization\nIn Figure 6, we show the learned entity and re-\nlation embeddings of BERT and ERICABERT on\nDocRED’s dev set by t-distributed stochastic neigh-\nbor embedding (t-SNE) (Hinton and Roweis, 2002).\nWe label points with different colors to represent\nits corresponding category of entities or relations13\nin Wikidata and only visualize the most frequent10\nrelations. From the ﬁgure, we can see that jointly\ntraining LMLM with LED and LRD leads to a more\ncompact clustering of both entities and relations\nbelonging to the same category. In contrast, only\ntraining LMLM exhibits random distribution. This\nveriﬁes that ERICA could better understand and\nrepresent both entities and relations in the text.\n12In practice, we randomly initialize 100 entity marker\npairs.\n13(Key, value) pairs for relations deﬁned in Wikidata are:\n(P176, manufacturer); (P150, contains administrative territo-\nrial entity); (P17, country); (P131, located in the administra-\ntive territorial entity); (P175, performer); (P27, country of\ncitizenship); (P569, date of birth); (P1001, applies to jurisdic-\ntion); (P57, director); (P179, part of the series).\nBERT: entity\n \nentity\nMISC\nORG\nPER\nLOC\nTIME\nNUM\nERICA-BERT: entity\n \nentity\nTIME\nMISC\nORG\nLOC\nPER\nNUM\nBERT: relation\n \nrelation\nP17\nP131\nP1001\nP27\nP150\nP175\nP179\nP57\nP176\nP569\nERICA-BERT: relation\n \nrelation\nP176\nP150\nP17\nP131\nP175\nP27\nP569\nP1001\nP57\nP179\nFigure 6: t-SNE plots of learned entity and rela-\ntion embeddings on DocRED comparing BERT and\nERICABERT.\n6 Conclusions\nIn this paper, we present ERICA, a general frame-\nwork for PLMs to improve entity and relation un-\nderstanding via contrastive learning. We demon-\nstrate the effectiveness of our method on several\nlanguage understanding tasks, including relation\nextraction, entity typing and question answering.\nThe experimental results show that ERICA outper-\nforms all baselines, especially under low-resource\nsettings, which means ERICA helps PLMs better\ncapture the in-text relational facts and synthesize\ninformation about entities and their relations.\nAcknowledgments\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106501) and Beijing Academy of Arti-\nﬁcial Intelligence (BAAI). This work is also sup-\nported by the Pattern Recognition Center, WeChat\nAI, Tencent Inc.\nReferences\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\n3359\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nMarkus Eberts and Adrian Ulges. 2019. Span-based\njoint entity and relation extraction with transformer\npre-training. CoRR, abs/1909.07755.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv\npreprint arXiv:2101.03961.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In Proceedings of the 2nd Work-\nshop on Machine Reading for Question Answering ,\npages 1–13, Hong Kong, China. Association for\nComputational Linguistics.\nHarsha Gurulingappa, Abdul Mateen Rajput, Angus\nRoberts, Juliane Fluck, Martin Hofmann-Apitius,\nand Luca Toldo. 2012. Development of a bench-\nmark corpus to support the automatic extraction of\ndrug-related adverse effects from medical case re-\nports. Journal of biomedical informatics, 45(5):885–\n892.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR’06), volume 2, pages 1735–1742. IEEE.\nBin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu,\nNicholas Jing Yuan, and Tong Xu. 2020. BERT-\nMK: Integrating graph contextualized knowledge\ninto pre-trained language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 2281–2290, Online. Association for\nComputational Linguistics.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid O Séaghdha, Sebastian\nPadó, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2019. SemEval-2010 Task 8:\nMulti-way classiﬁcation of semantic relations be-\ntween pairs of nominals. In Proceedings of the\nWorkshop on Semantic Evaluations: Recent Achieve-\nments and Future Directions (SEW-2009), pages 94–\n99.\nGeoffrey E Hinton and Sam Roweis. 2002. Stochas-\ntic neighbor embedding. In Advances in neural in-\nformation processing systems 15: 16th Annual Con-\nference on Neural Information Processing Systems\n2002. Proceedings of a meeting held September 12,\n2002, Vancouver, British Columbia, Canada , vol-\nume 15, pages 857–864.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nYichen Jiang and Mohit Bansal. 2019. Avoiding rea-\nsoning shortcuts: Adversarial evaluation, training,\nand model development for multi-hop qa. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2019, July\n28, 2019, Florence, Italy, pages 2726–2736. Associ-\nation for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans. Transactions of the Associa-\ntion for Computational Linguistics, 8:64–77.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 1601–1611, Van-\ncouver, Canada. Association for Computational Lin-\nguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7, 2015, Con-\nference Track Proceedings.\nLingpeng Kong, Cyprien de Masson d’Autume, Lei Yu,\nWang Ling, Zihang Dai, and Dani Yogatama. 2020.\nA mutual information maximization perspective of\nlanguage representation learning. In Proceedings\nof 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Virtual Conference, April 26,\n2020, Conference Track Proceedings.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proceed-\nings of 8th International Conference on Learning\nRepresentations, ICLR 2020, Virtual Conference,\nApril 26, 2020, Conference Track Proceedings.\nXiao Ling, Sameer Singh, and Daniel S Weld. 2015.\nDesign challenges for entity linking. Transactions\nof the Association for Computational Linguistics ,\n3:315–328.\n3360\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization. In Proceedings of 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: the\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109–165. El-\nsevier.\nHao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng\nLi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2020.\nLearning from context or names? an empirical study\non neural relation extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3661–3672,\nOnline. Association for Computational Linguistics.\nMatthew E Peters, Mark Neumann, Robert L Logan IV ,\nRoy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP). Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Journal of Machine Learning Research ,\n21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nDan Roth and Wen-tau Yih. 2004. A linear program-\nming formulation for global inference in natural lan-\nguage tasks. In Proceedings of the Eighth Confer-\nence on Computational Natural Language Learn-\ning (CoNLL-2004) at HLT-NAACL 2004, pages 1–8,\nBoston, Massachusetts, USA. Association for Com-\nputational Linguistics.\nErik F Sang and Fien De Meulder. 2003. Intro-\nduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2016. Bidirectional attention\nﬂow for machine comprehension. In Proceedings of\n5th International Conference on Learning Represen-\ntations, ICLR 2017, Toulon, France, April 24, 2017,\nCon- ference Track Proceedings.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905. Association for Computational\nLinguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. In Proceed-\nings of International Conference on Machine Learn-\ning, pages 5926–5936. PMLR.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nCoLAKE: Contextualized language and knowledge\nembedding. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 3660–3670, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nAlon Talmor and Jonathan Berant. 2019. MultiQA: An\nempirical investigation of generalization and trans-\nfer in reading comprehension. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4911–4921. Association\nfor Computational Linguistics.\nHengzhu Tang, Yanan Cao, Zhenyu Zhang, Jiangxia\nCao, Fang Fang, Shi Wang, and Pengfei Yin. 2020.\nHin: Hierarchical inference network for document-\nlevel relation extraction. In Advances in Knowledge\nDiscovery and Data Mining-24th Paciﬁc-Asia Con-\nference, PAKDD 2020, Singapore, May 11, 2020,\nProceedings, Part I, volume 12084 of Lecture Notes\nin Computer Science, pages 197–209. Springer.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4 December\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\n3361\nworks for NLP1. Association for Computational Lin-\nguistics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. KEPLER: A\nuniﬁed model for knowledge embedding and pre-\ntrained language representation. Transactions of the\nAssociation for Computational Linguistics.\nDirk Weissenborn, Georg Wiese, and Laura Seiffe.\n2017. Making neural QA as simple as possible\nbut not simpler. In Proceedings of the 21st Con-\nference on Computational Natural Language Learn-\ning (CoNLL 2017), pages 271–280. Association for\nComputational Linguistics.\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:287–302.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of 8th International Confer-\nence on Learning Representations, ICLR 2020, Vir-\ntual Conference, April 26, 2020, Conference Track\nProceedings.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) . Association for Com-\nputational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, 8-14 December 2019, Vancou-\nver, BC, Canada.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019. DocRED: A large-scale\ndocument-level relation extraction dataset. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 764–777.\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu,\nMaosong Sun, and Zhiyuan Liu. 2020. Coreferen-\ntial reasoning learning for language representation.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7170–7186. Association for Computational\nLinguistics.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classiﬁcation via con-\nvolutional deep neural network. In Proceedings of\nCOLING 2014, the 25th International Conference\non Computational Linguistics: Technical Papers ,\npages 2335–2344. Dublin City University and Asso-\nciation for Computational Linguistics.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor An-\ngeli, and Christopher D Manning. 2017. Position-\naware attention and supervised data improve slot ﬁll-\ning. In Proceedings of the 2017 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 35–45. Association for Computational Lin-\nguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451. Association for Computa-\ntional Linguistics.\n3362\nAppendices\nA Training Details for Downstream\nTasks\nIn this section, we introduce the training details for\ndownstream tasks (relation extraction, entity typing\nand question answering). We implement all models\nbased on Huggingface transformers14.\nA.1 Relation Extraction\nDocument-level Relation Extraction For\ndocument-level relation extraction, we did ex-\nperiments on DocRED (Yao et al., 2019). We\nmodify the ofﬁcial code15 for implementation. For\nexperiments on three partitions of the original\ntraining set ( 1%, 10% and 100%), we adopt\nbatch size of 10,32,32 and training epochs of\n400,400,200, respectively. We choose Adam\noptimizer (Kingma and Ba, 2014) as the optimizer\nand the learning rate is set to 4 ×10−5. We\nevaluate on dev set every20/20/5 epochs and then\ntest the best checkpoint on test set on the ofﬁcial\nevaluation server16.\nSentence-level Relation Extraction For\nsentence-level relation extraction, we did ex-\nperiments on TACRED (Zhang et al., 2017)\nand SemEval-2010 Task 8 (Hendrickx et al.,\n2019) based on the implementation of Peng et al.\n(2020)17. We did experiments on three partitions\n(1%, 10% and 100%) of the original training set.\nThe relation representation for each entity pair is\nobtained in the same way as in pre-training phase.\nOther settings are kept the same as Peng et al.\n(2020) for fair comparison.\nA.2 Entity Tying\nFor entity typing, we choose FIGER (Ling et al.,\n2015), whose training set is labeled with distant\nsupervision. We modify the implementation of\nERNIE (Zhang et al., 2019) 18. In ﬁne-tuning\nphrase, we encode the entities in the same way\nas in pre-training phase. We set the learning rate to\n3 ×10−5 and batch size to 256, and ﬁne-tune the\n14https://github.com/huggingface/\ntransformers\n15https://github.com/thunlp/DocRED\n16https://competitions.codalab.org/\ncompetitions/20717\n17https://github.com/thunlp/\nRE-Context-or-Names\n18https://github.com/thunlp/ERNIE\nmodels for three epochs, other hyper-parameters\nare kept the same as ERNIE.\nA.3 Question Answering\nMulti-choice QA For multi-choice question an-\nswering, we choose WikiHop (Welbl et al., 2018).\nSince the standard setting of WikiHop does not\nprovide the index for each candidate, we then ﬁnd\nthem by exactly matching them in the documents.\nWe did experiments on three partitions of the origi-\nnal training data (1%, 10% and 100%). We set the\nbatch size to 8 and learning rate to 5 ×10−5, and\ntrain for two epochs.\nExtractive QA For extractive question answer-\ning, we adopt MRQA (Fisch et al., 2019) as the\ntestbed and choose three datasets: SQuAD (Ra-\njpurkar et al., 2016), TriviaQA (Joshi et al., 2017)\nand NaturalQA (Kwiatkowski et al., 2019). We\nadopt Adam as the optimizer, set the learning rate\nto 3 ×10−5 and train for two epochs. In the main\npaper, we report results on two splits (10% and\n100%) and results on 1% are listed in Table 11.\nB Generalized Language Understanding\n(GLUE)\nThe General Language Understanding Evalua-\ntion (GLUE) benchmark (Wang et al., 2018) pro-\nvides several natural language understanding tasks,\nwhich is often used to evaluate PLMs. To test\nwhether LED and LRD impair the PLMs’ per-\nformance on these tasks, we compare BERT,\nERICABERT, RoBERTa andERICARoBERTa. We fol-\nlow the widely used setting and use the [CLS] to-\nken as representation for the whole sentence or\nsentence pair for classiﬁcation or regression. Ta-\nble 9 shows the results on dev sets of GLUE Bench-\nmark. It can be observed that both ERICABERT and\nERICARoBERTa achieve comparable performance\nthan the original model, which suggests that jointly\ntraining LED and LRD with LMLM does not hurt\nPLMs’ general ability of language understanding.\nC Full results of ablation study\nFull results of ablation study (DocRED, WikiHop\nand FIGER) are listed in Table 10.\nD Joint Named Entity Recognition and\nRelation Extraction\nJoint Named Entity Recognition (NER) and Re-\nlation Extraction (RE) aims at identifying enti-\nties in text and the relations between them. We\n3363\nDataset MNLI(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE\nBERT 84.0/84.4 88.9 90.6 92.4 57.2 89.7 89.4 70.1\nERICABERT 84.5/84.7 88.3 90.7 92.8 57.9 89.5 89.5 69.6\nRoBERTa 87.5/87.3 91.9 92.8 94.8 63.6 91.2 90.2 78.7\nERICARoBERTa 87.5/87.5 91.6 92.6 95.0 63.5 90.7 91.5 78.5\nTable 9: Results on dev sets of GLUE Benchmark. We report matched/mismatched (m/mm) accuracy for MNLI,\nF1 score for QQP and MRPC, spearman correlation for STS-B and accuracy for other tasks.\nDataset DocRED WikiHop (m) FIGER\nSize 1% 10% 100% 1% 10% 100% 100%\nBERT 28.9 44.9 54.5 37.9 53.1 73.1 72.7\n-NSP 30.1 45.2 54.6 38.2 53.6 73.3 72.6\n-NSP+LED 34.4 47.6 55.8 41.1 59.8 74.8 73.8\n-NSP+L\nT+\nc ,T+\nc\nRD 34.8 46.4 54.7 37.4 52.2 72.8 72.6\n-NSP+L\nT+\ns,T+\ns\nRD 33.9 47.3 55.5 38.0 51.2 72.5 73.5\n-NSP+LRD 35.9 48.0 55.6 37.2 52.0 72.7 74.0\nERICABERT 36.0 48.3 55.9 40.2 58.1 73.9 74.7\nTable 10: Full results of ablation study. We report test IgF1 on DocRED, dev accuracy on the masked (m) setting\nof WikiHop and test micro F1 on FIGER.\nSetting SQuAD TriviaQA NaturalQA\nBERT 15.8 28.7 31.5\nMTB 11.2 22.0 28.4\nCP 12.5 25.6 29.4\nERICABERT 51.3 51.4 42.9\nRoBERTa 22.1 40.6 34.0\nERICARoBERTa 57.6 51.3 57.6\nTable 11: Results (F1) on extractive QA (SQuAD, Triv-\niaQA and NaturalQA) on 1% split.\nModel\nCoNLL04 ADE\nNER RE NER RE\nBERT 88.5 70.3 89.2 79.2\nERICABERT 89.3 71.5 89.5 80.2\nRoBERTa 89.8 72.0 89.7 81.6\nERICARoBERTa 90.0 72.8 90.2 82.4\nTable 12: Results (F1) on joint NER&RE.\nadopt SpERT (Eberts and Ulges, 2019) as the base\nmodel and conduct experiments on two datasets:\nCoNLL04 (Roth and Yih, 2004) and ADE (Gu-\nrulingappa et al., 2012) by replacing the base en-\ncoders (BERT and RoBERTa) withERICABERT and\nERICARoBERTa, respectively. We modify the imple-\nmentation of SpERT19 and keep all the settings the\nsame. From the results listed in Table 12, we can\nsee that ERICA outperforms all baselines, which\nagain demonstrates the superiority of ERICA in\n19https://github.com/markus-eberts/spert\nhelping PLMs better understand and represent both\nentities and relations in text.",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.7342444062232971
    },
    {
      "name": "Computer science",
      "score": 0.6956079006195068
    },
    {
      "name": "Relation (database)",
      "score": 0.6224306225776672
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6102579832077026
    },
    {
      "name": "Computational linguistics",
      "score": 0.5769158601760864
    },
    {
      "name": "Association (psychology)",
      "score": 0.5286657810211182
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5260381698608398
    },
    {
      "name": "Linguistics",
      "score": 0.51523357629776
    },
    {
      "name": "Joint (building)",
      "score": 0.47206738591194153
    },
    {
      "name": "Engineering",
      "score": 0.140936940908432
    },
    {
      "name": "Data mining",
      "score": 0.10064965486526489
    },
    {
      "name": "Philosophy",
      "score": 0.09748324751853943
    },
    {
      "name": "Epistemology",
      "score": 0.057186663150787354
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 89
}