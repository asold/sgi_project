{
  "title": "Cross-Aware Early Fusion With Stage-Divided Vision and Language Transformer Encoders for Referring Image Segmentation",
  "url": "https://openalex.org/W4389371459",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Cho, Yubin",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2753354121",
      "name": "Yu Hyunwoo",
      "affiliations": [
        "Sogang University"
      ]
    },
    {
      "id": "https://openalex.org/A2283612873",
      "name": "Kang Suk-Ju",
      "affiliations": [
        "Sogang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201770677",
    "https://openalex.org/W3216551675",
    "https://openalex.org/W4200631575",
    "https://openalex.org/W4283029876",
    "https://openalex.org/W2302548814",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W2973233205",
    "https://openalex.org/W3023463084",
    "https://openalex.org/W3003423830",
    "https://openalex.org/W3156800342",
    "https://openalex.org/W4318954130",
    "https://openalex.org/W2899002405",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2999725795",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W6842806116",
    "https://openalex.org/W4382449692",
    "https://openalex.org/W4312877428",
    "https://openalex.org/W4312543911",
    "https://openalex.org/W3172522282",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6790019176",
    "https://openalex.org/W6798805250",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W2798556392",
    "https://openalex.org/W3034325957",
    "https://openalex.org/W4224988000",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W4309181071",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3034692043",
    "https://openalex.org/W3108748824",
    "https://openalex.org/W3035097537",
    "https://openalex.org/W3169998662",
    "https://openalex.org/W3187664142",
    "https://openalex.org/W2489434015",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W4386076034",
    "https://openalex.org/W3099166112"
  ],
  "abstract": "Referring segmentation aims to segment a target object related to a natural language expression. Key challenges of this task are understanding the meaning of complex and ambiguous language expressions and determining the relevant regions in the image with multiple objects by referring to the expression. Recent models have focused on the early fusion with the language features at the intermediate stage of the vision encoder, but these approaches have a limitation that the language features cannot refer to the visual information. To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision and Language Transformer encoders (CrossVLT), which allows both language and vision encoders to perform the early fusion for improving the ability of the cross-modal context modeling. Unlike previous methods, our method enables the vision and language features to refer to each other's information at each stage to mutually enhance the robustness of both encoders. Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment. By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion. In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks.",
  "full_text": "IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024 1\nCross-aware Early Fusion with Stage-divided Vision\nand Language Transformer Encoders for Referring\nImage Segmentation\nYubin Cho, Hyunwoo Yu, and Suk-Ju Kang, Member, IEEE\nAbstract—Referring segmentation aims to segment a target\nobject related to a natural language expression. Key challenges\nof this task are understanding the meaning of complex and\nambiguous language expressions and determining the relevant\nregions in the image with multiple objects by referring to the\nexpression. Recent models have focused on the early fusion\nwith the language features at the intermediate stage of the\nvision encoder, but these approaches have a limitation that the\nlanguage features cannot refer to the visual information. To\naddress this issue, this paper proposes a novel architecture,\nCross-aware early fusion with stage-divided Vision and Language\nTransformer encoders (CrossVLT), which allows both language\nand vision encoders to perform the early fusion for improving\nthe ability of the cross-modal context modeling. Unlike previous\nmethods, our method enables the vision and language features\nto refer to each other’s information at each stage to mutually\nenhance the robustness of both encoders. Furthermore, unlike the\nconventional scheme that relies solely on the high-level features\nfor the cross-modal alignment, we introduce a feature-based\nalignment scheme that enables the low-level to high-level features\nof the vision and language encoders to engage in the cross-modal\nalignment. By aligning the intermediate cross-modal features in\nall encoder stages, this scheme leads to effective cross-modal\nfusion. In this way, the proposed approach is simple but effective\nfor referring image segmentation, and it outperforms the previous\nstate-of-the-art methods on three public benchmarks.\nIndex Terms —Referring image segmentation, cross-aware\nearly fusion, feature-based cross-modal alignment\nI. I NTRODUCTION\nR\nEFERRING image segmentation [5]–[12] is one of the\nmost fundamental and challenging vision-language tasks\nto highlight regions corresponding to the language description\nfor the properties of the target object. It can be applied in var-\nious applications such as human-robot interaction and image\nediting. Unlike the conventional single-modal segmentation\n(e.g., instance or semantic segmentation [13]–[18]) based\non fixed category conditions, referring image segmentation\naims to determine the region according to various linguistic\nexpressions that are not predetermined. In this task, the images\nwith dense objects have complicated relationships between\nthe target object and other objects. In addition, linguistic\nexpressions are ambiguous because the linguistic meaning\nYubin Cho and Hyunwoo Yu contributed equally to this work. (Correspond-\ning author: Suk-Ju Kang.)\nYubin Cho is with the School of Artificial Intelligence, Sogang University,\nSeoul, 04017, Republic of Korea (e-mail: dbqls1219@sogang.ac.kr).\nHyunwoo Yu, and Suk-Ju Kang are with the School of Electronic En-\ngineering, Sogang University, Seoul, 04017, Republic of Korea (e-mail:\nhyunwoo137@sogang.ac.kr; sjkang@sogang.ac.kr).\nFig. 1: Architectures of various fusion approaches for referring\nimage segmentation. (a) Late fusion approach (e.g., VLT [1],\nCRIS [2]) that fuses in the transformer decoder after the en-\ncoder feature extraction. (b) Previous early fusion approaches\n(e.g., LA VT [3], PLV [4]) that unidirectionally refer to the\nlanguage features in the vision encoder. (c) Our CrossVLT\nthat bidirectionally performs cross-aware early fusion at each\nstage to interconnect both encoders for mutual enhancement.\ncan be interpreted in various ways depending on the per-\nspectives. Thus, the context of the image should be taken\ninto account to interpret the linguistic expression with the\nappropriate meaning. To resolve these problems, we address\ntwo main challenges in the referring image segmentation task.\nOne is regarding the fusion that refers to the cross-modal\ninformation from each modality features. The other challenge\nis the alignment that embeds the vision and language features\narXiv:2408.07539v1  [cs.CV]  14 Aug 2024\n2 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\ninto the same space to improve the fusion performance [19].\nThe first challenging point, cross-modal fusion , makes it\npossible to refer to the mutual information between an image\nand a language expression. Several fusion approaches have\nbeen proposed for this task. The fusion approaches can be cate-\ngorized into late fusion and early fusion. As shown in Fig. 1(a),\nmost typical late fusion approach [1], [2], [20] leverages the\ndecoder architectures that fuse the final features extracted\nfrom the vision and language encoders. This approach, which\ndepends only on the decoder architectures for the cross-modal\nfusion, cannot effectively utilize the rich encoder information.\nRecently, the early fusion approach [3], [4], [21] performs the\nfusion using the vision features as a query and the language\nfeatures as a key-value in the intermediate stages of the vision\nencoder. We call this early fusion approach as vision-only\nearly fusion. As illustrated in Fig. 1 (b-1), previous early\nfusion methods [3], [21] perform the vision-only early fusion\nby passing the final language features to each vision stage.\nAnother vision-only early fusion method [4] provides the\nintermediate language encoder features to each vision stage, as\ndescribed in Fig. 1 (b-2). By achieving superior improvements,\nthese early fusion methods demonstrate that the vision-only\nearly fusion approach is more effective than the late fusion\napproach for the cross-modal interactions. However, due to\nunidirectional early fusion, the language encoder cannot uti-\nlize the visual information. Therefore, the vision-only early\nfusion approach [3], [4] has a limitation in interpreting the\nunrestricted language expressions into the appropriate meaning\nthat considers the context of the image in this task.\nThe second challenging point, alignment, is to embed the\ncross-modal features of each encoder into the same embed-\nding space for effective vision-language feature fusion. To\nperform cross-modal alignment, vision-language pretraining\ntasks [22]–[24] use the text-to-image contrastive learning that\nembeds positive pairs closely and negative pairs distantly. Spe-\ncially, [24] demonstrates that aligning the cross-modal features\nbefore performing the fusion is beneficial for the fusion. To\napply the contrastive learning to the pixel-level prediction\ntask, the text-to-pixel contrastive learning [2] based on the\nconventional text-to-image method is employed to promote\nthe relation between two modalities at the pixel-level. This\nscheme has improved the performance of the vision-language\nalignment in this task. However, these approaches use only\nthe final features of the vision and language encoders or the\noutput logits to align the cross-modal features. Therefore, the\nintermediate features of the vision and language encoders,\nwhich have rich information where low-level features contain\nthe structural (or syntactic) information and high-level features\ncontain the semantic global information [25], [26], cannot be\nsufficiently aligned to the joint embedding space.\nThis paper proposes a novel architecture for referring image\nsegmentation, Cross-aware early fusion with stage-divided Vi-\nsion and Language Transformer encoders (CrossVLT), which\nmutually enhances the robustness of each encoder by extend-\ning the early fusion approach to a language encoder as well\nas a vision encoder. Typically, the vision encoder consists of\nthe multiple stages that are separated by the resolution of the\nfeature map; such a structure, which is divided into multiple\nstages based on specific criteria (e.g., pooling the feature\nresolution), is called a stage-divided structure in this paper.\nDifferent from the vision encoders, the language encoders are\nnot commonly designed with the stage-divided structure. To\nenable to bidirectionally exchange the cross-modal information\nof two encoders at each stage, our CrossVLT is designed with\nthe stage-divided language and vision encoders as shown in\nFig. 1 (c). Unlike previous methods [3], [4], our language en-\ncoder considering the visual contexts provides more contextual\nlanguage features to the vision stages. Our vision and language\nencoders can exchange richer information at each stage and\njointly improve their ability to understand the context of im-\nages and expressions by considering each other’s perspectives.\nTherefore, our method advances the ability of the cross-modal\ncontext modeling to deal with the complicated images and\nthe ambiguous language expressions. In addition, unlike the\nconventional scheme that relies solely on the final features\nfor the cross-modal alignment, we introduce a feature-based\nalignment scheme where the low-level to high-level features\nparticipate in the cross-modal alignment. Prior to performing\nthe fusion, our scheme can sufficiently align the intermediate\ncross-modal features to the joint embedding space, leading to\na more effective fusion at each stage. We also employ the\nalignment loss to circumvent the conflict with a task loss.\nOur scheme better allows the vision and language encoders\nto learn the cross-modal correlation by improving the ability\nof aligning the cross-modal features.\nWe demonstrate the effectiveness of the proposed method by\nachieving the competitive performance on three public datasets\nfor referring image segmentation. In addition, we empirically\ndivide the stages of the language encoder based on the optimal\nposition where the cross-modal alignment and cross-aware\nfusion with the vision features of the corresponding vision\nstage can be effectively performed. Our contributions are\nsummarized as follows.\n1) We propose a novel network for referring image seg-\nmentation, CrossVLT, which leverages the stage-divided\nvision and language transformer encoders to perform\nthe cross-aware early fusion and mutually enhances the\nrobustness of each encoder.\n2) We introduce a feature-based alignment scheme that\nenables the low-level to high-level features of the vision\nand language encoders to engage in the cross-modal\nalignment to improve the ability of aligning the inter-\nmediate cross-modal features. Our scheme leads to a\nmore effective cross-modal fusion at each stage by being\napplied prior to performing the fusion.\n3) Our CrossVLT is simple yet effective, and outperforms\nthe previous state-of-the-art methods on three widely\nused datasets for referring image segmentation.\nII. R ELATED WORKS\nA. Fusion for Referring Image Segmentation\nDifferent from the conventional segmentation task [13]–\n[18] based on fixed categories, referring image segmentation\naims to find the target object according to the unrestricted\nlanguage expressions. In the referring image segmentation\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 3\nFig. 2: Overview of CrossVLT, consisting of the stage-divided vision and language encoders, the feature-based alignment, and\nthe segmentation decoder. At each stage, the vision and language encoders consider each other’s features through cross-aware\nfusion to capture the rich contextual information in each encoder. The feature-based alignment is used to better embed the\nvision and language features into the same space by applying the contrastive learning to the intermediate stages of each encoder.\ntask, various methods [5], [7], [10], [27]–[29] have been\nintroduced to fuse vision and language features. Hu et al.\n[5] and RRN [27] concatenated language features encoded\nby LSTM [30] and vision features encoded by CNNs [31]\nto generate the fused features. CMSA [7] also concatenated\neach feature extracted from vision and language encoders, then\nused the self-attention to captures the long-range dependencies\nbetween visual and linguistic features. CMPC [28] adopted\nthe bilinear fusion to associate spatial regions with correlated\nlinguistic features of the entity and attribute words. To improve\nthe multi-modal interaction towards more important words, Ye\net al. [10] proposed a multi-modal feature encoder that fuses\nthe visual and linguistic features by using a dual convolution\nLSTM framework. Liu et al. [29] used the element-wise\nmultiplication for the fusion.\nRecent methods [1], [2], [20], [32] have used a cross-\nmodal fusion module after extracting features from the uni-\nmodal encoders. VLT [1] adopted a transformer encoder-\ndecoder structure that extracts vision features in the encoder\nand fuses vision and language features in the transformer\ndecoder. SeqTR [32] also utilized the transformer encoder-\ndecoder structure after the feature extraction and fusion. Re-\nSTR [20] used transformer-based feature extractor and the\nvisual-linguistic transformer encoder. CRIS [2] extracted the\nencoder features from the image encoder and language encoder\nof CLIP [22] and fused them in a vision-language decoder.\nFor better cross-modal fusion, recent methods [3], [4], [21]\nperformed the early fusion in the vision encoder stages instead\nof fusing after feature extraction. EFN [21] conducted the\ncross-attention using linguistic features as a key-value in the\nintermediate stages of the vision encoder. LA VT [3] also\nperformed the language-aware visual attention using the final\nlanguage features of BERT [33] as a key-value on all stages\nof Swin [34]. PLV [4] provided the intermediate language\nfeatures to the vision stages for the language-aware visual\nattention. These studies have demonstrated the effectiveness of\nthe vision-only early fusion, but the enhancement of the lan-\nguage encoder was not considered. Unlike previous methods,\nwe propose the cross-aware early fusion with stage-divided\nvision and language transformer encoders. In our method, both\nvision and language encoders jointly perform the early fusion\nto better understand expressions for accurate segmentation.\nB. Alignment for Vision-Language Task\nContrastive learning, which exploits positive pairs closely\nand negative pairs distantly, has been effectively used in\nvision-language tasks for alignment. In vision-language pre-\ntraining tasks, CLIP [22] and ALIGN [23] applied the vision-\nlanguage contrastive loss for cross-modal matching using\nmassive web data that consists of image-text pairs. In addition,\nALBEF [24] proposed the align-before fusion framework that\napplies the image-text contrastive loss to the cross-modal fea-\ntures of the final encoder layers before fusing them. The align-\nbefore fusion framework demonstrated that the cross-modal\nfeatures alignment is enhanced by considering the intermediate\nvision-language features. In referring image segmentation,\nCRIS [2] performed text-to-pixel contrastive learning using the\nknowledge distillation from CLIP [22] that applies the text-to-\nimage contrastive learning.\nDifferent from previous methods, we propose the feature-\nbased alignment using contrastive loss based on the features\nof the intermediate stages in each encoder. It leverages the\ntext-to-pixel contrastive learning and the align-before fusion.\nHowever, to the best of our knowledge, this is the first attempt\nthat considers the features of the intermediate stages for\ncross-modal alignment in referring image segmentation. We\nalso explored the appropriate design of the loss that can be\napplied to the intermediate layers for better alignment without\nconflicting with the task loss.\nIII. METHODOLOGY\nAs illustrated in Fig. 2, we introduce CrossVLT, which is\ndesigned to mutually enhance the robustness of vision and\n4 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nlanguage encoders and improve the ability of cross-modal\nfeature alignments. First, from low-level to high-level stages,\neach stage of the two encoders extracts vision and language\nfeatures, and these features are effectively mapped to the\ncross-modal embedding space by performing feature-based\nalignment in the intermediate layers of the two encoders.\nThen, the cross-aware early fusion is performed to enrich the\ncontextual information of each modality. Finally, the simple\nsegmentation decoder utilizes vision encoder features from\neach fusion block to generate a output mask for a target object.\nA. Stage-divided Vision and Language Encoders\nCrossVLT is designed with a stage-divided language en-\ncoder as well as a stage-divided vision encoder to mutually\nenhance the robustness of both encoders by jointly performing\nearly fusion. Given an image-text pair, the vision stage takes an\nimage as an input and the language stage takes a language ex-\npression as an input. Each stage is indexed as i = 1, 2, ··· , n.\nStage-divided vision encoder. For the dense prediction\ntask, we adopt a Swin transformer [34] organized into four\nstages to capture long-range visual dependencies and extract\nhierarchical features defined as Vi ∈ RHiWi×Ci . We append\na vision query fusion layer as the last layer of each vision\nstage, which consists of the multi-head cross-attention and\nfeed-forward block for fusion with language features. Note\nthat Hi,Wi, and Ci denote the height, width, and channel\ndimension of the feature maps at the ith vision stage.\nStage-divided language encoder.Unlike typical language\nencoders [33], [35] without dividing stages, we newly de-\nsign a stage-divided language encoder based on the BERT-\nbase model [33], which is a representative transformer-based\nlanguage encoder, to jointly perform cross-aware early fusion\nwith the vision encoder. The stage-divided language encoder\nis divided into four stages to correspond to each stage of\nthe vision encoder. Unlike the vision encoder, where stages\nare distinctly divided according to resolution, the language\nencoder has no standard to divide stages clearly. Empirically,\nthe number of layers in each stage is set to [6,2,2,2], which\ncan sufficiently extract the linguistic information at low-level.\nFor more details, please refer to Table IV. In the 2nd to 4th\nlanguage stages, we replace self-attention mechanism of the\nfirst encoder layer with the language query cross-attention\nmechanism for fusion with vision features. Each language\nstage extracts the language features Li ∈ RT×D, where T\nand D denote the length of the expression and the feature\ndimension. The first token of the language features is a special\n[CLS] token that is encoded to include representative infor-\nmation of all language tokens for understanding at sentence-\nlevel. The [CLS] token of Li is defined as CLSi ∈ R1×D.\nB. Cross-aware Early Fusion\nThe cross-aware fusion block fuses the vision and language\nfeatures by traversing through each stage of both encoders al-\nternately to extract robust features considering the perspective\nof the other modality. The contextual information of the vision\nfeatures is enriched by referring to the linguistic information\nrelevant to each pixel of the vision feature maps. As presented\nFig. 3: (a) The cross-aware fusion block fuses the cross-\nmodal information bidirectionally. (b) The vision query fusion\nlayer consists of two cross attentions with downsampling to\nconsider language-aware multi-scale vision features. (c) The\nfusion layer using language features as a query.\nin Fig. 3, the vision query fusion layer of the ith vision\nstage takes the language features Li and the vision features\nVi as inputs. The fusion layer performs the multi-head cross\nattention using vision features as queries and language features\nas key-value pairs. Then, the feed-forward block extracts the\nlanguage-aware vision features Mi ∈ RHi Wi ×Ci . Specifically,\nthe fusion process of the vision stage is described as follows.\nMHCA(Q, K , V ) = Softmax(Q · K T /\np\ndk) · V , (1)\ncMi = MHCA(Vi, Li) +Vi , Mi = FFN (cMi) +Vi , (2)\nDi = Down(Mi) , bFi\nV = MHCA(Di, Li) +Di , (3)\nFi\nV = FFN (bFi\nV ) +Di , (4)\nwhere Q, K , V and dk denote queries, keys, values and\ndimensions of keys. MHCA(·) and FFN (·) indicate the multi-\nhead cross attention and the feed-forward block. Down(·)\ndenotes 1\n2 ×downsampling. The intermediate vision features\ncMi ∈ RHi×Wi×Ci are used for the segmentation decoder\nand the language-aware vision features Mi are downsam-\npled except for the final stage. The downsampled vision\nfeatures evolve into the language-aware vision features Fi\nV ∈\nRHi+1Wi+1×Ci+1 through the multi-head cross attention and\nfeed-forward. Then, Fi\nV is passed to the next vision stage and\nthe language query fusion layer of the next language stage.\nAfterward, the language query fusion layer of the (i + 1)th\nlanguage stage takes the vision features Fi\nV and language\nfeatures Li as inputs. The multi-head cross attention is per-\nformed in this fusion layer, which uses the language features\nas queries and vision features as key-value pairs to understand\nthe linguistic meanings from visual perspectives. Then, the\nvision-aware language features Fi\nL ∈ RT×D are extracted by\nthe feed-forward block and residual connections. The fusion\nprocess of the language stage is described as follows:\nbFi\nL = MHCA(Li, Fi\nV ) +Li , Fi\nL = FFN (bFi\nL) +bFi\nL , (5)\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 5\nFig. 4: The structure of the conventional alignment and our\nfeature-based alignment. (a) The final features are solely\nresponsible for aligning the vision and language features. (b)\nThe low-level to high-level features engage in the alignment\nfor a more comprehensive alignment of the intermediate cross-\nmodal features.\nwhere bFi\nL denotes the intermediate language features. The\nvision-aware language features Fi\nL are passed to the subse-\nquent language layers.\nC. Feature-based Alignment\nIn most previous methods [2], [22], [24], the final features\nof the vision and language encoders are solely responsible\nfor the cross-modal feature alignment as illustrated in Fig.\n4. Unlike these methods, our feature-based alignment scheme\nengages the low-level to high-level features in the cross-modal\nalignment to more effectively embed the intermediate features\nof the vision and language encoders into the cross-modal\nembedding space. We now explain the position where the\nfeatures are aligned for the effective cross-modal fusion and\ndescribe how to compute the alignment loss.\nFeature alignment position.The position of the alignment\nis in front of the cross-aware fusion block at each stage.\nThat is, the vision features Vi and language features Li are\nadopted to conduct the feature-based alignment. This posi-\ntioning makes it better for the vision and language encoders\nto learn the cross-modal correlation by grounding the vision\nand language features used for the fusion.\nFeature alignment loss.We use the text-to-pixel contrastive\nloss as an alignment loss for this pixel-level prediction task. As\nillustrated in Fig. 5, the vision features Vi and [CLS] token of\nlanguage features CLSi are used to compute the text-to-pixel\ncontrastive loss Lalign. Vision and language linear projections\ntransform Vi and CLSi into the same feature dimension D\n′\n,\nrespectively. The transformed vision features zi\nV ∈ RHi Wi ×D\n′\nand the transformed language features zi\nL ∈ R1×D\n′\nare used\nto obtain a similarity map. The feature alignment loss is\ncalculated as follows:\nLij\nalign =\n(\n−log(σ(Sim(zij\nV , zi\nL)/τi)) j ∈ Z+\n−log(1 − σ(Sim(zij\nV , zi\nL)/τi)) j ∈ Z−, (6)\nLalign = 1\n|Z|\nnX\ni=1\nX\nj∈Z\nLij\nalign , (7)\nFig. 5: The scheme of the alignment loss using vision feature\ntokens and a language [CLS] token to embed the cross-modal\nfeatures into the same space.\nwhere Z, Z+ and Z− denote the set of pixels, relevant pixels\n(positive) and irrelevant pixels (negative) for language expres-\nsion, Sim is a cosine similarity, τ is learnable temperature\nparameters, and σ is a sigmoid function. We leveraged a\nsigmoid function with learnable parameters for loss calculation\nto circumvent the conflict with a task loss. Lalign optimizes\nthe networks so that relevant pixels are embedded close to the\nlanguage features and irrelevant pixels are embedded far apart.\nD. Segmentation Decoder\nThe segmentation decoder is designed with a simple struc-\nture, where three decoder blocks are stacked to verify the\neffectiveness of our encoders. The decoder block consists of\ntwo layers stacked using 3×3 convolution, batch normalization\nand a ReLU function. The decoder features are upsampled\nusing the bilinear interpolation, and fed into the next decoder\nblock after concatenating them with the vision features cMi\nextracted by vision query cross attention module of the ith\nvision stage. The final prediction mask is projected into a\nbinary class mask by conducting a 1 × 1 convolution. We\nuse the binary cross-entropy loss Ltask for network training.\nThus, the final loss function is as follows.\nLtask = − 1\nN\nNX\nj=1\n[yjlog(pj) + (1− yj)log(1 − pj)] , (8)\nLtotal = Ltask + λ · Lalign , (9)\nwhere N, yj and pj denote the total number of the pixels, the\ntruth value and the predicted probability for the jth pixel.\nIV. E XPERIMENTS\nA. Datasets\nRefCOCO & RefCOCO+. RefCOCO [42] and RefCOCO+\n[42] are widely used datasets for referring segmentation and\nwere collected from MSCOCO dataset. RefCOCO contains\n19,994 images with 142,209 language expressions for 50,000\nobjects, and RefCOCO+ contains 19,992 images with 141,564\nexpressions for 49,856 objects. Each expression in RefCOCO\nand RefCOCO+ contains 3.5 words on average and each\n6 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nTABLE I: Performance comparison with previous state-of-the-art methods using oIoU (%). Models used the transformer\nstructure have a ✓ mark . † indicates that the code or pretrained model is not available. The best results are in bold, and the\nsecond-best results are underlined .\nMethod Backbone Transformer RefCOCO RefCOCO+ G-Ref\nblock val test A test B val test A test B val test\nRRN [27] ResNet101 55.33 57.26 53.93 39.75 42.15 36.11 - -\nCMSA [7] ResNet101 58.32 60.61 55.09 43.76 47.60 37.89 - -\nMAttNet [36] ResNet101 56.51 62.37 51.70 46.67 52.39 40.08 47.64 48.61\nBRINet † [37] ResNet101 60.98 62.99 59.21 48.17 52.32 42.11 - -\nCMPC [28] ResNet101 61.36 64.53 59.64 49.56 53.44 43.23 - -\nLSCM † [38] ResNet101 61.47 64.99 59.55 49.34 53.12 43.50 - -\nMCN [39] DarkNet53 62.44 64.20 59.71 50.62 54.99 44.69 49.22 49.40\nEFN † [21] ResNet101 62.76 65.69 59.67 51.50 55.24 43.01 - -\nBUSNet † [40] ResNet101 63.27 66.41 61.39 51.76 56.87 44.13 - -\nMRLN [12] DarkNet53 63.62 65.57 60.50 53.59 56.78 47.15 51.23 51.02\nISFP † [29] DarkNet53 65.19 68.45 62.73 52.70 56.77 46.39 52.67 53.00\nLTS † [41] DarkNet53 ✓ 65.43 67.76 63.08 54.21 58.32 48.02 54.40 54.25\nSeqTR [32] DarkNet53 ✓ 67.26 69.79 64.12 54.14 58.93 48.19 55.67 55.64\nVLT [1] DarkNet53 ✓ 65.65 68.29 62.73 55.50 59.20 49.36 52.99 56.65\nReSTR † [20] ViT-B ✓ 67.22 69.30 64.45 55.78 60.44 48.27 - -\nCRIS † [2] CLIP-R101 ✓ 70.47 73.18 66.10 62.27 68.08 53.60 59.87 60.36\nLA VT [3] Swin-B ✓ 72.73 75.82 68.79 62.14 68.38 55.10 61.24 62.09\nCrossVLT (Ours) ResNet101 ✓ 70.03 73.83 64.88 59.42 64.83 49.69 59.20 60.15\nCrossVLT (Ours) Swin-B ✓ 73.44 76.16 70.15 63.60 69.10 55.23 62.68 63.75\nimages contains 3.9 objects of the same category on average.\nExpressions in RefCOCO+ do not contain words about abso-\nlute locations, and thus, it is more challenging than RefCOCO.\nG-Ref. G-Ref [43] is also widely used for referring seg-\nmentation, which was collected from Amazon Mechanical\nTurk. G-Ref contains 26,711 images with 104,560 expressions\nfor 54,822 objects. Compared to RefCOCO and RefCOCO+,\nG-Ref has more complex expressions containing 8.4 words\non average, and thus, it is a more challenging dataset than\nRefCOCO and RefCOCO+.\nB. Implementation Details\nExperimental settings. Our method was implemented in\nPyTorch [44]. The vision encoder was initialized with Swin-\nB [34] pretrained on ImageNet, and the language encoder\nwas initialized using the official pretrained weights of BERT-\nbase [33] (uncased version). The segmentation decoder was\nrandomly initialized. We trained the model for 40 epochs\nwith a batch size of 16 on RTX 3090 GPU. We used the\nAdamW optimizer with the learning rate of 3e-4 and adopted\nthe polynomial learning rate decay scheduler. Input images\nwere resized to 480 × 480 pixel resolution. The maximum\nsentence length was set to 21 words including the [CLS]\ntoken for three datasets. During the inference, post-processing\noperations were not applied.\nEvaluation metrics . Following previous studies, we\nused the overall intersection-over-union (oIoU), the mean\nintersection-over-union (mIoU), and precision at 0.5, 0.7,\nand 0.9 thresholds. The oIoU is the ratio between the total\nintersection regions and total union regions of all test sets.\nThe mIoU is the average value of IoUs between the prediction\nTABLE II: Main ablation study on RefCOCO val. set. Align:\nFeature-based alignment. Fusion: Cross-aware early fusion.\nAlign Fusion P@0.5 P@0.7 P@0.9 mIoU oIoU\n81.61 72.48 33.70 72.31 70.77\n✓ 84.44 75.77 34.16 74.04 72.06\n✓ 85.36 76.59 35.79 75.27 73.17\n✓ ✓ 85.82 77.49 35.97 75.48 73.44\nmask and the ground truth of all test sets. The precision is the\npercentage of test sets that have an IoU score higher than the\nthreshold.\nC. Comparison with the State-of-the-Art\nIn Table I, we evaluated our CrossVLT with previous state-\nof-the-art methods on three widely used datasets for referring\nsegmentation using the oIoU metric. Our method surpassed\nother previous methods on all evaluation splits of all datasets.\nCompared to the state-of-the-art LA VT on the RefCOCO,\nCrossVLT had improved performance by 0.71%, 0.34%, and\n1.36% on each split, respectively. Further, our model outper-\nformed other state-of-the-art methods on the more challenging\nRefCOCO+. In addition, on the G-Ref that contains the most\nchallenging data pairs, our CrossVLT achieved performance\nimprovements by 1.44% and 1.66% on the validation and test\nsets, respectively. These improvements indicate that CrossVLT\neffectively aligns and fuses the cross-modal features, and better\nunderstand the complex linguistic expressions and images with\ndense objects than the previous methods.\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 7\nFig. 6: Comparison of t-SNE results with the late fusion\napproach. Red: Language [CLS] token. Blue: Relevant pixel\nwith language expression. Green: Irrelevant pixel.\nD. Ablation Study\nTo verify the effectiveness of the main components in our\nmethod, we conducted extensive experiments as follows.\nCross-aware early fusion and Feature-based alignment.\nWe analyze the effectiveness of the cross-aware early fusion\nand the feature-based alignment. In this experiment, the base-\nline model performs the cross-modal fusion only at the last\nencoder stage. As displayed in Table II, our cross-aware early\nfusion improves the baseline by 2.96% and 2.4% in mIoU and\noIoU scores, respectively. In addition, our alignment scheme\nimproves the baseline by 1.73% and 1.29% in mIoU and oIoU\nscores, respectively. Modeling the cross-aware early fusion and\nfeature-based alignment together shows the best performance.\nThese results demonstrate that the cross-aware early fusion and\nfeature-based alignment are effective for modeling the cross-\nmodal context aggregation in referring image segmentation.\nIn Fig. 6, we visualized t-SNE results of our CrossVLT com-\npared to the late fusion approach. The t-SNE results exhibit\nthe distribution of relevant pixel tokens and irrelevant pixel\ntokens with language [CLS] token in cross-modal embedding\nspace. In t-SNE of our CrossVLT, the relevant pixel tokens are\nembedded closer to the language token than in the late fusion\nresults by learning the cross-modal interactions. Therefore, this\nresult visually demonstrates the effectiveness of our approach.\nIn Table III (a), we experimented with applying the cross-\naware early fusion to each encoder stage incrementally. Apply-\ning the cross-aware early fusion on all stage led to significant\nimprovements of 2.96% and 2.4% in mIoU and oIoU scores.\nThese results indicate that cross-aware early fusion with the\nstage-divided both encoders enables the exchange of abundant\ninformation between the vision and language encoders. In\nTable III (b), we experimented with applying the feature-based\nalignment to each encoder stage incrementally. Applying the\nfeature-based alignment on all stages also improved perfor-\nmance by 0.79% and 0.48% in mIoU and oIoU scores. These\nresults indicate that our scheme helps to align the intermediate\ncross-modal features for effective fusion.\nImportance of applying the alignment and fusion to-\ngether in every stage. In Table III (c), we evaluated the\neffectiveness of applying the alignment and fusion together\nin every stage. The performance improved as the number of\nTABLE III: Ablation studies on the validation set of Ref-\nCOCO. ‘Bi’ and ‘Uni’ indicate bidirectional and unidirectional\nfusion, respectively. (w/) : with the feature-based alignment.\n(w/o) : without the feature-based alignment.\nP@0.5 P@0.7 P@0.9 mIoU oIoU\n(a) Stages that applied only cross-aware early fusion\n[4] 81.61 72.48 33.70 72.31 70.77\n[3, 4] 84.47 75.17 34.89 74.25 72.29\n[2, 3, 4] 84.99 76.12 35.51 74.93 72.83\n[1, 2, 3, 4] 85.36 76.59 35.79 75.27 73.17\n(b) Stages that applied only feature-based alignment\n[4] 83.00 73.35 33.97 73.25 71.58\n[3, 4] 83.22 73.56 33.76 73.49 71.69\n[2, 3, 4] 83.57 74.79 33.90 73.62 71.84\n[1, 2, 3, 4] 84.44 75.77 34.16 74.04 72.06\n(c) Stages that applied alignment and fusion together\n[4] 83.00 73.35 33.97 73.25 71.58\n[3, 4] 84.84 76.56 35.68 74.61 72.42\n[2, 3, 4] 85.37 76.90 35.92 74.99 72.82\n[1, 2, 3, 4] 85.82 77.49 35.97 75.48 73.44\n(d) Effectiveness of the cross-aware fusion\nUni (w/o) 84.96 76.44 35.31 74.84 72.82\nUni (w/) 85.45 77.19 35.93 75.05 73.03\nBi (w/o) 85.36 76.59 35.76 75.28 73.17\nBi (w/) 85.82 77.49 35.97 75.48 73.44\n(e) Applying different loss to all intermediate encoder stages\nAuxiliary loss 84.89 76.36 35.66 74.96 72.62\nAlignment loss 85.82 77.49 35.97 75.48 73.44\nTABLE IV: Comparison of the performance on the number\nof layers in each stage of the language encoder. Models are\ntrained without pretrained weights.\nNumber of layers P@0.5 P@0.7 P@0.9 mIoU oIoU\n[2, 2, 2, 6] 84.24 75.56 34.63 74.04 72.03\n[3, 3, 3, 3] 84.53 76.17 35.51 74.39 72.09\n[6, 2, 2, 2] 84.91 77.09 36.43 74.90 72.52\n[8, 2, 1, 1] 84.46 76.43 36.25 74.50 72.19\nstages to which alignment and fusion are applied increased.\nThat is, the best performance occurred when they are applied\nat all stages. Compared with the setting of [4], the mIoU and\noIoU scores at the setting of [1, 2, 3, 4] increased by 2.23%\nand 1.86%, respectively. We take this as evidence that applying\nthe fusion and alignment together at every stage is effective\nby considering the low-level to high-level information of the\ncross-modalities.\nImportance of the cross-aware fusion.In Table III (d), all\nmodels performed the early fusion to exclude the effectiveness\nof the early fusion and to purely verify the effectiveness of the\ncross-aware fusion. The ‘Uni’ fusion ( i.e. vision-only early\nfusion), where only vision stages perform the early fusion\nwith the intermediate features of the language stage, showed\nthe degraded performance compared with the ‘Bi’ fusion\nmodels (i.e. cross-aware early fusion). The results indicate that\nthe cross-aware fusion enables to capture the rich contextual\n8 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nTABLE V: Performance comparison under fair conditions\nusing the same backbone and language encoder.\nMethod P@0.5 P@0.7 P@0.9 mIoU oIoU\nEFN [21] 82.55 73.27 31.68 72.95 70.76\nVLT [1] 83.24 72.81 24.64 71.98 70.89\nLA VT [3] 84.46 75.28 34.30 74.46 72.73\nOurs 85.82 77.49 35.97 75.48 73.44\nTABLE VI: Ablation study of using different baseline methods\non RefCOCO val. set. All experiments use Swin-B as the\nvisual backbone and BERT-base as the language encoder.\nMethod P@0.5 P@0.7 P@0.9 mIoU oIoU\nVLT [1] 83.24 72.81 24.64 71.98 70.89\nOurs + VLT [1] 85.73 77.81 35.56 75.33 73.60\nCGFormer [45] 87.23 78.69 38.77 76.93 74.75\nOurs + CGFormer [45] 88.17 79.90 39.88 77.75 75.63\ninformation by considering each other’s perspectives.\nSuitability of alignment loss.We experimented on replac-\ning our loss with common segmentation auxiliary loss to help\nuncover the role of alignment loss. In Table III (e), apply-\ning alignment loss perform much better, whereas applying\ncommon auxiliary loss on all stages rather causes conflict in\nlearning the encoder features and degrades performance. This\nindicates that the design of our alignment loss is more suitable\nfor applying to intermediate encoder features and beneficial for\nthe cross-modal alignments.\nOptimal settings for the stage-divided language encoder.\nIn the language encoder, determining the criteria for dividing\nthe stages is unclear. We empirically found the optimal hyper\nparameters for designing an effective stage-divided language\nencoder. We experimented with the number of layers in each\nstage by limiting the total number of layers in the language\nencoder to 12 based on the BERT-base model [33]. As shown\nin Table IV, the setting [6, 2, 2, 2] performed the best under\nthe same conditions without using pretrained weights. The\nsetting [2, 2, 2, 6] cannot fully extract the low-level linguistic\ninformation. The setting [8, 2, 1, 1] is unsuitable for utilizing\nthe high-level semantic information. Therefore, we adopted the\noptimal setting [6, 2, 2, 2].\nFair comparison using the same backbone.In Table V,\nwe compared the proposed model with the other state-of-the-\nart models fairly, using the same vision and language encoders\non all models. Swin-B [34] and BERT-base [33] were used as\nthe vision and language encoders, respectively. Our CrossVLT\nsurpassed the previous models under the same conditions.\nThis result demonstrates that the performance improvement of\nCrossVLT was not the effect of the particular backbone and\nlanguage encoder (i.e., Swin-B [34] and BERT-base [33]).\nComparison with different baseline methods.In Table VI,\nwe applied our method to different state-of-the-art methods\n(i.e., VLT and CGFormer) that consist of more complicated\ncross-modal decoder. As shown in Table VI, both models\ncombined with our method showed significant improvements\nin oIoU performance of 2.71% and 0.88% compared to VLT\nFig. 7: Precision-Recall (PR) curves of our model and two\nablation models on RefCOCO validation set.\nFig. 8: Visualizing contextual similarity of all word features\nin each stage.\nand CGFormer, respectively. These results indicate that our\nmethod can be applied to other methods and leads to additional\nperformance gains for other methods.\nPrecision-recall analysis. In Fig. 7, we analyzed the\nprecision-recall (PR) curves of our model and the ablation\nmodels. The area under the PR curve (AUC-PR) summarizes\nthe overall performance of the model across different threshold\nvalues. A higher AUC-PR indicates a better-performing model.\nAs shown in Fig. 7, our full model (red solid) had the highest\nAUC-PR compared to other ablation models. From around 0.7\nrecall, our full model maintained its advantage in precision\nover the alignment ablation model (green dashed).\nE. Visualizations\nVisualizing language features. Fig.8 showed that each\nstage of the language encoder captures different information.\nThe lower stage contains the syntactic information and the\nhigher stage captures long-range relations. Our method ex-\nploits this abundant information of the intermediate features\nfor the cross-modal fusion and alignment.\nQualitative results.In Fig. 9, we present qualitative results\nof our full model and other ablated models ( i.e. without\ncross-aware early fusion, without feature-based alignment and\nwithout both components) to demonstrate the effectiveness of\neach component. As shown in examples of Fig. 9, our full\nmodel segmented the target regions more elaborately than\nthe other ablated models. These qualitative results indicate\nthat applying both the feature-based alignment and cross-\naware early fusion is an effective approach for referring image\nsegmentation.\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 9\nFig. 9: Visualization predictions of our full model and other ablated models on the test set of RefCOCO. Basic model represents\na ablated model removed the cross-aware fusion and feature-based alignment.\nFig. 10: Qualitative results compared with the proposed and previous state-of-the-art methods on different types of images and\nlanguage expressions. To understand the expression of (c), the back number of the target object is “200999”.\nIn Fig. 10, we visualized the segmentation results of our\nmethod and the two state-of-the-art methods: the late fusion\nmodel [1] and the vision-only early fusion model [3]. For\nFig. 10 (a) and (b), which include the complex expressions\nand complicated images with dense objects, our method high-\nlighted the target regions more accurately than the previous\nmodels. For the other four examples that include ambigu-\nous expressions, our method correctly segmented the targets\ncorresponding to the expressions, whereas other methods\nincorrectly predicted the objects and uncertainly segmented\nthe regions. These results indicate that CrossVLT can better\nunderstand the context of images and expressions containing\ncomplicated relationships.\nIn Fig. 11, given the different expressions that describe the\nsame target, we consistently predicted the target, whereas the\nother method inconsistently predicted the object. These results\nverify that our method enables dealing with various language\nexpressions by enhancing the robustness of both encoders.\nThe Robustness of CrossVLT.As shown in Fig. 12 (a),\nwhen previous methods encountered language expressions\nwith typos (e.g. “seond” and “blu”), they are confused in\nunderstanding the meaning of expressions. Additionally, as\nshown in Fig. 12 (b), informal language expressions (e.g.\n“hitta” and “pic”) also made it difficult for the network to\ncapture the context of the expressions in previous methods.\nGiven these challenging types of expressions, our CrossVLT,\nunlike other state-of-the-art models, correctly determined the\ntarget regions by referring to the visual perspectives in under-\nstanding the meaning of the expressions. These results indicate\nthat our method enhances the robustness of each encoder and\nthe ability of understanding the contexts.\nV. C ONCLUSION\nThis paper proposed a novel network for referring image\nsegmentation, Cross-aware Early Fusion with Stage-divided\nVision and Language Transformer Encoders (CrossVLT),\nwhich leverages cross-modal features alternately traversing\nthrough each stage of the two transformer encoders to mutually\nenhance the robustness of each encoder. We also introduced\nthe feature-based alignment scheme that performs contrastive\n10 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nFig. 11: Visualization two examples of our method and the previous state-of-the-art method (LA VT [3]) on various language\nexpressions that describe the same object in the image.\nFig. 12: Visualizations of our method and the previous state-of-the-art methods on challenging types of linguistic expressions.\nlearning using the features of the intermediate levels in each\nencoder to enhance the capability of aligning the cross-modal\nfeatures. Experiments demonstrated the effectiveness of our\nmethod on three public datasets. We hope our method can\nmotivate further research for various multi-modal tasks and the\nfeature-based alignment scheme can be applied by designing\nthe alignment loss to suit their tasks.\nACKNOWLEDGMENTS\nThis research was supported by by Samsung Electron-\nics under Grant IO201218-08232-01, the MSIT (Ministry\nof Science and ICT), Korea, under the ITRC (Informa-\ntion Technology Research Center) support program (IITP-\n2023-RS-2023-00260091) supervised by the IITP (Institute\nfor Information & Communications Technology Planning &\nEvaluation), by the National Research Foundation of Korea\n(NRF) grant funded by the Korea government (MSIT) (No.\n2021R1A2C1004208), and by the National Research Founda-\ntion of Korea (NRF) grant funded by the Korea government\n(MSIT) (No. 2020M3H4A1A02084899).\nREFERENCES\n[1] H. Ding, C. Liu, S. Wang, and X. Jiang, “Vision-language transformer\nand query generation for referring segmentation,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision , 2021, pp.\n16 321–16 330.\n[2] Z. Wang, Y . Lu, Q. Li, X. Tao, Y . Guo, M. Gong, and T. Liu, “Cris: Clip-\ndriven referring image segmentation,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n11 686–11 695.\n[3] Z. Yang, J. Wang, Y . Tang, K. Chen, H. Zhao, and P. H. Torr, “Lavt:\nLanguage-aware vision transformer for referring image segmentation,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 18 155–18 165.\n[4] Y . Liao, A. Zhang, Z. Chen, T. Hui, and S. Liu, “Progressive language-\ncustomized visual feature learning for one-stage visual grounding,”IEEE\nTransactions on Image Processing , vol. 31, pp. 4266–4277, 2022.\n[5] R. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural\nlanguage expressions,” in European Conference on Computer Vision .\nSpringer, 2016, pp. 108–124.\n[6] R. Hu and A. Singh, “Unit: Multimodal multitask learning with a unified\ntransformer,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2021, pp. 1439–1449.\n[7] L. Ye, M. Rochan, Z. Liu, and Y . Wang, “Cross-modal self-attention\nnetwork for referring image segmentation,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2019,\npp. 10 502–10 511.\n[8] S. Qiu, Y . Zhao, J. Jiao, Y . Wei, and S. Wei, “Referring image\nsegmentation by generative adversarial learning,” IEEE Transactions on\nMultimedia, vol. 22, no. 5, pp. 1333–1344, 2019.\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 11\n[9] H. Shi, H. Li, Q. Wu, and K. N. Ngan, “Query reconstruction network\nfor referring expression image segmentation,” IEEE Transactions on\nMultimedia, vol. 23, pp. 995–1007, 2020.\n[10] L. Ye, Z. Liu, and Y . Wang, “Dual convolutional lstm network for refer-\nring image segmentation,” IEEE Transactions on Multimedia , vol. 22,\nno. 12, pp. 3224–3235, 2020.\n[11] L. Lin, P. Yan, X. Xu, S. Yang, K. Zeng, and G. Li, “Structured\nattention network for referring image segmentation,” IEEE Transactions\non Multimedia, vol. 24, pp. 1922–1932, 2021.\n[12] G. Hua, M. Liao, S. Tian, Y . Zhang, and W. Zou, “Multiple relational\nlearning network for joint referring expression comprehension and\nsegmentation,” IEEE Transactions on Multimedia , pp. 1–13, 2023.\n[13] B. Shuai, H. Ding, T. Liu, G. Wang, and X. Jiang, “Toward achieving\nrobust low-level and high-level scene parsing,” IEEE Transactions on\nImage Processing, vol. 28, no. 3, pp. 1378–1390, 2018.\n[14] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and\nA. Torralba, “Semantic understanding of scenes through the ade20k\ndataset,” International Journal of Computer Vision , vol. 127, no. 3, pp.\n302–321, 2019.\n[15] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, “Semantic\nsegmentation with context encoding and multi-path decoding,” IEEE\nTransactions on Image Processing , vol. 29, pp. 3520–3533, 2020.\n[16] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efficient design for semantic segmentation\nwith transformers,” Advances in Neural Information Processing Systems,\nvol. 34, pp. 12 077–12 090, 2021.\n[17] M.-H. Guo, C.-Z. Lu, Q. Hou, Z. Liu, M.-M. Cheng, and S.-M.\nHu, “Segnext: Rethinking convolutional attention design for semantic\nsegmentation,” Advances in Neural Information Processing Systems ,\nvol. 35, pp. 1140–1156, 2022.\n[18] J.-h. Shim, H. Yu, K. Kong, and S.-J. Kang, “Feedformer: Revisiting\ntransformer decoder for efficient semantic segmentation,” inProceedings\nof the AAAI Conference on Artificial Intelligence , vol. 37, no. 2, 2023,\npp. 2263–2271.\n[19] J. Yang, J. Duan, S. Tran, Y . Xu, S. Chanda, L. Chen, B. Zeng,\nT. Chilimbi, and J. Huang, “Vision-language pre-training with triple\ncontrastive learning,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 15 671–15 680.\n[20] N. Kim, D. Kim, C. Lan, W. Zeng, and S. Kwak, “Restr: Convolution-\nfree referring image segmentation using transformers,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 18 145–18 154.\n[21] G. Feng, Z. Hu, L. Zhang, and H. Lu, “Encoder fusion network with co-\nattention embedding for referring image segmentation,” in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2021, pp. 15 506–15 515.\n[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nConference on Machine Learning . PMLR, 2021, pp. 8748–8763.\n[23] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.\nSung, Z. Li, and T. Duerig, “Scaling up visual and vision-language\nrepresentation learning with noisy text supervision,” in International\nConference on Machine Learning . PMLR, 2021, pp. 4904–4916.\n[24] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H.\nHoi, “Align before fuse: Vision and language representation learning\nwith momentum distillation,” Advances in neural information processing\nsystems, vol. 34, pp. 9694–9705, 2021.\n[25] I. Tenney, D. Das, and E. Pavlick, “Bert rediscovers the classical nlp\npipeline,” in Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , 2019, pp. 4593–4601.\n[26] M. E. Peters, M. Neumann, L. Zettlemoyer, and W.-t. Yih, “Dissect-\ning contextual word embeddings: Architecture and representation,” in\nProceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, 2018, pp. 1499–1509.\n[27] R. Li, K. Li, Y .-C. Kuo, M. Shu, X. Qi, X. Shen, and J. Jia, “Referring\nimage segmentation via recurrent refinement networks,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2018, pp. 5745–5753.\n[28] S. Huang, T. Hui, S. Liu, G. Li, Y . Wei, J. Han, L. Liu, and B. Li, “Refer-\nring image segmentation via cross-modal progressive comprehension,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 10 488–10 497.\n[29] C. Liu, X. Jiang, and H. Ding, “Instance-specific feature propagation for\nreferring segmentation,” IEEE Transactions on Multimedia , pp. 3657–\n3667, 2022.\n[30] H. Sak, A. W. Senior, and F. Beaufays, “Long short-term memory\nrecurrent neural network architectures for large scale acoustic modeling,”\nin Interspeech, 2014, pp. 338–342.\n[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification\nwith deep convolutional neural networks,” Advances in neural informa-\ntion processing systems , vol. 25, pp. 1097–1105, 2012.\n[32] C. Zhu, Y . Zhou, Y . Shen, G. Luo, X. Pan, M. Lin, C. Chen, L. Cao,\nX. Sun, and R. Ji, “Seqtr: A simple yet universal network for visual\ngrounding,” in Computer Vision–ECCV 2022: 17th European Confer-\nence, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXV .\nSpringer, 2022, pp. 598–615.\n[33] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” in\nProceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 2019, pp. 4171–4186.\n[34] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021, pp. 10 012–10 022.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, pp. 6000—-6010, 2017.\n[36] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L.\nBerg, “Mattnet: Modular attention network for referring expression\ncomprehension,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 1307–1315.\n[37] Z. Hu, G. Feng, J. Sun, L. Zhang, and H. Lu, “Bi-directional relationship\ninferring network for referring image segmentation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 4424–4433.\n[38] T. Hui, S. Liu, S. Huang, G. Li, S. Yu, F. Zhang, and J. Han, “Linguistic\nstructure guided context modeling for referring image segmentation,” in\nEuropean Conference on Computer Vision. Springer, 2020, pp. 59–75.\n[39] G. Luo, Y . Zhou, X. Sun, L. Cao, C. Wu, C. Deng, and R. Ji, “Multi-task\ncollaborative network for joint referring expression comprehension and\nsegmentation,” in Proceedings of the IEEE/CVF Conference on computer\nvision and pattern recognition , 2020, pp. 10 034–10 043.\n[40] S. Yang, M. Xia, G. Li, H.-Y . Zhou, and Y . Yu, “Bottom-up shift\nand reasoning for referring image segmentation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2021, pp. 11 266–11 275.\n[41] Y . Jing, T. Kong, W. Wang, L. Wang, L. Li, and T. Tan, “Locate\nthen segment: A strong pipeline for referring image segmentation,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 9858–9867.\n[42] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, “Modeling\ncontext in referring expressions,” in European Conference on Computer\nVision. Springer, 2016, pp. 69–85.\n[43] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy,\n“Generation and comprehension of unambiguous object descriptions,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 11–20.\n[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” Advances in\nneural information processing systems , vol. 32, pp. 8024–8035, 2019.\n[45] J. Tang, G. Zheng, C. Shi, and S. Yang, “Contrastive grouping with\ntransformer for referring image segmentation,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2023, pp. 23 570–23 580.\n12 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nYubin Cho received the B.S. degree with double\nmajor in Mechanical Engineering and Artificial In-\ntelligence from Sogang University, South Korea, in\n2022. She is currently working toward the M.S.\ndegree with the School of Artificial Intelligence,\nSogang University. Her current research interests\ninclude computer vision, multi-modal learning and\ndeep learning.\nHyunwoo Yu received the B.S. degree in Physics\nfrom Kangwon University, South Korea, in 2021. He\nis currently working toward the Ph.D. degree with\nthe School of Electronic Engineering, Sogang Uni-\nversity, South Korea. His current research interests\ninclude computer vision, multi-modal learning and\npixel-level scene understanding.\nSuk-ju Kang (Member, IEEE) received the B.S.\ndegree in electronic engineering from Sogang Uni-\nversity, Seoul, South Korea, in 2006, and the Ph.D.\ndegree in electrical and computer engineering from\nthe Pohang University of Science and Technology,\nPohang, South Korea, in 2011. From 2011 to 2012,\nhe was a Senior Researcher with LG Display Co.,\nLtd., Seoul, where he was a Project Leader for\nresolution enhancement and multiview 3-D system\nprojects. From 2012 to 2015, he was an Assistant\nProfessor of Electrical Engineering with Dong-A\nUniversity, Busan, South Korea. He is currently a Professor of Electronic\nEngineering with Sogang University, Seoul. His current research interests\ninclude computer vision, image analysis and enhancement, video processing,\nmultimedia signal processing, digital system design, and deep learning. He\nwas a recipient of the IEIE/IEEE Joint Award for Young IT Engineer of the\nYear in 2019 and the Merck Young Scientist Award in 2022. He served as an\nAssociate Editor for IEEE Transactions on Circuits and Systems for Video\nTechnology from 2023.\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 13\nTABLE VII: Precision, mean IoU (%) and overall IoU(%) of the proposed method on three standard benchmark datasets.\nDataset P@0.5 P@0.6 P@0.7 P@0.8 P@0.9 mIoU oIoU\nRefCOCO\nval 85.82 82.80 77.49 67.21 35.97 75.48 73.44\ntest A 88.92 86.25 81.62 69.79 36.68 77.54 76.16\ntest B 81.35 77.74 72.27 62.63 37.64 72.69 70.15\nRefCOCO+\nval 76.41 73.09 68.49 59.09 31.39 67.27 63.60\ntest A 82.43 79.57 74.80 64.65 32.89 72.00 69.10\ntest B 67.19 63.31 58.89 49.93 29.15 60.09 55.23\nG-Ref val 74.75 70.45 64.58 54.21 28.57 66.21 62.68\ntest 71.54 66.38 59.00 48.21 23.10 62.09 63.75\nAPPENDIX\n• In Appendix A, we provide the detailed quantitative\nresults of our model on three public referring image\nsegmentation datasets.\n• In Appendix B, we provide the visualizations of our\nsegmentation results, which are better than the ground\ntruth.\n• In Appendix C, we provide additional visualizations on\nvarious types of language expressions.\nAPPENDIX A\nDETAILED QUANTITATIVE RESULTS\nTable VII shows the detailed precision, mIoU(%) and\noIoU(%) scores of our model on RefCOCO, RefCOCO+, and\nG-Ref datasets to complement the quantitative results in Table\nI of the main paper.\nAPPENDIX B\nVISUALIZATION OF PREDICTIONS BETTER THAN GROUND\nTRUTH\nIn Fig. 13, we visualized our predictions in which the target\nregions were segmented more precisely than the ground truth.\nIn the ground truth of the first and second rows, the annotations\nincluded non-target regions (e.g. part of the table). In the\nground truth of the third and fourth rows, the annotations\nexcluded the target regions (e.g. part of the body). However,\nour predictions included the detailed regions of target objects\naccurately.\nAPPENDIX C\nVISUALIZATION OF ADDITIONAL RESULTS\nFrom Figs. 14 to 17, we visualized the additional qualitative\nresults on various types of language expressions to clearly\nprove the high level of competence in understanding the\ncontext of images and language expressions. As shown in\nFig. 14, our model could understand expressions describing\nrelative locations between the objects (e.g. “Man standing\nbehind the man holding hat”). Compared to the previous\nmethods, our method better processed the attribute words (e.g.\n“frost”, “horizontal” and “empty”) as displayed in Fig. 15.\nIn Fig. 16, our method also recognized colors of the objects\n(e.g. “white area” and “blue part of table”) more accurately\nFig. 13: Visualization of our predictions that include the\ndetailed regions of the target object more accurately than the\nground truth.\nthan other methods. Moreover, in Fig. 17, we visualized more\nexamples on predicting the same target object described by\ndifferent language expressions to complement the qualitative\nresults in Fig. 8 of the main paper. Our model consistently\ndetermined the target object by robustly dealing with different\nlanguage expressions, whereas other previous models dealt\nwith them inconsistently.\n14 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nFig. 14: Qualitative results of the proposed and previous state-of-the-art methods on various language expressions describing\nlocations.\nCHO et al.: CROSS-AW ARE EARLY FUSION WITH STAGE-DIVIDED VISION AND LANGUAGE TRANSFORMER ENCODERS 15\nFig. 15: Qualitative results of the proposed and previous state-of-the-art methods on various language expressions describing\nthe attribute of the objects.\n16 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 26, 2024\nFig. 16: Qualitative results of the proposed and previous state-of-the-art methods on various language expressions containing\ncolors.\nFig. 17: Visualization additional examples of our method and previous state-of-the-art methods on various language expressions\ndescribing the same object in the image. Our method consistently determines the target regions by robustly dealing with various\nexpressions, whereas other methods inconsistently predict the target regions.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8359405994415283
    },
    {
      "name": "Encoder",
      "score": 0.7193880677223206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6096083521842957
    },
    {
      "name": "Computer vision",
      "score": 0.5021419525146484
    },
    {
      "name": "Segmentation",
      "score": 0.45959174633026123
    },
    {
      "name": "Natural language",
      "score": 0.45365846157073975
    },
    {
      "name": "Image segmentation",
      "score": 0.4297046661376953
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.41959455609321594
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34320586919784546
    },
    {
      "name": "Natural language processing",
      "score": 0.32223203778266907
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}